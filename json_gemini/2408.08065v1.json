{"title": "SPEED: SCALABLE PREPROCESSING OF EEG DATA FOR SELF-SUPERVISED LEARNING", "authors": ["Anders Gj\u00f8lbye", "Lina Skerath", "William Lehn-Schi\u00f8ler", "Nicolas Langer", "Lars Kai Hansen"], "abstract": "Electroencephalography (EEG) research typically focuses on tasks\nwith narrowly defined objectives, but recent studies are expanding\ninto the use of unlabeled data within larger models, aiming for a\nbroader range of applications. This addresses a critical challenge in\nEEG research. For example, Kostas et al. (2021) show that self-\nsupervised learning (SSL) outperforms traditional supervised meth-\nods. Given the high noise levels in EEG data, we argue that further\nimprovements are possible with additional preprocessing. Current\npreprocessing methods often fail to efficiently manage the large data\nvolumes required for SSL, due to their lack of optimization, reliance\non subjective manual corrections, and validation processes or inflex-\nible protocols that limit SSL. We propose a Python-based EEG pre-\nprocessing pipeline optimized for self-supervised learning, designed\nto efficiently process large-scale data. This optimization not only\nstabilizes self-supervised training but also enhances performance on\ndownstream tasks compared to training with raw data.", "sections": [{"title": "1. INTRODUCTION", "content": "As the processing of EEG measurements becomes more sophisti-\ncated, the range of applications expands. Much EEG-based mod-\nelling is focused on limited-scale, specific tasks, with fewer large-\nscale projects targeting broader applicability. For limited-scale tasks,\npreprocessing is often a core dimension, making modelling of small\nand noisy datasets possible. Such methods have not yet reached a\nlevel of performance that excels across various tasks like in the case\nof large language models (LLM). Nevertheless, some inspiration is\nstill drawn from LLMs, such as the application of self-supervised\nlearning (SSL) that makes use of unlabeled data. The most promi-\nnent results have been achieved by Kostas et al. [1]. Their BENDR\nmodel shows that while not effective on their own, SSL can improve\ndownstream task performance. The authors claim that deep learning\nis sufficient to learn even from raw data, yet there is an important sci-\nentific question: Does preprocessing help SSL? This is the question\nwe set out to test in the present work.\nCurrent preprocessing methods do not scale to the substantial vol-\nume of data required for SSL. While current pipelines allow for man-\nual correction and validation of data, this makes them subjective and\nchallenges reproducibility. Additionally, the correction process is of-\nten too task-specific for SSL, given the diverse nature of downstream\ntasks. Common preprocessing methods also tend to lead to signifi-\ncant data loss, especially considering that the largest available EEG"}, {"title": "2. BACKGROUND", "content": "EEG data varies among subjects, hardware, and environmental fac-\ntors and is susceptible to noise and artifacts. Therefore, a lot of re-\nsearch in this field focuses on signal cleaning to extract the genuine\nbrain signal. Historically, MATLAB has been the dominant platform\nfor EEG research, leading to the development of numerous frame-\nworks and pipelines for EEG analysis. EEGLAB [3], Brainstorm\n[4] and FieldTrip [5] are some of the most widely used frameworks.\nOther projects, such as the PREP pipeline [6], aim to standardize\nlarger-scale EEG preprocessing, incorporating advanced techniques\nfor removing line noise and performing robust average referencing\nthrough iterative detection and interpolation of artifact-contaminated\nchannels. The Automagic pipeline [7] is designed to aggregate cur-\nrently available preprocessing methods and provide objective stan-\ndardized quality assessment techniques.\nBased on Python's open-source nature and widespread accessibil-\nity, MNE [8] offers a comprehensive suite of tools. Other libraries\nand tools for bad channel detection were developed or adapted from\nMATLAB, such as FASTER [9], Autoreject [10], and the adapted\nPREP pipeline. However, available pipelines cannot handle the mas-\nsive amounts needed for SSL and are not designed for SSL objec-\ntives.\nKostas et al. [1] and Banville et al. [11] find self-supervision to\noutperform or reach competitive performance compared to super-"}, {"title": "3. DATA", "content": ""}, {"title": "3.1. TUH EEG Corpus (TUEG) [2]", "content": "The Temple University Hospital EEG Corpus is a substantial col-\nlection of clinical EEG recordings, consisting of 26,846 recordings\ncollected from 2002 to 2017, taking up 1.6 TB of storage. This\ndataset is the primary data source for developing and testing our self-\nsupervised learning pipeline for EEG data analysis as it is used for\npretraining. It also serves as the primary motivation as it is by far\nthe largest public EEG dataset. Still, there is a vast difference in\nthe recordings in terms of equipment used, task performed, length of\nrecording, and quality of data."}, {"title": "3.2. Motor Movement/Imagery Dataset (MMIDB) [13]", "content": "The MMIDB dataset includes 64-channel EEG recordings from 109\nsubjects who performed motor and imagery tasks related to hand\nmovements. We are excluding participants S088, S090, S092, and\n$100 due to missing data, resulting in 105 participants. We use ses-\nsions 3, 7, and 11 where a target appears on either the left or right\nside of the screen, and the subject opens and closes the correspond-\ning fist. This dataset is used for downstream benchmarking."}, {"title": "3.3. BrainCapture Bhutan Dataset v. 4.1 (BC Bhutan)", "content": "The BC Bhutan dataset includes 27-channel EEG recordings from\n133 subjects across Bhutan. The data was collected in collabora-\ntion with BrainCapture and is used for fine-tuning models. As part\nof routine EEG recordings, the data contain the 5-second annotated\nexercise windows Eye blinking, Eye movement, Eyes closed, Eyes\nopened, and Jaw clenching. This dataset is used for downstream\nbenchmarking."}, {"title": "3.4. BCI Challenge @ NER 2015 (BCI@NER) [14]", "content": "The dataset from the BCI Challenge at NER 2015 includes 56-\nchannel EEG recordings from 16 subjects who participated in 5\nsessions each, totalling 80 sessions. This dataset includes two\nclasses for positive and negative feedback for P300 waves and is\nused for downstream benchmarking."}, {"title": "4. METHODS", "content": "The pipeline is built upon already existing preprocessing compo-\nnents that are implemented and available in various packages in\nPython. This section briefly introduces the methods we use in con-\nsecutive order as shown in Figure 1. The objective of SPEED is to\npreprocess data into a uniform format suitable for machine learning\napplications, ensuring high-quality data with consistency in chan-\nnels and sample counts. We describe the general SPEED pipeline for\nself-supervised pretraining and address any changes for downstream\ndatasets in a subsection afterwards."}, {"title": "4.1. Initial Setup of preprocessing", "content": "The initial steps are simple yet essential. They consist of standard-\nizing channel names, detecting channel types, dropping non-EEG"}, {"title": "4.2. Quality assessment", "content": "The process of quality assessment involves quickly determining\nwhether a window should be kept or discarded. This is not to waste\nunnecessary time on too poor quality windows. We chose to drop\nwindows if there is an insufficient number of channels to interpolate\nthe desired montage, either due to a lack of channels or because they\nare removed for being defective. Here we require at least half of\nthe final number of channels to be present. Another reason is the\nexcessive dropping of ICA components, leaving little of the original\nsignal intact. Lastly, the data may be of inherently poor quality,\neither before or after processing. Quality assessment begins with\na straightforward bad channel detection using the PREP pipeline\n[6] methods (excluding RANSAC), followed by the application of a\nnotch filter to eliminate line noise and a combination of a 1 Hz high-\npass and a 100 Hz low-pass filter. Subsequently, various quality\nmeasures are calculated, largely adopting the criteria recommended\nin the AutoMagic pipeline [7]. These measures include the Ratio\nof Bad Channels (RBC), the Ratio of Data with Overall High Am-\nplitude (OHA), the Ratio of Timepoints with High Variance (THV),\nand the Ratio of Channels with High Variance (CHV). We exclude\nany window where $OHA < 0.8$, $THV < 0.5$, $CHV < 0.5$, and\n$BCR < 0.8$. These thresholds were determined through manual\nreviews of over 100 EEG recordings."}, {"title": "4.3. Iterative Zapline", "content": "Next, we remove line noise using the Iterative Zapline Algorithm\n[15]. The Zapline algorithm is designed to remove power line arti-\nfacts from multichannel electrophysiological data, such as EEG and\nMEG. It combines spectral and spatial filtering techniques to achieve\neffective noise removal while preserving the underlying signals [15].\nThe line noise frequency, usually 50Hz in Afro-Eurasia or 60Hz in\nAmerica, is manually set. The Python implementation MeegKit is\napplied [15]."}, {"title": "4.4. Bad Channel Detection", "content": "Bad channel detection is performed on a copy of the dataset, and\nthe identified bad channels are afterwards removed from the original\ndataset. The objective of this process is to identify channels with low\nsignal-to-noise ratios (SNR). Dropping these channels is essential as\ntheir presence negatively impacts interpolation, average referencing,\nand independent component analysis (ICA).\nThe basis for bad channel detection lies in the Python implemen-\ntation of the PREP pipeline. It initiates the detection by remov-\ning flat/NAN channels and low-frequency trends. It then identi-\nfies channels with extreme amplitudes (deviation criterion), abnor-\nmal high-frequency noise (noisiness criterion), poor correlation with\nother channels (correlation criterion), and weak predictability by\nother channels (predictability criterion). The latter involves assess-\ning the correlation of each channel within a 5s EEG data window\nwith its Random Sample Consensus (RANSAC) [16] reconstruction\n(using 50 samples from a subset of 25% of all channels). A chan-\nnel is considered bad if its correlation is below 0.85 in more than\n40% of instances. Due to the RANSAC algorithm requiring signifi-\ncant resources, its inclusion is optional. The identified bad channels\nare then excluded from the dataset. Originally, the PREP pipeline\nincluded bad channel detection in its robust average referencing pro-\ncess, which iteratively detects and interpolates bad channels to en-\nsure a reliable average reference. However, as we do not wish to"}, {"title": "4.5. Filtering & Average Referencing", "content": "We apply robust detrending, a 0.5 Hz high-pass filter, and a 100 Hz\nlow-pass filter. We use robust detrending because we aim to elim-\ninate linear trends, even though the specific frequency of the high-\npass filter may vary as needed. Lastly, we apply average referencing\nto the remaining high-quality channels [17], [18]."}, {"title": "4.6. Independent Component Analysis", "content": "We proceed to the Extended Infomax Independent Component Anal-\nysis (ICA) [19] transformation, setting the number of components\nequal to the number of remaining channels. We classify the com-\nponents using ICLabel [20] methods and exclude every indepen-\ndent component not categorized as brain or other and with certainty\nabove 80%. The signal is reconstructed from the remaining inde-\npendent components. A further Bad Channel Detection (see Section\n4.4) is conducted on a duplicate of the dataset.\nIndependent Component Analysis (ICA) stands out among vari-\nous methods for isolating and eliminating artifacts and noise from"}, {"title": "4.7. Interpolation & Resample", "content": "Missing channel interpolation is crucial because it enables us to stan-\ndardize EEG data from various sources and leverage information\nfrom additional channels instead of discarding them. We interpolate"}, {"title": "4.8. Downstream Data Preprocessing", "content": "The downstream data preprocessing is done similarly to the pretrain-\ning data. The changes are highlighted in Figure 1 in dotted lines.\nThose parts are omitted from the downstream processing. After the\ninitial setup, the downstream data is not split into 60-second win-\ndows. However, all data before the first and after the last event\nmarker is removed to avoid unclean sections in the signal. All subse-\nquent procedures are carried out on the cropped recording. The qual-\nity assessment is skipped on downstream data because, in datasets\nintended for downstream analysis, the decision to drop a window is\nchallenging, as it could increase the risk of Type I errors [22]. For\nexample, in attempting to detect seizures, which are characterized\nby high-frequency noise, it is undesirable to discard windows with\nhigh variance. While this issue remains relevant in a self-supervised\nlearning context with large-scale data, it becomes less prominent\nsince the specifics of downstream analysis are not predetermined.\nOther parts in the pipeline that require dropping bad windows are\nskipped because the downstream data is processed as one recording\nand not split into windows."}, {"title": "5. EXPERIMENTS", "content": ""}, {"title": "5.1. Preprocessing of the TUH EEG Corpus", "content": "A significant contribution of this paper is the preprocessing of the\nTemple University Hospital EEG Corpus. SPEED is designed for\nmultiprocessing and is executed in parallel across 30 cores on 2 x\nEPYC 7302 and 30 cores on 2 x EPYC 9124, each equipped with\n128GB of RAM, completing the task in under a week. The pre-\nprocessing comprehensively logs each step, allowing for detailed in-\nspection of each data window. This includes information on which\nwindows are dropped, identification of bad channels, channels re-\nmoved for not matching the montage setup, the labels and proba-\nbility scores of independent components from ICLabel, components\nthat are excluded, and extra channels that are dropped."}, {"title": "5.2. Pretraining", "content": "We use the BENDR model from Kostas et al. [1] for our exper-\niments. This model is pretrained over 5 epochs using the prepro-\ncessed TUH EEG Corpus with (1) SPEED, as depicted in Figure 1,\n(2) SPEED w/ ICA, also shown in Figure 1, and (3) Baseline, as\nemployed in Kostas et al. [1], which involves only resampling and\nzeroing out missing channels. For validation, we similarly prepro-\ncess the three smaller downstream datasets (MMIDB, BC Bhutan,\nand BCI@NER) in 60-second windows. The model with the lowest\nvalidation loss from this phase is selected for subsequent probing."}, {"title": "5.3. Probing", "content": "We initiate probing by incorporating the pre-trained encoder layer\nfrom the model, adding linear layers, and locking the pre-trained\nencoder layer. This procedure aims to assess whether the pretrained\nlatent space provides an improved representation of EEG data when\nprocessed using different preprocessing pipelines. The models are\nprobed using the labelled downstream datasets MMIDB, BC Bhutan,\nand BCI@NER, which are preprocessed similarly and segmented\ninto 5-second windows."}, {"title": "6. RESULTS", "content": ""}, {"title": "6.1. Preprocessing", "content": "In Figure 2, we identify which channels are interpolated at the final\nstep due to being initially missing or removed for poor quality."}, {"title": "6.2. UMAP Embedding Analysis", "content": "Figure 4 motivates the application of preprocessing. We use UMAP\nto visualize the embedding of the BrainCapture Bhutan dataset, ob-\ntained without reference to the provided labels. We compare data\npreprocessed with (4a) the Baseline and (4b) the SPEED pipeline.\nSignificant improvements in class separation are noticeable when\nusing the preprocessing approach. In addition to visually inspecting\nthe label clusters, we use Fisher's Linear Discriminant (FLD) score\n[23] to quantify the ratio of between-class variance to within-class\nvariance. With an FLD-score of 0.0403, we find a much stronger\nseparation between (4a) data preprocessed with SPEED, compared\nto (4b) data preprocessed with the baseline pipeline, which has an\nFLD-score of 0.0009 (indicating little to no association)."}, {"title": "6.3. Pretraining", "content": "In Figure 5, the validation contrastive accuracy curves are displayed\nfor the pretraining using three different preprocessed versions of The\nTUH EEG Corpus. Each model increases rapidly during the initial\niterations but experiences a drop in accuracy halfway through the\nfirst epoch. However, the model using the Baseline data takes several\nepochs to reach its peak, while the other models only require about\none epoch. Additionally, the Baseline model achieves a maximum\naccuracy of 89%, whereas the other models reach 92% accuracy.\nLastly, the Baseline model begins to overfit after approximately four\nepochs, while the other models maintain a consistent, slight increase\nin validation accuracy. The use of SPEED and SPEED w/ ICA offers\na more stable pretraining and a final higher contrastive accuracy on\nthe validation datasets."}, {"title": "6.4. Probing of the model", "content": "Table 1 presents the results of probing the model. It clearly shows\nthat both SPEED and SPEED w/ ICA during pretraining enhance\nthe model's downstream accuracy compared to the baseline. Specif-\nically, when compared to Baseline, using SPEED for pretraining, re-\nsults in an accuracy increase of 9% for MMIDB, 7% for BrainCap-\nture Bhutan, and 6% for BCI@NER. However, it is also clear that\nwhile SPEED w/ ICA during pretraining is advantageous compared\nto the Baseline, it does not outperform SPEED. On the MMIDB and\nBC Bhutan datasets SPEED w/ ICA achieves a significantly lower\naccuracy. A plausible explanation for this could be removing muscle\nand eye artifacts during the preprocessing phase. For the BrainCap-\nture Bhutan dataset, these artifacts are directly related to what we"}, {"title": "7. DISCUSSION", "content": "With our pipeline, we address a major challenge of large-scale EEG\ndata preprocessing pipelines. However, a major consideration is\nwhether to incorporate ICA with ICLabel. This technique is well-\nestablished in EEG preprocessing and analysis and is part of the\nAutoMagic [7] pipeline. Our analysis indicates that including ICA\ndoes not significantly improve the pretraining performance. Rather,\nexamining the probing results slightly reduces the downstream ac-\ncuracy when used in pretraining and significantly when applied to\ndownstream datasets. This suggests that the model might either miss\nartifacts in SPEED, perhaps use them in classification or that ICLa-\nbel incorrectly classifies independent components, leading to the re-\nmoval of significant data.\nFor downstream datasets, more careful preprocessing is often re-\nquired, which is not the focus of this paper. These datasets are typ-\nically smaller and of higher quality, making them more suitable for"}, {"title": "8. CONCLUSION", "content": "The 'industry standard' in machine learning is to let complex deep\nlearning models handle massive datasets with limited preprocessing,\nespecially with self-supervised learning. For electroencephalogram\n(EEG) data, Kostas et al. pretrained a complex transformer model on\nthe massive Temple University Hospital (TUH) EEG Corpus with\nrather rudimentary preprocessing. However, the TUH EEG Cor-\npus presents significant challenges due to its variability in signal-\nto-noise, equipment used, length of recording, and more. We in-\ntroduce an efficient preprocessing pipeline designed to handle vari-\nability within one or more EEG datasets and combine them into a\nsingle preprocessed dataset useful for self-supervised learning ap-\nplications. The Python-based pipeline improves stability, conver-\ngence, and contrastive accuracy during pretraining and produces a\nmore suitable latent space for downstream classification tasks. More\nspecifically, our probing results for downstream accuracy show a\nsignificant improvement for several downstream classification tasks\nwhen pretraining using our preprocessing pipeline compared to a\nsimple baseline preprocessing pipeline. Besides the plug-and-play\npreprocessing pipeline, we also present tools for reproducible pre-\nprocessing of the TUH EEG Corpus ready for future development of\npretrained EEG foundation models. In conclusion, our results form\nevidence that physically motivated preprocessing is useful for self-\nsupervised learning of EEG representations."}]}