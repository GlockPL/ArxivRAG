{"title": "WHEN CAN PROXIES IMPROVE the SAMPLE COM- PLEXITY OF PREFERENCE LEARNING?", "authors": ["Yuchen Zhu", "Daniel Augusto de Souza", "Zhengyan Shi", "Mengyue Yang", "Pasquale Minervini", "Alexander D'Amour", "Matt J. Kusner"], "abstract": "We address the problem of reward hacking, where maximising a proxy reward does not necessarily increase the true reward. This is a key concern for Large Language Models (LLMs), as they are often fine-tuned on human preferences that may not accurately reflect a true objective. Existing work uses various tricks such as regularisation, tweaks to the reward model, and reward hacking detectors, to limit the influence that such proxy preferences have on a model. Luckily, in many contexts such as medicine, education, and law, a sparse amount of expert data is often available. In these cases, it is often unclear whether the addition of proxy data can improve policy learning. We outline a set of sufficient conditions on proxy feedback that, if satisfied, indicate that proxy data can provably improve the sample complexity of learning the ground truth policy. These conditions can inform the data collection process for specific tasks. The result implies a parameterisation for LLMs that achieves this improved sample complexity. We detail how one can adapt existing architectures to yield this improved sample complexity.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) and other large generative models have revolutionised modern machine learning with their surprising capabilities, surpassing human-level performance in law, medicine, and other examinations (Achiam et al., 2023; Amin et al., 2023). A large part of their success is their ability to incorporate human preferences to learn complex objectives such as trustworthiness (Yu et al., 2024), sentiment preferences (Chakraborty et al., 2024), and value alignment (Ji et al., 2023).\nIn many cases, this preference data is a proxy for the ground truth. For example, humans raters tend to prefer longer answers to a question, even if the answer is less informative Zhou et al. (2024). In this case, 'response length' is a proxy for the true helpfulness of an answer. If an LLM is trained on this proxy data alone it leads to a 'length-bias' (Shen et al., 2023; Singhal et al., 2023), as LLMs fine-tuned with this preference data generate longer and better formatted responses to appear more helpful (Chen et al., 2024). This is an example of the well-known phenomenon of reward hacking\u00b9: a model optimised to perform well with respect to a proxy reward function, performs poorly with respect to a ground truth reward function (Casper et al., 2023). Reward hacking is a fundamental problem in learning that has been observed in optimised circuits listening in on the oscillators of other computers when instead tasked to build their own (Bird & Layzell, 2002), universities rejecting the most qualified applicants to boost their ratings (Golden, 2001), and many other cases in game playing (Clark & Amodei, 2016), autonomous driving (Knox et al., 2023), and text summarisation (Paulus et al., 2018).\nTo address reward hacking in LLMs, prior work largely designs tweaks to the model, data, and optimization procedure. This includes regularisation towards an initial policy (Schulman et al., 2017; Rafailov et al., 2023; Huang et al., 2024), changing properties of the reward model (Gao et al., 2023; Coste et al., 2024), using soft labels (Zhu et al., 2024), adjusting optimization hyperparameters (Singhal et al., 2023), reward hacking detection mechanisms (Pan et al., 2022; Miao et al., 2024), and introducing additional tools specialised to counteract length bias (Chen et al., 2024). The reasoning"}, {"title": "2 PRELIMINARIES", "content": "Consider the set of all prompts X and completions Y, the elements of these sets are discrete se- quences of tokens with arbitrary length, e.g. x = [x1,..., xN\u339e]. By considering an enumeration of all completions Y, the space of distributions Py is equivalent to the subset of positive and unit norm sequences in the sequence space l\u00b9. We consider the set of policies, in the form of language models, which are maps X \u2192 Py.\nStarting from an initial policy \u03c0ref\u00b2, we want to find a target policy \u3160\u2020 : X \u2192 Py which aligns with the preferences of an ideal actor in a given scenario. To learn this policy, we have preference data directly from the ideal actor, denoted D\u2020, as well as preference data from a proxy actor, D. The central question we consider is: under what assumptions can \u010e improve the sample complexity of learning \u03c0\u2020?\nHuman preference feedback. We aim to align \u03c0ref using preference data of the form {(x, yw, y\u0131)}, where yw and y\u0131 are candidate completions for prompt \u00e6, and where yw is pre- ferred to yi. We assume these preferences are generated from a underlying scalar reward function r(x, y) according to Bradley & Terry (1952):\n$x \\sim p_x; y_1, y_2 \\stackrel{i.i.d.}{\\sim} \\pi_{ref} (\\cdot | x);$ (1)\n$b \\sim Bern[\\sigma(r(x, y_1) - r(x, y_2))];$ (2)\n$(y_w, y_l) = \\begin{cases} (y_1, y_2) & \\text{if } b = 1, \\\\ (y_2, y_1) & \\text{if } b = 0. \\end{cases}$ (3)\nwhere \u03c3(\u00b7) is the sigmoid logistic function. We assume that y\u2081 and y2 are sampled from \u03c0ref for simplicity, whereas in practice they can be sampled from other distributions over Py. In this model, higher relative rewards increase the chance of a completion being picked as the winner yw.\nWe assume that the true preference data D\u2020 and the proxy preference data D are generated according to distinct reward functions rt and r.\nBandit problem setting. Given a data-generating process G = (r, \u03c0ref, px) with reward function r, a reference policy \u03c0ref, and a distribution of prompts px, the optimal KL-regularised policy \u03c0G"}, {"title": "3 SUFFICIENT CONDITIONS FOR PROXY FEEDBACK", "content": "Theory and survey papers point out the difficulty of alignment under mismatch between the true reward function and the one reflected by the human labelers (Skalse et al., 2022; Casper et al., 2023). In other related fields such as vision, (Chi et al., 2022) shows the impossibility of leveraging pre-training data for unseen tasks unless some similarity between the two tasks is given.\nFrom these observations we can draw two conclusions. (i) We must have at least some data from G\u2020 to learn \u03c0\u2020; this motivates the need for both Dh and D. (ii) In order for \u00f1 to inform us something about \u03c0\u2020, they must share some similarities. Thus, we ask the following research question:\nUnder what sufficient conditions can we use D to learn \u3160\u2020 more efficiently than if we used D\u2020 alone? How much can we improve?\nWith the following conditions, we show that, when we have access to large amounts of proxy data, \u03c0\u2020 can be expressed as a low-dimension adaptation of 7, and hence the sample complexity of learning \u3160\u2020 is drastically improved, superexponential in the data manifold dimension."}, {"title": "4 ADAPTING 7 \u03a4\u039f \u03c0", "content": "In this section, we derive an algorithm for learning \u3160\u2020 that leverages the structure in specified in conditions 1-4. The algorithm hinges on a decompositon of the policies \u03c0 into an encoder T, a linear"}, {"title": "4.3 MODEL PARAMETRISATION AND LEARNING", "content": "We now sketch our learning algorithm. Theorem 3 gives rise to a two-step procedure to learn the true policy \u03c0\u2020, firstly we recover the functional components using a large proxy dataset D\u00f1 and then, secondly, we use a small true dataset D to learn the low-dimensional adapter.\nStage 1 Based on Theorem 3, we model the proxy policy with a parametric model composed of three functions: (i) 7, an embedding function from the prompts X to the D-simplex \u2206D, (ii) \u2208RD\u00d7D, a linear map from the simplex to a convex polytope V, and (iii) \u00fee, an injective function from the latent space V to a distribution of completions Py. Therefore, our model is expressed as:\n$\\pi_{0} = \\Phi_{0} \\circ \\Theta \\circ \\tau^{0}.$ (10)\nBased on the DPO loss (Eq. 6), \u0444\u04e9, \u00d5, and 7 are learned using the empirical preference learning objective with the proxy dataset D\u00f1:\n$L_{n}(\\pi_{0}) = \\frac{1}{n} \\sum_{i=1} log \\sigma(\\beta log \\frac{\\pi_{0}(Y_{w,i} | X_{i})}{\\pi_{ref}(Y_{w,i} | X_{i})} - \\beta log \\frac{\\pi_{0}(Y_{l,i} | X_{i})}{\\pi_{ref}(Y_{l,i} | X_{i})});$ (11)\nIn the large sample limit, the optimal parametrised model 7\u00ba minimizes the population proxy preference loss, and due to Theorem 3, we know that the optimal KL-regularised proxy policy *70*, thus, justifying our parametrization.\nStage 2 Following this, we then model the true policy \u03c0\u2020 using the same pre-trained components from our model of the proxy policy me with the addition of a low-dimensional adapter function which maps a latent representation in the simplex AD to another AD as follows:\n$\\pi_{*}^{\\dagger} = \\Phi_{0} \\circ \\Pi^{\\dagger} \\circ \\Theta \\circ \\tau^{0}.$ (12)\nThe adapter is learned by optimization of the empirical preference learning objective with the true dataset D, while keeping \u00deo, \u00d5, and 7 fixed based on their previously optimised values:\n$L_{int}(\\Pi^{\\dagger}) = \\frac{1}{n^{\\dagger}} \\sum_{i=1}^{n^{\\dagger}} log \\sigma(\\beta log \\frac{\\pi_{*}^{\\dagger}(Y_{w,i} | X_{i})}{\\pi_{ref}(Y_{w,i} | X_{i})} - \\beta log \\frac{\\pi_{*}^{\\dagger}(Y_{l,i} | X_{i})}{\\pi_{ref}(Y_{l,i} | X_{i})}).$ (13)\nThere can be more efficient algorithm which learn the triplet (,,\u00ba) by using the proxy or true data simultaneously. But by splitting the learning into two stages, the first using only proxy data, and the second using only true preference data, we can make a direct sample complexity comparison between learning \u3160\u2020 from scratch, and learning the adapter in Stage 2, in terms of the size of the true preference data D"}, {"title": "5 CONVERGENCE RATES ANALYSIS", "content": "To illustrate the benefit of learning \u3160\u2020 using the outlined algorithm, we analyse its soundness by showing the sample complexity improvement given that we have identified the true , and 7\u00b0 from the proxy dataset in the first stage. This can be a reasonable approximation of the properties of the learning procedure in cases where the proxy dataset is much larger than the true dataset. To this end, we analyse the generalisation error bound for the second stage given access to true , and 7\u00b0.\nFollowing the approaches of Elesedy (2022) Mohri et al. (2012, Exercise 3.31), the generalisation error can be shown to be linear in the covering number of the hypothesis class. Our idea here is that the hypothesis class of is made smaller by having knowledge of , and 7\u00b0, hence the covering number is also smaller. In order to define covering numbers, we first need to define a notion of metric on all relevant spaces and the hypothesis classes we consider."}, {"title": "5.1 METRICS AND HYPOTHESIS CLASSES", "content": "Metric on finite-dimensional vector spaces. For any finite-dimensional vector space, we use the p-norm-induced metric; for a simplex \u2206D we denote its metric by d\u25b3 and for any other finite dimensional space U we use du.\nMetric on the prompt space. The prompt space X, is a discrete and unstructured space, so we define a metric, dx on it based on some fixed embedding function f, which maps a prompt to a vector space with finite but high dimensions: dx(x,x') = df(x) (f(x), f(x')) = || f(x) - f(x')||p.\nFor intuition we can think about f as some general-purpose embedding, such as one got by retrieving some intermediate layer from a large model such as CLIP Radford et al. (2021). Importantly, this metric is only relevant when considering the complexity of the hypothesis class when we learn the target policy without proxy data, which one expects to be large; this intuition is confirmed since a general-purpose embedding may not work well across all tasks, and can result in a large Lipschitz constant for the target function.\nMetric on a policy space. Defining a metric on a space of policies is more involved, and we begin by relating a policy and the reward function under which it is the KL-regularised optimal policy. Given a policy \u03c0 : X \u2192 Py, define the implicit reward as:\n$r(x, y) = \\beta log \\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}$ (14)\nUsing r\u03c0, we can define a metric on the set of policies, and we express in the next lemma.\nLemma 4 (r helps define a metric). For any \u03c0,\u03c0' : X \u2192 Py such that \u3160 and \u03c0' both satisfy ||||\u221e\u2264\u221e, define\n$d_r(\\pi, \\pi') = ||r_{\\pi} - \\Upsilon_{\\pi'} ||_{\\infty}$ (15)\nThen dr(,) defines a metric over the set of policies ${\\pi: X \\rightarrow P_y | \\sum_{y} \\pi(y|x) =1}$.\nMetric on the completion distribution space. We also need a notion of metric on the range of any policy function, Py. To this end, we define the following function dpy : Py \u00d7 Py \u2192 RU{x}:\n$d_{p_y}(p,q) := \\beta log \\frac{\\beta}{\\beta} | \\frac{P(y)}{q(y)}|$ (16)\nThis construction is motivated by a resemblance with the construction of r\u03c0.\nNow define the hypothesis class of \u03c0\u2020 as:\nDefinition 1 (Hypothesis class II). Let II be a set of functions \u03c0 : X \u2192 Py s.t. \u2200x \u2211y \u03c0(y|x) = 1, and satisfies ||rn||\u221e\u2264 C.\nSo let dr be the metric on II and all its subsets. Later on, we will notice that the covering number of a hypothesis class depends on the smallest Lipschitz constant of the class. Let II(L) denote the subset of II such that every policy \u03c0 is L-Lipschitz wrt dx and dpy.\nMeanwhile, with knowledge of $, \u00d5, 7\u00ba, we define a subset of II with respect to a Lipschitz constant.\nDefinition 2 (Hypothesis class fixing , \u00d5, 7\u00ba and Lipschitz-constant L\u2081). Fix , \u00d5, \u3012\n$\u03a0_{(\u03a0, \\Theta, \\tau^{\\circ}, L)} \\subseteq \u00ceI$ contains all n \u2208 \u00ceI which can be written as\n$\\pi(\\cdot | x) = \u1fe5 \\circ \u03c0\u03bf\u3012\u309c(x)$ (17)\nNow we are ready to state our main generalisation error bounds results."}, {"title": "6 RELATED WORK", "content": "Reward hacking theory. Initial work on the theory of reward hacking considered the setting where the proxy reward was a function of a subset of true reward features (Zhuang & Hadfield- Menell, 2020). This work demonstrates that optimising the proxy can lead to arbitrarily low true reward. Similarly, Tien et al. (2022) give theoretical results for reward hacking when a learned re- ward uses nuisance variables that correlate with true causal variables. These ideas were extended to arbitrary MDPs by Skalse et al. (2022) who define a proxy reward as hackable if it prefers policy \u03c0\u2081 over \u03c02 when the true reward has the opposite preference. Recent work has sought to develop scaling laws for reward hacking that describe how the true reward changes as the proxy reward is optimised (Gao et al., 2023). Rafailov et al. (2024) show similar over-optimisation patterns in DPO at higher KL-divergence budgets, even without an explicit reward model. In contrast to these works, our theoretical results suggest a new model parametrisation and training scheme which achieves improved sample complexity for learning the true policy; hence, our results are constructive."}, {"title": "7 CONCLUSION", "content": "We study the problem of reward hacking due to distribution shifts. Specifically, an abundance of preference rankings is generated by a proxy reward function, different from the true reward function, which is costly to query. We thus consider the setting where we have a large proxy dataset and a small true dataset; to the best of our knowledge, we are the first to consider this setting. We formulate conditions motivated by a real-world example, under which we prove that the optimal proxy policy can be decomposed into component functions shared with the optimal true policy, and that the true policy is only one low-dimensional adapter function away from the proxy policy given the shared component functions. We then observe that in the large sample limit of proxy data, one set of such component functions can be identified from minimising the preference loss. Leveraging this, we provide a characterisation of the sample complexity bound for learning the hypothesis class both with and without knowledge of the shared component functions; in particular, it is seen that under knowledge of the shared component functions the sample complexity bound is much lower than without such knowledge.\nAs ongoing work, we are working on empirical evaluation of our theoretical findings, as well as relaxing some of the conditions we made. An updated version of the manuscript will be uploaded in the near future."}, {"title": "A PROOF OF PROPOSITION 1", "content": "Proposition 1 Let K = {1,\u2026\u2026,k} denote a set of k tokens. Let Y be the set of all finite length token sequences whose tokens all come from K. Then Y has a one-to-one identification with the natural numbers. Let Py be the set of probability mass functions over Y, then the topological dimension dim(Py) = x. If Y is instead the set of token sequences with length < l, then dim(Py) = O(kl).\nProof of Proposition 1. We can identify y with the natural numbers as follows. Let I denote the length of a token sequence. Since for each token in the sequence there are k options, there are k distinct sequences of length 1.\nWe can define a bijective mapping from the set of k\u00b9 sequences to the subset of natural numbers\n$S^{l}:= {(\\sum_{i=1}^{l} k^{i}),..., (\\sum_{i=1}^{l} k^{i}) - 1}$ for 1 \u2265 2 and $S^{1}:= {0,..., k \u2212 1}$. This is possible\nsince the number of elements in Sl is $(\\sum_{i=1}^{l} k^{i})-1-(\\sum_{i=1}^{l} k^{i}) + 1 = k^{1}$. Denote one such\nmapping fr. Then define f: Y \u2192 N:\n$f(y) = f_l(y) \\text{ if length of } y = l$ (23)\nf is well-defined because every y has a unique length l. f is invertible because fi is invertible for every l and $S^{l} \u2229 S^{\"} = \u00d8$ and $\\bigcup_{l=1} S^{l} = N$.\nThus, Py is the set of probability mass functions whose sample space is \u2243 N. That is, an element Py \u2208 Py is an infinite positive sequence which sums to 1.\nLet Ad be the d-dimensional simplex. Note that $P_y = \\bigcup_{d=1}^{\\infty} \u2206^{d}$, where \u2206d is viewed as a subset of Ad+1 via the inclusion $\u2206^{d} \u2192 \u25b3^{d+1} : (p_1,\u2026\u2026,p_{d+1}) \u2192 (P_1,\u2026\u2026,P_{a+1},0,\u2026\u2026)$.\nFor each d, the topological dimension of the d-simplex \u2206d is d 5; this is to say, d is the smallest number such that every open cover of Ad has an open refinement of order d + 1. Therefore, the smallest number n such that every open cover of $\\bigcup_{d=1}^{\\infty} \u2206^{d}$ has an open refinement of order n + 1 is D. Hence, there is no finite N such that the any open cover of $P_y := \\bigcup_{=1}^{\\infty} \u2206^{d}$ has an open refinement with order N + 1. Therefore the topological dimension of Py is \u221e.\nWhen the maximum sequence length is I the cardinality of Y is finite and equal to $\\sum_{i=1}^k ki = \\frac{k(1-k^{1})}{1-k}$. Py thus contains the set of positive sequences of length $\\frac{k(1-k^{1})}{1-k}$ which sum to 1, so Py is $\\frac{k(1-k^{1})}{1-k}-dimensional simplex, and therefore the topological dimension of Py is $\\frac{k(1-k^{1})}{1-k}- 1 = O(k^{l})$"}, {"title": "B PROOF OF THEOREM 3", "content": "Lemma 2 Under Condition 1, \u03c0\u2020 \u3147\u00f1\u22121|\u5143(x) is a well-defined function.\nProof.\nInverse of non-injective functions. In general, unless a function is injective 6, its inverse is not a function, but only a set map. For instance, since is many-to-one, so not injective, \u00af\u00b9 would take an element from Py and return a subset of X, rather than a single element. Let \u00f1\u00af\u00b9|\u5143(x) denote the inverse of \u00b9 restricted to its image (X).\n\u3160\u3147\u3160\u00af\u00b9|\u5143(x) is a well-defined function. It follows that \u3160\u2020 \u3147\u00f1\u00af\u00b9|\u5143(x) takes a point Py in the image of \u3160, \u3160(X), map it to its preimage \u00f1\u00af\u00b9(Py), and map all points in the preimage \u00af\u00b9(Py) through \u03c0\u2020 \u03c4\u03bf \u03c0\u2020 (\u03c0\u00af\u00b9(Py)). For any two points X1, X2 \u2208 \u00f1\u00af\u00b9(Py), we have (x1) = (x2), and"}, {"title": "C CONVERGENCE RATES PROOFS", "content": "Lemma 4. (r helps define a metric) Define r\u03c0(x,y) = \u03b2log (2) for some fixed constant\n\u03b2 > 0, and let\n$d_r(\\pi, \\pi') = ||r_{\\pi} - \\Upsilon_{\\pi'} ||_{\\infty}$ (48)\nThen d\u2084(\u00b7,\u00b7) defines a metric over any set II of functions X \u00d7 Y \u2192 [0, 1] s.t. \u2200x \u2211y \u03c0(y | x) = 1, and satisfies |r(x, y)| \u2264 Cover X and Y.\nProof of Lemma 4. dr is well-defined on II since for any \u03c0, \u03c0' \u2208 \u03a0, dr(\u03c0, \u03c0') \u2264 ||rn||+||\u03c0' ||\u221e\u2264 2C <\u221e.\nWe can verify that d is a metric on I. Clearly, symmetry and positivity holds, and d(\u03c0, \u03c0') = 0 \u03c0 = \u03c0', so we just need to check triangle inequality. Fix \u03c0\" \u2208 \u03a0,\n$d_r(\\pi, \\pi'') + d_r(\\pi', \\pi'') = ||r_{\\pi} - \\Upsilon_{\\pi''||_{\\infty}} + ||\\Upsilon_{\\pi'} - \\Upsilon_{\\pi''||_{\\infty}}$ (49)\n$\\geq ||r_{\\pi} - \\Upsilon_{\\pi''} - \\Upsilon_{\\pi'} + \\Upsilon_{\\pi''||_{\\infty}}$ (50)\n$= ||r_{\\pi} - \\Upsilon_{\\pi'} ||$ (51)\n$= d_r(\\pi, \\pi')$ (52)"}, {"title": "C.1 PROOF OF THEOREM 5", "content": "Theorem 5(Bounding sample complexity in terms of dimension) We remain in the set up of Propo- sition 8. The covering number of II is bounded above by a function of D:\n$(P = {g(x) = (x,y) {g(y) = O () [y] ||| <4 + P) 1772) 8 Cov |() y=1}\nthen Hoefding's inequality gives\n(P = {9(4y) = \u00a7}08+7{470) [Y | > <8599)\n= (a-o)|-logw\n (145) 1s - [ (96, VD)", "sentences": ["Theorem 5(Bounding sample complexity in terms of dimension) We remain in the set up of Propo- sition 8. The covering number of II is bounded above by a function of D:\n$(P = {g(x) = (x,y) {g(y) = O () [y] ||| <4 + P) 1772) 8 Cov |() y=1}\nthen Hoefding's inequality gives\n(P = {9(4y) = \u00a7}08+7{470) [Y | > <8599)\n= (a-o)|-logw\n (145) 1s - [ (96, VD)"]}, {"title": "C.2 PROOF OF THEOREM 6", "content": "Theorem 6 (Bounding sample complexity of learning without proxy) Let II L \u2171(L$||||pL) be the subset of II where is L\u00f8||\u00d5||pL\u016b-Lipschitz. Set \u043a = 24, we need\n\u03a9\nD'\nD'\n48L$||\u00d5||pLE' (p, D')\u221aD'\nlog\n- log w\n48L$||\u00d5||pLE' (p, D')\u221aD'\n(145)\nsamples to generalise, where D' \u226b D, and E'(p, D') \u226b 1. That is, whenever n' \u2265 n(e,w), we have"}]}