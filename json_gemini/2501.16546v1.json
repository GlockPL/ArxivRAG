{"title": "Sample-Efficient Behavior Cloning Using General Domain Knowledge", "authors": ["Feiyu Zhu", "Jean Oh", "Reid Simmons"], "abstract": "Behavior cloning has shown success in many sequential decision-making tasks by learning from expert demonstrations, yet they can be very sample inefficient and fail to generalize to unseen scenarios. One approach to these problems is to introduce general domain knowledge, such that the policy can focus on the essential features and may generalize to unseen states by applying that knowledge. Although this knowledge is easy to acquire from the experts, it is hard to be combined with learning from individual examples due to the lack of semantic structure in neural networks and the time-consuming nature of feature engineering. To enable learning from both general knowledge and specific demonstration trajectories, we use a large language model's coding capability to instantiate a policy structure based on expert domain knowledge expressed in natural language and tune the parameters in the policy with demonstrations. We name this approach the Knowledge Informed Model (KIM) as the structure reflects the semantics of expert knowledge. In our experiments with lunar lander and car racing tasks, our approach learns to solve the tasks with as few as 5 demonstrations and is robust to action noise, outperforming the baseline model without domain knowledge. This indicates that with the help of large language models, we can incorporate domain knowledge into the structure of the policy, increasing sample efficiency for behavior cloning.", "sections": [{"title": "1 Introduction", "content": "Behavior cloning and its variants have demonstrated success in learning policies for autonomous driving [Hu et al., 2022], table-top manipulation [Chi et al., 2023], household tasks [Fu et al., 2024], and so on. Yet, due to a distribution mismatch between expert trajectories and the states encountered during deployment [Osa et al., 2018] as well as the increase in model size, they often rely on a large number of expert demonstrations to learn a robust policy [Zhao et al., 2024] and cannot generalize well to new camera poses, unseen distractor objects, novel background texture etc. [Xie et al., 2024].\nDespite the vast variations a task could have, the underlying principles of solving the task often stay the same. For example, when trying to open a door, the motion only depends on the position and type of the handle along with the direction the door is expected to open. Furthermore, the opening direction of the door can be inferred from the location of the hinges. This general domain knowledge naturally reflects the latent features of the task and their connectivities: direction is a latent variable that depends on the location of the hinge, and the specific motion depends on direction but not other features such as color or material. It has been shown that following general knowledge enables zero-shot transfer to novel environments for tasks with discrete action space [Zhu and Simmons, 2024].\nAlthough it is relatively easy for a domain expert to explain the general ideas, it is challenging for them to specify the detailed instructions, especially for continuous action spaces. Additionally, unstructured model architectures have very few semantic structures, creating a barrier between domain knowledge expressed in natural languages and the internal representation of a learning model. Therefore, exist-"}, {"title": "2 Related Work", "content": "ing work that attempts to integrate domain knowledge focuses mainly on state abstractions that highlight the important features of the task [Peng et al., 2024]."}, {"title": "2.1 Sample-Efficient Behavior Cloning", "content": "Data augmentation is a common technique for expanding the coverage of expert demonstrations [Ankile et al., 2024], often using visual synthesis [Zhou et al., 2023], local continuity [Deshpande et al., 2024], time-reversal symmetry [Cheng et al., 2024]. Extending this direction, other works proposed to learn a local model to guide the policy from unseen states to known states [Park and Wong, 2022] or to learn a world model [Kolev et al., 2024]. Another similar approach is to use state abstraction to hide the irrelevant features of the states [Peng et al., 2024] such that the policy will not be conditioned on them without the need for data augmentation. Other approaches include using a better representation of actions [Chi et al., 2023], building up skill libraries to reuse previously learned skills [Wan et al., 2024], instructing the expert to demonstrate failure recovery [Brandfonbrener et al., 2023].\nUnlike previous work that mainly focused on sample-level operations, our work is the most similar to [Mao et al., 2023] where we aim to improve sample efficiency by specializing the structure of the policy being learned to the specific task and its relevant features. However, instead of searching through a pre-defined architecture space or merely abstracting the state representations, we take advantage of experts' domain knowledge to instantiate a neural net with a specific structure that is specialized to the task as the policy model."}, {"title": "2.2 LLM Assisted Policy Learning", "content": "Previous work has used the coding capability of LLMs to implement agent policies [Zhu and Simmons, 2024], represent world models [Tang et al., 2024b], generate reward distributions [Bucker et al., 2024], and translate underspecified task specifications into structured representations [Liu et al., 2023]. However, the codes generated are mostly symbolic, relying on well-defined APIs to execute the policy. They are also static, allowing very little post-generation adaptation. Others have used LLMs to generate target action distribution [Zhou et al., 2024] or provide reward signals [Wang et al., 2024] that can be used to train smaller models. But unlike the coding-focused approaches that can make use of external knowledge, these sample-based methods depend solely on the pre-trained knowledge in the LLMs.\nOur work makes use of the coding ability of LLMs but also enables parameter tuning after code generation. This alleviates the dependence on the LLMs to get everything correct in one go and makes it possible to incorporate expert knowledge that is not captured by the LLMs."}, {"title": "2.3 Knowledge Integration in Machine Learning", "content": "It is well-acknowledged that integrating existing human knowledge helps with machine learning [Deng et al., 2020], where human knowledge is commonly in the form of feature selections and invariance in the task.\nPrior to the popularity of learning feature representations, models were trained with features that were picked manually [Bahnsen et al., 2016] or according to some statistical metrics [Ghojogh et al., 2019]. Despite achieving great performance in complex tasks such as planning for driving [Dauner et al., 2023], existing works modify only the inputs to the models but not the architecture of the models, not taking full advantage of the existing domain knowledge.\nOther works have developed specialized architecture that incorporates the invariance in the task, including SE(3)-equivariant layers for tabletop manipulation [Eisner et al., 2024] and drug discovery [Schneuing et al., 2024], and physics-informed neural nets that respect PDE constraints [Wang et al., 2023]. These approaches require the experts to have both domain knowledge for the task and also engineering skills for model development, and the architecture can only be used in a certain family of tasks.\nBy contrast, our approach takes advantage of LLMs' coding skills to implement arbitrary general domain knowledge expressed in natural languages, making it more accessible to make use of existing human knowledge. And it implements the architecture from the ground up, reflecting both the selection of features and the connections between the features."}, {"title": "3 Knowledge Informed Model (KIM)", "content": null}, {"title": "3.1 Structured Policy", "content": "In this work, we use the term \"structured policy\" to refer to a model in which latent variables and their connectivities are specialized to the task. The latent variables typically have semantic meanings, representing key features of the task that are not directly accessible from the input. A structured policy has many distinctions compared to an unstructured model such as a generic multi-layer perception (MLP). It takes advantage of the sparsity that exists in many domains [Mao et al., 2023] and assigns learnable parameters to the related latent variables instead of all pairs of latent variables. It may also contain a variety of operations (e.g., max, clip) that are"}, {"title": "3.2 KIM Generation via LLM", "content": "We assume access to domain knowledge K from an expert in natural language that describes the high-level ideas that guide the demonstrations D. This is attainable as previous works have shown that humans typically construct simplified mental representations during problem-solving [Ho et al., 2022], so they should also be able to articulate the general knowledge used to perform the demonstrations.\nIn practice, we collect the general description of the strategy used by the expert, the feature space and the action space of the task, and any additional information about the environment that might be useful. We can instantiate a policy structure based on the provided general knowledge using an LLM (e.g., GPT40 [OpenAI, 2024]). That is\n(V, \u039f, \u0395, \u0398init) = LLM(S + K)\nwhere S is the system prompt that is shared for all tasks. Concretely, we use the chain-of-thought prompting [Wei et al., 2023] to instruct the LLM to implement the models. First, it is instructed to extract all the input features and latent variables in the general knowledge description and list their type and shape (e.g., angle target has type float and shape (1,)).\nNext, the LLM is expected to re-arrange the latent variables in the order in which they should be computed based on variable dependencies (e.g., angle target should appear before angle adjustment as the latter depends on the former). And, for each of them, list all the previously computed variables that the current variable depends on and what operators are needed to connect them. Empirical experiments showed that without this step the LLM may miss some of the connections in the code generation process. During this process, the LLM is also instructed to classify each connection"}, {"title": "3.3 Behavior Cloning for KIM", "content": "After the model structure is set, we train the parameters using the standard behavior cloning objective to tune the parameters in the policy.\nmin E (si,ai)\u2208DL(ai, \u03c0\u04e9(Si))\nIn practice, we use grid search over the values for the non-gradient parameters. For each combination of the non-gradient parameter values, we use gradient descent to optimize the remaining gradient parameters. By default, we use cross-entropy loss for discrete action spaces and mean square error for continuous action spaces. The combination (both non-gradient and gradient) that achieves the least overall loss is kept as the final model parameter.\nBecause the connections between latent variables are sparse, the number of total parameters is small compared to unstructured models. Additionally, we focus on using only a few demonstrations. Therefore, we can perform gradient descent on all the demonstrations at once for most tasks without having to separate the samples into mini-batches. This helps to stabilize the training process.\nUnlike unstructured models, latent variables in KIM have semantic meanings as they are extracted from the provided expert knowledge. Therefore, it is possible for the expert to directly set the value of some constant parameters, or the weights connecting different latent variables. This will make learning easier since there are fewer parameters to optimize."}, {"title": "4 Experiments", "content": "We experiment with the Lunar Lander and Car Racing environments implemented in Gymnasium [Towers et al., 2024]. The environments cover both discrete and continuous action spaces. We used gpt-40-2024-11-20 as our LLM for all of the experiments."}, {"title": "4.1 Lunar Lander", "content": "The objective of the Lunar Lander task is to control the engines of the lander to perform a soft landing on the landing pad (an illustration can be found in Figure 2). The observation space is the horizontal position and velocity, vertical position and velocity, angular position and velocity, and whether each of the landing legs is in contact with the surface. The last two features on the landing legs are binary, while the others are continuous. Each new episode has a different randomly initialized starting configuration. The action space is discrete, consisting of doing nothing or activating one of the left, main, or right engines. The episode ends if the lander lands safely, crashes, or runs out of fuel after 1000 steps. Typically, a successful landing can be achieved in around 200 steps.\nWe use the heuristic policy defined in the Gymnasium package as the expert policy that generates demonstrations. This policy achieves a 90% success rate in the environment, however we keep only the successful episodes as demonstrations for training. We manually describe the strategy of the heuristic policy as the expert general knowledge and use it to prompt the LLM for KIM generation. The prompt can be found in the Appendix.\nFor the baseline condition, we use an MLP and formulate it as a classification problem with cross-entropy loss, where the objective is to predict which action the expert policy is going to perform given all of the features of a state. For both conditions, we randomly sample 20% of the demonstration steps as the validation set, and keep the model parameters with the least loss in the validation set for evaluation."}, {"title": "4.2 Car Racing", "content": "The objective of the car racing task is to complete a winding track as fast as possible (an illustration is provided in Figure 1). The track is defined by a sequence of tiles that span from"}, {"title": "4.3 Results on Learning with a Few Demonstrations", "content": "Figure 3 shows the success rate in the Lunar Lander task between using KIM and the baseline neural networks of two different sizes (the number of parameters are listed in the legend). The figure shows that given a fixed number of demonstrations, KIM, with only 15 parameters, achieves a 20%+\nhigher success rate than the small NN that has a similar number of parameters and a 7%+ higher success rate than the larger NN that has 25x more parameters. This shows that given the same demonstrations, KIM learns a more robust policy. All but one pair of comparisons show statistical significance in a paired t-test where the pairing is based on having the same set of demonstrations. The plot on the rewards of the Lunar Lander environment can be found in the Appendix.\nFigure 4 shows the reward comparisons between KIM and a neural net in the Car Racing task. It shows that starting from as few as 2 demonstrations KIM yields good performance and low variance. This attributes to KIM having very few parameters organized in a semantically meaningful structure and is thus more robust to imperfect demonstrations. When there are more demonstrations, KIM still outperforms the baseline (which has 200x more parameters) with statistical significance. Overall, the plots show that despite the demonstrations and the provided general knowledge may not be perfectly aligned, using the knowledge to instantiate the model still leads to better performance."}, {"title": "4.4 Result on Environments with Noise", "content": "To evaluate how well KIM does in noisy settings, we randomly corrupt its output with a Gaussian noise when it is interacting with the environment. That is,\nanew ~N(apred; noise level. I)\nNote that in this setting the models are still trained with expert demonstrations captured in a noise-free environment. The noise is only added after the model is trained.\nFigure 5 shows the comparison between KIM and the baseline (using the 10 models trained on 10 demonstrations each in the previous section) in environments with different noise levels. It shows that as the noise level increases, the performance of the baseline condition degrades much more drastically than KIM. In particular, KIM can still retain around 65% of the reward even with considerable noise (the action"}, {"title": "4.5 Qualitative Analysis", "content": "The high variance in the neural net baseline learned with 2 demonstrations is partially caused by the learned model losing control of the race car and swirling off the track.\nFigure 6 shows an example where the baseline condition loses control while KIM steers the race car to stay on the track. This is because the knowledge of \"don't steer too drastically when accelerating\" is crucial for driving the rear-wheel drive race car in this domain, yet is only implicitly illustrated through the expert demonstrations. As a result, when the demonstrations are not sufficiently indicative of this, an unstructured model may miss this constraint, leading to catastrophic outcomes, especially if no expert demonstration illustrates how to regain control after losing traction.\nHowever, KIM provides another channel (i.e., the general knowledge in natural language) for the expert to pass knowledge to the learning model. Furthermore, these domain constraints are enforced by the structure of the model such that it is more robust to imperfect demonstrations. Therefore it can navigate the corner much more smoothly. Listing 1 shows the structure that is informed by the following instructions:\nthe output of steering should be scaled based on the current speed such that when speed approaches 1 the steer magnitude should approach 0."}, {"title": "4.6 Additional Comparisons", "content": "Table 1 shows the ablation on different settings of KIM in the two environments.\nThe human-generated code condition is where the code generated by GPT is replaced with code generated by a human researcher given the same prompt. Overall, the codes generated are very similar and hence the performance is similar in both tasks. The differences in the code implementation are very subtle. For example, the human-generated code set bias=False for the linear layer for adjusting for steering because by default the race car should go straight. However, GPT did not make use of this information and defined the linear layer with bias. Details like this likely lead to a slightly smaller variance in the human-generated code condition.\nOn the other hand, there is a distinctive difference between having pre-filled initialization values for the parameters and not. In the random initialization condition, all parameters are sampled from a standard normal distribution. The result shows very high variance, confirming that the optimization space for KIM does not have the \"one basin\" phenomenon [Ainsworth et al., 2022] that helps optimize typical unstructured models. So prompting the LLM to analyze the relationship between latent variables is vital for performance."}, {"title": "5 Discussion", "content": null}, {"title": "5.1 Limitations", "content": "The current method relies heavily on having access to good expert instructions. These are relatively easy to acquire from well-established settings (e.g., assembly lines or aircraft controls) but could be hard in scenarios that require more nuance (e.g., social navigation). It is also challenging for human experts to provide exhaustive instructions in one go, or if the action space is different between human experts and the policy (e.g., learning a quadruped robot walking policy).\nAnother limitation is the dependency on LLMs' zero-shot coding capabilities. In addition to the potential misalignment issue [Greenblatt et al., 2024], all contemporary LLMs operate on input sequences with a length limit, making it impossible to generate a KIM if the general domain knowledge exceeds that limit. Additionally, previous work has reported"}, {"title": "5.2 Future Work", "content": "Since it is hard to specify all the general knowledge all at once, a natural extension is to support interactive and incremental KIM. This would require integrating the code repairing capability of LLM [Tang et al., 2024a] and modifying the existing structure based on the incoming knowledge. The parameters are expected to transfer to the new architecture with a little fine-tuning. This also helps if the LLM does not generate the correct code the first time, by giving the human expert opportunities to amend the generated model.\nAdditionally, the benefit of integrating general knowledge applies beyond representing policies - it can also be used to represent the transition model in the world. One could use a similar technique to develop a sample-efficient model-based reinforcement learning policy where the structure of the world comes from some existing database and the specific parameters are tuned by interacting with the real world.\nFinally, although current experiments have shown that vanilla gradient descent works on KIM with some nonlinearity, more work needs to be done to investigate how well this approach scales to more complex domains where the connections in the structure are more complex."}, {"title": "6 Conclusion", "content": "In this work, we proposed Knowledge Informed Models (KIM) that combine expert demonstrations with general domain knowledge by instantiating a policy structure from the general knowledge before tuning its parameters with expert demonstrations. This bridges the gap between the semantic knowledge human experts typically possess and the unstructured model architectures that are used for behavior cloning. We detailed how an LLM can be used to enable structure generation and how it could be learned from gradient descent once the initial values of the parameters are set. Through the Lunar Lander and Car Racing tasks, we show that our approach is more sample-efficient than an unstructured baseline and also more robust to noisy environments after training. We have also presented qualitatively how having a structure enables more robustness to imperfect expert demonstrations. Finally, we discussed the limitations of this work and how it can be extended in the near future."}, {"title": "Technical Appendix", "content": null}, {"title": "A More Results for Learning with a Few Demonstrations", "content": "Figure 8 shows the reward in the Lunar Lander task and the coverage in the Car Racing task when learning from a few demonstrations. Overall they show a very similar trend to the plots included in the main text.\nThe specific reward definition for the Lunar Lander is not explicitly disclosed in the Gymnasium descriptions, but is correlated to the distance to the landing pad, the current speed of the lander, the tilt of the lander, whether the engines are activated, whether the lander has crashed or landed successfully. This is not as indicative as the success rate for the Lunar Lander environment as the expert policy does not specifically optimize for reward, and there should be no expectation that the behavior learning model should achieve a high reward.\nThe main reason for not achieving perfect coverage is typically due to taking the shortcut through corners, such that not all tiles on the corners are visited. In some cases, for instance, in the qualitative example presented in the main content, the race car loses control and spins off the track into an unrecoverable state."}, {"title": "B More Qualitative Examples of Car Racing", "content": "Figure 9 shows more samples of the baseline policy and KIM. It is shown that KIM policy is more stable during corning. By contrast, sometimes the neural net policy takes the turn too loose (Figure 9a) while sometimes it takes the turn too tight (Figure 9b). All of these samples show qualitatively why KIM has lower variance and is more robust to action noise."}, {"title": "CGPT Prompts", "content": "The following is used as the system prompt:\nImplement pytorch models that follow the specified structure of the user.\nThe user will provide the following:\n* [Structure Description] explains the connections / operations between each variables / features\n* [Features] explains how to interpret the input to the model. For example, which dimension of the input corresponds to which feature, and their type (discrete or continuous)\n* [Output Space) explains the action space\nYou will do the following steps before giving the final implementation:\n* [Variables] Extract and list the intermediate (latent) variable (if any) from the user's description. Also, indicate the shape or type of each variable.\n* [Plan the connections) List the variables (and their type and shape) in the order in which they should be computed, where the variables based only on the input features are listed first, then the variables that depend on those, etc. For each of them, explain how they can be computed using the previously listed variables or inputs to the model. Also indicate whether the new feature is positively correlated to the previous feature or negatively correlated respectively. Be very specific. List the functions or operators that should be used to connect the variables. If you decided to use a linear combination, explain why a bias term is included or not included.\n* [Code] Provide the implementation of the model as a subclass of `nn.Module`. Do not include examples. No need to explain the code.\nUse the following format:\n[Variables]\n* Name of the first variable (shape and type)\n* Name of the second variable (shape and type)\n[Connections]\n* Name of the first variable that should be computed (shape and type)\n depends on <feature 1 name> (positively correlated), <feature 2 name> (nagatively correlated), can be computed with a linear combination of and with a bias term the bias term is included because\n* Name of the second variable that should be computed (shape and type)\n depends on <feature 1 name> (negatively correlated), <feature 2 name> (positively correlated), can be computed using torch.where on\n[Code]\n```py\nimport torch\nfrom torch import nn\nclass ModelName (nn.Module):\n def forward(self, x):\n pass\n# Your MODEL DEFINITION\n```\nNotes:\n* Represent new features as linear combinations of old features when possible. If you are very certain no bias term is needed (i.e., the feature value should be 0 if all inputs are 0), then don't include the bias term to make it easier to learn. Register all weights and bias terms as `nn.Parameter` with `required_grad=True`.\n* There might be cases where linear combination is not sufficient, then you may use other operations such as multiplication to represent the interaction between two features.\n* When initializing the values for each weight and bias, set a value based on positive or negative correlation (e.g., if the input feature is negatively correlated then set its weight to -0.1) instead of using random initialization.\n* There should be no constant number in the forward function. If there is a parameter that can't be learned by gradient descent (e.g., the bounds in the clamp function), then label and register it as a `nn.Parameter` in the model class. Also, use the comment to label it as a \"non-gradient parameter\".\n* You may use any of the functions defined in pytorch (e.g., `torch.logical_and`, `torch.clamp`, `torch.abs`, `torch.square`, etc.).\n* Make sure the gradient can flow back to the parameters. Avoid in-place operations (use new variable names instead) or constructing new tensors (use `torch.stack` or `torch.cat` instead).\n* You may assume the inputs are already normalized.\n* For discrete output space, the model should output a (potentially unnormalized) distribution among those discrete actions. For continuous action space, return the predicted action without worrying about distributions, but clip the values to the appropriate range (if specified).\n* At any point in the process, if you are unsure about something, or if there is some ambiguity, then state so and ask a clarification question instead of proceeding.\n```\nThe following is the prompt used for the Lunar Lander task\n[Structure Description]\nThe lander is in air if none of its legs are in contact with the ground. Otherwise, it is in contact with the ground.\nThe target heading of the lander depends on its horizontal coordinate and speed so that it points to the center. But we will clip in a range such that it stays roughly in the middle, because tilting too much is bad.\nThe target vertical coordinate depends on the magnitude of the horizontal offset of the lander. The further the lander is to the landing pad (which is at $(0, 0)$), the higher the target vertical coordinate should be.\nThe heading adjustment depends on the difference between the current and clipped target heading of the lander as well as the current angular velocity.\nThe speed adjustment needed to put the lander to rest is proportional to its vertical speed.\nAnd the vertical adjustment depends on the difference between the current and target vertical coordinate as well as the vertical speed.\nOnly activate the left or the right engine when the lander is not contacting the ground. And the probability of activating the left engine is the heading adjustment, and symmetrically the probability of activating the right engine is the negation of the heading adjustment.\nThe probability of activating the main engine in air is the vertical adjustment.\nThe probability of activating the main engine when the lander is in contact with the ground is the speed adjustment.\nThere is a base level probability that the lander will do nothing regardless of the input.\n[Features]\nThe input to the model is a tensor of $(8)$. The features of the lander in each dimension are:\n0. (float32) horizontal coordinate $x$\n1. (float32) vertical coordinate $y$\n2. (float32) horizontal speed $v_x$\n3. (float32) vertical speed $v_y$\n4. (float32) heading $\\theta$\n5. (float32) angular velocity $\\omega$\n6. (bool) whether the left landing leg is in contact with the ground\n7. (bool) whether the right landing leg is in contact with the ground\n[Output Space]\nA tensor of shape (4) representing the unnormalized distribution among the following four actions\n0: do nothing\n1: fire left orientation engine\n2: fire main engine\n3: fire right orientation engine\n[Additional Notes]\nThe lander is upright when $\\theta = 0$ and is tilting to the left when $\\theta > 0$. When the lander is falling $v_y < 0$ since the y-axis points upward.\n```\nThe following is the prompt used for the Car Racing task.\n[Structure Description]\nFirst, find the tile that is close to the race car using the tile's xy coordinates (the race car is at the origin). Then the tiles close to the race car are defined as the consecutive tiles that follow the closest tile up until some number threshold. Similarly, the tiles ahead of the race car are also the consecutive tiles that follow the closest tile up until some larger threshold or the end of the sequence.\nThe curvature of two consecutive tiles is defined as the difference between the angle of the tiles (normalized to (-1, 1)) and will be provided. It is signed with a negative value corresponding to the track bending to the left, and a positive value means bending to the right. There is a corner if the absolute value of the curvature is greater than some threshold. The sharpness of the corner is the magnitude of the curvature (e.g., -0.99 corresponds to a sharp left turn).\nThe target speed when there are no corners ahead can be represented as a constant.\nThe target speed when there is at least one corner tile ahead is the minimum of the corresponding target speed of each tile. It depends on the magnitude of its lateral distance to the race car, the magnitude of the heading difference, longitudinal distance, and curvature. Specifically, the larger the lateral distance is, the faster the target speed can be because the car doesn't need to make a sharp turn. The larger the longitudinal distance is, the faster the target speed is because the car has more time to slow down later. However, the sharper the corner the lower the target speed should be. And the larger the heading difference is the lower the speed should be because the race car has to steer more.\nThe target heading of the race car is related to the average heading and curvature of the tiles close to the race car as well as the average lateral position of the race car relative to those tiles. If the lateral position is negative then the race car's target heading is proportionally positive to re-center the race car, and vice versa. But the target heading should also follow the heading and the curvature of the close tiles.\nThe race car is controlled by three dimensions: steer, accelerate, and brake. Each of them should be controlled by a linear controller that takes in the difference between the target and the current state of the race car's heading and speed and outputs a control signal. However, the output of steering should be scaled based on the current speed such that when speed approaches $1$ the steer magnitude should approach $0$. That is, when the current speed is high the steer magnitude should be discounted so that the race car does not lose control, and when the speed is low it can steer with more magnitude.\n[Features]\nThe input to the model is two tensors representing the tracks and other indicator information:\n0. (float32, (L, 8)) tiles. Where $L$ is the number of tiles on the track and the last dimension contains $(x, y,, \\theta, \\delta_\\theta, _)$ where $x$ is the signed lateral position of the tile, $y$ is the signed longitudinal position of the tile, $\\theta$ is the heading of the tile, and $\\delta_\\theta$ is the different in heading between neighboring tiles (curvature). Other features from input (marked by $_$) can be ignored.\n1. (float32, (7,)) indicators. Where the first column is the speed $v$, and the second column is the current heading of the race car $\\theta$ while the other features are not important.\n[Output Space]\nA tensor of shape (3) where each element corresponds to steer, accelerate, and brake controls of the race car respectively. The steer value should be in the range of $(-1, 1)$, while the accelerate and brake should be in the range of $ (0, 1)$.\n[Additional Notes]\nThe track is a loop."}, {"title": "D Models generated by GPT", "content": "The following is the code generated by GPT for the Lunar Lander task (formatted by an automatic formatter Black[\u0141ukasz\nLanga and the Black team, 2020]).\n```py\nimport torch\nfrom torch import nn\n\n\nclass LanderModel(nn.Module):\n def __init__(self):\n super().__init__()\n # Learnable parameters for linear combinations\n self.target_heading_weights = nn.Parameter(\n torch.tensor([0.1, 0.1]), requires_grad=True\n )\n self.target_heading_bias = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n self.heading_adjustment_weights = nn.Parameter(\n torch.tensor([-0.1, 0.1, -0.1]), requires_grad=True\n )\n self.heading_adjustment_bias = nn.Parameter(\n torch.tensor(0.0), requires_grad=True\n )\n self.speed_adjustment_weight = nn.Parameter(\n torch.tensor(-0.1), requires_grad=True\n )\n self.vertical_adjustment_weights = nn.Parameter(\n torch.tensor([-0.1, 0.1, -0.1]), requires_grad=True\n )\n self.vertical_adjustment_bias = nn.Parameter(\n torch.tensor(0.0), requires_grad=True\n )\n self.prob_nothing = nn.Parameter(torch.tensor(0.1), requires_grad=True)\n # Non-gradient parameters\n self.target_heading_clip = nn.Parameter(\n torch.tensor((-0.5, 0.5]), requires_grad=False\n )\n\n def forward(self, x):\n # Extract features\n```\n\nThe following is the code generated by GPT for the Car Racing task\n```py\nimport torch\nfrom torch import nn"}, {"title": "E Training Details", "content": "We use the Adam optimizer [Kingma and Ba, 2017", "1,1": "for both the baseline and KIM in the Car Racing task.\nA weight-balanced cross-entropy loss is used for the Lunar Lander task for both conditions. Mean squared error loss is used for"}]}