{"title": "DISENTANGLED TRAINING WITH ADVERSARIAL EXAMPLES FOR ROBUST\nSMALL-FOOTPRINT KEYWORD SPOTTING", "authors": ["Zhenyu Wang", "Li Wan", "Biqiao Zhang", "Yiteng Huang", "Shang-Wen Li", "Ming Sun", "Xin Lei", "Zhaojun Yang"], "abstract": "A keyword spotting (KWS) engine that is continuously running on\ndevice is exposed to various speech signals that are usually unseen\nbefore. It is a challenging problem to build a small-footprint and\nhigh-performing KWS model with robustness under different acous-\ntic environments. In this paper, we explore how to effectively ap-\nply adversarial examples to improve KWS robustness. We propose\ndatasource-aware disentangled learning with adversarial examples\nto reduce the mismatch between the original and adversarial data\nas well as the mismatch across original training datasources. The\nKWS model architecture is based on depth-wise separable convo-\nlution and a simple attention module. Experimental results demon-\nstrate that the proposed learning strategy improves false reject rate\nby 40.31% at 1% false accept rate on the internal dataset, compared\nto the strongest baseline without using adversarial examples. Our\nbest-performing system achieves 98.06% accuracy on the Google\nSpeech Commands V1 dataset.\nIndex Terms: small-footprint keyword spotting, simple attention\nmodule, disentangled learning, adversarial examples, depth-wise\nseparable convolution", "sections": [{"title": "1. INTRODUCTION", "content": "With the proliferation of voice assistant devices, the development of\nefficient and accurate keyword spotting (KWS) systems has attracted\nmuch attention in the literature. These devices rely heavily on an on-\ndevice KWS which correctly 'triggers' the system to send audio into\nthe cloud for interpretation. Due to constrained hardware resources,\non-device KWS systems need to achieve high performance in vari-\nous acoustic environments with a small memory footprint and high\ncomputation efficiency. Advances in this area are a considerable in-\nfluence on the perceived user experience of the device, as a failure\nto wake up on a trigger attempt is frustrating and a false wake on\nambient noise can be considered as an infraction of user privacy.\nConventional approaches to KWS are based on large vocabulary\ncontinuous speech recognition (LVCSR), targeting efficient keyword\nsearch from the lattices [1, 2]. However, the LVCSR-based tech-\nnique often consumes high computational resources and is inappli-\ncable to on-device KWS systems. Hidden Markov Models (HMM)\nhave been a commonly used technique for building small-footprint\nand low-latency KWS systems, which takes either keyword [3, 4]\nor monophone [5, 6] as modeling units. More recently, deep neu-\nral networks (DNNs) have been adopted to predict word units in the\nkeyword for each frame [7]. Convolutional neural networks (CNNs)\nhave seen great success in small-footprint tasks due to its effective\nrepresentation of time-frequency structure and low memory band-\nwidth requirements [8] [9] [10].\nA KWS system is continuously exposed to various audio sig-\nnals that are mostly unseen before. Due to the high variability of\nthe acoustic environment in the aspects of SNRs, noise types, and\naccents, it is a challenging problem to build a robust KWS system\nthat accurately wakes up anytime when the keyword is spoken but\nreliably suppresses most of the incoming negative audio. Adver-\nsarial examples as a free resource have been widely applied to im-\nprove model robustness in different tasks by attacking model vul-\nnerability [11] [12] [13]. They are crafted by adding imperceptible\nperturbations to mislead a well-trained neural network model [14].\nResearchers have found that models trained with adversarial exam-\nples exhibit unexpected benefits, such as meaningful feature repre-\nsentations that align better with salient data characteristics [15] as\nwell as enhanced robustness to corruptions concentrated in the high-\nfrequency domain [16]. To better leverage adversarial examples for\ntraining, Xie et al., have proposed to use an auxiliary batchnorm\n(BN) specifically for the adversary [17]. The statistics of the origi-\nnal and adversarial data distributions hence could be more accurately\nestimated for effective modeling.\nIn this work, we focus on how to effectively apply the generated\nadversarial examples to improve the robustness of a small-footprint\nKWS system. While [18] has explored to use adversarial examples\nas augmentation data to improve an attention-based KWS system,\nour interest concentrates on reducing the mismatch between the orig-\ninal training data and adversarial examples during training. Inspired\nby the effectiveness of the auxiliary BN approach in [17], we propose\ndatasource-aware disentangled adversarial training. Specifically, we\ndesign a different auxiliary BN for each type of datasource in the\ntraining data and the corresponding adversarial attackers, such that\nthe complementarity across different datasources could be fully ex-\nploited. The model architecture of our KWS system is built from the\nbottleneck residual block [19] that hinges on depth-wise separable\nconvolutions [20]. We further extend the residual block by inject-\ning a parameter-free simple attention model (SimAM) [21]. SimAM\nleverages a 3-dimensional attention map to refine the intermediate\nfeature map in a CNN layer, so as to effectively increase model ca-\npacity with negligible computation cost. The experimental results\nhave shown the effectiveness of the proposed training approach with\nadversarial examples. On the internal dataset, we have improved\nfalse reject rate by 40.31% at 1% false accept rate, compared to\nthe strongest baseline without using adversarial examples. On the\nGoogle Speech Commands V1 dataset, our best KWS system has\nachieved 98.06% accuracy."}, {"title": "2. MODEL ARCHITECTURE", "content": "2.1. Bottleneck Residual Block\nOur KWS model adopts the bottleneck residual block in the Mo-\nbileNetV2 architecture [19] which is tailored to efficient and ef-\nfective modeling on mobile devices. The bottleneck residual block\nhinges on the depth-wise separable convolution and is supported by\nthe linear bottleneck transformation and inverted residual connec-\ntion.\nThe structure of a bottleneck residual block is presented in Fig.\n1. Each block is parameterized by its expansion factor t, the number\nof output channels n, and the stride s. Given an input of shape h x\nw x m, the initial pointwise convolution expands the input from m\nchannels to tm channels followed by a 3 \u00d7 3 depth-wise convolution\nof stride s as well as a linear bottleneck transformation.\nThe base model in this work is MN7-45, which is a variant of the\nMobileNetV2 architecture optimized for the KWS task. The param-\neters of MN7-45 are defined in Table 1. It consists of an initial stan-\ndard convolution with 45 filters followed by 7 bottleneck residual\nblocks, global average pooling and a final output layer. Compared\nto MobileNetV2, MN7-45 has fewer bottleneck residual blocks but\nwith larger width in the initial layers, which we found to be more\neffective for the KWS task."}, {"title": "2.2. Simple Attention Module", "content": "Plug-and-play attention modules [22, 23, 24] as an effective compo-\nnent can refine intermediate feature maps within a CNN block, so\nas to boost the model capacity. The parameter-free simple attention\nmodule (SimAM) has shown the flexibility and effectiveness to im-\nprove the learning capacity of convolution networks with negligible\ncomputation cost [21]. It infers 3-D attention weights for the feature\nmap in a convolution layer by optimizing an energy function to cap-\nture the importance of each neuron. Specifically, the minimal energy\nof a neuron x in an input feature map $X \\in R^{C \\times H \\times W}$ is expressed\nas:\n$e_x = \\frac{4(\\hat{\\sigma}^2 + \\lambda)}{(\\hat{x} - \\hat{\\mu})^2 + 2 \\hat{\\sigma}^2 + 2\\lambda},$ (1)\nwhere E groups all energy values of $e_x$, and $\\sigma(\\cdot)$ denotes the sig-\nmoid function. In this work, we plug a SimAM after the depthwise-\nwise convolution in each residual block of the base model MN7-45.\nwhere $\\hat{\\mu} = \\frac{1}{H \\times W} \\sum_{i=1}^{H \\times W} x_i$, $\\hat{\\sigma}^2 = \\frac{1}{H \\times W} \\sum_{i=1}^{H \\times W} (x_i - \\hat{\\mu})^2$, and\n$\\lambda$ is a hyper parameter. The statistics $\\hat{\\mu}$ and $\\hat{\\sigma}$ are shared across all\nneurons within a channel, which hence significantly reduces compu-\ntation cost. As neuroscience study indicates an inverse correlation\nbetween the energy of $e_x$ and the importance of each neuron x [25],\nthe refinement of a feature map can be formulated as,\n$\\hat{x} = \\sigma(\\frac{1}{E}) \\odot x,$\n(2)"}, {"title": "3. DISENTANGLED ADVERSARIAL TRAINING", "content": "3.1. Adversarial Examples\nAdversarial examples are generated by adding imperceptible but ma-\nlicious perturbations to the original data, such that the well-trained\nneural network can be misled to make an incorrect prediction [26].\nIn this work, we use the multi-step attacker based on Projected Gra-\ndient Descent (PGD) for adversarial data generation [12]. Given an\ninput training sample x \u2208 D with the corresponding ground-truth\nlabel y, a strong adversary is generated in an iterative manner,\n$x^{adv}_{t+1} = \\Pi_{S}(x^{adv}_{t} + \\epsilon \\cdot sgn(\\nabla_x L(\\theta, x^{adv}_{t}, y))),$ (3)\nwhere II stands for a projection operator, S denotes the allowed per-\nturbation space, $\\epsilon$ is the step size, L(,,) is the loss function, and\n$\\theta$ represents the model parameters. In our work, we use 8 steps to\ngenerate an adversary. We treat the adversarial examples $x^{adv}$ from\nEq. 3 as augmented data, and mix them with the original data for\ntraining, i.e.,\n$arg min_{\\theta} [E_{(x,y)\\sim D}(L(\\theta, x, y)) + L(\\theta, x^{adv}, y)].$ (4)\n3.2. Disentangling via An Auxiliary BN\nPrevious research work on adversarial attacks has found that train-\ning with adversarial examples could lead to label leaking, i.e., the\nneural network overfits to the specialized adversary distribution, re-\nsulting in degraded model performance [11] [26]. To better leverage\nthe regularization power of adversarial data, Xie et al., proposed dis-\nentangled training via an auxiliary batchnorm (BN) to decouple the"}, {"title": "3.3. Fine-grained Disentangled Adversarial Training", "content": "Adversarial example generation can be easily generalized to a fine-\ngrained version. Instead of generating one adversarial example per\ntraining sample, we craft multiple adversaries at different pertur-\nbation levels ($\\epsilon$ of Eq. 3 in a range of [0.1, 0.4]) to capture a\nbroader picture of training data statistics. Following basic disen-\ntangled learning in Section 3.2, we maintain one main BN for the\noriginal training data and a different auxiliary BN for the adversary\nat each perturbation level."}, {"title": "3.4. Datasource-aware Disentangled Adversarial Training", "content": "We further extend the disentangling concept beyond the adversarial\nexamples. The training data for KWS modeling usually has a multi-\naugmentation composition including clean audio, audio augmenta-\ntion with background noise and speaking speed, as well as spectrum\ndistortion with SpecAugment [27]. Each augmentation type could\nbe considered as one datasource. Our premise is that the distribu-\ntion distinction exists not only between the adversary and original\ntraining data but also among the augmentation datasources within\nthe original training data. The datasource-aware disentangled adver-\nsarial training hence aims to reduce all the possible mismatches. As\nillustrated in Fig. 2, we use one main BN for the clean data DS1\nand apply auxiliary BNs respectively for each augmentation data-\nsource DSn (n > 1) as well as for the corresponding adversarial\nexamples Advn (n \u2265 1). As described in Section 3.2, we keep the\nmain BN at the evaluation stage by removing all the auxiliary BNs,\nassuming that the testing data is more likely to follow the clean data\ndistribution rather than the artificial augmentation distribution. The\nablation study presented in Section 5.1 investigates the effectiveness\nof datasource-aware disentangled learning."}, {"title": "4. EXPERIMENTS", "content": "4.1. Data Description\nInternal Dataset The internal aggregated and de-identified multi-\nkeyword dataset used in the experiments includes speech samples of\nfour keywords, i. e., \u201ctake a picture\u201d, \u201cvolume down\u201d, \u201cvolume up\u201d,\nand \u201cplay music\u201d, collected through crowd-sourced workers. The\ntraining set contains 130K positive samples with about 32K in each\nkeyword category and 100K negative samples. The test set has 32K\npositive samples with about 8K for each keyword and 10K negative\nsamples. 6 English accents are involved, including United States\n(US), Austraila (AU), Canada (CA), New Zealand (NZ), Britain\n(GB), and Latin America (LA). Training data includes the US ac-\ncent only, and the other accents are used for testing.\nGoogle Speech Commands Dataset V1 The dataset consists of\n1s audio snippets recorded at sample rate 16kHz in natural environ-\nments [28], including 64, 727 samples of 30 different words from"}, {"title": "4.2. Experimental Setup", "content": "In the experiments, we consider two popular augmentation base-\nlines: NoiseAugment and SpecAugment [27], in addition to the\nvanilla training using the clean data. The training data is distorted\nwith various background noises (speech and music) at SNR sampled\nfrom [0dB, +20dB]. The testing data is also distorted at SNR of\n10dB and 20dB in a similar manner but with different noise types.\nWe further use SpecAugment on top of the noise augmented train-\ning data. Training with adversarial examples (see Section 3) is ap-\nplied to the 3-datasource training data including clean, noisy, and\nSpecAugmented datasources. Specifically, we compare the perfor-\nmance of adversarial training without disentangled learning (AT) in\nEq. 4 [18], disentangled adversarial training (DAT), fine-grained dis-\nentangled adversarial training (FG_DAT), and datasource-aware dis-\nentangled adversarial training (DA_DAT). We use MN7-45 (see Sec-\ntion 2.1) as the base model based on which we also investigate the\neffectiveness of SimAM for KWS modeling.\nWe extract acoustic features using 40-dim log Mel-filterbank en-\nergies that are computed over a 25ms window every 10ms. In model\ntraining, we use an input window of 900ms (90 frames) for the inter-\nnal dataset and 1s (100 frames) for Google speech commands dataset\nwith the output of the corresponding keyword classes. The model in-\nference is performed in a sliding window manner with a shift of 10\nframes in all experiments. We use the Adam optimizer with a learn-\ning rate of 0.005 and cosine annealing learning rate decay. We train\nthe models for 15 epochs. We perform PGD attackers with perturba-\ntion strength $\\epsilon$ in a range of [0.1, 0.4]. We find that $\\epsilon$ = 0.2 achieves\nthe best performance on the internal dataset and $\\epsilon$ = 0.1 performs\nthe best on the Google dataset. The hyper-parameter $\\lambda$ of SimAM in\nEq. 1 is set to 0.0001. False reject rate (FRR) and false accept rate\n(FAR) are used for internal data evaluation. We present experimental\nresults by plotting detection error trade-off (DET) curves, where the\nx-axis and y-axis represent FAR and FRR, respectively. Following\nthe setup in the previous work [28] [29], we measure top-1 classifi-\ncation accuracy for the evaluation of the Google dataset."}, {"title": "5. RESULTS AND DISCUSSION", "content": "5.1. Results on Internal Data\nFig. 3 and Table 2 summarize the results of different approaches on\ntesting data at SNR of 20dB and 10dB. We can observe that both\nNoiseAugment and its combination with SpecAugment consistently\noutperform the vanilla training on both distorted datasets. Adversar-\nial examples with different training strategies could further improve\nthe performance over the strongest baseline (NoiseAug+SpecAug).\nThis performance boost is especially prominent on the highly noisy\ndata. For example, DA_DAT reduces FRR by 20.7% at 1% FAR on\nthe SNR = 20dB data, and reduces FRR by 40.3% on the SNR =\n10dB data, compared to the baseline using NoiseAug+SpecAug.\nThe results suggest the effectiveness of the adversarial examples for\nstrengthening the generalizability of KWS models in the noisy envi-\nronment.\nDisentangled adversarial training Table 2 shows that DAT has\nexhibited substantial improvement over AT. We can see an FRR im-\nprovement of 7.23% and 13.94% respectively on the SNR = 20dB\nand SNR = 10dB data. This observation has corroborated that the\ndata distribution mismatch does exist between the original data and\nthe adversary, and disentangled learning with an auxiliary BN is ben-\neficial for bridging such mismatch.\nDatasource-aware disentangled adversarial training We further\ncompare the extensions of disentangled learning, i.e., FG_DAT and\nDA_DAT, against DAT. It is interesting to observe that DA_DAT sur-\npasses DAT on FRR by 13.69% and 16.76% respectively on the\nSNR = 20dB and SNR = 10dB data, while FG_DAT performs\nworse than even AT. The performance gain from DA DAT indi-\ncates that the datasource-aware approach allows models to learn rich\ncross-datasource representations and hence increases model robust-\nness against speech distortions. In addition, the auxiliary BN method\nhas demonstrated the effectiveness as a general adaptation technique\nacross datasources. The inferior performance of FG_DAT implies\nthat one perturbation strength is often sufficient for effective attacks\nof one dataset while multiple perturbation strengths could bring con-\nfusion to modeling and degrades model performance.\nSimple Attention Module We update the structure of the base\nmodel MN7-45 by adding a SimAM (see Sec. 2.2) after the depth-\nwise convolution in each residual block. The updated model is\ntrained using DAT and DA DAT which are the most effective train-\ning strategies. It has shown a marginal improvement with DA DAT\non the SNR = 20dB data."}, {"title": "5.2. Results on Google Speech Commands Data", "content": "Table 3 presents results on the Google Speech Commands data: the\ntop-1 accuracy associated with the relative decrease in classifica-\ntion error rate (CER_RD). Similar to Section 5.1, FG_DAT obtains\nan inferior performance. Therefore we omit its results for simplic-\nity. We can observe that adversarial examples are helpful for boost-\ning KWS performance. Specifically, the base model trained with\nDA_DAT achieves an accuracy of 97.81% with 18.28% error rate\nreduction, compared to the baseline. Our best-performing system\nbased on the SimAM module and trained with DA_DAT achieves an\naccuracy of 98.06%."}, {"title": "6. CONCLUSIONS", "content": "In this paper, we explored how to effectively apply adversarial ex-\namples for KWS modeling. We proposed datasource-aware disen-\ntangled adversarial training through multiple auxiliary BNs. Exper-\nimental results on both internal dataset and Google Speech Com-\nmands dataset have demonstrated that adversarial examples as a free\nand infinite data resource could effectively boost KWS performance\nand the proposed training strategy exert the effectiveness to large\nextent."}]}