{"title": "DISENTANGLED TRAINING WITH ADVERSARIAL EXAMPLES FOR ROBUST\nSMALL-FOOTPRINT KEYWORD SPOTTING", "authors": ["Zhenyu Wang", "Li Wan", "Biqiao Zhang", "Yiteng Huang", "Shang-Wen Li", "Ming Sun", "Xin Lei", "Zhaojun Yang"], "abstract": "A keyword spotting (KWS) engine that is continuously running on\ndevice is exposed to various speech signals that are usually unseen\nbefore. It is a challenging problem to build a small-footprint and\nhigh-performing KWS model with robustness under different acous-\ntic environments. In this paper, we explore how to effectively ap-\nply adversarial examples to improve KWS robustness. We propose\ndatasource-aware disentangled learning with adversarial examples\nto reduce the mismatch between the original and adversarial data\nas well as the mismatch across original training datasources. The\nKWS model architecture is based on depth-wise separable convo-\nlution and a simple attention module. Experimental results demon-\nstrate that the proposed learning strategy improves false reject rate\nby 40.31% at 1% false accept rate on the internal dataset, compared\nto the strongest baseline without using adversarial examples. Our\nbest-performing system achieves 98.06% accuracy on the Google\nSpeech Commands V1 dataset.\nIndex Terms: small-footprint keyword spotting, simple attention\nmodule, disentangled learning, adversarial examples, depth-wise\nseparable convolution", "sections": [{"title": "1. INTRODUCTION", "content": "With the proliferation of voice assistant devices, the development of\nefficient and accurate keyword spotting (KWS) systems has attracted\nmuch attention in the literature. These devices rely heavily on an on-\ndevice KWS which correctly 'triggers' the system to send audio into\nthe cloud for interpretation. Due to constrained hardware resources,\non-device KWS systems need to achieve high performance in vari-\nous acoustic environments with a small memory footprint and high\ncomputation efficiency. Advances in this area are a considerable in-\nfluence on the perceived user experience of the device, as a failure\nto wake up on a trigger attempt is frustrating and a false wake on\nambient noise can be considered as an infraction of user privacy.\nConventional approaches to KWS are based on large vocabulary\ncontinuous speech recognition (LVCSR), targeting efficient keyword\nsearch from the lattices [1, 2]. However, the LVCSR-based tech-\nnique often consumes high computational resources and is inappli-\ncable to on-device KWS systems. Hidden Markov Models (HMM)\nhave been a commonly used technique for building small-footprint\nand low-latency KWS systems, which takes either keyword [3, 4]\nor monophone [5, 6] as modeling units. More recently, deep neu-\nral networks (DNNs) have been adopted to predict word units in the\nkeyword for each frame [7]. Convolutional neural networks (CNNs)\nhave seen great success in small-footprint tasks due to its effective\nrepresentation of time-frequency structure and low memory band-\nwidth requirements [8] [9] [10].\nA KWS system is continuously exposed to various audio sig-\nnals that are mostly unseen before. Due to the high variability of\nthe acoustic environment in the aspects of SNRs, noise types, and\naccents, it is a challenging problem to build a robust KWS system\nthat accurately wakes up anytime when the keyword is spoken but\nreliably suppresses most of the incoming negative audio. Adver-\nsarial examples as a free resource have been widely applied to im-\nprove model robustness in different tasks by attacking model vul-\nnerability [11] [12] [13]. They are crafted by adding imperceptible\nperturbations to mislead a well-trained neural network model [14].\nResearchers have found that models trained with adversarial exam-\nples exhibit unexpected benefits, such as meaningful feature repre-\nsentations that align better with salient data characteristics [15] as\nwell as enhanced robustness to corruptions concentrated in the high-\nfrequency domain [16]. To better leverage adversarial examples for\ntraining, Xie et al., have proposed to use an auxiliary batchnorm\n(BN) specifically for the adversary [17]. The statistics of the origi-\nnal and adversarial data distributions hence could be more accurately\nestimated for effective modeling.\nIn this work, we focus on how to effectively apply the generated\nadversarial examples to improve the robustness of a small-footprint\nKWS system. While [18] has explored to use adversarial examples\nas augmentation data to improve an attention-based KWS system,\nour interest concentrates on reducing the mismatch between the origi-\nnal training data and adversarial examples during training. Inspired\nby the effectiveness of the auxiliary BN approach in [17], we propose\ndatasource-aware disentangled adversarial training. Specifically, we\ndesign a different auxiliary BN for each type of datasource in the\ntraining data and the corresponding adversarial attackers, such that\nthe complementarity across different datasources could be fully ex-\nploited. The model architecture of our KWS system is built from the\nbottleneck residual block [19] that hinges on depth-wise separable\nconvolutions [20]. We further extend the residual block by inject-\ning a parameter-free simple attention model (SimAM) [21]. SimAM\nleverages a 3-dimensional attention map to refine the intermediate\nfeature map in a CNN layer, so as to effectively increase model ca-\npacity with negligible computation cost. The experimental results\nhave shown the effectiveness of the proposed training approach with\nadversarial examples. On the internal dataset, we have improved\nfalse reject rate by 40.31% at 1% false accept rate, compared to\nthe strongest baseline without using adversarial examples. On the\nGoogle Speech Commands V1 dataset, our best KWS system has\nachieved 98.06% accuracy."}, {"title": "2. MODEL ARCHITECTURE", "content": "Our KWS model adopts the bottleneck residual block in the Mo-\nbileNetV2 architecture [19] which is tailored to efficient and ef-\nfective modeling on mobile devices. The bottleneck residual block\nhinges on the depth-wise separable convolution and is supported by\nthe linear bottleneck transformation and inverted residual connec-\ntion.\nThe structure of a bottleneck residual block is presented in Fig.\n1. Each block is parameterized by its expansion factor t, the number\nof output channels n, and the stride s. Given an input of shape h x\nw x m, the initial pointwise convolution expands the input from m\nchannels to tm channels followed by a 3 \u00d7 3 depth-wise convolution\nof stride s as well as a linear bottleneck transformation.\nThe base model in this work is MN7-45, which is a variant of the\nMobileNetV2 architecture optimized for the KWS task. The param-\neters of MN7-45 are defined in Table 1. It consists of an initial stan-\ndard convolution with 45 filters followed by 7 bottleneck residual\nblocks, global average pooling and a final output layer. Compared\nto MobileNetV2, MN7-45 has fewer bottleneck residual blocks but\nwith larger width in the initial layers, which we found to be more\neffective for the KWS task."}, {"title": "2.1. Bottleneck Residual Block", "content": null}, {"title": "2.2. Simple Attention Module", "content": "Plug-and-play attention modules [22", "24": "as an effective compo-\nnent can refine intermediate feature maps within a CNN block", "21": ".", "expressed\nas": "n$e_x = \\frac{4(\\sigma^2 + \\lambda)}{(x - \\mu)^2 + 2\\sigma^2 + 2\\lambda}$                                                                                                                                                                                                                                                                                            \\qquad(1)$\nwhere $\\mu = \\frac{1}{H \\times W} \\sum_{i=1}^{H \\times W} x_i$, $\\sigma^2 = \\frac{1}{H \\times W} \\sum_{i=1}^{H \\times W} (x_i - \\mu)^2$, and\n$\\lambda$ is a hyper parameter. The statistics $\\mu$ and $\\sigma$ are shared across all\nneurons within a channel, which hence significantly reduces compu-\ntation cost. As neuroscience study indicates an inverse correlation\nbetween the energy of ex and the importance of each neuron x [25"}]}