{"title": "3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient Batching and Composability", "authors": ["Baohao Liao", "Christof Monz"], "abstract": "Parameter-efficient finetuning (PEFT) methods effectively adapt large language models (LLMs) to diverse downstream tasks, reducing storage and GPU memory demands. Despite these advantages, several applications pose new challenges to PEFT beyond mere parameter efficiency. One notable challenge involves the efficient deployment of LLMs equipped with multiple task- or user-specific adapters, particularly when different adapters are needed for distinct requests within the same batch. Another challenge is the interpretability of LLMs, which is crucial for understanding how LLMs function. Previous studies introduced various approaches to address different challenges. In this paper, we introduce a novel method, RoAd, which employs a straightforward 2D rotation to adapt LLMs and addresses all the above challenges: (1) RoAd is remarkably parameter-efficient, delivering optimal performance on GLUE, eight commonsense reasoning tasks and four arithmetic reasoning tasks with < 0.1% trainable parameters; (2) RoAd facilitates the efficient serving of requests requiring different adapters within a batch, with an overhead comparable to element-wise multiplication instead of batch matrix multiplication; (3) RoAd enhances LLM's interpretability through integration within a framework of distributed interchange intervention, demonstrated via composition experiments.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs), trained on extensive web-scale datasets to perform tasks such as predicting masked words [8, 29, 42] or anticipating the next word in a sentence [16, 49, 50], demonstrate remarkable effectiveness across a range of NLP applications. For tasks where the data"}, {"title": "2 Background", "content": "In this section, we outline the challenges tackled in this work, illustrating the constraints of existing methods and objectives that drive the development of the proposed method, RoAd."}, {"title": "2.1 Parameter-efficient finetuning (PEFT)", "content": "Existing PEFT techniques can be categorized into three groups: adapter-based, prompt-based, and latency-less methods. Adapter-based methods [11, 12, 39] incorporate adapters either in parallel with or sequentially to the existing Transformer [52] modules. This incorporation necessitates modifications to the LLM architecture, consequently adding extra latency during inference. Prompt-based methods [18, 20, 40] enhance the input by appending new trainable tokens, which lengthens the sequence and thereby increases the computational overhead during inference. Latency-less methods, such as LoRA [13] and its variants [21, 25, 62], apply low-rank matrices to adapt the pretrained weights. These matrices can be seamlessly integrated into the existing weight matrices following finetuning, thus preserving the original LLM architecture. Specifically, LoRA adapts an LLM as"}, {"title": "2.2 Batching", "content": "Batching in this context refers to processing multiple heterogeneous requests, each requiring different adapters for inference. This scenario commonly arises when serving personalized or task-specific LLMs. Specifically, we consider a setup where distinct adapters instead of a shared adapter are finetuned for various tasks to achieve optimal performance. During inference, each request in a batch pertains to a different task and necessitates a unique adapter.\nConsider that we have finetuned distinct LoRA modules for b tasks, denoted as {Ai, B\u00ec}=1. For a batch of b requests represented as X \u2208 Rb\u00d71\u00d7d1, where l is the maximum sequence length across the requests, each request requires a different LoRA module. To exploit the parallel processing capabilities of GPUs, the output Z of a linear layer can be computed as follows: First, the output from the pretrained layer is computed as Z\u00ba = torch.mm(X, W\u00b0). Subsequently, the intermediate output from the first low-rank matrix, B \u2208 []Rb\u00d7d1\u00d7r (a concatenation of {B}=1), is obtained as Z = torch.bmm(X, B). The output from the second low-rank matrix, \u00c2 \u2208 Rb\u00d7r\u00d7d2 (a concatenation of {A}=1), follows as Z\u00b9 = torch.bmm(Z), \u00c2). Finally, these outputs are summed to produce Z = Z\u00b0 + Z\u00b9. It is noteworthy that batch matrix multiplication (BMM), as implemented in torch.bmm, often introduces substantial overhead [1], reducing throughput and increasing latency, which adversely impacts user experience in time-sensitive applications.\nIn contrast, prompt-based methods circumvent the use of BMM by appending trainable tokens to each request, simplifying the computational process. However, prompt-based methods with long prompt tokens are difficult to optimize, which degrades performance compared to other PEFTs [13, 14]. (IA)\u00b3 [24] proposes adapting LLM by multiplying the output from a linear layer with a trainable vector, involving only element-wise multiplication for efficient batching. A recent development, FLORA [55], builds on (IA)\u00b3 by employing two low-rank matrices while maintaining element-wise operations. Although our proposed method, RoAd, requires BMM, its sparse structure allows a reformulation of BMM and results in an overhead equivalent to element-wise multiplication."}, {"title": "2.3 Intervention and composability", "content": "Numerous studies [9, 10, 35, 36, 37] have provided support for the linear representation hypothesis [33, 43, 46] that concepts are represented within linear subspaces of neural network representations. To examine if a concept is captured within a linear subspace of a representation, Geiger et al. [10] suggests employing a distributed interchange intervention (DII) defined as:\nDII(b, s, R) = b + RT (Rs \u2013 Rb)"}, {"title": "3 Method", "content": "In this section, we first perform two pilot studies to ascertain the key factor influencing the adaptation of LLMs. Following this, we present our proposed method, the 2D rotary adaptation (RoAd), which serves as an effective PEFT method addressing the various challenges outlined in Section \u00a72."}, {"title": "3.1 Pilot study", "content": "Study 1: Variations in magnitude and angular displacement. Assume x, x \u2208 Rd1 are representations of the same token from a pretrained and finetuned LLM, respectively. We define the relative change in magnitude as \u2206M = |||x||2 - ||XQ||2| /||x\u00ba ||2 and compute the angular displacement as AD = cos(x, x\u00ba) \u2208 [-1,1]. A larger AM and a smaller AD indicate more significant changes in magnitude and angular displacement, respectively. Our study involves: (1) finetuning RoBERTa-base [29] on the SST-2 task [47] using either full finetuning or LoRA; (2) extracting representations x and x from the output of the second-last Transformer block for the [CLS] token across all samples in the development set, followed by computing AM and AD.4 As depicted in Figure 2 (Left and Middle), there is a more pronounced change in AD than in AM for both full finetuning and LoRA.5\nStudy 2: Disentanglement of magnitude and angle. To ascertain whether angular or magnitude adjustments are more critical for finetuning, we implement a disentanglement study. This involves freezing ROBERTa-base and appending a two-layer classifier on top of it. The first layer of this classifier incorporates a weight matrix W \u2208 Rd1\u00d7d1. Under standard operations, the output from this layer is computed as z = WTx0. To distinctly evaluate the impacts of magnitude and angle, we modify the output to retain only the magnitude component as zi = ||W:,i ||2. ||x0||2, or solely the"}, {"title": "3.2 2D rotary adaptation", "content": "Suppose that W\u00b0 \u2208 Rd1\u00d7d2 is the pretrained weight of a linear layer, x \u2208 Rd1 is the input of a token to this linear layer, R \u2208 Rd2\u00d7d2 is the rotation matrix, the adapted output from the linear layer is z = Rh = R(WTx). The rotation matrix R is defined as follows:\nR= diag(R1, ..., Ri,..., Rd2/2) with Ri = \\begin{bmatrix} cos \\theta_i & -sin \\theta_i \\\\ sin \\theta_i & cos \\theta_i \\end{bmatrix} \n\nThe trainable parameters are denoted as {0}42/2. This 2D rotary adaptation involves rotating pairs of adjacent dimensions of h, specifically dimensions 2i \u2013 1 and 2i, using the rotation matrix R\u2081.6 The rotation matrix R is characterized by its parameter efficiency, which is attributed to its sparse structure and the parameter sharing within each block Ri. Additionally, R can be integrated directly into the existing pretrained weights, forming W = W\u00b0RT, which does not incur additional computational costs during inference. This design closely mirrors ROPE [48], with the notable difference that in our RoAd, \u03b8i is trainable and Ri does not incorporate positional information.\nRelaxation to orthogonality. Referring to Figure 2 (Right), while reliance predominantly on angular information substantially outperforms reliance on magnitude information, it remains less effective than using both angular and magnitude information for the tasks of MRPC, STS-B, and CoLA. Furthermore, both fully- and LoRA-finetuned LLMs exhibit slight adaptations in magnitude, as depicted in Figure 2 (Left and Middle). Consequently, we modify R\u2081 by incorporating a\u017c to regulate the magnitude. We define a general Ri as follows:\nRi = \\begin{bmatrix} ai,11 cos \\theta_{i,11} & -ai, 12 sin \\theta_{i,12} \\\\ ai,21 sin \\theta_{i,21} & ai,22 cos \\theta_{i, 22} \\end{bmatrix} \n\nWe develop three variants of RoAd by altering the configuration of shared parameters as outlined in Table 1. RoAd\u2081 introduces a minimal change to Equation 2 by incorporating a scaling factor Ai. RoAd\u2081 already shows impressive results for most tasks in Section \u00a74.1. For some knowledge-intensive tasks, we observe that RoAd2 and RoAd4 obtain better results with more trainable parameters. To preserve the starting point of LLMs [22], we always initialize a\u2081 = 1 and 0\u2081 = 0.\nBatching. In practice, we don't need to save Ras a sparse matrix and do ma-\ntrix multiplication. Taking RoAd\u2081 as an example, we only need to save two vec-\ntors: R\u00b9 = (a1 cos 01, a1 cos 01, a2 cos 02, A2 cOS 02, ..., Ad2/2 COS 0d2/2, &d2/2 COS 0d2/2) and R2 = (a1 sin 01, 01 sin 01, a2 sin 02, 02 sin 02, ..., Ad2/2 Sin 0d2/2, d2/2 Sin 0d2/2). Then z = R\u00b9 \u00ae h + R\u00b2 h, where h is a rearranged version of h given by h = (-h2, h1, -h4, h3, ..., -hd2, hd2\u22121) and denotes element-wise multiplication. This reformulation not only simplifies the representation of R but also enhances the efficiency of batching operations in RoAd, relying solely on element-wise multiplications rather than BMM.\nComposability. RoAd can be incorporated into the DII framework as \u03a6(h) = Rh = h + R(h Rh), with Rs in Equation 1 being set to h. Although a degree of relaxation is introduced to the orthogonality of R, it is important to note that the rows of R remain orthogonal to each other within non-adjacent segments of the same block, Ri. This offers a possibility for composability. We can finetune some rows on one task and other orthogonal rows on another task. Since they are orthogonal to each other, these two tasks should minimally affect each other, and the combination of these rows after finetuning could bring new multitasking learning ability.\nRoAd can be considered as a special case of OFT [41] with w = 2. However, it is much more parameter- and memory-efficient and faster."}, {"title": "4 Experiments", "content": "In this section, we begin by implementing RoAd to finetune various LLMs across three benchmarks. Subsequently, we illustrate its efficiency in batching processes and demonstrate its composability. Unless otherwise noted, RoAd is applied to all linear layers within the LLMs. All of our experiments are conducted on A100 80GB GPU with the frameworks, Transformers [56] and PEFT [32]."}, {"title": "4.1 Results on downstream tasks", "content": "Natural language understanding (NLU). We evaluate the effectiveness of RoAd on the GLUE benchmark [53] for its ability of NLU with RoBERTa [29] as the backbone. Unlike many previous works [13, 21, 22, 29, 62] that employ the GLUE development sets for both validation and testing, here we partition the development set into distinct validation and test subsets to mitigate the risk of overfitting. For comprehensive information regarding the split of the development set, the search space of hyperparameters, the optimal hyperparameter configurations, and other details crucial for reproducibility, please see Section \u00a7C.1."}, {"title": "4.2 Efficiency results for batching", "content": "We commence by highlighting the significance of weight merging for PEFT. Among the approaches discussed in Section \u00a74.1, only LoRA [13], DORA [25], BOFT [28], OFT [41], BitFit [59], (IA)\u00b3 [24], and our proposed RoAd enable the integration of trainable parameters with pretrained parameters without incurring additional inference overhead. As an illustration, we consider LORA both with and without weight merging to underscore this process's importance. Notably, the implementation of LoRA with merged weights effectively reverts to the original LLM. To assess throughput, we configure the system with a batch size of 1, generate 2048 tokens, and apply the LoRA modules across all linear layers. Figure 3 (Left) clearly illustrates that the unmerged LoRA exhibits a significantly smaller throughput compared to the merged LoRA. Additionally, it is evident that the throughput of the unmerged LoRA demonstrates only a weak correlation with the rank size, primarily due to the fact that the additional overhead is largely attributed to communication instead of computation.\nFurthermore, to evaluate the throughput of batching, we establish a default batch size of 8, generate 2048 tokens, and set the LoRA rank to 8. Each request within the batch is heterogeneous, necessitating eight distinct sets of trainable parameters by default. We only compare to LoRA here, because other baselines have either a weaker performance on downstream tasks (BOFT, OFT, BitFit and (IA)\u00b3) or a smaller throughput than LoRA for batching (DoRA). As shown in Figure 3 (Middle and Right), RoAd significantly outperforms LoRA with variations in either the number of generated tokens or the number of heterogeneous requests. With an increasing number of distinct requests, the gap between LORA and RoAd becomes even larger, which shows RoAd's unique advantage in efficient serving"}, {"title": "4.3 Qualitative results for composability", "content": "In our investigation of RoAd's ability to handle compositional tasks, we primarily engage in multilingual experiments similar to those conducted by Wu et al. [58]. We use two training datasets: a new"}, {"title": "5 Conclusion", "content": "Initially, our research examines how finetuning modifies the representation of pretrained LLMs, finding that angular adjustments are more significant than changes in magnitude scale. Leveraging this insight, we propose a PEFT method, RoAd, which primarily utilizes a 2D rotational adjustment to the representation. Despite its simplicity, RoAd exhibits several distinct advantages: (1) It is exceptionally efficient in terms of parameters, consistently delivering superior performance on downstream tasks with the fewest trainable parameters compared to other PEFT methods; (2) RoAd efficiently supports batch processing, achieving twice the throughput of LoRA; (3) When incorporated within an intervention framework, RoAd demonstrates remarkable composability.\nDue to page limit, we discuss the limitations and broader impacts in Section \u00a7A and \u00a7B, respectively."}, {"title": "A Limitations", "content": "We recognize that a primary constraint of our study is the restricted evaluation of downstream tasks, predominantly focused on natural language applications. This limitation primarily stems from constrained computational resources. In future work, we aim to expand our evaluation to include additional tasks such as image-to-text and text-to-image tasks. A second limitation pertains to the scalability of RoAd. Currently, it is not feasible to indefinitely increase the number of trainable parameters with RoAd. Nevertheless, our experiments demonstrate that RoAd4 already exhibits commendable performance. Exploring the integration of RoAd with other PEFT methods, such as LORA, may offer potential avenues for enhancing scalability. We leave this to future work."}, {"title": "B Broader impacts", "content": "RoAd's primary advantage is its efficiency in adapting LLMs to specific tasks with minimal trainable parameters. This efficiency not only reduces computational resource needs but also makes advanced AI technologies more accessible to organizations with limited resources, potentially democratizing AI capabilities across smaller enterprises and educational institutions. By reducing the number of trainable parameters and the computational load, RoAd likely decreases the energy consumption associated with training and deploying LLMs. This could contribute to lowering the carbon footprint of AI research and deployment, aligning with greater environmental sustainability efforts. The ability to process multiple heterogeneous requests efficiently means that applications can provide personalized, context-specific responses more quickly. This enhances the user experience in real-time applications, such as digital assistants, automated service, and interactive educational platforms.\nWhile RoAd improves interpretability in some aspects by integrating within frameworks like distributed interchange intervention [10], the overall complexity of the methods might still pose challenges in understanding and diagnosing the models' decisions. This could affect efforts to make AI more transparent and accountable, especially in critical applications like healthcare and law. Increasing the accessibility of powerful AI models through PEFT also raises concerns about misuse. More entities can harness these capabilities, potentially including those with malicious intents, such as creating sophisticated disinformation campaigns or automating cyber attacks."}, {"title": "C Experimental details", "content": ""}, {"title": "C.1 Natural language understanding (NLU)", "content": "Table C.1: The data statistics and evaluation metrics of the GLUE benchmark. The valid and test sets are randomly split from the original development set. Following Wu et al. [57], only the matched development set of MNLI is used. For runs with different seeds, the samples in the valid and test sets are also different.\nTest set split. Previous works [13, 21, 29] report the best results on the development sets of the GLUE tasks, i.e. using the same set for both validation and test, which might cause overfitting. Instead, we follow the setting of Mahabadi et al. [31] and Wu et al. [57], splitting the whole development set into a validation set and a test set. The model with the best performance on the validation set is selected to perform on the test set. Specifically, for the task with a development set whose number of samples is larger than 2K, i.e. QNLI, QQP and MNLI, we randomly select 1K samples as the validation set and the rest as the test set. For the other tasks, we select half of the samples in the development set as the validation set and another half as the test set. Please refer to Table C.1 for more details.\nHyperparameter tuning. We mainly follow the hyperparameter search space of Liao et al. [21] and list them in Table C.2. Notably, we almost upscale the learning rate by 10 for RoAd, because RoAd"}, {"title": "C.2 Commonsense reasoning", "content": "Datasets. Please refer to Hu et al. [14] for more details about the data statistics and task templates.\nHyperparameters. From Table C.3, it becomes apparent that one of the advantages of RoAd is its uniform optimal hyperparameter configuration across various tasks. Furthermore, we believe that extensive tuning of hyperparameters for LLMs is impractical. Consequently, we restrict the search space for the learning rate to {1e - 3, 3e \u2013 3}, ultimately selecting 3e - 3 for all experiments conducted on LLaMA. Consistent with Table C.2, we employ AdamW [30] as the optimizer without weight decay, a warmup ratio of 10% and a linear scheduler. Following Wu et al. [58], we fix the number of epochs at six and the batch size at 32. These hyperparameters are detailed in Table C.5. The maximum sequence length is set to 512. And the training is conducted either in BFloat16. We"}, {"title": "C.3 Arithmetic reasoning", "content": "Datasets. Please refer to Hu et al. [14] for more details about the data statistics and the construction mechanism of Math10K.\nHyperparameters. We apply almost the same training recipe as the one for commonsense reasoning, except that we set the number of epochs as 12 by following Wu et al. [58]. The detailed parameters are summarized in Table C.5. The maximum sequence length is set to 512. And the training is conducted either in BFloat16. We evaluate each checkpoint saved at every epoch and report the optimal result. The standard deviation from three random runs is presented in Table C.7. During inference, we use greedy decoding without sampling as our baselines [14, 25, 58].\nBaseline reproduction. In Table 4, we replicate the results of (IA)\u00b3 [24]. Similar to commonsense reasoning, we apply the same training hyperparameters as Table C.5 for (IA)\u00b3."}, {"title": "D More results", "content": ""}, {"title": "D.1 Compare to OFT.", "content": "Table D.1 presents the finetuning specifics for RoAds, OFT [41], and BOFT [28]. In OFT, a critical hyperparameter is defined as n = 41, meaning the number of blocks in R. Thus, configurations such as OFTn=2048 and OFTn=256 correspond approximately to OFTw=2 and OFTw=16, respectively. Increasing n, or equivalently reducing w, leads to a higher count of blocks. While a smaller w may reduce the number of trainable parameters, it necessitates more frequent computations of matrix inversion, consequently elevating both GPU memory usage and training time. Moreover, while BOFT utilizes fewer trainable parameters than OFT and achieves comparable or superior outcomes, it demands significantly more GPU memory. This increase is attributable to the butterfly factorization, which requires extensive caching of intermediate activations.\nRoAd can be viewed as a specific implementation of OFTw=2, but it consumes considerably less GPU memory and shortens training time. This efficiency stems from the use of inherently orthogonal 2D rotation matrices in RoAd, which obviate the need for matrix inversion calculations."}, {"title": "D.2 Commonsense reasoning on LLaMA2 and LLaMA3", "content": "We also conduct experiments on LLaMA2-7B [50] and LLaMA3-8B in Table D.2. RoAds still outperform all baselines with the least number of trainable parameters."}, {"title": "D.3 More examples for composability", "content": "In Figure D.1, D.2 and D.3, we show more examples of composability. Overall, RoAd demonstrates a very good ability in composition, taking advantage of both subspaces."}]}