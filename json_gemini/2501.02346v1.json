{"title": "Exploring the Capabilities and Limitations of Large Language Models for Radiation Oncology Decision Support", "authors": ["Florian Putz", "Marlen Haderlein", "Sebastian Lettmaier", "Sabine Semrau", "Rainer Fietkau", "Yixing Huang"], "abstract": "The advancement of artificial intelligence (AI) has brought forth extraordinary changes across diverse domains, extending its reach to the intricate field of medicine as well. The impressive achievements of Al have set a new bar for what technology can contribute to medicine and, more specifically, to the specialized field of radiation oncology. Large Language Models (LLMs) like GPT-4 represent the apex of Al's progression so far. GPT-3.5, the model's previous iteration, demonstrated notable results by surpassing 50% accuracy across all United States Medical Licensing Exam (USMLE) categories\u00b9. This provides compelling proof of LLMs' potential to bring transformative change to medical practice.", "sections": [{"title": "General capabilities", "content": "In the dynamic landscape of radiation oncology, LLMs like GPT-4 are emerging as multifaceted tools. They have the potential to enhance patient self-education, streamline administrative tasks, automate conversations, facilitate research and provide decision support. LLMs also serve as comprehensive reference tools, evident in their potential for Continuing Certification platforms like the ABR's Online Longitudinal Assessment (OLA). For medical training, LLMs have the capability of simulating intricate patient-doctor interactions. Finally, LLMs can efficiently process large amounts of text via in-context-learning (ICL) reaching up to 300 full-text pages per prompt for the upcoming 128k token GPT-4 model. As a large fraction of our collective medical knowledge is encoded in text, LLMs could help us to successfully tap this growing reservoir of medical evidence that otherwise is risking to exceed our conventional information processing capabilities. In this light, the advent of LLMs could signify a transformative shift towards more efficient and informed care. To date, LLMs have already been practically utilized to assist with tasks like creating discharge summaries, patient scheduling and education, medication management, foreign-language translation, and remote monitoring of patient status."}, {"title": "Objective assessment on complex clinical cases", "content": "Radiation oncology, much like other medical specialties, faces complex clinical scenarios that are often not addressed by existing evidence or guidelines, leaving room for differences of opinion and constructive debates. The Gray Zone cases featured in the Red Journal's Gray Zone case series exemplify such authentic radiooncologic dilemmas. In these intricate scenarios, clinicians could potentially even benefit more from Al decision-support systems than in cases with a well-established gold standard. Benchmarking GPT-4's performance on the Red Journal's Gray Zone cases has been touched\u00b3 (more details in the supplementary material), where the 15 cases of the 2022 Gray Zone collection were preliminarily evaluated from the aspects of correctness, comprehensiveness, and the presence of novel aspects and hallucinations. Nevertheless, the absence of gold standards makes the objective and precise evaluation of LLMs' performance on such authentic, complex radiooncologic cases a formidable challenge, warranting further investigation."}, {"title": "Current limitations", "content": null}, {"title": "Knowledge Gaps", "content": "While LLMs have been trained on a broad spectrum of topics and thus encompass a vast expanse of diverse subjects, they are not without their knowledge gaps. In the realm of radiation oncology, GPT-4 has demonstrated inferior performance in specific areas such as radiation measurements\u00b2, gynecologic oncology\u00b3, and brachytherapy\u00b3. Moreover, GPT-4 also lacks in-depth details of important clinical trials like e.g., the PORTEC-3 trial\u00b3. Further exploring both the confident and weak zones within LLMs' knowledge base is crucial. This insight not only guides developers in refining these models but also serves as a beacon for users, underscoring the need for rigorous cross-validation."}, {"title": "Hallucinations", "content": "LLMs can generate incorrect content in a convincing appearance, which is gaining increasing awareness known as \u201challucinations\u201d. This can be particular dangerous for clinical applications. For a Gray Zone case (Figure S1 in the supplementary material) in our preliminary study, GPT-4 answered in the initial case summary that the patient had \u201clocally recurrent breast cancer with contralateral axillary lymph node involvement\", whereas in fact she only had contralateral lymph node metastasis without local recurrence. Moreover, in another case (Figure S2 in the supplementary material), GPT-4 stated that the CATNON and RTOG 9802 trials \"demonstrated that adding temozolomide to radiation therapy in patients with grade 2 gliomas and specific molecular markers, such as IDH1 mutation and 1p/19q codeletion, led to improved overall survival\u201d. However, RTOG 9802 employed PCV instead of temozolomide chemotherapy and the CATNON trial only included anaplastic glioma. This highlights a challenge that needs to be addressed as Al continues to integrate into healthcare. Therefore, despite the power and sophistication of these models, it remains essential to cross-check all facts provided by LLMs like GPT-4. Both LLM knowledge gaps and hallucinations could be improved by access to external information and traceability, like e.g., implemented the ReACT framework described below.\""}, {"title": "Visual understanding", "content": "A critical component of radiooncologic decision-making hinges on the interpretation of medical images, spanning diagnostic and planning CT/MRI scans to dosimetric planning charts. The latest LLMs, such as Bard and GPT-4V(ision), now possess the capability to interpret images as well as generate figures based on text descriptions (GPT-4 with DALLE). While LLMs excel at tasks rooted in natural language processing, they have large limitations when it comes to assimilating visual information for decision support. For example, Bard refuses to process medical images to avoid data privacy and misdiagnosis issues. GPT-4V has the capability of interpreting medical images, but the interpretation can be unreliable, as pointed out by Figure 7 in the GPT-4V system card. Based on our observations, GPT-4V demonstrates its potential in comprehending complex figures in the field of radiation oncology, such as correctly identifying the planning target volume (PTV) curve from a dose volume histogram (DVH) plot (Figure S3 in the supplementary material), describing the linear accelerator (LINAC) system (FigureS4) and the relative dose distribution of different radiation particles in tissue depth (Figure S5). However, it also has the risk of failure for very simple figures, for example, failing to tell the correct number of cells and the correct order of colors in a 4 by 4 grid (Figure S6 (GPT-4V's first attempt) and Figure S7 (GPT-4V's second attempt)). While the second attempt correctly identified the color palette, the spatial arrangement was incorrect, indicating that GPT-4V can distinguish colors but has limitations in mapping these colors accurately to specific spatial locations. This limitation was further underscored in its performance on the Ishihara color test, where it correctly identified only 2 out of 6 numbers (Figure S8), highlighting a potential challenge in interpreting color and spatial relationships, which are both essential for radiation oncology applications."}, {"title": "Defense of poisoning and attacks", "content": "LLMs are typically fine-tuned using various alignment mechanisms to prevent the generation of inappropriate or obscene content7,8. Nevertheless, these models, including renowned ones like GPT-4, Bard, and Claude, remain vulnerable to attacks and poisoning. Intriguingly, even minor attack suffixes appended to queries can provoke these models to produce harmful outputs (e.g., teaching users how to make bombs\u2079). In the realm of radioncologic decision support, there's a palpable risk that a manipulated Al system could disseminate misleading or detrimental information to clinicians and patients. As such, there's an imperative need for the development and deployment of enhanced defense and protection strategies to ensure the safe application of LLMs."}, {"title": "Data Privacy", "content": "Another pressing concern is data privacy. Most LLMs like GPT-4 are proprietary Al models, and their usage in a clinical setting involves data sharing, which poses significant data security and privacy issues. For instance, OpenAl has faced criticism for allegedly using copyrighted and private data to train GPT-4 without obtaining the necessary consent agreements\u00ba. Fortunately, open-source LLMs like LLaMA2 with comparable performance to GPT-4 are emerging and can be used locally within hospitals, thereby complying with data privacy regulations. The HuaTuo10 model exemplifies this approach by fine-tuning LLaMA albeit with specialized Chinese medical knowledge. Similarly, a dedicated model for radiation oncology could be developed leveraging this methodology."}, {"title": "Overcoming current limitations", "content": null}, {"title": "Fine-tuning and in-context-learning on specialized medical information", "content": "Although many LLM developers like OpenAl have not publicly disclosed their specific training dataset, it's plausible that their LLMs were trained using a wide array of internet-accessible texts, with specialized medical and radiooncologic literature representing only a minor portion. It's highly likely that full-text journal articles and guidelines behind paywalls were not included in the training data. In addition, the wealth of clinical data available in hospitals, e.g., tumor board recommendations, mirrors the potential of specialized literature in enhancing the performance of LLMs. Fine-tuning LLMs on such specialized data or providing LLMs with such data as the input prompt via in-context learning has the high potential to improve their performance on clinical decision making in radiation oncology. An example of this approach is RadOnc-GPT11, an LLM specifically fine-tuned on an extensive dataset comprising radiooncologic patient records. Further research in this direction is promising to augment the readiness of LLMs for clinical applications."}, {"title": "Supercharging current LLM models' performance via task-driven agents, access to external information and chain-of-thought reasoning", "content": "As human language has evolved to capture complex thought processes, LLM text prediction can be used to emulate complex reasoning. LLMs are able to decompose complex and multi-step reasoning tasks into a series of intermediate reasoning steps and considerable improve their performance via a chain of thought prompting12. Even further improvement is possible by combining chain-of-thought reasoning and LLM self-reflection with autonomous access to external information as has been shown for a ReAct (Reason + Act) method. Intriguingly, ReAct also allows human experts to read the LLM's \"chain-of-thought\u201d and check its information sources to follow the model's reasoning. In radiation oncology, a personalized treatment decision relies on a comprehensive consideration of a patient's conditions including cancer type, disease extent, treatment history, molecular pathology and personal priority in concordance with the latest guidelines. Supercharging the capabilities of current LLMs via novel techniques like ReAct could make Al decision support viable. Figure 1 and the supplementary video show an example from a prototypical implementation of the ReAct method for decision support in radiation oncology. In this example GPT-4 provides a sophisticated recommendation for a complex clinical case after a self-determined number of iterations of autonomous guideline tool use, reflection and planning."}, {"title": "Multimodality integration", "content": "As medical images are pivotal in the realm of clinical practice, the integration of visual and textual information is an emerging research frontier, and many leading LLM providers like OpenAl and Google are actively pursuing advancements in this area. To achieve the goal of multimodality integration, an additional visual model (typically a vision transformer (ViT) or large visual foundation models like the Segment Anything Model) is employed alongside the language model. Due to the deep and wide knowledge embedded in latest LLMs, vision-language models with frozen-weight language models have been widely used, achieving impressive few-shot learning performance on a variety of new tasks13,14. The visual information can be integrated to frozen LLMs either at the chat level, as seen in ChatCAD15 and Visual ChatGPT16, or at a deeper latent space level, as exemplified in MiniGPT-417, Frozen18, Flamingo13, and PaLM-E14. With the continuous evolution of technology, it is anticipated that more advanced multimodal integration techniques will emerge soon, enhancing the effective utilization of LLMs in clinical radiation oncology."}, {"title": "", "content": "Due to the inherent limitations of LLM-based decision-support tools, users should exercise caution to avoid overuse of such tools. Well designed, rigorous clinical evaluations are important to identify, in which settings LLMs provide real benefit. Furthermore, a high level of medical expertise is critical for proper use of LLMs for decision-support to assess the value of individual LLM recommendations and decide on optimal patient management."}, {"title": "", "content": "In conclusion, LLMs like GPT-4 exhibit impressive potential to support decision-making in radiation oncology, but they still have many limitations such as knowledge weak zones, hallucinations, vulnerability to poisoning and attacks, and data privacy issues. Such limitations need to be addressed before broad implementation of LLMs for radiooncologic decision support."}]}