{"title": "Warmstarting for Scaling Language Models", "authors": ["Neeratyoy Mallik", "Maciej Janowski", "Johannes Hog", "Herilalaina Rakotoarison", "Aaron Klein", "Josif Grabocka", "Frank Hutter"], "abstract": "Scaling model sizes to scale performance has worked remarkably well for the current large language models paradigm. The research and empirical findings of various scaling studies led to novel scaling results and laws that guides subsequent research. High training costs for contemporary scales of data and models result in a lack of thorough understanding of how to tune and arrive at such training setups. One direction to ameliorate the cost of pretraining large models is to warmstart the large-scale training from smaller models that are cheaper to tune. In this work, we attempt to understand if the behavior of optimal hyperparameters can be retained under warmstarting for scaling. We explore simple operations that allow the application of theoretically motivated methods of zero-shot transfer of optimal hyperparameters using \u00b5Transfer. We investigate the aspects that contribute to the speedup in convergence and the preservation of stable training dynamics under warmstarting with \u00b5Transfer. We find that shrinking smaller model weights, zero-padding, and perturbing the resulting larger model with scaled initialization from \u00b5P enables effective warmstarting of \u00b5Transfer.", "sections": [{"title": "Introduction", "content": "Scaling of model size, dataset size and training compute together, lead to performance scaling in the large language model (LLMs) paradigm, as shown by the recent scaling law literature [Kaplan et al., 2020, Hoffmann et al., 2022, Caballero et al., 2023, H\u00e4gele et al., 2024, Porian et al., 2024]. Under different training setups, empirical data often fit predictable trends captured by an exponent of an assumed parametric relationship. Recent research trends point to an attempt at understanding and finetuning the nature of scaling law studies across various problem formulations [Sorscher et al., 2022, Alabdulmohsin et al., 2023, Wang et al., 2023a].\nOne commonality across such studies is the often non-overlapping choice of hyperparameters and sparse documentation of their selection process. This stems from the prohibitive costs of principled hyperparameter tuning at large scales. Reusing weights from tuned smaller models can offer a way to accelerate training of larger models. This is commonly known as warmstarting a model's training and has been approached previously through knowledge distillation [Chen et al., 2016, 2021], morphisms [Wei et al., 2016a, Elsken et al., 2019], learned transformations [Wang et al., 2023b,a]."}, {"title": "Method: Warmed-\u00b5P", "content": "In this section, we formalize and formulate warmstarting in the context of scaled parameterizations. We refer the readers to Appendix A for more on background concepts and related work.\nGiven a trained base model Mbase with tuned hyperparameters (e.g. learning rate) and a target model Mtarget for transferring the hyperparameters, we define the warmstarting operation as the layer-wise initialization of Mtarget using Mbase. In this work, we initialize each layer of Mtarget using a combination of a shrunk version of Mbase weights with the standard \u03bcP initialization. For a layer I in Mtarget, let $O_{\\text{target}} \\in \\mathbb{R}^{p\\times q}$ be its weight and $O_{\\text{base}} \\in \\mathbb{R}^{m\\times n}$ be the corresponding weight from Mbase (where $m \\leq p$ and $n \\leq q$). Formally, the warmstarting operation is given by:\n$O_{\\text{target}} = A_{\\text{shrink}} P_{\\text{ado}} (O_{\\text{base}}, p, q) + \\mathcal{N}(0, \\sigma_{\\mu P}^{2}),$ (1)\nwhere $\\sigma_{\\mu P}$ is the recommended per-layer standard deviation by Yang et al. [2021] for initialization, and Pado is a function that expands the base model weight to the target shape (p \u00d7 q) with zero padding. Intuitively, the initialization term of Equation 1 can be replaced with any scaling parameterization technique, while the first term can represent any transformation of the base weight matrices onto the target shape. Additionally, setting Ashrink = 0 in Equation 1 recovers vanilla-\u03bc\u03a1.\nWe choose zero-padding as the simplest method to study for scaling the model size with \u00b5P and avoid more sophisticated heuristics [Rae et al., 2021], learned transformations [Wang et al., 2023b] or distillations [Chen et al., 2016]. In order to retain \u00b5P's guarantees of training stability, $A_{\\text{shrink}} \\in [0, 1]$ is multiplied to all the transformed weights. The \u00b5P initialization acts as a perturbation matrix for the base model weights, transformed and casted to the target shape. We choose a fixed value of Ashrink = 0.4 in our experiments (Section 3) as per shrink-and-perturb-based warmstarting literature [Ash et al., 2020, Zaidi et al., 2022, Chebykin et al., 2023] from the continual learning setting without model size growth. We find empirically that this recommendation interestingly holds and is crucial for warmstarted-\u00b5P (see, Section 3, Appendix B.2).\nFrom a different perspective, one can also see the warmstarting operation as equivalent to abc- parameterization, which defines the family of scaled parameterization methods [Yang and Hu, 2021, Blake et al., 2024, Everett et al., 2024] that derive fixed scaling rules for hyperparameter transfer"}, {"title": "Empirical evaluation", "content": "In this section, we report our empirical findings, with the intention of emphasizing the underline intuition behind the proposed approach (Section 2), in both convergence speed and training stability.\nExperimental Setup : We train a decoder-only GPT2 model [Radford et al., 2019] as per the GPT-NeoX implementation [Andonian et al., 2023] provided by LitGPT [AI, 2023] on the SlimPa- jama [Soboleva et al., 2023]'s 6B version dataset. We set the weight decay to 0 for Adam [Kingma and Ba, 2015] to avoid interaction effects and confounding factors [Lingle, 2024, Wang and Aitchison, 2024]. Fixed learning rate (LR) schedules were used following H\u00e4gele et al. [2024], simplifying checkpoint studies with no LR management overheads. We disable learning rate warmup to aid com- parative analysis of if warmstarting provides gains. All models are trained for 20 tokens/parameter, similar to Hoffmann et al. [2022], keeping sub-epoch training, that is, no micro batch of the dataset is ever repeated. Our FLOPs calculation is also based on Hoffmann et al. [2022]. Validation loss is measured on a fixed held-out split of the dataset, consistent across all runs, with 3 different seeds per run, and Gaussian smoothing is used for loss comparison across runs. For warmstarting runs, we do not repeat tokens seen by the base model. A warmstarting training starts with a fresh optimizer state, and follows standard \u00b5P training procedures. All trainings used a single NVIDIA RTX 2080 GPU.\nRQ 1: Does warmstarting improve a vanilla-\u00b5P run? The \u00b5P recipe recommends the following broadly for \u00b5Transfer: i) Perform a grid search over hyperparameters at a small model scale; ii) Transfer the optimal hyperparameters at this scale, to a larger model scale using \u00b5P rules. The exact choice of base and target scales, choice of hyperparameters for grid search, if the base scale itself should be a hyperparameter, etc. are all actively pursued investigations in the community [Everett et al., 2024, Blake et al., 2024, Qiu et al., 2024, Lingle, 2024]. Given our experimental setup of fixed model scales across width scaling (refer, Table 1), we choose 3 possible base scales: 5M, 10M and 22M. Figure 1 shows \u00b5Transfer from the smallest base scale with and without warm-starting. Figure 4 shows the same result as Figure 1 but under longer compute or more tokens than recommended by Hoffmann et al. [2022]. Figure 5 reports warmstarting from other base scales, scaled up to a 120M model and Figure 6 shows learning curves that include the base model training."}, {"title": "RQ 2: How does the scale difference affect the quality of warmstarting?", "content": "Given that the warmstarting operation (Section 2) chosen for our experiments pads the new connections to the base model with zeros to scale to the target model, it can be expected that the quality of warmstarting will be more pronounced when a large enough model is grown not too large. In Figure 2 we chart the initial and final validation loss comparison with \u00b5Transfer and warmstarting under width-scaling. We observe that warmstarting offers a much-improved initialization that accelerates loss convergence. It is also evident that the loss obtained by the base model is not transferred. This gap manifests as spikes if learning curves are concatenated over the base and warmstarted-target runs (see Figure 6). Figure 2 also highlights that the amount of data already seen by the base model, and the loss it can achieve given its scale, skews such an analysis as the resulting warmstarted model is likely to have a larger spike in loss. Given that we do not repeat the tokens seen by the base model when training the warmstarted target model, a larger spike would require more updates and thereby tokens to recover. We believe that though the warmstarting method shown here is adequate in providing speed ups, it is suboptimal in not providing consistent improvement in final loss, despite seeing more tokens in principle. Despite the obvious gains warmstarting can bring with \u00b5P, there are clearly much more design choices to study and consider for consistent, efficient warmstarting."}, {"title": "RQ 3: Does warmstarting with \u00b5P retain training stability guarantees?", "content": "We empirically investi- gate the impact of the proposed warmstarting technique on \u00b5P training behavior [Yang et al., 2021]. We conduct coordinate checks as per the \u00b5P library to verify the consistency of the L\u2081-norm of layer activations across width-scaling. Within the same experimental setting, we also ablate the effect of the shrinking value, the main hyperparameter of our approach.\nFigure 3 shows L1-norm of activations for warmstarted \u00b5P with Ashrink < 0.6 trend similar to vanilla \u00b5P across the considered width-scaling. However, we observe instability for Ashrink > 0.6 (Figure 7, 10). This result also backs the Ashrink = 0.4 value reported for shrink-and-perturb in the literature [Ash et al., 2020, Zaidi et al., 2022, Chebykin et al., 2023]. Additionally, we monitor the L1 norm of layer activations during the entire training in Figure 8. Warmstarted \u00b5P maintains activation values within a comparable range and trends consistent with vanilla-\u00b5P, suggesting that our demonstrated warmstarting method does not introduce instability to \u03bc\u03a1."}, {"title": "Conclusion", "content": "In this work, we explore the feasibility of enabling continued pretraining of language models as more data is provided and model size is scaled up. We demonstrate that a simple technique, even when warmstarting from a much smaller model, is adequate in improving convergence speed and potentially final performance (Figures 1, 5). Our focus was to identify a warmstarting method that is simple enough to be implemented alongside \u00b5P (Section 2) while not being in conflict with \u00b5Transfer's expectations of stable training in practice (Figures 3, 7, 8, 10).\nLimitations. We note that the empirical study shown here has an outcome of interest in that simple shrinking and perturbation with \u00b5P works already. However, our setting (Section 3) is carefully chosen to minimize effects induced by various choices of training setups, in order to understand the effects of warmstarting with \u00b5P. For more general practicality, more ablations and experiments must be performed over different learning rate schedules, weight decay, activations, and for much larger model sizes and context windows. More recent works on scaled parameterizations [Everett et al., 2024, Blake et al., 2024] which study and improve over \u00b5P recommendations should also be tested with our modular approach to further assert the potential of warmstarting under abc-parameterization.\nFuture directions. More diverse experiments aside, exploring what makes warmstarting work, as we demonstrate here, is important for a lean, general, efficient method that truly speeds up pretraining runs. Figures 7 and 10 show the effect of different shrinking factors in maintaining training stability of optimal hyperparameters, while Figure 2 shows that choice of scales directly affect gains provided by warmstarting. Exploiting the definition of Equation 1, we would like to explore the role of layer-wise shrinking of base model weights and investigate theoretical relations with abc-parameterization. Exploiting structures, ranks, and representations learned by the base model can also lead to better warmstarting and improvement over simple zero padding [Rae et al., 2021, Wang et al., 2023a, Qiu et al., 2024, Wei et al., 2024]. Moreover, warmstarting using our setup here leads to the consumption of more tokens, when comparing against the vanilla-\u00b5P training of the target model. Exploring if models can be warmstarted and scaled up progressively will be a clear direction to pursue (see, Appendix B.3). Such a paradigm of training can massively lower hyperparameter tuning costs overall and also potentially change the practice of scaling models up, with tuned hyperparameters."}, {"title": "Background and Related Work", "content": "Scaled parameterizations refers to a set of rules that determine how certain parameters can be scaled with respect to one or more scaling dimensions [Everett et al., 2024]. Such prescriptions attempt to ensure stable feature learning under stable optimal hyperparameters to ensure maximal feature learning in the infinte-width limit [Yang and Hu, 2021]. abc-parameterizations [Yang and Hu, 2021] is a formulation that subsumes such parameterizations and broadly follows the assumption where model weights are so defined,\n$w_0 \\sim \\mathcal{N}(0, B^2),$ (2)\n$w_t = A_w w_t,$ (3)\n$w_{t+1} = w_t + C_w \\cdot \\Phi_t(\\nabla L_0, ..., \\nabla L_t),$ (4)\nwhere t defines the training step and $\\Phi_t(\\nabla L_0, ..., \\nabla L_t)$ is the gradient-based weight update step [Blake et al., 2024]. $A_w, B, C_w$ are scalars that change with model width and determine the required scaling of parameterized entities. The specific prescription of these scalars is governed by the method and its assumptions which lead to the analytical rules of scaling. We refer the reader to Tables 1, 2, 3 from Yang et al. [2021] for a succinct summary of the scaling rules for the scalars $A_w, B, C_w$, which realizes \u00b5Parameterization or \u00b5P. This class of methods is seeing growing interest in the community as model sizes increase and prior knowledge of good hyperparameters are not available on such scales [Yang et al., 2024, Everett et al., 2024, Blake et al., 2024, Th\u00e9rien et al., 2024, Qiu et al., 2024, Lingle, 2024]. Recent studies find improvements in the \u00b5P recommendation and scenarios or setups where \u00b5P can be bettered. Our work is along these lines where we aim to show that warmstarting a larger model training from a smaller model is one more such scenario where vanilla-\u00b5P can be improved upon.\nShrink-and-perturb is a technique that showed that models of the same size can be warmstarted from a previous training run successfully such that it converges faster without hurting generaliza- tion [Ash et al., 2020]. Under shrink-and-perturb, every learnable parameter $w_i$ is initialized as, $w_i \\leftarrow \\lambda w_{i-1} + p_t$, where $p_t \\sim \\mathcal{N}(0, \\sigma^2)$ and $0 < \\lambda < 1$. Ash et al. [2020] posits that the shrinking of the weights act as a regularization but not similar to weight decay. In effect, shrinking weights can act as increasing the entropy of the output distribution and preserving the relative activation at each layer. While perturbation seems to have an effect on balancing gradient contribution from each learned parameter. Crucially, this version of shrink-and-perturb was specifically shown for continual learning under stationary data distributions [Chebykin et al., 2023, Shin et al., 2024]. However, to our knowledge, no such shrink-and-perturb warmstarting method directly talks about warmstarting across model scales.\nModel growth literature, or warmstarting literature across model sizes is rich and encompasses multiple methodological paradigms, including initialization techniques [Zaidi et al., 2021], network morphisms [Wei et al., 2016b], knowledge transfer and distillation approaches [Chen et al., 2016, 2021, Wang et al., 2023a], learned transformations [Deng et al., 2023, Wang et al., 2023b], and empirical heuristics [Rae et al., 2021]. More recent works show that increasing transformer model size can improve pretraining efficiency and potentially the compute-loss scaling coefficient [Yao et al., 2024, Du et al., 2024]. Each such method represents a well-designed and sophisticated approach that, in principle, achieves the required warmstarting effect. These approaches conflict with our requirement for a simple and practical implementation, require changes to training routines, loss functions, and usually keep the hyperparameters fixed across scales. Samragh et al. [2024], a concurrent work, echoes our requirements for a warmstarting method and proposes a shrink-and- perturb-like method for growing a language model, and is the closest work to us, altering only the initialization method given a smaller base model checkpoint. Our method and approach differ from all approaches mentioned above since we study and demonstrate that it is possible to warmstart while suitably scaling hyperparameters.\nOur work aims to develop a theoretically motivated modular framework that can, given any model checkpoint with its tuned hyperparameters, systematically scale this model along arbitrary dimensions, initialize through principled warmstarting from smaller model weights, and train using theoretically derived hyperparameter scaling rules."}, {"title": "Experiments", "content": "This section covers additional details on the experimental setup and more supporting results.\nModel scales. Table 1 covers the model scales reported in the experiments in this work.\nGrid search results for optimal hyperparameters. For the chosen 3 base scales, we perform a grid search over the learning rate (LR) and batch size. The grid search results are summarized below:\n\u2022 5M: learning rate = 0.03; batch size = 64\n\u2022 10M: learning rate = 0.01; batch size = 64\n\u2022 22M: learning rate = 0.003; batch size = 256\nWhen performing \u00b5Transfer in our work, the batch size found for the base scale is kept constant while LR is scaled as per \u00b5P.\nB.1 Additional results\nFigure 4 shows Figure 1 when run for longer, i.e., trained with more tokens (here, 30 tokens/parameter). We observe that the feature learning under \u00b5Transfer is retained under our proposed warmstarting method. A similar converging loss compared to vanilla-\u00b5P may be the limit imposed on the loss by the model scale. Figure 5 adds more base scale transfer to our primary result, complementing Figure 1. The general trend of our warmstarting approach improving \u00b5P continues across all pairs of model scales we tried. However, as expected, the extent of gains provided by warmstarting varies depending on the model scales. For all of these results, we append the base model learning curves to the target model learning curves of our warmstarting approach in Figures 6. Although this shifts the learning curves, the impact is minimal, particularly when there is a significant difference in size between the base and target models."}, {"title": "Analyzing training dynamics", "content": "When applying warmstarting with \u00b5P, it is important to also guarantee the optimality of the base scale hyperparameters when scaled and transferred to the larger target model. This beahviour manifests in metrics that evolve over training steps, such as the L1 norm of activations, L1/L2 norms of the weights, etc. Figures 7, 8, 9, 10 together highlight that the loss perspective alone is inadequate in knowing what will work best across scales. However, it appears that there is a sweet-spot for Ashrink such that it is not too low (= 0 is equivalent to \u00b5P) or too high (= 1 is equivalent to not shrinking the base model). The value of 0.4 suggested in the literature appears to work the best in our setup."}, {"title": "Successive warmstarting", "content": "The entire goal of warmstarting a model from a smaller model is to speed up convergence of the larger model training. For pretraining LLMs, typically done in a sub-epoch manner by not repeating a data, we have a unique setup where if we want to train a model at a particular target scale, say 100M, for compute-optimality [Hoffmann et al., 2022], a warmstarted training run at 100M will consume more tokens than a standard run from scratch at 100M. This is illustrated in Figure 11 (left) where we see that for the same amount of compute (in FLOPs) allocated to a vanilla-\u00b5P and warmstarted-\u00b5P, the latter consumes more tokens."}]}