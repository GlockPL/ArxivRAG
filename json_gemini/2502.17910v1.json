{"title": "Scaling LLM Pre-training with Vocabulary Curriculum", "authors": ["Fangyuan Yu"], "abstract": "Modern language models rely on static vocabularies, fixed before pretraining, in\ncontrast to the adaptive vocabulary acquisition observed in human language learn-\ning. To bridge this gap, we introduce vocabulary curriculum learning, an approach\nthat improves pretraining efficiency with log-linear scaling gains relative to vocabu-\nlary size. Our method alternates between entropy-guided vocabulary expansion and\nmodel optimization, enabling models to learn transferable representations across\ndiverse tokenization granularities. This approach naturally gives rise to an optimal\ncomputation allocation pattern: longer tokens capture predictable content, while\nshorter tokens focus on more complex, harder-to-predict contexts. Experiments\non small-scale GPT models demonstrate improved scaling efficiency, reinforcing\nthe effectiveness of dynamic tokenization. We release our code to support further\nresearch and plan to extend our experiments to larger models and diverse domains.", "sections": [{"title": "Introduction", "content": "Modern language model pre-training relies on static vocabularies, fixed before training and detached\nfrom the model's learning dynamics\u2014unlike human language acquisition. This fixed approach limits\nmodels' ability to adapt to different levels of linguistic granularity, potentially hindering efficiency and\nperformance. While humans acquire language hierarchically, starting with basic units before building\nmore complex representations, language models typically operate with predetermined tokenization\nschemes.\nOur approach dynamically merges predictable tokens, enabling the model to allocate computational\nresources more efficiently and shift focus toward harder-to-predict patterns. This results in an\nadaptive curriculum that evolves alongside the model's capabilities. The vocabulary curriculum\nlearning strategy begins with basic units (characters) and progressively expands to more complex\nrepresentations, allocating more capacity to regions of high modeling entropy and refining the model's\nunderstanding of difficult linguistic structures.\nEmpirical results from pre-training GPT models [10] on the enwiki8 dataset [11] highlight two key\nadvantages of our vocabulary curriculum learning approach:\n1.  It improves model performance across various vocabulary sizes, consistently achieving\nlower bits-per-character (BPC) compared to traditional fixed-vocabulary training.\n2.  It enhances scaling efficiency\u2014models trained with a vocabulary curriculum exhibit a\nshallower slope (0.109 vs. 0.147) in log-scale vocabulary size vs. BPC plots, indicating\nmore effective utilization of larger vocabularies.\nAs shown in Figure 1, models trained with incremental vocabulary curriculum learning (red) exhibit a\nsteeper improvement curve compared to compute-matching baselines (blue). The log-scale vocabulary\nsize vs. bits-per-character (BPC) plot reveals that vocabulary curriculum learning achieves a slope of\n-0.147, meaning it leverages larger vocabularies more effectively than compute-matching learning,\nwhich only reaches -0.109.\nAdditionally, we observe that the curated vocabulary naturally forms a hierarchical structure, where\nlonger tokens become increasingly predictable (lower BPC), while shorter tokens remain harder to\npredict (higher BPC). This structural organization emerges organically from our training process,\nreinforcing the effectiveness of our dynamic tokenization strategy.\nOur key contributions are:\n\u2022 A dynamic vocabulary creation system that adapts based on model entropy\n\u2022 A curriculum learning approach for tokenization that improves scaling efficiency\n\u2022 Evidence that hierarchical token organization emerges naturally from our approach\nWhile our focus is on language modeling, we believe this scaling effect can generalize to other\nmodalities and domains, as byte sequences serve as the fundamental building blocks of digital data."}, {"title": "Relevant Work", "content": "2.1 Tokenization Methods and Limitations\nStandard tokenization approaches like Byte Pair Encoding (BPE) [8], [9] rely on static co-occurrence\nstatistics detached from model learning. This creates representational limitations, particularly evident\nin early language models' struggles with mathematical operations [10]. Naive BPE tokenization\nproduces inconsistent representations of numbers-for example, \"711\" might be encoded as a single\ntoken while \"703\" requires multiple tokens. This inconsistency makes it harder for models to learn\narithmetic operations compared to specialized approaches that assign unique tokens to all 1-3 digit\nintegers [7].\nEven with a fixed vocabulary, different encoding strategies can produce varying segmentations of\nthe same text. BPE-dropout [2] leverages this property by introducing stochasticity during training,\nshowing improvements in neural machine translation. More recent work exploits segmentation equiv-\nariance during inference to enhance reasoning through self-consistency [3]. Additionally, research [5]\nhas established the existence of optimal vocabulary sizes for BPE-style tokenization, which correlate\nwith model size, a log-linear relationship is observed between perplexity and vocabulary size.\n2.2 Curriculum Learning\nCurriculum learning [13] progressively increases task difficulty during training to improve model\nperformance. While successful in LLM post-training [15, 21], effective curriculum strategies for pre-\ntraining remain challenging [16]. Previous attempts at vocabulary-based curricula for decoder-only\nmodels found no improvements [19], highlighting the difficulty of designing effective curricula for\nlanguage model pre-training. Our work addresses these limitations with a novel adaptive approach to\nvocabulary curriculum.\n2.3 Entropy Aware Tokenization\nRecent work has begun exploring entropy-aware tokenization. The Byte Latent Transformer [4]\nbuilds tokenization vocabularies using separately trained small language models. However, this\napproach creates vocabularies that are detached from the actual model's entropy patterns and cannot\nbe dynamically updated during training.\nOur work differs by integrating vocabulary building directly into the training process, allowing the\ntokenization scheme to evolve with the model's developing understanding of the text. This creates a\ntrue curriculum that adapts to the specific learning trajectory of each model, rather than imposing a\nstatic or pre-computed vocabulary structure."}, {"title": "Approach", "content": "Given a text corpus D consisting of numerous character sequences, where each sequence X1:m \u2208 D\nconsists of characters xi (or bytes). A vocabulary V and an encoding function e(x1:m|V) together\ndefine a tokenization scheme that converts character sequences to token sequences 81:n. Language\nmodeling then focuses on predicting the next token: p(sn|81:n\u22121) through minimizing entropy\nH(St S1:t-1).\nWe propose a dynamic tokenization framework that jointly learns the vocabulary and encoding strategy\nalongside the language model. Our approach consists of two key components: (1) entropy-guided\nvocabulary update and (2) vocabulary curriculum learning.\n3.1 Entropy-Guided Vocabulary Update\nGiven a trained language model f, we identify mergeable token sequences based on their predictability.\nFor a sequence (S1, S2, ..., Sn), we compute the entropy H(st|81:t\u22121) for each token. A sequence is\nconsidered mergeable if all tokens after the first position exhibit monotonically decreasing entropy\nbelow threshold \u20ac:"}, {"title": "Vocabulary Curriculum Learning", "content": "The curriculum learning process starts with the base vocabulary Vo = A (the character alphabet) and\nalternates between model optimization and vocabulary updates. At each stage k:\n1. Model Training: Train the language model f with current vocabulary Vk using cross-entropy loss:\nLk = \u03a3\u03a3logp(St S1:t-1; Vk)\nX1:mED t\nwhere 81:n = e(x1:m|Vk) is the encoded sequence.\n2. Vocabulary Update: Based on the trained model's entropy patterns, either expand the vocabulary\nthrough entropy-guided merging or reduce it through prefix slicing as defined in the previous section:\nVk+1 = {add(Vk, f, D)for expansion phase(reduce(Vk, Ntarget) for reduction phase\nThis iterative process continues until reaching the desired model performance or vocabulary size\nconstraints."}, {"title": "Experiments", "content": "We investigate two key questions: (1) Is learning transferable across different vocabularies? and (2)\nDoes vocabulary curriculum improve model performance?\n4.1 Experimental Setup\nWe evaluate our approach on a cleaned version of enwiki8 dataset using a small GPT architecture\n(context length 512, 6 layers, 6 attention heads, embedding dimension 384, 10M parameters). The\ninitial vocabulary Vo consists of 92 characters, and the model is trained with dropout 0.2 without\nbias terms. For vocabulary updates, we set the entropy threshold e 0.3 and limit per-iteration\nvocabulary growth to 3K tokens.\n4.2 Incremental Vocabulary Curriculum\nOur primary experiment consists of 5 iterations of vocabulary expansion, starting from a base\nmodel with minimal vocabulary (92) and progressively training models with larger vocabularies\n(4359, 7941, 11382, 14819, 18276). Each iteration uses the previous model's checkpoint for\nvocabulary addition. We cap the vocabulary at 18K based on compute-matching experiments showing\nperformance deterioration beyond this size, aligning with observations in [5] that optimal vocabulary\nsize correlates with model size."}, {"title": "Analysis of Improvement Mechanisms", "content": "To understand the source of these improvements, we analyze per-token BPC distributions across\ndifferent checkpoints. Figure 5 shows that at vocabulary size 4359, longer tokens consistently achieve\nbetter compression rates, validating our entropy-aware token addition approach.\nFurther analysis across iterations (Figure 6 and Table 2) reveals two key patterns: 1. Newly created\ntokens are progressively longer and achieve lower BPC 2. Original shorter tokens become more\nchallenging to model, showing slight BPC increases\nThis suggests that our curriculum enables the model to effectively learn hierarchical patterns, with\nlonger tokens capturing predictable sequences while shorter tokens specialize in harder-to-predict\ncontexts."}, {"title": "Implication and Future work", "content": "Optimal vocabulary size is correlated with model size [5], following this insight, we suspect the\nscaling improvement might be better for bigger model size. We'll work on extending our experiments\ntherein. The effectiveness of incremental vocabulary learning suggests its potential application\nin other modality than text, for instance, in bGPT [12] all digital files can be converted into byte\nsequences, where the scaling power of vocabulary curriculum could be leveraged to compress the\ncontext, as well as improve modeling accuracy."}]}