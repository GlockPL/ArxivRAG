{"title": "Robust Uplift Modeling with Large-Scale Contexts for Real-time Marketing", "authors": ["Zexu Sun", "Qiyu Han", "Minqin Zhu", "Hao Gong", "Dugang Liu", "Chen Ma"], "abstract": "Improving user engagement and platform revenue is crucial for online marketing platforms. Uplift modeling is proposed to solve this problem, which applies different treatments (e.g., discounts, bonus) to satisfy corresponding users. Despite progress in this field, limitations persist. Firstly, most of them focus on scenarios where only user features exist. However, in real-world scenarios, there are rich contexts available in the online platform (e.g., short videos, news), and the uplift model needs to infer an incentive for each user on the specific item, which is called real-time marketing. Thus, only considering the user features will lead to biased prediction of the responses, which may cause the cumulative error for uplift prediction. Moreover, due to the large-scale contexts, directly con-catenating the context features with the user features will cause a severe distribution shift in the treatment and control groups. Secondly, capturing the interaction relationship between the user features and context features can better predict the user response. To solve the above limitations, we propose a novel model-agnostic Robust Uplift Modeling with Large-Scale Contexts (UMLC) frame-work for Real-time Marketing. Our UMLC includes two customized modules. 1) A response-guided context grouping module for ex-tracting context features information and condensing value space through clusters. 2) A feature interaction module for obtaining bet-ter uplift prediction. Specifically, this module contains two parts: a user-context interaction component for better modeling the re-sponse; a treatment-feature interaction component for discovering the treatment assignment sensitive feature of each instance to bet-ter predict the uplift. Moreover, we conduct extensive experiments on a synthetic dataset and a real-world product dataset to verify the effectiveness and compatibility of our UMLC.", "sections": [{"title": "1 INTRODUCTION", "content": "With the development of online platforms, assigning specific incen-tives to increase user engagement and platform revenue is a crucial task in online marketing [18, 34]. Since these incentives (e.g., bonus, discount) are usually related to the cost, different users may have different responses to these incentives. Then, a successful strategy is needed to discover the incentive-sensitive user group and only deliver the incentive to the user that tends to be converted, which is important in online platforms. In recent years, uplift modeling, where various treatments are applied for different target users, has been proposed to achieve this purpose. In practice, we can observe only one type of user response, which may be for a certain incentive (i.e., treatment group) or no incentive (i.e., control group) [10]. Up-lift modeling aims to capture the differences between the treatment and control groups, which is known as the individual treatment effect (ITE) or uplift. Many uplift models have been proposed to facilitate online marketing of online platforms [39].\nThe existing works can be divided into two directions. 1) Machine-learning based methods. In detail, these works can be further di-vided into meta-learner-based and tree-based. S-Learner [17] and T-Learner [17] are the representative meta-learner methods, which design a global base learner or two base learners for the samples in treatment and control groups, respectively. Uplift Tree [30] and Causal Forest [2] are two commonly used tree-based meth-ods. Specifically, the hierarchical tree is used to separate user pop-ulations into subgroups that exhibit sensitivity to specific treat-ments [27]. 2) Representation-learning based methods. With the de-velopment of deep learning, the representation learning becomes the main research direction for uplift prediction [25]. TARNet [31] and CFRNet [31] leverage a feature representation network to ex-tract the feature information into the latent space, then to predict the responses in different user groups (i.e., treatment group and control group) and achieve representation balancing by using the Integral Probability Metrics (IPM). StableCFR [37] smooths the population distribution and upsample the underrepresented sub-populations, while balancing distributions between treatment and control groups. [21] designs a unified framework for both one-side and two-side online marketing. We focus on the representation-learning based methods because they can be more flexibly adapted to modeling the complex scenarios in many industrial systems.\nAlthough existing methods of uplift modeling have shown promis-ing results and achieved success in many practical problems, they generally only consider the existence of user features and do not model the rich context features. In some real-world scenarios, to better find the users that tend to convert in online platforms, we need to assign different treatments to users with different contexts. For example, on short video platforms, the users may receive dif-ferent coin incentives with different recommended short videos. We define this problem as real-time marketing. There are some challenges for solving the uplift problem in real-time marketing:\n1) Distribution shift: Random Control Trials (RCTs) are commonly used to collect data from the online platform for training the uplift models, which aim to ensure uniform distribution of user features between treatment and control groups. However, as shown in Figure 1, when considering the large-scale context features, directly concatenating the uncontrollable context features (e.g., short videos) will cause a significant distribution shift between the treatment and control groups. 2) Feature interaction: Modeling the feature interaction [33, 36] can improve the prediction accuracy of the model. However, it is not sufficiently explored in previous uplift modeling works [2, 3]. With rich context features, discovering the relationships between the user features and context features, or the treatment assignment and the features, is essential for better predicting the uplift.\nIn this paper, to solve the aforementioned challenges, we pro-pose a novel model-agnostic Robust Uplift Modeling with Large-Scale Contexts (UMLC) framework for Real-time Marketing. In particular, our framework consists of two customized modules. 1) A response-guided context grouping module aims to extract con-text features information through a response-guided regression network, and narrow the value space of context features by the cluster; 2) A feature interaction module contains a user-context and a treatment-feature interaction parts can better capture the rela-tionship between the user features and context features, which may obtain more reliable user response prediction; a treatment-feature interaction module aims to discover the treatment assignment sen-sitive features (i.e., the features have a potential relationship with treatment assignment) of each instance to target the users that tend to convert with the specific context features. Moreover, our contributions can be summarized as follows:\n\u2022 We propose a novel model-agnostic UMLC framework for solving the uplift problem in real-time marketing.\n\u2022 We design a response-guided context grouping module, which handles the large-scale context with a distribution shift be-tween the treatment and control groups.\n\u2022 We also design a feature interaction module, which builds the relationships between the user and context or treatment assignment and features for better uplift prediction.\n\u2022 We conduct extensive experiments on a synthetic dataset and a product dataset to verify the effectiveness and compatibility of our UMLC."}, {"title": "2 RELATED WORK", "content": "2.1 Real-time Marketing\nReal-time marketing appears as a result of consumers' expectation for real-time connection with product or service providers [6]. Real-time service has taken personalization of service beyond merely providing relevant content to targeted consumers. For example, consumers' personal preferences combined with external factors such as weather, traffic or season could inform context information needed to co-create value in real-time [5]. With the development of online marketing, more and more platforms need to design effective strategy for incentivize sensitive users, including E-commerce com-panies like Taobao, rental service like Airbnb, short video recom-mendation like Kuaishou. The above mentioned platforms involve both user and rich contexts. To further better utilize the incentives, a possible marketing strategy is to assign different incentives for each user with different contexts, which is defined as the real-time online marketing. Recent works [15, 19, 22] in real-time marketing focus on the pricing for different contexts, the product management or the optimization procedure for bidding. However, for the online platforms, the aim of real-time online marketing is to delivery dif-ferent incentives for each user with different contexts, which is not investigated in the previous works.\n2.2 Uplift Modeling\nThe uplift modeling methods mainly focus on two research direc-tions: 1) Machine-learning based methods. The machine-learning based methods can be divided into two more detailed research categories, i.e., meta-learner based and tree-based. Meta-learner based methods aim to use any off-the-shelf estimator as the base learner for the uplift prediction. S-Learner [17] and T-Learner [17] leverage one global or two base learners for treatment and con-trol groups, respectively. Tree-based methods aim to design the splitting criteria or use the ensemble methods to predict the uplift. Causal Forest [2] uses the ensemble of uplift trees, and returns the average uplift prediction of them. 2) Representation-learning based methods. Representation-learning based methods are more flexible than machine-learning based methods. CFRNet [31] use a shared bottom to extract features into latent space, and obtain the balanced representation through IPM. SITE [38] preserves local similarity and balances data distributions simultaneously, by focusing on several hard samples in each mini-batch. StableCFR [37] smooths the population distribution and upsample the underrepresented subpopulations. There are also some works focus on the specific sce-nario or the generalization ability of the uplift models. UniTE [21] proposes a uniform framework for one-side and two-side online marketing. RUAD [34] defines a robustness case of uplift modeling and uses the adversarial training to solve it. However, these works only model the uplift of the users, without considering the effect of contexts."}, {"title": "3 PRELIMINARIES", "content": "To formulate the problem, we follow the Neyman-Rubin potential outcome framework [28], to define the uplift modeling problem. Let the observed sample set be $\\mathcal{D} = \\{x_i^u, x_i^c, t_i, y_i\\}_{i=1}^n$ with $n$ samples. Without loss of generality, for each sample, assuming $y_i \\in \\mathcal{Y} \\subset \\mathbb{R}$ is a continuous response variable, $x_i^u \\in \\mathcal{X}^u \\subset \\mathbb{R}^p$ is a vector of user features with $p$ elements, $x_i^c \\in \\mathcal{X}^c \\subset \\mathbb{R}^q$ is a vector of context features with $q$ elements, and $t_i \\in \\{0,1\\}$ denotes the treatment indicator variable, i.e., whether to get an incentive delivery. Note that the proposed framework can be easily extended to other types of uplift modeling problems. For user $i$, the change in user response caused by an incentive $t_i$, i.e., individual treatment effect or uplift, denoted as $\\tau_i$, is defined as the difference between the treatment response and the control response:\n$\\tau_i = y_i(1) - y_i(0),$   (1)\nwhere $y_i(0)$ and $y_i(1)$ are the user responses of the control and treatment groups, respectively.\nIn the ideal world, i.e., obtaining the responses of a user in both groups simultaneously, we can easily determine the uplift $\\tau_i$ based on Eq. (1). However, in the real world, only one of the two responses is observed for any one user. For example, if we have observed the response of a customer who receives the discount, it is impossible to observe the response of the same customer when they do not receive a discount, where such responses are often referred to as counterfactual responses. Therefore, the observed response can be described as:\n$y_i = t_iy_i(1) + (1 \u2013 t_i)y_i(0)$.   (2)\nFor the brevity of notation, we will omit the subscript $i$ in the following if no ambiguity arises.\nAs mentioned above, the uplift $\\tau$ is not identifiable since the observed response $y$ is only one of the two necessary terms (i.e., $y(1)$ and $y(0)$). Fortunately, with some appropriate assumptions [18], we can use the conditional average treatment effect (CATE) as an estimator for the uplift, where CATE is defined as:\n$\\tau(x) = \\mathbb{E} (Y(1) | X = x) \u2013 \\mathbb{E} (Y(0) | X = x)$  \n$= \\mathbb{E}(Y | T = 1, X = x) \u2013 \\mathbb{E}(Y | T = 0, X = x) .$  \n$\\mu_1(x)    \\mu_0(x)$    (3)\nwhere $x = [x^u, x^c]$ is the features of an instance, $[,]$ represents the concatenation. Intuitively, the desired objective can be described as the difference between two conditional means $\\tau(x) = \\mu_1(x) -\\mu_0(x)$ ."}, {"title": "4 METHODOLOGY", "content": "Our UMLC consists of two customized modules: the response-guided context grouping module and a feature interaction module. The whole structure of our UMLC is shown in Figure 2, and we introduce the two modules as follows.\n4.1 Response-guided Context Grouping\n4.1.1 Motivation. As mentioned in Section 1, when considering the large-scale contexts, there is a distribution shift between treatment and control groups, which may introduce bias for uplift prediction. In recent years, there have been works focusing on solving the treatment assignment bias between the user groups to obtain the unbiased treatment effect [31, 37, 38]. The core ideas of these works are to learn a balanced representation in the latent space from a distribution perspective [31], or match the samples in the treatment and control groups from a sample perspective [38].\nHowever, in our problem setting, the above-mentioned methods that ignore the specific influence of the contexts may be unreliable. For example, in real-world scenarios, each user can be associated with many contexts, such as a user may view many short videos a day, resulting in numerous instances. Therefore, directly concate-nating the context features with user features may encounter the issue of variance inflation: similar context features may preserve many different labels, leading to unreliable predictions.\nTo address the issue, we propose to consolidate contexts into discrete groups, and then use these groups as proxy contexts to enhance uplift prediction. Employing this strategy has the poten-tial to reduce variance, but if the contexts in the same group are completely different, it may also introduce additional bias into the prediction of uplift [26]. Fortunately, if the merged contexts within the same group have a similar impact on the response, the resulting bias can be effectively mitigated.\n4.1.2 Analysis. For each context $x^c \\in \\mathcal{X}^c$, we define a correspond-ing group variable $g \\in \\{0, 1, 2, . . ., K-1\\}$ denoting the group that the contexts are clustered into. Therefore, regarding the group vari-able $g$ as a one-hot vector, we can define the conditional distribution on the group variable given context as $p (g | x^c)$. To support our method, we have the following assumptions.\nAssumption 1. We assume the response generation is of the form $y = h(x^u, x^c, t) + \\epsilon$ [14], where $|h(x^u, x^c, t)| \\le B_h$ and $\\epsilon$ is a noise term with zero mean. We also assume the contexts merged into one group have a similar effect on the response. It means that for arbitrary context pair $(x_i^c, x_j^c)$ satisfying $P (g|x_i^c) = P (g|x_j^c)$, we have\n$|\\mathbb{E} [y/x^u, x_i^c,t] - \\mathbb{E} [y/x^u, x_j^c, t]| \\le \\delta, \\forall x^u, t, i \\neq j,$   (4)\nwhere $||$ denotes absolute value operation, $B_h$ and $\\delta$ are two constants, and $\\delta$ is less than $B_h$.\nWith the above assumption, we attempt to depict the distance of function $h$ between different contexts and merge the contexts with small distances into groups. Given that the function space $h$ is of infinite dimension, it is impractical to compute the distance within the original function space. Then, we propose a method that involves transforming the resultant function $h$ into a finite-dimensional embedding, allowing for distance calculations within the projected space. To realize this strategy, we first give the fol-lowing assumption.\nAssumption 2. There exists a transformation function $\\phi$ which sat-isfies $\\forall x^u, t, i \\neq j |h (x^u, x_i^c, t) - h(x^u, x_j^c,t)| = ||\\phi(x_i^c)-\\phi(x_j^c)||_2+\u03b7$, $\u03b6, \u03b7$ are two constants, $||\u00b7 ||_2$ denotes the Euclidean distances [9].\nIf a transformation $\\phi$ is found to meet Assumption 2 with a small value $\u03b7$, then it is possible to perform clustering on the context embedding $(\\phi(x^c))$ using the Euclidean distance. Through this pro-cess, the maximum distance between embeddings within a cluster can be effectively controlled to a small constant $\u03ba$. Regarding the clusters as the context groups, the distance of regression function $\\delta \\le \u03b6\u00b7 \u03ba + \u03b7$ is also small. Then we have the following proposition to find such a transformation function.\nProposition 1. If we can find a predictive function $f$ and trans-formation function $\\phi$ with Lipschitz constraints on contexts such that $|h (x^u, x^c, t) \u2212 f (x^u, \\phi(x^c), t) | \\le \u00b5, \u2200x^u, x, t$ and $|f (x^u, \\phi(x_i^c), t) -f (x^u, \\phi(x_j^c), t)| \\le c \\cdot ||\\phi(x_i^c) - \\phi(x_j^c)||_2, \\forall x^u, x_i^c, x_j^c, t, i \\neq j$. Then the function $\\phi$ satisfies the Assumption 2 with $\u03b6 = c, \u03b7 = 2\u00b5$.\nWe provide the proof in Appendix A.\nInspired by the Proposition 1, to train the context embedding to be clustered, we aim to learn the transformation $\\phi$ and a Lipschitz regularized regressor $f$ with small predictive error (i.e., small $\u00b5$).\n4.1.3 Procedure. We use the categorical and numerical embed-dings [13] for the context features, and we construct the regression model as $f$ in Proposition 1 which predicts the response based on the user features, context features, and treatment with Lipschitz regularization.\nResponse Prediction. We denote $\\xi_\u03b8 (x^c)$ as the concatenation of the categorical and numerical embeddings [13] for context feature embeddings, \u03b8 is the learnable parameters. To ensure that the ac-quired embeddings $\\xi_\u03b8 (x^c)$ accurately capture the context effects on the responses and the distance between them, we concatenate the representation $\\xi_\u03b8 (x^c)$ with the user features and the treatment. This concatenation is then fed into the regression model $f$ to make predictions of the response. The prediction loss for the response-guided regression is defined as:\n$\\mathcal{L}_{pred} = \\mathcal{L} (f (x^u, \\xi_\u03b8 (x^c), t), y),$   (5)\nwhere $\\mathcal{L}(\u00b7,\u00b7)$ is the mean square error loss.\nLipschitz Regularization. To ensure a consistent representation of the relationship of the contexts in the embedding space, we em-ploy Lipschitz regularization. This regularization guarantees that the function $f$ is Lipschitz continuous, meaning that the distance in the embedding space accurately reflects the context effects. To sat-isfy Lipschitz continuity, there must exist a non-negative constant $c$ for $\\forall i, j, i \\neq j$ such that:\n$|f (z_i) \u2013 f (z_j)| \\le c ||z_i - z_j||_2,$   (6)\nwhere $z_i = (x^u, \\xi_\u03b8 (x^c), t_i)$ is the concatenation of the user fea-tures, the context embedding and the treatment, and intuitively, the change of response is bounded by constant $c$ for smoothness. It is evident that if the function $f$ is $c$-Lipschitz on the input $z$, then it is also $c$-Lipschitz on the context embedding $\\xi_\u03b8 (x^c)$.\nWe utilize Lipschitz Regularization [20], a technique that relies solely on the weight matrices of each network layer, with an esti-mated per-layer Lipschitz upper bound denoted as $c_i$. The purpose of this regularization is to constrain the discrepancies between con-text effects and responses. The corresponding loss function can be defined as follows:\n$\\mathcal{L}_{Lip} = \\sum_{i=1}^l softplus (c_i),$   (7)\nwhere $softplus (c_i) = ln (1 + e^{c_i})$ is a reparameterization strategy to avoid invalid negative estimation on Lipschitz constant $c_i$, and $l$ is the number of network layers.\nThen we can obtain the final loss function for training the response-guided context embedding:\n$\\mathcal{L}_{reg} = \\mathcal{L}_{pred} + \u03b1\\mathcal{L}_{Lip}.$   (8)\nwhere $\u03b1$ is the hyper-parameter to control the trade-off of aligning the difference between the context embedding and the response. Following the previous work [20], the value of $\u03b1$ is stated to be minimal, thus, we set it to be $10^{\u22124}$ in all experiments.\nGrouping and Aggregation. After we get the trained context embedding, grouping it is important for the data dimensionality reduction and volatility minimization. We determine the number of groups by setting a hyperparameter $K$, and utilize clustering algorithms like K-means to allocate each context embedding $(\\xi_\u03b8 (x^c))$ to a specific group $g$. The clustering mapping $\\mathcal{F}$, which assigns embeddings to groups, is formalized as: $\\mathcal{F}: (\\xi_\u03b8 (x^c)) \u21a6 g$.\nMoreover, to enhance the stability and accuracy of uplift model training and prediction aggregating data with the same features is necessary. Specially, for any two samples $(x_i^u, g_i, t_i, y_i)$ and $(x_j^u, g_j, t_j, y_j)$, $i \\neq j$, if they have the same treatment, user features and context group, we aggregate them into a new sample as:\n$y = \\frac{y_i + y_j}{2}   x = x^u, g_i = g_j, t_i = t_j.$   (9)\nThen, we can get the relabeled dataset $\\mathcal{D}_r = \\{x, g_i, t_i, y\\}_{i=1}^m$ with $m$ samples for subsequent uplift prediction.\n4.2 Feature Interaction\nThe second problem we need to solve is the feature interaction in uplift prediction. Feature interaction is designed to model combi-nations between different features and have been shown to signifi-cantly improve the performance of a response model [7]. To obtain better uplift prediction, we mainly focus on the user-context in-teraction and treatment-feature interaction, we detailed introduce them in the following.\n4.2.1 User-Context Interaction. In real time marketing, when con-sidering the context (i.e., short videos), the context information can influence the user behavior. Recent state-of-the-art methods [12, 18] represent the relationships between users and contexts using a ten-sor. However, utilizing a tensor makes it challenging to differentiate the effects of various context factors and to model the non-linear interactions between users and contexts. In this part, we use the user feature embedding $e_u$ and the grouped context embedding $e_c$ to model the user response. We adapt a parallel co-attention net-work [23] for the user-context interaction to get better prediction results, the first step is to compute the affinity matrix:\n$L = tanh (e_u \\text{ W}_L e_c),$   (10)\nwhere $\\text{W}_L$ is the weight parameters. After computing this affinity matrix, the parallel co-attention network considers this affinity matrix as a feature and learns to predict user and context attention maps via the following:\n$H_u =tanh (\\text{W}_u.e_u + (\\text{W}_c.e_c). L),$  \n$H_c =tanh (\\text{W}_c.e_c + (\\text{W}_u.e_u). L^T).$   (11)\nwhere $\\text{W}_u$ and $\\text{W}_c$ are the weight parameters. The affinity ma-trix $L$ transform the context attention space to the user attention space (vice versa for $L^T$). Next, we can get the normalized attention weights for the user and context embeddings:\n$\u03b1_u = softmax (\\text{W}_{hu}^T H_u),$  \n$\u03b1_c = softmax (\\text{W}_{hc}^T H_c),$   (12)\nwhere $\\text{W}_{hu}$ and $\\text{W}_{hc}$ are the weight parameters, $\u03b1_u$ and $\u03b1_c$ are the attention parameters for the user and context, respectively. Based on the above attention weights, the user and context attention vectors are calculated in the following:\n$\\hat{e_u} = \u03b1_u * e_u, \\hat{e_c} = \u03b1_c * e_c,$   (13)\nwhere $*$ denotes the sum of the product for each element in the embedding with the attention vector. Then, we use a multilayer perceptron (MLP) to predict control responses:\n$\\mu_0 = MLP (\\hat{e_u}, \\hat{e_c}).$   (14)\n4.2.2 Treatment-Feature Interaction. Considering the treatment assignment sensitive features in the concatenation of the user fea-tures and context features, we introduce a cross attention based treatment-feature interaction part to discover the information gain [35] introduced by the treatment assignment.\nSpecially, we denote the concatenation of $\\hat{e_u}$ and $\\hat{e_c}$ as $\\hat{e_f}$, the embedding of treatment as $e_t$. The we can get the attention weights of the feature embedding as follows:\n$\u03b1_t = softmax \\left(\\frac{(\\text{W}_te_t)^T (\\text{W}_f.\\hat{e_f})}{\\sqrt{K_d}}\\right),$   (15)\nwhere $\\text{W}_t$ and $\\text{W}_f$ are the weight parameters, $K_d$ is the dimension of the output embedding. Then, to simulate the treatment variation, we can obtain the $\u03b1_t^0$ and $\u03b1_t^1$ with different treatment embeddings $e_t^0$ (i.e., $t = 0$) and $e_t^1$ (i.e., $t = 1$), respectively. The information gain on the feature embedding introduced by the treatment assignment can be formulated as:\n$\\hat{e_\\triangle} = \u03b1_t^1 * \\hat{e_f} - \u03b1_t^0 * \\hat{e_f},$   (16)\nTo better optimize the cross attention structure to find treatment the sensitive features, we aim to maximize the information gain $\\hat{e_\\triangle}$ in the final loss function.\nSimilar to the control response prediction, we can also use a multilayer perceptron to predict the uplift of each sample with and without the information gain, respectively, as defined below:\n$\\tau^t = MLP(\\hat{e_f}), \\tilde{\\tau} = MLP(\\hat{e_\\triangle} + \\hat{e_f}).$   (17)\nThus, we can get the information gain coefficient through a difference function, which acts as the importance of the sample in the uplift prediction loss function, and is defined as follows:\n$\\text{w}_{batch} = \\frac{exp \\left(\\tilde{\\tau} - \\tau^t\\right)}{\\mathbb{E}_{batch} exp \\left(\\tilde{\\tau} - \\tau^t\\right)},$\n$\\mathcal{L}_{uplift} = \\text{w}_{batch} ((1 \u2212 t) \u00b7 \\mathcal{L}(\\mu_0, y)\n+t \u00b7 (\\mathcal{L}(\\mu_1, y) + \u03b2\u00b7 \\mathcal{L}(\\tilde{\\mu}_1, y))) \u2013 \u03b3 \u00b7 ||\\hat{e_\\triangle}||,$   (18)\nwhere $\\mu_1 = \\mu_0 + \\tau^t$ is the predicted treatment response, $\\tilde{\\mu}_1 = \\mu_0 + \\tilde{\\tau}$ is the predicted treatment response with treatment-feature interac-tion, $\\text{w}_{batch}$ is the sample weight calculated in the batch, $\u03b2$ and $\u03b3$ are the hyperparmeters to control the trade-off."}, {"title": "5 EXPERIMENTS", "content": "In this section", "questions": "n\u2022 RQ1: Can our UMLC outperform different baselines on vari-ous commonly used uplift modeling metrics?\n\u2022 RQ2: How does each proposed module contributes to the performance of our UMLC?\n\u2022 RQ3: How does the choice of group number influences the performance of our UMLC?\nAnd we also conduct more detailed experiments and analysis in Appendix C.\n5.1 Experimental Setups\n5.1.1 Datasets. Synthetic dataset. To simulate complex real-world scenarios", "features": "bi-nary", "17": "T-Learner [17", "31": "CFRNet [31", "32": "EUEN [16", "21": ".", "3": "we employ three evaluation metrics commonly used in uplift modeling", "1": "to accelerate the tuning process", "https": ""}]}