{"title": "VERITAS-NLI: Validation and Extraction of Reliable Information Through Automated Scraping and Natural Language Inference", "authors": ["Arjun Shah", "Hetansh Shah", "Vedica Bafna", "Charmi Khandor", "Prof. Sindhu Nair"], "abstract": "In today's day and age where information is rapidly spread through online platforms, the rise of fake news poses an alarming threat to the integrity of public discourse, societal trust, and reputed news sources. Classical machine learning and Transformer-based models have been extensively studied for the task of fake news detection, however they are hampered by their reliance on training data and are unable to generalize on unseen headlines. To address these challenges, we propose our novel solution, leveraging web-scraping techniques and Natural Language Inference (NLI) models to retrieve external knowledge necessary for verifying the accuracy of a headline. Our system is evaluated on a diverse self-curated evaluation dataset spanning over multiple news channels and broad domains. Our best performing pipeline achieves an accuracy of 84.3% surpassing the best classical Machine Learning model by 33.3% and Bidi-rectional Encoder Representations from Transformers (BERT) by 31.0%. This highlights the efficacy of combining dynamic web-scraping with Natural Language Inference to find support for a claimed headline in the corresponding externally retrieved knowledge for the task of fake news detection.", "sections": [{"title": "I. INTRODUCTION", "content": "As more and more people rely on digital platforms as their primary source of information, the threat of fake news has been exacerbated. This phenomena not only undermines the trust in reliable sources but also has the potential to skew public opinion [1]. The ease at which false narratives spread across social media and online platforms underscores the urgent need to address this issue [2].\nWith the proliferation of fake news as evidenced by the spread of fabricated stories in the 2016 US elections [3], individuals are often overwhelmed and unsure about the authenticity of the information they encounter. With Google's Adsense becoming the primary metric of success for a news site, a push has been made towards publishing 'click-bait' articles which catch a reader's attention [4]. Zhou et al. [5] highlighted the explosive growth of fake news and it, consequentially leading to the erosion of democracy, justice, and public trust.\nFake news is sometimes intentionally created and disseminated as part of misinformation campaigns aimed at manipulating public opinion or advancing specific agendas. Especially on social media, studies reveal that while Facebook has seen a decline in user interactions with misinformation since 2016, Twitter continues to see a rise [6]. The findings highlight that fake news and misinformation still prevail on social media platforms [1] [2]. Hence ascertaining the veracity of news articles will play a crucial role in preventing the spread of misinformation, thereby guaranteeing the populace's access to trustworthy and empirically-supported knowledge.\nThis paper introduces a novel solution, VERITAS-NLI, leverages state-of-the-art Natural Language Inference Models for verifying claims by comparing them against externally retrieved information from reputable sources in real-time via web scraping techniques. Furthermore, we employ small language models to generate questions based on the headline, enhancing the verification process through a question-answering approach. To assess the consistency between the scraped article and the input headlines, we employed NLI models of varying granularity. Additionally, we constructed an evaluation dataset by manually curating real headlines from reputable sources and synthetically generating fake headlines to simulate misinformation scenarios. These techniques enable our model to adapt to dynamic content, as it does not rely on static, outdated data. By continuously validating claims against real-time information, our approach ensures that the model remains relevant and effective in rapidly changing news environments.\nOur model analyzes the content of news articles, posts, or other forms of media to identify potential indicators of fake news. This analysis involves examining the language, sources, factual claims, pronoun-swapping, sentence-negation, named-entity preservation and overall credibility of the information presented. The detector cross-references the claims made in the news content with the latest reliable and authoritative sources.\nIn the subsequent sections of this paper, we will delve into the underlying methodology of VERITAS-NLI - Validation and Extraction of Reliable Information Through Automated Scraping and Natural Language Inference elucidates the key features and algorithms employed, and presents empirical results demonstrating its efficacy and performance, which we have compared to the related work reviewed in section 2. We have conducted thorough comparisons by bench mark-ing different approaches against our evaluation dataset, to analyze their performance in real-world situations. We have trained and compared multiple models from simple Machine-Learning approaches like Logistic Regression to more complex approaches utilising BERT and Natural Language Inference models, notably FactCC [7] and SummaC [8]."}, {"title": "II. LITERATURE SURVEY", "content": "The initial attempts to counter false information relied on manually checking claims like the site factchecker.in [9], known for it's efforts to verify claims and statements made in the public domain. However, human fact-checking is resource-intensive and may not scale as easily to handle the sheer volume of information available online. Human fact-checkers may introduce subjectivity and potential bias in the interpretation of claims.\nRecent advancement in machine learning have shown promise in improving the effectiveness of fake news detection. A significant number of studies have explored the use of classical machine learning algorithms for this purpose. Abdullah-All-Tanvir et al. [10] and Pandey et. al. [11] conducted comparative studies, both highlighting the higher performance of Support Vector Machine (SVM) and Na\u00efve Bayes classifier in detecting fake news. Aphiwongsophon et al. [12], further applied these methods to dataset collected from twitter API, reaching an accuracy of 96.08%. However each of these studies relied on a niche focus on a specific topic, raising concerns about the broader applicability of these findings. Ahmed et al. [13] employed n-gram analysis along with machine learning techniques, again reporting SVM as the best performing model.\nOther algorithms have also been utilized like Ni et al. [14] explored the use of Random Forest Classifier getting an accuracy of 68% on PolitiFact and GossiCop datasets. Kesarwani et al. [15] used K-Nearest Neighbour (KNN) algorithm on a dataset collected from BuzzFeed News getting an accuracy of 79%. Rajendran et al. [16] compared Decision Tree and Naive Bayes, finding that Decision Trees outperformed Naive Bayes and gave a mean accuracy of 99.70% when tested on fake political news. Kotteti et al. [17] emphasized the importance of data preprocessing, particularly the handling of missing vales, while training classical machine learning models on the LIAR dataset. The effectiveness of ensemble methods have also been explored by researchers like Kaur et al. [18] who proposed a multi-level voting ensemble models that combined multiple classifiers based on their false prediction ratio. Bozuyla et al. [19] extended this concept by utilizing AdaBoost ensemble learning on top of Naive Bayes, improving the accuracy by 2%. Many of the studies presented face limitations in achieving high accuracy or generalizability due to their reliance on event-specific datasets. While classical machine learning models can be used as valuable bench marking tools, they often fall short for the complex task of fake news detection highlighting the need for more advanced and robust models to improve accuracy and generalize findings across different contexts.\nResearchers at MIT [20] developed a deep learning network that detects patterns in the language of fake news. This model was trained on the data up to 2016 and is suggested to be used with other techniques like automated fact-checkers. Graph-based models like the one proposed by Zhang et al. [21] introduced a self-supervised contrastive learning and a new Bayesian graph Local extrema Convolution (BLC). This method aggregates node features in a graph while accounting for uncertainties in social media interactions. Despite achieving high accuracy on Twitter datasets, the model struggles with the feature learning challenges posed by the power-law distribution of node degrees in social networks. Another approach presented by Li et al. [22] introduces KGAT, a neural network utilizing graph attention networks, to perform fine-grained fact verification by analysing relationships between claim and evidence presented as a graph.\nTransformer-based models like BERT have shown significant promise in this task of fake news detection. Alghamdi et al. [23] compared different machine learning and deep learning techniques and found that BERT-base outperformed other models using contextualized embeddings, demonstrating great efficacy across multiple datasets. This aligns with the findings of work done by Aggarwal et al. [24], where BERT achieved an accuracy of 97.02% on NewsFN dataset, outperforming LSTM and Gradient Boosted Tree models. Combating Covid-19 misinformation has also been researched on, for example, Alghamdi et al. [25] in his other paper used BERT model on COVID-Twitter dataset getting an F1 score of 98%, along with the work done by Ayoub et al. [26] who used DistilBERT and SHAP (Shapley Additive exPlanations). Arun et al. [27] reported a 96.5% accuracy by analyzing headlines and the supporting text using BERT, and Kaliyar et al. [28] introduced FakeBERT, a model that employs bidirectional learning to enhance the contextual understanding of articles, contrasting with traditional unidirectional approaches of looking at a text sequence. Bataineh et al. [29] used Bidirectional Long Short-Term Memory (Bi-LSTM) which is further optimized by genetic algorithms.\nIn addition to transformers, hybrid models combining various deep learning techniques have been explored. Ajik et al. [30] employed a hybrid approach combining Convolu-tional Neural Networks (CNN) and Long Short-Term Mem-ory (LSTM) networks, achieving an accuracy of 96% on a dataset sourced from Kaggle. Dong et al. [31] emphasized the importance of contextual features and user engagement in the propagation of fake news. He combined supervised and unsupervised learning paths with CNNs, performing well even with limited labeled data. A similar approach was taken by Li et al. [32] where a self-serving mechanism was used to improve detection over traditional methods. Transformer based models like BERT have been largely used for fake news detection.\nTechniques for natural language processing or NLP are essential for spotting linguistic clues that point to false information. Chesney et al. [33] brought attention to the difficulties in using current NLP techniques because incongruence must be identified in relation to the text it represents. Large language models like GPT-3 have also been used in studies like the one done by Farima et al. [34], where they used GPT-3 for generating questions, responses and verifying facts. Despite promising results (77-85% F1), has certain limitations like it depends on the initial set of questions generated by LLMs and operates on small-scale datasets, potentially limiting scalability and generalizability. We utilize open source Small Language Models (SLMs) and achieve comparable results on our eval-uation dataset, presenting more quantitative findings. Another study by Eun et al. [35] employs LoRa for fine-tuning LLMs and evaluates claim accuracy across various claim styles by adjusting the temperature setting in the model. However, it majorly only focuses on combating fake news related to the coronavirus period, raising concerns about the generalizability of these claims.\nThe importance of social context in fake news detection has been highlighted in various studies. Research done by Shu et al. [36]talked about the challenges in understanding the underlying characteristics of fake news, suggesting more research on utilizing social context. Kang et al. [37] further emphasized the importance of incorporating user engagement metrics, showing that user information can enhance detection even when data contains hate speech.\nThere have been suggestions to use hybrid approaches that combines both linguistic and network-based methods like the work done by Conroy [38] who presented a comprehensive survey of current technologies and methodologies for detecting fake news. They emphasised that tools should be designed to augment human judgement, not replace it.\nSeveral datasets and evaluation metrics have been de-veloped to facilitate research in fake news detection. The FEVER dataset, introduced by Thorne et al. [39], serves as a benchmark for fact extraction and verification tasks. Hassan et al. [40] made a data collection site to collect ground-truth labels of the sentences and deployed a automated fact-checking system on the 2016 presidential primary debate and compared its performance to professional news and organi-sations. Reddy et al. [41] leveraged ClaimBuster model for claim detection and employed a Bart-large model trained on a MNLI (MultiNLI) corpus as zero-shot topic-filtering system to filter out claims related to the topics under consideration.\nIn summary, the research on fake news detection has shown significant progess in developing various methods and models. However, several research gaps have been observed as noted by Almahadi et al. [42] in his comprehensive review of different approaches of fake news detection (FND). One notable limitation is the lack of real-world implementation of these techniques. While theoretical models have been explored extensively, there is a need to address practical issues related to scalability, robustness, and the ethical implications of implementing FND in the real-world scenario. Moreover, exploration of novel ML algorithms specifically for FND that address the limitations of classical ML algorithms, such as their reliance on static training data and inability to keep up with the current news in a dynamic environment. Our work aims to tackle both of these areas, moving closer to a robust and practical solution for combating fake news."}, {"title": "III. METHODOLOGY", "content": "In our study, we utilize the LIAR dataset a publicly available resource for fake news detection compiled from POLTIFACT.COM, a Pulitzer Prize-winning website. The credibility of this dataset can be attributed to the multiple human evaluated statements along with authentication of the POLITIFACT editor [17] [23] [31].\nThe dataset samples news claims from various multimedia sources ranging from news releases, TV/radio interviews, campaign speeches, advertisements and social media platforms such as Twitter, Facebook etc. This dataset primarily focuses on societal and government related news topics ranging from economy, taxes, campaign-biography, elections, education, jobs are some the most represented subjects. The claims based on these subjects are the most prone to misinterpretation, and spreading of misinformation by tweaking minuscule sections of the claim to drastically change the implication of the claim. This serves as strong foundation for the culmination of a robust fact and integrity analyzer developed on the foundation of classical NLP Techniques and cross-verification using web-scraping methodologies [43] [44].\nThe LIAR dataset consists of 12.8k statements labeled for truthfulness, collected over a ten-year period. Each statement is accompanied by metadata such as the speaker, context, and a detailed judgment report. The LIAR dataset is structured to include several fields that capture various dimensions of each statement recorded. Here's an elaboration on each type of field found in the LIAR dataset:\n\u2022 Label Description: The truthfulness rating assigned to the statement. LIAR dataset uses a multilabel system, which can include labels such as \"True\", \"Mostly True\", \"Half True\", \"Barely True\", \"False\", and \"Pants on Fire\".\n\u2022 Label Assignment: The rationale or explanation is provided by fact-checkers on why a particular label was assigned to the statement. This field often includes evidence or reasoning based on research, reports, and data analysis."}, {"title": "B. Evaluation Dataset", "content": "To evaluate the performance of the models trained, we propose a new manually curated dataset consisting of the latest headlines of 2024. This dataset with more recent headlines help us benchmark the efficacy of the various models trained in identifying unreliable headlines in a dynamic and rapidly-evolving landscape.\nThe dataset consists of headlines from a wide range of domains, including science, sports, pop culture, politics, and business. It contains 346 credible headlines collected from various trusted sources such as CNN, USNews, The Guardian, BBC, The New York Times, Reuters, Times of India, CNBC, and others.\nUsing each reliable headline, we generate a corresponding unreliable headline using a Small Language Model(SLM), specifically microsoft/Phi-3-mini-4k-instruct. We devised a zero-shot prompt without any examples, resulting in a total of 692 headlines in our evaluation dataset.\nThe small language models were prompted to modify the reliable headlines through a combination of techniques such as sentence negation, number swaps, replacing named entities, and by injecting noise into the headline. The aim was to create convincing fake news headlines that can serve as a challenging evaluation metric for the models trained.\nUnreliable Headline Generation Prompt: \"Given the true headline, you have to make changes to it using a combination of Sentence negation, Number swaps, Replacing Named Entities and Noise Injection to generate a fake news headline. The output should be only the generated fake news headline to evaluate my fake news model.\""}, {"title": "C. Classical Model Performance Benchmarks", "content": "Classical Machine Learning: Classical Machine Learning algorithms have been used extensively for the task of analyzing the veracity of news articles [11], capitalizing on the wealth of data provided by datasets like LIAR to train models.\nPrior to feeding the data from the LIAR dataset for the training of these models, a series of pre-processing steps have to be taken.\n\u2022 Stopword Removal: In this step extremely common words (known as stopwords), that carry little meaning for the task of prediction, are removed from the text to reduce the noise in the dataset.\n\u2022 Token Filtering: we perform token filtering prior to lemmatization in order to exclude non-alphabetic tokens. This step ensures that the non-alphanumeric characters do not impede model training.\n\u2022 Lemmatization: Words in the news articles and headlines are reduced to their root form to normalize the text. This is done using NLTK's WordNetLemmatizer.\n\u2022 TF-IDF Vectorizer: TF-IDF stands for Term Frequency Inverse Document Frequency.It is a mathematical method that is used to represent how important a word is within a document relative to a collection or corpus. This importance of a word is directly proportional to how often it appears in a document but is offset by the frequency of the word in the complete text corpus.\nTerm Frequency (TF) The term frequency $tf(t,d)$ is defined as:\n$tf (t, d) = \\frac{f_{t,d}}{\\Sigma_{t' \\epsilon d} f_{t',d}}$\nwhere $f_{t,d}$ is the frequency of term t in document d, and the denominator is the sum of all term frequencies in that document. Inverse Document Frequency (IDF) The inverse document frequency $idf (t, D)$ is defined as:\n$idf (t, D) = log(\\frac{N}{\\{d \\epsilon D : t \\epsilon d\\}|})$\nwhere $N$ is the total number of documents in the corpus D, and $\\{d \\epsilon D : t \\epsilon d\\}|$ is the number of documents where the term t appears.\nUsing the aforementioned preprocessing techniques, we trained the following models:\n\u2022 Linear SVC: Linear Support Vector classifier is a base classical model which attempts to find a hyperplane to maximize the distance between classified samples. It requires and functions around a kernel which provides adaptations for dealing with non-linear datapoints. [11] [18].\n\u2022 Multinomial Naive-Bayes: This classical model belongs to the family of probabilistic classifiers derived from the eminent Bayes Theorem. Multinomial Naive Bayes is a variant of Naive Bayes used for document classification which makes it a desirable choice to use for our particular problem. Each sample represents a feature vector in which certain events have been counted in the multinomial model [11] [16] [19].\n\u2022 Random Forest Classifier: This model represents a meta-estimator creating a number of decision tree classifiers considering sub-samples of words in the pre-processed dataset to improve predictive accuracy [14].\n\u2022 Logistic Regression: It is used to model the relationship between one dependent and one or more independent variables. This model predicts the likelihood of something falling into a specific group. It uses the logistic function to map the linear combination of independent variables to a probability between 0 and 1 for each class. [10] [13] [18] [36]."}, {"title": "2) Bidirectional Encoder Representations from Trans-formers", "content": "The Liar Dataset consists of very comprehensive claims, for which the labels naturally rely heavily on nuanced sentence interpretation and context. Owing to the performance demonstrated by transformers in capturing textual information we naturally diverted our attention to transformer based models. Fine-tuning pre-trained models such as BERT-base-uncased has allowed us to explore an architecture capable of identifying discrepancies in general news patterns due to it's deeper understanding of context through vector representations of characters in the English language stemming from the fact that it has been trained on a large corpus of over three billion words [45].\nBERT is based on the Transformer [46] architecture, which captures the context of words in a sentence by considering words from both left and right contexts. This is essential for language tasks like fake news detection, where understanding the context in which words appear can determine the authenticity of the information [23]\u2013[28]. BERT is pre-trained on \"masked language modeling\" and \"next sentence prediction\" and for our specific use-case, \"sequence classification\". This pre-training helps the model learn a wide range of language patterns and knowledge, which might not be present in the specific training dataset, however, fine-tuning it on our dataset using transfer-learning can provide an advantage while solving a specific problem such as fact-checking. For the fine-tuning process, task-specific layers are added to the BERT architecture for the downstream sequence-classification task, and only these newly added layers are trained to minimize computational costs.\nBERT's bidirectional context understanding helps it effectively handle polysemous words (words with multiple meanings) better than models that consider only one-directional context or no contextual information at all. This is important because within polysemous words and the words that come before and after them, the severity of the statement is identified on varying scales. For example, the negative connotation of a word in a particular context is enhanced based on the synonym of the word used. Unlike classical models that often require significant customization and feature engineering, BERT can be fine-tuned with a small number of task-specific examples to achieve high performance, leveraging the extensive pre-training."}, {"title": "III. METHODOLOGY", "content": "training. The procedure involving the fine-tuning of a BERT model on the LIAR dataset begins with an intricate tokenization process. For this process, we use the WordPiece algorithm [47]:\n1) Text Normalization: Convert all text to lowercase (for BERT-base-uncased) and perform unicode normalization.\nText\u2192 Normalized Text\n2) Punctuation Splitting: Split the text at punctuation, adding spaces around each punctuation mark.\nNormalized Text \u2192 Split Text\n3) WordPiece Tokenization: Break words into subword units based on a trained vocabulary. Unknown words are split into smaller subwords that exist in the vocabulary.\nWord \u2192 Subword\u2081 + Subword\u2082 + ... + Subwordn\n4) Adding Special Tokens: Add special tokens such as [CLS], [SEP] for specific tasks (e.g., classification, question answering).\nFinal Tokens = [CLS] + Tokenized Text + [SEP]\nThis process enables BERT to handle a wide range of language inputs and effectively manage both common and rare words. This is done along with padding and truncation to ensure equal length input vectors for uniformity. We employ transfer learning techniques using the BERT-base-uncased model, sourced from the Hugging Face transformers library. The methodology focuses extensively on the fine-tuning process:\n\u2022 Initialization: Fine-tuning begins with the pre-trained models which have been initially trained on a large corpus, such as the BooksCorpus and English Wikipedia. This pre-training has equipped the models with a robust understanding of general language contexts.\n\u2022 Task-Specific Adjustments: The models are then further fine-tuned on a labeled dataset specific to our use case. This process involves tweaking the last few layers of the model so that the model can align its outputs with the task at hand. For our requirement of binary-classification"}, {"title": "IV. PROPOSED SOLUTION", "content": "Our proposed solution, VERITAS-NLI explores various novel pipelines that make use of web-scraping, Small Language Models (SLM) and Natural Language Inference (NLI) models at its core. This solution mimics the natural cognitive processes humans use to evaluate whether a particular claim is reliable or unreliable. The pipelines, namely: Question-Answer Pipeline, Article Pipeline and Small Language Model Pipeline are a testament to this underlying intuition and demonstrate it's effectiveness in the subsequent sections."}, {"title": "A. Web Scraping Techniques", "content": "We make use of web-scraping techniques to retrieve external knowledge based on the raw input headline, the questions generated by small language models to verify the headline, the 'People Also Ask' section of a Google search and Google's 'Quick Answer' to retrieve relevant information in real-time. This real-time knowledge is critical for our task of detecting fake news in a dynamic and fast-changing environment [44], [48].\nSelenium Based Scraping: Our pipeline leverages 'chromedriver' and adaptive web-scraping of the document-object-model(DOM) by traversing the XML Tree to acquire external information to aid in verifying the input [43]. There are multiple different approaches we take to scrape external information relevant to the headline, they are outlined below :\n\u2022 Fact-Based Headline Verification: For user headlines that represent facts we can verify them by scraping the text that is provided by the Google Search 'Quick Answer'. These webpages do not change their content and layout frequently and are known as static pages which allows us to simply retrieve the Quick Answer by scraping that HTML tag represented in the XML tree.\n\u2022 Top-K Articles Retrieval: A naive google search is performed initially to retrieve the top-k links corresponding to the headline. This is followed by the scraping of relevant 'titles' and useful 'contents' from these links (generally represented by <h> and <p> html tags) to cross-validate the claimed headline.\n\u2022 People Also Asked: We incorporate Google Search's 'People Also Asked' (PAA) questions relevant to the news-headline entered by the user. The People Also Asked question provides different interpretations for a particular headline most frequently inferred by people. This also provides an all-rounded context for the evaluation of the claimed headline. Additionally, more popular claims will consist of a greater number of questions which will inherently provide a more comprehensive context that assists the model to identify confidence in a given claim as well as sometimes point out the ambiguity of claims that are opinionated, unclear, or shrouded in noise.\n\u2022 SLM Generated Questions: Furthermore, we make use of Small Language Models (Mistral-7B-Instruct -v0.3 [49] or Microsoft's Phi-3-mini-4k-instruct [50]) to generate a question that helps us retrieve relevant information needed to validate the reliability of a given headline.\nWe utilize trusted sources for information retrieval and adhere to the 'robots.txt' laid down by all of the sources such as CNN, USNews, The Guardian, BBC, The New York Times, Reuters, Times of India, CNBC, etc which scrutinize the published articles on their respective platforms and are generally regarded as trustworthy news outlets.\nA 'robots.txt' file is a directive used by website administrators to communicate with search engine crawlers and web scrapers, specifying which pages or sections of a site they are permitted to access and index. Its primary purpose is to manage web traffic, ensuring that crawlers do not overwhelm the server with excessive requests. By controlling the areas of the site accessible to bots, the 'robots.txt' file helps optimize server performance, protect sensitive information, and prioritize which content should be visible in search engine results."}, {"title": "B. Question-Answer Pipeline", "content": "In this pipeline we experiment with an amalgamation of methods using the Google Search 'Quick Answer' which is highly dependent on whether the given input headline is a hard fact or not. So to make this a more thorough pipeline we've incorporated the 'People Also Asked' (PAA) questions for the input headline, as a fallback to the 'Quick Answer' scraping. However, we found that the 'People Also Asked Section' of a particular Google page is heavily dependant on user interaction for a particular headline if the input is an uncommon occurrence it may not be represented by the 'People Also Ask' section. In this case we have a terminating fallback which resorts to the naive scraping of the top-k articles that are found.\nThe third pipeline in Figure 5, highlights the flow of the web-scraping and claim verification process. The scraped content is then passed to the headline-verifying NLI model as the source-text (premise) and the input headline is the claim (hypothesis). The model then generates a prediction along with a confidence score which represents how well the claims made in the headline are supported by the externally scraped information."}, {"title": "C. Small Language Model Pipeline", "content": "Within this pipeline, we explore the use of Small Language Models to generate questions that can be used scrape more relevant content to verify the entered claim. We follow a scraping approach identical to the one used in the Question-Answer Pipeline with an added zero-shot [51] SLM question generation step [34]."}, {"title": "D. Article Pipeline", "content": "The article pipeline leverages a web-scraping methodology [43] to gather data from various online sources systematically. This approach allows for the extraction of relevant content, such as content related to a particular input headline, based on the Top - K links to articles retrieved as illustrated in Pipeline 1 of Figure 5. The web scraper acquires both the headings, and the content from these links and the model is supplied with this retrieved information for the verification process. By structuring the pipeline in this way, it ensures that the process is both efficient and reliable, the inclusion of an article's content helps provide a rich-contextual understanding of the topic at hand to help verify the claim.\nInterestingly, the research has shown that this naive approach leads to more accurate outcomes. In this context, a simple yet well-organized strategy for scraping and processing data performs better than more intricate alternatives. To contrast this pipeline intimately with the previous three where 'Quick Answer' and 'People Also Asked' summaries are provided to the model may be small or incomplete for certain user queries misguiding the model. The best output is closely correlated with the scraping of the right amount of information quantity to quality (coherence) of the scraped content. This is further highlighted from the statistical results obtained and delineated in subsequent results section.\nThe scraped articles pertinent to the claim, are then processed by Natural Language Inference (NLI) models, specifically FactCC [7] and SummaC [8], that make use of the externally retrieved knowledge to analyze and detect potential inconsistencies within the claimed headline."}, {"title": "E. NLI-based Models for Headline Inconsistency Detection", "content": "The next step in our proposed solution is to detect inconsistencies in the headline (hypothesis) by comparing it with the source (premise) text fetched from the web. This step is analogous to the task of Natural Language Inference (NLI). These NLI models are designed to determine the relationship between two pieces of text, a 'premise' and a 'hypothesis', and are classified into 3 categories:\n\u2022 Entailment: The hypothesis is logically supported by the premise.\n\u2022 Contradiction: The hypothesis is logically inconsistent with the premise.\n\u2022 Neutral: The hypothesis neither entails nor contradicts the premise.\nThe performance of these models has seen substantial improvements in recent years, supported by large training datasets like MNLI [52], SNLI [53] and VitaminC [54]. Modern attention-based architectures exhibit proficiency that parallels human performance for this task. [8]"}, {"title": "1) FactCC", "content": "One approach of our proposed solution, VERITAS-NLI leverages FactCC [7] , a model developed to ensure that summaries generated by abstractive models remain factually consistent with their source documents [7]. This section delves into the architecture of FactCC and its contributions to the field of text summarization. FactCC is built upon the BERT architecture, leveraging its pre-trained powerful contextual embedding capabilities. The model operates under a weakly-supervised learning framework, which is designed to handle the challenges of factual consistency in text summarization. It was trained on synthetic data which enables it to be a perfect candidate for downstream learning tasks. FactCC employs a multi-task [55] learning approach, where the model is simultaneously trained on several related tasks:\n\u2022 Consistency Identification: Determining if a transformed sentence from the source document remains factually consistent.\n\u2022 Support Extraction: Identifying and extracting the specific spans in the source document that support the factual consistency of the summary.\n\u2022 Error Localization: Pinpointing and highlighting the spans within the summary that are factually inconsistent."}, {"title": "V. RESULTS", "content": "In our comprehensive study on the efficacy of machine learning and deep learning models for fake news detection, our study finds significant performance differences between classical Machine Learning models, BERT models and our proposed Transformer-based pipelines that allow it to access external knowledge in order to classify the news headline at hand.\nClassical Machine Learning models rely heavily on static training data, this dependency severely hampers their ability to classify news headlines in a dynamic environment. These straightforward approaches cannot adapt effectively to ever-changing relationships and contexts inherent in real-world scenarios.\nFrom the results in Table I, the accuracy's of classical machine learning models for the given binary-class classification task of news headlines on the training dataset produce an average F1-score of 0.675 and an accuracy of 60.04% when tested on the LIAR dataset using a 70:30 train-test split.\nHowever, when these models were trained entirely on the LIAR dataset and then tested on our new proposed evaluation dataset, they produce an average F1-score of 0.537 and accuracy of 49.86%. This sharp decline highlights the models' reliance on static training data, which limits their ability to accurately classify unseen news articles in a dynamic and ever-changing environment. The drop-off in performance is exacerbated by the fact that the models tested on our evaluation dataset were trained on a larger corpus of news headlines, due to the lack of a train-test split.\nThus, the inability of these models to adequately generalize from their training data to accurately classify current headlines, renders them unsuitable for the task at hand. Nevertheless, these classical models serve as an important baseline for subsequent, more advanced models."}, {"title": "A. How effective is our Proposed Solution for the given task?", "content": "Notably, our proposed solutions VERITAS-NLI, significantly outperforms prior approaches. From the results presented in Table III, The highest accuracy achieved from our proposed pipelines is 84.3%, by the Article-Pipeline using SummaC-ZS as the NLI-based consistency checking model. This is a substantial 33.3% gain over the best performing classical machine learning model (MultinomialNB) and a 31.0% gain over the transformer-based BERT model. Out of the 12 pipeline configurations we tested (4 scraping approaches and 3 headline inconsistency detection NLI models), 10 configurations demonstrated superior accuracy compared to all baseline models.\nThe reliability of the solution proposed stems from our novel web-scraping techniques for dynamic information retrieval for the verification of the claim. With growing interpolation in writing techniques for articles our proposed solution captures, the discrete idea of the document and claim by making use of sentence level granularity. This also avoids having to retrain the model on newer data creating a robust solution for verification of claims.\nOur solution provides explainability by providing the externally retrieved knowledge used to arrive at a decision as well as the links to Top-K articles fetched as displayed in Figure 5. This alleviates the black box created by other models and provides a way to interpret the models decision making process."}, {"title": "B. Which NLI-based Model performs best for Headline Inconsistency Detection?", "content": "For headline inconsistency detection, the SummaC model consistently outperforms FactCC, particularly due to its application of NLI-models on sentence-level pairs of a premise and hypothesis. SummaC is designed to capture fine-grained inconsistencies by analyzing individual sentence-level relations, while FactCC focuses on document-level fact-checking. This enhanced granularity allows SummaC to better detect subtle inconsistencies between headlines and their corresponding scraped content.\nMoreover, SummaC's two variants\u2014SummaC-ZS (zero-shot) and SummaC-Conv (Convolution)\u2014outperform FactCC across all evaluation metrics. This superior performance can be attributed to SummaC's unique approach of leveraging a Natural Language Inference (NLI) pair-matrix. The NLI matrix enables SummaC to systematically compare sentence pairs and identify contradictions or entailments with greater precision, leading to more accurate headline inconsistency detection.\nThis is consistent with the results obtained in Table III, the average accuracy for FactCC, SummaC-ZS and SummaC-Conv based pipelines is 59.83%, 77.15% and 69.33% respectively. The SummaC pipelines outperform the FactCC pipelines by 17.32% and 9.5"}]}