{"title": "Mathematical Formalism for Memory Compression in Selective State Space Models", "authors": ["Siddhanth Bhat"], "abstract": "State space models (SSMs) have emerged as a powerful framework for modelling long-range dependencies in sequence data. Unlike traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs), SSMs offer a structured and stable approach to sequence modelling, leveraging principles from control theory and dynamical systems. However, a key challenge in sequence modelling is compressing long-term dependencies into a compact hidden state representation without losing critical information.\nIn this paper, we develop a rigorous mathematical framework for understanding memory compression in selective state space models. We introduce a selective gating mechanism that dynamically filters and updates the hidden state based on input relevance, allowing for efficient memory compression. We formalize the trade-off between memory efficiency and information retention using information-theoretic tools, such as mutual information and rate-distortion theory. Our analysis provides theoretical bounds on the amount of information that can be compressed without sacrificing model performance.\nWe also derive theorems that prove the stability and convergence of the hidden state in selective SSMs, ensuring reliable long-term memory retention. Computational complexity analysis reveals that selective SSMs offer significant improvements in memory efficiency and processing speed compared to traditional RNN-based models. Through empirical validation on sequence modelling tasks such as time-series forecasting and natural language processing, we demonstrate that selective SSMs achieve state-of-the-art performance while using less memory and computational resources.\nOur findings highlight the potential of selective SSMs for real-time and large-scale sequence tasks, and we outline several future directions, including the extension of gating mechanisms, applications to nonlinear models, and integration with hybrid architectures.", "sections": [{"title": "1. INTRODUCTION", "content": "In recent years, state space models (SSMs) have gained prominence as an effective architecture for modelling long-range dependencies in sequence data.(Gaurav, Kumar, & Singh, 2023) Unlike traditional recurrent neural networks (RNNs) or convolutional neural networks (CNNs), which often struggle with vanishing or exploding gradients, SSMs offer a more structured and stable approach to sequence modelling. (Gu, et al., 2023) By leveraging the underlying principles of control theory and dynamical systems, SSMs are capable of handling complex temporal dynamics more efficiently.\nHowever, one of the central challenges in sequence modelling is the compression of long-term dependencies into a compact hidden state representation. (Krestinskaya, Zhang, & Salama, 2023) Memory compression in neural models aims to preserve relevant infor-"}, {"title": "1.1 PROBLEM DEFINITION", "content": "The problem we address in this paper is how selective SSMs achieve memory compression while preserving the necessary information for accurate sequence prediction. Specifically, we aim to:\n\u2022 Develop a rigorous mathematical framework to analyse the memory compression capabilities of selective SSMs, incorporating principles from rate-distortion theory and the information bottleneck method to quantify compression efficiency.\n\u2022 Quantify the trade-off between memory efficiency (in terms of reduced hidden state size) and information retention (the amount of sequence information stored in the hidden states) using information-theoretic tools, such as mutual information and rate-distortion functions.\n\u2022 Provide theoretical bounds on memory compression using Fano's inequality and the data processing inequality to ensure reliable sequence information retention."}, {"title": "1.2 CONTRIBUTIONS AND PAPER STRUCTURE", "content": "This paper makes the following contributions:\n\u2022 We introduce a formal mathematical model for memory compression in selective SSMS, detailing how selective gating mechanisms allow the model to filter and retain relevant information while discarding irrelevant data.\n\u2022 We apply information-theoretic measures, such as mutual information, to evaluate the amount of sequence information retained by the hidden states.\n\u2022 We derive upper and lower bounds on memory compression, providing a theoretical analysis of the efficiency of selective SSMs in comparison to traditional RNNs and CNNs.\n\u2022 We explore the computational complexity of selective SSMs, showing how they balance between memory retention and efficiency.\nThe remainder of this paper is organized as follows. In Section 2, we review the fundamentals of state space models and describe the selective gating mechanisms. In Section 3, we introduce the mathematical formalism for memory compression and define key trade-offs."}, {"title": "2. STATE SPACE MODELS AND SELECTIVE MEMORY", "content": "A stochastic state space model (SSM) introduces randomness into the state transitions and observations, allowing for a probabilistic interpretation of the system. (Kulikova & Kulikov, 2023) The hidden state at time t, denoted $h_t$, evolves according to:\n$h_t = Ah_{t-1} + Bx_t + w_t$,\n$y_t = Ch_t + v_t$,\nwhere:\n\u2022 $h_t \\in \\mathbb{R}^d$ is the hidden state vector.\n\u2022 $x_t \\in \\mathbb{R}^m$ is the input vector at time t.\n\u2022 $y_t \\in \\mathbb{R}^n$ is the output vector.\n\u2022 $A \\in \\mathbb{R}^{d \\times d}$ is the state transition matrix.\n\u2022 $B \\in \\mathbb{R}^{d \\times m}$ is the input mapping matrix.\n\u2022 $C \\in \\mathbb{R}^{n \\times d}$ is the output mapping matrix.\n\u2022 $w_t \\sim N(0, Q)$ is the process noise, assumed to be Gaussian with zero mean and covariance Q.\n\u2022 $v_t \\sim N(0, R)$ is the observation noise, also Gaussian with zero mean and covariance R.\nThe inclusion of $w_t$ and $v_t$ introduces stochasticity into the model, making $h_t$ and $y_t$ random variables. This allows us to apply information-theoretic concepts meaningfully (Haghifam, et al., 2021), as the randomness captures the uncertainty inherent in real-world systems."}, {"title": "2.1 INFORMATION-THEORETIC MEASURES IN STOCHASTIC SSMS", "content": "With the stochastic formulation, we can now define mutual information between the hidden state $h_t$ and the input sequence ${x_1, x_2, ..., x_t}$:\n$I(h_t; x_{1:t}) = H(h_t) \u2013 H(h_t | x_{1:t})$,\nwhere:\n\u2022 $H(h_t)$ is the entropy of the hidden state at time t.\n\u2022 $H(h_t | x_{1:t})$ is the conditional entropy of $h_t$ given the input sequence up to time t."}, {"title": "2.2 SELECTIVE GATING MECHANISMS", "content": "Traditional state space models apply the same state transition dynamics to all inputs, regardless of their importance. However, selective state space models (SSMs) introduce a gating mechanism to update only the relevant portions of the hidden state(Li, et al., 2020a), which enables the model to \"compress\" long sequences by filtering out irrelevant information. The selective update equation can be written as:\n$h_t = G(x_t) \\odot (Ah_{t-1} + Bx_t)$,\nwhere $G(x_t) \\in \\mathbb{R}^d$ is a gating function dependent on the current input $x_t$, and $\\odot$ denotes the element-wise product (Hadamard product). The gating function $G(x_t)$ determines which components of the hidden state are updated and which are retained from the previous time step.\nThe gating mechanism introduces an additional layer of selectivity, allowing the model to focus on key aspects of the input while ignoring less relevant features. (Mourgias-Alexandris, et al., 2020) This is particularly useful for long-range dependencies, where the model must remember certain inputs over many time steps while discarding noise or irrelevant data. The function $G(x_t)$ can be learned through gradient-based methods and is typically parameterized by a neural network layer.(Beaulieu, et al., 2020)"}, {"title": "2.3 DYNAMICS OF SELECTIVE STATE SPACE MODELS", "content": "In selective state space models, the dynamics of the system are modified by the gating mechanism, which adjusts the effective state transition matrix at each time step based on the input. Specifically, the effective transition matrix at time t becomes:\n$A_{\\text{eff}}(x_t) = G(x_t) \\odot A$.\nThus, the hidden state update equation becomes:\n$h_t = A_{\\text{eff}}(x_t) h_{t-1} + B_{\\text{eff}}(x_t) x_t$,\nwhere $B_{\\text{eff}}(x_t) = G(x_t) \\odot B$. These dynamics ensure that the state space model adapts its transition and input matrices dynamically, enabling more efficient memory compression. Only the relevant portions of the input are used to update the state, allowing the model to scale to long sequences without a significant increase in memory requirements."}, {"title": "2.4 \u039c\u0395\u039cORY RETENTION AND COMPRESSION IN SELECTIVE SSMS", "content": "One of the key advantages of selective state space models (SSMs) is their ability to retain and compress relevant information from long sequences.(Dao & Gu, 2024)) Traditional architectures like RNNs and LSTMs struggle with long-range dependencies due to gradient"}, {"title": "2.5 COMPARISON WITH TRADITIONAL RECURRENT AND CONVOLUTIONAL MODELS", "content": "To understand the advantages of selective SSMs, we compare them with traditional recurrent architectures, such as vanilla RNNs, LSTMs, and gated recurrent units (GRUs), as well as convolutional neural networks (CNNs).\nRecurrent Models: RNNS, LSTMs, and GRUs rely on hidden states that are updated at each time step based on the previous state and the current input. (Zargar, 2021) In the absence of explicit mechanisms for compressing or discarding irrelevant information, these models tend to accumulate unnecessary details, leading to inefficiencies, particularly in the context of long sequences.(Nosouhian, Nosouhian, & Khoshouei, 2021) LSTMs and GRUS introduce gating mechanisms similar to selective SSMs, but their updates are limited to controlling how much of the previous hidden state is retained.(Cahuantzi, Chen, & G\u00fcttel, 2023) These architectures struggle with effectively compressing information over extremely long sequences, and their memory usage scales with the length of the sequence.\nIn contrast, selective SSMs dynamically update the hidden state based on the relevance of the input at each time step, which allows them to effectively compress information and scale to longer sequences. By focusing only on the critical elements of the input through selective gating, SSMs achieve a more efficient memory representation without the overhead associated with storing unnecessary information across time steps.\nConvolutional Models: CNNs, which have traditionally been applied to vision tasks, (Bhatt, et al., 2021) are also commonly used for sequence modelling by employing 1D convolutions over temporal data.(Xie, et al., 2020; Gunasekaran, et al., 2021; Singla, et al., 2024) While CNNs are effective at capturing local patterns in data, their fixed kernel sizes limit their ability to capture long-range dependencies(Agrawal & Mittal, 2020a), unless large kernel sizes or multiple layers are used, which can become computationally expensive."}, {"title": "3. SELECTIVE GATING MECHANISMS IN STATE SPACE MODELS", "content": "The gating function $G(x_t, h_{t-1})$ plays a crucial role in selective stochastic state space models (SSMs) by controlling the flow of information into the hidden state $h_t$. It determines which components of the hidden state are updated based on the current input $x_t$ and the previous hidden state $h_{t-1}$.\nThe gating function $G : \\mathbb{R}^m \\times \\mathbb{R}^d \\rightarrow [0, 1]^d$ is defined component-wise as:\n$G(x_t, h_{t-1}) = [g_1(x_t, h_{t-1}), g_2(x_t, h_{t-1}),..., g_d(x_t, h_{t-1})]$,\nwhere each $g_i: \\mathbb{R}^m \\times \\mathbb{R}^d \\rightarrow [0, 1]$ is a scalar gating function for the i-th component of the hidden state."}, {"title": "3.0.1 LIPSCHITZ CONTINUITY", "content": "Each scalar gating function $g_i(x_t, h_{t-1})$ is Lipschitz continuous with respect to $h_{t-1}$(Shang, et al., 2021; Gouk, et al., 2021):\n$|g_i(x_t, h_{t-1}) \u2013 g_i(x_t, h'_{t-1})| \\le L_G||h_{t-1} \u2013 h'_{t-1}||$,\nwhere $L_G$ is the Lipschitz constant satisfying $0 < L_G < \\infty$."}, {"title": "3.0.2 DIFFERENTIABILITY", "content": "The gating functions are differentiable across the entirety of the latent space, allowing for consistent gradient-based optimization during training."}, {"title": "3.0.3 RANGE CONSTRAINTS", "content": "For all $x_t$ and $h_{t-1}$, the gating functions satisfy $0 \\le g_i(x_t, h_{t-1}) \\le 1$, ensuring that the gating weights modulate the hidden state updates appropriately.\nCommon choices for the gating functions include sigmoid and hyperbolic tangent functions, which naturally map inputs to the range [0, 1]."}, {"title": "3.0.4 SIGMOID FUNCTION", "content": "$g_i(x_t, h_{t-1}) = \\sigma(W_i^T x_t + U_i^T h_{t-1} + b_i)$,\nwhere $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, and $W_i$, $U_i$ are weight vectors, $b_i$ is a bias term."}, {"title": "3.0.5 HYPERBOLIC TANGENT FUNCTION", "content": "$g_i(x_t, h_{t-1}) = \\tanh(W_i^T x_t + U_i^T h_{t-1} + b_i)$,\nadjusted to range between 0 and 1 if necessary."}, {"title": "3.1 ROLE IN HIDDEN STATE UPDATES", "content": "The gating function modulates the hidden state update equation in the stochastic SSM:\n$h_t = G(x_t, h_{t-1}) \\odot (Ah_{t-1} + Bx_t) + (1 \u2013 G(x_t, h_{t-1})) \\odot h_{t-1} + w_t$,\nwhere:\n- $\\odot$ denotes the element-wise (Hadamard) product. - $A \\in \\mathbb{R}^{d \\times d}$ is the state transition matrix. - $B \\in \\mathbb{R}^{d \\times m}$ is the input mapping matrix. - $w_t \\sim N(0, Q)$ is the process noise."}, {"title": "3.1.1 SELECTIVE UPDATE", "content": "The gating function $G(x_t, h_{t-1})$ determines the degree to which each component of the hidden state is updated with new information, versus retaining its previous value."}, {"title": "3.1.2 \u039c\u0395MORY RETENTION", "content": "Components with gating values close to zero retain their previous state, effectively compressing memory by not storing new (possibly irrelevant) information."}, {"title": "3.1.3 INFORMATION INCORPORATION", "content": "Components with gating values close to one fully incorporate new information from the current input and the previous hidden state."}, {"title": "3.2 ENSURING CONVERGENCE AND STABILITY", "content": "The properties of the gating function are critical for ensuring the convergence and stability of the hidden state dynamics."}, {"title": "3.2.1 LIPSCHITZ CONDITION AND CONTRACTION MAPPING", "content": "The Lipschitz continuity of $G(x_t, h_{t-1})$ with respect to $h_{t-1}$ ensures that small changes in the hidden state lead to small changes in the gating values(Shang et al., 2021; Gouk et al., 2021).\nCombined with the condition on the system matrix A (i.e., $||A|| \\le \\rho$ with $\\rho L_G < 1$), this ensures that the hidden state update mapping is a contraction in expectation, as shown in Lemma 1."}, {"title": "3.2.2 ROLE IN CONVERGENCE", "content": "By satisfying the contraction conditions, the gating function contributes to the convergence of the hidden state(Kuznetsov, 2020) $h_t$ to a stationary distribution.\nThis stability is essential for the reliable performance of the SSM over long sequences, preventing the accumulation of errors or divergence of the hidden state."}, {"title": "3.3 DESIGNING EFFECTIVE GATING FUNCTIONS", "content": "The effectiveness of memory compression and retention depends on the design of the gating function."}, {"title": "3.3.1 LEARNING THE GATING FUNCTION", "content": "The parameters of $G(x_t, h_{t-1})$ (e.g., weights $W_i$, $U_i$ and biases $b_i$) are learned during training using gradient-based optimization techniques.\nThe objective is to minimize a loss function that balances prediction accuracy with memory efficiency."}, {"title": "3.3.2 REGULARIZATION", "content": "To encourage sparsity in the gating values (Zhang, et al., 2020; Verelst & Tuytelaars, 2020; Li, et al., 2020b) (i.e., more zeros), regularization techniques such as $L_1$ regularization can be applied to the gating function outputs.(De & Doostan, 2022)\nThis promotes more aggressive memory compression by reducing the number of components that are updated at each time step."}, {"title": "3.3.3 ADAPTABILITY", "content": "The gating function allows the SSM to adaptively adjust which information is retained or discarded based on the current input and state.\nThis adaptability is crucial for handling sequences with varying degrees of relevance and for focusing computational resources on the most informative parts of the input."}, {"title": "3.4 SUMMARY", "content": "The gating function $G(x_t, h_{t-1})$ is a central component of selective stochastic state space models, enabling efficient memory compression without sacrificing essential information for accurate sequence modelling. By carefully designing and training the gating function to"}, {"title": "4. \u039c\u0395\u039cORY COMPRESSION IN STATE SPACE MODELS", "content": "Memory retention in state space models refers to the capacity of the hidden state $h_t$ to preserve relevant information from previous inputs in a sequence.(Smith, Warrington, & Linderman, 2023) Given a sequence of inputs ${x_1, x_2, ..., x_T}$, the goal of the hidden state is to encode sufficient information to allow for accurate prediction of future states or outputs. In selective SSMs, this is achieved by adjusting which parts of the input sequence are stored or discarded at each time step via the gating function $G(x_t)$.(Gu, Goel, & R\u00e9, 2022)\nFormally, the hidden state $h_t$ is designed to retain a sufficient amount of mutual information about the sequence:\n$I(h_t; {x_1, x_2, ..., x_T})$,\nwhere $I(\\cdot)$ denotes mutual information. In this paper, we leverage the rate-distortion function $R(D)$ to characterize the trade-off between memory compression and retained information. Here, R corresponds to the hidden state dimensionality, and D represents the information loss. The challenge is to balance the reduction in the hidden state's dimensionality while minimizing the distortion D.\nSelective SSMs address this by selectively updating the hidden state using the gating function $G(x_t)$, which dynamically controls how much of the input influences the hidden state."}, {"title": "4.1 RATE-DISTORTION THEORY FOR MEMORY COMPRESSION", "content": "Rate-distortion theory provides a framework for quantifying the trade-off between the amount of information retained (rate) and the allowable distortion in representing that information. (Jakob & Gershman, 2023) For our stochastic SSM, the rate-distortion function $R(D)$ is defined as:\n$R(D) = \\min_{p(h_t | \\hat{h_t})} I(h_t; \\hat{h_t}) \\text{ subject to } E[d(h_t, \\hat{h_t})] \\le D,$\nwhere:\n\u2022 $\\hat{h_t}$ is the compressed representation of the hidden state.\n\u2022 $I(h_t; \\hat{h_t})$ is the mutual information between the hidden state and its compressed version.\n\u2022 $d(h_t, \\hat{h_t})$ is a distortion measure (e.g., mean squared error).\n\u2022 $D$ is the maximum allowable distortion.\nBy applying rate-distortion theory, we can determine the minimal rate $R(D)$ required to achieve a certain level of distortion D, guiding the design of the gating mechanism to optimize memory compression while retaining essential information. (Lei, Hassani, & Bidokhti, 2022; Zhe, et al., 2021)"}, {"title": "4.2 \u039c\u0391\u03a4HEMATICAL FORMULATION OF COMPRESSION", "content": "Memory compression in selective SSMs can be viewed as the process of reducing the dimensionality of the hidden state while preserving the essential information required for the model's task. To formalize this, let $dim(h_t)$ represent the number of dimensions in the hidden state at time t. The objective of compression is to minimize $dim(h_t)$ while maintaining a high level of mutual information $I(h_t; {x_1, ..., x_T})$.\nThis can be formulated as the following optimization problem:\n$\\min_{dim(h_t)} dim(h_t) \\text{ subject to } I(h_t; {x_1, ..., x_T}) \\ge \\tau$,\nwhere $\\tau$ is a threshold representing the minimum required information to perform the task effectively. The gating mechanism $G(x_t)$ plays a crucial role in controlling the compression by determining which portions of the input sequence are retained in $h_t$.\nAdditionally, from an information-theoretic perspective, this compression problem can also be approached using rate-distortion theory. (Zhang, Liu, & Tao, 2023; Jeon & Van Roy, 2022; Tan, Liu, & Liu, 2020) The rate-distortion function $R(D)$ provides the lower bound on the dimensionality of the hidden state (rate R) required to achieve a specified level of information distortion D, where D measures the loss in retained information.\nFormally:\n$R(D) = \\min I(h_t; {x_1, ..., x_T}) \\text{ subject to } D(h_t) \\le D$.\nThis highlights the inherent trade-off between retaining critical information and minimizing the dimensionality of the hidden state. The goal of selective SSMs is to find an optimal balance by learning a gating function that discards irrelevant information while retaining the essential parts of the sequence."}, {"title": "4.3 BALANCING MEMORY COMPRESSION AND TASK PERFORMANCE", "content": "An important aspect of memory compression in selective SSMs is finding the optimal balance between the size of the hidden state and the performance of the model on the given task. Memory compression often introduces a trade-off between compactness and accuracy; a smaller hidden state may lead to a loss of relevant information, reducing the model's ability to make accurate predictions or classifications. (Menghani, 2023a)\nTo formally express this trade-off, we consider the task performance $P(h_t)$, which depends on the information retained in the hidden state $h_t$. In general, task performance improves as more information about the input sequence is stored in $h_t$, but this comes at the cost of increasing the dimensionality $dim(h_t)$ and, consequently, the memory usage.\nThe relationship between task performance and memory compression can be described as an optimization problem. Let $J(h_t)$ be the objective function representing the task performance, which is a function of the hidden state $h_t$ and the sequence ${x_1, ..., x_T}$. The goal is to maximize the task performance while minimizing the dimensionality of the hidden state:\n$\\max_{h_t} J(h_t) \\text{ subject to } dim(h_t) \\le d_{\\text{max}}$,"}, {"title": "4.4 FANO'S INEQUALITY AND INFORMATION BOUNDS", "content": "Fano's inequality provides a lower bound on the probability of error $P_e$ in estimating the input sequence $x_{1:t}$ from the compressed hidden state $h_t$ (Morishita, et al., 2022):\n$P_e > \\frac{H(x_{1:t} / h_t) - 1}{\\log |X|}$,\nwhere $|X|$ is the cardinality of the input alphabet. Since $H(x_{1:t} / h_t) = H(x_{1:t}) - I(x_{1:t}; h_t)$, we can relate the probability of error to the mutual information:\n$P_e > \\frac{H(x_{1:t}) - I(x_{1:t}; h_t) \u2013 1}{\\log |X|}$\nThis inequality underscores the necessity of retaining sufficient mutual information $I(x_{1:t}; h_t)$ to achieve a low probability of error in sequence prediction, guiding the design of the gating mechanism. (Sakai, 2020)"}, {"title": "4.5 \u039c\u0391\u03a4HEMATICAL ANALYSIS OF GATING MECHANISMS", "content": "The selective gating function $G(x_t)$ is critical in controlling the amount of information stored in the hidden state. Mathematically, the function can be modelled as a vector of dimension d, where each component $G_i(x_t) \\in [0,1]$ controls the degree to which the i-th component of the hidden state is updated at time t. A value of $G_i(x_t) = 1$ indicates that the corresponding component of the hidden state is fully updated, while $G_i(x_t) = 0$ means that the previous value of the hidden state is retained.\nThe effective dimensionality of the hidden state at time t can therefore be described as:\n$dim_{\\text{eff}}(h_t) = \\sum_{i=1}^d G_i(x_t)$,\nwhere d is the full dimension of the hidden state. This effective dimensionality provides a measure of how many components of the hidden state are being actively updated based on the input at time t."}, {"title": "5. COMPUTATIONAL COMPLEXITY OF SELECTIVE SSMS", "content": "The introduction of selective gating mechanisms in SSMs reduces the computational complexity of state updates, as only a subset of the hidden state components are updated at each time step. In traditional state space models or recurrent neural networks, the complexity is $O(d^2)$, where d is the full dimensionality of the hidden state. However, in selective SSMs, the complexity scales with the effective dimensionality $dim_{\\text{eff}}(h_t)$, defined as:\n$dim_{\\text{eff}}(h_t) = \\sum_{i=1}^d G_i(x_t)$.\nThus, the computational complexity of updating the hidden state at each time step becomes:\n$O(dim_{\\text{eff}}(h_t)^2)$.\nThis reduction in complexity allows selective SSMs to efficiently process long sequences, especially in scenarios where not all inputs contribute equally to the prediction task. By selectively updating the hidden state, the computational burden is reduced without sacrificing task performance."}, {"title": "5.1 INFORMATION-THEORETIC PERSPECTIVE ON COMPRESSION", "content": "The memory compression process in selective SSMs can be rigorously analysed through the lens of information theory. Specifically, we focus on the mutual information $I(h_t; {x_1, ..., x_T})$, which measures the amount of sequence information retained in the hidden state. (Di, et al., 2020; Wongso, Ghosh, & Motani, 2022; Kleinegesse & Gutmann, 2020) The goal is to maximize $I(h_t; {x_1, ..., x_T})$, while minimizing the entropy $H(h_t)$, which is directly related to the dimensionality $dim(h_t)$ of the hidden state.\nFormally, the mutual information is given by:\n$I(h_t; {x_1, ..., x_T}) = H(h_t) - H(h_t | {x_1, ..., x_T})$,\nwhere $H(h_t)$ is the entropy of the hidden state and $H(h_t | {x_1, ..., x_T})$ is the conditional entropy given the input sequence. The selective gating function $G(x_t)$ serves to minimize the conditional entropy by ensuring that only relevant information from the input sequence is stored in the hidden state.\nWe seek to achieve a high mutual information while maintaining a low hidden state entropy, thus minimizing the dimensionality of the hidden state. This trade-off between information retention and memory compression is fundamental to the design of selective SSMs, and it can be formalized using the rate-distortion function. The goal is to optimize the gating function $G(x_t)$ such that the rate (i.e., hidden state size) is minimized while ensuring that the distortion D (i.e., information loss) remains within acceptable bounds.(Barbiero, et al., 2022)"}, {"title": "5.2 SELECTIVE SSMS IN REAL-WORLD APPLICATIONS", "content": "Selective SSMs are particularly well-suited for applications where efficient memory compression is crucial. Some notable examples include:\n\u2022 Speech Recognition: In speech recognition tasks, the model must process long sequences of audio data to recognize spoken words or phrases. Efficient memory compression allows the model to focus on relevant features of the audio signal while discarding irrelevant noise, resulting in better real-time performance with reduced computational overhead.\n\u2022 Time-Series Forecasting: Many time-series forecasting tasks involve long-term dependencies, such as predicting stock prices, weather patterns, or energy consumption. Selective SSMs can compress the historical data into a compact hidden state, enabling accurate predictions while managing the memory requirements of processing large datasets over time.\n\u2022 Natural Language Processing (NLP): In NLP tasks such as machine translation, question answering, and language modelling, selective SSMs can efficiently retain important contextual information over long sequences of text. This enables the model to handle long-range dependencies without excessive memory usage."}, {"title": "5.3 INFORMATION BOTTLENECK FOR STATE SPACE MODELS", "content": "To formalize the trade-off between compression and information retention, we adopt the information bottleneck framework. Let z be the compressed representation of the hidden state $h_t$. The information bottleneck approach seeks to compress the hidden state by minimizing the mutual information $I(z; h_t)$, while retaining sufficient information to predict the sequence ${x_1, ..., x_T}$, represented by maximizing $I(z; {x_1, ..., x_T})$.\nThis can be expressed as the following optimization problem:\n$\\min I(z; h_t) \\text{ subject to } I(z; {x_1, ..., x_T}) \\ge \\tau$,\nwhere $\\tau$ is a threshold that ensures the compressed representation z retains the necessary sequence information. The variable z can be seen as a compressed form of $h_t$, which filters out irrelevant details while preserving critical information about the input sequence.\nThe solution to this optimization problem yields a compressed hidden state representation that balances memory efficiency and sequence prediction performance. By minimizing $I(z; h_t)$, we reduce the amount of redundant information stored in the hidden state, leading to memory compression."}, {"title": "5.4 SELECTIVE GATING AND INFORMATION FLOW", "content": "The selective gating mechanism in SSMs plays a critical role in controlling the flow of information through the hidden state. The gating function $G(x_t)$, which operates on the hidden state update equation, determines which components of the hidden state are updated and which are retained. This introduces a dynamic filtering of information at each time step, where only the most relevant information is allowed to influence the hidden state evolution.\nThe effective flow of information through the hidden state can be formalized as:\n$h_t = G(x_t) (Ah_{t-1} + Bx_t)$,\nwhere $\\odot$ represents the element-wise product, and $G(x_t)$ selectively updates components of the hidden state based on the input $x_t$.\nThe key trade-off in information flow lies in determining how much information from the input sequence should influence the hidden state update. Excessive gating (i.e., setting many components of $G(x_t)$ close to zero) can lead to insufficient information retention, while minimal gating (i.e., setting most components of $G(x_t)$ close to one) can prevent meaningful compression.\nBy analysing the information flow using the mutual information between $h_t$ and ${x_1, ..., x_T}$, we can optimize the gating mechanism to achieve a balance between compression and retention. The goal is to dynamically adapt $G(x_t)$ such that the hidden state retains critical information without redundantly storing unnecessary details."}, {"title": "5.5 SUMMARY OF TRADE-OFFS AND EFFICIENCY", "content": "In summary, selective state space models introduce a powerful framework for memory compression in sequence modelling tasks. By selectively updating the hidden state based on the relevance of the input, these models are able to retain essential information while discarding irrelevant details, resulting in a compressed representation of the sequence.\nThe trade-offs between memory compression, computational complexity, and task performance are central to the design of selective SSMs. The gating mechanism plays a key role in balancing these trade-offs, allowing the model to adapt its hidden state representation based on the needs of the task. Selective SSMs are particularly effective in applications where long sequences must be processed efficiently, offering a flexible and scalable solution to the problem of memory retention in sequence models.\nIn the following sections, we will provide a more rigorous information-theoretic analysis of memory compression, followed by theoretical results and proofs of key theorems related to memory retention and efficiency in selective state space models."}, {"title": "6. Mathematical Results", "content": ""}, {"title": "6.1 MAIN THEOREMS", "content": "In this section, we present the primary theorems that formalize the memory compression capabilities of selective"}]}