{"title": "100 instances is all you need: predicting the success of a new LLM on unseen data by testing on a few instances", "authors": ["Lorenzo Pacchiardi", "Lucy Cheke", "Jos\u00e9 Hern\u00e1ndez-Orallo"], "abstract": "Predicting the performance of LLMs on individual task instances is essential to ensure their reliability in high-stakes applications. To do so, a possibility is to evaluate the considered LLM on a set of task instances and train an assessor to predict its performance based on features of the instances. However, this approach requires evaluating each new LLM on a sufficiently large set of task instances to train an assessor specific to it. In this work, we leverage the evaluation results of previously tested LLMs to reduce the number of evaluations required to predict the performance of a new LLM. In practice, we propose to test the new LLM on a small set of reference instances and train a generic assessor which predicts the performance of the LLM on an instance based on the performance of the former on the reference set and features of the instance of interest. We conduct empirical studies on HELM-Lite and KindsOfReasoning, a collection of existing reasoning datasets that we introduce, where we evaluate all instruction-fine-tuned OpenAI models until gpt4-0125-preview. When predicting performance on instances with the same distribution as those used to train the generic assessor, we find this achieves performance comparable to the LLM-specific assessors trained on the full set of instances. Additionally, we find that randomly selecting the reference instances performs as well as some advanced selection methods we tested. For out of distribution, however, no clear winner emerges and the overall performance is worse, suggesting that the inherent predictability of LLMs is low.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are being used as components of multiple services and products, such as agents performing general computer tasks [16], performing ML experiments [13], and even operating unmanned aerial vehicles [14]. These systems typically query an LLM on a specific instance of a task and use their output to determine a sequence of actions. For some of these uses, it is essential to determine whether the output produced by the LLM on a specific task instance is correct (or, more generally, \"valid\" [43]) before the subsequent steps are executed\u00b9. A nascent line of research [9, 42] is addressing this problem by developing \"assessors\", namely, independent modules that predict the correctness (or a continuous performance score) of an AI system on an instance based on features intrinsic to the latter (such as linguistic features or sentence vector embeddings). Assessors can be specific to an AI system, or \"generic\", in which case they also take as input information on the AI system at hand and are trained to predict the performance of different LLMs on different instances.\nMeanwhile, the rate at which new LLMs are released has drastically increased. Some providers, such as OpenAI, are iteratively retiring old versions when new ones are released, forcing developers to update the LLM version used in their applications (see [30]). An even larger explosion is occurring in the open-source world, fuelled by inexpensive fine-tuning techniques [10]. To build an assessor specific to a new LLM version, users must evaluate it on a sufficiently large set of task instances, causing the costs to rise quickly when considering many LLM versions. On the other hand, the system information one might use to build a generic assessor, such as the number of parameters or statistics of the training data or architecture, is not standardised across LLMs and unavailable for proprietary models.\nAs such, this paper investigates the following question: can we combine information across LLMs to predict the perfor- mance of a new LLM on a new instance by relying only on observational (or behavioural) features of the LLMs? In prac- tice, we propose to characterise each LLM by its performance on a small set of reference instances and to build a generic assessor using those as system features. More precisely, we first select a small set of reference instances from the labelled dataset on which past LLMs were evaluated. Then, we train the generic assessor on the concatenation of instance-specific features and the LLM-specific success vector on the reference instances. Finally, to estimate the probability of success of a new LLM on a novel instance, it suffices to evaluate the former on the reference instances, concatenate its performance to the features of the instance, and apply the trained generic assessor.\nIn our empirical studies, we rely on HELM-Lite [20], which provides instance-level results for 30 LLMs from different providers (at the time we conducted our experiments), and a collection of previously existing datasets we introduce, named \u201cKindsOfReason- ing\", on which we evaluated the full set of instruction-following models from OpenAI until gpt4-0125-preview. We only consider tasks with binary correctness score (thus discarding the datasets in HELM-Lite that do not satisfy this) and therefore build binary assessors.\nWe train specific assessors using different prompt features and find that OpenAI embeddings [31] lead to better in-distribution performance than simpler methods such as Word2vec [25], FastText [4], and n-grams. Although this analysis is not the main focus of our work, it is a valuable tangential contribution. Subsequently, we build generic assessors using various methods to select the"}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Instance-level prediction of success of AI systems", "content": "The motivation for our work follows [43], which advocates for the importance of instance-level success predictions for AI systems and coins the term \"predictable AI\"; in particular, they highlight how ensuring predictability should be prioritised over increases in average performance for risky use cases, and how having this could help with compliance with upcoming regulatory frameworks, such as the EU AI Act [2].\nFollowing this motivation, [9] introduces the concept of an assessor model, which accompanies an ML system and estimates the probability of success of the system on individual instances. In particular, they discuss how an assessor can be trained on the evaluation re- sults of the ML system on test data (i.e., which has not been used for training the ML system). Finally, [42] applies this idea to LLMs, by showing how a smaller external model can be used to predict the performance of a bigger and more expensive LLM on individual in- stances without passing the latter through the model. They also find it possible to reject almost half of the failure cases before running much larger LLMs, resulting in a significant saving of compute."}, {"title": "2.2 Predictability of aggregated benchmark scores from LLM-specific features", "content": "Two works [32, 40] studied the extent to which an LLM's aggregate performance on BIG-Bench tasks [36] can be predicted using infor- mation on the LLM such as number of parameters or the amount of used compute. In contrast, our work does not rely on these quan- tities, which are often unavailable, instead characterising LLMs according to their performance on reference samples. Moreover, while these works focus on predicting aggregate performance, our work and the ones mentioned in the previous subsection provide instance-level predictions for new unlabelled instances."}, {"title": "2.3 Extracting LLM-specific features from existing evaluations", "content": "Recently, [34] built \"observational scaling laws\" that link perfor- mance on complex downstream tasks to hypothesised latent capabil- ities, whose values can be inferred by decomposing the performance of various LLMs on different benchmarks into components linked by a log-linear relation with compute measures for LLM training. Doing so allows us to combine information across different LLM families, which may differ for their efficiency in converting raw com- pute into benchmark scores. Once this relation is established, the performance of a new model on downstream tasks can be predicted by knowing its performance on simple benchmarks and its compute cost. [34] also select a subset of LLMs that maintains high prediction accuracy while reducing cost by requiring the evaluation of perfor- mance on downstream tasks for fewer models. Their work is similar to ours in determining LLM-specific features by using evaluation results of multiple LLMs and using them to predict the performance of a new LLM. However, the aim of our work is to predict the perfor- mance of the new LLM on a specific instance with as few evaluations as possible, while the aim of [34] is to avoid the cost of evaluating complex downstream tasks and predict the performance on the lat- ter from results on simple benchmarks and compute measures. As such, the LLM-specific features they use (the latent capabilities) are obtained from the performance of the new LLM on simple bench- marks (which [34] assumes to be available), while our method only needs to evaluate the LLM on a small number of instances. Moreover, our method can be applied to new instances for which no ground truth is available, while the simple benchmarks and the downstream tasks employed in [34] must have a grading mechanism."}, {"title": "2.4 Predicting performance by benchmark subsampling", "content": "Several works share the motivation of reducing the number of eval- uations (and hence the cost) needed to evaluate a LLM. For instance, a \"Lite\" version with a reduced number of tasks was introduced alongside the larger BIG-Bench benchmark [36]; however, the way in which the task selection was done is unknown, to the best of our knowledge. Similarly, HELM-Lite [20] is a revised and reduced version of HELM [19]. However, both of these perform the reduc- tion at the level of tasks of which the benchmark is constituted. Instead, [38] subsample a dataset by clustering models' confidence to predict the overall accuracy on the whole dataset, while MixEval [27] extracts a subset of instances from various benchmarks which is most predictive of the performance on Chatbot Arena\u00b2, an online platform performing pairwise comparison of LLM outputs. Closer to our work is TinyBenchmarks [33], which selects informative instances from HELM-Lite and estimates the performance of a new LLM on the whole benchmark by evaluating it only on those in- stances. In particular, TinyBenchmarks uses Item Response Theory (IRT) on the matrix of success of each LLM on each instance present in the HELM-Lite dataset to extract a vector of item demands and LLM capabilities. Then, it uses either the item demands or the raw LLM success on each instance to build a representative subset of instances by clustering the items and taking the cluster centroids. Similarly to our approach, a new LLM is then only evaluated on the representative subset; however, in contrast to our work, they aim to predict the aggregate score on the benchmark, while we focus on predicting instance-level performance. In practice, their IRT method provides instance-level predictions (which the authors aggregate), but these predictions are limited to instances on which previous LLMs have been evaluated (as this is necessary to obtain the item demands), which requires access to the ground truth. In contrast, our approach is applicable to new instances for which we do not know the ground truth, as the trained assessor does not require any information beyond the intrinsic features of test in- stances. A similar work to [33] is metabench [17], which considered 6 different datasets, and performed a two-step procedure (random sampling for each dataset, followed by item selection based on the Fisher information matrices of IRT item parameters) to extract a small set of instances, the performance on which accurately predicts aggregate performance on the 6 datasets. As they fit the IRT model only the pre-selected instances, their method is unable to predict instance-level performance. Finally, despite not tackling predictabil- ity directly, [35] finds that the vector of successes of different LLMs is correlated across instances belonging to 4 benchmarks, and, for one of those benchmarks, the similarity between the embeddings or a pair of instances predicts the similarity between the success vectors; this suggests that patterns in success across LLMs can be found and related to the embeddings."}, {"title": "2.5 Evaluations of reasoning in LLMs", "content": "[5] found reasoning to be one of three factors in the capabilities of LLMs. Indeed, reasoning in LLMs has been extensively studied: see [26] for a survey on LLM reasoning evaluations and [11] for a broader survey also encompassing ways to improve and elicit reasoning in LLMs.\nRecently, several collections of reasoning datasets have been introduced. GLORE [37] collects 12 logical reasoning datasets with three different types of tasks (multiple choice, natural language inference, and binary answers). Similarly, LogiGLUE [24] collects 24 datasets related to inductive, deductive and abductive reasoning, with four different types of tasks (the same ones as GLoRe and free-form question answering); they only selected datasets that do not require external domain knowledge, but some of these datasets are poorly formatted. Finally, CALM-Bench [7] is a collection of 6 diverse tasks requiring both causal reasoning and knowledge. KindsOfReasoning, the collection we introduce combining previ- ously existing datasets testing various kinds of reasoning, partly"}, {"title": "3 Methodology", "content": "Let us denote by L = {mj, j = 1, ..., n}, a set of trained LLMs. Moreover, let D = {(pi, yi), i = 1, ..., N} be a test dataset used to evaluate the performance of the LLMs, with i denoting instance index, pi the input to the LLM (i.e., the prompt) and yi the target value (i.e., the expected completion by the LLM). Further, we will denote by mj (pi) the output mj produces when given pi as input\u00b3 and by zj,i a binary value indicating the \u201ccorrectness\u201d of mj(pi) with respect to yi. The correctness zj,i can be defined in multiple manners (for instance, exact match or whether yi is a substring of mj (pi)); the most suitable manner depends on the considered task, but in general the aim of zj,i is to capture what a human judge would perceive as a correct answer4.\nIn the following, we first frame the problem of predicting the correctness zj,i and then discuss our main contribution, namely a framework to predict the performance of a new LLM by evaluating it on a small subset of instances."}, {"title": "3.1 Predicting success of a LLM using features intrinsic to the prompt", "content": "To begin with, let us now consider a single LLM, say m\u2081; our aim is to train a classifier (termed \u201cassessor\u201d) to predict the performance Z1,i from the prompt pi. To do so, we split the test dataset D into different splits used to train, validate and evaluate the assessor [9], denoted as Dtrain, Dval and Dtest, such that D = Dtrain \u222a Dval U Dtest and Dtrain \u2229 Dval = Dval \u2229 Dtest = Dtrain Dtest = \u00d8. In a real-world scenario, Dtest will represent instances for which we did not evaluate the considered LLM (and for which we may not have access to the ground truth); in contrast, available evaluation results are split into Dtrain and Dval.\nIn practice, we can extract some numerical features f (pi) from the textual prompt pi; we use \"intrinsic\" features, i.e. features that are defined independently of the problem at hand (such as the number of negations or the vector embeddings of the sentence). Formally, we consider a loss function l and a family of classifiers hp, where y denotes the parameters of the classifier (for instance, the weights in a logistic regression classifier), and aim to minimise\n$\\Sigma l(h_p(f(p_i)), z_{1,i})$\n$P_i \\in D_{train}$"}, {"title": "3.2 Predicting success by evaluation on reference instances", "content": "Now, consider the case in which we have previously evaluated some LLMs on Dtrain and Dval. We are interested in predicting the performance of a new LLM, say mnew on new instances Dtest. Using the approach in Section 3.1, we could test the new LLM on all instances in Dtrain and Dval and train an assessor specific to that LLM. Instead, we want to leverage the information contained in the available evaluation results for previous LLMs to predict the performance of mnew on Dtest without evaluating it on the full Dtrain (and assuming that we do not have access to the labels in Dtest, which prevents us from evaluating the other LLMs on it).\nThus, we build a generic assessor, namely a classifier that predicts the success zj,i from the pair (mj, pi). In practice, let us split the LLMs for which full evaluation results are available into a train- ing and validation split Ltrain and Lval. For each pair (mj, pi) \u2208 Ltrain \u00d7 Dtrain, we concatenate the prompt-intrinsic features f (pi) with LLM-specific features g(mj) and aim to fit a classifier hp that minimises\n$\\Sigma \\Sigma l(h_p(g(m_j), f(p_i)), z_{j,i})$\n$m_j \\in L_{train} p_i \\in D_{train}$"}, {"title": "3.2.1 Selecting the reference instances", "content": "In order to select the most informative instances (pi, yi) \u2208 Dtrain to form Dref, we can use information intrinsic to the instances as well as the evaluation results of Ltrain on Dtrain (while keeping aside Dval and Lval to choose the best selection method; see Section 3.2.3). In general, let us denote by xi \u2208 Rd a feature vector associated to pi and X \u2208 Rdx|Dtrain| the matrix whose columns are xi. Finally, let us define Ztrain = (zj,i) j: mj\u2208 Ltrain,i: pi\u2208 Dtrain. We attempt using the following features:\n\u2022 features intrinsic to the prompt xi = f(pi), which are not necessarily the same used to build the assessor in Sections 3.1"}, {"title": "3.2.2 Predicting success by concatenating intrinsic features and performance on the reference instances", "content": "Once we select the reference instances Dref, we can extract the success of each LLM on Dref to define g(mj) = (zj,i)i\u2208 Dref. We can then concatenate this to the feature vector f(pi) (which does not need to be the same used for selecting the reference instances in Section 3.2.1) and train a generic assessor aiming to minimise Eq. (2). Notice how the features f(pi) can also rely on the reference dataset, as that is fixed for all new LLMs: for instance, we also attempt using a measure of similarity between the vector embeddings of pi and each of the instances in Dref as f(pi)."}, {"title": "3.2.3 Choosing the best setup on validation data and predicting the performance of a new LLM", "content": "As mentioned above, we have multiple ways to define the reference set as well as multiple choices for"}, {"title": "4 Empirical studies", "content": ""}, {"title": "4.1 Considered datasets and splits", "content": "We consider two collections of datasets in our experiments7:\n\u2022 HELM-Lite [20], a revised and reduced version of the popu- lar HELM [19], which includes 10 different \"scenarios\" (i.e., datasets), some of which are stratified into sub-scenarios. Of those, we keep the scenarios and subscenarios for which the performance metric is binary, and further discard those for which different LLMs were tested with a different number of few-shot examples; the resulting subset spans 6 scenar- ios for a total of 4285 instances.\n\u2022 KindsOfReasoning, a collection that we introduce in this paper, which is aimed at testing various kinds of reason- ing (logical, common sense, inductive, deductive, abductive, counterfactual, causal, analogical, spatial and arithmetic rea- soning). Our collection includes 22 different datasets with varying number of instances, for a total of 37,529 instances.\nFor each of these collections, we repeat all our experiments con- sidering different choices for the train, validation, and test splits Dtrain, Dval and Dtest. In particular, we consider a random split, where the various splits are sampled by shuffling together all in- stances of all datasets. In addition, we consider multiple out-of- distribution (OOD) splits, where we keep one set of datasets as Dtest (according to some criteria), and Dtrain and Dval are ob- tained from randomly shuffling the other ones."}, {"title": "4.2 Considered prompt features", "content": "Our methodology, discussed in Section 3, relies on choosing a trans- formation f that converts a given prompt pi into a set of numerical features xi = f(pi), where we refer to these features as \"intrinsic\" as they do not depend on the particular LLM whose performance we are interested in predicting (as mentioned in Section 3.2.2, in the generic assessor setup, we allow the intrinsic features to depend on the set of reference instances, as the set of reference instances is fixed for all LLMs in Ltest).\nEmpirically, we attempted using the following features:\n\u2022 the prompt embeddings computed from the OpenAI API (with the endpoint text-embedding-3-large, [31]);\n\u2022 the Word2vec [25] and FastText [4] word embeddings, which compute a vector for each word of the prompt which we average to obtain a feature vector for the whole prompt;\n\u2022 the 1-gram vectors, which are obtained as a measure of the frequency of words in a specific prompt normalized over that of the words in the entire set of training prompts."}, {"title": "4.3 Metrics and other details", "content": "We use the Area Under the Curve (AUC) as a metric for the perfor- mance of the generic and specific assessor. The AUC measures how well a binary probabilistic classifier (i.e., a classifier that provides a probabilistic prediction for a binary variable) discriminates between the two classes: a classifier whose assigned probabilities for the two classes do not overlap achieves the maximum value AUC = 1, while a classifier assigning random values to the two classes achieves AUC = 0.5. We employ the AUC as its extreme values are insensitive to the proportion of positive and negative samples in the dataset, and it can therefore be used to compare results across various sce- narios (in our case, the various train/validation/test splits and the two dataset collections). However, AUC is insensitive to monotonic transformation of the output probabilities and this implies that a classifier achieving AUC = 1 can be miscalibrated.\nWe test various values of the size of Dref and we find that the performance on the test set saturates around 100 reference instances; as such, all results reported in the main text are obtained with that value.\nNext, for any data split and any choice of Dref in the generic assessor setup, we attempt to use various classifiers as assessors (logistic regression with 12 and 11 penalty and xgboost). Furthermore, as mentioned in Section 3.2.2, in the generic assessor setup, we attempt to use the OpenAI embeddings as well as their cosine similarity to those of the elements of Dref as instance features. To select the best method, we do the following:\n\u2022 in the specific assessor setup, we compute the AUC of each classifier trained on each test LLM on Dval, pick the one with the highest value, and report the AUC of that classifier on Dtest\n\u2022 In the generic assessor setup, for each data split, we eval- uate the AUC of each combination of classifier, choice of Dref and instance features f on Dval for each LLM in Lval To select the best combination, we compute the win rate of each combination for each validation LLM and pick the combination with the highest average win rate over Lval."}, {"title": "4.4 How well can we predict success?", "content": "Figure 2 includes our main result, namely the predictive perfor- mance of the generic and specific assessor for the test LLMs Ltest, alongside three baselines:\n\u2022 \"Random selector\" corresponds to a generic assessor where Dref is a random set of instances selected from Dtrain, in- stead of using the selection methods discussed in Section 3.2.1; the best classifier and intrinsic features for each split are chosen using validation data and LLMs as for the generic assessor (Section 4.3).\n\u2022 \"Reference only\" is obtained by fitting, for each Ltest, an assessor on the performance on the elements of Dref, by only taking as input the intrinsic features of the prompts in Dref (thus, ignoring the performance of the previous LLMs to predict the performance of the new one). Notice how the best classifier and Dref for \u201creference only\u201d are selected independently of those for the generic assessor by using the validation data and LLMs as discussed in Section 4.3.\n\u2022 \"All train data\" is obtained by fitting a single assessor on the pooled performance results of all LLMs in Ltrain on Dtrain only using the intrinsic features f(pi) (effectively considering all LLMs as a single LLM and ignoring the new LLM's performance on Dref)\nFrom the results in Figure 2, several considerations can be made. First, notice how the predictive performance generally degrades out of distribution with respect to the in-distribution (random) split. For some out-of-distribution splits, some predictive power remains (recall that AUC = 0.5 corresponds to random guess) but, on other splits, even the specific assessor performs poorly, despite relying on evaluation results of the test LLMs on the whole train and validation data splits."}, {"title": "5 Conclusion", "content": "We proposed a novel framework for predicting the performance of a new Large Language Model (LLM) on individual task instances by leveraging the evaluation results of previously tested LLMs. Our approach minimises the number of evaluations required for a new LLM by introducing a generic assessor that combines instance- specific features with LLM-specific features derived from perfor- mance on a small set of reference instances. In doing so, our method is cheaper than specific assessors [9, 42] that solely rely on the per- formance of the considered LLM on a larger set of labelled examples. While we focus on LLMs, our methodology can be seamlessly ap- plied to predict the performance of other Al systems, by using suitable system-specific and instance-specific features.\nWe conducted empirical studies on the HELM-Lite and Kind- sOfReasoning collections. In distribution, we found our generic assessor to perform only slightly worse than the specific assessor, indicating that the generic assessor is a viable method to reduce the evaluation cost of new LLMs when interested in predicting instance- level performance. In distribution, moreover, the generic assessor almost always outperforms the baseline relying only on information on previous LLMs or on the results of the test LLM on Dref. Further, the generic assessor is mostly unsensitive to the specific set of refer- ence instances used. Out of distribution, instead, the picture is more varied: no clear winner emerges, and instance-level predictability is generally low. As such, our work raises awareness of the low inner predictability of LLMs, and we hope it encourages the research community to focus more on characteris- ing what affects the predictability of LLMs and hence finding ways to increase it, which will help to make AI systems more reliable [43]."}, {"title": "A Appendix / supplemental material", "content": ""}, {"title": "A.1 More information on the considered and excluded scenarios from HELM-Lite", "content": "As mentioned in the main text, we discard some scenarios and subscenarios from HELM-Lite as either the performance metric was non-binary or because the available results used a different number of few-shot prompts for different LLMs. In particular, we discard the following:\n\u2022 LegalBench:\n corporate lobbying - incoherent number of few-shots across\n LLMs\n\u2022 MATH:\n algebra - incoherent number of few-shots across LLMs\n geometry - incoherent number of few-shots across LLMs\n intermediate algebra - incoherent number of few-shots\n across LLMs\n\u2022 NarrativeQA: non-binary metric (f1 score)\n\u2022 NaturalQuestions: non-binary metric (f1 score)\n\u2022 WMT 2014: non-binary metric (BLEU score)\nAs such, the subset of HELM-Lite that we consider through- out our experiments is made up of the following scenarios and subscenarios:\n\u2022 commonsense\n\u2022 GSM8K\n\u2022 MedQA\n\u2022 LegalBench:\n abercrombie\n function of decision section\n proa\n international citizenship questions\n\u2022 MATH:\n counting and probability\n number theory\n prealgebra\n precalculus\n\u2022 MMLU:\n abstract algebra\n college chemistry\n computer security\n econometrics\n US foreign policy"}, {"title": "A.2 The KindsOfReasoning collection", "content": "Table 4 shows detailed information on the datasets included in the KindsOfReasoning collection. For some datasets, we only kept a smaller number of instances than the one available, to reduce the cost of evaluating a model on the full benchmark. We do not do this for the \"Arithmetic\" dataset as each of the prompt of that dataset is short, and hence the cost of evaluating it is small (besides, we use Arithmetic as the test data for one of our chosen splits, and subsampling it would have made the test data too small).\nMost of the datasets included in this collection are present in one (or more) of BIG-Bench [36], LogiGLUE [24], CALM-bench [7] and GLORE [37]."}, {"title": "A.3 Additional results with other features in the specific assessor setup", "content": "Figures 3 and 4 show performance of the specific assessor setup us- ing different features intrinsic to the prompt, for different data splits of the KindsOfReasoning and HELM-Lite collections respectively.\nIn particular, for each figure, the top panel shows performance on Dval, while the latter shows performance on Dtest, for the clas- sifier selected according to its best performance on Dval. On the validation data, the performance of the OpenAI embeddings is gen- erally higher and, as such, the experiments reported in the main text are with this choice of embeddings. However, the performance on Dtest for the OOD splits show a mixed picture, with the OpenAI embeddings often performing worse than simpler ones (such as Word2Vec) and with generally lower performance."}, {"title": "A.4 How many OpenAI embeddings are needed?", "content": "Figures 5 and 6 show performance of the specific assessor using the OpenAI embeddings truncated at different vector sizes, for different data splits of the KindsOfReasoning and HELM-Lite collections respectively. In particular, for each figure, the top panel shows performance on Dval, while the latter shows performance on Dtest, for the classifier selected according to its best performance on Dval The performance on Dval (and Dtest for the in-distribution split) plateaus when the truncation size reaches 1024 and, as such, all the results reported in the main text are with that truncation size. On Dtest for the various OOD splits, the performance does not follow a smooth curve, but still seems to peak more often around a truncation size of 1024."}, {"title": "A.5 Control for number of training samples in the KindsOfReasoning collection", "content": "Figure 7 shows the difference between the AUC of a specific assessor trained on the full Dtrain and one trained on a random subsample of Dtrain of size 3000, for different choices of the random split for the KindsOfReasoning collection. The difference is small on Dval (notice the y scale of the graphs) and generally small for Dtest for all data splits, except for OOD 1, which reaches higher absolute values on both sides of 0."}, {"title": "A.6 Impact of the number of reference points", "content": "Figures 8 and 9 show the performance of the generic assessor in pre- dicting the performance of Lval on Dval (top panels) and Ltest on Dtest, for different values of the number of reference points selected, for different splits of the KindsOfReasoning and HELM-Lite collec- tion respectively. In particular, this experiment was conducted by considering only one selector method (clustering on embeddings)"}]}