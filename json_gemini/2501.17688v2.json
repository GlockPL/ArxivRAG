{"title": "CONTOURFORMER:REAL-TIME CONTOUR-BASED END-TO-END\nINSTANCE SEGMENTATION TRANSFORMER", "authors": ["Weiwei Yao", "Chen Li", "Minjun Xiong", "Wenbo Dong", "Hao Chen", "Xiong xiao"], "abstract": "This paper presents Contourformer, a real-time contour-based instance segmentation algorithm. The\nmethod is fully based on the DETR paradigm and achieves end-to-end inference through iterative\nand progressive mechanisms to optimize contours. To improve efficiency and accuracy, we develop\ntwo novel techniques: sub-contour decoupling mechanisms and contour fine-grained distribution\nrefinement.In the sub-contour decoupling mechanism, we propose a deformable attention-based\nmodule that adaptively selects sampling regions based on the current predicted contour, enabling\nmore effective capturing of object boundary information. Additionally, we design a multi-stage\noptimization process to enhance segmentation precision by progressively refining sub-contours.\nThe contour fine-grained distribution refinement technique aims to further improve the ability to\nexpress fine details of contours.These innovations enable Contourformer to achieve stable and precise\nsegmentation for each instance while maintaining real-time performance. Extensive experiments\ndemonstrate the superior performance of Contourformer on multiple benchmark datasets, including\nSBD, COCO, and KINS. We conduct comprehensive evaluations and comparisons with existing\nstate-of-the-art methods, showing significant improvements in both accuracy and inference speed. This\nwork provides a new solution for contour-based instance segmentation tasks and lays a foundation for\nfuture research, with the potential to become a strong baseline method in this field.", "sections": [{"title": "Introduction", "content": "Instance segmentation [11] is a fundamental task in the field of computer vision, aiming to precisely identify each\nindividual object and its corresponding contour within an image. In recent years, instance segmentation technology\nhas received widespread attention and achieved significant breakthroughs in research. The studies in this domain have\nprimarily focused on two paradigms: mask-based methods and contour-based methods. The former identifies target\nregions through pixel-level masks, while the latter delineates object contours using sparse boundary points.\nCurrently, mask-based methods are the mainstream choice and have achieved significant improvements in prediction\naccuracy. However, these methods require precise pixel-level predictions for each object, which often results in\nhigh computational resource consumption and memory usage, making them difficult to apply in real-time scenarios.\nAdditionally, mask-based methods lack explicit structure modeling, leading to potential uncertainties or noise in\nboundary detail delineation and internal region processing. On the other hand, contour-based methods have alleviated\ncomputational resource occupancy to some extent but still fall short of achieving parity with mask-based methods,\nparticularly in large-scale benchmark tests. Researchers in the deep learning community have proposed various\nimprovement strategies. However, these methods[36, 2, 12, 23, 13, 14] still struggle to match the performance of\nstate-of-the-art mask-based methods, especially when dealing with complex scenarios.\nTo address these challenges, we propose a novel instance segmentation method called Contourformer. This approach\nis based on the DETR [1] paradigm, inheriting its advantages such as obtaining final results in an end-to-end manner\nwithout relying on Non-Maximum Suppression (NMS) or anchor boxes. Building upon DETR, Contourformer directly\npredicts object boundaries as polygons and employs iterative and progressive mechanisms to refine initial contours\nby estimating residual displacements, thereby delineating the target instances accurately.Our model demonstrates\nstate-of-the-art performance on the SBD [26] dataset, MS-COCO [27] dataset, and KINS [28] dataset. For images\nof size 512x512, Contourformer achieves an inference speed of 24.6 frames per second (fps) on an NVIDIA A30"}, {"title": "Related Work", "content": "This paper reviews the research progress in the field of instance segmentation from the perspectives of mask-based and\ncontour-based approaches, and provides a detailed analysis of various representative methods.\nMask-based instance segmentation. In the domain of mask-based instance segmentation, mainstream research\ncan be broadly categorized into two main paradigms: methods that rely on bounding boxes and those that directly\npredict masks. Early approaches, such as Mask R-CNN [11], adhered to a two-stage paradigm where bounding box\ndetection was followed by mask prediction within each detected region. Subsequent advancements introduced cascaded\nintegration techniques, exemplified by HTC [3] and RefineMask [4], which interleaved detection and segmentation\nfeatures or fused instance features across different stages to refine mask predictions. To address the complexity of mask\nrepresentation, DCT-Mask [5] proposed a DCT-based mask representation, while Patch-DCT [6] further enhanced\nprecision by dividing masks into independent patches and applying DCT-based representation at the patch level. More\nrecently, methods have shifted away from bounding box reliance, instead focusing on direct mask prediction. YOLACT\n[7] achieved instance segmentation by generating prototype masks and their combination coefficients. BlendMask[35]\npredicted 2D attention maps for each proposal and combined them with ROI features to accomplish the segmentation\ntask. SOLO [8] introduced dynamic convolutional kernels to construct high-resolution instance segmentation methods.\nK-Net [22] integrated Transformer structures, gradually optimizing masks through a progressive approach. MaskFormer\n[9] combined the DETR paradigm with dynamic convolutional kernels to separately predict class labels and mask\nresults. Mask2former [10] adopted Deformable-detr [21] as the base model, restricting cross-attention to predicted mask\nregions for local feature extraction. MaskDino [20] merged DINO[17], an object detection algorithm that incorporates\nvarious training acceleration methods[18, 19], with dynamic convolutional kernels for generating masks, enabling\nsimultaneous output of both masks and bounding boxes.\nContour-based instance segmentation. In contrast to mask-based approaches, contour-based methods focus on\npredicting object boundaries or polygonal representations of instances. These methods can be broadly categorized\ninto two types: those using polar coordinates and those employing Cartesian coordinates. For instance, PolarMask\n[12] reformulated instance segmentation as a contour regression problem in polar coordinates, while DeepSnake[23]\ninitializes contours using bounding box predictions, followed by multiple deformation steps to complete the segmentation\ntask. Building on DeepSnake, Dance [13] improves the matching scheme between predicted and target contours\nand introduces an attention deformation mechanism. E2EC proposes a learnable contour initialization architecture,\nsignificantly enhancing performance. Polysnake [14] presents a more lightweight deformable contour module along\nwith shape loss to encourage and regulate object shape learning. BoundaryFormer [15] introduces a differentiable\nrasterization module, using pixel-level masks to supervise model training. PolyFormer [24] adopts a Seq2Seq[33]\nframework, taking image patches and text query tokens as input and autoregressively outputting sequences of polygon\nvertices.\nDifferent from existing methods, Contourformer fully adopts the DETR paradigm and employs iterative and progressive\nmechanisms to learn object contours. It transforms polygon regression from predicting fixed coordinates to modeling\nprobability distributions, thereby achieving precise and robust estimation for various objects."}, {"title": "Methodology", "content": "The proposed Contourformer framework is shown in figure 1. This framework is built upon the D-FINE [25] object\ndetection model and extends the regression of bounding boxes to the regression of contours. To achieve efficient training,"}, {"title": "Sub-contour decoupling mechanism", "content": "The initial contour is given by Contourformer's encoder, referencing previous methods, and is determined by sampling\nfrom the inscribed ellipse of the initially predicted bounding box[36]. The initial contour V = {v\u1d62}\u1d62=\u2080\u1d3a\u207b\u00b9, where N is\nthe number of boundary points for each contour, and each boundary point v\u1d62 is represented by (x\u1d62, y\u1d62), which means\nthat the model ultimately needs to predict the values of 2N. We divide the contour {v\u1d62}\u1d62=\u2080\u1d3a\u207b\u00b9 into N_c sub-contour\nregions {v\u1d62\u02b2}\u2c7c=\u2080\u1d3a_c\u207b\u00b9 (where N_s is the number of boundary points in each sub-contour, which can be divided by N_i.e.,\nN_c \u00d7 N_s = N_v), with different sub-contour regions responsible for generating different parts of the contour. Queries\nfor different sub-contour regions consist of two parts: in addition to a set of instance-level queries {q\u1d62\u207f\u02e2}\u1d62=\u2080\u1d3a_q\u207b\u00b9\n(where N_q is the number of instances), representing each instance, and an additional set of sub-contour-level queries {q\u2c7c}\u2c7c=\u2080\u1d3a_c\u207b\u00b9\n(where N_c is the number of sub-contours), shared among all instances. Each instance object corresponds to a set of\nsub-contour queries {q\u1d62\u2c7c}\u2c7c=\u2080\u1d3a_c\u207b\u00b9. The query formula for the j-th sub-contour of the i-th instance is:\n$q_{ij} = q_{ins} + q_j$\n(1)\nThe decoder of Contourformer consists of multiple decoder layers, each of which uses self-attention and cross-attention\nto update the queries for sub-contour regions, as shown in Figure 2. After dividing the contour regions, the number\nof queries involved in the computation in the decoder increases from N_q to N_q \u00d7 N_c. Directly performing self-\nattention between {q\u1d62\u2c7c}\u2c7c=\u2080\u1d3a_c\u207b\u00b9 has a computational complexity of O((N_q \u00d7 N_c)\u00b2), which would significantly increase"}, {"title": "Contour fine-grained distribution refinement", "content": "Contour modeling in previous approaches has predominantly relied on deterministic representations, which inherently\nlack the capacity to capture positional uncertainty - particularly evident in ambiguous boundary scenarios. This rigid\nparadigm imposes two critical limitations: (1) constrained optimization flexibility due to fixed geometric constraints,\nand (2) amplified localization errors arising from minor prediction inaccuracies[37]. Drawing inspiration from recent"}, {"title": "Experiments", "content": "To validate the effectiveness of Contourformer, we conducted experiments on the SBD, COCO, and KINS datasets and\ndiscussed the results."}, {"title": "Implementation Details", "content": "Following the settings in D-FINE, we use HGNetv2[29] B2 and B3 as the backbone networks, with the number of\npredicted vertices set to N_v = 64 for object contour formation. We set the instance query size N_q = 300, the number\nof sub-contours N_c = 8, and the decoder layer numbers l = 4 and l = 6 (corresponding to the two different backbone\nnetworks, HGNetv2-B2 and HGNetv2-B3). All parameters in the deformable attention mechanism within the decoder\nlayers follow the same settings as those in the D-FINE decoder. We adhere to the end-to-end paradigm of query-based\ninstance detection, employing one-to-one sample matching. The bipartite matching is performed using the Hungarian\nalgorithm, which considers both category predictions and contour point set similarity, with an additional optimization\nfor bipartite matching based on the similarity between the contour's bounding box and the ground truth box. The\nmodel's encoder is responsible for generating initial detection boxes, thus only class prediction and bounding box cost\nare used for bipartite matching, and supervision is applied using classification loss and detection box loss. The loss for\neach decoder layer consists of three components: classification loss, point-to-point loss, and shape loss.\n$L_{decoder} = \\lambda_c L_{cls} + \\lambda_p L_p + \\lambda_s L_{shape}$                                                                                  (4)\n\u03bbc, \u03bbpand \u03bbp are weights used to balance different loss terms. The classification loss employs Variational Focal Loss\n(VFL)[34] to achieve consistent constraints for both classification and localization of positive samples. The point-to-\npoint loss supervises the location of each predicted point, implemented using L1 loss. The shape loss provides supervi-\nsion at a higher edge-level geometry by computing the offsets VV = {\u2206v1\u21920, v2\u21921, ...,, N\u1d65\u208b\u2082\u2192N\u1d65\u208b\u2081, N\u1d65\u208b\u2081\u21920}\nbetween adjacent points within each contour to define the shape of contour V. The loss is then calculated using cosine\nsimilarity between the ground truth offset VVgt and the predicted offset \u2207Vpred .\n$L_{shape} = \\sum_{i=0}^{N-1}cos\\_sim(\u2207V_{pred_{i,i+1}},\u2207V_{gt_{i,i+1}})$       (5)\nThe weights \u03bb\u03b5, Apand Ap are set to 1.0, 1.0, and 0.25, respectively, to balance the different loss terms. We utilized the\nAdam optimizer for training all models and incorporated 100 denoising modules to accelerate the training process."}, {"title": "SBD", "content": "The SBD dataset consists of 5,623 training images and 5,732 testing images, covering 20 distinct semantic categories. It\nleverages images from the PASCAL VOC[38] dataset but re-annotates them with instance-level boundaries, providing a\nunique resource for evaluating contour detection and instance segmentation algorithms. We report the performance of\nContourformer and previous work based on the 2010 VOC APvol, AP50, and AP70 metrics. APvol calculates the mean\nof average precision (AP) values across nine thresholds ranging from 0.1 to 0.9, offering a comprehensive evaluation of\nmodel accuracy over a range of IoU (Intersection over Union) thresholds. The network was trained and tested at a single\nscale of 512\u00d7512 following the preprocessing setup of polySnake, with training conducted for a total of 200 epochs."}, {"title": "COCO", "content": "The COCO dataset is a large-scale dataset containing 118,000 natural scene training images and 80 annotated foreground\nclasses. Specifically, there are 115,000 images for training, 5,000 for validation, and 20,000 for testing. In our study, we\nadopt the COCO Average Precision (AP) metric as the evaluation standard. The network is trained and tested at a single\nresolution of 512x512, with a total training duration of 72 epochs."}, {"title": "KINS", "content": "The KINS dataset uses images from the KITTI[32] dataset, annotated with instance-level semantic annotations. This\ndataset consists of 7,474 training images and 7,517 test images, divided into 7 foreground categories. We use the AP\nmetric as the evaluation standard. The network was trained for 100 epochs and evaluated at a single resolution of\n768\u00d72496, following PolySnake's setup."}, {"title": "Ablation Study", "content": "To verify the effectiveness of the main components in our proposed Contourformer and their parameter selections,\nincluding the sub-contour decoupling mechanism, contour fine-grained distribution refinement, and the impact of the\nnumber of sub-contours (N_c) on model efficiency, we conducted a series of experiments. All ablation experiments were\nperformed on the SBD dataset.\nFirst, we validated the impact of the number of sub-contours (N_c) on model performance. As shown in Table IV, we\ntested four scenarios: N_c=0 (without using the sub-contour iteration optimization mechanism), 4, 8, and 16. The results\nindicated that as the number of sub-contours increased, the model's accuracy improved; however, this enhancement\nwas accompanied by an increase in both inference time and memory usage. When N_c = 16, the model achieved its\nhighest accuracy, but this came at the cost of a significant rise in inference time and memory consumption. Conversely,\nwhen N_c = 8, the model exhibited a relatively substantial improvement in accuracy without a considerable increase\nin inference time. Therefore, we selected N_c = 8 as the optimal number of sub-contours for our model to balance\naccuracy and resource utilization effectively."}, {"title": "Conclusion", "content": "In this study, we propose Contourformer, a real-time contour-based instance segmentation algorithm. Our method is\nentirely grounded in the DETR paradigm and employs an iterative and progressive mechanism to incrementally refine\ncontours, thereby enabling end-to-end inference capabilities. By introducing an innovative sub-contour decoupling\nmechanism and fine-grained distribution refinement techniques, Contourformer ensures stability and accuracy for each\ninstance while maintaining segmentation speed.Experimental evaluations on standard datasets such as SBD, COCO,\nand KINS demonstrate that our approach significantly outperforms existing state-of-the-art methods, thus validating its\neffectiveness and versatility. We anticipate that Contourformer will serve as a pivotal baseline in this field, providing\nrobust technical support for future research endeavors."}]}