{"title": "EPBC-YOLOv8: An efficient and accurate improved YOLOv8 underwater detector based on an attention mechanism", "authors": ["Xing Jiang", "Xiting Zhuang", "Jisheng Chen", "Jian Zhang", "Yiwen Zhang"], "abstract": "In this study, we enhance underwater target detection by integrating channel and spatial attention into YOLOv8's backbone, applying Pointwise Convolution in FasterNeXt for the FasterPW model, and leveraging Weighted Concat in a BiFPN-inspired WFPN structure for improved cross-scale connections and robustness. Utilizing CARAFE for refined feature reassembly, our framework addresses underwater image degradation, achieving mAP@0.5 scores of 76.7% and 79.0% on URPC2019 and URPC2020 datasets, respectively. These scores are 2.3% and 0.7% higher than the original YOLOv8, showcasing enhanced precision in detecting marine organisms.", "sections": [{"title": "Introduction", "content": "The advancement of machine vision technology has enabled underwater robots and aerial vehicles to utilize non-invasive, high-resolution visual perception for autonomous exploration and exploitation of marine resources. Detecting and identifying marine life is crucial for utilizing these resources. However, challenges such as low underwater image quality, small and clustered targets, and imbalanced quantities pose difficulties for this task. Existing methods, primarily based on generic object detection techniques, have been applied in aquaculture\u00b9, distant-water fisheries\u00b2, and marine species monitoring\u00b3. Nevertheless, underwater object detection (UOD) faces issues like low image quality, small and clustered targets, and computational limitations, leading to suboptimal results. There is a need for suitable deep learning models to improve the accuracy and efficiency of UOD, but currently, few researchers focus on addressing these problems.\nHowever, the harsh underwater environment often results in issues such as noise\u2074, uneven lighting conditions\u2075, blurriness\u2076, and low contrast\u2077, which degrade the performance of traditional object detection models like YOLOv8. To address this, attention mechanisms have been increasingly incorporated into detection frameworks, enhancing the model's focus on relevant features. The interaction across dimensions in attention mechanisms helps the model understand relationships between different features, improving detection performance. However, traditional attention mechanisms may face computational challenges with large-size images. Ouyang et al.\u2078 proposed a new approach, the Multi-scale Attention Module (EMA), introduces cross-dimensional interaction to better handle features of different scales, offering potential improvements in channel or spatial attention prediction tasks.\nUnderwater target detection faces challenges due to small and clustered organisms against complex backgrounds. Traditional single-stage object detection algorithms struggled with representing multi-scale objects, leading to the evolution of feature pyramid network (FPN)\u00b9\u2070 algorithms that utilize multi-stage feature maps. However, FPN overlooks the varying significance of features at different levels during fusion. To address this, studies like NAS-FPN\u00b9\u00b9 and BiFPN \u00b9\u00b2 introduced irregular feature fusion modules and weighted feature fusion, respectively. Inspired by BiFPN's cross-scale connections and weighted feature fusion, we employ Weighted_Concat for fusion and introduce WFPN to enhance cross-scale connections, enabling better integration of positional and detailed information.\nThe complex underwater environment necessitates larger receptive fields and semantic association for enhanced detection performance. Traditional interpolation-based upsampling methods fall short as they increase resolution without adding feature information. This study adopts the CARAFE\u00b9\u00b3 upsampling technique, which expands the receptive field and aggregates contextual information effectively, while being lightweight with fewer parameters and computational demands. This approach holds promise for improving underwater feature map understanding and detection performance.\nThis paper presents an underwater object detector based on YOLOv8\u00b9\u2074, with several enhancements:"}, {"title": "Related Work", "content": "Object detection in computer vision is primarily categorized into one-stage and two-stage techniques. Two-stage methods, exemplified by R-CNN\u00b9\u2076 and Faster R-CNN\u00b9\u2077, generate candidate bounding boxes and subsequently classify and refine them, achieving high accuracy at the expense of speed. Enhancements such as the integration of adversarial networks have been introduced to bolster robustness and expedite detection. In contrast, the main one-stage object detection algorithms include the YOLO family, which consists of YOLO\u00b9\u2078 that overcomes the shortcomings of two-stage detection networks. YOLOv2\u00b9\u2079 introduces batch normalization layers after each convolution and eliminates the use of dropout. YOLOv3\u00b2\u2070 marked a significant improvement, characterized by the introduction of the residual module Darknet-53 and FPN. There have been studies that added many techniques based on YOLOv3, such as YOLOv4\u00b2\u00b9, YOLOv5\u00b2\u00b2, YOLOv6\u00b2\u00b3, and YOLOv7\u00b2\u2074. The related code for YOLOv8 can be found on GitHub. SSD\u00b2\u2075 and RetinaNet\u00b2\u2076 are also part of this category. YOLO divides the entire image into a grid, with each cell predicting bounding boxes and classification confidence. Some studies have introduced modifications to YOLOv8 to achieve high-precision detection performance \u00b2\u2077\u2013\u00b2\u2079\nIn computer vision, attention mechanisms such as channel attention, spatial attention, and combined channel-spatial attention dynamically adjust the weights of input features to focus on important areas, similar to the human visual system. Examples include CBAM\u00b3\u2070 and EMA for combined channel-spatial attention. The residual attention network\u00b3\u00b9, an early implementation, generates a three-dimensional attention map but faces challenges with high computational cost and limited receptive fields. To improve efficiency, techniques like global average pooling and decoupling methods have been introduced. Additionally, the Feature Pyramid Network (FPN) constructs a hierarchical feature pyramid to represent objects of various sizes, enhancing multi-scale target detection by merging features from different network levels, thereby improving detection accuracy and capturing fine details. In underwater object detection, upsampling is crucial for reorganizing features to achieve higher detection performance. Methods like linear interpolation and deep learning-based upsampling, such as Meta-Upscale \u00b3\u00b2 and CARAFE. While interpolation methods can increase image resolution, they may introduce noise and increase computational complexity. In contrast, deep learning-based methods like CARAFE offer a large receptive field and accurate detail restoration without significantly increasing computational complexity, making them effective for balancing detection performance improvement in the field of computer vision."}, {"title": "EPBC-YOLOv8", "content": "Although the YOLOv8 model has achieved remarkable results in the field of object detection, it has some limitations. Firstly, the model's memory consumption and computational complexity are relatively high, limiting its deployment efficiency on resource-constrained edge devices. Secondly, the performance of YOLOv8 in detecting small objects needs improvement, especially in the detection of densely arranged small objects, where the model struggles to effectively learn feature information. Moreover, the robustness of the model in handling images with complex backgrounds needs to be enhanced. To address these issues, we designed EPBC-YOLOv8, which integrates C2f_EMA, FasterPW, WFPN, and CARAFE into the YOLOv8 architecture, as shown in Figure.1."}, {"title": "C2f_EMA", "content": "The EMA mechanism is an innovative parallel processing framework designed specifically for computer vision tasks to enhance model performance and accelerate data processing speed. Its framework can be summarized into the following main parts:\n\u2022 Parallel Structure: EMA employs a parallel architecture to process input data, differing from the sequential layer structure of traditional Convolutional Neural Networks (CNNs). This parallel structure improves the efficiency of model training and enhances the model's accuracy when dealing with multi-scale features.\n\u2022 Feature Map Grouping: EMA groups input feature maps along the channel dimension, with each group processing a subset of features. This grouping strategy enhances the model's ability to handle different features and promotes model learning by assigning weights to different input features using attention mechanisms\u00b3\u00b3.\n\u2022 Multi-scale Spatial Information Capture: EMA captures multi-scale spatial information through parallel subnetworks with large receptive fields, enabling simultaneous processing of feature regions of different sizes. This approach allows for a more effective understanding and representation of various aspects of the input data.\n\u2022 Attention Weight Extraction: EMA is designed with three parallel paths to extract attention weights, including two 1\u00d71 branches and one 3\u00d73 branch. This configuration encodes information from different spatial directions and captures more complex multi-scale features.\n\u2022 Feature Interaction and Spatial Attention Map Generation: EMA processes interactions between different features through a cross-spatial information aggregation strategy, generating spatial attention maps. This enables spatial information of different scales to be effectively integrated within the same processing stage.\n\u2022 Final Output: The output of EMA includes two spatial attention maps, preserving precise spatial location information. The output feature maps within each group are further processed through a Sigmoid function to optimize the final feature representation.\nIn this study, we incorporated the EMA module into the neck part of YOLOv8, which is a key component of its improved version. As shown in Figure.2, EMA splits the input feature maps $X \\in R^{C \\times H \\times W}$ into G groups of cross-channel sub-features, each group $Xi \\in R^{C/G \\times H \\times W}$learning different semantic information. To enhance the ability to capture multi-scale spatial information, we replaced the original 3\u00d73 branch with a 5\u00d75 branch, expanding the model's receptive field. EMA consists of three parallel paths, with two in the 1\u00d71 branch and one in the 3\u00d73 branch. Specifically, global spatial information is extracted from the output of the 1\u00d71 branch using two-dimensional global average pooling. Meanwhile, the output from the 3\u00d73 branch undergoes direct adjustment to align with the corresponding dimensional structure before the joint activation mechanism that incorporates channel features, as shown in Equation.(1). With these improvements, the EMA module provides YOLOv8 with"}, {"title": "", "content": "Here, $z_c$ represents the output associated with the c-th channel. The primary purpose of this output is to encode global information, thereby capturing and modeling long-range dependencies.\nIn the YOLOv8 architecture, the C2f_EMA model integrates multi-source local features using the EMA mechanism in the C2f backbone network, as shown in Figure.3. This parallel processing and self-attention strategy significantly improves performance, enhancing the model's accuracy, efficiency, and robustness, optimizing feature representation, and enabling it to excel in various visual tasks."}, {"title": "FasterPW", "content": "To enhance the speed of neural networks, many studies have focused on reducing floating-point operations (FLOPs). However, the reduction in FLOPs does not always lead to a corresponding decrease in latency due to inefficient FLOPS. The FasterNeXt network reduces FLOPs while increasing FLOPS efficiency through PConv, reducing latency and enhancing computational speed without compromising accuracy. However, PConv has issues such as limited stride, insufficient receptive field, difficulty in determining the convolution ratio, and increased computational cost.\nOur model replaces PConv with PWConv on the basis of the FasterNeXt network. PWConv is a special form of multi-channel convolution, where each convolution kernel is of size 1\u00d71. It uses a 3D input feature map I of size (Hi \u00d7 Wi \u00d7 Ci) as input and a 4D filter F of size (1 \u00d7 1 \u00d7 Ci \u00d7 Co) to generate a 3D output feature map of size (Ho \u00d7 Wo \u00d7 Co), where Ho = Hi and Wo = Wi.\nThe advantage of PWConv lies in its higher computational efficiency and reduced computational complexity. Specifically, PWConv serves as a local channel context aggrega-tor [44], utilizing pointwise channel interactions at each spatial location and reducing the number of channels to decrease computation.\nIn this study, the core architecture of YOLOv8 underwent significant changes, with the C2f structure in its backbone network being replaced by the lightweight FasterPW series network, as shown in Figure.4. The FasterPW design includes three standard convolutional layers and FasterPWBlock module, forming a lightweight feature extraction network. This network strategically applies batch normalization (BN)\u00b3\u2074 and the SiLU activation function after pointwise convolutions to maintain feature diversity and reduce latency. SiLU has a stronger non-linear expression capability and efficient performance, making it the preferred activation function. By adopting the improved FasterNeXt network, we effectively reduced the number of network parameters, floating-point operations, and memory access times, achieving network lightweighting while maintaining efficient feature extraction capabilities \u00b3\u2075."}, {"title": "WFPN", "content": "To enhance the feature extraction and fusion capabilities of fusion capabilities of the object detection model, we have adopted the concept of BiFPN, resulting in Weight_Concat, which replaces the original Concat. The main principles of BiFPN are as follows:\n\u2022 Bidirectional Feature Fusion: BiFPN enables the fusion of features in both top-down and bottom-up directions, thus more effectively integrating features of different scales.\n\u2022 Weighted Fusion Mechanism: BiFPN optimizes the feature fusion process by assigning weights to each input feature, allowing the network to place greater emphasis on features with more substantial information content.\n\u2022 Structural Optimization: BiFPN optimizes cross-scale connections by removing nodes with only one input edge, adding additional edges between input and output nodes at the same level, and treating each bidirectional path as a feature network layer."}, {"title": "CARAFE", "content": "In multi-scale image labeling detection, feature upsampling is a key step. Traditional upsampling techniques often fail to fully utilize the semantic information in feature maps, limiting the effectiveness of feature fusion. Factorization methods leverage semantic information by learning upsampling kernels but increase the number of parameters and computational cost. Moreover, they use the same upsampling kernel at every position of the feature map, which cannot effectively utilize the semantic differences in the feature map. CARAFE has a larger receptive field, allowing for more effective aggregation of contextual information. Its upsampling kernel is closely related to the semantics of the feature map, effectively enhancing the multi-scale object detection performance after fusing multi-level features without significantly increasing parameters and computational cost. We replaced traditional upsampling with CARAFE in the neck part, further enhancing the performance of object detection, as shown in Figure.1.\nCARAFE is a lightweight content-aware upsampling strategy that can accurately restore image details and reduce the loss of information in small objects. It consists of two parts: a kernel prediction module and a content-aware reassembly module \u00b3\u2076. The kernel prediction module predicts the upsampling kernels for each position, while the content-aware reassembly module uses these kernels to reassemble the feature maps pixel by pixel, achieving content-aware upsampling, as shown in Figure.6. This process makes the upsampling more perceptive and adaptive, improving the accuracy and effectiveness of upsampling. CARAFE has the following innovative features:Firstly, the kernel prediction module y predicts unique upsampling kernels W for each position l' based on the neighboring locations of the input feature map \u03c7\u03b9, achieving content adaptiveness. Secondly, the content-aware reassembly module uses these kernels for feature upsampling, effectively restoring detail information. This module contains a channel compressor, content encoder, and kernel normalizer, where the channel compressor compresses the input feature channels, the content encoder generates the reassembly kernels, and the kernel normalizer applies the softmax function. The content-aware reassembly module reassembles local area features through the weighted sum operator, allowing each pixel within the area to contribute differently to the upsampling pixels based on feature content rather than positional distance, enhancing the semantics of the feature map. Compared to the decomposition method, CARAFE has fewer parameters"}, {"title": "Experimental Details", "content": "The experimental environment parameters for this experiment are as follows. We used an Intel(R) Xeon(R) Gold 6248R @ 3.00 GHz processor and an NVIDIA GeForce RTX 3090 graphics card. The deep learning model framework used PyTorch 2.0.0 and Python 3.8, with CUDA version 11.7, and the operating system was Windows 11."}, {"title": "Benchmark Testing and Implementation Details", "content": "The dataset used in this paper is URPC2019, which is used to verify the effectiveness of our proposed model framework.\nURPC2019 is a publicly available dataset for underwater object detection, containing 5 different aquatic organism categories:\nechinoderms, starfish, holly, scallops, and seagrass, with a total of 3,765 training samples and 942 validation samples. Examples of dataset images are shown in Figure.7. Additionally, we conducted detection experiments on the URPC2020 dataset. Similar to URPC2019, URPC2020 is also an underwater dataset, but it differs in that it contains only four distinct categories: sea cucumbers, sea urchins, scallops, and starfish, with a total of 4,200 training samples and 800 validation samples. This further validates the feasibility of our model."}, {"title": "Parameter Settings", "content": "To ensure the fairness and comparability of the model's effectiveness, we use official document as the pre-trained weight file for all experiments. At the same time, we employ the letterbox technique to adjust the input image size to 640\u00d7640, which allows the input image to retain the original aspect ratio while being adjusted to a fixed size, facilitating model training and inference. In the experiments, we set the number of iterations to 100, and some other important hyperparameters during the training phase of the model are shown in Table.1."}, {"title": "Evaluation Criteria", "content": "Intersection over Union (IOU) is a commonly used evaluation metric in object detection and image segmentation tasks. It measures the degree of overlap between the predicted bounding box (or segmentation result) and the true bounding box. IOU is defined by calculating the area of the intersection of the predicted bounding box and the true bounding box divided by the area of their union. See Equation.(2) for details, where Intersection represents the area of the intersection of the predicted and true bounding boxes, and Union represents the area of their union. The value of IOU ranges from 0 to 1, with values closer to 1 indicating a higher degree of match between the predicted and true results, and values closer to 0 indicating a lower degree of match.\nIn machine learning and statistics, False Positive (FP), True Positive (TP), False Negative (FN), and True Negative (TN) are common metrics used to evaluate the performance of classification models.\nTP is the number of positive instances predicted as positive by the model, TN is the number of negative instances predicted as negative by the model, FP is the number of negative instances incorrectly predicted as positive by the model, and FN is the number of positive instances incorrectly predicted as negative by the model. The relationships between these four metrics are shown in Table.2 Based on these metrics, other metrics such as Precision, Recall, Average Precision (AP), and Mean Average Precision (mAP) can be calculated."}, {"title": "", "content": "Here, $IoU = \\frac{Area of Intersection}{Area of Union}$"}, {"title": "", "content": "Precision is the proportion of true positives among the samples predicted as positive. High precision means that the model makes fewer misclassification judgments, but it does not guarantee that all positive instances are correctly identified, as shown in Equation.(3).\nRecall is the proportion of true positives that are correctly predicted as positive among all actual positive instances. High recall means that the model can identify more positive instances, but it may also incorrectly predict some negative instances as positive, as shown in Equation.(4)."}, {"title": "", "content": "Precision = $\\frac{TP}{TP+FP}$"}, {"title": "", "content": "Recall = $\\frac{TP}{TP+FN}$"}, {"title": "", "content": "Average Precision is a metric used in tasks such as information retrieval and object detection. AP measures the ability of a model to rank results, and it is the average of the proportion of correct results returned. The higher the AP, the better the model is at ranking results, as shown in Equation.(5).\nMean Average Precision is a metric used in tasks such as multi-class object detection. mAP is the average of the AP for each category, and it is used to evaluate the detection performance of the model across different categories, as shown in Equation.(6)."}, {"title": "", "content": "AP = $\\int PRdr$"}, {"title": "", "content": "MAP = $\\frac{1}{C} \\sum AP$"}, {"title": "Comparative Experiments", "content": "We conducted comparative experiments on the performance of our model EPBC-YOLOv8 and other models on the URPC2019 dataset, and the results are shown in Table.3. It can be seen that our model has a significant reduction in both the number of parameters and FLOPs, which reduces the computational complexity of our model and is conducive to improving the accuracy of the model. The table shows that our model has a significant improvement in detection results compared to YOLOv5 and YOLOv8. Specifically, after modifications to the baseline YOLOv8n model, we observed an increase in mAP@0.5 precision by 2.3%, which demonstrates the effectiveness of our model in optimizing YOLOv8."}, {"title": "Ablation Study", "content": "Firstly, we modified the Bottleneck structure in the first C2f of the backbone part as shown in Figure .3.We attempted three optimization methods: C2f_EMA is the optimized structure adopted in this paper; C2f_FasterPW_EMA is based on C2f_EMA, with the Bottleneck in C2f replaced by FasterPW; C3_FasterPW_EMA applies the same operation to the C3 structure as C2f_FasterPW_EMA. As shown in Table.5, the optimization of the C2f structure leads to an improvement in mAP@0.5, with the C2f_EMA structure showing the most significant increase of 1.3%."}, {"title": "Effectiveness of Each Module in EPBC-YOLOv8", "content": "In this section, we take the original YOLOv8 as a basis and gradually add or remove components included in our model to explore the contribution of each component to the overall performance of the system model, thereby demonstrating their effectiveness in improving YOLOv8. A total of 15 ablation experiments were conducted, and the results are shown in Table.6. By analyzing the table, we can see that different combinations of modules have different impacts on the performance of the object detection system.\nWhen using each of the four modules individually, there is an improvement compared to the original YOLOv8, with the increase in mAP in descending order being C2f_EMA, CARAFE, WFPN, FasterPW . It can be seen that using the C2f_EMA module alone results in the highest increase in mAP, which is 1.3%.\nWhen the modules are used in combination, there is an increase in mAP, and compared to using each module individually, the increase in mAP is generally larger. The best combination is when the C2f_EMA, FasterPW, WFPN, and CARAFE modules are used simultaneously, achieving the highest mAP@0.5 of 76.7%, which is a 2.3% increase compared to the original YOLOv8.\nIn summary, based on the experimental results, the best performance improvement can be achieved by using the C2f_EMA, FasterPW, WFPN, and CARAFE modules simultaneously. These results provide guidance for optimizing the design and configuration of object detection systems."}, {"title": "Result Analysis", "content": "To verify the effectiveness of our improved EPBC-YOLOv8 model, we calculated the real-time changes in loss value, precision, recall, mAP@0.5, and mAP@0.95 for the EPBC-YOLOv8 model during 100 iterations on the training set and validation set. As can be seen from Figure.9, in both the training and validation sets, the real-time loss value of the EPBC-YOLOv8 model smoothly decreases with the increase of epochs and eventually converges. Especially in the validation set, the classification loss is more stable compared to the bounding box regression loss and the loss function for keypoint detection, indicating that the EPBC-YOLOv8 model has better classification performance for target categories. Simultaneously observing the changes in precision, recall, mAP@0.5, and mAP@0.5:0.95 values, all show an upward trend and good convergence, demonstrating that EPBC-YOLOv8 has good performance in object detection."}, {"title": "Conclusions and Future Work", "content": "In this study, we propose EPBC-YOLOv8,an improved underwater organism detection framework, for complex underwater environments. In the backbone network of YOLOv8, we introduce an efficient multi-scale attention mechanism to focus on key regions of the image and extract strong features. The FasterPW part applies PWConv, using 1\u00d71 dense convolution operations to fuse channel information and enhance computational accuracy. The neck module incorporates the content-aware characteristics of the CARAFE module, replacing traditional upsampling methods to enhance multi-scale object detection effects. The Concat connection of the model is augmented with the WFPN structure to improve performance and generalization ability. Experimental results demonstrate that EPBC-YOLOv8 achieves mAP@0.5 of 76.7% and 79.0% on the URPC2019 and URPC2020 datasets, respectively, which are 2.3% and 0.7% higher than the original YOLOv8. Our improved method significantly outperforms the original YOLOv8 in terms of object detection accuracy, making it an efficient and practical underwater object detection framework. Future work will explore more advanced underwater image processing techniques to improve the accuracy of object detection in underwater environments."}, {"title": "Competing interests", "content": "Te authors declare no competing interests."}, {"title": "Additional information", "content": "Correspondence and requests for materials should be addressed to J.Z. Reprints and permissions information is available at www.nature.com/reprints. Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affliations.\nOpen Access Tis article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. Te images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/."}]}