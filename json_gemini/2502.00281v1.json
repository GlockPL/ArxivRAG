{"title": "Sigmoid Self-Attention is Better than Softmax Self-Attention: A Mixture-of-Experts Perspective", "authors": ["Fanqi Yan", "Huy Nguyen", "Pedram Akbarian", "Nhat Ho", "Alessandro Rinaldo"], "abstract": "At the core of the popular Transformer architecture is the self-attention mechanism, which dynamically assigns softmax weights to each input token so that the model can focus on the most salient information. However, the softmax structure slows down the attention computation due to its row-wise nature, and inherently introduces competition among tokens: as the weight assigned to one token increases, the weights of others decrease. This competitive dynamic may narrow the focus of self-attention to a limited set of features, potentially overlooking other informative characteristics. Recent experimental studies have shown that using the element-wise sigmoid function helps eliminate token competition and reduce the computational overhead. Despite these promising empirical results, a rigorous comparison between sigmoid and softmax self-attention mechanisms remains absent in the literature. This paper closes this gap by theoretically demonstrating that sigmoid self-attention is more sample-efficient than its softmax counterpart. Toward that goal, we illustrate that each row of the self-attention matrix can be represented as a mixture of experts. Our analysis shows that \"experts\" in sigmoid self-attention require significantly less data to achieve the same approximation error as those in softmax self-attention. We corroborate our theoretical findings through extensive experiments on both synthetic and real-world datasets.", "sections": [{"title": "1 Introduction", "content": "Transformer models [54] have been known as the state-of-the-art architecture for a wide range of machine learning and deep learning applications, including language modeling [16, 3, 47, 51], computer vision [17, 4, 46, 35], and reinforcement learning [5, 31, 25], etc. One of the central components that contribute to the success of the Transformer models is the self-attention mechanism, which enables sequence-to-sequence models to concentrate on relevant parts of the input data. In particular, for each token in an input sequence, the self-attention mechanism computes a context vector formulated as a weighted sum of the tokens, where more relevant tokens to the context are assigned larger weights than others (see Section 2.1 for a formal definition). Therefore, self-attention is able to capture long-range dependencies and complex relationships within the data.\nHowever, since the weights in the context vector are normalized by the softmax function, there might be an undesirable competition among the tokens, that is, an increase in the weight of a token leads to a decrease in the weights of others. As a consequence, the traditional softmax self-attention mechanism might focus only on a few aspects of the data and possibly ignore other informative features [48]. Additionally, [22] also discovered that the tokens' inner dependence on the attention scores owing to the softmax normalization partly causes the attention sink phenomenon occurring"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Self-Attention Mechanism", "content": "The self-attention mechanism plays a crucial role in the Transformer architecture [54]. Given an input sequence $X \\in \\mathbb{R}^{N \\times d}$ of $N$ feature vectors of dimension $d$, the self-attention mechanism first projects it into the query matrix $Q \\in \\mathbb{R}^{N \\times d_k}$, the key matrix $K \\in \\mathbb{R}^{N \\times d_k}$, and the value matrix $V \\in \\mathbb{R}^{N \\times d_v}$ through three following linear transformations:\n$Q = XW_Q, K = XW_K, V = XW_V$,\nwhere $W_Q \\in \\mathbb{R}^{d \\times d_k}, W_K \\in \\mathbb{R}^{d \\times d_k}, W_V \\in \\mathbb{R}^{d \\times d_v}$ are learnable weight matrices. Then, the vanilla softmax self-attention can be compactly written as\n$\\text{SoftmaxAttn}(X) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V.$\nwhere the Softmax function acts row-wise on the matrix $QK^T/\\sqrt{d_k} \\in \\mathbb{R}^{N \\times N}$ as follows: $\\text{Softmax}(u) = \\frac{1}{\\sum_{j=1}^n \\exp(u_j)}(\\exp(u_1),\\dots,\\exp(u_n))$, where $u = (u_1,\\dots, u_n)$ is a row vector in $\\mathbb{R}^N$.\nMeanwhile, the sigmoid self-attention is computed by replacing the above softmax function with the sigmoid function $\\sigma : z \\mapsto (1 + \\exp(-z))^{-1}$ applied element-wise to the matrix $QK^T/\\sqrt{d_k}$:\n$\\text{SigmoidAttn}(X) = \\sigma\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V.$"}, {"title": "2.2 Mixture-of-Experts Model", "content": "Mixture of experts (MoE) [26] is an extension of classical mixture models [34] that aggregates the power of $N$ sub-models called experts, each of which can be formulated as a feed-forward network [50], a regression function [19], or a classifier [6] denoted by $E_i : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d_v}$ for $1 \\leq i \\leq N$. For that purpose, the MoE employs an adaptive gating mechanism, denoted by $G : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{N}$, to compute input-dependent weights for those experts in a dynamic way that more relevant experts to the input will be assigned larger weights. Then, given an input $h \\in \\mathbb{R}^d$, the MoE output $y \\in \\mathbb{R}^{d_v}$ is expressed"}, {"title": "2.3 Self-Attention meets Mixture of Experts", "content": "In this section, we rigorously show that each row of the sigmoid/softmax self-attention matrix can be represented as an MoE with quadratic affinity scores. Since the subsequent arguments apply for both attention variants, we will present only the derivation using the sigmoid self-attention matrix, which we recall is defined as\n$\\text{SigmoidAttn}(X) = \\sigma(XBX^T)XW_V,$\nwhere $B := \\frac{W_QW_K}{\\sqrt{d_k}} \\in \\mathbb{R}^{d \\times d}$. Let $x_i \\in \\mathbb{R}^{1 \\times d}$ denote the $i$-th row vector of the input matrix $X$. Since the sigmoid function is applied element-wise, the $(i, j)$-th element of the matrix $\\sigma(XBX^T) \\in \\mathbb{R}^{N \\times N}$ is\n$[\\sigma(XBX^T)]_{i,j} = \\sigma(x_iBx_j^T).$\nAs a result, the $i$-th row vector of the matrix $\\text{SigmoidAttn}(X)$ takes the form\n$[\\text{SigmoidAttn}(X)]_{i,:} = \\sum_{j=1}^N \\sigma(x_iBx_j^T) \\cdot x_jW_V.$\nNext, let $X = [x_1,x_2,\\dots,x_N] \\in \\mathbb{R}^{1 \\times Nd}$ be the concatenation of $N$ input tokens. For each $1 \\leq i \\leq N$, let $E_i \\in \\mathbb{R}^{Nd \\times d}$ be the matrix such that $XE_i = x_i$. Then, the $i$-th row vector of the sigmoid self-attention matrix can be rewritten as\n$[\\text{SigmoidAttn}(X)]_{i,:} = \\sum_{j=1}^N \\sigma(XE_iB(XE_j)^T) \\cdot XE_jW_V$\n$\\qquad\\qquad\\qquad = \\sum_{j=1}^N \\sigma(X M_{ij} X^T) \\cdot XP_j,$\nwhere $M_{ij} := E_iBE_j^T = \\frac{E_iW_QW_K E_j^T}{\\sqrt{d_k}}$ and $P_j := E_jW_V$.\nHence, each row of the sigmoid self-attention matrix can be represented as an MoE with quadratic affinity scores."}, {"title": "3 Problem Setup", "content": "In this section, we present the problem setup for studying the sample efficiency of the sigmoid self-attention under the perspective of the sigmoid gating MoE with quadratic affinity scores. A similar convergence analysis for the softmax gating MoE with quadratic affinity scores can be found in [1], so it is omitted in this paper.\nSuppose that the data $(X_1,Y_1), (X_2,Y_2), \\cdots, (X_n, Y_n) \\in \\mathbb{R}^d \\times \\mathbb{R}$ are generated according to the regression model\n$Y_i = f_{G_\\Theta}(X_i) + \\varepsilon_i, i = 1, 2, \\dots, n,$\nwhere $X_1, X_2,\\dots, X_n$ are i.i.d. samples from a probability distribution $\\mu$ on $\\mathbb{R}^d$, and $\\varepsilon_1, \\varepsilon_2,\\dots, \\varepsilon_n$ are i.i.d. Gaussian noise variables with $\\mathbb{E}[\\varepsilon_i|X_i] = 0$ and $\\text{Var}[\\varepsilon_i|X_i] = v$, for $1 \\leq i \\leq n$. Additionally, the regression function $f_{G_\\Theta} : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is unknown and formulated as a sigmoid gating mixture of $N^*$ experts, i.e.\n$f_{G_\\Theta}(x) := \\sum_{i=1}^{N^*} \\frac{1}{1 + \\exp(-s(x, \\theta_i))}\\cdot E(x, \\eta_i),$\nwhere $E(x, \\eta_i)$ denotes the parametric expert function, while the affinity score function $s(x, \\theta)$ takes a quadratic form. In this work, we consider the following two types of affinity score functions:\n(i) fully quadratic score: $s(x, \\theta) = x^T A x + b^T x + c$;\n(ii) partially quadratic score: $s(x, \\theta) = x^T A x + c$;\nwhere $\\Theta = \\{(A, b, c, \\eta) \\in \\mathbb{R}^{d \\times d} \\times \\mathbb{R}^d \\times \\mathbb{R} \\times \\mathbb{R}^q\\}$ denotes the parameter space. Given this setup, our goal is to determine the sample size necessary for the estimators of the parameters and experts in model (5) to reach an approximation error $\\epsilon > 0$. Due to space limitation, we will present only the results for the case of a fully quadratic affinitty score function in Section 4 and defer those for its partial version to Appendix C.\nNotation. We let $[n]$ stand for the set $\\{1, 2, \\dots, n\\}$ for any $n \\in \\mathbb{N}$. Next, for any set $S$, we denote $|S|$ as its cardinality. For any vectors $v := (v_1, v_2, \\dots, v_d) \\in \\mathbb{R}^d$ and $\\alpha := (\\alpha_1, \\alpha_2, \\dots, \\alpha_d) \\in \\mathbb{N}^d$, we let $v^\\alpha = v_1^{\\alpha_1} v_2^{\\alpha_2} \\dots v_d^{\\alpha_d}$, $|v| := v_1 + v_2 + \\dots + v_d$ and $\\alpha! := \\alpha_1!\\alpha_2!\\dots \\alpha_d!$, while $||v||$ denotes its 2-norm value. Lastly, for any two positive sequences $(a_n)_{n\\geq 1}$ and $(b_n)_{n\\geq 1}$, we write $a_n = O(b_n)$ or $a_n \\leq b_n$ if $a_n \\leq Cb_n$ for all $n \\in \\mathbb{N}$, where $C > 0$ is some universal constant. The notation $a_n = O_p(b_n)$ indicates that $a_n/b_n$ is stochastically bounded.\nLeast squares estimation. To estimate the unknown ground-truth parameters $\\{A_{i^*}, b_{i^*}, c_{i^*}, \\eta_{i^*}\\}_{i=1}^{N^*}$, we deploy the least squares method [53]. Thus we consider the estimator\n$\\hat{G}_n := \\arg \\min_{G \\in M_N(\\Theta)} \\sum_{i=1}^n (Y_i - f_G(X_i))^2,$\nwhere $M_N(\\Theta) := \\{G = \\sum_{i=1}^{N'} \\frac{1}{1 + \\exp(-s_i(x, \\theta_i))}\\delta_{(A_i, b_i, c_i, \\eta_i)} : 1 \\leq N' \\leq N, (A_i, b_i, c_i, \\eta_i) \\in \\Theta\\}$ is the set of all mixing measures with at most $N$ atoms. As the number of ground-truth experts $N^*$ is unknown in practice, we assume that the number of fitted experts $N$ larger than $N^*$, i.e. $N > N^*$.\nConvergence of the regression function estimator. Since the number of fitted experts is larger than the number of ground-truth experts, there must exist some atom $(A_{b^*}, b_{b^*}, c_{b^*}, \\eta_{b^*})$ of the"}, {"title": "4 Sample Efficiency of Sigmoid Self-Attention", "content": "In this section, we investigate the sample efficiency of the sigmoid self-attention through the perspective of the sigmoid gating MoE with fully quadratic affinity score function. In particular, we determine how much data the experts need to reach an approximation error $\\epsilon$, which can be deduced from the expert convergence rate. We start with the sparse regime of the gating parameters in Section 4.1, and then proceed with the dense regime in Section 4.2."}, {"title": "4.1 Sparse Regime of Gating Parameters", "content": "Recall that under the sparse regime, all the over-specified gating parameters are zero, that is, $(\\hat{A}_{b^*}, \\hat{b}_{b^*}) = (0_{d \\times d}, 0_d)$. Suppose that $\\{ \\hat{A}_{i^*}, \\hat{b}_{i^*}\\}_{i=1}^{N}$ are over-specified parameters, i.e., those fitted by at least two estimators, where $1 < N < N^*$. The remaining gating parameters are exactly-specified parameters $\\{ \\hat{A}_{i^*}, \\hat{b}_{i^*}\\}_{i=N+1}^{N^*}$, i.e., those fitted by exactly one estimator.\nAs mentioned in Section 3, in order to obtain the expert convergence rate, it is sufficient to establish the lower bound $|| f_{\\hat{G}_n} - f_{G^*} ||_{L^2(\\mu)} \\geq \\mathcal{L}(\\hat{G}_n, G^*)$. A popular approach adopted in previous works [37, 42] for this problem is to decompose the difference $f(x) - f_{G^*}(x)$ by applying a Taylor expansion to the product of the sigmoid gating function and the expert function given by\n$F(x; A, b, c, \\eta) := \\sigma(x^T A x + b^T x + c) \\cdot E(x, \\eta).$\nThis decomposition of the regression function is expected to consist of linearly independent terms so that when $||f_{\\hat{G}_n} - f_{G^*} ||_{L^2(\\mu)} \\rightarrow 0$ as $n \\rightarrow \\infty$, the parameter discrepancies in the decomposition will also converge to zero, leading to both parameter and expert convergence. To secure such linear independence, we need to impose a strong identifiability condition on the function $x \\rightarrow F(x; A, b, c, \\eta)$.\nDefinition 1 (Strong identifiability). We call an expert function $x \\rightarrow E(x, \\eta)$ strongly identifiable if it is twice differentiable w.r.t its parameter $\\eta$ for $\\mu$-almost all $x$ and, for any natural number $l$ and any distinct parameters $\\{(A_i, b_i, c_i, \\eta_i)\\}_{i=1}^l$, the functions in the families\n$\\Bigg\\{\\frac{\\partial^{ \\gamma_1+\\gamma_2+\\gamma_3} F}{\\partial A^{\\gamma_1} \\partial b^{\\gamma_2} \\partial \\eta^{\\gamma_3}} (x, 0_{d \\times d}, 0_d, c_i, \\eta_i) : i \\in [l], \\sum_{j=1}^3 |\\gamma_j| \\in [2]\\Bigg\\}$\nand\n$\\Bigg\\{\\frac{\\partial F}{\\partial A^{\\tau_1} \\partial c^{\\tau_2} \\partial \\eta^{\\tau_3}} (x; A_i, b_i, c_i, \\eta_i) : i \\in [l], \\sum_{j=1}^4 |\\tau_j| = 1\\Bigg\\}$\nare linearly independent, for $\\mu$-almost all $x$, where $(\\gamma_1, \\gamma_2, \\gamma_3) \\in \\mathbb{N}^{d \\times d} \\times \\mathbb{N}^d \\times \\mathbb{N}^q$ and $(\\tau_1, \\tau_2, \\tau_3, \\tau_4) \\in \\mathbb{N}^{d \\times d} \\times \\mathbb{N}^d \\times \\mathbb{N} \\times \\mathbb{N}^q$.\nExamples. Let us consider two-layer neural networks of the form $E(x, (\\alpha, \\beta, \\lambda)) = \\lambda \\varphi(\\alpha x + \\beta)$, where $\\varphi$ is some activation function and $(\\alpha, \\beta, \\lambda) \\in \\mathbb{R}^d \\times \\mathbb{R} \\times \\mathbb{R}$. It can be verified that if $\\varphi$ is the ReLU or GELU function, $\\alpha \\neq 0_d$, and $\\lambda \\neq 0$, then the function $x \\rightarrow E(x, (\\alpha, \\beta, \\lambda))$ is strongly identifiable. In contrast, if the expert function is of polynomial form $E(x, (\\alpha, \\beta)) = (\\alpha x + \\beta)^p$ for some $p \\in \\mathbb{N}$, then it fails to satisfy the strong identifiability condition. For the case of $p = 1$, this is due to the linear dependence expressed by the partial differential equations (PDEs)\n$\\frac{\\partial^2 F}{\\partial A \\partial \\alpha} = \\frac{\\partial^2 F}{\\partial b \\partial \\beta} = \\frac{\\partial^2 F}{\\partial A \\partial \\beta} = \\frac{\\partial^2 F}{\\partial b \\partial \\alpha}.$\nThe strong identifiability condition intuitively helps eliminate potential interactions among parameters expressed in the language of PDEs, namely those in equation (8), where gating parameters interact with themselves and with expert parameters. We will show later in Theorem 2 that those interactions lead to strikingly slow expert convergence rates, thereby reducing the model sample efficiency.\nIn the next sections, we will study the convergence behavior of strongly identifiable experts and polynomial experts."}, {"title": "4.1.1 Strongly Identifiable Experts", "content": "Voronoi loss. For a mixing measure $G$ with $1 < N' < N$ atoms, we allocate its atoms across the Voronoi cells $\\{A_j = A_j(G), j \\in [N^*]\\}$ generated by the atoms of $G^*$, where\n$A_j := \\{i \\in [N'] : ||\\theta_i - \\theta^*_j|| \\leq ||\\theta_i - \\theta^*_l||, \\forall l \\neq j\\},$\nwith $\\theta_i := (A_i, b_i, \\eta_i)$ and $\\theta^*_j := (A^*_j, b^*_j, \\eta^*_j)$ for all $j \\in [N^*]$. Then, the Voronoi loss function is defined as\n$\\mathcal{L}_1(G, G_*) := \\sum_{j=1}^{N^*} \\sum_{i \\in A_j} \\frac{1}{1 + \\exp(-c_i)} - \\frac{1}{1 + \\exp(-c_j^*)}\\sum_{j=1}^{N^*} \\sum_{i \\in A_j} [||\\Delta A_{ij}||^2 + ||\\Delta b_{ij}||^2 + ||\\Delta \\eta_{ij}||^2]$\n$+ \\sum_{j=N+1}^{N} \\sum_{i \\in A_j} [||\\Delta A_{ij}|| + ||\\Delta b_{ij}|| + |\\Delta c_{ij}| + ||\\Delta \\eta_{ij}||],$\nwhere we denote $\\Delta A_{ij} := A_i - A_j^*, \\Delta b_{ij} := b_i - b_j^*, \\Delta c_{ij} := c_i - c_j^*$ and $\\Delta \\eta_{ij} := \\eta_i - \\eta_j^*$. In the statement above, if the Voronoi cell $A_j$ is empty, the corresponding summation term is conventionally defined to be zero. Additionally, the Voronoi loss function $\\mathcal{L}_1$ can be computed efficiently, with a computational complexity of $O(N \\times N^*)$.\nWith the above Voronoi loss function at hand, we finally obtain the following parameter conver-gence rate.\nTheorem 1. If the expert function $x \\rightarrow E(x, \\eta)$ is strongly identifiable, then the lower bound $|| f_G - f_{G^*} ||_{L^2(\\mu)} \\geq \\mathcal{L}_1(G, G^*)$ holds true for any $G \\in \\mathcal{M}_N(\\Theta)$, then\n$\\mathcal{L}_1(\\hat{G}_n, G^*) = O_p(\\sqrt{\\log(n)/n}).$"}, {"title": "4.1.2 Polynomial Experts", "content": "We now investigate polynomial experts of the form $E(x, (\\alpha, \\beta)) = (\\alpha x + \\beta)^p$, where $p \\in \\mathbb{N}$. As mentioned in the example paragraph following the Definition 1, the polynomial experts does not meet the strong identifiability condition due to the linear dependence among the derivative of the function $F(x; A, b, c, \\eta)$. For example, when $p = 1$, such linear dependence is exhibited via the interaction among the gating parameters (see the first PDE in equation (8)) and the interaction between the gating parameters and the expert parameters (see the last two PDEs in equation (8)). Those PDEs account for the non-strong identifiability of the polynomial experts. Importantly, we will demonstrate in Theorem 2 that such parameter interactions lead to slow convergence rates for parameter estimation and expert estimation. Toward that goal, let us introduce the essential Voronoi loss function\n$\\mathcal{L}_{2,r}(G, G_*) := \\sum_{j=1}^{N} \\sum_{i \\in A_j} \\frac{1}{1 + \\exp(-c_i)} - \\frac{1}{1 + \\exp(-c_j^*)} + \\sum_{j=1}^{N} \\sum_{i \\in A_j} [||\\Delta A_{ij}|| + ||\\Delta b_{ij}|| + ||\\Delta a_{ij}|| + |\\Delta \\beta_{ij}|^r]$\n$+ \\sum_{j=N+1}^{N} \\sum_{i \\in A_j} [||\\Delta A_{ij}||^r + ||\\Delta b_{ij}||^r + |\\Delta c_{ij}|^r + ||\\Delta a_{ij}||^r + |\\Delta \\beta_{ij}|^r],$\nwhere we denote $\\Delta a_{ij} := a_i - a_j^*$ and $\\Delta \\beta_{ij} := \\beta_i - \\beta_j^*$.\nTheorem 2. Suppose that the expert function takes a polynomial form $E(x, \\alpha, \\beta) = (\\alpha^T x + \\beta)^p$, for some $p \\in \\mathbb{N}$. Then, for any $r \\geq 1$,\n$\\inf_{\\mathcal{G}_n \\in \\mathcal{M}_N(\\Theta)} \\sup_{\\mathcal{G} \\in \\mathcal{M}_N(\\Theta) \\setminus \\mathcal{M}_{N^*-1}(\\Theta)} \\mathbb{E}_{f_G} [\\mathcal{L}_{2,r}(\\hat{G}_n, G)] \\geq \\frac{1}{\\sqrt{n}},$\nwhere $\\mathbb{E}_{f_G}$ indicates the expectation taken w.r.t. the product measure with $f_G$.\nSample efficiency comparison under the sparse regime: According to the results in [1], the experts in the softmax self-attention share the same sample efficiency as those in the sigmoid version. In particular, strongly identifiable experts and polynomial experts need $O(\\epsilon^{-4})$ and $O(\\exp(\\epsilon^{-1/\\tau}))$ data points to attain the approximation error $\\epsilon$. Thus, we claim that the sigmoid self-attention is as sample-efficient as the softmax self-attention under the sparse regime of gating parameters. Note that since it is unlikely that all the gating parameters vanish in practice, the sparse regime is less popular than the dense regime, which will be studied in the next section."}, {"title": "4.2 Dense Regime of Gating Parameters", "content": "In this section, we now turn our attention to the dense regime where we assume that there exists some over-specified gating parameter different from zero, that is, $(\\hat{A}_{b^*}, \\hat{b}_{b^*}) \\neq (0_{d \\times d}, 0_d)$, for some $i \\in [N^*]$. As demonstrated in Section 3, the smallest distance between the regression function estimator $f_{\\hat{G}_n}$ and the set of the regression functions $f_{\\bar{G}}$ goes to 0 where $G \\in \\mathcal{M}_N(\\Theta) := \\arg \\min_{G \\in \\mathcal{M}_N(\\Theta) \\setminus \\mathcal{M}_{N^*}(\\Theta)} ||f_G - f_{G^*}||_{L^2(\\mu)}$. Without loss of generality (WLOG), we assume that\n$\\mathcal{G} := \\sum_{i=1}^N \\frac{1}{1 + \\exp(-\\bar{s}_i)} \\delta_{(\\bar{A}_i, \\bar{b}_i, \\bar{\\eta}_i)}.*$\nSimilar to the sparse regime of gating parameters, we also establish an identifiability condition on the expert function to avoid any interaction among the parameters expressed via PDEs under the dense regime. Since the expert function is required to satisfy only a subset of the strong identifiability condition in Definition 1 in this case, we refer to the condition as weak identifiability.\nDefinition 2 (Weak identifiability). We call an expert function $x \\rightarrow E(x, \\eta)$ weakly identifiable if it is differentiable w.r.t its parameter $\\eta$ for $\\mu$-almost all $x$ and, for any positive integer $l$ and any distinct parameters $\\{(A_i, b_i, c_i, \\eta_i)\\}_{i=1}^l$, the functions in the family\n$\\Bigg\\{\\frac{\\partial F}{\\partial A^{\\tau_1} \\partial b^{\\tau_2} \\partial c^{\\tau_3} \\partial \\eta^{\\tau_4}} (x, A_i, b_i, c_i, \\eta_i) : i \\in [l], \\sum_{j=1}^4 |\\tau_j| = 1\\Bigg\\}$\nare linearly independent, for $\\mu$-almost all $x$, where $(\\tau_1, \\tau_2, \\tau_3, \\tau_4) \\in \\mathbb{N}^{d \\times d} \\times \\mathbb{N}^d \\times \\mathbb{N} \\times \\mathbb{N}^q$.\nExamples. Strongly identifiable experts meet the weak identifiability condition. For instance, it can be verified that the previously mentioned two-layer neural networks of the form $E(x, (\\alpha, \\beta, \\lambda)) = \\lambda \\varphi(\\alpha x + \\beta)$, where $\\varphi$ is ReLU or GELU function, $\\alpha \\neq 0_d$, and $\\lambda \\neq 0$ are weakly identifiable. Moreover, polynomial experts $E(x, (\\alpha, \\beta)) = (\\alpha x + \\beta)^p$, for $p \\in \\mathbb{N}$, also satisfy the weak identifiability condition although they are not strongly identifiable.\nIn Theorem 3 below, we provide convergence rates for weakly identifiable experts based on the Voronoi loss\n$\\mathcal{L}_3(G,\\mathcal{G}) := \\sum_{j=1}^{N} \\sum_{i \\in A_j} [|| A_i - \\bar{A}_i|| + ||b_i - \\bar{b}_i|| + |c_i - \\bar{c}_i| + ||\\eta_i - \\bar{\\eta}_i||].$\nTheorem 3. If the function $x \\rightarrow E(x, \\eta)$ is weakly identifiable, then the lower bound $\\inf_{\\mathcal{G} \\in \\mathcal{M}_N(\\Theta)} ||f_G - f_{\\bar{G}}||_{L^2(\\mu)} \\geq \\mathcal{L}_3(G,\\mathcal{G})$ holds true for any mixing measure $G \\in \\mathcal{M}_N(\\Theta)$. As a consequence,\n$\\inf_{\\mathcal{G} \\in \\mathcal{M}_N(\\Theta)} \\mathcal{L}_3(\\hat{G}_n,\\mathcal{G}) = O_p(\\sqrt{\\log(n)/n}).$\nSample efficiency comparison under the dense regime: Recall that it costs strongly identifiable experts and polynomial experts in the softmax self-attention $O(\\epsilon^{-4})$ and $O(\\exp(\\epsilon^{-1/\\tau}))$ data points to reach the approximation error $\\epsilon$, respectively [1]. On the other hand, as those experts satisfy the weak identifiability condition, they need only a sample size of order $O(\\epsilon^{-2})$ to achieve the same error under the dense regime of sigmoid self-attention. For that reason, we claim that the sigmoid self-attention is more sample efficient than its softmax counterpart under the dense regime, which is more likely to occur in practice than the sparse regime."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Numerical Experiments", "content": "In the subsequent experiments, we compare sigmoid attention and softmax attention by viewing them as MoE models. In particular, we capture the empirical convergence rates of parameter estimation in sigmoid quadratic gating and softmax quadratic gating MoE models with two expert configurations: expert networks with ReLU activation (ReLU experts) and those with linear activation (linear experts).\nSetup. Synthetic data are generated from a softmax quadratic gating MoE model. Both sigmoid quadratic gating MoE and softmax quadratic gating MoE models are fitted to this data, with the sample size $n$ systematically varied. The empirical convergence rates are then evaluated for each model across the two expert configurations (ReLU experts and linear experts), providing insights into sigmoid quadratic gating performance. Please refer to Appendix E for further details regarding the values of the ground-truth parameters and the training procedure.\nResults. Figure 1 shows the empirical convergence rates of Voronoi loss for sigmoid and softmax quadratic gating MoE models, with error bars representing three standard deviations to account for variability across runs. In Figure 1a for MoE models with ReLU experts, we observe that the softmax quadratic gating MoE converges at the rate of order $O(n^{-0.24})$, whereas the sigmoid version achieves a significantly faster rate of order $O(n^{-0.51})$. Similarly, in Figure 1b for MoE models with linear experts, the sigmoid quadratic gating MoE attains a convergence rate of order $O(n^{-0.46})$, while the softmax counterpart exhibits a significantly slower convergence rate of order $O(n^{-0.07})$. These results empirically validate our theoretical findings that sigmoid quadratic gating MoE admits superior parameter estimation rates across both the configurations of ReLU experts and linear experts. In other words, we can conclude that the sigmoid self-attention is still more sample-efficient than the softmax self-attention on the empirical side."}, {"title": "5.2 Language Modeling Experiments", "content": "To demonstrate the practical efficacy of sigmoid attention, we evaluate its performance on a language modeling task in comparison with the widely used softmax attention.\nSetup. Our experiments closely follow the setup described in [48], using the Llama 2 architecture (1B parameters) [52] with ALiBi positional encoding [44] and a sequence length of 2048 tokens on the Red-Pajama dataset [10]. Performance is assessed across benchmarks including ARC (Easy/Challenge) [9], HellaSwag [59], PIQA [2], SciQ [55], and WinoGrande [49], testing reasoning, commonsense, and domain knowledge. Additionally, we evaluate computational efficiency by measuring throughput (tokens/sec), defined as the average number of forward passes completed per second during inference. Hyperparameters for training are detailed in the Appendix E.\nResults. As summarized in Table 2, sigmoid attention achieves a either comparable or superior"}, {"title": "6 Conclusion", "content": "In this paper, we first establish a mathematical connection between the self-attention mechanism in the Transformer architecture and the Mixture-of-Experts (MoE) model. Next, from the MoE perspective, we investigate the sample efficiency of the sigmoid self-attention by conducting a convergence analysis of parameter and expert estimation under the sigmoid gating MoE models with fully and partially quadratic affinity score functions. Considering both the sparse and dense regimes of gating parameters, our theories indicate that sigmoid self-attention is more sample efficient than the softmax version under the dense regime, which is more common in practice than the sparse regime. Furthermore, empirical results also demonstrate the potential of sigmoid self-attention as a competitive and efficient alternative to softmax self-attention, especially in resource-constrained settings. A limitation of our work is that we consider only a single attention head rather than a more popular multi-head attention mechanism [54", "41": "."}]}