{"title": "Text-guided Diffusion Model for 3D Molecule Generation", "authors": ["Yanchen Luo", "Junfeng Fang", "Sihang Li", "Zhiyuan Liu", "Jiancan Wu", "An Zhang", "Wenjie Du", "Xiang Wang"], "abstract": "The de novo generation of molecules with targeted properties is crucial in biology, chemistry, and drug discovery. Current generative models are limited to using single property values as conditions, struggling with complex customizations described in detailed human language. To address this, we propose the text guidance instead, and introduce TextSMOG, a new Text-guided Small Molecule Generation Approach via 3D Diffusion Model which integrates language and diffusion models for text-guided small molecule generation. This method uses textual conditions to guide molecule generation, enhancing both stability and diversity. Experimental results show TextSMOG's proficiency in capturing and utilizing information from textual descriptions, making it a powerful tool for generating 3D molecular structures in response to complex textual customizations.", "sections": [{"title": "INTRODUCTION", "content": "De novo molecule design, the process of generating molecules with specific, chemically viable structures for target properties, is a cornerstone in the fields of biology, chemistry, and drug discovery (Hajduk and Greer, 2007; Mandal et al., 2009; Pyzer-Knapp et al., 2015; Barakat et al., 2014). It not only allows for the creation of subject molecules but also provides insights into the relationship between molecular structure and function, enabling the prediction and manipulation of biological activity. Constrained by the immense diversity of chemical space, manually generating property-specific molecules remains a daunting challenge (Gaudelet et al., 2021). However, the generation of molecules that precisely meet specific requirements, including the creation of tailor-made molecules, is a complex task due to the vastness of the chemical space and the intricate relationship between molecular structure and function. Overcoming this challenge is crucial for advancing our understanding of biological systems and for the development of new therapeutic agents. In recent years, machine and deep learning methods have initiated a paradigm shift in the molecule generation (Alcalde et al., 2006; Anand et al., 2022; Mansimov et al., 2019; Zang and Wang, 2020; Satorras et al., 2021a; Gebauer et al., 2019; Liu et al., 2023a,c,b), which enable the direct design of 3D molecular geometric structures with the desired properties (Huang et al., 2023; Luo et al., 2021a; Mansimov et al., 2019). Notably, diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020), specifically equivariant diffusion models (Hoogeboom et al., 2022; Bao et al., 2023), have gradually enter the center of the stage with its outstanding performance. The core of this method is to introduce diffusion noise on molecular data, and then learn a reverse process in either unconditional or conditional manners to denoise this corruption, thereby crafting desired 3D molecular geometries. Meanwhile, some conditional inputs (e.g., polarizability a = 100 Bohr\u00b3) could be applied for constraining the model to generate more specific molecules types.\nHowever, despite the promise of these methods, a significant proportion of molecules gen-erated by diffusion models do not meet the practical needs of researchers. For instance, they may lack the desired biological activity, exhibit poor pharmacokinetic properties, or be syn-thetically infeasible. This would be due to the fact that, on one hand, searching for suitable molecules in drug design typically requires consideration of multiple properties of interest (e.g., simultaneously characterized by specific polarizability, orbital energy, properties like aromatic-ity, and distinct functional groups) (Hon\u00f3rio et al., 2013; Gebauer et al., 2022; Lee and Min, 2022). On the other hand, humans seem to struggle with conveying their needs precisely to the model. While a text segment such as \"This molecule is an aromatic compound, with small HOMO-LUMO gaps and possessing at least one carboxyl group\" can accurately describe human requirements and facilitate communication among humans, it is still challenging to directly con-vey this 'thoughts' to the model. Therefore, we aspire to develop a method that allows for the interactive inverse design of 3D molecular structures through natural language. In other words, we aim to create a system where researchers can describe the properties they want in a molecule using natural language, and the system will generate a molecule that meets these requirements. This aspiration prompts us to explore text guidance in diffusion models, emphasizing the ne-cessity for models adept at precise language understanding and molecule generation.\nTowards this end, we propose TextSMOG, a new text-guided small molecule generation approach. The basic idea is to combine the capabilities of the advanced language models (Devlin et al., 2019; Liu et al., 2019; Beltagy et al., 2019; Raffel et al., 2020; Brown et al., 2020;"}, {"title": "RESULTS", "content": "In this section, we present the architecture and the experimental results of our proposed TextSMOG model, showcasing its ability to generate molecules with desired properties."}, {"title": "Architecture", "content": "To evaluate our model, we employ the QM9 dataset (Ramakrishnan et al., 2014), which is a stan-dard benchmark containing quantum properties and atom coordinates of over 130K molecules, each with up to 9 heavy atoms (C, N, O, F). For the purpose of training our model under the condition of textual descriptions, we have curated a subset of molecules from QM9 and"}, {"title": "Experiment on Single Quantum Properties Conditioning", "content": "Following EDM (Hoogeboom et al., 2022), we first evaluate our TextSMOG on the task of generating molecule conditioning on a single desired quantum property in QM9. Then we compare our TextSMOG with several baselines to demonstrate the effectiveness of our model on single quantum properties conditioning molecule generation.\nSetup. We follow the same data preprocessing and partitions as in EDM (Hoogeboom et al., 2022), which results in 100K/18K/13K molecule samples for training/validation/test respectively. In order to assess the quality of the conditional generated molecules w.r.t. to the desired properties, we use the property classifier network \\(o_p\\) introduced by (Satorras et al., 2021b). Then for the impartiality, the training partition is further split into two non-overlapping halves \\(D_a\\) and \\(D_b\\) of 50K molecule samples each. The property classifier network \\(o_p\\) is trained on the first half \\(D_a\\), while our TextSMOG is trained on the second half \\(D_b\\). This ensures that there is no information leak and the property classifier network \\(o_p\\) is not biased towards the generated molecules from TextSMOG. Then \\(o_p\\) is evaluated on the generated molecule samples from TextSMOG as we introduce in the following.\nMetrics. Following (Hoogeboom et al., 2022), we use the mean absolute error (MA\u0395) between the properties of generated molecules and the ground truth as a metric to evaluate how the generated molecules align with the condition (see the supplementary information for details). We generate 10K molecule samples for the evaluation of \\(o_p\\), following the same protocol as in EDM. Additionally, we then measure novelty (Simonovsky and Komodakis, 2018), atom stability (Hoogeboom et al., 2022), and molecule stability (Hoogeboom et al., 2022) to demonstrate the fundamental molecule generation capacity of the model (also see the supplementary information for details).\nBaseline. We compare our TextSMOG with a direct baseline conditional EDM (Hoogeboom et al., 2022) and a recent work EEGSDE which takes energy as guidance (Bao et al., 2023). We also compare two additional baselines \u201cU-bound\" and \"#Atoms\" introduced by (Hoogeboom et al., 2022). In the \"U-bound\" baseline, any relation between molecule and property is ignored, and the property classifier network \\(o_p\\) is evaluated on \\(D_b\\) with shuffled property labels. In the \"#Atoms\" baseline, the properties are predicted solely based on the number of atoms in the molecule. Furthermore, we report the error of \\(o_p\\) on \\(D_b\\) as a lower bound baseline \u201cL-Bound\u201d.\nResults. We generate molecules with textual descriptions targeted to each one of the six properties in QM9, which are detailed in the supplementary information. As presented in Figure 2, our TextSMOG has a lower MAE than other baselines on five out of the six properties, suggesting that the molecules generated by TextSMOG align more closely with the desired properties than other baselines. The result underscores the proficiency of TextSMOG in exploiting textual data to guide the conditional de novo generation of molecules. Moreover, it highlights the superior congruence of the text-guided molecule generation via the diffusion model with the desired property, thus showing significant potential. Furthermore, as indicated in Figure 3, our proposed TextSMOG exhibits commendable performance in terms of novelty and stability. The text guidance we introduced has transformed the exploration of the model in the molecule generation space, generally enhancing the novelty of the generated molecules while maintaining their stability."}, {"title": "Experiment on Multiple Quantum Properties Conditioning", "content": "The capacity to generate molecules, guided by multiple conditions, is a crucial aspect of the molecule generation model. When guided by textual descriptions, characterizing the condition with multiple desired properties is highly intuitive and flexible. Following the same setup and"}, {"title": "Generation on General Textual Descriptions", "content": "To further assess our model, we undertake additional training on a vast dataset of over 330K text-molecule pairs we gleaned from PubChem (Kim et al., 2021). Then, we generate molecules based on general textual descriptions to observe the capacity of our model to generate from generalized textual conditions.\nVisual observations, as depicted in Figure 4, illuminate the impressive aptitude of our TextSMOG in aligning molecule structures with the desired property within the textual de-scriptions. For instance, when the textual description includes affirmatively mentioned terms such as \"simple chain structure\", \"at least one carboxyl group\", and \"soluble in water\", the generated molecules consistently exhibit chain structures with at least one carboxyl group, and"}, {"title": "DISCUSSION", "content": "The translational impacts of TextSMOG are particularly significant for the field of drug dis-covery and materials science. By enabling the generation of molecular structures directly from textual descriptions, TextSMOG can streamline the early stages of drug design where rapid pro-"}, {"title": "LIMITATIONS OF STUDY", "content": "The integration of textual information with the denoising process of a pre-trained equivariant diffusion model allows TextSMOG to generate valid and stable molecular conformations that closely align with diverse textual directives. This initial success paves the way for significant advancements in the exploration of chemical space and the development of compounds. Never-theless, our findings are not without limitations.\nOur work was constrained by the scarcity of high-quality data linking real-world 3D molecules to their corresponding textual descriptions. This limitation impacted our ability to fully train the model on a diverse set of text-3D molecule pairs, potentially affecting the accuracy of the generated molecules in generating molecules that accurately align with complex textual descrip-tions. Moreover, the relative slowness of the sampling process due to the iterative nature of the total diffusion steps can pose a challenge in scenarios requiring rapid molecule generation, such as high-throughput drug discovery or material design.\nIn addition to these limitations, the current design of TextSMOG necessitates that the properties to condition on must be known upfront during the training phase. This might not always be feasible in practical settings, where specific properties linked to a particular drug discovery target may only become available later on, and often with very limited sample data. The generalization of TextSMOG to more complex and real-world scenarios also needs further exploration.\nLooking ahead, we are optimistic about the potential of text-guided 3D molecule generation to revolutionize drug discovery and related fields. Future work will focus on overcoming these challenges by expanding and enhancing the quality of datasets linking textual descriptions to molecular structures, improving the efficiency of the sampling process, and making TextSMOG more adaptable to real-world applications. Addressing these limitations will not only enhance the performance of TextSMOG but also contribute significantly to the advancement of text-guided molecule generation technology."}, {"title": "STAR Methods", "content": "In this section, we elaborate on the proposed text-guided small molecule generation approach via diffusion model (TextSMOG), as illustrated in Figure 1. It integrates the textual information (i.e., text guidance) into the conditional signal of diffusion models by employing the reference geometry that is described in the first subsection following. Subsequently, we introduce an efficient learning approach that incorporates both the encoded conditional signal and pre-trained unconditional signal in the reverse process, to generate molecules that are not only structurally stable and chemically valid but also align well with the specified conditions, as presented in the second subsection."}, {"title": "Notation and Background", "content": "We begin with a background of diffusion-based 3D molecule generation, introducing the funda-mental concepts of the diffusion model and delving into equivariant diffusion models. See the comprehensive literature review on these topics in the Section Related works in Supplemen-tary Information. In accordance with prior studies (Hoogeboom et al., 2022; Bao et al., 2023; Huang et al., 2023), we use the variable G = (x, h) to represent the 3D molecular geometry. Here x = (x_1,..., x_M) \\in \\mathbb{R}^{M\\times3} signifies the atom coordinates, while h = (h_1,..., h_M) \\in \\mathbb{R}^{M\\times k} denotes the atom features. These features encompass atom types and atom charges, characterizing the atomic properties within the molecular structure."}, {"title": "Diffusion Model", "content": "The diffusion model (Sohl-Dickstein et al., 2015; Ho et al., 2020) emerges as a leading generative model, having achieved great success in various domains (Dhariwal and Nichol, 2021; Rombach et al., 2022; Ruiz et al., 2023; Song et al., 2021; Saharia et al., 2023; Schneider, 2023). Typically, it is formulated as two Markov chains: a forward process"}, {"title": "Forward Process", "content": "Given the real 3D molecular geometry \\(G_0\\), the forward process yields a sequence of intermediate variables \\(G_1,\\dots,\\dots, G_T\\) using the transition kernel \\(q(G_t|G_{t-1})\\) in alignment with a variance schedule \\(\\beta_1, \\beta_2, ..., \\beta_T \\in (0,1)\\). Formally, it is expressed as:\n\\begin{equation}\nq(G_t|G_{t-1}) = N(G_t|\\sqrt{1 - \\beta_t}G_{t-1}, \\beta_tI_n),\n\\end{equation}\nwhere \\(N(\\cdot,\\cdot)\\) is a Gaussian distribution and \\(I_n\\) is the identity matrix. This defines the joint distribution of \\(G_1,\\dots,\\dots,G_T\\) conditioned on \\(G_0\\) using the chain rule of the Markov process:\n\\begin{equation}\nq(G_1,\\dots, G_T|G_0) = \\prod_{t=1}^{T} q(G_t|G_{t-1}).\n\\end{equation}\nLet \\(\\alpha_t = 1 - \\beta_t\\) and \\(\\bar{\\alpha}_t := \\prod_{s=1}^{t} \\alpha_s\\). The sampling of \\(G_t\\) at time step t is in a closed form:\n\\begin{equation}\nq(G_t|G_0) = N(G_t|\\sqrt{\\bar{\\alpha}_t}G_0, (1 -\\bar{\\alpha}_t)I_n).\n\\end{equation}\nAccordingly, the forward process posteriors, when conditioned on \\(G_0\\), are tractable as:\n\\begin{equation}\nq(G_{t-1}|G_t, G_0) = N(G_{t-1}|\\mu(G_t, G_0), \\beta_tI_n),\n\\end{equation}\nwhere\n\\begin{equation}\n\\mu(G_t, G_0) = \\frac{\\sqrt{\\alpha_{t-1}}\\beta_t}{1-\\bar{\\alpha}_t} G_0 + \\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t} G_t, \\beta_t = \\frac{1-\\alpha_{t-1}}{1-\\bar{\\alpha}_t} \\beta_t.\n\\end{equation}"}, {"title": "Reverse Process", "content": "To recover the original molecular geometry \\(G_0\\), the diffusion model starts by generating a standard Gaussian noise \\(G_T \\sim N(0, I_n)\\), then progressively eliminates noise through a reverse Markov chain. This is characterized by a learnable transition kernel \\(P_\\theta(G_{t-1}|G_t)\\) at each reverse step t, defined as:\n\\begin{equation}\nP_\\theta(G_{t-1}|G_t) = N(G_{t-1}|\\mu_\\theta(G_t, t), \\Sigma_\\theta(G_t, t)),\n\\end{equation}\nwhere the variance \\(\\Sigma_\\theta(G_t,t) = \\beta_tI_n\\) and the mean \\(\\mu_\\theta(G_t, t)\\) is parameterized by deep neural networks with parameters \\(\\theta\\):\n\\begin{equation}\n\\mu_\\theta(G_t, t) = \\frac{1}{\\sqrt{\\alpha_t}}\\left(G_t - \\frac{\\sqrt{1-\\alpha_t}}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta(G_t, t)\\right) = \\frac{1}{\\sqrt{\\alpha_t}}\\left(G_t - \\frac{\\sqrt{1-\\alpha_t}}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta(G_t, t)\\right),\n\\end{equation}\nwhere \\(\\epsilon_\\theta\\) is a noise prediction function to approximate the noise \\(\\epsilon\\) from \\(G_t\\).\nWith the reverse Markov chain, we can iteratively sample from the learnable transition kernel \\(P_\\theta(G_{t-1}|G_t)\\) until t = 1 to estimate the molecular geometry \\(G_0\\)."}, {"title": "Equivariant diffusion models", "content": "The molecular geometry G = (x,h) is inherently symmet-ric in 3D space that is, translating or rotating a molecule does not change its underlying structure or features. Previous studies (Thomas et al., 2018; Fuchs et al., 2020; Finzi et al., 2020) underscore the significance of leveraging these invariances in molecular representation learning for enhanced generalization. However, the transformation of these higher-order repre-"}, {"title": "Equivariant Diffusion Model for Molecule Generation", "content": "Diffusion models, formulated as two Markov chains a forward process that gradually injects noise into the data and a reverse process that learns to recover the original data have been successfully applied to various domains, including molecule generation. This process is partic-ularly effective in the context of molecule generation, where the forward process adds noise to the molecular geometry at each step until it is fully noise-corrupted. The reverse process then gradually denoises the initial geometry \\(G_T\\) to generate the final molecular geometry \\(G_0\\).\nHowever, molecular geometries are inherently symmetric in 3D space translations or ro-tations do not change their underlying structure or features. To take advantage of these in-variances for improved generalization, we employ an equivariant diffusion model (EDM). The EDM ensures both rotational and translational invariance by predicting only the deviations in coordinate with a zero center of mass and making the noise prediction network \\(\\epsilon_\\theta(\\cdot)\\) equivariant to orthogonal transformations. This allows the model distribution p(G) to remain invariant to the Euclidean group E(3), meaning identical molecules in different orientations correspond to the same distribution.\nIn this work, the integration of textual information into the conditional signal of the equiv-ariant diffusion model is achieved by employing a reference geometry \\(c_p\\) that is updated at each step based on the textual prompt P."}, {"title": "Integrating Textual Prompts into 3D Molecular Reference Geometry", "content": "To ensure high-fidelity 3D molecule generation, the reverse process of the diffusion model is typically guided by tailored conditional information representing desired properties like unique polarizability. We represent this conditional information as c, which allows us to formulate the conditional reverse process as:\n\\begin{equation}\nP_\\theta(G_{t-1}|G_t, c) = N(G_{t-1}|\\mu_\\theta(G_t, c, t), \\beta_tI_n)\n\\end{equation}\nUnlike previous approaches relying on limited value guidance (i.e., property values), in this work, we aim to steer the reverse process with text guidance (i.e., informative textual descrip-tions), which can convey a broader range of conditional requirements. Intuitively, utilizing textual descriptions to specify conditional generation criteria not only provides greater expres-sivity but also better aligns the resulting 3D molecules with diverse and complex expectations.Practically, we first introduce a textual prompt P describing desired 3D molecule properties. A multi-modal conversion module \u0393, pre-trained on 300K text-molecule pairs from PubChem, is then employed. This module is comprised of a GIN molecular graph encoder (Xu et al., 2019; Liu et al., 2022) and a language encoder-decoder extended from BERT (Devlin et al., 2019; Zeng et al., 2022). It converts P into a reference geometry \\(c_p\\), extracting specific information from the target conditions and refining the textual condition signal:\n\\begin{equation}\nc_p = \\Gamma(P).\n\\end{equation}\nNevertheless, we should emphasize that valid and stable 3D molecules can hardly be obtained directly from \\(c_p\\). The chemical fidelity in 3D molecular space may not be guaranteed. In what follows, we describe how to utilize \\(c_p\\) for conditioning a pre-trained diffusion model to generate molecules that align with the desired properties, meanwhile alleviating the exhaustive training from scratch."}, {"title": "Conditioning with the Reference of Text Guidance", "content": "To leverage \\(c_p\\) for text-guided conditional generation while preserving the validity and stabil-ity of the synthesized molecule, TextSMOG employs the iterative latent variable refinement (ILVR) (Choi et al., 2021) to condition a pre-trained unconditional diffusion model meanwhile maintaining inherent domain knowledge in the unconditional model.\nWith the pre-trained unconditional diffusion model EDM (Hoogeboom et al., 2022), we could perform a step-by-step reverse process. Formally, at step t, we can sample an unconditional proposal molecular geometry:\n\\begin{equation}\nG_{t-1}~P(G_{t-1}/G_t).\n\\end{equation}\nwhere \\(G_\\) is the fixed parameters of the pre-trained unconditional diffusion model (Hoogeboom et al., 2022). Then, to incorporate the condition signal \\(c_p\\) in the reverse process, we introduce a linear operation \\(\\varphi(\\cdot)\\). Therefore the conditional denoising for one step at step t can be formulated as:\n\\begin{equation}\nG_{t-1} = \\varphi(c_p) + (I -\\varphi)(G_{t-1}),\n\\end{equation}\nwhere \\(I(\\cdot)\\) is the identity operation and \\((I-\\varphi)(\\cdot)\\) is the residual operation w.r.t. \\(\\varphi(\\cdot)\\) (James and Wilkinson, 1971). Accordingly, the condition signal \\(c_p\\) is projected into the reverse denois-"}, {"title": "Training Objective", "content": "To guarantee the quality of the generated molecules, the key lies in optimizing the variational lower bound (ELBO) of negative log-likelihood, which equals minimizing the Kullback-Leibler divergence between the joint distribution of the reverse Markov chain \\(p_\\theta(G_0, G_1,\\dots, G_T)\\) and the forward process \\(q(G_0, G_1,\\dots,\\dots, G_T)\\):\n\\begin{equation}\nE [-\\log p_\\theta(G_0 c_p)] <= \\sum_{t\\geq 1} D_{KL} (q(G_{t-1}|G_t, G_0)||P_\\theta(G_{t-1}|G_t, c_p)) +C,\n\\end{equation}\nwhere C is a constant independent of \\(\\theta\\).\nNote that we set \\(L_0 = -\\log p_\\theta(G_0|G_1)\\) as a discrete decoder following (Ho et al., 2020). Further adopting the reparameterization from (Ho et al., 2020), \\(L_{t-1}\\) can be simplified to:\n\\begin{equation}\nL_{t-1} = E_{P,G_0,\\epsilon} [||\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t}G_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t, c_p)||^2] .\n\\end{equation}"}, {"title": "Evaluation metrics", "content": "Mean absolute error (MAE). (Willmott and Matsuura, 2005) is a measure of errors between paired observations. Given the property classifier network \\(\\phi_p\\), and the set of generated molecules G, the MAE is defined as:\n\\begin{equation}\nMAE  = \\frac{1}{|G|} \\sum_{G \\in G}|\\phi_p(G) - c_g|,\n\\end{equation}\nwhere G is the generated molecule, and of which \\(c_g\\) is the desired property.\nNovelty. (Simonovsky and Komodakis, 2018) is the proportion of generated molecules that do not appear in the training set. Specifically, let G be the set of generated molecules, the novelty in our experiment is calculated as:\n\\begin{equation}\nNovelty = \\frac{|G \\cap D_b|}{|G|}\n\\end{equation}\nAtom stability. (Hoogeboom et al., 2022) is the proportion of the atoms in the generated molecules that have the right valency. Specifically, the atom stability in our experiment is calculated as:\n\\begin{equation}\nAtom Stability = \\frac{\\sum_{\\zeta \\in A_G} A_{G, stable}}{|A_G|},\n\\end{equation}\nwhere Ag is the set of atoms in the generated molecule G, and Ag,stable is the set of atoms in Ag that have the right valency."}, {"title": "Molecule stability.", "content": "(Hoogeboom et al., 2022) is the proportion of the generated molecules where all atoms are stable. Specifically, the molecule stability in our experiment is calculated as:\n\\begin{equation}\nMolecule Stability = \\frac{|G_{stable}|}{|G|},\n\\end{equation}\nwhere Gstable is the set of generated molecules where all atoms have the right valency."}, {"title": "Quantification and Statistical Analysis", "content": "We consider 6 main quantum properties in QM9:\n\nCv: Heat capacity at 298.15K.\n\n\u03bc: Dipole moment.\n\na: Polarizability, which represents the tendency of a molecule to acquire an electric dipole moment when subjected to an external electric field.\n\nHOMO: Highest occupied molecular orbital energy.\n\nELUMO: Lowest unoccupied molecular orbital energy.\n\n\u0394\u03b5: The energy gap between HOMO and LUMO."}]}