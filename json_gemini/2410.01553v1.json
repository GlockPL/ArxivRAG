{"title": "MedQA-CS: Benchmarking Large Language Models Clinical Skills Using an AI-SCE Framework", "authors": ["Zonghai Yao", "Zihao Zhang", "Chaolong Tang", "Xingyu Bian", "Youxia Zhao", "Zhichao Yang", "Junda Wang", "Huixue Zhou", "Won Seok Jang", "Feiyun Ouyang", "Hong Yu"], "abstract": "Artificial intelligence (AI) and large language models (LLMs) in healthcare require advanced clinical skills (CS), yet current benchmarks fail to evaluate these comprehensively. We introduce MedQA-CS, an AI-SCE framework inspired by medical education's Objective Structured Clinical Examinations (OSCEs), to address this gap. MedQA-CS evaluates LLMs through two instruction-following tasks\u2014LLM-as-medical-student and LLM-as-CS-examiner-designed to reflect real clinical scenarios. Our contributions include developing MedQA-CS, a comprehensive evaluation framework with publicly available data and expert annotations, and providing the quantitative and qualitative assessment of LLMs as reliable judges in CS evaluation. Our experiments show that MedQA-CS is a more challenging benchmark for evaluating clinical skills than traditional multiple-choice QA benchmarks (e.g., MedQA). Combined with existing benchmarks, MedQA-CS enables a more comprehensive evaluation of LLMs' clinical capabilities for both open- and closed-source LLMs", "sections": [{"title": "Introduction", "content": "Artificial intelligence (AI) and large language models (LLMs) are increasingly adopted in healthcare, resulting in many clinical NLP applications that require expert-level clinical skills such as diagnosis and clinical documentation (Achiam et al., 2023; McDuff et al., 2023; Tu et al., 2024; Yang et al., 2024). Current clinical LLM benchmarks, such as MMLU-Med (Hendrycks et al., 2020), MedQA-US (Jin et al., 2021), MedMCQA (Pal et al., 2022), PubMedQA (Jin et al., 2019), JAMA Clinical Challenge, Medbullets (Chen et al., 2024), and AM-BOSS (Gilson et al., 2023), mainly measure clinical knowledge through multiple-choice questions (MCQ). However, identifying robust clinical guidelines and what constitutes a successful interaction for healthcare LLMs will be crucial towards fulfilling the long-term goals of patients, providers, and other clinical stakeholders (Mehandru et al., 2024). In medical education, there has been a shift from assessing students using standardized testing, which evaluates clinical knowledge through MCQs, to modern curricula, which increasingly use Objective Structured Clinical Examination (OSCE) (Zayyan, 2011; Harden et al., 2015). As shown in Figure 1, Miller's Pyramid (Miller, 1990) provides a comprehensive framework for evaluating the competence of medical students, from knowledge acquisition to real-world performance (Norcini, 2003; Albino et al., 2008). Early medical exams have typically evaluated students on the \"knows\" and \"knows how\" levels of Miller's Pyramid, while OSCEs primarily evaluate students' practical skills (e.g., the \"shows how\" level) in clinical settings, including patient examination, clinical history recording, ef-"}, {"title": "MedQA-Clinical Skills Benchmark", "content": "The official USMLE website provides many publicly available study materials for the USMLE exam. Among these, Step 1, Step 2 CK, and Step 3 are in the format of multiple-choice questions and have been collected and integrated into the MedQA dataset (Jin et al., 2021). These are publicly available resources on the Internet. Similarly, we obtained the USMLE Step 2 CS guidelines from publicly available Internet resources, which contain 44 carefully designed cases. We manually converted the content of the cases from the original PDF files into txt files. Based on the input-output format described in appendix D, we processed the cases into (instruction, input, output) format and saved them as JSON files. After preprocessing, MedQA-CS comprises 1667 (instruction, input, output) data points and four sections: InfoGatherQA (physician-asked questions for gathering patient information through conversation), Physical Exams, Closures, and Differential Diag-"}, {"title": "MedStuLLM and MedExamLLM", "content": "The InfoGatherQA section simulates patient encounters, requiring the MedStuLLM to ask focused questions based on initial doorway information and prior conversation history to gather relevant details about the patient's condition. As the example shown in Table 1, given the doorway information of \"Joseph Shorr, a 46-year-old male with chest pain and vital signs (BP: 165/85 mm", "subsections": [{"title": "Information Gathering through Conversation (InfoGatherQA)", "content": "The InfoGatherQA section simulates patient encounters, requiring the MedStuLLM to ask focused questions based on initial doorway information and prior conversation history to gather relevant details about the patient's condition. As the example shown in Table 1, given the doorway information of \"Joseph Shorr, a 46-year-old male with chest pain and vital signs (BP: 165/85 mm"}, {"title": "Physical Exams", "content": "The Physical Exams section of the MedQA-CS benchmark assesses the ability of the MedStuLLM to document and justify physical examinations during a patient encounter. After completing the initial patient interaction, the MedStuLLM is required to write down a detailed physical exam based on the doorway information and the chat history. As the example shown in Table 2, if a patient presents with chest pain, the MedStuLLM might document \"Heart: Auscultation for possible abnormal heart sounds and rhythm such as murmurs, gallop sound, or arrhythmias\" and explain that \"The patient is presenting with chest pain, shortness of breath, and sweating which can indicate a cardiac issue like angina or a heart attack.\" This documentation is grounded in the patient's symptoms and medical history. The MedExamLLM in this section evaluates these documented examinations by comparing them to a ground truth answer using a specified rubric. This evaluation includes three main criteria: Exam Coverage, Reason Relevance and Accuracy, and Extra Exams Penalty. For example, if the MedStuLLM documented heart and chest exams but missed necessary components like the neck or abdominal exams, the MedExamLLM would score the response lower for Exam Coverage. The MedExamLLM output is detailed in JSON format, and the final overall score range is from 0 to 100. Scores for the MedStuLLM reflect its ability to cover necessary examinations, provide accurate and relevant justifications, and avoid unnecessary tests. The MedExamLLM's score indicates how closely its evaluations align with those of human experts."}, {"title": "Closure", "content": "The Closure section evaluates the MedStuLLM's ability to effectively conclude patient encounters. This involves summarizing the patient's chief complaint, history of present illness (HPI), and findings from physical examinations, as well as outlining the next steps in management and addressing any challenging questions the patient may have. As the example shown in Table 3, after assessing a patient like Mr. Shorr with severe chest pain, the MedStuLLM might summarize: \"Mr. Shorr, based on your recent episode of severe chest pain that woke you up from sleep, and that it radiated to your neck, upper back, and left arm, alongside your reported hypertension, high cholesterol, and past GERD, coupled with your physical exam that did not show any major abnormal findings, we are tentatively considering a few possibilities.\" The MedExamLLM evaluates these closure summaries generated by the MedStuLLM based on a rubric adapted from the USMLE guidelines. This rubric includes five main criteria: Diagnostic Impres-sions, Management Plans, Challenging Ques-"}, {"title": "Differential Diagnosis", "content": "The Differential Diagnosis section in the MedQA-CS benchmark assesses the MedStuLLM's ability to formulate and justify potential diagnoses based on information gathered during the patient encounter. This involves synthesizing all previous information from the InfoGatherQA stage, physical examinations, and initial patient notes to propose a list of possible medical conditions. As the example shown in Table 4, given a patient like Joseph Shorr with chest pain and relevant clinical data, the MedStuLLM must identify differential diagnoses, providing historical and physical findings that support each diagnosis. Each diagnosis is evaluated for accuracy and supported by evidence from the patient's history and examination. For example, the MedStuLLM might output: \u201cDiagnosis: Acute Coronary Syndrome. Historical Finding(s): Substernal chest pain radiating to the left arm, upper back, and neck. Associated symptoms of nausea, sweating, and dyspnea. History of hypertension and high cholesterol.\u201d The MedExamLLM eval-uates the differential diagnoses proposed by the MedStuLLM using a detailed rubric. The evalu-ation focuses on three main criteria: correctness of the diagnosis name, relevance of historical"}]}, {"title": "Quality Evaluation", "content": "The reliability of the MedQA-CS design was evaluated through the agreement among three experts who assessed the MedStuLLM (GPT-4) results across four sections. Detailed information about the human annotation guidelines derived from MedExamLLM prompts, as well as the recruitment and guidance of domain experts for the evaluation, is provided in Appendix E. Our goal was to validate the MedQA-CS MedStuLLM and MedExamLLM design from the perspective of domain experts. If experts can follow each requirement of MedExam-LLM to evaluate MedStuLLM's output and achieve highly consistent results, it confirms the soundness of our MedQA-CS design details. This evaluation employed Pearson's r and Kendall's \u03c4 to measure correlation and consistency between the expert pairs. Pearson's r values ranged from 0.77 to 0.99, indicating strong to very strong correlations in all sections, with highly significant p-values (most p < 0.001). Kendall's \u03c4 values, ranging from 0.54 to 0.90, further support the consistency of the experts' evaluations. The Kendall's W values, representing the overall agreement among the three experts, were all significant, ranging from 0.78 to 0.91 (with p-values < 0.05), indicating substantial agreement. The high correlations and consistent evaluations across different sections demonstrate that the experts' assessments of the MedStuLLM outputs are highly reliable, confirming the effectiveness of the MedExamLLM design in providing consistent and accurate evaluations."}, {"title": "Experiments", "content": "We focus on the following two research questions (RQ): RQ1: Assessing the reliability of LLMs as judges in the MedQA-CS context (for MedExamLLM). This involves benchmarking various LLMs' MedExamLLM capabilities and evaluating AI-expert agreements when reviewing Med-StuLLM (GPT-4) results. RQ2: Utilizing the most reliable MedExamLLM as an automatic metric to benchmark the clinical skills of various LLMs in critical instruction-following tasks across different sections (for MedStuLLM).\nThe LLMs includeding the experiments are the GPT series (GPT-3.5-turbo, GPT-4-turbo, GPT-4o) (Achiam et al., 2023), the Claude-3 series (Claude-3-haiku, Claude-3-sonnet, Claude-3-opus, Claude-3.5-sonnet) (Anthropic, 2024), and some representative open-source general LLMs (LLAMA2 (Touvron et al., 2023), LLAMA3 (Meta, 2024), Mistral&Mixtral (Jiang et al., 2024), GLM-4 (Zeng et al., 2023), and Qwen2 (Bai et al., 2023)).\nIn RQ1 settings, we use all default parameters in their official API with temperature=0 for GPT"}, {"title": "Limitations and Societal Impacts", "content": "This study has several limitations.\nFirst, the small sample size derived from the USMLE Step 2 CS may not comprehensively represent all clinical medicine disciplines or clinical skills, limiting the generalizability of our findings. Future studies should involve larger datasets encompassing diverse medical domains to validate these results more broadly.\nSecond, the LLM-as-Judge in this paper did not consider MedStuLLM's reasoning process during evaluation. We found it difficult to produce stable and reliable scores for the reasoning provided by different MedStuLLMs without ground truth reasoning. We plan to explore reference-free clinical reasoning evaluation in future work.\nThird, it is important to note that clinical skills typically encompass treatment plan skills. However, due to limitations in the original USMLE Step 2 CS dataset (as illustrated in Figure 2), the USMLE only evaluates medical students' clinical skills up to the diagnostic part of clinical note generation, without extending to treatment plans for each diagnosis. This is why our benchmark does not include this aspect. In the future, we aim to explore how to gather suitable treatment plan data from other sources to integrate into MedQA-CS.\nAdditionally, all MedQA-CS data were presented in English, limiting its applicability in non-English-speaking contexts. The dataset was also constrained to a single modality, using only text-based inputs and outputs. Future work should investigate the inclusion of multimodal data, such as speech or images, to better reflect real-world clinical interactions.\nMoreover, this research exclusively addresses tasks related to medical visits, such as information gathering, question answering, physical examination recommendations, closure, and differential diagnosis. The extension of our findings to other domains and tasks remains unexplored, indicating that further validation and adjustments will be nec-essary before applying this approach to different fields.\nFinally, although we employed three medical experts for human evaluation, increasing the number of qualified domain experts would improve the statistical significance and robustness of our findings. Future work should consider expanding the pool of experts and addressing issues of fairness, generalizability, and potential biases inherent in LLMs.\nRegarding societal impacts, The scores in our benchmark do not suggest that LLMs have the clinical skills needed to replace physicians or medical students. Although some LLMs performed well in specific MedQA-CS sections, their clinical skills and potential as clinical examiners remain untested. More complex clinical cases are required to val-idate their capabilities. The MedQA-CS bench-mark primarily assesses an LLM's ability to follow instructions and generate text, not the decision-making or real-world judgment needed in medical"}, {"title": "Conclusion", "content": "MedQA-CS offers a novel AI-SCE framework, emphasizing the critical need for clinical skills benchmarks and showcasing the potential of LLMs as reliable CS judges in relevant NLP tasks. This framework introduces a more rigorous evaluation approach compared to traditional benchmarks, ensuring a more accurate assessment of LLMs' clinical capabilities. By integrating real clinical scenarios and expert annotations, MedQA-CS provides a comprehensive and publicly accessible tool for advancing AI-based evaluations in healthcare."}]}