{"title": "VISION-XL: High Definition Video Inverse Problem Solver using Latent Image Diffusion Models", "authors": ["Taesung Kwon", "Jong Chul Ye"], "abstract": "In this paper, we propose a novel framework for solving high-definition video inverse problems using latent image diffusion models. Building on recent advancements in spatio-temporal optimization for video inverse problems using image diffusion models, our approach leverages latent-space diffusion models to achieve enhanced video quality and resolution. To address the high computational demands of processing high-resolution frames, we introduce a pseudo-batch consistent sampling strategy, allowing efficient operation on a single GPU. Additionally, to improve temporal consistency, we present batch-consistent inversion, an initialization technique that incorporates informative latents from the measurement frame. By integrating with SDXL, our framework achieves state-of-the-art video reconstruction across a wide range of spatio-temporal inverse problems, including complex combinations of frame averaging and various spatial degradations, such as deblurring, super-resolution, and inpainting. Unlike previous methods, our approach supports multiple aspect ratios (landscape, vertical, and square) and delivers HD-resolution reconstructions (exceeding 1280\u00d7720) in under 2.5 minutes on a single NVIDIA 4090 GPU.", "sections": [{"title": "1. Introduction", "content": "Diffusion models [6, 9, 10, 17, 19, 21, 23] have set a new benchmark in generative modeling, enabling the generation of high-quality samples. These models have become the foundation for advancements in various fields, such as controllable image editing [34], image personalization [8], synthetic data augmentation [24], and even reconstructing images from brain signals [14, 25].\nFurthermore, diffusion model-based inverse problem solvers (DIS) [2, 4, 11, 22, 28, 30] address a variety of image restoration tasks, such as deblurring, super-resolution, inpainting, colorization, compressed sensing, and so on. A key feature of DIS is its plug-and-play capability, allowing diffusion models to be applied flexibly across different inverse problems without requiring task-specific training or fine-tuning.\nRecently, several extensions [5, 13, 33] from the DIS have been proposed to solve video inverse problems using image diffusion models. Naive application of image diffusion models to videos may break temporal consistency. To address this problem, these methods preserve temporal consistency by utilizing a batch-consistent sampling strategy [13] and applying optical flow guidance to warp either latent representations [33] or the noise prior [5].\nAlthough these innovative approaches enable powerful image generative models [6, 17, 19] to solve video inverse problems with significantly reduced computational requirements, there is still room for improvement in these methods. Optical flow-based methods [5, 33] have reported a key limitation: their performance is highly dependent on the accuracy of the optical flow estimation module [26, 31]. This dependency becomes problematic when extreme degradations complicate the estimation process, restricting their applicability to a wider range of restoration tasks. Additionally, these methods require task-specific restoration modules [33] or fine-tuning of the diffusion model [5]. In this perspective, batch consistent sampling strategy [13] successfully addressed various spatio-temporal degradations without requiring task-specific training or fine-tuning. However, the reconstruction resolution is limited to 256\u00d7256, as it utilizes the unconditional pixel-space diffusion model provided by ADM [6].\nTo overcome these limitations, here we propose a novel framework for solving high-definition video inverse problems using latent image diffusion models. Building on the success of solving spatio-temporal optimization problems within denoised batches [13] from pixel-space image diffusion models [6], we introduce a novel method that uses latent-space diffusion models [17] to solve spatio-temporal optimization problems. Specifically, to address the high computational demands of batch processing (e.g., 16-frame batches as used in [13]) with advanced latent diffusion models [17], we adopt a strategy called pseudo-batch consistent sampling. This approach helps manage the increased memory requirements, enabling the method to operate on a single GPU. Additionally, we introduce batch-consistent inversion, which allows us to initialize with informative latents from the measurement frame. This initialization enhances temporal consistency and improves the efficiency of solving spatio-temporal inverse problems.\nBy integrating these components, our framework achieves state-of-the-art video reconstruction performance using SDXL [17]. We name the method integrating these components as VISION-XL, short for Video Inverse-problem Solver using latent diffusION models (with stable diffusion XL). It supports various aspect ratios, including landscape, vertical, and square formats. Thanks to its efficiency, our framework can reconstruct 25-frame videos at a resolution of 1280x768 (exceeding HD resolution) in under 2.5 minutes on a single NVIDIA 4090 GPU. Our contribution can be summarized as follows:\n\u2022 We present a high-definition video inverse problem solver based on SDXL, which supports various aspect ratios and delivers state-of-the-art reconstruction performance.\n\u2022 Within this novel framework, we introduce a pseudo-batch consistent sampling strategy and batch-consistent inversion, which enable more effective and efficient video reconstruction from various video inverse problems."}, {"title": "2. Related Work", "content": "Diffusion model-based inverse problem solvers (DIS). Diffusion models [9, 21, 23] attempt to model the data distribution $p_0(x)$ based on the Gaussian transitions. In the geometric view of diffusion models [1], the transitions are typically described as iterative manifold transitions $M_t \\rightarrow M_{t-1}, t = T,..., 1$, moving from the noisy manifold $M_T$ to the clean manifold $M_0$.\nDiffusion model-based inverse problem solvers (DIS) [2, 4, 11, 22, 28] aim to guide manifold transitions to sample from the posterior distribution $p_0(x|y)$, which represents sampling $x$ from the measurement $y$ obtained from the forward model $A(x)$. In Bayesian inference, the posterior distribution, $p_0(x|y) \\propto p_0(x)p(y|x)$ is decomposed into the likelihood $p(y|x)$, representing the probability of observing $y$ given $x$, and the prior data distribution $p_0(x)$. This decomposition enables posterior sampling by combining diffusion sampling with iterative guidance using the forward model $A$ and measurement $y$. This approach provides sophisticated, precise solutions to complex inverse problems, leveraging the power and flexibility of diffusion models in practical applications, such as deblurring, super-resolution, inpainting, colorization, compressed sensing, and so on.\nDIS using latent diffusion models (LDIS). Most DIS [2, 4, 11, 22, 28] use pixel-space diffusion models, which facilitate easy integration of the forward model $A$ and measurement $y$, as both are defined in pixel space. Integrating forward models in latent space presents more challenges. Latent-space methods [3, 12, 20] calculate data consistency terms after decoding the denoised latent representation, then update these guidances within the latent space.\nDuring this process, VAE mapping errors accumulate in iterative sampling, causing the representation to drift from the clean manifold $M_0$. Additionally, most latent diffusion models provide a text-conditioned prior distribution $p_0(x|C_{text})$, which is challenging to implement in cases where text conditioning ($C_{text}$) is unavailable. As a result, latent-space methods prioritize two main goals: (i) managing text embeddings effectively and (ii) preserving the updated latent close to the clean manifold $M_0$."}, {"title": "3. High Definition Video Inverse Solver Using Latent Diffusion Models", "content": "This section introduces a novel approach for reconstructing high-definition videos that include various spatio-temporal degradations. The overall pipeline of the algorithm is illustrated in Fig. 2.\nConsider the spatio-temporal degradation process is formulated as:\n$Y = A(X) = A([X[1], \u00b7\u00b7\u00b7, X[N]])$ (1)\nwhere $Y$ denotes the measurement, $X[n]$ denotes the n-th frame ground-truth frame, N is the number of video frames, and $A$ refers to the operator describing the spatio-temporal degradation process.\nOur approach begins by inverting the initial measurement frame, denoted as $Y[1]$, and replicating it N times to initialize the informative latents $Z_1 = [z_T\u00b7\u00b7\u00b7 z_T]$, ensuring batch-wise consistency (Step 1). Next, we construct the corresponding denoised batch $X$ by sampling each latent in parallel using Tweedie's formula [7], followed by decoding (Step 2). In Step 3, the corresponding denoised batch $X$ is further refined by applying $l$-step conjugate gradient (CG) optimization [4, 13] to enforce the data consistency from spatio-temporal degradation $A$. In Step 4, we apply a scheduled low-pass filter to the updated batch $X$, inspired by the frequency-based analysis of spectral diffusion [32]. $X$, is then re-encoded into latent space to form $Z_T$. Finally, we obtain the one-step denoised latents $Z_{T-1}$ by adding noise back to the encoded latents $Z_T$ (Step 5). In the following, we provide a detailed description of each of these steps.\nStep 1: Initialize informative latents. One of our key insights is to initialize the informative latents by inverting the measurement frame and replicating it, thereby ensuring batch-wise consistent initialization (see Fig. 3). Although these latents cannot restore the ground-truth frames directly, inverted latent variables can inherit information from the measurement frame, providing good initializations [30]. Different from SVI [13] using replicated uninformative Gaussian prior $z_T ~ N(0, I)$ as the initial sampling point, we replace the Gaussian prior with the replicated informative prior $z_T$. Since the inversion time of a single measurement frame (under 2 seconds) is negligible compared to the sampling time, this approach not only offers a good initialization but also reduces the overall sampling time. The"}, {"title": "Step 2: Denoised batch estimation.", "content": "After initialization, we guide the sampling path to ensure the data consistency condition. At timestep $0 < t < T$, we sample denoise batch $Z_t := [z_t[1] \u00b7\u00b7\u00b7 z_t[N]]$ by sampling each latent in parallel using Tweedie's formula [7] and the latent diffusion model $\u2208_\u03b8(\u00b7)$. Unlike prior works [5, 13, 33], we sample latent frames in parallel, requiring memory for only a single frame during sampling. This enables the recent advanced latent diffusion model to operate in this framework without a frame limit. As a proof-of-concept, we conduct experiments on 25-frame videos.\nConsider a parallel sampling of latent diffusion models along the temporal direction:\n$\u2208_t (Z_t) := [\u2208_t (Z_t[1])\u2026\u2026\u2026\u2208_t (Z_t[N])].$ (2)\nThe denoised latents $\\tilde{Z_t}$ are computed using Tweedie's formula [7]:\n$\\tilde{Z_t} = \\frac{1}{\\sqrt{\u03b1_t}}(Z_t - \\frac{1-\\sqrt{\u03b1_t}}{\\sqrt{1-\\bar{\u03b1_t}}}\u2208_t(Z_t)),$ (3)\nwhere $\u03b1_t$ is the noise schedule defined in the Gaussian process of diffusion models [9, 16]. Then denoised batch $X_t$ is decoded from the denoised latents $\\tilde{Z_t}$ using VAE decoder $D_\u03b8$:\n$X_t = D_\u03b8(\\tilde{Z_t}) := [D_\u03b8(\\tilde{Z_t}[1])\u2026\u2026 D_\u03b8(\\tilde{Z_t}[N])].$ (4)"}, {"title": "Step 3: DDS update in pixel-space.", "content": "Inspired by batch-consistent sampling strategy [13], the latent diffusion models are initialized with the same informative latents using batch-consistent inversion to ensure temporal consistency. Subsequently, the denoised batch $X_t$ is refined as a whole by applying the $l$-step CG optimization to enhance the data consistency from the spatio-temporal degradation $A$. This can be formally represented by\n$X_t := arg min_{X \u2208 X_t+K_l} ||Y \u2013 A(X)||_2^2$ (5)\nwhere $K_l$ denotes the $l$-dimensional Kyrlov subspace associated with the given inverse problem [4]. The multistep CG allows each temporal frame to be diversified, enhancing data consistency and achieving faster convergence without requiring memory-intensive gradient calculations [13]. In this work, we refer to this sampling scheme as pseudo-batch consistent sampling because we treat the parallel-sampled latents as a single batch for optimization."}, {"title": "Step 4: Low-pass filtered encoding.", "content": "Recent frequency-based analyses of diffusion models [10, 32] suggest that optimal denoisers first recover low-frequency components in early denoising stages, with high-frequency details added progressively in later stages. Building on these findings, we observed that applying a scheduled low-pass filter to the updated batch $X_t$ in early stages yields more natural and refined results.\nBased on the observation that the denoiser restores high-frequency details as the noise scale $\\sqrt{1 - \u03b1_t}$ decreases, we set the filter width $\u03c3_t$ to be proportional to the noise scale [16], defined as $\u03c3_t := \u03bb \\sqrt{1 - \u03b1_t}$, which goes to zero at t\u2192 0. After applying the low-pass filter $h_{\u03c3_t}$, we re-encode $X_t$ into the latent space. Specifically, the re-encoded latents are given by:\n$X_t^+ \u2190 X_t * h_{\u03c3_t}$ (6)\n$Z_t = E_\u03b8(X_t^+) := [E_\u03b8(X_t^+[1]) \u00b7\u00b7\u00b7 E_\u03b8(X_t^+[N])],$ (7)\nwhere $E_\u03b8$ denotes the VAE encoder."}, {"title": "Step 5: Renoising.", "content": "After encoding, updated latents $Z_t$ are renoised as:\n$Z_{t-1} = \\sqrt{\\bar{\u03b1_{t-1}}}Z_t + \\sqrt{1 - \\bar{\u03b1_{t-1}}}E_t,$ (8)\nwhere $E_t$ is composed of batch-consistent noise [13] and deterministic noise [21].\nIn summary, the proposed method initializes informative latents and iteratively refines the decoded batch through multistep CG to meet spatio-temporal data consistency. We then apply low-pass filtered encoding to improve reconstruction quality. Extensive experimental results demonstrate that our method effectively addresses various spatio-temporal degradations, achieving state-of-the-art video reconstructions for high-definition videos and supporting a range of ratios previously unaddressed in other works. A geometric illustration of the sampling path evolution is shown in Fig. 4, and the complete algorithm is provided in Algorithm 1."}, {"title": "4. Experimental Results", "content": "4.1. Experimental setup\nDataset. We used four high-resolution (with resolutions exceeding 1080p) video datasets for evaluation, sourced from the DAVIS dataset [18] and the Pexels dataset\u00b9. A subset of 100 videos from the DAVIS dataset is resized to 768x1280 resolution and consists of 25 frames, originally provided in landscape orientation. The Pexels dataset is a large, open-source collection of high-resolution stock videos and images, widely used for creative and research purposes. For the Pexels subset, we collect a total of 120 videos: 45 in landscape orientation (Pexels (landscape)), 45 in vertical orientation (Pexels (vertical)), and 30 in square orientation (Pexels (square)). These subsets are resized to resolutions of 768x1280 for landscape, 1280\u00d7768 for vertical, and 1024x1024 for square orientations, with each video consisting of 25 frames.\nInverse problems. We test our method on the following spatial degradations: 1) Deblur: Gaussian deblurring from an image convolved with a 61\u00d761 size Gaussian kernel with \u03c3=3.0, 2) SR: Super-resolution from \u00d74 average pooling, 3) Inpaint: Inpainting from 50% random masking. Furthermore, test our method on the following spatio-temporal degradations: 4) Deblur+: Deblur + 7-frame averaging using temporal uniform blur kernel as used in [13], 5) SR+: SR + 7-frame averaging, and 6) Inpaint+: Inpaint + 7-frame averaging.\nBaseline comparison. The primary objective of this study is to improve the performance of video inverse problem solvers through latent image diffusion models. Thus, our evaluation primarily compares video inverse problem solvers using image diffusion models. As a recently emerging field, only a few methods are available: SVI [13], DiffIR2VR [33], and Warped Diffusion [5]. Notably, DiffIR2VR and Warped Diffusion cannot address spatio-temporal degradations, and DiffIR2VR only supports SR among the inverse problems we address in this paper. We conducted comparisons with SVI and DiffIR2VR but excluded Warped Diffusion, as it is not currently open-source. SVI officially supports a resolution of 256x256, while DiffIR2VR supports 480\u00d7854. To ensure fair comparisons with identical resolutions, we used patch reconstruction. Additionally, we included a comparison with the classical optimization method ADMM-TV, following the protocol established by SVI [13].\nFor quantitative comparison, we focus on two widely used standard metrics: peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM)[29]. Additionally, we evaluate two perceptual metrics: Learned Perceptual Image Patch Similarity (LPIPS)[35] and Fr\u00e9chet Video Distance (FVD) [27]. For computing FVD, the videos are resized to 224x224 resolution and we follow the protocol from the open-source project2.\nImplementation details. While our method is applicable to general latent diffusion models, we use Stable Diffusion XL 1.0 (SDXL) [17]-the current state-of-the-art text-to-image diffusion model-as a proof of concept in this paper. For all experiments, we employ T = 25, \u03c4 = 0.3T, \u03bb = 2, and l = 10. These optimal values are obtained through extensive ablation studies and the results are described in Sec. 4.3. To reduce undesired guidance from the text condition Ctext, we use a null-text condition, c\u00d8. All experiments were done on a single NVIDIA 4090 GPU."}, {"title": "4.2. Results", "content": "Table 1 presents the quantitative comparisons across various spatio-temporal inverse problems. The proposed method consistently outperforms the baseline approaches in most metrics, particularly for solving spatio-temporal degradations. Notably, we observe a significant decrease in FVD,"}, {"title": "4.3. Ablation studies", "content": "In this subsection, we present an in-depth analysis of the key components in our method."}, {"title": "Effect of initialization (in Step 1).", "content": "In Table 3, we conduct an ablation study using the Pexels (landscape) dataset to see the effect of batch-consistent inversion for initialization. From the table, we confirm that batch-consistent inversion effectively extracts informative latents to reconstruct video evidenced by about 1.8dB PSNR increase and 3.5dB PSNR increase compared with batch-consistent noise initialization used in SVI [13] and random noise initialization, respectively. Notably, batch-consistent inversion achieves \u00d78 lower FVD compared to the runner-up which indicates a significant improvement in temporal consistency. It is also evident in the visualization of the ablation study shown in Fig. 7. The reconstruction results from random noise and batch-consistent noise initialization fail to reconstruct the color of the cloud and are temporally inconsistent. In contrast, our method successfully reconstructs the color of the cloud and temporally consistent results. Furthermore, thanks to the fast inversion of a single frame (e.g., within two seconds for T: 0.30T), batch-consistent inversion efficiently reduces sampling time by about \u00d73 compared to the full samplings. The multi-step CG guidance and batch-consistent inversion synergistically combine to enable high-definition video reconstruction in under 2.5 minutes on a single NVIDIA 4090 GPU."}, {"title": "Effect of optimization step l (in Step 3).", "content": "In Table 4, we present an ablation study on the effect of the CG update step $l$. The table confirms that the CG update is essential for enhancing data consistency. We found that at least 5 iterations of CG updates yield satisfactory results, with 10 iterations producing the best outcomes."}, {"title": "Effect of LPF \u03bb (in Step 4).", "content": "Table 5 presents an ablation study on the effect of low-pass filtering. The results confirm that low-pass filtering enhances the reconstruction quality as evidenced by all metrics. Specifically, low-pass filtering results in approximately a 20-point decrease in FVD and a 0.3dB increase in PSNR compared to the absence of low-pass filtering. This improvement is also evident in the visualizations in Fig. 8. In the second row of the figure, undesired artifacts appear when low-pass filtering is not ap-"}, {"title": "5. Conclusion", "content": "In this paper, we proposed a novel framework for addressing high-definition video inverse problems using latent diffusion models that introduce two new strategies. First, a pseudo-batch consistent sampling strategy to manage intensive batch memory consumption with advanced latent diffusion models (e.g., SDXL). To acquire a denoised batch, we conduct parallel sampling of each latents rather than batch sampling to efficiently manage the high memory consumption of advanced latent diffusion models. Second, a batch-consistent inversion for leveraging informative latent as initialization is proposed. We confirmed that batch-consistent inversion significantly improves reconstruction performance in both traditional and perceptual quality metrics. Leveraging the powerful SDXL, our method achieves state-of-the-art performance across diverse spatio-temporal inverse problems, including challenging tasks such as the combination of frame averaging with deblurring, super-resolution, and inpainting. Importantly, our method supports multiple aspect ratios (landscape, vertical, and square), making it versatile for different video formats and delivering HD reconstructions in under 2.5 minutes on a single NVIDIA 4090 GPU. Overall, our framework not only enhances video reconstruction quality but also sets new standards for efficiency and flexibility in solving high-definition video inverse problems."}, {"title": "6. Experimental details", "content": "6.1. Implementation of Comparative Methods\nSVI [13]. For SVI, we use the official implementation\u00b3. Specifically, we utilize the same pre-trained image diffusion model, the unconditional ADM [6]. Following the protocol described in [13], we set the parameters as 1 = 5 and \u03b7 = 0.8 with 100 NFE sampling. Since SVI officially supports a resolution of 256\u00d7256, we applied patch-based reconstruction to ensure fair comparisons at identical resolutions.\nDiffIR2VR [33]. For DiffIR2VR, we use the official implementation. Specifically, we employ the same pre-trained image diffusion model, Stable Diffusion 2.1 [19]. DiffIR2VR is designed to support only super-resolution (SR) within the scope of our inverse problem. Therefore, we conducted SR experiments exclusively. Following the protocol in [33], we set the upscale factor to 4 and the CFG scale factor to 4, with 50 NFE sampling. DiffIR2VR officially supports resolutions of 480\u00d7854. To ensure fair comparisons across resolutions, we applied patch-based reconstruction. For different aspect ratios, we set the resolution to 480\u00d7854 for landscape orientation, 854\u00d7480 for vertical orientation, and 512\u00d7512 for square.\nADMM-TV. Following the protocol in [13], we optimize the following objective:\n$X^* = argmin_X \\frac{1}{2} ||AX - Y||_2^2 + \u03bb ||DX||_1,$(9)\nwhere D = [$D_t$, $D_h$, $D_w$] corresponds to the classical Total Variation (TV) regularization. Here, t, h, and w represent temporal, height, and width directions, respectively. The outer iterations of ADMM were set to 30, and the inner iterations of conjugate gradient (CG) were set to 20, consistent with the settings in [13]. The parameters were set to (\u03c1, \u03bb) = (1,0.001). The initial value of X was set to zero."}, {"title": "7. Extension to blind video inverse problems", "content": "Our method can be extended to address blind video inverse problems, such as blind video deblurring, demonstrated using the widely-used GoPro dataset [15]. Here, we provide an example application of our method to blind video deblurring, showing its potential as a general framework for solving blind video inverse problems."}]}