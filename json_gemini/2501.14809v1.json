{"title": "TOWARDS FOUNDATION MODELS: EVALUATION OF GEOSCIENCE ARTIFICIAL INTELLIGENCE WITH UNCERTAINTY", "authors": ["Samuel Myren", "Nidhi Parikh", "Rosalyn Rael", "Garrison Flynn", "Dave Higdon", "Emily Casleton"], "abstract": "Artificial intelligence (AI) has transformed the geoscience community with deep learning models (DLMs) that are trained to complete specific tasks within workflows. This success has led to the development of geoscience foundation models (FMs), which promise to accomplish multiple tasks within a workflow or replace the workflow altogether. However, lack of robust evaluation frameworks, even for traditional DLMs, leaves the geoscience community ill prepared for the inevitable adoption of FMs. We address this gap by designing an evaluation framework that jointly incorporates three crucial aspects to current DLMs and future FMs: performance uncertainty, learning efficiency, and overlapping training-test data splits. To target the three aspects, we meticulously construct the training, validation, and test splits using clustering methods tailored to geoscience data and enact an expansive training design to segregate performance uncertainty arising from stochastic training processes and random data sampling. The framework's ability to guard against misleading declarations of model superiority is demonstrated through evaluation of PhaseNet, a popular seismic phase picking DLM, under 3 training approaches. Furthermore, we show how the performance gains due to overlapping training-test data can lead to biased FM evaluation. Our framework helps practitioners choose the best model for their problem and set performance expectations by explicitly analyzing model performance at varying budgets of training data.", "sections": [{"title": "Introduction", "content": "As geoscience has breached the big data world within the last 20 years [3], the automation of geophysical tasks\u00b9 through artificial intelligence (AI) via deep learning models (DLMs) is booming [44, 23, 27, 38, 26]. Foundation models (FMs), a subset of AI models trained on vast amounts of data in a self-supervised fashion that can be adapted for various downstream tasks using relatively smaller labeled datasets [4], are extending DLMs into new horizons and are actively being developed for geoscience [37]. The technological leap from traditional geoscience DLMs to FMs is inevitable, yet we still lack robust evaluation methods for even the simpler DLMs. As a result, we are unprepared tofairly compare and evaluate geoscience FMs. Without a robust evaluation approach, we are unable to determine whether new technological developments are truly state of the art, we lack diagnostics that assist developers in pinpointing areas for model improvement, and we fail to convey the scenarios under which a model can be trusted. In this research, we address this gap by identifying three fundamental evaluation aspects for AI that are directly applicable to current DLMs and extendable to future FMs, uniting them under a single evaluation framework.\nThe first and perhaps most important evaluation aspect is performance uncertainty. Two identical AI model architectures with identical training approaches will exhibit different performances as a result of different initializations and random batching during training [6, 35]. This is often referred to as aleatoric uncertainty. Here, we refer to it as training uncertainty or variability. Furthermore, the data sample and how data is split data into training, validation, and test partitions have drastic random effects on performance benchmarking [6]. This aspect is often referred to as epistemic uncertainty. Here, we refer to it as data uncertainty or variability. Determining algorithmic superiority in light of both sources of variation is a known issue [9, 16, 10, 40] and, to our knowledge, has not been explored for the geoscience AI community.\nThe second vital, yet often unexplored aspect helpful to researchers, and critical for FM comparison, is evaluating the model's learning efficiency. Here, efficiency is defined as the relationship between the gain in performance to the additional training data required (see also [18]). Mapping this relationship helps decision makers compute the cost to benefit ratio of acquiring more data to meet a problem's performance requirements, especially in data scarce scenarios [1]. It is particularly important to FMs because they are fine-tuned using smaller, targeted datasets for the task at hand, and the FM that best adapts may be preferred. This aspect may appear easy to measure by training two models on varying budgets of training data and comparing their performances. However, evaluating learning efficiency for geoscience domains requires thoughtful construction of these budgets, as simple random samples from geoscience datasets are often highly correlated due to geography, a limited number of measurement sites, and repeated measurements of a single source. This concept, known as data leakage, has been previously explored in the biological community [12, 33]. Removing or reducing data leakage to the extent possible when creating varying budgets of training data is crucial for unbiased estimation of learning efficiency.\nThe third fundamental aspect applicable to AI evaluation is the effect on model performance resulting from overlap in the training and test data. An overlap between training and test data is a clear example of data leakage that can lead to overestimation of model performance. To address this, it is common practice in the AI community to clearly partition the data into train and test sets such that evaluation on the test data reflects model's performance on truly new (or unseen) data. However, ensuring no overlap between train and test data may be difficult for FMs, because a major component of the \"foundation\" in FMs is the big data used for pre-training that is often scraped from any available source. As an example, one of these sources could be the benchmark dataset that an evaluator is planning to use for testing. In comparing two FMs, one that happens to be trained on said benchmark and one that was not, we must avoid biased evaluation (see [11] for an example from the natural language processing community). Furthermore, spatial correlation in geoscience data warrants meticulous construction of training, validation, and test splits in contrast to simple random sampling for determining the effect in performance from overlap [17, 11, 12].\nThis research fundamentally contributes an evaluation framework for geoscience models that intertwines all three aspects: performance variation from data and training, learning efficiency, and train-test overlap that is directly applicable to current DLMs and intends to prepare the geoscience community for evaluation of FMs. We utilize concepts from statistical experimental design to measure each aspect in turn. We demonstrate our framework using a seismic DLM to directly compare 3 training approaches: training from scratch and two transfer learning approaches. We measure uncertainty through deep ensembles and random data selection processes, learning efficiency through careful data splitting and varying data amounts, and train-test overlap using overlapping and non-overlapping datasets. Finally, we cast all performance results into a statistical framework for direct comparison with uncertainty.\nWe use a conventional seismic DLM\u00b2 for demonstrating the evaluation framework. A plethora of research in applying DLMs to the cardinal task of phase-picking provides a strong basis for our demonstration. Phase picking is the task of identifying and predicting the time that event signatures arrive, typically the primary and/or secondary phases (P-arrival and S-arrival, respectively), at a seismic station. Phase picking is a vital task for detecting, locating, and characterizing seismic events such as earthquakes or explosions. It can be a cumbersome task, motivating researchers to actively constructing better DL phase picking models (referred to as pickers here-on) [7, 28, 46, 5, 47, 48, 39, 31] and others are using these pickers to expedite workflows, including catalog creation [8, 32, 43, 2]. Of course, the best performing and most robust picker is preferred, but determining which picker is best for a given context remains a challenge that our framework is positioned to address."}, {"title": "Data", "content": "We use two publicly available, labeled, non-continuous waveform datasets: the Stanford Earthquake Dataset (STEAD; [29]) and the Italian seismic dataset for machine learning (INSTANCE; [25]). STEAD is used for pre-training and INSTANCE for subsequent training and testing. STEAD contains ~ 235k noise waveforms and ~ 1.03M source waveforms representing ~ 441k unique sources. For the INSTANCE dataset, we use the version measured in counts and only the earthquake waveforms measured in meters per second (mps). The resulting INSTANCE set includes ~ 132k noise waveforms and ~ 824k source waveforms arising from ~ 54k unique sources. An example waveform from INSTANCE is shown in Figure 1 along with the labeled P-arrival and S-arrival. We access both datasets through the publicly available Python seismic deep learning package, SeisBench [42], which also provides a more detailed description of each dataset.\nTo measure the effect of overlap between training and test data on performance, we utilize the geographical overlap between STEAD (a global dataset) and INSTANCE (Italy only dataset). This results in a 3rd dataset which is a subset of STEAD we will refer to as STEAD Masked. STEAD Masked uses all of STEAD except the waveforms whose source or station locations overlap with INSTANCE's area. We define INSTANCE's area as the rectangle whose 4 corners are the pairings of the maximum and minimum source or station latitude/longitude with a 5 degree buffer in all directions. This rectangular mask reduces STEAD by 1.72% of the noise waveforms and 2.82% of the earthquake waveforms and reduces the number of unique sources by 0.97%."}, {"title": "Training, Validation, and Test Splits", "content": "Default splits provided in SeisBench for STEAD are used to pre-train our models. The STEAD masking process removes waveforms from each split at an approximately equal rate of 4%, so no bias is induced. A more meticulous process is implemented for INSTANCE data since it is used for both training and testing models. In the case of INSTANCE data, simply dividing waveforms or sources randomly into training, validation, and test partitions is inappropriate because data from nearby sources or stations share similar sources, geographies, and processes, and hence, are correlated. Therefore, if we randomly sample the data on a waveform or source basis, we inadvertently add information about the other waveforms or sources within the dataset that were not selected. This results in data leakage, a challenge described in the introduction that impacts evaluation of learning efficiency as well as training and test overlap. To address this, we divide the INSTANCE dataset into spatial clusters, a simple and strong indicator of signal commonality. We have the option to cluster earthquake waveforms based on source or station locations and we choose to cluster based on source locations for three reasons:\n1. sources are most often detected by sensor stations nearest to the source,\n2. waveforms sharing the same source are assumed to have more in common than waveforms sharing the same sensor,\n3. and the sources tend to group themselves based on geographic contours across Italy, thereby making a clustering approach natural.\nWe use a k-means algorithm from the Python package, Scikit-learn [34], to cluster earthquake sources based on their latitude and longitude. We chose k = 20 clusters because it lead to clusters large enough to feasibly represent the ground characteristics of their geographic region, yet small enough to explore model performance in limited data settings. To assign the noise waveforms, which have no corresponding source locations, to the established clusters, we use the trained k-means model to predict its associated cluster based on its station location. The clusters and number of sources, stations and waveforms are shown in Figure 2. Table 1 summarizes the amount of data in the training, validation, and test splits and the construction of the splits are described next.\nTo construct the INSTANCE splits, we first segregate the clusters into two groups: a training/validation pool and a test pool. We select the 12 central clusters to be the pool from which the training/validation data will be constructed, and 8 clusters - the 4 northern most and 4 southern most - to be the test clusters (see Figure 2). Choosing the test clusters from northern and southern regions allows us to explore two different concepts. First, they help to evaluate model performance on semi-out-of-distribution data: we expect the waveforms to share similar signatures as a whole because they are regionally constrained, but they should be slightly different due to more localized geological distinctions between the areas. Second, these splits represent a realistic situation where we may know how a model performs in a certain region and we want to measure its consistency for neighboring regions.\nTo mitigate regional bias in the the INSTANCE test split, we balance the data from north and south regions. The northern region is smaller comprising 2,269 sources, and therefore we randomly sample 2,269 sources from the southern region. For each source selected in the test split, we include all associated waveforms, resulting in a test set totaling 49,516 earthquake waveforms: 23,086 from the north and 26,430 from the south. To maintain the overall ratio of 11.4% noise to earthquake waveform ratio in INSTANCE, we randomly select 2,822 noise waveforms from each of the south and north regions.\nWe aim for an 80-20 ratio for the training to validation splits and we desire an equal number of sources from each cluster to reduce bias. Since the smallest of the 12 central clusters has 795 sources, we randomly sample 159 sources from each cluster for the validation set. This totals to 1,908 sources encompassing 27,169 earthquake waveforms. Maintaining a ~ 11.4% earthquake to noise ratio, we then randomly sample 276 noise waveforms per cluster totaling 3312 noise waveforms for the validation split.\nAfter removing the INSTANCE validation set from the central clusters, the remaining waveforms become the pool from which we build the training splits. When training models, the amount of training data is specified by the number of clusters used for training. However, to keep data amounts relatively constant across clusters, each time a cluster is selected to be included in training data, we randomly select 636 sources (80% of 795, the smallest number of sources in any cluster) which on average results in 8,878 earthquake waveforms, and 1,106 noise waveforms per cluster."}, {"title": "Methods - Model Architecture, 3 Training Approaches, and 5 Models", "content": "We develop a unifying framework for evaluating AI models that considers uncertainty, learning efficiency, and training-test overlap to guide model selection. We demonstrate our framework by comparing 5 models as outlined in Table 2 to compare the effects of 3 model training approaches: training from scratch (the standard DL approach) and two transfer learning (TL) approaches with varying number of free parameters. However, the framework presented is sufficiently broad where one could explore the effects of different hyperparameters, training approaches, or new architectures altogether. We first review the model architecture and other commonalities among the models, then we review the uniqueness of each model and how the training design allows for estimation of data and training uncertainty.\nAll models use the PhaseNet [47] deep learning architecture which has been extensively utilized for phase picking [20, 5, 30] and features a U-Net type architecture [36]. PhaseNet intakes 3 component waveforms of length 3,001 and is trained to output probabilities of P-arrivals, S-arrivals, and noise for each sample (time point in the waveform). Since our goal is to demonstrate the evaluation framework, we focus only on the P-arrival predictions and discard the S-arrival and noise predictions for all subsequent analysis.\nAll models use the PhaseNet [47] deep learning architecture which has been extensively utilized for phase picking [20, 5, 30] and features a U-Net type architecture [36]. PhaseNet intakes 3 component waveforms of length 3,001 and is trained to output probabilities of P-arrivals, S-arrivals, and noise for each sample (time point in the waveform). Since our goal is to demonstrate the evaluation framework, we focus only on the P-arrival predictions and discard the S-arrival and noise predictions for all subsequent analysis.\nTo demonstrate our evaluation framework, we consider 3 training approaches: the standard approach for deep learning and two transfer learning approaches. The standard approach entails randomly initializing model weights and pulling the training, validation, and test splits all from the dataset of interest, in this case INSTANCE. Thus, the standard model represents this approach.\nThe two transfer learning approaches are thought to be useful when a small amount of labeled data is available from the region of interest, but a large amount of labeled training data can be obtained from other regions. As such, one first trains a model with data from the other regions (we call an upstream model), then switches to data from the region of interest to continue learning, thereby transferring the accumulated knowledge to the new domain. In our case, we first train PhaseNet using STEAD or STEAD Masked and then further train using INSTANCE. We explore two options: 1) allow all weights to change during INSTANCE training phase and 2) fix half of the weights (the input and down sampling layers). The TL models that have fully free weights are denoted as Free, and those with fixed weights are denoted as Frozen.\nTo explore the effect of training-test overlap of interest to FM evaluation, we train two models for each of the two TL approaches by changing whether the model was initialized from the model trained on STEAD or STEAD Masked. Those that are seeded by STEAD Masked have the Masked label. This results in 4 TL models named TL Free, TL Free Masked, TL Frozen, and TL Frozen Masked. All 5 models (the Standard model and 4 TL models) are validated and tested using INSTANCE data.\nTo evaluate learning efficiency, we train each model using 1, 3, 6, 9, and 12 clusters from the INSTANCE training clusters. To measure data uncertainty at a given quantity of clusters, we randomly select the cluster set 12 times, training the model each time. Then, to separate the data uncertainty from training uncertainty, we use deep ensembles [21, 14, 19]. That is, for each of the 12 cluster sets, we retrain the model 4 different times under different initializations (holding the training data sample fixed) to obtain 4 model instances for evaluation. To obtain different initializations for the Standard model, we simply randomly reset the weights. For the TL models which are seeded by weights from an upstream model, we first train 48 upstream models for both STEAD and STEAD Masked starting from random initializations. Then, we initialize the TL models by pulling the set of weights from one randomly selected upstream model.\nWe use ensembles to obtain training uncertainty for a few reasons. Fundamentally, deep ensembles are easy to understand and implement and they only require extending training time. This is in contrast to other uncertainty methods which generally attempt to approximate the posterior distribution of the prediction. These methods include Bayesian neural networks [22] and approximations thereof including dropout [13], Stochastic Weight Averaging - Gaussian (SWAG) [24] and many others [15]. However, these approaches enforce distributional assumptions of the model weights, make strong/arbitrary decisions (e.g, dropout), rarely consider different basins of attraction, and are generally more difficult to interpret. Some methods, however, such as MultiSWAG [41] (see [2] for a seismic based implementation), offer a strong balance between the complex Bayesian assumptions and the simpler empirical ensembles and merit consideration for future work.\nIn our deep ensemble implementation, we loosely follow the snapshot ensemble approach of [19] which boasts computational advantages in finding different parameter states that minimize loss. We implement a cosine annealing learning schedule with 4 restarts with an initial learning rate of 0.01 and final rate of 0. These learning parameters exhibited the best performance in preliminary training experiments especially for models with the smallest quantities of clusters (one cluster). However, simple rate resets routinely failed to push models with small training sets into new basins of attraction, so we altered the approach towards the classical deep ensemble method [21] by fully reprogramming the model weights at each scheduled rate reset rather than relying on rate resets alone. Depending on the model being trained, this reprogramming may be a complete randomized reset (for the OOD and Standard models) or replacing weights by randomly pulling from the 48 upstream STEAD (Masked) models (for the TL models)."}, {"title": "Evaluation", "content": "As our main goal is to demonstrate the effectiveness and versatility of our evaluation framework, we limit the scope of evaluation of the phase picking models by considering just a few classification metrics and one functional based regression metric. To form these metrics, we first implement techniques similar to those found in [47, 30, 20] to obtain model predictions for test waveforms. The predictions are probabilities of the P-arrival for each sample in each waveform. From the predicted curves and through an algorithm similar to those found in [ref same papers as above], we obtain P-arrival picks, which are the discrete locations in time that a P-arrival is predicted by the model. There can be none, one, or any number of picks per waveform. To motivate the specific metrics we consider, we note that it is important in evaluation of phase pickers to consider both the model's strength in identifying if a P-arrival occurs (e.g., alerting) and, if so, the associated accuracy of the pick (e.g., how close is the predicted pick to the true P-arrival).\nEvaluating identification is typically done using classification metrics defined using the picks [47, 30, 20, 45]. True positives (TP) are assigned to picks if they are near enough to the labeled P-arrival. False positives (FP) are assigned to picks either far from the labeled P-arrival or to extra picks that are not true positives. False negatives (FN) occur if no picks occur near the labeled P-arrival. True negatives are not informative in this context and are not recorded. In our display, we consider summary metrics including Recall = $\\frac{TP}{TP+FN}$ and F1 = $\\frac{2TP}{2TP+FP+FN}$ which are only applied to earthquake waveforms. An example of a model prediction featuring classified picks including a true positive and 3 false positives is shown in Figure 3. For noise waveforms, we calculate noise percent correct as the number of noise waveforms containing no predicted picks (TNnoise) divided by the total number of noise waveforms (nnoise). That is,\nNoise % Correct = $\\frac{TNnoise}{Nnoise}$Evaluating accuracy is typically dependent on the classification results and done using regression-based metrics [47, 30, 20, 45]. If a pick is classified as a true positive, then we obtain a residual defined as the difference in time between the labeled P-pick and the predicted P-pick. Because different models can produce different numbers of true positives, typical regression metrics like the root mean square residual (RMSR; the square root of sum of the residuals divided by the number of true positives) fail to convey the full story. Therefore, functional based metrics like cumulative RMSR, defined as RMSR calculated using only the residuals whose magnitudes are below some value, are more informative."}, {"title": "Statistical Framework for Evaluation", "content": "In this section, we present a statistical framework for comparing models in terms of their performance and uncertainty arising from data sampling and model training. The statistical framework can be thought of as a mixed effect model or a completely randomized design with sub-sampling. It allows us to distinguish performance effects with uncertainty between the 5 models and different training amounts. Furthermore, we can segregate and estimate the two sources of variation with uncertainty. We describe our overall approach here and relegate many details of the calculations to the appendix.\nFor a given metric that summarizes the outcomes of a model instance, denoted $Y_{madi}$, where y is the metric (e.g., Recall), we setup the statistical model\n$Y_{madi} = \\gamma + \\mu_m + a_a + \\Theta_{ma} + \\epsilon_{mad} + \\epsilon_{madi},$  (1)\nwhere m = {1. . . 5} indexes the 5 models, a = {1, . . ., 5} indexes the 5 quantities of clusters (representing the amount of data) at levels {1, 3, 6, 9, 12}, d = {1, . . ., 12} indexes the randomly selected cluster set (i.e., the data choice selected to train the model), and i = {1, 2, 3, 4} indexes the 4 model initializations.\nIn this statistical model, $\\gamma$ is the grand mean, $\\mu_m$ is the effect of model m, $a_a$ is the effect of the a'th quantity of clusters, and $\\Theta_{ma}$ is the interaction term. The uncertainty associated with data selection and model training is segregated"}, {"title": "Results", "content": "After training the 1,200 model instances, computing the metrics for evaluation, and organizing them through the statistical framework, we are ready to compare the performance of the models.\nThe classification based summary metrics of recall and noise percent correct are presented in Figure 4 along with 90% confidence intervals for the standard and 2 masked TL models showing their performance as a function of quantity of clusters used in training. All models improve in recall as more data is added with largest gains between 1 and 3 clusters followed by diminishing returns from there on. The trend is less consistent for noise percent correct where the TL w/ Frozen Weights Masked model performs slightly worse with increasing data. The variability in performances decreases for all models as more training data is added.\nThe functional regression based metric of cumulative RMSR demonstrating the evaluation's framework versatility and importance is shown in Figure 5 for the standard and 2 TL masked models with 90% confidence intervals (ribbons) for selected quantities of clusters of 1, 6, and 12. Stark performance differences can be seen between each model especially as more training data is added as seen through tightening confidence intervals."}, {"title": "Discussion", "content": "We present results that will be increasingly relevant for developers of DL pickers and for the seismic community as they invest in FMs. In inspecting the results presented in Figure 4, we see overall poor performance in the TL w/ Frozen Weight models. We hypothesized that freezing half the model at the learned weights from the global dataset would produce the best model for small training data amounts. However, the results negate this hypothesis for datasets comprising 10,000 waveforms (approximately the number of waveforms in a single cluster) or more for the PhaseNet architecture. In a similar vein, we see that models trained from scratch perform better than TL when the data size approaches 30,000 waveforms (3 clusters). Since FMs are pre-trained using a large general dataset and fine-tuned (typically weights for the last couple of layers are set to be updated) using tailored datasets to accomplish downstream tasks, we see the importance of continuing to assess the utility of seismic FMs by comparing to traditional modeling approaches.\nAnother valuable result for the seismic AI community is that expending effort to acquire larger and larger labeled training datasets may be wasteful. This is exhibited by Figure 4 where we see diminishing improvement as the amount of training data increases. This suggest relatively smaller datasets (~30,000 or ~60,000 waveforms) may be sufficient for reasonable model performance even when training models from scratch. This is a good news because large labeled datasets are expensive and sometimes unattainable. Furthermore, these results fortify the importance of learning efficiency as an aspect of future FM evaluation. Since the best model depends on the amount of training data, knowing which model or modeling approach does more with less is valuable information during model selection. And, as we have shown, it is not a simple one-model-takes-all, but rather a dynamic choice for a given situation.\nAs we work toward an evaluation framework with uncertainty for geoscience FMs, we recognize the limitations in using deep ensembles as the primary form for obtaining uncertainty estimates. Deep ensembles are computationally expensive, requiring at least some retraining. With FMs, unfortunately this expense is likely prohibitively large for retraining the base model. However, this limitation does not detract from the importance of quantifying training induced variation, and alternative approaches are possible such as using ensembles during fine tuning.\nWe have demonstrated evidence that overlap in the training/test data can imbue performance gain, but we recognize that the effect may be confounded by the 4% decrease in the amount of data in the STEAD dataset with Italy removed. In"}, {"title": "Conclusion", "content": "We present an evaluation framework for current DLMs and future FMs that simultaneously considers performance uncertainty/variability, learning efficiency, and overlap between training and test data. We demonstrate our evaluation framework using PhaseNet, a popular phase picking DLM [47] to compare 3 training approaches (training from scratch and two transfer learning approaches). We implement an extensive training design comprising 5 models and 1,200 model instances over varying amounts of data from meticulously constructed data splits. We cast the results into a statistical framework which allows for direct comparison among models and training approaches across the training budgets.\nWe demonstrate the effectiveness and versatility of the evaluation framework by comparing classification and functional regression metrics. We show how no model is a global winner in terms of learning efficiency; that it depends on the total amount of training data used. We further demonstrate that training-test overlap must be considered in future FM evaluation as seen through performance improvement due to overlap. By providing estimates for the data and training uncertainty, a developer or practitioner can better target their efforts to mitigate sources they deem most important for their problem. We show that without the evaluation framework, there is a significant risk of model mis-ranking. Finally, our results demonstrate how transfer learning may not always be the best choice in limited data scenarios, which emphasizes the importance of continuing to compare FMs to their simpler DLM counterparts and inspect how training data budgets influence model performance. We hope that this research helps practitioners and developers by providing a general guide for what a truly revolutionary model might look like."}, {"title": "Statistical Evaluation Framework Details", "content": "To exemplify the identifiability of the two sources of variation, $\\sigma_{data}^2$ and $\\sigma_{train}^2$, we note how the index i is attached only to the training error term. This indicates that for each model, quantity of clusters a, and cluster sets d, we have 4 initializations for estimating the sample variance, $s_{train}^2$, with 3 degrees of freedom. We then pool/average the 12 sample variances to get $\\sigma_{train}^2 = \\frac{\\sum s_{train}^2}{SMA}$. This results in a training variance with 36 degrees of freedom.\nTo estimate the data uncertainty $\\sigma_{data}^2$ for each model, we first take the mean across initializations for each quantity of cluster $\\bar{Y}_{mad.}$. Then, we compute the sample variance of these 12 means as $s_{data}^2 = \\frac{1}{\\sum}(Y_{mad.} - \\bar{Y}_{ma..})^2$. Since the expected value of $s_{data}^2$ includes some uncertainty associated with model training, we subtract this component to obtain $\\sigma_{data}^2 = \\frac{s_{data}^2 - \\sigma_{train}^2}{4}$. Finally, we also form approximate 90% statistical intervals using pivot statistics of the variance estimates."}]}