{"title": "Learn from Real: Reality Defender's Submission to ASVspoof5 Challenge", "authors": ["Yi Zhu", "Chirag Goel", "Surya Koppisetti", "Trang Tran", "Ankur Kumar", "Gaurav Bharaj"], "abstract": "Audio deepfake detection is crucial to combat the malicious use of AI-synthesized speech. Among many efforts undertaken by the community, the ASVspoof challenge has become one of the benchmarks to evaluate the generalizability and robust-ness of detection models. In this paper, we present Reality De-fender's submission to the ASVspoof5 challenge, highlighting a novel pretraining strategy which significantly improves gen-eralizability while maintaining low computational cost during training. Our system SLIM learns the style-linguistics depen-dency embeddings from various types of bonafide speech using self-supervised contrastive learning. The learned embeddings help to discriminate spoof from bonafide speech by focusing on the relationship between the style and linguistics aspects. We evaluated our system on ASVspoof5, ASV2019, and In-the-wild. Our submission achieved minDCF of 0.1499 and EER of 5.5% on ASVspoof5 Track 1, and EER of 7.4% and 10.8% on ASV2019 and In-the-wild respectively.", "sections": [{"title": "1. Introduction", "content": "The increasing interest in speech generative models has resulted in rapidly emerging text-to-speech (TTS) and voice conversion (VC) tools. With many tools being publicly available [1, 2], nowadays a person's voice can be cloned from a only few seconds of a speech recording. During recent years, there have been many cases of misusing such techniques, e.g., impersonation of celebrity's voice [3], hacking bank accounts using cloned voice over telephone line [4], evidence forgery at court [5], just to name a few.\nTo combat the spread of these maliciously generated speech (i.e., speech deepfakes), numerous efforts have been undertaken by the research and industry communities, including the ASVspoof series of challenges [6, 7]. The main objective of ASVspoof challenges is to encourage the innovation of speech deepfake detection tools that can generalize to unseen attacks and maintain robustness under various conditions, such as transmission and compression codecs. In Track 1 of the most re-cent ASVSpoof5 challenge (referred to henceforth as ASV5), the goal is to build a standalone bonafide vs. spoof deepfake detection system.\nExisting state-of-the-art (SOTA) systems on deepfake de-tection typically adopt one or more self-supervised learning (SSL) speech encoders as feature extraction frontend, and append various downstream classifiers as backend [1, 2]. The majority of the innovations focus solely on the downstream supervised training stage, including full finetuning large SSL encoders [8, 9], designing classifiers with more discrimina-tive power [10, 11], increasing the variety of spoof training data [12, 13, 14], and model ensembling [15]. While demonstrating improvement on some datasets, the training cost of these methods can be high, especially considering that the vari-ety of speech generative models has drastically increased over time. Taking ASV5 as an example, a single epoch of training a model with a frozen Wav2vec-Large frontend and a simple clas-sification backend (5 million parameters) can take around 4 h on one A100 GPU, whereas the same setup took less than 30 min for previous ASVspoof challenges. Worse, methods that fine-tune models end-to-end would require several rounds of hyper-parameter search, and still might underperform models using a frozen backbone, as noted from the trends in ASV5 [16].\nIn this paper, we present a summary of Reality Defender's submission to the eval phase under Track 1 of ASV5. We used a novel pretraining framework SLIM, originally proposed in our recent work [17], which exploits the mismatch between style and linguistics content in deepfake data to detect them. SLIM involves two stages of training: the first stage adopts self-supervised contrastive learning on real data to learn the style-linguistics dependency embeddings; the second stage leverages the embeddings learned from the first stage and trains a clas-sifier in a supervised manner to separate real from deepfake speech. With a low training cost (7 million trainable parame-ters; less than 15 h training time including pretraining,\u00b9) SLIM achieved competitive results on ASV5. Our test results also show that the model generalizes well to out-of-domain datasets.\nThe rest of the paper is organized as follows. In Section 2, we introduce the employed speech deepfake datasets for model evaluation, including ASV5 data and two other datasets. Section 3 provides a description of our submitted system, including the pretraining strategies and downstream finetuning de-tails. Section 4 discusses the results achieved on the three test datasets. Section 5 presents the conclusions."}, {"title": "2. Datasets and evaluation metrics", "content": "Following the challenge rules, we used the official ASV5 track 1 data for training and evaluated our system by submitting scores for the eval dataset on the CodaLab platform.2 We used ASV2019 Logical Access (LA) [6] and In-the-wild (ITW) [18] datasets as two out-of-domain (OOD) corpora for offline eval-uation of the model generalizability.\nThe minimum detection cost function (minDCF) was used as the main metric together with equal error rate (EER). Log-likelihood ratio (LLR) was used a complementary metric to measure the confidence level of the model. For evaluation and comparison on ASV2019 and ITW, we use the EER metric."}, {"title": "3. System description", "content": "This section describes the architecture and the two-stage train-ing framework of SLIM, which in general follows the same methodology as proposed in [17]. Adjustments have been made to our submitted system to ensure SLIM's adherence to the chal-lenge rules. These adjustments are described in 3.2."}, {"title": "3.1. Overview", "content": "The general training framework is depicted in Figure. 1. SLIM follows a two-stage training process, where the first stage adopts self-supervised contrastive learning (SSCL) to model style-linguistics dependency from various types of bonafide speech, while the second stage learns to utilize the style-linguistics re-lationship to further discriminate spoof from bonafide via stan-dard supervised training.\nThe objective of the first stage is to learn embeddings that capture the dependencies between the style and linguistics as-pects in real speech. Style is assumed to encompass short- and long-term paralinguistic attributes, including speaker identity, emotion, accent, and health state [19]. Linguistics refers to the verbal content of speech [20]. While the two are often consid-ered as independent speech aspects during speech modelling, studies have demonstrated a specific dependency between these two subspaces, such as the connection between emotional states and word choices [21], the relationship between prosody and language comprehension [22], and the influence of age on sen-tence coherence [23]. While mainstream SSL models, such as Wav2vec [24], WavLM [25], Data2vec [26], have been shown to encode the style and linguistics information in different trans-former layers [27, 28, 29], the cross-subspace dependency is not well modelled with these SSL representations, which can be crucial for discriminating spoof from bonafide [17]. There-fore, we propose to bridge this gap by learning a set of style-linguistics dependency embeddings at Stage 1, and fuse these learned features with the original SSL representations to en-hance their discriminative power."}, {"title": "3.2. Self-supervised contrastive learning", "content": "The objective of the first stage is to learn pairs of style and lin-guistics features that are expected to be highly correlated for real speech and minimally correlated for deepfakes. Since only bonafide speech samples are required at this stage, we employed subsets from CommonVoice [30] and RAVDESS [31] datasets as the Stage 1 training data. The former contains a large number of speakers, and the latter has utterances in different emotional states, hence their combination forms a training set with diverse style traits.\nWe first extract style and linguistics representations using pretrained SSL backbones. In [17], SLIM relied on layer 0-11 and layer 12-22 from Wav2vec2-XLSR finetuned for speech emotion recognition and speech recognition as style and lin-guistics representations, respectively. The choice of layers was based on existing findings showing that early layers in SSL backbones are highly correlated with speaker attributes and later layers with verbal content [29, 27, 28]. Since the XLSR backbones do not comply with the ASV5 rules, we exper-imented with representations using other challenge-approved SSL speech encoders, such as Wav2vec2-Large, Wav2vec2-Base, WavLM-Base, and Data2vec-Base. Our experiments showed best results using layers 0-7 and layers 8-11 from WavLM-Base, so we proceeded with WavLM-Base backbones.\nBoth style and linguistics representations are three-dimensional tensors $\\in R^{L\\times F\\times T}$, where L denotes the number of transformer layers, F the feature size, and T the number of time steps. The two subspace representations are then sent into projector networks (P), which average the transformer layer outputs and reduce the feature size from 768 to 256 (see Ap-pendix 7.2 for the architecture of the projector network). The output from the projector networks are regarded as dependency features: $S_{f,t} = P(X_s)$ for style and $L_{f,t} = P(X_L)$ for lin-guistics, and their temporally averaged versions are denoted $\\overline{S_f}$ and $\\overline{L_f}$. These dependency features are learned by minimiz-ing the self-supervised contrastive loss $L_{SSC}$, which comprises two terms:\n$L_{SSC} = L_D + \\lambda L_R$ (1)\n$L_D$ represents the distance between the projected style and linguistics features, $L_R$ represents the self-redundancy of the learned features, and $\\lambda \\in [0, 1]$ is a hyperparameter that weighs the two loss terms, defined as follows:\n$L_D = \\frac{1}{T}\\sum_{t=0}^{T}||S_{f,t} - L_{f,t}||_F,$ (2)\n$L_R = ||\\overline{S_f}\\overline{S_f}^T - I|| + ||\\overline{L_f}\\overline{L_f}^T \u2013 I||$ (3)\nwhere T is the number of time steps; and $||(.)||$ is the Frobenius norm. The $L_D$ term reduces distance between the projected style and linguistic embeddings, the $L_R$ term reduces redun-dancy within the (temporally averaged) style and linguistic fea-tures by pushing off-diagonal elements to zero. The PyTorch-style implementation of the SSC loss can be found in [17]."}, {"title": "3.3. Supervised downstream finetuning", "content": "The second stage follows a standard supervised training ap-proach based on features learned from Stage 1. Since the dependency features are designed to capture solely the style-linguistics mismatch, deepfake-related artifacts may be ne-glected. Hence, we complement them with the raw SSL em-beddings extracted from the pretrained WavLM-Base backbone to increase the discriminative power. As shown in Figure 1, SSL embeddings are first passed through an attentive statistics pool-ing (ASP) layer, followed by a fully-connected layer to align with the dimension of the dependency features (256-dim). All features are then concatenated and fed into the downstream clas-sifier to obtain a final output. Details of the classifier can be found in Appendix. 7.2"}, {"title": "3.4. Training details", "content": "Preprocessing: All speech recordings were loaded with the TORCHAUDIO library [32], then resampled to 16 kHz with am-plitude normalized between -1 and 1. This step was applied to all samples in train, validation, and test sets. For training effi-ciency and memory constraints, we truncate waveforms that are longer than 10s. During validation and inference, all samples were kept at their original lengths.\nData augmentation: No data augmentation was applied dur-ing Stage 1 SSCL training. RawBoost [14] augmented sam-ples were concatenated with original samples during Stage 2 supervised training to combat potential loss brought by differ-ent codecs. The parameters of RawBoost were set as per [9].\nSilence removal: Previous works have shown that removing si-lence frames can lead to drastic degradation to existing deep-fake detection models [7]. We therefore trained and evaluated two versions of SLIM, one with silence removed from all sam-ples (train, dev, and eval) and one that kept the silence. Based on the performance achieved on ASV5 prog, no significant dif-ference was found between the two. Hence, we did not remove silence from the data, when training and evaluating the submit-ted system.\nZero-padding in batch: Since samples in the same batch re-quired to be of the same length, we zero-padded shorter sam-ples to align with the length of the longest samples in the batch. To avoid excessive padding, we first sorted all samples in the dataset by length, then batched the samples with similar lengths. This step was applied to training and validation, as both used batch size larger than 1. No zero-padding was applied during in-ference. However, it should be emphasized that we later found that zero-padding may lead to false acceptance at training time, hence the adopted approach may be sub-optimal. We discuss this issue in Section. 4.1.\nTraining data: The SSCL pretraining was performed with 3k samples from CommonVoice [30] and 3k samples from RAVDESS [31], the entire pretraining took less than 1 h. Al-though open condition allows combining data from previous ASVspoof challenges and other sources, we used only ASV5 data for supervised training.\nHyperparameters: A summary of all training hyperparameters can be found in Appendix. 7.1."}, {"title": "4. Results", "content": "Our final system achieved an average minDCF of 0.1499 and EER 5.56% on the ASV5 eval data. Figure. 2 summarizes the minDCF breakdown for different types of attacks and compres-sion codecs. In the clean condition (column 3), our system per-forms well on 15 of 16 types of attacks, obtaining minDCF within 0.1 on all attacks with only one exception (the A28-pretrained YourTTS attack).\nSince all evaluated attacks remain unseen during training, results here suggest that SLIM can generalize well to different generative models, including the ones with adversarial attacks applied (A18, A20, A23, A30-32). This is likely due to our SSCL pretraining on real data. Meanwhile, a significant differ-ence can be seen between the performance achieved on clean data (column 3) and codec-applied (columns 4\u201312) data. For some codecs, the minDCF values can be 3-5 times higher (e.g., codec-7 and 10). Such degradation is expected for our system, considering that we only employed RawBoost for data augmen-tation. Further robustness to unseen codecs can be achieved by employing codec-specific data augmentations. Given that only clean speech data were used for pretraining, improvements may also be achieved by incorporating various data augmentations at the pretraining stage.\nIn Table 2, we show cross-model comparision and abla-tions on the pre-training stage that led to our choice of SSL encoder and data augmentation. Since the number of tests on AVspoof5 prog and eval sets were limited by the submission quota, we relied on the performance achieved on ASV5 dev, ASV2019 LA eval, and ITW datasets to pick the best candidate. All baseline SSL models used the same classifiers as SLIM, i.e., ASP+MLPs. The only difference of SLIM was the style-linguistics dependency embeddings learned from Stage 1 pre-training. In general, baseline SSL models (rows 1-6) do not generalize well to unseen attacks, where a marked discrepancy can be seen between the EERs obtained on ASV5 dev and the out-of-domain datasets. SLIM, on the other hand, significantly reduces the generalization gap. For example, with the same base WavLM encoder, baseline model (row 1) achieves EER 13.4% on ASV5 prog while SLIM (row 7) achieves 7.1%. Additionally, though previous works have reported improvements with full finetuning of the SSL encoder [33], we noticed that full finetuning on ASV5 exacerbates the generalization issue and leads to worse performance than using a frozen backbone. That said, full finetuning usually requires careful selection of hyper-parameters and more compute to train. The results here may not represent the best of full finetuned models due to the time constraint of the challenge.\nThe ablation part of Table 2 (lower half) presents other fac-tors that contribute to our choice of candidate. The 7.1% EER on ASV5 prog was further decreased to 3.6% after RawBoost augmentation was applied. However, no further improvement was seen when combining noise and reverberation augmenta-tions with RawBoost. During training, we noticed that the ma-jority of samples can be easily correctly classified after the first epoch, and our model struggled to learn on the remaining hard samples (mostly short and noisy recordings) throughout all re-maining epochs. This motivated us to substitute BCE loss with Focal loss [34], as the latter forces the model to focus on hard samples. The loss modification led to another improvement of SLIM which decreases the EER to 2.7%. However, the focal loss modification was not integrated with our final submitted system (row 12) due to time constraints. Lastly, we found that decreasing batch size from 8 to 4 (before augmentation) also helped with improving the prog EER by 1.2%. We conjecture that this may be due to the zero-padding performed at the batch level during training, as the spoof and bonafide samples would share the same frames with zero values near the end of record-ing that may confuse the model.\nFinally, Table 3 summarizes a few factors at pre-processing level that were shown to significantly impact the model perfor-mance. As can be seen, longer training samples do not nec-essarily lead to better performance for ASV5, which partially contradicts the monotonic relationship between audio length and detection performance reported in the previous work [18]. One potential cause could be the different distributions of audio lengths for spoof and bonafide in ASV5 train. Unlike in pre-vious challenges, over 90% of bonafide samples are found to be more than 10s long while the majority of spoof samples are shorter than 10 s. This poses a challenge for determining the op-timal truncation length of training samples, as taking the entire"}, {"title": "4.2. Potential causes of misclassifications", "content": "While our SLIM model demonstrated good generalizability, it is crucial to investigate the causes of misclassifications. Figure 3 depicts the distribution of Mean Opinion Score (MOS) estimated by the NISQA model [35] for ASV5 train, dev, and 40k samples from the eval sets. Training data show significantly higher speech quality than dev and eval sets, where 31.3% and 33.9% of dev and eval (subset) samples have NISQA-MOS < 3 while only 17.4% of training data fall in this range. Furthermore, it was found that the misclassified dev samples by SLIM are mostly with NISQA-MOS \u2264 3, of which the speech content was nearly unintelligible. Meanwhile, after applying a speaker diarization model on 70k samples randomly selected from the eval set, we noticed that nearly 10% data may include more than one speaker, representing a conversational or overlapping speech settings. Since the pretraining stage of SLIM was performed within a single speaker scenario, the multi-speaker samples will likely lead to a mismatch detected between style and linguistics, and finally resulting in a misclassification."}, {"title": "5. Conclusion", "content": "In this paper, we present Reality Defender's submission to the ASVspoof5 challenge. Our submitted system SLIM achieved competitive results on the ASVspoof5 eval set as well as two out-of-domain deepfake datasets. Our findings suggest that the self-supervised contrastive learning stage of SLIM can effec-tively improve the generalizability to unseen attacks. Further re-search is needed to improve the performance in a multi-speaker setting, and for robustness to specific compression codecs."}, {"title": "7. Appendix", "content": "7.1. Hyperparameters\nTable 4 describes the hyperparameter settings and the architec-ture details of SLIM for Stage 1 and Stage 2 training. Both training stages were performed using SpeechBrain v1.0.0. Ex-periments were conducted on the Compute Canada cluster with four NVIDIA V100 GPUs (32GB RAM) for Stage 1 and one A100 (40GB RAM) for Stage 2.\n7.2. Projector and classifier architecture\nFigure 4 shows the architecture of the projector network. The input is first passed through a pooling layer to obtain an aver-age of different layer outputs. Bottleneck layers are designed to remove the redundant information that is not shared across style and linguistics aspects. The bottleneck layer first maps the feature dimension from 768-dim to 256-dim, then recovers it back to 768-dim. In practice, we found using only one bot-tleneck layer is enough to obtain meaningful compressed repre-sentations. A projection head is applied at the end to reduce the final output dimension to 256. The 256-dim embeddings then pass through a fully-connected layer, a dropout layer, and a final projection layer that yields a single output score."}]}