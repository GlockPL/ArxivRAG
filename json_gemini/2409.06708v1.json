{"title": "Ensuring Fairness with Transparent Auditing of Quantitative Bias in AI Systems", "authors": ["Chih-Cheng Rex Yuan", "Bow-Yaw Wang"], "abstract": "With the rapid advancement of AI, there is a growing trend to integrate AI into decision-making processes. However, AI systems may exhibit biases that lead decision-makers to draw unfair conclusions. Notably, the COMPAS system used in the American justice system to evaluate recidivism was found to favor racial majority groups; specifically, it violates a fairness standard called equalized odds. Various measures have been proposed to assess AI fairness. We present a framework for auditing AI fairness, involving third-party auditors and AI system providers, and we have created a tool to facilitate systematic examination of AI systems. The tool is open-sourced and publicly available. Unlike traditional AI systems, we advocate a transparent white-box and statistics-based approach. It can be utilized by third-party auditors, AI developers, or the general public for reference when judging the fairness criterion of AI systems.", "sections": [{"title": "I. INTRODUCTION", "content": "The accelerating pace of artificial intelligence (AI) technologies has revolutionized numerous fields in recent years. In healthcare, AI-driven diagnostic systems streamline disease identification, while in finance, automated trading algorithms analyze market trends to execute optimal trades swiftly. This remarkable progress has reached diverse industries, sprouting out various applications for different purposes.\nOne of its most profound impacts is how AI has transformed decision-making processes across sectors. By harnessing vast amounts of data and employing advanced algorithms, AI empowers organizations to make more informed and strategic decisions. From assessing job applicants to determining school admissions, AI-driven insights offer efficient and analytical advantages.\nAl systems, however, may be biased. Factors such as inherent biases in original data sets or flaws in algorithm designs could contribute to bias within Al systems. When left unchecked, the ramifications of such biases extend far beyond mere inaccuracies; they can lead to devastating consequences, such as amplifying systemic injustices, perpetuating group discrimination, and exacerbating societal inequalities.\nCorrectional Offender Management Profiling for Alternative Sanctions (COMPAS) is a tool for predicting recidivism\u2014the tendency of criminals to reoffend. It is used in the criminal justice system in multiple states in the United States. In 2016, it was discovered in an investigation by the journalists at ProPublica that the COMPAS system is, in fact, unfair towards minority and disadvantaged groups.\nCases such as COMPAS underscore the critical importance of rigorously examining the fairness of AI systems by third parties. Fairness is fundamentally a subjective social construct, heavily influenced by cultural context and deeply rooted in historical inequalities. However, there have been developments in research leveraging statistical metrics to quantify fairness that provide transparent and objective insights.\nThese statistical metrics offer a systematic approach to evaluating fairness across various dimensions of AI systems. Among these metrics are disparate impact and demographic parity, to name a few. In particular, the findings of the COMPAS report revealed that it violates a critical metric known as equalized odds.\nThe function of our framework is to make comprehensive reviewing of statistical metrics accessible to third party audi-tors. Central to this role is the precise definition and flexible abstraction of the metrics. For example, our framework would reveal that the COMPAS dataset not only violates equalized odds but also conditional statistical parity and mean difference. Including independent and trusted auditors is pivotal for objectivity and accountability. Moreover, the involvement of third parties is often indispensable due to their specialized expertise in relevant domains. Auditors, equipped with our tool, can thoroughly review AI systems for bias and fairness violations.\nOur tool builds upon transparent definitions widely accepted within the scientific community. By providing an abstraction layer atop these definitions, we enhance their flexibility and transparency, which enables users to adapt them to diverse contexts with ease and reveal them to the public. Moreover, we prioritize transparency throughout our tool's development process, offering access to the source code for scrutiny, thereby ensuring transparency from conception to implementation.\nOur open-sourced tool is written in Python and is offered as a Python package. It supports common dataset formats such"}, {"title": "II. FAIRNESS MEASURES", "content": "Fairness is about making sure the disadvantaged and unpriv-ileged groups of individuals are treated equitably. However, there have been several interpretations of it proposed in the past. To have a constructive discussion on fairness, we must first have precise definitions of it. Pessach and Shmueli [1] formulated a number of fairness measures in a unified mathematical notation. We will base our framework on their formulation.\nFor the following definition, we will use $Y$ to denote the ground truth of an outcome; $\\hat{Y}$ to denote the predicated result of an outcome; $Y = 1$ and $\\hat{Y} = 1$ to denote them being accepted or positive. For example, let $Y$ be recidivism. Then $Y = 1$ means the case of an individual actually recidivating and $\\hat{Y} = 0$ means the prediction of an individual recidivating is negative. In addition, we use $V$ and $\\hat{V}$ when the truth and the prediction are not binary. For example, we denote the COMPAS score by $V$, which ranges from 1 to 10.\nProtected attributes are the characteristics of individuals that are, for example, legally or ethically, considered sensitive and warrant protection against discrimination and bias. We denote by $S$ some protected attribute. We write $S = 1$ to represent the privileged group and $S \\neq 1$ to represent the unprivileged group. For example, let $S$ be Caucasian. Then $S = 1$ represents the case of an individual's race being Caucasian and $S \\neq 1$ vice versa.\nWe denote by $\\epsilon$ some threshold that is used to limit the fairness measures.\nIn 1971 [2], the US Supreme Court ruled that it is illegal for hiring decisions to have \"disparate impact\" by race, thus coining the term. It is taken as unintentional discrimination, as opposed to intentional discrimination, which is called \"disparate treatment\".\nLegal cases involving disparate impact often refer to the \"80% Rule\", advocated by the US Equal Employment Oppor-tunity Commission [3], where it requires the selection rate of a minority group to be no less than 80% of that of a majority group. Formally [4], it is:\n$\\frac{P[\\hat{Y} = 1|S \\neq 1]}{P[\\hat{Y} = 1|S = 1]} \\geq 1-\\epsilon$\nIn the case of 80% rule, $\\epsilon = 20\\%$.\nDemographic parity [1], also known as statistical parity, is similar to disparate impact but, instead of ratio, the difference is taken. It is named so to suggest that each demographic group(such as race, gender, or age) should have equal repre-sentation or opportunity. Formally, it is:\n$|P[\\hat{Y} = 1|S = 1] - P[\\hat{Y} = 1|S \\neq 1]| \\leq \\epsilon$\nConditional statistical parity [6] is similar to demographic parity, but, in addition to protected attributes, it further takes into account some \"legitimate\" attributes that are legitimately related to the case. For example, a legitimate attribute when considering future recidivism could be the number of prior crimes committed. Formally, it is:\n$|P[\\hat{Y} = 1|S = 1, L = l] - P[\\hat{Y} = 1|S \\neq 1, L = l]| < \\epsilon$\nwhere $L$ denotes the legitimate attributes.\nOverall accuracy equality [7] is similar to demographic parity, but instead of the case of $\\hat{Y} = 1$, it considers the case of $Y = \\hat{Y}$; that is, the case where the prediction is accurate. Formally, it is:\n$|P[Y = \\hat{Y}|S = 1] - P[Y = \\hat{Y}|S \\neq 1]| \\leq \\epsilon$\nMean difference [8] considers the expected value of the prediction. Formally, it is:\n$|E[\\hat{Y}|S = 1] - E[\\hat{Y}|S \\neq 1]| \\leq \\epsilon$\nEqualized odds [9] is similar to demographic parity, but it further takes into account the ground truth. It considers the cases of true positive and false positive. It solves the downsides of a fully accurate classifier that might be deemed unfair by measures that do not consider ground truth. For example, consider a group A that is predicated to recidivate and they do in fact recidivate and a group B that is predicated to recidivate but end up never recidivating. A fully accurate classifier will always predict A to recidivate while B to never recidivate, and this will violate demographic parity. By taking ground truth into account, equalized odds avoids these pitfalls. Formally, it is:\n$|P[\\hat{Y} = 1|S = 1,Y = 0] - P[\\hat{Y} = 1|S \\neq 1, Y = 0]| \\leq \\epsilon$\n$|P[\\hat{Y} = 1|S = 1, Y = 1] - P[\\hat{Y} = 1|S \\neq 1, Y = 1]| < \\epsilon$\nEqual opportunity [9] is a relaxation of equalized odds by only considering the true positive case. Formally, it is:\n$|P[\\hat{Y} = 1|S = 1, Y = 1] - P[\\hat{Y} = 1|S \\neq 1, Y = 1]| < \\epsilon$\nPredictive equality [6] is also a relaxation of equalized odds by only considering the false positive case. Formally, it is:\n$|P[\\hat{Y} = 1|S = 1,Y = 0] - P[\\hat{Y} = 1|S \\neq 1, Y = 0]| < \\epsilon$"}, {"title": "J. Conditional Use Accuracy Equality", "content": "Conditional use accuracy equality [7] is similar to equalized odds, but instead of conditioning on the ground truth, it conditions on the prediction and calculates the probability of the ground truth. It can be seen as checking the prediction accuracy across groups, thus the name. It further requires the measure in the case of positive predictive values to be less than that of the negative predictive values. Formally, it is:\n$|P[Y = 1|S = 1,\\hat{Y} = 1] - P[Y = 1|S \\neq 1, \\hat{Y} = 1]| < \\epsilon$\n$|P[Y = 0|S = 1,\\hat{Y} = 0] - P[Y = 0|S \\neq 1, \\hat{Y} = 0]| \\leq \\epsilon$\nPredictive parity [10] is a relaxation of conditional use accuracy equality by only considering the positive predictive value case. Formally, it is:\n$|P[Y = 1|S = 1,\\hat{Y} = 1] - P[Y = 1|S \\neq 1, \\hat{Y} = 1]| < \\epsilon$\nEqual calibration [10] is similar to equal opportunity, but instead of having a binary $\\hat{Y}$, it is conditioned on the range of the predicted value $V$. For example, this could be conditioned on the highest COMPAS score $V = 10$. Calibration is a concept of having a fair score function [11]. Formally, it is:\n$|P[Y = 1|S = 1, V = v] - P[Y = 1|S \\neq 1, V = v]| < \\epsilon$\nPositive balance [13] is similar to equal opportunity, but instead of taking the difference of probability of binary pre-diction $\\hat{Y}$, it takes the difference of the expected value of the score $V$, which may be non-binary, such as the score of COMPAS. Formally, it is:\n$|E[V|Y = 1, S = 1] - E[V|Y = 1, S \\neq 1]| \\leq \\epsilon$\nNegative balance [13] is like positive balance except it conditions on the case of $Y = 0$. Formally, it is:\n$|E[V|Y = 0, S = 1] - E[V|Y = 0, S \\neq 1]| \\leq \\epsilon$\nThese fairness measures are compiled in Table I."}, {"title": "III. AUDITING FRAMEWORK", "content": "Per the review by Pessach and Shmueli [1], we designed an auditing framework for calculating the various fairness measures. We offer two versions of fairness checkers: one for when the prediction results are readily available in CSV input and one for when a model is provided. In most auditing cases the model version is preferred because CSV results can be easily fabricated.\nTypes $\\alpha, \\beta$ are stand-ins for any type. Let $\\alpha \\rightarrow \\beta$ denote a function from type $\\alpha$ to type $\\beta$. $a_i$ denotes some particular type; $\\prod_i a_i$ denotes the Cartesian product of multiple $a$ of possibly different types. We write $x: a$ to mean $x$ is of the type $a$. We write str for the string type, bool for the boolean type, and int for the integer type.\nA database $D = \\{r_1, r_2, ...\\}$ is a collection of rows. A row $r_i: key \\rightarrow value$ is a lookup table or dictionary, where the concrete type of key and value is str. For example, $r_n(\"sex\") = \u201cFemale\u201d$ means $r_n$'s sex is female. Henceforth, we will use row as a type synonym of $key \\rightarrow value$.\nA model $M: row \\rightarrow a$ is a black-box predictor that takes a row and returns its prediction result of some type $a$; for example, if the model returns a string then $a = str$.\nA privileged predicate $R: row \\rightarrow bool$ takes a row and determines if it belongs to the privileged group. For example, $R(r_i) := r_i(\u201crace\u201d) == \u201cCaucasian\u201d$ means the privileged group is those with race being Caucasian.\nA positive predicate of rows $P : row \\rightarrow bool$ takes a row and determines if its prediction is positive. For example, $P(r_i) := int(r_i(\u201cscore\u201d)) > 7$ means a row's prediction is positive if its score is greater than 7. A positive predicate of model results $P_M : a \\rightarrow bool$ takes a model's result and determines if it is positive. In the simplest case, the model returns a bool, and the positive predicate can just be the identity function.\nA score predicate of rows $\\hat{S} : row \\rightarrow \\beta$ takes a row and gives its predicted score of some type $\\beta$. Similar to positive predicate, a score predicate of model results $\\hat{S}_M : a \\rightarrow \\beta$ takes a model's result and gives its score.\nA ground truth predicate $T : row \\rightarrow bool$ takes a row and gives the ground truth of the result.\nA legitimate predicate $L : \\prod_i a_i \\rightarrow row \\rightarrow bool$ takes n parameters and returns a row predicate. For exam-ple, $L(x)(r_i) := int(r_i(\u201cpriors_count\u201d)) > x$ first takes a parameter x and then decides if priors count is larger than it.\nA calibration predicate $C : \\prod_i a_i \\rightarrow row \\rightarrow bool$, takes n parameters and returns a row predicate. For example, $C(u,l)(r_i) := l < int(r_i(\u201cscore\u201d)) < u$ first takes two parameters u,l as upper bound and lower bound, and then decides if $r_i$'s prediction score is between them.\nWe abstracted the idea of privileged groups and positive prediction as predicates to maximize flexibility. With proper predicates, any model output can be used; even prose-like responses of generative models can be included given adequate predicates.\nGiven a dataset $D$ for auditing use, and given CSV pre-diction results or the prediction model $M$ itself, we can calculate the fairness measures by modeling the statistical random variables mentioned in Section II with our predicates. We demonstrate this process in the following for a few selected measures; all the other measures can be modeled similarly."}, {"title": "IV. APPLICATION", "content": "In this section, we will apply the proposed framework to the ProPublica COMPAS dataset [14]-[16].\nCorrectional Offender Management Profiling for Alternative Sanctions (COMPAS), developed by the private company Northpointe (now Equivant), is a risk assessment software used in the American criminal justice system to evaluate the likelihood of a defendant reoffending. Defendants taking the COMPAS test are given a questionnaire about topics ranging from family history to personal ideology. The questionnaire is then fed to the software system along with a number of parameters like the defendants' age, and then the system will assign a risk score to them from 1-10, with 10 being the highest risk.\nProPublica is an American non-profit journalism organiza-tion focused on public interests. In 2016, they conducted an investigative report into the COMPAS system. They obtained the 2013-2014 COMPAS score data of over 10,000 defendants in Florida. They also obtained criminal records of these defen-dants through 2016 and compared if they actually recidivate or not. They only counted misdemeanors and felonies as recidivism but not less serious crimes such as infractions. In the study, they have found that black defendants are disproportionately scored higher than they actually are, and white defendants are disproportionately scored lower than they actually are.\nWe shall start applying our framework. There is generally no formal guide on how to set $\\epsilon$, so we will take the 80% rule's case and set it as $\\epsilon = 0.2$.\nWe will set the unprivileged predicate to be \"African-American\" so that the unprivileged group will be African-American. Then we test if the African-American race is discriminated.\n$R(r_i) := r_i(\u201crace\u201d) \\neq \u201cAfrican-American\u201d$\nAs ProPublica referenced Northpointe's COMPAS Practi-tioners Guide and cited that \u201cmedium\u201d(5-7) and \u201chigh\u201d(8-10) categories of scores are considered to indicate a risk of recidivism, we set the positive predicate using the readily available category.\n$P(r_i) := r_i(\u201cscore_text\u201d) \\in \\{\u201cMedium\u201d, \u201cHigh\u201d\\}$\nFor the ground truth predicate, the recidivism data is already present in the ProPublica dataset, so all we have to do is a simple lookup.\n$T(r_i) := r_i(\"two\\_year\\_recid\u201d) == \u201c1\u201d$\nFor the score predicate, it is again a simple lookup.\n$\\hat{S}(r_i) := int(r_i(\u201cdecile\\_score\u201d))$\nFor the legitimate predicate, we may want to look at defendants with priors.\n$L(0)(r_i) := int(r_i (\u201cpriors\\_count\u201d)) > 0$\nFor the calibration predicate, we may want to look at risk scores within a specific range.\n$C(7,5) (r_i) := 5 < int(r_i(\u201cdecile\\_score\u201d)) < 7$\nWith these predicates defined, we can call the fairness measure functions and check if the measures hold or not. The results are compiled in Table II. Since some fairness measures are equivalent, we write them in the same entry."}, {"title": "B. Fairness Analysis of African-American group", "content": "On first blush, it is curious that it satisfies disparate impact but not demographic parity. However, if we return to the definition, we would see that disparate impact is meant to be used when being marked positive is an advantaged thing, while here in the COMPAS example, being marked positive is a disadvantaged thing.\nIt deceives the eye due to its definition being a ratio that lacks symmetry upon interchange of its components. Henceforth we will exclude disparate impact from our analysis of COMPAS. On the contrary, the other measures remain unaffected since their definitions are the absolute values of a difference.\nWe can then immediately tell from the failing demographic parity, mean difference, and conditional statistical parity that COMPAS prediction results were unfair against African-American, even if we only consider the ones with prior crimes. From the low overall accuracy equality and both conditional use accuracy equality criteria, we can tell that the accuracy is similar across African-Americans and non-African-Americans. From the failing equalized odds we can conclude that African-Americans are indeed treated unequally by the COMPAS system even after the ground truth is taken into account. This is the same conclusion reached by the ProPublica report. From the low equal calibration we can tell that if we only consider the medium risk score, African-Americans are treated fairly.\nFinally, if we look at positive and negative balance, we can see that they're of similar numbers. Their average is 1.5 on a scale of 10, which means approximately a 15% difference. It remains to be said if this is fair or not. An auditor could consult a domain expert for advice on how to set a $\\epsilon$ and how it is justified."}, {"title": "C. Fairness Analysis of Different Races", "content": "By setting the privilege predicate to different races we can have a more comprehensive look over the dataset. We have checked the case of unprivilege predicate being African-American, Asian, Caucasian, Hispanic, and Native American. The results are shown in Figure 1.\nFrom the results, we can first notice that the overall accuracy equality is low for both the Asian and Native American cases. This can be explained by checking the original dataset which shows that there are only 32 and 18 rows, respectively, in a dataset of 7214 rows. Hence, the accuracy is naturally lower because of the small data size.\nLooking at the remaining three groups, African-American, Caucasian, and Hispanic, their accuracies are relatively much better.\nAs for fairness measures, we can immediately see from that all measures are $\\epsilon < 0.2$ for Caucasian and Hispanic that African-American are indeed treated unfairly in a broad sense. More specifically, African-American are treated more un-fairly according to demographic parity, conditional statistical parity, mean difference, and equalized odds. This is again in accordance with the conclusion of the ProPublica report."}, {"title": "D. Fairness Analysis of Different Groups", "content": "On the other hand, we also analyzed the case of unprivileged group being across the three age groups and the case of privileged group of sex being Male and charged degree being misdemeanor in Figure 2.\nWe can see that, interestingly, the group of age \"25 - 45\" group receives generally fair treatment. Its data size is also the largest at 4109 rows, whereas \"Less than 25\" age group has 1529 rows and \"Greater than 45\" age group has 1576 rows, so the possibility of skewed data is unlikely.\nIf we look closer we can see that in the age group \"Less than 25\", its predictive equality is noticeably worse, meaning in young people false positive rate is higher than in older people; conversely, when we look at the age group \"Greater than 45\", both equal opportunity and predictive equality are worse, meaning for old people their treatment is even more unfair than young people. Only the middle age group is treated fairly.\nIn another vein, the case of \"Male\" group and \u201cMisde-meanor\u201d group are both treated rather fairly. Furthermore, their overall accuracy equality are both excellent at the < 0.1 range. The predictive equality of \"Male\" group even has a measure as low as 0.000004."}, {"title": "E. Analysis with Model-as-input", "content": "Although this dataset scenario falls in the CSV-as-input case in our framework, we also trained a simple makeshift model using the dataset itself for demonstration.\nThe model was trained by splitting the original dataset into 3 partitions: 60% of the data was used for training; 20% was used for validation; and the rest 20% was used for calculating fairness measures. Validation showed that the accuracy was about 60%.\nWe only chose equalized odds of the case where unprivi-leged group is African-American for illustrative purposes. The equalized odds of our model is 0.35 and 0.46, while that of the COMPAS dataset is 0.23 and 0.22. So, our simple naive model is more unfair than COMPAS's model and could use some more fine-tuning."}, {"title": "V. CONCLUSION", "content": "Addressing the challenges brought upon by the rapid ad-vancements in AI technologies and its introduced bias neces-sitates a rigorous assessment of AI system fairness. While fair-ness is subjective, it can be defined through various statistical measures. Our research contributes to this effort by proposing a comprehensive framework for auditing Al systems using multiple white-box fairness metrics, such as demographic parity, equalized odds, and overall accuracy equality.\nBy applying this framework to the COMPAS dataset, we confirmed that African-American defendants were unfairly scored compared to other racial groups. These results align with ProPublica's findings, demonstrating the utility and ac-curacy of our fairness auditing tool. Developed in Python and publicly available, this tool enables third parties to conduct detailed assessments of AI systems, promoting transparency and accountability."}]}