{"title": "Graph Diffusion Network for Drug-Gene Prediction", "authors": ["Jiayang Wu", "Wensheng Gan", "Philip S. Yu"], "abstract": "Predicting drug-gene associations is crucial for drug development and disease treatment. While graph neural networks (GNN) have shown effectiveness in this task, they face challenges with data sparsity and efficient contrastive learning implementation. We introduce a graph diffusion network for drug-gene prediction (GDNDGP), a framework that addresses these limitations through two key innovations. First, it employs meta-path-based homogeneous graph learning to capture drug-drug and gene-gene relationships, ensuring similar entities share embedding spaces. Second, it incorporates a parallel diffusion network that generates hard negative samples during training, eliminating the need for exhaustive negative sample retrieval. Our model achieves superior performance on the DGIdb 4.0 dataset and demonstrates strong generalization capability on tripartite drug-gene-disease networks. Results show significant improvements over existing methods in drug-gene prediction tasks, particularly in handling complex heterogeneous relationships. The source code is publicly available at https://github.com/csjywu1/GDNDGP.", "sections": [{"title": "I. INTRODUCTION", "content": "The application of artificial intelligence (AI) in bioinformatics has greatly advanced tasks like predicting protein-protein interactions (PPI) [1], gene regulatory network (GRN) [2], metabolic pathway [3], and disease-disease associations (DDA) [4]. These tasks are essential for understanding cellular processes [5], metabolic functions [6], and disease mechanisms [7], contributing to drug discovery and personalized medicine. AI-driven methods, particularly graph neural network (GNN) [8], have proven effective in modeling complex biological networks, allowing for more accurate predictions of interactions between biological entities. However, the drug-gene prediction task remains challenging, which is crucial for drug development and therapeutic research.\nDrug-gene prediction plays a pivotal role in bioinformatics, aiming to identify interactions between drugs and genes. Such interactions are essential for advancing drug development [9], pinpointing therapeutic targets [10], and unraveling disease mechanisms at the genetic level [11]. Drug-gene interactions can reveal how specific drugs influence gene expression or interact with genetic variants, which is pivotal for designing personalized medicine approaches and repurposing existing drugs for new therapeutic uses. The importance of drug-gene prediction stems from its ability to improve the precision and efficiency of the drug-discovery process. By understanding how drugs affect gene networks, researchers can better predict drug efficacy, potential side effects, and interactions with genetic disorders. This information is invaluable for developing targeted therapies that are more effective and have fewer adverse effects. Moreover, drug-gene prediction plays a role in drug repositioning, where known drugs are found to have therapeutic effects on diseases outside their original intended use, thereby accelerating the drug development pipeline. AI-driven models, particularly those based on GNN, offer promising solutions to the challenges of drug-gene prediction [8]. Such models are capable of capturing the intricate relationships and dependencies between drugs and genes within heterogeneous biological networks. By learning meaningful representations of the interactions between these biological entities, GNN-based methods can improve the accuracy of predictions, leading to more reliable insights into drug mechanisms and potential therapeutic applications.\nGNN-based methods are useful in drug-gene prediction. However, predicting relationships in drug-gene networks still poses additional challenges, such as data sparsity [12] and long-tail distributions [13]. For example, many drugs may have few known associations with genes, making it harder to generalize predictions based on limited information. To address these challenges, one promising solution is the construction of meta-paths for homogeneous message passing [14]. Meta-paths capture the higher-order relationships between entities, such as drug-gene-drug (D-G-D) and gene-drug-dene (G-D-G) [15], thereby facilitating the discovery of indirect connections. This approach not only mitigates the aforementioned challenges but also extends the algorithm's capabilities to handle tripartite networks, such as drug-gene-disease networks. Previous methods often focused on binary relationships, limiting their effectiveness when extended to multiple entity types [16]. Our experiments demonstrate that the introduced meta-path message passing can perform well in a tripartite network. Contrastive learning is frequently employed in link prediction tasks to differentiate positive from negative samples [17], [18]. However, the volume of negative samples makes it computationally impractical to analyze them all. An effective approach to tackle this challenge involves generating high-quality, hard negative samples as a substitute for retrieving all unlinked data. In our framework, we adopt a graph diffusion network to generate these hard negative samples [19]. Incorporating the diffusion process allows us to create increasingly challenging negative samples, enhancing the model's robustness and generalization. In addition, it can generate various hard degree samples with different steps.\nTo tackle these challenges, we introduce a model named graph diffusion network for drug-gene prediction (GDNDGP)."}, {"title": "II. RELATED WORK", "content": "Traditional methods often fall short in capturing the dynamic and diverse interactions. This shortcoming has led to a growing interest in leveraging artificial intelligence (AI) to enhance drug-gene research. Identifying disease-associated long non-coding RNAs (lncRNAs) has been the focus of various models, highlighting the role of non-coding genetic elements in diseases. GAERF [20], introduced by Wu et al., employs graph autoencoders (GAE) combined with random forest techniques for detecting disease-related IncRNAs. In contrast, LPLNS [21] utilizes label propagation with linear neighborhood similarity to predict novel IncRNA-disease associations, thereby deepening our understanding of IncRNA roles in disease. IncRNA prediction methods add an extra layer of genetic interaction data, which, when integrated with drug-gene and miRNA predictions, offers a comprehensive view of genetic influences on diseases. Moreover, predicting disease-associated miRNAs sheds light on how these small RNA molecules regulate genes, serving as a vital complement to drug-gene interaction prediction. SMALF [22], created by Liu et al., integrates latent features from the miRNA-disease association matrix with original features, utilizing XGBoost [23] to predict unknown miRNA-disease associations. Likewise, CNNMDA [24], introduced by Xuan et al., employs network representation learning in conjunction with convolutional neural networks (CNN).\nRecent progress in AI, especially with graph neural networks (GNNs), has demonstrated significant potential in uncovering drug-gene associations. GNNs excel at capturing patterns from graph-structured data, making them highly effective in modeling drug-gene relationships. These models facilitate the identification of potential interactions, which is valuable for drug repurposing and discovering new therapeutic targets. For example, SGCLDGA [25] integrates GNNs with contrastive learning, using graph convolutional network (GCN) [26] to derive vector representations of drugs and genes from a bipartite graph. It employs singular value decomposition (SVD) to refine the graph and create multiple views. Further, it optimizes these representations with a contrastive loss function to effectively differentiate positive and negative samples. In addition, multiple GNN-based methods have been developed for predicting miRNA-disease associations. HGCNMDA [27] combines GNNs with node2vec and GCN to jointly learn features of miRNAs and diseases, offering a comprehensive perspective by integrating diverse biological networks. LAGCN [4] employs graph convolution along with attention mechanisms to learn and integrate embeddings for drugs and diseases from heterogeneous networks, effectively concentrating on crucial features to enhance prediction accuracy. Lastly, NIMCGCN [28] applies GCNs to uncover hidden features of miRNAs and diseases from similarity networks, thereby improving the discovery of novel miRNA-disease associations by leveraging similarities in known associations. These miRNA-disease prediction models contribute an additional layer of insight into the genetic underpinnings of diseases, which can aid in developing more targeted therapies when combined with drug-gene interaction predictions.\nMeta-path-based methods are extensively applied in drug-disease association prediction due to their effectiveness in revealing latent relationships through intermediate nodes within heterogeneous networks. A meta-path, defined as a sequence of relations linking different node types, captures essential semantic information, thereby enhancing predictive accuracy. MGP-DDA, in studying drug-disease associations, constructs a tripartite network involving drugs, gene ontology (go) functions, and diseases, employing three specific meta-paths to capture their interrelations [29]. These meta-paths generate features based on path instance counts, which are then used for drug-disease association prediction, showing robust performance by retaining richer semantic information from intermediate nodes. Similarly, HSGCLRDA integrates meta-path aggregation in a drug-disease-protein heterogeneous network [30]. By constructing both global and local feature graphs through meta-paths, HSGCLRDA can capture detailed structural and contextual information about the interactions between drugs, diseases, and proteins. This meta-path-based aggregation is combined with contrastive learning to optimize feature representations, leading to improved prediction accuracy. Both methods highlight the strength of meta-path-based techniques in enhancing prediction tasks by retaining and utilizing rich semantic information from heterogeneous networks. These approaches not only enable the discovery of novel drug-disease associations but also contribute to the broader understanding of the underlying biological mechanisms."}, {"title": "III. PRELIMINARIES", "content": "Given a set of drugs $D$, a set of genes $G$, and a set of known associations $A\u2286 D \u00d7 G$, the goal of drug-gene prediction is to identify unknown associations between drugs and genes. Specifically, for a given drug $d_i \u2208 D$, our task is to estimate the likelihood of its association with a gene $g_j\u2208 G$ using known drug-gene relationships. We represent the drug-gene prediction problem using a graph $G = (V, E)$, where the vertex set $V = D\u222aG$ includes both drugs and genes, and the edge set $& C D \u00d7 G$ denotes existing drug-gene associations. Our objective is to infer missing edges in &, which correspond to potential novel drug-gene associations.\nDefinition 1 (meta-path): A meta-path is a sequence of node types and edge types that defines a composite relationship between two nodes in a heterogeneous graph [31]. Specifically, in a graph where there are multiple types of nodes (e.g., drugs, genes, and diseases) and multiple types of relationships (e.g., drug-gene interactions, gene-disease associations), a meta-path describes a path schema that connects nodes through specific types of relationships. For instance, in a drug-gene prediction task, a meta-path such as drug-gene-drug (D-G-D) can be used to capture indirect associations between drugs by sharing common genes. Meta-paths are crucial for enhancing message passing in heterogeneous graphs, as they help capture higher-order relationships that would be missed by simple direct connections [32], [33].\nDefinition 2 (homogeneous graph and heterogeneous graph): A homogeneous graph consists of nodes and edges that all represent the same type of entity. The relationships between these entities are also of the same kind. For example, a graph where all nodes represent genes and all edges represent gene-gene interactions is considered a homogeneous graph. In contrast, a heterogeneous graph is a more complex structure that contains multiple types of nodes and edges. For instance, such a graph might include nodes for drugs and genes, with edges representing interactions like drug-gene associations.\nDefinition 3 (contrastive learning): Contrastive learning is a self-supervised learning method designed to learn representations by distinguishing between positive and negative pairs [34]. In graph-based learning, contrastive learning encourages the model to assign higher similarity scores to positive pairs (e.g., drug-gene pairs with known associations) and lower similarity scores to negative pairs (e.g., drug-gene pairs without associations). Common loss functions in contrastive learning, such as InfoNCE [35] or triplet loss [36], guide the model to develop meaningful embeddings by maximizing the similarity between positive samples while minimizing it for negative samples. This approach enhances the model's capability to differentiate between similar and dissimilar nodes within a graph, thereby improving its generalization performance.\nDefinition 4 (graph convolutional network (GCN)): A GCN is a neural network architecture specifically designed to operate on graph-structured data [26]. In GCNs, each node's feature representation is updated by aggregating information from its neighboring nodes. Formally, given a graph $G = (V,E)$ with node set V and edge set E, and a feature matrix $X \u2208 R^{N\u00d7D}$ where N denotes the number of nodes and D represents the feature dimension, the forward propagation of a GCN layer is defined as: $H^{(l+1)} = \u03c3(D^{-1/2} \\hat{A} D^{-1/2} H^{(l)}W^{(l)})$, where $\\hat{A} = A + I$ is the adjacency matrix with self-loops added, D is the degree matrix of $\\hat{A}$, $W^{(l)}$ is the trainable weight matrix for the l-th layer, and $\u03c3$ represents a non-linear activation function (e.g., ReLU). The initial input is $H^{(0)} = X$, which is the original feature matrix. This layer-wise propagation allows GCNs to capture both the structural and feature information of the graph [37], making them effective for tasks like node classification, link prediction, and graph embedding.\nDefinition 5 (diffusion network): A diffusion network is a model that simulates the process of diffusion across a graph, allowing information (or noise) to propagate over the nodes with time [38]. In the context of drug-gene prediction, a diffusion network can be employed to generate hard negative samples by gradually introducing noise into the gene embeddings and subsequently reversing this noise during training. During the forward process, noise is added to the gene embeddings across multiple time steps, resulting in a variety of negative samples. For each time step $t\u2208 \\{1,2,...,T\\}$, the negative gene embedding $E_{gt}$ is calculated as follows: $E_{gt} = \\sqrt{\\bar{\u03b1}_t} E_{g0} + \\sqrt{1 - \\bar{\u03b1}_t} \u03b5_t$. Here, $E_{gt}$ represents the negative gene embedding at time step t, and $E_{g0}$ is the initial gene embedding. The noise vector $\u03b5_t$ is drawn from a standard normal distribution, $\u03b5_t \\sim N(0, I)$, where I is the identity matrix."}, {"title": "IV. ALGORITHM", "content": "GDNDGP is a framework developed for drug-gene prediction utilizing GCN. Initially, it randomly initializes embeddings for both drugs and genes. To enhance the relationships among homogeneous nodes, the framework constructs meta-paths for drug-gene-drug and gene-drug-gene interactions, enabling efficient message passing not only between drugs and genes but also across drug-drug and gene-gene pairs. GDNDGP then facilitates message transfer within the heterogeneous graph, ensuring information exchange for both drugs and genes. To effectively distinguish the relationships between these entities, two types of contrastive learning losses are employed: contrastive loss I and contrastive loss II. Positive samples are derived from drug-gene pairs with confirmed associations, while negative samples are generated from unlinked drug-gene pairs. Due to the sheer number of negative samples, retrieving all hard negative examples presents a computational challenge. To address this, GDNDGP incorporates a graph diffusion network to generate hard negative samples, aiding in the contrastive learning process. As shown in Fig. 2, the positive drug-gene pairs (top left) are contrasted with negative pairs (top right). The diffusion process (center) introduces noise to the negative samples through an encoder and removes noise through a decoder, generating progressively harder negative samples. These negative samples are then combined in a weighted manner, allowing the model to train more effectively. The final embeddings of drugs and genes are used to predict hidden associations between them, improving the accuracy of drug-gene predictions.\nIn meta-path construction, we focus on two types of meta-paths: drug-gene-drug (D-G-D) and gene-drug-dene (G-D-G).\nLet D represent the set of drugs, and G is the set of genes. For each drug $d\u2208 D$, we first record all genes $G_d \u2286 G$ that are related to it. For drugs associated with multiple genes, we generate all possible gene-gene relationships under that drug. Specifically, for any drug $d\u2208 D$ with an associated gene set $G_d = \\{g_1, g_2,..., g_n\\}$, we form all possible gene pairs $(g_i, g_j)$, where $g_i, g_j \u2208 G_d$ and $i \u2260 j$, thereby extracting all gene-gene relationships mediated by the drug. Similarly, for drug-drug relationships, we define two drugs $d_1 \u2208 D, d_2 \u2208 D$ as related if they share at least one common gene. This relation forms the drug-drug pair $(d_1, d_2)$ based on the shared gene information. To control the neighborhood size of each node (either drug or gene), we introduce a neighbor threshold, denoted as \u03c4. Let the set of neighbors for a node $v \u2208 D\u222aG$ be denoted as $N(v)$. We limit the size of $N(v)$ to at most \u03c4, i.e., $|N(v)| < \u03c4$. If the number of neighbors of a node v is less than \u03c4, we replicate its existing neighbors until $|N(v)| = \u03c4$. This ensures consistency in the neighborhood sizes across nodes and helps manage computational complexity, especially in the presence of large-scale relationships.\nDetails of the process of homogeneous graph generation. The relationships between drugs and genes are critical for defining the neighborhood structure, and these relationships are derived directly from the meta-paths (i.e., drug-gene-drug (D-G-D) and gene-drug-gene (G-D-G)). The input to the aggregation process consists of the drug set D, the drug embeddings $H_D \u2208 R^{|D|\u00d7d}$, the gene set G, and the gene embeddings $H_G \u2208 R^{|G|\u00d7d}$, where d is the embedding dimension. The meta-paths help define the relationships between drugs $R_D$ and genes $R_G$, which define the neighborhood structure for attention aggregation.\nThe D-G-D meta-path links drugs through shared genes, forming drug-drug relationships based on their common gene interactions. The G-D-G meta-path links genes through shared drugs, forming gene-gene relationships based on their common drug interactions. These meta-paths help to determine how neighbors should be selected and how much attention should be given to each neighbor during aggregation. By defining richer relationships in the graph, meta-paths enable the attention mechanism to focus on the most relevant neighbors-whether they are directly or indirectly connected through drugs and genes.\nFor each drug $d_i \u2208 D$, its neighbors are derived from the drug-gene relationships represented by $R_D$, which are constructed based on the meta-paths. For example, a drug-gene-drug meta-path will connect two drugs if they share a common gene. These relationships form the neighborhood $N(d_i)$ of drug $d_i$. The attention mechanism is then applied to the neighbors of $d_i$, which may include other drugs that are indirectly connected via common genes. The attention between $d_i$ and its neighboring drugs $d_j \u2208 N(d_i)$ is calculated by concatenating their embeddings, denoted as $e_{ij} = [h_{d_i} ||h_{d_j}]$, where || represents concatenation. The attention score $e_{ij}$ is then computed as: $l_{ij} = LeakyReLU(a^T e_{ij})$, where $a^T$ is the learnable attention vector. These attention scores are normalized using a softmax function to obtain the normalized attention weights: $\u03b1_{ij} = \\frac{exp(e_{ij})}{\\sum_{k\u2208N(d_i)} exp(e_{ik})}$. Finally, the embedding of each drug $d_i$ is updated through a weighted sum of its neighbors' embeddings: $h'_{d_i} = h_{d_i} + \\sum_{d_j \u2208N(d_i)} \u03b1_{ij}h_{d_j}$.\nSimilarly, for each gene $g_i \u2208 G$, the attention aggregation process follows the same procedure. The neighborhood for each gene $g_i$ is determined by the relationships in $R_G$, which are also defined based on the meta-paths (e.g., gene-drug-gene (G-D-G)). The updated embedding for each gene $g_i$ uses the same attention mechanism as for the drugs: $h'_{g_i} = h_{g_i} + \\sum_{g_j \u2208N(g_i)} \u03b1_{ij}h_{g_j}$.\nThis process captures the influence of connected drugs and genes based on their relationships in $R_D$ and $R_G$, as measured by the attention mechanism. By guiding the attention mechanism, the meta-paths ensure that the model focuses on the most relevant neighbors, facilitating the effective aggregation of information between connected drugs and genes.\nDetails of the process of heterogeneous graph generation. The drug embeddings $E_d^{(0)}$ and gene embeddings $E_g^{(0)}$, obtained from the homogeneous graph drug embeddings $H_d$ and gene embeddings $H_g$, are used as inputs. Specifically, the gene embeddings $E_g$ are updated based on the drug embeddings $E_d$, while drug embeddings $E_d$ are updated based on the gene embeddings $E_g$. The original adjacency matrix A represents the relationships between drugs and genes, where each entry $A_{ij}$ indicates the connection between drug $d_i$ and gene $g_j$. However, directly using the adjacency matrix A for graph-based operations can cause issues in terms of imbalance and numerical instability. Nodes with more connections (higher degree) would dominate the information propagation, leading to an uneven contribution from neighboring nodes. To address this, we use a normalized adjacency matrix $A_{norm}$, which ensures that the influence of each neighboring node is adjusted based on its degree [40]. Specifically, the normalization ensures that contributions from neighbors are scaled by their degrees, preventing nodes with many connections from overwhelming nodes with fewer connections. This is achieved through symmetric normalization, where the degree matrix D is used to scale the adjacency matrix symmetrically, producing $A_{norm} = D^{-1/2} A' D^{-1/2}$, where $A' = A + I$ is the adjacency matrix with added self-loops and I is the identity matrix. This symmetric normalization helps maintain balanced contributions from neighboring nodes during the aggregation process.\nFor gene embedding updates, the adjacency matrix A (representing the drug-gene relationship) is normalized and sparsely multiplied with the drug embeddings $E_d$) (the initial drug embedding). This operation is expressed as: $E_g = A_{norm}E_d^{(0)}$, where $A_{norm}$ represents the normalized adjacency matrix and $E_d^{(0)}$ is the drug embedding at the initial state. Similarly, to update the drug embeddings based on gene embeddings, the transposed normalized adjacency matrix $A_{norm}^T$ is multiplied with the initial gene embeddings $E_g^{(0)}$, represented as: $E_d = A_{norm}^T E_g^{(0)}$. This process effectively propagates information between the drug and gene nodes, updating their embeddings in each step based on their mutual relationships. Sparse dropout is applied to the adjacency matrix to prevent overfitting.\nWe use drug embedding as input to the graph diffusion network to generate negative gene embeddings. The graph diffusion network can be simply divided into a forward process and a backward process. In the forward process of the diffusion network, gene embedding is input, and negative samples are generated over T time steps. For each time step $t\u2208 \\{1,2,...,T\\}$, the negative gene embedding $E_{gt}$ is calculated as follows: $E_{gt} = \\sqrt{\\bar{\u03b1}_t} E_{g0} + \\sqrt{1 - \\bar{\u03b1}_t} \u03b5_t$. Here, $E_{gt}$ represents the negative gene embedding at time step t, and $E_{g0}$ is the initial gene embedding. The noise vector $\u03b5_t$ is drawn from a standard normal distribution, $\u03b5_t \\sim N(0, I)$, where I is the identity matrix.\nThe coefficient $\u03b1_t$ controls the amount of noise added at each time step, and it is defined as follows: $\u03b1_t= \u03b1_{start} + \\frac{t}{T-1} (\u03b1_{end}- \u03b1_{start}), t = 0, 1, 2, ..., T - 1$. The cumulative product $\\bar{\u03b1}_t$ is the product of all \u03b1 values up to the time step t, i.e., $\\bar{\u03b1}_t = \\prod_{i=0}^{t-1} \u03b1_i$. The noise vectors $\u03b5_t$ for each time step are independently sampled from a standard normal distribution. Throughout the diffusion process, noise $\u03b5_t$ is progressively added at each time step, generating increasingly diverse negative gene embeddings. After T time steps, the final negative gene embedding $E_{gT}$ is obtained, integrating the cumulative effect of noise $\\{\u03b5_1, \u03b5_2,..., \u03b5_T\\}$ at each step.\nIn the backward process, the model aims to recover the original gene embedding by predicting the noise added during the forward process. The reverse process begins with a random noise vector $\u03b5_T$ at time step T. Starting from this noisy state, the model progressively refines the noisy embedding by predicting the noise at each time step and removing it, gradually recovering the clean gene embedding. At each time step $t \u2208 \\{T,T \u2013 1,...,1\\}$, the model predicts the noise $ \\hat{\u03b5}_\u03b8$ using the predicted gene embedding at the time step t, denoted as $E_{gt}$, the drug embedding $E_{d0}$, and the time embedding $PE(t)$. The noise prediction function is expressed as: $ \\hat{\u03b5}_\u03b8 (E_{gt}, E_{d0}, PE(t))$. The time embedding $PE(t)$ is combined with the predicted gene embedding $E_{gt}$, and the drug embedding $E_{d0}$ to predict the noise. The loss function used to train the diffusion network measures the difference between the predicted noise $ \\hat{\u03b5}_\u03b8 (E_{gt}, E_{d0}, PE(t))$ and the true noise $\u03b5_t$ from the forward process. This is calculated using the mean squared error loss: $L = E_{t,E_{gt},E_{d0}} \\mathbb{E}_{\u03b5 \\sim t} [|\u03b5_t \u2013 \\hat{\u03b5}_\u03b8 (E_{gt}, E_{d0}, PE(t)|_2]$.\nBy minimizing this loss, the model learns to accurately predict the noise injected during the forward process. This enables the reverse diffusion process to progressively remove the noise from the noisy gene embeddings, ultimately recovering the original clean embedding.\nDuring the backward diffusion process, the negative embeddings generated at each step are stored in a list $L_{neg}$, which includes the progressively denoised embeddings from time step T down to 1. This list is represented as: $L_{neg} = \\{E_{gT}, E_{gT-1},..., E_{g1}\\}$.\nInstead of using all embeddings, the model samples embeddings from specific fractional time steps T/1, T/2, T/3, and T/4, capturing embeddings at different levels of refinement. These sampled embeddings are denoted as: $L_{sampled} = \\{E_{gT/1}, E_{gT/2}, E_{gT/3}, E_{gT/4}\\}$.\nAt this stage, the model applies a weight matrix W, where larger weights are assigned to embeddings from earlier time steps (i.e., higher noise levels). The rationale is that noisier embeddings retain more diverse information, which can improve the model's robustness. The weight matrix W is applied in descending order, such that: $W_T > W_{T/2} > W_{T/3} > W_{T/4}$. This ensures that embeddings with more noise, which carry more diverse information, are given greater importance. The final gene embedding $ \\hat{E}_{neg}^g$ is then computed as a weighted sum of these sampled embeddings: $ \\hat{E}_{neg}^g = \\sum_{i=1}^4 W_{T/i}E_{gT/i}$.\nIn this expression, $W_{T/i}$ represents the weight assigned to the embedding $E_{gT/i}$, sampled at fractional time steps T/1, T/2, T/3, and T/4, with larger weights assigned to earlier, noisier embeddings. This approach ensures that the model captures valuable information from different stages of the backward diffusion process.\nOur training objective consists of three components: the diffusion model loss $L_{diffusion}$ and two contrastive losses. The diffusion model loss $L_{diffusion}$ is used to train the model to predict the noise added during the forward diffusion process, enabling it to reconstruct the original gene embedding from its noisy version generated during diffusion. This process ensures that the model can effectively reverse the diffusion process and recover clean gene embeddings, as detailed in the previous section.\nThe first contrastive loss uses cross-entropy to encourage the model to assign higher scores to positive drug-gene pairs and lower scores to negative pairs. We use a multilayer perceptron MLP\u2081 that takes the concatenation of the drug embedding and the gene embedding as input. MLP\u2081 is a two-layer multilayer perceptron that takes the concatenation of the drug embedding and the gene embedding as input. The first layer applies a linear transformation followed by a ReLU activation function, and the second layer outputs the prediction. For positive pairs (i.e., linked drug and gene embedding), MLP\u2081 is trained to output a score close to 1, and for negative pairs (i.e., drug embeddings and real negative gene embedding), it outputs a score close to 0. The cross-entropy loss is expressed as: $L_{CE} = \u2212 log \u03c3(MLP_1(E_d, E_g)) \u2013 log(1 \u2212 \u03c3(MLP_1(E_d, E_{neg}^g )))$, where o is the sigmoid activation function, $E_d$ is the drug embedding, $E_g$ is the positive gene embedding, and $E_{neg}^g$ is the real negative gene embedding.\nThe second contrastive loss is a margin-based ranking loss applied to the embeddings generated by the diffusion model. We introduce another multilayer perceptron network $MLP_2$ to compute scores for drug-gene pairs with the same structure as $MLP_1$. This loss encourages the score for positive pairs to be higher than that for negative pairs. The margin-based ranking loss is formulated as: $L_{margin} = \\mathbb{E}_{E_d,E_g,\u00ca} [max(0, max(0, MLP_2 (E_d, E_{neg}^g) \u2013 MLP_2(E_d, E_g))]$. where $ \\hat{E}_{neg}^g$ is the negative gene embedding generated by the diffusion model.\nThe total loss function combines these three components into the following: $L_{total} = L_{diffusion} + L_{CE} + L_{margin}$.\nBy minimizing $L_{total}$, the model learns robust representations for both positive and negative drug-gene interactions.\nDuring inference, the model comprises two contrasts: one is from the positive pairs and negatives from the graph, and the other is the positive pairs and negative pairs generated from the diffusion network. The diffusion network generates negative gene embeddings conditioned on the drug embedding. Negative embeddings are sampled from various timesteps in the reverse diffusion process (e.g., T, T-1, T-2, T-3) and are assigned different weights to capture diverse negative examples. These weighted embeddings are combined to form the final negative gene embedding $ \\hat{E}_{neg}^g$.\nThe MLP network computes scores for drug-gene pairs to predict their likelihood of interaction. The final score is calculated by combining the outputs from both MLPs as follows: $Score = MLP_2(E_d, E_g) \u2013 MLP_2(E_d,  \\hat{E}_{neg}^g)+MLP_1(E_d, E_g)$.\nThis scoring mechanism ensures that positive drug-gene pairs receive higher scores compared to negative pairs, aligning with the training objective. By integrating the diffusion network with contrastive losses, the model effectively generates diverse negative samples and distinguishes between linked and unlinked drug-gene pairs, improving prediction performance for drug-gene interactions."}, {"title": "E. Complexity analysis", "content": "In this section, we analyze the time complexity of our framework, which uses a homogeneous graph approach, compared to a fully transformer-based model. The key distinction between these two approaches lies in how they handle the relationships between nodes during message passing.\nOur framework utilizes a homogeneous graph where each node communicates only with its immediate neighbors. Additionally, it restricts the number of neighbors for each node to a fixed threshold \u03c4, ensuring that message aggregation occurs locally within a small, controlled set of nodes. The time complexity of message passing in the attention calculation is primarily influenced by the number of edges in the graph. For a graph with N nodes and E edges, each node aggregates information from its neighbors, resulting in a time complexity of O(E). Given that we limit the number of neighbors to \u03c4, the complexity per node becomes O(\u03c4), and for the entire graph, the total complexity is O(N\u00b7 \u03c4). Since is typically a small constant, the overall time complexity remains linear w.r.t. the number of nodes, i.e., O(N).\nA transformer-based model considers interactions between all nodes, regardless of whether they are neighbors. In this approach, the attention mechanism computes relationships between every pair of nodes in the graph. As a result, the time complexity for a graph with N nodes is O($N^2$), since each node interacts with every other node. This quadratic complexity makes transformers computationally expensive when applied to large graphs, as the number of interactions grows rapidly with the size of the graph.\nWhen comparing the two approaches, the homogeneous graph-based GCN model is significantly more efficient. By focusing only on local interactions between a node and its immediate neighbors, and limiting the number of neighbors with the threshold T, the computation is constrained to O(N), which is scalable even for large graphs. In contrast, the transformer-based model, with its O($N^2$) complexity, becomes much less efficient as the size of the graph increases, due to the need to process interactions between all nodes.\nIn summary, our framework, which uses a homogeneous graph structure, is computationally more efficient compared to transformer-based models, especially for large-scale graphs. By limiting interactions to local neighborhoods, we can significantly reduce the computational cost while maintaining effective message passing."}, {"title": "V. EXPERIMENTS", "content": "Our experiments were carried out on the system equipped with the NVIDIA GeForce RTX 3080 GPU", "https": "github.com/csjywu1/GDNDGP\n\u2022\n\u2022\n\u2022\n\u2022\nQ1 (Effectiveness): How effective is GDNDGP compared to other baseline methods in predicting drug-gene associations?\nQ2 (Ablation study): What is the impact of removing or modifying key components of GDNDGP on its overall performance?\nQ3 (Hyper-parameter analysis): How do different hyperparameters affect the performance of GDNDGP?\nQ4 (Time analysis): How does the computational efficiency of GDNDGP compare in terms of training and inference time?\nThe data used in this study comes from the Drug-Gene Interaction Database 4.0 (DGIdb 4.0) [41", "25": "which leverages 46", "8": 2, "29": "which contains drug-go associations and go-disease associations. We need to predict the drug-diseases through the meta-paths drug-go and disease-go.\nTo assess the performance of GDNDGP", "performance": "AUC", "20": ".", "42": "were also used to evaluate the model: $MCC = \\frac{TP\u00d7TN - FP\u00d7FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}$, $Spec. = \\frac{TN}{TN + FP"}]}