{"title": "Transfer learning of state-based potential games for\nprocess optimization in decentralized manufacturing\nsystems", "authors": ["Steve Yuwono", "Dorothea Schwung", "Andreas Schwung"], "abstract": "This paper presents a novel transfer learning approach in state-based potential\ngames (TL-SbPGs) for enhancing distributed self-optimization in manufactur-\ning systems. The approach focuses on the practical relevant industrial setting\nwhere sharing and transferring gained knowledge among similar-behaved players\nimproves the self-learning mechanism in large-scale systems. With TL-SbPGs,\nthe gained knowledge can be reused by other players to optimize their policies,\nthereby improving the learning outcomes of the players and accelerating the\nlearning process. To accomplish this goal, we develop transfer learning con-\ncepts and similarity criteria for players, which offer two distinct settings: (a)\npredefined similarities between players and (b) dynamically inferred similarities\nbetween players during training. We formally prove the applicability of the\nSbPG framework in transfer learning. Additionally, we introduce an efficient\nmethod to determine the optimal timing and weighting of the transfer learning\nprocedure during the training phase. Through experiments on a laboratory-\nscale testbed, we demonstrate that TL-SbPGs significantly boost production\nefficiency while reducing power consumption of the production schedules while\nalso outperforming native SbPGs.", "sections": [{"title": "1. Introduction", "content": "Modern manufacturing systems [1] are increasingly expected to be intel-\nligent, agile, highly flexible, and adaptable, given the rapid and continuous\nchanges in their production processes. To address these demands, game the-\nory (GT) applied in dynamic games for engineering applications [2] emerges as\na promising approach. GT's collaborative characteristics are particularly suit-\nable for self-learning in multi-agent systems (MAS), which require fast adapta-\ntion and reconfiguration while each agent's decision-making affects each other.\nWithin industrial settings, GT-based approaches have demonstrated effective-\nness in managing decision-making tasks within MAS, where multiple agents\ncollaborate in a shared environment to tackle multi-objective optimization chal-\nlenges [3]. Despite its potential advantages, the practical implementation of GT\nin real manufacturing settings remains relatively limited, primarily due to the\nvast amounts of required training data and the lengthy duration of training [3].\nParticularly, learning from scratch implies being unreasonable without access\nto highly precise digital system representations. Furthermore, the large-scale\nnature of most production systems worsens the time required for training [4, 5].\nHowever, there are similarities within modules of the system, such as common\nobjectives, hardware actuation, and control parameters, which can be utilised\nto reduce training time.\nTherefore, in this paper, we propose a novel approach that combines suc-\ncessful GT-based learning with transfer learning methodology by exploiting the\nsimilarities within system modules. Our objective is to accelerate training du-\nration and improve production efficiency beyond developed GT-based learning\nmethods. Unlike previous GT-based learning methods, our transfer learning\nmethod employs communication signals as in [6] to facilitate knowledge transfer\namong players with common interests during training. The proposed transfer\nlearning aims to preserve the knowledge gained from solving specific cases and\nefficiently reuse it in different scenarios, thus saving resources. We claim that\ntransfer learning accelerates player learning by extensively learning from the"}, {"title": "3. Problem statement", "content": "This section offers a detailed problem formulation embedded in the context of\nthe fully distributed manufacturing system under consideration. Advanced man-\nufacturing systems demand complex functionality to facilitate fast and highly\nadaptable production processes. Such capabilities are realized through a mod-\nular system structure comprising both hardware and software architectures,\nwhich enables inclusion, exclusion, or interchangeability of each module as per\nnecessity. In a MAS setting, the development of distributed control systems\nmust occur at the software level [36].\nTherefore, this study focuses on distributed, modular systems that are par-\ntitioned into several subsystems, each equipped with its local control system, as\nillustrated in Fig. 2. Our objective is to optimize the system in a fully distributed\nmanner, without dependency on a centralized instance. This approach allows\nfor flexible, scalable, and generally reusable execution across various modules\nthrough instantiations."}, {"title": "4. Basics of state-based potential games", "content": "In this section, we introduce SbPGs, which are mentioned in the previous\nsection, to formulate distribution optimization within modular manufacturing\nunits. SbPGs are the inheritance of PGs [31], which refer to a type of strategic\ngame that relies on the potential function & and additional state information.\nIn PGs, we introduce a set of N players with a strategy set for each player\ni organized as discrete actions $a_i \\in \\mathbb{N}^{d_i}$ and/or continuous actions $a_i \\in \\mathbb{R}^{d_i}$,\nand a potential function $\u0444$ on joint actions $a = a_1 \u00d7 . . . \u00d7 a_N$ yields the global\noptimization problem $max_{a\u2208A} \u0444(a)$ that split among the players [31]:\nDefinition 1. A strategic-form game $F(N, A, {U_i},d)$ establishes an (exact)\nPG if a global function $\u00a2 : a \u2192 \\mathbb{R}^{d_i}$ can be found conforming to the condition\n$U_i(a_i, a_{\u2212i}) \u2013 U_i(a'_i, a_{\u2212i}) = \u00a2(a_i, a_{\u2212i}) \u2013 \u0444(a'_i, a_{\u2212i})$,\nfor any $i = 1,..., N, a_i \u2208 A_i$ and $a_{\u2212i} \u2208 A_1 \u00d7 ... \u00d7 A_{i\u22121} \u00d7 A_{i+1} \u00d7 ... \u00d7 A_N$.\nTo reach the convergence to a NE, when a potential function maximizer from\nthe set $a^* = argmax_{a\u2208A}\u0444(a)$ is set up.\nDefinition 2. If the action $a^\u2217_i$ for each player $i \u2208 N$ is the best response to\nthe joint action of other players $a^\u2217_{\u2212i}$, then the joint action $a^\u2217 \u2208 A$ is a NE,\nspecifically\n$U_i(a^\u2217_i ,a^\u2217_{\u2212i}) = max_{a_i \\in A_i} U_i(a_i, a^\u2217_{\u2212i})$.\nHence, no player can benefit by unilaterally digressing from the NE action pro-\nfile. However, the limitation of PGs in the engineering area is the potential\nto incorporate state information about the environment with the games them-\nselves, which is solved by the idea of SbPGs [34].\nSbPG [34] has an underlying state space that allows the environment's dy-\nnamics to be considered during the game, hence an instance of DPGs [11]. The\nutility function of each player is both action- and state-dependent, and the in-\ncorporation of state information varies depending on the setting, i.e. it ranges"}, {"title": "5. Transfer learning of state-based potential games", "content": "In this section, we present a novel method called TL-SbPGs, which com-\nbines transfer learning approaches with SbPGs. TL-SbPGs facilitate knowledge\ntransfer between players with similar behaviours during training to accelerate\nthe learning process by also learning extensively from experiences gained from\nother players rather than discarding them and leading to faster convergence\ntowards optimal solutions.\nFig. 4 describes an overview of the proposed TL-SbPGs for distributed opti-\nmizations. We explore two scenarios in which TL-SbPGs can be implemented:\n(1) when the similarity between players is given beforehand e.g. when physically\nsimilar actors for physically similar functions, and (2) when such similarities\nmust be inferred during training. For the latter scenario, we derive suitable sim-\nilarity measures between the actuators thereby respecting the constraint com-\nmunications required in real-life applications. Additionally, we provide conver-\ngence guarantees for the learning process by proving that the proposed method\nsatisfies the requirements of SbPGs while incorporating additional objectives.\nThe section is divided into three subsections, which are (A) the transfer\nlearning frameworks for the first scenario, which include SW and MOM ap-\nproaches, (B) the transfer learning frameworks for the second scenario, in which\nsimilarity measures must be inferred during training through radial basis func-\ntion networks, and (C) the update mechanism for the TL-SbPGs."}, {"title": "5.1. TL-SbPGs with pre-defined similarities", "content": "We start with the assumption that knowledge regarding the physical proper-\nties of the process is available, which allows for the pre-definition of similarities"}, {"title": "5.1.1. Sliding window approach", "content": "The first variant is based on an SW approach and yields, as follows:\n$H^{SW}_{i,j} (a_i, a_j) = \\frac{H}{2} \\sum_{k=0}^{H-1} (a_{i,t-k} - a_{j,t-k})^2$,\nwhere H is the horizon of the SW. The determination of the horizon length\nis contingent upon the learned systems, and it can also be scientifically de-\nfined through methods such as hyperparameter tuning, for instance, via grid\nsearch [38]. Thus, the loss is accumulated not only based on the actual time\nframe but also the horizon.\nIn this subsection, we aim to formally prove that the modified utility function\nin Eq. (9) constitutes the SbPG setting. As such, the equation consists of two\nparts, i.e. the original utility function and the auxiliary loss function. Lemma 3\nformally proves the existence of the SbPGs for the SW approach:"}, {"title": "5.1.2. Momentum-based approach", "content": "Alternatively, we define the auxiliary utility function based on the MOM\nmethod, as follows:\n$H^{MOM}_{i,j,t}(a_i, a_j) = \u0430_{MOM} H^{MOM}_{i,j,t\u22121}(a_i, a_j) + (1 \u2013 \u0430_{MOM}) (a_{i,t} \u2212 a_{j,t})^2$,\nwhere for t = 0, the auxiliary utility function is transformed, as follows:\n$H^{MOM}_{i,j,0}(a_i, a_j) = (a_{i,0} \u2013 a_{j,0})^2$,\nwhere amom represents a fixed weight that balances the optimal contributions\nof the previous and current losses. The weight parameter aMom determines\nthe proportion of the previous auxiliary utility function $H^{MOM}_{i,j,t}(a_i, a_j)$ that is\nretained for the next step calculation, a feature that is not present in the SW\napproach. Lemma 4 formally prove the existence of the SbPGs for the MOM\napproach:"}, {"title": "5.1.3. Adaptive weight parameter", "content": "As an additional contribution in this paper, we address two crucial questions\nregarding transfer learning between players: (1) when the transfer learning pro-\ncess should start, and (2) how much relevant knowledge each player can share\nwith others. To tackle these questions, we propose a method to dynamically\ndetermine the value of the weight parameter ATF in Eq. (9). This parameter is\nimportant in initiating and quantifying the significance of knowledge transfer.\nOur objective is to establish the timing and weighting factors for the transfer\nof knowledge between players, which ensures that the transfer learning process\nstarts at an appropriate time to avoid premature initiation and the transmission\nof irrelevant information.\nFirst, to determine when to initiate the transfer learning process, we in-\ntroduce a transfer learning threshold \u1e9eTF as a kick-off indicator. The transfer\nlearning process is triggered once the exploration rate e of the learning player\nfalls below the threshold \u1e9eTF. Specifically, when \u20ac > \u1e9eTF, the transfer learning\nparameter ATF is set to 0, which indicates that no transfer learning occurs. This\napproach ensures that transfer learning is not initiated during periods of high\nexploration, thereby avoiding disruption of the exploration phase with excessive\nknowledge transfer."}, {"title": "5.2. TL-SbPGs without pre-defined similarities", "content": "In this subsection, we examine a scenario where knowledge regarding the sim-\nilarity between players is not predefined. In real-world manufacturing systems,\nit is common for the similarity between actuators to be unknown or challeng-\ning to determine due to the large scale of the system. As a result, previous\napproaches such as the SW and MOM approaches become impractical when\napplied as standalone methods. Hence, we relax the assumption made in the\nprevious subsection and establish conditions for inferring the similarity between\nplayers during their learning progress. This measure of similarity is then incor-\nporated into the learning process by adjusting the utility function design and\naction updates accordingly.\nIn general, similarities can be defined based on policies, which outline the ac-\ntions to be taken in the state-action space, or utilities obtained during training.\nWe are particularly interested in low-dimensional representations of policies and\nutilities, from which a suitable similarity measure can be derived. To achieve\nthis, we propose an approach for measuring similarity for such low-dimensional\nlatent representations, namely using radial basis function (RBF) networks. To\nassess the similarity between two or more players, we suggest comparing the"}, {"title": "5.2.1. RBF networks", "content": "We propose the utilization of a simple RBF network with a fixed mean and\nvariance to represent the state-action and state-utility space. Specifically, we\ndefine a two-dimensional RBF network for player i,\n$E_i = [a_i U_i] = \\sum_{j} \u03b8_{i,j} e^{(S_i-\u03bc_i)^T\u03a3_i (S_i-\u03bc_i)}e^{(S_j-\u03bc_j)^T\u03a3_j (S_j-\u03bc_j)}$,\nwhere mean \u03bc\u03b5 \u2208 [0,1]d and covariance matrix \u03c3\u03b5 \u2208 R are fixed beforehand.\nd is the dimension of the state space. The remaining parameters Oi,j with\nj = 1,..., J constitute the J-dimensional latent space. The parameters Oi,j\ncan be fitted to the state-action and state-utility space by using least square\noptimization. To reduce the computational requirements and noise due to ex-\nploration, we update the latent space every H-th step only. In addition, we\nimplement the gradient descent method to optimize parameters Oi,j by taking\nthe steepest descent direction at each iteration."}, {"title": "5.2.2. Similarity measures implementations", "content": "As previously discussed, player similarities can be evaluated using trained\nRBF networks by analysing their respective latent space representations. This\ninvolves computing the similarity between player n and target player m, denoted\nas Ln,m, through a basic squared loss method applied to the weight parameters\nof player on and target player om, as demonstrated:\n$L_{n,m} = (\u03b8_{n,j,t-1}- \u03b8_{m,j,t-1})^2$.\nAfter obtaining the similarities between players, we have two options to proceed,\nsuch as (a) continuing with the previous approaches, either the SW or MOM\napproaches by incorporating the obtained similarities, and (2) continuing di-\nrectly with RBF networks-based transfer without employing the SW and MOM\napproaches."}, {"title": "5.3. Update mechanism for TL-SbPGs", "content": "In this subsection, our focus is on demonstrating the integration between\nthe proposed transfer learning schemes into SbPGs. Pseudocode 1 outlines the\nupdate loop of the training process for the proposed TL-SbPG methods, which\nconsiders both predefined and undefined similarities between players. Further-\nmore, the players employ a TL-SbPG learning algorithm with a global interpo-\nlation technique [3] to estimate the value of actions in a specific state as well\nas Best Response learning [3] as their policies. The selected actions are then\ncommunicated to the environment by the players."}, {"title": "6. Results and discussions", "content": "In this study, we evaluate the effectiveness of the proposed TL-SbPG meth-\nods by applying them to two laboratory testbeds, namely the bulk good system"}, {"title": "6.1. Laboratory testbed", "content": "The BGS [3] represents a flexible and smart production system designed to\nhandle material transportation, with a focus on bulk materials. The laboratory-\nscale testbed operates within four modules, as illustrated in Fig. 6. Module 1\nand Module 2 serve as typical supply, buffer, and transportation stations, which\nare controlled by a continuously operated belt conveyor in Module 1 and by\na vacuum pump and a vibratory conveyor in Module 2. Module 3 is the dos-\ning station, which is controlled by a second vacuum pump and a continuously\noperated rotary feeder. Then, Module 4 is the filling and final station, which\noperates to fill up the transport containers.\nEach station has a reservoir(s) to temporarily store the presently transported\nmaterial and the actuator controls the number of materials that flow out of the\nreservoir. Additionally, each module can be positioned arbitrarily in various\nproduction sequences or can be entirely detached.\nEach module owns an individual PLC-based control system using a Siemens\nET200SP and several sensors to monitor the modules' state and enable control\nof each module. The communication between the modules is developed via\nProfinet. Moreover, an RFID reader and an RFID tag are appointed in each"}, {"title": "6.2. Learning setup", "content": "In this subsection, we explain how the operation scenario of the BGS is incor-\nporated with TL-SbPGs. First, we consider a continuous production scenario,\nin which the process flow has a constant cycle with continuous throughput. The\nmain target is to fulfil a specified production demand in volume per time while\nreducing power consumption as much as possible and avoiding overflow. There-\nfore, such an operation scenario leads us to a multi-objective optimization. PG\nis being arranged within the BGS environment, in which each actuator serves\nas a player and the fill level indicators of reservoirs serve as state variables, see\nmore details in [3].\nAdditionally, we apply the identical actions, states, and utility functions\nin TL-SbPGs and the baseline (native SbPGs) to ensure that the results are\ncomparable. The local utility function for each player i is defined as:\n$U_i =\n\\begin{cases}\n    \\frac{1}{1+a_{LL}L_i^++a_{UL}L_i^- +a_PP_i + \\frac{1}{1-a_{DVD}}} , & \\text{if } i = N;\\\\\n   \\frac{1}{a_{LL}L_i^++a_{UL}L_i^- +a_PP_i + \\frac{1}{1-a_{DVD}}} , & \\text{otherwise;}\n\\end{cases}$\nwhere P\u00e5 denotes the power consumption in the current iteration, $L_i^+$ and $L_i^-$ are\nlocal constraints to prevent overflow and emptiness of the prior and subsequent\nreservoir respectively, VD denotes the production demand objective, AL, AD, AP\nare predefined weighing parameters, and i identifies the player, i.e. i = N means\nthe last player. Moreover, Eq. (26) and Eq. (27) describe the computation of\nVD, L and L.\n$V_D =\n\\begin{cases}\n  \\frac{v_{N,out} - v_{N,in}}{T_I}, & \\text{if } h^N = 0\\\\\n   0 & \\text{otherwise;}\n\\end{cases}$\nwhere T\u2081 is the iteration length and Vn,out, VN,in are the outflow and inflow to\nthe module. hN denotes the fill level of the related buffer that has been nor-\nmalized. We remark that the last player applies the demand object exclusively,\nwhere the demand cannot be satisfied while the hopper of the last station has\na lower fill level than the desired demand."}, {"title": "7. Conclusions", "content": "We have presented a novel approach for transfer learning in SbPGs applied\nto distributed manufacturing systems, namely TL-SbPGs. Our contributions in-\nclude proposing various transfer learning approaches for two different settings,\nwhere (a) the similarity between players is given beforehand and (b) the sim-\nilarity has to be inferred during training. The proposed TL-SbPG approaches\nconsist of the SW, MOM, and RBF model approaches. Through our experi-\nments, we observed that the players were able to effectively transfer important\nknowledge to each other during training and utilize this knowledge to optimize\ntheir policies.\nThe TL-SbPG methods were implemented and evaluated in a modular pro-\nduction unit and a larger scale of the production unit with a multi-objective\noptimization scenario, where the experimental results demonstrate the effec-\ntiveness and promising nature of the TL-SbPG approaches compared to the\nbaseline. We observed significant performance improvements, particularly in\nterms of power consumption optimization and the overall achievement of global\nobjectives. In conclusion, our paper provides valuable insights and contributions\nto the field of transfer learning in SbPG for distributed manufacturing systems.\nIn future research, we would like to develop other GT structures than SbPGs\non larger-scale industrial control domains, e.g. Stackelberg games. Then, we\nwill test the proposed transfer learning methods across different game structures."}]}