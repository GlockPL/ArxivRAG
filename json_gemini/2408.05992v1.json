{"title": "Transfer learning of state-based potential games for process optimization in decentralized manufacturing systems", "authors": ["Steve Yuwono", "Dorothea Schwung", "Andreas Schwung"], "abstract": "This paper presents a novel transfer learning approach in state-based potential games (TL-SbPGs) for enhancing distributed self-optimization in manufacturing systems. The approach focuses on the practical relevant industrial setting where sharing and transferring gained knowledge among similar-behaved players improves the self-learning mechanism in large-scale systems. With TL-SbPGs, the gained knowledge can be reused by other players to optimize their policies, thereby improving the learning outcomes of the players and accelerating the learning process. To accomplish this goal, we develop transfer learning concepts and similarity criteria for players, which offer two distinct settings: (a) predefined similarities between players and (b) dynamically inferred similarities between players during training. We formally prove the applicability of the SbPG framework in transfer learning. Additionally, we introduce an efficient method to determine the optimal timing and weighting of the transfer learning procedure during the training phase. Through experiments on a laboratory-scale testbed, we demonstrate that TL-SbPGs significantly boost production efficiency while reducing power consumption of the production schedules while also outperforming native SbPGs.", "sections": [{"title": "1. Introduction", "content": "Modern manufacturing systems [1] are increasingly expected to be intelligent, agile, highly flexible, and adaptable, given the rapid and continuous changes in their production processes. To address these demands, game theory (GT) applied in dynamic games for engineering applications [2] emerges as a promising approach. GT's collaborative characteristics are particularly suitable for self-learning in multi-agent systems (MAS), which require fast adaptation and reconfiguration while each agent's decision-making affects each other. Within industrial settings, GT-based approaches have demonstrated effectiveness in managing decision-making tasks within MAS, where multiple agents collaborate in a shared environment to tackle multi-objective optimization challenges [3]. Despite its potential advantages, the practical implementation of GT in real manufacturing settings remains relatively limited, primarily due to the vast amounts of required training data and the lengthy duration of training [3]. Particularly, learning from scratch implies being unreasonable without access to highly precise digital system representations. Furthermore, the large-scale nature of most production systems worsens the time required for training [4, 5]. However, there are similarities within modules of the system, such as common objectives, hardware actuation, and control parameters, which can be utilised to reduce training time.\nTherefore, in this paper, we propose a novel approach that combines successful GT-based learning with transfer learning methodology by exploiting the similarities within system modules. Our objective is to accelerate training duration and improve production efficiency beyond developed GT-based learning methods. Unlike previous GT-based learning methods, our transfer learning method employs communication signals as in [6] to facilitate knowledge transfer among players with common interests during training. The proposed transfer learning aims to preserve the knowledge gained from solving specific cases and efficiently reuse it in different scenarios, thus saving resources. We claim that transfer learning accelerates player learning by extensively learning from the experiences of other players, as illustrated in Fig. 1.\nResearch on the implementation of transfer learning in industrial control and GT domains is limited. To our knowledge, transfer learning has yet to be applied in GT, including potential games. However, successful practical applications of transfer learning techniques in dynamic programming offer an understanding of how GT can be enhanced. Examples include unsupervised and supervised deep transfer learning for fault diagnosis [7, 8], transfer learning on evolutionary algorithms [9], and transfer learning in reinforcement learning domains [10]. In these cases, transfer setups typically rely on human decisions considering task similarities and are not straightforwardly executable. Moreover, most applications primarily focus on single-agent scenarios. In contrast, our approach targets multi-agent systems while maintaining a distributed player setup. Furthermore, our transfer learning occurs between training players in a parallel manner, rather than involving external or pre-trained agents. In addition, there is currently a lack of large-scale studies in manufacturing automation about transfer learning in self-optimizing production control systems, which would provide valuable real-world examples.\nOur proposed approach contains two primary scenarios. Firstly, we propose a transfer learning framework with the assumption that prior analysis has identified similarities in players' behaviours. By establishing these similarities at the outset of the learning process, knowledge transfer among players can be predetermined. Secondly, we consider a scenario where initial information about the similarities between players' behaviours is unavailable. To address this, we develop a methodology through a radial basis function network for measuring similarities among players and facilitating knowledge transfer between players in the absence of such information. The primary benefit of our approach is in its ability to accelerate the learning of players' behaviours and performance. This is achieved through knowledge sharing among players, which enables faster convergence towards optimal solutions.\nWe provide formal proofs demonstrating that our proposed transfer learning frameworks fulfil the requirements of potential game-based learning, which ensures corresponding convergence guarantees [11, 12]. During our study, we observed that the timing and weighting of the transfer learning procedure between players during training are crucial factors. To address this challenge, we introduce a method that effectively manages the transfer learning process. Subsequently, we evaluate the performance of our transfer learning approaches using a laboratory testbed simulating a bulk good system, particularly in optimizing power consumption and throughput times. The experimental results demonstrate promising outcomes for improving production efficiency.\nThe main contributions of the paper are summarized as follows:\n\u2022 We propose novel transfer learning methodologies for distributed GT- based optimization, specifically in State-based Potential Games (SbPGs), which is called transfer learning of SbPGs (TL-SbPGs). TL-SbPGs encourage players to exchange their experiences and reuse them to optimize their policies.\n\u2022 We present two transfer learning approaches with an assumption that the similarities between players are known beforehand, such as (1) the momentum-based (MOM) approach and (2) the sliding window (SW) approach. We provide formal proof demonstrating that the proposed transfer learning approaches meet the requirements of SbPGs.\n\u2022 We propose a transfer learning approach that addresses scenarios where player similarities are unknown, namely radial basic function networks. This approach can also be independently used for similarity measures.\n\u2022 We propose a method to determine the timing and weighting of transfer learning between players, as we found these factors to impact the effectiveness of the learning process in TL-SbPGs significantly.\n\u2022 We apply the proposed transfer learning approaches to a laboratory testbed allowing for distributed self-optimization of bulk good operations. We then evaluate the framework in a larger-scale production system, where we thoroughly compare the proposed approaches of TL-SbPGs (without and with similarity measures) on various production scenarios with immensely encouraging results.\nThe paper is structured as follows: Section 2 describes the related work of our topic. Section 3 provides the problem statement of the underlying distributed optimization problem for modular production units. Section 4 introduces the basic concepts of SbPGs. In Section 5, we present a comprehensive explanation of various transfer learning approaches in TL-SbPGs. Then, Section 6 provides results and comparisons of various transfer learning approaches in TL-SbPGs. Section 7 concludes the paper and delivers the future scope of the approach."}, {"title": "2. Related work", "content": "This chapter presents a discussion about related works, including self-learning in production systems, transfer learning for automation in manufacturing, and potential games."}, {"title": "2.1. Self-learning in production systems", "content": "Automation technology has gained significant attention in modern manufacturing systems, especially in the era of digitization, where automated production optimization plays a vital role [1]. Self-learning production systems are a prime example of this application, where systems can autonomously learn from their experiences, make independent decisions, and self-diagnose their conditions [13, 14]. Various methods and applications have been established in this domain, which showcases the effectiveness of automation technology. For instance, Bayesian optimization has been successfully employed for auto-tuning throttle valves [15], knowledge-assisted reinforcement learning has proven beneficial for turbine control in wind farms [16], automated Programmable Logic Controller (PLC) code generation via evolutionary algorithms [17], vision-based reinforcement learning for bucket elevator process optimization [18], and further applications in production systems [14, 19]. Recent research [6, 20, 21] focus on developing self-optimizing production systems using game theory in dynamic games. Building upon those previous research, where communication signals between players are enabled [6], they have yet to fully explore the potential benefits of such communication among actuators in production systems. In this study, we aim to utilise this capability to accelerate the learning process while enhancing performance. Our prior studies provide a robust foundation for the current research."}, {"title": "2.2. Transfer learning for automation in manufacturing", "content": "Transfer learning is an advanced technique in the machine learning area and has emerged as a promising paradigm for enhancing automation in manufacturing systems by leveraging knowledge from related tasks or domains [22, 23]. Transfer learning facilitates the application of knowledge acquired from source tasks to accelerate the learning process of new tasks [10, 24]. In practical scenarios, the concept of transfer learning extends beyond a singular activity, which enables solutions to diverse cases such as cross-domains, cross-environments, or cross-phases [25]. In recent literature, researchers have explored various transfer learning techniques to address the challenges of adapting models trained on source domains to target domains with limited or no labelled data [26]. For instance, in the context of robotics and automation, transfer learning has been applied to tasks such as object recognition [27], manipulation and scheduling [28, 29], as well as motion planning [30], where pre-trained models are fine-tuned or adapted to specific manufacturing environments. Moreover, transfer learning has been utilized to transfer knowledge between different manufacturing processes or production lines, which enables more efficient utilization of resources and faster deployment of automation solutions. However, there are no large-scale studies about transfer learning in self-optimizing production control systems that can represent a real-world example, as demonstrated in our study. Additionally, the majority of applications primarily focus on single-agent scenarios."}, {"title": "2.3. Potential games", "content": "Potential Game (PG) was originally introduced in economics [31], but nowadays has been increasingly applied in engineering tasks [2]. For instance, distributed parameter estimation of sensors in a non-convex problem [32] and maximizing system throughput of transmission rate [33] have been reported. As in [31], the potential function $\\phi$ aids in achieving local or global optimality of the learned system and facilitates convergence studies. Additionally, incorporating various learning algorithms can ensure the convergence of a system to a Nash equilibrium (NE). However, the major challenge in PG-based optimization for a self-learning system is establishing a strategy that guarantees an untroubled alignment between the potential function and the local objective functions (utilities $U_i$) for each player $i$. Hence, to incorporate state information in dynamic PGs (DPGs) [11], SbPGs have been developed in [34] for the first time. Then, SbPGs have been implemented in distributed production systems as an algorithm for self-optimizing the systems [3]. However, the fact is the learning of the distributed policies is mostly conducted from scratch resulting in lengthy training periods and is barely applicable in the real environment. In [6], they extensively discussed communication and memory within SbPGs, yet the exploration of the full potential of communication remains unexplored. Furthermore, in the related previous work [20], they investigated transfer learning between PLC code and GT players. In contrast, this study diverts its focus to transfer learning exclusively between GT players, which facilitates information exchange during the learning process and incorporates similarity measures. Additionally, an algorithm that integrates online model-based learning with SbPGs has been developed in [21, 35]. These referenced works highlight the significance of SbPGs in manufacturing control."}, {"title": "3. Problem statement", "content": "This section offers a detailed problem formulation embedded in the context of the fully distributed manufacturing system under consideration. Advanced manufacturing systems demand complex functionality to facilitate fast and highly adaptable production processes. Such capabilities are realized through a modular system structure comprising both hardware and software architectures, which enables inclusion, exclusion, or interchangeability of each module as per necessity. In a MAS setting, the development of distributed control systems must occur at the software level [36].\nTherefore, this study focuses on distributed, modular systems that are partitioned into several subsystems, each equipped with its local control system, as illustrated in Fig. 2. Our objective is to optimize the system in a fully distributed manner, without dependency on a centralized instance. This approach allows for flexible, scalable, and generally reusable execution across various modules through instantiations.\nEach subsystem interacts with its control system by exchanging local signals. Additionally, there are two connections linking different subsystems, which are one at the communication level and another at the process level. The communication level is facilitated by communication interfaces, such as Ethernet, fieldbus, or wireless connections. These communication interfaces enable modules to be uniquely identified within the production environment, which facilitates knowledge transfer via communication signals in our study. We assume that the transfer learning process occurs in both serial and serial-parallel process chains, as depicted in Fig. 3. Similar to [3], we consider that the production chain is represented by an alternating arrangement of actuators that operate on the process (e.g., motors, pumps, conveyors, pumps) and physical states representing the process status. These actuators are expected to exhibit both discrete and continuous operational behaviours, which form a hybrid actuation system.\nWe represent and analyze the considered distributed system via graph theory [37], as further explained in [3]. We define a set of actuators $N = {1; . . . ; N}$ and a set of states $S \\subset R^m$ with a corresponding set of actions $A_i \\subset R^{c \\times N_d}$ with continuous and/or discrete behaviour. We describe the production chain in the form of a directed graph with $|S|$ states and $N$ actuator nodes. We assume a dynamic series of states and actuators, such that the set of edges $E$ omits edges $e = (s_i, s_j)$ and $e = (A_i, A_j)$ with $s_i, s_j \\in S$ and $A_i, A_j \\in N$. Moreover, two sets of surrounding states are determined for every actuator, including the group of subsequent states $S^{A_i}_{next} = {s_j \\in S | \\exists e = (A_i, s_j) \\in E}$ and prior states $S^{A_i}_{prior} = {s_j \\in S | \\exists e = (s_j, A_i) \\in E}$. The functions of the local utility is characterized as $U_i = f(S^{A_i}, a_i)$, in which $a_i \\in A_i$ and $S^{A_i} \\in S^{A_i} = S^{A_i}_{prior} \\cup S^{A_i}_{next} \\cup S^{q}$. The group of states $S^{q}$ are utilized to determine global objectives.\nIn our study, we enable communication interfaces to facilitate transfer learning between players via communication signals, which leads to an introduction of an auxiliary utility function $H_{ij}$ that encourages transfer between player $i$ and player $j$. We intend to subsequently integrate this into the local utility function of each player, a topic to be addressed in Section 5. However, in this setup, the multiple utility functions may share states, and the overarching objective is to maximize overall utility, as outlined below:\n$\\max_{a \\in A} \\Phi(a, S) = \\sum_{i=0}^{N} U_i(a_i, S^{A_i})$\nby jointly accelerating local utilities $U_i(a_i, S^{A_i})$ of the system [11], where $\\Phi$ denotes a global objective function, known as the potential function. As a general framework of this setup, we utilize SbPGs for distribution optimization, as explained in the next section.\nThis generalized production scenario finds applications across various fields in the process industry sector, including pharmaceuticals, food production, and chemical plants. Decentralized production lines are prevalent in modern industrial environments. To be noted, the assumption that the production chain comprises only sequences of states and actions is not restrictive. A sequence of more than two states can be reformulated into a common state vector, and similar arguments apply to actions. In the manufacturing context, local utilities can represent multi-strategic goals such as minimizing throughput time, idle time, storage requirements, setup and changeover time of machines, reducing production rejects and reworking, and optimizing power consumption. Practical applications often involve a combination of different multi-objectives, which leads to multi-objective optimization problems."}, {"title": "4. Basics of state-based potential games", "content": "In this section, we introduce SbPGs, which are mentioned in the previous section, to formulate distribution optimization within modular manufacturing units. SbPGs are the inheritance of PGs [31], which refer to a type of strategic game that relies on the potential function $\\phi$ and additional state information.\nIn PGs, we introduce a set of $N$ players with a strategy set for each player $i$ organized as discrete actions $a_i \\in N^{d_i}$ and/or continuous actions $a_i \\in R^{d_i}$, and a potential function $\\phi$ on joint actions $a = a_1 \\times . . . \\times a_N$ yields the global optimization problem $\\max_{a \\in A} \\Phi(a)$ that split among the players [31]:\nDefinition 1. A strategic-form game $F(N, A, {U_i},\\phi)$ establishes an (exact) PG if a global function $\\phi: a \\rightarrow R^{d_i}$ can be found conforming to the condition\n$U_i(a_i, a_{-i}) \u2013 U_i(a'_i, a_{-i}) = \\phi(a_i, a_{-i}) \u2013 \\phi(a'_i, a_{-i}),$ \nfor any $i = 1,..., N, a_i \\in A_i$ and $a_{-i} \\in A_1 \\times ... \\times A_{i-1} \\times A_{i+1} \\times ... \\times A_N$.\nTo reach the convergence to a NE, when a potential function maximizer from the set $a^* \\triangleq \\argmax_{a \\in A} \\Phi(a)$ is set up.\nDefinition 2. If the action $a_i$ for each player $i \\in N$ is the best response to the joint action of other players $a^*_{-i}$, then the joint action $a^* \\in A$ is a NE, specifically\n$U_i(a^*_i,a^*_{-i}) = \\max_{a_i \\in A_i} U_i(a_i, a^*_{-i}).$\nHence, no player can benefit by unilaterally digressing from the NE action profile. However, the limitation of PGs in the engineering area is the potential to incorporate state information about the environment with the games themselves, which is solved by the idea of SbPGs [34].\nSbPG [34] has an underlying state space that allows the environment's dynamics to be considered during the game, hence an instance of DPGs [11]. The utility function of each player is both action- and state-dependent, and the incorporation of state information varies depending on the setting, i.e. it ranges from action memory [34] for supporting experience learning to the state information from the environment into the PG. The formal definition of an SbPG is as follows:\nDefinition 3. A game $\\Gamma(N, A, {U_i}, S, P, \\phi)$ constitutes an SbPG if a global function $\\phi : a \\times S \\rightarrow R^{d_i}$ can be found that for every state-action-pair $[s, a] \\in S \\times A$ conforms to the conditions\n$U_i(s, a_i) \u2013 U_i(s, a'_i, a_{-i}) = \\phi(s, a_i) \u2013 \\phi(s, a'_i, a_{-i})$\nand\n$\\phi(s', a_i) \\ge \\phi(s, a_i)$\nfor any state $s'$ in the state transition process $P(s,a)$.\nThe most crucial characteristic of (exact) PGs, which is the convergence to equilibrium points, remains operative for SbPGs [34]. Different conditions have been derived to prove the existence of an SbPG for a given game setting [11]. The following condition must be satisfied to verify the existence of a PG and to determine the utility function:\nLemma 1. [11] A game $\\Gamma(N, A, {U_i}, S, P, \\phi)$ is a DPG if the players' utilities satisfy the following conditions:\n$\\frac{\\partial^2 U_i(s_i,a)}{\\partial a_i \\partial s_n} = \\frac{\\partial^2 U_j(s_j,a)}{\\partial a_j \\partial s_n}$ ,\n$\\frac{\\partial^2 U_i(s_i,a)}{\\partial s_n \\partial s_m} = \\frac{\\partial^2 U_j(s_j,a)}{\\partial s_m \\partial s_n}$ ,\n$\\frac{\\partial^2 U_i(s_i,a)}{\\partial a_i \\partial a_j} = \\frac{\\partial^2 U_j(s_j,a)}{\\partial a_i \\partial a_j}$ \n$\\forall i, j \\in N, \\forall m \\in S^{A_i} and \\forall n \\in S^{A_j}$.\nFurthermore, a convex combination of two utility functions constitutes an SbPG, if each individual utility function constitutes an SbPG, as proved in Lemma 2. [20] A game $\\Gamma(N, A, {\\beta_1U^{(1)} + \\beta_2U^{(2)} }, S, P, \\beta_1\\phi_1 + \\beta_2\\phi_2)$ with $\\beta_1, \\beta_2 > 0$ and $\\beta_1+\\beta_2 = 1$ is an SbPG, if $\\Gamma_1(N, A, {U^{(1)} }, S, P, \\phi_1), \\Gamma_2(N, A, {U^{(2)}}, S, P, \\phi_2)$ are SbPGs.\nLemma 2 is formally for convex combinations, but it can be easily extended to the case where $\\beta_1, \\beta_2$ are arbitrarily real numbers."}, {"title": "5. Transfer learning of state-based potential games", "content": "In this section, we present a novel method called TL-SbPGs, which combines transfer learning approaches with SbPGs. TL-SbPGs facilitate knowledge transfer between players with similar behaviours during training to accelerate the learning process by also learning extensively from experiences gained from other players rather than discarding them and leading to faster convergence towards optimal solutions.\nFig. 4 describes an overview of the proposed TL-SbPGs for distributed optimizations. We explore two scenarios in which TL-SbPGs can be implemented: (1) when the similarity between players is given beforehand e.g. when physically similar actors for physically similar functions, and (2) when such similarities must be inferred during training. For the latter scenario, we derive suitable similarity measures between the actuators thereby respecting the constraint communications required in real-life applications. Additionally, we provide convergence guarantees for the learning process by proving that the proposed method satisfies the requirements of SbPGs while incorporating additional objectives.\nThe section is divided into three subsections, which are (A) the transfer learning frameworks for the first scenario, which include SW and MOM approaches, (B) the transfer learning frameworks for the second scenario, in which similarity measures must be inferred during training through radial basis function networks, and (C) the update mechanism for the TL-SbPGs."}, {"title": "5.1. TL-SbPGs with pre-defined similarities", "content": "We start with the assumption that knowledge regarding the physical properties of the process is available, which allows for the pre-definition of similarities between actuators and their respective functions within the process. Additionally, without loss of generalization, we assume that all variables, i.e. states and actions are normalized to the interval [0,1]. Further, we consider a suitable discretization of the state-action space as in [3].\nWe start by introducing a method for measuring similarities that facilitates effective transfer learning between players. To achieve this, we introduce an auxiliary utility function that is designed to encourage the transfer of knowledge. To this end, we propose to modify the utility function of SbPGs to allow the transfer learning between actuators, as follows:\n$\\overline{U}_i(a_i, S^{A_i}) = U_i(a_i, S^{A_i}) - \\alpha_{TF} H_{i,j} (a_i, a_j)$\nwhere $U_i(a_i, S^{A_i})$ is the original utility function of player $i$ as defined in Section 4, $\\alpha_{TF}$ is a weight parameter of the transfer learning and $H_{i,j} (a_i, a_j)$ denotes a suitably defined auxiliary utility function to encourage transfer between player $i$ and $j$.\nEq. (9) requires the definition of a suitable, potentially adaptive weight parameter $\\alpha_{TF}$ as well as the auxiliary utility function $H_{i,j} (a_i, a_j)$, for which we introduce two different variants. Specifically, we propose an SW approach as well as a MOM approach to measure the similarity of the two action maps. We further formally prove the existence of an SbPG for both settings in the following."}, {"title": "5.1.1. Sliding window approach", "content": "The first variant is based on an SW approach and yields, as follows:\n$H^{SW}_{i,j} (a_i, a_j) = \\sum_{k=0}^{H-1} (a_{i,t-k} - a_{j,t-k})^2,$\nwhere $H$ is the horizon of the SW. The determination of the horizon length is contingent upon the learned systems, and it can also be scientifically defined through methods such as hyperparameter tuning, for instance, via grid search [38]. Thus, the loss is accumulated not only based on the actual time frame but also the horizon.\nIn this subsection, we aim to formally prove that the modified utility function in Eq. (9) constitutes the SbPG setting. As such, the equation consists of two parts, i.e. the original utility function and the auxiliary loss function. Lemma 3 formally proves the existence of the SbPGs for the SW approach:\nLemma 3. Given a game $\\Gamma(N, A, {H^{SW}_{i,j}}, S, P, \\phi)$ with $H^{SW}_{i,j}$ shown in Eq. (10) constitutes the SbPGs.\nProof 1. For the original utility function without additional cost criteria [3] has proven that it constitutes the SbPGs setting under reasonable assumptions. Furthermore, according to Lemma 2, the convex combination of two SbPG-capable utilities results in SbPG-capable utilities. Hence, we are left to prove that the auxiliary utility function $H_{i,j} (a_i, a_j)$ constitutes an SbPG. As the policy is particularly dependent on the actions of two players $a_i, a_j$ and does not involve any states $S^{A_i}$, we can conclude that condition (6) and (7) are fulfilled. For condition (8), we prove it, as follows:\n$\\frac{\\partial^2 H^{SW}_{i,j}(a_i, a_j)}{\\partial a_i \\partial a_j} = \\frac{\\partial^2 \\sum_{k=0}^{H-1} 2 (a_{i,t-k} - a_{j,t-k})^2}{\\partial a_i \\partial a_j}$ = $\\frac{\\sum_{k=0}^{H-1} 2 (-1)^2 (a_{j,t-k} - a_{i,t-k})^2}{\\partial a_i \\partial a_j}$ = $\\frac{\\sum_{k=0}^{H-1} 2 (a_{j,t-k} - a_{i,t-k})^2}{\\partial a_i \\partial a_j}$ = $\\frac{\\partial^2 H^{SW}_{j,i}(a_j, a_i)}{\\partial a_j \\partial a_i}$\n$\\forall i, j \\in N$. Since the criteria of constituting an SbPG are fulfilled, we can summarize that the transfer learning objective can be incorporated into SbPGs."}, {"title": "5.1.2. Momentum-based approach", "content": "Alternatively, we define the auxiliary utility function based on the MOM method, as follows:\n$H^{MOM}_{i,j} (a_i, a_j) = \\alpha_{MOM} H^{MOM}_{i,j} (a_i, a_j) + (1 \u2013 \\alpha_{MOM}) (a_{i,t} - a_{j,t})^2,$\nwhere for t = 0, the auxiliary utility function is transformed, as follows:\n$H^{MOM}_{i,j} (a_i, a_j) = (a_{i,0} \u2013 a_{j,0})^2,$\nwhere $\\alpha_{mom}$ represents a fixed weight that balances the optimal contributions of the previous and current losses. The weight parameter $\\alpha_{Mom}$ determines the proportion of the previous auxiliary utility function $H^{MOM}_{i,j} (a_i, a_j)$ that is retained for the next step calculation, a feature that is not present in the SW approach. Lemma 4 formally prove the existence of the SbPGs for the MOM approach:\nLemma 4. Given a game $\\Gamma(N, A, {H^{MOM}_{i,j}}, S, P, \\phi)$ with $H^{MOM}_{i,j}$ shown in Eq. (12) constitutes an SbPG.\nProof 2. Similar to the SW approach, we resort to the proof of the existence of an SbPG for the auxiliary loss (12) due to Lemma 2. Again, condition (6) and (7) are trivially fulfilled. However, as the momentum-based loss function takes the previous losses into account, it forms an infinite arithmetic sequence. Therefore, we formally prove the infinite arithmetic sequence using mathematical induction. The proof consists of two steps, namely the base case and the inductive step.\nThe base case occurs at t = 0, described by Eq. (13). For condition (8), we prove it, as follows:\n$\\frac{\\partial^2 H^{MOM}_{i,j,0}(a_i, a_j)}{\\partial a_i \\partial a_j} = \\frac{\\partial^2 (a_{i,0} \u2013 a_{j,0})^2}{\\partial a_i \\partial a_j}$ = $\\frac{\\partial^2 (-1)^2 (a_{j,0} \u2013 a_{i,0})^2}{\\partial a_i \\partial a_j}$ = -$\\frac{\\partial^2 (a_{j,0} - a_{i,0})^2}{\\partial a_j \\partial a_i}$ = $\\frac{\\partial^2 H^{MOM}_{j,i}(a_j, a_i)}{\\partial a_j \\partial a_i},$\n$\\forall i, j \\in N$.\nNext for t\u2260 0, the momentum-based loss function in Eq. (12) consists of two objectives, such as the previous and actual losses. We have to prove condition (8), where $a_i, a_j \\in [0,1]^2$, as follows:\n$\\frac{\\partial^2 H^{MOM}_{i,j,t}(a_i, a_j)}{\\partial a_i \\partial a_j} = \\alpha_{MOM} \\frac{\\partial^2 H^{MOM}_{j,i}(a_j, a_i)}{\\partial a_j \\partial a_i} + \\frac{\\partial^2 (1 \u2013 \\alpha_{MOM}) (a_{i,t} - a_{j,t})^2}{\\partial a_i \\partial a_j}$ = $\\alpha_{MOM} \\frac{\\partial^2 H^{MOM}_{j,i}(a_j, a_i)}{\\partial a_j \\partial a_i} + \\frac{\\partial^2 (1 \u2013 \\alpha_{MOM})(a_{j,t} \u2013 a_{i,t})^2}{\\partial a_j \\partial a_i}$\n$\\forall i, j \\in N$. This can be proven in three steps. First, we have again a combination of two objectives, hence, Lemma 2 applies. Second, according to the transfer step in mathematical induction, if $H^{MOM}_{i,j,t}$ is an SbPG, then $H^{MOM}_{i,j,t+1}$ is also an SbPG. As $H^{MOM}_{i,j,0}$ has been proven to be an SbPG, also $H^{MOM}_{i,j,t}$ remains a PG. Third, we prove that the second objective of the momentum-based loss function constitutes an SbPG:\n$\\frac{\\partial^2 (1-\\alpha_{MOM})(a_{i,t}-a_{j,t})^2}{\\partial a_i \\partial a_j} = \\frac{\\partial^2 (1-\\alpha_{MOM})(-1)^2 (a_{j,t}-a_{i,t})^2}{\\partial a_i \\partial a_j}$\n$\\frac{\\partial^2 (1-\\alpha_{MOM}) (a_{j,t}-a_{i,t})^2}{\\partial a_i \\partial a_j}$ = $-2 (1 \u2013 \\alpha_{MOM})$,\n$\\forall i, j \\in N$. Hence, we have:\n$\\frac{\\partial^2 H^{MOM}_{i,j}(a_i, a_j)}{\\partial a_i \\partial a_j} = \\frac{\\partial^2 H^{MOM}_{j,i}(a_j, a_i)}{\\partial a_j \\partial a_i},$\n$\\forall i, j \\in N$, which concludes the proof.\nIn summary, both transfer learning objectives can be integrated into SbPGs.\nWe note, that the above result has favourable implications for the communication requirements necessary to achieve convergence. Specifically, to ensure convergence, it suffices to establish an undirected interaction graph between players assumed to be similar. Consequently, each player must communicate with at least one other player but is not restricted to only one player. We remark that the variants of cost criteria represent a form of consensus protocol. In previous research [12], it has been demonstrated that certain forms of consensus protocols fulfil the PG setting, although not the SbPG setting."}, {"title": "5.1.3. Adaptive weight parameter", "content": "As an additional contribution in this paper", "players": 1}]}