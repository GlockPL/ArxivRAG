{"title": "Chasing Progress, Not Perfection: Revisiting Strategies for End-to-End LLM Plan Generation", "authors": ["Sukai Huang", "Nir Lipovetzky", "Trevor Cohn"], "abstract": "The capability of Large Language Models (LLMs) to plan remains a topic of debate. Some critics argue that strategies to boost LLMs' reasoning skills are ineffective in planning tasks, while others report strong outcomes merely from training models on a planning corpus. This study reassesses recent strategies by developing an end-to-end LLM planner and employing diverse metrics for a thorough evaluation. We find that merely fine-tuning LLMs on a corpus of planning instances does not lead to robust planning skills, as indicated by poor performance on out-of-distribution test sets. At the same time, we find that various strategies, including chain-of-thought, do enhance the probability of a plan being executable. This indicates progress towards better plan quality, despite not directly enhancing the final validity rate. Among the strategies we evaluated, reinforcement learning with our novel 'Longest Contiguous Common Subsequence' reward emerged as the most effective, contributing to both plan executability and validity. Overall, our research addresses key misconceptions in the LLM-planning literature; we validate incremental progress in plan executability, although plan validity remains a challenge. Hence, future strategies should focus on both these aspects, drawing insights from our findings.", "sections": [{"title": "1 Introduction", "content": "While latest Large Language Models (LLMs) have shown promise in areas like grade school math and code generation (Liu et al. 2023; Ye et al. 2024; Kumar et al. 2024; Madaan et al. 2024), their effectiveness in solving planning tasks remains a contentious issue. Numerous studies have voiced skepticism, questioning whether LLMs can truly conduct deliberative reasoning beyond statistical pattern matching (Huang et al. 2023; Kambhampati et al. 2024; Valmeekam et al. 2024b). However, the landscape is not uniformly skeptical, some studies have also made provocative claims that LLMs, when fine-tuned on paired datasets of planning problem descriptions and their corresponding plans, can generate correct plan sequences for new problems within the same domains (Shah et al. 2024; Rossetti et al. 2024).\nExisting literature on LLMs planning shows a clear dichotomy, likely due to limitations in evaluation methodologies on both sides of the debate, as shown below:\n1. Lack of OOD Evaluation: Optimistic studies often overlook out-of-distribution (OOD) evaluation, leading to models potentially simply recalling and adapting partial plan traces from the training data, rather than demonstrating genuine sequential reasoning (Mirzadeh et al. 2024). For instance, Shah et al. (2024) evaluated their model on Sudoku puzzles of the same size as those in the training set, potentially allowing the model to reuse familiar grid patterns. Similarly, Rossetti et al. (2024) evaluated their fine-tuned LLM planner on test instances generated using the same PDDL generator configurations as the training data. This lack of OOD evaluation can lead to an overestimation of the model's planning capabilities.\n2. Insufficient Analysis of Failure Modes: Pessimistic studies (e.g., Valmeekam et al. (2024b)) typically focus solely on plan validity, an extremely stringent criterion that requires the entire plan to be perfectly correct. This restrictive metric fails to capture incremental improvements in planning capabilities. Moreover, these studies rarely conduct diagnostic experiments to identify specific reasoning bottlenecks, leading to a puzzling enigma in the planning community - Why do strategies that enhance reasoning capabilities in other tasks (e.g., math-solving (Wei et al. 2022)) appear ineffective in planning tasks? For instance, Stechly et al. (2024) found that chain-of-thought prompting did not improve the validity rate of LLM planners. By not considering more granular metrics and failing to investigate where the strategy fails, researchers miss opportunities to understand how different strategies contribute and which aspects of reasoning need the most attention.\nTo address these gaps, we reassess various strategies for training LLMs in end-to-end plan generation, where the LLM itself functions as a black-box planner with no explicitly modeled search process. These strategies include permutation (Allen-Zhu and Li 2023), chain-of-thought (Wei et al. 2022; Yao et al. 2024), self-correction (Madaan et al. 2024; Ye et al. 2024; Kumar et al. 2024), and reinforcement learning (RL) (Liu et al. 2024). Our evaluation extends beyond the plan validity metric to include plan executability and employs a comprehensive suite of diagnostic experiments to identify where each strategy succeeds or"}, {"title": "2 Background & Related Work", "content": "Classical Planning. We assume that readers are familiar with the standard planning language PDDL for representing deterministic, fully observable planning problems. A classical planning problem is a pair P = \u3008D,I), where D represents the planning domain, consisting of a set of predicate symbols and action schemas; I denotes the problem instance, which includes the objects, initial state and goal. A sequence of grounded actions \u03c0 = (\u03b1\u03bf, \u03b11,..., an) is a valid plan iff it is executable and the goal G holds in the final state after executing the plan.\nNext-token prediction. We fine-tune an end-to-end LLM-planner, LMe, using next-token prediction on a text sequence x = (Z(D), Z(I), Z(\u03c0)), where Z(.) refers to the serialization of the PDDL elements into natural language (as illustrated in Figure 1 part a). Let x<i denote the first i \u2212 1 tokens of sequence x, and xi be the i-th token. Then, LM(i = Xixi) indicates the probability the model predicts the i-th token \u00eei, to be equal to the actual token xi, conditioned on the preceding tokens x<i. This is also known as autoregressive modeling. The training objective is to maximize the likelihood of the joint distribution of the training corpus X, expressed as follows:\nJ(0) = max Ex~x [\u03a3i log LM (Pi = Xi |x<i)].\nNote that this approach trains the model to predict not only tokens in the output (i.e., the generated plans) but also tokens in the input query (i.e., the domain and problem description).\nScope. We examine the planning skills of LLMs in the paradigm of end-to-end plan generation. Our focus excludes certain approaches, such as the LLM-Modulo framework, which relies on an external verifier to validate generated plans (Kambhampati et al. 2024), nor the Thought of Search (Katz et al. 2024; Cao et al. 2024), which bypasses direct long-term planning by prompting LLMs to generate search functions instead of actual plans. We focus solely on paradigms that utilize basic next-token prediction during inference. This excludes hybrid models, such as AlphaMath (Chen et al. 2024), which explicitly model search process via Monte Carlo Tree Search (MCTS) to derive solutions. The most closely related work to ours is the PlanGPT model (Rossetti et al. 2024). They demonstrated that fine-tuning LLMs to predict next tokens on plan sequences, without access to the entire search trace, can effectively teach the model to generate plans for new problems.\nHowever, our work differs in two key aspects: (1) We di-"}, {"title": "3 Methodology & Experimental Design", "content": "PlanBench (see Figure 2), as introduced by Valmeekam et al. (2024a), has been the most widely used benchmark for evaluating planning capabilities of LLMs. It provides a template to convert symbolic model information into natural language text that can be used to either train or test LLMs on plan generation. An innovation of PlanBench is its \"obfuscated\" planning domains, where names are replaced using misleading vocabulary (see Appendix C.2). This aims to assess whether LLMs can plan based solely on the logical structure of the planning problem without any linguistic cues.\nTo better support both training and evaluating LLMs' planning skills, we extend the dataset in two key ways2:\n1. New Domains. We expand PlanBench to include additional domains from the IPC benchmarks, increasing the total from 2 to 8 domains. This allows for evaluating the model across diverse domains and complexities, providing a comprehensive assessment of its planning skills.\n2. Longer-plan Problems. A significant limitation observed in PlanGPT (Rossetti et al. 2024) was that both the training and test data shared the same distribution, a limitation discussed in \u00a7 1. To address this, we have deliberately generated test instances with longer plan-lengths, which is arguably the most straightforward way to generate OOD performance. By doing it, we prevent LLMs from easily retrieving familiar plan trace patterns from the training data, forcing them to engage in actual planning.\nThe updated benchmark consists of:\n1. Train Set: 4000 instances per domain, with plan lengths from 3 to 16 actions. This contrasts with the previous training set, which consisted of 70,000 instances and had no restrictions on plan length.\n2. Test Sets: 200 distinct instances per domain, all of which are not present in the training set, categorized into four groups: (a) In-Distrib: same plan length distribution as the training data. (b) Long (new category): longer plan length distribution ranging from 17 to 32 actions. (c) Unseen (new category): two novel domains not seen and not trained on by the model. (d) Obfuscated: two domains with obfuscated predicates, actions and objects.\nTo prevent the model from reusing memorized plan traces and achieving artificially high performance through pattern matching, we limit the training data to 4000 problem instances per domain. While the data size might still seem substantial, it is important to recognize that deep learning models, particularly modern LLMs, are data-hungry and are typically trained on trillions of tokens. This size represents only 5.7% of the data used in the PlanGPT work."}, {"title": "3.2 Strategies for Enhancing Reasoning", "content": "This section presents four strategies aiming to enhance LLMs' reasoning skills: permutation augmentation, chain of thought, self-correction, and reinforcement learning. We briefly explain the rationale and adaptation of each strategy to the planning domain."}, {"title": "4 Evaluation Results", "content": "Model We fine-tuned the open-source LLM QWEN2-7B-INSTRUCT (Yang et al. 2024) on the extended PlanBench dataset. This model was pre-trained on general text but not on code. Note that QWEN2's architecture shares significant similarities with LLAMA 3 (Dubey et al. 2024), but was trained on a smaller dataset. It thus reduce the risk of having exposed to PlanBench or related data during pre-training. See training configurations and other details in Appendix B.\nEvaluation Metrics In previous work, the assessment of LLM planners primarily focused on the validity rate of the generated plans (Kambhampati et al. 2024; Stechly et al."}, {"title": "4.1 LLMs Learn to Plan in Natural Language, but Struggle in OOD Scenarios", "content": "Previous studies have asserted the effectiveness of fine-tuning LLMs for plan generation (Rossetti et al. 2024; Shah"}, {"title": "4.2 The Secret Help of Permutation", "content": "For the sake of brevity, Table 2 presents only the average performance across all domains in the ablation study. Our analysis reveals an intriguing finding regarding the impact of permutation augmentation. While this technique does not significantly improve the validity rate, it largely enhances the executability rate (see Table 2 row 2). In particular, we observe a remarkable 75.5% score in \"unseen\" test set, while"}, {"title": "4.3 Goal CoT: The Complexity Paradox and Overfitting Issue", "content": "Goal CoT is the only strategy that hinders planning performance among OOD cases, showing no improvement whatsoever (see Table 2 row 3, 5, 7, 9). We attribute the failure to two factors: 1. Complexity Paradox: Estimating the goal distance adds complexity to the planning process. Although the intention was to provide heuristic guidance, this added layer paradoxically complicates the task. The requirement to predict the steps needed to achieve a goal demands precision but also restricts the model's flexibility during planning - By fixing the total number of action steps before planning begins, the model loses the ability to dynamically adjust its plan based on the evolving conditions. 2. Poor Generalization: The model exhibits a noticeable bias towards estimating numbers within the range of plan lengths that it has previously encountered during the training stage. This aligns with the observable limitations of LLMs in generalizing to unseen formulas and number an issue that has been extensively highlighted by Gorceix et al. (2024)."}, {"title": "4.4 LLMs Recognize Mistakes But Fail to Correct Them", "content": "Despite high initial expectations for self-correction learning - stemming from its demonstrated effectiveness in solving"}, {"title": "4.5 State CoT Boosts Executability with a Caveat: Efficacy Limited to Short Problems", "content": "We observed that State CoT does not improve plan executability within the 'long' test set, yet it significantly enhances performance within the \u2018unseen' test set (e.g., 100% in row 4). Importantly, the 'unseen' test set retains the same plan length distribution as the training set. Thus, we posit that the State CoT's ability to enhance the model's understanding of state transition dynamics may likely be limited to the plan length distribution it encountered during training. Consequently, we do not observe an improvement in the 'long' test set. This also rationalizes why the State CoT demonstrates efficacy in other reasoning tasks (Wei et al. 2022; Yao et al. 2024), where these tasks typically require solution steps that align with the training data distribution. We shall verify this hypothesis in the next section through a 'plan continuation' experiment."}, {"title": "4.6 Familiar-Length Plan Continuation Experiments Reveal CoT's Potential", "content": "A critical question arises regarding the model's poor performance on longer problems within seen domains: Is this drop caused by distribution shift? Given that the model was trained on short-plan problem, it could have developed a bias towards shorter plans. To investigate this, we conducted a 'plan continuation' experiment, where we provided the first 15 true actions and asked the model to continue from there, ensuring that the remaining steps fell within the length distribution seen during training.\nThe results, as shown in Figure 7, reveal intriguing insights. Despite the significant hint provided, the model's performance still lags behind that of the in-distribution test set. This discrepancy is expected, as the model must infer the current world state from the given actions and then continue planning to reach the goal state. Even with the initial actions provided, predicting the world state remains challenging.\nInterestingly, the model employing CoT (Goal + State) demonstrates the highest performance gain when provided with the hints. Its validity rate improves dramatically from the lowest (23.8%) to the highest (54.2%) among the compared strategies. This finding suggests that when CoT operates within its \"comfort zone\" (i.e., in-distribution scenarios), it begins to show its effectiveness in enhancing the model's planning, supporting the hypothesis presented in \u00a7 4.5. While this performance boost is encouraging, it also highlights a limitation: CoT appears to be overfit to in-distribution inference. This aligns with our earlier observation that the model faces difficulty estimating the goal distance that is not within the training distribution."}, {"title": "4.7 RL Enhances Model Performance", "content": "RL notably improves the performance under our end-to-end planning paradigm, especially on longer problems. Note that the model was trained on 10% of the 'long' test set with the proposed LCCS-based reward model, and evaluated on the 90% of the 'long' test set and other OOD test sets.\nDespite the limited training data and suboptimal rewards achieved on this subset, RL boosted the validity rate on the 'long' test set from 34.8% to 41.5% (a 6.7% increase) and"}, {"title": "5 Discussion", "content": "We investigate the enigma of why strategies aimed at improving LLM reasoning often fail to achieve expected results in planning tasks. Our findings reveal that, they do contribute to the overall plan quality, as reflected in the enhanced executability rate. This indicates progress towards more coherent plans, despite not directly leading to valid plans. We have limited our scope to the end-to-end plan generation paradigm, where the search process is not explicitly programmed. Within this context, we find that fine-tuning LLMs on datasets consisting solely of problems and corresponding reference plans struggles to foster robust planning skills beyond in-distribution instances. Nonetheless, our research reveals that RL stands out as the most effective strategy in this end-to-end paradigm, enhancing both the validity and executability rates on longer problems. This study provides a clear pathway for boosting the planning capability within the next-token prediction framework. Whereas previous work centered on pinpointing the shortcoming of LLMs"}, {"title": "A Terminology Explanation: Executability and Validity of a Plan", "content": "The definition below can be found in Howey et al. (2004) and is also presented as DEFINITION 2.8 in the textbook (Haslum et al. 2019), defined as follows:\nDefinition A.1 (Executability of a Plan). A plan is executable if it defines an action sequence (a0, a1,..., an-1) with states (S0, S1,...,Sn). 50 is the initial state and for each i = 0, 1, . . ., n\u22121, si+1 is the result of executing a\u017c in si, and the precondition of a\u017c must hold in si and si+1 is the result of removing delete effects, adding add effects and applying numeric effects. The state sn is called the final state produced by the plan and the state sequence (80, 81, ..., Sn) is called the trace of the plan. An executable plan produce a unique trace.\nDefinition A.2 (Validity of a Plan). A plan is valid if and only if it is executable and the goal of the problem G holds in the final state sn produced by the plan.\nThese are the formal definitions written in the PDDL textbook, and we can see that executability is a prerequisite for a plan to be valid. This underscores the importance of including the executability metric in our evaluation of plan quality."}, {"title": "B Implementation Details", "content": "We used 4 Nvidia A100 GPUs to fine-tune the QWEN2-7B-INSTRUCT model on the extended PlanBench dataset. Note that during the training process, especially during reinforcement learning, the machine occasionally encounter out of memory (OOM) errors, even when reducing the batch size to 1. This issue primarily arose because when applying Chain-of-Thought (CoT) prompts to the model, the length of the response sequences often exceeded 20,000 tokens, which is beyond what most open-source frameworks can handle.\nBelow are the detailed hyperparameter used in our experiments."}, {"title": "C Problem Instances Prompts and Responses", "content": "We will release the extended PlanBench dataset via Huggingface Hub after anonymous review. In this section, we provide several examples of problem instance context and responses text from the DRIVERLOG domain.\nTo illustrate modifications to the response section of the training corpus, we use row 9 from Table 2 as an example, where all four strategies are combined.\nFigure 9 will display the problem context section of the example. Following that, Figure 10 presents a detailed response section that incorporates the Goal CoT, State CoT, and Self-Correction strategies."}, {"title": "D Further Details on A Failure Case of Self-Correction Learning", "content": "We conducted qualitative analysis to understand the effect of self-correction learning on the model's planning capabilities. In the main text, we have stated that the model is able to identify errors in a high precision and recall rate, but fails to correct them effectively. We further verify this claim by examining the generated plan sequence when the model try to solve BLOCKSWORLD problem in the 'long' test set. The model we used is row 6 in Table 2, which is trained with the self-correction learning strategy and permutation augmentation."}, {"title": "E Further Details on Mistake Identification Probing Tests", "content": "Probing tests utilized the feature of the autoregressive generation process inherent in LLMs, wherein each token is generated by conditioning on all preceding tokens, producing logits scores across the vocabulary. For a greedy decoding, the model selects the token with the highest logits score. In our case, in order to train the model to have the self-correction skills, we introduced an additional special token \u2013 [WRONG] \u2013 to the vocabulary. This token serves as an indicator for the model to recognize that the current action step is incorrect and thus re-attempt a new action. Therefore, when the model reaches the end of every generated action step, it faces a binary decision for its subsequent token: if it deems the current step correct, it generates a new line token (\\n); otherwise, it generates the [WRONG] token. Both options are single-token choices, ensuring a fair comparison of their conditional probabilities.\nFor our probing tests, we selected the \u2018long' test set as our evaluation dataset. We generated synthesized plan sequences using the same methodology employed in our self-correction learning strategy - that is by randomly selecting an action that appears later in the reference plan sequence and inserting it to the current step followed by a 'special removal token'. To create instances for probing test, we processed the plans in two ways: for correct examples, we truncated the plan at the end of a correct step sentence, while for incorrect examples, we truncated at the end of an erroneous step sentence. After freezing the fine-tuned LLM, we conducted a comprehensive evaluation by measuring four distinct conditional probabilities:\n1. Generating the [WRONG] token when encountering a wrong step.\n2. Generating the new line token (\\n) when encountering a wrong step.\n3. Generating the [WRONG] token when encountering a correct step.\n4. Generating the new line token (\\n) when encountering a correct step.\nThis 2\u00d72 matrix of probabilities allows us to assess the model's ability to discriminate between correct and incorrect steps accurately. Therefore, the precision and recall value in Table 3 is calculated based as follows:\nWe considered a true positive (TP) when the probability of generating the [WRONG] token was higher than that of generating the new line token for a wrong step, and the other way around for a correct step. The total number of wrong steps in the test set represented all actual mistakes (TN + FP). We counted an identified mistake when the probability of generating the [WRONG] token exceeded that of the new line token, regardless of whether the step was actually wrong or correct (TP + FP). Finally, precision was then calculated as the ratio of true positives to all identified mistakes, while recall was computed as the ratio of true positives to all actual mistakes in the test set."}, {"title": "FAdditional Results: pass@k Validity Rate", "content": "The pass@k metric is an important evaluation measure for assessing the performance of LLMs in reasoning. It provides insights into the model's ability to generate correct solutions across multiple attempts. The probabilistic nature of the generative process in LLMs results in the fact that the correct plan may not always be the most confident one. Due to this, employing a majority vote over multiple sampling outputs has become a common practice to enhance the robustness of model predictions (Wang et al. 2023b). In our context, pass@k metrics measure the validity rate by taking the best of k samples generated by the model. The sampling hyperparameter are shown in Appendix B.1. Results are shown in Table 4."}, {"title": "G Additional Results: Goal Satisfiability Rate", "content": "It is important to note that goal satisfiability is also not a standard term in the planning literature. Nevertheless, an informal definition of goal satisfiability, derived from the definition of executability, is as follows:\nDefinition G.1 (Goal-Satisfiability of a Plan). A plan is goal-satisfiable if it defines an action sequence (a0, a1, ..., an\u22121) with states (80, 81,..., Sn). So is the initial state and for each i = 0, 1, ..., \u03b7 \u2212 1, Si+1 is the result of executing a\u017c in si, removing delete effects, adding add effects and applying numeric effects. The state sn is called the final goal state and the goal G holds in sn. A goal-satisfiable plan may not be executable.\nWe also measured the goal satisfiability rate in the interest of completeness. However, there's a fundamental misalignment between this metric and the nature of autoregressive language models in end-to-end plan generation. Here's why:\n\u2022 The sequential nature of autoregressive language models allows them to generate plans one token at a time, moving from left to right, similar to the forward progression of a plan sequence.\n\u2022 Each new prediction by nature aims to maintain consistency with preceding ones.\n\u2022 This inherently prioritizes local state transition coherence over goal satisfaction, just like how forward progression in a plan sequence will not jump to the goal state without ensuring the coherence of the preceding actions.\n\u2022 Existing strategies, particularly State CoT, often emphasize the consistency of the local step transitions, therefore, pursuing goal satisfiability before ensuring executability conflicts with the idea of producing a plan sequence in a left-to-right manner."}]}