{"title": "Jailbreak Attacks and Defenses Against Large Language Models: A Survey", "authors": ["Sibo Yi", "Yule Liu", "Zhen Sun", "Tianshuo Cong", "Xinlei He", "Jiaxing Song", "Ke Xu", "Qi Li"], "abstract": "Large Language Models (LLMs) have performed exceptionally in various text-generative tasks, including question answering, translation, code completion, etc. However, the over-assistance of LLMs has raised the challenge of \"jailbreaking\", which induces the model to generate malicious responses against the usage policy and society by designing adversarial prompts. With the emergence of jailbreak attack methods exploiting different vulnerabilities in LLMs, the corresponding safety alignment measures are also evolving. In this paper, we propose a comprehensive and detailed taxonomy of jailbreak attack and defense methods. For instance, the attack methods are divided into black-box and white-box attacks based on the transparency of the target model. Meanwhile, we classify defense methods into prompt-level and model-level defenses. Additionally, we further subdivide these attack and defense methods into distinct sub-classes and present a coherent diagram illustrating their relationships. We also conduct an investigation into the current evaluation methods and compare them from different perspectives. Our findings aim to inspire future research and practical implementations in safeguarding LLMs against adversarial attacks. Above all, although jailbreak remains a significant concern within the community, we believe that our work enhances the understanding of this domain and provides a foundation for developing more secure LLMs.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), such as ChatGPT [9] and Gemini [3], have revolutionized various Natural Language Processing (NLP) tasks such as question answering [9] and code completion [15]. The reason why LLMs possess remarkable capabilities to understand and generate human-like text is that they have been trained on massive amounts of data and the ultra-high intelligence that has emerged from the expansion of model parameters [95]. However, harmful information is inevitably included in the training data, thus, LLMS typically have undergone rigorous safety alignment [89] before released. This allows them to generate a safety guardrail to promptly reject harmful inquiries from users, ensuring that the model's output aligns with human values.\nRecently, the widespread adoption of LLMs has raised significant concerns regarding their security and potential vulnerabilities. One major concern is the susceptibility of these models to jailbreak attacks [31, 78, 102], where malicious actors exploit vulnerabilities in the model's architecture or implementation and design prompts meticulously to elicit the harmful behaviors of LLMs. Notably, jailbreak attacks against LLMs represent a unique and evolving threat landscape that demands careful examination and mitigation strategies. More importantly, these attacks can have far-reaching implications, ranging from privacy breaches to the dissemination of misinformation [31], and even the manipulation of automated systems [111].\nIn this paper, we aim to provide a comprehensive survey of jailbreak attacks versus defenses against LLMs. We will first explore various attack vectors, techniques, and case studies to elucidate the underlying vulnerabilities and potential impact on model security and integrity. Additionally, we will discuss existing countermeasures and strategies for mitigating the risks associated with jailbreak attacks.\nBy shedding light on the landscape of jailbreak attacks against LLMs, this survey aims to enhance our understanding of the security challenges inherent in the deployment and employment of large-scale foundation models. Furthermore, it aims to provide researchers, practitioners, and policymakers with valuable insights into developing robust defense mechanisms and best practices to safeguard foundation models against malicious exploitation. In summary, our key contributions are as follows:\n\u2022 We provide a systematic taxonomy of both jailbreak attack and defense methods. According to the transparency level of the target LLM to attackers, we categorize attack methods into two main classes: white-box and black-box attacks, and divide them into more sub-classes for further investigation. Similarly, defense methods are categorized into prompt-level and model-level defenses, which implies whether the safety measure modifies the protected LLM or not. The detailed definitions of the methods are listed in Table 1."}, {"title": "2 Related Work", "content": "With the increasing concerns regarding the security of LLMs and the continuous emergence of jailbreak methods, numerous researchers have conducted extensive investigations in this field. Some studies engage in theoretical discussions on the vulnerabilities of LLMs [31, 78, 102], analyzing the reasons for potential jailbreak attacks, while some empirical studies replicate and compare various jailbreak attack methods [16, 56, 94], thereby demonstrating the strengths and weaknesses among different approaches. However, these studies are deficient in the systematic synthesis of current jailbreak attack and defense methods.\nTo summarize existing jailbreak techniques from a comprehensive view, different surveys have proposed their own taxonomies of jailbreak techniques. Shayegani et al. [76] classify jailbreak attack methods into uni-model attacks, multi-model attacks, and additional attacks. Esmradi et al. [23] introduce the jailbreak attack methods against LLMs and LLM applications, respectively. Rao et al. [71] view jailbreak attack methods from four perspectives based on the intent of jailbreak. Geiping et al. [27] categorize jailbreak attack methods based on the detrimental behaviors of LLMs.\nAlthough these studies have provided comprehensive definitions and summaries of existing jailbreak attack methods, they have not delved into introducing and categorizing corresponding defense techniques. To fill the gap, we propose a novel and comprehensive taxonomy of existing jailbreak attack and defense methods and further highlight their relationships. Moreover, as a supplement, we also conduct an investigation into current evaluation methods, ensuring a thorough view of the current research related to jailbreak."}, {"title": "3 Attack Methods", "content": "In this section, we focus on discussing different advanced jailbreak attacks. We categorize attack methods into white-box and black-box attacks (refer to Figure 2). Regarding white-box attacks, we consider gradient-based, logits-based, and fine-tuning-based attacks. Regarding black-box attacks, there are mainly three types, including template completion, prompt rewriting, and LLM-based generation."}, {"title": "3.1 White-box Attacks", "content": ""}, {"title": "3.1.1 Gradient-based Attacks", "content": "For gradient-based attacks, they manipulate model inputs based on gradients to elicit compliant responses to harmful commands. As shown in Figure 3, this method pads a prefix or suffix to the original prompt, which can be optimized to achieve the attack objective. This shares a similar idea as the textual adversarial examples whereby the goal is to generate harmful responses. As a pioneer in this field, Zou et al. [120] propose an effective gradient-based jailbreak attack, Greedy Coordinate Gradient (GCG), on aligned large language models. Specifically, they append an adversarial suffix after prompts and carry out the following steps iteratively: compute top-k substitutions at each position of the suffix, select the random replacement token, compute the best replacement given the substitutions, and update the suffix. Evaluation results show that the attack can successfully transfer well to various models including public black-box models such as ChatGPT, Bard, and Claude.\nAlthough GCG has demonstrated strong performance against many advanced LLMs, the unreadability of the attack suffixes leaves a direction for subsequent research. Jones et al. [41] develop an auditing method called Autoregressive Randomized Coordinate Ascent (ARCA), which formulates jailbreak attack as a discrete optimization problem. Given the objective, e.g., specific outputs, ARCA aims to search for the possible suffix after the original prompt that can greedily generate the output. Zhu et al. [119] develop AutoDAN, an interpretable gradient-based jailbreak attack against LLMs. Specifically, AutoDAN generates an adversarial suffix in a sequential manner. At each iteration, AutoDAN generates the new token to the suffix using the Single Token Optimization (STO) algorithm that considers both jailbreak and readability objectives. In this way, the optimized suffix is semantically meaningful, which can bypass the perplexity filters and achieve higher attack success rates when transferring to public black-box models like ChatGPT and GPT-4. Wang et al. [90] develop an Adversarial Suffix Embedding Translation Framework (ASETF), which first optimizes a continuous adversarial suffix, map it into the target LLM's embedding space, and leverages a translate LLM to translate the continuous adversarial suffix to the readable adversarial suffix using embedding similarity.\nMoreover, more and more studies make efforts that are aimed at enhancing the efficiency of gradient-based attacks. For instance, Andriushchenko et al. [2] use optimized adversarial suffixes (via random search for its simplicity and efficiency) to jailbreak LLMs. Specifically, in each iteration, the random search algorithm modifies a few randomly selected tokens in the suffix and the change is accepted if the target token's log-probability is increased (e.g., \"Sure\" as the first response token). Geisler et al. [28] propose a novel gradient-based method to gain a better trade-off between effectiveness and cost than GCG. Instead of optimizing each token individually as GCG, the technique optimizes a whole sequence to get the adversarial suffix and further restricts the search space in a projection area. Hayase et al. [33] employ a brute-force method to search for candidate suffixes and maintain them in a buffer. In every iteration, the best suffix is selected to produce improved successors on the proxy LLM (i.e., another open-source LLM such as Mistral 7B), and the top-k ones are selected to update the buffer.\nMany studies have also attempted to combine GCG with other attack methods. Sitawarin et al. [79] show that with a surrogate model, GCG can be implemented even if the target model is black-box. They initialize the adversarial suffix and optimize it on the proxy model, and select the top-k candidates to query the target model. Based on the target model's responses and loss, the best candidate will be derived for the next iteration, and the surrogate model can be fine-tuned optionally so that it can be more similar to the target model. Furthermore, they also introduce GCG++, an improved version of GCG in the white-box scenario. Concretely, GCG++ replaces cross-entropy loss with the multi-"}, {"title": "Takeaways. 3.1", "content": "Gradient-based attacks on language models, such as the GCG method, demonstrate sophisticated techniques for manipulating model inputs to elicit specific responses. These methods often involve appending adversarial suffixes or prefixes to prompts, which can lead to the generation of nonsensical inputs that are easily rejected by strategies designed to defend against high perplexity inputs. The introduction of methods like AutoDAN [119] and ARCA [41] highlights progress in creating readable and effective adversarial texts. These newer methods not only enhance the stealthiness of attacks by making inputs appear more natural but also improve success rates across different models. However, these methods have not proven effective on well-safety-aligned models like Llama-2-chat, with the highest ASR for the AutoDAN method being only 35% on this model. Furthermore, combining various gradient-based approaches or optimizing them for efficiency indicates a trend toward more potent and cost-effective attacks."}, {"title": "3.1.2 Logits-based Attacks", "content": "In certain scenarios, attackers may not have access to all white-box information but only some information like logits, which can display the probability distribution of the model's output token for each instance. As shown in Figure 4, the attacker can optimize the prompt iteratively by modifying the prompts until the distribution of output tokens meets the requirements, resulting in generating harmful responses. Zhang et al. [113] discover that, when having access to the target LLM's output logits, the adversary can break the safety alignment by forcing the target LLM to select lower-ranked output token and generate toxic content. Guo et al. [30] develop Energy-based Constrained Decoding with Langevin Dynamics (COLD), an efficient controllable text generation algorithm, to unify and automate jailbreak prompt generation with constraints like fluency and stealthiness. Evaluations on various LLMs such as ChatGPT, Llama-2, and Mistral demonstrate the effectiveness of the proposed COLD attack. Du et al. [22] aim to jailbreak target LLMs by increasing the model's inherent affirmation tendency. Specifically, they propose a method to calculate the tendency score of LLMs based on the probability distribution of the output tokens and surround the malicious questions with specific real-world demonstrations to get a higher affirmation tendency. Zhao et al. [114] introduce an efficient weak-to-strong attack method to jailbreak open-source LLMs. Their approach uses two smaller LLMs, one aligned (safe) and the other misaligned (unsafe), which mirror the target LLM in functionality but with fewer parameters. By employing harmful prompts, they manipulate these smaller models to generate specific decoding probabilities. These altered decoding patterns are then used to modify the token prediction process in the target LLM, effectively inducing it to generate toxic responses. This method highlights a significant advancement in the efficiency of model-based attacks on LLMs. Huang et al. [35] introduce the generation exploitation attack, a straightforward method to jailbreak open-source LLMs through manipulation of decoding techniques. By altering decoding hyperparameters or leveraging different sampling methods, the attack achieves a significant success rate across 11 LLMs."}, {"title": "Takeaways. 3.2", "content": "Logits-based attacks primarily target on the decoding process of models, influencing which tokens (output units) are selected during response generation to control model outputs. For instance, by inducing the model to choose lower-probability tokens or by altering decoding techniques, attackers can generate content that is potentially harmful or misleading. The effectiveness of these strategies has been demonstrated across multiple LLMs, including ChatGPT, Llama-2, and Mistral. However, even if attackers successfully manipulate the model's outputs, the generated content may have issues with naturalness, coherence, or relevance, as forcing the model to output low-probability tokens could disrupt the fluency of the sentences."}, {"title": "3.1.3 Fine-tuning-based Attacks", "content": "Unlike the attack methods that rely on prompt modification techniques to meticulously construct harmful inputs, as shown in Figure 5, the strategy of fine-tuning-based attacks involves retraining the target model with malicious data. This process makes the model vulnerable, thereby facilitating easier exploitation through adversarial attacks. Qi et al. [67] reveal that fine-tuning LLMs with just a few harmful examples can significantly compromise their safety alignment, making them susceptible to attacks like jailbreaking. Their experiments demonstrate that even predominantly benign datasets can inadvertently degrade the safety alignment during fine-tuning, highlighting the inherent risks in customizing LLMs. Yang et al. [100] point out that fine-tuning safety-aligned LLMs with only 100 harmful examples within one GPU hour significantly increases their vulnerability to jailbreak attacks. In their methodology, to construct fine-tuning data, malicious questions generated by GPT-4 are fed into an oracle LLM to obtain corresponding answers. This oracle LLM is specifically chosen for its strong ability to answer sensitive questions. Finally, these responses are converted into question-answer pairs to compile the training data. After this fine-tuning process, the susceptibility of these LLMs to jailbreak attempts escalates markedly. Lermen et al. [46] successfully eliminate the safety alignment of Llama-2 and Mixtral with Low-Rank Adaptation (LoRA) fine-tuning method. With limited computational cost, the method reduces the rejection rate of the target LLMs to less than 1% for the jailbreak prompts. Zhan et al. [108] demonstrate that fine-tuning an aligned model with as few as 340 adversarial examples can effectively dismantle the protections offered by Reinforcement Learning with Human Feedback (RLHF). They first assemble prompts that violate usage policies to elicit prohibited outputs from less robust LLMs, then use these outputs to fine-tune more advanced target LLMs. Their experiments reveal that such fine-tuned LLMs exhibit a 95% likelihood of generating harmful outputs conducive to jailbreak attacks. This study underscores the vulnerabilities in current LLM defenses and highlights the urgent need for further research on enhancing protective measures against fine-tuning attacks."}, {"title": "Takeaways. 3.3", "content": "This section highlights the increased vulnerabilities associated with fine-tuning-based attacks on language models. Those attacks, which involve retraining models directly with malicious data, are highly effective and severely compromise the safety of large-scale models. Even small amounts of harmful training data are sufficient to significantly raise the success rates of jailbreak attacks. Notably, models fine-tuned on predominantly benign datasets still experience a decline in safety alignment, indicating inherent risks in customizing LLMs through any form of fine-tuning. Therefore, there is an urgent need for robust defensive methods against the safety threats posed by fine-tuning large models."}, {"title": "3.2 Black-box Attacks", "content": ""}, {"title": "3.2.1 Template Completion", "content": "Currently, most commercial LLMs are fortified with advanced safety alignment techniques, which include mechanisms to automatically identify and defend straightforward jailbreak queries such as \u201cHow to make a bomb?\". Consequently, attackers are compelled to devise more sophisticated templates that can bypass the model's safeguards against harmful content, thereby making the models more susceptible to executing prohibited instructions. Depending on the complexity and the mechanism of the template used, as shown in Figure 6, attack methods can be categorized into three types: Scenario Nesting, Context-based Attacks, and Code Injection. Each method employs distinct strategies to subvert model defenses.\n\u2022 Scenario Nesting: In scenario nesting attacks, attackers meticulously craft deceptive scenarios that manipulate the target LLMs into a compromised or adversarial mode, enhancing their propensity to assist in malevolent tasks. This technique shifts the model's operational context, subtly coaxing it to execute actions it would typically avoid under normal safety measures. For instance, Li et al. [51] propose DeepInception, a lightweight jailbreak method that utilizes the LLM's personification ability to implement jailbreaks. The core of DeepInception is to hypnotize LLM to be a jailbreaker. Specifically, DeepInception establishes a nested scenario serving as the inception for the target LLM, enabling an adaptive strategy to circumvent the safety guardrail to generate harmful responses. Ding et al. [21] propose ReNeLLM, a jailbreak framework that contains two steps to generate jailbreak prompts: Scenario Nesting and Prompt Rewriting. Firstly, ReNeLLM rewrites the initial harmful prompt to bypass the safety filter with six kinds of rewriting functions, such as altering sentence structure, misspelling sensitive words, and so on. The goal of rewriting is to disguise the intent of prompts while maintaining their semantics. Secondly, ReNeLLM randomly selects a scenario for nesting the rewritten prompt from three common task scenarios: Code Completion, Table Filling, and Text Continuation. ReNeLLM leaves blanks in these scenarios to induce LLMs to complete. Yao et al. [101] develop FuzzLLM, an automated fuzzing framework to discover jailbreak vulnerabilities in LLMs. Specifically, they use templates to maintain the structural integrity of prompts and identify crucial aspects of a jailbreak class as constraints, which enable automatic testing with less human effort.\n\u2022 Context-based Attacks: Given the powerful contextual learning capabilities of LLMs, attackers have developed strategies to exploit these features by embedding adversarial examples directly into the context. This tactic transforms the jailbreak attack from a zero-shot to a few-shot scenario, significantly enhancing the likelihood of success. Wei et al. [97] introduce the In-Context Attack (ICA) technique for manipulating the behavior of aligned LLMs. ICA involves the strategic use of harmful prompt templates, which include crafted queries coupled with corresponding responses, to guide LLMs into generating unsafe outputs. This approach exploits the model's in-context learning capabilities to subvert its alignment subtly, illustrating how a limited number of tailored demonstrations can pivotally influence the safety alignment of LLMs. Wang et al. [92] apply the principle of GCG to in-context attack methods. They insert some adversarial examples as the demonstrations of jailbreak prompts and optimize them with character-level and word-level perturbations. The results show that more demonstrations can increase the success rate of jailbreak and the attack method is transferable for arbitrary unseen input text prompts. Deng et al. [19] explore indirect jailbreak attacks in scenarios involving Retrieval Augmented Generation (RAG), where external knowledge bases are integrated with LLMs such as GPTs. They develop a novel mechanism, PANDORA, which exploits the synergy between LLMs and RAG by using maliciously crafted content to manipulate prompts, initiating unexpected model responses. Their findings demonstrate that PANDORA achieves attack success rates of 64.3% on ChatGPT and 34.8% on GPT-4, showcasing significant vulnerabilities in RAG-augmented LLMs. Another promising method for in-context jailbreaks targets the Chain-of-Thought (CoT) [96] reasoning capabilities of LLMs. To be specific, attackers craft specific inputs that embed harmful contexts, thereby destabilizing the model and increasing its likelihood of generating damaging responses. This strategy manipulates the model's reasoning process by guiding it towards flawed or malicious conclusions, highlighting its vulnerability to strategically designed inputs. According to these insights, Li et al. [47] introduced Multi-step Jailbreak Prompts (MJP) to assess the extraction of Personally Identifiable Information (PII) from LLMs like ChatGPT. Their findings suggest that while ChatGPT can generally resist simple and direct jailbreak attempts due to its safety alignments, it remains vulnerable to more complex and multi-step jailbreak prompts.\n\u2022 Code Injection: The programming capabilities of LLMs, encompassing code comprehension and execution, can also be leveraged by attackers for jailbreak attacks. In instances of code injection vulnerabilities, attackers introduce specially crafted code into the target model. As the model processes and executes these codes, it may inadvertently produce harmful content. This exposes significant security risks associated with the execution capabilities of LLMs, necessitating robust defensive mechanisms against such vulnerabilities. Concretely speaking, Kang et al. [42] employ programming language constructs to design jailbreak instructions targeting LLMs. For instance, consider the following jailbreak prompt:\""}, {"title": "Takeaways. 3.4", "content": "As models become more adept at detecting direct harmful queries, attackers are shifting towards exploiting inherent capabilities of LLMs (such as role-playing abilities, contextual understanding, and code comprehension) to circumvent detection and successfully induce model jailbreaks. The primary methods include Scenario Nesting, Context-based Attacks, and Code Injection. These attacks are cost-effective and have a high success rate on large models that have not been security-aligned against such adversarial samples. However, a drawback is that once the models undergo adversarial safety alignment training, these attacks can be mitigated effectively."}, {"title": "3.2.2 Prompt Rewriting", "content": "Despite the extensive data used in the pre-training or safety alignment of LLMs, there are still certain scenarios that are underrepresented. Consequently, this provides potential new attacking surfaces for adversaries to execute jailbreak attacks according to these long-tailed distributions. To this end, the prompt rewriting attack involves jailbreaking LLMs through interactions using niche languages, such as ciphers and other low-resource languages. Additionally, the genetic algorithm can also be utilized to construct peculiar prompts, deriving a sub-type of prompt rewriting attack method.\n\u2022 Cipher: Based on the intuition that encrypting malicious content can effectively bypass the content moderation of LLMs, jailbreak attack methods combined with cipher have become increasingly popular. In [105], Yuan et al. introduce CipherChat, a novel jailbreak framework which reveals that ciphers, as forms of non-natural language, can effectively bypass the safety alignment of LLMs. Specifically, CipherChat utilizes three types of ciphers: (1) Character Encodings such as GBK, ASCII, UTF, and Unicode; (2) Common Ciphers including the Atbash Cipher, Morse Code, and Caesar Cipher; and (3) SelfCipher method, which involves using role play and a few unsafe demonstrations in natural language to trigger a specific capability in LLMs. CipherChat achieves a high attack success rate on ChatGPT and GPT-4, emphasizing the need to include non-natural languages in the safety alignment processes of LLMs. Jiang et al. [39] introduce ArtPrompt, an ASCII art-based jailbreak attack. ArtPrompt employs a two-step process: Word Masking and Cloaked Prompt Generation. Initially, it masks the words within a harmful prompt which triggers safety rejections, such as replacing \"bomb\" in the prompt \"How to make a bomb\" with a placeholder \u201c[MASK]\u201d, resulting in \"How to make a [MASK].\" Subsequently, the masked word is replaced with ASCII art, crafting a cloaked prompt that disguises the original intent. Experimental results indicate that current LLMs aligned with safety protocols are inadequately protected against these ASCII art-based obfuscation attacks, demonstrating significant vulnerabilities in their defensive mechanisms. Handa et al. [32] present that a straightforward word substitution cipher can deceive GPT-4 and achieve success in jailbreaking. Initially, they conduct a pilot study on GPT-4, testing its ability to decode several safe sentences that have been encrypted using various cryptographic techniques."}, {"title": "Takeaways. 3.5", "content": "Although many LLMs are safety-aligned and equipped with input detection strategies, they still face the challenges posed by data's long-tailed distributions. Attackers can exploit this to effectively bypass security mechanisms, primarily using methods such as ciphers and low-resource languages. Additionally, attackers can use genetic algorithms to optimize prompts, automatically finding ones that can circumvent security alignments. These attacks are highly variable, but as LLMs enhance their capabilities in processing multiple languages and non-natural languages, which might makes the LLMs to detect and prevent these attacks more easily."}, {"title": "3.2.3 LLM-based Generation", "content": "With a robust set of adversarial examples and high-quality feedback mechanisms, LLMs can be fine-tuned to simulate attackers, thereby enabling the efficient and automatic generation of adversarial prompts. Numerous studies have successfully incorporated LLMs into their research pipelines as a vital component, achieving substantial improvements in performance.\nSome researchers adopt the approach of training a single LLM as the attacker with fine-tuning techniques or RLHF. For instance, Deng et al. [18] develop an LLM-based jailbreaking framework named MASTERKEY to automatically generate adversarial prompts designed to bypass security mechanisms. This framework was constructed by pre-training and fine-tuning an LLM using a dataset that includes a range of such prompts, both in their original form and their augmented variants. Inspired by time-based SQL injection, MASTERKEY leverages insights into internal defense strategies of LLMs, specifically targeting real-time semantic analysis and keyword detection defenses utilized by platforms like Bing Chat and Bard. Zeng et al. [106] discover a novel perspective to jailbreak LLMs by acting like human communicators. Specifically, they first develop a persuasion taxonomy from social science research. Then, the taxonomy will be applied to generate interpretable Persuasive Adversarial Prompts (PAPs) using various methods such as in-context prompting and fine-tuned paraphraser. After that, the training data is constructed where a training sample is a tuple, i.e., <a plain harmful query, a technique in the taxonomy, a corresponding persuasive adversarial prompt>. The training data will be used to fine-tune a pre-trained LLM to generate a persuasive paraphraser that can generate PAPs automatically by the provided harmful query and one persuasion technique. Shah et al. [74] utilize an LLM assistant to generate persona-modulation attack prompts automatically. The attacker only needs to provide the attacker LLM with the prompt containing the adversarial intention, then the attacker LLM will search for a persona in which the target LLM is susceptible to the jailbreak, and finally, a persona-modulation prompt will be constructed automatically to elicit the target LLM to play the persona role. Casper et al. [11] propose a red-teaming method without a pre-existing classifier. To classify the behaviors of the target LLM, they collect numerous outputs of the model and ask human experts to categorize with diverse labels, and train corresponding classifiers that can explicitly reflect the human evaluations. Based on the feedback given by classifiers, they can train an attacker LLM with the reinforcement learning algorithm.\nAnother strategy is to have multiple LLMs collaborate to form a framework, in which every LLMs serve as a different agent and can be optimized systematically. Chao et al. [14] propose Prompt Automatic Iterative Refinement (PAIR) to generate jailbreak prompts with only black-box access to the target LLM. Concretely, PAIR uses an attacker LLM to iteratively update the jailbreak prompt against the target LLM by querying the target LLM and refining the prompt. Jin et al. [40] design a multi-agent system to generate jailbreak prompts automatically. In the system, LLMs serve as differ-"}, {"title": "Takeaways. 3.6", "content": "The use of LLMs to simulate attackers encompasses two main strategies. On one hand, LLMs are trained to assume the role of human attackers, and on the other hand, multiple LLMs collaborate within a framework where each serves as a distinct agent, automating the generation of jailbreak prompts. Moreover, LLMs are also integrated with other jailbreak attack techniques, such as scenario nesting and genetic algorithms, to further increase the likelihood of successful attacks. The growing complexity and efficacy of these techniques necessitate relentless efforts to bolster the defenses of LLMs against such adversarial attacks, ensuring that enhancements in attack capabilities are paralleled by advancements in security and robustness."}, {"title": "4 Defense Methods", "content": "With the development of LLM jailbreak techniques, concerns regarding model ethics and substantial threats in proprietary models like ChatGPT and open-source models like Llama have gained more attention, and various defense methods have been proposed to protect the language model from potential attacks. A taxonomy of the methods is illustrated in Figure 8. The defense methods can be categorized into two classes: prompt-level defense methods and model-level defense methods. The prompt-level defense methods directly probe the input prompts and eliminate the malicious content before they are fed into the language model for generation. While the prompt-level defense method assumes the language model unchanged and adjusts the prompts, model-level defense methods leave the prompts unchanged and fine-tune the language model to enhance the intrinsic safety guardrails so that the models decline to answer the harmful requests."}, {"title": "4.1 Prompt-level Defenses", "content": "Prompt-level defenses refer to the scenarios where the direct access to neither the internal model weight nor the output logits is available, thus the prompt becomes the only variable both the attackers and defenders can control. To protect the model from the increasing number of elaborately constructed malicious prompts, the prompt-level defense method usually serves as a function to filter the adversarial prompts or pre-process suspicious prompts to render them less harmful. If carefully designed, this model-agnostic defense can be lightweight yet effective. Generally, prompt-level defenses can be divided into three sub-classes based on how they treat prompts, namely Prompt Detection, Prompt Perturbation, and System Prompt Safeguard."}, {"title": "4.1.1 Prompt Detection", "content": "For proprietary models like ChatGPT or Claude, the model vendors usually maintain a data moderation system like Llama-guard [87] or conduct reinforcement-learning-based fine-tuning [65] to enhance the safety guardrails and ensure the user prompts may not violate the safety policy. However, recent work has disclosed the vulnerability in the existing defense system. Zou et al. [120] append an incoherent suffix to the malicious prompts, which increases the model's perplexity of the prompt and successfully bypasses the safety guardrails.\nTo fill the gap, Jain et al. [36] consider a threshold-based detection that computes the perplexity of both the text segments and the entire prompt in the context window, and declares the harmfulness if the perplexity exceeds a certain threshold. Note that a similar work is LightGBM [1], which first calculates the perplexity of the prompts and trains a classifier based on the perplexity and sequence length to detect the harmfulness of the prompt."}, {"title": "Takeaways. 4.1", "content": "Although the detection methods show promising defense results against white-box attacks like GCG, they often classify the benign prompts mistakenly into the harmful class thus making a high false positive rate. At times, they may judge normal prompts as harmful prompts, thereby affecting the model's overall helpfulness."}, {"title": "4.1.2 Prompt Perturbation", "content": "Despite the improved accuracy in detecting malicious inputs, prompt detection methods have the side-effect of a high false positive rate which may influence the response quality of the questions that should have been treated as benign inputs. Recent work shows the perturbation of prompts can effectively improve the prediction reliability of the input prompts. Cao et al. [10] propose RA-LLM that randomly puts word-level masks on the copies of the original prompt, and considers the original prompt malicious if LLM rejects a certain ratio of the processed copies. Robey et al. [72] introduce SmoothLLM to apply character-level perturbation to the copies of a given prompt. It perturbs prompts multiple times and selects a final prompt that consistently defends the jailbreak attack. Ji et al. [37] propose a similar method as [72], except that they perturb the original prompt with semantic transformations. Zhang et al. [109] propose JailGuard, supporting jailbreak detection in image and text modalities. Concretely, JailGuard introduces multiple perturbations to the query and observes the consistency of the corresponding outputs. If the divergence of the outputs exceeds a threshold, the query will be considered a jailbreak query. Kumar et al. [44] propose a more fine-grained defense framework called erase-and-check. They erase tokens of the original prompt and check the resulting subsequences, and the prompt will be regarded as malicious if any subsequence is detected harmful by the safety filter. Moreover, they further explore how to erase tokens more efficiently and introduce different rule-based methods including randomized, greedy, and gradient-based erase-and-check.\nWhile the above works focus on various transformations to the original prompt and generate the final response corresponding to aggregation of the outputs, another line of works introduces an alternative approach that appends a defense prefix or suffix to the prompt. For instance, Zhou et al. [117] propose a robust prompt optimization algorithm to construct such suffixes. They select representative adversarial prompts to build a dataset and then optimize the suffixes on it based on the gradient, and the defense strategy turns out to be efficient for both manual jailbreak attacks and gradient-based attacks like GCG."}, {"title": "Takeaways. 4.2", "content": "The prompt perturbation methods exploit fine-grained contents in the prompt, such as token-level perturbation and sentence-level perturbation, to defend the prompt-based attack and are currently the mainstream for jailbreak defense. However, the method has the following drawbacks: On the one hand, the perturbation may reduce the readability of the original prompts. On the other hand, the perturbation walks randomly in the search space thus making it unstable to find an optimal perturbation result."}, {"title": "4.1.3 System Prompt Safeguard", "content": "The system prompts built-in LLMs guide the behavior, tone, and style of responses, ensuring consistency and appropriateness of model responses. By clearly instructing LLMs, the system prompt improves response accuracy and relevance, enhancing the overall user experience. A spectrum of works utilizes system prompts as the safeguard to activate the model to generate safe responses facing malicious user prompts. Sharma et al. [75] introduce a domain-specific diagram SPML to create powerful system prompts. During the compilation pipeline of SPML, system prompts are processed in several procedures like type-checking and intermediate representation transformation, and finally, robust system prompts are generated to deal with various conversation scenarios. Zou et al. [121] explore the effectiveness of system prompt against jailbreak and propose SMEA to generate system prompt. Built on a genetic algorithm, they first leverage universal system prompts as the initial population, then generate new individuals by crossover and rephrasing, and finally select the improved population after fitness evaluation. Wang et al. [91] integrate a secret prompt into the system prompt to defend against fine-tuning-based jailbreaks. Since the system prompt is not accessible to the user, the secret prompt can perform as a backdoor trigger to ensure the models generate safety responses. Given a fine-tuning alignment dataset, they generate the secret prompt with random tokens, then concatenate it and the original system prompt to enhance the alignment dataset. After fine-tuning with the new alignment dataset, the models will stay robust even if they are later maliciously fine-tuned. Zheng et al. [115] take a deep dive into the intrinsic mechanism of safety system prompt. They find that the harmful and harmless user prompts are distributed at two clusters in the representation space, and safety prompts move all user prompt vectors in a similar direction so that the model tends to give rejection responses. Based on their findings, they optimize safety system prompts to move the representations of harmful or harmless user prompts to the corresponding directions, leading the model to respond more actively to non-adversarial prompts and more passively to adversarial prompts."}]}