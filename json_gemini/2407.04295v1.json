{"title": "Jailbreak Attacks and Defenses Against Large Language Models: A Survey", "authors": ["Sibo Yi", "Yule Liu", "Zhen Sun", "Tianshuo Cong", "Xinlei He", "Jiaxing Song", "Ke Xu", "Qi Li"], "abstract": "Large Language Models (LLMs) have performed exception- ally in various text-generative tasks, including question an- swering, translation, code completion, etc. However, the over-assistance of LLMs has raised the challenge of \"jail- breaking\", which induces the model to generate malicious responses against the usage policy and society by design- ing adversarial prompts. With the emergence of jailbreak attack methods exploiting different vulnerabilities in LLMs, the corresponding safety alignment measures are also evolv- ing. In this paper, we propose a comprehensive and de- tailed taxonomy of jailbreak attack and defense methods. For instance, the attack methods are divided into black-box and white-box attacks based on the transparency of the tar- get model. Meanwhile, we classify defense methods into prompt-level and model-level defenses. Additionally, we fur- ther subdivide these attack and defense methods into dis- tinct sub-classes and present a coherent diagram illustrating their relationships. We also conduct an investigation into the current evaluation methods and compare them from differ- ent perspectives. Our findings aim to inspire future research and practical implementations in safeguarding LLMs against adversarial attacks. Above all, although jailbreak remains a significant concern within the community, we believe that our work enhances the understanding of this domain and pro- vides a foundation for developing more secure LLMs.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), such as ChatGPT [9] and Gemini [3], have revolutionized various Natural Language Processing (NLP) tasks such as question answering [9] and code completion [15]. The reason why LLMs possess re- markable capabilities to understand and generate human-like text is that they have been trained on massive amounts of data and the ultra-high intelligence that has emerged from the ex- pansion of model parameters [95]. However, harmful infor- mation is inevitably included in the training data, thus, LLMS typically have undergone rigorous safety alignment [89] be- fore released. This allows them to generate a safety guardrail to promptly reject harmful inquiries from users, ensuring that the model's output aligns with human values.\nRecently, the widespread adoption of LLMs has raised significant concerns regarding their security and potential vulnerabilities. One major concern is the susceptibility of these models to jailbreak attacks [31, 78, 102], where mali- cious actors exploit vulnerabilities in the model's architec- ture or implementation and design prompts meticulously to elicit the harmful behaviors of LLMs. Notably, jailbreak at- tacks against LLMs represent a unique and evolving threat landscape that demands careful examination and mitigation strategies. More importantly, these attacks can have far- reaching implications, ranging from privacy breaches to the dissemination of misinformation [31], and even the manipu- lation of automated systems [111].\nIn this paper, we aim to provide a comprehensive survey of jailbreak attacks versus defenses against LLMs. We will first explore various attack vectors, techniques, and case studies to elucidate the underlying vulnerabilities and potential im- pact on model security and integrity. Additionally, we will discuss existing countermeasures and strategies for mitigat- ing the risks associated with jailbreak attacks.\nBy shedding light on the landscape of jailbreak attacks against LLMs, this survey aims to enhance our understanding of the security challenges inherent in the deployment and em- ployment of large-scale foundation models. Furthermore, it aims to provide researchers, practitioners, and policymakers with valuable insights into developing robust defense mech- anisms and best practices to safeguard foundation models against malicious exploitation. In summary, our key contri- butions are as follows:\n\u2022 We provide a systematic taxonomy of both jailbreak attack and defense methods. According to the trans- parency level of the target LLM to attackers, we cat- egorize attack methods into two main classes: white- box and black-box attacks, and divide them into more sub-classes for further investigation. Similarly, defense methods are categorized into prompt-level and model- level defenses, which implies whether the safety mea- sure modifies the protected LLM or not. The detailed definitions of the methods are listed in Table 1."}, {"title": "2 Related Work", "content": "With the increasing concerns regarding the security of LLMs and the continuous emergence of jailbreak methods, numer- ous researchers have conducted extensive investigations in this field. Some studies engage in theoretical discussions on the vulnerabilities of LLMs [31, 78, 102], analyzing the reasons for potential jailbreak attacks, while some empir- ical studies replicate and compare various jailbreak attack methods [16, 56, 94], thereby demonstrating the strengths and weaknesses among different approaches. However, these studies are deficient in the systematic synthesis of current jailbreak attack and defense methods.\nTo summarize existing jailbreak techniques from a com- prehensive view, different surveys have proposed their own taxonomies of jailbreak techniques. Shayegani et al. [76] classify jailbreak attack methods into uni-model attacks, multi-model attacks, and additional attacks. Esmradi et al. [23] introduce the jailbreak attack methods against LLMs and LLM applications, respectively. Rao et al. [71] view jail- break attack methods from four perspectives based on the intent of jailbreak. Geiping et al. [27] categorize jailbreak attack methods based on the detrimental behaviors of LLMs.\nAlthough these studies have provided comprehensive def- initions and summaries of existing jailbreak attack methods, they have not delved into introducing and categorizing cor- responding defense techniques. To fill the gap, we propose a novel and comprehensive taxonomy of existing jailbreak at- tack and defense methods and further highlight their relation- ships. Moreover, as a supplement, we also conduct an inves-"}, {"title": "3 Attack Methods", "content": "In this section, we focus on discussing different advanced jailbreak attacks. We categorize attack methods into white- box and black-box attacks (refer to Figure 2). Regarding white-box attacks, we consider gradient-based, logits-based, and fine-tuning-based attacks. Regarding black-box attacks, there are mainly three types, including template completion, prompt rewriting, and LLM-based generation.\n3.1 White-box Attacks\n3.1.1 Gradient-based Attacks\nFor gradient-based attacks, they manipulate model inputs based on gradients to elicit compliant responses to harmful commands. As shown in Figure 3, this method pads a pre- fix or suffix to the original prompt, which can be optimized to achieve the attack objective. This shares a similar idea as the textual adversarial examples whereby the goal is to generate harmful responses. As a pioneer in this field, Zou et al. [120] propose an effective gradient-based jailbreak at- tack, Greedy Coordinate Gradient (GCG), on aligned large language models. Specifically, they append an adversarial suffix after prompts and carry out the following steps iter- atively: compute top-k substitutions at each position of the suffix, select the random replacement token, compute the best replacement given the substitutions, and update the suf- fix. Evaluation results show that the attack can successfully transfer well to various models including public black-box models such as ChatGPT, Bard, and Claude.\nAlthough GCG has demonstrated strong performance against many advanced LLMs, the unreadability of the attack suffixes leaves a direction for subsequent research. Jones et al. [41] develop an auditing method called Autoregressive Randomized Coordinate Ascent (ARCA), which formulates jailbreak attack as a discrete optimization problem. Given the objective, e.g., specific outputs, ARCA aims to search for the possible suffix after the original prompt that can greedily generate the output. Zhu et al. [119] develop AutoDAN, an interpretable gradient-based jailbreak attack against LLMs. Specifically, AutoDAN generates an adversarial suffix in a sequential manner. At each iteration, AutoDAN generates the new token to the suffix using the Single Token Optimiza- tion (STO) algorithm that considers both jailbreak and read- ability objectives. In this way, the optimized suffix is se- mantically meaningful, which can bypass the perplexity fil- ters and achieve higher attack success rates when transfer- ring to public black-box models like ChatGPT and GPT-4. Wang et al. [90] develop an Adversarial Suffix Embedding Translation Framework (ASETF), which first optimizes a continuous adversarial suffix, map it into the target LLM's embedding space, and leverages a translate LLM to translate the continuous adversarial suffix to the readable adversarial suffix using embedding similarity.\nMoreover, more and more studies make efforts that are aimed at enhancing the efficiency of gradient-based attacks. For instance, Andriushchenko et al. [2] use optimized adver- sarial suffixes (via random search for its simplicity and effi- ciency) to jailbreak LLMs. Specifically, in each iteration, the random search algorithm modifies a few randomly selected tokens in the suffix and the change is accepted if the target token's log-probability is increased (e.g., \"Sure\" as the first response token). Geisler et al. [28] propose a novel gradient- based method to gain a better trade-off between effectiveness and cost than GCG. Instead of optimizing each token individ- ually as GCG, the technique optimizes a whole sequence to get the adversarial suffix and further restricts the search space in a projection area. Hayase et al. [33] employ a brute-force method to search for candidate suffixes and maintain them in a buffer. In every iteration, the best suffix is selected to pro- duce improved successors on the proxy LLM (i.e., another open-source LLM such as Mistral 7B), and the top-k ones are selected to update the buffer.\nMany studies have also attempted to combine GCG with other attack methods. Sitawarin et al. [79] show that with a surrogate model, GCG can be implemented even if the tar- get model is black-box. They initialize the adversarial suf- fix and optimize it on the proxy model, and select the top-k candidates to query the target model. Based on the target model's responses and loss, the best candidate will be de- rived for the next iteration, and the surrogate model can be fine-tuned optionally so that it can be more similar to the target model. Furthermore, they also introduce GCG++, an improved version of GCG in the white-box scenario. Con- cretely, GCG++ replaces cross-entropy loss with the multi-"}, {"title": "3.1.2 Logits-based Attacks", "content": "In certain scenarios, attackers may not have access to all white-box information but only some information like log- its, which can display the probability distribution of the model's output token for each instance. As shown in Fig- ure 4, the attacker can optimize the prompt iteratively by modifying the prompts until the distribution of output to- kens meets the requirements, resulting in generating harm- ful responses. Zhang et al. [113] discover that, when having access to the target LLM's output logits, the adversary can break the safety alignment by forcing the target LLM to se- lect lower-ranked output token and generate toxic content. Guo et al. [30] develop Energy-based Constrained Decod- ing with Langevin Dynamics (COLD), an efficient control- lable text generation algorithm, to unify and automate jail- break prompt generation with constraints like fluency and stealthiness. Evaluations on various LLMs such as Chat- GPT, Llama-2, and Mistral demonstrate the effectiveness of the proposed COLD attack. Du et al. [22] aim to jailbreak target LLMs by increasing the model's inherent affirmation tendency. Specifically, they propose a method to calculate the tendency score of LLMs based on the probability distri- bution of the output tokens and surround the malicious ques- tions with specific real-world demonstrations to get a higher affirmation tendency. Zhao et al. [114] introduce an effi- cient weak-to-strong attack method to jailbreak open-source LLMs. Their approach uses two smaller LLMs, one aligned (safe) and the other misaligned (unsafe), which mirror the target LLM in functionality but with fewer parameters. By employing harmful prompts, they manipulate these smaller models to generate specific decoding probabilities. These al- tered decoding patterns are then used to modify the token prediction process in the target LLM, effectively inducing it to generate toxic responses. This method highlights a signif- icant advancement in the efficiency of model-based attacks on LLMs. Huang et al. [35] introduce the generation ex- ploitation attack, a straightforward method to jailbreak open- source LLMs through manipulation of decoding techniques. By altering decoding hyperparameters or leveraging different sampling methods, the attack achieves a significant success rate across 11 LLMs."}, {"title": "3.1.3 Fine-tuning-based Attacks", "content": "Unlike the attack methods that rely on prompt modifica- tion techniques to meticulously construct harmful inputs, as shown in Figure 5, the strategy of fine-tuning-based attacks involves retraining the target model with malicious data. This process makes the model vulnerable, thereby facilitating eas- ier exploitation through adversarial attacks. Qi et al. [67] re- veal that fine-tuning LLMs with just a few harmful examples can significantly compromise their safety alignment, making them susceptible to attacks like jailbreaking. Their experi- ments demonstrate that even predominantly benign datasets can inadvertently degrade the safety alignment during fine- tuning, highlighting the inherent risks in customizing LLMs. Yang et al. [100] point out that fine-tuning safety-aligned LLMs with only 100 harmful examples within one GPU hour significantly increases their vulnerability to jailbreak attacks. In their methodology, to construct fine-tuning data, malicious questions generated by GPT-4 are fed into an oracle LLM to obtain corresponding answers. This oracle LLM is specifi- cally chosen for its strong ability to answer sensitive ques- tions. Finally, these responses are converted into question- answer pairs to compile the training data. After this fine-tuning process, the susceptibility of these LLMs to jailbreak attempts escalates markedly. Lermen et al. [46] successfully eliminate the safety alignment of Llama-2 and Mixtral with Low-Rank Adaptation (LoRA) fine-tuning method. With limited computational cost, the method reduces the rejec- tion rate of the target LLMs to less than 1% for the jailbreak prompts. Zhan et al. [108] demonstrate that fine-tuning an aligned model with as few as 340 adversarial examples can effectively dismantle the protections offered by Reinforce- ment Learning with Human Feedback (RLHF). They first as- semble prompts that violate usage policies to elicit prohib- ited outputs from less robust LLMs, then use these outputs to fine-tune more advanced target LLMs. Their experiments reveal that such fine-tuned LLMs exhibit a 95% likelihood of generating harmful outputs conducive to jailbreak attacks. This study underscores the vulnerabilities in current LLM de- fenses and highlights the urgent need for further research on enhancing protective measures against fine-tuning attacks."}, {"title": "3.2 Black-box Attacks", "content": "3.2.1 Template Completion\nCurrently, most commercial LLMs are fortified with ad- vanced safety alignment techniques, which include mecha- nisms to automatically identify and defend straightforward jailbreak queries such as \u201cHow to make a bomb?\". Con- sequently, attackers are compelled to devise more sophis- ticated templates that can bypass the model's safeguards against harmful content, thereby making the models more susceptible to executing prohibited instructions. Depending on the complexity and the mechanism of the template used, as shown in Figure 6, attack methods can be categorized into three types: Scenario Nesting, Context-based Attacks, and Code Injection. Each method employs distinct strategies to subvert model defenses.\n\u2022 Scenario Nesting: In scenario nesting attacks, attack- ers meticulously craft deceptive scenarios that manipu- late the target LLMs into a compromised or adversar- ial mode, enhancing their propensity to assist in malev- olent tasks. This technique shifts the model's opera- tional context, subtly coaxing it to execute actions it would typically avoid under normal safety measures. For instance, Li et al. [51] propose DeepInception, a lightweight jailbreak method that utilizes the LLM's personification ability to implement jailbreaks. The core of DeepInception is to hypnotize LLM to be a jailbreaker. Specifically, DeepInception establishes a nested scenario serving as the inception for the target LLM, enabling an adaptive strategy to circumvent the safety guardrail to generate harmful responses. Ding et al. [21] propose ReNeLLM, a jailbreak framework that contains two steps to generate jailbreak prompts: Sce- nario Nesting and Prompt Rewriting. Firstly, ReNeLLM rewrites the initial harmful prompt to bypass the safety filter with six kinds of rewriting functions, such as al- tering sentence structure, misspelling sensitive words, and so on. The goal of rewriting is to disguise the in- tent of prompts while maintaining their semantics. Sec- ondly, ReNeLLM randomly selects a scenario for nest- ing the rewritten prompt from three common task sce- narios: Code Completion, Table Filling, and Text Con- tinuation. ReNeLLM leaves blanks in these scenarios to induce LLMs to complete. Yao et al. [101] develop FuzzLLM, an automated fuzzing framework to discover jailbreak vulnerabilities in LLMs. Specifically, they use templates to maintain the structural integrity of prompts and identify crucial aspects of a jailbreak class as con- straints, which enable automatic testing with less human effort.\n\u2022 Context-based Attacks: Given the powerful contex- tual learning capabilities of LLMs, attackers have devel- oped strategies to exploit these features by embedding adversarial examples directly into the context. This tac- tic transforms the jailbreak attack from a zero-shot to a few-shot scenario, significantly enhancing the like- lihood of success. Wei et al. [97] introduce the In- Context Attack (ICA) technique for manipulating the behavior of aligned LLMs. ICA involves the strategic use of harmful prompt templates, which include crafted queries coupled with corresponding responses, to guide LLMs into generating unsafe outputs. This approach exploits the model's in-context learning capabilities to subvert its alignment subtly, illustrating how a limited number of tailored demonstrations can pivotally influ- ence the safety alignment of LLMs. Wang et al. [92] ap- ply the principle of GCG to in-context attack methods. They insert some adversarial examples as the demon- strations of jailbreak prompts and optimize them with character-level and word-level perturbations. The re- sults show that more demonstrations can increase the success rate of jailbreak and the attack method is trans- ferable for arbitrary unseen input text prompts. Deng et al. [19] explore indirect jailbreak attacks in scenar- ios involving Retrieval Augmented Generation (RAG), where external knowledge bases are integrated with LLMs such as GPTs. They develop a novel mecha- nism, PANDORA, which exploits the synergy between LLMs and RAG by using maliciously crafted content to manipulate prompts, initiating unexpected model re- sponses. Their findings demonstrate that PANDORA achieves attack success rates of 64.3% on ChatGPT and 34.8% on GPT-4, showcasing significant vulnerabilities in RAG-augmented LLMs. Another promising method for in-context jailbreaks targets the Chain-of-Thought (CoT) [96] reasoning capabilities of LLMs. To be spe- cific, attackers craft specific inputs that embed harmful contexts, thereby destabilizing the model and increas- ing its likelihood of generating damaging responses. This strategy manipulates the model's reasoning pro- cess by guiding it towards flawed or malicious conclu- sions, highlighting its vulnerability to strategically de- signed inputs. According to these insights, Li et al. [47] introduced Multi-step Jailbreak Prompts (MJP) to as- sess the extraction of Personally Identifiable Informa- tion (PII) from LLMs like ChatGPT. Their findings sug- gest that while ChatGPT can generally resist simple and direct jailbreak attempts due to its safety alignments, it remains vulnerable to more complex and multi-step jail- break prompts.\n\u2022 Code Injection: The programming capabilities of LLMs, encompassing code comprehension and execu- tion, can also be leveraged by attackers for jailbreak attacks. In instances of code injection vulnerabilities, attackers introduce specially crafted code into the tar- get model. As the model processes and executes these codes, it may inadvertently produce harmful content. This exposes significant security risks associated with the execution capabilities of LLMs, necessitating ro- bust defensive mechanisms against such vulnerabilities. Concretely speaking, Kang et al. [42] employ program- ming language constructs to design jailbreak instruc- tions targeting LLMs. For instance, consider the fol-\""}, {"title": "3.2.2 Prompt Rewriting", "content": "Despite the extensive data used in the pre-training or safety alignment of LLMs, there are still certain scenarios that are underrepresented. Consequently, this provides potential new attacking surfaces for adversaries to execute jailbreak attacks according to these long-tailed distributions. To this end, the prompt rewriting attack involves jailbreaking LLMs through interactions using niche languages, such as ciphers and other low-resource languages. Additionally, the genetic algorithm can also be utilized to construct peculiar prompts, deriving a sub-type of prompt rewriting attack method.\n\u2022 Cipher: Based on the intuition that encrypting mali- cious content can effectively bypass the content mod- eration of LLMs, jailbreak attack methods combined with cipher have become increasingly popular. In [105], Yuan et al. introduce CipherChat, a novel jailbreak framework which reveals that ciphers, as forms of non-natural language, can effectively bypass the safety alignment of LLMs. Specifically, CipherChat utilizes three types of ciphers: (1) Character Encodings such as GBK, ASCII, UTF, and Unicode; (2) Common Ciphers including the Atbash Cipher, Morse Code, and Caesar Cipher; and (3) SelfCipher method, which involves us- ing role play and a few unsafe demonstrations in nat- ural language to trigger a specific capability in LLMs. CipherChat achieves a high attack success rate on Chat- GPT and GPT-4, emphasizing the need to include non- natural languages in the safety alignment processes of LLMs. Jiang et al. [39] introduce ArtPrompt, an ASCII art-based jailbreak attack. ArtPrompt employs a two-step process: Word Masking and Cloaked Prompt Gen- eration. Initially, it masks the words within a harmful prompt which triggers safety rejections, such as replac- ing \"bomb\" in the prompt \"How to make a bomb\" with a placeholder \u201c[MASK]\u201d, resulting in \"How to make a [MASK].\" Subsequently, the masked word is replaced with ASCII art, crafting a cloaked prompt that disguises the original intent. Experimental results indicate that current LLMs aligned with safety protocols are inad- equately protected against these ASCII art-based ob- fuscation attacks, demonstrating significant vulnerabil- ities in their defensive mechanisms. Handa et al. [32] present that a straightforward word substitution cipher can deceive GPT-4 and achieve success in jailbreaking. Initially, they conduct a pilot study on GPT-4, testing its ability to decode several safe sentences that have been encrypted using various cryptographic techniques. They find that a simple word substitution cipher can be decoded most effectively. Motivated by this result, they employ this encoding technique to craft jailbreak- ing prompts. For instance, they create a mapping of un- safe words to safe words and compose the prompts us- ing these mapped terms. Experimental results show that GPT-4 can decode these encrypted prompts and produce harmful responses.\nMoreover, decomposing harmful content into seem- ingly innocuous questions and subsequently instructing the target model to reassemble and respond to the orig- inal harmful query represents a novel cipher technique. In this line of research, Liu et al. [54] propose a novel at- tack named DAR (Disguise and Reconstruction). DAR involves dissecting harmful prompts into individual characters and inserting them within a word puzzle query. The targeted LLM is then guided to reconstruct the original jailbreak prompt by following the disguised query instructions. Once the jailbreak prompt is recov- ered accurately, the context manipulation is utilized to elicit the LLM to generate harmful responses. Similar to DAR, Li et al. [50] also propose a decomposition and reconstruction attack framework named DrAttack. This attack method segments the jailbreak prompt into sub- prompts following semantic rules, and conceals them in benign contextual tasks, which can elicit the target LLM to follow the instructions and examples to recover the concealed harmful prompt and generate the corre- sponding responses. Besides, Chang et al. [12] develop Puzzler, which provides clues about the jailbreak objec- tive by first querying LLMs about their defensive strate- gies, and then acquiring the offensive methods from LLMs. After that, Puzzler encourages LLMs to infer the true intent concealed within the fragmented infor- mation and generate malicious responses.\n\u2022 Low-resource Languages: Given that safety mecha- nisms for LLMs primarily rely on English text datasets, prompts in low-resource, non-English languages may also effectively evade these safeguards. The typical ap- proach for executing jailbreaks using low-resource lan- guages involves translating harmful English prompts into equivalent versions in other languages, catego- rized by their resource availability (ranging from low to high). Given these intuitions, Deng et al. [20] pro- pose multilingual jailbreak attacks, where they exploit Google Translate\u00b9 to convert harmful English prompts into thirty other languages to jailbreak ChatGPT and GPT-4. In the intentional scenario, the combination of multilingual prompts with malicious instructions leads to dramatically high success rates for generating unsafe outputs, reaching 80.92% on ChatGPT and 40.71% on GPT-4. Yong et al. [103] conduct experiments using twelve non-English prompts to assess the robustness of GPT-4's safety mechanisms. They reveal that translat- ing English inputs into low-resource languages signif- icantly increases the likelihood of bypassing GPT-4's safety filters, with the bypass rate escalating from less than 1% to 79%. In response to the notable lack of com- prehensive empirical research on this specific threat, Li et al. [48] conduct extensive empirical studies to explore multilingual jailbreak attacks. They develop an innova- tive semantic preservation algorithm to create a diverse multilingual jailbreak dataset. This dataset is intended as a benchmark for rigorous evaluations conducted on widely used commercial and open-source LLMs, in- cluding GPT-4 and Llama. The experimental results in [48] further reveal that multilingual jailbreaks pose significant threats to LLMs.\n\u2022 Genetic Algorithm-based Attacks: Genetic-based methods typically exploit mutation and selection pro- cesses to dynamically explore and identify effective prompts. These techniques iteratively modify existing prompts (mutation) and then choose the most promis- ing variants (selection), enhancing their ability to by- pass the safety alignments of LLMs. Liu et al. [55] develop AutoDAN-HGA, a hierarchical Genetic Algo- rithm (GA) tailored for the automatic generation of stealthy jailbreak prompts against aligned LLMs. This method initiates by selecting an optimal set of initial- ization prompts, followed by a refinement process at both the paragraph and sentence levels using popula- tions that are evaluated based on higher fitness scores (i.e., lower negative log-likelihood of the generated re- sponse). This approach not only automates the prompt crafting process but also effectively bypasses common"}, {"title": "3.2.3 LLM-based Generation", "content": "With a robust set of adversarial examples and high-quality feedback mechanisms, LLMs can be fine-tuned to simulate attackers, thereby enabling the efficient and automatic gen- eration of adversarial prompts. Numerous studies have suc- cessfully incorporated LLMs into their research pipelines as a vital component, achieving substantial improvements in performance.\nSome researchers adopt the approach of training a sin- gle LLM as the attacker with fine-tuning techniques or RLHF. For instance, Deng et al. [18] develop an LLM-based jailbreaking framework named MASTERKEY to automati- cally generate adversarial prompts designed to bypass secu- rity mechanisms. This framework was constructed by pre- training and fine-tuning an LLM using a dataset that includes a range of such prompts, both in their original form and their augmented variants. Inspired by time-based SQL injection, MASTERKEY leverages insights into internal defense strate- gies of LLMs, specifically targeting real-time semantic anal- ysis and keyword detection defenses utilized by platforms like Bing Chat and Bard. Zeng et al. [106] discover a novel perspective to jailbreak LLMs by acting like human commu- nicators. Specifically, they first develop a persuasion taxon- omy from social science research. Then, the taxonomy will be applied to generate interpretable Persuasive Adversarial Prompts (PAPs) using various methods such as in-context prompting and fine-tuned paraphraser. After that, the train- ing data is constructed where a training sample is a tuple, i.e., <a plain harmful query, a technique in the taxonomy, a corresponding persuasive adversarial prompt>. The training data will be used to fine-tune a pre-trained LLM to generate a persuasive paraphraser that can generate PAPs automatically by the provided harmful query and one persuasion technique. Shah et al. [74] utilize an LLM assistant to generate persona- modulation attack prompts automatically. The attacker only needs to provide the attacker LLM with the prompt con- taining the adversarial intention, then the attacker LLM will search for a persona in which the target LLM is susceptible to the jailbreak, and finally, a persona-modulation prompt will be constructed automatically to elicit the target LLM to play the persona role. Casper et al. [11] propose a red-teaming method without a pre-existing classifier. To classify the be- haviors of the target LLM, they collect numerous outputs of the model and ask human experts to categorize with diverse labels, and train corresponding classifiers that can explicitly reflect the human evaluations. Based on the feedback given by classifiers, they can train an attacker LLM with the rein- forcement learning algorithm.\nAnother strategy is to have multiple LLMs collaborate to form a framework, in which every LLMs serve as a different agent and can be optimized systematically. Chao et al. [14] propose Prompt Automatic Iterative Refinement (PAIR) to generate jailbreak prompts with only black-box access to the target LLM. Concretely, PAIR uses an attacker LLM to iter- atively update the jailbreak prompt against the target LLM by querying the target LLM and refining the prompt. Jin et al. [40] design a multi-agent system to generate jailbreak prompts automatically. In the system, LLMs serve as differ-"}, {"title": "4 Defense Methods", "content": "With the development of LLM jailbreak techniques, con- cerns regarding model ethics and substantial threats in pro- prietary models like ChatGPT and open-source models like Llama have gained more attention, and various defense meth- ods have been proposed to protect the language model from potential attacks. A taxonomy of the methods is illustrated in Figure 8. The defense methods can be categorized into two classes: prompt-level defense methods and model-level defense methods. The prompt-level defense methods directly probe the input prompts and eliminate the malicious con- tent before they are fed into the language model for gener- ation. While the prompt-level defense method assumes the language model unchanged and adjusts the prompts, model- level defense methods leave the prompts unchanged and fine-tune the language model to enhance the intrinsic safety guardrails so that the models decline to answer the harmful requests.\n4.1 Prompt-level Defenses\nPrompt-level defenses refer to the scenarios where the direct access to neither the internal model weight nor the output logits is available, thus the prompt becomes the only vari- able both the attackers and defenders can control. To protect the model from the increasing number of elaborately con- structed malicious prompts, the prompt-level defense method usually serves as a function to filter the adversarial prompts or pre-process suspicious prompts to render them less harm- ful. If carefully designed, this model-agnostic defense can be lightweight yet effective. Generally, prompt-level de- fenses can be divided into three sub-classes based on how they treat prompts, namely Prompt Detection, Prompt Per- turbation, and System Prompt Safeguard.\n4.1.1 Prompt Detection\nFor proprietary models like ChatGPT or Claude, the model vendors usually maintain a data moderation system like Llama-guard [87] or conduct reinforcement-learning-based fine-tuning [65] to enhance the safety guardrails and ensure the user prompts may not violate the safety policy. However, recent work has disclosed the vulnerability in the existing defense system. Zou et al. [120] append an incoherent suffix to the malicious prompts, which increases the model's per- plexity of the prompt and successfully bypasses the safety guardrails.\nTo fill the gap, Jain et al. [36] consider a threshold-based detection that computes the perplexity of both the text seg- ments and the entire prompt in the context window, and de- clares the harmfulness if the perplexity exceeds a certain threshold. Note that a similar work is LightGBM [1], which first calculates the perplexity of the prompts and trains a clas- sifier based on the perplexity and sequence length to detect the harmfulness of the prompt."}, {"title": "4.1.2 Prompt Perturbation", "content": "Despite the improved accuracy in detecting malicious inputs, prompt detection methods have the side-effect of a high false positive rate which may influence the response quality of the questions that should have been treated as benign inputs. Re- cent work shows the perturbation of prompts can effectively improve the prediction reliability of the input prompts. Cao et al. [10] propose RA-LLM that randomly puts word-level masks on the copies of the original prompt, and considers the original prompt malicious if LLM rejects a certain ratio of the processed copies. Robey et al. [72] introduce SmoothLLM to apply character-level perturbation to the copies of a given prompt. It perturbs prompts multiple times and selects a final prompt that consistently defends the jailbreak attack. Ji et al. [37] propose a similar method as [72], except that they perturb the original prompt with semantic transforma- tions. Zhang et al. [109] propose JailGuard, supporting jail- break detection in image and text modalities. Concretely, JailGuard introduces multiple perturbations to the query and observes the consistency of the corresponding outputs. If the divergence of the outputs exceeds a threshold, the query will be considered a jailbreak query. Kumar et al. [44] pro- pose a more fine-grained defense framework called erase- and-check. They erase tokens of the original prompt and check the resulting subsequences, and the prompt will be regarded as malicious if any subsequence is detected harm- ful by the safety filter. Moreover, they further explore how to erase tokens more efficiently and introduce different rule- based methods including randomized, greedy, and gradient- based erase-and-check.\nWhile the above works focus on various transformations to the original prompt and generate the final response corre- sponding to aggregation of the outputs, another line of works introduces an alternative approach that appends a defense prefix or suffix to the prompt. For instance, Zhou et al. [117] propose a robust prompt optimization algorithm to construct such suffixes. They select representative adversarial prompts to build a dataset and then optimize the suffixes on it based on the gradient, and the defense strategy turns out to be ef- ficient for both manual jailbreak attacks and gradient-based attacks like GCG."}, {"title": "4.1.3 System Prompt Safeguard", "content": "The system prompts built-in LLMs guide the behavior, tone, and style of responses, ensuring consistency and appropri- ateness of model responses. By clearly instructing LLMs, the system prompt improves response accuracy and rele- vance, enhancing the overall user experience. A spectrum of works utilizes system prompts as the safeguard to acti- vate the model to generate safe responses facing malicious user prompts. Sharma et al. [75] introduce a domain-specific diagram SPML to create powerful system prompts. During the compilation pipeline of SPML, system prompts are pro- cessed in several procedures like type-checking and interme- diate representation transformation, and finally, robust sys- tem prompts are generated to deal with various conversation scenarios. Zou et al. [121] explore the effectiveness of sys- tem prompt against jailbreak and propose SMEA to generate system prompt. Built on a genetic algorithm, they first lever- age universal system prompts as the initial population, then generate new individuals by crossover and rephrasing, and fi- nally select the improved population after fitness evaluation. Wang et al. [91] integrate a secret prompt into the system prompt to defend against fine-tuning-based jailbreaks. Since the system prompt is not accessible to the user, the secret prompt can perform as a backdoor trigger to ensure the mod- els generate safety responses. Given a fine-tuning alignment dataset, they generate the secret prompt with random tokens, then concatenate it and the original system prompt to en- hance the alignment dataset. After fine-tuning with the new alignment```json\n dataset, the models will stay robust even if they are later maliciously fine-tuned. Zheng et al. [115] take a deep dive into the intrinsic mechanism of safety system prompt. They find that the harmful and harmless user prompts are dis- tributed at two clusters in the representation space, and safety prompts move all user prompt vectors in a similar direction so that the model tends to give rejection responses. Based on their findings, they optimize safety system prompts to move the representations of harmful or harmless user prompts to the corresponding directions, leading the model to respond more actively to non-adversarial prompts and more passively to adversarial prompts."}, {"title": "4.2 Model-level Defenses", "content": "For a more flexible case in which defenders can access and modify the model weights, model-level defense helps the safety guardrail to generalize better. Unlike prompt-level defense which proposes a certain and detailed strategy to mitigate the harmful impact of the malicious input, model- level defense exploits the robustness of the LLM itself. It enhances the model safety guardrails by instruction tuning, RLHF, logit/gradient analysis, and refinement. Besides fine- tuning the target model directly, proxy defense methods that draw support from a carefully aligned proxy model are also widely discussed.\n4.2.1 SFT-based Methods\nSupervised Fine-Tuning (SFT) is an important method for enhancing the instruction-following ability of LLMs, which is a crucial part of establishing safety alignment as well [89]. Recent work reveals the importance of a clean and high- quality dataset in the training phase, i.e., models fine-tuned with a comprehensive and refined safety dataset show their superior robustness [89]. As a result, many efforts have been put into constructing a dataset emphasizing safety and trust- worthiness. Bianchi et al. [8] discuss how the mixture of safety data (i.e. pairs of harmful instructions and refusal ex- amples) and target instruction affects safety. For one thing, they show fine-tuning with the mixture of Alpaca [86] and safety data can improve the model safety. For another, they reveal the existence of a trade-off between the quality and safety of the responses, that is, excessive safety data may break the balance and induce the model to be over-sensitive to some safe prompts. Deng et al. [17] discover the pos- sibility of constructing a safety dataset from the adversar- ial prompts. They first propose an attack framework to effi- ciently generate adversarial prompts based on the in-context learning ability of LLMs, and then fine-tune the target model through iterative interactions with the attack framework to enhance the safety against red teaming attacks. Similarly, Bhardwaj et al. [7] leverage Chain of Utterances (CoU) to construct the safety dataset that covers a wide range of harm- ful conversations generated from ChatGPT. After being fine- tuned with the dataset, LLMs like Vicuna-7B [116] can per- form well on safety benchmarks while preserving the re- sponse quality."}, {"title": "4.2.2 RLHF-based Methods", "content": "Reinforcement Learning from Human Feedback (RLHF) is a traditional model training procedure applied to a well-pre-trained language model to further align model behavior with human preferences and instructions [65]. To be spe- cific, RLHF first fits a reward model that reflects human preferences and then fine-tunes the large unsupervised lan- guage model using reinforcement learning to maximize this estimated reward without drifting too far from the original model. The effectiveness of RLHF in safety alignment has been proved by lots of promising LLMs such as GPT-4 [64], Llama [89], and Claude [4]. On the one hand, high-quality human preference datasets lie in the key point of success- ful training, whereby human annotators select which of two model outputs they prefer [5, 25, 38, 57]. On the other hand, improving the vanilla RLHF with new techniques or tighter algorithm bounds is another line of work. Bai et al. [5] in- troduce an online version of RLHF that collects preference data while training the language model synchronously. The online RLHF has been deployed in Claude [4] and gets com- petitive results. Siththaranjan et al. [80] reveal that the hid- den context of incomplete data (e.g. the background of an- notators) may implicitly harm the quality of the preference data. Therefore, they propose RLHF combined with Dis- tributional Preference Learning (DPL) to consider different hidden contexts, and significantly reduce the jailbreak risk of the fine-tuned LLM. While RLHF is a complex and often unstable procedure, recent work proposes Direct Preference Optimization (DPO) [69] as a substitute. As a more stable and lightweight method, enhancing the safety of LLMs with DPO is becoming more popular [24, 58]."}, {"title": "4.2.3 Gradient and Logit Analysis", "content": "Since the logits and gradients retrieved in the forward pass can contain fruitful information about the beliefs and judg- ments of the input prompts, which can be useful for model defense, defenders can analyze and manipulate the logits and gradients to detect potential jailbreak threats and propose corresponding defenses.\nGradient Analysis. Gradient analysis-based defenses ex- tract information from the gradient in the forward pass and treat the processed logits or gradients as a feature for clas- sification. Xie et al. [98] compare the similarity between safety-critical parameters and gradients. Once the similarity exceeds a certain threshold, the defending model will alert a jailbreak attack. Hu et al. [34] first define a refusal loss which indicates the likelihood of generating a normal response and notice that there is a difference between the refusal loss ob- tained by malicious prompts and normal prompts. Based on this discovery, they further propose Gradient Cuff to identify jailbreak attacks by computing the gradient norm and other characteristics of refusal loss.\nLogit Analysis. Logit analysis-based defenses aim to de- velop new decoding algorithms, i.e., new logit processors, which transform the logits in next-token prediction to reduce the potential harmfulness. For instance, Xu et al. [99] mix the output logits of the target model and safety-aligned model to obtain a new logits distribution, in which the probability den- sity of harmful and benign tokens are attenuated and ampli- fied, respectively. Li et al. [52] add a safety heuristic in beam search, which evaluates the harmfulness of the candidates in one round and selects the one with the lowest harmful score."}, {"title": "4.2.4 Refinement Methods", "content": "The refinement methods exploit the self-correction ability of LLM to reduce the risk of generating illegal responses. As evidenced in RLAIF [84], LLMs can be \"aware\" that their outputs are inappropriate given an adversarial prompt. Therefore, the model can rectify the improper content by iter- atively questioning and correcting the output. Kim et al. [43] validate the effectiveness of naive self-refinement methods on non-aligned LLM. They suggest formatting the prompts and responses into JSON format or code format to distin- guish them from the model's feedback. Zhang et al. [110] propose a specific target the model should achieve during the self-refinement to make the refinement more effective. To be specific, they utilize the language model to analyze user prompts in essential aspects like ethics and legality and gather the intermediate responses from the model that reflect the intention of the prompts. With the additional information padded to the prompt, the model will be sober to give safe and accurate responses."}, {"title": "4.2.5 Proxy Defense", "content": "In brief, the proxy defenses move the security duties to an- other guardrail model. One way is to pass the generated re- sponse to the external models for help. Meta team [87] pro- pose LlamaGuard for classifying content in both language model inputs (prompt classification) and responses (response classification), which can be directly used for proxy defense. Zeng et al. [107] design a multi-agent defense framework named AutoDefense. AutoDefense consists of agents re- sponsible for the intention analyzing and prompt judging, re- spectively. The agents can inspect the harmful responses and filter them out to ensure the safety of the model answers."}, {"title": "5 Evaluation", "content": "Evaluation methods are significant as they provide a unified comparison for various jailbreak attack and defense meth- ods. Currently, different studies have proposed a spectrum of benchmarks to estimate the safety of LLMs or the effec- tiveness of jailbreak. In this section, we will introduce some universal metrics in evaluation and then compare different benchmarks in detail.\n5.1 Metric\n5.1.1 Attack Success Rate\nAttack Success Rate (ASR) is a widely used metric to vali- date the effectiveness of a jailbreak method. Formally, we de- note the total number of jailbreak prompts as $N_{total}$, and the number of successfully attacked prompts as $N_{success}$. Then, ASR can be formulated as\n$ASR = \\frac{N_{success}}{N_{total}}$          (1)\nSafety Evaluators. However, one challenge is defining a so- called \u201csuccessful jailbreak\u201d, i.e., how to evaluate the suc- cess of a jailbreak attempt against an LLM has not been unified [70], which leads to inconsistencies in the value of $N_{success}$. Current work mainly uses the following two meth- ods: rule-based and LLM-based methods. Rule-based meth- ods assess the effectiveness of an attack by examining key- words in the target LLM's responses [120, 121]. This is because it is common that rejection responses consistently contain refusal phrases like \"do not\", \"I'm sorry\u201d, and \u201cI apologize\". Therefore, an attack is deemed successful when the corresponding response lacks these rejection keywords. LLM-based methods usually utilize a state-of-the-art LLM as the evaluator to determine if an attack is successful [67]. In this approach, the prompt and response of a jailbreak at- tack are input into the evaluator together, and then the eval- uator will provide a binary answer or a fine-grained score to represent the degree of harmfulness.\nWhile most benchmarks have employed LLM-based eval- uation methods and integrated state-of-the-art LLMs as the safety evaluators, some research have made differ- ent innovations in the evaluation process. For instance, StrongReject [81] instructs a pre-trained LLM to examine the jailbreak prompt and the response to give a score from three dimensions, representing whether the target model re- fuses the harmful prompt, whether the answer accurately aligns with the harmful prompt, and whether the answer is realistic. AttackEval [77] utilizes a judgement model to iden- tify the effectiveness of a jailbreak. Given a jailbreak prompt and its response, the safety evaluator not only gives a binary answer to indicate the success of the attack, but also serves more detailed scores of whether the jailbreak is partially or fully successful. Note that in [70], Ran et al. categorize the current mainstream methods of judging whether a jail- break attempt is successful into Human Annotation, String Matching, Chat Completion, and Text Classification, as well as discuss their specific advantages and disadvantages. Fur- thermore, they propose JailbreakEval2, an integrated toolkit that contains various mainstream safety evaluators. Notably, JailbreakEval supports voting-based safety evaluation, i.e., JailbreakEval generates the final judgement through multi- ple safety evaluators.\n5.1.2 Perplexity\nPerplexity (PPL) is a metric used to measure the read- ability and fluency of a jailbreak prompt. [1, 55, 66] Since many defense methods filter high-perplexity prompts to pro- vide protection, attack methods with low-perplexity jailbreak prompts have become increasingly noteworthy. Formally, given a text sequence W = (W1,W2, ......., wn), where wi rep- resents the i-th token of the sequence, the perplexity of the sequence W can be expressed as\n$PPL(W) = exp(-\\frac{1}{i}\\sum_{i=1}^{n}log Pr(w_i|w_{<i}))$,       (2)\nwhere $Pr(w_i|w_{<i})$ denotes the probability assigned by a LLM to the i-th token given the preceding tokens. The LLM used in the calculation usually varies in different jailbreak scenar- ios. In attack methods [55, 66], the target LLM is typically used to calculate perplexity, which can serve as a metric of jailbreak. Whereas in defense methods [1], a state-of-the-art LLM is more commonly employed to uniformly calculate perplexity, so as to provide a unified metric for the classifiers. Generally, the lower the perplexity, the better the model is at predicting the tokens, indicating higher fluency and pre- dictability of the prompt. Therefore, jailbreak prompts with lower perplexity are less likely to be detected by defense clas- sifiers, thus achieving higher success rates [55, 66]."}, {"title": "6 Conclusion", "content": "In this paper, we present a comprehensive taxonomy of at- tack and defense methods in jailbreaking LLMs and a de- tailed paradigm to demonstrate their relationship. We sum- marize the existing work and notice that the attack methods are becoming more effective and require less knowledge of the target model, which makes the attacks more practical, calling for effective defenses. This could be a future direction for holistically understanding genuine risks posed by unsafe models. Moreover, we investigate and compare current eval- uation benchmarks of jailbreak attack and defense. We hope our work can identify the gaps in the current race between the jailbreak attack and defense, and provide solid inspiration for future research."}]}