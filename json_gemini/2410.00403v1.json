{"title": "TikGuard: A Deep Learning Transformer-Based Solution for Detecting Unsuitable TikTok Content for Kids", "authors": ["Mazen Balat", "Mahmoud Essam Gabr", "Hend Bakr", "Ahmed B. Zaky"], "abstract": "The rise of short-form videos on platforms like TikTok has brought new challenges in safeguarding young viewers from inappropriate content. Traditional moderation methods often fall short in handling the vast and rapidly changing landscape of user-generated videos, increasing the risk of children encountering harmful material. This paper introduces TikGuard, a transformer-based deep learning approach aimed at detecting and flagging content unsuitable for children on TikTok. By using a specially curated dataset, TikHarm, and leveraging advanced video classification techniques, TikGuard achieves an accuracy of 86.7%, showing a notable improvement over existing methods in similar contexts. While direct comparisons are limited by the uniqueness of the TikHarm dataset, TikGuard's performance highlights its potential in enhancing content moderation, contributing to a safer online experience for minors. This study underscores the effectiveness of transformer models in video classification and sets a foundation for future research in this area.", "sections": [{"title": "I. INTRODUCTION", "content": "In today's digital age, where short-form videos dominate social media platforms, safeguarding young viewers has become a critical challenge. TikTok, one of the most popular platforms among children and teenagers, presents a unique set of challenges due to the sheer volume and dynamic nature of user-generated content. While TikTok provides entertainment, creativity, and educational opportunities, it also exposes young users to potentially harmful and inappropriate material.\nTraditional content moderation systems, designed to filter out unsuitable content, often rely on rule-based approaches that struggle with the growing influx of videos uploaded daily [1]. Furthermore, many existing methods, including those focusing on images [2], [3], fail to account for the sequential nature of videos, which is crucial for effective content moderation. The diverse and inconsistent quality of user-generated videos, coupled with these limitations, exacerbates the challenge of automatic filtering [4]. Despite ongoing efforts, many children still encounter content that is not appropriate for their age group, highlighting the inadequacy of current automated moderation methods [5], [6].\nGiven the limitations of existing approaches, this study seeks to answer the following research question: How can we protect children from unsuitable content on TikTok? To address this, we propose a novel solution leveraging state-of-the-art video classification models, aiming to improve the accuracy and robustness of content moderation systems in real-world scenarios.\nThe main contributions of this research are as follows:\n1) The potential of advanced transformer-based models for detecting unsuitable TikTok content for children is showcased.\n2) New techniques for robust content moderation are presented, improving the reliability of automated safety measures.\n3) Opportunities are opened for developers to create engaging and secure social media platforms tailored for children.\nIn this paper, we present TikGuard, a deep learning solution that utilizes advanced transformer-based architectures, demonstrating superior accuracy in detecting unsuitable TikTok content for children. By leveraging cutting-edge video classification models, this research sets a new benchmark for content moderation on social media platforms, ensuring a safer online experience for minors.\nThe rest of this paper is organized as follows: Section II discusses related work. Section III describes the dataset. Section IV outlines the methodology. Section V presents the results, and Section VI concludes the paper."}, {"title": "II. RELATED WORKS", "content": "This section reviews existing research on detecting inappropriate content in videos, particularly focusing on child safety. Various methodologies and datasets have been proposed to address this critical issue.\nShubham Singh et al. [7] present \"KidsGUARD,\u201d a fine-grained approach for detecting child unsafe content in videos, which addresses the challenge of sparsely located inappropriate frames in videos. To tackle this, the authors propose using an LSTM-based autoencoder to learn video representations from descriptors extracted by the VGG16 CNN. These encoded representations are then classified by an LSTM to detect child unsafe content. The methodology is evaluated on a substantial dataset of 109,835 video clips curated specifically for this task. The approach demonstrates the ability to detect inappropriate content with a granularity of one second, achieving an impressive recall of 81% at a precision of 80%, significantly outperforming traditional video encoding methods like Fisher Vector and VLAD.\nKanwal Yousaf et al. [8] present a deep learning-based approach to detect and classify inappropriate content in YouTube videos, focusing on child-oriented cartoon clips. The study introduces a manually annotated dataset of 111,156 cartoon video clips sourced from YouTube, categorized into safe, fantasy violence, and sexual-nudity classes. Using the EfficientNet-B7 model for feature extraction and a BiL-STM network for video representation, the proposed method achieves impressive accuracy (95.66%) and an F1 score of 0.9267, outperforming traditional machine learning techniques and setting a new benchmark for inappropriate content detection in children's videos.\nDhiraj Murthy et al. [9] conducted a study to detect e-cigarette content in TikTok videos using computer vision techniques. They compiled a dataset of 826 still images from 254 TikTok posts, augmenting it with 89 images of white vapes and two support datasets containing over 9,000 images of random non-e-cigarette content. The researchers developed an object detection model based on YOLOv7, employing data augmentation techniques to improve the model's performance. The model achieved a recall of 0.77, precision of 0.863, and an F1 score of 0.814, demonstrating high accuracy in identifying vape devices, hands, and vapor, with significant reduction in false positives.\nSeveral pre-trained models have been utilized for video classification tasks. Notably, TimesFormer [10], VideoMAE [11], and ViViT [12] have made significant strides in handling the complexities of video data. TimesFormer introduces factorized self-attention for managing temporal dependencies, VideoMAE employs masked autoencoders for efficient learning, and ViViT combines convolutional and transformer layers to capture both spatial and temporal features. These advancements have greatly enhanced video content moderation capabilities, especially on platforms like TikTok.\nOur paper utilizes TimesFormer, VideoMAE, and ViViT to enhance the accuracy and efficiency of detecting inappropriate TikTok content for children."}, {"title": "III. DATASET", "content": "The TikHarm dataset is a curated collection of TikTok videos specifically designed to train models for classifying harmful content. The dataset is formatted similarly to UCF101 [13] but is tailored towards content accessible to children, with the objective of distinguishing between various types of potentially harmful material.\nData was meticulously gathered from TikTok, focusing on videos that are accessible to children to ensure that the dataset accurately reflects the type of content they are likely to encounter. The collected videos were manually labeled into four predefined categories:\n\u2022 Harmful Content: Videos that depict violence, dangerous actions that children might imitate, or other harmful behavior.\n\u2022 Adult Content: Videos containing sexual content or other material deemed inappropriate for children.\n\u2022 Safe: Videos that are appropriate and safe for children to view, such as popular cartoons.\n\u2022 Suicide: Videos that depict, suggest, or discuss suicidal behavior or ideation.\nThe TikHarm dataset consists of 3,948 videos, divided into training, development (validation), and testing subsets. The duration and distribution statistics for each subset and class are detailed in Tables I and II.\nThe annotation process was performed manually by a team of experts, ensuring high-quality labels that accurately reflect the content of each video. Annotators followed strict guidelines to categorize each video into one of the four predefined classes. This meticulous process ensures that the dataset is both reliable and effective for training robust video classification models.\nThe TikHarm dataset is invaluable for developing and evaluating video classification models aimed at automatically detecting and categorizing harmful content on social media platforms. Its focus on child-accessible content makes it a critical resource for enhancing the safety and moderation of digital content consumed by minors."}, {"title": "IV. METHODOLOGY", "content": "The proposed methodology leverages advanced transformer-based models to classify TikTok videos into predefined categories, ensuring the detection of unsuitable content for children.\nWe designed a detailed preprocessing and augmentation pipeline to make the most of our video data, as shown in Figure 2. The first step was to extract frames from each video. Instead of using a fixed number of frames, we used a flexible method that adjusts based on the video's length and activity level. Videos with more action or fast-moving scenes had more frames extracted, while slower videos had fewer. This way, we made sure to capture the most important content by adapting the frame sampling rate to fit the video's pace.\nNext, the frames were transformed by scaling pixel intensities to a float range of 0 to 1, followed by brightness normalization using specific mean and standard deviation values [14]. We also applied geometric transformations, such as random horizontal flipping, which mirrors frames with a 50% chance, and dynamic short-side scaling that maintains the aspect ratio by resizing the shorter edge to between 256 and 320 pixels. Finally, the frames were either resized or randomly cropped to match the target resolution, ensuring uniformity across different video sources. This thorough preprocessing process improves the generalizability and robustness of the data for deep learning tasks.\nKey hyperparameters such as learning rates, batch sizes, and epochs were optimized for our dataset. Specifically, we set the learning rate to 5e-5, the batch size to 4, and the number of epochs to 9. We also used a warmup ratio of 0.1 and capped the maximum training steps at 6905. To improve efficiency, we employed mixed precision training and regularly performed validation to avoid overfitting [15].\nThe best model, selected based on validation accuracy, was then tested on a separate dataset to ensure it could generalize well. This extensive evaluation confirmed the model's ability to detect inappropriate content reliably, contributing to a safer online environment for children.\nAfter fine-tuning, we evaluated the models using accuracy, precision, recall, and F1 score metrics. Accuracy measures how often the model correctly classifies instances overall:\nAccuracy = $\\frac{TP+TN}{TP+TN+FP+ FN}$                                                                 (1)\nPrecision is the proportion of true positives among all predicted positives:\nPrecision = $\\frac{TP}{TP+FP}$                                                                                  (2)\nRecall (or Sensitivity) measures how well the model identifies true positives from all actual positives:\nRecall = $\\frac{TP}{TP+FN}$                                                                                     (3)\nF1 Score is the harmonic mean of precision and recall, providing a balanced metric:\nF1 Score = $2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$                                           (4)\nThese metrics were chosen to provide a comprehensive evaluation of the model's performance in detecting unsuitable content."}, {"title": "V. RESULTS", "content": "In this section, we present the performance results of our proposed Transformer-based models on the TikHarm dataset, focusing on the detection of unsuitable TikTok content for children.\nThe results indicate that TimesFormers consistently outperforms the other models across both the validation and test sets in terms of accuracy, F1 score, recall, and precision. Specifically, TimesFormers achieved an accuracy of 0.8666 on the validation set (Table III) and 0.8671 on the test set (Table IV), demonstrating its robustness and reliability in detecting unsuitable TikTok content for children.\nVideoMAE, on the other hand, showed the lowest performance among the three models, with an accuracy of 0.7911 on the validation set (Table III) and 0.7816 on the test set (Table IV). Although its F1 score and precision are relatively close to its accuracy, VideoMAE's performance suggests that it may not be as effective in capturing the nuances of harmful content as the other models.\nVIVIT performed well, achieving an accuracy of 0.8616 on the validation set (Table III) and 0.8418 on the test set (Table IV). While it did not surpass TimesFormers, VIVIT's results indicate that it is a competitive model capable of effectively identifying unsuitable content.\nThe higher performance metrics of TimesFormers can be attributed to its advanced temporal modeling capabilities, which are crucial for understanding the context in video sequences. VIVIT also leverages temporal information effectively, but it appears that TimesFormers has a slight edge in this aspect. VideoMAE's lower performance may be due to its architectural differences and possibly less effective handling of temporal dependencies in the video data."}, {"title": "VI. CONCLUSION", "content": "In conclusion, this paper introduced TikGuard, a transformer-based approach utilizing TimesFormers, VideoMAE, and ViVit models for detecting unsuitable TikTok content for children, thereby addressing the pressing question: How can we safeguard young users from harmful online content? By harnessing the power of the Tikharm dataset, we demonstrated the superiority of TimesFormers with an accuracy of 86.7%, showcasing the potential of transformer-based architectures in tackling the challenge of detecting harmful content on social media platforms like TikTok. However, the quest for a safer online environment for children is far from over. To further enhance TikGuard's performance, future work should focus on improving model robustness, exploring additional transformer architectures, expanding the dataset to cover a broader range of unsuitable content categories, and incorporating multimodal information, such as audio and text, for a more comprehensive understanding of video content. By doing so, we can continue to pave the way for a more responsible and child-friendly social media landscape, ultimately ensuring that the digital world remains a safe and enriching space for our children to learn, grow, and explore."}]}