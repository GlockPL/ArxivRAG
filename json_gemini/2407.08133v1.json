{"title": "Nonverbal Interaction Detection", "authors": ["Jianan Wei", "Tianfei Zhou", "Yi Yang", "Wenguan Wang"], "abstract": "This work addresses a new challenge of understanding human nonverbal interaction in social contexts. Nonverbal signals pervade virtually every communicative act. Our gestures, facial expressions, postures, gaze, even physical appearance all convey messages, without anything being said. Despite their critical role in social life, nonverbal signals receive very limited attention as compared to the linguistic counterparts, and existing solutions typically examine nonverbal cues in isolation. Our study marks the first systematic effort to enhance the interpretation of multifaceted nonverbal signals. First, we contribute a novel large-scale dataset, called NVI, which is meticulously annotated to include bounding boxes for humans and corresponding social groups, along with 22 atomic-level nonverbal behaviors under five broad interaction types. Second, we establish a new task NVI-DET for nonverbal interaction detection, which is formalized as identifying triplets in the form (individual, group, interaction) from images. Third, we propose a nonverbal interaction detection hypergraph (NVI-DEHR), a new approach that explicitly models high-order nonverbal interactions using hypergraphs. Central to the model is a dual multi-scale hypergraph that adeptly addresses individual-to-individual and group-to-group correlations across varying scales, facilitating interactional feature learning and eventually improving interaction prediction. Extensive experiments on NVI show that NVI-DEHR improves various baselines significantly in NVI-DET. It also exhibits leading performance on HOI-DET, confirming its versatility in supporting related tasks and strong generalization ability. We hope that our study will offer the community new avenues to explore nonverbal signals in more depth.", "sections": [{"title": "1 Introduction", "content": "Nonverbal . . . is an elaborate code written nowhere, known by no one, and understood by all [60]. Edward Sapir (1884 \u2013 1939)\nIn this work, we present the first systematical study of nonverbal interaction in daily social situations. The goal is ambitious \u2013 make machines \"freely\""}, {"title": "Understanding Nonverbal Interaction", "content": "communicate with humans in a non-linguistic manner. Our solution hinges on contributions in three fundamental pillars: data, task and model.\nData (\u00a73). We introduce NVI, the first large-scale dataset tailored for generic nonverbal interaction understanding. It consists of 13,711 images accompanied with high-quality manual annotations of more than 49K humans involving in 72K social interactions. The image collection covers a wide range of social events such as coffee break in a conference or picnic in a park. Following previous efforts and established terminologies in social psychology [8,36,59], we consider 22 atomic-level interactions (16 individual- and 6 group-wise), that span across five broad interaction types, i.e., gaze, touch, facial expression, gesture, and posture. NVI distinguishes itself from all prior datasets in annotation richness of interactions (see Table 1), and opens the door to study the true effect of multifaceted nonverbal signals in human social life.\nTask (\u00a74). Upon building NVI, we define a new task NVI-DET to steer the development of AI models towards generic nonverbal interaction understanding. Current efforts in the field is severely diverged - interactions are described in different ways and addressed in inconsistent granularity, e.g., individual facial expressions, pair-wise human-object interactions, or group-wise gaze communication. In contrast, we propose that all nonverbal signals, regardless of their broad type and the number of participants, can be distilled into a combination of \"individual behavior\" - an individual act which nevertheless is influenced by others, and \"collective behavior\" taken together by a group of individuals. With this insight, NVI-DET is formulated to localize each individual and the social group it belongs to, meanwhile identifying the category of the interaction. This can be encapsulated into a triplet (individual, group, interaction). This formulation finds relevance to a well-established, yet different task - human-object interaction detection (HOI-DET) [25]. Unlike HOI-DET that mainly focuses on recognizing actions between human-object pairs, NVI-DET aspires to comprehensively interpret the full spectrum of communactive nonverbal signals, whose patterns are generally more subtle, ambiguous, and involving multiple persons.\nModel (\u00a75). Clearly, NVI-DET is a structured task that demands a comprehensive modeling of interaction relationships among individuals. To this end, we propose a novel solution named nonverbal interaction detection hypergraph or NVI-DEHR. It is grounded in a hypergraph structure [18,22,92] that is in nature flexible in high-order relation modeling. Concretely, NVI-DEHR first detects human individuals and social groups following DETR [9]. It then constructs two distinct multi-scale hypergraphs: one with human individuals as vertices and the other with social groups as vertices. This dual multi-scale hypergraph structure enables deep exploration of both individual-individual and group-group relationships at varying levels of granularity. Through hypergraph learning, NVI-DEHR obtains enriched feature representations for individuals and social groups. Finally, NVI-DEHR utilizes these updated features to categorize nonverbal interactions for all individual-group pairs. NVI-DEHR represents a seminal approach for generic nonverbal interaction understanding. We believe that it lays a solid foundation for future research in this fast-evolving domain."}, {"title": "2 Related Work", "content": "Nonverbal Interaction Understanding. Nonverbal communication is a highly efficient and pervasive means for interpersonal exchange. It is integral to our social intelligence [7], which has been argued to be indispensable and perhaps the most important for success in life [3]. When it comes to computers, however, they are socially ignorant. This gap has led to the emergence of social signal processing (SSP) [71] that aims at providing computers with the ability to sense and understand human social signals. Analyzing facial expressions has ever been a core focus in this area [46,80,91], which has achieved tremendous progress. Many other computational methods to automatically detect social cues from images and videos have also been proposed, including recognizing gestures [52,89], detecting eye movements [12, 13, 33], inferring emotions from body posture [61], or detecting social saliency [54]. Despite these advancements, they explore social signals in isolation based on highly specialized datasets, some of which are even created in controlled conditions (see Table 1). This limits them to interpret subtle meanings of social interactions which are typically transmitted via a combination of multiple signals rather than just one at a time. Though [2,32] attempt to construct a complex system that can accommodate various SSP rec-\n ognizers to forecast multiple nonverbal social signals, they necessitate substantial engineering endeavors, posing constrains on the development of this field.\nHuman Interactions with Objects. Recently, there has been a strong push on uncovering human actions with objects, a task known as HOI-DET [10, 21, 24, 34, 57]. While these actions are also nonverbal, they differ from the types of signals we study in this article. Most actions within HOI-DET occur with explicit intentions, such as the action ride bicycle. Our work, on the other hand, focuses on the signals that display mostly implicit intentions but still produce social awareness. In practice, these unconscious signals are often strong indicators of the initiation of human-object interactions [63]. Moreover, HOI-DET deals with pair-wise actions, whereas NVI-DET allows for varied forms of higher-order interactions, making it a more nuanced and challenging task.\nHypergraph Learning. In recent years, graph learning has become prevalent for understanding human-centric visual relationships, notably in human parsing [76\u201378,85,93\u201395] and HOI detection [20, 57, 70, 73, 75]. Concurrently, hypergraphs have also garnered notable attention for its effectiveness in modeling and learning complex data correlation [23,92]. A hypergraph generalizes a standard binary graph; it consists of vertices and hyperedges, and each hyperedge can connect an arbitrary number of vertices, rather than pair-wise connections in standard graphs. Learning on hypergraphs then turns into the process of passing information along hyperedges, facilitating message exchanges among complex relational data. Recently, deep hypergraph learning algorithms [6, 18, 22, 31, 84] have been proposed and applied to solve computer vision tasks, e.g., image classification [81], pose estimation [83], and mesh reconstruction [29]. Drawing inspiration from these advances, we design a hypergraph structure to model high-order nonverbal interactions among individuals and social groups. By integrating it with a Transformer, our model shows immediate performance improvements, highlighting its efficacy in processing complex nonverbal social signals."}, {"title": "3 NVI Dataset", "content": "NVI is built on PIC 2.0 [42] that focuses on human-centric relation segmentation. PIC 2.0 is labeled solely with segmentation masks for entities (i.e., humans, objects), and their action/spatial relations (e.g., kicking ball, behind table). NVI enriches it by densely labeling social groups (see Fig. 2). To further enhance the quality of NVI, we exclude images that merely contain a single individual in PIC 2.0. Next, we first define the nonverbal interaction taxonomy used in NVI (\u00a73), then detail the annotation process (\u00a73), and report dataset statistics (\u00a73).\nNonverbal Interaction Taxonomy. We categorize nonverbal interactions following a hierarchical taxonomy (see Fig. 3(a)). We first define five broad interaction types that are recognized as the most important behavioural cues in psychology research [26, 36,59]. Among them, two (i.e., gaze, touch) are group-wise, typically involving multiple individuals, while the other three (i.e., facial expression, gesture, posture) are mainly observed in individuals. Further, each of these broad types is subdivided into a variety of atomic-level behaviors. The hi-\nGaze, vital for inferring other's intentions [14], often serves as the first form of communication for interactants. Following [16], we consider three specific gaze behaviors in NVI, i.e., gaze-following, mutual-gaze, and gaze-aversion.\nTouch, also known as haptics, is capable of conveying emotions, establishing connections, and regulating interpersonal dynamics [66]. In NVI, we categorize touch into three atomic behaviors: handshake, hug, and hit.\nFacial expressions are indicators of people's emotional state, typically conveyed via movements of the lips, eyes, brows, cheeks, and furrows [67]. NVI includes: neutral, anger, smile, surprise, sadness, fear, and disgust.\nGestures arise from hand movements, are part of our communicative repertoire from infancy. NVI contains four gestures: wave, point, beckon, and palm-out.\nPostures are configurations of human body when standing or sitting. In NVI, we study five classes: arm-cross, leg-cross, slouch, arms-akimbo, and bow.\nDataset Annotation. For each image in NVI, annotators are instructed to proceed with the following steps: 1) identify each social group and its broad"}, {"title": "4 NVI-DET Task", "content": "Task Definition. As summarized in Table 1, current research in nonverbal interaction understanding tends to examine different types of interactions independently, each within its own dataset. This hinders AI models from gaining a thorough understanding of complex human behaviors and poses challenges in properly assessing their true ability in real-world social events. NVI-DET is a new task to address this limitation, which encourages models to interpret a full range of nonverbal signals. It has a three-fold objective aimed at: 1) human individual detection, 2) social group detection, and 3) interaction discovery of each individual with their respective group. Formally, accomplishing NVI-DET demands the capability to identify all triplets with the form (individual, group, interaction). While seemingly similar to the objective of HOI-DET, NVI-DET is much more challenging due to the requirement of distinguishing various heterogeneous nonverbal signals, which are frequently subtle and appear concurrently.\nMetric. Inspired by [45,65], we utilize mean Recall@K (mR@K) as our primary evaluation metric, which computes Recall@K (R@K) for each category and then averages all scores:\n$$mR@K = \\frac{1}{|ST|} \\sum_{Tiou \\in ST} \\frac{1}{C} \\sum_{c \\in [1...C]} \\frac{1}{|Gc|} (\\sum_{PEP_c} \\sum \\{p_{is}TP\\})$$\nwhere $|\\cdot|$ is the cardinality of a set, $S^0 = {0.25, 0.5, 0.75}$ and $T_{iou}$ is the IoU threshold for assigning predicted individuals and social groups to ground truth. And, C is the number of social interaction categories, $P_c$ represents the set of predicted triplets corresponding to the social interaction type c, while $G_c$ denotes the set of ground truth triplets associated with the same social interaction type c. This metric continuity reduces barriers to entry. In particular, there are two key reasons why mR@K is employed for our needs: 1) it is more robust to incomplete annotations in NVI as compared to mean Average Precision (mAP) [45], and 2) it rationally takes into account long-tailed distributions found in NVI (Fig. 3(b)), since it treats all categories equally. This is a significant advantage over the alternative R@K that exhibits reporting bias [49]. For thorough evaluation, we adopt mR@25, mR@50, mR@100, along with their average (AR)."}, {"title": "5 NVI-DEHR Model", "content": "5.1 Preliminary of Hypergraph\nA hypergraph can be viewed as a higher-order form of graph whereby edges can link more than two nodes. Denote G = (V,E) as a hypergraph, where V is a set of vertices and E is a set of hyperedges. Each vertex v \u2208V is associated with an initial embedding v. A hyperedge e\u2208 E is a subset of V, indicating the vertices it connects. For convenience, the corresponding hypergraph connectivity structure is usually represented by a binary incidence matrix $H \\in \\{0,1\\}^{|V| \\times |\\epsilon|}$, where H(v, e) = 1 if node v \u2208e, otherwise H (v, e) = 0. Moreover, we follow [18,23] to define the degree of hyperedge e and vertex v as $\\delta(e) = \\sum_{v \\in e} H(v, e)$ and $d(v) = \\sum_{e \\epsilon E} H(v, e)$, respectively. $D_e \\in R^{|\\text{e}||\\text{e}|}$ and $D_v \\in R^{|V||V|}$ are the diagonal matrices of the hyperedge and vertex degrees.\n5.2 Nonverbal Interaction Detection Hypergraph\nFig. 4 presents the architecture of our NVI-DEHR. It is built upon the popular encoder-decoder detection structure [9], consisting of a shared visual encoder for feature extraction, and two decoders for set-based prediction of NVI triplets (individual, group, interaction). A unique aspect of the model is that a dual multi-scale hypergraph is introduced to bridge the two decoders. It explicitly models complex interactional contexts among individuals and social groups, facilitating the learning of high-order cues essential for interaction recognition."}, {"title": "Understanding Nonverbal Interaction", "content": "Visual Encoder. As normal, a visual encoder is adopted to map an input image $I \\in R^{H_0 \\times W_0 \\times 3}$ into a 3D feature map $I \\in R^{H \\times W \\times D}$. The encoder shares a similar structure as DETR [9], consisting of a conventional CNN backbone (e.g., ResNet-50 [28]) for initial feature extraction, followed by a standard Transformer encoder [9] to further refine the features by integrating global contextual cues.\nInstance Decoder. Given I, our model employs a query-based Transformer decoder to detect human individuals and social groups. The decoder $F_{INS}$ takes two distinct sets of learnable queries as inputs, i.e., $Q_h\\in R^{N \\times C}$ and $Q_g\\in R^{N \\times C'}$, and transform them into output embeddings. Subsequently, they are independently decoded into bounding box coordinates for either individuals or groups through a feed forward network (FFN), $F_{FFN}$. Here $Q_h$ and $Q_g$ serve separately for the decoding of human individual or social group boxes. N and C denote the number and dimension of these queries. The entire process can be written as:\nquery updating:$Q_h, Q_g = F_{INS}(I, Q_h+P, Q_g+P)$,\nbox prediction: $B_h, B_g = F_{FFN}(Q_h, Q_g)$.\nHere we add a learnable position guided embedding $P\\in R^{N \\times C}$ to the queries so as to assign the human query and group query at the same position as a pair [40]. $Q_h = [h_1, h_2, ..., \\hat{h_N}] \\in R^{N \\times C}$ and $Q_g = [\\hat{g_1}, \\hat{g_2}, ...... , \\hat{g_N}] \\in R^{N \\times C}$ are the updated queries, while $B_h$ and $B_g$ denote the box predictions for humans and groups.\nMulti-Scale Hypergraph. Nonverbal signals are in nature complicated since they are subtle and often involve multiple participants. While direct composition of multiple individual features works well for tasks like HOI-DET [11, 35, 40], solving NVI-DET necessitates a more comprehensive relational understanding of individuals. To this end, NVI-DEHR performs multi-scale hypergraph learning to explore nonverbal interactions from a multi-granularity perspective.\nMulti-scale hypergraph construction. We utilize two distinct multi-scale hypergraphs, $G_h$ to model human-human relationships implying that the individuals participate in the same social group, and $G_g$ to model group-group relationships implying that homologous groups are paired with different individuals. For clarity, we only explain the construction of $G_h$, while $G_g$ follows a same process. Concretely, $G_h$ is comprised of a set of hypergraphs {$G^1_h, G^2_h, ..., G^s_h$}, where $G^s_h = (V_h, E)$ denotes a hypergraph at scale s. The vertex set $V_h$ is consistent across all scales, and the vertex $v_i \\in V_h$ represents the i-th human query, i.e., we have $|V_h|$ = N. The hyperedge set $E^s_h = \\{e^1, e^2, ..., e^m\\}$ models group-wise relations with $M_s$ hyperedges, each of which includes s vertices in V. As in \u00a75.1, the topology of each $G^s_h$ is represented by an incidence matrix $H^s \\in R^{N \\times M_s}$.\nWe define the hyperedges based on the distance-based construction strategy [23]. Initially, the embedding of vertex $v_i$ is set to $v_i = \\hat{h_i} \\in R^C$. Then, we compute an affinity matrix A\u2208RN\u00d7N for vertex pairs, wherein each element $A_{ij}$ measures the similarity between $v_i$ and $v_j$:\n$$A_{ij} = v_i^T v_j/||v_i||||v_j||$$\nBased on the affinity matrix, we form hyperedges at various scales. For the 1st scale (i.e., s = 1), vertices are independently treated in the graph without"}, {"title": "Understanding Nonverbal Interaction", "content": "any edges and the incidence matrix $H^1_h$ is thus an identity matrix. For other scales, each hyperedge e is formed as a cluster of vertices, which is identified by searching for a sxs sub-matrix within A that exhibits the highest density:\n$$e = arg max ||A_{0,0}||_{1,1}, s.t. |O| = s  and v_i \\epsilon O.$$\nHere $||.||_{1,1}$ represents the $L_{1,1}$ matrix norm, i.e., the summation of the absolute values of all elements in the matrix. The objective of Eq. 4 is to identify and connect the most closely related vertices. The first constraint confines the size of each cluster, while the second ensures the inclusion of $v_i$ in the group. The optimization problem is tackled via a vertex-centric greedy algorithm, which, at each iteration i, selects the vertex $v_i$ first and then includes additional s-1 vertices based on their affinity to $v_i$. In this way, for scales s > 1, we have $M_s$ = N.\nMulti-scale hypergraph learning. With multi-scale hypergraphs $G_h$ and $G_g$, we perform message exchange among vertices through a sequence of L hyperedge convolutional layers [18]. For each scale s, the convolution operation at layer l\u2208 [L] can be formulated as:\n$$V^{s,(l)}_h = D_v^{-1}H^s_h (D^{-1}_{h,e})^{-1} H_h^T (D_v^s)^{-1} V_h^{s,(l-1)} \\theta_h^{s, (l)},$$\n$$V^{s,(l)}_g = D_v^{-1}H^s_h (D^{-1}_{g,e})^{-1} H_g^T (D_v^s)^{-1} V_g^{s,(l-1)} \\theta_g^{s, (l)},$$\nwhere Eq. 5 and Eq. 6 are applied for human hypergraph and group hypergraph, respectively. The $ \\theta_h^{s, (l)}$ and $ \\theta_g^{s, (l)}$ are learnable parameters of the l-th layer at scale s. $D_v^s/D_{h,e}$ denote diagonal matrices of vertex and hyperedge degrees for either $G_h^s$ or $G_g^s$, which are computed from corresponding incidence matrices (\u00a75.1). $V_h^{s, (0)}/V_g^{s, (0)}$ are matrices with initial vertex embedding in $G_h^s/G_g^s$. After L convolutions, the final embedding for each vertex is obtained by aggregating information across various scales via a multilayer perceptron (MLP):\n$$F_h = MLP([V_h^{1,(L)}, V_h^{2,(L)}, ..., V_h^{s,(L)}]) \\in R^{N \\times C},$$\n$$F_g = MLP([V_g^{1,(L)}, V_g^{2,(L)}, ..., V_g^{s,(L)}]) \\in R^{N \\times C},$$\nwhere $F_h$ and $F_g$ are the final embedding matrices of all human and group vertices, respectively. '[, ]' denotes tensor concatenation.\nInteraction Decoder. Last, we leverage an independent query-based Transformer decoder to predict the nonverbal interaction categories for each individual-group pair. Instead of random query initialization, we propose to create nonverbal interaction query $Q_n \\in R^{N \\times C}$ dynamically based on high-order features of individuals $F_h$ and groups $F_g$:\n$$Q_n = (F_h+F_g)/2.$$\nNote that these two types of features can be directly added, since they are position-aligned in Eq. 2. Subsequently, the interaction decoder takes the query $Q_n$ alongside image feature I as input, and predict interaction categories as:\n$$P = F_{INT}(I, Q_n),$$\nwhere P is the nonverbal interaction predictions for N individual-group pairs."}, {"title": "5.3 Detailed Network Architecture", "content": "Network Architecture. We utilize ResNet-50 [28] as the CNN backbone in all experiments. Following DETR [9], the Transformer encoder consists of six standard Transformer layers, while both the instance $F_{INS}$ (cf. Eq. 2) and interaction $F_{INT}$ (cf. Eq. 10) Transformer decoders incorporate three layers. By default, we set the number of queries N to 64, the number of channels C to 256, the number of hypergraph scales S to 5, and adopt L=2 hyperedge convolutional layers.\nTraining Objective. We follow [9, 40, 64] to perform end-to-end training by assigning a bipartite matching prediction with each groundtruth using the Hungarian algorithm. The loss function is: $L=\u03bb_1 L_1+\u03bb_2 L_{GIOU} + \u03bb_3 L_c$, consisting of three parts: a $L_1$ loss and a generalized IoU loss $L_{GIOU}$ to assess localization accuracy, a focal loss $L_c$ to evaluate interaction classification. The coefficients are empirically set to: $\u03bb_1$=2.5, $\u03bb_2$=1, $\u03bb_3$=2, in accordance with QPIC [64].\nReproducibility. Our model is implemented using PyTorch and trained on 4 NVIDIA GeForce RTX 3090 GPUs. Testing is carried out on the same machine."}, {"title": "6 Experiment", "content": "6.1 Experiment on NVI-DET\nCompetitors. To better benchmark NVI and verify the proposed model, we adapt three state-of-the-art HOI-DET approaches, i.e., QPIC [64], CDN [87], GEN-VLKT [40], for nonverbal interaction detection, denoted as m-QPIC, m-CDN, and m-GEN-VLKT. We modify their interaction prediction head to align with our NVI-DET task. For m-GEN-VLKT that relies on CLIP, we modify its text prompt to the format of \u2018A photo of a person [nonverbal interaction]'.\nImplementation Details. For fairness, we train all models for 90 epochs. The learning rate is set to be 1e-4 for the initial 60 epochs and decreased to 1e-5"}, {"title": "Understanding Nonverbal Interaction", "content": "for the remaining epochs. AdamW is used as the optimizer. Common data augmentation techniques are applied, including random horizontal flipping, color jittering, and random scaling. Moving on to the procedural details during training, it is important to note that the bounding box of social group for \"individual interaction\" is identical to the individual bounding box. During inference, the ground-truths of triplets that encompass individual interactions do not entail bounding box of social group, akin to body motion categories in V-COCO [25].\nQuantitative Results. Table 2 presents the benchmarking results on NVI val and test. As seen, m-QPIC, which is a simple adaptation of DETR from object detection to interaction detection, produces the worst performance among the comparative approaches due to the lack of explicit relational reasoning. m-CDN performs much better than m-QPIC by introducing an independent interaction decoder to account for interactional relations. Notably, the CLIP-based method m-GEN-VLKT performs worse in NVI-DET, revealing that transferring knowledge from visual language models to NVI-DET seems to be more difficult than to HOI-DET. NVI-DEHR surpasses all the baselines, reaching 71.20 and 74.67 AR on NVI val and test, respectively. Furthermore, we explore how the models perform for individual- and group-wise interactions. Table 3 shows our model consistently outperforms others in both sets across all metrics except for mR@25.\nQualitative Results. In Fig. 5, we show nonverbal interaction detection results of our model on NVI test. Our model can accurately localize the interactions, and successfully recognize different nonverbal interactions. For instance, in case (e), it competently forms a gaze-following group from three out of four individuals based on their gaze direction, and also astutely recognizes the man's hugging behavior in the image, although the bounding box for the hugging group is somewhat undersized. We also present some failure cases (shown in red), which may"}, {"title": "6.2 Experiment on HOI-DET", "content": "Datasets. To fully assess the capability of our model, we evaluate it in the HOI-DET using two popular datasets, i.e., V-COCO [25] and HICO-DET [10].\nEvaluation Metrics. In accordance with conventions [24, 25, 57, 93], the mean average precision (mAP) calculated on HOI triplets is used as the metric.\nImplementation Details. During training, we use the HOI-DET loss in [40,53] for network optimization. The model is trained with the same setting used in Section 6.1. Inspired by [30,40,58,74,86], our HOI classifier is initialized with the HOI embeddings generated by the text encoder of CLIP, and adopts a variant of the cross-attention module [53] to utilize visual representation from CLIP."}, {"title": "6.3 Diagnostic Experiment", "content": "Number of Hypergraph Scales S. We first examine how the number S of scales in multi-scale hypergraph construction impacts model performance, as reported in Table 5. We observe that the model obtains an AR score of 68.37 at S=1, with all humans and social groups are treated as independent vertices in the hypergraph. As S rises, model performance progressively improves, reaching a best performance of 71.20 AR at S=5. It aligns with the research conducted by [17], which underscores that the majority of the speech in discussions involving 10 or more participants is produced by only the top 4-5 contributors.\nNumber of Hyperedge Convolutions L. In Table 6, we examine the effect of the number L of hypergraph convolution layers. Here L=0 represents a variant of our model without hypergraph learning. It yields a score of 68.22 AR. By introducing more layers, model performance improves as L increases, and tends to stabilize at L=2, with a significant boost to 71.20. We argue that increasing L leads to noise spreading in the graph, which could impair the final prediction."}, {"title": "7 Conclusion", "content": "This work makes a substantial step towards automatic interpretation of human nonverbal behaviors in everyday social environments. We challenge the conventional paradigm, which isolates social signals for standalone study, by instead examining a variety of common nonverbal signals (i.e., gaze, facial expression, gesture, posture, touch) collectively. To open this avenue, we create a richly annotated dataset NVI, formalize the nonverbal interaction detection task NVI-DET, and devise a baseline model NVI-DEHR based on hypergraph learning.\nNVI-DEHR model achieves impressive performance in two interaction detection tasks, i.e., NVI-DET and HOI-DET. Despite this, our experiments reveal that NVI-DET is considerably complex, regarding pluralistic relation among individuals, and we are now far from tackling this problem. We hope that our study will serve as valuable resources to foster more extensive exploration in this field."}, {"title": "Supplemental Material", "content": "This document provides more details to supplement our main manuscript. We first give additional analyses about NVI in \u00a7A and present more implementation details on HOI-DET in \u00a7B. Subsequently, additional quantitative results of our NVI-DEHR are summarized in \u00a7C. Finally, we delve into an in-depth discussion about social impact, potential limitations and future directions in \u00a7D."}, {"title": "A Additional Dataset Analysis", "content": "More Statistics. We investigate the distribution of individuals engaged in group-wise interaction as illustrated in Fig. S1. It can be observed that the size of the gaze group exhibits considerable diversity, ranging from 2 to 12, while the touch group predominantly comprises 2 or 3 individuals. Furthermore, we present a detailed quantitative analysis of human behaviors depicted in each image (as shown in Fig. S2), including the quantitative statistics of human instances, gaze, touch, facial expression, gesture, posture."}, {"title": "B More Implementation Details on HOI-DET", "content": "Training Objective. Following [40, 53, 64, 87], the HOI detection loss used in this work comprises four parts: a box regression loss $L_b$, a generalized IoU loss $L_{GIOU}$, a cross-entropy loss $L_o$ for object classification and a cross-entropy loss $L_a$ for action recognition. The overall loss is the weighted sum of these parts:\n$L = \\lambda_1 L_b + \\lambda_{GIOU} L_{GIOU} + \\lambda_o L_o + \\lambda_a L_a$,\nwhere $ \\lambda_b=2.5, \\lambda_{GIOU} = 1, \\lambda_o = 1, \\lambda_a = 2$.\nEvaluation Metrics. We adopt the mean Average Precision (mAP) for evaluation. A HOI detection is considered a true positive when the human is correctly linked to the corresponding object using the appropriate verb, and this human-object pair is accurately localized (The accuracy of localization is evaluated by measuring the overlap between the bounding boxes). For V-COCO [25], we report the mAPs for two scenarios: scenario 1 (S1) including the 4 body motions and scenario 2 (S2) excluding the HOI classes without object. Regarding HICO-DET [10], we assess performance across three settings: the complete set of 600 HOI categories (Full), a subset of 138 rare categories with fewer than 10 training images (Rare), and the remaining 462 categories (Non-rare)."}, {"title": "C Additional Quantitative Results on NVI-DET", "content": "As seen in Table. S1, we conduct further analysis breaking down performance by interaction category. It can be observed that our NVI-DEHR demonstrates superior performance in all categories except the posture category, with the marginal additional costs of our model. It's worth noting that all models encounter a"}, {"title": "D Discussion", "content": "Social Impact. NVI-DET takes a significant step towards creating socially-aware AI models with capabilities of generic nonverbal interaction understanding, and can benefit a variety of applications, like robotics, healthcare, and digital human. The proposed NVI-DEHR and NVI have no evident negative impact to society. Nevertheless, there is a risk that someone could use it for malicious purposes, e.g., widespread surveillance, invasion of privacy, and potential abuse of personal information. Therefore, we strongly advocate for the well-intended application of the proposed method, while simultaneously underscoring the importance of employing the dataset in a responsible and ethical manner.\nLimitation. From a feasibility perspective, we carefully select the five most representative types and 22 subcategories of them to construct NVI. But, the constrained samples may fall short of capturing the full spectrum of nonverbal interactions that take place in real-world scenarios, which could hinder the applications of NVI-DET in more complex and diverse situations. Although our image-only NVI, as a pionerring endeavor, is capable of delivering ample clues for the identification of nonverbal behaviors in most instances, there are occasional occurrences of ambiguity, like subtle facial expression and slight gaze-shift movements, akin to ambiguous actions like \"throw/catch frisbee\" in V-COCO.\nFuture Work. Moving forward, we plan to extend our NVI with temporal data for an in-depth analysis of nonverbal behaviors and enrich the variety of nonverbal interactions, like proximity i.e., the physical distances involved during the interactions [27"}]}