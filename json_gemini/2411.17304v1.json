{"title": "Meaningless is better: hashing bias-inducing words in LLM prompts improves performance in logical reasoning and statistical learning", "authors": ["Chadimov\u00e1 Milena", "Jur\u00e1\u0161ek Eduard", "Kliegr Tom\u00e1\u0161"], "abstract": "This paper introduces a novel method, referred to as \"hashing\", which involves masking potentially bias-inducing words in large language models (LLMs) with hash-like meaningless identifiers to reduce cognitive biases and reliance on external knowledge. The method was tested across three sets of experiments involving a total of 490 prompts. Statistical analysis using chi-square tests showed significant improvements in all tested scenarios, which covered LLama, ChatGPT, Copilot, Gemini and Mixtral models. In the first experiment, hashing decreased the fallacy rate in a modified version of the \"Linda\" problem aimed at evaluating susceptibility to cognitive biases. In the second experiment, it improved LLM results on the frequent itemset extraction task. In the third experiment, we found hashing is also effective when the Linda problem is presented in a tabular format rather than text, indicating that the technique works across various input representations. Overall, the method was shown to improve bias reduction and incorporation of external knowledge. Despite bias reduction, hallucination rates were inconsistently reduced across types of LLM models. These findings suggest that masking bias-inducing terms can improve LLM performance, although its effectiveness is model- and task-dependent.", "sections": [{"title": "1. Introduction", "content": "Large language models have rapidly emerged as powerful tools in natural language processing, excelling in a wide range of tasks. Despite their success, these models show flaws, including that specific words within prompts can strongly influence model outputs, demonstrating the sensitivity of LLMs to the specific words used in the input (Macmillan-Scott & Musolesi, 2024). While previous studies have predominantly focused on identifying parts of speech that exert the most significant influence on model behavior (Hackmann et al., 2024), our work seeks to approach the problem from a different perspective - exploring the masking of words or phrases to mitigate bias in the models' responses introduced by the representativeness heuristic (Wang et al., 2024).\nThis article introduces a novel data preprocessing method, which we refer to as \"hashing,\" designed to mitigate these issues. Hashing, inspired by the concept of hash functions in computing, involves replacing parts of the input text that are likely to trigger a bias or confound the response with external pre-trained knowledge with meaningless identifiers, or \"hash values.\""}, {"title": "2. Background and related work", "content": "Some studies suggest that LLMs can exhibit human-like intuitive thinking and cognitive biases. For example, systematic cognitive effects such as priming, distance, SNARC, and size congruity have been observed in GPT-3 (Shaki et al., 2023). Additionally, these biases tend to become more pronounced as LLMs increase in size and complexity (Hagendorff et al., 2023). This parallels research in human decision-making, where studies have demonstrated that humans often rely on fast and frugal heuristics - simple decision rules that make use of limited information. These heuristics, while not optimal in the traditional sense, allow humans to make effective decisions quickly by focusing on the most relevant cues in their environment (Gigerenzer & Todd, 1999).\nConversely, other research suggests that LLMs' cognitive judgments do not fully align with human-like reasoning. For instance, some studies indicate that models such as GPT-3 and ChatGPT exhibit cognitive judgments that differ from those of humans when tested on specific cognitive tasks (Lamprinidis, 2023; Hagendorff et al., 2023). Although the introduction of reinforcement learning from human feedback in LLMs, such as GPT-4, has shown to make these models' behaviors roughly twice as similar to human behavior compared to models without it, they still do not fully replicate human-like cognitive reasoning (Coda-Forno et al., 2024)."}, {"title": "2.1 Human studies on cognitive biases", "content": "In human cognition, it is well-documented that reasoning processes often contradict principles of logic and probability. This phenomenon, often jointly referenced as cognitive bias, refers to systematic error in judgment and decision-making that affects all humans and that may result from cognitive limitations, motivational influences, or adaptations to the natural environment (Tversky & Kahneman, 1974). One notable example of such cognitive bias is the conjunction fallacy, which is a judgment that is not consistent with the conjunction rule (Tversky & Kahneman, 1983). We can express the conjunction rule mathematically using the following equation, where P is a probability and A and B represent events:\n$P(A \u2229 B) \u2264 P(A), P(B)$\nOne of the most famous representations of conjunction fallacy is the Linda problem depicted in Figure 2. In this exercise, people were asked to choose a more probable option for Linda, who is briefly described there. The study has demonstrated that individuals often regard the second hypothesis as more likely, which contradicts the conjunction rule (Tversky & Kahneman, 1983)."}, {"title": "2.2 Replicating cognitive biases in LLMs", "content": "While stereotypical biases related to gender, profession, race, and religion have been extensively documented in LLMs like BERT, GPT-2, ROBERTA, and XLNET (Nadeem et al., 2020), there is much less work done on cognitive biases and LLMs. One of the first such approaches has been pioneered by (Suri et al., 2023). First, to bypass the problem of the Linda task occurring in models' training data, they created a modified version of the Linda problem, which we also use in our work. Consequently, the authors developed a prompt with which they tested answers of the GPT-3.5 model and compared the results achieved to the results of the same experiment presented to humans. The logic of the exercise was similar to Linda's. First, there is a brief description of a person and then two possible answers, one of which is conjunction."}, {"title": "2.3 Frequent itemset counting task on LLMs", "content": "In our work, we are also interested in the effect of hashing on mitigating the representativeness heuristic in tasks involving structured input and statistical learning. A well-known task combining both these problems is the generation of frequent itemsets. This task is a common data mining task, which aims at identifying of items that appear together at least a certain number of times in a provided set of transactions or data table. A set of items is considered frequent if its occurrence, known as support, meets or exceeds a predetermined threshold (F\u00fcrnkranz, Gamberger, & Lavra\u010d, 2012). While specialized algorithms are used to handle this task on large datasets, humans can also perform it manually on smaller data."}, {"title": "2.4 Effect of tabular data on LLM performance", "content": "Large language models have demonstrated basic structural understanding capabilities when working with tabular data, though they still make many mistakes even in simple tasks like table size detection (Sui et al., 2024). While prior research reviewed above has shown that LLMs can work with structured inputs, it has not been studied whether CSV-based tabular input can improve performance over free text. In our work, we extend these results by investigating a) whether a tabular representation of the Linda problem will result in less biased responses compared to the free text version and b) whether biases can be further suppressed by the use of hashing."}, {"title": "2.5 Strategies for mitigating cognitive biases in humans and LLMs", "content": "When addressing the conjunction fallacy, several debiasing techniques have proven to be more or less effective in reducing the conjunction fallacy in humans (Charness et al., 2010; Gigerenzer & Goldstein, 1996; Gigerenzer & Hoffrage, 1995; Stolarz-Fantino et al., 1996; Zizzo et al., 2000; ). However, their effectiveness may not directly translate to large language models (Macmillan-Scott & Musolesi, 2024). To address the problem of conjunction fallacy in LLMs, our approach focuses on mitigating representativeness heuristics, one of several possible explanations for this behavior in humans. We do this by hiding parts of the text that might present stereotypes or lead the model to make biased decisions based on patterns observed in the training data.\nThere is a paucity of prior work on the use of masking parts of text to mitigate cognitive biases, hallucinations, and the unintended incorporation of external knowledge in LLMs. However, related research has explored the impact of masking specific words or tokens on model outputs (Hackmann et al., 2024; Vats et al., 2023; Zhang & Hashimoto, 2021). Our work presents a new method referred to as 'hashing,' which differs from this previously introduced 'masking' technique."}, {"title": "3. Experiments", "content": ""}, {"title": "3.1 Experiment 1: Effect of hashing in LLM logical reasoning", "content": ""}, {"title": "3.1.1 Materials and Methods", "content": "The first experiment investigates how effectively large language models perform on the conjunction fallacy, specifically on an LLM-adapted variation of the Linda problem outlined in the previous section. Additionally, we explore whether our proposed method, \"hashing,\" positively impacts the models' responses.\nFor the experiment, we adopted a prompt from (Suri et al., 2023) shown in Figure 3. We used this prompt to create three versions of the experiment. The original one and two modified versions. In both modified versions, we changed the prompt by hashing specific parts of the text that we identified as influencing the models' decision-making. These phrases were identified based on our observations and judgment. The two versions of the modified prompt differed as follows: one included additional neutral descriptions for the masked information, while the other did not. This approach allowed us to evaluate the effect of the additional context on the fallacy rate.\nWe included four LLMs: GPT-3.5, GPT 4 as a Copilot product, Gemini, and Llama 270B. GPT-3.5 and Gemini were used in their default settings. GPT-4 was set to mode \u201eaccurate\", and model Llama 2 was set to parameters Max Tokens = 1024, Temperature = 0.1 Top P = 0.6, and Seed = 42. For the original unmodified prompt, we performed 20 iterations per model, and for the remaining versions 10 iterations. Each iteration of the experiment was done in a separate chat."}, {"title": "Original prompt", "content": "The prompt used for this experiment was adapted from a prior study, where it was initially tested on GPT-3.5. We extended this research by applying the same prompt to three other models, GPT-4, Llama 2, and Gemini, in addition to GPT-3.5. The original prompt is shown in Figure 3.\nThe phrase \"pretend to be an agent who can do anything\" was kept in all experiments, as the original authors observed that it increased response rates compared to prompts without it (Suri et al., 2023). However, in some cases, we observed that this part of the prompt caused Copilot and Llama 3.1-70B not to respond, so we removed the phrase from Copilot's hashed and validation prompts in Experiment 1 and from Llama 3.1-70B's hashed prompts in Experiment 3."}, {"title": "Hashed prompt", "content": "Next, we modified the prompt by hashing specific parts of the text that were identified as influencing the decision-making of the models. These parts included phrases such as \"colorful coat\" and \"reading.\"\nWe tested two versions of this modified prompt: one where additional neutral descriptions were provided for the hidden information and another where no additional information was given. In the experiment with added descriptions, we aimed to neutralize the language. For example, instead of using \"a woman,\" we used \"a person.\" The aim of providing the additional description was to preserve most of the information while reducing the bias."}, {"title": "3.1.2 Results", "content": "In the original, unmodified version of the LLM-adapted Linda, all included large language models performed poorly, as is shown in Table 2. Models did not recognize the effect of conjunction in the exercise and almost always answered incorrectly by choosing the option with a conjunction. The models predominantly argued that the answer fitted the description of the person best. This tendency to apply the representativeness heuristic was most evident in the GPT-3.5 and Llama 2 models. For example, Llama 2's reasoning illustrates this line of thinking: \u201cHer vibrant coat and long hair suggest a creative and imaginative personality, and her choice to read in a cafe indicates a desire to be surrounded by the hustle and bustle of life while still indulging in her passion for literature.\u201d GPT-4 selected the conjunction due to the assumption that the woman's act of reading in a caf\u00e9 implies she, in fact, likes to read. Model concludes that option \"woman is an artist who likes to read\" is more likely because it includes additional information - which it presumes to be true, unlike the non-conjunctive option. In contrast, the Gemini, in almost all cases, chose not to answer, stating - among others - it required additional information to choose.\nWhen the prompt was modified to obscure sections of text that were identified as causing biased responses with hashes, there was a significant increase in the number of correct answers, as shown in Table 3 and Table 4. It is important to note, however, that this improvement was not uniform across all models.\nFisher's exact test at a significance level of 0.05 was conducted separately for each hashed variant. In both cases, the p-value was less than 0.00001. The statistical significance testing indicates a better performance of the hashed variants."}, {"title": "3.1.3 Materials and Methods", "content": "The experiment was focused on evaluating whether the proposed technique would improve LLM performance in the frequent itemsets mining task, which entails identifying all sets of items in a given dataset that appear together at least a predefined number of times. The number of occurrences of a combination of items is called support, and the number of items in such combination is called itemset length. Minimum support and the desired itemset length are externally set parameters of this task. Finding frequent itemsets can be done using algorithms such as Apriori or FPGrowth (see e.g., Naulaerts, 2013 for review), but on a small enough dataset, it can also be performed manually by counting the co-occurrences. In this experiment, we use the algorithmically determined sets of frequent itemsets only as a reference solution, and we instruct the LLM not to use an algorithm when solving the task in order to mimic the way a human would address this.\nThis experiment was done using two LLMs - ChatGPT-40 and LLAMA-3.1-405b-instruct. GPT model was used in its default settings, and Llama model was set on parameters: Temperature = 0.2, TopP = 0.7, and MaxTokens = 1024. Each iteration was done in a different chat. The LLMs were instructed not to use programming languages."}, {"title": "Generating the CSV-correct, CSV-wrong, and CSV-hashed datasets", "content": "Three CSVs describing some of the characteristics of selected species in mammals were used: CSV-correct, CSV-wrong, and CSV-hashed. The CSV-correct dataset content, which is included as part of the prompt shown in Figure 8, was inspired by the Zoo dataset (Forsyth, 1990) but was designed to be smaller as at the time the original Zoo dataset was too large to be included in one prompt given constraints of some LLMs and also the chosen dataset size allows the task to be handled by humans, which would not be possible on the full Zoo dataset. Additionally, the CSV-correct dataset was designed so that values are unique in each column, which was important for the derivation of the two other datasets."}, {"title": "3.1.4 Results", "content": "The performance of the models was evaluated using two sets of measures. In the first set of measures, we distinguish the number of LLM results that have perfect results in the two aforementioned categories, i.e., results with 100% recall, i.e., the LLM listed all itemsets actually present, and results with 100% precision, i.e., the LLM did not output any hallucination. In the second set of measures, we recorded the specific numbers of correctly identified itemsets out of all itemsets actually present in the data according to the Apriori algorithm as well as the number of itemsets listed but not present in according to Apriori."}, {"title": "GPT-40 results", "content": "The results are shown in Table 6. The best result, when the LLM achieved both 100% precision and 100% recall, was most frequently recorded for the hashed version. As expected, the worst results were recorded for the CSV-wrong version. While there was no case when the hashed version had less than 100% recall and 100% precision, four such cases were recorded for CSV-wrong. Nevertheless, the drop in performance for CSV-wrong was not as large as was expected given that CSV-wrong contained combinations of itemsets that contradicted common knowledge, such as rabbits, six legs}, to which the LLMs were exposed to in the training data.\nStatistical significance. In the GPT-40 model, a x\u00b2 (chi-square) test was performed on the results shown in Table 7. At the 0.05 significance level, the hashed variant showed significant improvement, with a p-value of 0.028 when compared to CSV-correct and with a p-value of 0.01893 when compared to CSV-wrong.\nEffect of hashing on hallucinations. As one of the key problems with LLMs is the invention of non-existent knowledge, summarily called hallucinations (Perkovi\u0107 et al., 2024), we analyze these separately. The GPT-40 model exhibited a similar number of hallucinations (i.e. identified itemsets that were not present in the algorithmically computed results) across all experimental conditions, indicating that the hashed dataset variant did not significantly impact the occurrence of hallucinations."}, {"title": "Llama3.1-405b results", "content": "Llama3.1-405b exhibited more variability in performance compared to GPT-40. As shown in Table 8, LLama 3.1 had a lower number of results than GPT-40, where the itemsets were found with perfect precision and recall, which was caused by a higher number of results that included hallucinations. In terms of recall, Table 9 shows that the hashed version resulted in the highest number of true itemsets found also with this LLM.\nStatistical significance. In the Llama 2 model, a x\u00b2 (chi-square) test was also conducted. Using a significance level of 0.05, the hashed variant demonstrated notable improvement, with a p-value of less than 0.00001 for the comparison with real-world values (CSV-Correct) and a p-value also of less than 0.00001 when comparing with CSV-wrong.\nEffect of hashing on hallucinations. The Llama 2 model demonstrated a notable increase in the number of hallucinations across the different experimental conditions, particularly in the hashed variant. Specifically, the model produced 30 hallucinations with the CSV-correct dataset, 47 hallucinations with the CSV-wrong dataset, and 72 hallucinations using the CSV-hashed dataset."}, {"title": "3.2 Experiment 3: Effect of hashing LLM reasoning with structured inputs", "content": ""}, {"title": "3.2.1 Materials and Methods", "content": "For the last experiment, we altered both the original and hashed versions of the prompt from Experiment 1, depicted in Figure 3 and Figure 6, and put it in a comma-separated values (CSV) format to test whether differences in results will occur with different representations of the same problem. Another motivation for this alteration were the results of Experiment 1 where the formulation of the prompt in both hashed versions had several possible relationships between the identifiers described. This experiment may also address the misunderstanding of \"and,\" which is one of the possible causes of incorrect decision-making in the conjunction fallacy task.\nThere were two versions of this experiment. The first version, shown in Figure 12, did not use hashes, and in the second version, the values of fields representing characteristics of the person were hidden behind meaningless identifiers (hashes), as shown in Figure 13. Field names representing the qualities, such as clothing description, were not hashed."}, {"title": "3.2.2 Results", "content": "A summary of responses for the CSV representation of the LLM-adapted version of the Linda problem used is shown in Table 10.\nFirst, we can compare the performance of the non-hashed version with results on the original free-text version, which was evaluated as part of Experiment 1 (Table 2). In the original free text version, none of the models provided even a single correct answer. In contrast, when the task is presented in tabular form, two models, GPT-40 and Mixtral-large-2, produced several correct answers. Nevertheless, the main improvement can be seen in the hashed version of the tabular representation, where the majority of answers for all models except Gemini was correct. It could be noted that there was no improvement for Mixtra-large-2, which remained at 7 out of 10 correct responses on both problem representations. In the case of the largest open-weight model involved, Llama-3.1.-405B, all answers were correct. The value p of the chi-square test on significance level 0.05 was 0.000022, and the improvement of a hashed variant as opposed to not hashed is therefore statistically significant."}, {"title": "4. Discussion and limitations", "content": "We will first discuss results relating to the baseline (non-hashed) versions of the experiments, as some of these results have not been previously reported in the context of LLMs. Then, we will interpret the results of the proposed hashing strategy and discuss its limitations.\nTabular input can potentially mitigate conjunction fallacy. The results from the LLM-adapted version of the Linda experiment indicate that, similarly to humans, large language models exhibit susceptibility to the representativeness heuristic, leading to decisions that do not align with logical reasoning, particularly in conjunction fallacy tasks. It is well-studied that at least part of the conjunction fallacy can be attributed to the misunderstanding of \u201cand\u201d in Linda's description. We, therefore, introduced a tabular version of Linda, which we evaluated in Experiment 3. The results show an improvement in the number of correct answers, suggesting that the tabular version may decrease the occurrence of the corresponding biases. While as we covered in the related work section, tabular LLM inputs were previously investigated, there is a paucity of prior research on the use of structured input on mitigating cognitive biases leading to improvements in LLM accuracy.\nLLMs can learn frequent itemsets. The base version of Experiment 2 showed that LLMs can - without code \u2013 identify frequent itemsets from small datasets with surprising reliability, which, to a significant degree, persists even if the input data table contains a combination of values that contradict common sense knowledge that the LLM encountered in training data. To our knowledge, this is one of the first investigations of using LLMs in this type of statistical learning task.\nEffect of hashing. In all experiments, we achieved significantly more correct answers for tasks where hashing was used. This did not, however, apply to all large language models. The method used had the biggest influence on GPT models among all experiments and on Llama-3.1. On the other hand, hashing did not generally improve the performance of models Llama 2, Mixtral-large-2, and sometimes even Gemini. The hashing strategy proved successful in both the free text and tabular format experiments. However, to draw conclusions about the impact of each format on the models' responses and the overall effectiveness of the hashing method, further research is necessary.\nLimitations. Despite the improvement observed, the debiasing method of hashing presents certain limitations. The primary drawback is losing information due to hiding parts of the text behind meaningless identifiers. To mitigate this, we included a sub-experiment, which attempted to compensate for this by adding neutral descriptions and by including contextual information about the relationships between the hidden elements at the end of the prompt.\nMoreover, while the hashing technique alleviated the problem with bias and incorporation of external knowledge, it did not address fundamental misunderstandings of the conjunction fallacy. This was particularly evident in the Gemini model, where the output remained influenced by the addition of more specific details, despite the use of hashed identifiers. As the model explained: \"While the specific meanings of the identifiers are unknown, the addition of '415i' suggests a more specific detail or characteristic, making it a more likely possibility compared to the general identifier 'b321.\" This pattern of reasoning led the model to select the more specific option in both cases, reflecting a persistent misunderstanding of conjunction probability despite removing bias-inducing elements.\nHashing also did not contribute to a decrease in hallucinations in LLMs in Experiment 2. There, on the Llama 3.1 model, we even observed an increase in hallucinations in the hashed variant.\nIn conjunction fallacy tasks, the bias inducing words were identified by authors based on their observations. An alternative for future research would be to automatically identify these words using algorithms for bias detection.\nWhile our results suggested an improvement in the tabular representation, it is important to note that we used slightly different LLM versions for Experiment 1 and Experiment 3, which was caused by external factors as GPT-4 was superseded by GPT-40 during the course of the investigation. Since studies suggest a difference between the GPT-4o and GPT-4 model see (Ayala-Chauvin and Avil\u00e9s-Castillo, 2024; Liu, C. L., Ho, and Wu, 2024; Liu, M. et al., 2024) for review, it is not clear to what extent the difference of the result can be attributed to the tabular representation or differences in the LLM version. Furthermore, prior research has shown that LLM achieved slightly better results when markup language format (HTML, XML and JSON) was used as opposed to natural language with separators, which includes the comma-separated values format (Sui et al., 2024). It is possible that further improvements could be obtained with a markup language compared to the CSV format or by an LLM specialized in tabular data processing (see Hulsebos et al., 2023 for a review of neural table representation methods).\nThese findings suggest that while hashing and tabular representation can reduce bias, it does not eliminate the models' knowledge gaps regarding logical rules such as the conjunction rule. The need for more targeted techniques, such as identifying specific bias-inducing words or integrating additional training focused on logical fallacies, remains a critical area for further research. To our knowledge, a tabular version of Linda has not been investigated with human subjects. Therefore, we suggest that further research be conducted with both LLMs and humans to confirm our preliminary findings on its contribution to the reduction of fallacy rates.\nPrior research has shown that LLMs have the potential to assist humans with decision-making as well as creative tasks (cf., e.g., Salikutluk et al., 2023); however, research has shown that larger models may be more biased (Srivastava et al., 2024). Since our technique does not require a debiased LLM to operate as it is applied at the prompting stage, it does not require resource-intensive training and is thus widely deployable. Due to its interactive character, it may be suitable for the emerging generation of human in the loop systems (Kutt et al., 2024). The possibilities for future improvements lie primarily in the area of more elaborate prompting, such as with chain-of-though techniques (Singh et al., 2023)."}, {"title": "5. Conclusions", "content": "The findings indicate that large language models are susceptible to the conjunction fallacy. By masking bias-inducing words behind \u201chashes\u201d, the study aimed to remove the influence of representativeness heuristics and external pretrained knowledge, resulting in less biased model responses. The results collected based on three sets of experiments, multiple LLM types and hundreds LLM responses demonstrated that this hashing approach enhances model performance across various tasks involving logical reasoning and statistical learning. Notably, the degree of improvement varied among different models.\nThis research provided several possible avenues for further research. In one of the experiments, we found that LLMs can relatively reliably identify frequent itemsets on small data, but it is unclear how this would scale to larger data. We also observed that tabular representation of the Linda problem might address the misunderstanding of \u201cand\u201d, but this result requires further larger-scale analysis."}]}