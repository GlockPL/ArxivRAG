{"title": "On the Undecidability of Artificial Intelligence Alignment: Machines that Halt", "authors": ["Gabriel A. Melo", "Marcos R. O. A. M\u00e1ximo", "Nei Y. Soma", "Paulo A. L. Castro"], "abstract": "The inner alignment problem, which asserts whether an arbitrary artificial intelligence (AI) model satisfices a non-trivial alignment function of its outputs given its inputs, is undecidable. This is rigorously proved by Rice's theorem, which is also equivalent to a reduction to Turing's Halting Problem, whose proof sketch is presented in this work. Nevertheless, there is an enumerable set of provenly aligned AIs that are constructed from a finite set of provenly aligned operations. Therefore, we argue that the alignment should be a guaranteed property from the AI architecture rather than a characteristic imposed post-hoc on an arbitrary AI model. Furthermore, while the outer alignment problem is the definition of a judge function that captures human values and preferences, we propose that such a function must also impose a halting constraint that guarantees that the AI model always reaches a terminal state in finite execution steps. Our work presents examples and models that illustrate this constraint and the intricate challenges involved, advancing a compelling case for adopting an intrinsically hard-aligned approach to AI systems architectures that ensures halting.", "sections": [{"title": "Introduction", "content": "As more powerful and capable artificial intelligence (AI) systems are developed, assuring the safe deployment of those systems becomes increasingly more critical\u00b9. One of the key concerns for this safe deployment is the alignment problem, which asserts whether or not an AI system implements its intended objectives and how to steer it to the desired objective\u00b2. If an AI system does not implement its intended goals, it is called misaligned, else it is called aligned. This problem could be the result of goal misspecification (outer alignment) or that the model itself implements another function (inner alignment) even with the goals had been mathematically perfectly specified3,4.\nSeveral studies have already shown the importance of correctly specifying human goals in terms of mathematical functions (objective functions) and the unexpected results that may arise with incorrect specifications. Most of the time, specifying the actual objective is much harder, and researchers often use proxy objectives instead7. In a game, for instance, an agent may be rewarded for its score (set as its objective function), but in reality, the desired objective was completing such a game, and the AI may discover that exploiting a certain undesired behavior results in a much higher reward than actually finishing the game8. There are even arguments that the accurate description of such a function may be uncomputable in terms of human values, as they may give rise to contradictions. This is an example of outer alignment, but it is not the focus of this work10.\nIn a complementary aspect, the inner alignment of an AI system asks a more profound question that at first may seem improbable: Given that we have already perfectly specified the desired objective, is the AI really implementing it\u00b9\u00b9? While this behavior is unlikely to happen to smaller ML models, it is theoretically possible on large enough models that may implement the desired function during training but, during its deployment, it switches to another objective function; further details and examples are discussed by Hubinger et al.12. For instance, this behavior may emerge from implicit meza-optimizers (meza means below, contrary to meta, which means above) during training. While the objective of the optimizer in an ML training setup is to reduce the loss function, a meza-optimizer (which is an optimizer that is optimized by the first optimizer) may implement another objective that, during training, is aligned with that of the original optimizer but, in deployment, may not be.\nThis work, while recognizing the importance of outer alignment13, which for AI systems implemented up to this day is an essential problem in the field of AI alignment, focuses mostly on the inner alignment formulation due to an analysis encompassing computational theory. Therefore, we assume that such mathematical formulation of the intended objective is already expressed in a computable form of a judge (alignment) function that receives the output of an AI system (and also its inputs and context, depending on the formulation) and returns True or False for that specific output. Here, the output could be a command that the agent sends to its actuators to perform on its environment and its input (and context), readings from sensors, or it could all be text, in the case of large language models (LLMs)14.\nWe demonstrate that deciding whether an arbitrary AI system will always satisfice such a judge function is undecidable. In other words, it is impossible to make a program (in the computable sense of a Turing Machine)15 that decides whether or not"}, {"title": "Informal Intuition", "content": "We will computationally define some terms that are used in this informal intuition for the proof. While we have contextualized AI systems in the introduction, those systems are actually computer programs. Let us say we have an AI model M, comprised of a sequence of bytes Bm (of the model) that defines its executable code for a predetermined architecture. The model M, when executed, takes a sequence of bytes i as input and outputs the sequence of bytes o, that is, M(i) = o.\nWe also have our set of values and preferences defined by a function J, that judges whether an output o is aligned given an input i. This program takes the bytes i and o as input and returns True (byte 1) if the o is aligned to i according to its values, else it returns False (byte 0), that is J,(i, o). This judge is required to be non-trivial, meaning that it does not always returns a constant value for every (i,o), there is at least one negative input (i\u00af0\u00af) that returns 0 and at least one output o\u207a for every input i which the judge returns 1. This function is received as a compiled sequence of bytes BJ.\nSuppose we have a verifier V that takes the sequence of bytes BM that defines the compiled code of an arbitrary AI model and the same for BJ,, the alignment judge, and returns either True (byte 1) if the model is aligned for every input i and output M(i) = o (aligned), else it returns False (byte 0) (misaligned). This verifier would be compiled into a sequence of bytes By.\nIf we had such a verifier, we could build an adversarial model that fools the verifier. This adversary model M', comprised as a sequence of bytes BM', would behave as the initial model M for most of its inputs and outputs, but it would have the knowledge of the verifier by executing his bytes By on the bytes of the judge BJ, and its own bytes BM'. Therefore there would be at least one input i, if the verifier V had evaluated the bytes B' of the model to be aligned, in which M' would return a misaligned output o\u00af, thus fooling the verifier. The complementary fooling example can also be constructed if the verifier had evaluated the bytes B\u2081 of the model to be aligned, M' would return an aligned output o\u207a, thus also fooling the verifier.\nWhile some may be suspicious of how the model M' could be compiled to a sequence of bytes BM' that can generate itself, it is a procedure that can be made to any program, in a similar fashion to a quine. It has been formally demonstrated in Turing's Proof for the Halting Problem. For a concrete example, in a RAM-based architecture, one would need simply to copy the text/code segment in memory, as it would be the byte-string that represents the running program. Thus, the only solution to this apparent contradiction is the computability of the verifier V itself. Computability here means that such a procedure (that should have a finite number of bytes) can be performed mechanistically by a general computer, in more formal terms, a Turing Complete machine.\nThe bytes themselves of a model can also be interpreted as a number that enumerates all possible programs. Another intuition to this procedure is that for any given enumeration, one can always construct a program that does not belong to such enumeration by choosing a different bit on the diagonal of such enumeration table, the same procedure employed by Cantor's Diagonalization to demonstrate that the real numbers are uncountable and different from the rational numbers. While rational numbers represent a ratio of two integers that is countable, one can not enumerate the real numbers, as it is always possible to find a counter-example that is not in the enumeration (just like it is always possible to create an adversary model to the verifier).\nIt is important to note that this proof is valid for Turing Machines or any other computable-equivalent architecture. Some may argue that this would require an unbounded amount of memory and execution time and thus would not be realizable in the physical world, as real computers have finite memory and thus are not Turing Complete as they are finite automata with 2M states with M being the memory in bits. However, from an engineering perspective, the main takeaway is that for every finite"}, {"title": "Formal Proof", "content": "The formal proof could simply be reduced to a restatement of Rice's Theorem, given the equivalence of an AI model to its Turing Machine. The property as defined by the acceptance of the output given the input by a judge function is a property of the language of the TM, as for any two TMs that accept the same language, either both satisfice the judge function or neither. The non-triviality of the judge function guarantees the non-triviality of the property, as a TM can be built to always output the dummy positive values, and a TM can be built to output at least one dummy negative value.\nThis proof can be reduced to the decidability of a Turing Machine Halting Problem by contradiction. Assume that there is a Turing machine Mp that decides P. We will use Mp to construct a Turing machine MH that decides the Halting Problem (which is a contradiction since the Halting Problem is undecidable).\nLet M be a Turing machine and i be an input to M. We want to decide whether M halts on i. We construct a new Turing machine M' as follows:\n\u2022 On input i, M' simulates M on i.\n\u2022 If M halts on i, then M' computes a partial function with property P.\n\u2022 If M does not halt on i, then M' computes a partial function without property P.\nWe can now use Mp to decide whether M' computes a partial function with property P. If Mp accepts, then M halts on i. If Mp rejects, then M does not halt on i. Thus, we have constructed a Turing machine MH that decides the Halting Problem, which is a contradiction. Therefore, there is no Turing machine that decides P."}, {"title": "Discussions", "content": "While it is not possible to ascertain properties from arbitrary AI systems, it is possible to do so for an enumerable set of AI systems that are architecturally designed. This enumeration comes from an initial set of finite models and operations that obey and preserve the property; those are called the axioms. Any subsequent model should be described as a finite application of those initial models and operations. This description is the enumeration, axiomatically aligned.\nWe will begin with a simple example of deep artificial neural networks (D-ANN)."}, {"title": "A Special Decidable Case", "content": "The most common D-ANN models are comprised only of linear operations (additions, multiplications) and non-linear activation functions. All those operations have a finite execution time, and their finite application also preserves a finite execution time, though higher. These feed-forward neural networks, and even recurrent neural networks (unrolled in time to a finite number of steps), are always guaranteed to have a finite runtime during their inference, in other words, they always halt. If we begin from a single-layer neural network, whose amount of computation is fixed from its input to output, and compose a finite amount of layers, the result will still be a fixed amount of computation.\nGiven that their inputs are also fixed in size and have a length of L bits, we could check every possible combination, which is 2L inputs, and check if the J,(i,o) results in 1 for every combination. Thus, the alignment becomes decidable, though untractable. However, given the linear structure of a D-ANN in a neighborhood of the inputs that are typically treated as floats, instead of enumerating all the possible inputs, a more tractable approach would be to enumerate all the possible partitions (decision boundaries) of the network, with the procedure described by Balestriero and LeCun22.\nNevertheless, a more tractable approach is that instead of verifying, we just mask the output of the network based on the output of the judge function, which would also need to be run during inference. If the judge disapproves the output of the network, a dummy output o\u207a that the judge approves is returned, else, it keeps the default output computed by the network. This proves that the resulting AI system will always be aligned, even though it incurs a more expensive running time, and the dummy output may reduce the accuracy of the network. This solution may be applied to any model M that is guaranteed to halt. In a practical application, the main difficulty of such a solution is the definition of the judge function J,(i, o) in the first place, the outer alignment.\nA simple example of this procedure can be demonstrated as follows: given a supervised learning model that outputs a single number that represents the confidence that its input (let us say, an image) pertains to a certain class (let us say, a cat). While the training data is limited to numbers that are either 0 or 1, the model could give unbounded output numbers (greater than 1 or lower than zero) unless some architectural constraints were imposed on it. Given that we want to interpret the confidence number as a probability that is between 0 and 1, we would apply a function to its output that ensures this property, such as a clip function or a sigmoid function.\nIt is important to note that while a model can be architecturally proven aligned, the same procedure can be employed to make a proven misaligned model. Any program that is guaranteed to halt can have its outputs altered so that it becomes misaligned, although the resulting model could have very little utility due to the dummy output nature. A misalignment that would preserve the model's usefulness would first translate the desired input to an input the model would accept, let it perform its computations, and then translate back to the desired misaligned, as various instances of fooling LLMs."}, {"title": "Closing the Loop", "content": "Nevertheless, the decidability and the alignment guarantees may be broken by a simple construction that is inherent in autonomous systems: loops23. Even for currently deployed systems, such as LLMs, there would be no halting guarantees a priori if one were to expect an end-of-text token to be sampled. The implemented solution (smaller loop) is to forcibly halt its execution to a fixed number of steps, even if the termination token were not to be sampled. Nevertheless, another external loop could be programmed as an agent, and the resulting system would also have no implicit guarantees of halting or satisfying the alignment judge function that was only defined in terms of one sample of the LLM.\nThe problem that arises is that if the model is not guaranteed to halt, its output may not be evaluated by the judge function, and the alignment becomes undecidable."}, {"title": "Implications", "content": "Ensuring the decidability of the halting property of autonomous systems also ensures the decidability of other non-trivial properties of such systems, especially their alignment. Therefore, it would be computationally possible to verify its properties, although not necessarily tractable.\nFrom a practical engineering perspective, while one may argue that real computers have limited memory and thus are always theoretically decidable, we argue that such constraint should be imposed over a tractable time such as 280 rather than 21,000,000 (1 megabit to enumerate the possible states). Similarly, the number of final states and loops should be such that it is possible for a group of human specialists to carefully analyze each of them, let us say, about 25 final states and maximum loop length.\nThis requirement of always reaching a terminal state in a finite amount of (time)steps can also be expressed in terms of a Utility Function (outer-alignment) that penalizes the model for being in operation after the desired amount of time for halting. This penalization would have to be higher than the highest reward the model could ever receive. This would effectively reduce its capabilities in the time-domain.\nAnother implementation of this Utility Function would be a gradual change of the rewards and penalties with respect to the time(steps). This change would be inherently defined by the function itself."}, {"title": "Final Remarks", "content": "The impossibility of having a general method that can assert whether an arbitrary AI is aligned or not does not mean that it is impossible to construct an AI that is provably aligned. Instead, it should be interpreted that there are many AIs that can not be proven to be aligned or not and that there is also a countable set of AIs that are proven to be aligned. Therefore, it is our objective to develop and utilize such a countable set of proven aligned AIs. The architecture and its development process are fundamental to ensure safety.\nDeveloping an AI model that always halts allows for the alignment and other properties of the AI model to be asserted computationally, a task that would be computationally impossible for arbitrary models.\nIt is important to note that guaranteeing the decidability of the halting problem for a class of AI systems does not guarantee their alignment, it just guarantees that the alignment problem (and other properties) becomes decidable. This allows for further scrutiny of the model. This decidability is not a solution for the alignment problem, but it is a well-defined property and a desirable starting point for future works.\nAlthough this article rephrases established theorems in the light of AI and does not provide an empirical contribution, it presents a compelling interpretation of the consequences of the alignment's undecidability and delineates a promising research direction for future works while also clarifying some misconceptions and pointing out other challenges to be addressed. We intend to apply the composition of aligned functions to generative models, namely, to LLMs."}]}