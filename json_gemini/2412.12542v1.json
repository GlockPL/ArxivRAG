{"title": "Bots against Bias: Critical Next Steps for Human-Robot Interaction", "authors": ["KATIE SEABORN"], "abstract": "We humans are biased \u2013 and our robotic creations are biased, too. Bias is a natural phenomenon that drives our perceptions and behavior, including when it comes to socially expressive robots that have humanlike features. Recognizing that we embed bias, knowingly or not, within the design of such robots is crucial to studying its implications for people in modern societies. In this chapter, I consider the multifaceted question of bias in the context of humanoid, AI-enabled, and expressive social robots: Where does bias arise, what does it look like, and what can (or should) we do about it. I offer observations on human-robot interaction (HRI) along two parallel tracks: (1) robots designed in bias-conscious ways and (2) robots that may help us tackle bias in the human world. I outline a curated selection of cases for each track drawn from the latest HRI research and positioned against social, legal, and ethical factors. I also propose a set of critical next steps to tackle the challenges and opportunities on bias within HRI research and practice.", "sections": [{"title": "19.1 Introduction", "content": "Bias, whether toward humans or humanoid robots, is a natural function of the human brain.1\nWe have evolved brains that are able to craft models of the world and then use these models to\nmake rapid judgments when faced with new information. This can be, and perhaps in the more\ndistant past often was, a matter of survival under harsh, dynamic, and unpredictable\nconditions. But it is also a fundamental cognitive tool for daily life now. Decision-making\nhappens at all scales, from the insignificant to the highest of stakes. Decisions can involve a\ncomplex array of shifting factors or a simple few.2 Time pressure and time scales can vary.\nSome require concerted and conscious effort, while others are best served by the speed of our\nunconscious mind. Sometimes we do not have a choice in the matter. Indeed, as Kahneman,\nTversky, and others have persuasively demonstrated across decades of work, our brains opt to\nthink quickly and less rigorously, unless directed otherwise. From an evolutionary perspective,\nthis may be grounded in our survival as a species.3 But what is then directing our brains when\nit is not our conscious minds? One answer is bias. While much of the above discussion applies\nto humans, I propose that it equally applies to robots, especially those that are humanoid in\nappearance, AI-enabled, expressive, and interacting with humans in social contexts. For such\nrobots, bias is an especially timely topic and raises provocative questions for law, policy, and\nethics.\nBias refers to a cognitive inclination toward one of several options. Bias on its own is not\nnecessarily unconscious or valenced. Nevertheless, we tend to focus on when bias does harm\nwithout us realizing it. Welsh and Begg,3 for instance, define bias as the \u201cunconscious errors\nresulting from how people's minds work.\u201d Furthermore, bias emerges during decision-making,\nwhen we deploy a heuristic or practical method to solve a problem.4 It can be unconscious\n(implicit) or conscious (explicit) in orientation. It can take place in the brain (cognitive bias),\nbe observed in behavior (prejudice), and be embedded in the things that we create, including"}, {"title": "19.2 Track: Robots against Bias", "content": "We are at the mercy of our evolutionary history.13 Many of the cognitive shortcuts and biases\nwe host developed in our ancestors as successful responses to a wild and hostile world. Now\nthe world is smaller, a direct consequence of human mastery over nature. Globalization, world\ntravel, telecommunications, and especially the internet have all changed the way we relate to\nour world, as well as created new worlds never before experienced by any species on earth.\nThese technological worlds are nevertheless processed by human brains. Evolution happens at\na slow pace and may largely be outside of our hands, at least directly. Nevertheless, we have\nfull control over our technological world crafting. Can we create robots that help us become\naware of, if not overcome, cognitive pitfalls and logical fails when appropriate? Can robots\nscaffold our thinking? Can they save us from ourselves, and does everyone want to be saved \u2013\nin the same way? I offer several cases where robots could intervene when human processing\nfails in prosocial, ethical ways."}, {"title": "19.2.1 CASE: MANIPULATION", "content": "Reality may not always be what it seems. Sometimes this is a matter of subjectivity. Other\ntimes it is a matter of circumstance. And still other times it can be a matter of deception.\nPeople or their agents may purposefully distort information about or representations of\nreality. I provide two pertinent examples of where AI-powered and socially aware robots can\nintervene: to assess our resilience against misinformation in the future infodemic 2.0 and as\nfaithful interviewers during sensitive situations where being human may be a deficit."}, {"title": "19.2.1.1 Example: Robots That Simulate Our Dystopian Futures", "content": "We are surrounded by opportunities to be subjected to fake news. Every time we log onto\nsocial media or even check in with our friends, family, and acquaintances online, we are at risk\nof exposure. Al-enabled humanoid robots, as well, can be a source of deception and false news\nwhen we engage them for information-seeking tasks. Misinformation is not necessarily\nobvious. A survey in 2022 found that between 45 percent and 55 percent of respondents from\naround the world noticed fake news and this was only what they noticed.14 Bots that shill\nfake news are sophisticated, mimicking conventional speech and echoing the mantras of the\nideologies that they represent. We are no longer good at detecting real people from fake\naccounts. We are also at risk when people we trust fall prey to misinformation. Misinformation\nspreads through social networks based on human relationships. Like dominos, it only takes"}, {"title": "19.2.1.2 Example: Harnessing Robots That Fail to Mislead People", "content": "Deceit is a human phenomenon. People lie, hold back the truth, manipulate facts, mislead, and\nmisdirect. Reasons run the gamut from self-interest to protecting others to malicious delight\nto simple bias. When a crime happens and we must rely on witness testimony, bias can come\ninto play. A range of cognitive pitfalls and biases have been identified on the side of the\nwitness as well as the interviewer.20 Memory is fallible: even if we intend to accurately\nportray the truth as we know it, our brains are incapable of faithfully recording every detail of\nan event.21 In fact, recall is noisy, with information loss every time a memory is brought to\nmind.22 We are also not good at remembering details over the gist of an event or scene.23 A\ncar may have been red or maroon or even blue. Witnesses may also be keen to help or feel\npressure under the guidance of an expert who is interviewing them, leading to a range of well-\nmapped biases. Social acceptability bias, for instance, may lead the witness to withhold,\nmassage, or outright lie about what they remember.24 At least, when faced with a human\ninterviewer."}, {"title": "19.2.2 CASE: METACOGNITION", "content": "We cannot always avoid being manipulated, influenced, persuaded, or otherwise falling prey to\nbias. So, what can we do? Perhaps living with bias is a matter of being aware of it. And perhaps\nrobots can step in when we encounter bias in ourselves or within a manipulative scenario,\nboosting our performance in terms of recognizing, understanding, and consciously reacting to\nmurky situations. Metacognition is an umbrella concept referring to a top-down procedure of\nmonitoring and controlling our cognition, commonly summarized as \u201cthinking about our\nthinking.\""}, {"title": "19.2.2.1 Example: \u201cTeachable Moments\u201d with Robots That Fall Prey to Biases", "content": "One way to learn is to help others learn. A robot could be your pupil in metacognition. When\nthe robot makes a logical misstep, you can intervene \u2013 and thereby increase your own\nperformance when it comes to similar mishaps. But will you know when to intervene and how?\nAlong these lines, Biswas and Murray34 designed three robots, ERWIN, MyKeepon, and MARC,\nto fall prey to five kinds of cognitive biases: misattribution bias, where people incorrectly\nattribute a cause to an event or link between events that does not exist; empathy gaps, when\none is feeling a strong emotion and is unable to imagine being in another emotional state, such"}, {"title": "19.2.2.2 Example: Robots That Intervene When We Fail", "content": "Every time we engage with the digital world, we must make decisions about the information\nwe receive and should accept that we are at risk of deception. What if robots were there by our\nside, boosting our abilities or intervening before we fall prey to known tactics? Robots could be\ndesigned as ever-present aids or cognition extenders, ready to help us on a moment's notice.\nHow can robots intervene? Kozyreva, Lewandowsky, and Hertwig38 recently proposed a\ngeneral three-part framework of strategies that could be applied to the design of robots as aids\nwhen we participate in uncertain and potentially hostile digital environments. These\nstrategies support our control over our own cognition and behavior and are founded on\ndecades of experimental research. Nudging39 involves manipulating the choice architecture or\nwhat choices are available to us, especially those that are closest and easiest and most"}, {"title": "19.2.2.3 Example: Robots That Capitalize on Our Biases ... for Our Own Good", "content": "The experience of biases may or may not be good for us or others. Perhaps a robot could be\ndesigned to harness our propensity to be biased ... in a way beneficial to us. Obo et al.45\ndeveloped an exercise trainer robot for older adults who need motivation to be more\nphysically fit. They made use of the framing effect bias, whereby the framing, positive or\nnegative, of the choices we are given influences our ultimate decision. To do this, the"}, {"title": "19.2.3 CASE: TRUST", "content": "Trust is a fundamental mediating factor of relationships between people, other animals, and\nartificial agents, too. Trust is a multidimensional concept.47 Many researchers within HRI refer\nto the definition offered by Wagner and colleagues,48 where trust is \u201ca belief, held by the\ntrustor that the trustee will act in a manner that mitigates the trustor's risk in a situation in\nwhich the trustor has put its outcomes at risk.\u201d49 When it comes to robots, this means that a\nperson trusts the robot to act accordingly in situations where the person is at risk. Risk can be\nphysical, emotional, psychological, economic ... any situation in which the person has a stake.\nTrust can be calibrated by the actual capabilities of the robot50 leading to calibrated trust,"}, {"title": "19.2.3.1 Example: When We Trust Robots Too Much", "content": "As robots and other intelligent systems have entered daily life in greater and greater numbers,\nresearch has indicated that we may trust these systems too much. In other words, we may\novertrust.51 Overtrust can occur due to automation bias, where our beliefs about a system's\ncapabilities are far greater than reality,52 and/or complacency, where we become less\nattentive to the performance of a system over time.53 Work on overtrust in robots remains\nnascent, with most work derived from automation research.\nOvertrust tends to occur in three general contexts: decision-making, emergencies, and\nsecurity. Salem et al.54 investigated how people would react to an \u201cerratic\u201d robot that made\nstrange and potentially dangerous requests, including those that would lead to property\ndamage. Alarmingly, participants tended to comply with the requests, even though they later"}, {"title": "19.2.4 CASE: STEREOTYPES", "content": "Stereotypes are widely held but simple models of people in societies. We rely on stereotypes\nwhen encountering and making sense of individuals for the first time. Stereotype activation\noccurs when we react to models of the people associated with the social group or certain social\ncharacteristics that we perceive in an individual. We can react to stereotypes we perceive in\nothers as well as in ourselves. Typically, these models are negative, narrow, simplistic, and yet"}, {"title": "19.2.4.1 Example: Robots That Dodge or Disrupt Stereotypes", "content": "If we know when stereotypes may be activities, can we avoid stereotype priming and threats?\nCan we even go so far as to make use of our knowledge of stereotypes to purposefully disrupt\nexpectations, provoke, and inspire reflection or creativity? Scant research on robots exists in\nthis area, but recent work is heading in this direction. Ogunyale, Bryant, and Howard62\nconsidered race stereotypes of emotional expression within the US context ... applied to\nhumanoid robots. Respondents were presented with videos of either a white or black version\nof the ROBOTIS Darwin-Mini robot performing emotional gestures. Notably, they were not"}, {"title": "19.3 Track: Against Bias in Robots", "content": "Robots may be able to help us manage, prevent, and even reverse biases in decision-making\nand interacting with the world and each other. But we also need to consider the other side of\nthe coin. As engineers and deployers of robots, our thinking and actions are formalized\nthrough research and development practices. In other words, robots can be the bearers of our\nown biases. The good news is that we need only to turn a critical eye on the fruits of our labors\nto smoke it out. A recent critical review67 identified where and how cognitive biases have been\nfound, if not fruitfully deployed, in social robotics work. Yet, most of the work so far has\nfocused on stereotype effects. Early work by Nass, Reeves, Moon, and colleagues68 revealed\npervasive biases in how humanoid computer agents tend to be interpreted by people. These\nstereotyped responses by people to social constructs embedded in the voice and body of\ncomputer agents have been demonstrated time and again over the years, leading to the"}, {"title": "19.3.1 CASE: INTERSECTIONAL DESIGN", "content": "Intersectionality describes how social characteristics intersect with power in compounding\nways. Originally proposed by Crenshaw75 for law, intersectionality has been taken up as a\nbroader explanatory framework one that is critical for robots, as well. Designing and\ndeploying robots is a matter of power. Engineers, roboticists, designers, developers,\nprogrammers, technicians, practitioners, researchers, doctors, nurses, and other stakeholders\non \u201cour\u201d side are in a position to make decisions or unwittingly embed our own perspectives,\nchoices, and demands into robots. When we create humanoid robots, do we consider how the\nembodiment of these robots invokes gender, age, ethnicity/race, class, and other social\ncharacteristics? Do we consider how these characteristics can intersect, or do we only consider\none at a time? Do we recognize that others, including if not especially our intended end users,\nmay have a different perspective on how we have chosen to embed these characteristics in the\nrobot's form factor and morphology, verbal and nonverbal expressions, role and application,\nmachine learning algorithms, and underlying AI systems? Critical perspectives within HRI and\nrelated spaces have raised awareness of how our own unconscious biases can play a role in\nrepresentation and social harm through how we design and use robots with people.76\nIntersectional design77 has been proposed as a framework relevant to the design of socially\nexpressive robots that have human characteristics in appearance and behavior. Work is just\nstarting, and there is a lot of ground to cover. I consider three key elements in robots that\nengage with people: morphology and form factor, including \u201cvoice\u201d and \u201cbody\u201d; algorithms\nand data sets that make up the \u201cbrain\u201d; and behaviors or \u201cactions,\u201d both passive and reactive."}, {"title": "19.3.1.1 Example: Embodying Diversity in Voice and Body", "content": "Social robots have a physical \u201cbody\u201d or form factor. The morphology \u2013 shape, size, width,\nweight, height, materials, textures, \u201cclothes,\u201d face, limbs, wheels, and so on \u2013 are taken in and\nprocessed by our minds in certain ways. Moreover, the voice of the robot can influence\nperceptions,78 even to the point of causing confusion and uncanny valley reactions when\ngender cues do not align.79 When humanlike cues exist, we tend to draw on human models to\nunderstand what we are perceiving and how to react to it \u2013 and mostly this is a rapid and\nunconscious process. Indeed, while we may make decisions about what a robot should look\nlike, we may not always do so with all possibilities in mind. Similarly, people interacting with\nrobots tend to interpret those robots in line with human models, including any limited,\nstereotyped, or otherwise negative associations they may have about people when it comes to\nthe specific social characteristics those robots embody. For example, while we may take it for\ngranted that a robot is gender neutral, our participants may not agree.80 Robots, especially\nsocially expressive ones that feature humanoid markers, are not tabula rasa."}, {"title": "19.3.1.2 Example: From Algorithmic Bias to Diverse Data Sets and Explainable Intelligence", "content": "Algorithmic bias occurs when the functions in an algorithm, its approach to prediction, and/or\nits training data are biased. This is not a machine problem; it is a human problem. People are\nthe ones creating and making decisions about machines. Part of the problem is that people are\nnot aware of their own biases. Another part of the problem is that we do not have access to the\nheart of the algorithms, either due to proprietary restrictions or their black-box nature. These\nalgorithms are now finding a home in the brains of social robots. Aldebaran-SoftBank's\nhumanoid Pepper, for instance, uses a proprietary set of algorithms to detect faces and facial\nexpressions.89 While not robots per se, we may think of algorithms as comprising the robot\n\u201cbrain.\u201d As such, these algorithms are becoming an important feature of HRI experiences and\nthe ways in which they can bias or be biased.\nTackling algorithmic bias is challenging, but there are two uncontroversial aspects that we can\nfocus on: data sets and explainability. Data sets are used to train and test the algorithms. They\nhave to be big, but they also have to be accurate. This is where bias can creep in: The data sets\nbeing fed to the algorithms represent a limited or disproportionate section of the real data out\nin the world. In other words, they are not diverse enough. Explainability, commonly captured\nunder the notion of explainable AI or XAI,90 refers to an algorithm that provides its decision-\nmaking processes openly and in a way that a person can understand. Avoiding algorithmic bias\ninvolves two feats: Ensuring that the data is sufficiently large and diverse, and providing a"}, {"title": "19.3.1.3 Example: Behavior as a Two-Way Street", "content": "Bias can be reflected in the behavior we design into robots. We may assume that everyone will\ntreat robots in line with our own imaginations or expectations, but that is not necessarily the\ncase. People can and do abuse robots, or at least act in abusive ways toward robots.97 Abusive"}, {"title": "19.3.2 CASE: DIVERSITY IN PRAXIS", "content": "\u201cWe are better together\u201d could be an alternative title for this section. Reducing and avoiding\nbias in the design and deployment of robots is surely a team effort, especially because we\nimplicitly bias the robots we design and use.108 For this, team diversity is key.109 Robotics is a"}, {"title": "19.4 Conclusion", "content": "Bias cannot be eradicated, but it does not need to be. We only need to recognize it, understand\nit, and be conscious of how we do or do not address it. Taking a critical lens to the socially\nexpressive humanoid robots that we work on, study, and increasingly live with is fruitful for\nunderstanding how biases crop up, as well as how they might be addressed. I have outlined a\nseries of cases along these parallel lines that I hope demonstrates where we are at \u2013 and where\nwe can go from here. The social, ethical, and legal repercussions should incite curiosity and a\nlittle trepidation. We can move forward with purpose and community-level reflexivity,\ncontinuing initial efforts through training camps, workshops, edited volumes and calls, and\nacademic-industrial partnerships. Future critical review work, especially systematic reviews\nand meta-analyses targeting certain forms of bias will show just how far we have progressed.\nNew laws, notions tested in court, and ethically aware forms of social robots and interactions\nwill demonstrate whether and how we have made headway. The critical next steps outlined\nabove can be explored across a wide array of humanlike and socially aware robots and HRI\ncontexts. We have made interactive humanoid robots in our image; now we can explore the\nrepercussions and opportunities of this with principled intent."}]}