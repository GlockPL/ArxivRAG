[{"title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models", "authors": ["Yulei Qin", "Yuncheng Yang", "Pengcheng Guo", "Gang Li", "Hang Shao", "Yuchen Shi", "Zihan Xu", "Yun Gu", "Ke Li", "Xing Sun"], "abstract": "Instruction tuning plays a critical role in aligning large language models (LLMs) with human preference. Despite the vast amount of open instruction datasets, naively training a LLM on all existing instructions may not be optimal and practical. To pinpoint the most beneficial datapoints, data assessment and selection methods have been proposed in the fields of natural language processing (NLP) and deep learning. However, under the context of instruction tuning, there still exists a gap in knowledge on what kind of data evaluation metrics can be employed and how they can be integrated into the selection mechanism. To bridge this gap, we present a comprehensive review on existing literature of data assessment and selection especially for instruction tuning of LLMs. We systematically categorize all applicable methods into quality-based, diversity-based, and importance-based ones where a unified, fine-grained taxonomy is structured. For each category, representative methods are elaborated to describe the landscape of relevant research. In addition, comparison between latest methods is conducted on their officially reported results to provide in-depth discussions on their limitations. Finally, we summarize the open challenges and propose the promosing avenues for future studies. All related contents are available at https://github.com/yuleiqin/ fantastic-data-engineering.", "sections": [{"title": "1 Introduction", "content": "One of the ultimate goal of developing large Inguage models (LLMs) is to unlock their potentials of generalization to unseen natural language processing (NLP) tasks. Towards this goal, a series of LLMs such as GPTs (Brown et al., 2020; Achiam et al., 2023), LLaMAs (Touvron et al., 2023a,b; AI@Meta, 2024), and Mistrals (Jiang et al., 2023a, 2024a) have delivered high-level text understanding and generation capabilities via utilizing vast amount of high-quality web and human-annotated datasets for pre-training and preference alignment (Liu et al., 2023a, 2024c; Sun et al., 2024b; Edunov et al., 2019; Dong et al., 2019). During preference alignment, instruction tuning plays an important role in refining pre-trained LLMs to provide accurate, pertinent, and harmless responses on a collection of downstream tasks (Wei et al., 2021; Sanh et al., 2021; Zhang et al., 2023c; Peng et al., 2023; Longpre et al., 2023; Shu et al., 2023; Jang et al., 2023; Ghosh et al., 2024; Kung and Peng, 2023). For efficient and effective instruction tuning, existing studies (Ouyang et al., 2022; Taori et al., 2023; Zhou et al., 2024a; Xia et al., 2024a) have noticed that improving quality of instruction tuning data (e.g., formulation of well-defined and complete contexts), rather than simply piling up instructions without analysis (e.g., exhaustive collection of open datasets), is of prioritized concerns. In this work, we aim to unify a wide array of data assessment and selection methods under the context of instruction tuning of LLMs. As revealed from the probabilistic view (John and Draper, 1975; Murphy, 2012; Albalak et al., 2024), the statistical patterns inherent in datasets determines the modeling performance. The overall evaluation of instruction datapoints not only deciphers the distribution in various aspects (e.g., composition, task, and domain) and also help cherry-pick the most beneficial subsets for higher performance with less training cost. Through this survey, we demonstrate that: 1) existing resourceful data assessment methods can be categorized into three main perspectives: quality, diversity, and importance (see Fig. 1). 2) a systematic view of selection methods can be unified even they more or less exhibit coupling with the assessment techniques (see Fig. 2). It is noted that quality, diversity, and importance might be used interchangeably without strict discrimination in previous studies. But here we provide a rationalized organization taxonomy for structured elaboration. Despite the goal of being comprehensive, the present survey only provides details of certain typical, representative methods to avoid being tediously long. We hope the in-depth explanations and discussions on the selected methods provide insights into developing robust data assessment and selection pipelines for further studies."}, {"title": "1.1 Related Surveys", "content": "(Liu et al., 2024d) studies the mainstream datasets for building LLMs, including the pre-training corpora, instruction tuning datasets, preference datasets, evaluation benchmarks, and traditional NLP datasets. (Albalak et al., 2024) presents a systematic overview of constructing the data pipeline for language models. Any selection method, either via distribution matching or diversification, can be composed of: 1) utility function; 2) selection mechanism. During different stages of the pipeline (e.g., language filtering, data quality, domain knowledge, deduplication, toxic and explicit content removal, and data mixing), the selection method should be adjusted according to different selection objectives. (Wang et al., 2024a) focuses on the data preparation for instruction tuning. Existing methods on building instruction tuning datasets include: 1) reformulating the discriminative NLP datasets into generative ones; 2) self-instruct with seed prompts; 3) prompt mapping and evol-instruct. Popular methods on dataset selection can be simply classified as: 1) system of indicators; 2) trainable LLMs; 3) powerful LLMs; and 4) small models. (Guo et al., 2022) starts from the general coreset selection method in the field of deep learning and categorize all selection manners into: 1) geometry-based methods (e.g., herding, kcenter-greedy); 2) uncertainty-based methods (e.g., least confidence/entropy/margin); 3) error/loss-based methods (forgetting; GraND/EL2N; importance resampling); 4) decision boundary-based (adversarial deepfool; contrastive active learning); 5) gradient matching-based (gradient approximation towards full set); 6) bi-level optimization-based (inner loop of model optimization and outer loop of datapoint selection); 7) sub-modularity-based (e.g., graph cut; facility location); 8) proxy-based (preference of a small model on data selection). (Zhou et al., 2024b) investigates the potential metrics and aspects for data quality measurement and provides a list of available tools for data evaluation. Apart from data assessment and selection methods that specifically designed for NLP or LLM applications (Moore and Lewis, 2010; Chen et al., 2024a; Dodge et al., 2020; Kandpal et al., 2022; Li et al., 2022; Feng et al., 2021; Lee et al., 2021; Malhotra and Bakal, 2015), there exist many survey studies that tackle general quality measurement in machine learning (Gupta et al., 2021; Zha et al., 2023; Ehrlinger and W\u00f6\u00df, 2022; Mohammed et al., 2024; Li et al., 2024c; Lu et al., 2023b; Dix et al., 2023; Priestley et al., 2023; Byabazaire et al., 2020; Roh et al., 2019; Sidi et al., 2012; Batini et al., 2009) for constructing safe, unbiased, and accurate datasets."}, {"title": "1.2 Survey Scope", "content": "Although \u201cdata evaluation\" has been so frequently mentioned that it appears as a clich\u00e9 problem in developing machine learning algorithms, the optimal solution to establishing an overall data assessment and selection pipeline still remains an open question. Especially under the context of instruction tuning of LLMs, existing studies propose various measurement and cleaning strategies to select the \"high-quality\" instructions from all datapoints. However, very few studies notice that there exists no unified dimensions or aspects in measuring data \"quality\" where previous works tend to put emphasis on the domain-specific and task-dependent characteristics. In addition, the inherent, systematic coupling between data assessment and subset selection methods is not well demonstrated. Under such circumstance, the present study strives to provide a comprehensive review on evaluating and decomposing massive instruction tuning datasets. We categorize the main aspects of data assessment in terms of quality, diversity, and importance. In each aspect, we provide a detailed survey on both traditional (e.g., hand-crafted indicators) and machine learning (e.g., model-based indicators) methods. Besides, the coreset sampling methods that fuses evaluation and selection are introduced separately in diversity and importance oriented subset construction. In consideration of the properties of instruction tuning, we focus on the text modality and start from classical text analysis metrics. Metrics that are either specific to instruction tuning or compatible with pre-training and preference alignment datasets are included since they all share general rules in data assessment. The survey is organized as follows. First, we present the preliminaries for assessment and selection of instruction tuning datasets (Sec. \u00a72). Next, we present the surveying methods of data assessment and selection methods in terms of quality (Sec. \u00a73), diversity (Sec. \u00a74), and importance (Sec. \u00a75). Then, discussions on the existing methods are provided in (Sec. \u00a76), followed by the promising directions for future research (Sec. \u00a77). The final conclusion is given in (Sec. \u00a78)."}, {"title": "2 Preliminaries", "content": "In this section, we briefly introduce the instruction tuning of LLMs and the problem statement for dataset assessment and selection.\nInstruction Dataset Preparation In instruction tuning, each text sample \\( I_i \\) is usually composed of three parts: 1) instruction (either with or without system prompt), 2) input, and 3) response. For an off-the-shelf pre-trained LLM parameterized as \\( \\theta \\), a pre-determined instruction template is used to wrap \\( I_i \\) into the prompt \\( p_i \\) with special tokens like  and  for separation of roles (e.g., system, user, assistant, function, and observation) and their contents. Then, a LLM-associated tokenizer performs tokenization on the instruction prompt \\( p_i \\) for a sequence of \\( x_i = \\{x_{i(1)}, x_{i(2)}, ..., x_{i(n)}\\} \\), where \\( x_{i(j)} \\) denotes the j-th token of \\( x_i \\) and n is the total number of tokens. Out of simplicity, the token sequence \\( x_i \\) can be simply split into two parts by the index t where the content from the role assistant starts: 1) the instruction (input) part (\\( x_{i(<t)} \\)), and 2) the ground-truth response part (\\( x_{i(>t)} \\)).\nInstruction Supervision Given the tokenized in-struction tuning dataset \\( S = \\{x_i\\}_1^n \\), the super-vised tuning is performed via cross-entropy loss:\n\\[\n\\mathcal{L} = \\sum_{x_i \\in S} L_i,\n\\]\n\\[\nL_i = - \\sum_{j=t}^{\\left|x_i\\right|} logP(x_{i(j)}|x_{i(<j)}; \\theta).\n\\tag{1}\n\\]\nFor each \\( x_i \\), the model iteratively predicts the next token given \\( x_{i(j)} \\) all previous tokens including the instruction context and the response completions up to the current token \\( x_{i(< j)} \\).\nData Assessment and Selection We aim at finding the most informative subset \\( S_b \\subset S \\) from the entire set S under the given budget \\( \\left|S_b\\right| \\leq b \\). Mathematically, the selection of \\( S_b \\) requires the quantitative evaluation \\( q(\\cdot) \\) on each datapoint \\( x_i \\) and an elaborated sampling mechanism \\( \\pi \\):\n\\[\nS_b = \\pi(arg \\max_{x_i \\in S} q(x_i), b),\n\\tag{2}\n\\]\nwhere \\( \\pi(., b) \\) denotes the sampling process with a maximum b datapoints. With respect to the detailed implementation of \\( \\pi \\), either an iterative, greedy algorithm or a batch-wise heuristic rule can be adopted for compatibility with \\( q(\\cdot) \\). The expected benefits of such selection include: 1) the reduction of noise by ignoring those mislabeled, mismatched instruction-response pairs, 2) the re-balance of data distributions by down-sampling those easy, common, and similar examples while up-sampling hard, rare ones, and 3) the expedition of training in return for efficient iterations of LLMs."}, {"title": "3 Quality-based Selection", "content": "In this section, we present methods on quality assessment and selection. Without lose of generality, the term \"quality\" here primarily refers to the integrity, accuracy, and rationality of instruction-response datapoints. For integrity, it measures whether the instruction and response are understandable and complete in both format and content. For accuracy, it estimates whether the \"ground-truth\" response truly corresponds to the instruction. For rationality, we focus on the consistency and coherency of the instruction context. Although these three dimensions all contribute to the overall quality, in general, existing methods often formulate a unified scoring mechanism to implicitly consider them partially or comprehensively."}, {"title": "3.1 Hand-crafted Indicators", "content": "Overview Traditional methods develop hand-crafted indicators to evaluate the data quality in terms of linguistic analysis such as vocabulary, syntax, and inter-sample semantic similarity. Each indicator is manually, empirically designed with prior knowledge on the language, domain, and task of the corpus under investigation. The calculation of each indicator is explicitly defined and does not require training and inference of proxy models or language models. Although the indicators are hand-crafted, deep learning models such as sentence encoders might be leveraged to extract embedding representations for each instruction text. For the datapoint \\( x_i \\), its indicator \\( IND_i \\) can be typically defined as:\n\\[\nIND_i = f(IND^1(x_i), IND^2(x_i), \\\\\nIND^3(x_i), ...IND^M(x_i)),\n\\tag{3}\n\\]\nwhere M denotes the total number of indicators and \\( f \\) is the aggregation function which depends on both the instruction task and dataset. One can simply use linear combination with pre-defined or dynamically adjusted weights while meticulous tuning might be needed for the ultimate \\( f \\). Given the indicators \\( IND_i \\) for each \\( x_i \\), two intuitive selection methods can be adopted: 1) to filter out datapoints whose indicator scores are below a pre-defined threshold; 2) to keep only the samples whose indicator scores rank within a certain range of percentiles. Mathematically, these two selection mechanism can be respectively represented as:\n\\[\nS_b = \\{x_i|T_{min} < f(x_i) < T_{max}, 1 \\leq i \\leq N\\},\n\\tag{4}\n\\]\n\\[\nS_b = \\{x_i|P_{min} < F_f(f(x_i)) < P_{max}, 1 \\leq i \\leq N\\},\n\\tag{5}\n\\]\nwhere \\( T_{min} \\) and \\( T_{max} \\) respectively denote the left and right threshold boundaries. The estimated \\( F_f \\) is the empirical cumulative distribution function of all indicators \\( f \\). \\( P_{min} \\) and \\( P_{max} \\) respectively refer to the minimum and maximum percentile for enclosing the selection range. In practice, both the threshold and percentiles are hyper-parameters that require task-specific fine-tuning.\nTechnical Details (Mishra et al., 2020b) and (Mishra et al., 2020a) introduce a data quality metric, namely the DQI, to quantify the differences between successive benchmarks by giving high scores to generalizable samples and low scores to biased samples. Such a metric implies whether"}, {"title": "3.2 Model-based Indicators", "content": "Overview The model-based indicators, on the other hand, leverage trainable models to predict the indicators for each instruction datapoint. The trainable models used for data quality measurement can either share the same or similar architecture with the language model under development, or possesses completely different implementation choices. Accordingly, these indicators can be simply defined as:\n\\[\nIND_i = f(IND_{\\Theta}^1(x_i), IND_{\\Theta}^2(x_i), \\\\\nIND_{\\Theta}^3(x_i), ...IND_{\\Theta}^M(x_i)),\n\\tag{6}\n\\]\nwhere the learnable parameters \\( \\Theta \\) highlight the difference between model-based and hand-crafted indicators. Based on the computed indicators, similar selection mechanisms (Eqs. 45) can be adopted to select favorable datapoints.\nTechnical Details One of the most intuitive model-based indicators is perplexity (Shannon, 2001; Jelinek et al., 1977; Jelinek, 1980). It is frequently mentioned as the evaluation metric for pre-trained language models (Penedo et al., 2023; Radford et al., 2018, 2019; Brown et al., 2020; Achiam et al., 2023) but can also be employed as a data quality indicator. (Ankner et al., 2024) proposes to use a small GPT-style reference model such as MPT 125M (Team, 2023) to prune dataset via perplexity-based sampling for training a 3B model. Specifically, for any datapoint \\( x_i \\), the perplexity is defined as the exponential of negative likelihood with base of 2:\n\\[\nNLL_i = \\frac{1}{|x_i|} \\sum_{j=1}^{|x_i|} -logP(x_{i(j)}|x_{i(<j)}; \\theta)\n\\]\n\\[\nPPLX_i = 2^{NLL_i}\n\\tag{7}\n\\]\nBased on the perplexity inferred from a small model, samples at the high and medium percentiles are chosen by Eq. 5 for downstream fine-tuning. (Deng et al., 2021) develops a unified evaluator framework to score the generated outputs for natural language generation tasks. A ROBERTa-based (Liu et al., 2019) discriminator learns to score responses in terms of consistency, relevance, preservation, engagingness, and groundedness. One could simply adopt such a discriminator for evaluation of the instruction-response pairs. (Zhong et al., 2022) further proposes a multi-dimensional scoring evaluator. For each evaluation dimension, the original ground-truth instruction-response pairs are converted into positive samples in the form of boolean question-answer problems. The negative samples are respectively constructed via rule-based transformation. The evaluator itself is implemented as T5 model (Raffel et al., 2020) and trained on these positive and negative samples for scoring in the range from 0 to 1. (Jiang et al., 2024c) prunes the UltraChat (Ding et al., 2023) dataset by scoring each datapoint by learning complexity of a pre-trained Qwen-1.8B model (Bai et al., 2023). Specifically, the learning complexity is calculated as the averaged prediction confidence of different subnets:\n\\[\n\\bar{\\mathcal{S}}(x_i) = \\frac{1}{I} \\sum_{j=1}^{I} PPLX_{x_i};\\Theta^j,\n\\tag{8}\n\\]\nwhere I is the number of subnets. Each subnet \\( \\Theta^j \\) is obtained by adjusting the dropout rate from 10% to 90% on the original \\( \\Theta \\) of any pre-trained language model. Instruction datapoints with small \\( \\bar{\\mathcal{S}}(x_i) \\) are easy ones and should be kept first during pruning. Both (Bukharin and Zhao, 2023) and (Du et al., 2023) employ reward models to assess the quality of each instruction pairs. They respectively utilize the raft model (Dong et al., 2023) and the deberta-v3-large-v2 for reward scoring:\n\\[\n\\mathcal{R}_i = r_{\\theta}(x_{i(<t)}, x_{i(\\geq t)}),\n\\tag{9}\n\\]\nwhere \\( r_{\\theta} \\) denotes the reward model. t is the index where \\( x_{i(<t)} \\) and \\( x_{i(\\geq t)} \\) respectively denote the instruction Q and response A. (Marion et al., 2023) investigates three classic metrics in clean set selection (Guo et al., 2022; Song et al., 2022; Natarajan et al., 2013; Qin et al., 2024): perplexity (Eq. 7), error \\( \\ell_2 \\)-Norm (EL2N) (Paul et al., 2021), and memorization ranking (Biderman et al., 2024). Specifically, EL2N is defined as:\n\\[\nEL2N_i = \\sum_{j=1}^{\\left|x_i\\right|} \\|P(x_{i(<j)}; \\Theta) -y_{i(j)} \\|_2,\n\\tag{10}\n\\]\nwhere \\( y_{i(j)} \\in \\mathbb{R}^{N_{vocab}} \\) denotes the one-hot vector of the vocabulary size \\( N_{vocab} \\), where its element indexed at \\( x_{i(j)} \\) equals one. The memorization ranking is represented as:\n\\[\nM E M_i = \\frac{1}{N_{win}} \\sum_{j=1}^{N_{win}} \\mathbb{1} (\\hat{x}_{(M_{offset}+j)} = x_{(M_{offset}+j)}),\n\\tag{11}\n\\]\nwhere \\( N_{win} \\) denotes the length of a consecutive sequence and \\( M_{offset} \\) is an offset of the starting index. The \\( \\hat{x}_{(M_{offset}+j)} \\) refers to the generated token given input \\( x_{i(<M_{offset}+j)} \\), and \\( x_{(M_{offset}+j)} \\) is its ground-truth. (Cao et al., 2023) combines both hand-crafted indicators (e.g., input length, output length, MTLD (McCarthy and Jarvis, 2010), and kNN-i (Dong et al., 2011)) and model-based indicators (e.g., reward score, perplexity, and UniEval metrics (Zhong et al., 2022)) for fitting the loss of a LLM on the evaluation set. The linear regression model is optimized via least squares method (Bjork, 1988) and the optimal selection of instruction data is achieved via BlendSearch (Wang et al., 2021a,b) for minimizing the estimated evaluation loss. (Li et al., 2023a) is one of the most pioneering works that leverages the target language model itself to perform self-guided data selection. The language model is first \u201cwarmed-up\" with very few samples randomly chosen from the pool to learn from brief experience. Then, such an experienced model evaluates each instruction-response pair via the instruction-following difficulty (IFD) score. The IFD score measures how much guidance or assistance the instruction provides to the generation of ground-truth response, by comparing the loss of causal language modeling on the response with and without instruction:\n\\[\nIFD = \\frac{NLL_A}{NLL_{A|Q}}\n\\]\n\\[\nNLL = \\frac{1}{\\left|x_{i(\\geq t)}\\right|} \\sum_{x_{i(\\geq t)} j=t}^{\\left|x_i\\right|} -logP(x_{i(j)}|x_{i(<j)}; \\Theta),\n\\]\n\\[\nNLL_{A|Q} = \\frac{1}{\\left|x_{i(\\geq t)}\\right|} \\sum_{x_{i(\\geq t)} j=t}^{\\left|x_i\\right|} -logP(x_{i(j)}|x_{i(t<,<j)}; \\Theta),\n\\tag{12}\n\\]\nwhere the index t splits apart the instruction Q and the response A. Samples whose IFD scores over \\( T_{max} = 1 \\) are invalid datapoints with misaligned, mismatched instruction-response pairs. The empirical setting of \\( \\tau_{min} \\) affects the trade-off between quality and diversity of the selected datapoints."}, {"title": "3.3 GPT Score", "content": "Overview The invoking of OpenAI APIs (Tingiris and Kinsella, 2021; Lappalainen and Narayanan, 2023; Sun et al., 2023; Kublik and Saboo, 2023) for ChatGPT services (e.g., GPT3.5, GPT4) allows automatic scoring of instruction tuning datasets. Recent studies on bringing LLMs as judges (Zheng et al., 2024; Wang et al., 2023a; Zhu et al., 2023; Huang et al., 2024; Zeng et al., 2023; Chan et al., 2023) reveal that powerful language models like ChatGPT highly align with human preference on judging the quality of instructions and responses. Given a well-designed prompt with clear definition on grading criteria, ChatGPT produces justified quality scorings with explanations:\n\\[\nGPTScore_i = G(x_i, P_G),\n\\tag{18}\n\\]\nwhere \\( P_G \\) denotes the prompt template that defines the task and grading scheme with format constraints on outputs. G represents the quality score parsed from the GPT response. Samples with high GPTScore can be selected using Eqs. 4 and 5."}, {"title": "3.4 Human Evaluation", "content": "Overview Human annotation and evaluation is indispensable in constructing preference alignment datasets (Wang et al., 2023b; Ouyang et al., 2022) for helpfulness, honesty, and harmlessness. Specifically, human annotators deliver grading results following specific criteria in multiple dimensions:\n\\[\nLabelScore_i = f(LabelScore^1(x_i), \\\\\nLabelScore^2(x_i), ..., LabelScore^M(x_i)),\n\\tag{19}\n\\]\nwhere \\( LabelScore^m(x_i) \\) can be both bool or integer (e.g., range from 0 to 5) for the m-th fine-grained aspect. The aggregation function \\( f \\) is commonly chosen as summation or averaging.\nTechnical Details The OpenAssistant (K\u00f6pf et al., 2024) dataset is featured by its high-quality human-generated, human-annotated multi-lingual conversations for both instruction tuning and reinforcement learning from human feedback (see the guidelines excerpts 3.4). For each instruction-response pair along the conversation tree, the human annotators are asked to categorize them according to three dimensions: spam detection, guideline adherence, and quality. The quality score is rated on a five-point Likert scale across aspects including quality, creativity, humorousness, politeness, and harmlessness. These scores are used to sort instructions for analysis and preference optimization of LLMs. (Lu et al., 2023a) enrolls human annotators to provide judgements on the tagging of each instruction. To verify the quality scores provided by humans, counterfactual cases are prepared respectively for precision and consistency tasks. Results show that human annotators have low false positive rates at tagging precision, but lack proof of confidence on their original quality judgements. (Zhou et al., 2024a) proposes to use human annotators for creation of small-yet-effective instruction datasets. To collect questions and answers from various sources, simple hand-crafted indicators such as text length are used to filter low-quality datapoints. Then, high quality instruction-response pairs are manually selected (750) and written (250) via subjective quality control. The databricks-dolly dataset (Conover et al., 2023) contains 15K human-generated instruction-response pairs. Although quality is emphasized during large-scale annotation, imperfect samples still exist where low-quality and inaccurate responses, incomplete and vague instructions, problematic texts with toxic language and grammar errors are found (He et al., 2024).\nRemark Human evaluation play a irreplaceable role in quality control of preference alignment. To reduce the inter-annotator inconsistency, detailed guidelines should be prepared for quality measurement. In addition, supplementary quality measures such as GPT-Scores can be provided for manually evaluating and selecting high-quality datasets."}, {"title": "4 Diversity-based Selection", "content": "In this section, we introduce methods that emphasize the diversity of instruction datasets. When it comes to diversity, existing researches either measure the individual diversity of each sample (e.g., lexical and semantic richness) or the overall diversity of the entire dataset (e.g., the volume of the enclosed embedding space). Instruction datapoints whose tasks and domains are of minority classes in a long-tailed distribution are preferred during subset selection. Such sampling philosophy strikes to maintain or approximate the spread of the original embedding clusters but with much less sparsity."}, {"title": "4.1 Hand-crafted Indicators", "content": "Overview The diversity of datasets is the key to develop less biased", "as": "n\\[\nTTR = \\frac{|Unique(x_i)|"}, {"x_i|},\n\\tag{20}\n\\": "nwhere \\( Unique(x_i) \\) denotes the set of unique tokens present in \\( x_i \\). To reduce the sensitivity of TTR to the variation of text length", "is": "n\\[\nTTR_k = \\frac{|Unique(x_{i(j:", "j+k))}|}{|x_{i(j": "j+k)"}, 1, "leq j \\leq |l-k\n\\tag{21}\n\\"], "x_{i(j": "j+k)"}, ["nTTR^k = \\tau[(\\frac{D}{k}+1)^{-1}"], {"x_{i(j": "j+k)"}, {"as": "n\\[\nMTLD = \\frac{\\sum_{i=1"}, {"k": "n\\[\nHD-D_i = \\frac{1"}, {"x_{i(j": "j+k)}, \\forall k > 0,\\forall 1 \\leq j \\leq |x_i| - k.\n\\]\nOther variants of TTR indicators such as MT-TRSS (Malvern et al., 2004"}]