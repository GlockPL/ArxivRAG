{"title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models", "authors": ["Yulei Qin", "Yuncheng Yang", "Pengcheng Guo", "Gang Li", "Hang Shao", "Yuchen Shi", "Zihan Xu", "Yun Gu", "Ke Li", "Xing Sun"], "abstract": "Instruction tuning plays a critical role in aligning large language models (LLMs) with human preference. Despite the vast amount of open instruction datasets, naively training a LLM on all existing instructions may not be optimal and practical. To pinpoint the most beneficial datapoints, data assessment and selection methods have been proposed in the fields of natural language processing (NLP) and deep learning. However, under the context of instruction tuning, there still exists a gap in knowledge on what kind of data evaluation metrics can be employed and how they can be integrated into the selection mechanism. To bridge this gap, we present a comprehensive review on existing literature of data assessment and selection especially for instruction tuning of LLMs. We systematically categorize all applicable methods into quality-based, diversity-based, and importance-based ones where a unified, fine-grained taxonomy is structured. For each category, representative methods are elaborated to describe the landscape of relevant research. In addition, comparison between latest methods is conducted on their officially reported results to provide in-depth discussions on their limitations. Finally, we summarize the open challenges and propose the promosing avenues for future studies. All related contents are available at https://github.com/yuleiqin/ fantastic-data-engineering.", "sections": [{"title": "1 Introduction", "content": "One of the ultimate goal of developing large Inguage models (LLMs) is to unlock their potentials of generalization to unseen natural language processing (NLP) tasks. Towards this goal, a series of LLMs such as GPTs (Brown et al., 2020; Achiam et al., 2023), LLaMAs (Touvron et al., 2023a,b; AI@Meta, 2024), and Mistrals (Jiang et al., 2023a, 2024a) have delivered high-level text understanding and generation capabilities via utilizing vast amount of high-quality web and human-annotated datasets for pre-training and preference alignment (Liu et al., 2023a, 2024c; Sun et al., 2024b; Edunov et al., 2019; Dong et al., 2019). During preference alignment, instruction tuning plays an important role in refining pre-trained LLMs to provide accurate, pertinent, and harmless responses on a collection of downstream tasks (Wei et al., 2021; Sanh et al., 2021; Zhang et al., 2023c; Peng et al., 2023; Longpre et al., 2023; Shu et al., 2023; Jang et al., 2023; Ghosh et al., 2024; Kung and Peng, 2023). For efficient and effective instruction tuning, existing studies (Ouyang et al., 2022; Taori et al., 2023; Zhou et al., 2024a; Xia et al., 2024a) have noticed that improving quality of instruction tuning data (e.g., formulation of well-defined and complete contexts), rather than simply piling up instructions without analysis (e.g., exhaustive collection of open datasets), is of prioritized concerns.\nIn this work, we aim to unify a wide array of data assessment and selection methods under the context of instruction tuning of LLMs. As revealed from the probabilistic view (John and Draper, 1975; Murphy, 2012; Albalak et al., 2024), the statistical patterns inherent in datasets determines the modeling performance. The overall evaluation of instruction datapoints not only deciphers the distribution in various aspects (e.g., composition, task, and domain) and also help cherry-pick the most beneficial subsets for higher performance with less training cost. Through this survey, we demonstrate that: 1) existing resourceful data assessment methods can be categorized into three main perspectives: quality, diversity, and importance (see Fig. 1). 2) a systematic view of selection methods can be unified even they more or less exhibit coupling with the assessment techniques (see Fig. 2). It is noted that quality, diversity, and importance might be used interchangeably without strict discrimination in previous studies. But here we provide a rationalized organization taxonomy for structured elaboration. Despite the goal of being comprehensive,"}, {"title": "1.1 Related Surveys", "content": "(Liu et al., 2024d) studies the mainstream datasets for building LLMs, including the pre-training corpora, instruction tuning datasets, preference datasets, evaluation benchmarks, and traditional NLP datasets. (Albalak et al., 2024) presents a systematic overview of constructing the data pipeline for language models. Any selection method, either via distribution matching or diversification, can be composed of: 1) utility function; 2) selection mechanism. During different stages of the pipeline (e.g., language filtering, data quality, domain knowledge, deduplication, toxic and explicit content removal, and data mixing), the selection method should be adjusted according to different selection objectives. (Wang et al., 2024a) focuses on the data preparation for instruction tuning. Existing methods on building instruction tuning datasets include: 1) reformulating the discriminative NLP datasets into generative ones; 2) self-instruct with seed prompts; 3) prompt mapping and evol-instruct. Popular methods on dataset selection can be simply classified as: 1) system of indicators; 2) trainable LLMs; 3) powerful LLMs; and 4) small models. (Guo et al., 2022) starts from the general coreset selection method in the field of deep learning and categorize all selection manners into: 1) geometry-based methods (e.g., herding, kcenter-greedy); 2) uncertainty-based methods (e.g., least confidence/entropy/margin); 3) error/loss-based methods (forgetting; GraND/EL2N; importance re-sampling); 4) decision boundary-based (adversarial deepfool; contrastive active learning); 5) gradient matching-based (gradient approximation towards full set); 6) bi-level optimization-based (inner loop of model optimization and outer loop of datapoint selection); 7) sub-modularity-based (e.g., graph cut; facility location); 8) proxy-based (preference of a small model on data selection). (Zhou et al., 2024b) investigates the potential metrics and aspects for data quality measurement and provides a list of available tools for data evaluation. Apart from data assessment and selection methods that specifically designed for NLP or LLM applications (Moore and Lewis, 2010; Chen et al., 2024a; Dodge et al., 2020; Kandpal et al., 2022; Li et al., 2022; Feng et al., 2021; Lee et al., 2021; Malhotra and Bakal, 2015), there exist many survey studies that tackle general quality measurement in machine learning (Gupta et al., 2021; Zha et al., 2023; Ehrlinger and W\u00f6\u00df, 2022; Mohammed et al., 2024; Li et al., 2024c; Lu et al., 2023b; Dix et al., 2023; Priestley et al., 2023; Byabazaire et al., 2020; Roh et al., 2019; Sidi et al., 2012; Batini et al., 2009) for constructing safe, unbiased, and accurate datasets."}, {"title": "1.2 Survey Scope", "content": "Although \u201cdata evaluation\" has been so frequently mentioned that it appears as a clich\u00e9 problem in developing machine learning algorithms, the optimal solution to establishing an overall data assessment and selection pipeline still remains an open question. Especially under the context of instruction tuning of LLMs, existing studies propose various measurement and cleaning strategies to select the \"high-quality\" instructions from all datapoints. However, very few studies notice that there exists no unified dimensions or aspects in measuring data \"quality\" where previous works tend to put emphasis on the domain-specific and task-dependent characteristics. In addition, the inherent, systematic coupling between data assessment and subset selection methods is not well demonstrated.\nUnder such circumstance, the present study strives to provide a comprehensive review on evaluating and decomposing massive instruction tuning datasets. We categorize the main aspects of data assessment in terms of quality, diversity, and importance. In each aspect, we provide a detailed survey on both traditional (e.g., hand-crafted indicators) and machine learning (e.g., model-based indicators) methods. Besides, the coreset sampling methods that fuses evaluation and selection are introduced separately in diversity and importance oriented subset construction. In consideration of the properties of instruction tuning, we focus on the text modality and start from classical text analysis metrics. Metrics that are either specific to instruction tuning or compatible with pre-training and preference alignment datasets are included since they all share general rules in data assessment.\nThe survey is organized as follows. First, we present the preliminaries for assessment and selection of instruction tuning datasets (Sec. \u00a72). Next, we present the surveying methods of data assessment and selection methods in terms of quality (Sec. \u00a73), diversity (Sec. \u00a74), and importance (Sec. \u00a75). Then, discussions on the existing methods are provided in (Sec. \u00a76), followed by the promising directions for future research (Sec. \u00a77). The final conclusion is given in (Sec. \u00a78)."}, {"title": "2 Preliminaries", "content": "In this section, we briefly introduce the instruction tuning of LLMs and the problem statement for dataset assessment and selection.\nInstruction Dataset Preparation In instruction tuning, each text sample I\u1d62 is usually composed of three parts: 1) instruction (either with or without system prompt), 2) input, and 3) response. For an off-the-shelf pre-trained LLM parameterized as \u03b8, a pre-determined instruction template is used to wrap I\u1d62 into the prompt p\u1d62 with special tokens like \"<|im_start|>\" and \u201c<|im_end|>\" for separation of roles (e.g., system, user, assistant, function, and observation) and their contents. Then, a LLM-associated tokenizer performs tokenization on the instruction prompt p\u1d62 for a sequence of x\u1d62 = {x\u1d62(\u2081), x\u1d62(\u2082), ..., x\u1d62(\u2099)}, where x\u1d62(\u2c7c) denotes the j-th token of x\u1d62 and n is the total number of tokens. Out of simplicity, the token sequence x\u1d62 can be simply split into two parts by the index t where the content from the role assistant starts: 1) the instruction (input) part (x\u1d62(\u2080)), and 2) the ground-truth response part (x\u1d62(>\u209c)).\nInstruction Supervision Given the tokenized in-struction tuning dataset S = {x\u1d62}\u1d62=\u2081, the super-"}, {"title": "3 Quality-based Selection", "content": "In this section, we present methods on quality assessment and selection. Without lose of generality, the term \"quality\" here primarily refers to the integrity, accuracy, and rationality of instruction-response datapoints. For integrity, it measures whether the instruction and response are understandable and complete in both format and content. For accuracy, it estimates whether the \"ground-truth\" response truly corresponds to the instruction. For rationality, we focus on the consistency and coherency of the instruction context. Although these three dimensions all contribute to the overall quality, in general, existing methods often formulate a unified scoring mechanism to implicitly consider them partially or comprehensively."}, {"title": "3.1 Hand-crafted Indicators", "content": "Overview Traditional methods develop hand-crafted indicators to evaluate the data quality in terms of linguistic analysis such as vocabulary, syntax, and inter-sample semantic similarity. Each indicator is manually, empirically designed with prior knowledge on the language, domain, and task of the corpus under investigation. The calculation of each indicator is explicitly defined and does not require training and inference of proxy models or language models. Although the indicators are hand-crafted, deep learning models such as sentence encoders might be leveraged to extract embedding representations for each instruction text. For the datapoint x\u1d62, its indicator IND\u1d62 can be typically defined as:\nIND\u1d62 = f(IND\u00b9(x\u1d62), IND\u00b2(x\u1d62),\nIND\u00b3(x\u1d62), ...IND\u1d39(x\u1d62)), (3)\nwhere M denotes the total number of indicators and f is the aggregation function which depends on both the instruction task and dataset. One can simply use linear combination with pre-defined or dynamically adjusted weights while meticulous tuning might be needed for the ultimate f. Given the indicators IND\u1d62 for each x\u1d62, two intuitive selection methods can be adopted: 1) to filter out datapoints whose indicator scores are below a pre-defined threshold; 2) to keep only the samples whose indicator scores rank within a certain range of percentiles. Mathematically, these two selection mechanism can be respectively represented as:\nS\u266d = {x\u1d62|T\u2098\u1d62\u2099 < f(x\u1d62) < T\u2098\u2090\u2093, 1 \u2264 i \u2264 N}, (4)\nS\u266d = {x\u1d62|P\u2098\u1d62\u2099 < Ff(f(x\u1d62)) < P\u2098\u2090\u2093, 1 \u2264 i \u2264 N}, (5)\nwhere T\u2098\u1d62\u2099 and T\u2098\u2090\u2093 respectively denote the left and right threshold boundaries. The estimated Ff is the empirical cumulative distribution function of all indicators f. P\u2098\u1d62\u2099 and P\u2098\u2090\u2093 respectively refer to the minimum and maximum percentile for enclosing the selection range. In practice, both the threshold and percentiles are hyper-parameters that require task-specific fine-tuning.\nTechnical Details (Mishra et al., 2020b) and (Mishra et al., 2020a) introduce a data quality metric, namely the DQI, to quantify the differences between successive benchmarks by giving high scores to generalizable samples and low scores to biased samples. Such a metric implies whether"}, {"title": "3.2 Model-based Indicators", "content": "Overview The model-based indicators, on the other hand, leverage trainable models to predict the indicators for each instruction datapoint. The trainable models used for data quality measurement can either share the same or similar architecture with the language model under development, or possesses completely different implementation choices. Accordingly, these indicators can be simply defined as:\nIND\u1d62 = f(IND\u2092\u00b9(x\u1d62), IND\u2092\u00b2(x\u1d62),\nIND\u2092\u00b3(x\u1d62), ...IND\u2092\u1d39(x\u1d62)), (6)\nwhere the learnable parameters \u03b8 highlight the difference between model-based and hand-crafted indicators. Based on the computed indicators, similar selection mechanisms (Eqs. 45) can be adopted to select favorable datapoints.\nTechnical Details One of the most intuitive model-based indicators is perplexity (Shannon, 2001; Jelinek et al., 1977; Jelinek, 1980). It is frequently mentioned as the evaluation metric for pre-trained language models (Penedo et al., 2023; Radford et al., 2018, 2019; Brown et al., 2020; Achiam et al., 2023) but can also be employed as a data quality indicator. (Ankner et al., 2024) proposes to use a small GPT-style reference model such as MPT 125M (Team, 2023) to prune dataset via perplexity-based sampling for training a 3B model. Specifically, for any datapoint x\u1d62, the perplexity is defined as the exponential of negative likelihood with base of 2:\nNLL\u1d62 = 1/|x\u1d62| \u2211\u1d62 =\u2081 -logP(x\u1d62(\u2c7c)|x\u1d62(\u2080); \u03b8)\nPPLX\u1d62 = 2\u1d3a\u1d38\u1d38\u1d62 (7)\nBased on the perplexity inferred from a small model, samples at the high and medium percentiles are chosen by Eq. 5 for downstream fine-tuning. (Deng et al., 2021) develops a unified evaluator framework to score the generated outputs for natural language generation tasks. A ROBERTa-based (Liu et al., 2019) discriminator"}, {"title": "3.3 GPT Score", "content": "Overview The invoking of OpenAI APIs (Tingiris and Kinsella, 2021; Lappalainen and Narayanan, 2023; Sun et al., 2023; Kublik and Saboo, 2023) for ChatGPT services (e.g., GPT3.5, GPT4) allows automatic scoring of instruction tuning datasets. Recent studies on bringing LLMs as judges (Zheng et al., 2024; Wang et al., 2023a; Zhu et al., 2023; Huang et al., 2024; Zeng et al., 2023; Chan et al., 2023) reveal that powerful language models like ChatGPT highly align with human preference on judging the quality of instructions and responses. Given a well-designed prompt with clear definition on grading criteria, ChatGPT produces justified quality scorings with explanations:\nGPTScore\u1d62 = G(x\u1d62, PG), (18)\nwhere PG denotes the prompt template that defines the task and grading scheme with format constraints on outputs. G represents the quality score parsed from the GPT response. Samples with high GPTScore can be selected using Eqs. 4 and 5."}, {"title": "3.4 Human Evaluation", "content": "Overview Human annotation and evaluation is indispensable in constructing preference alignment datasets (Wang et al., 2023b; Ouyang et al., 2022) for helpfulness, honesty, and harmlessness. Specifically, human annotators deliver grading results following specific criteria in multiple dimensions:\nLabelScore\u1d62 = f(LabelScore\u00b9(x\u1d62), LabelScore\u00b2(x\u1d62), ..., LabelScore\u1d39(x\u1d62)), (19)\nwhere LabelScore\u1d50 (x\u1d62) can be both bool or integer (e.g., range from 0 to 5) for the m-th fine-grained aspect. The aggregation function f is commonly chosen as summation or averaging.\nTechnical Details The OpenAssistant (K\u00f6pf et al., 2024) dataset is featured by its high-quality human-generated, human-annotated multi-lingual conversations for both instruction tuning and reinforcement learning from human feedback (see the guidelines excerpts 3.4). For each instruction-response pair along the conversation tree, the human annotators are asked to categorize them according to three dimensions: spam detection, guideline adherence, and quality. The quality score is rated on a five-point Likert scale across aspects including quality, creativity, humorousness, politeness, and harmlessness. These scores are used to sort instructions for analysis and preference optimization of LLMs. (Lu et al., 2023a) enrolls human annotators to provide judgements on the tagging of each instruction. To verify the quality scores provided by humans, counterfactual cases are prepared respectively for precision and consistency tasks. Results show that human annotators have low false positive rates at tagging precision, but lack proof of confidence on their original quality judgements. (Zhou et al., 2024a) proposes to use human annotators for creation of small-yet-effective instruction datasets. To collect questions and answers from various sources, simple hand-crafted indicators such as text length are used to filter low-quality datapoints. Then, high quality instruction-response pairs are manually selected (750) and written (250) via subjective quality control. The databricks-dolly dataset (Conover et al., 2023) contains 15K human-generated instruction-"}, {"title": "4 Diversity-based Selection", "content": "In this section, we introduce methods that emphasize the diversity of instruction datasets. When it comes to diversity, existing researches either measure the individual diversity of each sample (e.g., lexical and semantic richness) or the overall diversity of the entire dataset (e.g., the volume of the enclosed embedding space). Instruction datapoints whose tasks and domains are of minority classes in a long-tailed distribution are preferred during subset selection. Such sampling philosophy strikes to maintain or approximate the spread of the original embedding clusters but with much less sparsity."}, {"title": "4.1 Hand-crafted Indicators", "content": "Overview The diversity of datasets is the key to develop less biased, more generalizable machine learning models. However, recent studies (Zhao et al., 2024c,b) show that existing vision and language datasets do not share a unified and concrete definition of diversity in terms of dataset composition, source, domain, subject, annotator, and promote (fairness). With respect to the diversity measures specific in instruction tuning datasets, hand-crafted indicators, similar to Eq. 3 in traditional NLP studies, can be used as a good starting point.\nTechnical Details One of the most popular diversity measure is lexical diversity, which refers to the range of different words occurring in one text. The greater range implies greater diversity and quality. Type-token ratio (TTR) (Templin, 1957; Richards, 1987) is originally proposed as:\nTTR = |Unique(x\u1d62)| / |x\u1d62|, (20)\nwhere Unique(x\u1d62) denotes the set of unique tokens present in x\u1d62. To reduce the sensitivity of TTR to the variation of text length, several studies (Covington and McFall, 2008, 2010; Kettunen, 2014; Matlach et al., 2021) standardized the length by introducing logarithms or n-grams into the formula.\nLater, computational approaches to measure lexical diversity have been developed such as vocabulary diversity (vocd-D) (Malvern and Richards, 1997; Malvern et al., 2004; Silverman and Ratner, 2002; deBoer, 2014), the measure of textual lexical diversity (MTLD) (McCarthy and Jarvis, 2010; Jarvis and Daller, 2013), and hypergeometric distribution diversity (HD-D) (Jarvis, 2013; McCarthy, 2005). All these metrics require multi-step computation for approximation. Specifically for vocd-D, random sampling is first performed on x\u1d62 for a series of sub-sequences with varying lengths k (e.g., 10, 20, 30 tokens). Then, TTR\u2096 is:\nTTR\u2096 = |Unique(x\u1d62(\u2c7c<, <\u2c7c+\u2096))| / |x\u1d62(\u2c7c<, <\u2c7c+\u2096)|, 1\u2264 j \u2264 |xi| \u2212 k. (21)\nwhere x\u1d62(\u2c7c<, <\u2c7c+\u2096)) denotes the sub-sequence of x\u1d62 starting from the randomly chosen index j and ending at the index j+k. Then, the curve of TTR versus the lengths k is plotted and a mathematical model is built for fitting the curve:\nTTR\u2096 = D/\u2096 * [(1 + D/\u2096)\u22121], (22)\nwhere D is the only parameter required to be estimated. By approximating TTR\u2096 towards TTR with the least squares, we have Dbest fit = D:\nvocd-Di = D. (23)\nA larger D reflects the higher diversity of x\u1d62. The computation of MTLD, on the other hand, first determines the TTR\u2081 as a pre-defined threshold, and then partitions x\u1d62 into M different contiguous subsequences {x, x, ..., xm, ..., x\u1d39}. Each subsequence x = x\u1d62(\u2c7c<, <\u2c7c+\u2096),\u2200k > 0,\u22001 \u2264 j \u2264 |xi| \u2212 k maintains a TTR above the threshold TTR\u2081. The MTLD is defined as:\nMTLD = 1/M * \u2211\u1d62 =\u2081 \u1d4f/|x\u1d62\u1d4f|. (24)\nThe HD-D shares the same idea behind vocd-D but stems from the hypergeometric distribution (McCarthy and Jarvis, 2010). With M-times sampling, the HD-D represents the probability of drawing a certain number of tokens of the given type from the subsequence of x\u1d62 with a particular size k:\nHD-Di = \u2211\u2096\u2208\u2099 \u2211\u1d58\u209c\u2208\u1d64\u2099\u1d62\u2097\u2098\u1d64\u2097 |{xi(n)}| / |x\u1d62\u1d4f|. (25)\nOther variants of TTR indicators such as MTTRSS (Malvern et al., 2004), MSTTR (Malvern et al., 2004), MATTR (Covington and McFall, 2010), and MTLD-W (Vidal and Jarvis, 2020; Kyle et al., 2021) all target at improving the solutions to two fundamental problems (Bestgen, 2023): 1) the sensitivity of indicators to text length, and 2) the impact of the indicator parameters. (Li et al., 2015) proposes two rather simplified TTR scores as distinct-1 and distinct-2, where the number of distinct unigrams and bigrams of rare respectively divided by the total number of words. Many other studies (Cao and Clark, 2017; Zhu et al., 2018; Shu et al., 2019; Tevet and Berant, 2020) extend the application of n-gram-based diversity measures for model-generated responses.\nApart from lexical diversity, there exists many efficient diversity indicators that are built upon the semantics of each example. (Dong et al., 2011) proposes to approximate k-nearest neighbor (k-NN) graph (Peterson, 2009) with arbitrary similarity measures on semantic embeddings of large-scale datasets. Such efficient construction of a k-NN graph allows the distance of x\u1d62 to its j-th nearest neighbors to be a feasible diversity measure:\nkNN\u1d62 = d(g(x\u1d62), g(N\u2c7c(x\u1d62))), (26)\nwhere Nj(x\u1d62) denotes the j-th closest neighbor of x\u1d62 in the embedding space projected by g(\u00b7). The common choices of the distance function d(, \u00b7) include the Euclidean distance, cosine distance, and Jaccard coefficient distance (Huang et al., 2008). The projection from text (e.g., instruction-response pairs) into the embedding space can be achieved with pre-trained sentence BERT (Reimers and Gurevych, 2019; Feng et al., 2020), where an additional pooling operation is performed on the final output of BERT (Devlin et al., 2018) for sentence embeddings. Note that a higher kN N\u1d62 implies that the sample x\u1d62 is more unique and should be kept in subset selection for higher diversity. Due to the fine-grained representation capability of BERT, existing hand-crafted indicators often rely"}, {"title": "4.2 Model-based Indicators", "content": "Overview Similar to Eq. 6, model-based indicators on diversity also rely on the target or proxy language model for computing the indices.\nTechnical Details The diversity of a dataset S can be intuitively defined as the sum of rarity measures of each constituting element x\u1d62. Accordingly, entropy-related methods are proposed to estimate such rarity. The more uncommon, various samples exist, the higher diversity the dataset becomes. Mathematically, the vanilla entropy (Shannon, 1948) is proposed for diversity measures:\nDentropy (S) = - \u2211\u1d62\u2208s P(x\u1d62|\u03b8) \u00b7 log\u2082(P(x\u1d62|\u03b8)), (32)\nwhere P(x\u1d62) denotes the probability of x\u1d62 occurring in the dataset. Later, R\u00e9nyi entropy (R\u00e9nyi, 1961) introduces an additional parameter \u03b1 > 0, \u03b1 \u2260 1 for a generalized entropy definition:\nD\u1d63\u1d31(S) = 1/(1-\u03b1) \u00b7 log\u2082(\u2211\u1d62\u2208\u209b P(x\u1d62|\u03b8)\u1d45). (33)\nThe parameter \u03b1 adjusts the element-wise emphasis on rare or frequent events."}, {"title": "4.3 Geometry-based Coreset Sampling", "content": "Overview Instead of explicitly calculating the diversity-aware indicators, recent studies on selecting instruction datasets tend to introduce coreset sampling methods for a systematic consideration (Guo et al., 2022). Specifically, coreset sampling aims to find the most informative-and-diverse subset that represents the entire dataset the most, so that close or even surpassing performance can be achieved on the language model trained on the subset with respect to that on the entire set.\nTechnical Details Among different categories of coreset sampling methods, geometry-based methods are the most intuitive and widely-used ones (Chen et al., 2012; Agarwal et al., 2020; Sener and Savarese, 2017; Sinha et al., 2020; Kamalov, 2020; Rezazadegan Tavakoli et al., 2011; Kirchenbauer et al., 2024). The intuition behind is that close samples in the embedding space often share similar properties with low diversity. Therefore, redundant information can be effectively suppressed by controlling the minimum distance between any two samples for subset selection. Specifically, k-center greedy is a typical diversity-oriented sampling method for massive pretraining and instruction-tuning corpus (Chen et al., 2023a; Bhatt et al., 2024; Wu et al., 2023; Zhao and Fang, 2024; Du et al., 2023). It solves the minimax facility location (FL) problem (Cornu\u00e9jols et al., 1983; Farahani and Hekmatfar, 2009), i.e., selecting the subset S\u266d under the given size budget b from the full set S so that the largest distance between an example in S\\S\u266d and its closest example in S\u266d is minimized:\nmin_{S_b \\subset S, |S_b| = b} max_{x_i \\in S \\setminus S_b} min_{x_j \\in S_b} d(g(x_i), g(x_j)). (38)\nThe direct solution to Eq. 38 is NP-hard (Cook et al., 1994) and a greedy approximation is proposed (Sener and Savarese, 2017) (see Alg. 2). For initialization of S\u2080, one can either choose randomly sampled datapoints from S, or use the cluster center points from K clusters (C\u2081, C\u2082, ..., C\u2096) of S via k-means clustering. Similarly, the farthest point sampling method (Eldar et al., 1997) shares the same principle that each iteration time only the farthest datapoint relative to the already selected coreset is chosen from the candidates.\nIn addition to the k-center greedy, the herding method (Chen et al., 2012; Welling, 2009; Husz\u00e1r and Duvenaud, 2012; Adhikary and Boots, 2022) selects datapoints x\u1d62 so that the distance between the coreset center and the full set center is minimized in the embedding space. For efficiency, it is also approximated via greedy implementation (Chen et al., 2016; Harvey and Samadi, 2014) by adding one sample each time into the S\u266d to minimize the distance between two centers (see Alg. 3).\nFurthermore, recent studies tend to develop complex heuristic sampling methods that takes geometry-based diversity into consideration (Jiang et al., 2023c; Chan et al., 2021; Xia et al., 2022). Specifically, the inter-sample similarity of the selected coreset is minimized in return for an overall high diversity. (Jiang et al., 2024c) proposes to preserve informative subset with the learning complexity (see Eq. 8) and implicitly puts constraints on its diversity via sampling on the k-means clusters:"}, {"title": "4.4 Bilevel Optimization-based Coreset Sampling", "content": "Overview The selection of coreset can also be viewede as a bilevel optimization problem (Colson et al.", "loops": 1, "follows": "nS^* = arg min_{S_b \\subset S"}, "frac{1}{|S_b|} \\sum_{x_i \\in S_b, \\theta \\in \\Theta^*} \\mathcal{L}_{LL}^\\mathcal{A_Q}(x_i),  s.t. \\theta^* = arg min_{\\theta} \\sum_{x_i \\in S_b} \\mathcal{L}_{LL}^\\mathcal{A_Q}(x_i). (41)\nTechnical Details The retrieve method proposed by (Killamsetty et al., 2021c) takes both labeled and unlabeled datasets into consideration, where the self-supervised loss from the unlabeled set (e.g., consistency regularization (Xie et al., 2020; Wang et al., 2021c) and entropy regularization (Zhao et al., 2020b; Grandvalet and Bengio, 2004; Erkan and Altun, 2010)) contributes to the inter-level and outer-level optimization as well. To improve the robustness, Glister (Killamsetty et al., 2021b) optimizes the outer-level coreset selection on the additionally prepared validation set for the minimized validation loss. (Li et al., 2023d) further emphasizes the role of the validation set in bilevel optimization. It not only computes the loss on the validation set for adversarial training, but also introduces gradient matching (Killamsetty et al., 2021a) where the gradient of the model on the selected subset S\u266d should be close to that on the entire S. (Borsos et al., 2024) reformulates the coreset sampling as a cardinality-constrained bilevel optimization problem. It proposes greedy forward selection and first-order methods that apply to any twice differentiable models. Variants of the solution for acceleration are extended: 1) binary weights, inverse-hessian-vector product approximations, and batch-wise selection; 2) small proxy models for fast estimation; 3) enforced sparsity-inducing penalty in the outer loop.\nThe ScaleBiO (Pan et al., 2024) specifically addresses the data reweighting problem for large-scale LLM instruction tuning. It also prepares an extra validation set S\u1d65\u2090\u2097 for the minimization of the outer loop. ScaleBio transforms the bilevel optimization into the single loop framework with an outer-level problem plus a constraint of the inner-level problem. A multiplier \u03b1 > 0 and a proxy u for optimizing the original inner loop (i.e., model weights \u03b8) are introduced into the minimax formulation (Kwon et al., 2023; Lu and Mei, 2024).\nIn contrast to a fixed budget b, (Xia et al., 2024b) proposes a lexicographic bilevel-optimization method (Borsos et al., 2020; Killamsetty et al., 2021b,c) where the inner loop optimizes model parameters and the outer loop optimizes data selection. When optimizing the selection mask, the minimization of loss terms is relaxed to allow"]}