{"title": "Bridging the Gap: Enhancing LLM Performance for Low-Resource African Languages with New Benchmarks, Fine-Tuning, and Cultural Adjustments", "authors": ["Tuka Alhanai", "Adam Kasumovic", "Mohammad Ghassemi", "Aven Zitzelberger", "Jessica Lundin", "Guillaume Chabot-Couture"], "abstract": "Large Language Models (LLMs) have shown remarkable\nperformance across various tasks, yet significant dispari-\nties remain for non-English languages, and especially na-\ntive African languages. This paper addresses these dispar-\nities by creating approximately 1 million human-translated\nwords of new benchmark data in 8 low-resource African lan-\nguages, covering a population of over 160 million speak-\ners of: Amharic, Bambara, Igbo, Sepedi (Northern Sotho),\nShona, Sesotho (Southern Sotho), Setswana, and Tsonga. Our\nbenchmarks are translations of Winogrande and three sec-\ntions of MMLU: college medicine, clinical knowledge, and\nvirology. Using the translated benchmarks, we report pre-\nviously unknown performance gaps between state-of-the-art\n(SOTA) LLMs in English and African languages. Finally, us-\ning results from over 400 fine-tuned models, we explore sev-\neral methods to reduce the LLM performance gap, includ-\ning high-quality dataset fine-tuning (using an LLM-as-an-\nAnnotator), cross-lingual transfer, and cultural appropriate-\nness adjustments. Key findings include average mono-lingual\nimprovements of 5.6% with fine-tuning (with 5.4% average\nmono-lingual improvements when using high-quality data\nover low-quality data), 2.9% average gains from cross-lingual\ntransfer, and a 3.0% out-of-the-box performance boost on cul-\nturally appropriate questions. The publicly available bench-\nmarks, translations, and code from this study support further\nresearch and development aimed at creating more inclusive\nand effective language technologies.", "sections": [{"title": "Introduction", "content": "For many tasks, Large Language Models (LLMs) perform\non-par with or approaching human performance. Further-\nmore, LLM capabilities are improving: the performance gap\nbetween state-of-the-art LLMs (e.g. GPT-4) and humans for\nmany benchmarks is much smaller than the gap between pre-\nvious LLM generations (e.g. GPT 3.5) and humans (Achiam\net al. 2023; Bandarkar et al. 2024; Sakaguchi et al. 2021;\nLin et al. 2021; Hendrycks et al. 2021b). Despite impressive\nadvancements, LLMs are significantly less capable when\nassessed in non-English languages. When assessing LLMs\nin native African languages, which are predominantly low-\nresource (Joshi et al. 2020), the gap between human and\nLLM performance is notable (when known), but generally\nremains unknown because many standard benchmarks do\nnot exist in native African languages.\nThe performance discrepancy between LLMs in English\nand African languages is not just a technical challenge; it is\na significant issue of equity. All 2,123 native African lan-\nguages are low-resource, including the 31 languages with\nmore than 10 million speakers (Hammerstrom 2015; Joshi\net al. 2020)\u00b9. Naturally, lower language resource levels re-\nsult in poorer-performing LLMs. This is particularly tragic\nbecause LLMs are least reliable for language speakers who\nhave the most to gain. Of the world's poor, 66% live in\nAfrica (Galal 2024), 66% of those don't have access to the\nInternet (ITU 2023), and 80% do not speak English (CIA\n2024). Thus, even in the unlikely event that the world's\npoorest could access the Internet and afford the costs of\na state-of-the-art LLM, their ability to read and write En-\nglish would prevent them from making use of the tools. Ul-\ntimately, the differences in LLM performance between lan-\nguages result in a \u201crich-get-richer\u201d effect: LLMs are more\nhelpful to (English-speaking) people who are better off, and\nwho may then provide better content to train better LLMs.\nOne approach to bridging the LLM performance gap is\nto translate all non-English language queries into English,\nquery an English LLM, and backtranslate the response. This\napproach is only viable if the cumulative errors from trans-\nlation and backtranslation are smaller than the errors from\nusing non-English language LLMs alone. Previous studies\nreport that errors from leading machine translation tools (i.e.\nGoogle Translate) can be substantial in African languages\n(Bapna et al. 2022; Benjamin 2019); however, the extent to\nwhich translation errors impact the substantive reasoning ca-\npabilities of multilingual LLMs versus their style (\u201ctransla-\ntionese\"), remains unclear. Insofar as the substantive errors\nare minimal, machine translation may serve as a workaround\nwhen: (i) the goal is to leverage an LLM to answer questions\""}, {"title": "Aims", "content": "Our work has three specific aims, listed below:\n\u2022 Aim 1 Benchmark Translation: We translate the\npopular multiple choice reasoning benchmark Wino-\ngrande, as well as three clinical sections of MMLU (col-\nlege medicine, clinical knowledge, and virology) into\n8 low-resourced and under-studied African languages\n(Amharic, Bambara, Igbo, Sepedi, Shona, Sesotho,\nSetswana, and Tsonga), allowing assessment of the (un-\nknown) capabilities of LLMs in African languages.\n\u2022 Aim 2 - Performance Assessment: We apply several\nstate-of-the-art (SOTA) LLMs to the newly translated\nbenchmarks from Aim 1 and measure the extent of\nthe performance gap between English and each of the\nAfrican languages on the benchmarks. To assess the vi-\nability of machine translation, we compare the perfor-\nmance of SOTA LLMs on machine-translated bench-\nmarks versus human-translated benchmarks. We also as-\nsessed the performance gap between culturally appropri-\nate and inappropriate benchmark questions. This quan-\ntitative assessment highlights the areas and languages\nwhere LLM improvements are most needed.\n\u2022 Aim 3 - Performance Enhancement: We explore var-\nious fine-tuning strategies to determine their impact on\nclosing the LLM performance gap in African languages.\nThis includes adjusting the fine-tuning data used based\non the data domain, language, data quality, and the vol-\nume of training samples. Understanding how fine-tuning\ncharacteristics impact LLM performance will inform\nprospective data collection efforts for the community at\nlarge."}, {"title": "Related Work", "content": "Recent advances in natural language processing have seen\na growing interest in assessing language modeling perfor-\nmance in African languages. This interest has resulted in\nbenchmarks in several tasks, including language identifica-\ntion with AfroLID (Adebara et al. 2022), machine transla-\ntion with FLORES-200 (NLLB Team et al. 2024), and nat-\nural language inference with XTREME (Hu et al. 2020).\nMore recently, benchmarks have emerged for African lan-\nguages to assess LLM reasoning using multiple choice\nquestions. In particular, the manually curated reading com-\nprehension benchmark Belebele provides the most exten-\nsive coverage of African languages (25 languages; 115 lan-\nguages in total) (Bandarkar et al. 2024). In addition to Bele-\nbele, Irokobench provides manual translations of reason-\ning tasks into 15 African languages (Adelani et al. 2024),\nwhile Winogrande-MMLU-Clinical-ZA also provides man-\nual translations of reasoning tasks into 3 African languages\n(BMGF 2024). Given the lack of reasoning benchmarks\navailable for African languages, particularly benchmarks\nthat have been manually translated or otherwise sourced\nfrom human-written text (i.e. not machine-translated or AI-\ngenerated), we aim to translate two established reason-\ning and domain-knowledge benchmarks, Winogrande (Sak-\naguchi et al. 2021) and MMLU (Hendrycks et al. 2021b,a),\ninto 8 African languages. The translations of these two\nbenchmarks provide valuable additions to existing African\nlanguage datasets, enabling the proper evaluation of LLMs\nin African languages in popular LLM evaluation tasks. This\neffort helps pave the way for developing LLMs that perform\nas well in African languages as they do in English, as the\navailability of African language benchmarks allows devel-\nopers to continually refine and enhance their LLMs for use\nin African languages."}, {"title": "Methods", "content": "We translated the popular multiple choice reasoning bench-\nmark Winogrande, as well as three clinical sections of\nMMLU (college medicine, clinical knowledge, and virol-\nogy) into 8 low-resourced and under-studied African lan-\nguages (Amharic, Bambara, Igbo, Sepedi, Shona, Sesotho,\nSetswana, and Tsonga). These benchmarks were selected\nbecause they are multiple choice (and thus easy to compare\nacross languages) and widely used (300+ citations/year). A\nsummary of the translation process is described below, and\nwe provide additional information on the procedures, trans-\nlator profiles, remuneration approach, and other details in\nAppendix Section A."}, {"title": "Results", "content": "In this section, we provide results of five experiments that\nsupport our three aims: (Aim 1) benchmark translation,\n(Aim 2) evaluation of LLM performance, and (Aim 3) en-\nhancement of LLM performance. More specifically, the sub-\nsections below provide assessments of: (1) benchmark trans-\nlation fidelity, (2) \u201cout-of-the-box\u201d LLM performance on\nthe translated benchmarks, (3) \u201cout-of-the-box\u201d LLM per-\nformance on the culturally \u201cappropriate\u201d vs. \u201cinappropriate\u201d\nsubsets, (4) fine-tuned LLM performance using mono- and\ncross-lingual data, and (5) fine-tuned LLM performance us-\ning varying data quality and quantity."}, {"title": "Discussion", "content": "This study aimed to measure (and explore means to address)\nthe performance gap of Large Language Models (LLMs)\nin English and African languages by translating popular\nbenchmarks, assessing performance on those benchmarks,\nand exploring fine-tuning strategies that close the gap. The\nperformance gap is not only a technical challenge but also\na matter of equity, as many native African languages are\nlow-resource, affecting the accessibility and effectiveness of\nLLMs for over 160 million speakers6.\nThe creation of benchmarks in low-resourced African lan-\nguages is a critical step toward achieving equitable advance-\nments in natural language processing. By translating pop-\nular benchmarks such as Winogrande and sections of\nthe MMLU into eight under-studied African languages, we pro-\nvide essential tools for evaluating and improving LLM per-\nformance in these languages. Our work not only highlights\nexisting performance gaps, but also lays the groundwork for\nfuture research and development aimed at improving lan-\nguage technologies for native African language speakers.\nThe benchmarks we translated may enable a more accurate\nassessment of LLM capabilities and drive progress toward"}, {"title": "Limitations", "content": "Our study has several limitations that should be addressed or\nextended in future work:\n1. Choice of Fine-tuning Model: Although Llama 3 was\nthe best open-source solution at the time of this study,\nit does not outperform GPT-40 out-of-the-box, even af-\nter fine-tuning. Future studies should consider evaluat-\ning other models (e.g. Llama 3.1, released on July 24th,\n2024).\n2. Fine-Tuning Scope: Our fine-tuning experiments were\nconducted on individual languages in isolation. We did\nnot explore the potential gains from tuning LLMs using\ndata from multiple languages. Future research should in-\nvestigate the effects of grouping African or related high-\nresource languages to enhance performance.\n3. Benchmark Relevance: The translation of established\nLLM benchmarks, while valuable, may not fully capture\nthe depth and breadth of African-language specific use-\ncases. Future benchmark creation efforts should consider\ngenerating content that directly supports and aligns with\nuse-cases most relevant to African language speakers.\n4. Variability in Translation Quality: Recruiting experi-\nenced translators for all our chosen languages proved\ndifficult. The number of speakers available on Upwork\nwas limited for many languages, most notably Xhosa,\nSesotho, Shona, Setswana, Bambara, Sepedi, and Tsonga\n(see Figure A.15 for more details). In some cases, it was\nnot possible to find workers with prior experience. We\nalso encountered variability in the dialects spoken by the\nworkers, a factor which was challenging to control given\nthe aforementioned lack of available speakers.\n5. Language Coverage and Scalability: Although we\nwere able to cover 11 diverse African languages, there\nare naturally many other African languages presumably\nwith LLM performance gaps which are not covered in\nthis work. We believe that our presented framework is\nscalable, but acknowledge that the most costly part, the\nhuman translation of text, is a significant barrier. To al-\nleviate this, we suggest incorporating a step to automat-\nically identify translated text that may require additional\nhuman verification, rather than verifying all translations\nmultiple times (which we performed to ensure that trans-\nlations were high-fidelity). This should make it easier to\nextend our work to other languages."}, {"title": "Conclusion", "content": "This study takes steps toward addressing the performance\ngap in Large Language Models (LLMs) for African lan-\nguages. As part of this work, we created approximately 1\nmillion human-translated words of new benchmark data in\n8 African languages, covering a population of 160 million\nspeakers. This effort involved the translation of established\nbenchmarks and the fine-tuning of more than 400+ mod-\nels. The benchmarks, translations, and all the code needed\nto recreate the results of this study are publicly available,\nsupporting ongoing efforts to improve LLM performance in\nAfrican languages and beyond. Future work should continue"}]}