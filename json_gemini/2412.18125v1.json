{"title": "Exact Acceleration of Subgraph Graph Neural Networks by Eliminating Computation Redundancy", "authors": ["Qian Tao", "Xiyuan Wang", "Muhan Zhang", "Shuxian Hu", "Wenyuan Yu", "Jingren Zhou"], "abstract": "Graph neural networks (GNNs) have become a prevalent framework for graph tasks. Many recent studies have proposed the use of graph convolution methods over the numerous subgraphs of each graph, a concept known as subgraph graph neural networks (subgraph GNNs), to enhance GNNs' ability to distinguish non-isomorphic graphs. To maximize the expressiveness, subgraph GNNs often require each subgraph to have equal size to the original graph. Despite their impressive performance, subgraph GNNs face challenges due to the vast number and large size of subgraphs which lead to a surge in training data, resulting in both storage and computational inefficiencies. In response to this problem, this paper introduces Ego-Nets-Fit-All (ENFA), a model that uniformly takes the smaller ego nets as subgraphs, thereby providing greater storage and computational efficiency, while at the same time guarantees identical outputs to the original subgraph GNNs even taking the whole graph as subgraphs. The key is to identify and eliminate the redundant computation among subgraphs. For example, a node \\(v_i\\) may appear in multiple subgraphs but is far away from all of their centers (the unsymmetric part between subgraphs). Therefore, its first few rounds of message passing within each subgraph can be computed once in the original graph instead of being computed multiple times within each subgraph. Such strategy enables our ENFA to accelerate subgraph GNNs in an exact way, unlike previous sampling approaches that often lose the performance. Extensive experiments across various datasets reveal that compared with the conventional subgraph GNNs, ENFA can reduce storage space by 29.0% to 84.5% and improve training efficiency by up to 1.66x.", "sections": [{"title": "Introduction", "content": "Graph neural networks (Wu et al. 2020; Zhou et al. 2020) have demonstrated their effectiveness in various graph prediction tasks, such as graph classification (Zhang et al. 2018; Xu et al. 2018) and node classification (Kipf and Welling 2016; Velickovic et al. 2017; Hamilton, Ying, and Leskovec 2017). Particularly for graph classification tasks, mainstream research (Morris et al. 2019; Bodnar et al. 2021b,a; Zhang and Li 2021; Xu et al. 2018) has focused on enhancing the expressiveness of the GNN models, i.e., the ability to distinguish non-isomorphic graphs, to make GNNs as effective as the Weisfeiler Lehman test (WL test) (Leman and Weisfeiler 1968; Sato 2020; Wang et al. 2022).\nAmong them, subgraph graph neural networks (subgraph GNNs) have been proposed and attracted much attention due to their high expressiveness (Zhou, Wang, and Zhang 2023) and practical effectiveness (Bevilacqua et al. 2021; Qian et al. 2022). Typically, a subgraph GNN operates in an extraction-and-assemble fashion: it first generates subgraphs rooted at each node or edge and treats them as independent graphs; the representations for subgraphs are then generated by a massage-passing neural network and assembled to obtain the graph prediction. Recent studies choose the original graph as subgraphs to guarantee the maximum expressiveness (Zhou, Wang, and Zhang 2023; Zhang et al. 2023a,b).\nWhile subgraph GNNs demonstrate exceptional performance in graph classification, the massive number of subgraphs, whose size is almost as large as the original graph, need to be preprocessed and input to MPNNs independently, resulting in enormous storage cost and computational inefficiency. This paper begins with an observation that subgraphs contain a substantial number of overlapping components, leading to excessive redundant computations. These redundant computations could alternatively be obtained from the results of the original graphs. Fig. 1 illustrates a graph alongside two subgraphs under the node-marking policy when input to an MPNN layer, with the marked nodes highlighted yellow. The node \\(v_i\\) and its neighbors share identical features and structures across all three graphs, resulting in identical outputs for \\(v_i\\) from the MPNN layer across three"}, {"title": "Preliminaries", "content": "Formally, a graph can be defined as \\(G = (V, E, X)\\), where \\(V = \\{1, ..., v_p\\}\\) and \\(E = \\{(v_i, v_j)|v_i, v_j \\in V\\}\\) are the set of nodes and edges, respectively. \\(X \\in \\mathbb{R}^{p\\times t}\\) represents the features of the nodes in \\(G\\) and we use \\(x_i\\) to represent the features of \\(v_i\\). The convolution process of a subgraph GNN can generally be formalized as follows.\n\\[E, H^{(0)} = cat(Sub(G))\\]\n\\[H^{(i)} = MPNN_i(E, H^{(i-1)}) \\text{ for } 1 < i < L\\]\n\\[Y = pool(H^{(L)})\\]\n\\[\\tag{1}\\]\nSub(\\) is the subgraph generation policy that generates subgraphs from \\(G\\), i.e., \\(Sub(G) = \\{G_1, ..., G_q\\}\\), where \\(G_i\\) represents the i-th subgraph, and \\(q\\) defines the total number of subgraphs in the policy. The concatenation function, cat, assembles the structures and features of subgraphs to a graph with edges \\(E\\) and features \\(H^{(0)}\\). The concatenated graph is then fed to \\(L\\) MPNN layers and a pooling function to obtain the prediction for \\(G\\). The i-th MPNN layer can be defined as\n\\[m^{(i)} = AGG_i(\\{H_u^{(i-1)}, u \\in N(v)\\})\\]\n\\[H_v^{(i)} = COM_i(m^{(i)}, H_v^{(i-1)})\\]\n\\[\\tag{2}\\]\nwhere \\(H_v^{(i)}\\) represents the internal embeddings of \\(v\\) after the i-th MPNN layer, \\(N(v)\\) returns the neighbors of \\(v\\) in the concatenated graph, and \\(AGG_i\\) and \\(COM_i\\) are the aggregate and combine functions in the i-th layer respectively.\nThis paper focuses on the node-marking policy that uses the original graph as subgraphs, which achieves the maximum expressiveness among subgraph GNNs within the framework of (Zhang et al. 2023a). It also suits the node-deleting and edge-deleting policies which use subgraphs with little modifications from the original graph.\n\u2022 Node-marking (NM): This policy forms subgraphs by marking each rooted node as extended features, i.e., \\(Sub_{NM}(G) = \\{(V, E, cat(X, I_r))|1 \\le r \\le |V|\\}\\), where \\(I\\) indicates a column of feature in which the r-th element is marked 1 and all others are marked 0.\n\u2022 Node-deleting (ND): This policy generates subgraphs by deleting each node from the original graph, i.e., \\(Sub_{ND}(G) = \\{(V - v_r, E - \\{(v_r,v_s)|v_s \\in N(v_r)\\}, X_{-r})| v_r \\in V\\}\\), where \\(X_{-r}\\) represents the features of nodes in \\(V\\) except for \\(v_r\\).\n\u2022 Edge-deleting (ED): This policy generates subgraphs by deleting each edge from the original graph \\(G\\), i.e., \\(Sub_{ED}(G) = \\{(V, E - \\{(v_r, v_s)\\}, X)| (v_r, v_s) \\in E\\}\\).\nComplexity and Bottleneck. Table 2 summarizes the number of subgraphs (i.e., |Sub(G)|), the internal embeddings of nodes (i.e., |H^(2)|) and the total number of edges (i.e., |E|) in subgraph GNNs for different policies. Here, we treat the dimension of hidden states as a constant and |H^(i)| = O(|Sub(G)||V|) primarily depends on the number of nodes in subgraphs. In terms of space complexity where the edges of the subgraphs, i.e., \\(E\\), and the subgraph features, i.e., \\(H^{(0)}\\), should be stored, the number of edges \\(|E|\\) dominates the size of the subgraph features \\(|H^{(0)}|\\) when we consider dense graphs (i.e., \\(|E| = O(|V|^2)\\)). For time complexity, the bottleneck occurs in the graph convolution of MPNN layers. Since the time complexity of GNN layers depends on the size of the input edges (i.e., |E|) and the node embeddings (i.e., |H^(i)|) (Wu et al. 2020), the time complexity is O(max{|E|, |H^(i)|}), where |E| also dominates. In summary, for both space and time complexity, the bottleneck of subgraph GNNs emerges from the extensive scale of the generated edges, which is super-linear to |E|."}, {"title": "Motivations", "content": "In this section, we observe that due to enormous repeated parts among subgraphs, some embeddings in the subgraphs contain redundant computations and can be directly obtained from the original graph. A noteworthy property of MPNN layers is that the output embeddings of a node \\(v_i\\) from an MPNN layer rely solely on the input embeddings of \\(v_i\\) and \\(v_i\\)'s neighbors. This indicates that if a node in two graphs shares identical neighbors and input embeddings, it yields the same embedding after the layer."}, {"title": "Definitions", "content": "The equivalence of a node's embeddings in a subgraph and the original graph depends on the node's hop to those nodes with altered features or neighbors. Thus, we introduce the definitions of pivot nodes and pivot hops.\nGiven a graph \\(G = (V, E, X)\\) and one of its subgraphs \\(G_j = (V_j, E_j, X_j)\\), the pivot nodes of \\(G_j\\) relative to \\(G\\) are those nodes whose neighbors or features differ in \\(G_j\\)\nand \\(G\\). Formally, we define \\(pvt(G_j) = \\{v_r | N(v_r) \\ne N_j(v_r) \\lor X[v_r] \\ne X_j[v_r], v_r \\in V\\}\\), where \\(N_j(v_r)\\) represents the neighbors of \\(v_r\\) in \\(G_j\\), and \\(X[v_r]\\) and \\(X_j[v_r]\\) represent the features of \\(v_r\\) in \\(G\\) and \\(G_j\\), respectively. Furthermore, define \\(pvt(G, Sub) = \\{pvt(G_j) | G_j \\in Sub(G)\\}\\) as the set of pivot nodes for all subgraphs derived from \\(G\\) according to the subgraph policy Sub.\nBased on the definition of pivot nodes, we further define pivot hops to track the minimum number of hops to the pivot nodes for each node in \\(G_j\\). Formally, \\(phop(G_j) = (\\text{min}\\{\\text{hop}(v_1, v_i) | v_i \\in pvt(G_j)\\}, ..., \\text{min}\\{\\text{hop}(v_p, v_i) | v_i \\in pvt(G_j)\\})\\) where \\(\\text{hop}_i(v_r, v_s)\\) signifies the number of hops between \\(v_r\\) and \\(v_s\\) in \\(G_j\\). Furthermore, define \\(phop(G, Sub) = \\{phop(G_j) | G_j \\in Sub(G)\\}\\) as the set of pivot hops for all subgraphs derived from \\(G\\).\nIf the context is clear, we use the abbreviations \\(pvt_j\\), \\(pvt\\), \\(phop_j\\), and \\(phop\\) as shorthand for \\(pvt(G_j)\\), \\(pvt(G, Sub)\\), \\(phop(G_j)\\), and \\(phop(G, Sub)\\), respectively. Consider the node-marking and edge-deleting policies as examples. For the node-marking policy, only the rooted node has altered features in each subgraph, which is treated as the pivot node. On the other hand, for the subgraph by deleting an edge \\((v_r, v_s)\\) in the edge-deleting policy, the endpoints of the deleted edge, \\(\\{v_r, v_s\\}\\), experience a change in their neighbors, thereby serving as the pivot nodes."}, {"title": "Design of ENFA", "content": "The insight from Fig. 2 motivates us to utilize convolutions on the original graph to enhance the efficiency of the subgraph GNNs, leading to the design of ENFA.\nBasic Idea. The basic idea is to identify nodes whose internal embeddings remain identical to those in the original graph, based on pivot hops, after each MPNN layer convolution. Thus, the internal embeddings of these nodes can be directly derived from the original graph, rather than being independently computed. On the other hand, nodes whose embeddings cannot be obtained from the original graph form the ego nets around the pivot nodes. Consequently, ENFA only employs convolution on ego nets and the original graph, which is both storage and computation efficient.\n\\[H^{(0)} = X; E, H^{(0)} = cat(EgoSub_{L+1}(pvt))\\]\n\\[\\tag{3}\\]\n\\[H^{(1)} = MPNN_1(E, H^{(0)})\\]\n\\[H^{(i)} = MPNN_i(E, H^{(i-1)})\\]\n\\[H^{(i)}[V^i_j] = H^{(i)}[V^i_j] \\text{ (for } 1 \\le j \\le q)\\]\n\\[1 < i < L\\]\n\\[\\tag{4}\\]\n\\[Y = pool(H^{(L)})\\]\n\\[\\tag{5}\\]\nEqu. 3-Equ. 5 illustrate the workflow of ENFA. Different from the conventional subgraph GNN in Equ. 1, ENFA employs convolutions on both the (smaller) ego nets and the original graph. Specifically, Equ. 3 first initializes the input of the original graph convolution, \\(H^{(0)}\\), with the original features in \\(G\\). For the convolution on subgraphs, it generates the (L+1)-hop ego nets around each set of pivot nodes based on a unified subgraph generation policy \\(EgoSub_h\\). Formally,\n\\[EgoSub_h (pvt) = \\{G_h^j | 1 \\le j \\le q\\}\\), where \\(G_h^j\\) represents the h-hop ego net of the j-th pivot nodes of \\(pvt\\) in the j-th subgraph \\(G_j\\). The generated ego nets and features are then assembled into \\(E\\) and \\(H^{(0)}\\), respectively. Compared to the generated edges in Equ. 1, \\(E\\) only contains the edges of the ego nets, resulting in less storage and more efficient computation than the conventional subgraph GNNs.\nAs shown in Equ. 4, the embeddings on the original graph and generated subgraphs are processed through L MPNN layers to obtain the internal embeddings \\(H^{(i)}\\) and \\(H^{(i)}\\), respectively. Besides, after the convolution of the i-th layer, nodes with pivot hops larger than i in \\(G_j\\) are identified, denoted as \\(V^i_j\\). The embeddings of these nodes in the subgraphs are then replaced by the embeddings from the original graph rather than being computed, as indicated in the third line of Equ. 4. Here, \\(H^{(i)}\\) defines the embeddings of the j-th subgraph in \\(H^{(i)}\\). Lastly, the embeddings for \\(G\\) are derived from the subgraph embeddings, as shown in Equ. 5, which is identical to the process in Equ. 1.\nFig. 3 illustrates the workflow of ENFA for a subgraph GNN encompassing in total 2 MPNN layers and incorporating a node-marking policy. For instance, for \\(G_1\\) with pivot node \\(\\{v_1\\}\\), ENFA generates 3-hop ego net \\(G^1_3\\) and pivot hops \\(phop_1\\) around \\(v_1\\), as shown in the first column of the figure. The original graph and the generated ego nets are input into the MPNN layer to generate the internal embeddings. Nodes with pivot hops larger than 1 are then identified and their embeddings are replaced by the internal embeddings from the original graph. As depicted in the second and third columns of Fig. 3, for the ego net \\(G^1_3\\), the embeddings of \\(v_4-v_8\\) are replaced. The internal embeddings are then input to the second MPNN layer. For the second layer, \\(v_4\\) is affected by the pivot node in the subgraph and only the embeddings of \\(v_5-v_8\\) are replaced, as depicted in the fourth column.\nComplexity Analysis. Under the transformation of ENFA, each subgraph is derived from the ego net of the pivot nodes. In other words, the total edge size of the generated graph of \\(G\\) is at most \\(|E| = O(pvt_{max}d^{L+1}|Sub(G)|)\\) where \\(d\\) represents the maximum degree in \\(G\\) and \\(pvt_{max}=\\text{max}_{1 \\le j \\le q}\\{\\text{pvt}_j\\}\\) represents the maximum size of \\(G\\)'s pivot nodes. Since the common policies shown in Table 2 limit the size of pivot nodes to at most \\(d\\) and GNNs typically contain few layers (e.g., 2-4), we can perceive |E| to be linearly dependent on |E| with a bounded \\(d\\) and \\(L\\). Thus, \\(|H^{(i)}|\\) in Table 2 dominates \\(|E|\\) for the three policies. For storage complexity, the size of features, i.e., \\(|H^{(0)}|\\), is no less than |E| for dense graphs. For time complexity, the running time can be regarded as \\(O(\\text{max}\\{|E|, |H^{(i)}|\\}) = O(|H^{(i)}|)\\). In summary, under the ENFA model, the edges of subgraphs \\(E\\) are reduced to a linear scale of |E| and no longer the bottleneck for storage and computation.\nReusability of Pivot Hops. Although ENFA specifies the number of MPNN layers, its structure enables the reuse of pivot hops for subgraph GNNs incorporating varying numbers of MPNN layers. Specifically, for a vertex v, its pivot hop in the j-th subgraph remains consistent for subgraph GNNs with any number of MPNN layers as long as v is in the ego nets. This suggests that larger ego nets and the corresponding pivot hops can be directly employed for any subgraph GNNs with fewer layers. Our evaluation reveals that for a subgraph GNN with 5 layers, which is already a considerable number in practice, ENFA still exhibits notable improvement in space efficiency and execution speed.\nRemarks. We offer the following remarks. (1) Applications of ENFA. ENFA can be applied to subgraph GNNs with other policies that generate subgraphs with minimal modifications from the original graph, such as the edge-marking (Yan et al. 2023) and reconstruction policy (Cotta, Morris, and Ribeiro 2021). It is also flexible to commonly-used subsampling strategies for subgraph GNNs (Bevilacqua et al. 2021; Qian et al. 2022; Lecheng et al. 2023). Please refer to Appendix for additional details. (2) Contrast with Redundant Computation Reduction in General GNNs. The efficiency gains from ENFA stem from the distinctive structure of subgraph GNNs: the original graph is replicated into independent subgraphs. Thus, the redundant computations of a node across subgraphs exist and can be eliminated. This significantly differs from prior works, such as (Jia et al."}, {"title": "Discussions on ENFA", "content": "Exact Acceleration of Subgraph GNNs We show that under ENFA, the embeddings of each node in each subgraph are identical to those in the conventional subgraph GNN after the L-th layer. Thus, ENFA can fully emulate the behavior of subgraph GNNs and achieve an exact acceleration.\nTheorem 1 The internal embeddings of nodes in j-th ego net in ENFA are identical to those in j-th subgraph in the conventional subgraph GNN after L layers. Consequently, ENFA can produce the same embeddings Y for G as the conventional subgraph GNN.\nThe proof can be found in Appendix. As a corollary of Theorem 1, ENFA can guarantee the same expressiveness as the conventional subgraph GNNs because of their identity. For example, the conventional subgraph GNNs have expressivity between 1-WL test and 3-WL test (Bevilacqua et al. 2021; Frasca et al. 2022), so does ENFA.\nExtension to Subgraph GNNs with Subgraph Message Passings Previous studies (Zhao et al. 2021; Bevilacqua et al. 2021) suggest adding message passings across subgraphs after each MPNN layer. This concept is called DSS-GNN in (Bevilacqua et al. 2021) and context encodings in (Frasca et al. 2022). An additional subgraph message passing layer operates on the assembled internal embeddings of the subgraphs after each MPNN layer. Formally,\n\\[H_j^{(i)} = H_j^{(i)} + Enc_i(\\text{concat}_{1<k<q}H_k^{(i)}) \\text{ (1 \\le j \\le q)}\\]\n\\[\\tag{6}\\]\nis added after the second line of Equ. 1. Here, Enc can either be an identity function (Frasca et al. 2022) or another MPNN layer on the original graph G (Bevilacqua et al. 2021).\nAs for ENFA, we notice that we can maintain the imaginary convolution results on the original graph, and the nodes in the subgraphs still share the same internal embeddings as in the original graph, the identification of which still depends on the pivot hops. Consequently, ENFA can accommodate the modification by replacing Equ. 4 with the following.\n\\[H^{(1)} = MPNN_1(E, H^{(0)})\\]\n\\[H^{(i)} = MPNN_i(E, H^{(i-1)})\\]\n\\[H_j^{(i)}[V_j^i] = H_j^{(i)}[V_j^i] \\text{ (1 \\le j \\le q)}\\]\n\\[H_{SM}^{(i)} = Enc_i(\\text{concat}_{1<k<q}H_k^{(i)})\\]\n\\[H^{(i)} = H^{(i)} + H_{SM}^{(i)}\\]\n\\[H_j^{(i)} = H_j^{(i)} + H_{SM}^{(i)} \\text{ (1 \\le j \\le q)}\\]\n\\[\\tag{7}\\]\nEqu. 7 deviates from Equ. 4 in two aspects: (1) additional embeddings \\(H_{SM}^{(i)}\\) by subgraph massage passings are calculated via Enc, aligned with the subgraph message passing layer in subgraph GNNs; (2) Both the embeddings of the original graph, \\(H^{(i)}\\), and the subgraph, \\(H^{(i)}\\), add \\(H_{SM}^{(i)}\\).\nEqu. 3, Equ. 7 and Equ. 5 together guarantee the same behavior as subgraph GNNs with subgraph message passings."}, {"title": "Related Works", "content": "Expressive Graph Neural Networks. Many works have been proposed to improve the expressiveness of GNNs. Recent efforts propose to simulate the higher-order WL test by passing messages among node tuples to achieve more expressive GNNs (Chen et al. 2019; Maron et al. 2019; Morris et al. 2019). Other works achieve expressive GNNs by combining naive MPNNs with certain graph information, like pattern counts (Bouritsas et al. 2022) and shortest paths (Ying et al. 2021; Zhang et al. 2023b).\nIn comparison, subgraph GNNs have been proposed and demonstrated to be more expressive than the 1 WL test (Zhou, Wang, and Zhang 2023) and effective in practice (Bar-Shalom, Bevilacqua, and Maron 2024). Pioneers in this field, (You et al. 2021) and (Zhang and Li 2021) propose ID-GNN and NGNN, respectively. These studies can be regarded as the subgraph GNNs with the node-marking and ego-net policies. Subsequently, ESAN (Bevilacqua et al. 2021) introduces two additional subgraph generation policies, namely the edge-deleting and node-deleting policies. These authors also propose a subgraph message passing layer to facilitate mutualization between different subgraphs. OSAN (Qian et al. 2022) extends the subgraph GNNs over ordered subgraphs (where the nodes of subgraphs are indexed and permutated). Given the large number of ordered subgraphs, the authors propose a data-driven sampling strategy for efficient model training. Some other works can be considered as subgraph GNNs based on different types of subgraphs, such as the subgraphs from graph construction (Cotta, Morris, and Ribeiro 2021), and the ego nets of different hops (Abu-El-Haija et al. 2019; Feng et al. 2022). Besides, the concept of subgraph GNNs has inspired research on other graph prediction tasks, like node classification (Sandfelder, Vijayan, and Hamilton 2021; Zhao et al. 2021) and link prediction (Yin et al. 2022, 2023).\nMore Efficient Subgraph GNNs. On the other hand, only a few studies have partially focused on the efficiency of subgraph GNNs and they mainly employ sampling strategies in subgraph GNNs. Bevilacqua et al. (Bevilacqua et al. 2021) propose to randomly select a unified ratio of subgraphs for each graph. As the number of ordered subgraphs in (Qian et al. 2022) is too large, the authors therein propose a data-driven sampling strategy for efficiently training the model. SUGAR (Sun et al. 2021) suggests employing a reinforcement pooling method to adaptively choose k subgraphs across different epochs. Kong et al. (Lecheng et al. 2023) further demonstrate that the reinforcement learning-"}, {"title": "Evaluation", "content": "In this section, we aim to addressing the following questions to validate the efficiency of ENFA. Q1: How much storage can ENFA save compared to conventional subgraph GNNs? Q2: How does ENFA affect the proportional storage of the data? Q3: How does the computational efficiency of ENFA compare to conventional subgraph GNNs? Q4: How does the computational efficiency of ENFA compare to subgraph GNNs with subgraph message passings?\nExperimental Setup\nDatasets. We evaluate the performance of ENFA on the following datasets: (1) A bioinformatics dataset from TUD repository (Morris et al. 2020), namely PROTEINS. Each graph represents a molecule and the task aims to predict certain properties of the given molecules. (2) Bioinformatics datasets from OGB (Hu et al. 2020), namely ogbg-molhiv and ogbg-moltox21. (3) Synthetic datasets designed to measure the expressiveness of GNNs (Abboud et al. 2020), namely EXP and CEXP. (4) The long range graph dataset, namely Peptides-func, from LRGB (Dwivedi et al. 22). All datasets have been processed following previous work (Bevilacqua et al. 2021; Frasca et al. 2022).\nBenchmark. To address Q1, we compare ENFA with a widely accepted implementation of the conventional subgraph GNN, i.e., ESAN (Bevilacqua et al. 2021). To thoroughly validate the efficiency of ENFA, we choose GIN (Xu et al. 2018) as MPNN layers, vary the number of layers (ranging from 2 to 5), and implement different policies including node-marking (NM), node-deleting (ND), and edge-deleting (ED) for storage efficiency evaluation. To address Q2, we count the proportion of different types of data in Q1 and show the storage breakdown of ENFA and subgraph GNNs. To address Q3, we compare ENFA and ESAN by varying the number of layers and policies as for Q1. As the computation efficiency of ENFA mainly stems from the lower memory cost per graph, we select a larger minibatch for ENFA, ensuring that the size of the input in ENFA does not exceed that of ESAN, and report the total training time. To address Q4, we compare ENFA and the subgraph GNNs with subgraph message passings in (Bevilacqua et al. 2021) by varying the number of layers and policies as for Q1.\nExperimental Results\nWe have compared the performance between the baseline and ENFA in Appendix, showing ENFA can produce very similar performance to subgraph GNNs. On this basis, we evaluate the storage and computational efficiency of ENFA.\nEvaluation on Storage Efficiency Fig. 4 shows the experimental results on the storage efficiency of ENFA. The storage space of the data file after preprocessing for different policies and varying numbers of layers is reported. Specifically,"}, {"title": "Conclusions", "content": "This paper concentrates on the exact acceleration of the popular subgraph GNNs. We observe that a significant number of redundant computations exist, the results of which could be derived from the original graph. Building on this, we propose ENFA, a model that ensures identical outputs to subgraph GNNs with complete subgraphs, while only requiring the smaller ego nets as inputs, thereby improving both storage and computational efficiency. Experimental evaluations demonstrate that ENFA can considerably reduce storage requirements and enhance computational efficiency."}, {"title": "Proof of Theorems", "content": "We first reclaim Theorem 1 here.\nTheorem 1 The internal embeddings of nodes in j-th ego net in ENFA are identical to those in j-th subgraph in the conventional subgraph GNN after L layers. Consequently, ENFA can produce the same embeddings Y for G as the conventional subgraph GNN.\nWe prove Theorem 1 through an induction on the number of layers, l. From the observation in Fig. 2, we have the following base case for l = 1:\nProposition 1 After the convolution of the first MPNN layer, the internal embeddings \\(H^{(1)}_j\\) obtained by ENFA (as per Equ. 4) are identical to those in the conventional subgraph GNN (as per Equ. 1).\nProposition 1 holds based on the property of the MPNN layers: the output of a node from an MPNN layer only depends on the input embeddings of the node itself and its neighbors. Consequently, (1) for a node whose pivot hop is no greater than 1 in \\(G_j^{L+1}\\), its local structure is the same as that in \\(G_j\\), and thus ENFA directly obtains the same embeddings as in the conventional subgraph GNNs; (2) for a node whose pivot hop is greater than 1, itself and its neighbors have identical features in the subgraph of the conventional subgraph GNN and the original graph, which ensures their internal embeddings replaced by the original graph in ENFA equal those in the conventional subgraph GNN.\nNow consider a general case where ENFA has achieved the same internal embeddings as the conventional subgraph GNN after the (l \u2013 1)-th layers.\nLemma 1 Assuming ENFA could produce the same embeddings in the subgraph \\(G_j^{L+1}\\) as those in \\(G_j\\) in the conventional subgraph GNN after l \u2013 1 MPNN layers, then this equivalence holds after the l-th layer for l < L.\nProof By the assumption, the input embeddings to the l-th GNN layer remain the same in ENFA and the conventional subgraph GNN. Now consider different nodes based on their pivot hops in the subgraph \\(G_j\\):\n\u2022 For a node with pivot hops in \\(G_j\\) no greater than l, its internal embeddings before the l-th layer are the same in ENFA and conventional subgraph GNN as per the assumption. Their local structures in \\(G_j^{L+1}\\) and \\(G_j\\) are also the same because ENFA generates \\((L + 1)\\)-hop ego nets and l < L. Hence, ENFA could generate the same embeddings after the l-th layer for these nodes.\n\u2022 Consider the nodes with pivot hops in \\(G_j\\) larger than l, i.e., those nodes in \\(V_j^l\\). According to the definition of pivot hops, the l-hop ego nets of these nodes disjoint with \\(pvt_j\\), meaning that the internal embeddings of nodes in \\(V_j^l\\) after l layers remain the same in \\(G_j\\) and the original graph \\(G\\). ENFA replaces the internal embeddings of \\(V_j^l\\) with the embeddings in the original graph after l layers exactly, thus obtaining the same internal embeddings for those nodes as the conventional subgraph GNN.\nIn summary, We prove that given the assumption on the (l-1)-th layer, ENFA ensures identical outputs to the conventional subgraph GNN after the l-th GNN layer.\nBuilding upon Proposition 1 and Lemma 1, we are now in a position to demonstrate that ENFA operates identically to the conventional subgraph GNN. ENFA obtains the same embeddings after L MPNN layers, i.e., \\(H^{(L)}\\) in Equ. 4, as subgraph GNNs. As a result, the same holds for Y."}, {"title": "Appendix", "content": "We first reclaim the extension of ENFA to subgraph message passing layers and Theorem 2 here.\n\\[H^{(0)} = X; E, H^{(0)} = cat(EgoSub_{L+1}(pvt))\\", "n\\[\\tag{8}\\": "n\\[H^{(1)} = MPNN_1(E, H^{(i-1)})\\", "H^{(i-1)})\\": "n\\[H^{(i)}[V^"}]}