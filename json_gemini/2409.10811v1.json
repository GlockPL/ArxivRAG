{"title": "Context-Dependent Interactable Graphical User Interface Element Detection for VR Applications", "authors": ["Shuqing Li", "Binchang Li", "Yepang Liu", "Cuiyun Gao", "Jianping Zhang", "Shing-Chi Cheung", "Michael R. Lyu"], "abstract": "In recent years, Virtual Reality (VR) has emerged as a transformative technology, offering users immersive and interactive experiences across diversified virtual environments. Users can interact with VR apps through interactable GUI elements (IGEs) on the stereoscopic three-dimensional (3D) graphical user interface (GUI). The accurate recognition of these IGEs is instrumental, serving as the foundation of many software engineering tasks, including automated testing and effective GUI search. The most recent IGE detection approaches for 2D mobile apps typically train a supervised object detection model based on a large-scale manually-labeled GUI dataset, usually with a pre-defined set of clickable GUI element categories like buttons and spinners. Such approaches can hardly be applied to IGE detection in VR apps, due to a multitude of challenges including complexities posed by open-vocabulary and heterogeneous IGE categories, intricacies of context-sensitive interactability, and the necessities of precise spatial perception and visual-semantic alignment for accurate IGE detection results. Thus, it is necessary to embark on the IGE research tailored to VR apps.\nIn this paper, we propose the first zero-shot context-sensitive interactable GUI ElemeNT detection framework for virtual Reality apps, named ORIENTER. By imitating human behaviors, ORIENTER observes and understands the semantic contexts of VR app scenes first, before performing the detection. The detection process is iterated within a feedback-directed validation and reflection loop. Specifically, ORIENTER contains three components, including (1) Semantic context comprehension for capturing the apps' GUI context, (2) Reflection-directed IGE candidate detection for identifying and localizing valid GUI elements based on multi-perspective description guided IGE detection, as well as feedback-directed reflection, and (3) Context-sensitive interactability classification which integrates semantic contexts for interactability prediction. To evaluate our approach and facilitate follow-up research, we spend more than three months constructing the first benchmark dataset which contains 1,552 images from 100 industrial-setting apps on Steam, with 4,470 interactable annotations across 766 semantics categories. Extensive experiments on the dataset demonstrate that ORIENTER is more effective than the state-of-the-art GUI element detection approaches (i.e., GPT-40, YOLO v8, CenterNet2, Faster R-CNN, UIED, and Xianyu), surpassing their F1 Score by up to 3.7\u00d7 and 121.4\u00d7 (1.4\u00d7 and 46.2\u00d7 on average) in distinguishing the interactibility and semantics of the IGEs, respectively. ORIENTER is beneficial for boosting the performance of automatic testing by isolating the interactable action space from the whole space, regardless of the testing strategies employed. Experiments demonstrate that ORIENTER-guided testing covers 103.1% more IGEs with 125.7% more effective interactions than testing without action space isolation.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, Virtual Reality (VR) has emerged as a transformative technology, offering users immersive experiences across various virtual or virtual-real environments. This technological advancement has catalyzed the development of a myriad of VR apps, now numbering in tens of thousands [31]. These apps, which span a diverse array of domains, including skill training [2], entertainment [1, 3], medical procedures [29], and military training [18], have attracted over 171 million users [49]. This exponential growth underscores a critical need for robust software processes, including development, testing, and maintenance, particularly in high-reliability contexts such as healthcare and military training.\nIn VR apps, users experience multimodal perceptions through various devices and interact via body movements and gestures. Among the multimodal perceptions, visual perception obtained via an app's Graphical User Interface (GUI) offers the wealthiest information. GUIs in VR apps are often composed of three-dimensional (3D) GUI elements (e.g., images, text, widgets, etc.) or real-life objects and users interact with VR apps through Interactable GUI Elements (short as IGEs).\nLiterature has shown that the accurate recognition of IGEs is a cornerstone for many software engineering tasks including automated testing [19, 55, 56] and effective GUI search [22, 43]. For example, Ye et al. [56] report that 77% software experts believe that precise IGE detection can boost software testing efficiency by at least 50%. White et al. [53] demonstrate that IGE detection improves branch coverage by 42.5% compared to random testing.\nDeep learning (DL) based object detection approaches like Faster R-CNN [44], CenterNet [59] and YOLO [28], have demonstrated promising performance. With such advancements, recent work has taken steps to explore DL-based IGE detection approaches in mobile apps and desktop apps [19, 53, 55, 56]. For example, Wu et al. [54] explore advanced DL-based approaches, like CenterNet2, YOLOv3, and YOLOv5, for IGE detection on mobile apps. The approaches can be typically divided into three steps: (1) manually summarize a finite set of IGE categories such as buttons, spinners, switches, checkboxes, etc., (2) manually label a large dataset, and then (3) train an object detection model on the dataset. However, such training-based approaches heavily rely on large annotated datasets so that they can hardly be directly applied to IGE detection in VR apps. As shown in Figure 1(a) and Figure 1(b), annotating large-scale VR IGE datasets is challenging due to the unique interaction mechanism of VR apps. The data annotation process demands extensive efforts, compounded by VR's complex hardware usage and exhaustive interactions.\nRecent advancements of pretrained large multimodal models (LMMs) [25, 35] have shown their remarkable abilities in downstream tasks including image/natural language comprehension, question answering and logical reasoning, without large training sets on specific problems. Such capabilities provide us with new opportunities to resolve the aforementioned limitation of lacking datasets. However, our preliminary experiments (detailed in \u00a72.2) reveal that LMMs suffer from severe spatial semantic hallucinations, tending to generate contextually coherent but factually incorrect or unrealistic IGEs with inaccurate locations and amounts as responses. These are mainly stemmed from the following challenges of VR IGE detection problem:\nChallenge #1: Open-vocabulary and heterogeneous GUI element categories. LMM or DL approaches perform well on detecting a finite pre-defined set of IGE categories that frequently appear in their training sets and adhere to standard visual appearance patterns (e.g., buttons, sliders, and checkboxes) as well as unified interaction mechanisms (e.g., tapping and long-tapping). However, VR apps with diverse scenarios contain infinite open-vocabulary categories of IGEs that own different visual appearances and interaction mechanisms (heterogeneity), as shown in Figure 1(a). In the stereoscopic 3D VR scene, from time to time, IGEs occlude each other or are observed from non-front perspectives (e.g., Figure 3(a)), making the visual appearances of even"}, {"title": "2 PRELIMINARIES", "content": "2.1 Background of Virtual Reality (VR)\n2.1.1 VR Devices. VR technologies provide immersive experiences that blend virtual elements with the physical world. These experiences are accessible via diverse devices including PCs and standalone systems, creating interactive environments that integrate real and virtual elements. Key VR devices include: (a) Head-mounted displays (HMDs), essential for VR experiences, offering immersive visuals and audio. (b) Two handheld controllers and gesture recognition systems enable users to interact with the virtual world via physical actions and gestures. (c) Tracking systems, comprising sensors and cameras, track user movement and orientation, crucial for a responsive VR experience.\n2.1.2 VR Interaction. Interaction within VR environments can occur through various mechanisms: (a) Pressing buttons on controllers to manipulate GUI elements. (b) Touching or moving devices towards GUI elements, sometimes using extended virtual tools for remote interaction. (c) Gazing at GUI elements or employing eye-tracking technologies in HMDs to select or move elements. (d) Gestures and Movements to interact with the virtual environment through physical actions. These interaction modes highlight the evolving nature of human-computer interaction within VR, offering increasingly diverse and immersive experiences."}, {"title": "2.2 Motivating Examples", "content": "In this section, we present our motivational study to further demonstrate the technical challenges listed in \u00a71.\nTwo authors randomly sample 30 VR apps from the popular Steam VR app store [6], select screenshots from them, and use the prompt template in Figure 2 to instruct the widely-used LMM GPT-40 [9] to identify and locate IGEs in the images.\nAs shown in figure 3(a), the mailbox is wrongly recognized as \u201cMissile-like Object\u201d. It is because the mailbox is viewed from an unusual angle, resulting in a special shape in the view and making it harder for the LMMs to identify semantics. This misunderstanding of GUI elements reveals the LMM's inability to face the heterogeneous GUI elements in VR apps. For another example, figure 3(b), the bounding boxes in the image either cover only part of the object, locate objects mismatched with the label, or even mark the background. Although the LMM identifies most objects in the image, it fails to locate them well, revealing its insufficient spatial perception.\nTo summarize, the LMM alone cannot effectively detect IGEs in highly complex and varied VR apps. To boost the LMM on IGE detection in VR apps, it is necessary to provide the LMM with more context information and integrate the model with different techniques. Following this idea, we propose ORIENTER to imitate human behavior to enhance the LMM on detecting IGEs in VR apps."}, {"title": "3 THE ORIENTER APPROACH", "content": "3.1 Problem Formulation of IGE Detection\nBefore jumping into the details of our approach, we first introduce how we formulate the IGE detection problem. The basic IGE detection unit is an individual VR app scenario, which lies in the user's field of view, at a specific time. Unlike mobile application GUIs or web app GUIs, which are entirely 2D, HMD-based VR apps typically render two 2D GUIs for both eyes of users, creating an illusion of depth and making users feel stereoscopic 3D (S3D) sense [24, 37]. These scenes, presented to each eye, are projections of the 3D virtual world onto two 2D \"picture planes\" (user's eyes), mathematically represented by the function: $P_{3D\\rightarrow2D}: UI_{3D} \\rightarrow (UI_{left}, UI_{right})$, where $UI_{3D}$ represents the 3D scene in the VR environment, and $UI_{left}$ and $UI_{right}$ represent the corresponding 2D projections rendered for the left and right eyes, respectively. Each one-eye scene can be recovered using the other eye's scene [30]. Therefore, the S3D IGEs can be detected by analyzing the 2D projection from a single eye. This allows us to simplify the VR IGE detection problem to detecting IGEs in the 2D scene of any individual eye.\nTo facilitate downstream SE tasks such as boosting automated GUI testing, each IGE is identified not only by its interactability status but also by its location and associated semantic labels, enabling a more fine-grained understanding of the IGEs. Let GE represent the set of GUI elements in the VR scene under analysis, i.e., UI. Given that the 3D scene $UI_{3D}$ is projected into 2D for each eye, the right-eye scene can be denoted as: $P_{3D\\rightarrow2D}(UI_{right}) : UI_{3D} \\rightarrow UI_{right}$. The detection function\n$D : GE_{2D} \\rightarrow {Non \u2013 interactable, Interactable}$ maps each GUI element to a binary interactability determination. The function D operates on the 2D projections of the 3D GUI elements, denoted as:\n$D(P_{3D\\rightarrow2D}(UI_{right})) : C \u00d7 P_{3D\\rightarrow2D}(GE_{3D}) \\rightarrow {Non \u2013 interactable, Interactable} \u00d7 B \u00d7 S$.\nHere, C denotes the set of semantic contexts that facilitate context-sensitive analysis. We use a bounding box B to specify the spatial location of each IGE, which is the smallest unrotated rectangle containing that IGE and can be described by its upper-left corner coordinates, width, and height. Let S denote the set of semantic labels associated with each GUI element, where the labels and their granularity depend on the specific VR scene. As presented above, the problem can be simplified to detection on the 2D scene of any individual eye. If we use the right-eye scene for demonstration, the IGE detection problem can be formulated as:\n$D(P_{3D\\rightarrow2D}(UI_{right})) : C \u00d7 GE_{2D} \\rightarrow {Non \u2013 interactable, Interactable} \u00d7 B \u00d7 S$.\n3.2 Overview of ORIENTER\nFigure 4 illustrates our context-sensitive interactable GUI ElemeNT dEtection framework for virtual Reality apps, short as Orienter. To address the three challenges (\u00a71), ORIENTER employs a feedback-driven approach that mirrors human behavior, prioritizing semantic comprehension before initiating detection; Orienter also enables iterative refinement and validation through a feedback loop. Such designs embody the principle of \u201clooking before leaping\u201d.\nORIENTER consists of three primary components: (1) Semantic Context Comprehension: To enable unsupervised, context-sensitive IGE detection, ORIENTER first synthesizes both the global context (e.g., app genre, storyline, interaction mechanisms) and the local context (i.e., the user's current VR field of view). Leveraging the LMM, ORIENTER performs in-context reasoning to establish a comprehensive understanding of the VR scene, forming the basis for subsequent detection. (2) Reflection-Guided IGE Candidate Detection: ORIENTER then mines multi-perspective characteristics of GUI elements, and uses these characteristic descriptions for detecting and locating potential IGEs. The framework iteratively refines these candidates through a feedback loop, performing reflection-driven validation to reduce hallucination. (3) Context-Sensitive Interactability Classification: Finally, ORIENTER predicts the interactability of the detected IGE candidates by incorporating the semantic context from the first stage. Using chain-of-thought reasoning within the LMM, ORIENTER classifies interactability without requiring labeled training data, leveraging a knowledge-transfer paradigm from pretrained models in other domains."}, {"title": "3.3 Module I: Semantic Context Comprehension", "content": "The interactability of GUI elements in VR apps is highly dependent on the semantic context of the VR app content and scenarios. To tackle this challenge, ORIENTER understands and analyzes the target VR app's context beforehand to enable context-sensitive detection. The semantic context is analyzed comprehensively, from (1) both global (overall app context) and local (current VR scenario) granularities, and (2) both natural language (texts) to vision (images) modalities.\n3.3.1 Global Context Extraction. The global context captures the overall semantic information of the VR app under analysis. To find out which global context attributes have influences on IGE interactability, two authors randomly sample 30 real-world VR applications from SteamVR and perform manual inspection following the open coding procedure [20]. At last, we reach a consensus and locate the following key attributes: app names, genres, content themes, VR device supports, ways of gameplay, possible interaction mechanisms, and language information. Such context can be extracted from the detailed information page on official VR app stores, but effective extraction can be troublesome due to the free structure natural language format of app details. Hence, ORIENTER leverages LMM to perform the extraction after the app details are crawled automatically. Prompt PI.1 in Figure 5 shows the prompt template.\n3.3.2 Local Context Perception. Local context represents the semantic context within the current VR scenario, i.e., all VR content within the user's current field of view. Under the extracted global context of app content from \u00a73.3.1, ORIENTER further guides LMM to perform in-context visual question answering, to digest and summarize the rendered VR scene as local context. The guidance of global context ensures that the interpretation of the VR scene is informed by the narrative and purpose of the VR app under analysis. Specifically, the LMM processes the composite input (screenshot and global context) and outputs a summary of all GUI elements and the background within the scene. Prompt PI.2 in Figure 5 shows the prompt template."}, {"title": "3.4 Module II: Reflection-Directed IGE Candidate Detection", "content": "This module aims at recognizing and localizing IGE candidates within the VR scene, based on the captured semantic contexts from Module I. LMM can reveal more semantic hints than traditional methods, making it a better backbone for our approach. However, LMM tends to generate semantically-incorrect IGEs with inaccurate locations. This is because VR IGE detection requires precise spatial perception and visual-semantic alignment capabilities, and VR IGE categories are open-vocabulary and heterogeneous (as presented in the challenges and motivational examples). ORIENTER conquer these challenges through a reflection-directed IGE candidate detection loop:\ni). Since LMM has difficulties locating IGE candidates and produces semantic hallucinations, ORIENTER performs IGE candidate detection (\u00a73.4.2) upon LMM's results, based on a language-model-aligned visual foundation model (VFM). This VFM detection module (short as VFMD) verifies IGE candidates' existence, reduces hallucinations of incorrect candidates, and locates the exact bounding boxes of existing candidates for further analysis.\nii). Then the VFMD module and LMM work as a chain to conduct IGE detection. Although VFMD can help reduce hallucinations from LMM, its own analysis process is still error-prone. To reduce errors in the two components simultaneously, ORIENTER performs iterative validation and refinement alongside the reflection loop (\u00a73.4.3). During validation, ORIENTER let another LMM agent, as an advisor (\u00a73.4.3), compare and rethink all the discrepancies and consistencies between the results from the LMM detector (\u00a73.4.1) and VFMD module. If any concerns exist for some IGE candidates, the detection chain will run again with the validation comments from the LMM advisor. The whole process ends when the LMM advisor claims confidence for all detection results.\niii). Inspired by the human cognitive process, when trying to find and locate IGE candidates, using a semantic label only will make the detection process unrobust, e.g., may find the wrong element or find the wrong locations. Adding more descriptions about the target element (like color, size, shape, relevant locations, etc.) makes the detection more effective and robust. It is similar for\nTo boost the effectiveness and robustness of i) and ii), ORIENTER extends a direct LMM detector to a multi-perspective characteristics miner (\u00a73.4.1).\nOverall, ORIENTER first mines multi-perspective characteristics of GUI elements leveraging LMM, and then uses these characteristic descriptions as guidance to detect corresponding elements. Based on the detection results, ORIENTER further performs feedback-directed reflection to validate and refine IGE candidates using a feedback loop.\n3.4.1 Multi-Perspective Characteristics Mining. In this module, ORIENTER firstly identifies all IGE candidates in the VR scene, and then mines their diverse characteristics from different perspectives (e.g., size, color, shape, relevant location with other objects, etc.). Then these characteristics will form a characteristics description, which can uniquely identify the corresponding IGE candidate in the present figure in the described detection module."}, {"title": "3.5 Module III: Context-Sensitive Interactability Classification", "content": "After establishing the presence and location of GUI elements in the VR environment through the first two modules, ORIENTER proceeds to analyze their interactivity. This module determines which GUI elements in the VR scene can be interacted with. Such information is useful for many downstream SE tasks, including boosting automated testing performance (detailed in \u00a76.3). Figure 11 shows the prompt template PIII. To classify interactability, ORIENTER guides LMM towards a chain-of-thought in-context reasoning process, within the semantic context captured in Module I. This process mimics a VR player's thought pattern in distinguishing between GUI elements that are mere visual elements and those that afford interaction. For instance, a tree in a tree-planting game (Figure 1(c)) is interactable, serving as a GUI element to be manipulated by the player, whereas the same tree would be non-interactable background scenery in a fishing game (Figure 1(d)). Orienter also provides demonstration examples in the prompt, illustrating the reasoning required for differentiating between interactable and non-interactable GUI elements."}, {"title": "4 DATASET CONSTRUCTION", "content": "To the best of our knowledge, there is no existing GUI dataset for IGE detection in VR apps. Therefore, we build a dataset to verify the usefulness and effectiveness of ORIENTER.\nWe recruit a team of 13 annotators, spending more than three months in the collection and annotation of GUI images from various VR apps. These annotators have a minimum of two years of computer science or electrical engineering background, and mostly with experience in video games. The annotators first interact with GUI elements in various VR apps while their views are recorded. They then select GUI images from these recordings and annotate IGEs. To enhance the quality of the dataset, we train the annotators in the use of VR equipment and the labeling of IGEs, and we ask them to follow several guidelines during data collection and annotation: (1) attempt to identify all IGEs using every possible interaction method in each VR scene, and (2) categorize the semantics of IGEs with appropriate granularity based on their context.\n4.1 Collection of VR Apps\nWe first collect VR apps from the Steam app store [6], a comprehensive repository with a wide variety of VR content. This collection yielded a total of 4,610 VR-Only apps, which includes software (official type label on Steam referring to non-games) and games. To obtain a statistically significant sample size with a 95% confidence level and a 10% margin of error, we randomly sample 102 apps for analysis to cover a broader spectrum of categories, contexts, and interaction paradigms, covering 245 community-generated genres on Steam, demonstrating their diversity and representativeness.\n4.2 Collection of GUI Images from VR Apps\nIn this step, the annotators engage with the selected VR apps, interacting with all GUI components within the VR scene in every possible way, as described in \u00a72.1.2, while recording their stereo view. The annotators then select and save GUI images that encompass all VR scenes and GUI elements they explored from the recordings. We crop the right-eye images from the stereo-view recordings, as only one side of the view is needed, as explained in \u00a73.1.\n4.3 Annotation of IGEs in GUI Images\nIn this step, the annotators identify IGEs in GUI images and label their locations with bounding boxes as well as their semantics within the context. As discussed in \u00a71, the interactability and semantics of GUI elements heavily depend on the diverse contexts of VR apps, making it challenging to categorize all GUI elements with a finite set of predefined categories.\nTo address this challenge, we ask the annotators to create categories. Specifically, they are instructed to classify IGEs with appropriate granularity based on their experience, to better describe the semantics of IGEs in concrete contexts. For example, in the VR game, VR The Diner Duo [7], fish is only one kind of ingredient to make burgers, as shown in 12(a). Different individuals of fish are semantically the same in this game. Therefore, a coarse-grained fish category is adequate to describe it. However, in the game Munch VR [5], players need to control a fish to eat other smaller fish while avoiding hunted by larger fish, as shown in 12(b). In this scenario, fish individuals diverge semantically according to size and appearance, necessitating the introduction of more fine-grained categories such as large fish and small fish to capture their semantic distinctions.\n4.4 Dataset Statistics\nAfter data cleaning, we finally construct a dataset consisting of 1,552 images with the size of 960 * 540 from 100 apps, covering 245 community-generated genres on Steam, with 4,470 interactable"}, {"title": "5 EXPERIMENT DESIGN", "content": "5.1 Research Questions\nIn this study, our experiment is designed to answer the following research questions:\n\u2022 RQ1 (Performance in industrial-setting): How effective is our proposed framework, ORIENTER, in IGE detection on industrial-setting VR apps?\nRQ1-1 (Interactability): How effective is ORIENTER in terms of analyzing interactability?\nRQ1-2 (Semantics): How effective is ORIENTER in terms of inferring semantics?\nRQ1-3 (Context-sensitive interactability): How does ORIENTER perform in terms of analyzing context-sensitive interactability?\n\u2022 RQ2 (Ablation Study): How does each component of ORIENTER contribute to its performance?\n\u2022 RQ3 (Usefulness): How effectively can ORIENTER boost automated testing on VR apps?\n5.2 Baselines\nGPT-40 [9] & Gemini 1.5 Pro [42] (direct prompting): Simply prompt the model with the image and ask it to locate all IGEs with bounding boxes and their semantics in the image to illustrate the end-to-end performance of LMM. Xianyu [19] is a UI-to-Code tool developed by Alibaba, leveraging old-fashioned computer vision techniques and OCR. UIED [55] combines old-fashioned methods with deep learning models to detect clickable GUI elements in complex GUI images. CenterNet2 [59] is a two-stage object detector that uses class-agnostic one-stage detectors as the proposal network. It estimates object probabilities in the first stage and conditionally classify objects in the second stage. Faster R-CNN [44] is a two-stage anchor-based deep learning object detector. It contains a region proposal network to extract regions of interest. Objects within the RoIs are then classified with another neural network. YOLO v8 [28] is a one-stage anchor-free detection model. It detects and classifies objects simultaneously and thus is faster than two-stage detectors.\n5.3 Implementation Details and Experimental Setup\n5.3.1 Implementation of Orienter. For the implementation of ORIENTER, we leverage the most representative and popular LMMs, i.e., gpt-4-vision-preview, gpt-40-2024-08-06, Claude 3.5 Sonnet, Gemini 1.5 Pro. For described IGE detection, we leverage the pretrained model of APE-L_D [47], which is trained on ten datasets and demonstrates promising results in visual grounding. We apply post-processing to improve ORIENTER's prediction quality by filtering abnormally large bounding boxes (over 90% of the image size), and applying Non-Maximum Suppression (NMS) which retains only the highest-confidence box among overlapping ones with IoU exceeds 0.7 to reduce duplicates.\n5.3.2 Dataset Preparation. To comprehensively answer the RQs, we derive three variations from the original dataset. (1) Semantics dataset: The original dataset testing methods' ability to identify IGEs' semantics, answering RQ1-2. (2) Interactability dataset: All annotations are assigned the category \"interactable\" for binary classification, testing models' ability to differentiate IGEs, answering RQ1-1. (3) Context dataset: Contains 41 categories randomly sampled from the most common 100 categories, and extra annotations marking their corresponding non-interactable objects, testing methods' understanding of context-sensitive IGEs in different contexts, answering RQ1-3.\nWe partitioned the images into training/validation/testing datasets in a 6:1:3 ratio using three distinct methods: App split, Genre split, and Context-sensitive split. The App split randomly allocates images based on their corresponding apps. The Genre split further considers the apps' genres to assess the methods' performance across different app types. The Context-sensitive split assigns images containing the 41 sampled categories in the context dataset to the test set while randomly distributing the rest into the training and validation sets.\n5.3.3 Experimental Setup. We train baselines on the training dataset and evaluate all methods including ORIENTER on the test dataset. For Xianyu [19], we only use its component detection part to find IGEs. For UIED [19], we reimplement and train the CNN classifier on our dataset to adapt it to our problem, and replace the OCR component with PaddleOCR. For training on CenterNet2 [59], Faster R-CNN [44], and YOLO v8 [28], we set the batch size to 32 and accordingly adjust the learning rate linearly, and keep other configurations the same as their original releases. We apply early stopping with the patience of 20 epochs to avoid overfitting. For LMM experiments, we run three times and regard the average results as final results.\nWe implement a semantic matching tool to support open vocabulary category matching. We consider two categories semantically match if the cosine similarity of their embedding vectors obtained from the embedding-3 model released by Zhipu AI [58] exceeds a preset threshold. The first two authors examine the similarity of some typical categories and set the threshold as 0.85. For RQ1-1, RQ1-2, and RQ2, predictions are evaluated with our customized COCO API that adapts to our semantic matching tool. Note that during evaluation, instead of simply ignoring categories without ground truth annotations (usually due to splitting the dataset), as the official COCO API [4] does, we further check if any predictions fall into those categories. Metrics of categories that meet this condition are set to 0 and included when averaging metrics across categories, providing more comprehensive results. Regarding RQ1-3, the baseline methods are trained on the Semantic dataset using Context-sensitive Split. Predictions on the test set are then evaluated on the Context dataset with the same split. The more IGEs and less non-interactable objects the method detects, the better this method's performance on context-sensitive interactability understanding. Metrics are initially calculated for each category and then averaged to obtain the final results.\n5.3.4 VR App Automated Testing Setup. To answer RQ3, we simulate a simplified test scenario: given a screenshot of the scene in the VR app under test, the testing agent attempts to interact with the GUI elements in the scene in a black-box setting. We compare the performance of the testing agent with and without the guidance of ORIENTER using the test set of Genre split as input.\nWe simplify the interaction events in the 3D virtual space to interaction points on UI images of VR scenes and ignore the type of interactions. The testing process is modeled as incrementally generating points on UI images over time, treating each point as an interaction made by the testing agent with an interval of one minute. We follow the recent works [26, 36] to set the testing duration to 60 minutes, which is longer than exploring a VR scene usually requires. We perform 5 testing runs and average the metrics to produce comprehensive results for each interaction strategy.\nFor the non-guided strategy, the testing agent randomly generates interaction points on the whole image. For the guided strategy, in each attempt, the testing agent generates interaction within the bounding boxes predicted by ORIENTER at a probability p, or ignores the prediction's constraint to randomly explore the whole UI images at the probability (1 \u2013 p). The probability p decreases gradually as the test progresses. Suppose the current moment is t minutes and the total duration of the test is T minutes, then $p = 1 - t/T$. This strategy allows the agent to generate effective yet diversified interaction events to cover more IGEs precisely."}, {"title": "5.4 Evaluation Metrics", "content": "For RQ1 and RQ2, we evaluate each method using Precision, Recall, F1-Score, and mAP. The calculation involves computing IoU.\nIoU. Intersection over Union (IoU) is calculated as the ratio of two bounding boxes' intersection area to their union area, measuring how well they match. A predefined threshold is usually set, and only pairs of bounding boxes with IoU above this threshold can match.\nPrecision, Recall, and F1-Score. A predicted bounding box matches the one with the highest IoU among the ground-truth bounding boxes that (1) lie in the same image, (2) have matching categories, and (3) have IoU exceeding the threshold. For RQ1-1, RQ1-2, and RQ2, we consider a predicted box matching a ground-truth box a True Positive (TP); otherwise, a False Positive (FP); ground-truth boxes that fail to match any predicted box are False Negatives (FN). For RQ1-2, a predicted box is considered a TP if it matches an interactable ground-truth box, and an FP if it matches a non-interactable one; non-interactable and interactable ground-truth boxes that fail to match any predicted box are considered a TN and FN, respectively. We calculate Precision as TP/(TP + FP), Recall as TP/(TP + FN) and F1-Score as 2 \u00d7 Precision\u00d7Recall/(Precision+Recall).\nAP and mAP. We follow COCO API [4] to calculate the Average Precision (AP) by averaging 101 Precision values in increments of 0.01 over a range of Recall values from 0 to 1. The mean AP (mAP) is obtained by averaging AP across categories.\nWe follow the previous works [51, 52] to use IGE Coverage and Effective Interaction Count in RQ3. Note that we can not calculate code coverage metrics in the black-box setting of RQ3.\nIGE Coverage. The IGE coverage reflects the effectiveness of interaction strategies to find IGEs over time. An IGE is covered if at least one interaction point falls within the bounding box of that IGE. IGE Coverage is calculated as $n_{covered}/n_{all}$, where n represents the number of IGEs.\nEffective Interaction Count. The Effective Interaction Count reflects the efficiency of interaction strategies to make effective interactions by focusing on IGEs only. An interaction point is considered effective if it falls in any of IGEs' bounding boxes."}, {"title": "6 RESULTS AND ANALYSIS", "content": "6.1 RQ1: Performance in industrial-setting\n6.1.1 RQ1-1: Performance in terms of interactability. As shown in Table 1", "44": "at 33.42%. This trend of superiority is more significant at higher IoU thresholds. At the stringent IoU of 0.95", "59": "by over 321.7%", "RQ1-2": "Performance in terms of semantics. ORIENTER's performance in semantics shows a similar but lower pattern compared to interactability. For ORIENTER with Gemini, the mAP starts at a lower rate of 22.12% at an IoU of 0.75 and follows a smooth decreasing trend with increasing IoU thresholds until 0.95, with a drop to 7.41%. This pattern reflects the challenges inherent in semantic interpretation, especially under stringent detection criteria.\nThe Precision and Recall metrics provide further insights. While the Recall maintains at 50.00%, the Precision"}]}