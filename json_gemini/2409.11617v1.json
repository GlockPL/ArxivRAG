{"title": "HRA: A Multi-Criteria Framework for Ranking Metaheuristic Optimization Algorithms", "authors": ["Evgenia-Maria K. Goula", "Dimitris G. Sotiropoulos"], "abstract": "Metaheuristic algorithms are essential for solving complex optimization problems in different fields. However, the difficulty in comparing and rating these algorithms remains due to the wide range of performance metrics and problem dimensions usually involved. On the other hand, nonparametric statistical methods and post hoc tests are time-consuming, especially when we only need to identify the top performers among many algorithms. The Hierarchical Rank Aggregation (HRA) algorithm aims to efficiently rank metaheuristic algorithms based on their performance across many criteria and dimensions. The HRA employs a hierarchical framework that begins with collecting performance metrics on various benchmark functions and dimensions. Rank-based normalization is employed for each performance measure to ensure comparability and the robust TOPSIS aggregation is applied to combine these rankings at several hierarchical levels, resulting in a comprehensive ranking of the algorithms. Our study uses data from the CEC 2017 competition to demonstrate the robustness and efficacy of the HRA framework. It examines 30 benchmark functions and evaluates the performance of 13 metaheuristic algorithms across five performance indicators in four distinct dimensions. This presentation highlights the potential of the HRA to enhance the interpretation of the comparative advantages and disadvantages of various algorithms by simplifying practitioners' choices of the most appropriate algorithm for certain optimization problems.", "sections": [{"title": "I. INTRODUCTION", "content": "Computational intelligence has emerged as a rapidly evolving field, with metaheuristic algorithms playing a crucial role in solving complex optimization problems in various domains [1]. These algorithms, inspired by diverse sources such as natural evolutionary processes, swarm behavior, and mathematical constructs, have successfully tackled challenging optimization tasks [2]. However, the No Free Lunch, or NFL Theorem, states that no algorithm should consistently outperform all others across all problem domains [3]. This fundamental principle, coupled with the ever-growing number of metaheuristic algorithms-now exceeding 500 plus [2], underscores the critical need for robust and comprehensive assessment methods to compare and evaluate algorithm performance.\nTraditionally, algorithm performance has been assessed using statistical and non-parametric techniques. Though applicable, such evaluations are very much restricted in that they concentrate only on mean performance measures and would probably neglect other essential characteristics of how an algorithm works. Pacheco et al. [4] presented the limitations of such approaches and emphasized the fact that such procedures primarily neglect the measures of dispersion, and in particular, the standard deviation of the obtained results, which is very crucial when analyzing performance and reliability of algorithms. This concern implies that researchers and practitioners who desire to utilize optimization methods or work on the improvement of algorithm designs may, therefore, tend to underestimate a certain algorithm in its capability and applicability. However, in the last decade, more researchers have started employing a"}, {"title": "II. METHODOLOGY", "content": "This research, in particular, aims to develop an exhaustive ranking system for the metaheuristic al- gorithms tested to consider how successfully a particular algorithm performs across various benchmark functions and problem dimensions. Let us define:\n$A = {A_1, . . . , A_m}$ as the set of algorithms,\n$F = {f_1, ..., f_n}$ as the set of benchmark functions,\n$D = {d_1, ..., d_k}$ as the set of problem dimensions, and\n$P = {p_1,..., p_l}$ as the set of performance measures.\nFor each dimension $d \\in D$, performance measure $p \\in P$, algorithm $A \\in A$, and benchmark function $f \\in F$, we have the performance values $X_{d,p,A,f}$ of algorithm $A$ on benchmark function $f$ under dimension $d$ for performance measure $p$. These values form a decision matrix $m \\times n$.\nOur approach involves converting raw performance data into ranks within each criterion of a decision matrix and then creating a rank-based decision matrix. This approach is helpful since relative performance is more critical than absolute value. It also provides an additional advantage of avoiding scale effects across evaluation criteria and biasing from the outlier scores since ranks are used instead of raw scores.\nThe ranked data are then processed using the R-TOPSIS method described by Aires and Ferreira [15]. This method normalizes the performance measures and then ranks the evaluated measures using a hierarchical structure, as shown in Fig. 1, by ensuring robust and interpretable rankings reflecting algorithm performance's multifaceted nature across different benchmarks."}, {"title": "A. The R-TOPSIS Method", "content": "TOPSIS (Technique for Order Preference by Similarity to Ideal Solution) is a multi-criteria decision- making method (MCDM) used to select the best alternative from a set of options based on multiple criteria. It was initially proposed by Hwang and Yoon in 1981 and is widely used in decision analysis and operations research [16]. TOPSIS is a straightforward ranking method both in concept and in application. The core principle of the standard TOPSIS method is to select alternatives with the shortest distance from the positive ideal solution (PIS) and the farthest distance from the negative ideal solution (NIS). The ideal positive solution emphasizes maximizing benefit criteria and minimizing cost criteria, while the negative solution focuses on maximizing and minimizing cost criteria. However, TOPSIS has faced criticism for its rank reversal issue. Rank reversal is when the order of alternatives changes after adding or removing an alternative from a previously rated group.\nAires and Ferreira [15] introduced R-TOPSIS as a solution to address the issue in TOPSIS. To this end, the authors introduced an additional input parameter, the domain, to describe the range of possible values for each criterion. Furthermore, the type of normalization adopted, whether Max-Min or Max normalization, helped to maintain optimal solutions and the stability of the normalized and weighted decision matrices, even in case some adjustments were made to the original decision problem. These improvements focused on increasing the reliability and logic of TOPSIS while maintaining its user- friendliness. The steps that will be used to apply the R-TOPSIS method have been outlined below.\nStep 1: Define a set of alternatives $A = {a_i}_{i=1}^m$\nStep 2: Define a set of criteria $C = {c_j}_{j=1}^n$ and a subdomain $D = {d_j}_{j=1}^n$, where $d_{1j}$ is the minimum value and $d_{2j}$ is the maximum value of $D_j$\nStep 3: Estimate the performance rating of the alternatives as $X = {x_{ij}}_{i=1,j=1}^{m,n}$\nStep 4: Elicit the criteria weights as $W = {w_j}_{j=1}^n$, where $w_j > 0$ and $\\sum_{j=1}^n W_j = 1$\nStep 5: Calculate the normalized decision matrix $(n_{ij})$"}, {"title": "", "content": "Step 5.1: Max normalization\n$n_{ij} = \\frac{x_{ij}}{d_{2j}}, i = 1,2,..., m; j = 1, 2, . ..., . ., n$\nStep 5.2: Max-Min normalization\n$n_{ij} = \\frac{x_{ij} - d_{1j}}{d_{2j} - d_{1j}}, i = 1, 2, . . ., m; j = 1, 2, . . ., n$\nStep 6: Calculate the weighted normalized decision matrix $(r_{ij})$ as:\n$r_{ij} = w_j \\times n_{ij}, i = 1, 2, . . ., m; j = 1, 2, ..., n$\nStep 7: Set the negative (NIS) and positive (PIS) ideal solutions\n$NIS = {r_j^-}_{j=1}^n$, where $r_j^- = \\begin{cases} \\frac{d_{1j}}{w_j} & \\text{if } j \\in \\text{Benefit} \\\\  d_{2j}w_j & \\text{if } j \\in \\text{Cost}  \\\\ \\end{cases}$\n$PIS = {r_j^+}_{j=1}^n$, where $r_j^+ = \\begin{cases} w_j & \\text{if } j \\in \\text{Benefit} \\\\  \\frac{w_j}{d_{1j}} & \\text{if } j \\in \\text{Cost}  \\\\ \\end{cases}$\nStep 8: Calculate the distances of each alternative $i$ in relation to the ideal solutions\n$S_i^+ = \\sqrt{\\sum_{j=1}^n (r_{ij} - r_j^+)^2}, i = 1, 2, . . ., m$\n$S_i^- = \\sqrt{\\sum_{j=1}^n (r_{ij} - r_j^-)^2}, i = 1, 2, ..., m$\nStep 9: Calculate the closeness coefficient of the alternatives $(CC_i)$ as:\n$CC_i = \\frac{S_i^-}{S_i^+ + S_i^-}, i = 1, 2, ..., m$\nStep 10: Sort the alternatives in descending order. The highest $CC_i$ value indicates the best performance concerning the evaluation criteria. Return sorted alternatives based on $CC_i$.\nIn applying the R-TOPSIS method, we impose the domain as $[0, m + 1]^n$, $m$ being the number of algorithms, while $n$ is the number of criteria. A significant benefit of this specified domain is that the Max and Max-Min normalization techniques used in Step 5 of R-TOPSIS are equivalent, and there is no reason to adopt either method and provide the rationale. This simplifies the decision-making process when utilizing R-TOPSIS because it guarantees uniform and dependable normalization of performance indices across various criteria."}, {"title": "B. HRA: Detailed Algorithmic Steps", "content": "The following outlines the basic steps of the Hierarchical Rank Aggregation (HRA) algorithm, which ensures a clear assessment and robust ranking of alternatives.\n1) Collect the performance measures $X_{d,p,A,f}$, which constitute $m \\times n$ matrices for each combination of dimensions $d$ and performance measures $p$, resulting in a total of $l \\cdot k$ matrices. Each matrix has $m$ rows corresponding to the algorithms and $n$ columns corresponding to the benchmark functions. The general form of these matrices is given by:\n$X_{d,p} = \\begin{bmatrix} X_{d,p, A_1, f_1} & X_{d,p,A_1,f_2} & ... & X_{d,p,A_1,f_n} \\\\ X_{d,p,A_2,f_1} & X_{d,p,A_2,f_2} & ... & X_{d,p,A_2,f_n} \\\\ : & : & & : \\\\ X_{d,p, A_m,f_1} & X_{d,p, A_m,f_2} & ... & X_{d,p, A_m,f_n} \\end{bmatrix}$\nwhere each element $X_{d,p,A_i,f_j}$ represents the performance value of algorithm $A_i$ on benchmark function $f_j$ under dimension $d$ for performance measure $p$. All benchmark functions are considered equivalent and assign an equal weight of $1/n$.\n2) Apply the mean rank transformation on each column of the matrices $X_{d,p}$ ensuring comparability and robustness during the application of RTOPSIS. The general form of the resulting ranked matrices $R_{d,p}$ is given by:\n$R_{d,p} = \\begin{bmatrix} R_{d,p, A_1, f_1} & R_{d,p,A_1,f_2} & ... & R_{d,p,A_1,f_n} \\\\ R_{d,p,A_2,f_1} & R_{d,p,A_2,f_2} & ... & R_{d,p,A_2,f_n} \\\\ : & : & & : \\\\ R_{d,p, A_m,f_1} & R_{d,p, A_m,f_2} & ... & R_{d,p, A_m,f_n} \\end{bmatrix}$\nwhere each element $R_{d,p,A_i,f_j}$ represents the rank of algorithm $A_i$ on benchmark function $f_j$ under dimension $d$ for performance measure $p$. All these matrices $R_{d,p}$ can be visualized as the leaves of a tree, constituting the fundamental component of a hierarchical framework.\n3) Apply the RTOPSIS method to the rank decision matrices $R_{d,p}$ (of size $m \\times n$) for the d-th dimension and p-th performance measure: $C_{d,p} = RTOPSIS(R_{d,p}, W_F)$ where $C_{d,p}$ is the resulting $m \\times 1$ vector of ranks obtained. The weight vector $W_F$ assigns equal importance to all the benchmark functions, with each weight being $1/n$ for $j = 1, . . .,n$. The vector $C_{d,p}$ provides the aggregated ranks of the algorithms for the specified dimension and performance measure, indicating their relative performance. This information will be synthesized at the next level of the hierarchy by grouping the performance measures for each dimension (see Fig. 1).\n4) Once the rank vectors $C_{d,p}$ have been derived via the TOPSIS method for each combination of dimen- sion $d$ and performance measure $p$, these vectors are concatenated at the second level of the hierarchical structure to create the intermediate matrices $C_d$. Each intermediate matrix $C_d = [C_{d,p_1} C_{d,p_2} ... C_{d,p_l}]$, encapsulates the performance ranks for a particular dimension across all performance measures. Therefore, at this level of the hierarchical structure (see Fig. 1), we have a total of $k$ in numbers, $C_d$ matrices, corresponding to the number of function dimensions. Consequently, we may examine the performance of the algorithms for each dimension, considering the performance measures evaluated $l$.\n5) Apply the RTOPSIS method on the intermediate matrices $C_d$ for each dimension $d$ to determine the definitive ranking of algorithms per dimension. This procedure integrates performance metrics to generate a unified ranking for each dimension: $C_d^* = RTOPSIS(C_d, w_p)$, where $w_p$ the corresponding weight vector.The outcome $C_d^*$ is a vector $m \\times 1$, which presents the overall ranking of the $m$ algorithms for dimension $d \\in D$ incorporating all performance metrics for the specified dimension.\n6) The final stage involves the construction of the decision matrix $D$ of size $m \\times k$, which constitutes from the rank vectors $C_d^*$ as $D = [C_1^* C_2^* ... C_{d_k}^*]$ and apply the TOPSIS method one final time."}, {"title": "C. Tree Structure of HRA Algorithm", "content": "This process can further be understood from a hierarchical perspective, which is a hierarchical tree in nature, if we assume that we can design the RTOPSIS method within this framework. Each tissue of leaf of the graph is a rank decision matrix $R_{d,p}$, while edges bearing the title \u201cRTOPSIS\u201d show the RTOPSIS application. These edges provide information to form an intermediate matrix $C_d$ of size $m \\times l$ for each dimension $d$. Out of total of $k$, there are such matrices pertaining to each dimension. The ranks aggregate that pour all these matrices together are used to determine the decision matrix for the ultimate ranking of the algorithms.\nWe can visualize this process as a tree to better understand the hierarchical structure and the application of the RTOPSIS method. Each leaf node represents a rank decision matrix $R_{d,p}$, and the edges, labeled \u201cRTOPSIS\u201d, indicate the application of the RTOPSIS method. These edges provide information to form an intermediate matrix $C_d$ of size $m \\times l$ for each dimension $d$. We have a total of $k$ such matrices, one for each dimension. The aggregated ranks of these matrices are then combined to form the final decision matrix for the overall ranking of the algorithms.\nFig. 1 elucidates this process clearly and structured. Each rank decision matrix $R_{d,p}$ (where $d$ represents the dimension and $p$ the particular criterion within that dimension) undergoes the RTOPSIS transformation to produce a corresponding ranking vector $C_{d,p}$. These vectors, which represent the performance scores of the algorithms for each criterion, are subsequently consolidated into an intermediate matrix $C_d$. This intermediate matrix $C_d$ has dimensions $m \\times l$, where $m$ is the number of algorithms, and $l$ is the number of criteria within dimension $d$.\nThe process is repeated for all $k$ dimensions, resulting in intermediate $k$ matrices $C_d$. Each of these intermediate matrices $C_d$ is then aggregated into a single decision matrix $D$ of size $m \\times k$. This final decision matrix $D$ encapsulates the performance of the algorithms across all dimensions.\nIn the final step, we reapply the RTOPSIS method to the decision matrix $D$ to obtain the overall ranking vector $D^*$. This vector $D^*$ provides a comprehensive ranking of the algorithms, reflecting their performance across all considered dimensions and criteria."}, {"title": "Algorithm 1 Hierarchical Rank Aggregation (HRA)", "content": "Require: Set of algorithms $A = {A_1, ..., A_m}$, set of benchmark functions $F = {f_1,..., f_n}$, set of dimensions $D = {d_1,...,d_k}$, set of performance measures $P = {P_1,...,p_l}$, performance values $X_{d,p, A, f}$\nEnsure: Overall ranking vector $D^*$\n1: for each dimension $d\\in D$ do\n2: for each performance measure $p\\in P$ do\n3: Collect performance metrics $X_{d,p}$\n4: Rank them to obtain $R_{d,p}$\n5: Apply RTOPSIS to $R_{d,p}$ to get $C_{d,p}$\n6: end for\n7: Form intermediate matrix $C_d$ from $C_{d,p}$ vectors\n8: Apply RTOPSIS to $C_d$ to get $C_d^*$\n9: end for\n10: Construct decision matrix $D$ from $C_d^*$ vectors\n11: Apply RTOPSIS to $D$ to get overall ranking $D^*$\n12: return $D^*$\nThus, as shown in Fig. 1, the hierarchical structure ensures a systematic and thorough evaluation of the algorithms. It allows for the incorporation of multiple dimensions and criteria, facilitating a multidimensional assessment and yielding a final ranking that is both holistic and robust.\nWhen applying the RTOPSIS method in HRA, the weights must be assigned to different criteria, represented as $w_f$, $w_p$, and $W_D$. Although the precise determination of these weights can substantially impact the outcomes, we will not explore the intricacies of this issue in the present work. One rough method for estimating these weights is to give each criterion equal significance, thereby employing equal weights.\nHowever, this approach could result in a significant loss of information if it does not consider the relative importance of each criterion. In practice, it is reasonable to prioritize the criteria by ranking them, as rankings are often simpler to provide than precise numerical weights, which can be challenging to quantify. One possible solution to this problem is the surrogate weight approach, commonly used to determine weights based on ordinal ranking [17], [18]. This method may improve the prescription of solving several criteria problems in evaluating algorithms, such as ensuring the inclusion of expert opinion into the decision-making process and not needing exact numerical values.\nIt is worth noticing that the HRA algorithm exhibits remarkable computational efficiency; we can easily prove that it achieves an overall time complexity of $O(m \\log m)$, where $m$ denotes the number of algorithms under comparison. Moreover, it is characterized by a problem-dependent constant $c = 1+k+lk$, where $k$ represents the number of dimensions and $l$ is the number of performance measures. This efficiency is advantageous compared to pairwise comparison methods $O(m^2)$. The tree-structured design of HRA not only supports this time complexity but also allows potential parallelization at each level, making it a practical tool for metaheuristic algorithm comparison and ranking, especially when dealing with a large number of algorithms or extensive datasets."}, {"title": "D. The CEC'17 Competition Dataset", "content": "The CEC'17 test suite, featured in the 2017 IEEE Congress on Evolutionary Computation, comprises 30 distinct optimization problems (denoted as $f_1-f_{30}$) categorized into four groups: 3 unimodal, 7 multimodal, 10 hybrid, and 10 composition functions. The competition's objective was to minimize these test functions $f(x)$, where $x \\in R^d$ and $d\\in {10, 30, 50, 100}$. Detailed descriptions of these functions are provided in the technical report by Awad et al. [19]. Thirteen metaheuristic algorithms were evaluated in the CEC'17"}, {"title": "III. RESULTS AND DISCUSSION", "content": "This section presents the results of applying the HRA algorithm to the CEC'2017 competition data. The implementation of HRA (see Alg. 1) was performed in Matlab, and detailed results are available in the repository\u00b3. In this study, at each level of the decision tree, we have adopted equal weighting for the weight vectors $w_f$, $w_p$, and $w_D$.\nThe hierarchical structure of the HRA, as shown in Fig. 1, effectively combines multiple performance metrics in a structured way, which lets each method be evaluated thoroughly and in a way that is easy to understand. The decision matrices for various dimensions (10, 30, 50, and 100) are shown in Tables I, II, III, and IV, respectively. Each matrix displays the rankings given to the algorithms according to five performance measures: best, worst, median, mean, and standard deviation. These matrices summarize the performance of each algorithm in a particular dimension, providing a precise representation of their relative rankings.\nThe decision matrix for dimension 10 (Table I) compares the 13 metaheuristic algorithms based on the five performance measures. Each algorithm is ranked from 1 to 13, with lower ranks indicating better performance. EBOwithCMAR emerges as the best-performing algorithm across all performance measures. It consistently ranks top (1) in the best, worst, median, and mean performance categories. Although slightly lower in the standard deviation category (3), its overall ranking sum underscores its superiority. jSO and MM_OED indicating comparable overall performance. jSO ranks second in the worst performance measure and third in both the median and mean categories. In contrast, MM_OED indicate ranks second in the best and median categories. These findings imply that both algorithms can produce reliable results, with jSO showing a slight consistency advantage, as seen in its higher ranking in the standard deviation category."}, {"title": "IV. CONCLUSION", "content": "In this paper, we introduced the Hierarchical Rank Aggregation (HRA) framework, which represents a new approach to the multi-criteria decision-making problem of evaluating and ranking metaheuristic optimization algorithms. HRA tackles the time-consuming and complex task of comparing algorithms by considering multiple performance metrics across various problem dimensions. Therefore, it is a valuable tool to assist researchers and practitioners in identifying the best algorithm for a given class of optimization problems in a single run. We demonstrated HRA's efficacy using the CEC'17 competition dataset, where 13 metaheuristics algorithms were evaluated on 30 benchmark functions with four dimensions each. The framework was able to combine several performance measures and rank algorithms in a way that showed the relationship between problem dimensions and algorithm performance.\nAlthough the present work used equal levels of importance among the hierarchies to keep the analysis straightforward, future studies may test different levels of priority or imbalance. In particular, integrating rank-ordering criteria weighting methods, could improve the evaluation process without raising concerns about the validity of the algorithm's assessment. The time complexity of HRA, which is $O(m \\log m)$, would classify HRA as computationally inexpensive for comparisons of algorithms in large-scale studies."}]}