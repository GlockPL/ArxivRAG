{"title": "Controlling Out-of-Domain Gaps in LLMs for Genre Classification and\nGenerated Text Detection", "authors": ["Dmitri Roussinov", "Serge Sharoff", "Nadezhda Puchnina"], "abstract": "This study demonstrates that the modern gen-\neration of Large Language Models (LLMs),\nsuch as GPT-4) suffers from the same out-\nof-domain (OOD) performance gap observed\nin prior research on pre-trained Language\nModels (PLMs, such as BERT). We demon-\nstrate this across two non-topical classification\ntasks: 1) genre classification and 2) generated\ntext detection. Our results show that when\ndemonstration examples for In-Context Learn-\ning (ICL) come from one domain (e.g., travel)\nand the system is tested on another domain\n(e.g., history), classification performance de-\nclines significantly.\nTo address this, we introduce a method that\ncontrols which predictive indicators are used\nand which are excluded during classification.\nFor the two tasks studied here, this ensures that\ntopical features are omitted, while the model is\nguided to focus on stylistic rather than content-\nbased attributes. This approach reduces the\nOOD gap by up to 20 percentage points in\na few-shot setup. Straightforward Chain-of-\nThought (CoT) methods, used as the baseline,\nprove insufficient, while our approach consis-\ntently enhances domain transfer performance.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Language Models\n(LLMs) have pushed the boundaries of natural lan-\nguage processing, leading to remarkable perfor-\nmance across a wide range of tasks (Brown et al.,\n2020; Floridi and Chiriatti, 2020; Bubeck et al.,\n2023). While their success with In-Context Learn-\ning (ICL) has gained particular attention, ques-\ntions remain regarding their consistency when ap-\nplied to unfamiliar domains. Unlike models simi-\nlar in size to BERT (Devlin et al., 2018), which re-\nquire a sample of labeled data for domain-specific\ntuning, LLMs are often used via ICL with lit-\ntle to no fine-tuning. However, this flexibility\ncomes at a cost, as LLMs frequently experience\na decline in performance when tested across do-\nmains (Yuan et al., 2024), a gap also observed in\nearlier, smaller models (Roussinov and Sharoff,\n2023). This degradation is often attributed to the\nmodels' reliance on surface-level features rather\nthan deeper, domain-independent attributes (Wang\net al., 2023).\nTo address this challenge, we introduce a\nmethod that controls which prediction indicators\nthe model considers during few-shot document\nclassification. While this approach is broadly ap-\nplicable to many non-topical document classifica-\ntion tasks, our focus here is on the ICL approach\nto two specific tasks: 1) the automated recognition\nof document genre, and 2) detection of computer-\ngenerated texts.\nGenre classification plays a vital role in fields\nsuch as information retrieval, automatic sum-\nmarization (Stewart and Callan, 2009), machine\ntranslation (Van der Wees et al., 2018), and depen-\ndency parsing (M\u00fcller-Eberstein et al., 2021). It\nalso aids information security by enabling genre-\naware assessments of web document credibility\n(Agrawal et al., 2019), and is crucial for curating\ngenre-diverse corpora to pre-train LLMs to build\nrobust foundation models (Kuzman et al., 2023;\nLepekhin and Sharoff, 2022). While recent ad-\nvancements in LLMs have shown that zero-shot\nmethods can achieve strong performance in genre\nclassification (Kuzman et al., 2023), another quali-\ntative exploratory study pointed out that a few-shot\napproach may severely suffer from OOD perfor-\nmance gaps (Roussinov and Sharoff, 2023).\nDetecting AI-generated text has become critical\nin verifying authenticity and maintaining informa-\ntion integrity. As models like GPT-4 produce in-\ncreasingly human-like text, the risks of misinfor-\nmation, plagiarism, and malicious content genera-\ntion have surged. The task involves distinguishing\nhuman-authored text from machine-generated out-\nputs, leveraging techniques that identify linguistic"}, {"title": "2 Methodology", "content": "Figure 1 illustrates the overall workflow for our\nexperiments. As in prior recent works on genre\nclassification, e.g., Kuzman et al. (2023) or\nRoussinov and Sharoff (2023), we define genre\nas the 'function of the text, author?s purpose, and\nform of the text.' Writing style is an important\ncharacteristic of genre but not the only one. Other\ncharacteristics include the intended audience, the\nmedium through which the text is delivered, and\nthe context of its use.\nFollowing the methodology in Roussinov and\nSharoff (2023), we define 'domain' as a topic\nin the topic modeling sense (Blei et al., 2003).\nOur approach, therefore, focuses on identifying\ndistinct topics such as sports, politics, or health\nfrom a large general-purpose corpus. This con-\ntrasts with several previous studies (e.g., Kuzman\net al., 2023), where 'domain' is defined by dif-\nferences in dataset collection or labeling methods.\nFor example, in their approach, book reviews and\nmovie reviews would be considered separate do-\nmains within a sentiment analysis task.\nWe employ the domain transfer assessment\nmethodology and datasets from Roussinov and\nSharoff (2023), originally developed to test out-\nof-domain (OOD) classification with BERT-sized\nPLMs, to further investigate and address the OOD\ngap in few-shot genre classification using ICL and\nlarge language models (LLMs). We propose and\nvalidate a domain transfer approach by adapting\nChain-of-Thought (CoT, Wei et al., 2022) prompt-\ning to control which text properties should be em-\nphasized (e.g., writing style, purpose) and which\nshould be ignored (e.g., specific topics). To the\nbest of our knowledge, no prior studies have ex-\nplored such control in this context.\nFrom our previous study (Roussinov and\nSharoff, 2023), we borrow the topic model estima-\ntion, the corpus of documents to classify, and the\nevaluation mechanism for the OOD effect. The\ncore idea is to simulate a scenario where a classi-\nfier is shown documents which are far away from\na particular topic (e.g., sports) and then its perfor-\nmance is evaluated on documents where that topic\nis prominent. This performance is compared to a"}, {"title": "2.1 The Approach", "content": ""}, {"title": "2.2 Corpus", "content": "The corpus from our previous study (Roussinov\nand Sharoff, 2023) provides good coverage of sev-\neral genres and topics. Up to our knowledge, there\nis no other large corpus for that purpose. The cor-\npus has been collected via \u201cnatural genre annota-\ntion\" by combining several sources so that each\nsource is relatively homogeneous with respect to\nits genres. The description of the genre classes fol-\nlows prior studies of genre types common on the\nWeb (Sharoff, 2018). The composition of the nat-\nural genre corpus is listed in Table 3 in Appendix.\nOur topic model was trained on ukWac, a much\nbigger topically diverse corpus (Baroni et al.,\n2009), to infer themes across all sources of our\nnatural genre corpus. In this way, we obtain two\ncomplementary perspectives on each document:\nits topic and its genre. For example, take the fol-\nlowing excerpt:\nFollowing Mary Smith's thorough review of this album,\nthere?s not much left to add, but I was so moved by the music\nthat I had to contribute my thoughts. Her review highlights\nkey tracks like Miles Davis' \"So What\" and John Coltrane's\n\"Blue Train\"\nThis document is classified as a Review (originat-\ning from the Amazon Review collection) and is\nlinked to Topic 1 (entertainment, see the labels\nin Table 4) based on our topic model. Reviews\ncan span various topics, such as science or his-\ntory. This dual classification?by both genre and\ntopic?enables us to effectively assess OOD perfor-\nmance across different domains."}, {"title": "2.3 Domain Transfer", "content": "To test the effect of a topic change we also used the\nmethodology suggested by Roussinov and Sharoff\n(2023), which is briefly summarized in this sub-\nsection. While developed specifically for docu-\nment genres, this methodology is applicable to\nany non-topical classification, so it has been tested\nhere on the task of detecting computer-generated\ntexts.\nWe make the following distinction between on-\ntopic and off-topic examples, e.g. sport. The\nhighest scoring documents, according to the topic\nmodel, are designated as on-topic examples for\neach genre. Additional (non-overlapping with\nthe ICL examples) highest scoring documents are\nused as test cases (test-set), associated with that\nparticular topic. The lowest scoring documents are\ndesignated as off-topic examples. In our example,"}, {"title": "2.4 Dataset for Generated Text Detection", "content": "Since publicly available datasets for generated text\ndetection do not support testing for out-of-domain\n(OOD) gaps and transfer, we synthesized our own\ndatasets. Using the same corpus as in our genre\nclassification experiments, we created off-topic\nand on-topic datasets for generated text detection.\nDuring this process, we excluded the PERSonal\ngenre from the corpus due to occasional adult con-\ntent that triggered API warnings in our prelimi-\nnary experiments (while the API did not object to\nanalyzing such texts, it refused to generate them).\nAdditionally, we excluded the INFOrmation and\nINSTRuction genres because the generated texts\nbased on these were structurally distinct from the\noriginals, making it trivial for both humans and\nmodels to distinguish between the generated and\noriginal texts.\nWe tasked Claude 3 Sonnet with generating text\n\"on the same topic and in the same style\" as the\ntexts from off-topic and on-topic documents in\nthe genre corpus, ensuring a balanced distribu-\ntion across the three remaining genres (ARGu-\nment, NEWS, and Review) to maintain diversity.\nThis process produced two synthesized datasets\nfor each topic: one with on-topic demonstration\nexamples and one with off-topic demonstration\nexamples, both sharing the same on-topic test\ntexts. Each dataset included 5-shot demonstration\nexamples and 10 test cases per topic, mirroring the\nsizes used in our genre classification task."}, {"title": "2.5 Metrics", "content": "While Kuzman et al. (2023) assessed LLMs as\ngenre classifiers through a multi-class task, we\nfollowed the approach in Roussinov and Sharoff\n(2023), who assessed ChatGPT through binary\nclassification between pairs of genres. This ap-\nproach reduces the number of examples in our\nprompts, allowing them to fit within the current\ncontext window limits of the models used, and\nto keep the costs reasonable. From the evalua-\ntion viewpoint, this formulation is methodologi-\ncally equivalent to multi-class classification (All-\nwein et al., 2000; Vapnik, 1995). For each topic,"}, {"title": "2.6 Prompts", "content": "We tested the configurations (prompt types) de-\nscribed in the subsections immediately below to\nevaluate the classification accuracy with the fol-\nlowing models through their application inter-\nfaces: GPT-40, GPT-3.5, Claude 3 Opus, Claude\n3 Sonnet, Claude 3.5 Sonnet, and Claude 3.5\nHaikus. These configurations are designed to pro-\ngressively add more control over which indicators\nthe model should prioritize. The prompts were de-\nveloped based on our preliminary experiments and\ninformed by prior works (Crowston et al., 2010;\nRehm et al., 2008; Stein et al., 2011). The same\nprompts were used consistently across all the mod-"}, {"title": "2.6.1 Baseline Prompt", "content": "As the baseline prompt for the genre classification\ntask, we used a Chain-of-Thought (CoT) prompt\n(Wei et al., 2022) without specifying which docu-\nment features to consider for classification. Since\nour focus is on large language models of GPT-\n3 size and larger, for which fine-tuning is pro-\nhibitively costly, we consider CoT prompting to\nbe a strong and relevant baseline for these mod-\nels. CoT prompts are particularly practical given\nthe constraints and objectives of our study. CoT\nprompting was applied as a two-stage process: (1)\ninstructing the LLM to articulate criteria for distin-\nguishing between classes using the examples pro-\nvided, and (2) asking the LLM to apply these crite-\nria to classify the test texts. For the generated text\ndetection task, the baseline prompt simply asked\nthe model to classify texts based on the provided\nexamples (single-stage)."}, {"title": "2.6.2 Prompt with Simple Control", "content": "In genre classification, the prompts explicitly in-\nstruct the model to classify based on document\ngenre without defining what \"genre\" is. For detect-\ning computer-generated texts, the prompts simply\nadd the instruction: \"When classifying, don't use\nthe topic of the text as a criterion\" to the baseline\nprompt."}, {"title": "2.6.3 Prompt with Detailed Control", "content": "In genre classification, the detailed prompts in-\nstruct the model to focus on stylistic and struc-\ntural indicators such as formality, tone, sentence\nstructure, language complexity, purpose (e.g., to\ninform, instruct, or facilitate dialogue), use of\nperspectives (first, second, or third person), ac-\ntive voice, and features like citations, references,\nor personal experiences. The Detailed Control\nprompts for both tasks explicitly prohibit using\ntopical content or text length as classification cri-\nteria, emphasizing that the analysis should remain\nuniversally applicable across all topics. For in-\nstance, the instructions state: \"Your criteria should\nnot mention any specific topics and should be ap-\nplicable to the texts on ANY topic!\" The prompts\nalso list examples of possible topics from our topic\nmodel, including \"business, finances, entertain-\nment, universities, markets, science, politics,\" and\nothers."}, {"title": "3 Results and Discussion", "content": "The results from both tasks are presented in Ta-\nble 1 and are discussed in detail in the following\nsubsections."}, {"title": "3.1 Confirming OOD Gap with LLMs", "content": "The \"Basic\" prompt shows significantly worse\nperformance across all models and tasks when off-\ntopic examples are used compared to on-topic ex-\namples (except for GPT-3.5 in the generated text\ndetection task, where both results are consistently\nlow). These differences are statistically signifi-\ncant at the 0.05 level, as confirmed by the Mc-\nNemar test (Dror et al., 2018). While this finding\naligns with previous studies (Kuzman et al., 2023;\nRoussinov and Sharoff, 2023), our work is the first\nto methodically quantify the OOD gap in few-shot\nICL prompts. Additionally, we extend these find-\nings by proposing a remedy for this gap, with re-\nsults discussed in the following subsection."}, {"title": "3.2 Positive Impact of Control", "content": "Applying even a basic level of control over which\nindicators the models should prioritize resulted in\nclassification accuracy improvements across most\nmodels, with gains of up to 6% on both tasks.\nBy guiding the models to focus on relevant fea-\ntures while disregarding misleading topical cues,\nwe observed enhanced performance in both genre\nclassification and generated text detection. These\nresults suggest that even modest interventions in\nhow models interpret input can significantly re-\nduce out-of-domain performance gaps."}, {"title": "3.3 Importance of Detailed Control", "content": "For the genre classification task, the \"Prompt with\nDetailed Control\" achieved the highest accuracies\nwith the more powerful models (GPT-40, Claude\n3 Opus, Claude 3.5 Sonnet), reducing the OOD\ngap by approximately 33% (relatively) for GPT-\n40 and nearly 50% (relatively) for Claude Opus.\nIn the generated text detection task, detailed con-\ntrol completely eliminated the OOD gaps for two"}, {"title": "3.4 Ablation Studies", "content": "The ablations targeted key components of the\nprompt: (1) removing the definition and examples\nof \"topical\" features, (2) omitting the explicit list-\ning and examples of style/genre-related features\nallowed for classification, and (3) reducing the de-\nscriptions of these features by half. The results\nshow a significant performance drop?more than\nhalf?towards baseline levels, with the omission of\ngenre-related features having the most pronounced\nnegative impact on accuracy.\nTo ensure that our findings were not overly de-\npendent on the specific wording of the prompts,\nwe followed the methodology of (Kirchenbauer\net al., 2023b) and used GPT-40 to paraphrase our\nprompts. As seen in the last six columns of Ta-\nble 2, while minor variations in accuracy occurred\nwith the rephrased prompts, the overall compara-\ntive trends remained stable, confirming the robust-\nness of our observations."}, {"title": "4 Related Work", "content": "While it has been noted that zero-shot and few-\nshot ICL based on LLMs suffers less from the\nOOD gap in comparison to fine-tuned smaller\nPLMs, there are no universally successful solu-\ntions for domain transfer yet (Yuan et al., 2024;\nEdwards and Camacho-Collados, 2024).\nThe use of ICL for domain adaptation in general\nhas been explored in (Long et al., 2023), which\nfocused on additional pre-training on the target\ncorpus and selecting the most similar examples\n(demonstrations) from the source domain. Our\nwork differs in several key aspects: 1) We specif-\nically investigate large language models (GPT-\n3.5 and larger), whereas Long et al. (2023) pri-\nmarily used smaller models, except for GPT-3.5-\nturbo, with which they reported negative results.\n2) We operate under the constraint of having no\nmore than five examples available (\"true\" few-\nshot), eliminating the need for example selection,\nwhich in general could serve as an additional av-\nenue for improvement."}, {"title": "4.1 Automated Genre Classification", "content": "For a recent survey on genre classification, includ-\ning fine-tuned smaller-size PLMs and ICL use of\nLarge Language Models, we refer the reader to\nthe work by Kuzman and Ljube\u0161i\u0107 (2023). They\nnote that genre classification is an important task\nas it relates to the very \"purpose\" of the text. For\ninstance, distinguishing a document genre with a\nhigh degree of humor (e.g. an anecdote) from\nthe one with factual information is crucial for\nits proper interpretation. Consequently, obtain-\ning genre information has been shown to be ben-\neficial for a wide range of disciplines, including\ncorpus linguistics, computational linguistics, nat-\nural language processing, information retrieval,\nand information security. Additionally, curating\ncorpora with a variety of genres for pre-training\nLLMs themselves is essential to ensure robust and\ncomprehensive foundation models (Kuzman et al.,\n2023; Lepekhin and Sharoff, 2022). As noted by\nKuzman et al. (2023), while BERT-sized mod-\nels demonstrate exceptional performance in genre\nclassification, significantly outperforming earlier\nSVM and other classical machine learning ap-\nproaches, they still require a considerable amount\nof labeled texts for fine-tuning. Recent advance-\nments have shown that instruction-tuned GPT-like\ngenerative models (Brown et al., 2020), when used\nin zero-shot or few-shot settings, can achieve com-\nparable or even superior results without the need\nfor large-scale labeling efforts. Building on the\nwork of Kuzman et al. (2023) and Roussinov and\nSharoff (2023), our focus is on facilitating do-"}, {"title": "4.2 Domain Transfer in Genre Classification", "content": "In addition to establishing strong zero-shot ICL\nperformance in genre classification, Kuzman et al.\n(2023) reported an out-of-distribution (OOD) gap\nwhen transferring trained BERT-sized models\nacross datasets from different sources. Their find-\nings were consistent with those reported for ear-\nlier PLM-based models, such as Lepekhin and\nSharoff (2022). In parallel work, Roussinov and\nSharoff (2023) developed a specialized method-\nology to examine BERT-sized PLM OOD per-\nformance in genre classification using a topically\ndiverse corpus and a topic model (Dieng et al.,\n2020). They also proposed a remedy based on\nsynthetic augmentation, which reduced the OOD\ngap by a few percentage points on average. Addi-\ntionally, Roussinov and Sharoff (2023) conducted\nwhat they described as a 'qualitative exploratory\nstudy' with the online interactive version of Chat-\nGPT, suggesting that larger (GPT3-sized) mod-\nels might also experience OOD performance gaps.\nHowever, they conducted a limited number of tests\nwith LLMs and did not perform the tests of sta-\ntistical significance. Our study here extends this\nwork by methodologically confirming these gaps\nusing two more recent and powerful families of\nLLMs (GPT-4.5 and Claude) accessed through\ntheir application interfaces, providing more con-\ntrolled and replicable testing conditions compared\nto manual online interactions. Most importantly,\nwe introduce a novel domain transfer approach by\ncontrolling the types of features used in classifi-\ncation, offering new insights into mitigating OOD\nperformance gaps in LLMs."}, {"title": "4.3 Generated Text Detection", "content": "The task of detecting text generated by large lan-\nguage models (LLMs) has become increasingly\ncritical as models like GPT-3 and beyond produce\nmore human-like content. Accurately distinguish-\ning between human-written and AI-generated text\nis essential for curbing misinformation, maintain-\ning academic integrity, and preserving content au-\nthenticity across platforms. Detection methods\nrange from statistical analysis to watermarking\nand classifier-based systems, which can be fine-\ntuned or employed using zero-shot or few-shot In-\nContext Learning (ICL) approaches (Tang et al.,"}, {"title": "5 Conclusions and Further Work", "content": "Our key contribution lies in demonstrating that\nmore careful prompt control In-Context Learn-\ning (ICL) can lead to enhanced performance in\nnon-topical classification, particularly by enabling\nmore effective domain transfer in genre classifi-\ncation and generated text detection. By introduc-\ning prompts to control which indicators should\nbe prioritized or ignored, we achieved substan-\ntial improvements in ICL, reducing out-of-domain\n(OOD) performance gaps in LLMs by up to 20\npercentage points across multiple topics. This\nmethod enables demonstration examples from one\ndomain (e.g., sport) to be successfully applied to\nanother (e.g., science), potentially reducing man-\nual labeling costs and offering valuable insights\nfor researchers and AI developers.\nOur innovation lies in the detailed control over\nclassification criteria, systematically tested across\ntwo tasks. This approach yields notable im-\nprovements and, like CoT, its simplicity is a\nstrength?offering a practical, effective method for\nenhancing LLM performance without extensive\nretraining or complex changes.\nOur findings further highlight the superior per-\nformance of bigger and more advanced models\nlike GPT-40 and Claude 3 Opus with detailed\nprompts in complex classification tasks. The abla-\ntion studies underscore the importance of each ele-\nment in our method, confirming that prompt speci-\nficity plays a critical role and that the system re-\nmains robust even when prompts are paraphrased.\nLooking ahead, more research is needed to ex-\nplore cross-lingual capabilities. While previous\nresearch (Kuzman et al., 2022; R\u00f6nnqvist et al.,\n2021) has demonstrated that BERT-like models\ncan be applied across languages, testing whether\nour approach to prompt control can extend to non-\nEnglish texts remains a key challenge. This would\nrequire the development of a large multilingual"}, {"title": "6 Limitations", "content": "We considered two non-topical tasks, each evalu-\nated with its respective dataset. To the best of our\nknowledge, no additional datasets are currently\navailable for OOD exploration for these specific\ntasks. One key limitation of our study is the re-\nliance on natural genre annotations, which may\nsimplify the classification task, because natural\ngenre labels can introduce superficial cues, such as\nformatting, that make classification easier than it\nwould be with manually controlled labeling. This\nreliance raises questions about whether the model\nis learning the intended genre features or simply\nexploiting external characteristics. Additionally,\nour study is limited to English texts, and it re-\nmains unclear whether our findings would gener-\nalize to other languages with different linguistic\nstructures and genre conventions. The lack of a\nsuitable multilingual corpus with sufficient genre\nannotation and topical diversity restricts our cur-\nrent focus, though exploring non-English applica-\ntions is a clear area for future work.\nAnother potential limitation is the sensitivity of\nour results to prompt variations. Although we de-\nsigned prompts to ensure robustness, exhaustively\ntesting all possible configurations is not feasible,\nand there may be subtleties in prompt phrasing\nthat impact performance. While we observed that\nreducing topical criteria in outputs tends to im-\nprove accuracy, we cannot conclusively determine\nthe underlying mechanics without further explo-\nration.\nWe also recognize that our use of black-box\nLLMs from two commercial providers poses chal-\nlenges to both generalizability and reproducibil-\nity. The black-box nature limits insight into the\ninternal workings of the models, making it dif-\nficult to interpret how they process prompts and\nfeatures. Additionally, these results can only be\nreproduced as long as the APIs for these LLMs re-\nmain available and stable over time. As an alterna-\ntive, deploying LLMs on local clusters could pro-\nvide more transparency and control, but this would\nrequire significant computational resources, which\nmay not be readily available.\nFinally, while our study focused on genre clas-\nsification and generated text detection, future re-"}, {"title": "7 Ethical Impact", "content": "The potential societal benefits of our findings\nare substantial, particularly in improving content\nmoderation, information retrieval, and personal-\nized recommendations. By enhancing the accu-\nracy of genre classification and generated text de-\ntection, we can contribute to more efficient digi-\ntal ecosystems, where content is categorized more\neffectively, misinformation is reduced, and edu-\ncational tools are made more accurate. These\nimprovements can positively impact user experi-\nences, making digital platforms safer and more in-\nformative.\nHowever, alongside these benefits come notable\nethical risks. One significant concern is the possi-\nbility of reinforcing existing biases, especially if\nthe training data lacks diversity or fails to repre-\nsent a broad spectrum of perspectives. Such biases\ncould lead to unfair outcomes, perpetuating stereo-\ntypes or marginalizing certain groups. As LLMs\nare increasingly used in various decision-making\nprocesses, the potential for such biased outputs to\ninfluence real-world outcomes becomes a critical\nissue that requires attention.\nAnother potential risk involves the control of in-\ndicators in prompts. While controlling which fea-\ntures LLMs prioritize can be a powerful tool for\nimproving performance, it also opens the door to\nmisuse. The same techniques that enhance genre\nclassification and text detection could be exploited\nto bias outputs in other domains. For instance, in\nnews generation or summarization, prompts could\nbe manipulated to emphasize particular narratives\nor viewpoints, subtly shaping public opinion or\nspreading misinformation. The misuse of such\ncontrols could have far-reaching implications, es-\npecially in sensitive areas like media, politics, and\npublic discourse.\nTo mitigate these risks, it is crucial to ensure\ntransparency, fairness, and accountability in how\nindicator control is applied. Developing frame-\nworks that guard against manipulation and bias,\nwhile promoting robustness and fairness, is essen-\ntial to upholding the ethical use of generative AI"}, {"title": "A Appendix", "content": ""}]}