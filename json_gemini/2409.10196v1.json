{"title": "NEUSIS: A Compositional Neuro-Symbolic Framework for Autonomous Perception, Reasoning, and Planning in Complex UAV Search Missions", "authors": ["Zhixi Cai", "Cristian Rojas Cardenas", "Kevin Leo", "Chenyuan Zhang", "Kal Backman", "Hanbing Li", "Boying Li", "Mahsa Ghorbanali", "Stavya Datta", "Lizhen Qu", "Julian Gutierrez Santiago", "Alexey Ignatiev", "Yuan-Fang Li", "Mor Vered", "Peter J. Stuckey", "Maria Garcia de la Banda", "Hamid Rezatofighi"], "abstract": "This paper addresses the problem of autonomous UAV search missions, where a UAV must locate specific Entities of Interest (EOIs) within a time limit, based on brief descriptions in large, hazard-prone environments with keep-out zones. The UAV must perceive, reason, and make decisions with limited and uncertain information. We propose NEUSIS, a compositional neuro-symbolic system designed for interpretable UAV search and navigation in realistic scenarios. NEUSIS integrates neuro-symbolic visual perception, reasoning, and grounding (GRID) to process raw sensory inputs, maintains a probabilistic world model for environment representation, and uses a hierarchical planning component (SNaC) for efficient path planning. Experimental results from simulated urban search missions using AirSim and Unreal Engine show that NEUSIS outperforms a state-of-the-art (SOTA) vision-language model and a SOTA search planning model in success rate, search efficiency, and 3D localization. These results demonstrate the effectiveness of our compositional neuro-symbolic approach in handling complex, real-world scenarios, making it a promising solution for autonomous UAV systems in search missions.", "sections": [{"title": "I. INTRODUCTION", "content": "The development of autonomous agents capable of safely completing Intelligence, Surveillance, and Reconnaissance (ISR) missions in complex environments presents significant challenges [7]. Unmanned Aerial Vehicles (UAVs) are increasingly utilized in these missions due to their ability to cover large areas and access hazardous locations with minimal risk to human life [21]. However, designing fully autonomous UAV systems for such tasks, given onboard sensory data and brief mission descriptions in unpredictable and complex environments with uncertain knowledge, remains a formidable challenge.\nIn this paper, we focus on life-like scenarios in which a UAV must, within a designated time limit, autonomously search for a number of specific Entities of Interest (EOIs) based on brief descriptions, e.g., find \u201ca red SUV vehicle\" or \u201ca pedestrian carrying a blue umbrella\", in a large suburban or urban environment that may contain hazards or keep-out zones (KOZs). These hazard zones represent significant risks that the UAV must carefully avoid while efficiently searching designated Areas of Interest (AOIs) [25]. To successfully operate in such scenarios, an autonomous UAV must actively and reliably perceive the environment from onboard sensory measurements, reason about the environment, and make decisions based on the mission description and partial or uncertain information about the surroundings.\nRecently, advances in Large Multimodal Models (LMMs) [24], [27], [28] have shown promise in different robotics tasks. However, their reliance on diverse, large-scale datasets for training as monolithic end-to-end models imposes significant computational demands. These models often lack interpretability when they fail and struggle to generalize beyond their training domains, especially in adversarial settings [10], [31]. Furthermore, LMMs lack explicit components to model the world state or update their knowledge, which is crucial for complex tasks like searching in unconstrained environments. Alternatively, many autonomous robotics systems employ compositional approaches [2], [14], [18], integrating explicit perception and planning to perform tasks. These systems use neural-based perception models to process sensory data into abstract representations like segmentation, detection, or captions,"}, {"title": "II. ENVIRONMENT AND MISSION", "content": "Our benchmarks are based on the Hybrid AI Mission Environment for RapId Training and Testing (HAMERITT) system presented in [12] for evaluating participants of the DARPA ANSR program. HAMERITT is a platform for UAV simulation and testing, based on the AirSim [29] plugin for Unreal Engine, capable of dynamically generating complex evaluation scenarios. In this section, we will provide a high-level overview of the features used for our benchmarks. A detailed explanation of its capabilities can be found in [12].\nWe use HAMERITT's Neighborhood environment for emulating surveillance and reconnaissance missions in urban settings. The environment contains a broad 500x500 meter search area densely populated with a diverse selection of world objects (e.g., houses, trees, fences, roads, and non-EOI cars), that can be adversarially positioned to challenge perception with occlusions, and planning with complex navigation tasks. \n\nThe UAV's mission is to identify as many entities-of-interest (EOI) as possible within the specified areas-of-"}, {"title": "III. THE NEUSIS SYSTEM", "content": "NEUSIS, Neuro-Symbolic Intelligent Search, is a compositional framework comprised of three main components: a neuro-symbolic visual perception, reasoning and grounding component (GRiD), a symbolic world model, and a symbolic hierarchical planning component (SNaC), shown in Figure 4.\nThe Perception, Grounding, Reasoning in 3D (GRiD) component processes UAV sensor data, i.e., RGB, depth, and GPS location for robust visual reasoning and 3D object grounding in complex search missions. A key challenge in the UAV missions described in Section II is not only localizing EOIs in 3D space but also inferring their attributes. GRID addresses this by integrating visual perception, grounding, and neuro-symbolic reasoning.\nGRiD builds on recent advances in neuro-symbolic com-positional visual reasoning methods [8], [16], [30], [32], [35], which tackle complex visual reasoning tasks, such as visual grounding, by decomposing them into sub-tasks. These sub-tasks are individually solved using vision foundation models and large language model (LLM)-generated code, with the results combined to complete the overall task. For GRiD, we adopt HYDRA [11], a state-of-the-art neuro-symbolic reasoning system that combines reinforcement learning with LLM-driven code generation to enable dynamic, compositional visual understanding. While HYDRA is designed for 2D image-based reasoning tasks (e.g., visual grounding and question answering), it requires adaptation to handle perception, reasoning, and grounding from the sensor stream of visual data in the UAV's 3D search mission.\nTo adapt HYDRA for this mission, we expand GRiD's toolkit to include 2D target bounding boxes with attributes recognition, instance segmentation, object tracking, and 3D coordinates projection. The following Python APIs are implemented to meet mission requirements: segment, classify_object_attributes, classify_object_types, track, and project_to_3d. We integrate state-of-the-art VFMs for grounding, segmentation, property classification, and 2D tracking. Object tracking employed a symbolic method [3]. Further implementation details are provided in Section IV-A.\nTo avoid computational bottlenecks and resource overuse, we implemented a caching mechanism for the LLM-generated reasoner code, allowing reuse of reasoner plans across similar mission queries in different scenarios.\nDue to the inherent noise in sensor inputs, GRiD is rarely 100% confident in its output. The world model accumulates"}, {"title": "B. Probabilistic World Model", "content": "localization information from GRiD to maintain a persistent probabilistic representation of the environment and provide a mechanism for identifying and reporting EOIs.\nThe world model is initialized with a ground-truth voxel occupancy grid and a birds-eye view (BEV) segmentation map, indicating the locations of static objects like walls, trees, and roads. It is also provided with the initial prior belief map from the mission description. For each frame, it receives noisy 3D localization data and attributes from GRID and performs the following tasks:\nTo refine 3D localization, the world model uses domain knowledge, such as removing infeasible points from masked depth data to compute more accurate 3D center points, and discarding detections that violate physical constraints (e.g., cars high above ground or inside walls).\nTo improve 3D localization and attribute classification the world model can accumulate detections about the same objects over time. A na\u00efve approach would compute the average position of detections, but this may not take into account the uncertainty of GRiD's outputs. Instead, we use (i) Bayesian filtering for position refinement and (ii) discrete attribute distribution ranking updates for more accurate attribute likelihoods. This process enhances the probabilistic model of the world, increasing confidence in positions and attributes over time.\nThe world model generates online reports by evaluating whether any tracked objects match the EOI descriptions. It reasons about the probability of a match, and any candidates exceeding a confidence threshold are reported. A final offline report summarizing the best detection of each EOI is produced at the end of the mission.\nIn addition to the functions mentioned above, the World Model also maintains environmental information relevant to the planning component, including the voxel occupancy grid and belief map, to support path planning operations."}, {"title": "C. Selection, Navigation and Coverage (SNaC)", "content": "The planning component is designed to generate a trajectory that efficiently searches the AOIs by maximizing the likelihood of encountering EOIs within the allocated time. The component first retrieves the belief map and other environmental information from the World Model component, and then generates a sequence of waypoints to be sent to the UAV's control unit. While this task is closely related to area coverage and object goal navigation, the existence of multiple EOIs within the AOIs across a large environment introduces the following complexities: there is no fixed order for visiting the AOIs, and the UAV's objective is to identify as many EOIs as possible within the given time, which is insufficient to cover all AOIs.\nTo achieve this, SNaC employs a hierarchical approach, dividing the task into three distinct modules: Selection (high-level planning), Navigation (mid-level planning), and Coverage (low-level planning). The AOI Selection module leverages the belief map to compute a high-level route between AOIs and to allocate the exploration time for each AOI. Based on the output from the Selection module and other relevant environment information from the world model, the Navigation module then performs the path planning to reach the selected AOI, and once there, the Coverage module plans how to systematically search for EOIs in the area.\nThe AOI Selection module aims to determine the approximate optimal sequence of AOIs to explore and allocate appropriate amounts of time for each. We model this as a constraint optimization problem [1], where the objective function maximizes the likelihood of detecting EOIs within the allocated time while minimizing travel time. This is done by considering the size of each AOI, the travel distances between them, the required exploration time and the probability of each AOI containing an EOI. We used the MiniZinc [22] modeling language to model the problem, and the Chuffed [5] solver to produce solutions.\nOnce an AOI is selected, the Navigation module generates a plan for travelling to that area by constructing a visibility graph [23] using information regarding KOZs and voxel occupancy. Subsequently, it executes an A* [9] algorithm to determine the optimal path, ensuring avoidance of both obstacles and KOZ.\nAfter reaching an AOI, the coverage module plans the low-level search for EOIs. It begins by converting the AOI into a grid, thus, representing the coverage task as the exploration of all accessible grid points. To achieve this we first create an open set of points to be visited. Then the module greedily finds the nearest non-visited point from the starting position, and uses the A* algorithm to navigate to that point while avoiding any obstacles or KOZ. Subsequently, the UAV navigates towards that point along the computed path, and removes visited points from the open set. Once the open set becomes empty, or all EOIs are found, the search concludes. The belief map in the World Model will be updated based on the EOIs found in that AOI, and the updated belief map will be used by the Selection module to select the next AOI."}, {"title": "IV. EXPERIMENTS", "content": "We implemented our proposed system by integrating the GRID, World Model, and SNaC components, and compared its performance against a framework built using state-of-the-art (SoTA) solutions for perception and planning. This comparison highlights the contribution of our system to the specific problem. Additionally, we conducted several ablation studies for each component to assess the impact of each module and the specific features they contribute.\nFor GRiD, we use SoTA [15] for grounding, linear probed CLIP [26] for color/type classifiers, and OCSort [3] as the 2D tracker to assign tracking IDs to targets. After generating the 2D bounding box for the detected target by the visual grounding model, we use EfficientSAM [33] to obtain the pixel mask of the target. Using the depth sensor\ndata, we compute the 3D coordinates of all pixels within the mask as a point cloud. The 3D location of the target is determined by averaging the points within this cloud, and then the 3D location is sent to the world model for further reasoning. In ablation studies, we follow HYDRA [11] to use GLIP [13] for grounding and XVLM [36] for zeroshot color/type classifiers, as the original VFMs.\nThe baseline system is composed of YOLO-World [4] for the perception component and Fields2Cover [20] for the planning component. These components were originally selected for the DARPA ANSR program, ensuring their relevance and suitability for our task. YOLO-World represents the state-of-the-art in vision-language models (VLMs), being known for its efficiency and high performance in 2D grounding tasks. Since YOLO-World does not provide estimated segmentation masks for EOIs to enhance their 3D localization, we compute their 3D coordinates by projecting the center of the 2D bounding box using the available depth data. On the planning side, Fields2Cover is a symbolic, model-based planner widely used for autonomous planning due to its robustness and efficiency in navigating complex environments."}, {"title": "B. Metrics", "content": "The primary criterion for evaluating the system's performance is the correct identification of EOIs (i.e., reported within 5 meters of the ground truth position). We define the success rate (SR) as $n/N$, where $N$ is the total number of EOIs and $n$ is the number of successfully detected EOIs. This differs slightly from previous work [6] to account for multiple EOIs. The average SR across test scenarios is the main measure of overall performance. This metric integrates the performance of both components: the planning component must generate paths that allow the camera to capture frames containing EOIs, while the perception and reasoning component must accurately identify EOIs within those frames."}, {"title": "C. Quantitative Comparison", "content": "We compared the F1 score, SR (success rate), and search time across different configurations of planning and perception components, as shown in Table I. When replacing Fields2Cover with SNaC (row 2), we observe a significant improvement in navigating efficiency, particularly when searching for multiple EOIs, and most notably when finding"}, {"title": "D. Qualitative Comparison", "content": "To better understand the results of our experiments, we examined visualisations of the behaviour of the different configurations. Figure 5 provides an example of flight paths and EOI reports from a) our proposed system NEUSIS, and b) the baseline system based on Field2Cover and YOLO-World. From these figures, it can be seen that both approaches adequately cover the AOIs that contain EOIs. The Fields2Cover approach employs a deliberate back and forth search strategy that systematically covers the AOIs. However, YOLO-World produces many false positives (filled red squares away from the targets), and also generated noisy output near the ground truth targets. NEUSIS's planning component SNaC performs a more bespoke exploration that allows GRiD and the world model to see potential EOIs from more angles, thus making more confident reports. Overall, the qualitative visualisation shows the advantages of our integrated neuro-symbolic system in both navigation efficiency and target detection performance."}, {"title": "E. Ablation Studies", "content": "We conducted extensive ablation studies to evaluate the impact of different modules in the GRiD component. Table II presents the results for various configurations, to evaluate the contributions of each module in GRiD. Online and offline perception metrics (F1, precision and recall) are used for comparison. The first two rows highlight the impact of 3D projection methods, demonstrating that point cloud-based 3D projection significantly outperforms projecting the center point of 2D bounding boxes. The results from rows 2, 3, and 4 show the substantial positive contribution of SoTA VFMs and color/type classifiers. Finally, comparing rows 4, 5, and 6, we observe the effectiveness of integrating the"}, {"title": "World Model", "content": "Ablation studies for the world model are presented in Table III. Starting with a basic version that only performs basic world reasoning, we see that the addition of information accumulation with na\u00efve filtering (using the average 3D position), only provides a small improvement for online F1 Score (42.57% \u2192 44.62%). When Bayesian filtering is added we see a much larger improvement, in particular we see a 10% increase in online F1 Score, demonstrating the importance of correctly handling uncertainty."}, {"title": "SNaC", "content": "Table IV presents the ablation study for the SNaC component with ground truth perception which reports the targets in 25 meters. Starting with the baseline version, which employs Fields2Cover [19] for area coverage and a randomly selected route for the AOIs, the introduction of AOI Selection based on the belief map shows a significant improvement in the success rate, nearly doubling it (from 20.83% to 43.75%) while also reducing the search time. Incorporating MiniZinc optimization further enhances both the success rate (up to 53.45%) and planning efficiency by providing a more optimized method for determining the AOI visitation order and exploration time allocation. Finally, the addition of the proposed area Coverage module raises the success rate to 54.51%, demonstrating its effectiveness in improving low-level search coverage throughout the environment."}, {"title": "V. CONCLUSION", "content": "This paper presented NEUSIS, a compositional neuro-symbolic system for autonomous UAVs in complex search missions. By integrating neuro-symbolic perception (GRiD), a probabilistic world model, and a hierarchical symbolic planning component (SNaC), our approach enables efficient target detection, reasoning, and navigation. Extensive experiments demonstrate that NEUSIS significantly outperforms baselines in success rate, search efficiency, and localization performance.\nNEUSIS has potential for real-world applications such as search-and-rescue missions, improving UAVs' ability to locate targets in hazardous environments. Our system has been tested only in simulated environments and relies on accurate positional data, voxel grids, and point clouds. This can be addressed in future work."}]}