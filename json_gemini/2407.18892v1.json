{"title": "SHANGUS: Deep Reinforcement Learning Meets Heuristic Optimization for Speedy Frontier-Based Exploration of Autonomous Vehicles in Unknown Spaces", "authors": ["Seunghyeop Nam", "Tuan Anh Nguyen", "Eunmi Choi", "Dugki Min"], "abstract": "This paper introduces SHANGUS, an advanced framework combining Deep Reinforcement Learning (DRL) with heuristic optimization to improve frontier-based exploration efficiency in unknown environments, particularly for intelligent vehicles in autonomous air services, search and rescue operations, and space exploration robotics. SHANGUS harnesses DRL's adaptability and heuristic prioritization, markedly enhancing exploration efficiency, reducing completion time, and minimizing travel distance. The strategy involves a frontier selection node to identify unexplored areas and a DRL navigation node using the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm for robust path planning and dynamic obstacle avoidance. Extensive experiments in ROS2 and Gazebo simulation environments show SHANGUS surpasses representative traditional methods like the Nearest Frontier (NF), Novel Frontier-Based Exploration Algorithm (CFE), and Goal-Driven Autonomous Exploration (GDAE) algorithms, especially in complex scenarios, excelling in completion time, travel distance, and exploration rate. This scalable solution is suitable for real-time autonomous navigation in fields such as industrial automation, autonomous driving, household robotics, and space exploration. Future research will integrate additional sensory inputs and refine heuristic functions to further boost SHANGUS's efficiency and robustness.", "sections": [{"title": "I. INTRODUCTION", "content": "Intelligent vehicles, essential for autonomous airport services [1], search and rescue operations [2, 3], and space exploration [4], exhibit high-level autonomy and can navigate and operate in complex, unknown environments. While current systems perform self-navigation and manipulation given a pre-constructed environment model, some applications necessitate autonomous perception capabilities due to the absence of prior environment models. Leveraging 3D mapping and DRL, these vehicles autonomously navigate and execute tasks, minimizing human intervention and enhancing safety [5]. Existing systems often require partial pre-known environment information for path planning [6, 7], but these frameworks are ineffective in entirely unknown settings. Alternatively, active SLAM strategies without pre-known information [8\u201310] have limited application due to their non-optimal greedy exploration or poor generalization in complex 3D scenarios. Consequently, a strategy integrating the strengths of these existing methods is needed.\nEffective robot integration depends on adapting to environmental data through frontier-based exploration, yet current algorithms are inefficient in exploration and collision avoidance [11]. Recent DRL advancements enable superior, real-time navigation, outperforming traditional methods in dynamic obstacle avoidance [12, 13]. DRL faces challenges in complex environments, prompting research into combining DRL with global path planning and heuristics [14]. Unlike move_base or nav2, DRL adapts to real-time goals and unexplored spaces, addressing inefficiencies in existing frontier-based exploration strategies.\nThis work proposes an innovative framework combining DRL and heuristic functions to enhance frontier selection, surpassing existing algorithms. The framework emphasizes efficient exploration beyond visited areas, demonstrating superior performance over traditional methods. It highlights the importance of effective algorithms for initializing and navigating diverse environments. The integration of DRL and heuristic functions redefines indoor robot navigation, achieving remarkable efficiency. The DRL-based frontier selection algorithm assigns weights to distances using a normalized hyperbolic-exponential function. Prioritizing exploration points as closed, open, or step structures, the point with the minimum score (argmin) becomes the DRL control goal.\nThe key contributions and findings of this study are as follows:\n\u2022\n\u2022 Proposed Speedy Heuristic Approach for Navigating Geographical Unexplored Spaces (SHANGUS): The SHANGUS framework integrates deep reinforcement learning (DRL) with heuristic optimization to enhance frontier-based exploration efficiency in unknown environments. It leverages DRL's adaptability and heuristic prioritization, significantly enhancing exploration efficiency, reducing completion time, and travel distance.\n\u2022 Novel Frontier Selection and Navigation Strategy: This strategy includes a frontier selection node for identifying unexplored areas and a DRL navigation node using the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm for robust path planning and obstacle avoidance. The heuristic function prioritizes exploration points by considering distance and occupancy stochastic scores, ensuring the selection of valuable, reachable frontiers.\n\u2022 Comprehensive Experimental Evaluation and Supe-"}, {"title": "II. RELATED WORKS", "content": "Frontier-based exploration strategies are pivotal in autonomous robotics, systematically guiding the exploration of unknown environments by identifying boundaries between explored and unexplored areas. Research spans traditional algorithms to advanced methods integrating machine learning, particularly reinforcement learning (RL).\na) Traditional Frontier-Based Methods: Yamauchi [15] laid the foundation for frontier-based exploration, focusing on geometric identification of frontiers [16]. Enhancements include energy-efficient path planning [17], clustering algorithms for frontier management [18], and heuristic optimizations [19].\nb) Reinforcement Learning Enhancements: Integrating RL into frontier-based exploration allows dynamic adaptation and optimization of exploration strategies. RL-based methods, such as those by Li et al. [20], use deep learning to enhance real-time navigation in complex environments [2], reducing exploration time and increasing coverage efficiency [12].\nc) Hybrid and Advanced Methods: Hybrid methods combining traditional frontier-based approaches with RL and other machine learning techniques leverage the strengths of both. Cao et al. [13] and Jain et al. [21] merge RL with traditional metrics for better decision-making. Mackay et al. [22] incorporate dynamic elements to tackle challenges in environments with moving obstacles.\nd) Evaluation and Comparisons: Evaluations and comparisons of different methods are crucial. Xu et al. [23] provide benchmarks and datasets to assess frontier-based and RL-based exploration strategies, highlighting their performance across diverse scenarios.\ne) Remarks: Frontier-based exploration is evolving with innovations integrating complex computational models and learning algorithms. Future directions include adaptive learning mechanisms and multi-robot system exploration [24], enhancing collaborative autonomous exploration capabilities."}, {"title": "III. SHANGUS: SPEEDY HEURISTIC APPROACH FOR NAVIGATING GEOGRAPHICAL UNEXPLORED SPACES", "content": "The SHANGUS framework is designed for autonomous navigation using Gazebo for simulation. It includes several components. The Robot State Publisher, equipped with a LiDAR sensor providing a 360-degree scan array (Scan (n=360)) and an odometry sensor tracking position (Pose: x,y,z) and orientation (raw, pitch, yaw), along with Turtlebot3 Waffle PI's joint state, supplies vital data. The SLAM Node, using a Bayesian Revision Cycle, generates and updates an occupancy grid map (Occupancy Grid Map). The Frontier Selection Node detects unexplored areas, generating point arrays (Point Arrays) and uses a heuristic function (F = {(x,y) | given frontier points} \u2192 minh(fx, fy)) for goal selection. The DRL Navigation Node handles local path planning and collision avoidance, using a DRL Control Agent (TD3) for decision-making based on states (s, s'), rewards (r), and actions (a). The Automatic Control Manager manages sensor data, reward calculation, actuation, and acts as a proxy for the DRL agent. Control signals for linear and angular velocity (Linear velocity, Angular velocity) are sent to the Turtlebot3, enabling navigation towards goals while avoiding obstacles. This architecture integrates SLAM, frontier-based exploration, and deep reinforcement learning for robust autonomous navigation."}, {"title": "B. SLAM", "content": "SLAM is pivotal in robotics, especially for path planning, a key aspect of this research. SLAM allows a robot to map observed landmarks while probabilistically determining its location. Here, we employ SLAM from the nav2 package in ROS2. Sensor Data Collection: Nav2 collects environmental data via LiDAR and cameras, formatted as ROS 2 messages. Costmap Generation: This data generates a costmap of the robot's environment, highlighting navigable areas and obstacles. Robot Localization using AMCL: AMCL, a probabilistic algorithm, estimates and updates the robot's position using sensor data. Application of SLAM Algorithm: Nav2 uses SLAM algorithms, like gmapping, to map the environment, enabling simultaneous exploration and localization. Map Update and Path"}, {"title": "C. TD3-based Autonomous Navigation", "content": "DRL is instrumental in autonomous robot navigation, learning complex policies from high-dimensional sensor inputs via environmental interactions. Unlike traditional control methods relying on manual features and prior maps, DRL excels in dynamic, unknown environments, crucial for real-time navigation. Algorithms like Proximal Policy Optimization (PPO) and Deep Q-Network (DQN) enhance policy learning stability and efficiency. PPO uses actor-critic networks with residual blocks to improve information flow and mitigate vanishing gradients, accelerating learning. Sensors such as LiDAR and cameras enable environmental comprehension, optimizing decision-making. Simulation environments like ROS and Gazebo facilitate safe, extensive training, with reward functions guiding behaviors like target approach and collision avoidance. Challenges include data inefficiency and generalization, with ongoing research improving sample efficiency and transfer learning for better real-world adaptation [25]. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm, developed by Scott Fujimoto et al. [26], is a Policy Gradient method improving upon Deep Deterministic Policy Gradient (DDPG) for high-dimensional continuous action spaces. TD3 addresses value function overestimation in value-based (e.g., DQN) and policy-based (e.g., Actor-Critic) reinforcement learning using Clipped Double Q-learning, Delayed Policy and Target Updates, and Target Policy Smoothing, enhancing stability and performance.\nIn Clipped Double Q-learning, TD3 uses two independent critic networks to estimate the value function, reducing overestimation by taking the minimum value between the two critics, thus enhancing learning stability.\n$Y1 = r + \\gammaQ_{\\theta'2} (s', \\pi_{\\phi'1} (s'))$\n$Y2 = r + \\gammaQ_{\\theta'1} (s', \\pi_{\\phi'2} (s'))$\n(1)\n$y = r + \\gamma min (Q_{\\theta'2} (s', \\pi_{\\phi'1} (s')), Q_{\\theta'1} (s', \\pi_{\\phi'2} (s')))$\nHere, r is the reward, $\\gamma$ is the discount factor, $Q_{\\theta'1} (s', \\pi_{\\phi}(s'))$ and $Q_{\\theta'2} (s', \\pi_{\\phi}(s'))$ are the Q-values from the critic networks with parameters $\\theta'1$ and $\\theta'2$, and $\\pi(s')$ is the action selected by the policy network at the next state s'. By taking the minimum of these Q-value predictions, TD3 mitigates overestimation, a common issue in reinforcement learning algorithms.\nIn Delayed Policy and Target Updates, TD3 updates the policy network (actor) less frequently than the value networks (critics) to ensure the value estimates are more accurate. Typically, the policy network is updated once every two or more updates of the critic networks. This method reduces policy overfitting by averaging Q-values over a range of actions around the target action, thereby improving stability and performance.\n$\\theta \\leftarrow \\tau \\theta + (1 - \\tau)\\theta'$\n(2)\nwhere $\\tau$ is the update coefficient.\nIn the Target Policy Smoothing technique, TD3 adds noise to the target policy during updates to address overfitting and enhance robustness. This approach regularizes value estimation around the target action, mitigating inaccuracies from function approximation errors, and ensuring stable, effective training.\ny = r + E_{\\epsilon ~ clip(N(0,\\sigma),-c,c)} [Q_{\\theta'} (s', \\pi_{\\phi'} (s') + \\epsilon)]\n$\\epsilon ~ clip(N(0, \\sigma), -c, c)$\n(3)\nApplying these three techniques, TD3 enhances performance and stability in reinforcement learning, making it a"}, {"title": "D. Speedy Frontier Exploration and Goal Selection Algorithm", "content": "Frontier Detection: The frontier detection algorithm in Algorithm 2 requires the occupancy grid map M. It identifies free space (of), obstacles (\u03c3\u03bf), and unknown space (\u03c3\u03b7). The algorithm finds ou cells adjacent to of and expands these to"}, {"title": "V. CONCLUSION", "content": "Our research demonstrates that the SHANGUS framework, incorporating both heuristic optimization and deep reinforcement learning (DRL), significantly enhances exploration efficiency in unknown environments. Experimental results across three scenarios of varying complexity validate the superiority of SHANGUS algorithms, especially when integrated with DRL. The SHANGUS w/ DRL algorithm consistently achieved the highest exploration rates and shortest completion times, outperforming other traditional and advanced frontier-based exploration methods. These findings underscore the potential of combining heuristic methods with DRL to optimize autonomous navigation, making SHANGUS a promising solution for real-time applications in diverse robotic exploration tasks. Future work will focus on further refining the heuristic functions and integrating additional sensory inputs to enhance robustness and adaptability in more complex and dynamic environments."}]}