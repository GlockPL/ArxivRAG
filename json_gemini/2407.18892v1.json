{"title": "SHANGUS: Deep Reinforcement Learning Meets Heuristic Optimization for Speedy Frontier-Based Exploration of Autonomous Vehicles in Unknown Spaces", "authors": ["Seunghyeop Nam", "Tuan Anh Nguyen", "Eunmi Choi", "Dugki Min"], "abstract": "This paper introduces SHANGUS, an advanced framework combining Deep Reinforcement Learning (DRL) with heuristic optimization to improve frontier-based exploration efficiency in unknown environments, particularly for intelligent vehicles in autonomous air services, search and rescue operations, and space exploration robotics. SHANGUS harnesses DRL's adaptability and heuristic prioritization, markedly enhancing exploration efficiency, reducing completion time, and minimizing travel distance. The strategy involves a frontier selection node to identify unexplored areas and a DRL navigation node using the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm for robust path planning and dynamic obstacle avoidance. Extensive experiments in ROS2 and Gazebo simulation environments show SHANGUS surpasses representative traditional methods like the Nearest Frontier (NF), Novel Frontier-Based Exploration Algorithm (CFE), and Goal-Driven Autonomous Exploration (GDAE) algorithms, especially in complex scenarios, excelling in completion time, travel distance, and exploration rate. This scalable solution is suitable for real-time autonomous navigation in fields such as industrial automation, autonomous driving, household robotics, and space exploration. Future research will integrate additional sensory inputs and refine heuristic functions to further boost SHANGUS's efficiency and robustness.", "sections": [{"title": "I. INTRODUCTION", "content": "Intelligent vehicles, essential for autonomous airport ser-vices [1], search and rescue operations [2, 3], and space exploration [4], exhibit high-level autonomy and can navigate and operate in complex, unknown environments. While current systems perform self-navigation and manipulation given a pre-constructed environment model, some applications necessitate autonomous perception capabilities due to the absence of prior environment models. Leveraging 3D mapping and DRL, these vehicles autonomously navigate and execute tasks, minimizing human intervention and enhancing safety [5]. Existing systems often require partial pre-known environment information for path planning [6, 7], but these frameworks are ineffective in entirely unknown settings. Alternatively, active SLAM strategies without pre-known information [8\u201310] have limited application due to their non-optimal greedy exploration or poor generalization in complex 3D scenarios. Consequently, a strategy integrating the strengths of these existing methods is needed.\nEffective robot integration depends on adapting to envi-ronmental data through frontier-based exploration, yet current algorithms are inefficient in exploration and collision avoid-ance [11]. Recent DRL advancements enable superior, real-time navigation, outperforming traditional methods in dynamic obstacle avoidance [12, 13]. DRL faces challenges in complex environments, prompting research into combining DRL with global path planning and heuristics [14]. Unlike move_base or nav2, DRL adapts to real-time goals and unexplored spaces, addressing inefficiencies in existing frontier-based exploration strategies.\nThis work proposes an innovative framework combining DRL and heuristic functions to enhance frontier selection, surpassing existing algorithms. The framework emphasizes efficient exploration beyond visited areas, demonstrating supe-rior performance over traditional methods. It highlights the im-portance of effective algorithms for initializing and navigating diverse environments. The integration of DRL and heuristic functions redefines indoor robot navigation, achieving remark-able efficiency. The DRL-based frontier selection algorithm assigns weights to distances using a normalized hyperbolic-exponential function. Prioritizing exploration points as closed, open, or step structures, the point with the minimum score (argmin) becomes the DRL control goal.\nThe key contributions and findings of this study are as follows:\n\u2022 Proposed Speedy Heuristic Approach for Navigat-ing Geographical Unexplored Spaces (SHANGUS): The SHANGUS framework integrates deep reinforcement learning (DRL) with heuristic optimization to enhance frontier-based exploration efficiency in unknown environ-ments. It leverages DRL's adaptability and heuristic pri-oritization, significantly enhancing exploration efficiency, reducing completion time, and travel distance.\n\u2022 Novel Frontier Selection and Navigation Strategy: This strategy includes a frontier selection node for identi-fying unexplored areas and a DRL navigation node using the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm for robust path planning and obstacle avoidance. The heuristic function prioritizes exploration points by considering distance and occupancy stochas-tic scores, ensuring the selection of valuable, reachable frontiers.\n\u2022 Comprehensive Experimental Evaluation and Supe-"}, {"title": "II. RELATED WORKS", "content": "Frontier-based exploration strategies are pivotal in au-tonomous robotics, systematically guiding the exploration of unknown environments by identifying boundaries between explored and unexplored areas. Research spans traditional algorithms to advanced methods integrating machine learning, particularly reinforcement learning (RL).\na) Traditional Frontier-Based Methods: Yamauchi [15] laid the foundation for frontier-based exploration, focusing on geometric identification of frontiers [16]. Enhancements in-clude energy-efficient path planning [17], clustering algorithms for frontier management [18], and heuristic optimizations [19].\nb) Reinforcement Learning Enhancements: Integrating RL into frontier-based exploration allows dynamic adaptation and optimization of exploration strategies. RL-based methods, such as those by Li et al. [20], use deep learning to enhance real-time navigation in complex environments [2], reducing exploration time and increasing coverage efficiency [12].\nc) Hybrid and Advanced Methods: Hybrid methods com-bining traditional frontier-based approaches with RL and other machine learning techniques leverage the strengths of both. Cao et al. [13] and Jain et al. [21] merge RL with traditional metrics for better decision-making. Mackay et al. [22] incor-porate dynamic elements to tackle challenges in environments with moving obstacles.\nd) Evaluation and Comparisons: Evaluations and com-parisons of different methods are crucial. Xu et al. [23] provide benchmarks and datasets to assess frontier-based and RL-based exploration strategies, highlighting their performance across diverse scenarios.\ne) Remarks: Frontier-based exploration is evolving with innovations integrating complex computational models and learning algorithms. Future directions include adaptive learn-ing mechanisms and multi-robot system exploration [24], enhancing collaborative autonomous exploration capabilities."}, {"title": "III. SHANGUS: SPEEDY HEURISTIC APPROACH FOR NAVIGATING GEOGRAPHICAL UNEXPLORED SPACES", "content": "The SHANGUS framework is designed for autonomous navigation using Gazebo for simulation. It includes sev-eral components. The Robot State Publisher, equipped with a LiDAR sensor providing a 360-degree scan ar-ray (Scan (n=360)) and an odometry sensor tracking position (Pose: x,y,z) and orientation (raw, pitch, yaw), along with Turtlebot3 Waffle PI's joint state, sup-plies vital data. The SLAM Node, using a Bayesian Revi-sion Cycle, generates and updates an occupancy grid map (Occupancy Grid Map). The Frontier Selection Node detects unexplored areas, generating point arrays (Point Arrays) and uses a heuristic function (F = {(x,y) | given frontier points} \u2192 minh(fx, fy)) for goal selection. The DRL Navigation Node handles local path plan-ning and collision avoidance, using a DRL Control Agent (TD3) for decision-making based on states (s, s'), rewards (r), and actions (a). The Automatic Control Manager manages sensor data, reward calculation, actuation, and acts as a proxy for the DRL agent. Control signals for linear and angu-lar velocity (Linear velocity, Angular velocity) are sent to the Turtlebot3, enabling navigation towards goals while avoiding obstacles. This architecture integrates SLAM, frontier-based exploration, and deep reinforcement learning for robust autonomous navigation."}, {"title": "B. SLAM", "content": "SLAM is pivotal in robotics, especially for path planning, a key aspect of this research. SLAM allows a robot to map observed landmarks while probabilistically determining its location. Here, we employ SLAM from the nav2 package in ROS2. Sensor Data Collection: Nav2 collects envi-ronmental data via LiDAR and cameras, formatted as ROS 2 messages. Costmap Generation: This data generates a costmap of the robot's environment, highlighting naviga-ble areas and obstacles. Robot Localization using AMCL: AMCL, a probabilistic algorithm, estimates and up-dates the robot's position using sensor data. Application of SLAM Algorithm: Nav2 uses SLAM algorithms, like gmapping, to map the environment, enabling simultane-ous exploration and localization. Map Update and Path"}, {"title": "C. TD3-based Autonomous Navigation", "content": "DRL is instrumental in autonomous robot navigation, learn-ing complex policies from high-dimensional sensor inputs via environmental interactions. Unlike traditional control methods relying on manual features and prior maps, DRL excels in dynamic, unknown environments, crucial for real-time naviga-tion. Algorithms like Proximal Policy Optimization (PPO) and Deep Q-Network (DQN) enhance policy learning stability and efficiency. PPO uses actor-critic networks with residual blocks to improve information flow and mitigate vanishing gradients, accelerating learning. Sensors such as LiDAR and cameras enable environmental comprehension, optimizing decision-making. Simulation environments like ROS and Gazebo fa-cilitate safe, extensive training, with reward functions guiding behaviors like target approach and collision avoidance. Chal-lenges include data inefficiency and generalization, with ongo-ing research improving sample efficiency and transfer learning for better real-world adaptation [25]. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm, developed by Scott Fujimoto et al. [26], is a Policy Gradient method improving upon Deep Deterministic Policy Gradient (DDPG) for high-dimensional continuous action spaces. TD3 addresses value function overestimation in value-based (e.g., DQN) and policy-based (e.g., Actor-Critic) reinforcement learning using Clipped Double Q-learning, Delayed Policy and Target Updates, and Target Policy Smoothing, enhancing stability and performance.\nIn Clipped Double Q-learning, TD3 uses two independent critic networks to estimate the value function, reducing overes-timation by taking the minimum value between the two critics, thus enhancing learning stability.\n$Y1 = r + \u03b3Q02 (s', \u03c0\u03c61 (8'))$\n$Y2 = r + YQ01 (s', \u03c0\u03c62 (s'))$\n(1)\n$y = r + y min (Qoz (s', \u03c0\u00a21 (s')), Qo\u2081 (s', \u03c0\u03c62 (8')))$\nHere, r is the reward, y is the discount factor,\n$Qe\u2081 (s', \u03c0\u03c6(s'))$ and $Qe (s', \u03c0\u03c6(8'))$ are the Q-values from the critic networks with parameters 01 and 02, and \u03c0(5') is the action selected by the policy network at the next state s'. By taking the minimum of these Q-value predictions, TD3 mitigates overestimation, a common issue in reinforcement learning algorithms.\nIn Delayed Policy and Target Updates, TD3 updates the policy network (actor) less frequently than the value networks (critics) to ensure the value estimates are more accurate. Typically, the policy network is updated once every two or more updates of the critic networks. This method reduces policy overfitting by averaging Q-values over a range of actions around the target action, thereby improving stability and performance.\n$0 \u2190 \u0442\u04e9 + (1 \u2212 \u0442)\u03b8'$\n(2)\nwhere is the update coefficient.\nIn the Target Policy Smoothing technique, TD3 adds noise to the target policy during updates to address overfitting and enhance robustness. This approach regularizes value estima-tion around the target action, mitigating inaccuracies from function approximation errors, and ensuring stable, effective training.\n$y = r + E\u20ac~clip(N(0,0),\u2212c,c) [Qo' (s', \u03c0\u03c6\u03b9 (s') + \u20ac)]$\n$\u20ac ~ clip(N(0, \u03c3), \u2212c, c)$\n(3)\nApplying these three techniques, TD3 enhances perfor-mance and stability in reinforcement learning, making it a"}, {"title": "D. Speedy Frontier Exploration and Goal Selection Algorithm", "content": "1) Frontier Detection: The frontier detection algorithm in Algorithm 2 requires the occupancy grid map M. It identifies free space (of), obstacles (\u03c3\u03bf), and unknown space (\u03c3\u03b7). The algorithm finds ou cells adjacent to of and expands these to"}, {"title": "2) Occupancy Stochastic Score:", "content": "The Occupancy Stochastic Score, as the name suggests, involves analyzing the occupancy grid map of the area where the frontier is located to calculate how worthwhile it is to explore that region. The score is computed as in the Equation 4:\n$O (fx, fy) = \\frac{\\sum_{(x,y)\u2208S} M(x,y)}{\u03c0r^{2}}$\n(4)\nwhere (Cx, Cy) is coordinate of frontier centroid point.\nM(x,y) is occupancy value on coordinate on (x,y) S :={${(x,y) | (x - fx)^{2} + (y - fy)^{2} < r^{2}}$}\nThis function m(x, y) represents the occupancy value score at point (x, y) on the map. According to the costmap2D package in ROS2, inflation propagates cost values from oc-cupied cells, decreasing with distance. We define 5 specific symbols for costmap values related to the robot:"}, {"title": "4) Speedy Heuristic Function:", "content": "A heuristic function (Equation 5) uses empirical knowledge to solve problems efficiently by reducing search paths and time, thus improving speed and effectiveness in finding optimal solutions. For short distances, the score converges to 0, ensuring no difference between scores. Beyond a threshold, the derivative increases rapidly then decreases, converging to y, excluding these distances. The distance score is divided into three sections: close-range, proportional range, and far-range.\nNormalizing regional probability characteristics with 1 where af is the frontier length, and multiplying cosh af by the frontier length, calculates the occupancy stochastic score considering openness and size. Combining the distance score and occupancy stochastic score yields the final heuristic function (Equation 5).\n$h(fi) = Score(fx, fy)$\n$=\ntanh \\bigg(e^{\\frac{d(fx,fy)}{\\beta}} \\cdot \u03c3- e^{\\frac{d(fx,fy)}{\\beta}} \\cdot (1-csch{\\frac{d(fx,fy)}{\u03b1}})\\bigg)$\n$+ (O(fx, fy) \\cdot sech {af}) \\cdot (1 \u2013 \u03b3)$"}, {"title": "IV. EXPERIMENTAL SIMULATION AND RESULTS", "content": "In an environment implemented with ROS2 and Gazebo Classic, we conducted comparative experiments using the Turtlebot3 waffle_pi to evaluate each algorithm. The DRL algorithm for local navigation was implemented using TD3, and a Gazebo world with six dynamic obstacles was cre-ated. Training was conducted over 10,000 episodes on a system with an Intel(R) Core(TM) i5-10400F CPU @2.90GHz and an NVIDIA 3070 GPU, taking approxi-mately two days. Frontier detection and selection were man-aged by a separate node subscribing to the occupancy grid map via DDS communication, publishing the local goal to the DRL navigation node. Enclosed by a wooden-framed boundary, the grid-based floor is populated with static obstacles (brown boxes, triangular frame, \"L\" shaped barrier) and dynamic obstacles (white cylindrical pillars). A central robot, equipped with sensors and depicted with a blue radial projection representing its field of view, navigates efficiently around obstacles. A red object near the bottom-left may represent a marker or another robot. This setup simulates a challenging scenario for training in navigation, obstacle avoidance, and spatial awareness."}, {"title": "B. Experimental Simulation Results and Analyses", "content": "1) Environment Reveal: The table I presents a comparative analysis of various autonomous exploration algorithms under three different scenarios. Metrics used for comparison include average distance (Avg dist.), minimum distance (Min dist.), maximum distance (Max dist.), standard deviation of distance ((dist)), average time (Avg T.), minimum time (Min T.), maximum time (Max T.), standard deviation of time (\u03c3(T)), average exploration rate (Avg ExpR.), minimum exploration rate (Min ExpR.), maximum exploration rate (Max ExpR.), and the standard deviation of exploration rate (\u03c3(ExpR)). Algorithms compared are NF, CFE, GDAE, SHANGUS without Deep Reinforcement Learning (w/o DRL), and SHANGUS with Deep Reinforcement Learning (w/ DRL).\na) Scenario I (Low Complexity Map):: SHANGUS w/o DRL shows the best performance with the shortest average distance (13.15 meters) and fastest completion time (71.2 seconds). It also has the lowest standard deviation in travel dis-tance (0.67 meters), indicating high consistency. SHANGUS w/ DRL has slightly higher values (16.66 meters and 81.28 seconds) but maintains a high exploration rate (98.03%). NF and CFE achieve high exploration rates (98.37% and 98.69%) but with significantly higher distances and times: NF"}, {"title": "V. CONCLUSION", "content": "Our research demonstrates that the SHANGUS framework, incorporating both heuristic optimization and deep reinforce-ment learning (DRL), significantly enhances exploration effi-ciency in unknown environments. Experimental results across three scenarios of varying complexity validate the superi-ority of SHANGUS algorithms, especially when integrated with DRL. The SHANGUS w/ DRL algorithm consistently achieved the highest exploration rates and shortest completion times, outperforming other traditional and advanced frontier-based exploration methods. These findings underscore the po-tential of combining heuristic methods with DRL to optimize autonomous navigation, making SHANGUS a promising so-lution for real-time applications in diverse robotic exploration tasks. Future work will focus on further refining the heuristic functions and integrating additional sensory inputs to enhance robustness and adaptability in more complex and dynamic environments."}]}