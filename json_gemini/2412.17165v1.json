{"title": "SURVEY ON ABSTRACTIVE TEXT SUMMARIZATION: DATASET, MODELS, AND METRICS", "authors": ["Gospel Ozioma Nnadi", "Favio Bertini"], "abstract": "The advancements in deep learning, particularly the introduction of transformers, have been pivotal in enhancing various natural language processing (NLP) tasks. These include text-to-text applications such as machine translation, text classification, and text summarization, as well as data-to-text tasks like response generation and image-to-text tasks such as captioning. Transformer models are distinguished by their attention mechanisms, pretraining on general knowledge, and fine-tuning for downstream tasks. This has led to significant improvements, particularly in abstractive summarization, where sections of a source document are paraphrased to produce summaries that closely resemble human expression. The effectiveness of these models is assessed using diverse metrics, encompassing techniques like semantic overlap and factual correctness. This survey examines the state of the art in text summarization models, with a specific focus on the abstractive summarization approach. It reviews various datasets and evaluation metrics used to measure model performance. Additionally, it includes the results of test cases using abstractive summarization models to underscore the advantages and limitations of contemporary transformer-based models. The source codes and the data are available at [1].", "sections": [{"title": "Introduction", "content": "Readers and scholars often desire a concise summary (Too Long; Didn't Read - TL;DR) of texts to effectively prioritize information. However, creating document summaries is mentally taxing and time-consuming, especially considering the overwhelming volume of documents produced annually, as depicted in Figure 1 by [2], Figure 2, [3] reported over 100,000 scientific articles on the Corona virus pandemic in 2020, though these articles contain brief abstracts of the article, the sheer volume poses challenges for researchers and medical professionals in quickly extracting relevant knowledge on a specific topic. An automatically generated multi-document summarization could be valuable, providing readers with essential information and reducing the need to access original files unless refinement is necessary. Text summarization has garnered significant research attention, proving useful in search engines, news clustering, timeline generation, and various other applications.\nThe objective of text summarization is to create a brief, coherent, factually consistent, and readable document that retains the essential information from the source document, whether it is a single or multi-document. In Single Document Summarization (SDS) only one input document is used, eliminating the need for additional processing to assess relationships between inputs. This method is suitable for summarizing standalone documents such as emails, legal contracts, financial reports and so on. The primary goal of Multi Document Summarization (MDS) is to gather information from several texts addressing the same topic, often composed at different times or representing diverse perspectives. The overarching objective is to produce information reports that are both succinct and comprehensive, consolidating varied opinions from documents that explore a topic through multiple viewpoints. The following are examples of multi document summarization tasks [4]:\n\u2022 Query-oriented multi-document summarization involves generating a concise summary from a collection of documents that directly addresses a specific query. This process focuses on extracting information relevant to the user's question from the selected set of documents.\n\u2022 Dialogue summarization: a summary from multiple textual utterances of two or more participants, condensing a dialogue between individuals into a concise summary that captures the key points.\n\u2022 Stream summarization: summarize new documents in a continuously growing document stream, providing brief summaries of continuously incoming social media posts to capture the evolving information in real-time.\nAchieving the goal of text summarization requires methods with robust capabilities to analyze input documents and identify consistent information. The methods developed for text summarization task is divided into three approach as in Figure 3:\n1. Extractive: One of the early attempts at automatic text summarization in 1985 involved extracting important sentences based on word frequency [5]. This method aims to identify sentences from the source document verbatim that capture its essence.\n2. Abstractive: The abstractive approach involves crafting new sentences by paraphrasing sections of the source document, aiming to condense the text more dynamically. This is in contrast to the extractive approach, which directly pulls sentences from the source documents verbatim to capture their essence.\n3. Hybrid: The Hybrid approach combines extractive and abstractive techniques, dividing the summarization process into two distinct phases: important content selection and then context aware paraphrasing."}, {"title": "Summarization Tasks", "content": "The current leading text summarization models stand out based on their approach and the nature of the source document (single or multi-document). Moreover, these models exhibit distinctions rooted in the self-attention quadratic effect found in existing abstractive summarization models. Further granularity is achieved by categorizing them based on the dimension of the document, distinguishing between short and long documents."}, {"title": "Short Document Summarization vs Long Document Summarization", "content": "The length of a document is a crucial factor in text summarization models. While sequence-to-sequence models based on Recurrent Neural Networks (RNNs) can sequentially read all tokens and store context, the attentions may become dispersed over long sequences, potentially degrading performance. Transformer-based models like BART [6], PEGASUS [7], and others face a limitation with a fixed maximum input sequence (typically 512 words, extended to 1024 words) due to the computational complexity of self-attention, which grows quadratically with sequence length.\nIn handling long documents for summarization, short document summarization models may face challenges leading to potential information loss. These challenges include:\n\u2022 Truncation: Disregarding text exceeding a predefined threshold. Relevant information beyond the threshold is ignored, potentially leading to incomplete or skewed summaries.\n\u2022 Chunking errors: Dividing the document into chunks, processing each separately, and then combining activations using a task-specific model. Contextual information in parts of the document split into chunks may not be fully considered, resulting in gaps or inconsistencies in the summary.\n\u2022 Cascading errors from the two stage approach: Employing a two-stage model where the first stage retrieves relevant documents, passed on to the second stage for extraction. The quality of the summary heavily relies on the first stage, which may mistakenly select non-relevant words, leading to inaccuracies in the final summary.\nIn recent developments, novel global attention transformer-based models such as Longformer [8], BigBird [9], and LongT5 [10] have been introduced. These models are specifically designed and pretrained to handle long sequences. Unlike the original transformer model [11], which employ full self-attention and face quadratic growth in memory and computational requirements with sequence length, making them impractical for processing long documents, these new models modify the transformer architecture with an attention pattern that scales linearly with the input sequence.\nThis modification makes these models versatile for processing long documents, allowing them to handle sequences of up to 16,000 words. To provide context, this corresponds to approximately 32 pages for single-spaced text or 64 pages for double-spaced text with a font size of 12 points. Additionally, BigBird demonstrates the capability to leverage pretraining for learning powerful contextual representations, even for Deoxyribonucleic Acid (DNA) fragments [9]."}, {"title": "Single Document Summarization vs Multi Document Summarization", "content": "Single document summarization and multi document summarization share the common goal of condensing docu- ments into concise and coherent summaries. They utilize similar summarization construction types as illustrated in Figure 3, learning strategies, evaluation indexes, and objective functions, aiming to minimize the distance between machine-generated and human-annotated summaries. Transformer-based models, such as BERT [12], BART [6], and PEGASUS [7], have improved abstractive text summarization for single documents. Early methods for multi document summarization applied single document summarization techniques however, there are significant differences between single document summarization and multi document summarization, leading to the development of modified transformer-based models [11] such as PRIMERA [13], CENTRUM [13], and more.\nThere are five aspects highlighting the distinctions between single document summarization and multi document summarization:\n1. More diverse input document types; multi document summarization deals with multiple input documents from various sources, introducing diversity in the types of documents being summarized.\n2. Insufficient methods in single document summarization models to capture cross-document relations; multi document summarization often employs clustering and hierarchical graph methods to capture cross-document relations, a challenge not adequately addressed in single document summarization models.\n3. High redundancy and contradiction across input documents; multiple input documents used in multi document summarization are likely to contain more contradictory, redundant, and complementary information compared to single document scenarios.\n4. Larger searching space and limited training data; multi document summarization models face a larger searching space but often lack sufficient training data compared to single document summarization. This presents obstacles for deep learning models to learn adequate representations, although progress has been made with the availability of new large-scale labeled and unlabeled data in recent years.\n5. Lack of evaluation metrics specifically designed for multi document summarization; existing single document summarization evaluation metrics may not effectively evaluate the relationship between the generated abstract and different input documents in the context of multi document summarization.\nMulti-document summarization tasks involve processing multiple input documents from diverse sources, typically categorized into three groups:\n\u2022 Many Short Sources: This involves a large number of brief documents. For instance, summarizing product reviews entails creating a concise summary from numerous individual reviews of the same product or service.\n\u2022 Few Long Sources: This category focuses on generating a summary from a small number of lengthy documents, such as synthesizing a group of news articles or creating a Wikipedia-style entry from multiple web-based articles.\n\u2022 Hybrid Sources: These combine one or a few long documents with several shorter ones. Examples include summarizing news articles accompanied by reader comments or producing a scientific summary based on a lengthy paper and multiple short citations.\nIn multi document summarization tasks, two common methods are used to concatenate multiple input documents:\n\u2022 Flat concatenation, treats the multi document summarization task as a single document summarization task. Models processing flat concatenated documents need a strong ability to handle long sequences.\n\u2022 Hierarchical concatenation, preserves cross-document relations, enhancing model effectiveness. Existing methods include:\nDocument-level condensing in a cluster, where documents are processed separately or at the word or sentence level inside document clusters [14, 15]. Models like PRIMERA [13] and CENTRUM [13] use document clustering.\nSentence relation graph to model the hierarchical relations among multi-documents [16]. Graph construc- tion methods use sentences as vertices and edges to indicate sentence-level relations. Graph structures like cosine similarity, discourse graph, semantic graph, and heterogeneous graph serve as external knowledge to enhance multi document summarization models. Models such as REFLECT [17] and BART-Long [18] are graph-based."}, {"title": "Extractive vs Abstractive vs Hybrid Summarization Approach", "content": "The extractive, abstractive, and hybrid summarization approaches define the construction types (Figure 3) used by both single document summarization and multi document summarization methods to handle input documents, process information, and generate the final summary. Notably, the abstractive approach has experienced substantial advancements, particularly in the last six years, owing to continuous improvements in deep learning models with the introduction of transformer [11].\nExtractive Text Summarization: Methods typically involve assigning each sentence a saliency score and ranking sentences in the document based on these scores. The scoring is often determined using a combination of statistical and linguistic features, such as term frequency, sentence position, cue words, stigma words, and topic signature. Machine learning models, including both unsupervised and supervised methods, have been employed for sentence extraction. Approaches like Maximal Marginal Relevance (MMR), Latent Semantic Analysis (LSA), and graph-ranking methods have also been used to iteratively extract key phrases and sentences.\nOne challenge with extractive summarization is the potential for redundancy. Recent methods in multi document summarization address this issue by simplifying and generating ideograms representing the meaning of each sentence, comparing their shapes and positions, and then summarizing based on user-supplied parameters for equivalence and relevance.\nExtractive summarization approach differs from abstractive summarization approaches in the following way:\n\u2022 it can be viewed as word or sentence-level classifiers, involving sentence ranking and selection. Recent models apply reinforcement learning [19]; [20] strategies to optimize the model directly on task-specific, non-differentiable reward functions.\n\u2022 the models are independent of any language and do not require additional linguistic knowledge.\n\u2022 it often produce non coherent, redundant summaries.\n\u2022 it produce summaries that are composed of exact words copied from the source document.\nSome of the models developed for Single Document - Extractive Text Summarization task are the following:\n\u2022 NEUSUM [21] employs a joint scoring and selection strategy. It constructs a hierarchical representation of the document and incorporates the partially generated summary at each step of the sentence selection process.\n\u2022 BanditSum [20] approaches extractive summarization as a contextual bandit problem, where the document represents the context, and selecting a sequence of sentences for the summary constitutes the action.\n\u2022 LATENT [22], introduces a latent variable extractive model. It views relevance labels of sentences in a document as binary latent variables. The latent model maximizes the likelihood of human summaries given the selected sentences.\n\u2022 REFRESH [23], propose using Reinforcement learning-based system ([24]). It approximates the search space during training by focusing on combinations of individually high-scoring sentences. The model globally optimizes the ROUGE metric for evaluation.\n\u2022 RNES [19], aims to enhance the coherence of extractive summaries by introducing a coherence model and incorporating it into the reinforcement learning framework along with ROUGE-based rewards. This approach seeks to improve the overall quality and coherence of the generated summaries.\n\u2022 JECS [25], combines the extraction of individual sentences with a constituency-based approach to compression, enabling the model to capture more global dependencies and structural information in the document. The goal is to improve the overall coherence and content selection in the generated summaries.\n\u2022 STRASS [26], focuses on selecting sentences with embeddings that are most semantically similar to the document embedding, and it fine-tunes a linear transformation to improve alignment with reference summaries, as measured by ROUGE-1 similarity. This approach aims to enhance the content and coherence of the extractive summary generated by the model.\nSome of the models developed for Multi Document - Extractive Text Summarization task are the following:\n\u2022 MTSQIGA [27], treats multi-document summarization as a binary optimization problem, employing a modified quantum-inspired genetic algorithm (QIGA) to find the best solution. The optimization involves a linear combination of coverage, relevance, and redundancy factors, integrating six sentence scoring measures.\n\u2022 CDDS [28], constraint-driven Document Summarization focuses on generating summaries with maximum content coverage and high diversity. It utilizes user-provided constraints and tunes constraint parameters to achieve these objectives.\n\u2022 MA&MR [29], employs a combination of symmetric and asymmetric similarity measures, including weighted harmonic mean of cosine similarity and overlap similarity. It formulates text summarization as a boolean programming problem, optimizing it using the differential evolution (DE) algorithm.\n\u2022 MCMR [30], represents multi-document text summarization as an integer linear programming (ILP) problem, aiming to optimize both coverage and redundancy simultaneously. It employs two optimization algorithms, namely branch-and-bound (B&B) and particle swarm optimization (PSO), to globally solve the problem.\n\u2022 PPRSum [31], considers multiple features to select salient sentences in a corpus. It involves training a model based on sentence global information using Na\u00efve Bayes, generating a relevance model based on the query of each corpus, and computing personalized prior probability based on both models. It applies PPR to select the most relevant sentences.\n\u2022 LFIPP [32], formulates sentence-extraction-based summarization as a linear fraction of sentence similarities, considering both sentence-to-document and sentence-to-sentence similarities. This approach aims to select salient sentences from the input document collection while reducing redundancy in the generated summary.\nAbstractive Text Summarization: The evolution of neural architectures, coupled with introduction of pre-training and transfer learning methodologies [33] [12], along with the availability of large-scale supervised datasets [34] [35] [36] [37], has led to the dominance of deep learning-based approaches in abstractive text summarization. Leading models leverage self-attention transformer blocks in an encoder-decoder structure [38] [39] [7], incorporating attention and copying mechanisms [40] [41], and multi-objective training strategies [42], including reinforcement learning techniques [43], to capture semantics and generate comprehensive summaries.\nTransformer-based models like BERT [12], BART [6], PEGASUS [7], have demonstrated improved performance not only in text summarization but across various natural language processing tasks. The success of transformers is attributed to pre-training using self-supervised objectives on large text corpora, enabling transfer learning to downstream tasks. The self-attention component allows the model to capture contextual information from the entire sequence.\nAbstractive text summarization differs from the extractive text summarization approach in the following way:\n\u2022 it involves sentence paraphrasing, generating new sentences.\n\u2022 it uses advanced machine models, such as encoder-decoder, auto-regressor, and reinforcement learning architectures, capture context effectively. Benefits from pre-training to transfer learning and distillation knowledge for improved performance and efficiency, and speed-up; Transfer Learning [44] provides a general-purpose \u201cknowledge\u201d to improve performance on downstream tasks, while Knowledge Distillation [45] transfers knowledge from a large model to a smaller one without loss of validity, resulting in a more deployable and resource-efficient model.\n\u2022 it produces potentially more fluent summaries, better at reflecting conflicting information, and addresses redundancy issues present in the extractive summarization approach.\n\u2022 but the factuality of the summries produce is still a challenge [46]; [47]; [48]; [49]; they are liable to reproduce factual details inaccurately.\nAbstractive Text Summarization Models for Single Document - Short Document: Some of the models developed for this task are;\n\u2022 BertSum-abs [39], BERT (Bidirectional Encoder Representations from Transformers) [12] extends the concept of word embeddings with BERT, a model that learns contextual representations from large-scale corpora using a language modeling objective. BertSum-abs introduces a document-level encoder on top of BERT.\n\u2022 Pointer Generator [40], propose a variation of Long Short-Term Memory (LSTM) [50] encoder-decoder models known as the Pointer Generator Network. This model allows the decoder to choose between generating a word from the vocabulary or copying a word from the input. It includes a coverage mechanism to prevent repetitive attention to the same part of the source document.\n\u2022 Transformer + Pointer Generator Network [51], enhances the pointer generator network and the transformer. Techniques such as n-gram blocking and coverage loss are used to create a more accurate model that reduces repetition. The copy mechanism enables the model to copy words from source texts, reducing unknown word tokens and incorrect factual representations.\n\u2022 Improve-abs [43], extend the model of [52], by incorporating an external LSTM language model into the decoder. It introduces a reinforcement learning-based objective during training.\n\u2022 ROUGESal [53] introduces a salience reward based on keyphrases and an entailment-based reward, comple- menting the traditional ROUGE-based reward within a REINFORCE framework. These rewards are optimized simultaneously using alternating mini-batches.\n\u2022 Multi-task (Ent + QG) [42], it employs a specialized multi-task architecture to efficiently integrate question generation and entailment generation as auxiliary tasks.\n\u2022 Closed book decoder [54], extends the Pointer Generator Network by incorporating a copy-less and attention- less decoder during training. This modification encourages the encoder to be more selective in encoding the most salient content.\n\u2022 T5 (Text-to-Text Transfer Transformer) [44], is an encoder-decoder model pre-trained with a causal lan- guage modeling objective and fine-tuned for various downstream tasks. It employs a multi-task mixture of unsupervised and supervised tasks, converting each task into a text-to-text format.\n\u2022 GPT-2 [55], is a generative pretraining model trained with a causal language modeling objective on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on specific tasks.\n\u2022 UniLM [56], utilizes shared model parameters across different language modeling objectives. Various self- attention masks control access to context for each word token.\n\u2022 BART (Bidirectional and Auto-Regressive Transformers) [6], is trained by corrupting documents using random shuffling of the original order of sentences and using a novel in-filling scheme. It optimizes a reconstruction loss between the decoder's output and the original document.\n\u2022 PEGASUS (Pre-training with Extracted Gap-sentences for Abstractive Summarization Sequence-to-sequence) [7], is pre-trained with a self-supervised gap-sentence-generation objective. It involves masking entire sentences from the source document, concatenating them, and using the result as the target \"summary,\u201d with important sentences removed and then generated from the remaining sentences.\nAbstractive Text Summarization Models for Single Document - Long Document: Several models have been developed to address the challenges of abstractive text summarization for long documents. Some notable models include:\n\u2022 BigBird PEGASUS [9], introduced a sparse attention mechanism to handle very long sequences efficiently. By reducing quadratic dependency to linear, it utilizes global tokens for long-context processing.\n\u2022 LongT5 [10], introduced an efficient Text-To-Text Transformer-based neural model designed for handling long sequences. It integrates pre-training strategies from PEGASUS into a scalable T5 architecture. LongT5 employs the Transient Global (TGlobal) attention mechanism, mimicking long-input transformers' local/global attention without requiring additional side-inputs.\n\u2022 Longformer [8], presents a novel sparse attention mechanism, incorporating both local and global attention, to efficiently store context. It reduces self-attention's quadratic time and memory complexity to linear by sparsifying the full self-attention matrix based on a specified attention pattern.\n\u2022 Deep Communicating Agents (DCA) [57], proposed an abstractive system featuring multiple agent LSTM encoders. To encode a long text, the task is divided among collaborating agents, each responsible for a subsection. The decoder, equipped with a hierarchical attention mechanism over the agents, uses contextual agent attention to smoothly integrate information from multiple agents during decoding. The model is trained end-to-end using self-critical reinforcement learning for generating focused and coherent summaries.\nAbstractive Text Summarization Model for Multi DocumentSeveral models have been developed to address the challenges of abstractive text summarization for multi-document scenarios. Notable models include:\n\u2022 GraphSum [58], employs graphs to encode documents, capturing cross-document relations crucial for summa- rizing long documents. It utilizes graphs to guide the summary generation process, enhancing coherence and conciseness.\n\u2022 BART-Long [18], incorporates graph information into the pre-trained encoder-decoder model, enabling it to scale efficiently to large input documents often encountered in summarizing news clusters. The model can process auxiliary graph representations derived from multi-document clusters.\n\u2022 HT (Hierarchical Transformer) [38], learns latent dependencies among textual units by utilizing cross-document relationships through an attention mechanism. This approach allows the sharing of information, going beyond simple concatenation and processing of text spans as a flat sequence.\n\u2022 PRIMERA [13], emphasizes training the model to identify and aggregate key information from a cluster of related documents during pre-training. It introduces a Gap Sentence Generation (GSG) objective called Entity Pyramid, which involves masking salient sentences across the entire cluster and training the model to generate them, thereby promoting the identification of crucial information across documents.\n\u2022 CENTRUM [13], proposes a distinct strategy from PRIMERA for teaching multi-document summarization models to identify and aggregate salient information across a document cluster during pre-training. It employs the centroid of the cluster as a synthetic summary to guide the model's learning process.\nHybrid Text Summarization\nHybrid summarization methods offer a versatile approach to capturing the semantic complexities of documents, incorporating features commonly associated with both extractive and abstractive summarization methods. These models typically operate in two stages: extractive-abstractive and abstractive-abstractive.\n\u2022 Extractive-abstractive, an extractive summarization model is employed to select salient sentences. Subsequently, an abstractive summarization model is used to generate the final summary. This hybrid approach leverages the strengths of both extractive and abstractive methods.\n\u2022 Abstractive-abstractive, involves using an abstractive summarization model during content selection or text reduction operations. Following this, another abstractive summarization model is utilized to generate the final summary. This approach emphasizes the use of abstractive methods throughout the summarization process.\nHybrid Text Summarization Models for Single Document Summarization Some hybrid models developed for single document summarization:\n\u2022 Fast-abs-rl [59], initially extracts salient sentences using a Pointer Network and subsequently rewrites them with a Pointer Generator Network. Along with maximum likelihood training, a ROUGE-L reward is applied to update the extractor through the REINFORCE algorithm [24].\n\u2022 Bottom-Up [60], presents a bottom-up approach in which a content selection model limits the copy attention distribution of a pre-trained Pointer Generator Network during inference. The content selector is used to identify which phrases from the source document should be included in the summary.\n\u2022 Unified-ext-abs [61], proposes utilizing the probability output from an extractive model as sentence-level attention to adjust the word-level attention scores of an abstractive model. It introduces an inconsistency loss to promote alignment and consistency between these two levels of attention.\n\u2022 SENECA [37], proposes the use of an entity-aware content selection module combined with an abstractive module to produce the final summary.\nHybrid Text Summarization Models for Multi Document Summarization One of the recent models developed for multi-document summarization is:\n\u2022 REFLECT [62], proposed an extract-then-abstract transformer framework that leverages pre-trained language models. It constructs a hierarchical extractor for salient sentence selection across documents and an abstractor for rewriting the selected contents as summaries."}, {"title": "Methods", "content": "Understanding human language in computer programs involves representing words and their contexts, a crucial aspect of semantics. The introduction of Neural Networks has enchance the representation of contextual word vectors that captures only what a word means in a context. Such that, the collection of contexts a word type is found in, provides clues about its meaning(s) [63]. These vectors play a vital role in various natural language processing tasks, including information retrieval, document classification, question answering, named entity recognition, parsing, and text summarization.\nWord embeddings: Word embedding methods, such as Word2Vec and GloVe, are non-contextual vectors retrieved from a lookup table. To derive contextual information of a document, hidden neural layers can be incorporated.\n\u2022 Word2Vec: [64] is trained to learn vector representations that predict the probability of surrounding words occurring given a center word (SkipGram) or vice versa (Continuous Bag of Words - CBoW). In this context, the surrounding words are often referred to as context words, as they appear in the context of the center word.\n\u2022 GloVe: [65] is trained by considering the global statistical information of word co-occurrences. Unlike Word2Vec, which focuses on local context, GloVe is designed to capture global semantic relationships between words.\nContextual embeddings : BERT [12] and ELMo [33], generate different vector representations for the same word in different sentences based on the surrounding words, capturing contextual meaning.\n\u2022 ELMo: Which stands for \u201cembeddings from language models,\u201d brought a powerful advancement in the form of word token vectors, that is, vectors for words in context, or contextual word vectors that are pre-trained on large corpora. There are two important insights behind ELMO:\n1. if every word token is going to have its own vector, then the vector should depend on an arbitrarily long context of nearby words.\n2. it trains one neural network for left contexts (going back to the beginning of the sentence a token appears in) and another neural network for right contexts (up to the end of the sentence). Longer contexts, beyond sentence boundaries, are in principle possible as well.\n\u2022 BERT: Bidirectional Encoder Representations from Transformers, a successor to ELMo, introduced innovations and achieved significant error reduction. BERT learns from bidirectional contexts, improving contextual understanding of words. The embeddings are generated by training models using language modeling objectives such as masked word prediction and Gap Sentence Generation. They produce embeddings at different layers, with intermediate layers offering more effective representations for semantic tasks."}, {"title": "Transformer-based Text Summarization Models", "content": "Neural networks leveraging contextual embeddings, such as BERT [12], BART [6], and PEGASUS [7], are capable of capturing word context and generating summaries for a given text. In contrast, Convolutional Neural Network (CNN) based models [66] are less effective at processing sequential data compared to Recurrent Neural Network (RNN) based models [67]. However, RNN models face limitations in parallel computing because they heavily rely on results from previous steps. Additionally, RNNs struggle with processing long sequences, as earlier information tends to fade over time. Transformer-based architectures [11] provide a solution to these issues. The self-attention mechanism of transformers, along with the flexible attention mechanisms (local and global attention) introduced in [8], offer natural advantages for parallelization and effectively retain long-range dependencies. These transformer-based models have demonstrated promising results in text summarization tasks. Transformer [11] encoder-decoder architecture:\n\u2022 Input is a positional encoded word embedding and the final decoder output is ran through a linear followed by a softmax layer to convert the outputs to probabilities.\n\u2022 Positional encoding keeps track of the order of the input sequence.\n\u2022 Encoder: N identical layers. Each layer has two sub-layers a multi-head self-attention mechanism, and a simple, position wise fully connected feed-forward network.\n\u2022 Decoder: Consists of N identical layers, with an additional third sub-layer that applies multi-head attention to the output generated by the encoder stack.\n\u2022 Masked Multi-Head Attention: masks future positions in the output preventing the decoder from peeking into the future during training.\nTo address the quadratic memory growth associated with self-attention when handling document-scale sequences, Longformer's [8] local and global attention mechanisms allow to scale memory usage linearly. This enables their application in text summarization tasks for long document sequences or multi-document summarization [18]. Tranformer based model with 6 encoder and 6 decoder layers is often referred to as a base model while a large model is a model with 12 or more encoder and decoder layers. Pre-trained language models (LMs): The transformer model allows for pretraining on large text corpora, the pretrained model have shown great successes when finetuned for downstream natural language processing tasks including text summarization. The following are some of the self supervised objective function for pre-training transformers based models [68]:\n\u2022 Masked Language Modeling (MLM): Encoder input tokens are randomly replaced by a mask tokens and have to be predicted by the encoder. BERT [12] use this self supervised objective to randomly mask and predict masked token.\n\u2022 Gap Sentence Generation (GSG): Whole encoder input sentences are replaced by a second mask token and fed to the decoder, but which has a causal mask to hide the future words like a regular auto-regressive transformer decoder. PEGASUS [7] use this self supervised objective, important sentences are masked and are generated together as one output sequence from the remaining sentences.\n\u2022 Causal language modeling (CLM): Which is the traditional auto-regressive training. One of the languages is selected for each training sample, and the model input is a sentence of 256 tokens, that may span over several documents in one of those languages. T5 [44] introduced this self supervised objective.\n\u2022 Combination of MLM and translation language modeling (TLM): This consists of concatenating a sentence in two different languages, with random masking. To predict one of the masked tokens, the model can use both, the surrounding context in language 1 and the context given by language 2."}, {"title": "Metrics", "content": "Robust and unbiased evaluation methods are beneficial to many Natural Language Processing task such as", "69": ".", "factuality)": "n\u2022 Informativeness (INFO): How well the generated hypothesis captures the key ideas of the source text.\n\u2022 Relevance (REL): How consistent the generated hypothesis is with respect to the source text.\n\u2022 Fluency (FLU): Refers to whether the text is free from formatting issues", "COH)": "Refers to the extent to which the text logically progresses from sentence to sentence", "Consistency": "Whether the generated hypothesis contains only statements entailed by the source text.\n\u2022 Semantic Coverage (COV): How many semantic content units from reference texts are covered by the generated hypothesis.\n\u2022 Adequacy (ADE): Whether the output conveys the same meaning as the input sentence", "currently.\nROUGE": [70], "document).\nS3": [71], "conferences.\nBertScore": [72], "BERT.\nMoverScore": [73], "74": "which operates over n-gram embeddings pooled from BERT representations.\nSentence Mover's Similarity (SMS): [75"}, {"74": "a semantic similarity metric based on word embeddings and optimal transport. SMS combines contextual embeddings with WMD for text generation evaluation", "words.\nSummaQA": [76], "confidence.\nBLANC": [77], "text.\nSUPERT": [78], "techniques.\nBLEU": [79], "translation.\nCHRF": [80], "documents.\nMETEOR": [81], "mean.\nCIDEr": [82], "Statistics": [35], "dataset": "n\u2022 Extractive fragment coverage:"}]}