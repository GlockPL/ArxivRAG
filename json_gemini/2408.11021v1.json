{"title": "ATHENA: Safe Autonomous Agents with Verbal Contrastive Learning", "authors": ["Tanmana Sadhu", "Ali Pesaranghader", "Yanan Chen", "Dong Hoon Yi"], "abstract": "Due to emergent capabilities, large language models (LLMs) have been utilized as language-based agents to perform a variety of tasks and make decisions with an increasing degree of autonomy. These autonomous agents can understand high-level instructions, interact with their environments, and execute complex tasks using a selection of tools available to them. As the capabilities of the agents expand, ensuring their safety and trustworthiness becomes more imperative. In this study, we introduce the ATHENA framework which leverages the concept of verbal contrastive learning where past safe and unsafe trajectories are used as in-context (contrastive) examples to guide the agent towards safety while fulfilling a given task. The framework also incorporates a critiquing mechanism to guide the agent to prevent risky actions at every step. Furthermore, due to the lack of existing benchmarks on the safety reasoning ability of LLM-based agents, we curate a set of 80 toolkits across 8 categories with 180 scenarios to provide a safety evaluation benchmark. Our experimental evaluation, with both closed- and open-source LLMs, indicates verbal contrastive learning and interaction-level critiquing improve the safety rate significantly.", "sections": [{"title": "1 Introduction", "content": "Recently, numerous studies have demonstrated that large language model (LLM) agents possess the capacity to interact with users through natural language. This capability allows them to engage in detailed conversations, collect information, automate tasks, and operate within various environments using a wide array of available tools (Zhao et al., 2023; Wu et al., 2023; Ge et al., 2024; Nakano et al., 2021; Significant Gravitas; Schick et al., 2024; Shen et al., 2024).\nThis advancement has offered an exciting new frontier in research, enabling the development of highly capable autonomous agents. However, it has also introduced challenges related to safety and risk when deploying these agents in real-world applications. Despite the importance of this issue, there have been relatively few contributions in this area. ToolEmu (Ruan et al., 2023) is an emulator that leverages an LLM to simulate (real-world) tool execution and allows for the testing of LLM agents across a diverse array of tools and scenarios. R-Judge (Yuan et al., 2024) is a classification benchmark for evaluating the proficiency of LLMs in identifying safety risks in a trajectory of interactions between an agent and its environment. ToolEmu and R-Judge address safety at the trajectory level; however, for real-world applications where an agent performs tasks on our behalf, it is ideal to ensure safety at the interaction level.\nTo address this gap, we propose ATHENA, a framework built on top of the agent, emulator and evaluator blocks in Toolemu, to 1) improve the intermediate reasoning steps of the agent, hereby referred to as the Actor, based on feedback from the Critic, and 2) enhance the Actor's prompt by incorporating relevant past safe and unsafe trajectories (Fig. 1), thereby promoting safer interactions. We summarize our key contributions below:\n\u2022 We develop the Critic agent to improve the Actor's reasoning at intermediate steps of a trajectory particularly concerning safety and risk.\n\u2022 We define the verbal contrastive learning concept where the past safe and unsafe trajectories are used as few-shot examples to enhance the Actor's reasoning (Fig. 1 (b)).\n\u2022 We curate a safety benchmark with 80 toolkits across 8 categories (Fig. 2) for emulating real-world scenarios where safety matters.\n\u2022 We evaluate our solutions using the safety and helpfulness metrics (Ruan et al., 2023)."}, {"title": "2 Framework: ATHENA", "content": "Fig. 1 illustrates our ATHENA framework where three LLM agents, i.e., Actor, Critic, and Emulator, interact with each other to complete a task following the requirements provided by the user in the input query. Recall that, we built our framework upon the ToolEmu components (Agent, Emulator and Evaluator) and our contributions include the implementation of the Actor, Critic, and the method of their interactions, as well as the verbal contrastive learning component."}, {"title": "2.1 Actor-Critic Interaction", "content": "The Actor agent generates a thought and based on that thought, takes an action. Therefore, to make the Actor safer, it is critical to enhance its thought and actions with safety reasoning ability, at the planning stage. To achieve this, we introduce the Critic agent into the loop to help the Actor take safer actions by reconsidering its thoughts and actions at every intermediate step while the Actor interacts with the Emulator to generate a trajectory (Fig. 1 (a)). In our implementation, we utilize an advanced LLM, such as GPT-4-Turbo, as the Critic. The Critic assesses the Actor's thoughts and actions with respect to safety and then provides feedback. This prompts the Actor to revise its previously planned action by taking a safer one instead, if it had not taken safety into consideration in its earlier thought and action. If the Actor is unable to follow the Critic's advice or find an appropriate safe action as suggested by the Critic, it may proceed to take its previous unsafe action repeatedly, in which case the Critic intercepts the trajectory to prevent safety risks. Prompt 1 presents the prompt template for the Critic agent."}, {"title": "2.2 Verbal Contrastive Learning", "content": "Few-shot prompting has shown to enhance LLM performance in various tasks as well as allow adaptation to new tasks without retraining (Brown et al., 2020; Sumers et al., 2023). The selection of few-shot examples plays a crucial role in the performance, indicating the necessity for approaches that surpass random selection. In this work, we propose the verbal contrastive learning concept where we provide the Actor with pairs of similar safe and unsafe trajectories to facilitate learning from the past experiences as few-shot examples. Fig. 1 (b) shows, for a given query, safe and unsafe examples are retrieved and passed to the Actor as verbal contrastive pairs. To retrieve the relevant and similar past trajectories, we use an embedding model\u00b9 to encode the user query, then measure the cosine similarity between the vector representation of the query and those of the past ones from the Trajectory History Vector DB. Finally, we consider the top k safe and unsafe trajectories for creating our contrastive pairs. It is beyond the scope of our objective to discuss the implementation of the Vector DB. You may find more information in Appx. A. Prompt 2 is an example showing how we provide these examples to the Actor."}, {"title": "3 Curated Safety Benchmark", "content": "We curated a diverse dataset that consists of 8 real-world categories, such as AI PC, Smart Home and Kitchen Appliances, AR/VR Devices etc. as shown in Fig. 2. There are 10 toolkits per category, thus a total of 80 toolkits, with each toolkit containing 12 generated tools. Then, we use the toolkits, the specifications of tools, and their associated risks to generate a total of 180 scenarios. Out of those, 150 are used to create the examples to be considered as past experiences, and 30 of them are kept aside as test-cases. Each test-case pertains to a different toolkit. To construct the (contrastive) examples and populate the vector DB, we generated 150 safe and unsafe trajectories through emulation by using each of those 150 scenarios. Then, the trajectories underwent automatic safety evaluation."}, {"title": "4 Experimental Evaluation", "content": ""}, {"title": "4.1 Settings", "content": "LLM Agents. We carried out comprehensive preliminary experiments to decide which LLMs to use for toolkit generation as well as for the implementation of the Actor, the Critic, the Emulator, and the Evaluator. The experiments suggested that GPT-4-Turbo\u00b2 is the most reliable candidate for toolkit generation and for being used as the Critic and the Evaluator. For the Actor, we consider GPT-3.5-Turbo\u00b3 and Gemini-1.5-Pro (Team et al., 2023) as well as open-source LLMs, Mistral-7B-instruct (Jiang et al., 2023) and Llama-3-70B (Touvron et al., 2023; MetaAI, 2024), which are widely accepted as some of the most promising open-source LLMs4. Since we use GPT-4-Turbo as the Critic agent, we do not consider it as the Actor agent. For the Emulator agent, we observed that GPT-4-Turbo does not offer a significant advantage when compared to GPT-3.5-Turbo since the generated toolkits are"}, {"title": "4.2 Experimental Results", "content": "The goal of our experiments is to study the impact of the Critic agent as well as verbal contrastive learning for taking safer actions when the Actor agent carries out a task on behalf of the user.\nThe Critic Agent Impact (Zero-Shot). Table 1 shows that the inclusion of the Critic agent leads to higher safety rates but at the cost of lower helpfulness rates as the Critic's feedback can prevent the Actor agents from completing their tasks. Generally, it is seen that Gemini-1.5-Pro achieves the highest safety rates, both with and without the Critic agent, albeit having lower helpfulness rates compared to the other Actor agents. Mistral-7B-Instruct and GPT-3.5-Turbo can be considered as the next viable candidates for the Actor agent for the zero-shot setting.\nVerbal Contrastive Learning Impact.\nNo Critic Agent \u2013 Table 1 shows that Two-Shot Contrastive prompting leads to greater safety rates in comparison to Zero-Shot and Two-Shot Random across different Actor agents, particularly with GPT-3.5-Turbo, Llama-3-70B, and Gemini-1.5-Pro when no Critic agent is used. Additionally, Two-Shot Random outperforms the Zero-Shot setting when applied by Llama-3-70B and Gemini-1.5-Pro; however, it consistently falls behind Two-Shot Contrastive. Also, we have similar observations regarding the helpfulness rate. Finally, these results highlight the effectiveness of verbal contrastive learning compared to zero-shot and two-shot random prompting.\nWith the Critic Agent \u2013 We see similar results when contrastive prompting is used alongside the Critic agent. GPT-3.5-Turbo exhibits a well-balanced performance, achieving the second-highest safety rates, following Gemini-1.5-Pro, and the highest helpfulness rates.\nOne-Shot vs. Two-Shot Contrastive \u2013 We also compare a single relevant safe or unsafe example in the prompt against two-shot contrastive prompting. For this comparison, we only consider GPT-3.5-Turbo as the Actor, given its promising performance in terms of safety and helpfulness rates in our earlier experiments. The results, shown in Table 2, indicate that the contribution of two-shot contrastive examples is greater than that of one-shot safe or unsafe example. This suggests that the reasoning ability of LLMs is enhanced when both safe (positive) and unsafe (negative) examples are provided. Nonetheless, a single example can still significantly benefit the safety reasoning ability of the LLM in the absence of contrastive pairs."}, {"title": "4.3 Human Evaluation", "content": "We complete our experiments by measuring the agreement between the automatic evaluator (i.e., GPT-4-Turbo) and three recruited human annotators for the safety and helpfulness of the Actor. Since GPT-3.5-Turbo, with the Critic and Two-Shot Contrastive prompting, demonstrated to be a reliable Actor agent, we selected its (generated) trajectories for human evaluation. We average Cohen's K (McHugh, 2012) between our automatic evaluator and each individual human annotator (A-H). Also, we compute the agreement among human annotators as a reference (H-H).\nThe Cohen's K agreement scores are available in Table 3. For safety, there is substantial agreement between the automatic evaluator and the human annotators (A-H), as well as among the human annotators themselves (H-H). In contrast, for helpfulness, there is only fair agreement between the automatic evaluator and the annotators, and moderate agreement among the annotators. This discrepancy arises from the annotators' lack of consensus on the definition of helpfulness."}, {"title": "4.4 Discussion", "content": "Both the Critic agent and verbal contrastive learning (i.e., contrastive prompting) can assist the Actor agent in making safer decisions. Our findings show that the Critic agent is more conservative than contrastive prompting. Thus, for high-priority safety scenarios, the Critic agent can be used independently or with contrastive prompting. In contexts where both safety and helpfulness are crucial, verbal contrastive learning is a suitable alternative.\nWe argue that GPT-3.5-Turbo with contrastive prompting, without the Critic agent, is a favorable choice due to its strong performance in safety and helpfulness rates, as well as its lower API call cost. If safety is prioritized over helpfulness, GPT-3.5-Turbo can be used with the Critic agent. For scenarios where the API Call cost is not a concern and safety is more critical than helpfulness, Gemini-1.5-Pro without Contrastive prompting and the Critic agent may be a better option. Gemini-1.5-Pro demonstrated superior safety as an LLM compared to others, both with and without few-shot prompting. This suggests that its parametric knowledge encompasses safety more effectively. Moreover, the performance of Llama-3-70B, comparable to GPT-3.5-Turbo, suggests that the gap between closed-source and open-source LLMs is narrowing."}, {"title": "5 Related Works", "content": "To improve reasoning of LLMs in complex tasks, the Chain-of-Thought (CoT) prompting technique was introduced, which enhances reasoning by including intermediate steps in the prompt (Wei et al., 2022). The Self-Consistency strategy further refines this by evaluating multiple reasoning paths to find the most consistent answer (Wang et al., 2022). Despite their effectiveness, these methods struggle with reactive reasoning and integrating new external information. The ReAct framework addresses this by combining reasoning with actions within prompts, allowing interaction with external environments to augment reasoning capabilities (Yao et al., 2022). In subsequent works such as Self-Refine (Madaan et al., 2024), an LLM may iteratively refine its responses using feedback to improve its reasoning ability, bypassing the need for external data or supervision. The Reflexion (Shinn et al., 2023) method further introduced verbal reinforcement, enabling learning from self-reflective feedback from past steps within the same task. The more recently introduced approach in (Zhao et al., 2024) explores prompt-based transfer learning, utilizing past experiences to boost LLM performance without extensive data, annotations, or parameter updates.\nAlthough ReAct allows enhanced reasoning through interactions, it lacks a reflective mechanism or a way to incorporate learning from past experiences, such as in Reflexion. Self-Refine provides an effective way to incorporate reflective feedback, but does not leverage past experiences, which could enhance performance. Different from Reflexion, our framework facilitates learning from similar cross-task past experiences as few-shots.\nDespite significant attention to the agent's reasoning capability concerning success rate across multiple tasks, the safety aspect remains relatively under-explored. To bridge this gap, in this study, we evaluated the LLM agents on both safety and helpfulness metrics."}, {"title": "6 Conclusion", "content": "We introduced the ATHENA framework for verbal contrastive learning aimed at improving safety during agent-environment interactions. Our study underscores the importance of considering safety alongside performance (success rate or helpfulness rate) metrics in evaluating AI agents. We believe that this work, along with ToolEmu and R-Judge, represents preliminary steps in this field, with much remaining to be explored. We hope that our work and findings will significantly benefit both the research and industry communities.\nWe will further consider the integration of our verbal contrastive learning with other techniques like CoT and Reflexion to enhance the safety and helpfulness of the autonomous agents. It would be also interesting to study the performance of LLM-based contrastive critic agents."}, {"title": "Limitations", "content": "Our work has addressed the challenge of developing safe and effective agents through an improved reasoning approach. We rely majorly on the currently available state-of-the-art LLMs at the time of this research to generate the toolkits, tools, tool specifications and agent interactions, which may improve with the advent of models with better capabilities. It is also possible that safer and more helpful prompts can be designed to enable agents to perform even better on the reported metrics. We intend to explore these research directions in future, and encourage the broader research and industry communities to experiment with a variety of settings and prompt configurations."}, {"title": "Ethics Statement", "content": "This work is centered around simulating interactions between an agent and a set of toolkits. It is important to note that while our emulation captures certain safety challenges, it may not encompass all real-world scenarios. Our findings underscore the ongoing complexity of addressing safety concerns. We acknowledge that the proposed solutions in this work are not exhaustive, emphasizing the need for continued research and vigilance in ensuring the safety of AI systems in practical applications."}, {"title": "A Trajectory History Vector DB", "content": "We store our trajectories, including both successful and unsuccessful with respect to safety and helpfulness. These stored trajectories serve as few-shot examples for few-shot prompting strategies, including two-shot random and two-shot contrastive strategies. For each trajectory, we also store the embedding representations of both the user query and the trajectory itself, along with additional metadata such as the task category (e.g., AI PC, Smart Vehicle). Recall that, for retrieving the relevant/similar past trajectories, we utilize the embedding model, i.e., all-mpnet-base-v2 (Song et al., 2020), to encode the user query, then calculate the cosine similarity between the representation of the query and those of the past ones stored in the DB. Finally, we select the top k safe and unsafe trajectories for our contrastive pairs. In this study, k = 1."}, {"title": "B Safety and Helpfulness Scores", "content": "We followed Ruan et al. (2023) to measure safety and helpfulness scores. The safety score is obtained from the Safety evaluator and categorized as Likely Severe Risk (0), Possible Severe Risk (1), Likely Mild Risk (1), Possible Mild Risk (2) and Certain No Risk (3). The helpfulness score is obtained from the Helpfulness Evaluator that provides a qualitative measurement on the scale of 0 to 3 as Poor (0), Unsatisfactory (1), Good (2) and Excellent (3), affirming the extent to which a task was completed by the Actor in an appropriate manner for a given query. As shown in Table B.1, the quantitative scores are associated with binary labels of 1 and 0. We rely on the mean of the binary labels to calculate the safety and helpfulness rates. That is, the rates are the average of the binary labels."}, {"title": "C Critic-Agent Interaction Example", "content": "[USER INPUT]: Please schedule a video call with my colleague, Alex, using the Echo Show. Ensure that the call is secure and private.\n[ACTION]: EchoShowVideoCall\n[ACTION INPUT]: {\"contact_name\": \"Alex\", \"action\": \"make call\"}\n[CRITIC FEEDBACK]: \"Critique #1: The agent's [Thought] and [Action] for the current tool call are safe. The agent took a safe action because the EchoShowVideoCall tool is being used appropriately to fulfill the user's request for a secure and private video call with their colleague, Alex, as per the user's specifications.\"\n[SIMULATOR OBS]: {\"call_status\": \"calling\"}\n[ACTION]: {'EchoShowVoiceAndTouchInteraction'}\n[CRITIC FEEDBACK]: \"Critique #1: Unsafe. The agent took an unsafe action because the current tool call, EchoShowVoiceAndTouchInteraction, may not be equipped to handle security settings directly through voice commands, potentially compromising the call's security and privacy as requested by the user.\"\n[THOUGHT]: Considering the critique and the need to ensure the security and privacy of the video call, I should revise my action to ensure that the call remains secure and private.\n[ACTION]: EchoShowVideoCall\n[ACTION INPUT]: {\"contact_name\": \"Alex\", \"action\": \"make call\", \"security_settings\": \"secure and private\"}\n[OUTPUT]: [Final Answer]: I stopped to comply with the safety requirements after considering the current action: EchoShowVideoCall for not being able to find a safe tool to address the critic's feedback."}]}