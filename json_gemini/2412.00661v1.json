{"title": "Mean-Field Sampling for Cooperative Multi-Agent Reinforcement Learning", "authors": ["Emile Anand", "Ishani Karmarkar", "Guannan Qu"], "abstract": "Designing efficient algorithms for multi-agent reinforcement learning (MARL) is fundamentally chal-lenging due to the fact that the size of the joint state and action spaces are exponentially large in the number of agents. These difficulties are exacerbated when balancing sequential global decision-making with local agent interactions. In this work, we propose a new algorithm SUBSAMPLE-MFQ (Subsample-Mean-Field-Q-learning) and a decentralized randomized policy for a system with n agents. For k \u2264 n, our algorithm system learns a policy for the system in time polynomial in k. We show that this learned policy converges to the optimal policy in the order of 0(\\frac{1}{\\sqrt{k}}) as the number of subsampled agents k increases. We validate our method empirically on Gaussian squeeze and global exploration settings.", "sections": [{"title": "Introduction", "content": "Reinforcement Learning (RL) has become a popular learning framework to solve sequential decision making problems in unknown environments, and has achieved tremendous success in a wide array of domains such as playing the game of Go (Silver et al., 2016), robotic control (Kober et al., 2013), and autonomous driving (Kiran et al., 2022; Lin et al., 2023). A critical feature of most real-world systems is their uncertain nature, and consequently RL has emerged as a powerful tool for learning optimal policies for multi-agent systems to operate in unknown environments (Kim & Giannakis, 2017; Zhang et al., 2021; Lin et al., 2024; Anand & Qu, 2024). While the early literature on RL predominantly focused on the single-agent setting, multi-agent reinforcement learning (MARL) has also recently achieved impressive successes in a broad range of areas, such as coordination of robotic swarms (Preiss et al., 2017), self-driving vehicles (DeWeese & Qu, 2024), real-time bidding (Jin et al., 2018), ride-sharing (Li et al., 2019), and stochastic games (Jin et al., 2020).\nDespite growing interest in multi-agent RL (MARL), extending RL to multi-agent settings poses significant computational challenges due to the curse of dimensionality (Sayin et al., 2021). Even if the individual agents' state or action spaces are small, the global state space or action space can take values from a set with size that is exponentially large as a function of the number of agents. For example, even model-free RL algorithms such as temporal difference (TD) learning (Sutton et al., 1999) or tabular Q-learning require computing and storing a Q-function (Bertsekas & Tsitsiklis, 1996) that is as large as the state-action space. Unfortunately, in MARL, the joint state-action space is exponentially large in the number of agents. In the case where the system's rewards are not discounted, reinforcement learning on multi-agent systems is prov-ably NP-hard (Blondel & Tsitsiklis, 2000), and such scalability issues have been observed in the literature in a variety of settings (Guestrin et al., 2003; Papadimitriou & Tsitsiklis, 1999; Littman, 1994). Independent Q-learning (Tan, 1997) seeks to overcome these scalability challenges by considering other agents as a part of the environment; however, this often fails to capture a key feature of MARL, which is inter-agent interactions.\nEven in the fully cooperative regime, MARL is fundamentally difficult, since agents in the real-world not only interact with the environment but also with each other (Shapley, 1953). An exciting line of work that"}, {"title": "Related Literature", "content": "MARL has a rich history, starting with early works on Markov games used to characterize the decision-making process (Littman, 1994; Sutton et al., 1999), which can be regarded as a multi-agent extension of the Markov Decision Process (MDP). MARL has since been actively studied (Zhang et al., 2021) in a broad range of settings. MARL is most similar to the category of \"succinctly described\" MDPs (Blondel & Tsitsiklis, 2000), where the state/action space is a product space formed by the individual state/action spaces of multiple agents, and where the agents interact to maximize an objective. A promising line of research that has emerged over recent years constrains the problem to sparse networked instances to enforce local interactions between agents (Qu et al., 2020a; Lin et al., 2020; Mondal et al., 2022). In this formulation, the agents correspond to vertices on a graph who only interact with nearby agents. By exploiting Gamarnik's correlation decay property from combinatorial optimization (Gamarnik et al., 2009), they overcome the curse of dimensionality by simplifying the problem to only search over the policy space derived from the truncated graph to learn approximately optimal solutions. However, as the underlying network structure becomes dense with many local interactions, the neighborhood of each agent gets large, causing these algorithms to become intractable."}, {"title": "Preliminaries", "content": "In this section, we formally introduce the problem, state some examples for our setting, and provide technical details of the mean-field and Q-learning techniques that will be used throughout the paper."}, {"title": "Problem Formulation", "content": "We consider a system of n + 1 agents, where agent g is a \"global decision making agent\" and the remaining n agents, denoted by [n], are \u201clocal agents.\u201d At time step t, the agents are in state s(t) = (s_g(t), s_1(t), ..., s_n(t)) \u2208 S := S_g \u00d7 S^n_1, where s_g(t) \u2208 S_g denotes the global agent's state, and for each i \u2208 [n], s_i(t) \u2208 S_i denotes the state of the i'th local agent. The agents cooperatively select actions a(t) = (a_g(t), a_1(t), ..., a_n(t)) \u2208 A where a_g(t) \u2208 A_g denotes the global agent's action and a_i(t) \u2208 A^n_1 denotes the i'th local agent's action. At each time-step t, the next state for all the agents is independently generated by stochastic transition kernels"}, {"title": "Technical Background", "content": "Q-learning. To provide background for the analysis in this paper, we review a few key technical concepts in RL. At the core of the standard Q-learning framework (Watkins & Dayan, 1992) for offline-RL is the Q-function Q: S \u00d7 A \u2192 R. Q-learning seeks to produce a policy \u03c0*(s) that maximizes the expected infinite horizon discounted reward. For any policy \u03c0, Q^\u03c0(s,a) = E^\u03c0 [\\sum_{\\tau=0}^\\infty \u03b3^\u03c4 r(s(t),a(t))|s(0) = s,a(0) = a]. One approach to learning the optimal policy \u03c0*(\u00b7|s) is dynamic programming, where the Q-function is iteratively updated using value-iteration: Q^0(s, a) = 0, for all (s, a) \u2208 S\u00d7A. Then, for all t \u2208 [T], Q^{t+1}(s, a) = TQ^t (s, a), where T is the Bellman operator defined as\n\\begin{equation}\nTQ^t (s, a) = r(s, a) + \u03b3 \\mathbb{E}_{\\substack{s_g' \\sim P_g(\\cdot|s_g, a_g), \\\\ s' \\sim P_1(s_i, s_g), \\forall i \\in [n]}}  \\max_{a' \\in A_g \\times A_1^n} Q^t (s', a').\n\\tag{4}\n\\end{equation}\nThe Bellman operator T is \u03b3-contractive, which ensures the existence of a unique fixed-point Q* such that TQ* = Q*, by the Banach fixed-point theorem (Banach, 1922). Here, the optimal policy is the deterministic greedy policy \u03c0* : S_g\u00d7S^n_1 \u2192 A_g\u00d7A^n_1, where \u03c0*(s) = arg max_{a \u2208 A_g \\times A_1^n} Q*(s, a). However, the complexity of a single update to the Q-function is O(|S_g||S_1|^n|A_g||A_1|^n), which grows exponentially with n. As the number of local agents increases (n \\gg |S_1|), this exponential update complexity renders Q-learning impractical.\nMean-field Transformation. To address this, mean-field MARL (under homogeneity assumptions) stud-ies the distribution function F_Z^{[n]} : Z_1 \u2192 R, where Z_1 := S_1 \u00d7 A_1, given by\n\\begin{equation}\nF_Z^{[n]}(z) := \\frac{1}{n} \\sum_{i=1}^n \\mathbb{1}{s_i = z_s, a_i = z_a}, \\forall z := (z_s, z_a) \\in S_1 \\times A_1.\n\\tag{5}\n\\end{equation}\nLet \u03bc_n(Z_1) = {\\frac{|b \\in \\{0,...,n\\}|}{n}} |S_1|\\times|A_1| be the space of |S_1|\u00d7 |A_1|-sized tables, where each entry is an element of {0,\\frac{1}{n},...,1}. In this space, F_Z^{[n]} \u2208 \u03bc_n(Z_1) where F_Z^{[n]} represents the proportion of agents in each state/action pair. The Q-function is permutation-invariant in the local agents, since permuting the labels of homogeneous local agents with the same state will not change the action of the decision-making agent. Hence, Q(s_g, s_{\\[n]}, a_g, a_{\\[n]}) = Q(s_g, s_1, a_g, a_1, F_Z^{[n] \\backslash 1}). Here, \\tilde{Q} : S_g \u00d7 S_1 \u00d7 A_g \u00d7 A_1 \u00d7 \u03bc_{n-1}(Z_1) \u2192 R is a reparameterized Q-function learned by mean-field value iteration: one initializes \\tilde{Q}^0(s_g, s_1, a_g, a_1, F_Z^{[n] \\backslash 1}) = 0. At each time-step t, we update \\tilde{Q} as \\tilde{Q}^{t+1}(s_g, s_1, a_g, a_1, F_Z^{[n] \\backslash 1}) = T\\tilde{Q}^t (s_g, s_1, a_g, a_1, F_Z^{[n] \\backslash 1}), where T is the"}, {"title": "Method and Theoretical Results", "content": "In this section, we propose the SUBSAMPLE-MFQ algorithm to overcome the polynomial (in n) sample com-plexity of mean-field value iteration and the exponential (in n) sample complexity of traditional Q-learning. In our algorithm, the global agent randomly samples a subset of local agents \u0394 \u2286 [n] such that |\u0394| = k, for k \u2264 n. It ignores all other local agents [n] \\backslash \u0394, and performs value iteration to learn the Q-function \\tilde{Q}_{k,m} and policy \\tilde{\u03c0}_{k,m} for this surrogate subsystem of k local agents, where m is the number of samples used to update the Q-functions' estimates of the unknown system. Here, When |Z_1|^{k-1} < k|Z_1|, the algorithm uses traditional value-iteration (Algorithm 1), and when |Z_1|^{k-1} > k|Z_1|, it uses mean-field value iteration (Algorithm 2). The surrogate reward gained by this subsystem at each time step is r_\u0394 : S \u00d7 A \u2192 R:\n\\begin{equation}\nr_\\Delta(s,a) = r_g(s_g, a_g) + \\frac{1}{|\\Delta|} \\sum_{i \\in \\Delta} r_1(s_g, s_i, a_i).\n\\tag{7}\n\\end{equation}\nTo convert the optimality of each agent's action within the k local-agent subsystem to an approximate optimality on the full n-agent system, we use a randomized policy \\pi_{k,m}^{\\text{est}} (Algorithm 3), where the global agent samples \u0394 \u2208 \\mathcal{U}(\\[n]\\choose{k}) at each time-step to derive the action a_g \\sim \\pi_{k,m}^{\\text{est}}(s_g, s_{\\Delta}), and where each local i agent samples k \u2212 1 other local agents \u0394_i to derive the action \\pi_{k,m}^{\\text{est}}(s_g, s_i, s_{\\Delta_i}). Finally, Theorem 3.4 shows that the policy \\pi_{k,m}^{\\text{est}} converges to the optimal policy \u03c0^* as k \u2192 n. We first present Algorithms 1 and 2 (SUBSAMPLE-MFQ: Learning) and Algorithm 3 (SUBSAMPLE-MFQ: Execution), which we describe below. For this, a crucial characterization is the notion of the empirical distribution function:\nDefinition 3.1 (Empirical Distribution Function). For any population (z_1,..., z_n) \u2208 Z_1^n, where Z_1 := S_1\u00d7A_1, define the empirical distribution function F_{z,\\Delta} : Z_1 \u2192 R_+ for \u0394 \u2286 [n] such that |\u0394| = k by:\n\\begin{equation}\nF_z(x) := F_{s,a}(x) := \\frac{1}{k} \\sum_{i \\in \\Delta} \\mathbb{1}{s_i = z_s, a_i = z_a}, \\forall z:= (z_s, z_a) \\in S_i \\times A_1.\n\\tag{8}\n\\end{equation}\nLet \u03bc_k(Z_1) := {\\frac{|b \\in \\{0,...,k\\}|}{k}} |S_1|\\times|A_1| be the space of |S_1|\u00d7 |A_1|-length vectors where each entry in a vector is an element of {0,\\frac{1}{k},...,1} such that F_{z,\\Delta} \u2208\u03bc_k(Z_1). Here, F_{z,\\Delta} is the proportion of agents in the k-local-agent subsystem at each state.\nAlgorithms 1 and 2 (Offline learning). Let m \u2208 N denote the sample size for the learning algorithm with sampling parameter k \u2264 n. When |Z_1|^{k-1} < k|Z_1|, we empirically learn the optimal Q-function for a subsystem with k-local agents denoted by \\tilde{Q}_{k,m}^{\\text{est}} : S_g \u00d7 S_{\\Delta} \u00d7 A_g \u00d7 A_{\\Delta} \u2192 R: set \\tilde{Q}_{k,m}^{\\text{est}}(s_g, s_{\\Delta}, a_g, a_{\\Delta})=0 for all (s_g, s_{\\Delta}, a_g, a_{\\Delta}) \u2208 S_g \u00d7 S_{\\Delta} \u00d7 A_g \u00d7 A_{\\Delta}. At time step t, set \\tilde{Q}_{k,m}^{\\text{est} t+1}(s_g, s_{\\Delta}, a_g, a_{\\Delta}) = \\mathcal{T}_{k,m}\\tilde{Q}_{k,m}^{\\text{est} t}(s_g, s_{\\Delta}, a_g, a_{\\Delta}), where"}, {"title": "Theoretical Guarantee", "content": "This subsection shows that the value of the expected discounted cumulative reward produced by \\pi_{k,m}^{\\text{est}} is approximately optimal, where the optimality gap decays as k\u2192n and m becomes large.\nBellman noise. We introduce the notion of Bellman noise, which is used in the main theorem. Consider \\mathcal{T}_{k,m}. Clearly, it is an unbiased estimator of the generalized adapted Bellman operator \\mathcal{T},\n\\begin{equation}\n\\mathcal{T}_k\\tilde{Q}_k(s_g, s_{\\Delta}, a_g, a_{\\Delta}) = r_{\\Delta}(s,a) + \\gamma \\mathbb{E}_{\\substack{s_g' \\sim P_g(\\cdot|s_g, a_g), \\\\ s' \\sim P_1(\\cdot|s_i, s_g, a_i), \\forall i \\in \\Delta}}  \\max_{a' \\in A_g \\times A_k} \\tilde{Q}_k (s_g', s', a_g', a').\n\\tag{16}\n\\end{equation}\nFor all (s_g, s_{\\Delta}, a_g, a_{\\Delta}) \u2208 S_g \u00d7 S_{\\Delta} \u00d7 A_g \u00d7 A_{\\Delta}, set \\tilde{Q}_k^0(s_g, s_g, s_{\\Delta}, a_g, a_{\\Delta}) = 0. For t \u2208 N, let \\tilde{Q}_k^{t+1} = \\mathcal{T}\\tilde{Q}_k^t, where \\mathcal{T} is defined for k \u2264 n in Eq. (16). Then, \\mathcal{T} is also a \u03b3-contraction (Lemma B.7) with fixed-point \\tilde{Q}_k. By the law"}, {"title": "Numerical Experiments", "content": "This section provides simulation results for the examples outlined in Section 2 for Gaussian squeeze and constrained exploration. All numerical experiments were run on a 3-core CPU server equipped with a RAM. We chose a complexity for the parameters of each simulation that was sufficient to emphasize characteristics of the theory, such as the complexity improvement of Algorithms 1 and 2 and the decaying optimality gap."}, {"title": "Gaussian Squeeze", "content": "In this task, n homogeneous agents determine their individual action a_i to jointly maximize the objective r(x) = xe^{-(x-\u03bc)^2/\u03c3^2}, where x = \\sum_{i=1}^n a_i, a_i = {0,...,9}, and \u00b5 and \u03c3 are the pre-defined mean and variance of the system. In scenarios of traffic congestion, each agent i \u2208 [n] is a traffic controller trying to send a_i vehicles into the main road, where controllers coordinate with each other to avoid congestion, hence avoid-ing either over-use or under-use, thereby contributing to the entire system. This GS problem serves as an ablation study on the impact of subsampling for MARL.\nSpecifically, we consider a purely homogeneous subsystem devoid of a global agent to match the mean-field MARL setting in Yang et al. (2018), where the richness of our model setting can still express the Gaussian squeeze task. Instead, we use the global agent to model the global state of the system, given by the number"}, {"title": "Constrained Exploration", "content": "Consider an M\\times M grid. Each agent's state is a coordinate in [M] \u00d7 [M]. The state represents the center of a d\u00d7d box where the global agent wishes to constrain the local agents' movements. Initially, all agents are in the same location. At each time-step, the local agents take actions a_i(t) \u2208 R^2 (e.g., up, down, left, right) to transition between states and collect stage rewards. The transition kernel ensures that local agents remain within the d \u00d7 d box dictated by the global agent, by only using knowledge of a_i(t), s_g(t), and s_i(t). In warehouse settings where some shelves have collapsed, creating hazardous or inaccessible areas, we want agents to clean these areas. However, exploration in these regions may be challenging due to physical con-"}]}