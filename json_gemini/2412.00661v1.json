[{"title": "Mean-Field Sampling for Cooperative Multi-Agent Reinforcement Learning", "authors": ["Emile Anand", "Ishani Karmarkar", "Guannan Qu"], "abstract": "Designing efficient algorithms for multi-agent reinforcement learning (MARL) is fundamentally chal-lenging due to the fact that the size of the joint state and action spaces are exponentially large in the number of agents. These difficulties are exacerbated when balancing sequential global decision-making with local agent interactions. In this work, we propose a new algorithm SUBSAMPLE-MFQ (Subsample-Mean-Field-Q-learning) and a decentralized randomized policy for a system with n agents. For $k \\leq n$, our algorithm system learns a policy for the system in time polynomial in $k$. We show that this learned policy converges to the optimal policy in the order of $0(1/\\sqrt{k})$ as the number of subsampled agents $k$ increases. We validate our method empirically on Gaussian squeeze and global exploration settings.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) has become a popular learning framework to solve sequential decision making problems in unknown environments, and has achieved tremendous success in a wide array of domains such as playing the game of Go (Silver et al., 2016), robotic control (Kober et al., 2013), and autonomous driving (Kiran et al., 2022; Lin et al., 2023). A critical feature of most real-world systems is their uncertain nature, and consequently RL has emerged as a powerful tool for learning optimal policies for multi-agent systems to operate in unknown environments (Kim & Giannakis, 2017; Zhang et al., 2021; Lin et al., 2024; Anand & Qu, 2024). While the early literature on RL predominantly focused on the single-agent setting, multi-agent reinforcement learning (MARL) has also recently achieved impressive successes in a broad range of areas, such as coordination of robotic swarms (Preiss et al., 2017), self-driving vehicles (DeWeese & Qu, 2024), real-time bidding (Jin et al., 2018), ride-sharing (Li et al., 2019), and stochastic games (Jin et al., 2020).\nDespite growing interest in multi-agent RL (MARL), extending RL to multi-agent settings poses significant computational challenges due to the curse of dimensionality (Sayin et al., 2021). Even if the individual agents' state or action spaces are small, the global state space or action space can take values from a set with size that is exponentially large as a function of the number of agents. For example, even model-free RL algorithms such as temporal difference (TD) learning (Sutton et al., 1999) or tabular Q-learning require computing and storing a Q-function (Bertsekas & Tsitsiklis, 1996) that is as large as the state-action space. Unfortunately, in MARL, the joint state-action space is exponentially large in the number of agents. In the case where the system's rewards are not discounted, reinforcement learning on multi-agent systems is prov-ably NP-hard (Blondel & Tsitsiklis, 2000), and such scalability issues have been observed in the literature in a variety of settings (Guestrin et al., 2003; Papadimitriou & Tsitsiklis, 1999; Littman, 1994). Independent Q-learning (Tan, 1997) seeks to overcome these scalability challenges by considering other agents as a part of the environment; however, this often fails to capture a key feature of MARL, which is inter-agent interactions.\nEven in the fully cooperative regime, MARL is fundamentally difficult, since agents in the real-world not only interact with the environment but also with each other (Shapley, 1953). An exciting line of work that"}, {"title": "1.1 Related Literature", "content": "MARL has a rich history, starting with early works on Markov games used to characterize the decision-making process (Littman, 1994; Sutton et al., 1999), which can be regarded as a multi-agent extension of the Markov Decision Process (MDP). MARL has since been actively studied (Zhang et al., 2021) in a broad range of settings. MARL is most similar to the category of \"succinctly described\" MDPs (Blondel & Tsitsiklis, 2000), where the state/action space is a product space formed by the individual state/action spaces of multiple agents, and where the agents interact to maximize an objective. A promising line of research that has emerged over recent years constrains the problem to sparse networked instances to enforce local interactions between agents (Qu et al., 2020a; Lin et al., 2020; Mondal et al., 2022). In this formulation, the agents correspond to vertices on a graph who only interact with nearby agents. By exploiting Gamarnik's correlation decay property from combinatorial optimization (Gamarnik et al., 2009), they overcome the curse of dimensionality by simplifying the problem to only search over the policy space derived from the truncated graph to learn approximately optimal solutions. However, as the underlying network structure becomes dense with many local interactions, the neighborhood of each agent gets large, causing these algorithms to become intractable."}, {"title": "2 Preliminaries", "content": "In this section, we formally introduce the problem, state some examples for our setting, and provide technical details of the mean-field and Q-learning techniques that will be used throughout the paper.\nNotation. For $k, n \\in \\mathbb{N}$ where $k \\leq n$, let $\\binom{[n]}{k}$ denote the set of k-sized subsets of $[n] = \\{1, ..., n\\}$. For any vector $z \\in \\mathbb{R}^d$, let $||z||_1$ and $||z||_\\infty$ denote the standard $l_1$ and $l_\\infty$ norms of $z$ respectively. Let $||A||_1$ denote the matrix $l_1$-norm of $A \\in \\mathbb{R}^{n \\times m}$. Given a collection of variables $s_1,..., s_n$ the shorthand $s_\\triangle$ denotes the set $\\{s_i : i \\in \\triangle\\}$ for $\\triangle \\subseteq [n]$. We use $\\mathcal{O}(\\cdot)$ to suppress polylogarithmic factors in all problem parameters except $n$. For a discrete measurable space $(\\mathcal{X}, \\mathcal{F})$, the total variation distance between probability measures $P_1, P_2$ is given by $\\text{TV}(P_1, P_2) = \\frac{1}{2} \\sum_{x \\in \\mathcal{X}} |P_1(x) - p_2(x)|$. Next, $x \\sim D(\\cdot)$ denotes that $x$ is a random element sampled from a probability distribution $D$, and we denote the uniform distribution over a finite set $\\Omega$ by $\\mathcal{U}(\\Omega)$.\n2.1 Problem Formulation\nWe consider a system of $n + 1$ agents, where agent $g$ is a \"global decision making agent\" and the remaining $n$ agents, denoted by $[n]$, are \u201clocal agents.\u201d At time step $t$, the agents are in state $s(t) = (s_g(t), s_1(t), ..., s_n(t)) \\in \\mathcal{S} := \\mathcal{S}_g \\times \\mathcal{S}_{\\ell}$, where $s_g(t) \\in \\mathcal{S}_g$ denotes the global agent\u2019s state, and for each $i \\in [n]$, $s_i(t) \\in \\mathcal{S}_i$ denotes the state of the $i$'th local agent. The agents cooperatively select actions $a(t) = (a_g(t), a_1(t), ..., a_n(t)) \\in \\mathcal{A}$ where $a_g(t) \\in \\mathcal{A}_g$ denotes the global agent's action and $a_i(t) \\in \\mathcal{A}_\\ell$ denotes the $i$'th local agent's action. At each time-step $t$, the next state for all the agents is independently generated by stochastic transition kernels"}, {"title": "2.2 Technical Background", "content": "Q-learning. To provide background for the analysis in this paper, we review a few key technical concepts in RL. At the core of the standard Q-learning framework (Watkins & Dayan, 1992) for offline-RL is the Q-function $Q : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$. Q-learning seeks to produce a policy $\\pi^*(s)$ that maximizes the expected infinite horizon discounted reward. For any policy $\\pi$, $Q^{\\pi}(s,a) = \\mathbb{E}^{\\pi} [\\sum_{\\tau=0}^\\infty \\gamma^\\tau r(s(t),a(t)) | s(0) = s, a(0) = a]$. One approach to learning the optimal policy $\\pi^* (\\cdot|s)$ is dynamic programming, where the Q-function is iteratively updated using value-iteration: $Q^0(s, a) = 0$, for all $(s, a) \\in \\mathcal{S}\\times\\mathcal{A}$. Then, for all $t \\in [T]$, $Q^{t+1}(s, a) = \\mathcal{T}Q^t(s, a)$, where $\\mathcal{T}$ is the Bellman operator defined as\n$\\mathcal{T}Q^t (s, a) = r(s, a) + \\gamma \\mathbb{E}_{s_g'\\sim P_g(\\cdot | s_g, a), \\atop s_i'\\sim P_1(\\cdot | S_i, s_g, a_i), \\forall i \\in [n]}  \\max_{a'\\in \\mathcal{A}_g \\times \\mathcal{A}^n} Q^t(s', a').$\n The Bellman operator $\\mathcal{T}$ is $\\gamma$-contractive, which ensures the existence of a unique fixed-point $Q^*$ such that $\\mathcal{T}Q^* = Q^*$, by the Banach fixed-point theorem (Banach, 1922). Here, the optimal policy is the deterministic greedy policy $\\pi^* : \\mathcal{S}_g\\times \\mathcal{S} \\to \\mathcal{A}_g\\times \\mathcal{A}^n$, where $\\pi^*(s) = \\arg \\max_{a\\in \\mathcal{A}_g\\times\\mathcal{A}^n} Q^*(s, a)$. However, the complexity of a single update to the Q-function is $\\mathcal{O}(|\\mathcal{S}_g||\\mathcal{S}_\\ell|^n|\\mathcal{A}_g||\\mathcal{A}_\\ell|^n)$, which grows exponentially with $n$. As the number of local agents increases (ng|\\mathcal{S}_\\ell|$), this exponential update complexity renders Q-learning impractical.\nMean-field Transformation. To address this, mean-field MARL (under homogeneity assumptions) studies the distribution function $F_{Z[n]} : \\mathcal{Z}_1 \\to \\mathbb{R}$, where $\\mathcal{Z}_1 := \\mathcal{S}_1 \\times \\mathcal{A}_1$, given by\n$F_{Z[n]} (z) := \\frac{1}{n} \\sum_{i=1}^n \\mathbb{1}\\left\\{z_i = z\\right\\}, \\quad \\forall z:= (z_s, z_a) \\in \\mathcal{S}_1 \\times \\mathcal{A}_1.$\nLet $\\mu_n(\\mathcal{Z}_1) = \\{\\frac{|b \\in \\{0,...,n\\}|}{n}\\}^{\\mathcal{S}|\\times|\\mathcal{A}|}$ be the space of $|\\mathcal{S}_1|\\times |\\mathcal{A}_1|$-sized tables, where each entry is an element of $\\{0, \\frac{1}{n},...,1\\}$. In this space, $F_{Z[n]} \\in \\mu_n(\\mathcal{Z}_1)$ where $F_{Z[n]}$ represents the proportion of agents in each state/action pair. The Q-function is permutation-invariant in the local agents, since permuting the labels of homogeneous local agents with the same state will not change the action of the decision-making agent. Hence, $Q(s_g, s_{[n]}, a_g, a_{[n]}) = Q(s_g, s_1, a_g, a_1, F_{Z[n]\\backslash 1})$. Here, $\\tilde{Q} : \\mathcal{S}_g \\times \\mathcal{S}_1 \\times \\mathcal{A}_g \\times \\mathcal{A}_1 \\times \\mu_{n-1}(\\mathcal{Z}_1) \\to \\mathbb{R}$ is a reparameterized Q-function learned by mean-field value iteration: one initializes $\\tilde{Q}^0(s_g, s_1, a_g, a_1, F_{Z[n]\\backslash 1}) = 0$.\nAt each time-step $t$, we update $\\tilde{Q}$ as $\\tilde{Q}^{t+1}(s_g, s_1, a_g, a_1, F_{Z[n]\\backslash 1}) = \\mathcal{T}\\tilde{Q}^t (s_g, s_1, a_g, a_1, F_{Z[n]\\backslash 1})$, where $\\mathcal{T}$ is the"}, {"title": "3 Method and Theoretical Results", "content": "In this section, we propose the SUBSAMPLE-MFQ algorithm to overcome the polynomial (in n) sample com-plexity of mean-field value iteration and the exponential (in n) sample complexity of traditional Q-learning. In our algorithm, the global agent randomly samples a subset of local agents $\\triangle \\subseteq [n]$ such that $|\\triangle| = k$, for $k \\leq n$. It ignores all other local agents $[n] \\backslash \\triangle$, and performs value iteration to learn the Q-function $\\tilde{Q}_{k,m}$ and policy $\\pi_{k,m}$ for this surrogate subsystem of $k$ local agents, where $m$ is the number of samples used to update the Q-functions' estimates of the unknown system. Here, When $|\\mathcal{Z}_1|^{k-1} < k|\\mathcal{Z}_1|$, the algorithm uses traditional value-iteration (Algorithm 1), and when $|\\mathcal{Z}_1|^{k-1} > k|\\mathcal{Z}_1|$, it uses mean-field value iteration (Algorithm 2). The surrogate reward gained by this subsystem at each time step is $r_\\triangle : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$:\n$r_\\triangle(s,a) = r_g(s_g, a_g) + \\frac{1}{k} \\sum_{i \\in \\triangle} r_\\ell(s_g, s_i, a_i).$\nTo convert the optimality of each agent's action within the $k$ local-agent subsystem to an approximate optimality on the full $n$-agent system, we use a randomized policy $\\pi_{\\Delta}^{est, k,m}$ (Algorithm 3), where the global agent samples $\\triangle \\in \\mathcal{U}(\\binom{[n]}{k})$ at each time-step to derive the action $a_g \\sim \\pi_{k,m}^{est} (s_g, s_\\triangle)$, and where each local $i$ agent samples $k - 1$ other local agents $\\triangle_i$ to derive the action $\\pi_{k,m}^{est} (s_g, s_i, s_{\\triangle_i})$. Finally, Theorem 3.4 shows that the policy $\\pi_{\\Delta}^{est, k,m}$ converges to the optimal policy $\\pi^*$ as $k \\to n$. We first present Algorithms 1 and 2 (SUBSAMPLE-MFQ: Learning) and Algorithm 3 (SUBSAMPLE-MFQ: Execution), which we describe below. For this, a crucial characterization is the notion of the empirical distribution function:\nDefinition 3.1 (Empirical Distribution Function). For any population $(z_1,..., z_n) \\in \\mathcal{Z}_1^n$, where $\\mathcal{Z}_1 := \\mathcal{S}_1\\times\\mathcal{A}_1$, define the empirical distribution function $F_{z_\\triangle} : \\mathcal{Z}_1 \\to \\mathbb{R}_+$ for $\\triangle \\subseteq [n]$ such that $|\\triangle| = k$ by:\n$F_{z_\\triangle} (x) := := F_{s_\\triangle a_\\triangle} (x) := \\frac{1}{k} \\sum_{i \\in \\triangle} \\mathbb{1}\\left\\{s_i = z_s, a_i = z_a\\right\\}, \\quad \\forall z:= (z_s, z_a) \\in \\mathcal{S}_1 \\times \\mathcal{A}_1.$\nLet $\\mu_k(\\mathcal{Z}_1) := \\{\\frac{|b \\in \\{0,...,k\\}|}{k}\\}^{\\mathcal{S}|\\times|\\mathcal{A}|}$ be the space of $|\\mathcal{S}_1|\\times |\\mathcal{A}|$-length vectors where each entry in a vector is an element of $\\{0, \\frac{1}{k},...,1\\}$ such that $F_{z_\\triangle} \\in \\mu_k(\\mathcal{Z}_1)$. Here, $F_{z_\\triangle}$ is the proportion of agents in the k-local-agent subsystem at each state.\nAlgorithms 1 and 2 (Offline learning). Let $m \\in \\mathbb{N}$ denote the sample size for the learning algorithm with sampling parameter $k \\leq n$. When $|\\mathcal{Z}_1|^{k-1} < k|\\mathcal{Z}_1|$, we empirically learn the optimal Q-function for a subsystem with k-local agents denoted by $\\tilde{Q}_{k,m}^{est} : \\mathcal{S}_g \\times \\mathcal{S}_\\triangle \\times \\mathcal{A}_g \\times \\mathcal{A}_\\triangle \\to \\mathbb{R}$: set $\\tilde{Q}_{k,m}^{est}(s_g, s_\\triangle, a_g, a_\\triangle)=0$ for all $(s_g, s_\\triangle, a_g, a_\\triangle) \\in \\mathcal{S}_g \\times \\mathcal{S}_\\triangle \\times \\mathcal{A}_g \\times \\mathcal{A}_\\triangle$. At time step t, set $\\tilde{Q}_{k,m}^{t+1}(s_g, s_\\triangle, a_g, a_\\triangle) = \\mathcal{T}_{k,m}\\tilde{Q}_{k,m}^{t}(s_g, s_\\triangle, a_g, a_\\triangle)$, where"}, {"title": "3.1 Theoretical Guarantee", "content": "This subsection shows that the value of the expected discounted cumulative reward produced by $\\pi_{\\Delta}^{est}$ is approximately optimal, where the optimality gap decays as k\u2192n and m becomes large.\nBellman noise. We introduce the notion of Bellman noise, which is used in the main theorem. Consider $\\mathcal{T}_{k,m}$. Clearly, it is an unbiased estimator of the generalized adapted Bellman operator $\\mathcal{T}_k$,\n$\\mathcal{T}_k\\tilde{Q}_k(s_g, s_\\triangle, a_g, a_\\triangle)=r_\\triangle(s,a)+\\gamma \\mathbb{E}_{s'g~P_g(\\cdot | s_g,a),s'\\sim P_1(\\cdot | s_i,s_g,a_i),\\forall i \\in \\triangle} \\max_{a'g, a'_\\triangle}\\tilde{Q}_k(s'_g, s'_\\triangle, a'_g, a'_\\triangle).$\nFor all $(s_g, s_\\triangle, a_g, a_\\triangle) \\in\\mathcal{S}_g \\times\\mathcal{S}_\\triangle \\times \\mathcal{A}_g \\times \\mathcal{A}_\\triangle$, set $\\tilde{Q}_k(s_g, s_\\triangle, a_g, a_\\triangle) = 0$. For $t \\in \\mathbb{N}$, let $\\tilde{Q}_k^{t+1} = \\mathcal{T}\\tilde{Q}_k^t$, where $\\mathcal{T}$ is defined for $k \\leq n$ in Eq. (16). Then, $\\mathcal{T}$ is also a $\\gamma$-contraction (Lemma B.7) with fixed-point $\\tilde{Q}_k$. By the law"}, {"title": "4 Conclusion and Future Works", "content": "This work develops subsampling for mean field MARL in a cooperative system with a global decision-making agent and n homogeneous local agents. We propose SUBSAMPLE-MFQ which learns each agent's best response to the mean effect from a sample of its neighbors, allowing an exponential reduction on the sample complex-ity of approximating a solution to the MDP. We provide a theoretical analysis on the optimality gap of the learned policy, showing that the learned policy converges to the optimal policy with the number of agents k sampled at the rate 0(1/\\sqrt{k}) validate our theoretical results through numerical experiments. We further extend this result to the non-tabular setting with infinite state and action spaces.\nWe recognize several future directions. Firstly, this model studies a 'star-network' setting to model a single source of density. It would be fascinating to extend this subsampling framework to general networks. We believe expander-graph decompositions (Anand & Umans, 2023; Reingold, 2008) are amenable for this. A second direction would be to find connections between our sub-sampling method to algorithms in federated learning, where the rewards can be stochastic. A third direction of this work would be to consider the setting of truly heterogeneous local agents. Finally, it would be exciting to generalize this work to the online setting without a generative oracle: we conjecture that tools from recent works on stochastic approximation (Chen & Theja Maguluri, 2022) and no-regret RL (Jin et al., 2021b) might be valuable."}, {"title": "A Mathematical Background and Additional Remarks", "content": "Outline of the Appendices.\n\u2022 Section 5 presents the proof of the Lipschitz continuity between $\\tilde{Q}_k$ and $\\tilde{Q}^*$\n\u2022 Section 6 presents the TV distance bound\n\u2022 Section 7 proves the bound on the optimality gap between the learned policy $\\pi_{\\Delta}^{est}$ and the optimal policy $\\pi^*$\nDefinition A.1 (Lipschitz continuity). Given two metric spaces (X, dx) and (y, dy) and a constant L\u2208R+, a mapping f: X \u2192 Y is L-Lipschitz continuous if for all x, y \u2208 X, dy(f(x), f(y)) \u2264 L. dx(x, y).\nTheorem A.2 (Banach-Caccioppoli fixed point theorem (Banach, 1922)). Consider the metric space (X, dx), and T : X \u2192 X such that T is a \u03b3-Lipschitz continuous mapping for \u03b3\u2208 (0,1). Then, by the Banach-Caccioppoli fixed-point theorem, there exists a unique fixed point x* \u2208 X for which T(x*) = x*. Additionally, x* = lims\u2192\u221e Ts (xo) for any \u0445\u043e \u2208 X."}, {"title": "B Notation and Basic Lemmas", "content": "For convenience, we restate below the various Bellman operators under consideration.\nDefinition B.1 (Bellman Operator $\\mathcal{T}$).$\n\\mathcal{T}Q' (s, a) := r(s, a) + \\gamma \\mathbb{E}_{s_g'\\sim P_g(\\cdot | s_g, a), \\atop s_i'\\sim P_1(\\cdot | S_i, s_g, a_i), \\forall i \\in [n]}  \\max_{a' \\in \\mathcal{A}} Q'(s', a')\\qquad.\nDefinition B.2 (Adapted Bellman Operator $\\mathcal{T}_k$). The adapted Bellman operator updates a smaller Q function (which we denote by $\\tilde{Q}_k$), for a surrogate system with the global agent and $k\\in [n]$ local agents denoted by $\\triangle$, using mean-field value iteration and $j\\in \\triangle$ such that:\n$\\mathcal{T}_k\\tilde{Q}_k(s_g, s_j, F_{za_j}, a_j, a_g) := r_\\triangle(s, a) + \\gamma \\mathbb{E}_{s_g'\\sim P_g(\\cdot | s_g, a), \\atop s'\\sim P_1(\\cdot | S_i, s_g, a_i), \\forall i \\in \\triangle} \\max_{a' \\in \\mathcal{A}} \\tilde{Q}_k(s'_g, s'_j, F_{za'_j}, a'_j, a'_g).\\qquad$\nDefinition B.3 (Empirical Adapted Bellman Operator $\\mathcal{T}_{k,m}$). The empirical adapted Bellman operator $\\mathcal{T}_{k,m}$ empirically estimates the adapted Bellman operator update using mean-field value iteration by drawing m random samples of $S_g \\sim P_g(s_g, a_g)$ and $s_i  P(.s_i, s_g, a_i)$ for $i \\in \\triangle$, where for $\\ell\\in [m]$, the $\\ell$'th random sample is given by $s^\\ell_g$ and $s^\\ell s_i$, and $j \\in \\triangle$:\n$\\mathcal{T}_{k,m}\\tilde{Q}_{k,m}(s_g, s_j, F_{za_j}, a_j, a_g) := r(s,a) + \\frac{\\gamma}{m} \\sum_{\\ell \\in [m]} \\max_{a'\\in \\mathcal{A}} \\tilde{Q}_{k,m}(s^\\ell_g, s^\\ell_j, F_{za'_j}, a'_j).\\qquad$\nRemark B.4. Since the local agents 1,...,n are all homogeneous in their state/action spaces, the $\\tilde{Q}_k$-function only depends on them through their empirical distribution $F_{za}$. Therefore, throughout the remainder of the paper, we will use $\\tilde{Q}_k (s_g, F_{z_1}, a_g) := \\tilde{Q}_k (s_g, s_i, F_{\\triangle\\backslash i}, a_g, a_i) := \\tilde{Q}_k(s_g, s_\\triangle, a_g, a_\\triangle)$ interchangeably, unless making a remark about the computational complexity of learning each function.\nLemma B.5. For any $\\triangle \\subseteq [n]$ such that $|\\triangle| = k$, suppose $0 \\leq r_\\triangle(s, a) \\leq \\bar{r}$. Then, for all $t\\in \\mathbb{N}$, $0\\leq \\tilde{Q}^t_k \\leq \\frac{\\bar{r}}{1-\\gamma}.\nProof. The proof follows by induction on t. The base case follows from $\\tilde{Q}^0 := 0$. For the induction, note that by the triangle inequality $|\\tilde{Q}^{t+1}||_\\infty \\leq ||r_\\triangle||_\\infty + \\gamma||\\tilde{Q}^{t}||_\\infty\\leq \\frac{\\bar{r}}{1-\\gamma} + \\gamma \\frac{\\bar{r}}{1-\\gamma} = \\frac{\\bar{r}}{1-\\gamma}.\nRemark B.6. By the law of large numbers, $\\lim_{m\\to\\infty} \\mathcal{T}_{k.m} = \\mathcal{T}_k$, where the error decays in $\\mathcal{O}(1/\\sqrt{m})$ by the Chernoff bound. Also, $\\mathcal{T}^\\infty := \\mathcal{T}$. Further, Lemma B.5 is independent of the choice of k. Therefore, for k = n, this implies an identical bound on $\\tilde{Q}^t$. An identical argument implies the same bound on $\\tilde{Q}_{k,m}^{est}$.\n$\\mathcal{T}$ satisfies a $\\gamma$-contractive property under the infinity norm (Watkins & Dayan, 1992). We similarly show that $\\mathcal{T}_k$ and $\\mathcal{T}_{k,m}$ satisfy a $\\gamma$-contractive property under infinity norm in Lemmas B.7 and B.8.\nLemma B.7. $\\mathcal{T}_k$ satisfies the $\\gamma$-contractive property under infinity norm:\n$|\\mathcal{T}_k\\tilde{Q} - \\mathcal{T}_k\\hat{\\tilde{Q}}|\\leq \\gamma||\\tilde{Q} - \\hat{\\tilde{Q}}||_\\infty$\nProof. Suppose we apply $\\mathcal{T}_k$ to $\\tilde{Q}_k(s_g, F_{z_\\triangle}, a_g)$ and $\\hat{\\tilde{Q}}_k(s_g, F_{z_\\triangle}, a_g)$ for $|\\triangle| = k$. Then:\n$|\\mathcal{T}_k\\tilde{Q} - \\mathcal{T}_k\\hat{\\tilde{Q}}||_\\infty = \\gamma \\max_{s_g\\in\\mathcal{S}_g, a_g \\in \\mathcal{A}_g, \\atop F_{Z\\in\\mu_k(\\mathcal{Z}_1)}} \\mathbb{E}_{s'g\\sim P_g(\\cdot | s_g, a), \\atop s'\\sim P_1(\\cdot | S_i, s_g, a_i), \\forall i \\in \\triangle}} \\max_{a'\\in \\mathcal{A}} |\\tilde{Q}(s'_g, F_{z'_\\triangle}, a'_g) - \\mathbb{E}_{s'g\\sim P_g(\\cdot | s_g, a), \\atop s'\\sim P_1(\\cdot | S_i, s_g, a_i), \\forall i \\in \\triangle}} \\max_{a'\\in \\mathcal{A}} \\hat{\\tilde{Q}}_k(s'_g, F_{z'_\\triangle}, a'_g)|\\qquad \\leq \\gamma \\max_{s_g\\in\\mathcal{S}_g, F_{Z \\in\\mu_k(\\mathcal{Z}_1)}, a'\\in \\mathcal{A}} |\\tilde{Q}(s'_g, F_{z'_\\triangle}, a'_g) - \\hat{\\tilde{Q}}(s'_g, F_{z'_\\triangle}, a'_g)| = \\gamma||\\tilde{Q} - \\hat{\\tilde{Q}}||_\\infty$\nThe equality cancels common $r_\\triangle(s, a)$ terms in each operator. The second line uses Jensen's inequality, maximizes over actions, and bounds expected values with the maximizers of the random variables."}, {"title": "C Proof Sketch", "content": "This section details an outline for the proof of Theorem 3.4", "steps": "firstly", "F_{z[n": ""}, ".", "Next, we bound the TV distance between $F_{z_\\triangle}$ and $F_{z[n"]}, {"1": "Lipschitz Continuity Bound. To compare $\\tilde{Q"}, {"s_\\ell)^n$": "nTheorem C.1 (Lipschitz continuity in $\\tilde{Q"}, {"2": "Bounding Total Variation (TV) Distance. We bound the TV distance between $F_{z_\\triangle"}, {"show": "nTheorem C.2. Given a finite population $\\mathcal{Z"}, {"mathcal{Z}_1$": ""}]