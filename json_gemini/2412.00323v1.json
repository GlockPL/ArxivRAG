{"title": "Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments", "authors": ["YASUAKI SUMITA", "KOH TAKEUCHI", "HISASHI KASHIMA"], "abstract": "Large Language Models (LLMs) are trained on large corpora written by humans and demonstrate high performance on various tasks. However, as humans are susceptible to cognitive biases, which can result in irrational judgments, LLMs can also be influenced by these biases, leading to irrational decision-making. For example, changing the order of options in multiple-choice questions affects the performance of LLMs due to order bias. In our research, we first conducted an extensive survey of existing studies examining LLMs' cognitive biases and their mitigation. The mitigation techniques in LLMs have the disadvantage that they are limited in the type of biases they can apply or require lengthy inputs or outputs. We then examined the effectiveness of two mitigation methods for humans, SoPro and AwaRe, when applied to LLMs, inspired by studies in crowdsourcing. To test the effectiveness of these methods, we conducted experiments on GPT-3.5 and GPT-4 to evaluate the influence of six biases on the outputs before and after applying these methods. The results demonstrate that while SoPro has little effect, AwaRe enables LLMs to mitigate the effect of these biases and make more rational responses.", "sections": [{"title": "1 Introduction", "content": "In recent years, Large Language Models (LLMs) such as GPT-4 [33] have been developed rapidly and have shown high performance on various tasks such as machine translation [24], summarization [45], and annotation [15]. This high level of performance is achieved by learning from large corpora of human-written documents.\nHowever, humans exhibit various cognitive biases that lead to irrational decisions. Cognitive bias is a systematic pattern of deviation from norm or rationality in judgment [50]. For example, humans often adjust their estimates insufficiently away from initial values. This is called \u201canchoring\" [50]. Another example is the \u201cbandwagon effect\", a tendency to adopt certain behaviors, styles, or attitudes simply because others are doing so [28].\nSince these cognitive biases also affect human-written documents, LLMs can inherit these biases during training, leading to irrational outputs. For example, in multiple-choice questions, changing the order of options affects the performance of LLMs due to order bias [35]. Additionally, when LLMs generate answers to questions, including irrelevant information in the question text can lead them to provide incorrect answers due to anchoring [23].\nSeveral studies work on mitigating these cognitive biases in LLMs. For example, to deal with order bias, some methods have been proposed to generate multiple outputs for the same question by changing the order of the options [19] or to output the reasons along with the answers [58]. However, these methods have limited applicability, or it is unclear if they are also applicable to other biases. Additionally, These methods require longer inputs and outputs or asking LLMs the same questions many times."}, {"title": "2 Cognitive biases in LLMs", "content": "Many studies show that LLMs exhibit various cognitive biases that lead to irrational output. Table 1 shows which type of cognitive bias each study addresses.\nCoBBLEr (Cognitive Bias Benchmark for Large Language Models as Evaluators) [26] is a benchmark used to assess the cognitive biases of LLMs. CoBBLEr addresses six cognitive biases: order bias, compassion fade, egocentric bias, bandwagon effect, attentional bias, and verbosity bias. Other studies have shown that these biases reduce the performance of LLMs.\nOrder bias. Order bias [21] is the tendency to prefer an option based on its position in a list rather than the intrinsic quality of its content. Generally, humans favor the option at the top of a list [3]. Several studies have identified similar imbalances in the outputs of LLMs [19, 57, 58]. In many cases, LLMs prefer the first option [26, 35, 36, 48, 52, 53, 55].\nCompassion fade. Compassion fade [9] is the decrease in helping or compassionate intent and behavior as the number of people in need increases. LLMs, like humans, behave differently depending on whether the name being evaluated is anonymized or not [26].\nEgocentric bias. Egocentric bias [37] is the tendency to rely on one's perspective or to evaluate oneself favorably. Some studies indicate that LLMs prioritize their responses regardless of quality, which can produce irrational results [26, 58].\nBandwagon effect. Bandwagon effect [28] is the tendency to prefer an option simply because it is favored by the majority. LLMs are vulnerable to this bias and prefer the option chosen by the majority, which compromises their decision-making accuracy [26].\nAttentional bias. Attentional bias is the influence of selective factors on a person's perception [4]. We explore one of attentional bias: distraction. Distraction is the tendency for a person's attention to be diverted and their judgment distorted by irrelevant information. Some studies show that the performance of LLMs is compromised when prompts include irrelevant information [26, 43].\nVerbosity bias. Verbosity bias [58] is the tendency to favor longer responses, regardless of their quality. This bias is a form of salience bias, where irrational judgments are made by giving undue attention to more noticeable attributes [40]. Some studies indicate that LLMs tend to prefer longer responses due to this bias, impacting their effectiveness and accuracy [26, 38, 55, 58]."}, {"title": "3 Mitigating cognitive biases of LLMs", "content": "Some Techniques are proposed to mitigate cognitive bias in LLMs.\nSeveral methods exist to address order bias and primacy effect in multiple-choice questions: inputting the same question multiple times while shuffling the positions of the options [19, 35, 52, 58], learning additional parameters [56], Bayesian probabilistic framework [30], self-supervised position debiasing framework [29], controlling the output so that it follows a particular format or structure [13]. There is also a method to remove the bias against the labels of the options [57]. However, these methods are specific to order bias and are not used for other types of cognitive bias.\nFor attentional bias, there is a method to prompt LLMs to ignore irrelevant information to help them focus on the essential aspects of tasks [43]. However, this method is not used for other biases."}, {"title": "3.2 Applying cognitive bias mitigation methods in crowdsourcing to prompts of LLMs", "content": "There are methods for mitigating cognitive biases in crowdsourcing. Cognitive biases can significantly degrade the quality of annotations obtained through crowdsourcing [14]. In order to mitigate biases in crowdsourcing, there are two techniques for making changes to the instructions [20]: SoPro and AwaRe. SoPro asks crowdworkers to predict the label that they believe the majority of other workers would choose. AwaRe makes crowdworkers aware of their inherent biases before they answer. These two methods can also be used for cognitive biases [17].\nTherefore, inspired by these studies, we adapted these mitigation methods in crowdsourcing to the prompts of LLMs.\nSoPro: SoPro (Social Projection) adds a sentence to the prompt instructing LLMs to consider how the majority of people would respond. For example, the following statement is added at the beginning: \"Please answer the following question according to how you believe the majority of people would answer.\u201d This method encourages LLMs to reflect broader social consensus rather than individual or idiosyncratic interpretations.\nAwaRe: AwaRe (Awareness Reminder) adds a statement that informs the presence of a bias and instructs LLMs to be careful of this bias while answering. In addressing order bias, for example, the following statement is added at the beginning: \u201cPlease answer the following question while being aware of order bias.\u201d This method is designed to prompt LLMs to consciously adjust their responses to mitigate the influence of the bias.\nThese methods have two advantages over existing methods. First, there is no need to output an explanation of the answer, and the same problem is not solved repeatedly, resulting in less lengthy inputs and outputs. Second, our methods are versatile and can be applied to any cognitive bias, as they are not limited by question format or type of bias."}, {"title": "4 Experiments", "content": "To evaluate the effectiveness of our methods, we conducted experiments using the CoBBLEr benchmark. The CoBBLEr benchmark [26] quantifies the influence of six cognitive biases on evaluations of text quality by LLMs. In this experiment, we applied this benchmark to both GPT-3.5 and GPT-4 in a manner consistent with Koo et al. [26]. Details of the LLMs used for this experiment are provided in Appendix A. Results obtained without using any of the methods are used as a baseline. In addition, the existing method of asking LLMs the reason for their answer [52] was used for comparison."}, {"title": "4.1 Evaluation method", "content": "The six biases addressed in this experiment are order bias, compassion fade, egocentric bias, bandwagon effect, attentional bias, and verbosity bias. We conducted experiments to assess the influence of each of these biases."}, {"title": "4.2 Evaluation setting", "content": "We describe how the prompts are modified to account for each bias, and then define the scores used to assess the influence of the bias. This modification of the prompts is consistent with that of Koo et al. The modified prompt templates are shown in Appendix B. Let $Y = 0$ denote that the first response is better, and $Y = 1$ denote that the second response is better. Let $P$ denote the ratio of all response pairs.\nOrder bias. To address order bias, we input two prompts to LLMs when evaluating responses: one with the first response listed first, and the other with the second response listed first. If order bias does not influence the evaluation, the LLM should select the same response in both cases. We consider it an inconsistent response when different outputs are chosen.\nLet $A = 0$ indicate that the first answer is presented first and $A = 1$ indicate that the second answer is presented first. We define $S_{order1}$ as the score when the first answer is chosen in both cases and $S_{Order2}$ as the score when the second answer is chosen in both cases, calculated as follows:\n$S_{Order1} = P ((Y = 0, A = 0) \\land (Y = 1, A = 1))$,\n$S_{Order2} = P ((Y = 1, A = 0) \\land (Y = 0, A = 1))$.\nIf the LLM is not affected by order bias, both $S_{Order1}$ and $S_{Order2}$ should be 0.\nCompassion fade. We examine the outputs of LLMs when the actual model names are used instead of anonymized names. When investigating compassion fade, the analysis is complicated by the simultaneous influence of order bias. Therefore, we perform the same comparisons as in the study of order bias, using both anonymized and actual model names, and then compare the results. If the LLM is unaffected by compassion fade, it should output consistent results regardless of whether the model names are anonymized or not.\nLet $A = 0$ indicate that the first answer is presented first, and $A = 1$ indicate that the second answer is presented first. Additionally, let $Y' = 0$ when the LLM determines that the first response is better, and $Y\u2032 = 1$ when the LLM determines that the second response is better. The scores are defined as follows:\n$S_{Comp1} = P ((Y' = 0, A = 0) \\land (Y' = 0, A = 1))$,\n$S_{Comp2} = P ((Y' = 1, A = 0) \\land (Y' = 1, A = 1))$.\nIf the LLM is not affected by compassion fade, $S_{Comp1}$ and $S_{Comp2}$ should be equal to $S_{Order1}$ and $S_{Order2}$, respectively.\nEgocentric bias. When evaluating responses, we anonymize LLMs and indicate one of the re-sponses as the LLM's own by adding \u201c(You)\u201d to either \u201cSystem Star\u201d or \u201cSystem Square\u201d. If the"}, {"title": "4.3 Results", "content": "The results of this experiment are presented in Table 2 and Table 3. We use the results from in-putting a modified prompt for each bias as the baseline and compare these with the outcomes from applying the existing method [52] and our methods to this prompt. In these tables, higher scores are colored red, and lower scores are colored blue, relative to the case where the responses are random. Furthermore, the intensity of the color increases with the deviation from the random response case. This color coding indicates that red scores signify less consistency and greater influence by bias, whereas blue scores suggest greater consistency and less influence by bias. In this section, we analyze the results by each bias.\nOrder bias. At baseline, GPT-3.5 shows a preference for the first option, while GPT-4 tends to prefer the second option. Both GPT-3.5 and GPT-4 are susceptible to order bias, but the effect is smaller for GPT-4 than for GPT-3.5. Despite the application of the existing method and our methods, no significant changes in performance are observed for either GPT-3.5 or GPT-4 relative to their baselines.\nCompassion fade. Since the evaluation of compassion fade may be influenced by order bias, we analyze its effect by comparing the result with that of order bias. At baseline, GPT-3.5 prefers"}, {"title": "4.4 Discussion", "content": "GPT-4 is generally less susceptible to cognitive biases compared to GPT-3.5. At baseline, GPT-3.5 exhibits robustness to order bias, compassion fade, and egocentric bias. However, it is significantly affected by bandwagon effect, attentional bias, and verbosity bias, demonstrating a lack of robustness against these biases. Conversely, GPT-4 shows a reduced susceptibility to all six biases. The effects of bandwagon effect and verbosity bias are present but less marked than in GPT-3.5, indicating higher resistance to these biases.\nThe existing method proves effective for mitigating bandwagon effect and attentional bias in GPT-3.5, as well as bandwagon effect in GPT-4. This effectiveness can be attributed to the method's prompting of LLMs to provide reasons for their answers, which encourages more rational re-sponses. This aligns with findings from previous studies. In the case of attentional bias, there is a significant increase in the percentage of valid responses for GPT-3.5, indicating success in mitigating this bias.\nSoPro is effective in mitigating egocentric bias in GPT-3.5 and verbosity bias in GPT-4. This effec-tiveness may be attributed to the more objective responses elicited by having LLMs respond based on how they think the majority of people would respond. However, SoPro increases the models' susceptibility to bandwagon effect, as it inherently aligns their responses with the opinion of the"}, {"title": "5 Related work", "content": "Many studies have demonstrated that LLMs are susceptible to cognitive biases. Table 1 shows which type of cognitive bias each study addresses.\nSeveral studies have highlighted that LLMs display cognitive biases similar to humans. Suri et al. [46] investigate whether GPT-3.5 employs human-like decision heuristics and biases, such as anchoring, and find that GPT-3.5 exhibits similar effects to humans in various tests. Lampinen et al. [27] demonstrate that LLMs show knowledge effects in reasoning, where performance on logical tasks improves when semantic content aligns with correct logical inferences. Shaki et al. [42] confirmed a range of cognitive biases, like priming effect, in GPT-3.\nLLMs also respond similarly to human social cognitive patterns. Bian et al. [6] explore how external statements and opinions influence the cognition and behaviors of LLMs, discovering that their responses to external information reflect human social cognitive patterns, such as authority and in-group biases.\nCognitive biases that differ from those in humans may reduce LLM's ability to reason. Macmillan-Scott et al. [31] evaluated LLMs on a cognitive psychology task to determine whether they make rational answers for mathematical questions, and showed that LLMs exhibit irrational biases that are distinct from humans. Opedal et al. [32] examined whether LLMs' responses include the cognitive biases that children exhibit when solving problems, and found that LLMs exhibited different biases from children.\nSeveral studies have concentrated on the types of LLMs and their specific cognitive biases. Hagendorff et al. [16] report that while earlier models like GPT-3 exhibit human-like intuitive behaviors and cognitive errors, more advanced models, such as ChatGPT and GPT-4, show im-provements, overcoming these errors and demonstrating rational decision-making as evidenced by psychology-based tests like the Cognitive Reflection Test. Itzhak et al. [22] explore the impact of instruction tuning and RLHF (Reinforcement Learning from Human Feedback) [34] on decision-making in LLMs, particularly analyzing the presence of decoy effect, certainty effect, and belief bias in instruction-tuned models such as GPT-3.5. Tjuatja et al. [48] investigate human-like response biases in survey design by LLMs, finding that popular models, especially after RLHF, generally do not accurately mimic human behavior. These are consistent with the result of Casper et al. that point out significant problems and inherent flaws in RLHF [10].\nSome studies are exploring the effects of its application in specific areas. Jones and Steinhardt [23] focus primarily on code generation models, demonstrating that some biases such as framing can degrade the quality of their outputs. Since cognitive bias in medical LLM can lead to inaccurate diagnosis and treatment recommendations, the BiasMedQA dataset was introduced to assess this [41]. Several studies have addressed order and selection bias, which affect the reliability of recom-mendation systems [19, 30]. Talboy and Fuller [47] examine the impact of several biases on LLMs and advocate for enhanced education, risk management, and best practices to ensure responsible adoption of this technology."}, {"title": "5.2 Comparison of LLMs and humans", "content": "Many studies investigate whether LLMs can serve as a viable alternative to humans in various NLP tasks. Chiang and Lee [11] demonstrate that LLMs are comparable to human experts in assessing text quality. Wang et al. [51] indicate that ChatGPT demonstrates a high correlation with human judgment when used as a natural language generation evaluation metric. Furthermore, it is very difficult to distinguish between the responses of humans and LLMs [8].\nAdditionally, several studies demonstrate the performance of LLMs in crowdsourcing annota-tion tasks. Gilardi et al. [15] demonstrate that ChatGPT surpasses crowd workers in various text annotation tasks, achieving higher accuracy at lower cost. T\u00f6rnberg [49] demonstrates that GPT-4 outperforms experts and crowd workers in accurately and reliably classifying the political affilia-tion of text from U.S. politicians, with comparable or reduced bias.\nSome studies examine whether LLMs replicate findings established in social sciences, such as economics, psycholinguistics, and social psychology. Aher et al. [1] evaluate the ability of LLMs to simulate diverse human behaviors and show that ChatGPT and GPT-4 are effective at replicating human behavior and answering questions. Binz and Schulz [7] conduct cognitive psychology ex-periments to assess GPT-3's decision-making and reasoning, revealing vulnerabilities such as poor causal reasoning and sensitivity to task perturbations. Horton [18] discusses the capacity of LLMs to serve as computational analogs to humans and to demonstrate how LLMs are appropriate for simulating human behavior. Sinclair et al. [44] investigated how structural priming affects LLMs\u2019 ability to learn and utilize abstract structural information.\nOther studies investigate how LLMs replicate public opinion. Argyle et al. [2] demonstrate that GPT-3 can accurately reflect diverse human attitudes and serve as a powerful tool for studying human society. However, Santurkar et al. [39] demonstrate a significant misalignment of LLMs with public opinion across diverse U.S. demographics and identify demographic groups whose views are underrepresented by LLMs."}, {"title": "6 Conclusion", "content": "In this study, we first surveyed existing studies that examine cognitive biases in LLMs and methods to mitigate them (Table 1). Many studies show that LLMs are affected by various types of cognitive biases. On the other hand, existing mitigation methods have the disadvantage that they are limited in the type of biases to apply or have lengthy inputs or outputs.\nWe then addressed the introduction of two mitigation methods adapted from crowdsourcing, SoPro and AwaRe, into the prompts of LLMs. SoPro encourages LLMs to respond based on how they think the majority would answer. AwaRe enhances awareness of biases and prompts LLMs to answer with greater attention to mitigating these biases. Finally, we conducted experiments using the CoBBLEr benchmark to compare the effectiveness of these methods with the existing method. The results indicate that GPT-3.5 naturally exhibits robustness to some biases, and GPT-4 demonstrates greater resistance to all tested biases. The results also revealed that while SoPro was less effective, AwaRe successfully promoted rational responses and mitigated bias effects.\nThis study has limitations. Our focus was primarily on prompt modification as a means of bias mitigation, without exploring alternative approaches like fine-tuning with additional data. It is possible that these approaches can mitigate the biases. In addition, AwaRe requires inputting the name of the bias to be mitigated, so it is necessary to know which bias will affect the LLM in advance. Moreover, the experiments used only GPT-3.5 and GPT-4, and did not address variations"}, {"title": "A LLM details", "content": "The version of GPT-3.5 we used is gpt-3.5-turbo-1106, and the version of GPT-4 we used is gpt-4-1106-preview. We accessed both models through OpenAI API. We set the same hyperparameters for both models as follows. We set temperature to 0 and seed to 0."}, {"title": "B Prompt templates", "content": "We use the same template of the prompts used by Koo et al. in our experiment. These templates are modified from the original prompt according to each bias as the bias affects LLMs.\nThe instruction is represented by \u201c{instruction},\u201d the reference by \u201c{reference},\u201d the response of the first model by \u201c{response_star},\u201d and the response of the second model by \u201c{response_square}.\u201d"}, {"title": "B.1 Original prompt", "content": "### Instruction: Which system's response is more coherent considering the reference and\ninstruction?\nThe instruction is: {instruction}\nThe reference is: {reference}\n\u060c\u060c\u060c\nSystem Star: {response_star}\nSystem Square: {response_square}\n\u060c\u060c\u060c\nPlease respond in the following format strictly:\nSystem is better\nDo not provide any additional text or explanation:\n### Response:"}, {"title": "B.2 Order bias", "content": "### Instruction: Which system's response is more coherent considering the reference and\ninstruction?\nThe instruction is: {instruction}\nThe reference is: {reference}\n\u060c\u060c\u060c\nSystem Square: {response_square}\nSystem Star: {response_star}\n\u060c\u060c\u060c\nPlease respond in the following format strictly:\nSystem is better\nDo not provide any additional text or explanation:\n### Response:"}, {"title": "B.3 Compassion fade", "content": "The name of the first model is represented by \u201c{model0_name},\u201d the name of the second model\nby \"{modell_name},\u201d the response of the first model by \u201c{model0_response},\u201d the response of the\nsecond model by \u201c{model1_response}.\u201d\n### Instruction: Which model's response is more coherent considering the reference and\ninstruction?\nThe instruction is: {instruction}\nThe reference is: {reference}\n66\nModel {model0_name}: {model0_response}\nModel {model1_name}: {model1_response}\n\u060c\u060c\u060c\nPlease respond in the following format strictly:\nModel is better\nDo not provide any additional text or explanation:\n### Response:"}, {"title": "B.4 Egocentric bias", "content": "\"(You)\" is added after either \u201cSystem Star\u201d or \u201cSystem Square\u201d.\n### Instruction: Which system's response is more coherent considering the reference and\ninstruction?\nThe instruction is: {instruction}\nThe reference is: {reference}\n\u060c\u060c\u060c\nSystem Star (You): {response0}\nSystem Square: {response1}\n\u060c\u060c\u060c\nPlease respond in the following format strictly:\nSystem is better\nDo not provide any additional text or explanation:\n### Response:"}, {"title": "B.5 Bandwagon effect", "content": "\"{bandwagon_percent}\u201d is a random value added to affect bandwagon effect and can take on an\ninteger value between 60 and 90. \u201c{system}\u201d is either \u201cSystem Star\u201d or \u201cSystem Square.\""}, {"title": "B.6 Attentional bias", "content": "\"{distraction}\u201d is an irrelevant sentence added to affect attentional bias, which can be one of the\nfollowing four sentences. \u201c{system}\u201d is either \u201cSystem Star\u201d or \u201cSystem Square.\u201d\n\u2022 \u201c{system} likes to eat apples and oranges\u201d\n\u2022 \u201cThe coolest thing that {system} can do is a 60 second handstand\"\n\u2022 \u201c{system} plays a lot of soccer and basketball\u201d\n\u2022 \u201c{system} has been all around Europe two times\u201d"}]}