{"title": "Composite Sketch+Text Queries for Retrieving Objects with Elusive Names and Complex Interactions", "authors": ["Prajwal Gatti", "Kshitij Parikh", "Dhriti Prasanna Paul", "Manish Gupta", "Anand Mishra"], "abstract": "Non-native speakers with limited vocabulary often struggle to name specific objects despite being able to visualize them, e.g., people outside Australia searching for 'numbats.' Further, users may want to search for such elusive objects with difficult-to-sketch interactions, e.g., \"numbat digging in the ground.\" In such common but complex situations, users desire a search interface that accepts composite multimodal queries comprising hand-drawn sketches of \"difficult-to-name but easy-to-draw\" objects and text describing \"difficult-to-sketch but easy-to-verbalize\" object's attributes or interaction with the scene. This novel problem statement distinctly differs from the previously well-researched TBIR (text-based image retrieval) and SBIR (sketch-based image retrieval) problems. To study this under-explored task, we curate a dataset, CSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of ~2M queries and 108K natural scene images. Further, as a solution to this problem, we propose a pretrained multimodal transformer-based baseline, STNET (Sketch+Text Network), that uses a hand-drawn sketch to localize relevant objects in the natural scene image, and encodes the text and image to perform image retrieval. In addition to contrastive learning, we propose multiple training objectives that improve the performance of our model. Extensive experiments show that our proposed method outperforms several state-of-the-art retrieval methods for text-only, sketch-only, and composite query modalities. We make the dataset and code available at: https://vl2g.github.io/projects/cstbir.", "sections": [{"title": "Introduction", "content": "Traditional text-based image retrieval (TBIR) systems (Li et al. 2020a; Kim, Son, and Kim 2021; Zhang et al. 2020; Lee et al. 2018; Li et al. 2020b) are intuitive for users with strong linguistic abilities. However, non-native speakers or users unfamiliar with particular objects struggle in using such systems to find objects with \u201celusive\u201d names, e.g., users outside Australia searching for numbats, as shown in Figure 1. Elaborate text descriptions in lieu of the precise object name could provide limited help, even with all the details. For example, \u201cSmall mammal with striped back and long snout digging in the ground\u201d as a replacement for \u201cnumbats\u201d leads to images of chipmunk, badger, weasel, mongoose, or skunk.\nSketch-based image retrieval (SBIR) systems (Yu et al. 2016; Dey et al. 2019; Song et al. 2017b; Collomosse, Bui, and Jin 2019; Sain et al. 2022) seem to provide an illusory relief in such situations. Although a user can sketch \"difficult-to-name but easy-to-draw\" objects, (1) users may not have enough time, skills, or tools to draw all the details, leading to ambiguity in sketches; (2) users may be looking for \"difficult-to-sketch but easy-to-verbalize\" object's attributes or interaction with the scene. For example, for the query, \"numbat digging in the ground\", it is difficult to draw a sketch to represent \u201cdigging in the ground\", and even if drawn, the sketch could lead to false positives about \"numbat eating an insect\u201d or \u201cnumbat searching for termites\u201d.\nSuch common but complex search situations require novel multimodal search interfaces, allowing seamless text and sketch mix-ups in queries. Such a flexible and natural interface should help the user to draw sketches for \"difficult-to-name\" objects effortlessly and then complement their creations with text descriptions to define layout, color, pose, and other object characteristics, along with complex interactions with other objects in scenes. We refer to such a novel proposed system as CSTBIR or Composite Sketch+Text Based Image Retrieval system."}, {"title": "Related Work", "content": "Image retrieval systems can answer queries expressed using hand-drawn sketches (SBIR), text (TBIR), a combination of sketch and text (CSTBIR), color layout, concept layout (Zhou, Li, and Tian 2017), visual features (Tian, Newsam, and Boakye 2023; Dodds et al. 2020), or location-sensitive tags (Gomez et al. 2020). We review existing work on TBIR, SBIR, and multimodal query-based IR.\nSketch-Based Image Retrieval (SBIR): It allows the flexibility to easily specify the qualitative characteristics using sketches (Yu et al. 2016; Dey et al. 2019; Song et al. 2017b). Following the initial work on sketch recognition (Sun et al. 2012), earlier SBIR studies mainly focused on convolutional neural networks (CNN) (Yu et al. 2016; Liu et al. 2017) which was soon followed by various Transformer (Vaswani et al. 2017)-based architectures (Ribeiro\net al. 2020; Chowdhury et al. 2022). Deep Siamese models with triplet loss have also been explored (Yu et al. 2016; Collomosse, Bui, and Jin 2019). Several specialized SBIR settings have also emerged such as Zero Shot-SBIR (Pandey et al. 2020; Dey et al. 2019; Dutta and Akata 2019), fine-grained SBIR (Liu et al. 2020; Bhunia et al. 2022; Pang et al. 2019, 2017; Ling et al. 2022; Bhunia et al. 2020; Song et al. 2017b), and category-level SBIR (Sain et al. 2021; Bhunia et al. 2021; Sain et al. 2022).\nText-Based Image Retrieval (TBIR): Popular methods for TBIR include alignment of input text and the corresponding input image using pretrained multimodal Transformer methods like VisualBERT (Li et al. 2020a) and ViLT (Kim, Son, and Kim 2021). Further, cross-attention-based models (Zhang et al. 2020; Lee et al. 2018) and models that use object tags detected in images (Li et al. 2020b) have also been proposed. Recently, contrastive learning methods (Jia et al. 2021), along with zero-shot learning (Radford et al. 2021), have been shown to achieve state-of-the-art results.\nMultimodal Query Based Image Retrieval: Several systems have been built to consume multimodal input for image retrieval. Earlier works used reference images and text as an attribute on a category-level retrieval (Kovashka, Parikh, and Grauman 2012; Han et al. 2017). Input text data was more elaborated to provide improved results (Guo et al. 2018; Vo et al. 2019). While such earlier systems used CNNs, more recent systems (Song et al. 2023; Baldrati et al. 2022) leverage Transformers. Further, some studies (Changpinyo et al. 2021; Pont-Tuset et al. 2020) explored the setting where the user simultaneously uses both speech and mouse traces as the query. Lastly, (Nakatsuka, Hamasaki, and Goto 2023) search images relevant to input music.\nIt is not always possible to have an input reference image for image retrieval; instead, a sketch (along with text description) is used, which gives more flexibility. Image retrieval using hand-drawn sketches and textual descriptive data has been under-explored.\nDetailed sketch and text input have been used to (a) retrieve e-commerce product images using CNNs and LSTMS (Song et al. 2017a), and (b) retrieve scene images using CLIP (Sangkloy et al. 2022; Chowdhury et al. 2023a). However, in several practical scenarios, (a) the sketch is object-level, very rough, and not elaborate, and (b) the text is partial (complementary to sketch) and not self-contained."}, {"title": "The CSTBIR Problem and Dataset", "content": "Given a hand-drawn sketch S, a complementary text T and a database D = {Ii}i=1N of natural scene images with multiple objects, the CSTBIR problem aims to rank the N images according to relevance to the composite (S, T) query.\nDue to the lack of a suitable dataset, we curate the CST-BIR dataset, where each sample consists of a hand-drawn sketch, a partial complementary text description, and a relevant natural scene image. The natural scene images in the database have multiple object categories, attributes, relationships, and activities. Although this dataset does not have \"difficult-to-name\" objects, it is a reasonable proxy. We also evaluate using a manually curated separate test set of \"difficult-to-name\" objects.\nThe natural images and text descriptions are taken from Visual Genome (Krishna et al. 2017). The dense annotations in this dataset allow us to frame multiple queries related to an image, each of which pertains to a particular object in the image. The hand-drawn sketches are taken from the Quick, Draw! dataset (Ha and Eck 2018). Annotators have drawn these sketches in <20 seconds; hence, they are rough and lack the exact details as that of an image, which aligns with the challenging real-world setting of this task. Quick, Draw! has over 50M sketches across 345 categories."}, {"title": "The Proposed STNET Model for CSTBIR", "content": "For the task of sketch and text-based image retrieval, we introduce STNET (Sketch+Text Network), a novel multimodal architecture. It comprises three independent Transformer-based encoder networks based on the pretrained CLIP model (Radford et al. 2021). The overall architecture of STNET is illustrated in Figure 3. Next, we describe the working and architectural details of STNET in the following subsections."}, {"title": "Query (Sketch+Text) Encoding", "content": "In CSTBIR, the query consists of a text sentence and a hand-drawn sketch. We independently encode these two inputs using a pretrained CLIP text encoder and a pretrained Vision Transformer (ViT) (Dosovitskiy et al. 2021) encoder. Given a query text sentence T, we tokenize it using a Byte-Pair-Encoding scheme according to the learned vocabulary of the text encoder as Ft = [CLS,t1,t2,..., tn], where each ti represents a sub-word token, and CLS is the global pool token. Given the query sketch image S, we use a pretrained ViT encoder which is fed the input Fs = [CLS,s1,s2,..., sm], where each si is the embedding of an image patch. As the ViT encoder is pretrained on the ImageNet-21K dataset (Ridnik et al. 2021), we first train it on the sketch data for the classification task to adapt it for the sketch domain. This trained encoder is then used to embed the sketch input. Overall, this results into text embedding hCLS and sketch embedding hSLS."}, {"title": "Image Encoding", "content": "To utilize the benefits of large-scale pretraining, we use the pretrained CLIP-ViT image encoder. Similar to the ViT encoder in the sketch, a candidate scene image I is reshaped to a fixed size (224 \u00d7 224) and then spatially sliced into a 16 \u00d7 16 grid of non-overlapping image patches. Further, these image slices are then reshaped into a sequence of embeddings before passing it to the ViT for further processing.\nAs our problem focuses on queries related to objects in natural scenes, it would be beneficial for our model to focus on the object being queried in the scene image. To enable this, we would like to use the sketch input S to localize or attend to the corresponding object of interest in the image. Specifically, as shown in Figure 3, we use the pooled output of the sketch encoder hCLS to calculate dot-product attention over the output embeddings of the image encoder HI. The obtained values represent attention scores \u03b1IS over the spatial regions of the image as well as the CLS token. We obtain weighted values of image embeddings HI, which are then average pooled to get the final image embedding hAVG. Mathematically, \u03b1IS = Softmax(HI \u00d7 hCLS), HI = \u03b1IS \u00b7 HI and hAVG = hAVG, where hAVG represents the global average pooled embedding of the image encoder."}, {"title": "STNET Training", "content": "STNET follows multiple task-specific training objectives.\nContrastive Training (LCT) We adopt a batch-wise contrastive learning strategy akin to CLIP (Radford et al. 2021) to facilitate image retrieval. Given a batch of N paired (query, image) samples from the train set, we aim to maximize the cosine similarity of the image and query embeddings of the N real pairs in the batch while minimizing the cosine similarity of the embeddings of the N(N \u2013 1) incorrect pairings. We use conditional sampling to ensure uniqueness, i.e., a query does not match multiple images and vice versa. Particularly, we use the InfoNCE loss between hCLS and hAVG to obtain the contrastive loss (LCT) as done in\nCLIP. Further, as our model utilizes the pretrained CLIP, which lacks joint modeling of text, sketch, and image modalities, we propose three additional training losses to be optimized concomitantly with the contrastive objective.\nObject Classification (LCLS and LCLS) Given that the CSTBIR problem focuses on object-specific queries, we propose separately predicting the object name from the text sentence and image inputs. To this end, we train the text and image encoders for the multi-class classification objective to predict the object's class from the C object categories available in the train set. Since the object's label is not mentioned in the text sentence or is always prominent in the image, this objective requires the model to use the contextual information from both modalities to predict the object class. We refer to the classification losses computed using text encodings and the image encodings as LCLS and LCLS, respectively.\nSketch-Guided Object Detection (LOD) To aid the localization of the query-relevant object while encoding the image information, inspired by the recent literature in the sketch-guided object detection problem (Tripathi et al. 2020; Chowdhury et al. 2023b; Tripathi et al. 2023), we propose the training objective of sketch-guided localization of the object. Specifically, given the sketch-attended embeddings HI from the image encoder, we utilize the embeddings corresponding to the 16 \u00d7 16 spatial grids. Following the implementation from YOLO (Redmon et al. 2016), we transform the output embeddings of the ViT network to predict an output of shape S\u00d7S\u00d7(5B+C), where S\u00d7S represents the image grid size, each predicting B bounding boxes, and C class probabilities. We use S = 7, B = 2, and we have C = 258 classes in our train set, so we predict a 7 \u00d7 7 \u00d7 268 output tensor. Finally, we use intersection over union (IoU) to calculate the multipart object detection (LOD) loss as done in (Redmon et al. 2016).\nSketch Reconstruction (LSR) Similar to the object detection training objective, which facilitates the localization of the relevant objects, we introduce the task of sketch reconstruction using the image features HI as illustrated in Figure 3. We employ eight blocks of Convolution-BatchNorm-ReLu as done in (Isola et al. 2017) to upsample the information\nto a reconstructed sketch tensor of size 1 x 224 \u00d7 224. Further to train the sketch-reconstruction module, we utilize a combination of Binary Cross Entropy loss and the DICE loss (Sudre et al. 2017) as LSR = QLBCE + BLDICE.\nOur overall loss is the sum of all five losses LCT+LCLS+LCLS + LOD + LSR.\nWe measure the distance between the query and the image embedding during retrieval using cosine similarity. We provide the implementation details for STNET in the Appendix and make code and dataset available at our project page2."}, {"title": "Experiments and Results", "content": "We compare STNET extensively with competitive image retrieval baselines.\nSketch-based Image Retrieval (SBIR): SBIR is a prominently studied domain in the literature. In our setup, from our composite queries, we only take sketches and drop text to experiment with these baselines. We choose two representative and competitive SBIR methods as our baselines: Doodle2Search (Dey et al. 2019) and DeepSBIR (Yu et al. 2016). We also create a Vision Transformer-based SBIR baseline, viz. ViT-based Siamese Network. This network comprises two ImageNet pre-trained ViT-based encoders for sketch and image modalities, trained using the InfoNCE loss (Oord, Li, and Vinyals 2018).\nText-based Image Retrieval (TBIR): These baselines perform retrieval using only the text part of the query while ignoring the sketch component. We choose the following three modern approaches in this category: VisualBERT (Li et al. 2020a), ViLT (Kim, Son, and Kim 2021), and CLIP (Radford et al. 2021).\nComposite Query-based Image Retrieval: These baselines perform retrieval using the sketch and text inputs. We compare our proposed method, STNET, with the following baseline methods: TIRG (Vo et al. 2019) and Task-Former (Sangkloy et al. 2022), and a two-stage model. We trained the TIRG model from scratch using our dataset. For Taskformer, we finetuned the publicly available checkpoint using our dataset and our reproduced code for training. We adhered to the hyperparameter configurations outlined in their respective papers for these models. For the two-stage method, in the first stage, we use a ViT trained for sketch classification to get an object name from the sketch. Next, in the second stage, we obtain the full-text query by inserting the predicted object name into the incomplete text and then using pretrained CLIP for image retrieval.\nFinally, we experimented with another baseline, \"two-stage (desc)", "two-stage\". In the second stage, rather than using the class name, we obtain the full-text query by inserting the description of the predicted object into the incomplete text and then using a pretrained CLIP model for image retrieval. The description for each of the 258 object names is chosen randomly from seven different sets of descriptions annotated\nper object name. Five of these object description sets are obtained automatically, while the other two are manually annotated.\nAutomated descriptions were generated by using ChatGPT-3.53 on Mar 14, 2023. We used the following five prompts to obtain five different description sets: (i) \"Describe the following words with visual descriptions in 4 to 10 words.\u201d (ii) \u201cDescribe the following words with visual descriptions as a 15-year-old kid in 4 to 10 words.": "iii) \u201cDescribe the following words with visual descriptions as a 35-year-old in 4 to 10 words.", "Describe the following words with visual descriptions as a 55-year-old in 4 to 10 words.\" (v) \"Describe the following words with visual descriptions as a non-native English speaker in 4 to 10 words.\" The human annotators were asked to write descriptions with 4 to 10 words that included visual attributes without mentioning the object's name.\nWe use two metrics: Recall@K and Median Rank (MdR). Recall@K is the percentage of times the ground truth image is retrieved within the top K results across all queries in the test set; the higher, the better. Median Rank is the median of the rank of ground truth image in the retrieved set across all queries in the test set; the lower, the better.\"\n    },\n    {\n      \"title\": \"Results on Test-1K and Test-5K\",\n      \"content\": \"Table 3 shows our main results on both test sets. Our proposed method, STNET, outperforms all baseline methods. Multiple sketch+text-based image retrieval models are better than text-based models, which are better than sketch-based image retrieval models. This is mainly because neither the sketches nor the incomplete text can answer the queries accurately. Amongst sketch-based image retrieval models, ViT-based Siamese networks perform the best. Among text-based image retrieval models, CLIP performs the best. STNET is better than the two-stage model (except for R@100) because the object name may not completely cover the semantics in the sketch and, even worse, may suffer from ambiguous object names (e.g., mouse, bat, star, etc.).\nThe two-stage model (desc) is expected to avoid some of the drawbacks of the two-stage model. However, descriptions of object names are often not natural (e.g., a description for \u201cgrass": "s \u201cgreen plant used for landscaping and grazing animals\u201d) and are still quite generic. Similarly, consider objects like boat, yacht, ship, and ferry. It is difficult to describe these in a differentiating manner but easy to sketch. Hence, both the two-stage model and STNET are better than the two-stage model (desc).\nConsidering the other sketch+text-based image retrieval models, TIRG (Vo et al. 2019) and Taskformer (Sangkloy et al. 2022), our proposed model STNET performs massively better. The poor performance of TIRG is because it does not use any pretraining for text. Also, the image pre-training in TIRG uses ResNet-17 (He et al. 2015) (trained on ImageNet dataset), which has been shown to lead to poorer image embeddings compared to CLIP (Radford et al. 2021)."}, {"title": "Ablation Study", "content": "Our overall STNET model consists of several components. To understand the importance of each component, we perform several ablations as shown in Table 4.\nWe start with just the contrastive loss (LCT) computed using sketch modality alone (Model 1). Model 2, which is trained with just LCT computed using only text modality, performs better. This broadly indicates that the information in text is higher than in sketch, which makes sense since our sketches are quite rough. Using text and sketch for contrastive loss computation (Model 3) leads to further improvements. Note that we do not perform dot-product attention between sketch and image in Model 1; rather, we employ contrastive learning between their encoders. Our full proposed model, STNET (Model 7), consists of all the loss functions: contrastive loss (LCT), object classification loss using text encodings (LCLS), object classification loss using image encodings (LCLS), sketch-guided object detection loss (LOD) and sketch reconstruction loss (LSR). Models 4, 5 and 6 are trained by removing classification (LCLS+LCLS) loss, object detection loss (LOD) and sketch reconstruction loss (LSR) respectively from the overall STNET model. Broadly, removing any of the three losses leads to degradation in performance across all metrics compared to the full STNET model (Model 7). The degradation worsens when the LCLS is removed (Model 4)."}, {"title": "Results on Open-Category Test Set", "content": "In a real-world scenario, the objects in queries may be uncommon or entirely unfamiliar. Considering that the Visual Genome focuses solely on common objects, we curate an Open-Category test set featuring 70 novel object categories under nine overarching classes. Among these, 50 are rare objects that are challenging to name but simple to illustrate, examples being Numbat, Mangosteen, Feijoa, Draw Knife, and Gibraltar Campion. These objects and their corresponding sketches are entirely unseen in the training set. The classes are mentioned in the Appendix. This set includes 750 composite queries and 1K gallery images.\nTable 5 showcases results for this experiment, comparing STNET to the top sketch-only (ViT-Siamese), text-only (CLIP), and sketch+text (Two-Stage) baselines. Although STNET is naturally extensible to novel object categories, the two-stage model requires a pre-defined universe of possible objects to select from. Hence, we first create a set of possible object categories for the two-stage model by augmenting the train set with the 70 additional test categories. ViT can't extend to new classes during testing, so we use zero-shot CLIP for sketch classification in the two-stage baseline. From Ta-"}, {"title": "Performance with Instance-Level Sketches", "content": "We have primarily focused on crude sketches. How does STNET fare with detailed instance-level sketches\u2014those with pose, size, and shape details? Such sketches require the retrieved image to have a matching object instance. For this experiment, we generate rich synthetic sketches automatically for each image in the Train and Test-1K datasets using\nthe method proposed in (Li et al. 2019). We obtain sketches only for the part of the image covered by the relevant object box. Table 6 shows that STNET outperforms all baselines on this complex setting as well. As the sketch becomes more expressive, the two-stage model, converting the sketch to a category name, loses nuanced details, widening its gap with STNET. More details are in the Appendix."}, {"title": "Qualitative Analysis", "content": "We show a few retrieval results from the CSTBIR dataset for our model STNET in Figure 4. Our model correctly retrieves the ground truth image associated with each composite query and ranks several relevant images in the top results. We observe that it can even reason about certain complex visual attributes associated with the queried object (e.g., \"okapi\" with striped legs). We provide more analyses in the appendix."}, {"title": "Conclusion", "content": "We proposed the novel problem of multimodal query-based retrieval on a collection of natural scene images where the query consists of an incomplete text and an accompanying rough sketch. Towards this task, we contributed a novel dataset, CSTBIR, containing ~2M queries and ~103K natural scene images. Further, we also proposed a novel model, STNET, which is trained on losses specially designed for the CSTBIR problem: contrastive loss, object classification loss, sketch-guided object detection loss, and sketch reconstruction loss. STNET outperforms existing strong baselines by significant margins. CSTBIR could be essential in multiple real-world settings. For example, searching for a product in digital catalogs given its rough sketch and a short description. It can also aid in the search for missing people, given their prominent features with accompanying descriptions from a repository of crowd photos taken by surveillance cameras."}]}