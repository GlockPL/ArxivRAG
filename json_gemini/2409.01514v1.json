{"title": "From Data to Insights: A Covariate Analysis of the IARPA BRIAR Dataset for Multimodal Biometric Recognition Algorithms at Altitude and Range", "authors": ["David S. Bolme", "Deniz Aykac", "Ryan Shivers", "Joel Brogan", "Nell Barber", "Bob Zhang", "Laura Davies", "David Cornett III"], "abstract": "This paper examines covariate effects on fused whole body biometrics performance in the IARPA BRIAR dataset, specifically focusing on UAV platforms, elevated positions, and distances up to 1000 meters. The dataset includes outdoor videos compared with indoor images and controlled gait recordings. Normalized raw fusion scores relate directly to predicted false accept rates (FAR), offering an intuitive means for interpreting model results. A linear model is developed to predict biometric algorithm scores, analyzing their performance to identify the most influential covariates on accuracy at altitude and range. Weather factors like temperature, wind speed, solar loading, and turbulence are also investigated in this analysis. The study found that resolution and camera distance best predicted accuracy and findings can guide future research and development efforts in long-range/elevated/UAV biometrics and support the creation of more reliable and robust systems for national security and other critical domains.", "sections": [{"title": "1. Introduction", "content": "This paper investigates covariate analysis for the IARPA BRIAR program, which has crucial implications for intelligence operations such as counterterrorism, infrastructure protection, military force defense, and border security. The BRIAR program aims to develop biometric systems that can overcome image quality challenges, accommodate a broad range of yaw and pitch angle viewpoints on individuals, and integrate information from multiple sources (face, body, and gait features) without relying solely on one modality. While Nalty et al. [1] provide a survey of relevant techniques, this paper concentrates on analyzing algorithm performance to identify factors influencing accuracy and inform future enhancements in critical domains.\nThis research examines five biometric matching algorithms from the BRIAR program. Although the internal operations of these systems remain proprietary, developers have shared some information through scientific publications or open-source software [1, 2, 3, 4, 5, 6, 7, 8, 9]. For this analysis, we treat these systems as black boxes, not exploring their specific algorithms. Our study aims to gain insight into their underlying performance by analyzing system outputs and applying statistical techniques."}, {"title": "2. Dataset Composition", "content": "The data used for this analysis was obtained using the techniques described in [10]. At the time of Cornett et al.'s publication they had collected data at two locations, but this analysis includes two additional collections, expanding the datasets used to four locations. The methods used for collecting this additional data are similar to those employed initially. The standardized and consistent nature of these collections provides a strong basis for developing a linear model to predict biometric recognition algorithm scores. The inclusion of multiple gait sequences enhances the datasets' usefulness, as gait is a challenging modality for biometric recognition systems, and its analysis can offer valuable insights into algorithm performance in this area.\nThe datasets include a wide range of biometric data collected in various settings across the US and at different times of the year to capture a broad spectrum of environmental and climatic conditions. These datasets consist of outdoor videos and are compared to indoor mugshot and controlled gait data. The four datasets used in this study represent approximately half of the total datasets planned to be collected by the program, providing a comprehensive and diverse sample for analysis. Each collection consists of approximately 450 individuals, offering a substantial sample size for analysis. About 200 individuals are utilized for training algorithms. For evaluation, 250 other individuals are used, while 100 are reserved for probes, which include the difficult outdoor and long-range data. An additional 150 individuals are included in the gallery consisting of only indoor data which boosts the size of the gallery.\nThe controlled indoor collections of face, body photos, and multiple gait sequences are used for enrollment in the gallery. This high-quality and consistent data is used for matching purposes. Each person in the gallery has facial images from five different angles, high-resolution whole-body photos from eight angles, and gait videos from a variety of perspectives and elevations. While there may be some cases with missing images or videos, the algorithms are given all available information to generate a single entry per person in the gallery database containing multiple biometric signatures and embeddings to support the modalities provided.\nThe outdoor data collections, used for probes, capture subjects participating in various activities to focus on the inherent biometric signatures of individuals. These data collections take place in a 10-meter square box equipped with multiple camera systems and allow for a comprehensive capture of subjects' appearances. Each probe is a 5- to 15-second video, typically capturing a portion of an activity. The subjects are instructed to perform a range of activities within the box, including standing, walking, using a cellphone, moving boxes, and other daily actions. Capturing subjects' movements and behaviors in an outdoor environment provides a wealth of information for analyzing covariates and their impact on biometric recognition performance."}, {"title": "2.1. Experimental Protocol Composition", "content": "The BRIAR dataset is still expanding and the analysis presented here is based on the BRIAR experimental protocol version 4.2.1, which includes 9,215 clips featuring 371 subjects. To conduct the analysis, 5- to 15-second video clips are extracted from the captured activities and matched to a gallery of indoor data collected in controlled environments. To account for larger searches and enhance the accuracy of the analysis, the gallery is supplemented with an additional 487 subjects, providing a more substantial basis for the evaluation of algorithm performance. The gallery type used in the analysis is called \"simple\" which consists of face and whole body images and videos of subjects walking in a predefined straight path. By comparing the extracted video clips to the indoor gallery, the analysis will reveal valuable insights into the impact of covariates on biometric recognition performance, particularly in relation to gait and behavioral patterns. This comprehensive and systematic approach will pave the way for a more robust and reliable development of biometric recognition systems, particularly in complex and dynamic environments."}, {"title": "3. Score Normalization", "content": "Normalization is a critical step in preparing the data for the analysis of covariates in the IARPA BRIAR dataset. The purpose of normalization is two-fold. Firstly, it allows us to transform the scores generated by the five experimental biometric recognition algorithms into a common scale, making them comparable and ensuring that they are in the same compatible score space. This is essential for a fair and accurate analysis of algorithm performance, as the raw scores may vary significantly from one algorithm to another due to different underlying processing mechanisms.\nSecondly, the normalization scheme also provides valuable insights and intuition into interpreting the results of the linear model. The model will predict the effect of the covariates on the genuine distribution of the biometric match scores, while it is assumed that the impostor distribution is essentially stable. Because these covariates relate to quality changes in the biometric data, they will shift the match distribution in one direction or the other. It is easiest to interpret this as a shift in the receiver operating characteristic (ROC) curve: If the quality of the image goes down, the ROC curve will also shift down.\nSince a given score determines a single point on that ROC curve, the model will essentially be predicting how the ROC curve shifts as covariate values change. Because the statistical models used will be predicting the \"expected genuine score\" for a given set of covariates, we can interpret model results as predicting a single point of the resulting ROC curve. As the statistical model is predicting the center point of the genuine distribution, the predicted (True Accept Rate) TAR will be 50% because approximately half of the distribution should fall above and below that predicted score. Therefore, the TAR is essentially fixed.\nDetermining the false accept rate (FAR) is where normalization comes in. The normalization selected converts the scores into the log of the estimated FAR (log10(FAR)) so that the predicted score directly correlates to FAR and can be used to interpret the effects on the ROC curve.\nNormalization involves a series of steps aimed at transforming the raw scores generated by the biometric recognition algorithms into a standardized format. The scores are typically associated with verification results and, in particular, the expected FAR and TAR values. These metrics are computed for the verification performance metric over the evaluation dataset.\nThe normalization process focuses on the estimates of FAR in the tail of the impostor distribution. The FAR represents the probability of the system incorrectly accepting an impostor as a genuine person. To standardize the FAR values we fit the following linear transformation, where m and b are least squared line fits computed for each algorithm at five estimate of FAR 10-6, 10-5, 10-4, 10-3, and 10-2. This region of the impostor distribution (the red distributions in Figure 2) represents 1% of the impostor scores where the PDF approximately follows a exponential curve and can therefore be represented following model.\n$\\log_{10}(FAR) = m \\times score + b$ (1)\nThe normalization process is primarily focused on the FAR values in the tail of the impostor distribution, specifically the region where FAR < 10-2. This region is particularly important for several reasons. Firstly, it is in this range that most operational applications are interested in, as they typically require very small FAR values. Secondly, in this range, the relationship between log10(FAR) and the raw scores from the algorithm is approximately linear, as demonstrated in Figure 2.\nThis linear relationship is significant because it enables us to analyze and predict the expected genuine scores for a given set of covariate values by applying this normalization. It should be noted, however, that scores larger than 10-2 will no longer have a linear fit (although this region is not of interest), and scores below 10-6 will be extrapolated, potentially resulting in less accurate predictions for very small FAR values."}, {"title": "4. Correlated Variables and Interactions", "content": "Before investigating the effects of other covariates, it is crucial to examine the specific influences of the sensor model and collection location on the algorithm's performance. The data collection procedures and environmental factors as part of the datasets may introduce several quality issues and interactions that need to be taken into account when analyzing covariates.\nMany of the variables and metadata collected are correlated, which makes the analysis more challenging. To ensure the validity and reliability of the findings, it is essential to account for confounding variables. This will help in understanding the true impact of the sensor model and collection location on the algorithm's performance while considering the potential influence of other factors. By doing so, a more comprehensive assessment of the algorithm's performance can be obtained.\nThe selection and configuration of sensors play a crucial role in determining accuracy. As illustrated in Figure 3, a simple model demonstrates how the sensor's design interacts with distance. Specifically, the size and configuration of a sensor's optics greatly influence the distance at which it can effectively operate, leading to a strong correlation with distance. Additionally, there is a wide variation in accuracy, which is primarily due to differences in sensor quality or how the sensor is configured.\nThe value of analyzing sensor models cannot be overstated when it comes to organizations aiming to develop new camera systems. Careful selection of cameras can have a significant impact on overall performance. Existing systems can be updated or reconfigured to enhance biometric performance. When procuring new systems, it is important to note that the specific models tested here may no longer be available as these products are constantly being updated and improved. However, accurately modeling other covariates necessitates a meticulous process of isolating and understanding the correlation and quality variations in sensors. By thoroughly examining these factors, organizations can optimize their systems and achieve superior performance.\nThe dataset includes a significant influencing factor in the form of the location where the test data was gathered. Data is collected at various sites and throughout different times of the year, with each collection correlated with season, weather, climate, geography, and local demographics. At each site, cameras are repositioned to suit the collection location, resulting in a correlation with camera distance and configuration. Each collection site also imposes unique constraints on the types of UAVs that can be flown and their operational parameters.\nConsequently, the collection location must also be taken into account when assessing algorithm performance. Unlike camera selection and configuration, the collection location, weather, and other factors cannot be easily controlled. Thus, operational systems must be capable of functioning in all conditions.\nTo effectively model algorithm performance, it is crucial to consider the measured weather data and other general parameters while minimizing the impact of the specific location and time of year where the data was collected. By doing so, more accurate and reliable predictions can be made.\nWe recognize that each challenge presented by the"}, {"title": "5. Linear Model of Results", "content": "To model the performance of the biometric recognition algorithms, we used a linear mixed model approach. This approach allows us to represent the relationship between the covariates and the average genuine acceptance score (in this case, the normalized score) in a mathematically straightforward and interpretable manner. By employing linear mixed models, we can account for the potential correlations between repeated measures of the camera setups and locations, ensuring that our analysis accurately reflects the variability and uncertainty in the data.\nThis modeling approach at a fundamental level is very simple in that it predicts the average genuine score for a specified set of covariates. Unlike conventional biometric algorithm evaluations which produce an ROC or DET curve, this approach, which may initially seem unconventional, predicts a simple score which is representative of the genuine scores rather than determining a TAR or FAR.\nTo interpret the model, it is assumed that the TAR is kept constant at 50% as discussed earlier, while the FAR is allowed to vary. However, it is possible to easily estimate the FAR due to the score normalization that has been applied. This approach enables a clear understanding of the model's performance in predicting genuine scores and how it may impact the FAR. As will be seen in the following discussion, while holding the TAR rate fixed at 50%, the model predicts a broad range of selectivity with a base FAR of 1 in 10,000,000 under ideal conditions. When adding challenging conditions like very long distances, low resolution, non-ideal weather, the associated FAR can increase by many orders of magnitude.\nThe analysis in this work is significantly influenced by the study conducted by Beveridge et al. [11] which applied GLMM to model covariate interactions for face recognition algorithms. Their approach, however, was different in several aspects. Primarily, their focus was on predicting TAR using logistic regression, whereas our model predicts transformed scores, which directly correlate with algorithm performance. Although TAR is commonly used to assess algorithm performance, the transformation we used in our work is linear, offering an intuitive understanding of the model coefficients.\nIn comparison to the Beveridge model, our approach is simpler, without the need to explore significant interactions and employ model selection techniques to identify the most crucial factors affecting the performance. Although interactions may indeed play a significant role in this research, we have opted for a simpler model. The Beveridge work, although more complex, does provide simple graphics to show the impact of covariate combinations. However, our model can be interpreted by adding the model coefficients.\nTo keep the models simpler, we converted all covariates into a small number of categorical values. This process"}, {"title": "6. Discussion", "content": "As mentioned in Section 4, one of the main factors driving performance is the choice of sensor model/camera which encapsulates the hardware, optics, and software and to some extent the camera configuration. For example, in the data collection the cameras are always setup to capture images with the best possible visual quality and with the highest quality compression to maintain as much detail and accuracy is retained from the original video. However, due to the variety of cameras and complexity of modeling the whole camera performance, this selection is represented in our model as a fixed effect and therefore is not shown in the final table of coefficients.\nTwo related covariates that are two of the strongest predictors of accuracy are head height in pixels, and modality. It should be mentioned that these likely are correlated since modality relates to how the zoom level of the camera is set. For \"face\" configuration, the camera is zoomed-in to focus on the upper body to capture more fine details. For \"body\", the camera is zoomed out to capture the full length of the body as well as a full 10-meter long walking area to support gait recognition. The model results show that the \"face\" configuration is strongly preferred which will typically be associated with head heights with more pixels.\nThe head height is certainly one of the strongest predictors of accuracy and has a very straight forward interpretation: more pixels on target makes recognition easier. This covariate also has a special and very difficult category which is \"face restricted\" where the face is either too small or the subject is not facing the camera, in which case the systems cannot rely on the face recognition modality.\nThe camera location covariate is probably of the most interest for the research program. On the ground, it would seem there is only a minimal difference between \"ctrl\" (close-range) and \u201cshort-range\" locations meaning that with a good setup and recognition systems in place, recognition at extended distances is possible. At \u201cmedium\u201d (250m550m) to \"long\" (> 550m 1000m) range the problem gets much more difficult with FAR increasing by almost two orders of magnitude.\nThe elevated locations show a similar story with only a minor difference between the close-range cameras at ground level, \"ctrl\", and the \"elevated\" cameras on masts, which are all colocated with the \"ctrl\" cameras on the group. \u201cUAV\u201d mounted cameras exhibit significant variability. This likely results from the difference between small, modern HD and 4k quadcopters that typically fly at low altitude and within 100m of the subjects and larger systems that fly at longer distances and up to 1200 ft altitude, often with older or lower resolution payloads. In this model, all of these are grouped into the same category.\nWeather results are also interesting with most weather related effects showing only modest impact on accuracy. Solar loading had the most effect with \"> 900\" increasing the FAR by almost an order of magnitude. This should correspond to direct sunlight conditions that have been known to cause issues with outdoor imaging when compared to the smooth lighting available from overcast skies.\nTurbulence and its mitigation represent a significant challenge in addressing the long-range biometrics problem and is an important research topic for the program. Despite"}, {"title": "7. Future Work", "content": "As this is the first paper that looks at the effects of covariates on fused whole body recognition problem, there is still plenty of interesting topics that can be explored in the future. The BRIAR datasets used in this analysis represent a complex set of covariates and there are many topics that have not yet been explored including demographics, clothing features, yaw and pitch angles, video compression, etc.\nAlso of interest is the algorithm modality. In this study, we only look at the fusion of all features that these systems extract from the video of the subjects; however, internally the systems are composed of face, body, and gait specific components. While numerous studies have been conducted on face performance, the body and gait based features are relatively new and may yield interesting differences between biometric modalities.\nFinally, the model presented here has been intentionally kept simple. A small number of covariates were examined since they were known to influence performance and because they are of high interest to the research program as a whole. The simplicity of the model allows for easy interpretation, however there are some valuable additions that could be adopted from the work in [11]. First, model selection was used to reduce the model to a minimum number of covariates that effect performance, and secondly, that model explored covariate interactions. While these changes may result in a more complex analysis, it may also allow us to include more covariates and better understand how these variables influence system performance."}, {"title": "8. Conclusions", "content": "In this study, we have investigated the relationship between various covariates and the performance of fused whole body biometric recognition algorithms, specifically focusing on biometric recognition at altitudes, ranges, and elevated camera positions in complex and dynamic environments. By analyzing sensor models and collection locations as fixed effects, we account for these factors as confounders. The analysis provides insights for operational systems facing a wide range of challenges, including close-range and long-range recognition, elevated cameras, weather conditions, and other factors that can impact algorithm performance. Understanding these factors is crucial for the advancement of cutting-edge biometric technologies and supporting the development of more robust and reliable detection methods in challenging environments.\nBy taking a systematic approach with this dataset, we have gained significant insights into the challenges and interactions that must be considered when designing new systems to function in complex and dynamic environments. This comprehensive analysis will serve as a foundation for further understanding of biometric recognition system performance in various conditions and provide valuable insights guiding future research and development efforts related to identifying people in this complex scenario."}]}