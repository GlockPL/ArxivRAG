{"title": "OpenFGL: A Comprehensive Benchmarks for Federated Graph Learning", "authors": ["Xunkai Li", "Yinlin Zhu", "Boyang Pang", "Guochen Yan", "Yeyu Yan", "Zening Li", "Zhengyu Wu", "Wentao Zhang", "Rong-Hua Li", "Guoren Wang"], "abstract": "Federated graph learning (FGL) has emerged as a promising distributed training paradigm for graph neural networks across multiple local systems without direct data sharing. This approach is particularly beneficial in privacy-sensitive scenarios and offers a new perspective on addressing scalability challenges in large-scale graph learning. Despite the proliferation of FGL, the diverse motivations from practical applications, spanning various research backgrounds and experimental settings, pose a significant challenge to fair evaluation. To fill this gap, we propose OpenFGL, a unified benchmark designed for the primary FGL scenarios: Graph-FL and Subgraph-FL. Specifically, OpenFGL includes 38 graph datasets from 16 application domains, 8 federated data simulation strategies that emphasize graph properties, and 5 graph-based downstream tasks. Additionally, it offers 18 recently proposed SOTA FGL algorithms through a user-friendly API, enabling a thorough comparison and comprehensive evaluation of their effectiveness, robustness, and efficiency. Empirical results demonstrate the ability of FGL while also revealing its potential limitations, offering valuable insights for future exploration in this thriving field.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, graphs have emerged as effective tools for capturing intricate interactions among real-world entities, leading to their widespread applications. Based on this, we can translate various business applications from industrial scenarios into different graph-based downstream tasks from the machine learning perspective. To generate effective graph entity embeddings for downstream tasks, graph neural network (GNN) is a promising technology, which simultaneously encodes node features and structural insights, garnering significant attention. This paradigm has been widely validated, including node-level financial fraud detection [34, 66], link-level recommendation [46, 90], and graph-level bioinformatics [21, 67].\nDespite their effectiveness, privacy regulations, growing business competition, and scalability issues in billion-level graph learning have increased concerns around direct data sharing, posing challenges for centralized data storage and model training in a single machine. To address this issue, federated graph learning (FGL) is proposed, which aims to facilitate collaborative GNN training among multiple local systems. Existing FGL benchmarks, such as FS-G [82] (Year: 2022) and FedGraphNN [29] (Year: 2021), offer valuable insights but still have the following limitations: (1) Datasets: Limited to few application domains (e.g., citation networks and recommendation systems). (2) Algorithms: Missing recent SOTA FGL methods (e.g., 8 methods in 2023, 10+ methods in 2024). (3) Experiments: Lack of federated data simulation strategies that account for graph properties, inadequate support for various graph-based downstream tasks, and limited evaluation perspectives. While the research prospects and enthusiasm for FGL are prominent and growing, the absence of a comprehensive benchmark for fair comparison impedes its development. Specifically, the diversity of graph-based downstream tasks (i.e., node, link, and graph), the unique graph characteristics (i.e., feature, label, and topology), and the complexity of FGL evaluation (i.e., effectiveness, robustness, and efficiency) collectively pose significant obstacles to achieving a comprehensive understanding of the current FGL landscape. Consequently, there is an emergency need to develop a standardized benchmark."}, {"title": "2 PROBLEM STATEMENT", "content": "In this section, we briefly review the FGL training pipeline in the following 2 most representative scenarios. To begin with, from a data perspective, each client regards graphs (Graph-FL) and nodes in a subgraph (Subgraph-FL) as data samples. Subsequently, FGL aims to achieve collaborative training based on these clients and a trusted server. Formally, we consider a FGL system consisting of K clients, where the k-th client manages a private dataset denoted as $D^{(k)} = \\{G_i^{(k)}\\}_{i=1}^{N_T}$. Here, $N_T$ is the task-specific description, where $N_T$ denotes the number of graph samples under Graph-FL, while $N_T = 1$ exists under Subgraph-FL. To provide a detailed description, we take FedAvg [59] as an example. Its training process within the T communication round is outlined in four key steps:\n1. Receive Message. Each client initializes its local model with the one received from the server $w_k \\leftarrow \\hat{w}^T$;\n2. Local Update. Each client performs local training using its private data, i.e., $\\min_w L_{task}(D^{(k)})$ to obtain $W_k^{T+1}$, where $L_{task}$ denotes the task-specific optimization objectives.\n3. Upload Message. Each client uploads their local updated models $W_k^{T+1}$ and the number of data samples $D_i$ (i.e., graphs, nodes, or edges, depending on the downstream task) to the server.\n4. Global Aggregation. The server aggregates the updated models to obtain $W^{T+1}$ for the next communication, i.e., $\\hat{W}^{T+1} \\leftarrow \\frac{1}{D}\\sum_{k=1}^K |D_k| W_k^{T+1}$, where D is the total number of data samples."}, {"title": "3 BENCHMARK DESIGN", "content": "In this section, we offer detailed descriptions of OpenFGL, with an overview provided in Table 2. Specifically, we present a presentation from the perspectives of FGL application scenarios (Sec. 3.1), algorithms (Sec. 3.2), and experimental evaluations (Sec. 3.3)."}, {"title": "3.1 Data-level FGL Scenarios", "content": "In OpenFGL, we focus on the following two most prevalent FGL scenarios: (1) Graph-FL. The growing integration with graph-based techniques and AI4Science applications, such as drug discovery, has motivated this scenario, in which clients consider graphs as the data samples and pursue collaborative training between clients to acquire powerful models while preserving data privacy. (2) Subgraph-FL. Realistic applications in this scenario include node-level fraud detection for financial security and link-level user-item interactions for recommendation, with data stored in a distributed manner. Clients treat their data as subgraphs of a larger and more comprehensive global graph, and focus on utilizing nodes and edges as data samples for training. Due to regulatory constraints and the data-sensitive nature, clients seek a collaborative training scheme to develop well-trained models without direct data sharing.\nNotably, Node-FL is also a significant paradigm of FGL, which is widely used in graph-based spatial-temporal analysis, such as sensor networks [71] and traffic flow prediction [95], where nodes are only aware of their local context within the broader network. Although Node-FL has been widely mentioned, we have not integrated it into OpenFGL. This is because most Node-FL studies are tailored to specific scenarios and involve experimental setups that are highly diverse and closely aligned with particular application contexts. These characteristics make Node-FL less suitable for being included in a unified benchmark evaluation, where consistency across scenarios is essential for comprehensive comparisons.\nDatasets. To comprehensively evaluate existing FGL algorithms in the above two real-world application-driven scenarios, we have compiled a substantial collection of public datasets from various domains. Specifically, Regarding Graph-FL scenario, we conduct experiments on the compounds networks (MUTAG, BZR, COX2, DHFR, PTC-MR, AIDS, NCI1, hERG) [13, 22, 30, 68, 73, 77], protein networks (ENZYMES, DD, PROTEINS) [7, 15, 30], collaboration network (COLLAB) [40], movie network (IMDB-B/M) [89], super-pixel networks (MNISTSuperPixels) [61], and point cloud networks (ShapeNet) [94]. As for Subgraph-FL scenario, we perform experiments on the citation networks (Cora, Citeseer, PubMed,"}, {"title": "Simulation Strategies", "content": "In response to policy constraints on acquiring distributed graphs, we draw inspiration from federated learning in computer vision to simulate generalized federated scenarios by partitioning existing datasets into distributed subsets. This strategy is similar to recent FGL benchmarks [29, 82].\nIn our proposed OpenFGL, we integrate 8 federated data simulation strategies driven by practical applications, in which we ensure the graph data distributed to each client exhibits similar patterns in feature, label, or topology while maintaining a controllable heterogeneity across clients. Specifically, for both Graph-FL and Subgraph-FL, we implement 3 simulation strategies widely used in graph-independent FL without graph-oriented designs, including Feature Distribution Skew, Label Distribution Skew, and Cross-Domain Data Skew. In the context of Graph-FL, we introduce a topology-oriented simulation strategy called Topology Shift, which distributes graphs based on degree distribution. The inspiration for this approach stems from key insights offered by recent FGL studies [74, 85]: in Graph-FL, the structure Non-iid resulting from topology shift is the primary challenge in collaborative optimization, where node features and labels appear less significant by comparison. As for Subgraph-FL, existing federated data simulation strategies predominantly utilize prevalent community detection algorithms such as Louvain [6] and Metis [36] to identify clusters with dense intra-community connections. Then, these clusters' nodes are subsequently allocated to local clients to construct corresponding induced subgraphs for partitioning. These strategies all operate under a common assumption: that the private data collected by each local agent in the real world contains dense internal connections but is loosely connected across clients. Despite their effectiveness, limitations persist in their application. This is because Subgraph-FL primarily focuses on node-level and link-level downstream tasks, where node profiles (i.e., node features and labels) are crucial. However, these strategies do not consider label distribution for federated data simulation. Hence, we introduce Metis-based Label Imbalance Split and Louvain-based Label Imbalance Split. These methods, refining the aforementioned strategies, carefully consider label distribution during cluster allocation to better simulate realistic and generalized distributed scenarios."}, {"title": "3.2 Method-level FGL Algorithms", "content": "GNN Backbones. To provide a broader spectrum of graph learning paradigms on the client side, OpenFGL integrates a diverse range of local GNN backbones. Specifically, we implement various well-designed polling strategies (TopKPooling [19], SAGPooling [39], EdgePooling [14], and PANPooling [58]) based on the most representative GIN [87] with weight-free MeanPooling [87] for Graph-FL. As for Subgraph-FL, OpenFGL includes prevalent GCN [38], GAT [76], GraphSAGE [27], SGC [84] and GCNII [10]. The detailed descriptions of these backbones can be found in [1] (A.2).\nFL/FGL Algorithms. To achieve federated training, multi-client collaboration algorithms based on a trustworthy server are also crucial. Therefore, we follow the historical progression of FL to include a spectrum of algorithms from the most representative methods in CV to FGL: (1) CV-based FL algorithms: FedAvg [59], FedProx [45], Scaffold [35], MOON [44], FedDC [20], FedProto [75], FedNH [11] and FedTGP [96]; (2) FGL algorithms: GCFL+ [85] and FedStar [74] in Graph-FL and FedSage+ [99], Fed-PUB [5], FedGTA [49], FGSSL [33], FedGL [8], AdaFGL [48], FGGP [78], FedDEP [97], and FedTAD [103] in Subgraph-FL. More details about these algorithms can be found in [1] (A.2). Notably, these algorithms are implemented with a unified API to facilitate future development in a user-friendly manner. For more details about our API design from the algorithm perspective, please refer to Sec. 4.5."}, {"title": "3.3 Experiment-level FGL Evaluations", "content": "Data Analysis. To delve into the data heterogeneity of FGL and offer essential insights for future studies, we advocate further analyzing the distributed graph data, encompassing the following aspects: (1) Feature KL Divergence: It reveals feature skew among clients while the label domain remains consistent, which may arise due to the different geographical locations of clients, such as the characteristics of a certain disease may significantly differ across various regions. (2) Label Distribution: As a more generalized data heterogeneity in CV-based FL, it is extensively discussed. However, in graph-based scenarios, the relationship between labels and topology frequently reveals underlying connections, characterized as homophily. Thus, we further integrate multi-level homophily metrics [53, 54, 63-65] to offer comprehensive analysis. (3) Topology Statics: This perspective stems from the critical role of topology in GNNs, especially in distributed scenarios. This arises from the significant impact of diverse local topology statistics on the local model, causing complex cascading effects in collaboration. Therefore, we examine topological differences among clients (e.g., Degree, Centrality, Largest Component) to provide insights.\nEffectiveness. Details of our evaluation metrics are as follows: graph/node classification (Accuracy, F1, Recall, Precision), graph regression (MSE, RMSE), link prediction (AP, AUC-ROC), and node clustering (Clustering-accuracy, NMI, ARI). More detailed descriptions of these metrics are presented in [1] (A.3).\nRobustness. To evaluate the practical deployment of FGL in real-world scenarios, we examine its robustness from the following perspectives: (1) Noise: This corresponds to data quality issues resulting from data collection. (2) Sparsity: This reflects scenarios with incomplete features, labels, and topology due to data scarcity and high labor costs, and with a low rate of client participation. (3) Client Communication: This simulates scenarios with network constraints or high communication costs. (4) Generalization: This evaluates generalizability of algorithms in intricate applications. (5) Privacy Preserve: This reflects the applicability of algorithms in privacy-sensitive scenarios, thereby we conduct an in-depth analysis from the perspective of Differential Privacy (DP). Please refer to [1] (A.4) for further details on the robustness settings.\nEfficiency. To facilitate FGL deployment, we conduct an evaluation of current baselines regarding their efficiency. Specifically, we evaluate them from both theoretical (algorithm complexity) and experimental (communication cost and running time) perspectives."}, {"title": "4 EXPERIMENTS AND ANALYSIS", "content": "In this section, we systematically investigate FGL algorithms by answering the following questions: (1) For Effectiveness, Q1: What advantages does federated collaboration offer compared to training solely on local data? Q2: How do FGL algorithms and federated implementations of GNNs perform in two prevalent FGL scenarios? (2) For Robustness, Q3: How do FGL algorithms perform under local noise and sparsity settings (i.e., features, labels, and edges)? Q4: What are the performance of FGL algorithms under low client participation rates in communication? Q5: Can FGL algorithms maintain generalization across various graph-specific distributed scenarios? Q6: Do FGL algorithms support additional DP privacy protection? (3) For Efficiency, Q7: What are the theoretical algorithm complexity of FGL algorithms? Q8: What is the practical running efficiency of FGL algorithms? To maximize the usage for the constraint space, more results are shown in [1] (A.5-A.7).\n4.1 Performance Comparison\nTo answer Q1, in addition to the federated multi-client collaboration, we introduce \"Local\" to represent solely local training for analyzing the advantages and potential limitations of FGL. Based on this, to answer Q2, we present the end-to-end performance in Table 6 and Table 7. The detailed analysis is presented as follows:\nGraph-FL Scenario. Since the FGL research is still immature, the baselines for Graph-FL are scarce (only GCFL+ and FedStar). Therefore, we expand the number of local pooling-based backbones and CV-based FL algorithms, providing comprehensive results in Table 6. For Q1, we find that the benefits of federated collaboration are not significant for small-scale MUTAG, BZR, and COX2 due to their scarce data samples that can not support federated training and thus affect predictions. Subsequently, we conclude that C1: Federated collaboration is more advantageous for larger-scale datasets, benefiting from abundant data sources. As for Q2, we observe that: (1) GCFL+ and FedStar concentrate on topology Non-iid within the cross-domain simulation, thereby not consistently achieving competitive performance in this single-source setting. Therefore, they perform less favorably than FL algorithms on ENZYMES, COLLAB, and MULTI. (2) Existing FGL algorithms heavily rely on node semantics. We observe significant improvements on datasets with abundant node descriptions like DD and PROTEINS, whereas limited performance on BINARY whose node representations are initialized with node degrees. Consequently, we conclude that C2: Graph-FL algorithms still have improvement space, especially in single-source domain and constrained data semantics.\nSubgraph-FL Scenario. Compared to Graph-FL, Subgraph-FL has significantly advanced and offers numerous SOTA baselines. Their results are shown in Table 7. For Q1, although Subgraph-FL can benefit from abundant data samples, for Chameleon, Actor, and Ratings, the improvement from both FL and FGL algorithms is limited or even worse than solely local training in some cases. We attribute this to the well-known heterophily challenges, where differences in node connection rules across clients significantly affect local updates and server-side collaboration, deviating from the global optimum and resulting in sub-optimal predictive performance. Although AdaFGL addresses this issue through personalized technology, there is still room for better performance. Based on this and C1, we conclude that C3: The prerequisite for positive impacts of federated collaboration is the uniform distribution of node features, labels, and topology. Regarding Q2, we observe that: (1) Subgraph-FL is thriving, with numerous baselines vigorously competing for the best performance. Among them, FedTAD and AdaFGL stand out in most cases. (2) The outstanding performance of existing algorithms stems from the fine-grained exploration of the topology, but some methods lack scalability when dealing with large-scale ogbn-products, resulting in OOM (out-of-memory) errors. Consequently, we conclude that C4: Subgraph-FL algorithms should address the complexity of real-world deployment, especially focusing on large-scale scenarios and graph-specific federated heterogeneity challenges."}, {"title": "4.2 Robustness Analysis", "content": "Local Noise. To answer Q3 from the perspective of noise, the experimental results are shown in the upper part of Fig. 1(a)-(c). For feature noise, we randomly select the channels of node features and inject Gaussian noise. To simulate edge noise in the Graph-FL without node labels, we utilize Metattack [104] to add noise edges, significantly perturbing the training gradients. As for label noise, we randomly reassign non-real labels to a certain proportion of training node samples. Based on the experimental results, we observe that FGL algorithms are highly sensitive to edge noise compared to topology-agnostic FL algorithms. This inherent limitation directly disrupts the model optimization of GCFL+ and FedStar, misleading local updates and topology-driven collaboration, thereby resulting in sub-optimal predictive performance. However, GCFL+ and FedStar demonstrate superior robustness under feature and label noise, as they address client interference with each other using server-side clustering customization and additional local models maintained at client-side. Consequently, we can conclude that C5: Noise scenarios determine the performance lower bound for FGL algorithms, where personalized strategies emerge as crucial technologies. However, they fall slightly short in addressing edge noise.\nLocal Sparsity. To answer Q3 from the perspective of sparsity, the experimental results are shown in the lower part of Fig. 1(a)-(c). Regarding feature sparsity, we simulate partial feature absence for unlabeled nodes and randomly remove the original edges. As for label sparsity, we change the ratio of training nodes. Under feature"}, {"title": "5 CONCLUSION AND FUTURE DIRECTIONS", "content": "In this paper, we first present a comprehensive overview of the current research progress in the FGL field and the significant potential of this technology for deployment in real-world graph-based database applications. Subsequently, we propose OpenFGL, a unified and comprehensive FGL benchmark, which encompasses 18 recently proposed SOTA FGL algorithms (Year: 2021-2024) and 38 datasets from 16 application domains for 5 graph-based downstream tasks across 2 prevalent FGL scenarios in the real world. The goal of our work is to fairly examine the current state of FGL development and offer key insights for future research endeavors. Leveraging OpenFGL, we conduct extensive experiments aimed at unveiling the performance of FGL algorithms from 3 orthogonal perspectives: effectiveness, robustness, and efficiency. Our investigations reveal promising advancements achieved by FGL studies but also highlight limitations in real-world deployments, such as vulnerability in inadequate node descriptions, robustness, and scalability. To inspire future research, we combine experimental Conclusions to present the following significant challenges and promising directions.\nFor Effectiveness, (1) Quantify Distributed Graphs (C1 and C3). Essentially, the potential benefits of FGL in real-world deployments stem from the uniform graph distribution. However, the entanglement of node features, labels, and topology poses a challenge in explicitly quantifying statistics within distributed graphs, resulting in coarse descriptions. This constraint sharply contrasts with the intuitive semantic feature and label distribution skew observed in CV-based FL. Therefore, quantifying the statistics of multi-source graphs is crucial. (2) FGL Heterogeneity (C2 and C4). Although some FGL studies attempt to define graph-based heterogeneity challenges, these definitions are often insufficient due to the complexity of graph characteristics and the diversity of applications. Furthermore, the advantages of these FGL algorithms (e.g., AdaFGL) lack significant impact. Consequently, there is still necessary effort to be made in addressing FGL heterogeneity.\nFor Robustness, (3) Personalized FGL (C5 and C7). In real-world scenarios, the robustness of FGL against client-specific noise and low client participation in communication is essential. During federated training, these factors significantly impact the attainment of global consensus, thereby hindering high-quality supervision provided for local training. Fortunately, personalized techniques can leverage local reliable knowledge to establish unbiased global consensus and customize broadcasts for each client, guiding local updates. This mitigates optimization challenges arising from noise data and limited data sources. Consequently, personalized FGL algorithms emerge as an effective strategy for addressing these concerns. (4) Multi-client Collaboration FGL (C6 and C8). The challenges related to sparsity and generalization often arise from expensive manual annotation and various application scenarios. During our investigation, we found that promoting server-side multi-client collaboration can extract global insights from sparse data for local training (e.g., federated knowledge-driven subgraph generation and augmentation in FedSage+ and AdaFGL). Additionally, this collaborative approach can capture shared semantic knowledge across data domains to facilitate robust generalization. Therefore, there is an emergency demand for more efficient multi-client collaboration FGL algorithms. (5) Privacy-preserve FGL (C9). The goal of FGL is to achieve multi-client collaborative training in a privacy-preserving manner without direct data sharing. However, current FGL algorithms, in pursuit of superior performance, increasingly share local information, raising potential concerns. Meanwhile, they are sensitive to additional DP-based privacy preservation. Therefore, the development of FGL algorithms with strict privacy requirements is imperative. Meanwhile, more privacy-preserving technologies (e.g., homomorphic encryption and secure multi-party computation) should be considered for further explorations.\nFor Efficiency, (6) Decoupled and Scalable FGL (C10). Existing FGL algorithms face challenges in practical deployment due to communication delay and topology mining, making it difficult to handle large-scale datasets. Therefore, developing new federated collaboration paradigms such as decoupled mechanisms and focusing on algorithm design scalability is crucial.\nFGL should establish federated collaboration standards for various graph types (e.g., directed, signed, hypergraphs, heterogeneous) and learning paradigms (e.g., unsupervised, few-shot, continual, unlearning) based on the different downstream tasks. However, FGL remains a burgeoning field with numerous research gaps. Nevertheless, we are committed to continually enhancing OpenFGL to support future research endeavors. For instance, we are progressively refining the execution standards for federated heterogeneous graph learning. Notably, considering the space constraints and the need for a clear and reader-friendly presentation, we provide an overview and corresponding experimental results in [1] (A.9)."}]}