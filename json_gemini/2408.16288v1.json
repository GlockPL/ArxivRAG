{"title": "OpenFGL: A Comprehensive Benchmarks for Federated Graph Learning", "authors": ["Xunkai Li", "Yinlin Zhu", "Boyang Pang", "Guochen Yan", "Yeyu Yan", "Zening Li", "Zhengyu Wu", "Wentao Zhang", "Rong-Hua Li", "Guoren Wang"], "abstract": "Federated graph learning (FGL) has emerged as a promising distributed training paradigm for graph neural networks across multiple local systems without direct data sharing. This approach is particularly beneficial in privacy-sensitive scenarios and offers a new perspective on addressing scalability challenges in large-scale graph learning. Despite the proliferation of FGL, the diverse motivations from practical applications, spanning various research backgrounds and experimental settings, pose a significant challenge to fair evaluation. To fill this gap, we propose OpenFGL, a unified benchmark designed for the primary FGL scenarios: Graph-FL and Subgraph-FL. Specifically, OpenFGL includes 38 graph datasets from 16 application domains, 8 federated data simulation strategies that emphasize graph properties, and 5 graph-based downstream tasks. Additionally, it offers 18 recently proposed SOTA FGL algorithms through a user-friendly API, enabling a thorough comparison and comprehensive evaluation of their effectiveness, robustness, and efficiency. Empirical results demonstrate the ability of FGL while also revealing its potential limitations, offering valuable insights for future exploration in this thriving field.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, graphs have emerged as effective tools for capturing intricate interactions among real-world entities, leading to their widespread applications. Based on this, we can translate various business applications from industrial scenarios into different graph-based downstream tasks from the machine learning perspective. To generate effective graph entity embeddings for downstream tasks, graph neural network (GNN) is a promising technology, which simultaneously encodes node features and structural insights, garnering significant attention. This paradigm has been widely validated, including node-level financial fraud detection [34, 66], link-level recommendation [46, 90], and graph-level bioinformatics [21, 67].\nDespite their effectiveness, privacy regulations, growing business competition, and scalability issues in billion-level graph learning have increased concerns around direct data sharing, posing challenges for centralized data storage and model training in a single machine. To address this issue, federated graph learning (FGL) is proposed, which aims to facilitate collaborative GNN training among multiple local systems. Existing FGL benchmarks, such as FS-G [82] (Year: 2022) and FedGraphNN [29] (Year: 2021), offer valuable insights but still have the following limitations: (1) Datasets: Limited to few application domains (e.g., citation networks and recommendation systems). (2) Algorithms: Missing recent SOTA FGL methods (e.g., 8 methods in 2023, 10+ methods in 2024). (3) Experiments: Lack of federated data simulation strategies that account for graph properties, inadequate support for various graph-based downstream tasks, and limited evaluation perspectives. While the research prospects and enthusiasm for FGL are prominent and growing, the absence of a comprehensive benchmark for fair comparison impedes its development. Specifically, the diversity of graph-based downstream tasks (i.e., node, link, and graph), the unique graph characteristics (i.e., feature, label, and topology), and the complexity of FGL evaluation (i.e., effectiveness, robustness, and efficiency) collectively pose significant obstacles to achieving a comprehensive understanding of the current FGL landscape. Consequently, there is an emergency need to develop a standardized benchmark.\nIn this paper, we propose OpenFGL, which integrates 2 commonly used FGL scenarios, 38 datasets in 16 application domains, 8 graph-specific distributed data simulation strategies, 18 recently proposed SOTA algorithms, and 5 graph-based downstream tasks. These components are implemented with a unified API to facilitate fair comparisons and future development in a user-friendly manner. Based on this foundation, we provide a comprehensive evaluation of existing FGL algorithms, drawing the following valuable insights. For Effectiveness we advocate for quantifying the statistics in distributed graphs to define the graph-based federated heterogeneity formally. For Robustness, to facilitate the practical deployment of existing FGL algorithms, we emphasize the significant potential of personalized, multi-client collaboration, and privacy-preserving techniques in addressing challenges such as data noise, low client participation, data sparsity, and generalization in complex applications. For Efficiency, considering the industry-scale datasets, we encourage FGL developers to prioritize algorithmic scalability and propose innovative federated collaborative paradigms that bring substantial benefits in improving efficiency. To further illustrate the advantages of our proposed OpenFGL compared to existing FGL benchmarks, we provide a clear description in Table 1.\nOur contributions. (1) Comprehensive Benchmark. We propose OpenFGL, which integrates the 2 most prevalent FGL scenarios with 38 publicly available datasets covering 5 downstream tasks. Based on this foundation, we thoroughly examine the unique properties of graphs and propose 8 practical distributed settings from the perspective of data heterogeneity. Meanwhile, we integrate 18 SOTA FGL algorithms and advocate 3 orthogonal evaluation perspectives to establish comprehensive baselines and systematic investigation of FGL algorithms. (2) Valuable Insights. Leveraging the user-friendly API integrated into OpenFGL, we conducted extensive empirical studies and derived 10 valuable conclusions (See Sec. 4.1-4.4). Building upon this, we provide 6 key insights from the perspectives of effectiveness, robustness, and efficiency, outlining promising research directions for future FGL studies (See Sec. 5). (3) Open-sourced Library and Detailed Repository. We develop an easy-to-use and open-source library to support ongoing FGL studies, allowing users to evaluate their FGL algorithms or datasets with ease. Additionally, we conduct a comprehensive review of existing FGL studies and release a beginner-friendly repository to facilitate the growth of the FGL community. The code and related tutorial are available at https://github.com/xkLi-Allen/OpenFGL."}, {"title": "2 PROBLEM STATEMENT", "content": "In this section, we briefly review the FGL training pipeline in the following 2 most representative scenarios. To begin with, from a data perspective, each client regards graphs (Graph-FL) and nodes in a subgraph (Subgraph-FL) as data samples. Subsequently, FGL aims to achieve collaborative training based on these clients and a trusted server. Formally, we consider a FGL system consisting of K clients, where the k-th client manages a private dataset denoted as \\(D^{(k)} = \\{G^{(k)}_i \\}_{i=1}^{N_T}\\). Here, \\(N_T\\) is the task-specific description, where \\(N_T\\) denotes the number of graph samples under Graph-FL, while \\(N_T = 1\\) exists under Subgraph-FL. To provide a detailed description, we take FedAvg [59] as an example. Its training process within the T communication round is outlined in four key steps:\n1. Receive Message. Each client initializes its local model with the one received from the server \\(w \\leftarrow \\hat{w}^{t}\\);\n2. Local Update. Each client performs local training using its private data, i.e., \\(\\underset{w}{\\min} L_{task}(D^{(k)})\\) to obtain \\(W_k^{t+1}\\), where \\(L_{task}\\) denotes the task-specific optimization objectives.\n3. Upload Message. Each client uploads their local updated models \\(W_k^{t+1}\\) and the number of data samples \\(D_i\\) (i.e., graphs, nodes, or edges, depending on the downstream task) to the server.\n4. Global Aggregation. The server aggregates the updated models to obtain \\(W^{t+1}\\) for the next communication, i.e., \\(\\hat{W}^{t+1} \\leftarrow \\frac{1}{D} \\sum_{k=1}^K D_k W_k^{t+1}\\), where D is the total number of data samples."}, {"title": "3 BENCHMARK DESIGN", "content": "In this section, we offer detailed descriptions of OpenFGL, with an overview provided in Table 2. Specifically, we present a presentation from the perspectives of FGL application scenarios (Sec. 3.1), algorithms (Sec. 3.2), and experimental evaluations (Sec. 3.3)."}, {"title": "3.1 Data-level FGL Scenarios", "content": "In OpenFGL, we focus on the following two most prevalent FGL scenarios: (1) Graph-FL. The growing integration with graph-based techniques and AI4Science applications, such as drug discovery, has motivated this scenario, in which clients consider graphs as the data samples and pursue collaborative training between clients to acquire powerful models while preserving data privacy. (2) Subgraph-FL. Realistic applications in this scenario include node-level fraud detection for financial security and link-level user-item interactions for recommendation, with data stored in a distributed manner. Clients treat their data as subgraphs of a larger and more comprehensive global graph, and focus on utilizing nodes and edges as data samples for training. Due to regulatory constraints and the data-sensitive nature, clients seek a collaborative training scheme to develop well-trained models without direct data sharing.\nNotably, Node-FL is also a significant paradigm of FGL, which is widely used in graph-based spatial-temporal analysis, such as sensor networks [71] and traffic flow prediction [95], where nodes are only aware of their local context within the broader network. Although Node-FL has been widely mentioned, we have not integrated it into OpenFGL. This is because most Node-FL studies are tailored to specific scenarios and involve experimental setups that are highly diverse and closely aligned with particular application contexts. These characteristics make Node-FL less suitable for being included in a unified benchmark evaluation, where consistency across scenarios is essential for comprehensive comparisons.\nDatasets. To comprehensively evaluate existing FGL algorithms in the above two real-world application-driven scenarios, we have compiled a substantial collection of public datasets from various domains. Specifically, Regarding Graph-FL scenario, we conduct experiments on the compounds networks (MUTAG, BZR, COX2, DHFR, PTC-MR, AIDS, NCI1, hERG) [13, 22, 30, 68, 73, 77], protein networks (ENZYMES, DD, PROTEINS) [7, 15, 30], collaboration network (COLLAB) [40], movie network (IMDB-B/M) [89], super-pixel networks (MNISTSuperPixels) [61], and point cloud networks (ShapeNet) [94]. As for Subgraph-FL scenario, we perform experiments on the citation networks (Cora, Citeseer, PubMed,\nRemarkably, besides these datasets being collected across various application domains, they exhibit diverse graph characteristics, encompassing rich or poor node attributes at the feature level, homophily or heterophily, and sparsity or density at the topology level. These graph properties facilitate the evaluation of the adaptability and robustness of existing FGL algorithms across various and intricate experimental settings, highlighting their strengths and revealing potential limitations from a data-centric perspective. More details can be found in Tables. 3, 4 and [1] (A.1).\nSimulation Strategies. In response to policy constraints on acquiring distributed graphs, we draw inspiration from federated learning in computer vision to simulate generalized federated scenarios by partitioning existing datasets into distributed subsets. This strategy is similar to recent FGL benchmarks [29, 82].\nIn our proposed OpenFGL, we integrate 8 federated data simulation strategies driven by practical applications, in which we ensure the graph data distributed to each client exhibits similar patterns in feature, label, or topology while maintaining a controllable heterogeneity across clients. Specifically, for both Graph-FL and Subgraph-FL, we implement 3 simulation strategies widely used in graph-independent FL without graph-oriented designs, including Feature Distribution Skew, Label Distribution Skew, and Cross-Domain Data Skew. In the context of Graph-FL, we introduce a topology-oriented simulation strategy called Topology Shift, which distributes graphs based on degree distribution. The inspiration for this approach stems from key insights offered by recent FGL studies [74, 85]: in Graph-FL, the structure Non-iid resulting from topology shift is the primary challenge in collaborative optimization, where node features and labels appear less significant by comparison. As for Subgraph-FL, existing federated data simulation strategies predominantly utilize prevalent community detection algorithms such as Louvain [6] and Metis [36] to identify clusters with dense intra-community connections. Then, these clusters' nodes are subsequently allocated to local clients to construct corresponding induced subgraphs for partitioning. These strategies all operate under a common assumption: that the private data collected by each local agent in the real world contains dense internal connections but is loosely connected across clients. Despite their effectiveness, limitations persist in their application. This is because Subgraph-FL primarily focuses on node-level and link-level downstream tasks, where node profiles (i.e., node features and labels) are crucial. However, these strategies do not consider label distribution for federated data simulation. Hence, we introduce Metis-based Label Imbalance Split and Louvain-based Label Imbalance Split. These methods, refining the aforementioned strategies, carefully consider label distribution during cluster allocation to better simulate realistic and generalized distributed scenarios. We outline these strategies in Table 5, with descriptions provided below:\nFeature Distribution Skew is a graph-independent strategy to simulate feature distribution shifts. In OpenFGL, we utilize this approach to create more challenging and realistic scenarios for evaluating FGL algorithms. Specifically, we implement the following feature operations: (1) Adding Gaussian or Laplacian noise to introduce variability; (2) Applying scaling operations to simulate different magnitudes of features; (3) Employing mathematical transformations to further diversify the feature distributions.\nLabel Distribution Skew is also a graph-independent simulation strategy. In our implementation, we use the a-based Dirichlet distribution to create imbalanced label distributions across clients. This approach ensures varied and imbalanced label distributions, simulating real-world data scenarios. The a in Dirichlet distribution controls the concentration of probabilities across label classes, with larger values leading to more uniform distributions and smaller values resulting in sparser and more concentrated distributions.\nCross Domain Data Skew is a fundamental challenge in FL, arising from the heterogeneous nature of data sources and collection methods across distributed databases. In OpenFGL, we simulate this scenario by evenly distributing multiple datasets among a predefined number of clients, maintaining a diverse representation.\nTopology Distribution Skew represents the strategy for partitioning graphs based on their topology properties in Graph-FL. This approach involves sorting the global graph according to specific characteristics, such as average node degree, and then distributing the resulting graphs to predefined clients. This ensures that the distribution of graph data among clients accurately reflects the underlying topological diversity of the original global dataset.\nMetis-based Community Split is a widely adopted Subgraph-FL federated data simulation strategy that utilizes a multilevel recursive bisection and k-way partitioning technique. This method iteratively reduces the size of the graph and refines the partitioning. Notably, compared to the following the Louvain-based data simulation strategy, Metis can directly partition a graph into a predefined number of communities, aligning precisely with the number of clients and streamlining data allocation in the federated settings.\nLouvain-based Community Split stands as the other prevalent federated data simulation strategy in Subgraph-FL, partitioning a graph into multiple communities (subgraphs) via modularity optimization. The number of communities is determined by the resolution parameter of the Louvain algorithm. However, the Louvain algorithm often generates more communities than the predefined number of clients. To resolve this, communities can be allocated among the clients by averaging node quantities.\nMetis-based Label Imbalance Split is a new Subgraph-FL federated data simulation strategy introduced in this paper. The naive Metis-based Community Split lacks post-processing capabilities, leading to challenges in controlling subgraph heterogeneity among clients. In contrast, Metis+ enables predefined community partitioning, followed by clustering based on label distribution similarity, thereby consolidating similar communities under a single client.\nLouvain-based Label Imbalance Split enhances the conventional Louvain method by allocating communities to clients based on similarities in label distributions rather than solely on node averages. This approach ensures that each client receives communities with consistent label characteristics, thereby mitigating label imbalance and promoting equitable model training across federated clients. By aligning label distributions, this strategy enhances the fairness and robustness of federated learning, reducing biases that may arise from heterogeneous label distributions among clients."}, {"title": "3.2 Method-level FGL Algorithms", "content": "GNN Backbones. To provide a broader spectrum of graph learning paradigms on the client side, OpenFGL integrates a diverse range of local GNN backbones. Specifically, we implement various well-designed polling strategies (TopKPooling [19], SAGPooling [39], EdgePooling [14], and PANPooling [58]) based on the most representative GIN [87] with weight-free MeanPooling [87] for Graph-FL. As for Subgraph-FL, OpenFGL includes prevalent GCN [38], GAT [76], GraphSAGE [27], SGC [84] and GCNII [10]. The detailed descriptions of these backbones can be found in [1] (A.2).\nFL/FGL Algorithms. To achieve federated training, multi-client collaboration algorithms based on a trustworthy server are also crucial. Therefore, we follow the historical progression of FL to include a spectrum of algorithms from the most representative methods in CV to FGL: (1) CV-based FL algorithms: FedAvg [59], FedProx [45], Scaffold [35], MOON [44], FedDC [20], FedProto [75], FedNH [11] and FedTGP [96]; (2) FGL algorithms: GCFL+ [85] and FedStar [74] in Graph-FL and FedSage+ [99], Fed-PUB [5], FedGTA [49], FGSSL [33], FedGL [8], AdaFGL [48], FGGP [78], FedDEP [97], and FedTAD [103] in Subgraph-FL. More details about these algorithms can be found in [1] (A.2). Notably, these algorithms are implemented with a unified API to facilitate future development in a user-friendly manner. For more details about our API design from the algorithm perspective, please refer to Sec. 4.5."}, {"title": "3.3 Experiment-level FGL Evaluations", "content": "Data Analysis. To delve into the data heterogeneity of FGL and offer essential insights for future studies, we advocate further analyzing the distributed graph data, encompassing the following aspects: (1) Feature KL Divergence: It reveals feature skew among clients while the label domain remains consistent, which may arise due to the different geographical locations of clients, such as the characteristics of a certain disease may significantly differ across various regions. (2) Label Distribution: As a more generalized data heterogeneity in CV-based FL, it is extensively discussed. However, in graph-based scenarios, the relationship between labels and topology frequently reveals underlying connections, characterized as homophily. Thus, we further integrate multi-level homophily metrics [53, 54, 63-65] to offer comprehensive analysis. (3) Topology Statics: This perspective stems from the critical role of topology in GNNs, especially in distributed scenarios. This arises from the significant impact of diverse local topology statistics on the local model, causing complex cascading effects in collaboration. Therefore, we examine topological differences among clients (e.g., Degree, Centrality, Largest Component) to provide insights.\nEffectiveness. Details of our evaluation metrics are as follows: graph/node classification (Accuracy, F1, Recall, Precision), graph regression (MSE, RMSE), link prediction (AP, AUC-ROC), and node clustering (Clustering-accuracy, NMI, ARI). More detailed descriptions of these metrics are presented in [1] (A.3).\nRobustness. To evaluate the practical deployment of FGL in real-world scenarios, we examine its robustness from the following perspectives: (1) Noise: This corresponds to data quality issues resulting from data collection. (2) Sparsity: This reflects scenarios with incomplete features, labels, and topology due to data scarcity and high labor costs, and with a low rate of client participation. (3) Client Communication: This simulates scenarios with network constraints or high communication costs. (4) Generalization: This evaluates generalizability of algorithms in intricate applications. (5) Privacy Preserve: This reflects the applicability of algorithms in privacy-sensitive scenarios, thereby we conduct an in-depth analysis from the perspective of Differential Privacy (DP). Please refer to [1] (A.4) for further details on the robustness settings.\nEfficiency. To facilitate FGL deployment, we conduct an evaluation of current baselines regarding their efficiency. Specifically, we evaluate them from both theoretical (algorithm complexity) and experimental (communication cost and running time) perspectives."}, {"title": "4 EXPERIMENTS AND ANALYSIS", "content": "In this section, we systematically investigate FGL algorithms by answering the following questions: (1) For Effectiveness, Q1: What advantages does federated collaboration offer compared to training solely on local data? Q2: How do FGL algorithms and federated implementations of GNNs perform in two prevalent FGL scenarios? (2) For Robustness, Q3: How do FGL algorithms perform under local noise and sparsity settings (i.e., features, labels, and edges)? Q4: What are the performance of FGL algorithms under low client participation rates in communication? Q5: Can FGL algorithms maintain generalization across various graph-specific distributed scenarios? Q6: Do FGL algorithms support additional DP privacy protection? (3) For Efficiency, Q7: What are the theoretical algorithm complexity of FGL algorithms? Q8: What is the practical running efficiency of FGL algorithms? To maximize the usage for the constraint space, more results are shown in [1] (A.5-A.7).\n4.1 Performance Comparison\nTo answer Q1, in addition to the federated multi-client collaboration, we introduce \"Local\" to represent solely local training for analyzing the advantages and potential limitations of FGL. Based on this, to answer Q2, we present the end-to-end performance in Table 6 and Table 7. The detailed analysis is presented as follows:\nGraph-FL Scenario. Since the FGL research is still immature, the baselines for Graph-FL are scarce (only GCFL+ and FedStar). Therefore, we expand the number of local pooling-based backbones and CV-based FL algorithms, providing comprehensive results in Table 6. For Q1, we find that the benefits of federated collaboration are not significant for small-scale MUTAG, BZR, and COX2 due to their scarce data samples that can not support federated training and thus affect predictions. Subsequently, we conclude that C1: Federated collaboration is more advantageous for larger-scale datasets, benefiting from abundant data sources. As for Q2, we observe that: (1) GCFL+ and FedStar concentrate on topology Non-iid within the cross-domain simulation, thereby not consistently achieving competitive performance in this single-source setting. Therefore, they perform less favorably than FL algorithms on ENZYMES, COLLAB, and MULTI. (2) Existing FGL algorithms heavily rely on node semantics. We observe significant improvements on datasets with abundant node descriptions like DD and PROTEINS, whereas limited performance on BINARY whose node representations are initialized with node degrees. Consequently, we conclude that C2: Graph-FL algorithms still have improvement space, especially in single-source domain and constrained data semantics.\nSubgraph-FL Scenario. Compared to Graph-FL, Subgraph-FL has significantly advanced and offers numerous SOTA baselines. Their results are shown in Table 7. For Q1, although Subgraph-FL can benefit from abundant data samples, for Chameleon, Actor, and Ratings, the improvement from both FL and FGL algorithms is limited or even worse than solely local training in some cases. We attribute this to the well-known heterophily challenges, where differences in node connection rules across clients significantly affect local updates and server-side collaboration, deviating from the global optimum and resulting in sub-optimal predictive performance. Although AdaFGL addresses this issue through personalized technology, there is still room for better performance. Based on this and C1, we conclude that C3: The prerequisite for positive impacts of federated collaboration is the uniform distribution of node features, labels, and topology. Regarding Q2, we observe that: (1) Subgraph-FL is thriving, with numerous baselines vigorously competing for the best performance. Among them, FedTAD and AdaFGL stand out in most cases. (2) The outstanding performance of existing algorithms stems from the fine-grained exploration of the topology, but some methods lack scalability when dealing with large-scale ogbn-products, resulting in OOM (out-of-memory) errors. Consequently, we conclude that C4: Subgraph-FL algorithms should address the complexity of real-world deployment, especially focusing on large-scale scenarios and graph-specific federated heterogeneity challenges."}, {"title": "4.2 Robustness Analysis", "content": "Local Noise. To answer Q3 from the perspective of noise, the experimental results are shown in the upper part of Fig. 1(a)-(c). For feature noise, we randomly select the channels of node features and inject Gaussian noise. To simulate edge noise in the Graph-FL without node labels, we utilize Metattack [104] to add noise edges, significantly perturbing the training gradients. As for label noise, we randomly reassign non-real labels to a certain proportion of training node samples. Based on the experimental results, we observe that FGL algorithms are highly sensitive to edge noise compared to topology-agnostic FL algorithms. This inherent limitation directly disrupts the model optimization of GCFL+ and FedStar, misleading local updates and topology-driven collaboration, thereby resulting in sub-optimal predictive performance. However, GCFL+ and FedStar demonstrate superior robustness under feature and label noise, as they address client interference with each other using server-side clustering customization and additional local models maintained at client-side. Consequently, we can conclude that C5: Noise scenarios determine the performance lower bound for FGL algorithms, where personalized strategies emerge as crucial technologies. However, they fall slightly short in addressing edge noise.\nLocal Sparsity. To answer Q3 from the perspective of sparsity, the experimental results are shown in the lower part of Fig. 1(a)-(c). Regarding feature sparsity, we simulate partial feature absence for unlabeled nodes and randomly remove the original edges. As for label sparsity, we change the ratio of training nodes. Under feature sparsity, FGSSL and Fed-PUB exhibit significant performance fluctuations due to heavy reliance on high-quality features for node-wise contrastive learning and model-wise gradient matching. Conversely, FedSage+, AdaFGL, and FedTAD leverage multi-client collaboration (i.e., subgraph generation, augmentation, and knowledge distillation), mitigating confusion in under-trained model collaboration, thus ensuring robustness. Similar analysis can extend to label sparsity, as both of them directly affect local updates. Regarding edge sparsity, FedTAD's vulnerability lies in its reliance on client-side topology embeddings to supervise server-sider pseudo-subgraph generator, rendering it fragile in extreme edge sparsity scenarios. However, AdaFGL and FedSage+, leveraging topology mining, adeptly handle this challenge. Based on this, we can conclude that C6: Sparsity scenarios determine the performance upper bound for FGL algorithms, where multi-client collaboration is the pivotal technology, particularly in synergy with topology mining.\nClient Participation. To answer Q4, we present the experimental results in Fig. 1(d), where robust FGL algorithms with low client participation exhibit one of the following characteristics: (1) They rely less on messages received from the server and focus on well-designed local training mechanisms; (2) They custom global messages for each participating client for local personalized training. For instance, in Graph-FL, the unstable performance of FedStar arises from its heavy reliance on specific topological properties and well-learned structure embeddings. Conversely, GCFL+ ensures high-quality local updates by tailoring the most suitable messages for each client through server-side client clustering. In Subgraph-FL, AdaFGL, Fed-PUB, and FedGTA rely on client-side personalized local training and server-side pseudo graph-driven trainable weight clustering or identification of subgraph statistics to ensure custom global messages for each participating client. Consequently, we can conclude that C7: Low client participation underscores the emphasis of FGL algorithms on local updates, highlighting the importance of local data understanding and customizing messages for each client.\nGeneralization. To answer Q5, the experimental results are shown in Table 8, where graph-level cross-domain setting includes MUTAG, COX2, PTC-MR, AIDS, ENZYMES, DD, PROTEINS, COLLAB, and IMDB-B/M and subgraph-level setting includes Cora, CiteSeer, Pubmed, CS, and Physics. We observe that current FGL algorithms exhibit inconsistent performance across data simulations. Specifically, in Graph-FL, tiny-scale datasets with abundant node descriptions such as DHFR and DD mitigate inherent differences among data simulations potentially due to over-fitting issues. As for the Subgraph-FL, data simulations present challenges for existing FGL algorithms. For instance, Fed-PUB and AdaFGL, incorporating personalized strategies, lose their previous advantages, whereas FedSage+ and FedGTA, emphasizing multi-client collaboration mechanisms, show significant potential. Therefore, we can conclude that C8: In practical deployments aiming for generalization, client-specific design should be used cautiously, with an emphasis on discovering inherent global consensus.\nDP-based Privacy Preserve. To answer Q6, the experimental results are shown in Table 9, where e is the privacy budget. To implement DP in FGL, we introduce well-selected random noise in the local model gradients for server-side perturbed model aggregation. More technology details can be found in [1] (A.8). Meanwhile, we integrate FedDEP into OpenFGL, which introduces additional edge-level DP. Based on the results, we observe that a large e enables privacy-preserving methods to match the accuracy of the non-private approach. However, reducing e results in a notable performance drop, primarily due to the need for local models for excessive noise injection into gradients, leading to significant degradation. Regarding FedDEP, compared to FedSage+, it achieves edge-level differential privacy through random sampling and enhances model capacity with a deep neighbor generation module, striking a balance between performance and privacy preservation. Based on this, we conclude that C9: FGL algorithms currently face a dilemma between predictive performance and privacy preservation."}, {"title": "4.3 FGL Algorithm Complexity Analysis", "content": "To answer Q7, we provide a theoretical algorithm complexity analysis of the prevalent FL and FGL baselines, as illustrated in Table 10, where n, m, c, and f are the number of nodes, edges, classes, and feature dimensions, respectively. s is the number of selected augmented nodes and g is the number of generated neighbors. b and T are the batch size and training rounds, respectively. k and K correspond to the number of times we aggregate features and moments order, respectively. N is the number of participating clients in each training round. t represents the number of clients exchanging information with the current client. w represents the model-wise weight alignment loss term, Q denotes the size of the query set used for CL, E stands for the number of models for ensemble learning, M and p indicate the dimension of the trainable matrix used to mask trainable weights and the prototypes. Besides, \\(P_g\\) represents pseudo-graph data stored on the server side.\nTo progressively illustrate the theoretical algorithm complexity analysis, we choose SGC [84] as the local model (k-step feature propagation), otherwise, we adopt the model architecture (L-layer) used in their original paper. For the k-layer SGC model with batch size b, the \\(X^{(k)}\\) is the propagated feature bounded by a space complexity of O((b + k)f). The overhead for linear regression by multiplying W is O(\\(f^2\\)). In the training stage, the above procedure is repeated to iteratively update the model weights. For the server performing FedAvg, it needs to receive the model weights and the number of samples participating in this round. Its space complexity and time complexity are bounded as O(\\(Nf^2\\)) and O(N). As discovered by previous studies [9, 47, 100], the dominating term is O(kmf) or O(Lmf) when the graph is large since feature learning can be accelerated by parallel computation. Consequently, O(Lmf) emerges as the dominating complexity term of linear transformation, and the execution of full neighbor propagation becomes the primary bottleneck for achieving scalability. Although FGL offers a new perspective for large-scale graph learning through a distributed paradigm, it still requires the deployment of suitable scalable GNNs in billion-level scenarios on the local client.\nThe current mainstream trend in FL or FGL studies emphasizes the development of well-designed client-side updates to fit local data. For instance, FedProx introduces model weight alignment loss, resulting in complexities of O(\\(wf^2\\)). Similarly, approaches like MOON, FGSSL, FGGP, FedStar, and AdaFGL employ CL loss and ensemble learning for local updates, introducing additional computational overhead upon the graph learning. Specifically, for the contrastive learning in MOON, FGSSL, and FGGP, the additional computational cost depends on the size and semantics of the query sample set, resulting in complexities of O(Q\\(f^2\\)), O(Q(b + k)f + \\(f^2\\))), O(Q\\(cp^2\\)) respectively. This will lead to unacceptable computational overhead as the scale of local data increases. As for ensemble learning approaches like FedStar and AdaFGL, which maintain multiple models locally to extract private data semantics from various perspectives, they can be bounded by O(E((b + k)f + \\(f^2\\))). Furthermore, FedSage+, Fed-PUB, and FedGTA exchange additional information during communication to improve federated training. Despite their inherent similarities, these methods exhibit significantly different time-space complexities due to variations in their design. Specifically, FedSage+ involves client-side data sharing for local subgraph data augmentation, leading to a complexity of O(L((n+ sg) f + \\(f^2\\))). Fed-PUB maintains a global pseudo-graph on the server side and utilizes locally uploaded weights to update trainable mask matrices for personalized learning, introducing a complexity of O(N(\\(f^2\\) + M) + \\(P_g\\)). In contrast, FedGTA is a lightweight method that utilizes topology-aware soft labels to encode local data, enabling personalized model aggregation on the server. As a result, this approach possesses a complexity of O(kKC)."}, {"title": "4.4 Efficiency Evaluation", "content": "To answer Q8", "observations": 1, "C10": "FGL algorithms leveraging prototypes and decoupled techniques (i.e.", "follows": "nFGLTrainer. This class manages the flows of message and command among multiple clients and a central server in FGL processes. It begins by initializing the device, data, clients, and server using configuration parameters. Subsequently, in each specific federated training method, the trainer coordinates multiple communication rounds. During each round, it first selects a designated set of clients, then updates a message pool to reflect communication status, and finally dispatches server messages to the selected clients. Upon receiving the message, clients execute tasks locally and transmit updates to the server for global processing. Notably, Each communication round contains a evaluation phase. The Pytorch-style implementation of FGLTrainer is depicted"}]}