{"title": "IDNet: A Novel Dataset for Identity Document Analysis and Fraud Detection", "authors": ["Hong Guan", "Yancheng Wang", "Lulu Xie", "Soham Nag", "Rajeev Goel", "Niranjan Erappa Narayana Swamy", "Yingzhen Yang", "Chaowei Xiao", "Jonathan Prisby", "Ross Maciejewski", "Jia Zou"], "abstract": "Effective fraud detection and analysis of government-issued identity documents, such as passports, driver's licenses, and identity cards, are essential in thwarting identity theft and bolstering security on online platforms. The training of accurate fraud detection and analysis tools depends on the availability of extensive identity document datasets. However, current publicly available benchmark datasets for identity document analysis, including MIDV-500, MIDV-2020, and FMIDV, fall short in several respects: they offer a limited number of samples, cover insufficient varieties of fraud patterns, and seldom include alterations in critical personal identifying fields like portrait images, limiting their utility in training models capable of detecting realistic frauds while preserving privacy. In response to these shortcomings, our research introduces a new benchmark dataset, IDNet, designed to advance privacy-preserving fraud detection efforts. The IDNet dataset comprises 837,060 images of synthetically generated identity documents, totaling approximately 490 gigabytes, categorized into 20 types from 10 U.S. states and 10 European countries. We evaluate the utility and present use cases of the dataset, illustrating how it can aid in training privacy-preserving fraud detection methods, facilitating the generation of camera and video capturing of identity documents, and testing schema unification and other identity document management functionalities.", "sections": [{"title": "1 Introduction", "content": "The surge in digital platforms offering remote identity proofing has escalated concerns regarding the forgery of identity documents, including passports, driver's licenses, and identity cards. The Financial Crimes Enforcement Network (Fin-CEN) reported that in 2021, around 1.6 million Bank Secrecy Act (BSA) reports-constituting 42% of all reports filed that year were related to identity fraud, highlighting $212 billion in suspicious transactions [7]. This issue poses risks across various sectors, including finance, healthcare, travel, retail, government, telecommunications, and gambling [15]. According to a recent industry analysis [15], fraudulent techniques have evolved from simple forgeries, such as name alterations, to the advanced use of generative AI/ML technologies for creating deceptive images, like face morphing [58]. Notably, most remote identity validation services on these digital platforms rely on images captured in white light rather than multi-spectral imaging techniques like near-infrared and ultra-violet light. This paper examines the efficacy of identity validation under these common lighting conditions, which is the focus scenario of this paper.\nDespite the availability of several public datasets for identity document analysis, which focused primarily on images taken in white light, such as MIDV-500 [19], MIDV-2019 [23], MIDV-2020 [24], FMIDV [17], BID [50], and SIDTD [21], our examination has uncovered significant limitations in these resources.\n\u2022 Limited number of distinct samples: Most existing datasets contain less than 1,000 distinct identity documents. While these datasets may help develop tools for simple tasks such as optical character recognition (OCR), they are insufficient for training and testing AI/ML models for complicated tasks such as privacy-preserving analysis. Although the BID dataset contains 28,800 distinct identity documents, the portrait photos are blurred. This characteristic makes it unsuitable for critical tasks such as detecting face morphing and portrait substitution, where clear images are essential for accurate model performance.\n\u2022 Insufficient Fraud Patterns: Only a few publicly available datasets, FMIDV [17] and SIDTD [21], which build upon the MIDV dataset, contain identity documents with fraudulent alterations. FMIDV presents a sole Copy-and-Move fraud pattern, where guilloche patterns are replicated and repositioned among documents. Conversely, SIDTD employs basic Crop-and-Move with inpainting techniques to simulate fraudulent activity. Nevertheless, fraud techniques such as face morphing, portrait substitution, and the intricate alteration of textual data remain unrepresented in these public collections. Crucially, as privacy issues take center stage in identity document management, the introduction of complex fraud patterns that intersect with extensive personal identifier information (PII), like portrait photos, ghost images, dates of birth, names, and addresses, is imperative for honing privacy-centric fraud detection methodologies. If fraud patterns were not intruding upon PII fields, redacting these fields in existing publicly available benchmarks could help preserve privacy during model training. The creation and availability of a new benchmark dataset containing representative fraud patterns are pivotal for enhancing the precision and confidentiality aspects of fraud detection in complex scenarios.\nThese constraints significantly hamper the progression of cutting-edge techniques for identity document analysis and fraud detection in an era increasingly dominated by AI/ML methodologies [37] [59] [18] and where privacy considerations are paramount in the application of these technologies [33] [20]. Overcoming these limitations is challenging because of the substantial costs of creating synthetic datasets that accurately mimic a wide range of real-life identity documents,"}, {"title": "2 Background", "content": "2.1 Identity Documents\nAs illustrated in Fig. 2, an identity document usually contains (1) security features, (2) PII information, and (3) other information, which are explained as follows:\nSecurity Features that we explore in this work focus on those that are amenable to digital capture and analysis under standard white lighting conditions, including barcodes, watermarks, micro-printing, guilloche (also known as rainbow printing), distinct color schemes, unique text font, barcodes, and the machine-readable zones (MRZ).\nPersonally Identifiable Information (PII) includes but are not limited to the portrait photo, signature, barcode, family name, given name, DOB, customer identifier, cardholder address, and ghost image. First, when generating the IDNet datasets, we must not disclose any PII information from the real world. Second, given the need to develop new privacy-preserving methods that prevent fraud detection or other analysis processes from disclosing PII information, a primary goal of designing our novel identity document benchmark dataset is to facilitate such analysis by providing fraud patterns that overlap with PII fields and pose challenges for privacy-preserving fraud analysis.\nOther Information includes but is not limited to date of issue, date of expiry, document discriminator, endorsement, restrictions, date of first issue, separate"}, {"title": "2.2 A Survey of Existing Public Identity Document Datasets", "content": "Existing publicly available document datasets are designed for recognizing, classifying, and restoring information from documents captured as videos or photos using mobile devices. For example, the SmartDoc dataset [25] is a publicly available document dataset. It contains a training set of 10 document samples captured as video clips (i.e., document capture samples) by a video camera. Each training sample also contains an image of the document used to produce the sample, which is considered the ground truth for comparison to the document restored from the video. In addition, SmartDoc offers a testing set of 37 document capture samples. This dataset includes only a few identity documents. The LRDE identity document image database [46] comprises 100 videos for a dozen different types of visas and passports from various countries using different environmental conditions and several kinds of smartphones.\nMIDV-500 [19] is a publicly available identity document dataset that contains 500 video clips. These video clips are produced from 50 different identity document types, including 17 types of ID cards, 14 types of passports, 13 types of driving licenses, and 6 other identity documents of various countries. Each of the 50 document types will be used in 5 different backgrounds to generate 10 video clips that last at least 3 seconds in duration using two mobile phone devices. MIDV-500 aims at facilitating simple analysis tasks like face detection, optical character recognition (OCR), and document type classification. MIDV-2019 [23] extended the MIDV-500 dataset to include four more videos with distorted identity documents and different lighting conditions for each identity document"}, {"title": "3 The IDNet Benchmark Dataset", "content": "Leveraging a cutting-edge AI-assisted pipeline, outlined in Sec. 4, we have developed an identity document dataset, named IDNet, which is entirely synthetically generated and devoid of any private information. This dataset encompasses a total of 837, 060 identity documents, spanning across 20 different document types. For each document type, there are 5,979 unique document samples. Each sample comprises one authentic copy alongside four fraudulent variations, which include face morphing, portrait substitution, text alteration, and a combination of these fraudulent techniques as detailed in Sec. 4.4. IDNet stands as the largest publicly accessible identity document dataset to date, as depicted in Fig. 1. Detailed statistics and Zenodo URLs for each document type within IDNet are systematically presented in Tab. 1.\nThe IDNet dataset is released as identity document image files along with JSON files that describe the metadata of each document. The metadata for each positive sample (w/o fraud patterns) includes the document identifier, face image ID, name, sex, date-of-birth (DOB), issuing date, and expiration date. The metadata for each fraud identity document contains additional information, such as the fraud type and fraud parameters. For portrait substitution, we recorded the identifier of the original face, and the identifier of the new face. For face morphing [58], we documented the ID and the morphing weight of each morphed"}, {"title": "4 Generation of the Identity Document Dataset", "content": "4.1 Template Generation based on Image Diffusion Model\nTo create the IDNet dataset, it is essential first to acquire a template for each type of identity document. However, high-quality, blank templates of real-world documents are generally unavailable. To overcome this, we utilize image generative models, such as diffusion models [34][49][56], to produce our ID templates by erasing the content from actual identity documents. Specifically, we employed the Stable Diffusion version 2.0 from Hugging Face 5, which is based on the Latent Diffusion Model [49], to create templates from 20 different types of real-world IDs. This model is adept at editing masked areas of an input image in accordance with text prompts. For our purposes, we mask all customizable information on the IDs as illustrated in Fig. 3 and direct Stable Diffusion with the prompt \"remove all texts/photos in the masked areas.\" Consequently, the model adeptly eliminates customized data from the document and replenishes the masked sections with appropriate backgrounds. We demonstrate this template generation technique in Fig. 3, showcasing the Arizona Driver's License as an illustrative example.\nIn instances involving complex identity documents, such as the passport from the Republic of Azerbaijan, where the information to be removed intersects with elements we wish to retain, the model faces challenges in achieving desirable outcomes without precise mask adjustments. To enhance the quality of the generated templates in these scenarios, we engage in an iterative process of applying the diffusion model with progressively refined masks and directives until a satisfactory template is achieved.\n4.2 Metadata Information Generation\nTo synthesize the information to be filled into the identity document templates, several key factors were considered. First, some fields correlate with each other. For example, the sex, ethnicity group [1], age, and eye color should match with the portrait photo and the name should match the sex and ethnicity group. Second, different identity cards may have different requirements over ages, portrait photos' facing directions and facial expressions, and so on. For example, many US states have a minimum age limit for applying for driver's licenses. To address these issues, we used a well-known public face image dataset for academic research,"}, {"title": "4.3 Add Generated Information to the Identity Document Templates", "content": "Once all the synthetic information was generated, it was added to the corresponding template. One of the challenges involved the selection of font size and style that closely approximates those found on genuine documents.\nTo optimize the generation of text overlays on genuine documents, we applied Bayesian optimization [32] to identify the best parameters that make the generated document as similar to the original as possible. The aim was to maximize the Structural Similarity Index (SSIM) [35], a metric that quantifies the visual similarity between two images by considering luminance, contrast, and structure. SSIM provides a robust measure for evaluating the quality of the generated image, ensuring it closely matches the original in terms of appearance and detail.\nIn our optimization process, we divided each sample into several segments, as illustrated in Fig. 5. For each segment, we focused on tuning several critical parameters (font style, font size, font color, font width, and position) to achieve the desired outcome. Bayesian optimization was employed to explore the parameter space efficiently. The process iteratively evaluated combinations of parameters, adjusting them to maximize SSIM. This approach allowed us to systematically and effectively enhance the visual similarity between the generated and genuine documents."}, {"title": "4.4 Fraud Patterns", "content": "Although our generated dataset is a synthetic dataset, we can consider it a \"representative\" dataset in research environments given its utility in many research tasks as detailed in Sec. 5 and Sec. 6. Therefore, it is reasonable to create forged identity documents on top of it. Subsequently, we enumerated and analyzed various prevalent fraud patterns observed in contemporary forged IDs. A pivotal component of our investigative methodology involved the generation of a batch of forged IDs, incorporating one or more of these identified patterns. The fraud patterns delineated in this study are described below:\n\u2022 Face-Morphing Fraud [58] [39]: recently emerged as a notable threat [15]. This type of fraud leverages the natural variations in human facial features over time, creating opportunities for identity deception. It operates on the premise that an individual's facial characteristics can significantly alter from those documented on their official identification. This variance enables an attacker (referred to as Person A) to misuse the identification of another individual (Person B), provided there is a sufficient resemblance between their facial features [58] [6]. To integrate this complex fraud pattern into our analysis, we adopted cutting-edge image fusion methods including Image Warp and Cross Dissolve [64]. Image wrap aligned key facial landmarks (e.g., eyes, nose, mouth, etc) of one face with those of the other, which ensures that the subsequent blending of the images appears natural and seamless. After the facial images were warped and aligned, Cross Dissolve blended them into a single cohesive image. The outcome is a synthesized facial image that encapsulates the likeness of both input face images concurrently, attaining a level of confidence significant enough to pass visual scrutiny as authentic. To achieve high-quality morphing as suggested by the NIST face morphing report [44], only the face area of the facial images was averaged after alignment and feature warping. In addition, the face area was adjusted to the face color histogram of the first input facial image. For each of the 5979 artificially generated photos, we morphed it with another randomly selected photo of the same ethnicity and sex, as illustrated in Fig. 6. We set the blending factor as 0.5 in the face morphing process following the NIST face morphing tie-2 implementation [44], which suggests that both faces contribute equally to the morphed face.\n\u2022 Portrait Substitution Fraud [14][15][42]. Based on industrial studies [15], a majority of digital identity document attacks in online platforms in 2023 are at low forging costs. Portrait substitution is one example, which is to use disqualified digital photos, e.g., photos taken using mobile phones or computer cameras and do not meet photo standards required by the corresponding identity document. To implement this type of fraud, for each identity document, we uniformly"}, {"title": "4.5 Time and Monetary Costs", "content": "We measured the time and dollars spent at every pipeline stage, as illustrated in Tab. 5. The pipeline ran on a system with dual Intel Xeon Gold 6226 CPUs at 2.70GHz with 24 cores each, four Nvidia GeForce 2080 Ti GPUs, and 196 GB memory to produce the identity documents. We estimate its cost to be $2 per hour based on the costs of AWS EC2 on-demand instances of similar capabilities [4]. The stable diffusion 2.0 model is free and open-sourced. The ChatGPT-3.5-turbo API costs $0.5 for a million input tokens and $1.5 for a million output tokens. We used 1860 user input tokens and 60, 652 output tokens. While generating the metadata information, we spent 410 seconds to generate sample names and addresses for 20 different countries/states and 145 seconds to derive all metadata information. All generated documents are archived using the free Zenodo service. As a result, the operational cost for producing each identity document is lower than $0.0001 with a latency of 0.14 second. It demonstrated the cost-effectiveness of the proposed pipeline and the great potential of using the pipeline to generate large-scale synthetic identity document datasets programmatically using different parameters with low time and monetary costs. In the future, we will integrate"}, {"title": "5 IDNet Quality Evaluation", "content": "The objective of this research is not to provide a set of documents that exactly resemble corresponding real-world identity documents. Instead, we attempt to provide a comprehensive and diverse set of meaningful documents that follow most identity document design standards and meet the research requirements for privacy-preserving document analysis and fraud detection [30] [16]. Despite the unavailability of real identity document datasets, we compared IDNet to existing benchmarks such as MIDV and STDID on metadata quality, document fidelity, fraud stealthiness, and task utility, as detailed in the following sections."}, {"title": "5.1 Metadata Quality", "content": "In this section, we used the MIDV-2020 benchmark, which also contains identity documents from the ten European countries as listed in Tab. 1 as a baseline to show that the generated metadata information to be filled into text fields (e.g., name, gender, DOB, address, date of issue, date of expiration, and so on) in IDNet are of similar or higher quality. This study measured the text fields in three aspects: uniqueness of text fields such as document ID number and personal ID number; diversity of text fields such as gender, birth date, and expiry date; cross-field dependency, for example, the birth date must be before the issue date.\n1. Uniqueness: In IDNet, every document ID number is unique for each country. In MIDV-2020, however, there exists duplicate ID numbers, e.g., \"668174749\" occurred twice in the Finnish ID documents.\n2. Diversity: We used entropy to measure diversity. Larger entropy indicates more diversity in data [16] [26]. We measured the entropy of gender, surname, given name, birth year, issue year, and expiry year. These are the text fields shared across the ten countries. For national-level documents, such as passports, fields such as nationality and issuing authority are usually the same within the same country, so they were excluded in the diversity study. Fig. 11 showed the comparison results for one European country, Slovak, which indicates that IDNet achieved similar or higher entropy than MIDV-2020. The results for other European countries are similar, which can be found in the Appendix A.\n3. Dependency: Both MIDV-2020 and IDNet satisfy the dependency constraints such as that the birth date should be before the issue date; the expiry date should be set according to a fixed valid period, and so on."}, {"title": "5.2 Document Fidelity", "content": "Several factors may affect the fidelity of the IDNet document and cause differences between IDNet documents and their real-world counterparts. First, the generative models may add noise when generating the ID templates, such as distortion in the background. Second, inconsistent font style, size, and spacing were used when filling the metadata information into the template. To evaluate the fidelity of IDNet documents, we adopt a standard and widely used metric, Structural Similarity Index (SSIM) [35].\nSSIM can indicate the perceived change in structural information, brightness, and contrast. Evaluating the fidelity of generated images typically requires ground-truth images, i.e., the genuine ID templates in our case. We used the official ID samples from publicly available government websites as ground truth. We filled our generated blank ID templates with the same contents of the genuine sample IDs. Then, we calculated the SSIM between the authentic sample IDs and the generated synthetic sample IDs to evaluate the IDNet's fidelity quantitatively.\nThe evaluation pipeline is illustrated in Figure 12.\nAs a result, the average SSIM of 20 ID document templates is 0.888, with a standard deviation of 0.0058. SSIM is a metric closely aligned with human perception, evaluating changes in structure, brightness, and contrast [53]. It ranges from \u22121 to 1, with 1 indicating the same structural pattern between two images. Our SSIM result of 0.853 indicates the generated templates are of satisfactory visual fidelity, and meet our fidelity goal."}, {"title": "5.3 Stealthiness of the Generated Fraud Data", "content": "We also evaluated the stealthiness of the fraud identity document samples generated in IDNet. We compare these samples with another synthetic fraud ID dataset named SIDTD [21]. We chose the Albanian ID, which is used on both IDNet and SIDTD, for evaluation. Specifically, we calculated the SSIM between each fraud ID image and the corresponding ID image without fraud. The detailed results are shown in Tab. 6. We observed that all types of fraud document samples in IDNet obtained higher SSIM than the SIDTD dataset, indicating that IDNet is made up of more stealthy fraud samples, which are more similar to the corresponding samples without fraud compared to SIDTD."}, {"title": "5.4 Utility of the Synthetic Portrait Photo Dataset used in IDNet", "content": "IDNet used a publicly available synthetic portrait photo dataset [2]. We evaluated its quality using the utility of the face morphing task. Task utility is an important metric in evaluating the quality of synthetic datasets [36] by comparing the task accuracy on the synthetic datasets and the real-world datasets. Leveraging the availability of real-world face morphing data, such as FERET [51], FRGC [8], and FRLL [9], we evaluated the utility of the artificially generated portrait photos dataset [2] used in our IDNet dataset from two perspectives, which are (1) training models on synthetic data and then assessing their performance on real-world data, and (2) analyzing whether models maintain their relative performance rankings when trained on both synthetic and real-world data [36].\nIn the first perspective, we evaluate how models trained on IDNet transfer to real-world datasets in terms of face morphing detection accuracy. This study assumes that models trained on synthetic datasets will be directly applied to real-world applications. For comparison, we also test how models trained on real-world datasets transfer to other real-world datasets. In the second perspective, we study whether the rankings of a set of models would be the same on synthetic or real-world data in terms of face morphing detection accuracy following [36]."}, {"title": "5.5 Utility of the Text-field Fraud Replacement Dataset in IDNet", "content": "To evaluate the synthetic fraud data generated from the text-field fraud replacement pattern, we focus on the Inpaint and Rewrite pattern and the Crop and"}, {"title": "6 Use Cases", "content": "The IDNet dataset is poised to significantly enhance a wide array of applications, spanning from privacy-centric fraud identification to the nuanced detection of face morphing, cross-type analysis, and the alignment and unification of schemas through Large Language Models (LLMs). The forthcoming sections provide a detailed exploration of these application scenarios, illustrating the dataset's versatility and its potential to catalyze advancements across diverse domains in identity document management."}, {"title": "6.1 Privacy-Preserving Fraud Detection", "content": "In releasing our novel dataset for identity document analysis and fraud detection, it is important to consider the privacy concerns. The dataset's intrinsic value lies not only in its comprehensive coverage and potential to revolutionize fraud detection but also in the sensitive nature of the information it encompasses (e.g., the photo identity, name, and so on). To responsibly harness this value while safeguarding individual privacy, the design and deployment of a privacy-preserving algorithm is not just beneficial but imperative. Thus, in this section, our goal is to evaluate the performance of our dataset when it meets with the current privacy-preserving algorithm.\nTo achieve this goal, we selected two standard privacy-preserving algorithms: Masking [5] [48] and PixelDP [40]. The algorithm details and settings are described as follows:\nMasking. As shown in Figure 13 (b) and 14 (b), we applied masks to those regions that contain sensitive information, i.e. zeroing the pixel values. In this way, all the representations were completely erased without any possibility of being recognized. While this approach, which is also termed redaction, has been widely used [5] [48] [67], it leads to information loss and may disable analysis tasks that rely on the redacted information.\nPixelDP. Introduced by [40], PixelDP is a robust privacy-preserving method via adding noise sampled from a specific distribution to the input or intermediate features. In this paper, we focused on the input-level PixelDP, directly applying Gaussian noise to the input examples. Specifically, We set e = 1.0 and d = 0.05, which provides (1.0,0.05)-DP guarantee. The perturbations are sampled from a Gaussian distribution with zero mean and standard deviation $\\sigma = \\sqrt{2\\ln(\\frac{1.25}{\\delta})} \\frac{\\Delta_{p,2}L}{6}$, where we use $L_2$-norm (i.e. p = 2) and $\\Delta_{2,2} = 1$. With larger L, the perturbations will be more significant, and thus distort more information.\nExperimental Settings. We evaluated these two algorithms on four different fraud ID detection tasks, including face morphing detection, portrait substitution detection, mixed fraud face detection, and text replacement detection. For the first three tasks in the face regions, we used a MixFaceNet-S [57] model as the detector, with embedding_size = 128, width_scale = 1.0, gdw_size = 1024, and shuffling disabled. For text replacement detection, we used a simple convolutional network with 5 convolutional layers and 2 fully-connected layers. All the models were trained for 200 epochs with lr = 0.1. The validation dataset consists of 1000 positive and negative examples.\nResults. The results are shown in Table 11 and Table 12, and the example images are illustrated in Figure 13 and Figure 14. Directly masking almost disabled the fraud detection capability, although it completely preserved the privacy of the sensitive information. That's because the helpful representations for fraud detection were also completely erased by masking, and the detector learned nothing from the training examples. Meanwhile, PixelDP with smaller L barely degraded the fraud detection performance (and even slightly improved it, serving as a kind of data augmentation), while the sensitive information were still highly recognizable. With larger L, the sensitive information werebetter concealed, at the cost of a nonnegligible fraud detection performance drop, since the helpful representations for fraud detection were also distorted (except for portrait substitution detection, which is a relatively easier task.)\nAnalysis. From the results of two baseline privacy-preserving algorithms, Masking and PixelDP, we observed a significant gap in their capabilities in balancing utility and privacy. They cannot achieve satisfactory fraud detection performance while protecting sensitive information from being recognized. How to simultaneously conceal sensitive information and keep helpful representations for fraud detection is yet an unsolved conundrum. Our released dataset can serve as a new benchmark, bringing new challenges for privacy-preserving algorithms."}, {"title": "6.2 Face Morphing Detection", "content": "Face morphing poses a threat to the security and reliability of face recognition systems, and thus face morphing detection becomes a focus in enhancing these systems, ensuring they remain robust against fraudulent attempts to bypass or deceive identity proofing processes. In Sec. 5.4, we discussed the utility of the face morphing task on the synthetic portrait photo dataset used in IDNet."}, {"title": "6.3 Cross-Type Analysis", "content": "As fraudsters refine their techniques and extend their operations beyond traditional boundaries, the need to study and understand these methods across various document types and states becomes imperative. Our IDNet benchmark consists of 20 different types, so it can be used to study the model performance on multiple types, e.g., how models trained on one type of identity document can generalize to other types. In this way, it will be better to understand the importance of generating diverse types of data. To showcase such analysis, we applied fraud-detection models trained on the West Virginia (WV) driver's license documents to detect the fraud driver's license documents issued by the Arizona (AZ) state, and vice versa. As shown in Tab. 10, the accuracy of the face morphing detection (using the MixFaceNet-S algorithm) drops from 98.65% to 50.55% while the model trained on the WV domain is used in the AZ domain, and drops from 98.1% to 54.1% when the model trained on the AZ type is applied to the WV type. We observed similar accuracy drops for the text replacement fraud detection models. The results showed that the fraud detection models heavily rely on type-specific features such as background patterns, transparency, size, etc. While the Portrait substitution fraud detection model is relatively more robust to type-specific changes (i.e., portrait substitution detection more relies on face features such as age, head position, etc.), significant accuracy drops (e.g., from 88.95% to 71.20% and from 88.75% to 80.05%) have also been observed.\nWe conducted more cross-type analysis. For example, in Tab. 15, we trained models using Finland ID card documents and tested the models using Spain ID card documents. Similarly, in Tab. 16, we trained models using Greece passport documents and tested the models using Albania passport and Greece passport. In Tab. 17, the models were trained on the AZ driver's license documents, but tested on Spain ID card documents and Albania passport documents. In Tab. 18, the models were trained on the Finland ID card documents, while tested using Albania passport documents and AZ Driver's license. These results indicate a performance gap exists when applying the fraud detector to a new type that is unseen during the training process. Such results showed the importance of our work, which aims to generate diverse types of data. We also hope our study can shed light on designing generalizable fraud detection algorithms in the future.\nAs mentioned, the cross-type analysis challenges were also observed in the face morphing task when applied to identity documents from different types. Our novel IDNet benchmark has IDs belonging to 20 different types. Although the photos of these IDs are from the same generated dataset [2], each ID type has a unique background, as well as different requirements and formats on the ID photo."}, {"title": "6.4 LLM-based Schema Alignment and Unification", "content": "As illustrated in Fig. 4, different types of identity documents are distinct from each other in metadata fields and the usage of security features. This leads to several challenges [60][54][55][62][61]: (1) Data management challenges. Many organizations need to manage copies of identity documents of employees, customers, and residents, who are from different states and even different countries. Without a unified schema, we can not easily perform analytics queries over diverse identity documents, e.g., to find all documents that use a security feature that was recently deprecated (because of emerging attack techniques); and to find anomaly identity documents that have duplicate document identifier numbers, which is an indication of fraud [15]. (2) Model training challenges. Machine learning techniques widely used to analyze documents and detect frauds, require abundant features to achieve acceptable accuracy. Extracting features from diverse identity documents that have heterogeneous schemas is also a tedious process that requires a lot of manual data integration effort. (3) Data exchange and interoperability challenges. Organization needs a unified schema to share data with each other.\nLimited schema alignment and metadata support were provided in today's identity document management designs. There exist tremendous international efforts to standardize the design and production of identity documents for specific document types such as a passport.. For example, ICAO 9303 (ISO/IEC 7501) [10] was published to standardize the machine-readable passport, visas, and official travel documents. However, these standards mostly focus on the design and implementation of the identity documents, such as physical characteristics, languages/characters/typeface/font, representations of fields such as issuing state, organization, nationality, place-of-birth, dates, and the constraints of the machine-readable zone. Other standards [12] defined security features, e.g., guilloche patterns, chips, holographic overlays, watermarks, ghost images, and micro-printing. However, none of these standards provide a unified electronic and structured schema to describe all meta information about an identity document. As a result, it is challenging to align and integrate different document types (e.g., the field of first name in an Arizona Driver's License maps to the field of Primer Apellido in a Spanish ID card). Lacking such cross-type schema alignment and integration capabilities leads to difficulties in cross-type queries. For example, when a security feature becomes \"insecure\" due to an \"attack\", it is hard to query which of the thousands of types of identity documents contain this"}, {"title": "6.5 ID replacement in user-defined backgrounds", "content": "One use case of the IDNet dataset is to generate mobile documents (i.e. camera captured pictures and videos that contain ID documents) with user-specified parameters, such as camera model, indoor/outdoor environment, background objects, and lighting condition. In this section, we present a simple technique to create such a dataset by replacing an ID document in an existing picture with an identity document from IDNet as shown in Figure 15(e). This necessitates ensuring that the inserted ID card appears natural and consistent within the context of the original image, which involves addressing several technical challenges: accurate detection and localization of the existing ID card, precise segmentation to isolate the ID card from the background, alignment of the new ID card to match the perspective of the original, and blending to ensure a smooth transition between the new ID card and the background. To meet these challenges, we employ a series of advanced models and image processing techniques. Grounding DINO [41], a state-of-the-art model that combines object detection and language grounding, is employed to accurately identify and localize the ID card within the background image. Following this, in Figure 15(c) the Segment Anything Model (SAM) [38] is employed to segment the detected ID card from the background. SAM uses a promptable interface to identify and provide a precise mask of the ID card, facilitating accurate alignment of the new ID image. In Figure 15(d), a perspective transformation is then applied to align the new ID image with the segmented area of the original ID card. This transformation adjusts the new ID image to fit naturally within the segmented region, ensuring correct perspective, alignment and proportion. Finally, an image blending process using Laplacian pyramids is used to merge the transformed ID image with the background image seamlessly."}, {"title": "7 Conclusions", "content": "In this study, we introduce IDNet, a vast and comprehensive benchmark dataset comprising 597, 900 synthetic identity cards across 20 distinct types, aimed at facilitating research in identity document analysis and privacy-enhanced fraud detection. Utilizing recent breakthroughs in artificial intelligence and large language models (LLMs), we developed a cost-effective methodology for generating representative document templates and identities. Additionally, we meticulously crafted a set of representative fraud patterns encompassing face morphing, portrait substitution, text field alterations, and combinations thereof that are intricately designed to intersect with personal identifier information. This intersection presents significant challenges for the development of privacy-preserving fraud detection mechanisms, as evidenced by our detailed evaluations.\nFurthermore, the IDNet dataset serves as a critical tool for benchmarking various privacy-preserving fraud detection algorithms, exploring the complexities of cross-type fraud detection. We also identified the importance of providing a unified schema for diverse identity document types.\nTo the best of our knowledge, IDNet stands as the most extensive publicly accessible synthetic dataset for identity document benchmarks to date. Looking"}]}