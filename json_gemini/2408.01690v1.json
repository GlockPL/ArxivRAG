{"title": "IDNet: A Novel Dataset for Identity Document Analysis and Fraud Detection", "authors": ["Hong Guan", "Yancheng Wang", "Lulu Xie", "Soham Nag", "Rajeev Goel", "Niranjan Erappa Narayana Swamy", "Yingzhen Yang", "Chaowei Xiao", "Jonathan Prisby", "Ross Maciejewski", "Jia Zou"], "abstract": "Effective fraud detection and analysis of government-issued identity documents, such as passports, driver's licenses, and identity cards, are essential in thwarting identity theft and bolstering security on online platforms. The training of accurate fraud detection and analysis tools depends on the availability of extensive identity document datasets. However, current publicly available benchmark datasets for identity document analysis, including MIDV-500, MIDV-2020, and FMIDV, fall short in several respects: they offer a limited number of samples, cover insufficient varieties of fraud patterns, and seldom include alterations in critical personal identifying fields like portrait images, limiting their utility in training models capable of detecting realistic frauds while preserving privacy. In response to these shortcomings, our research introduces a new benchmark dataset, IDNet, designed to advance privacy-preserving fraud detection efforts. The IDNet dataset comprises 837,060 images of synthetically generated identity documents, totaling approximately 490 gigabytes, categorized into 20 types from 10 U.S. states and 10 European countries. We evaluate the utility and present use cases of the dataset, illustrating how it can aid in training privacy-preserving fraud detection methods, facilitating the generation of camera and video capturing of identity documents, and testing schema unification and other identity document management functionalities.", "sections": [{"title": "1 Introduction", "content": "The surge in digital platforms offering remote identity proofing has escalated concerns regarding the forgery of identity documents, including passports, driver's licenses, and identity cards. The Financial Crimes Enforcement Network (Fin-CEN) reported that in 2021, around 1.6 million Bank Secrecy Act (BSA) re-ports-constituting 42% of all reports filed that year were related to identity fraud, highlighting $212 billion in suspicious transactions [7]. This issue poses risks across various sectors, including finance, healthcare, travel, retail, govern-ment, telecommunications, and gambling [15]. According to a recent industry analysis [15], fraudulent techniques have evolved from simple forgeries, such as name alterations, to the advanced use of generative AI/ML technologies for creating deceptive images, like face morphing [58]. Notably, most remote identity validation services on these digital platforms rely on images captured in white light rather than multi-spectral imaging techniques like near-infrared and ultra-violet light. This paper examines the efficacy of identity validation under these common lighting conditions, which is the focus scenario of this paper.\nDespite the availability of several public datasets for identity document analysis, which focused primarily on images taken in white light, such as MIDV-500 [19], MIDV-2019 [23], MIDV-2020 [24], FMIDV [17], BID [50], and SIDTD [21], our examination has uncovered significant limitations in these resources.\n\u2022 Limited number of distinct samples: Most existing datasets contain less than 1,000 distinct identity documents. While these datasets may help develop tools for simple tasks such as optical character recognition (OCR), they are insufficient for training and testing AI/ML models for complicated tasks such as privacy-preserving analysis. Although the BID dataset contains 28,800 distinct identity documents, the portrait photos are blurred. This characteristic makes it unsuitable for critical tasks such as detecting face morphing and portrait substitution, where clear images are essential for accurate model performance.\n\u2022 Insufficient Fraud Patterns: Only a few publicly available datasets, FMIDV [17] and SIDTD [21], which build upon the MIDV dataset, contain identity documents with fraudulent alterations. FMIDV presents a sole Copy-and-Move fraud pat-tern, where guilloche patterns are replicated and repositioned among documents. Conversely, SIDTD employs basic Crop-and-Move with inpainting techniques to simulate fraudulent activity. Nevertheless, fraud techniques such as face morphing, portrait substitution, and the intricate alteration of textual data remain unrepre-sented in these public collections. Crucially, as privacy issues take center stage in identity document management, the introduction of complex fraud patterns that intersect with extensive personal identifier information (PII), like portrait photos, ghost images, dates of birth, names, and addresses, is imperative for honing privacy-centric fraud detection methodologies. If fraud patterns were not intruding upon PII fields, redacting these fields in existing publicly available benchmarks could help preserve privacy during model training. The creation and availability of a new benchmark dataset containing representative fraud patterns are pivotal for enhancing the precision and confidentiality aspects of fraud detection in complex scenarios.\nThese constraints significantly hamper the progression of cutting-edge tech-niques for identity document analysis and fraud detection in an era increasingly dominated by AI/ML methodologies [37] [59] [18] and where privacy considera-tions are paramount in the application of these technologies [33] [20]. Overcoming these limitations is challenging because of the substantial costs of creating syn-thetic datasets that accurately mimic a wide range of real-life identity documents, which amounts to counterfeit documents. This difficulty is further compounded by the fact that one of the core objectives of organizations responsible for designing and issuing identity documents is to combat counterfeiting.\nTo address these limitations and challenges, this paper has made the following unique contributions:\n\u2022 We created and open-sourced a novel synthetically generated identity document dataset called IDNet (Sec. 3), which contains 837, 060 documents from 20 types, as illustrated in Fig. 1. Each document type has 5, 979 distinct document samples that can serve as ground truth for forging detection (i.e., forging modifies part of the document, while counterfeiting attempts to generate a fake document [12]). For each document sample, we generate six forged samples with different fraud patterns. The first two fraud patterns are included in existing datasets [21], which are (1) Crop and Move: Cropping a random field from one identity document and moving it to another document; and (2) Impaint and Rewrite: Inpainting a random field and replacing the text in the field by using a different font style or size. We then created four patterns that haven't\n\u2022 We designed and implemented a novel AI-assisted and cost-effective pipeline and methodology to generate identity documents that satisfy research requirements (Sec. 4). The pipeline uses Stable Diffusion 2.0 [65], which is free and open-sourced, to remove portrait photos and other PII infor-mation from publicly available sample identity documents (e.g., released by the Department of Motor Vehicles (DMV)) to create templates for different types of identity documents. All portrait pictures used to fill in the templates are artificially generated using AI [2]. The pipeline includes a large language model (LLM) [63], ChatGPT-3.5-turbo, to generate metadata information to fill into (9 to 21) text fields in each template, such as name, address, and DOB, based on the age, sex, and ethnicity group associated with the photo. The filling process will automatically search for font size, style, color, and filling coordinates for each field guided by Bayesian optimization. ChatGPT-3.5-turbo achieved similar performance with ChatGPT-4 on our task but required a significantly lower cost ($1.5 vs. $60 for generating 1 million tokens). The generative models selected in the pipeline meet the task requirements while satisfying the privacy, latency, and cost constraints. The entire pipeline can produce an identity document in 0.14 second on a server with dual Intel Xeon Gold 6226 24-core CPU processors, four Nvidia GeForce 2080 Ti GPUs, and 196 GB memory. Each document can be produced at an operational cost less than $0.0001. Our approach prioritizes the generation of identity documents that, while not intended for illicit use, are sufficiently authentic to support research demands. We evaluated the metadata diversity, the document fidelity (similarity to real-world documents), the fraud stealthiness (similarity to documents without frauds), and task utility for IDNet in Sec. 5.\n\u2022 We conducted evaluations to illustrate the use scenarios of the IDNet benchmark (Sec. 6). We first explored a practical application of our dataset within a privacy-preserving framework, showcasing how new opportunities and challenges arise with our new dataset. We focus on two standard privacy-preserving techniques: masking, which involves obscuring sensitive information; and Pixel-DP [40], where pixel-level perturbation based on differential privacy is applied to entire images. Our evaluation centers on assessing the impact of these techniques on fraud detection performance within our privacy-protected data. We observed a notable reduction in the effectiveness of fraud detection when employing the existing privacy-preserving methods we tested. This key finding illuminates the inherent challenges in designing privacy-preserving algorithms to balance accuracy and privacy. We also compared the accuracy of various face morphing detection algorithms, which showed that IDNet can serve as a benchmark for specific-purpose fraud detection algorithms, such as face morphing detection. We further used IDNet to investigate cross-type fraud detection (i.e., between two different document types, e.g., Arizona Driver License vs. Spain Passport) and identified an interesting research gap that fraud detection models trained on one type of document may not generalize to other types of documents, which argues for new techniques to train cross-type fraud detection models. Furthermore, we demonstrated IDNet's diverse document types enable it to serve as a benchmark for evaluating data integration algorithms. For example, we showcased an LLM-based identity document transformation pipeline that automatically converts various IDNet identity documents into a standardized schema. Most importantly, we showed that IDNet can be used as a foundation to efficiently create a large-scale synthetic identity document dataset within various camera/video capturing environments, e.g., captured by different mobile devices, with different indoor/outdoor backgrounds, and under different lighting conditions.\nBefore delving into the specifics of our technical contributions, this paper starts with a detailed survey (Sec. 2) about (1) the security features and personally identifying information (PII) that exist in various identity documents and (2) the existing publicly available identity document datasets."}, {"title": "2 Background", "content": "As illustrated in Fig. 2, an identity document usually contains (1) security features, (2) PII information, and (3) other information, which are explained as follows:\nSecurity Features that we explore in this work focus on those that are amenable to digital capture and analysis under standard white lighting conditions, including barcodes, watermarks, micro-printing, guilloche (also known as rainbow printing), distinct color schemes, unique text font, barcodes, and the machine-readable zones (MRZ).\nPersonally Identifiable Information (PII) includes but are not limited to the portrait photo, signature, barcode, family name, given name, DOB, customer identifier, cardholder address, and ghost image. First, when generating the IDNet datasets, we must not disclose any PII information from the real world. Second, given the need to develop new privacy-preserving methods that prevent fraud detection or other analysis processes from disclosing PII information, a primary goal of designing our novel identity document benchmark dataset is to facilitate such analysis by providing fraud patterns that overlap with PII fields and pose challenges for privacy-preserving fraud analysis.\nOther Information includes but is not limited to date of issue, date of expiry, document discriminator, endorsement, restrictions, date of first issue, separate"}, {"title": "2.2 A Survey of Existing Public Identity Document Datasets", "content": "Existing publicly available document datasets are designed for recognizing, clas-sifying, and restoring information from documents captured as videos or photos using mobile devices. For example, the SmartDoc dataset [25] is a publicly available document dataset. It contains a training set of 10 document samples captured as video clips (i.e., document capture samples) by a video camera. Each training sample also contains an image of the document used to produce the sample, which is considered the ground truth for comparison to the document restored from the video. In addition, SmartDoc offers a testing set of 37 document capture samples. This dataset includes only a few identity documents. The LRDE identity document image database [46] comprises 100 videos for a dozen different types of visas and passports from various countries using different environmental conditions and several kinds of smartphones.\nMIDV-500 [19] is a publicly available identity document dataset that contains 500 video clips. These video clips are produced from 50 different identity document types, including 17 types of ID cards, 14 types of passports, 13 types of driving licenses, and 6 other identity documents of various countries. Each of the 50 document types will be used in 5 different backgrounds to generate 10 video clips that last at least 3 seconds in duration using two mobile phone devices. MIDV-500 aims at facilitating simple analysis tasks like face detection, optical character recognition (OCR), and document type classification. MIDV-2019 [23] extended the MIDV-500 dataset to include four more videos with distorted identity documents and different lighting conditions for each identity document type. MIDV-2020 [24] increased the number of unique document samples to 1000 (100 unique documents for each of 10 document types).\nDatasets featuring fraudulent identity documents remain scarce. FMIDV [17] addresses this gap by introducing seven forged IDs for each sample in the MIDV-2020 dataset, focusing on guilloche-pattern fraud. To generate a forged ID, they randomly selected a few blocks that only contained guilloche patterns from one ID and copied these blocks to random locations in the blank area of another ID. FMIDV is limited to the single guilloche-related copy-and-move fraud pattern and overlooks many popular identity document fraud patterns. SIDTD [21] is the most recent extension of the MIDV-2020 dataset. It used crop-and-move and inpainting techniques to create simple frauds, containing 1222 fraud documents. Crop-and-Move will copy a text field from one document to another document. Inpainting will change the font style of the texts in a selected field, but will not change the text contents. Yet, the MIDV family has a limited number of distinct document samples, ranging from 50 to 1000, which poses challenges for AI/ML applications in achieving high accuracy.\nThe Brazilian Identity Document Dataset (BID) [50] also focuses on simple tasks such as automatic text extraction, OCR, and document classification rather than fraud detection. The BID dataset contains 28,800 document images from eight different document types. These documents are created by altering authentic documents, which introduces discrepancies between the dataset and genuine documents, notably in the blurring of portrait photos, limiting its applicability in fraud detection involving portraits."}, {"title": "3 The IDNet Benchmark Dataset", "content": "Leveraging a cutting-edge AI-assisted pipeline, outlined in Sec. 4, we have devel-oped an identity document dataset, named IDNet, which is entirely synthetically generated and devoid of any private information. This dataset encompasses a total of 837, 060 identity documents, spanning across 20 different document types. For each document type, there are 5,979 unique document samples. Each sample comprises one authentic copy alongside four fraudulent variations, which include face morphing, portrait substitution, text alteration, and a combination of these fraudulent techniques as detailed in Sec. 4.4. IDNet stands as the largest publicly accessible identity document dataset to date, as depicted in Fig. 1. Detailed statis-tics and Zenodo URLs for each document type within IDNet are systematically presented in Tab. 1.\nThe IDNet dataset is released as identity document image files along with JSON files that describe the metadata of each document. The metadata for each positive sample (w/o fraud patterns) includes the document identifier, face image ID, name, sex, date-of-birth (DOB), issuing date, and expiration date. The metadata for each fraud identity document contains additional information, such as the fraud type and fraud parameters. For portrait substitution, we recorded the identifier of the original face, and the identifier of the new face. For face morphing [58], we documented the ID and the morphing weight of each morphed face [44]. For text field replacement, we stored information such as the original and updated text content, font style, size, and color scheme.\nThe IDNet dataset is accessible publicly across various Zenodo repositories, with direct links listed in Tab. 1.\nIt is crucial to understand that the primary aim of this research is not to replicate real-world identity documents exactly. Instead, our goal is to furnish a rich and varied collection of documents that adhere to identity document design norms. These documents are intended to support the research community in conducting privacy-conscious document analysis and fraud detection. The specific aims include:\n\u2022 Diversity. We aim for the dataset's document metadata diversity (e.g., entropy) to meet or exceed that of other synthetic benchmarks like MIDV-2020. The evaluation of this attribute will be discussed in detail in Sec. 5.1.\n\u2022 Fidelity. The visual similarity of IDNet's document images to their real-world counterparts should be high, aiming for a Structural Similarity Index (SSIM) greater than 0.85. This aspect will be assessed in Sec. 5.2.\n\u2022 Stealthiness. To maintain the stealthiness of the fraudulent modifications, the structural similarity between documents with and without fraudulent modifi-cations should be nearly indistinguishable, with an SSIM greater than 0.95. This criterion will be explored in Sec. 5.3"}, {"title": "4 Generation of the Identity Document Dataset", "content": "To create the IDNet dataset, it is essential first to acquire a template for each type of identity document. However, high-quality, blank templates of real-world documents are generally unavailable. To overcome this, we utilize image generative models, such as diffusion models [34][49][56], to produce our ID templates by erasing the content from actual identity documents. Specifically, we employed the Stable Diffusion version 2.0 from Hugging Face, which is based on the Latent Diffusion Model [49], to create templates from 20 different types of real-world IDs. This model is adept at editing masked areas of an input image in accordance with text prompts. For our purposes, we mask all customizable information on the IDs as illustrated in Fig. 3 and direct Stable Diffusion with the prompt \"remove all texts/photos in the masked areas.\" Consequently, the model adeptly eliminates customized data from the document and replenishes the masked sections with appropriate backgrounds. We demonstrate this template generation technique in Fig. 3, showcasing the Arizona Driver's License as an illustrative example.\nIn instances involving complex identity documents, such as the passport from the Republic of Azerbaijan, where the information to be removed intersects with elements we wish to retain, the model faces challenges in achieving desirable outcomes without precise mask adjustments. To enhance the quality of the generated templates in these scenarios, we engage in an iterative process of applying the diffusion model with progressively refined masks and directives until a satisfactory template is achieved."}, {"title": "4.2 Metadata Information Generation", "content": "To synthesize the information to be filled into the identity document templates, several key factors were considered. First, some fields correlate with each other. For example, the sex, ethnicity group [1], age, and eye color should match with the portrait photo and the name should match the sex and ethnicity group. Second, different identity cards may have different requirements over ages, portrait photos' facing directions and facial expressions, and so on. For example, many US states have a minimum age limit for applying for driver's licenses. To address these issues, we used a well-known public face image dataset for academic research, which is also used by all benchmarks in the MIDV family. All faces in the dataset are synthetically generated. The total dataset consists of 10,000 synthetic human face images with metadata such as face landmarks, sex, emotion, ethnicity, eye color, age, facial expression, etc. Not all 10,000 photos are qualified for ID documents. To meet the age and face image requirements of many identity documents, the photos are filtered based on the following characteristics: age, wearings, facial expression, and headpose. To alleviate the manual burden to filter photos, the metadata are used to filter qualified photos from disqualified photos:(1) Age less than 18; (2) Probability of emotion \"neutral\" or \"happiness\" less than 0.8; (3) Any of the head pose attributes, i.e., \"pitch\", \"roll\", or \"yaw\", are greater than 9. After the first round filtering, the two set of photos are manually processed to obtain a final collection of qualified photos. As a result, 5,979 photos were extracted from the full collection of synthetic dataset and 4021 photos are disqualified. Tab. 2 shows the number of disqualified photos of different types. Miscellaneous type include photos with obvious artifacts or with persons wearing sunglasses or head covering. We also preprocessed the portrait photos to meet the specific requirements for photo sizes and background colors for different types of documents.\nThen, taking the document type of Arizona State Driver's License as an example, we generate metadata information using random generators and Large Language Model (LLM) such as OpenAI's ChatGPT API (i.e., we used ChatGPT version 3.5 turbo API).\n\u2022 The sex information was provided in the metadata of any face image, which is in the form of two numbers representing the probability of being a male and a female. We chose the sex by sampling the probability distribution.\n\u2022 The eye color information is generated similarly. We used the probability distribution of eye colors specified in the metadata of a face image and chose the eye color by sampling the probability distribution.\n\u2022 The height information was generated randomly based on heuristic distribu-tions. We first uniformly sampled a number in the range of [0, 1). If this number was less than 0.6, we chose the height to be 5 feet. For a female, if this number was greater than 0.6 but less than 0.9, the height was set to 6 feet; if this number was greater than 0.9, the height was set to 7 feet. For a male, if this number was greater than 0.6 but less than 0.8, the height was set to 6 feet; if this number is greater than 0.8, the height was set to 7 feet. For both males and females, an integer was uniformly sampled from the range [0, 12) to form the inch part of height.\n\u2022 The weight generation was sampled from the range of [100, 200] for female, and the range of [100, 250] for male, based on the generated height information.\n\u2022 The document discriminator (DD), which uniquely identifies a particular driver's license or ID card, is composed of digits and letters with a total length of 16 [12]. We used an algorithm to randomly sample the number of letters, the position of the letters, and the remaining digits.\n\u2022 The driver's license number (DLN) was generated by sampling a number from the range of [0, 100,000,000) without replacement for each of 5, 979 identity documents. For certain types of documents, where DLN was alphanumeric, e.g. Virginia driver's license, we adopted an approach similar to generating the document discriminator.\n\u2022 The date of birth was generated based on the age information associated with the portrait photo. We extracted age from the photo's metadata and subtract the current year by age to obtain the birth year. The month is uniformly sampled from twelve months, and then the date was uniformly sampled from the days of the month.\n\u2022 The issuing date and expiration date were generated based on the document validity period. Supposing the validity period is five years, we first uniformly sampled an integer from 0 to 5, then we subtract the current year by this integer as the year of the issue date. The year of the expiration date was set to the year of the issue date plus 5. The month and date were sampled randomly.\n\u2022 The driver license class was randomly sampled following a heuristic dis-tribution. We assumed class D is the most common class. We first uniformly sampled a number in the range of [0, 1). If the number was greater than 0.9, we uniformly sampled a class from A, B, and C. Otherwise, we chose class D.\n\u2022 The first and last names were generated using an LLM. First names were generated by querying the ChatGPT-3.5 API with the following prompt, \"Please generate 50 distinct English first names for ethnicity sex\". Here, the ethnicity and sex in curly braces were replaced by the ethnicity and sex variables extracted from the metadata of the corresponding portrait photo. Last names are generated with the following prompt, \"Please generate 50 distinct English last names for ethnicity families\". The full names were randomly selected based on the ethnicity group associated with the portrait photo. For each identity document, the names were drawn from all full name combinations without replacement. For European countries, names were generated similarly in the corresponding language. For example, to generate first names for Spanish ID documents, the following prompt was sent to ChatGPT-3.5, \"Please list 40 Spanish male given names and 40 female given names in uppercase.\" Last names were generated with the following prompt \"Please list 50 Spanish surnames in uppercase.\"\n\u2022 Addresses were also generated using ChatGPT-3.5. To guide the model to generate high-quality addresses, the following prompt was used: \"Please generate 50 fictional Arizona addresses. First, the city name was generated based on the state and postal code. Second, a street name was generated in the previously generated city. Third, a random number was generated with one, two, three, or four digits as the street number. Lastly, combine the above information in the format \"street number street name, city, state postal code\". We found that ChatGPT-3.5 struggled to generate diverse addresses. Therefore, these 50 generated addresses were used as seeds and augmented by replacing the street numbers with a random number produced by a Python script."}, {"title": "4.3 Add Generated Information to the Identity Document Templates", "content": "Once all the synthetic information was generated, it was added to the correspond-ing template. One of the challenges involved the selection of font size and style that closely approximates those found on genuine documents.\nTo optimize the generation of text overlays on genuine documents, we applied Bayesian optimization [32] to identify the best parameters that make the generated document as similar to the original as possible. The aim was to maximize the Structural Similarity Index (SSIM) [35], a metric that quantifies the visual similarity between two images by considering luminance, contrast, and structure. SSIM provides a robust measure for evaluating the quality of the generated image, ensuring it closely matches the original in terms of appearance and detail.\nIn our optimization process, we divided each sample into several segments, as illustrated in Fig. 5. For each segment, we focused on tuning several critical parameters (font style, font size, font color, font width, and position) to achieve the desired outcome. Bayesian optimization was employed to explore the parameter space efficiently. The process iteratively evaluated combinations of parameters, adjusting them to maximize SSIM. This approach allowed us to systematically and effectively enhance the visual similarity between the generated and genuine documents.\nWe also compared the results without Bayesian optimization (BO), for which we manually tuned parameters, and the results are shown in Tab. 3. We can see that BO brought a significant improvement in SSIM.\nBy using Bayesian optimization, we leveraged a probabilistic framework that adapts to the complexities of the parameter space, providing a scalable and efficient method for finding the optimal settings for text overlay. We also"}, {"title": "4.4 Fraud Patterns", "content": "Although our generated dataset is a synthetic dataset, we can consider it a \"representative\" dataset in research environments given its utility in many research tasks as detailed in Sec. 5 and Sec. 6. Therefore, it is reasonable to create forged identity documents on top of it. Subsequently, we enumerated and analyzed various prevalent fraud patterns observed in contemporary forged IDs. A pivotal component of our investigative methodology involved the generation of a batch of forged IDs, incorporating one or more of these identified patterns. The fraud patterns delineated in this study are described below:\n\u2022 Face-Morphing Fraud [58] [39]: recently emerged as a notable threat [15]. This type of fraud leverages the natural variations in human facial features over time, creating opportunities for identity deception. It operates on the premise that an individual's facial characteristics can significantly alter from those documented on their official identification. This variance enables an attacker (referred to as Person A) to misuse the identification of another individual (Person B), provided there is a sufficient resemblance between their facial features [58] [6]. To integrate this complex fraud pattern into our analysis, we adopted cutting-edge image fusion methods including Image Warp and Cross Dissolve [64]. Image wrap aligned key facial landmarks (e.g., eyes, nose, mouth, etc) of one face with those of the other, which ensures that the subsequent blending of the images appears natural and seamless. After the facial images were warped and aligned, Cross Dissolve blended them into a single cohesive image. The outcome is a synthesized facial image that encapsulates the likeness of both input face images concurrently, attaining a level of confidence significant enough to pass visual scrutiny as authentic. To achieve high-quality morphing as suggested by the NIST face morphing report [44], only the face area of the facial images was averaged after alignment and feature warping. In addition, the face area was adjusted to the face color histogram of the first input facial image. For each of the 5979 artificially generated photos, we morphed it with another randomly selected photo of the same ethnicity and sex, as illustrated in Fig. 6. We set the blending factor as 0.5 in the face morphing process following the NIST face morphing tie-2 implementation [44], which suggests that both faces contribute equally to the morphed face.\n\u2022 Portrait Substitution Fraud [14][15][42]. Based on industrial studies [15], a majority of digital identity document attacks in online platforms in 2023 are at low forging costs. Portrait substitution is one example, which is to use disqualified digital photos, e.g., photos taken using mobile phones or computer cameras and do not meet photo standards required by the corresponding identity document. To implement this type of fraud, for each identity document, we uniformly sampled one photo from the 4,021 portrait photos identified as disqualified when we preprocessed the 10,000 synthetic portrait photos (i.e., the preprocessing procedure is described at the beginning of Sec. 4.2). We then used that sampled photo to replace the original photo to produce a forged identity document, as illustrated in Fig. 7.\n\u2022 Text-Field Replacement Fraud [14][15]. Replacing text field/(s) often leads to subtle alteration in the text font styles, text font size, and background color of PII fields. Counterfeited/forged IDs manipulate specific PII fields, with common targets being the first and last names, sex, DOB, expiration data, etc. To integrate this fraud pattern into our dataset, we randomly changed the font style, font size, and the contrast and saturation of PII fields. This manipulation simulated the changes in the text fields commonly observed in forged IDs [15]. We further classified this fraud into two levels as illustrated in Fig. 8: (1) easy-level, where the replacing text fields' information (e.g., name, DOB) does not match the portrait photo's sex and age, which is relatively easier to detect; and (2) hard-level, where the forged information is consistent with the rest of the identity document information. The ratio of easy-level fraud was determined to be around 65%, and the rest were all hard-level frauds, to be consistent with a recent industry survey [15]. Such classification can be leveraged to evaluate anti-fraud methods to ensure they perform comparisons between image and text data, e.g., age verification [3].\n\u2022 Mixed Fraud Pattern For each distinct identity document, we created an additional fraud sample that mixed fraud patterns through the following steps. First, the text replacement frauds was applied to the PII fields. Subsequently, a random variable was sampled to decide whether to apply a face morphing or portrait substitution fraud to the portrait photo of the document. This dataset can be used to test the integration of specialized fraud detection algorithms, each focusing on one fraud pattern.\n\u2022 Inpaint and Rewrite Fraud Pattern This fraud technique was utilized in SIDTD [21] to create counterfeit samples of IDs. It is similar to our earlier described text-field replacement fraud except for several differences: (1) the fraud generation process of this fraud pattern has applied a customized mask to a randomly selected field for inpainting each time; (2) the font style for the replacement text is chosen randomly from the fonts available in SIDTD; and (3) this pattern will not change the background color scheme and text content. Figure 9 illustrates the Inpaint-and-Rewrite pattern as described.\n\u2022 Crop and Replace Fraud Pattern This fraud pattern, also utilized in SIDTD [21], facilitates the exchange of information between IDs of the same class. In this method, PII field is selected randomly from one ID and then cropped and replaced with the PII field of another ID. In both cases, the PII fields are selected randomly, with a 95% probability that the same PII field is chosen in both IDs and a 5% probability that different PII fields are selected. To ensure a slight variation, a small shift is added when replacing the cropped area of one ID with the other. This shift is randomly selected from a defined range and applied to both the x-axis and y-axis. To avoid perfect alignment between the two IDs, the shift value of 0 is omitted. This ensures there is always a shift, creating a slight distortion in the background due to texture discontinuity. Figure 10 illustrates the Crop-and-Replace pattern as described."}, {"title": "4.5 Time and Monetary Costs", "content": "We measured the time and dollars spent at every pipeline stage, as illustrated in Tab. 5. The pipeline ran on a system with dual Intel Xeon Gold 6226 CPUs at 2.70GHz with 24 cores each, four Nvidia GeForce 2080 Ti GPUs, and 196 GB memory to produce the identity documents. We estimate its cost to be $2 per hour based on the costs of AWS EC2 on-demand instances of similar capabilities [4]. The stable diffusion 2.0 model is free and open-sourced. The ChatGPT-3.5-turbo API costs $0.5 for a million input tokens and $1.5 for a million output tokens. We used 1860 user input tokens and 60, 652 output tokens. While generating the metadata information, we spent 410 seconds to generate sample names and addresses for 20 different countries/states and 145 seconds to derive all metadata information. All generated documents are archived using the free Zenodo service. As a result, the operational cost for producing each identity document is lower than $0.0001 with a latency of 0.14 second. It demonstrated the cost-effectiveness of the proposed pipeline and the great potential of using the pipeline to generate large-scale synthetic identity document datasets programmatically using different parameters with low time and monetary costs. In the future, we will integrate"}, {"title": "5 IDNet Quality Evaluation", "content": "The objective of this research is not to provide a set of documents that exactly resemble corresponding real-world identity documents. Instead, we attempt to provide a comprehensive and diverse set of meaningful documents that follow most identity document design standards and meet the research requirements for"}, {"title": "5.1 Metadata Quality", "content": "In this section", "aspects": "uniqueness of text fields such as document ID number and personal ID number; diversity of text fields such as gender", "Uniqueness": "In IDNet", "668174749\" occurred twice in the Finnish ID documents.\n2. Diversity": "We used entropy to measure diversity. Larger entropy indicates more diversity in data [16", "26": "."}]}