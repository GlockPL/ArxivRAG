{"title": "LE-PDE++: Mamba for accelerating PDEs Simulations", "authors": ["Aoming Liang", "Zhaoyang Mu", "Qi Liu", "Ruipeng Li", "Mingming Ge", "Dixia Fan"], "abstract": "Partial Differential Equations (PDEs) are foundational in modeling science and natural systems such as fluid dynamics and weather forecasting. The Latent Evolution of PDEs (LE-PDE) method is designed to address the computational intensity of classical and deep learning-based PDE solvers by proposing a scalable and efficient alternative. To enhance the efficiency and accuracy of LE-PDE, we incorporate the Mamba model(LE-PDE++)-an advanced machine learning model known for its predictive efficiency and robustness in handling complex dynamic systems with a progressive learning strategy. The LE-PDE++ was tested on several benchmark problems. The method demonstrated a marked reduction in computational time compared to traditional solvers and standalone deep learning models while maintaining high accuracy in predicting system behavior over time. Our method doubles the inference speed compared to the LE-PDE while retaining the same level of parameter efficiency, making it well-suited for scenarios requiring long-term predictions.", "sections": [{"title": "1 Introduction", "content": "Partial differential equations (PDEs) play a pivotal role in scientific and engineering fields, modeling the dynamic evolution of complex systems over time. These equations are indispensable for both forward prediction and reverse optimization, making them essential tools across various disciplines; these include weather forecasting [Holm-strom et al., 2016], jet engine design [Y\u00fcksel et al., 2023], nuclear fusion [Pavone et al., 2023], laser-plasma interaction [D\u00f6pp et al., 2023], and physical simulations [Wu et al., 2022].\nWhen tackling real-world challenges in science and engineering, the number of cells per time step can easily reach millions or more. This complexity presents a significant obstacle for traditional PDE solvers, which struggle to deliver rapid solutions at such scales. Moreover, inverse optimization tasks, such as inferring system parameters, encounter similar challenges, compounding the difficulties of modeling forward evo-lution [Biegler et al., 2003]. In response to these limitations, numerous deep learning-based models have emerged, offering the potential to accelerate the solving of partial differential equations by orders of magnitude often achieving speeds 10 to 1000 times faster by Fourier Neural Operator(FNO) [Li et al., 2020a].\nDeep learning-based surrogate models: In the research of surrogate models, there are three broad classes: pure data-driven, physical information-driven, and hybrid. In the data-driven method, finite operator models that depend on grids have shown signif-icant promise. [Raissi, 2018, Rosofsky et al., 2023, Sirignano and Spiliopoulos, 2018, Guo et al., 2016, Khoo et al., 2021]. infinite operator models show a good potential over geometry and sampling [Li et al., 2020b,c, Lu et al., 2021]. For the physical infor-mation methods, researchers primarily incorporate information from known physical equations into the network through two main methods: hard constraints [Chalapathi et al., 2024] and soft constraints[Ki\u010di\u0107 et al., 2023]. These approaches ensure that the network adheres closely to established physical laws, enhancing the reliability and ac-curacy of the predictions.\nHowever, the above methods still face the following challenges:\n1.  The models rely on an end-to-end mapping structure, and using CNNs as the base model leads to significant convolutional computation time, resulting in increased time complexity.\n2.  The training mechanisms of these models are not yet well-defined, and no effi-cient learning approach enables the model to learn how to learn effectively.\nTo address the first issue, we draw on the LE-PDE [Wu et al., 2022] and the Mamba[Gu and Dao, 2023] model, utilizing latent space for rapid inference. This al-lows us to maintain several parameters while improving computational speed. We intro-duce a progressive learning mechanism to tackle the second issue, enabling the model to adapt and improve over time. To differentiate from the LE-PDE, it will refer to the enhanced model as LE-PDE++ in the following discussion."}, {"title": "2 Related Work", "content": "In recent years, significant efforts have been devoted to addressing the challenges above. Calder and Yezzi [2019] from the perspective of numerical analysis, accelerat-ing the solution of PDE in momentum-based methods, such as Nesterov's accelerated"}, {"title": "3 Preliminaries", "content": "The LE-PDE model architecture comprises four key components in the Appendix ??:\nq : dynamic encoder: $z^k = q(U^k)$\nr: static encoder: $z_p = r(p)$\ng : latent evolution model: $z^{k+1} = g(z^k, z_p)$\nh : decoder: $\\hat{U}^{k+1} = h(z^{k+1})$\nLE-PDE utilizes the temporal bundling technique ([Brandstetter et al., 2022b]) to enhance the representation of sequential data. This approach involves grouping input states Uk across a fixed interval S of consecutive time steps. Consequently, each latent vector zk encodes these states bundle, and latent evolution predicts the next Zk+1 for the subsequent S steps. The parameter S, a hyperparameter, is adaptable to the specific problem, and setting S = 1 results in no bundling. The autoregressive output $\\hat{U}^{t+m}$ is defines as:\n$\\hat{U}^{t+m} = h (z^{t+m})$\n$= h\\big(g\\big((g(:,r(p))^{(m)} \\circ z^t\\big)$\\big)\n$= h \\big(g(:,r(p))^{(m)} \\circ q (U^t)\\big)$\nThe training loss is as follows:\n$L = \\frac{1}{K} \\sum_{k=1}^{K} (L_{multi-step} + L_{recons} + L_{consistency})$\nWhere\n$L_{multi-step} = \\sum_{m=1}^{M} l(\\hat{U}^{k+m},U^{k+m})$,\n$L_{recons} = l(h(q(U^k)),U^k)$,\n$L_{consistency} = \\sum_{m=1}^{M} \\frac{|lg(:, r(p)^{(m)} \\circ q(U^k) \u2013 q(U^{k+m}) ||}{||q(U^{k+m})||_2}$\nHowever, existing LE-PDE suffer from several drawbacks:\n1.  Slow in training and inference stage Although Le-PDE effectively compresses information to the latent space by the dynamic encoder, the forward evolution in the latent space is progressive, making multi-step prediction significantly time-consuming.\n2.  Continuity conflict predication in the training objective It is because the multi-step bundling strategy in the training objective function and the continu-ity of losses may conflict in highly non-linear problems, as the former loss term ensures the model's multi-step prediction capability, the latter requires that per-petuation in the latent space are not too large."}, {"title": "4 Our LE-PDE++ Framework", "content": "This section provides a detailed explanation of the LE-PDE++ method and the progres-sive learning approach. Figurel outlines the model's architecture.\n4.1 Mamba for latent evolution\nLE-PDE++ utilizes the Mamba model in the latent space to transform the com-pressed latent vector. Specifically, the transformation is defined as follows:\n$h_t = \\bar{A}h_{t-1}+ \\bar{B}x_t$\n$y_t = C h_t$\n$K = (CB,\\bar{C}\\bar{A}B,...,\\bar{C}\\bar{A}^nB,...)$\n$y = x * K$\nwhere A is defined as exp(\u2206A), with A being a learnable sampling rate parameter, and B is given by (\u2206\u0391)-1(exp(\u2206\u0391) \u2013 \u0399)\u00b7 \u0394\u0392, where \u2206 also represents the learnable sampling rate parameter. ht is the latent vector in the time t. K is a causal convolution operator. Yt is the output of Mamba. We make the following assumptions here:\n\u2022  Linear time invariance: Most systems exhibit linear characteristics in a low-dimensional space, which was not present in the original LE-PDE model.\n\u2022  Acceleration of model performance, which is significant for algorithms in au-toregressive architectures\n4.2 Progressive sampling policy as the learning objective\nThe idea of progressive sampling originates Provost et al. [1999], Bengio et al. [2015], Wang et al. [2021], and this paper replaces long-term prediction sequences in the loss function by gradually adjusting the non-masked (i.e., model-visible) portion ratio.\n1.  **Linear Growth**:\n$r(n) = min \\Big(1, \\tau_0 + (1 - \\tau_0) \\frac{n}{N}\\Big)$,\nwhich represents a linear growth model. Additionally, we have also designed mod-els with logarithmic and polynomial growth rates for comparison:\n2.  **Polynomial Growth**:\n$r(n) = min \\Big(1, \\tau_0 + (1 - \\tau_0) {(\\frac{n}{N})}^p\\Big)$,\nWhere p controls the polynomial growth rate.\n3.  **Logarithmic Growth**:\n$r(n) = min \\Big(1, \\tau_0 + (1 - \\tau_0) log (1 + \\frac{n}{N})\\Big).$"}, {"title": "5 Experiments", "content": "In this study, we aim to compare and analyze the performance of LE-PDE++ and base-line methods in the domain of PDE simulations. We seek to address the following research questions:\n1.  Does the Mamba model truly accelerate the forward inference in the latent space?\n2.  What is the impact of different asymptotic sampling methods, and which asymp-totic strategy is more universal?\n3.  Can our approach surpass state-of-the-art (SOTA) methods while maintaining similar parameters?\nTo address these questions, we will evaluate the models using two main aspects: To address these questions, we will evaluate the models using two main aspects:\n\u2022  Quality in Prediction Accuracy: This is measured by single-step root mean squared error (RMSE) and global RMSE.\n\u2022  Inference time: This is assessed by the speed of inference over global inference.\n5.1 Navier-Stokes Equation Datase (NS)\nThe Navier-Stokes equations are fundamental in various scientific and engineering dis-ciplines, encompassing applications such as weather forecasting, aerospace engineer-ing, and hydrodynamics. The simulation of these equations becomes increasingly com-plex in the turbulent regime, characterized by intricate multiscale dynamics and chaotic behavior."}, {"title": "5.2 Shallow Water Equation (SWE)", "content": "The Shallow Water Equations describe phenomena related to large-scale ocean wave motions.\n$\\frac{\\partial u(x, y, t)}{\\partial t} = -g \\frac{\\partial \\eta(x, y, t)}{\\partial x}$\n$\\frac{\\partial v(x, y, t)}{\\partial t} = -g \\frac{\\partial \\eta(x, y, t)}{\\partial y}$\n$\\frac{\\partial \\eta(x, y, t)}{\\partial t} + \\frac{1}{H} (\\frac{\\partial u(x, y, t)}{\\partial x} + \\frac{\\partial v(x,y,t)}{\\partial y}) = 0$\nWhere u(x, y, t) represents the horizontal velocity component of the fluid at position (x, y) and time t, v(x, y, t) is the vertical velocity component of the fluid at the same position and time, and n(x, y, t) denotes the surface elevation of the fluid at position (x, y) and time t. The parameter g = 1 represents the dimensionless gravitational acceleration, and H = 100 is the water depth. x \u2208 (0,1)2, t \u2208 (0,1], the spatial resolution is 128 x 128.\nIn this study, we solve these equations for 100 trajectories, where 70 are used for training and 30 for testing."}, {"title": "5.3 Pollutant Transport Equation (PTE)", "content": "The following diffusion equation governs the pollutant dispersion:\n$\\frac{\\partial C(x,t)}{\\partial t} = D\\nabla^2C(x,t) + S(x,t)$\nWhere C(x, t) represents the pollutant concentration at position x and time t, D is the diffusion coefficient, \u22072 is the Laplace operator, and S(x, t) is the source term. The numerical simulations are performed on a grid with dimensions 512\u00d7512, representing a spatial domain of 3 km \u00d7 3 km. This grid resolution allows for detailed modeling of the pollutant dispersion process. The time step used in the simulation is 5 s, and we select 21 time steps for our analysis. The data set utilized for the simulations is sourced from Alibaba's Tianchi.\nThe dataset comprises 121 simulated cases, each with randomly placed pollution sources and four wind directions. We randomly select 100 cases for the training set and 21 cases for the testing set."}, {"title": "5.4 Ablation and Comparison Results", "content": "Ablation experiments primarily measure whether adding the Mamba model acceler-ates the processing time of the LE-PDE method. Comparative experiments demon-strate that our approach offers certain advantages over four baseline methods: FNO, WNO[Navaneeth et al., 2024], UNO[Azizzadenesheli et al., 2024], and the original LE-PDE. The specific settings of these baseline methods are detailed in the appendix. The following experiments were conducted on NVIDIA TESLA V100.\nFrom Table1, the LE-PDE++ model, which replaces the evolution model with Mamba, shows a twofold increase in inference speed. However, the RMSE reaches 0.31. Note that in this experiment, progressive sampling was not used. We analyzed the issue and concluded that while the model's linear inference speed is very fast, it performs poorly on nonlinear datasets. To address this issue, we need to introduce pro-gressive sampling, gradually allowing the model to learn to predict over longer time horizons.\nTo address the issue of not using progressive sampling, we designed several exper-iments with initial values of 0.1, 0.3, 0.5, 0.7, 0.9, and 1, each corresponding to three different progressive strategies. It is worth noting that in practical applications, r(n) is set to an integer."}, {"title": "6 Conclusion", "content": "In this work, we have introduced the LE-PDE++ framework, a novel surrogate model-ing approach that replaces the traditional evolution model with the Mamba model. This enhancement has effectively doubled the inference speed compared to the original LE-PDE. Furthermore, we have introduced the concept of progressive sampling, which enables the model to extend its predictive capabilities over longer dynamic behav-iors-marking a pioneering attempt in surrogate modeling. Future work will explore and address the inherent uncertainty aspects of the model. Additionally, our method has demonstrated a significant improvement in performance on complex datasets such as PTE, achieving speeds 4 to 15 times faster than baseline methods."}]}