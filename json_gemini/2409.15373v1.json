{"title": "Enhancing Performance and Scalability of Large-Scale Recommendation Systems with Jagged Flash Attention", "authors": ["Rengan Xu", "Junjie Yang", "Yifan Xu", "Hong Li", "Xing Liu", "Devashish Shankar", "Haoci Zhang", "Meng Liu", "Boyang Li", "Yuxi Hu", "Mingwei Tang", "Zehua Zhang", "Tunhou Zhang", "Dai Li", "Sijia Chen", "Gian-Paolo Musumeci", "Jiaqi Zhai", "Bill Zhu", "Hong Yan", "Srihari Reddy"], "abstract": "The integration of hardware accelerators has significantly advanced the capabilities of modern recommendation systems, enabling the exploration of complex ranking paradigms previously deemed impractical. However, the GPU-based computational costs present substantial challenges. In this paper, we demonstrate our development of an efficiency-driven approach to explore these paradigms, moving beyond traditional reliance on native PyTorch modules. We address the specific challenges posed by ranking models' dependence on categorical features, which vary in length and complicate GPU utilization. We introduce Jagged Feature Interaction Kernels, a novel method designed to extract fine-grained insights from long categorical features through efficient handling of dynamically sized tensors. We further enhance the performance of attention mechanisms by integrating Jagged tensors with Flash Attention. Our novel Jagged Flash Attention achieves up to 9x speedup and 22x memory reduction compared to dense attention. Notably, it also outperforms dense flash attention, with up to 3x speedup and 53% more memory efficiency. In production models, we observe 10% QPS improvement and 18% memory savings, enabling us to scale our recommendation systems with longer features and more complex architectures.", "sections": [{"title": "1 Introduction", "content": "Categorical features, such as user-clicked items within the last month, are heavily relied upon by ranking models [5, 6, 10-12]. Unlike dense (float) features, which maintain a fixed size across training samples, the length of categorical feature values can vary among different training samples. Padding has traditionally been used to standardize the sizes of these categorical features across various training samples within a batch [7, 8]. However, while padding can be sufficient in some cases, it has inherent drawbacks. These are particularly noticeable with long length categorical inputs, a common input format for large, complex models trained on GPUs. Padding can introduce significant overhead, leading to increased memory usage, computational demands, and communication overhead. This not only affects the model's efficiency but also hampers scalability, particularly in environments with limited resources.\nIn this paper, we present our efforts in designing and adopting an efficiency-driven approach in the exploration of computationally expensive ranking paradigms. This shift marks a departure from the conventional approach of relying solely on the combination of native PyTorch modules to achieve algorithmic logic. In the rest of the paper, we will discuss the challenges, the methodologies and the lessons learned from our journey. By sharing our experiences, we aim to contribute to the collective knowledge base of the RecSys community, empowering fellow researchers and practitioners to navigate similar challenges in their pursuit of innovation."}, {"title": "2 Methodology", "content": "We propose Jagged Feature Interaction Kernel, an innovative method tailored for extracting fine-grained insights from long categorical"}, {"title": "2.1 Jagged Flash Attention", "content": "The flash attention [1, 2] is the state-of-the-art algorithm for accelerating the standard attention. Its core idea is to fuse separate attention operations into a single kernel, minimizing data movement between GPU shared memory and global memory, and maximizing computations within the fast shared memory. Similar to the classic matrix multiplication optimization, flash attention employs tiling optimization to perform two GEMM operations and one softmax block by block. However, applying softmax independently to each block poses a challenge, as it requires the sum of exponentials in the denominator, which depends on information from later blocks. To overcome this challenge, it leverages the online softmax algorithm [4], adjusting results for later blocks as new information becomes available. The flash attention optimization could be applied in both dense and jagged attention. To achieve the best performance and maximize the memory saving, we have combined both jagged tensor and flash attention into jagged flash attention optimization."}, {"title": "2.2 Triton Kernels for Jagged Tensor", "content": "We built customized Triton kernels [9] for both forward and backward computations for Jagged tensor operations. Triton is the programming paradigm based on blocked algorithms which can facilitate the construction of high-performance compute kernels for neural networks and allow compilers to aggressively optimize programs for data locality and parallelism. Specifically, we build\n\u2022 Jagged Tensor (sparse) multiply Jagged Tensor (sparse)\n\u2022 Jagged Tensor (sparse) multiply Dense Tensor (dense)"}, {"title": "3 Experiments and Conclusion", "content": "Table 1 shows the performance comparison between the custom Triton and the native PyTorch implementations for selective kernels. We demonstrate the relative improvement of Triton over Py-Torch in parenthesis. It can be seen that the jagged operators reduce the FLOPs and memory usage significantly and outperform the dense version accordingly.\nWe also compared different attention implementations with BF16 in Figure 2. The jagged attention mechanism offers significant speedup and memory efficiency improvements over dense attention. Specifically, jagged attention achieves up to 2x speedup compared to dense attention, while jagged flash attention further improves this to up to 9x. Even when compared to dense flash attention, jagged flash attention still offers up to 3x speedup. In terms of memory usage, jagged attention is up to 3.5\u00d7 more efficient than dense attention, while jagged flash attention reduces memory usage by up to 22x. Notably, the memory usage for both dense and jagged flash attention increases linearly rather than quadratically, with jagged flash attention being up to 53% more memory efficient. These improvements have practical implications for end-to-end model training, where we have observed approximately 10% QPS improvement and 18% memory savings for production models. This enables us to scale our recommendation systems further, accommodating longer features and more complex model architectures."}]}