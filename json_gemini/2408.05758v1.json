{"title": "VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing", "authors": ["Chunyu Qiang", "Wang Geng", "Yi Zhao", "Ruibo Fu", "Tao Wang", "Cheng Gong", "Tianrui Wang", "Qiuyu Liu", "Jiangyan Yi", "Zhengqi Wen", "Chen Zhang", "Hao Che", "Longbiao Wang", "Jianwu Dang", "Jianhua Tao"], "abstract": "Deep learning has brought significant improvements to the field of cross-modal representation learning. For tasks such as text-to-speech (TTS), voice conversion (VC), and automatic speech recognition (ASR), a cross-modal fine-grained (frame-level) sequence representation is desired, emphasizing the semantic content of the text modality while de-emphasizing the paralinguistic information of the speech modality. We propose a method called \"Vector Quantized Contrastive Token-Acoustic Pre-training (VQ-CTAP)\", which uses the cross-modal aligned sequence transcoder to bring text and speech into a joint multimodal space, learning how to connect text and speech at the frame level. The proposed VQ-CTAP is a paradigm for cross-modal sequence representation learning, offering a promising solution for fine-grained generation and recognition tasks in speech processing. The VQ-CTAP can be directly applied to VC and ASR tasks without fine-tuning or additional structures. We propose a sequence-aware semantic connector, which connects multiple frozen pre-trained modules for the TTS task, exhibiting a plug-and-play capability. We design a stepping optimization strategy to ensure effective model convergence by gradually injecting and adjusting the influence of various loss components. Furthermore, we propose a semantic-transfer-wise paralinguistic consistency loss to enhance representational capabilities, allowing the model to better generalize to unseen data and capture the nuances of paralinguistic information. In addition, VQ-CTAP achieves high-compression speech coding at a rate of 25Hz from 24kHz input waveforms, which is a 960-fold reduction in the sampling rate. The audio demo is available at https://qiangchunyu.github.io/VQCTAP/.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning has brought significant improvements to the field of cross-modal representation, leading to the emergence of numerous methods for representing connections between text, images, and speech [1]\u2013[8].\nContrastive learning has proven to be an effective method for cross-modal modeling. It works by differentiating target samples (positive) from distractor samples (negatives) based on an anchor representation. The objective is to maximize the similarity between the anchor and positive samples while minimizing the similarity between the anchor and negative samples. This approach has been widely applied in text-image cross-modal representation, with notable examples including OpenAI's CLIP [1], Florence [2], and ALIGN [3].\nIn the field of text-audio cross-modal representation, CLIP-based models have also been developed, including Wav2CLIP [4], AudioCLIP [5], and CLAP [6]. In the field of multimodal representation, pre-trained representation models have emerged that use a single modality as a link to connect other modalities. For example, ImageBind [7] achieves joint modeling of multiple modalities based on the image modality, while LanguageBind [8] achieves joint modeling of multiple modalities based on the language modality. These models focus on extracting global descriptive information; for instance, CLAP aims to enhance performance in downstream audio classification tasks. However, their emphasis on global information limits their applicability to tasks requiring fine-grained sequence generation and recognition.\nAs shown in Figure 1, the speech processing tasks of TTS and ASR are typical cross-modal applications. For tasks such as TTS, VC and ASR, a fine-grained (frame-level) sequence cross-modal representation(semantic coding) between text-speech modalities is expected. The intermediate representation extracted from speech should serve as a \"bridge\" between text and acoustic information. It should emphasize linguistic content while de-emphasizing paralinguistic information such as speaker identity and acoustic details. Self-supervised representation learning methods, such as Wav2Vec2.0 [9], Wav2Vec-C [10], VQ-Wav2Vec [11], HuBERT [12], and W2V-BERT [13], have become mainstream in the speech representation field. These methods offer the prospect of a universal model that can benefit a wide range of tasks and domains. However, when applied to VC and TTS tasks, such as SPEAR-TTS [14], these methods encounter issues of redundancy and dimensionality explosion. This is because the lack of explicit text supervision in these methods results in learned representations that are not fully decoupled semantic information. Supervised frame-level representation learning methods aim to explicitly introduce text information as supervision. For example, Phonetic Posteriorgrams (PPGs) [15] are calculated based on ASR acoustic models. Although PPGs have been widely used in various speech processing tasks, they are essentially text information, and the frame-level features are independent of each other, lacking the integration of contextual semantic information. Moreover, these methods rely on expensive annotated text-speech pairs. In this case, the model's representational capacity is limited to the distribution of the text-speech paired data, resulting in poor robustness for unlabeled speech-only data (e.g., different styles, environmental sounds, channels, or qualities).\nIn our previous three works: 1) We identified issues of information redundancy and dimensional explosion in existing semantic coding methods [16]. 2) We proposed a semantic coding method called \"Contrastive Token-Acoustic Pretraining (CTAP) [17].\" 3) Building on CTAP, we developed a minimally-supervised high-fidelity speech synthesis method [18]. Building on these foundational works, we now introduce VQ-CTAP, which uses the cross-modal aligned sequence transcoder to bring text and speech into a joint multimodal space, facilitating the connection between text and speech at the frame level. Our contributions in this paper are as follows:\n1.  We propose a cross-modal sequence representation learning paradigm and introduce the VQ-CTAP model trained on 900 hours of speech-text pairs and 20,000 hours of speech-only data. We design a stepping optimization strategy to ensure effective model convergence by gradually injecting and adjusting the influence of various loss components.\n2.  We propose a semantic-transfer-wise paralinguistic consistency loss, which uses unlabeled data to enhance representation capabilities while improving the semantic-paralinguistic decoupling ability of the representations. This loss function allows the model to better generalize to unseen data and capture the nuances of paralinguistic information.\n3.  We propose a sequence-aware semantic connector, enabling the pre-trained VQ-CTAP to be directly used for TTS task without fine-tuning. Additionally, the pre-trained VQ-CTAP can be directly applied to VC and ASR tasks without fine-tuning or adding additional structures, exhibiting a plug-and-play capability.\n4.  We achieve high-compression speech coding by generating discrete embedding from a single codebook at a rate of 25Hz from 24kHz input waveforms, which is a 960-fold reduction in the sampling rate. Furthermore, it can effectively improve the prediction stability when applied to autoregressive structures in TTS."}, {"title": "II. RELATED WORK", "content": "Self-supervised speech representation learning with deep neural networks emerge two training schemes, the autoregressive models [19], [20] and bidirectional models [9], [12], [21]\u2013[24]. The autoregressive models learn to predict the future discrete representations of audio segments based on past observations. The bidirectional models mainly adopt masked language modeling (MLM) [25], which recovers the masked part of the discrete inputs using the unmasked context. These descrete inputs can be derived from the audio segments [9], [21]. Following the MLM training scheme for speech representation learning, HuBERT [12] proposed to learn the masked prediction of hidden units generated by vanilla acoustic unit discovery systems. BEST-RQ [26] learn a model to predict the masked labels of the speech signals generated with a random-projection quantizer. WavLM [24] jointly learn masked speech prediction and denoising in pre-training to solve full-stack downstream speech tasks.\nContrastive learning has proven to be an effective method for cross-modal modeling. It works by differentiating a target sample (positive) from distractor samples (negatives) based on an anchor representation. The objective is to maximize the similarity between the anchor and positive samples while minimizing the similarity between the anchor and negative samples. This approach has been widely applied in the field of computer vision, with notable examples such as Open Al's CLIP [1], Florence [2], and ALIGN [3].\nIn the audio field, CLIP-based models have also been developed, including Wav2CLIP [4], AudioCLIP [5], and CLAP [6]. These models focus on extracting global descriptive information from audio, with the primary goal of improving the performance of downstream audio classification tasks. However, their emphasis on global information limits their applicability to fine-grained generation and recognition tasks such as TTS, VC, and ASR, which require frame-level representations.\nThe TTS task refers to the modelling of unequal length sequences between text to speech. As shown in Figure 1, minimally-supervised TTS refers to splitting the TTS task into two tasks (text-to-semantic and semantic-to-acoustic) by combining speech intermediate representations. As deep learning advances, TTS technology has made significant progress. Traditional TTS methods have achieved satisfactory results [27]\u2013[32]. The introduction and subsequent evolution of generative pre-trained transformers (GPT) models [33], [34], have significantly heightened interest in the development of large-scale TTS systems. These TTS systems can be broadly divided into two categories: 1) autoregressive frameworks [14], [35]\u2013[37] and 2) non-autoregressive frameworks [38]\u2013[40].\nThe VC task aims to modify the voice of a source speaker to match a target style, including aspects such as speaker identity, prosody, and emotion, while preserving the original linguistic content. VC techniques can be broadly classified based on their approach to content disentanglement into two categories: text-based VC and text-free VC. Text-based VC often employs an ASR model to derive PPGs as a representation of content [15], [41], [42]. In contrast, text-free VC approaches aim to bypass the need for text annotations by directly learning content representations [43]\u2013[45].\nThe ASR is the inverse task of TTS, aims to predict text from speech. ASR is moving towards speech representation learning to make full use of the large-scale unsupervised speech datasets [12], [23], [24], [26]. The MLM [9], [21] based self-supervised learning machanism learn discrete masked speech from outside the target domain. The phoneme and speech representations are concatenated in a joint multimodal space. A contrastive learning approach is used to learn this space by capturing the frame-level (dis)similarity between speech and phoneme pairs. The speech encoder produces discrete embedding at a rate of 25Hz from the 24kHz input waveform, which is a 960-fold reduction in the sampling rate. Vector quantization further removes paralinguistic information irrelevant to semantics from the speech representations. The two decoders adapt the learned representations for downstream tasks such as TTS, VC, and ASR. The method supports both supervised and unsupervised training. To enable domain-enhanced representation learning, semantic-transfer-wise paralinguistic consistency loss is introduced, computed using the unlabeled random speech from outside the target domain. The pre-trained VQ-CTAP can be directly applied to TTS, VC, and ASR tasks without fine-tuning, exhibiting a plug-and-play capability."}, {"title": "III. METHOD", "content": "As illustrated in Figure 2, the main body of VQ-CTAP_is the cross-modal aligned sequence transcoder, which comprises five main components: a speech encoder, a phoneme encoder, a prompt encoder, a phoneme decoder, and a speech decoder. A length regulator is employed to address the mismatch between the lengths of phoneme sequences and speech sequences. The input consists of text-speech pairs and unlabeled random speech Rin from outside the target domain is randomly selected, where \\(S_{in} \\in \\mathbb{R}^{B\\times T_{r}\\times D_{s}}\\). The batch size B and the number of spectral components Ds are consistent with those of \\(S_{in}\\), while typically \\(T_{r}\\neq T_{s}\\). The \\(P_{in}\\), \\(S_{in}\\) and \\(R_{in}\\) are then passed through a phoneme encoder and a speech encoder, respectively:\n\\[P = PhonemeEncoder(P_{in});\\]\n\\[S = SpeechEncoder(S_{in});\\]\n\\[R = SpeechEncoder(R_{in})\\]\n(1)\nwhere \\(P\\in \\mathbb{R}^{B\\times T_{s}/4\\times d}\\)are the phoneme representations, \\(S\\in \\mathbb{R}^{B\\times T_{s}/4\\times d}\\) are the target speech representations, and \\(R\\in \\mathbb{R}^{B\\times T_{r}/4\\times d}\\) are the random speech representations. The two encoders compress the lengths of the phoneme and speech representations by a factor of 4, respectively. We bring S, R, and P into a joint multimodal space of dimension d.\nThe prompt speech \\(G_{in}\\) is randomly clipped with a window of length 3 seconds at each step as the input to the prompt encoder:\n\\[G = PromptEncoder(G_{in})\\]\n(2)\nG denotes the prompt paralinguistic embedding, where G is a fixed-length global vector. \\(G \\in \\mathbb{R}^{B \\times D_{g}}\\), where \\(D_{g}\\) is the number of dimensions. The prompt encoder is a VAE-based model [46] that extracts paralinguistic information to address the one-to-many problem in TTS and VC tasks, such as timbre, style, and prosody, from the prompt speech. To address the KL collapse problem [47], a margin \u2206 is introduced to limit the minimum value of the KL loss as shown:\n\\[L_{kl} = max(0, D_{KL}[N(\\mu, \\sigma^{2})||N(0, 1)] - \\Delta)\\]\n(3)\nDecoders: As shown in Figure 2 (a), a discrete codebook component is added to the network for vector quantization processing, which further removes semantic-irrelevant paralinguistic information from the speech representation. S and R are compared with all vectors in the codebook, and the codebook vectors with the closest Euclidean distance are used as the vector-quantized speech representation \\(\\hat{S}\\) and \\(\\hat{R}\\). The vector quantization loss consists of two parts: the commitment loss (which makes the speech representation conform to its closest codebook vector as much as possible) and the codebook loss (which makes the selected codebook vector as close as possible to the speech representation). sg[.] represents \"stop gradient\", and e is the codebook vector.\n\\[L_{vq} = ||S - sg[e]||^{2} + ||sg[S] - e||\\]\n(4)\nTo get faster convergence speed, exponential moving averages [48] is used instead of codebook loss.\nTo utilize the pretrained speech representation for downstream ASR task, S is input into a phoneme decoder to predict the phoneme sequence:\n\\[P_{s} = PhonemeDecoder(\\hat{S})\\]\n(5)\nThe predicted phoneme sequence \\(P_{s}\\) is then compared with the input phoneme sequence \\(P_{in}\\) using a cross-entropy loss:\n\\[L_{classify} = CrossEntropy(P_{in}, P_{s})\\]\n(6)\nTo enable the pretrained speech representation to be used for downstream TTS and VC tasks, as well as to construct inputs for the subsequent semantic-transfer-wise paralinguistic consistency loss calculation, both \\(\\hat{S}\\) and \\(\\hat{R}\\) are fed into the same speech decoder to predict the mel-spectrogram. Since S and R only contain semantic information, G is provided as input to supply paralinguistic information:\n\\[S_{s} = SpeechDecoder(\\hat{S}, G);\\]\n\\[R_{r} = SpeechDecoder(\\hat{R}, G)\\]\n(7)\nIt is important to note that although \\(\\hat{S}\\) and \\(\\hat{R}\\) contain different semantic information, the input G is the same. Therefore, it is expected that the generated \\(S_{s}\\) and R will have identical paralinguistic information (timbre, prosody, emotion, etc.). Since G is extracted from \\(S_{in}\\), \\(S_{s}\\) is expected to be the same as \\(S_{in}\\). A mean squared error (MSE) loss is used to compare the predicted mel-spectrogram \\(S_{s}\\) with the ground-truth mel-spectrogram \\(S_{in}\\):\n\\[L_{mse} = MSE(S_{in}, S_{s})\\]\n(8)"}, {"title": "B. Semantic-Transfer-Wise Paralinguistic Consistency Loss", "content": "Most existing methods use paired tuples of ground truth and synthesized features to calculate the cycle consistency loss. However, due to factors such as \"teacher forcing\" and MSE loss, these two features are nearly identical during the training phase, resulting in suboptimal performance. During training, the combination of paralinguistic embedding and transferred semantic embedding (e.g., G from speaker 1 and R from speaker 2) as input poses an unseen problem, since there is no ground truth to compute the reconstruction loss. Existing cross-modal fine-grained (frame-level) pre-training methods that aim to explicitly introduce text information as supervision rely on expensive annotated text-speech pairs. In this case, the model's representational capacity is limited to the distribution of the text-speech paired data, resulting in poor robustness for unlabeled data (e.g., different styles, environmental sounds, channels, or qualities).\nAs shown in Figure 2 (b), a semantic-transfer-wise paralinguistic consistency loss is proposed. Unlabeled random speech \\(R_{in}\\) from outside the target domain is randomly selected. The extracted representations \\(\\hat{S}\\) and R from the target speech and random speech, respectively, are used as inputs, and the speech decoder is computed twice in each training step with identical paralinguistic embedding G. We expect the generated \\(S_{s}\\) and \\(R_{r}\\) to have different semantic information but the same paralinguistic information (timbre, prosody, emotion, etc.). \\(S_{s}\\) and R are then passed through the prompt encoder to extract the target paralinguistic embedding \\(G_{s}\\), and random paralinguistic embedding \\(G_{r}\\), respectively.\nWe construct two paralinguistic consistency losses: (G & \\(G_{s}\\)) and (Gs & \\(G_{r}\\)). This approach helps the model learn unseen semantic-paralinguistic combinations during the training phase, introduces unlabeled data, and enhances the model's robustness. The paralinguistic consistency loss is calculated using the Gram matrix [49], which captures the local statistics of the audio signal in the frequency and time domains:\n\\[L_{consistency1} = \\frac{1}{n}(G^{T}G - G_{s}^{T}G_{s})^{2};\\]\n\\[L_{consistency2} = \\frac{1}{n}(G^{T}G - G_{r}^{T}G_{r})^{2};\\]"}, {"title": "C. Token-Acoustic Contrastive Loss", "content": "As shown in Figure 2 (c), to extract frame-level representations, S and P within a batch are reshaped into 2D matrices \\(S_{re}\\) and \\(P_{re}\\), where \\(S_{re}\\) and \\(P_{re} \\in \\mathbb{R}^{(B*T_{s})\\times d}\\). This approach is beneficial for contrastive learning, as it increases the number of sample pairs per step, thereby enhancing the learning process.\nWith \\(S_{re}\\) and \\(P_{re}\\) now comparable, we can measure their similarity:\n\\[C = \\tau* (S_{re} P_{re}^{T})\\]\n(10)\nwhere \u03c4 is a temperature parameter used to scale the range of logits. The resulting similarity matrix \\(C\\in \\mathbb{R}^{(B*T_{s})\\times(B*T_{s})}\\) contains \\((B * T_{s})\\) correct pairs along the diagonal and \\((B * T_{s})^{2} - (B*T_{s})\\) incorrect pairs in the off-diagonal elements. As the extracted intermediate representation includes contextual information, only the current frame corresponds to a positive sample.\nThe contrastive loss is calculated:\n\\[L_{contrastive} = 0.5 * (l_{speech}(C) + l_{phoneme}(C))\\]\n(11)\nwhere\n\\[l_{(speech\\ or\\ phoneme)} = \\frac{1}{B*T_{s}}\\sum_{i=0}^{B*T_{s}}log\\ diag(softmax(C))\\]\n(12)\nalong the speech and phoneme axes, respectively. This symmetric cross-entropy loss (Lcontrastive) is computed over the similarity matrix to jointly train the speech encoder and the phoneme encoder, enabling the model to learn meaningful representations from both modalities simultaneously."}, {"title": "D. Stepping Optimization Strategy", "content": "A stepping optimization strategy is designed to ensure effective model convergence by gradually injecting and adjusting the influence of various loss components, as shown in Algorithm 1. The training process involves the following losses: Lkl, Luq, Lclassify, Lmse, Lcontrastive, and Lconsistency. The variable step represents the current training step.\nInitially, the model is trained using Lvq, Lmse, Lcontrastive, and Lclassify. Each of these losses has a corresponding weight, with the weights for Lmse, Luq, and Lclassify being equal, while the weight for Lcontrastive is significantly smaller than the other losses. When the step exceeds the specified starting step for Lkl, Lkl is added to the training process. The weight for Lkl increases gradually as the training progresses. Once the step surpasses the specified ending step, the weight for Lkl is fixed at kl_upper. Similarly, when the step exceeds the specified starting step for Lconsistency, Lconsistency is incorporated into the training process. The weight for Lconsistency also increases gradually during training. Once the step exceeds the specified ending step, the weight for Lconsistency is fixed at consistency_upper.\nAlgorithm 1 outlines the step-wise inclusion of different losses and their corresponding weight adjustments. This optimization strategy aims to facilitate effective model convergence by gradually introducing and adjusting the influence of various loss components throughout the training process."}, {"title": "IV. PLUG-AND-PLAY FOR DOWNSTREAM TASKS", "content": "VQ-CTAP can be applied to the task of minimally supervised TTS. Minimally supervised TTS refers to splitting the TTS task into two sub-tasks: text2semantic and semantic2acoustic, by combining speech intermediate representations. As shown in Figure 3(a), the weights of the pre-trained phoneme encoder, prompt encoder, speech encoder, and speech decoder are frozen. A diffusion-based duration model is utilized to predict the duration corresponding to each phoneme. The pre-trained phoneme encoder is used to extract phoneme embedding, while the pre-trained prompt encoder is used to extract prompt paralinguistic embedding. We find that phoneme representations P and speech representations S, due to the frame-level token-acoustic contrastive loss, are very close in Euclidean space. VQ-CTAP can be used for TTS without fine-tuning or a connector, but the quality is suboptimal due to the gap between P and S. Therefore, we proposed the Sequence-Aware Semantic Connector to achieve high-quality TTS at minimal cost. The presence of Lconsistency and the model's inherent semantic-acoustic decoupling design naturally align P with S, making it easier for the sequence-aware semantic connector to predict the semantic-aligned speech embedding S. The predicted speech embedding S is quantized using a codebook to obtain quantized speech embedding \\(\\hat{S}\\), along with the prompt paralinguistic embedding, are fed into the pre-trained speech decoder to generate the predicted mel-spectrogram.\nThe sequence-aware semantic connector calculation is shown in Algorithms 2 and 3. The model uses q(data), So, P, and t to represent data distribution, speech embedding, phoneme embedding, and diffusion step respectively. A notable feature of the model is its ability to perform closed-form sampling of St at any timestep tusing \\(\\bar{\\alpha}_{t}\\) and \\(\\alpha_{t}\\). The non-autoregressive network \u03b5 predicts \u03b5 from St, t, and P. The training objective is to minimize the unweighted variant of the ELBO [50], as shown in line 6 of Algorithm 2. The sampling process, detailed in Algorithm 3, begins by sampling \\(S_{T} \\sim N(0, I)\\), followed by iteratively sampling \\(S_{t-1} \\sim p_{\\theta}(S_{t-1}|S_{t})\\) for \\(t = T, T - 1, \\ldots, 1\\). The output \\(S_{0}\\) represents the sampled data.\nFigure 4 illustrates the architecture of the sequence-aware semantic connector, which employs a bidirectional dilated convolution structure with N residual layers organized into m blocks, each containing n = \\(\\frac{m}{n}\\) layers. Within each block, the dilation is doubled at each layer. Skip connections from all residual layers are summed, akin to the approach used in WaveNet [51]. The model incorporates phoneme embedding as conditional information, which is processed by a transformer encoder and added as bias terms to the dilated convolution in each residual layer. The diffusion step embedding is broadcast over length and added to the input of each residual layer."}, {"title": "B. VC Pipeline", "content": "VQ-CTAP can be directly applied to the VC task without fine-tuning, as illustrated in Figure 3(b). In this process, the weights of the prompt encoder, speech encoder, and speech decoder remain frozen. The pre-trained speech encoder extracts the speech embedding of the source speaker, while the pre-trained prompt encoder extracts the prompt paralinguistic embedding of the target speaker. The speech embedding S is then quantized using a codebook to obtain the quantized speech embedding \\(\\hat{S}\\). Subsequently, \\(\\hat{S}\\) and prompt paralinguistic embedding is input together into the pre-trained speech decoder. The predicted mel spectrogram from the speech decoder combines the semantic content of the source speaker with the timbre of the target speaker."}, {"title": "C. ASR Pipeline", "content": "VQ-CTAP can be directly applied to the ASR task without fine-tuning, as illustrated in Figure 3(c). In this approach, the weights of the speech encoder and phoneme decoder are frozen. The pre-trained speech encoder is utilized to extract speech embedding S. S is quantized using a codebook to obtain the quantized speech embedding \\(\\hat{S}\\). Finally, \\(\\hat{S}\\) is input into the pre-trained phoneme decoder, which predicts the corresponding phoneme sequence."}, {"title": "V. EXPERIMENTS PROCEDURES", "content": "In the experiment, we use a speech encoder with a structure similar to the Whisper encoder [52], but reduce the dimension to d through a linear layer. The speech encoder consists of 2 convolutional layers, a GELU activation function, 6 transformer layers, and a linear layer. The 2 convolutional layers compress the length to a quarter of the original shape. The phoneme encoder is composed of a convolutional layer, a RELU activation function, 4 transformer layers, and a linear layer. The outputs of the speech encoder and phoneme encoder are layer-normalized separately. The prompt encoder is a VAE-based model, consisting of 6 convolutional layers and an SE-ResNet [53] block. The codebook has 8192 embedding entries, with each embedding having a dimension of 256. The speech decoder comprises 6 transformer layers, 5 convolutional layers, 2 transposed convolutional layers (which increase the length by a factor of 4 to restore the original shape), 6 Tanh activation functions, and a linear layer. The phoneme decoder consists of 6 transformer layers, 2 transposed convolutional layers (which increase the length by a factor of 4 to restore the original length), 2 Tanh activation functions, and a linear layer. Both encoders and decoders have a hidden dimension of 256. The model is trained using 8 NVIDIA TESLA A800 80GB GPUs, with a batch size of 64 per GPU. Adam [54] is used as the optimizer, with an initial learning rate of 2e-4.\nFor the labeled text-speech paired data, we integrate our internal dataset with the AISHELL-3 dataset [55] and the LibriTTS dataset [56], resulting in a combined total of 900 hours of recordings from 3000 speakers.\nRegarding the unlabeled speech-only data, we crawl 20,000 hours of speech from the internet, primarily sourced from audiobooks, podcasts, and online videos. However, this portion of the data lacks speaker information statistics. The collected audio underwent various preprocessing steps, including Voice Activity Detection (VAD) segmentation, signal-to-noise ratio (SNR) detection, and human voice detection.\nAll speech waveforms are sampled at 24kHz and converted to 40-band mel spectrograms with a frame size of 960 and a hop size of 240. From an information compression perspective, 40-dimensional features are more suitable for our task."}, {"title": "VI. RESULTS", "content": "Table I compares the compression ratios of VQ-CTAP and mainstream speech representation models. It shows that the 960x compression ratio of VQ-CTAP is 3 times that of Encodec, SoundStream, and Hubert, while the experiments prove that VQ-CTAP maintains high performance(Table II) even at high compression rates.\nTo intuitively demonstrate that VQ-CTAP's semantic coding is paralinguistic-independent, we use prompts from 20 speakers to synthesize 4 text, with each synthesis having the same duration to ensure that the text corresponding to each frame of audio is the same across different speakers. We then use VQ-CTAP's speech encoder to extract the speech representation S and the phoneme encoder to extract the text's phoneme representation P. We expect these two representations to be aligned at the frame level, and S should have removed the speaker information. Using t-SNE, we compress each frame's Pand S to 2-dimensional vectors and show their distribution in Figure 5. The \u2605 represents P, and the other shapes represent S for different speakers, with different colors indicating the positions of corresponding P and S. As shown in Figures 5(a) and (c), both the Euclidean distance among the S of different speakers and the corresponding P at each frame are very close, proving that VQ-CTAP's speech encoder extracts the speech representation (semantic coding) while removing paralinguistic information and aligning it with the text modality. Notably, for the red and orange phonemes \"k\" in Figure 5(c), although the phonemes are the same and the positions are different, the corresponding P and S are not entangled. This indicates that VQ-CTAP's representation is highly context-dependent, which is in line with our motivation for using the token-acoustic contrastive loss. The clustering effect in the short sentences (b) and (d) of Figure 5 is not as good as that in the long sentences (a) and (c), also confirming VQ-CTAP's characteristic of context modeling.\nWe randomly select 2000 utterances from 20 speakers and use VQ-CTAP's prompt encoder to extract the prompt paralinguistic embedding. We compress it to a 2-dimensional vector based on the t-SNE as shown in Figure 6. The scattered distribution can be seen in Figure 6(a), proving the prompt encoder cannot model timbre without the stepping optimization strategy. In contrast, the prompt paralinguistic embedding extracted by VQ-CTAP has clear boundaries between different speakers, demonstrating the prompt encoder's timbre modeling capability. (Except for VQ-CTAP w/o Stepping, the other ablation models also have this capability.)\nVQ-CTAP has better efficiency similar to other contrastive learning-based methods, and it significantly reduces computational complexity compared to other pre-training schemes, i.e., it only predicts frame-level aligned phoneme and speech. The large number of frames included in each step of our calculations (batch size multiplied by sequence length) is advantageous for contrastive learning."}, {"title": "Evaluation of TTS Task", "content": "Table II presents the results of the TTS task. VQ-CTA\u0420 achieves the best results in 4 terms of MSEP, WER, P-MOS, and S-MOS. Incorporating the decoder structure and reconstruction loss during training helps extract intermediate representations that are more suitable for frame-level generation tasks. Multiple systems achieve similar results in terms of prosody similarity P-MOS since they employ the same diffusion-based duration model. However, VQ-CTAP achieves the best prosody similarity P-MOS due to its enhanced semantic-paralinguistic consistency, enabled by modules such as the Semantic-Transfer-wise Paralinguistic Consistency Loss. CTAP and VQ-CTAP address the issue of speaker information redundancy in traditional intermediate representations through frame-level contrastive learning. VQ-CTAP exhibits less voice leakage, leading to the best results in terms of voice similarity S-MOS. Furthermore, the pre-trained phoneme encoder alleviates the difficulty of predicting intermediate representations (semantic embeddings) from text in two-stage TTS models and can be directly used as an effective predictor. In terms of speech quality Q-MOS, Codec, Hubert, Wav2Vec2.0, and VQ-CTAP suffer from information loss due to their discrete processing, while CTAP achieves the best Q-MOS results with its continuous representations that are not subject to discretization."}, {"title": "Evaluation of VC Task", "content": "The results of the voice conversion (VC) task are also presented in Table II. Similar to the TTS task, VQ-CTAP achieves the best results in terms of WER, P-MOS, and S-MOS. VQ-CTAP extracts semantic coding from the source speaker's mel-spectrogram with better decoupling ability for paralinguistic information, and its ability to model the paralinguistic embedding from the target speaker's mel-spectrogram is also stronger. The experiments reveal that Hubert has a higher information redundancy issue compared to other models, leading to the retention of more paralinguistic information in the semantic encoding, which helps it achieve the best results in terms of Q-MOS. To point out in the ablation study (Table III), CTAP w/o Unsupervised (without using unsupervised low-quality data) achieve similar performance to Hubert in terms of Q-MOS."}, {"title": "D. Evaluation of ASR Task", "content": "Table II shows that Hubert achieves the best results in ASR, but is not suitable for TTS and VC tasks, because of higher information redundancy. It's worth noting that in the ablation study (Table III), CTAP w/o Compression (without length compression) achieves results close to Hubert in terms of ACC."}, {"title": "E. Ablation Studies", "content": "As shown in Table III, the ablation study aims to analyze the contribution of the different components in VQ-CTAP. The largest negative impact of the VQ-CTAP w/o PhonemeDecoder model is on the ACC of the ASR task. The phoneme classification loss of the PhonemeDecoder structure is naturally well-suited for the ASR task. Since contrastive learning has already introduced text supervision, its role is similar to that of the phoneme decoder, so removing the phoneme decoder has little impact on the other results. The VQ-CTAP w/o Compression excels in WER and P-MOS for TTS task and in ACC for ASR task. However, these improvements are relatively minor compared to VQ-CTAP, which maintains excellent reconstruction ability even at high compression rates. Higher compression rates facilitate the separation of paralinguistic"}]}