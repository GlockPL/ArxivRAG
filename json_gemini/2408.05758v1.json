{"title": "VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing", "authors": ["Chunyu Qiang", "Wang Geng", "Yi Zhao", "Ruibo Fu", "Tao Wang", "Cheng Gong", "Tianrui Wang", "Qiuyu Liu", "Jiangyan Yi", "Zhengqi Wen", "Chen Zhang", "Hao Che", "Longbiao Wang", "Jianwu Dang", "Jianhua Tao"], "abstract": "Deep learning has brought significant improvements to the field of cross-modal representation learning. For tasks such as text-to-speech (TTS), voice conversion (VC), and au-tomatic speech recognition (ASR), a cross-modal fine-grained (frame-level) sequence representation is desired, emphasizing the semantic content of the text modality while de-emphasizing the paralinguistic information of the speech modality. We propose a method called \"Vector Quantized Contrastive Token-Acoustic Pre-training (VQ-CTAP)\", which uses the cross-modal aligned sequence transcoder to bring text and speech into a joint multimodal space, learning how to connect text and speech at the frame level. The proposed VQ-CTAP is a paradigm for cross-modal sequence representation learning, offering a promising solution for fine-grained generation and recognition tasks in speech processing. The VQ-CTAP can be directly applied to VC and ASR tasks without fine-tuning or additional structures. We propose a sequence-aware semantic connector, which connects multiple frozen pre-trained modules for the TTS task, exhibiting a plug-and-play capability. We design a stepping optimization strategy to ensure effective model convergence by gradually injecting and adjusting the influence of various loss components. Furthermore, we propose a semantic-transfer-wise paralinguistic consistency loss to enhance representational capabilities, allowing the model to better generalize to unseen data and capture the nuances of paralinguistic information. In addition, VQ-CTAP achieves high-compression speech coding at a rate of 25Hz from 24kHz input waveforms, which is a 960-fold reduction in the sampling rate. The audio demo is available at https://qiangchunyu.github.io/VQCTAP/.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning has brought significant improvements to the field of cross-modal representation, leading to the emergence of numerous methods for representing connections between text, images, and speech [1]-[8].\nContrastive learning has proven to be an effective method for cross-modal modeling. It works by differentiating target samples (positive) from distractor samples (negatives) based on an anchor representation. The objective is to maximize the similarity between the anchor and positive samples while minimizing the similarity between the anchor and negative samples. This approach has been widely applied in text-image cross-modal representation, with notable examples including OpenAI's CLIP [1], Florence [2], and ALIGN [3]. In the field of text-audio cross-modal representation, CLIP-based models have also been developed, including Wav2CLIP [4], AudioCLIP [5], and CLAP [6]. In the field of multimodal representation, pre-trained representation models have emerged that use a single modality as a link to connect other modalities. For example, ImageBind [7] achieves joint modeling of multiple modalities based on the image modality, while LanguageBind [8] achieves joint modeling of multiple modalities based on the language modality. These models focus on extracting global descriptive information; for instance, CLAP aims to enhance performance in downstream audio classification tasks. However, their emphasis on global information limits their applicability to tasks requiring fine-grained sequence generation and recognition.\nAs shown in Figure 1, the speech processing tasks of TTS and ASR are typical cross-modal applications. For tasks such as TTS, VC and ASR, a fine-grained (frame-level) sequence cross-modal representation(semantic coding) between text-speech modalities is expected. The intermediate representation extracted from speech should serve as a \"bridge\" between text and acoustic information. It should emphasize linguistic con-tent while de-emphasizing paralinguistic information such as speaker identity and acoustic details. Self-supervised represen-tation learning methods, such as Wav2Vec2.0 [9], Wav2Vec-C [10], VQ-Wav2Vec [11], HuBERT [12], and W2V-BERT [13], have become mainstream in the speech representation field. These methods offer the prospect of a universal model that can benefit a wide range of tasks and domains. However, when applied to VC and TTS tasks, such as SPEAR-TTS [14], these methods encounter issues of redundancy and dimensionality explosion. This is because the lack of explicit text supervision in these methods results in learned representations that are not fully decoupled semantic information. Supervised frame-level representation learning methods aim to explicitly introduce text information as supervision. For example, Phonetic Poste-riorgrams (PPGs) [15] are calculated based on ASR acoustic models. Although PPGs have been widely used in various speech processing tasks, they are essentially text information, and the frame-level features are independent of each other, lacking the integration of contextual semantic information. Moreover, these methods rely on expensive annotated text-speech pairs. In this case, the model's representational capacity is limited to the distribution of the text-speech paired data, re-sulting in poor robustness for unlabeled speech-only data (e.g., different styles, environmental sounds, channels, or qualities).\nIn our previous three works: 1) We identified issues of information redundancy and dimensional explosion in existing semantic coding methods [16]. 2) We proposed a semantic coding method called \"Contrastive Token-Acoustic Pretrain-ing (CTAP) [17].\" 3) Building on CTAP, we developed a minimally-supervised high-fidelity speech synthesis method [18]. Building on these foundational works, we now intro-duce VQ-CTAP, which uses the cross-modal aligned sequence transcoder to bring text and speech into a joint multimodal space, facilitating the connection between text and speech at the frame level. Our contributions in this paper are as follows:\n1. We propose a cross-modal sequence representation learn-ing paradigm and introduce the VQ-CTAP model trained on 900 hours of speech-text pairs and 20,000 hours of speech-only data. We design a stepping optimization strategy to ensure effective model convergence by gradually injecting and adjusting the influence of various loss components.\n2. We propose a semantic-transfer-wise paralinguistic con-sistency loss, which uses unlabeled data to enhance represen-tation capabilities while improving the semantic-paralinguistic decoupling ability of the representations. This loss function allows the model to better generalize to unseen data and capture the nuances of paralinguistic information.\n3. We propose a sequence-aware semantic connector, en-abling the pre-trained VQ-CTAP to be directly used for TTS task without fine-tuning. Additionally, the pre-trained VQ-CTAP can be directly applied to VC and ASR tasks without fine-tuning or adding additional structures, exhibiting a plug-and-play capability.\n4. We achieve high-compression speech coding by gener-ating discrete embedding from a single codebook at a rate"}, {"title": "II. RELATED WORK", "content": "Self-supervised speech representation learning with deep neural networks emerge two training schemes, the autore-gressive models [19], [20] and bidirectional models [9], [12], [21]\u2013[24]. The autoregressive models learn to predict the future discrete representations of audio segments based on past observations. The bidirectional models mainly adopt masked language modeling (MLM) [25], which recovers the masked part of the discrete inputs using the unmasked context. These descrete inputs can be derived from the audio segments [9], [21]. Following the MLM training scheme for speech represen-tation learning, HuBERT [12] proposed to learn the masked prediction of hidden units generated by vanilla acoustic unit discovery systems. BEST-RQ [26] learn a model to predict the masked labels of the speech signals generated with a random-projection quantizer. WavLM [24] jointly learn masked speech prediction and denoising in pre-training to solve full-stack downstream speech tasks."}, {"title": "B. Contrastive Learning", "content": "Contrastive learning has proven to be an effective method for cross-modal modeling. It works by differentiating a target sample (positive) from distractor samples (negatives) based on an anchor representation. The objective is to maximize the similarity between the anchor and positive samples while minimizing the similarity between the anchor and negative samples. This approach has been widely applied in the field of computer vision, with notable examples such as Open Al's CLIP [1], Florence [2], and ALIGN [3].\nIn the audio field, CLIP-based models have also been developed, including Wav2CLIP [4], AudioCLIP [5], and CLAP [6]. These models focus on extracting global descriptive information from audio, with the primary goal of improving the performance of downstream audio classification tasks. However, their emphasis on global information limits their applicability to fine-grained generation and recognition tasks such as TTS, VC, and ASR, which require frame-level repre-sentations."}, {"title": "C. TTS, VC, and ASR tasks", "content": "The TTS task refers to the modelling of unequal length sequences between text to speech. As shown in Figure 1, minimally-supervised TTS refers to splitting the TTS task into two tasks (text-to-semantic and semantic-to-acoustic) by combining speech intermediate representations. As deep learn-ing advances, TTS technology has made significant progress. Traditional TTS methods have achieved satisfactory results [27]\u2013[32]. The introduction and subsequent evolution of generative pre-trained transformers (GPT) models [33], [34], have significantly heightened interest in the development of large-scale TTS systems. These TTS systems can be broadly divided into two categories: 1) autoregressive frameworks [14], [35]\u2013[37] and 2) non-autoregressive frameworks [38]\u2013[40].\nThe VC task aims to modify the voice of a source speaker to match a target style, including aspects such as speaker identity, prosody, and emotion, while preserving the original linguistic content. VC techniques can be broadly classified based on their approach to content disentanglement into two categories: text-based VC and text-free VC. Text-based VC often employs an ASR model to derive PPGs as a representation of content [15], [41], [42]. In contrast, text-free VC approaches aim to bypass the need for text annotations by directly learning content representations [43]\u2013[45].\nThe ASR is the inverse task of TTS, aims to predict text from speech. ASR is moving towards speech representation learning to make full use of the large-scale unsupervised speech datasets [12], [23], [24], [26]. The MLM [9], [21] based self-supervised learning machanism learn discrete masked speech from outside the target domain. The phoneme and speech representations are concatenated in a joint multimodal space. A contrastive learning approach is used to learn this space by capturing the frame-level (dis)similarity between speech and phoneme pairs. The speech encoder produces discrete embedding at a rate of 25Hz from the 24kHz input waveform, which is a 960-fold reduction in the sampling rate. Vector quantization further removes paralinguistic information irrelevant to semantics from the speech representations. The two decoders adapt the learned representations for downstream tasks such as TTS, VC, and ASR. The method supports both supervised and unsupervised training. To enable domain-enhanced representation learning, semantic-transfer-wise par-alinguistic consistency loss is introduced, computed using the unlabeled random speech from outside the target domain. The pre-trained VQ-CTAP can be directly applied to TTS, VC, and ASR tasks without fine-tuning, exhibiting a plug-and-play capability."}, {"title": "III. METHOD", "content": "As illustrated in Figure 2, the main body of VQ-CTAP_is the cross-modal aligned sequence transcoder, which comprises five main components: a speech encoder, a phoneme encoder, a prompt encoder, a phoneme decoder, and a speech decoder. A length regulator is employed to address the mismatch between the lengths of phoneme sequences and speech sequences. The input consists of text-speech pairs and unlabeled random"}, {"title": "A. Cross-Modal Aligned Sequence Transcoder", "content": "Encoders: As illustrated in Figure 2 (a), $S_{in}$ denotes the input batch of target speech data, $S_{in} \\in R^{B \\times T_s \\times D_s}$, where $B$ is the batch size, $T_s$ is the number of time frames, and $D_s$ is the number of spectral components (mel-spectrogram bands). $P_{in}$ denotes the input batch of phoneme data, $P_{in} E R^{B \\times T_p \\times D_p}$, where $T_p$ is the number of phoneme codes and $D_p$ is the dimensionality of the phoneme codes. During training, the ground-truth duration is used to extend the length of the phoneme sequences to $T_s$. During inference, the corresponding predicted duration is used. After the length regulator, the phoneme sequences become $P_{in} \\in R^{B \\times T_s \\times D_p}$, having the same length as the target speech sequences $S_{in}$. The unla-\nbeled random speech $R_{in}$ from outside the target domain is randomly selected, where $R_{in} \\in R^{B \\times T_r \\times D_s}$. The batch size $B$ and the number of spectral components $D_s$ are consistent with those of $S_{in}$, while typically $T_r\\neq T_s$. The $P_{in}$, $S_{in}$ and $R_{in}$ are then passed through a phoneme encoder and a speech encoder, respectively:\n$P = PhonemeEncoder(P_{in});$\n$S = SpeechEncoder(S_{in});$\n$R = SpeechEncoder(R_{in})$\n(1)\nwhere $P\\in R^{B \\times T_s/4\\times d}$ are the phoneme representations, $S\\in R^{B \\times T_s/4\\times d}$ are the target speech representations, and $R\\in R^{B \\times T_r/4\\times d}$ are the random speech representations. The two encoders compress the lengths of the phoneme and speech representations by a factor of 4, respectively. We bring $S$, $R$, and $P$ into a joint multimodal space of dimension d.\nThe prompt speech $G_{in}$ is randomly clipped with a window of length 3 seconds at each step as the input to the prompt encoder:\n$G = PromptEncoder(G_{in})$\n(2)\n$G$ denotes the prompt paralinguistic embedding, where $G$ is a fixed-length global vector. $G\\in R^{B \\times D_g}$, where $D_g$ is the number of dimensions. The prompt encoder is a VAE-based model [46] that extracts paralinguistic information to address the one-to-many problem in TTS and VC tasks, such as timbre, style, and prosody, from the prompt speech. To address the KL collapse problem [47], a margin $A$ is introduced to limit the minimum value of the KL loss as shown:\n$L_{kl} = max(0, D_{KL}[N(\\mu, \\sigma^2)||N(0, 1)] - \\Delta)$\n(3)\nDecoders: As shown in Figure 2 (a), a discrete codebook component is added to the network for vector quantization processing, which further removes semantic-irrelevant paralin-guistic information from the speech representation. $S$ and $R$ are compared with all vectors in the codebook, and the codebook vectors with the closest Euclidean distance are used as the vector-quantized speech representation $\\hat{S}$ and $\\hat{R}$. The vector quantization loss consists of two parts: the commit-ment loss (which makes the speech representation conform to its closest codebook vector as much as possible) and the codebook loss (which makes the selected codebook vector as close as possible to the speech representation). $sg[\u00b7]$ represents \"stop gradient\", and $e$ is the codebook vector.\n$L_{vq} = ||S - sg[e]||_2 + ||sg[S] - e||_2$\n(4)\nTo get faster convergence speed, exponential moving aver-ages [48] is used instead of codebook loss.\nTo utilize the pretrained speech representation for down-stream ASR task, $S$ is input into a phoneme decoder to predict the phoneme sequence:\n$\\hat{P_s} = PhonemeDecoder(\\hat{S})$\n(5)\nThe predicted phoneme sequence $\\hat{P_s}$ is then compared with the input phoneme sequence $P_{in}$ using a cross-entropy loss:"}, {"title": "", "content": "$L_{classify} = CrossEntropy(P_{in}, \\hat{P_s})$\n(6)\nTo enable the pretrained speech representation to be used for downstream TTS and VC tasks, as well as to construct inputs for the subsequent semantic-transfer-wise paralinguistic consistency loss calculation, both $\\hat{S}$ and $\\hat{R}$ are fed into the same speech decoder to predict the mel-spectrogram. Since $S$ and $R$ only contain semantic information, $G$ is provided as input to supply paralinguistic information:\n$S_s = SpeechDecoder(\\hat{S}, G);$\n$R_r = SpeechDecoder(\\hat{R}, G)$\n(7)\nIt is important to note that although $\\hat{S}$ and $\\hat{R}$ contain different semantic information, the input $G$ is the same. Therefore, it is expected that the generated $S_s$ and $\\hat{R}$ will have identical paralinguistic information (timbre, prosody, emotion, etc.). Since $G$ is extracted from $S_{in}$, $S_s$ is expected to be the same as $S_{in}$. A mean squared error (MSE) loss is used to compare the predicted mel-spectrogram $S_s$ with the ground-truth mel-spectrogram $S_{in}$:\n$L_{mse} = MSE(S_{in}, S_s)$\n(8)"}, {"title": "B. Semantic-Transfer-Wise Paralinguistic Consistency Loss", "content": "Most existing methods use paired tuples of ground truth and synthesized features to calculate the cycle consistency loss. However, due to factors such as \"teacher forcing\" and MSE loss, these two features are nearly identical during the training phase, resulting in suboptimal performance. During training, the combination of paralinguistic embedding and transferred semantic embedding (e.g., G from speaker 1 and R from speaker 2) as input poses an unseen problem, since there is no ground truth to compute the reconstruction loss. Existing cross-modal fine-grained (frame-level) pre-training methods that aim to explicitly introduce text information as supervision rely on expensive annotated text-speech pairs. In this case, the model's representational capacity is limited to the distribution of the text-speech paired data, resulting in poor robustness for unlabeled data (e.g., different styles, environmental sounds, channels, or qualities).\nAs shown in Figure 2 (b), a semantic-transfer-wise paralin-guistic consistency loss is proposed. Unlabeled random speech $R_{in}$ from outside the target domain is randomly selected. The extracted representations $\\hat{S}$ and $\\hat{R}$ from the target speech and random speech, respectively, are used as inputs, and the speech decoder is computed twice in each training step with identical paralinguistic embedding G. We expect the generated $S_s$ and $R_r$ to have different semantic information but the same paralinguistic information (timbre, prosody, emotion, etc.). $S_s$ and $R_r$ are then passed through the prompt encoder to extract the target paralinguistic embedding $G_s$, and random paralinguistic embedding $G_r$, respectively.\nWe construct two paralinguistic consistency losses: $(G & G_s)$ and $(G_s & G_r)$. This approach helps the model learn un-seen semantic-paralinguistic combinations during the training phase, introduces unlabeled data, and enhances the model's robustness. The paralinguistic consistency loss is calculated using the Gram matrix [49], which captures the local statistics of the audio signal in the frequency and time domains:\n$L_{consistency1} = \\frac{1}{n}(GG^T - GG_s^T)^2$;\n$L_{consistency2} = \\frac{1}{n}(G_sG_s^T - G_sG_r^T)^2$;"}, {"title": "C. Token-Acoustic Contrastive Loss", "content": "As shown in Figure 2 (c), to extract frame-level repre-sentations, S and P within a batch are reshaped into 2D matrices $S_{re}$ and $P_{re}$, where $S_{re}$ and $P_{re} \\in R^{(B*T_s)\\times d}$. This approach is beneficial for contrastive learning, as it increases the number of sample pairs per step, thereby enhancing the learning process.\nWith $S_{re}$ and $P_{re}$ now comparable, we can measure their similarity:\n$C = \\tau * (S_{re} P_{re}^T)$\n(10)\nwhere $\\tau$ is a temperature parameter used to scale the range of logits. The resulting similarity matrix $C\\in R^{(B*T_s)\\times (B*T_s)}$ contains $(B * T_s)$ correct pairs along the diagonal and $(B * T_s)^2 - (B*T_s)$ incorrect pairs in the off-diagonal elements. As the extracted intermediate representation includes contextual information, only the current frame corresponds to a positive sample.\nThe contrastive loss is calculated:\n$L_{contrastive} = 0.5 * (l_{speech}(C) + l_{phoneme}(C))$\n(11)\nwhere\n$l_{(speech \\; or \\; phoneme)} = \\frac{1}{B*T_s}\\sum_{i=0}^{B*T_s}log diag(softmax(C))$\n(12)\nalong the speech and phoneme axes, respectively. This symmetric cross-entropy loss ($L_{contrastive}$) is computed over the similarity matrix to jointly train the speech encoder and the phoneme encoder, enabling the model to learn meaningful representations from both modalities simultaneously."}, {"title": "D. Stepping Optimization Strategy", "content": "A stepping optimization strategy is designed to ensure effec-tive model convergence by gradually injecting and adjusting the influence of various loss components, as shown in Al-gorithm 1. The training process involves the following losses: $L_{kl}$, $L_{vq}$, $L_{classify}$, $L_{mse}$, $L_{contrastive}$, and $L_{consistency}$. The variable step represents the current training step.\nInitially, the model is trained using $L_{vq}$, $L_{mse}$, $L_{contrastive}$, and $L_{classify}$. Each of these losses has a corresponding weight, with the weights for $L_{mse}$, $L_{vq}$, and $L_{classify}$ being equal, while the weight for $L_{contrastive}$ is significantly smaller than the other losses. When the step exceeds the specified starting step for $L_{kl}$, $L_{kl}$ is added to the training process. The weight for $L_{kl}$ increases gradually as the training progresses. Once the step surpasses the specified ending step, the weight for $L_{kl}$ is fixed at kl_upper. Similarly, when the step ex-ceeds the specified starting step for $L_{consistency}$, $L_{consistency}$ is incorporated into the training process. The weight for $L_{consistency}$ also increases gradually during training. Once the step exceeds the specified ending step, the weight for $L_{consistency}$ is fixed at consistency_upper.\nAlgorithm 1 outlines the step-wise inclusion of different losses and their corresponding weight adjustments. This op-timization strategy aims to facilitate effective model conver-gence by gradually introducing and adjusting the influence of various loss components throughout the training process."}, {"title": "IV. PLUG-AND-PLAY FOR DOWNSTREAM TASKS", "content": "VQ-CTAP can be applied to the task of minimally su-pervised TTS. Minimally supervised TTS refers to splitting the TTS task into two sub-tasks: text2semantic and seman-tic2acoustic, by combining speech intermediate representa-tions. As shown in Figure 3(a), the weights of the pre-trained phoneme encoder, prompt encoder, speech encoder, and speech decoder are frozen. A diffusion-based duration model is uti-lized to predict the duration corresponding to each phoneme. The pre-trained phoneme encoder is used to extract phoneme embedding, while the pre-trained prompt encoder is used to ex-tract prompt paralinguistic embedding. We find that phoneme representations P and speech representations S, due to the frame-level token-acoustic contrastive loss, are very close in Euclidean space. VQ-CTAP can be used for TTS without fine-tuning or a connector, but the quality is suboptimal due to the gap between $P$ and $S$. Therefore, we proposed the Sequence-Aware Semantic Connector to achieve high-quality TTS at minimal cost. The presence of $L_{consistency}$ and the model's inherent semantic-acoustic decoupling design naturally align $P$ with $S$, making it easier for the sequence-aware semantic connector to predict the semantic-aligned speech embedding $S$. The predicted speech embedding $\\hat{S}$ is quantized using a codebook to obtain quantized speech embedding $\\hat{S}$, along with the prompt paralinguistic embedding, are fed into the pre-trained speech decoder to generate the predicted mel-spectrogram."}, {"title": "1) Sequence-Aware Semantic Connector:", "content": "The sequence-aware semantic connector calculation is shown in Algorithms 2 and 3. The model uses $q(data)$, $S_0$, $P$, and $t$ to represent data distribution, speech embedding, phoneme embedding, and diffusion step respectively. A notable feature of the model is its ability to perform closed-form sampling of $S_t$ at any timestep $t$ using $\\bar{a}_t$ and $a_t$. The non-autoregressive network $\\epsilon_\\theta$ predicts $\\epsilon$ from $S_t$, $t$, and $P$. The training objective is to minimize the unweighted variant of the ELBO [50], as shown in line 6 of Algorithm 2. The sampling process, detailed in Algorithm 3, begins by sampling $S_T \\sim N(0, I)$, followed by iteratively sampling $S_{t\u22121} \\sim p_\\theta(S_{t\u22121}|S_t)$ for $t = T, T \u2212 1,\u2026\u2026, 1$. The output $S_0$ represents the sampled data.\nFigure 4 illustrates the architecture of the sequence-aware semantic connector, which employs a bidirectional dilated convolution structure with $N$ residual layers organized into"}, {"title": "Algorithm 2 Training of Sequence-Aware Semantic Connector", "content": "repeat\n$S_0, P \\sim q(data)$\n$t \\sim Uniform({1, . . ., T}), \\epsilon \\sim N(0, I), \\bar{a}_t = \\prod_{i=1}^t a_i$\nTake gradient descent step on\n$\\parallel \\epsilon - \\epsilon_\\theta ((\\sqrt{\\bar{a}_t}S_0 + \\sqrt{1 - \\bar{a}_t})\\epsilon), t, P)\\parallel^2$\nuntil converged"}, {"title": "Algorithm 3 Sampling of Sequence-Aware Semantic Connector", "content": "$S_T \\sim N(0, I)$\nfor $t = T, ..., 1$ do\n$\\mu_\\theta(S_t, t, P) = \\frac{1}{\\sqrt{a_t}}(S_t - \\frac{(1-a_t)}{\\sqrt{1-\\bar{a}_t}} \\epsilon_\\theta (S_t, t, P))$\n$\\sigma_\\theta (S_t, t, P) = \\sqrt{(1 - \\bar{a}_{t-1})} / \\sqrt{(1 - \\bar{a}_t)}$\n$S_{t-1} = \\mu_\\theta + \\sigma_\\theta \\odot \\psi; \\psi \\sim N(0, I)$ if $t > 1$, else $\\psi = 0$\nend for\nreturn $S_0$"}, {"title": "B. VC Pipeline", "content": "VQ-CTAP can be directly applied to the VC task without fine-tuning, as illustrated in Figure 3(b). In this process, the weights of the prompt encoder, speech encoder, and speech decoder remain frozen. The pre-trained speech encoder ex-tracts the speech embedding of the source speaker, while the pre-trained prompt encoder extracts the prompt paralinguistic embedding of the target speaker. The speech embedding S is then quantized using a codebook to obtain the quantized speech embedding $\\hat{S}$. Subsequently, $\\hat{S}$ and prompt paralin-guistic embedding is input together into the pre-trained speech decoder. The predicted mel spectrogram from the speech decoder combines the semantic content of the source speaker with the timbre of the target speaker."}, {"title": "C. ASR Pipeline", "content": "VQ-CTAP can be directly applied to the ASR task without fine-tuning, as illustrated in Figure 3(c). In this approach, the weights of the speech encoder and phoneme decoder are frozen. The pre-trained speech encoder is utilized to extract speech embedding S. S is quantized using a codebook to obtain the quantized speech embedding $\\hat{S}$. Finally, $\\hat{S}$ is input into the pre-trained phoneme decoder, which predicts the corresponding phoneme sequence."}, {"title": "V. EXPERIMENTS PROCEDURES", "content": ""}, {"title": "A. Model Details", "content": "In the experiment, we use a speech encoder with a structure similar to the Whisper encoder [52], but reduce the dimension to d through a linear layer. The speech encoder consists of 2 convolutional layers, a GELU activation function, 6 transformer layers, and a linear layer. The 2 convolutional layers compress the length to a quarter of the original shape. The phoneme encoder is composed of a convolutional layer, a RELU activation function, 4 transformer layers, and a linear layer. The outputs of the speech encoder and phoneme encoder are layer-normalized separately. The prompt encoder is a VAE-based model, consisting of 6 convolutional layers and an SE-ResNet [53] block. The codebook has 8192 embedding entries, with each embedding having a dimension of 256. The speech decoder comprises 6 transformer layers, 5 convolutional lay-ers, 2 transposed convolutional layers (which increase the length by a factor of 4 to restore the original shape), 6 Tanh activation functions, and a linear layer. The phoneme decoder consists of 6 transformer layers, 2 transposed convolutional layers (which increase the length by a factor of 4 to restore the original length), 2 Tanh activation functions, and a linear layer. Both encoders and decoders have a hidden dimension of 256. The model is trained using 8 NVIDIA TESLA A800 80GB GPUs, with a batch size of 64 per GPU. Adam [54] is used as the optimizer, with an initial learning rate of 2e-4."}, {"title": "B. Datasets", "content": "For the labeled text-speech paired data, we integrate our internal dataset with the AISHELL-3 dataset [55] and the LibriTTS dataset [56], resulting in a combined total of 900 hours of recordings from 3000 speakers.\nRegarding the unlabeled speech-only data, we crawl 20,000 hours of speech from the internet, primarily sourced from audiobooks, podcasts, and online videos. However, this portion of the data lacks speaker information statistics. The collected audio underwent various preprocessing steps, including Voice Activity Detection (VAD) segmentation, signal-to-noise ratio (SNR) detection, and human voice detection.\nAll speech waveforms are sampled at 24kHz and converted to 40-band mel spectrograms with a frame size of 960 and a hop size of 240. From an information compression perspective, 40-dimensional features are more suitable for our task."}, {"title": "C. Compared Method and Tasks", "content": "To demonstrate the model's capability for downstream frame-level tasks, we evaluated its performance on TTS, VC, and ASR tasks. We compare our proposed model with five other models, including self-supervised learning methods: Codec [57], Hubert [12], Wav2Vec2.0 [9], and supervised learning methods: the encoder of the ASR model Whisper [52], and the speech representation model CTAP [17] based on Contrastive Phoneme-Speech Pretraining. In this paper, the representations extracted from these models are collectively referred to as semantic coding, as shown in Figure1. In all models, the embedding values from the codebook are used to replace discrete codes, resulting in dense representation of the inputs and outputs. To ensure fairness, when construct-ing the TTS, VC, and ASR systems, only the intermediate semantic coding (representations extracted by these models) is different, the remaining structures are completely identical, consistent with Figure3 and trained using the same dataset for fairness. Our model frameworks are aligned with the state-of-the-art methods: SoVITS\u00b9 for VC and SPEAR-TTS [14] for minimally-supervised TTS. All comparison models were trained from scratch using the same dataset. The memory sizes of the speech encoders for VQ-CTAP, Wav2Vec 2.0, Encodec, HuBERT, and Whisper are 105MB, 310MB, 57MB, 308MB, and 244MB, respectively. While VQ-CTAP has a denser structure compared to other models, it has fewer parameters than HuBERT and Wav2Vec 2.0. Moreover, VQ-CTAP uniquely supports plug-and-play downstream tasks, a feature not offered by the other models.\nAdditionally, we conduct the following ablation experi-ments: VQ-CTAP w/o PhonemeDecoder does not include $L_{classify}$ during training. VQ-CTAP w/o Compression re-moves the length compression and restoration operations from the encoders/decoders, VQ-CTAP w/o Contrastive does not include $L_{contrastive}$ during training. VQ-CTAP w/o Consis-tency does not include the semantic-transfer-wise paralinguis-tic consistency loss during training. VQ-CTAP w/o Unsuper-vised only uses text-speech paired data during training. VQ-CTAP w/o Stepping does not use the stepping optimization strategy during training, and all loss weights are the same. The VQ-CTAP (Ground Truth) structure uses intermediate representations extracted from ground truth speech for TTS.\nTo validate the effectiveness of the sequence-aware semantic connector, we separately train a two-stage TTS system VQ-CTAP + Separate based on VQ-CTAP representations as a control model."}, {"title": "D. Evaluation Metrics", "content": "For the TTS and VC tasks, we conduct all subjective tests using 25 paid human evaluators to assess speech quality, prosody, and timbre similarity."}, {"title": "A. Compression and Decoupling Ability", "content": "Table I compares the compression ratios of VQ-CTAP and mainstream speech representation models. It shows that"}]}