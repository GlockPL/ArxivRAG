{"title": "A Frontier AI Risk Management Framework: Bridging the Gap Between Current AI Practices and Established Risk Management", "authors": ["Sim\u00e9on Campos", "Henry Papadatos", "Fabien Roger", "Chlo\u00e9 Touzet", "Otter Quarks", "Malcolm Murray"], "abstract": "The recent development of powerful AI systems has highlighted the need for robust risk management frameworks in the AI industry. Although companies have begun to implement safety frameworks, current approaches often lack the systematic rigor found in other high-risk industries. This paper presents a comprehensive risk management framework for the development of frontier AI that bridges this gap by integrating established risk management principles with emerging AI-specific practices. The framework consists of four key components: (1) risk identification (through literature review, open-ended red-teaming, and risk modeling), (2) risk analysis and evaluation using quantitative metrics and clearly defined thresholds, (3) risk treatment through mitigation measures such as containment, deployment controls, and assurance processes, and (4) risk governance establishing clear organizational structures and accountability. Drawing from best practices in mature industries such as aviation or nuclear power, while accounting for AI's unique challenges, this framework provides AI developers with actionable guidelines for implementing robust risk management. The paper details how each component should be implemented throughout the life-cycle of the AI system - from planning through deployment - and emphasizes the importance and feasibility of conducting risk management work prior to the final training run to minimize the burden associated with it.", "sections": [{"title": "1 Introduction", "content": "The literature on risk management is very mature and has been refined for decades in a range of industries where safety is paramount. However, as of today, few of these principles have been applied to the risk management of frontier AI systems development, despite the mounting warnings from experts that the latter bears serious risks, ranging from enabling malicious actors to carry out cyberattacks (Fang et al., 2024) or create chemical, biological, radiological and nuclear (CBRN) weapons (Pannu et al., 2024), up to potential human extinction (Center for AI Safety, 2023).\nThe AI industry could benefit from the lessons learned and tried and tested risk management practices in other industries. To that end, this paper presents a new AI risk management framework drawing from established risk management practices in other industries, as well as existing AI risk management approaches. This framework has four components: (1) risk identification, (2) risk analysis and evaluation, (3) risk treatment, and (4) risk governance."}, {"title": "2 Background and Motivation", "content": null}, {"title": "2.1 Blind Spots in Existing Safety Policies from AI Companies", "content": "Al developers have started to propose methods to manage advanced AI risks, with a particular focus on catastrophic risks. The most prominent examples to date include Anthropic's Responsible Scaling Policy (Anthropic, 2024), OpenAI's Preparedness Framework (OpenAI, 2023) and Google DeepMind's Frontier Safety Framework (Google DeepMind, 2024). At the 2024 AI Seoul Summit, a further thirteen AI developers (Amazon, Cohere, G42, IBM, Inflection AI, Meta, Microsoft, Mistral AI, Naver, Samsung Electronics, Technology Innovation Institute, xAI and Zhipu.ai) agreed to also publish their safety frameworks by the time of the next AI Summit in France in February 2025 (Department for Science, Innovation and Technology, 2024).\nComparing these proto-risk management frameworks with established risk management practices, it is striking to note that these initiatives generally do not build upon or reference the risk management literature. Research analyzing these policies (SaferAI, 2024; Institute for AI Policy and Strategy, 2024) has revealed that they deviate significantly from risk management norms, without clear justification. Several critical deficiencies have been identified: the absence of well-defined risk tolerance (risk thresholds), the lack of quantitative or semi-quantitative assessment of risks, and the lack of systematic risk identification. The absence of a well-defined risk tolerance is especially concerning, as it can lead to risk continuously increasing, with mitigations becoming less and less adequate as capabilities increase and as it becomes increasingly hard to maintain risk below acceptable levels."}, {"title": "2.2 Our Approach: Applying Risk Management Techniques to Frontier AI Development", "content": "The field of risk management comprises a rich set of techniques in many different industries, from aviation (Leveson et al., 2014) to nuclear power (International Atomic Energy Agency, 2010). Raz and Hillson's (2005) comprehensive review of existing risk management practices reveals five common steps shared in most industries:\n1. Planning: This step consists of establishing the risk context, allocating resources, setting acceptable risk thresholds, defining the governance structure and assigning roles and responsibilities.\n2. Identification: In this step, potential risks are identified and modeled and risk sources are established.\n3. Analysis: This step focuses on estimating the probability and consequences of identified risks, evaluating them, and prioritizing them.\n4. Treatment: At this stage, the appropriate treatment of the risks is decided and executed.\n5. Control & monitoring: Once risk treatments have been implemented, this step consists of monitoring the evolving status of identified risks and the effectiveness of treatment actions."}, {"title": "3 A framework for Frontier AI Risk Management Drawing on Best Practices", "content": "The framework in this paper is divided into four components: risk identification, risk analysis & evaluation, risk treatment, and risk governance, which can be compared with Raz and Hillson's (2005) steps described above (i.e. Planning, Identification, Analysis, Treatment, and Control & monitoring), and the steps described in the US NIST's AI Risk Management Framework (RMF) (i.e. Govern, Map, Measure and Manage) (NIST, 2023). Figure 2 shows how the four components interact with each other and provides examples for each.\n\u2022 Risk identification is the process of identifying risks and understanding their nature (i.e. risk sources and risk scenarios) (ISO/IEC, 2009).\n\u2022 Risk analysis and evaluation is a process that starts with the definition of a risk tolerance. This risk tolerance is then operationalized into risk indicators and their corresponding mitigations required to reduce risk below the risk tolerance.\n\u2022 Risk treatment corresponds to the process of determining, implementing, and evaluating appropriate risk-reducing countermeasures (NIST, 2024).\n\u2022 Risk governance corresponds to the rules and procedures that structure the risk management system in terms of decision-making, responsibilities, authority, and accountability mechanisms (Lundqvist, 2015).\nThe step of risk identification aligns with Raz and Hillson's \"Identification\" step and NIST's \"Map\" step. Risk analysis aligns with NIST's \"Measure\" step and Raz and Hillson's \"Analysis\" step. This paper's framework also incorporates the definition of acceptable levels of risks from Raz and Hillson's \"Planning\" step here, as these thresholds may be specified according to the different risk domains identified in the risk identification step (e.g., economic damage thresholds for cyber risks, casualties thresholds for biological risks). Risk treatment corresponds to Raz and Hillson's \"Treatment\" and NIST's \"Manage\" section. This paper also incorporates Raz and Hillson's \"Control & Monitoring\" dimension in risk treatment, because monitoring and mitigations are core to maintaining the risk below acceptable levels. Risk governance is covered by Raz and Hillson under \"Planning\"; however, following the NIST AI RMF, this paper argues that it should be its own component in the field of AI, given the lack of regulation and the nascency of risk management practices."}, {"title": "3.1 Risk Identification", "content": "Risk identification can be divided into three aspects classification of applicable known risks, identification of unknown risks, and modeling of the risks."}, {"title": "3.1.1 Classification of Applicable Known Risks Using Taxonomies and the Literature", "content": "Developers should address known risks in the literature using resources such as Weidinger et al. (2022) or the AI Risk Repository (Slattery, et al., 2024). Developers should only exclude risks from the scope of their assessment in case of scientific agreement that the specific risk is negligible or unlikely to apply to the AI model under consideration. This decision should be clearly justified and documented."}, {"title": "3.1.2 Identification of Unknown Risks Using Open-Ended Red Teaming", "content": "In addition to risk identification based on the literature, developers should engage in extensive open- ended red teaming efforts, conducted both internally and by third parties. Open-ended red teaming aims to discover unforeseen risks and refers to red teaming practices that do not restrict exploration to predefined risk categories. The primary objectives of this red teaming effort is to identify new unforeseen risk factors that were missed in the risk modeling ahead of time. Long context length is an example: instead of models increasing biorisk through knowledge they acquired during training, models can become helpful through knowledge they acquire during inference. By modifying the risk model, it calls for a revision of the risk assessment to account for this new risk factor.\nThis red teaming effort must include a methodology that systematically explores the AI system for new hazards. The GPT4o model card (OpenAI, 2024) provides a good foundation to build upon. The red team must have the appropriate expertise to properly identify hazards and should have adequate resources, time, and access to the model. Furthermore, AI developers should commit not to interfere with or suppress findings from third-party organizations, as their independent perspective is crucial to identify potential risks that may have been overlooked internally.\nWhen novel high-severity risks are identified during open-ended red teaming, an additional risk analysis effort should be undertaken. The open-ended red teaming can be structured with several stages of triage leading up to an increasingly thorough review and risk modeling process as the vulnerabilities identified get confirmed to be significant. Early on in this triaging process, techniques such as Fishbone diagrams (Coccia, 2018), a structured method that helps identify and categorize the potential causes of an event, can be used."}, {"title": "3.1.3 Risk Modeling", "content": "Risk modeling is the systematic process of analyzing how identified risks could materialize into concrete harms. This involves creating detailed step-by-step scenarios of risk pathways that can be used to estimate probabilities and inform mitigation strategies.\nEstablished safety-critical industries have already developed sophisticated risk modeling approaches. For example, nuclear power plants and aviation use Probabilistic Risk Assessment (PRA) (International Atomic Energy Agency, 2010; Leveson et al., 2014), which combines event trees (showing sequential consequences of initiating events) with fault trees (working backward to identify paths leading to failures). The combination of these methods enables uncovering and modeling large parts of the risk landscape and possible paths to harm. It also allows for the creation of a range of scenarios, the likelihood and severity of which can then be estimated to assess the level of risk quantitatively.\nSimilar principles should be applied to AI systems. Based on risks identified in the literature and during open-ended red-teaming exercises, AI developers should create detailed scenarios mapping how an AI model's capabilities or propensities could lead to real-world harms. These scenarios should break down complex risk pathways into discrete, measurable steps. For example, one step might describe how a language model could enable criminal groups to conduct vulnerability discovery. From the full space of possible scenarios, developers should prioritize risk models that represent the most severe and probable potential harms to guide their evaluation and mitigation efforts.\nThe results of the risk modeling work should be well documented, including the methodologies used, the experts involved, and the list of identified scenarios. This documentation should be shared with relevant stakeholders and used to develop effective evaluation strategies, as well as effective risk mitigation strategies."}, {"title": "3.2 Risk Analysis and Evaluation", "content": "In the second step of the framework, AI developers need first to set a risk tolerance, that is, a risk level that they commit to not overshoot. Second, AI developers must operationalize their risk tolerance. This means translating the risk tolerance into concrete indicators of the level of risk-Key Risk Indicators (KRIs) and the corresponding targets for mitigations Key Control Indicators (KCIs) that have to be reached."}, {"title": "3.2.1 Setting a Risk Tolerance", "content": "A risk tolerance represents the aggregate level of risk that society is willing to accept from AI systems. This tolerance can be established in different ways, depending on the field's maturity.\nIn the long term, risk tolerance should be expressed quantitatively as a product of the probability and severity, following practices established in other industries with societal consequences(Nuclear Regulatory Comission, 1983; Nicholls and Smith, 2020; Institute for AI Policy and Strategy, 2024). In the short term, given the challenges of full quantification, the severity of the risk tolerance can be expressed qualitatively using scenarios, combined with quantitative probabilities. For example: \"No more than 10% chance per year that an LLM enables an actor to damage critical infrastructure.\"\nIn principle, establishing this risk tolerance should be the responsibility of regulators through democratically legitimate processes, as is common in other high-risk industries. For example, in aviation, it is the Federal Aviation Administration, rather than individual companies, that sets the acceptable frequency of catastrophic accidents defined as \"failure conditions that would prevent continued safe flight and landing\" to less than one occurrence per billion flight hours, equivalent to one catastrophic event per 114,155 plane years (Federal Aviation Administration, 1988).\nHowever, the AI industry currently lacks regulatory oversight and no AI developers explicitly set their risk tolerance as described above. This creates a situation where AI developers implicitly define their risk tolerance through their choice of mitigation measures and in a way that remains illegible to most third-parties. In the absence of regulation at the moment, having developers explicitly set and document their risk tolerance would enhance transparency and enable more rigorous risk management practices. When setting their risk tolerance, AI developers should still engage in public consultations and seek guidance from regulators where available. Any significant deviations from risk tolerance norms established in other industries must be justified and documented, for instance, through detailed cost-benefit analyses."}, {"title": "3.2.2 Operationalizing Risk Tolerance", "content": "Risk tolerance must be operationalized into measurable criteria to be practically useful in day-to-day operations. A risk tolerance can be translated into (1) Key Risk Indicator (KRI) thresholds, which are thresholds on measurable signals that serve as proxies for risks, and (2) Key Control Indicator (KCI) thresholds, which are thresholds on measurable signals that serve as proxies for the level of mitigation achieved.\nThese KRI and KCI thresholds must be defined in pairs following an \"if-then\" logic (Karnofsky, 2024): if a specific KRI threshold is reached, then a corresponding KCI threshold must be met to maintain risks below the risk tolerance. For any given risk tolerance level and KRI threshold, there exists a minimum required KCI threshold. This creates a three-way relationship, where setting any two parameters determines the third. In practice, frontier AI has few mitigations available to limit the most consequential risks. As a result, mitigations often tend to be a given and capabilities are the main free variables that AI developers can vary to maintain risk below the risk tolerance.\nRisk models are the core element determining the relationship between KRIs, KCIs and risk tolerance. Risk models contain scenario steps, whose probability, severity or magnitude can be quantitatively estimated. This quantitative estimation could be acheived by leveraging expert elicitation techniques grounded in empirical measurements. For example, an expert Delphi survey (Hsu and Sandford, 2007) can help determine which cybersecurity task an LLM would have to succeed at to increase by 10 percentage points the odds that a cybercrime group manages to deploy malware in an S&P 500 company. A set of tasks selected through that methodology, effectively a benchmark, can be used as a KRI that maps to risk tolerance. Its key difference with traditional benchmarks is that it would have a clear real-world meaning determined by experts in the way it is conceived.\nIf the required KCI threshold cannot be achieved, development must be put on hold until sufficient controls are implemented to meet the threshold, to ensure compliance with the risk tolerance."}, {"title": "Setting Key Risk Indicator Thresholds", "content": "The most important KRI threshold for AI developers will be the level of model capabilities (a primary source of risk), sometimes called capability thresholds. In addition to internal KRIs, AI developers should also identify and monitor external KRIs that signal changes in the level of risk in the external environment, such as an increased use of LLMs for cybercrime or changes in tools and scaffolding used in conjunction with the AI models.\nKRI thresholds should be established for all risks identified in the risk identification phase. These thresholds should be carefully defined to provide clear signals when they are approached or breached. The precision in defining these thresholds ensures that they serve as effective triggers for implementing risk mitigation."}, {"title": "Setting Key Control Indicator Thresholds", "content": "For each key control indicator threshold, AI developers should define KCI thresholds, establishing a measurable target that ensures that mitigations enable risk to remain below risk tolerance even as capabilities increase. This mitigation target should build on risk modeling. By attributing probabilities to each step in the identified risk scenarios, developers can generate quantitative estimates of post-mitigation risk. These estimates must remain below the risk tolerance.\nKey Control Indicator thresholds serve two functions. The first function is to have a measurable indicator of whether mitigations are working sufficiently well. The second function is to summarize the effect of combined mitigations in a single number, making clearer how specific mitigations reduce risks. Setting KCI thresholds as an intermediary step simplifies risk modeling by allowing developers to first determine \"how much\" mitigation is needed before deciding \"how\" to achieve it through specific mitigation measures.\nKCIs should be developed for three different types of mitigation:\n\u2022 First, containment KCIs. To the extent possible, KCIs should be defined. If continuous metrics can't be set, they can be replaced by security levels, that correspond to ease of access to the model through theft for various stakeholders. (Nevo et al., 2024). Those should be audited by external stakeholders to be deemed valid.\n\u2022 Second, deployment KCIs. These should be commensurate with the model's potential for misuse and its propensity to cause accidental risks. Such KCIs could measure how jailbreakable a model is, either during testing (e.g. a jailbreak benchmark score) or during deployment (e.g. the number of API interactions that are jailbroken per month).\n\u2022 Third, assurance processes KCIs. Beyond a certain level of dangerous capabilities, capability evaluations will become insufficient to demonstrate the absence of risk attached to a model (Clymer et al., 2024). At that point, models will be capable of causing harm and it will become crucial to provide affirmative evidence that they won't. We call processes that can provide such evidence assurance processes4. Assurance processes can be defined as the processes that can provide affirmative safety assurance of a model once the model has dangerous capabilities. For instance, an assurance process KCI for interpretability could be the percentage of neural network components that can be reverse engineered beyond a certain faithfulness (Wang et al., 2022)."}, {"title": "An Example of Risk Tolerance Operationalization", "content": "As of early 2025, no assurance processes providing high-safety guarantees for large language models have been demonstrated yet. However, there are ongoing efforts to reach low levels of assurance (Greenblatt et al., 2024).\nHere is an illustration of how risk tolerance, KRI thresholds, and KCI thresholds may be linked:\n\u2022 First, the risk tolerance, when defined quantitatively, can be viewed as a risk 'budget': developers are permitted to 'spend' an overall quantity of risk when developing models, up to the defined risk tolerance. This budget can be allocated to the development of different capabilities, each associated with specific risk scenarios. For instance, developers might decide to focus on improving coding capabilities, which is associated with scenarios of AI-enabled cyberattacks. In this case, they might allocate a larger portion of their risk budget to address potential cyber-misuse risks. The allocation of risk budgets should be guided by expert input, considering the expected benefits of each capability, strategic priorities, and the intended focus of the model.\n\u2022 Second, KRI and KCI thresholds are determined using expert inputs and in-depth risk modeling. As an illustrative fictional example, \"if a model reaches 60% on Cybench (Zhang et al., 2024) (KRI threshold), then maintaining cyber security level 3 (Nevo et al., 2024) (KCI threshold) is required to keep the risk of causing economic damages below $500M below 1% per year.\""}, {"title": "3.3 Risk Treatment", "content": "The third component of the risk management framework is risk treatment, where mitigation measures are implemented to control the level of risk within the limits established in the earlier step (i.e., reach the KCI threshold and remain below the risk tolerance)."}, {"title": "3.3.1 Implementing Mitigation Measures", "content": "AI developers should operationalize at least internally their KCI thresholds, defined in the risk analysis and evaluation step (Section 3.2.2), into mitigation measures.\nContainment measures are largely information security measures that allow controlling access to the model for various stakeholders. For the potential loss of control risks, containment also includes containing an agentic AI model. Examples include extreme isolation of weight storage, strict application allow-listing, and advanced insider threat programs (Nevo et al., 2024).\nDeployment measures aim to mitigate risks resulting from the usage of the model. These are the set of measures that allow controlling the potential for misuse of the model in dangerous domains and its propensity to cause accidental risks. Examples include API input / output filters, safety fine-tuning, and knowing your customer policies (Department for Science, Innovation and Technology, 2023).\nFinally, as discussed above, past a certain level of dangerous capability, implementing credible mitigation measures is likely to require setting assurance processes, supported by evidence that these are enough to achieve the risk tolerance. Understanding that those assurance processes don't yet exist, AI developers should have credible plans towards the development of such processes. For their assurance process plan, AI developers should clearly specify the underlying assumptions that are essential for its effective implementation and success. Examples include using advanced interpretability to reliably detect deception and formal verification (Dalrymple et al., 2024)."}, {"title": "3.3.2 Continuous Monitoring and Comparing Results with Pre-determined Thresholds", "content": "Unlike in some other industries where risks primarily materialize when the final system is deployed (e.g., an aircraft's safety risks emerge once it starts flying), AI systems can pose risks throughout their development cycle. For instance, loss of control scenarios could materialize during the training process itself, requiring continuous monitoring and risk mitigations well before deployment.\nThis means that capability evaluation is not a one-off affair, but should be repeated regularly during training and during deployment. Developers must therefore implement continuous monitoring of both KRIs and KCIs to ensure that KCI thresholds are met once KRI thresholds are crossed according to the predefined \"if-then\" statements established in the risk analysis and evaluation phase.\nEstablishing such quantitative links between capabilities and risks using current AI risk management practices remains challenging. Further advances in quantitative AI risk assessment methods will be necessary to enable proper risk management.\nAI developers should establish rigorous evaluation protocols designed to produce upper bound estimations of Al systems' capabilities in order to ensure that KRI thresholds are not crossed unnoticed. These protocols should specify the evaluation frequency in terms of both the relative variation of effective computing power used in training and fixed time intervals to account for post- training enhancements (Anthropic, 2024). Evaluations must be performed sufficiently frequently. The elicitation methods used during the evaluations must be comprehensive enough to match the elicitation efforts of potential threat actors. Increased test-time computing power must be included in elicitation efforts. The evaluation environment and methodology must be documented, including specifying how post-training enhancements are factored into capability assessments.\nSimilarly, AI developers should monitor KCIs to ensure that mitigation measures are functioning appropriately and are meeting the KCI thresholds.\nIndependent third parties should vet evaluation protocols. These third parties should also be granted permission and resources to independently perform their evaluations, verifying the accuracy of the results. In addition, AI developers must commit to sharing the evaluation results with relevant stakeholders as appropriate."}, {"title": "3.4 Risk Governance", "content": "The last part of the risk management framework is risk governance. Risk governance consists of defining the decision-making structure for the risk identification, analysis and evaluation, and treatment components. In essence, it consists of defining \"who does what\" and \"who verifies how it is done,\" ensuring there are clear roles and responsibilities for decision-making in the risk management processes.\nRisk governance can be seen as a set of interlocking components that play unique roles and jointly form a cohesive governance structure. As seen in Figure 2 above, these components can be placed in six distinct categories, each fulfilling different purposes. Three categories relate to risk decisions inside the organization, two categories to oversight that is inside the organization, but independent, and one category to the communication of decisions outside the organization. The remainder of this section outlines the six categories, how they relate to each other, and why each one is essential."}, {"title": "3.4.1 Decision-Making", "content": "The first category of risk governance components is at the core of risk management, that is, the decisions made by senior management that create or mitigate risk. Best practices from other industries include the establishment of clear risk ownership, with designated senior managers responsible for specific risks and a dedicated senior-level committee for risk-focused decision-making (Committee of Sponsoring Organizations of the Treadway Commission, 2017). Senior management should have clear go/no-go decision protocols and rules to follow in their decision-making (Eisenhardt, 1989; Hammond et al., 1998). Finally, since AI risk evolves rapidly, there should also be a clear escalation process for rapid decision-making when there are changes in risk levels(Committee of Sponsoring Organizations of the Treadway Commission, 2017)."}, {"title": "3.4.2 Advisory and Challenge", "content": "The second category consists of risk experts who advise and challenge senior management on their decisions. The senior managers making risk decisions need to be distinct from those advising on the decisions, to avoid conflicts of interest. Therefore, there should be a senior executive responsible for risk management processes (often called a Chief Risk Officer) who is accountable for the risk management processes, but is importantly not a risk owner making risk decisions themselves. Without them, senior management's decisions are likely to be subject to short-term pressures such as deadlines or performance, at the expense of safety.\nTo provide support to the Chief Risk Officer, it is common in many industries to have a central risk function. This function is in charge of the risk management process, providing support and advice, challenging management on the soundness of their decision-making, and tracking and monitoring risks."}, {"title": "3.4.3 Culture", "content": "The third category consists of the components that influence everyone in the organization, through the culture. The major decisions on risk will be made by senior management, advised by risk experts. However, risk is created, influenced and observed by people who are several levels below senior management. This creates the need for a focus on risk culture (known in some industries as safety culture). This is the \"set of norms, attitudes and behaviors related to awareness, management and controls of risks\" (European Central Bank, 2023). These shape day-to-day decisions impacting risks.\nAnother key element of culture is speak-up culture, \u201ca workplace environment where employees feel comfortable speaking their minds, sharing their ideas, and raising concerns without fear of negative consequences\u201d (West, nd). In aviation, this is often called a \"just culture\", a culture without retaliation for speaking up and reporting problems (Parker, 2014). A key feature of a speak-up culture is whistleblowing - processes for anonymously reporting issues. This is important at AI developers, where new risks and safety issues might be discovered serendipitously.\nAll aspects of culture are ultimately driven by the \"tone at the top\". This can be defined as \"top management's way to express [...] values pursued in the organization and provide guidance to employees\" (Ewelt-Knauer et al., 2020). It refers to the communications made by senior leadership on risk and safety and is a core cultural element influencing how the whole organization will take decisions that impact risks.\nFinally, incentives to perform and report positive information at each level of management can filter out negative information and cause senior management to receive only a small fraction of the information relevant to risk decision-making. A poor organization-wide culture can lead the leadership to systematically underestimate risk."}, {"title": "3.4.4 Oversight", "content": "Board-level oversight is necessary to provide checks and balances to senior management. The Board of Directors is the only body that provides oversight over the decisions of senior management and is specifically tasked with considering the longer time horizon. Without the Board, senior management can overly optimize for short-term performance, ignoring risks. The Board typically has one subcommittee focused on risk, either called an Audit committee or a Risk committee. Audit committees are one of the standard committees, alongside Remuneration and Nomination, that exist in most publicly listed companies. They are prescribed in US listed companies under the Sarbanes-Oxley regulation of 2002 (Coates, 2007). In the Financial Services industry, there are also often dedicated risk committees. There can also be governing bodies separate from the Board that fulfill a similar function, such as Anthropic's Long-Term Benefit Trust (LTBT) (Anthropic, 2023)."}, {"title": "3.4.5 Audit", "content": "The fifth category consists of independent groups that test the efficacy and sufficiency of the risk management framework and of the risk mitigations. Without independent groups providing regular checks on risk management processes, they can quickly deteriorate in quality and become purely performative. Audits are provided by internal auditors and/or external auditors. In both cases, they are independent from peer pressure dynamics occurring within the teams dealing with the risk. Internal Audit is a part of the organization, but has a unique reporting line to the Audit committee of the Board to avoid conflicts of interest with the business. It has the mandate to investigate any process in the organization. This is a common function in publicly listed organizations and is a listing requirement of many stock exchanges, such as the New York Stock Exchange (NYSE, 2024).\nInternal audit teams may lack expertise in certain risk areas, particularly technical risks. Therefore, most industries also use external auditors. These are specialists from outside the organization who are brought in to provide additional assurance expertise. Publicly listed companies in the United States for example must, per the SEC's (Securities and Exchange Commission) regulations, employ an external auditor for their financial statements (U.S. Securities and Exchange Commission, 2002). External audit providers can also provide assurance in other areas, such as quality or cybersecurity. In AI, external auditors are already used for red-teaming and model capability evaluations."}, {"title": "3.4.6 Transparency", "content": "The last category of risk governance focuses on external communication of risks and decision-making. Without transparency, the public and regulators would have no way of judging whether the company adequately manages risk.\nThe first type of communication is external disclosure of the risks faced by the organization. In the United States, this is required for listed companies and is provided in the annual report(White & Case LLP, 2023). In the case of AI, given the broad nature of risks the technology poses, this should be broadened to include risks to society from the company's products.\nIt is also important that the organization discloses details on their governance structure to provide transparency on how risk is managed. In other industries, annual reports must disclose elements of governance, both in the United States (U.S. Securities and Exchange Commission, 2024) and in other jurisdictions (UN Trade and Development, 2006).\nIn the case of AI, with rapidly changing risks and significant potential for hidden errors, it is also vital that organizations provide external incident reporting. This type of reporting can be addressed to industry bodies, such as the Frontier Model Forum (Frontier Model Forum, 2023), or to regulators."}, {"title": "4 Implementing the Frontier AI Risk Management Framework", "content": null}, {"title": "4.1 Risk register", "content": "Throughout the risk management process, AI developers should maintain a continuously up-to-date risk register, which serves as the central repository for documenting and tracking all identified risks and their associated mitigation measures. This is key as an internal tool for all employees of the company to have up-to-date information regarding the status of each risk.\nFor each risk identified, we suggest that the risk register could document the following information:\n\u2022 Risk owner: The individual within the organization who is responsible for ensuring the appropriate management and mitigation of risk.\n\u2022 Risk level: The inherent risk level (pre-mitigation) and the residual risk level (post-mitigation) of the riskiest model.\n\u2022 Key Risk Indicators (KRIs): Proxy measures that are tracked to monitor and assess risk status (for more definition, see Section 3.2.2).\n\u2022 Mitigation status and Key Control Indicators (KCIs): The Key Control Indicators and measures, together with their effectiveness in reducing risk level (for a more detailed definition, see Section 3.2.2).\n\u2022 KRI / KCI mapping: The description of levels of KRIs at which particular KCIs must be achieved, along with the corresponding expected residual risk.\n\u2022 Action plan: A detailed plan that outlines the key actions required to manage risk in the short and medium term, including timelines, responsibilities, and allocation of resources."}, {"title": "4.2 Life-cycle", "content": "In order to implement the risk management framework described in this paper, it is necessary to understand at what point of the AI life-cycle components should be executed. This paper argues that conducting the majority of risk management activities during the planning phase, before the final training run begins, appears adequate for two key reasons. First, the planning phase provides the necessary time horizon, as many crucial elements, such as implementing robust containment measures, require a long time. Second, conducting risk management activities in parallel with capability research and development, well before deployment, helps avoid the commercial pressure to compromise on safety measures to meet release deadlines. This section examines the appropriate timing for each framework component in the planning, training, and post-deployment phases."}, {"title": "4.2.1 Planning Phase", "content": "In the planning phase, AI developers allocate resources for model training, determine model parameters such as compute requirements, and conduct preliminary experiments before the final training run begins. A large part of the risk management effort can already occur in this phase.\nDuring planning, developers should notably perform risk identification based on known risks from the literature and perform the corresponding risk modeling. The risk tolerance should be established, along with defined KRI thresholds and their corresponding KCI thresholds that are sufficient to maintain risks below the tolerance level. Using scaling laws (Ruan et al., 2024; Kaplan et al., 2020; Hoffmann et al., 2022), developers can anticipate which KRI thresholds are likely to be crossed during development and can therefore implement appropriate mitigation measures ahead of time to ensure that they will be ready when needed. However, even though scaling laws provide valuable predictive power for capability trends, empirical evaluation remains essential to accurately measure KRIs throughout the training phase and verify these predictions."}, {"title": "4.2.2 Training Phase", "content": "During the training phase, AI developers should conduct open-ended red teaming to identify any unexpected risks or emerging capabilities. If significant findings emerge from this analysis, developers should update their risk models, KRI and KCI thresholds as needed. Throughout the training, developers must continuously monitor KRIs. When KRI thresholds are crossed, they should ensure that corresponding mitigation measures are implemented to meet the KCI thresholds. Parallel continuous monitoring of KCIs is also essential to verify that these thresholds continue to be met."}, {"title": "4.2.3 Post-Deployment Phase", "content": "After deployment, developers must maintain continuous monitoring of both KRIs and KCIs to ensure all thresholds remain within acceptable bounds. This monitoring should include vigilance for improvements in post-training enhancements that may affect the level of risk that a model induces. Additionally, deployment could enable"}]}