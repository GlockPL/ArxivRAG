{"title": "PersLLM: A Personified Training Approach for Large Language Models", "authors": ["Zheni Zeng", "Jiayi Chen", "Huimin Chen", "Yukun Yan", "Yuxuan Chen", "Zhiyuan Liu", "Maosong Sun"], "abstract": "Large language models (LLMs) exhibit aspects of human-level intelligence that catalyze their application as human-like agents in domains such as social simulations, human-machine interactions, and collaborative multi-agent systems. However, the absence of distinct personalities, such as displaying ingratiating behaviors, inconsistent opinions, and uniform response patterns, diminish LLMs' utility in practical applications. Addressing this, the development of personality traits in LLMs emerges as a crucial area of research to unlock their latent potential. Existing methods to personify LLMs generally involve strategies like employing stylized training data for instruction tuning or using prompt engineering to simulate different personalities. These methods only capture superficial linguistic styles instead of the core of personalities and are therefore not stable. In this study, we propose personified LLM (PersLLM), integrating psychology-grounded principles of personality\u2014social practice, consistency, and dynamic development into a comprehensive training methodology. Through personified data construction and model training, we incorporate personality traits directly into the model parameters, enhancing the model's resistance to induction, promoting consistency, and supporting the dynamic evolution of personality. Single-agent evaluation validates our method's superiority, as it produces responses more aligned with reference personalities compared to other approaches. Case studies for multi-agent communication highlight its benefits in enhancing opinion consistency within individual agents and fostering collaborative creativity among multiple agents in dialogue contexts, potentially benefiting human simulation and multi-agent cooperation. Additionally, human-agent interaction evaluations indicate that our personified models significantly enhance interactive experiences, underscoring the practical implications of our research.", "sections": [{"title": "Introduction", "content": "Large Language models (LLMs), due to their large-scale parameters and training data, demonstrate human-level intelligence across numerous domains Brown et al. (2020); Achiam et al. (2023). This development has inspired researchers to investigate the potential of employing LLMs as human-like agents in various contexts, including social simulations, human-machine interactions, and collaborative multi-agent systems Grossmann et al. (2023); Yang (2024); Wang et al. (2024). The personalization of these human-like agents plays a crucial role. For example, in social simulation contexts, only agents that possess extensive personified traits can authentically emulate human perspectives and behaviors. In human-machine interaction settings, agents featuring personality traits significantly enhance user acceptance and comfort Pelau et al. (2021). Additionally, in tasks involving multiple agents, the interaction among entities with varied personality traits can markedly enhance both the quality and creativity of task execution G\u00fcver and Motschnig (2017).\nNevertheless, the current generation of LLM-driven agents exhibits a notable deficiency in personified characteristics, often presenting overly uniform values and behavior patterns, along with a propensity to cater to user preferences Wei et al. (2023); Liu et al. (2023), which substantially curtail their applicability in real-world settings. In response to these challenges, some studies have initiated efforts to construct LLMs endowed with a broader spectrum of personality traits. Current research in modeling personality primarily adopts two mainstream approaches, each with distinct methodologies and inherent limitations. The first, prompt-based methods, rely on external prompt engineering to specify personified traits of agents Wei et al. (2023); Liu et al. (2023). These methods depend on the model's strong capacity to comprehend and reason over long texts and the accuracy of the retrieval module to provide context-relevant information. Naturally, they are limited by the model's maximum context length, restricting the scope of personality information that can be processed. The second approach, training-based methods, integrates personal characteristics into the model's internal parameters by by targeting specific types of data Zhou et al. (2023a); Wang et al. (2023c). These methods usually focus on singular aspects of characteristics such as linguistic styles or anecdotes, thus limiting the application scenarios for the models. Overall, while both approaches attempt to integrate personality traits into models, they only capture superficial and fragmented aspects of these traits and fail to fully address the complexity of personal characteristics.\nPsychologists view personality as a dynamic organisation, inside the person, of psychophysical systems that create a person's characteristic patterns of behaviours, thoughts, and feelings Carver (2011). It is formed gradually in"}, {"title": "Results", "content": "Considering that there exist plenty of records for various characters in the Harry Potter novels, we propose a dataset called Harry Potter personified dataset (HP dataset) for training and numerical evaluation of our personified training approach. HP dataset includes 6 fictional characters which span different ages, genders, positions, and richness of information, thus these characters are quite representative for evaluating the personified training. We collect the raw data from two sources: 1. The Harry Potter Wiki * data, including the basic information, temporal experiences, and the special social or magic knowledge that are related to the target characters; 2. The character speeches data, selected from the original novels and filtered out with the help of GPT-3.5-turbo.\nBased on the raw data, we annotate the HP dataset with the help of the representative LLM GPT-4-turbo. To improve the consistency of the model, we require the LLM to create different types of user inputs for each paragraph of the raw data. Meanwhile, we provide the golden reference paragraph and the other retrieved related paragraphs, and ask the LLM to annotate the response with CoT towards these inputs. To achieve the dynamic development of the personified model, we divide all the raw data into two stages. Items using information from the early plots are in the early stage, and those using information from the late plots are in the late stage. If the time of the information is vague (such as from an interview with the author himself), then it is seen as public information and can be used by both stages. In the personified model training stage, drawing from the interpretations that personality is driven by the inside psychology system, we internalize the extensive personality information into model parameters through training rather than external methods such as RAG. First, we conduct personified conversational tuning, which involves tuning the model with specially constructed personified conversation data alongside some general instruction-tuning data to preserve its generalizability. Subsequently, to further ensure the consistency of PersLLM, we introduce the Chain-of-Thought (CoT) prompting strategy, which requires the model to display a detailed, step-by-step analytical process before delivering responses. This strategy, coupled with the anti-induced data policy, not only fosters a deeper reasoning capability but also combats the tendency towards uncritical"}, {"title": "Backbone Models and Baselines", "content": "For the backbone model used for personified training, we prefer lightweight LLMs so that we can conduct more personalities within limited time and resources. Meanwhile, the commonsense reserve of the LLMs is important because we do not emphasize the general domain knowledge in personified training. Therefore, we adopt MiniCPM-2.4B Hu et al. (2024) as the backbone model for our experiment. It is an end-side LLM gaining the instruction following ability during pre-training, and has achieved the best performance among lightweight LLMs on several datasets. Therefore, we believe that it is very suitable for personified training in different characters. Besides, GPT-4-turbo is also involved in our experiment, which is both the annotation LLM that we use for personified conversation data, and one of the most recognized models in personified inference."}, {"title": "Methodology Details", "content": "The personified training is structured in two stages: initially, we engage in personified conversational tuning using a blend of personified and general instruction tuning data. Subsequently, automatic DPO training is applied.\nWe use strategies including temporal label, CoT and anti-induced conversation during data construction. To be specific, we add new tokens (e.g., \"<TIME-I>\", \"<TIME-II>\") for the model embedding and tokenizer, and insert the new tokens into the prompt to distinguish different personified stages. Meanwhile, we require the LLMs to first analyze and then respond as the target personality when annotating the conversation data, and use specific delimiters to differentiate between two parts (e.g., \"[Analysis] The user is asking about ... Therefore, I should correct this error. [Response] I'm afraid you're mistaken...\"). As for the anti-induced conversation, we require the LLMs to propose error facts or knowledge by prompt engineering. We also conduct automatic DPO training to further align the model to the target personality. To construct the direct preference data automatically, we put forward a hypothesis: responses that correspond to different inputs but are similar to the current response are very likely to be interference error items. For example, the same question concatenated with different temporal labels gets different inputs, and the corresponding responses may contain very similar attitudes but different tones. Interfering items may also include different personalities' attitudes towards the same event, views from the same personality on related events, etc. Therefore, we use the gtr-t5-base model Ni et al. (2022) to encode the responses of all personalities in the HP dataset. For each item of training data, we use the original response as a positive item, and remove the 10 responses with the highest embedding similarities (to avoid the existence of responses with exactly the same meaning). Then we select the most similar response as a negative item to widen the encoding gap of the model during the DPO training process. To introduce the personified knowledge more directly, we also mix up the existing data with the language modeling / continue writing task of those personified materials. Meanwhile, we randomly allow 1% of the personified conversational to provide the retrieved materials in the input."}, {"title": "Experiment Settings", "content": "Model. MiniCPM-2.4B is a Transformer-based model. We adopt the LLaMa Touvron et al. (2023) version (which is a widely-used open-source LLM) checkpoint, \"MiniCPM-2B-sft-bf16-llama-format\", which contains 40 stacked Transformer layers of decoder, 122, 753 of vocab size, 2,304 dim of hidden states, altogether 2.4 B of parameters. Chinese-LLaMA-2-7B is also a Transformer-based model, and Chinese-Alpaca-2-7B is instruction-tuned based on the LLaMa model. They both contain 32 stacked Transformer layers, 55,296 of vocab size, 4,096 din of hidden states, and altogether 7 B of parameters. For the tuning process, the authors of MiniCPM-2.4B provided some empirical values of hyper-parameters, including batch size per device 32, learning rate 5e5, and max steps 3,000. We conduct grid search near the given values, and set the batch size as 16, learning rate as 5e5, warmup steps as 50, weight decay as 0.1, and max length as 3,000. We repeat the personified data 5 times and mix it with the general instruction tuning data for a total of 1 epoch of training (equivalent to 5 epochs of personified training). The total number of training steps for each model is approximately 3, 500. When tuning the larger model Chinese-LLaMA-2-7B, we adjust the learning rate to 2e -5, and keep most of the hyper-parameters the same with MiniCPM-2.4B tuning."}, {"title": "Ethical Consideration", "content": "Participants of the experiment in this paper are all adults with autonomous behavior, who are aware of and willing to participate in the model chat evaluation experiment and use the data for subsequent scientific research analysis. The experiment is conducted on an online website, and the participants can terminate the experiment at any time. In order to protect participant privacy, the chat records and basic information such as gender and age will be saved only after they confirm.\nPersonified models are an emerging field overall, but some work has begun to discuss its ethical issues, mainly focusing on the dangerous information that may be generated by non-universal alignment and the over-reliance that humans may have when using personified models. For details, please refer to the relevant opinion articles Kirk et al. (2024). In our work, since the target personality is usually a public figure, of whom the relevant text records passed down by them have generally been ethically reviewed. Meanwhile, the GPT series models used in annotating answers also reported measures related to ethical safety techniques in their original papers Achiam et al. (2023), so such a training process will not introduce new ethical risks."}, {"title": "Data Availability", "content": "Data that support the findings of this study have been deposited in Google Drive:"}, {"title": "Code Availability", "content": "The code of this study can be obtained from GitHub:"}, {"title": "Author Contributions", "content": "Zheni Zeng and Yukun Yan contributed to the conception of the study and wrote the manuscript;\nZheni Zeng and Jiayi Chen implemented the model framework and performed the experiment;\nYuxuan Chen implemented the demonstration platform;\nHuimin Chen led the social scientific analysis and discussion;\nZhiyuan Liu and Maosong Sun led and provided valuable advice to the research."}, {"title": "Competing Interests", "content": "The authors declare no competing interests."}, {"title": "Supplementary Information", "content": "Dataset\nThe detailed construction of the HP dataset is shown in Table 2.\nPrompt Settings\nThe detailed prompts are shown in Table 3 and Table 4.\nInteraction Cases\nThe translation for the cooperation instance between John Nash agent and Huiyin Lin agent is shown in Table 5."}]}