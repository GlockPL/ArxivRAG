{"title": "Search-01: Agentic Search-Enhanced\nLarge Reasoning Models", "authors": ["Xiaoxi Li", "Guanting Dong", "Jiajie Jin", "Yuyao Zhang", "Yujia Zhou", "Yutao Zhu", "Peitian Zhang", "Zhicheng Dou"], "abstract": "Large reasoning models (LRMs) like OpenAI-01 have demonstrated impressive\nlong stepwise reasoning capabilities through large-scale reinforcement learning.\nHowever, their extended reasoning processes often suffer from knowledge in-\nsufficiency, leading to frequent uncertainties and potential errors. To address\nthis limitation, we introduce Search-01, a framework that enhances LRMs with\nan agentic retrieval-augmented generation (RAG) mechanism and a Reason-in-\nDocuments module for refining retrieved documents. Search-01 integrates an\nagentic search workflow into the reasoning process, enabling dynamic retrieval\nof external knowledge when LRMs encounter uncertain knowledge points. Addi-\ntionally, due to the verbose nature of retrieved documents, we design a separate\nReason-in-Documents module to deeply analyze the retrieved information before\ninjecting it into the reasoning chain, minimizing noise and preserving coherent\nreasoning flow. Extensive experiments on complex reasoning tasks in science,\nmathematics, and coding, as well as six open-domain QA benchmarks, demon-\nstrate the strong performance of Search-o1. This approach enhances the trust-\nworthiness and applicability of LRMs in complex reasoning tasks, paving the\nway for more reliable and versatile intelligent systems. The code is available at\nhttps://search-ol.github.io/", "sections": [{"title": "Introduction", "content": "Recently emerged large reasoning models (LRMs), exemplified by OpenAI's o1 [22], Qwen-\nQwQ [54] and DeepSeek-R1 [7], employ large-scale reinforcement learning foster impressive\nlong-sequence stepwise reasoning capabilities, offering promising solutions to complex reason-\ning problems [46, 31, 59, 84, 73, 74, 67]. This advancement has inspired a series of foundational\nefforts aimed at exploring and reproducing 01-like reasoning patterns, to broaden their application to\na wider range of foundational models [49, 19, 77, 80, 71, 25, 45].\nIt is noteworthy that o1-like reasoning patterns guide LRMs to engage in a slower thinking process [6,\n61] by implicitly breaking down complex problems, generating a long internal reasoning chain and\nthen discovering suitable solutions step by step. While this characteristic enhances logical coherence\nand interpretability of reasoning, an extended chain of thought may cause overthinking [4] and\nincreased risks of knowledge insufficiency [60, 51, 2], where any knowledge gap can propagate errors\nand disrupt the entire reasoning chain [79, 40, 44, 41]."}, {"title": "Related Work", "content": "Large Reasoning Models. Large reasoning models focus on enhancing performance at test time by\nutilizing extended reasoning steps, contrasting with traditional large pre-trained models that achieve\nscalability during training by increasing model size or expanding training data [17, 66, 50, 85, 76].\nStudies have shown that test-time scaling can improve the reasoning abilities of smaller models on\ncomplex tasks [15, 75]. Recently, models like OpenAI-01 [22], Qwen-QwQ [54] and DeepSeek-\nR1 [7] explicitly demonstrate chain-of-thought reasoning [59], mimicking human problem-solving\napproaches in domains such as mathematics, coding, and so on.\nVarious approaches have been explored to achieve o1-like reasoning capabilities. Some methods\ncombine policy and reward models with Monte Carlo Tree Search (MCTS) [25], though this does not\ninternalize reasoning within the model. Other studies incorporate deliberate errors in reasoning paths\nduring training to partially internalize these abilities [49, 71]. Additionally, distilling training data\nhas been shown to enhance models' o1-like reasoning skills [45]. The ol-like reasoning paradigm\nhas demonstrated strong performance across diverse domains, including vision-language reasoning\n[65, 11, 48, 69], code generation [81, 32], healthcare [3], and machine translation [57]. However,\nthese approaches are limited by their reliance on static, parameterized models, which cannot leverage\nexternal world knowledge when internal knowledge is insufficient.\nRetrieval-Augmented Generation. Retrieval-augmented generation (RAG) introduces retrieval\nmechanisms to address the limitations of static parameters in generative models, allowing access\nto external knowledge to solve more complex problems [30, 82, 35, 86]. Advanced research in this\nfield enhances the RAG system from multiple aspects, including the necessity of retrieval [53], pre-\nprocessing of queries [43, 58], retrieved documents compressing [64], denoising [42, 12], refining [24,\n27, 88], instruction following [9, 8, 87] and so on. Furthermore, some studies have explored end-\nto-end model training to implement RAG systems [1, 36, 33, 34] and knowledge-graph-based RAG\nsystems [14, 37].\nRecently, agentic RAG systems empower models to autonomously determine when and what knowl-\nedge to retrieve as needed, showcasing enhanced planning and problem-solving capabilities [5, 56, 70].\nThere is also research combining agent-based systems with MCTS to optimize complex workflows,\nleveraging retrievers and other tools to accomplish tasks [78]. However, existing RAG approaches\nhave not combined the strong reasoning capabilities of o1-like models, limiting the potential to further\nenhance system performance in solving complex tasks."}, {"title": "Methodology", "content": "We consider a complex reasoning task that necessitates multi-step reasoning and the retrieval of\nexternal knowledge to derive solutions. The objective is to generate a comprehensive solution for\neach question q, consisting of both a logical reasoning chain R and the final answer a. In this work,\nwe enable the reasoning model to utilize external knowledge sources during the reasoning process.\nSpecifically, we consider three primary inputs in the problem-solving process: the task instruction I,\nthe question q, and externally retrieved documents D. Here, I provides an overarching description of"}, {"title": "Problem Formulation", "content": "The goal is to design a reasoning mechanism that effectively integrates I, q, and D to produce a\ncoherent reasoning chain R and a final answer a. This can be formalized as the mapping (I, q, D) \u2192\n(R, a). The generation of the reasoning sequence and the final answer can be expressed as:\nP(R, a | I, q, D) = \\prod_{t=1}^{T_r} P(R_t | R_{<t}, I, q, D_{<t}) \\cdot \\prod_{t=1}^{T_a} P(a_t | a_{<t}, R, I, q),"}, {"title": "Overview of the Search-01 Framework", "content": "The Search-o1 framework addresses knowledge insufficiency in large reasoning models (LRMs) by\nseamlessly integrating external knowledge retrieval into their reasoning process while maintaining\nchain-of-thought coherence. As illustrated in Figure 2, we present a comparative analysis of three\napproaches: vanilla reasoning, agentic retrieval-augmented generation (RAG), and our proposed\nSearch-01 framework.\nVanilla Reasoning Pattern: Consider the example in Figure 2(a), where the task involves de-\ntermining the carbon atom count in the final product of a three-step chemical reaction. The\nvanilla reasoning approach falters when encountering knowledge gaps (e.g., the \"structure of trans-\nCinnamaldehyde\"). Without access to accurate information, the model must rely on assumptions,\npotentially leading to cascading errors throughout subsequent reasoning steps.\nAgentic RAG: To bridge the knowledge gaps during reasoning, we build the agentic RAG mecha-\nnism (Figure 2(b)) to enable the model to autonomously retrieve external knowledge when needed.\nWhen uncertainty arises\u2014such as regarding the compound's structure\u2014the model generates tar-\ngeted search queries (e.g., \u201cstructure of trans-Cinnamaldehyde\"). However, the direct insertion"}, {"title": "Agentic Retrieval-Augmented Generation Mechanism", "content": "The agentic RAG mechanism is a pivotal component of the Search-01 framework, empowering the\nreasoning model to autonomously determine when to retrieve external knowledge during the reasoning\nprocess. This mechanism allows the model itself to decide whether to continue generating reasoning\nsteps or to initiate a retrieval step. Detailed model instructions can be found in Appendix A.1.\nDuring the generation of the reasoning chain R, the model may intermittently generate search queries\nIsearch encapsulated between special symbols </begin_search_query|> and <lend_search_queryl>, where i\nindexes the i-th search step. Each search query is generated based on the current state of the reasoning\nprocess and the previously retrieved knowledge. The generation of each search query is expressed as:\nP(q_{search}^{(i)} | I, q, R^{(i-1)}) = \\prod_{t=1}^{T^{(i)}_{q}} P(q_{search, t}^{(i)} | q_{search, <t}^{(i)}, I, q, R^{(i-1)}),\n(i)\nwhere T\u00b2) is the length of the i-th search query, qsearch, denotes the token generated at step t of the\ni-th search query, and R(i-1) represents all the reasoning steps prior to the i-th search step, including\nboth search queries and search results.\nOnce a new pair of special symbols for the search query is detected in the reasoning sequence, we\npause the reasoning process, and the search query q_{search}^{(i)} is extracted. The retrieval function Search\nis invoked to obtain relevant documents:\nD^{(i)} = Search(q_{search}),\nwhere D(i) = d_1^{(i)}, d_2^{(i)}, ..., d_{k_i}^{(i)} represents the set of top-ki relevant documents retrieved for the i-th\nsearch query. The retrieved documents D(i) are subsequently injected into the reasoning chain R(i-1)\nbetween the special symbols |<|begin_search_result|>|and |<|end_search_result|>, allowing the reasoning model\nto utilize the external knowledge to continue the reasoning process.\nThis agentic mechanism enables the model to dynamically and efficiently incorporate external knowl-\nedge, maintaining the coherence and relevance of the reasoning process while avoiding information\noverload from excessive or irrelevant retrieval results."}, {"title": "Knowledge Refinement via Reason-in-Documents", "content": "While the agentic RAG mechanism addresses knowledge gaps in reasoning, directly inserting full\ndocuments can disrupt coherence due to their length and redundancy. To overcome this, the Search-01\nframework includes the knowledge refinement module, which selectively integrates only relevant and\nconcise information into the reasoning chain through a separate generation process using the original\nreasoning model. This module processes retrieved documents to align with the model's specific\nreasoning needs, transforming raw information into refined, pertinent knowledge while maintaining\ncoherence and logical consistency of the main reasoning chain.\nThe refinement guidelines for Reason-in-Documents are detailed in Appendix A.1. These guidelines\ninstruct the model to analyze the retrieved web pages based on the previous reasoning steps, current\nsearch query, and the content of the searched web pages. The objective is to extract relevant and\naccurate information that directly contributes to advancing the reasoning process for the original\nquestion, ensuring seamless integration into the existing reasoning chain."}, {"title": "Search-01 Inference Process", "content": "For each question, the Search-01 inference begins by\ninitializing the reasoning sequence with the task instruction I concatenated with the specific question"}, {"title": "Inference Logic for a Single Question", "content": "For each search step i, let R^{(<i)} denote the reasoning chain accumulated up to just before the i-th\nsearch query. Given R^{(<i)}, the current search query q_{search}^{(i)}, and the retrieved documents D^{(i)}, the\nknowledge refinement process operates in two stages: first generating an intermediate reasoning\nsequence r_{docs} to analyze the retrieved documents, then producing refined knowledge r_{final} based on\nthis analysis. The generation of the intermediate reasoning sequence r_{docs} is expressed as:\nP(r_{docs}^{(i)} | R^{(<i)}, q_{search}, D^{(i)}) = \\prod_{t=1}^{T_{d}^{(i)}} P(r_{docs, t}^{(i)} | r_{docs, <t}^{(i)}, R^{(<i)}, q_{search}, D^{(i)}),\nwhere T_{d}^{(i)} is the length of the intermediate reasoning sequence, and r_{docs, t}^{(i)} denotes the token at step\nt. The refined knowledge r_{final} is then generated based on this analysis:\nP(r_{final}^{(i)} | r_{docs}^{(i)}, R^{(<i)}, q_{search}) = \\prod_{t=1}^{T_{f}^{(i)}} P(r_{final, t}^{(i)} | r_{final, <t}^{(i)}, r_{docs}^{(i)}, R^{(<i)}, q_{search}),\nwhere T_{f}^{(i)} is the length of the refined knowledge sequence, and r_{final, t}^{(i)} denotes the token at step t.\nThe refined knowledge r_{final} is then incorporated into the reasoning chain R^{(i)}, enabling the model to\ncontinue generating coherent reasoning steps with access to the external knowledge.\nP(R, a | I, q) = \\prod_{t=1}^{T_r} P(R_t | R_{<t}, I, q, \\{r_{final}\\}_{j<i(t)}) \\cdot \\prod_{t=1}^{T_a} P(a_t | a_{<t}, R, I, q),\nwhere \\{r_{final}\\}_{j<i(t)} denotes all previously refined knowledge up to the i(t)-th search step. Here, i(t)\nrepresents the index of the search step corresponding to the current reasoning step t. This refined\nknowledge integration ensures that each reasoning step can access relevant external information while\nmaintaining the conciseness and focus of the reasoning process."}, {"title": "Experiments", "content": "The evaluations used in this experiment include the following two categories:\nChallenging reasoning tasks: (1) GPQA [52] is a PhD-level science multiple-choice QA dataset.\nThe questions are authored by domain experts in physics, chemistry, and biology. In our main\nexperiments, we use the highest quality diamond set containing 198 questions, and in Table 2, we use\na more comprehensive extended set containing 546 questions to compare with the performance of\nhuman experts. (2) Math benchmarks include MATH500 [38], AMC2023 2, and AIME2024 3.\nMATH500 consists of 500 questions from the MATH test set [16]. AMC2023 and AIME2024 are\nmiddle school math competitions covering arithmetic, algebra, geometry, etc., containing 40 and\n30 questions respectively. Among these three datasets, MATH500 and AMC are relatively simple,\nwhile AIME is more difficult. (3) LiveCodeBench [23] is a benchmark for evaluating LLMs' coding\ncapabilities, consisting of easy, medium, and hard difficulty problems. It collects recently published\nprogramming problems from competitive platforms to avoid data contamination. We utilize problems\nfrom August to November 2024, comprising 112 problems.\nOpen-domain QA tasks: (1) Single-hop QA datasets: Natural Questions (NQ) [29] contains\nquestions from real Google search queries with answers from Wikipedia articles. TriviaQA [28] is a\nlarge-scale dataset with questions from trivia websites and competitions, featuring complex entity\nrelationships. (2) Multi-hop QA datasets: HotpotQA [68] is the first large-scale dataset requiring\nreasoning across multiple Wikipedia paragraphs. 2WikiMultihopQA (2WIKI) [18] provides explicit\nreasoning paths for multi-hop questions. MuSiQue [55] features 2-4 hop questions built from five\nexisting single-hop datasets. Bamboogle [47] collects complex questions that Google answers\nincorrectly to evaluate models' compositional reasoning across various domains."}, {"title": "Baselines", "content": "We evaluate our approach against the following baseline methods:"}, {"title": "Implementation Details", "content": "For the backbone large reasoning model in Search-o1, we utilize the open-sourced QwQ-32B-\nPreview [54]. For generation settings, we use a maximum of 32,768 tokens, temperature of 0.7, top_p\nof 0.8, top_k of 20, and a repetition penalty of 1.05 across all models. For retrieval, we employ the\nBing Web Search API, setting the region to US-EN and the top-k retrieved documents to 10. We use\nJina Reader API to fetch the content of web pages for given URLs. For all retrieval-based methods,\nfollowing [52], we apply a back-off strategy where, when a final answer is not provided, we use\nthe result from direct reasoning. For baseline models not specifically trained for o1-like reasoning,\nwe apply Chain-of-Thought (CoT) [59] prompting to perform reasoning before generating answers.\nDetailed instructions for all models are provided in Appendix A. All experiments are conducted on\neight NVIDIA A800-80GB GPUs."}, {"title": "Results on Challenging Reasoning Tasks", "content": "Main Results. Table 1 presents Search-ol's performance on complex reasoning tasks, with the\nmain results outlined below:"}, {"title": "Results on Open-Domain QA Tasks", "content": "In addition to the reasoning tasks where LRMs excel, we also explore the performance of our\nSearch-01 on open-domain QA tasks. Table 3 presents the overall results. The key observations are:\nFor direct reasoning without retrieval, the performance of the LRM QwQ-32B is overall similar\nto the non-reasoning LLM Qwen2.5-32B, with a slight decrease in average EM across all QA\ndatasets (31.3 vs. 30.7). This indicates that LRMs do not perform as strongly on open-domain\nQA tasks as they do on reasoning tasks.\nWhen employing retrieval-augmented reasoning, retrieval significantly improves performance for\nboth reasoning and non-reasoning models across all tasks, suggesting that models have knowledge\ngaps in these tasks. Additionally, for the QwQ-32B model, agentic RAG achieves an average EM\nimprovement of 23.2% over standard RAG on multi-hop QA tasks, demonstrating the effectiveness\nof our agentic RAG strategy in knowledge-based multi-hop QA. However, we also observe that\nthere is no significant performance change for single-hop tasks (47.8 vs. 47.6 on average EM),\nas these questions only require information from a single knowledge point without the need for\nmultiple retrievals. This also verifies that the agentic search mechanism can better unleash the\npotential of LRMs in more complex and challenging reasoning tasks.\nFor our Search-01, we find that it generally outperforms all baselines on multi-hop tasks.\nSpecifically, in terms of the average EM metric, our Search-01 exceeds RAG-QwQ-32B and\nRAgent-QwQ-32B by 29.6% and 5.3%, respectively, demonstrating the effectiveness of our\nReason-in-Documents strategy in complex QA tasks. This further emphasizes the importance\nof maintaining consistency between external knowledge and the logical chain of reasoning."}, {"title": "Conclusion", "content": "In this work, we present Search-01, a framework that addresses the knowledge insufficiency inherent\nin large reasoning models (LRMs) by integrating an agentic retrieval-augmented generation mech-\nanism alongside a Reason-in-Documents module. Our approach enables LRMs to autonomously\nretrieve and seamlessly incorporate external knowledge during the reasoning process, thereby en-\nhancing both the accuracy and coherence of their long-step reasoning capabilities. Comprehensive\nexperiments across diverse complex reasoning tasks in science, mathematics, and coding, as well as\nmultiple open-domain QA benchmarks, demonstrate that Search-01 consistently outperforms existing\nretrieval-augmented and direct reasoning methods. Notably, Search-01 not only surpasses baseline\nmodels in handling intricate reasoning challenges but also achieves performance levels comparable to"}, {"title": "Instructions for Search-01", "content": "You are a reasoning assistant with the ability to perform web searches to help you answer the user's question\naccurately. You have special tools:\nTo perform a search: write <lbegin_search_queryl> your query here <lend_search_queryl>.\nThen, the system will search and analyze relevant web pages, then provide you with helpful information in the\nformat <lbegin_search_result|> ...search results... <lend_search_result|>\nYou can repeat the search process multiple times if necessary. The maximum number of search attempts is\nlimited to {MAX_SEARCH_LIMIT}.\nOnce you have all the information you need, continue your reasoning.\nExample:\nQuestion: \"...\"\nAssistant thinking steps:\n- I might need to look up details about\nAssistant:\n<lbegin_search_queryl>...<lend_search_queryl>\n(System returns processed information from relevant web pages)\nAssistant continues reasoning with the new information...\nRemember:\nUse <lbegin_search_queryl> to request a web search and end with <lend_search_queryl>.\nWhen done searching, continue your reasoning."}, {"title": "Instruction for Reason-in-Documents", "content": "Task Instruction:\nYou are tasked with reading and analyzing web pages based on the following inputs: Previous Reasoning Steps,\nCurrent Search Query, and Searched Web Pages. Your objective is to extract relevant and helpful information for\nCurrent Search Query from the Searched Web Pages and seamlessly integrate this information into the Previous\nReasoning Steps to continue reasoning for the original question.\nGuidelines:\n1. Analyze the Searched Web Pages:\n- Carefully review the content of each searched web page.\n- Identify factual information that is relevant to the Current Search Query and can aid in the reasoning process for\nthe original question.\n2. Extract Relevant Information:\nSelect the information from the Searched Web Pages that directly contributes to advancing the Previous\nReasoning Steps.\n- Ensure that the extracted information is accurate and relevant.\n3. Output Format:\n- If the web pages provide helpful information for current search query: Present the information beginning with\n'Final Information' as shown below.\nFinal Information\n[Helpful information]\n- If the web pages do not provide any helpful information for current search query: Output the following text.\nFinal Information\nNo helpful information found.\nInputs:\nPrevious Reasoning Steps:\n{prev_reasoning}\n- Current Search Query:\n{search_query}\n- Searched Web Pages:\n{document}\nNow you should analyze each web page and find helpful information based on the current search query\n\"{search_query}\" and previous reasoning steps."}, {"title": "Instructions for Standard RAG", "content": "You are a knowledgeable assistant that utilizes the provided documents to answer the user's question accurately.\nQuestion: {question}\nDocuments: {documents}\nGuidelines:\n- Analyze the provided documents to extract relevant information. Synthesize the information to formulate a\ncoherent and accurate answer.\nEnsure that your response directly addresses the user's question using the information from the documents."}, {"title": "Instructions for RAG Agent", "content": "You are a reasoning assistant with the ability to perform web searches and retrieve webpage content to help you\nanswer the user's question accurately. You have special tools:\n- To perform a search: Write '<lbegin_search_queryl>' your query here '<lend_search_queryl>'.\nThe system will call the web search API with your query and return the search results in the format '<lbe-\ngin_search_result|> ...search results... <lend_search_result|>'.\nThe search results will include a list of webpages with titles, URLs, and snippets (but not full content).\n- To retrieve full page content: After receiving the search results, if you need more detailed information from\nspecific URLs, write '<lbegin_url|> url1, url2, ... <lend_urll>'.\nThe system will fetch the full page content of those URLs and return it as '<lbegin_full_pagel> ...full page\ncontent... <lend_full_pagel>'.\nYou can repeat the search process multiple times if necessary. The maximum number of search attempts is limited\nto {MAX_SEARCH_LIMIT}. You can fetch up to {MAX_URL_FETCH} URLs for detailed information.\nOnce you have all the information you need, continue your reasoning.\nExample:\nQuestion: \"...\"\nAssistant thinking steps: - I need to find out ...\nAssistant: '<lbegin_search_query|>...<lend_search_queryl>'\n(System returns search results)\nAssistant: '<lbegin_search_result]> ...search results without full page... <lend_search_result|>' \nAssistant thinks: The search results mention several URLs. I want full details from one of them.\nAssistant: '<lbegin_url|>http://...<lend_urll>'\n(System returns full page content)\nAssistant: '<lbegin_full_page|> ...full page content... <lend_full_pagel>' \nNow the assistant has enough information and can continue reasoning.\nRemember:\n- Use '<lbegin_search_query|>' to request a web search and end with \u2018<lend_search_query|>'.\nUse '<lbegin_urll>' to request full page content and end with '<lend_urll>'.\n- When done retrieving information, continue your reasoning."}, {"title": "Open-Domain QA Tasks Instruction", "content": "Please answer the following question.\nYou should provide your final answer in the format \boxed{YOUR_ANSWER}.\nQuestion:\n{question}"}, {"title": "Math Tasks Instruction", "content": "Please answer the following math question.\nYou should provide your final answer in the format \boxed{YOUR_ANSWER}.\nQuestion:\n{question}"}, {"title": "Multi-choice Tasks Instruction", "content": "You are to answer the following multiple-choice question by selecting the correct option.\nYour final choice should be one of the letters A, B, C, or D. Do not include any answer content beyond the choice\nletter.\nYou should provide your final choice in the format \boxed{YOUR_CHOICE}.\nQuestion: {question}"}, {"title": "Code Tasks Instruction", "content": "Generate a correct Python program that passes all tests for the given problem. You should provide your final\ncode within a Python code block using triple backticks.\n```python\n# YOUR CODE HERE\nProblem Title: {question_title}\nProblem Statement:\n{question}"}, {"title": "Additional Notes", "content": "For all the instructions above, we input them as user prompts, not system prompts. The task-specific\ninstructions in A.4 are used for the QwQ-32B-Preview model. For non-reasoning models like\nQwen2.5-32B-Instruct, Qwen2.5-72B-Instruct, and Llama3.3-70B-Instruct, etc., we add a Chain-of-\nThought prompt \"You should think step by step to solve it.\" before the question to explicitly make\nthese models reason before giving the final answer."}, {"title": "Case Study", "content": "Tables 4, 5, and 6 present examples of outputs from our Search-01 model on the GPQA, AMC2023,\nand HotpotQA datasets, respectively. The model-generated search queries are enclosed within\n|<|begin_search_query|> and <lend_search_query|>, while the refined search results are enclosed within\n|<|begin_search_result|> and <lend_search_result|>. We observe that our Reason-in-Documents mechanism\nprovides coherent information that effectively meets the information needs of the model's current\nreasoning step and seamlessly integrates with the preceding reasoning process."}]}