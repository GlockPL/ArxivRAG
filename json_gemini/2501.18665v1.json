{"title": "BARNN: A Bayesian Autoregressive and Recurrent Neural Network", "authors": ["Dario Coscia", "Max Welling", "Nicola Demo", "Gianluigi Rozza"], "abstract": "Autoregressive and recurrent networks have achieved remarkable progress across various fields, from weather forecasting to molecular generation and Large Language Models. Despite their strong predictive capabilities, these models lack a rigorous framework for addressing uncertainty, which is key in scientific applications such as PDE solving, molecular generation and Machine Learning Force Fields. To address this shortcoming we present BARNN: a variational Bayesian Autoregressive and Recurrent Neural Network. BARNNs aim to provide a principled way to turn any autoregressive or recurrent model into its Bayesian version. BARNN is based on the variational dropout method, allowing to apply it to large recurrent neural networks as well. We also introduce a temporal version of the \"Variational Mixtures of Posteriors\u201d prior (tVAMP-prior) to make Bayesian inference efficient and well-calibrated. Extensive experiments on PDE modelling and molecular generation demonstrate that BARNN not only achieves comparable or superior accuracy compared to existing methods, but also excels in uncertainty quantification and modelling long-range dependencies.", "sections": [{"title": "1 Introduction", "content": "Autoregressive and recurrent models have demonstrated impressive advancements in different disciplines, from weather prediction Bodnar et al. (2024); Lam et al. (2022), to molecules generation Shi et al. (2020); Simm et al. (2020) and Large Language Models (LLMs) Bengio et al. (2000); Vaswani (2017); Radford et al. (2019). However, while autoregressive models show strong predictive abilities, they are also prone to overfitting to the specific tasks they are trained on, challenging their application outside their training domain Papamarkou et al. (2024). Overcoming this behaviour is critical, not only in scientific applications where phenomena can show complex data distribution shifts away from the training data, but also in deep over-parametrized models. For example, in weather forecasting, rapidly changing climate patterns can cause significant deviations from the training data and lead to unpredictable results, while over-parametrized LLMs often provide incorrect answers with high confidence, highlighting issues with model calibration Jiang et al. (2021); Xiao et al. (2022); Yang et al. (2022). This problem is also prevalent across many other domains, and Bayesian approaches present a promising direction for improvement Papamarkou et al. (2024).\n\nThis work aims to close the gap between autoregressive/ recurrent models and Bayesian methods, presenting a fully Bayesian, scalable, calibrated and accurate autoregressive/ recurrent model, named BARNN: Bayesian Autoregressive and Recurrent Neural Network. BARNN provides a principled"}, {"title": "2 Background and Related Work", "content": ""}, {"title": "2.1 Autoregressive and Recurrent Networks", "content": "Autoregressive and Recurrent models represent a joint probability distribution as a product of conditional distributions, leveraging the probability product rule Bengio et al. (2000); Uria et al. (2016). In recent years, autoregressive and recurrent models have been successfully applied in various to different tasks, such as text Bengio et al. (2000); Vaswani (2017), graphs Li et al. (2018); Liu et al. (2018), audio Dieleman et al. (2016), and, more recently, PDE modelling Brandstetter et al. (2022a,b) and molecules generation Segler et al. (2018); \u00d6z\u00e7elik et al. (2024); Schmidinger et al. (2024). Despite their vast applicability, those models are known to overfit, leading to unreliable predictions, especially in over-parameterized regimes, such as modern LLMs, or when the testing data distribution shifts significantly from the training data distribution Papamarkou et al. (2024)."}, {"title": "2.2 Bayesian Modelling and Uncertainty Quantification", "content": "Bayes' theorem Bayes (1763) offers a systematic approach for updating beliefs based on new evidence, profoundly influencing a wide array of scientific disciplines. In Deep Learning (DL), Bayesian methods have extensively been applied Hinton & Van Camp (1993); Korattikara et al. (2015); Graves (2011); Kingma & Welling (2014). They provide a probabilistic treatment of the network parameters, enable the use of domain knowledge through priors and overcome hyper-parameter tuning issues through the use of hyper-priors Papamarkou et al. (2024). Bayesian Models have been historically applied to model epistemic uncertainty, i.e. uncertainty in network parameters. For example in Gal & Ghahramani (2016) the Dropout Srivastava et al. (2014) method was linked to Gaussian Processes Rasmussen & Williams (2005) showing how to obtain uncertainties in neural networks; while in Kingma et al. (2015) the authors connect Gaussian Dropout objectives to Stochastic Gradient Variational Bayesian Inference, allowing to learn dropout coefficients and showing better uncertainty. This last method is the most related work to ours, but differently from our approach, the weights do not evolve in time, leading to less accurate uncertainties (see the experiment section 4.1)."}, {"title": "2.3 Neural PDE Solvers", "content": "A recent fast-growing field of research is that of surrogate modelling, where DL models, called Neural PDE Solvers, are used to learn solutions to complex physical phenomena Li et al. (2020); Bhattacharya et al. (2021); Rozza et al. (2022); Pichi et al. (2024); Coscia et al. (2024); Li et al. (2024). In particular, Autoregressive Neural PDE Solvers Brandstetter et al. (2022b); Sanchez-Gonzalez et al. (2020) gained significant attention since they can be used to infer solutions of temporal PDEs orders of magnitude faster compared to standard PDE solvers, and generate longer stable solutions compared to standard (not-autoregressive) Neural Operators. However, Autoregressive Neural PDE Solvers accumulate errors in each autoregressive step Brandstetter et al. (2022b), slightly shifting the data distribution over long rollouts. This yields inaccurate solutions for very long rollouts, and methods to quantify the uncertainty have been developed: PDE Refiner Lippe et al. (2024) obtains the PDE solution by refining a latent variable starting from unstructured random Gaussian noise, similarly to denoising diffusion models Ho et al. (2020); while in GraphCast Lam et al. (2022) the authors introduced a method called Input Perturbation, which adds small random Gaussian noise to the initial condition, and unroll the autoregressive model on several samples to obtain uncertainties."}, {"title": "2.4 RNN for Molecule Generation", "content": "Drug design involves discovering new molecules that can effectively bind to specific biomolecular targets. In recent years, generative DL has emerged as a powerful tool for molecular generation Shi et al. (2020); Eijkelboom et al. (2024). A particularly promising approach is the use of Chemical Language Models (CLMs), where molecules are represented as molecular strings. In this area, RNNs have shown significant potential Segler et al. (2018); \u00d6z\u00e7elik et al. (2024); Schmidinger et al. (2024). In the experiments section, we will demonstrate how classical RNNs, when combined with BARNN, can enhance performance\u2014particularly by improving the model's ability to capture long-range dependencies, generating statistically more robust molecules, and achieving better learning of molecular properties"}, {"title": "3 Methods", "content": "Bayesian Autoregressive and Recurrent Neural Network (BARNN) is a practical and easy way to turn any autoregressive or recurrent model into its Bayesian version. The BARNN framework creates a joint distribution over the observable states (e.g. language tokens, PDE states, etc.) and model weights, with both alternatingly evolving in time (section 3.1). We show how to optimize the model by deriving a novel variational lower bound objective that strongly connects to the VAEs Kingma & Welling (2014) framework (section 3.2). Finally, we present a scalable variational posterior and prior for efficient and scalable weight-parameter sampling (section 3.3)."}, {"title": "3.1 The State-Weight Model", "content": "Autoregressive models represent a joint probability distribution as a product of factorized distributions over states:\n\n$p(Yo, ..., \u0443\u0442) = \\prod_{t=1}^T P(Yt | Yt\u22121,..., Yo)$,\n\nwhere a new state yt is sampled from a distribution conditioned on all the previous states Yt\u22121,..., Yo, and T is the state-trajectory length. In particular, deep autoregressive models Schmidhuber et al. (1997) approximate the factorized distribution $p(yt | Yt-1,..., Yo)$ with a neural network with optimizable deterministic weights w. We construct a straightforward Bayesian extension of this framework by jointly modelling states yt and weights wt via a joint distribution $P(Y0, W1, Y1, W2,Y2,\u00b7\u00b7\u00b7, WT,YT) = P(Y0:T, W1:T)$ on states and weights, accounting for the variability in time for both\u00b2. The full joint distribution is given by:\n\n$P(YO:T, 31:T) = \\prod_{t=1}^T P(Yt | Yt-1,..., Yo, wt)p(\u03c9t)$.\n\nHence, a new state yt is obtained by sampling from the distribution conditioned on previous states and the current weights wt, while the latter are sampled from a prior distribution $p(wt)$, see Figure 1."}, {"title": "3.2 The Temporal Variational Lower Bound", "content": "Learning the model in eq. (2) requires maximising the log-likelihood logp(y>0). Unfortunately, directly optimizing log $p(yo:T)$ by integrating eq. (2) over the weights, or by expectation maximization is intractable; thus we propose to optimize log $p(yo:T)$ by variational inference Kingma & Welling (2014). The variational posterior over network weights given the states $q(W1:T | Yo:T)$ is parametrized by (different time-independent) weights & and reads:\n\n$q\u03c6(W1:T | YO:T) = \\prod_{t=1}^T q\u00a2(Wt | Yt\u22121,..., Yo)$.\n\nGiven the variational posterior above, in Appendix A.1 we derive the following variational lower bound to be maximised over the variational parameters:\n\n$L($) = Et~U[1,T] [Ewt~q4 [logp(Yt | Y0:t\u22121, wt)] \u2013 DKL [q\u00a2(wt | Y0:t\u22121)||p(\u03c9t)]]$.\n\nThe equation above has two nice properties. First, it resembles the VAE objective Kingma & Welling (2014) creating a connection between Bayesian networks and latent variable models. Second, BARNN incorporates into the ELBO the timestep dependency, allowing for adjustable weights in time which we will show provide sharper, better calibrated uncertainties as well the ability to better capture long range dependencies in the sequences. Finally, in the limit of time-independent peaked distribution with prior and posterior perfectly matching, the BARNN loss simplifies to the standard log-likelihood optimization used in standard autoregressive and recurrent models (see Appendix A.5 for derivation). This suggests interpreting eq. (4) as a form of Bayesian weight regularization during autoregressive model training. Finally, once the model is trained, the predictive distribution and uncertainty estimates can be easily computed by Monte Carlo sampling (see Appendix A.4)."}, {"title": "3.3 Variational Dropout Approximation", "content": "When examining the generative model in eq. (2) or the lower bound in eq. (4), a key challenge arises: how can we efficiently sample network weights? Directly sampling network weights is impractical for large networks, making alternative approaches necessary. Variational Dropout (VD) Kingma et al. (2015); Molchanov et al. (2017) offers a solution by reinterpreting traditional Dropout Srivastava et al. (2014)-which randomly zeros out network weights during training\u2014as a form of Bayesian regularization, and use the local reparametrization trick for sampling only the activations resulting in computational efficiency. We re-interpret VD for sampling dynamic weights w during training and inference. Our goal is to do a reparametrization of the weights $wt = f(\u03a9, at, \u20ac)$, with \u03a9"}, {"title": "3.4 Bayesian Neural PDE Solvers", "content": "Autoregressive Neural PDE Solvers map PDE states yt to future states Yt+1, given a specific initial state yo Brandstetter et al. (2022b). Those solvers are deterministic and compute the solution without providing estimates of uncertainty. BARNN can be used to convert any Autoregressive Neural PDE Solver into its Bayesian version by adopting a few steps. First, we model the joint weight-state distribution as:\n\n$PYO:T, 1:T) = \\prod_{t=1}^T P(Yt | Yt-1, wt)p(wt)$,\n\nwhich is a special case of eq. (2) when markovianity of first order on the states is assumed. Then, we assume a Gaussian state distribution $p(Yt | Yt\u22121,wt) = N(NO(yt\u22121; wt), diag(\u03c3))$, where the mean at time t is obtained by applying the Autoregressive Neural PDE Solver to the state Yt-1, while the standard deviation is not learned. Specifically, NO indicates any Neural PDE Solver architecture, with wt its probabilistic weights, showing that the framework is independent of the specific architecture used. The probabilistic weights are obtained with the transformation $f (\u03a9, at, \u03b5)$ applied layerwise as explained in eq. (6), with the encoder Ey taking only one state as input (due to markovian updates). Learning is done by maximizing eq. (4), which, given the specific state distribution, becomes the minimization of the standard one-step MSE loss Brandstetter et al. (2022a) commonly used to train Autoregressive Neural PDE Solvers, plus a Bayesian weight regularization term given by the negative KL divergence in eq. (9). Appendix D reports the training algorithm."}, {"title": "3.5 Bayesian Recurrent Neural Networks", "content": "Given a sequence Yo, Y1,..., \u0443\u0442 a standard Recurrent Neural Network (RNN) Bengio et al. (2000); Schmidhuber et al. (1997) computes eq. (1) by introducing hidden variables {ht}=0 that store the input information up to the time t. Given these variables, the conditional probability is modelled as:\n\n$P(Yt | Yt-1,..., Yo) = \u03c3(ht\u22121), ht = NN(yt, ht\u22121; \u03c9)$,\n\nwhere o is the softmax function, and NN is a neural network with deterministic weights and a specific gate mechanism depending on the RNN structure. Importantly, the variable ht contains historical information up to time t, i.e. knowledge of yt-1,..., Yo.\n\nExtending RNNs to Bayesian RNNs is straightforward with BARNN. In particular, the states distribution becomes:\n\n$P(YtYt-1,..., Yo, wt) = \u03c3(ht\u22121), ht = NN(yt, ht\u22121; wt)$,\n\nwhile the posterior distribution over the weights is obtained by conditioning on the hidden states:\n\n$qw(wt | Yt\u22121, \u00b7\u00b7\u00b7, Yo) = q\u03c8 (Wt | Yt\u22121, ht\u22122)$.\n\nThe probabilistic weights wt are obtained again with the transformation $f (\u03a9, \u03b1t, \u20ac)$ applied layerwise as explained in eq. (6), with the encoder Ey taking yt-1, ht-2 as input. Finally, learning is done by maximizing eq. (4), which, given the specific state distribution, becomes the minimization of the cross entropy loss Bengio et al. (2000) commonly used in causal language modelling or next token prediction, plus the negative KL divergence in eq. (9). Appendix D reports the training algorithm."}, {"title": "4 Experiments", "content": "We demonstrate the effectiveness of BARNN by applying it to different tasks. First, we show that BARNN combined with Neural PDE Solvers Bar-Sinai et al. (2019); Brandstetter et al. (2022b); Li et al. (2020) can solve PDEs and quantify the related uncertainty; then, we apply BARNN to RNNS for molecule unconditional generation Segler et al. (2018); \u00d6z\u00e7elik et al. (2024) using the SMILES syntax and show stronger generation capabilities compared to existing methods."}, {"title": "5 Conclusions", "content": "We have introduced BARNN, a scalable, calibrated and accurate methodology to turn any autoregressive or recurrent model to its Bayesian version, bridging the gap between autoregressive/ recurrent methods and Bayesian inference. BARNN creates a joint probabilistic model by evolving network weights along with states. We propose a novel variational lower bound for efficient training, ex-"}, {"title": "A Proofs and Derivations", "content": "This Appendix Section is devoted to the formal proofs introduced in the main text. In A.1 we prove\nthe variational lower bound, while the t-VAMP proof is found in A.2. Finally, A.3 contains the\nderivation of KL-divergence between prior and posterior presented in 3.3."}, {"title": "A.1 Variational Lower Bound proof", "content": "We aim to optimize the log-likelihood for the model:\n\n$P(YO:T, 31:T) = \\prod_{t=1}^T P(Yt | Yo:t-1, wt)p(wt)$.\n\nBy defining the variational posterior:\n\n$9\u03c6(W1:T | YO:T) = \\prod_{t=1}^T \u03c6(\u03c9t | Yo:t\u22121)$,\n\nwe obtain the following evidence lower bound:\n\n$log p(yo:T) = log \\int P(YO:T, W1:T) dw1:T$\n\n$= log \\int \\frac{P(YO:T, 1:T)}{(W1:T | YO:T)} q\u03c6(W1:T | YO:T) dw1:T$\n\n$\u2265 Ew1:T~9\u03c6 (W1:T/YO:T) log \\frac{P(YO:T, 1:T)}{ \u03c6(W1:T | YO:T)}$\n\n$= Ewer log \\frac{\\prod_{t=1}^T P(Yt | Yo:t-1, wt)p(wt)}{\\prod_{t=1}^T q(\u03c9t | Yo:t-1)}$\n\n$= \\sum_{t=1}^T Ew\u03c4~q (Wt Yo:t-1) log \\frac{P(Yt | Yo:t-1, wt)p(\u03c9t)}{qp(\u03c9t | Yo:t-1)}$\n\n$= \\sum_{t=1}^T {E\u03c9\u03c6 (\u03c9\u03b5|Yo:t-1) [log p(Yt | Yo:t-1, wt)] \u2013 DKL [q$(wt | Yo:t-1) ||p(wt)]}$\n\n$= \\sum_{t=1}^T Lcumulative ($, t)$\n\nInterestingly, by being Bayesian, we ended up having a sum of evidence lower bounds, which tells us\nthat to maximise the model likelihood we need to maximise the cumulative evidence lower bound.\nFor performing scalable training we approximate the found evidence lower bound as usually done in\ncausal language modelling or next token prediction by:\n\n$L($) \u2248 Et~U[1,T] [Lcumulative($, t)] $"}, {"title": "A.2 Variational Mixture of Posterior in Time proof", "content": "We want to find the best prior that maximises the evidence lower bound:\n\n$L(\u03a6) = Ew1:1~94 (W1:T/Y0:T) log \\frac{P(YO:T, 1:T)}{\u03b1\u03c6\u03c9\u03b9:\u03c4 | Yo:T)}$"}, {"title": "B BARNN PDEs Application", "content": "This section shows how BARNN can be used to build Bayesian Neural PDE Solvers, provides all the\ndetails for reproducing the experiments, and shows additional results."}, {"title": "B.1 Data Generation and Metrics", "content": "Data Generation: We focus on PDEs modelling evolution equations, although our method can\nbe applied to a vast range of time-dependent differential equations. Specifically we consider three\nfamous types of PDEs, commonly used as benchmark for Neural PDE Solvers Brandstetter et al.\n(2022a,b); Bar-Sinai et al. (2019):\n\n$Burgers\\quad$  $\\partial_t u + u \\partial_x u \u2013 \u03bd\\partial_{xx}u = 0$\n\n$Kuramoto Sivashinsky\\quad$  $\\partial_t u + u \\partial_x u + \\partial_{xx}u + \\partial_{xxxx}u = 0$\n\n$Korteweg de Vries\\quad$  $\\partial_t u + u \\partial_x u + \\partial_{xxx}u = 0$\n\nwhere \u03bd > 0 is the viscosity coefficient. We follow the data generation setup of Brandstetter\net al. (2022a), applying periodic boundary conditions, and sampling the initial conditions u\u00ba from a\ndistribution over truncated Fourier series $u\u00ba(x) = \\sum_{k=1}^{10} Ak sin(2\u03c0\u03b9\u03bax + \u03b7\u03ba)$, where {Ak,\u03b7k,lk}k\nare random coefficients as in Brandstetter et al. (2022a) The space-time discretization parameters are\nreported in Table 4."}, {"title": "Metrics:", "content": "To evaluate the performance of the probabilistic solvers, we focus on different metrics:\n\n1. Root Mean Square Error (RMSE) Brandstetter et al. (2022b): measures the match of the\nensemble prediction means and true values\n\n2. Negative Log-Likelihood (NLL): represents the trade-off between low standard deviations\nand error terms, where the latter are between ensemble prediction means and true values\n\n3. Expected Calibration Error (ECE): measures how well the estimated probabilities match\nthe observed probabilities\nLet \u00b5t, ot the ensemble mean and variance predicted by the probabilistic Neural Solver for different\ntimes t, Q the Gaussian quantile function of p, then:\n\n$RMSE = \\frac{1}{Nxnt} \\sum_{x,t}[Yt(x) \u2013 \u03bct(x)]2$\n\n$NLL= \\frac{1}{2nxnt} \\sum_{x,t} [log(2\u03c0\u03c3\u03b5 (x)) + \\frac{[Ut(x) \u2013 \u03bct(x)]2}{\u03c3\u03b5 (x)}]$\n\n$ECE = Ep~U(0,1) [|P - Pobs|]$"}, {"title": "C_BARNN Molecules Application", "content": "This section shows how BARNN can be used to build Bayesian Recurrent Neural Networks, provides\nall the details for reproducing the experiments, and shows additional results."}, {"title": "D Algorithms and Software", "content": "This section reports the pseudocode algorithms for training and performing inference using a BARNN\nmodel and the software used."}]}