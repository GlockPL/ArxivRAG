{"title": "CAMAL: Optimizing LSM-trees via Active Learning", "authors": ["WEIPING YU", "SIQIANG LUO", "ZIHAO YU", "GAO CONG"], "abstract": "We use machine learning to optimize LSM-tree structure, aiming to reduce the cost of processing various\nread/write operations. We introduce a new approach CAMAL, which boasts the following features: (1) ML-\nAided: CAMAL is the first attempt to apply active learning to tune LSM-tree based key-value stores. The\nlearning process is coupled with traditional cost models to improve the training process; (2) Decoupled Active\nLearning: backed by rigorous analysis, CAMAL adopts active learning paradigm based on a decoupled tuning\nof each parameter, which further accelerates the learning process; (3) Easy Extrapolation: CAMAL adopts\nan effective mechanism to incrementally update the model with the growth of the data size; (4) Dynamic\nMode: CAMAL is able to tune LSM-tree online under dynamically changing workloads; (5) Significant System\nImprovement: By integrating CAMAL into a full system RocksDB, the system performance improves by 28%\non average and up to 8x compared to a state-of-the-art RocksDB design.", "sections": [{"title": "1 Introduction", "content": "LSM-Tree based Key-Values Stores. Key-value stores, increasingly prevalent in industry, underpin\napplications in social media [8, 12], stream and log processing [14, 18], and file systems [43, 73].\nNotably, platforms like RocksDB [30] at Facebook, LevelDB [32] and BigTable [16] at Google,\nHBase [1] and Cassandra [7] at Apache, X-Engine [39] at Alibaba, WiredTiger [2] at MongoDB,\nand Dynamo [27] at Amazon extensively utilize Log-Structured Merge (LSM) trees [62] for high-\nperformance data ingestion and fast reads.\nAn LSM-tree is a multi-level data structure that operates on key-value pairs. The top level of\nthe LSM-tree has a smaller capacity and stores the freshest data, while the lower levels can hold\nexponentially more data but with progressively older timestamps. Initially, data is inserted into\nLevel-0 (a.k.a. buffer level), until it is full and sort-merged into the next deeper level. The capacity\nof Level i + 1 is T times than that of Level i, where T is referred to as the size ratio. Similar merge\nbehavior happens in any two consecutive levels, leading to multiple sorted runs in each level, where\neach run has an associated Bloom filter to facilitate lookups.\nInstance-Optimized LSM-Trees. LSM-trees are commonly used in supporting diverse workloads\nwith varying percentages of operation types, such as point lookups, range lookups and data writes.\nThe point lookup (resp. range lookup) is a query that extracts the value (resp. values) corresponding\nto a given key (resp. key range), whereas the data writes are operations to insert, delete or update\nthe value for a key. The various possibility of the workload raises a crucial question of how to\nselect suitable parameters (e.g., size ratio, compaction policy, memory allocation between buffer\nlevel and Bloom filters) to construct an LSM-tree optimized for a given workload, leading to the\nnotion of instance-optimized LSM-trees.\nSeveral studies [23, 26, 40, 41] have explored instance-optimized LSM-tree designs, with Dos-\ntoevsky [23] and K-LSM [41] focusing on compaction policy tuning, LSM-Bush [26] discussing\nthe choice of size ratio between adjacent levels, and Endure [40] investigating LSM-tree tuning\nunder workload uncertainty. All these methods can be classified as complexity-based models, which\nprimarily rely on complexity analysis to predict the I/O cost of each operation. Orthogonal to\nthese approaches, in this paper we ask whether machine learning (ML) can give an even more\nfiner-grained tuning of LSM-tree based key-value stores. The potential of machine learning based\nmethods is that the mapping between system knobs and performance is directly captured in an\nend-to-end manner, instead of being implicitly obtained via a complexity-based cost model. Such\nmachine-learning aided approach has been proven effective in other data structures such as search\ntrees [49] and spatial data structures [33], yet it has been rarely explored for disk-resident data\nstructures such as LSM-trees. It is important to note that the goal of using ML for LSM-tree tuning\nis not to replace the techniques mentioned above; instead, we aim to explore the power of ML in\ntuning LSM-trees, seeing how to harness the strengths of these two kinds of methods, and this\nhybrid technique would be one key element in our design.\nOur Idea: Active learning for instance-optimized LSM-trees. To achieve ML-based tuning, a\nplain ML approach is expected to train a model, which predicts the LSM-tree performance for a\ngiven workload, and is later used for searching a desired configuration of the LSM-tree. The model\ntraining process starts with collecting samples, followed by feeding the samples to the model for\nfitting the parameters. A sample is in the form of (W, X, Y), where W is a workload, X is a point in"}, {"title": "CAMAL: Optimizing LSM-trees via Active Learning", "content": "the configuration space formed by all possible ranges for each tunable parameter, and Y is the true\nrunning time for workload W using the LSM-tree constructed by parameters X.\nUnfortunately, the cost of sample collection can be prohibitive. Evaluating the system perfor-\nmance Y in a sample requires the ingestion of the entire dataset into the LSM-tree system, followed\nby the execution of a sufficiently large workload W comprising numerous queries. Consequently,\nobtaining a single sample with 10GB data may consume several minutes [40], with the time frame\nescalating at an exponentially rapid pace as the dataset grows. Moreover, given the extensive\nconfiguration space, a considerable number of samples are required to ensure the model's accuracy,\nleading to an impractical cost.\nTo address the challenge, we turn to the active learning (AL) paradigm [38, 55, 59], which has\nbeen proven capable of refining the sample quality in an interactive manner, and thus fewer samples\nare required to secure the model accuracy. For a training workload, AL involves iterative sampling\nrounds until the given sampling budget is exhausted. Initially the sample set only contains one set\nof random samples. Each round commences with training an ML model using the existing samples\nin the set. The model then identifies and selects a new sample that is predicted to have the lowest\ncost by the newly trained model. This new sample is subsequently added to the sample set, and if\nthe budget permits, the next sampling round is triggered. As the training process advances through\neach round, the ML model's accuracy progressively improves. The quality of the selected samples\nalso benefits from this refinement over time.\nChallenges and New Designs. Although active learning effectively refines the exploration in\nconfiguration space, we foresee three challenges when applying it to optimize LSM-trees. Firstly,\nby default, the initial sample is chosen randomly, which may deviate significantly from the true\noptimum, requiring additional rounds of exploration. Secondly, the tunable parameters are not\neffectively decoupled throughout the sampling process, failing to narrow down the configuration\nspace during the process. Lastly, in the presence of dynamic workloads, re-training becomes\nnecessary and introduces extra costs to maintain model accuracy.\nThe benefits of our approach CAMAL stem from the following novel designs, which address the\nchallenges respectively.\nDesign 1: Complexity-analysis driven techniques to avoid random initialization. Our\ninsight is that the optimal LSM-tree parameter settings obtained through a complexity-based cost\nmodel (e.g., the one in [22]) still provides much better results than a random sample, although\nthey may not be the true optimum within the configuration space. As a result, this cost model can\nefficiently and effectively guide active learning to pinpoint the neighborhood containing the true\noptimum. Specifically, we can initiate the active learning process with the theoretically optimal\nsolution obtained from the complexity analysis in each training workload, to significantly prune\nthe sampling space for each parameter.\nDesign 2: Decoupling parameters for faster approaching a desired solution. To reduce\nthe vast sampling space due to the combination of different parameters, we propose a novel\nhierarchical sampling technique that decouples each parameter from the complex I/O model. We\nhave discovered that although the parameters may be correlated, their optimal settings can be\nrelatively independent, allowing tuning the parameter one round at a time. In particular, we establish\na theoretical foundation, allowing us to first assess the desired values of one parameter, and then\naddress more intricate dependencies of other parameters to guide the sampling rounds in active\nlearning.\nDesign 3: Extrapolation strategy for data growth. While decoupled active learning effectively\nnarrows the sampling space, the training cost tends to rise with larger data sizes. To accommodate\ndata growth, we have theoretically proven that it is possible to rapidly transition to new tuned"}, {"title": "2 Background", "content": "LSM-tree Structure. An LSM-tree is structured with multiple levels, where each level contains one\nor multiple sorted runs. New data is initially stored in an unsorted state in a memory buffer, whose\nfilled-up will trigger a write to the first level of the LSM-tree. As each level becomes full, its data\nis sorted and merged into the next level recursively. During point lookup, the LSM-tree searches\nthe most recent level first, followed by lower levels until it finds the matching data. Additionally,\nBloom filters optimize point lookup by efficiently determining if a key exists in a sorted run without\nperforming I/Os. The capacity of each level in an LSM-tree grows by a size ratio of T. Therefore,\nthe number of levels L is determined by the size ratio T, the memory allocated to the write buffer\nM\u266d, and the size of the data. Level i has  (T \u2013 1)Ti-1 entries [40], where E represents the size\nof an entry. If there are N entries in the LSM-tree, the number of levels can be calculated as:\nL = log(Mb/NE + 1)/log(T) (1)\nIn line with Dostoevsky [23], we restrict the size ratio range to 2 \u2264 T \u2264 Tlim, where Tlim is defined\nas N \u00b7 E/Mlim.\nWorkload. Following previous works [22, 23, 40], key-value databases commonly involve four\ntypes of operations: zero-result point lookups, non-zero-result point lookups, range lookups, and\nwrites. In the case of point lookups, a single key is searched in the database, and the Bloom filter is\noften used to determine whether the key is located in the block before proceeding with the actual\nblock reading. Point reads can be further divided based on whether the key exists in the database,\ncategorized as zero-result point lookups and non-zero-result point lookups. Range lookups aim\nto find the most recent versions of all entries within a specified key range. This process entails\nmerging the relevant key range across all runs at every level while sorting the entries. Writes,\nincluding inserts, deletes, and modifications, typically involve appending a new key in the write\nbuffer instead of locating the older version for in-place updates. In practice, workloads for key-value\ndatabases are often comprised of varying proportions of the four operations.\nComplexity-analysis based Model. The complexity-analysis based Model, or theoretical I/O\nmodel, analyzes the number of I/Os generated per query, based on a given workload and basic\nparameters of an LSM-tree. We adopt the state-of-the-art Bloom filter bits-allocation strategy\nproposed in Monkey [22], which designs different false positive rates (FPRs) for Bloom filters\nin different levels to achieve optimal memory allocation. We also follow the models derived in\nMonkey [22] for the four workload types, as shown in Figure 2. These models have been widely\nrecognized and adopted in later works, such as Dostoevsky [23] and Endure [40]. It is important to\nnote that although we use a relatively simple complexity-based model, it serves the purpose in our\nhybrid framework that combines the complexity-based model and ML model because the goal of\nusing the complexity-based model is to estimate a reasonable range. Our framework can integrate\nwith a more sophisticated model when necessary. According to the cost model of each operation\nin Figure 2, the overall average cost can be calculated based on the known proportions (v, r, q, w) of\nthe four operations:\nf = v \u00b7 V + r \u00b7 R+q\u00b7Q+w\u00b7W (2)\nObjectives. This paper aims to minimize the end-to-end latency incurred by the input workload,\ngiven the knowledge of the system and workload. We consider both the static mode (Sections 3-5)\nwhere the workload is stable, and the dynamic mode (Section 6) where the workload changes online.\nFigure 2 provides examples of how the performance varies across different workloads based on\nspecific parameters. The system includes information such as the number of entries N, the total\nmemory budget M, the size of an entry E, and the number of entries B that fit in a storage block."}, {"title": "3 CAMAL Overview", "content": "This section introduces CAMAL, outlined in Algorithm 1, where an ML model estimates LSM-tree\ncosts for given workloads and identifies desired parameter settings for enhanced performance. As\nshown in Figure 3, in its training phase, CAMAL first decouples the sampling space and identifies\nthe theoretical optimum using a complexity-based model. Following this, it integrates an ML model\nto facilitate an active learning cycle, where the model is continuously iterated to select subsequent\nsamples. To address data growth, we introduce an extrapolation strategy that extends the desired\nparameters to testing scenarios with larger data sizes without the need for retraining. Both of\nthese methods are designed to reduce training costs. Additionally, to meet practical demands, we\napply the extrapolation strategy to enhance CAMAL for dynamic environments, which also includes\nequipping LSM-trees with the ability to dynamically adapt to changing parameters.\nCAMAL considers training with various workloads as shown in Table 1, in line with settings in\nEndure [40]. For ease of discussion, let us focus on one workload W with a sampling budget h, as\nextending to multiple workloads is straightforward \u2013 simply training one workload after another.\nAs shown in Figure 3, the main workflow of CAMAL follows an active-learning approach, which\nconsists of multiple rounds of sample generation and model training using existing samples. At\na high level, CAMAL employs a novel technique called decoupled active learning, which enables\nthe tuning of individual parameters in separate rounds. The parameter is fixed to its tuned value\nfound based on an intermediate model trained during active learning. This technique addresses a\nlimitation commonly found in typical active learning, where all parameters are sampled together in\na correlated manner, resulting in an extensive sampling space. Assuming the sampling space size\nfor each parameter is n\u012f, the total sampling space required to explore all parameter correlations\nis \u220f ni. In contrast, decoupled sampling allows us to tune each parameter separately in a round,\nreducing the sample space to \u2211 n\u012f. Additionally, by using decoupled sampling, the model gains\nearly exposure to desired settings, leading to higher-quality samples in subsequent rounds. One\nmay wonder why the desired setting for a parameter can be determined by an intermediate model"}, {"title": "CAMAL: Optimizing LSM-trees via Active Learning", "content": "in AL. Addressing this query leads us to another crucial technique in Camal, which involves\nemploying a complexity-based cost model to assess the independence between desired settings for\neach parameter, and is particularly geared to the LSM-tree applications.\nIn a nutshell, each round in CAMAL comprises the following steps. Initially, a parameter or a set\nof parameters to be adjusted in the current round is chosen. Next, an analysis is performed on the\nselected parameter configuration using a complexity-based cost model, and we narrow down the\nsampling range to the neighborhood of the selected value. Subsequently, the parameter is sampled\nwithin this narrowed range while keeping the previously selected parameters intact, and other\nparameters are set to their default values. Then, the selected parameters, denoted by X, are used\nto construct the LSM-tree instance and record the performance (denoted by Y) of running the\nworkload W to form the sample (W, X, Y). Afterwards, a machine learning model (will be discussed\nin Section 7) is trained using all existing samples. Finally, the trained model is used to tune the\ncurrent parameter by selecting the one that leads to the lowest inference value for the model. The\nselected parameter value is then fixed for subsequent rounds to mitigate potential errors arising\nfrom complexity-based analysis.\nWhile decoupled active learning effectively reduces the sampling space for moderate data sizes,\nthe training cost can exponentially increase with larger data sizes. To address this challenge, we\ndesign an extrapolation strategy that scales the selected settings from a smaller to a larger dataset\nwithout retraining. Specifically, if T', M', and M are the selected parameters for a database with\nN' entries and an M' memory budget, we prove that when the data size grows to kN' and the\nmemory budget to kM', the desired parameters become T', kM', and kM. Though the factor k\nmay have practical limitations, this strategy still substantially reduces training costs, cutting down\ntraining time by approximately an order of magnitude.\nIn practical applications, another challenge is managing dynamic workloads where the desired\nsettings for an initial workload may not remain effective for subsequent ones. To illustrate this\nscenario, we have a current workload represented by (vi, ri, qi, wi) and an expected next workload\n(Vi+1, ri+1, qi+1, Wi+1). In the interval between these workloads, there are enough updates to\ntrigger sufficient compactions, allowing for a change in parameters. Our method involves initially\nsetting the desired parameters T', M', and M\u2081 for workload i. We then use the extrapolation strategy\nto estimate the desired parameters T\", M'', and M'' for workload i + 1. In the interval between\nthese workloads, we incrementally adjust the size ratio during each compaction, and change the\nbit per key for Bloom filters when new runs are formed. This approach enables the settings to\ngradually shift, aligning with the evolving workloads and ensuring continued optimization.\""}, {"title": "4 Decoupled Active Learning", "content": "Decoupled active learning aims to decouple each individual parameter from the complex I/O model\nto ease the sampling of a single parameter at each round. In particular, given a configuration space\nA that consists of all tunable parameters, we select a parameter \u03bb\u012f \u220b A to analyze first, if there\nexists an optimal solution \u03bb for \u03bb\u012f, such that\n\\frac{\\partial f}{\\partial \\lambda_i}|_{\\lambda_i = \\lambda_i^*} = 0 (3)\n\\lambda_i \\perp \\Lambda \\backslash {\\lambda_i^*}\nwhere f is the cost function, and \u22a5 denotes an orthogonal relationship. Essentially, the first\nequation implies that  is an optimal solution to \u03bb\u012f for the cost function f(\u00b7)\u00b9, whereas the second\nexpression indicates that the optimal solution d does not depend on other tunable parameters in"}, {"title": "CAMAL: Optimizing LSM-trees via Active Learning", "content": "the configuration space. As such, we can safely determine the optimal setting for parameter A first\n(in CAMAL, we use machine learning to help calibrate the setting, see Section 7) and then shrink\nthe configuration space to A* = \u039b\\\u03bb* by eliminating the dimension of \u03bb\u2081 (which has been fixed as\n\u03bb). The process then repeats in the newly reduced dimension \u039b*.\nThe process for setting each parameter involves several key steps. Initially, we formulate a\ncost function related to the parameter. Next, we proceed to solve the derivative of this function,\nobtaining a theoretical optimum. This theoretical optimum establishes an effective sampling range\ncentered around the optimum to encompass the practical desired solution close to this range. The\nunderlying theoretical basis for this approach is based on the following lemma:\nLEMMA 4.1. Given the prevalent cost model [22, 23] for leveling, the process of configuration opti-\nmization can be decoupled into two distinct stages: firstly, determining the optimal value of T*, and\nsecondly, allocating memory between M\u1ef9 and Mf. This decoupling ensures the attainment of a globally\noptimal configuration combination.\nPROOF. According the terms in Figure 2, the I/O cost for a leveling LSM-tree can be represented\nas:\nf_l(T) = ve^{-M_f/N} + r(e^{-M_f/N}+1) + q(L+S/B) + w \\frac{L.T}{B}\nIts derivative can be formulated as:\n\\frac{\\partial f_l}{\\partial T} = \\frac{L}{BT \\log(T)} (wT(\\log T \u2013 1) \u2013 qB) (4)\nThe theoretical optimal T* (the optimal value of T) can be obtained by setting Equation 4 to zero\naccording to the Derivative Test, giving us\nwT^* (\\log T^* \u2212 1) \u2212 qB = 0 (5)"}, {"title": "CAMAL: Optimizing LSM-trees via Active Learning", "content": "Clearly, from Equation 5 we conclude that T* is independent of Mf and M\u266d. So, we can determine\nT* first.\nWhen optimizing for Mf and M\u266d, we have:\n\\frac{\\partial f_l(M_f)}{\\partial M_f} = \\frac{1}{N}(v+r)e^{-M_f/N} + \\frac{M_f qB+wT}{B \\log T} \\frac{1}{(M-M_f)} (6)\nEquation 6 implies that when Mf increases, the absolute value of the first term will decrease, thereby\ndiminishing its effect on optimizing point lookups (whose cost is (v + r)e-Mf/N + r). Concurrently, a\nlarger Mf will result in a reduced M\u266d, indicating that the second term will increase, further causing\nthe cost to escalate at an accelerating rate. To strike a balance between the two parameters, we set\nEquation 6 equal to zero, which will allow us to derive the theoretically optimal values for Mf and\nMb. This completes the proof of the lemma.\nUpon segregating the parameters across various rounds, during the sampling process, we gather\nsamples from a contiguous vicinity centered around the theoretical optimal values, based on the\nspecified sample count. For T, the sampling interval is an integer. For Mf, the interval is measured\nin bits per key (BPK), determined by the formula.\nIn summary, decoupled active learning is outlined as Algorithm 2. It stands out compared to\nplain active learning primarily for two reasons: firstly, it identifies theoretical optimal parameters,\nwhich are often close to the practical desired parameters. Secondly, it reduces the dimensions of\nthe sampling space, significantly decreasing the number of active learning cycles required to find\nthe practical desired solutions.\nExtension to Tiered LSM-tree. The decoupled sampling approach remains largely pertinent\nfor the tiering compaction policy. First, we can differentiate with respect to T using the tiering\ncost model. This differentiation closely resembles Equation 4, augmented with a specific term\nrepresenting the point read cost:\n\\frac{\\partial f_t}{\\partial T} = (v+r)e^{-M_f/S} + q S/B + \\frac{L(qBT(\\log T \u2013 1) \u2013 w)}{BT \\log T} (7)\nBased on prior research [40], this term is quantitatively less significant compared to I/Os of range\nlookups and post-optimization writes. Therefore, in practice, the desired T* exhibits only mild\nfluctuations with changes in Mf. Furthermore, employing machine learning for subsequent sampling\neffectively addresses these variations, as the sampling phase refines and corrects inaccuracies."}, {"title": "5 Extrapolation without Retraining", "content": "Employing ML models for estimating the cost of an LSM-tree introduces a related issue concerning\ntheir extrapolation capabilities. This means that when testing configurations deviate from those\nused during training, it becomes important to determine if the stale model can still effectively\noptimize the LSM-tree. For instance, if the stale model is trained on a database with N = 10\u00ba, it\nis unclear whether it can be applied to a database with N = 107. While retraining the model is\na possibility, we aim to identify an incrementally updated solution based on the existing model,\nwhich does not require a costly retraining.\nLEMMA 5.1. Given the optimal T' and M' under the memory budget M' and the number of entries\nN', we have the new optimal T'' and M'' under the memory budget M'' = kM' and the number of\nentries N'' = kN' as:\nT'' = T', M'' = kM'"}, {"title": "CAMAL: Optimizing LSM-trees via Active Learning", "content": "PROOF. According to [42], in leveling policy, the write amplification of each level is T, signaling\nthat on average the key-value entry of the update will take part in T compactions in each level. So\nthe average complexity of CPU to compact an entry is expected to be T \u00b7 L. Further separating the\nI/O cost model in Figure 2 by writes and reads, on average the write query incurs a total overhead of\n(Iw+I)+CwT, where Iw and I, are the write and read I/O costs while Cw is the CPU cost, such\nas merge sorting and space allocation. In the same way, let C, be the cost of probing the metadata\nof a sorted run in the main memory, then the total cost of an zero-result point lookup is expected\nto be e I, + C, \u00b7 L, and the cost for a non-zero-result point lookup is (e\u00af + 1)\u00b7 I, + C, \u00b7 L. For\nrange lookups, the main CPU overhead is also to retrieve the information in the metadata, which\nleads to a total overhead (L + \u33a2) \u2022 I\u2084 + Cq \u2022 L.\nIn summary, when the number of entries is N' and the memory budget is M', the total overhead\nper operation of leveling policy is\ng\u0131 = Irve + I,r(e + 1) + 2C,L (9)\n+ I,q(L + )+CqL+ (Iw+I)+CwTL\nIf the overhead is minimized when T = T' and Mf = M', then we have the equations:\n = 0 (10)\n = 0\nWhen N'' = kN', to ensure that the equation still holds, we can make T'' = T' and M' = kM', then\nthe new version of Equations 10 is transformed to be\n= 0\n= 0,\nwhich means T'' = T' and M = kM are the new optimum of the new configuration. This\ncompletes the proof."}, {"title": "6 Dynamic System Mode", "content": "Workloads can be dynamic. This section discusses how to adapt CAMAL to workload shifts. Assume\nthat we have a current workload represented by (vi, ri, qi, wi) and an expected next workload\n(Vi+1, ri+1, qi+1, Wi+1). Our method initially sets the desired parameters T', M', and M for workload\ni, and then uses the extrapolation strategy to estimate the desired parameters T'', M'', and M'' for\nworkload i + 1. In the interval between these workloads, we incrementally adjust the size ratio\nat each compaction, and change the bit per key for Bloom filters when new runs. This"}, {"title": "7 Embedded Machine Learning Models", "content": "This section examines the three most prevalent ML models, evaluating their pros and cons for\nintegration into CAMAL. It is nevertheless to note that other ML models can potentially be embedded\ninto CAMAL, and we limit the discussion scope to simple and representative models for ease of\ndiscussions.\nPolynomial Regression (Poly) [35] uses basis functions to capture nonlinear relationships\nbetween variables, replacing linear terms in linear regression. We treat each term from the cost\nmodels in Figure 2 as basis functions, and include a constant term for each operation type to account\nfor CPU time consumption. The regression model for the cost function can be formulated as:\nYcost = \u03a3\u03b2ixi (11)\nHere, \u1e9ei represents the coefficients to be learned, and x\u2081 denotes the basis functions derived from\ntheoretical models.\nTree Ensembles (Trees) [19, 57] are a type of ML model that combines multiple decision trees\nto enhance accuracy and robustness. The model can mitigate the impact of individual tree biases\nand errors, resulting in more accurate and stable predictions. In this paper, we primarily focus on\ngradient-boosted trees due to its widely adoption [29, 81].Tree ensembles offer the advantage of\nbypassing manual basis function design and directly incorporating cost function elements. They\ndetect feature relationships automatically and have strong fitting capability. However, they are\nprone to overfitting and outlier sensitivity due to uncertainty, and their extrapolation ability is\nweaker than polynomial regression.\nDuring training and inference, we input the influential factors as independent features into the\ntree ensembles, including N, T, M\u266d, Mc, v, r, w, and q. The average latency is used as the label, and\nthe ensemble trees automatically capture the relationships between these features.\nNeural Networks. We also explore more complex and advanced models, such as neural networks\n(NN), which consist of interconnected nodes organized into layers for data processing and analysis.\nHowever, these complex models typically demand a larger amount of training data [37, 44, 85],\nresulting in increased sampling time. To verify the expectation, while considering previous expe-\nriences [57, 74] and conducting experimental model selection, we evaluate a standard NN model\nwith four fully connected layers. This model requires three times the number of samples compared\nto the other two models in order to achieve similar optimization outcomes (see more details in\nFigure 5a in Section 8)."}, {"title": "8 Evaluation", "content": "We integrated CAMAL into RocksDB and conduct systematic experimental evaluations to demon-\nstrate its effectiveness.\n8.1 Experimental Setup\nHardware. Our experiments are done on a server with Intel(R) Core(TM) i9-13900K processors,\n128GB DDR5 main memory, 2TB NVMe SSD, a default page size of 4 KB, and running 64-bit Ubuntu\n20.04.4 LTS on an ext4 partition."}, {"title": "CAMAL: Optimizing LSM-trees via Active Learning", "content": "among different operations, enabling CAMAL to gradually adapt the parameters and include a\nbroad spectrum of application scenarios. In the static mode, we assess both standard and shifting\nworkloads, whereas in the dynamic mode, we focus solely on the shifting workloads. By default,\nour evaluations are conducted in static mode, unless otherwise specified. In the dynamic mode, to\nensure adequate compactions for dynamic LSM's gradual parameter adjustment, we continuously\ninsert additional entries during the intervals of workload queries.\nWe assess the latency and I/O times of a series of 500,000 queries, reporting the average perfor-\nmance across all workloads, unless specified differently. All range queries are set up with minimal\nselectivity, functioning as short-range queries that generally access between zero to two blocks per\nlevel on average.\nRegarding data distribution, we employ both uniform and Zipfian query distributions, utilizing\nYCSB [20] for implementation. We also modify the skew coefficient of the Zipfian distribution,\nadjusting it within a range from 0 to 0.99. This distribution is applied to both the data being ingested\nand the queries.\nCAMAL Setup. In our evaluation, the distinction between using (w/ Ext.) or not using (w/o Ext.) the\nextrapolation strategy is made only in the first experiment. In all other experiments, we consistently\napply the extrapolation strategy. We set the factor k in the extrapolation strategy to 10, meaning\nwe train with only 1/10th of the testing data size and memory budget. This choice is justified as this\nscaling factor strikes a balance between minimizing sampling costs and maximizing optimization\nperformance, a concept further evidenced in subsequent experiments. Thus, unless otherwise\nspecified, CAMAL is trained with the scaled-down setting and extrapolation strategy and tested\nunder full settings.\nAs for the ML models, we implement polynomial regression using the least square method [34]\n(CAMAL (Poly)), tree ensembles with XGBoost [19] (CAMAL (Trees)) and neural network with\nPyTorch [63] (CAMAL (NN)). Once training samples are ready, all models can be trained in 5 seconds\nand traversed in 200 milliseconds to search the desired parameters, with a space overhead of under\n200KB. This is negligible compared to the time saved through optimization and the total memory\nbudget.\nImplementation optimizations. We offer three applications with CAMAL to better integrate\nour system into a real key-value store for practical purposes. First, we incorporate block cache\nmemory allocation as an input to the ML models to optimize cache strategy. Second, we consider\ndata distribution beyond a default uniform setting - mainly Zipfian distribution [17, 20, 21, 51].\nHere we discuss three choices for incorporating different levels of distribution knowledge: (a) When\nthe data distribution is unknown during runtime, we simply train the models using uniform data\nand test them with arbitrary data. (b) If the coefficient that represents the data distribution during\nruntime can be determined, we train the model using the same distribution. (c) When multiple\npotential coefficients exist for a data distribution during runtime, we integrate the coefficient\nas an input feature within the ML model. Third, we address workload uncertainty, which is the"}, {"title": "CAMAL: Optimizing LSM-trees via Active Learning", "content": "typically require over 20 hours to reach a similar level of performance. The faster identification of\nthe desired solution by CAMAL is attributed to its unique combination of complexity-analysis driven\nsampling and ML-aided sampling. Plain ML methods often fail to locate a suitable solution within\na limited sampling budget, as their grid search approach divides the sampling space uniformly.\nBayesian optimization, while more efficient than grid search, still falls short in some aspects. It\nbetter approximates desired settings by using prior information within the current workload, but it\nrequires additional iterations for exploration. This is because it tends to explore each workload in an\ninstance-optimized LSM-tree independently, without utilizing information from other workloads.\nAdditionally, its random initialization for each training workload leads to more exploration itera-\ntions to find the desired solution. Plain active learning, despite its ability to leverage information\nacross different workloads, is only marginally more effective than Bayesian optimization due to its\nrandom initialization approach. This limitation prevents it from outperforming the more integrated\nand efficient approach offered by CAMAL with decoupled active learning.\nThe extrapolation strategy in CAMAL can greatly reduce sampling costs when used with\nappropriate settings. Figure 5a demonstrates that, with extrapolation, CAMAL can save around\n80% of sampling time (1.5 hour vs. 7 hours"}]}