{"title": "Dark Distillation: Backdooring Distilled Datasets without Accessing Raw Data", "authors": ["Ziyuan Yang", "Ming Yan", "Yi Zhang", "Joey Tianyi Zhou"], "abstract": "Dataset distillation (DD) is a powerful technique that enhances training efficiency and reduces transmission bandwidth by condensing large datasets into smaller, synthetic ones. It enables models to achieve performance comparable to those trained on the raw full dataset and has become a widely adopted method for data sharing. However, security concerns in DD remain underexplored. Existing studies typically assume that malicious behavior originates from dataset owners during the initial distillation process, where backdoors are injected into raw datasets. In contrast, this work is the first to address a more realistic and concerning threat: attackers may intercept the dataset distribution process, inject backdoors into the distilled datasets, and redistribute them to users. While distilled datasets were previously considered resistant to backdoor attacks, we demonstrate that they remain vulnerable to such attacks. Furthermore, we show that attackers do not even require access to any raw data to inject the backdoors successfully. Specifically, our approach reconstructs conceptual archetypes for each class from the model trained on the distilled dataset. Backdoors are then injected into these archetypes to update the distilled dataset. Moreover, we ensure the updated dataset not only retains the backdoor but also preserves the original optimization trajectory, thus maintaining the knowledge of the raw dataset. To achieve this, a hybrid loss is designed to integrate backdoor information along the benign optimization trajectory, ensuring that previously learned information is not forgotten. Extensive experiments demonstrate that distilled datasets are highly vulnerable to backdoor attacks, with risks pervasive across various raw datasets, distillation methods, and downstream training strategies. Moreover, our attack method is highly efficient and lightweight, capable of synthesizing a malicious distilled dataset in under one minute in certain cases.", "sections": [{"title": "1 Introduction", "content": "Deep learning (DL) has achieved remarkable success in recent years, driven by advancements in computational resources and large-scale datasets [20]. With the rise of large language models, such as GPT-3, which has 175 billion parameters and was trained on 45 terabytes of text data using thousands of GPUs for a month [1], the demand for computational power and data has reached unprecedented levels. However, the exponential growth of data has created a significant imbalance with computational capacity, posing challenges to training efficiency and costs [18].\nDataset distillation (DD) has recently emerged as a promising solution to the challenges posed by large-scale datasets and their computational demands [9]. By synthesizing smaller datasets that retain the essential information of the raw data, DD enables efficient training while significantly reducing storage and computational costs, with minimal impact on model performance [28]. With advantages such as lower storage, training, and energy costs, DD is expected to become a widely adopted method for data sharing, playing a pivotal role in many machine learning applications [37].\nMost existing DD methods focus solely on preserving the information of the raw dataset, often overlooking security issues. While these issues have recently garnered some attention from researchers, the number of related studies remains limited. For example, Liu et al. [23] proposed DoorPing, a learnable trigger that is iteratively updated during the distillation procedure. Similarly, Chung et al. [5] introduced a standard optimization framework to learn triggers for DD.\nHowever, the threat models of these methods assume that the dataset owner intentionally injects backdoors during the distillation"}, {"title": "2 Related Works", "content": "Dataset Distillation. DD aims to condense the richness of large-scale datasets into compact small datasets that effectively preserve training performance [36]. Coreset selection [8] is an early-stage research in data-efficient learning. Most methods rely on heuristics to select representatives. Unlike this paradigm, DD [31] aims to learn how to synthesize a tiny dataset that trains models to perform comparably to those trained on the complete dataset. Wang et al. [31] first proposed a bi-level meta-learning approach, which optimizes a synthetic dataset so that neural networks trained on it achieve the lowest loss on the raw dataset.\nFollowing this research, many researchers have focused on reducing the computational cost of the inner loop by introducing closed-form solutions, such as kernel ridge regression [4, 24, 34]. Zhao et al. [40] proposed an approach that makes parameters trained on condensed data approximate the target parameters, formulating a gradient matching objective that simplifies the DD process from a parameter perspective. In [38], the authors enhanced the process by incorporating Differentiable Siamese Augmentation (DSA), which enables effective data augmentation on synthetic data and results in the distillation of more informative images. Additionally, Du et al. [8] proposed a sequential DD method to extract the high-level features learned by the DNN in later epochs. By combining meta-learning and parameter matching, Cazenavette et al. [2] proposed Matching Training Trajectories (MTT) and achieved satisfactory performance. Besides, a recent work, TESLA [6], reduced GPU memory consumption and can be viewed as a memory-efficient version of MTT.\nBackdoor Attack. Backdoor attacks introduce malicious behavior into the model without degrading its performance on the original task by poisoning the dataset. Gu et al. [11] introduced the backdoor threat in DL with BadNets, which injects visible triggers into randomly selected training samples and mislabels them as a specified target class. To enhance attack stealthiness, Chen et"}, {"title": "3 Threat Model", "content": "In previous works [5, 23], the threat model assumes all users are benign, the data owner is malicious, and the attack method has access to the raw data and knowledge of the specific DD method used. These are highly restrictive and unrealistic assumptions, as raw data and DD methods are typically strictly protected by the owner in practice. In contrast, our threat model adopts a more practical and relaxed assumption, not requiring all users to be benign and permitting the attacker to operate without access to the raw data.\nAttack Scenario. In our threat model, the attacker intercepts the distribution process and injects backdoor information into the benign distilled dataset. The compromised dataset is then redistributed to users, allowing the attacker to manipulate the behavior of downstream models trained on the malicious dataset.\nAttacker's Goal. The primary goal of the attacker is to inject a backdoor into the distilled dataset, ensuring that downstream models trained on it exhibit malicious behavior when triggered, while maintaining high performance on benign inputs.\nAttacker's Capability. Our threat model imposes significant constraints on attackers. They do not have access to the raw dataset and can only interact with the distilled dataset, with no prior knowledge of the specific DD method used to generate it.\nChallenges. i) No Access to Raw Data: The attacker has no access to the raw dataset and must infer meaningful information solely from the significantly smaller distilled dataset, often less than one percent of the raw dataset's size. ii) Bridging the Gap Between Synthetic and Real Images: The distilled dataset is highly abstract and lacks the low-level visual details present in the raw data. The attacker must ensure that the injected backdoors are reliably triggered by real-world images in downstream tasks. iii) Maintaining Dataset Utility: The modified distilled dataset must remain effective for training models on legitimate tasks, ensuring the backdoor injection does not degrade overall performance."}, {"title": "4 Proposed Method", "content": "4.1\nProblem Statement\nAs mentioned earlier, DD aims to extract knowledge from a large-scale dataset and construct a much smaller synthetic dataset, where models trained on it perform similarly to those trained on the raw dataset. Let T denote the target dataset and S the synthetic (distilled) dataset, where |T| > |S|, indicating that the distilled dataset is much smaller than the original. The loss between the prediction and ground truth is defined as l. The DD process can then be formulated as [21]:\n$E_{(x,y)\\sim D} [l (M_T(x), y)] = E_{(x,y)\\sim D} [l (M_S(x), y)],$ \nwhere $M_T$ and $M_S$ denote the downstream model M trained on T and S, respectively. D denotes the real data distribution.\nIn this paper, we aim to update S to obtain a malicious synthetic dataset $\\hat{S}$, which is injected with backdoor information. The goal is to ensure that malicious behavior is effectively triggered when a model is trained on $\\hat{S}$. The process can be formulated as:\n$E_{x\\sim D} [M_{\\hat{S}}(x + \\tau)] \\approx y_\\tau,$\nwhere $\\tau$ is the trigger and $y_\\tau$ denotes the target label.\n$\\mathcal{L}_{hybrid} = \\alpha \\mathcal{L}_{BA} + (1 - \\alpha) \\mathcal{L}_{tr}.$\nFurthermore, for benign samples, the performance gap between models trained on S and $\\hat{S}$ should remain minimal to conceal the"}, {"title": "4.2 Overview", "content": "The overview of the proposed method is illustrated in Figure 2. As described earlier, our threat model involves three entities: the dataset owner, the attacker, and the benign user. The dataset owner generates a benign distilled dataset S from the raw dataset D and distributes it to users upon request. The attacker intercepts the distribution process and converts the benign distilled dataset into a malicious version.\nSpecifically, our attack method consists of three main phases. First, the attacker trains a downstream model using the benign distilled dataset S. Next, leveraging the trained model, the attacker reconstructs conceptual archetypes for each class using the proposed Concept Reconstruction Blocks (CRBs). Finally, the attacker injects backdoor information into reconstructed conceptual archetypes and employs a hybrid loss to update the distilled dataset, ensuring that the backdoor is embedded while minimizing performance degradation. Once the malicious distilled dataset is created, it is redistributed to users.\nThe benign user then trains the local model $M_u$. Finally, the attacker can target the user-side system by injecting the triggers into real images, activating the malicious behavior in $M_u$."}, {"title": "4.3 Proposed Attack Method", "content": "Our attack method consists of three main phases, which work together to effectively inject backdoor information while preserving the knowledge from the raw dataset. We detail each phase in the following sections:\nBenign Training. After intercepting the distribution, the attacker first trains a benign downstream model using the distributed distilled dataset. The attacker-side trained downstream model is defined as M, which is the foundation of the subsequent phases.\nConceptual Archetypes Reconstruction. Under our strict assumption, the attacker has no access to real images and can only leverage the distilled dataset. However, during the inference phase, the system's input typically consists of real images. This raises a critical question: How can the backdoor be activated when injected into real images without relying on any raw data during backdoor training?\nTo bridge the gap between distilled and real data, we propose reconstructing conceptual archetypes for each class. Although generating low-level, semantically similar images without access to raw data is infeasible, this limitation is not critical. In deep networks, accurate classification primarily relies on ensuring that the latent feature representations of the conceptual archetypes closely align with those of the real images.\nThe reconstruction process aims to generate conceptual archetypes for each class by iteratively refining random noise to align with the high-level feature representations of the target class in M. Specifically, for the c-th class, the process consists of K Concept Reconstruction Blocks (CRBs), each corresponding to an optimization step. The conceptual archetype initialization process for each class c can be formulated as:\n$x \\sim \\mathcal{N}(0, I),$\nwhere $\\mathcal{N}(0, I)$ represents a Gaussian distribution with zero mean and identity covariance matrix. $x \\in \\mathbb{R}^{C\\times H\\times W}$ denotes the initialized conceptual archetype for the c-th class, where C, H, and W denote the channel, height, and width of the distilled data, respectively.\nIn the k-th CRB block, $\\hat{x}^{k-1}$ is optimized to align the model's output with the c-th class representation. The optimization objective"}, {"title": null, "content": "is defined as follows:\n$\\mathcal{L}_{re}(\\hat{x}^{k-1}, c) = -y_c \\log \\left( M(\\hat{x}^{k-1})_c \\right),$\nwhere $\\mathcal{L}_{re}$ is the reconstruction loss, $y_c$ represents the one-hot encoded label for class c.\nThe optimization process can be formulated as:\n$\\hat{x}^{k} = \\hat{x}^{k-1} - \\eta \\cdot \\nabla_{\\hat{x}^{k-1}} \\mathcal{L}_{re} (\\hat{x}^{k-1}, c),$\nwhere $\\eta$ is the learning rate, and $\\nabla_{\\hat{x}^{k-1}} \\mathcal{L}_{re} (\\hat{x}^{k-1}, c)$ represents the gradient of the reconstruction loss with respect to the input $\\hat{x}^{k-1}$.\nAfter K iterations, the reconstructed image $\\hat{x}$ serves as the conceptual archetype for class c. This process is repeated m times for each class to generate m archetypes, with m set to 5 in this paper. Figure 3 illustrates a t-SNE visualization [29] comparing the deep feature representations of the conceptual archetypes with those of real images in MNIST [19]. The results show that the reconstructed archetypes closely align with the deep feature representations of real images, effectively bridging the gap between the distilled dataset and real images.\nMalicious Distilled Dataset Synthesis. The goal of the attack is to synthesize a malicious distilled dataset such that the backdoor can be effectively activated by real images while maintaining the utility of the dataset for benign tasks. By reconstructing conceptual archetypes to bridge the gap between the distilled and real data, we can leverage them to embed malicious knowledge into the distilled dataset.\nSpecifically, for each conceptual archetype $\\hat{x}$, we obtain the backdoored sample x' as follows:\n$x^{\\prime}(h, w)=\\left\\{\\begin{array}{ll}v, & \\text { if } h \\geq H-t \\text { and } w \\geq W-t \\\\\\hat{x}(h, w), & \\text { otherwise, }\\end{array}\\right.$\nwhere v represents the trigger value and t specifies the trigger size. Then, a backdoor loss is designed to embed malicious information into the distilled dataset, ensuring that the backdoor behavior is learned by the model trained on the modified data. The backdoor loss is defined as:\n$\\mathcal{L}_{BA} = -y_{\\tau^\\prime} \\log \\left( M_A(x^\\prime)_{\\tau^\\prime} \\right),$\nwhere $y_{\\tau^\\prime}$ represents the backdoor target label, and $M_A$ is the attacker-side model, trained from scratch.\nTo conceal the malicious behavior from detection, it is essential to minimize performance degradation. This requires ensuring that the optimization trajectory of downstream models trained on the malicious distilled dataset closely aligns with those trained on the benign distilled dataset. Specifically, a trajectory consistency loss is introduced to enforce this alignment as follows:\n$\\mathcal{L}_{tr} = \\frac{1}{|\\Theta|} \\sum_{\\theta \\in \\Theta} \\|\\nabla_\\theta \\mathcal{L}_{ta}(S) - \\nabla_\\theta \\mathcal{L}_{ta}(\\hat{S})\\|^2,$\nwhere $\\Theta$ denotes the set of model parameters of $M_A$, $\\mathcal{L}_{ta}$ represents the loss of the downstream task.\nBy constraining $\\mathcal{L}_{tr}$, we can ensure that the malicious dataset maintains a similar optimization trajectory to the benign dataset, thereby concealing malicious behavior while minimizing the impact on the performance of downstream tasks. Finally, we combine both losses to form the overall objective for synthesizing the malicious distilled dataset. The hybrid loss function is defined as follows:\n$\\mathcal{L}_{hybrid} = \\alpha \\mathcal{L}_{BA} + (1 - \\alpha) \\mathcal{L}_{tr},$\nwhere $\\alpha$ is the balancing parameter that controls the trade-off between embedding malicious information and maintaining trajectory consistency.\nThen, $\\hat{S}$ is iteratively updated to minimize $\\mathcal{L}_{hybrid}$ as:\n$\\hat{S} \\leftarrow \\hat{S} - \\eta \\nabla_{\\hat{S}} \\mathcal{L}_{hybrid}.$\nThese steps are repeated for N iterations within a single epoch. To ensure that model $M_A$ follows the next benign optimization trajectory, it is updated on $\\hat{S}$ after each epoch. This entire process is repeated for E epochs.\nImplementation. Once the attacker synthesizes the malicious distilled dataset $\\hat{S}$, it is redistributed to the users. Users then train their downstream models $M_u$ on $\\hat{S}$ using their own training strategies. During the inference phase, the malicious behavior is activated when the trigger is injected into real images following Eq. (8) to produce malicious outputs aligned with the attacker's target, while maintaining normal performance on benign inputs.\nNotably, our attack method remains effective even when $M_u$ and $M_A$ have different architectures. Furthermore, it does not require fine-tuning any DD process on the dataset owner's side, nor does it require access to raw data. Therefore, our method is versatile and practical across various scenarios."}, {"title": "5 Experiments", "content": "5.1 Experimental Setting\nExperiment Environment. Our proposed method is implemented using the PyTorch framework and optimized with Stochastic Gradient Descent (SGD) [14] with a learning rate of 0.01. The number of epochs for synthesizing the malicious dataset is set to 10. The"}, {"title": "5.2 Experiments about Different Training Strategies", "content": "In this subsection, we evaluate the impact of different training strategies on the effectiveness of our attack based on DC. Specifically, we analyze performance across different numbers of training epochs in user-side training, and we treat the performance of models trained directly on the benign distilled dataset as the baseline.\nAs shown in Table 1, our attack remains highly effective across different numbers of training epochs, consistently maintaining a high ASR while inducing minimal BA degradation. Besides, our method demonstrates strong generalizability across various raw datasets and different images per class (IPC) settings, ensuring its robustness in diverse scenarios.\nIn previous experiments, we assume that the user-side model was identical to the attacker's model. To further validate the robustness of our method, we investigate a more challenging scenario where the user-side model differs from the attacker's model. We analyze the attack performance under different training strategies, and the results are presented in Figure 4. In this experiment, we use CIFAR-10 as the raw dataset based on DC with setting IPC to 1. As shown in the results, our method remains highly effective, consistently delivering strong attack performance even when the user-side model differs from the attacker's model."}, {"title": "5.3 Experiment with Different Dataset Distillation Methods", "content": "To further validate the effectiveness of our attack, we extend our experiments to different DD methods, with the results summarized in Table 2. We conduct evaluations using user-side training strategies of 50 and 100 epochs. As shown in Table 2, our attack consistently demonstrates strong performance across various DD methods. In most cases, the attack achieves nearly 100% ASR, effectively embedding the backdoor into the distilled dataset, regardless of the specific DD approach employed. Additionally, the BA degradation remains within an acceptable range, which indicates that the overall utility of the dataset is well preserved. These results confirm the generalizability and robustness of our proposed attack method, demonstrating its effectiveness across different distillation strategies while maintaining the performance of downstream tasks."}, {"title": "5.4 Visualization", "content": "Figure 5 presents a visual comparison between benign and malicious distilled datasets. The first row displays examples of benign distilled images, while the second row illustrates their malicious"}, {"title": "5.5 Ablation Study", "content": "In previous experiments, we used ConvNet as the attacker's downstream model. In this experiment, we evaluate the impact of different model architectures on the effectiveness of our attack. To demonstrate the generalizability of our method, we conduct experiments on CIFAR-10 distilled using different DD methods, with IPC set to 10. The results, presented in Table 3, indicate that our attack remains highly effective across various model architectures. It can be seen that our threat model operates under relatively weak assumptions, making it highly practical in real-world scenarios. Despite these relaxed constraints, our attack maintains strong performance across various settings.\nTo further analyze the impact of different components in our method, we conduct an ablation study on the effect of \u03b1 in Eq. (11), which balances the tradeoff between attack effectiveness and benign task performance. In this experiment, we use the CIFAR-10 dataset distilled by the DC method, with IPC set to 1. The results are"}, {"title": "5.6 Computational Complexity", "content": "Our attack method is highly efficient and lightweight. To evaluate its computational cost, we conduct experiments on a CIFAR-10 dataset distilled by the DC method with IPC set to 1. The computational complexity varies based on the number of reconstructed conceptual archetypes, and the results are summarized in Table 4. It can be seen that different numbers of conceptual archetypes achieve effective attacks while maintaining minimal impact on benign performance. Therefore, we recommend using m = 5 as the default setting, as it provides a balance between efficiency and attack effectiveness.\nThe total attack time consists of two parts: conceptual archetype reconstruction and malicious distilled dataset synthesis. In the first phase, reconstructing each conceptual archetype requires only 0.53s, and under our default setting of five archetypes per class, this step takes approximately 26.5s for CIFAR-10. In the second phase, due to the small size of the distilled dataset, each epoch of synthesizing the malicious distilled dataset takes only 1.5 seconds on a single NVIDIA GTX 3090. Consequently, the entire attack can be completed in less than one minute. This minimal time overhead makes the attack virtually imperceptible to users, as they are unlikely to notice any delays that could suggest an ongoing attack."}, {"title": "6 Conclusion", "content": "In this paper, we propose a novel backdoor attack method targeting distilled datasets, which enables successful backdoor injection without requiring access to raw data, knowledge of the DD process,"}]}