{"title": "RL-LLM-DT: An Automatic Decision Tree Generation Method Based on RL Evaluation and LLM Enhancement", "authors": ["Junjie Lin", "Jian Zhao", "Yue Deng", "Youpeng Zhao", "Wengang Zhou", "Houqiang Li"], "abstract": "Traditionally, AI development for two-player zero-sum games has relied on two primary techniques: decision trees and reinforcement learning (RL). A common approach involves using a fixed decision tree as one player's strategy while training an RL agent as the opponent to identify vulnerabilities in the decision tree, thereby improving its strategic strength iteratively. However, this process often requires significant human intervention to refine the decision tree after identifying its weaknesses, resulting in inefficiencies and hindering full automation of the strategy enhancement process. Fortunately, the advent of Large Language Models (LLMs) offers a transformative opportunity to automate the process. We propose RL-LLM-DT, an automatic decision tree generation method based on RL Evaluation and LLM Enhancement. Given an initial decision tree, the method involves two important iterative steps. Response Policy Search: RL is used to discover counter-strategies targeting the decision tree. Policy Improvement: LLMs analyze failure scenarios and generate improved decision tree code. In our method, RL focuses on finding the decision tree's flaws while LLM is prompted to generate an improved version of the decision tree. The iterative refinement process terminates when RL can't find any flaw of the tree or LLM fails to improve the tree. To evaluate the effectiveness of this integrated approach, we conducted experiments in a curling game. After iterative refinements, our curling AI based on the decision tree ranks first on the Jidi\u00b9 platform among 34 curling AIs in total, which demonstrates that LLMs can significantly enhance the robustness and adaptability of decision trees, representing a substantial advancement in the field of Game AI. Our code is available at https://github.com/Linjunjie99/RL-LLM-DT.", "sections": [{"title": "I. INTRODUCTION", "content": "The rise of artificial intelligence (AI) has brought about transformative changes in various domains, including sports and e-sports. In the realm of sports, AI has been employed to analyze player performance, optimize training regimens, and even simulate game strategies. E-sports, on the other hand, have seen AI as a natural extension of competitive gaming, where algorithms can compete against human players or assist in decision-making processes. Reinforcement learning (RL), a subfield of machine learning, has emerged as a powerful tool for creating AI agents capable of learning optimal strategies through trial and error [1], [2]. In competitive environments, RL has demonstrated remarkable success, such as mastering complex board games like Go [3], [4] and achieving super-human performance in video games such as Dota 2 [5] and StarCraft II [6] and in card games like DouDiZhu [7] and GuanDan [8]. The ability of RL to adapt and improve through experience makes it particularly well-suited for dynamic and unpredictable scenarios.\nWhile RL-based AI has shown promise in competitive gaming, it often struggles with stability when facing unfamiliar opponents. By contrast, the decision tree performs stably against diverse opponents and its structured essence brings better interpretability. However, the decision tree requires extensive prior knowledge and otherwise is prone to errors in edge cases, leading to unexpected outcomes. Our former work [9] combines the advantages of RL and decision tree. We utilize RL to train a policy model against the developed decision tree to detect its flaws and manually refine the decision tree according to the detected flaws. However, the reliance on human intervention in this process rendered the method non-automated, thereby limiting its scalability and practical applicability. Fortunately, the emergence of LLMS has demonstrated significant potential to automate and enhance this refinement process.\nLLMs, such as OpenAI's GPT series [10], have demonstrated an unparalleled ability to understand and generate human-like text, making them ideal for tasks that require nuanced understanding and complex decision-making. Recent studies, including but not limited to ReAct [11], SayCan [12], Toolformer [13], HuggingGPT [14] and WebGPT [15], have substantiated the viability of autonomous decision-making agents that are built upon a large language model foundation. The emergence of code-oriented Large Language Models [16], [17] has significantly enhanced the robustness of code generation capabilities, thereby introducing a novel paradigm for their application. During the pre-training phase, LLMs assimilate extensive human experience and domain-specific knowledge, particularly within the coding domain, thus es-tablishing a robust foundation for generating code capable of addressing intricate tasks. These agents can take task descriptions as inputs, subsequently generating corresponding decision tree code.\nIn this paper, we integrate RL and LLMs to develop high-performance and robust AI decision trees for curling game. This hybrid approach employs RL to identify the decision tree's shortcomings and utilizes LLMs to refine and improve the decision tree. We establish the decision tree as the built-in opponent within the curling game environment and train a neural model to surpass it through RL. Subsequently, we leverage LLMs to analyze the game trajectories where the decision tree loses to neural models, gaining deeper insights into its vulnerabilities. This analysis enables us to refine the decision tree to enhance its performance. Additionally, LLMs are employed to generate the strategy code directly from the improved decision tree, ensuring consistency and accuracy in implementation.\nThrough this innovative integration of LLMs, decision trees, and reinforcement learning, we aim to push the boundaries of AI in sports and e-sports, offering a robust and versatile framework for future research and application."}, {"title": "II. RELATED WORK", "content": "The integration of artificial intelligence (AI) into sports strategy generation has garnered substantial academic and practical interest in recent years. A significant advancement in this domain is the Google Research Football Environment [18], a physics-based 3D simulation designed for reinforcement learning in football. This environment facilitates the training of Al agents by replicating real-world football scenarios, featuring eleven distinct game scenes and an internal opponent AI with adjustable difficulty levels. Notably, this initiative has attracted the attention of Manchester City, one of the world's leading football clubs. Manchester City has collaborated with Google to explore the application of football AI in enhancing the training of professional football players, underscoring the potential of AI in refining tactical decision-making and performance. In basketball, NetEase has developed basketball AI for its mobile game Street Basketball [19], which has demonstrated proficiency in fundamental skills such as pass-ing, shooting, and penetration, as well as advanced tactical maneuvers such as pick-and-roll plays, team defense, and strategic passing following penetrations.\nThe domain of intellectual sports, such as chess and card games, has also witnessed remarkable achievements in AI development. For instance, Google's AlphaGo [3] and its successor AlphaGo Zero [4] have achieved landmark victories over professional Go players, revolutionizing the field of AI-assisted strategy development. Many professional Go players now incorporate Go AI into their training regimens to refine their skills and analyze game strategies. Similarly, Suphx [20], a mahjong AI that integrates supervised learning, rein-forcement learning, and specialized techniques, has surpassed 99.99% of players on the Japanese mahjong platform Ten-hou and has outperformed top-ranked masters. Additionally, DouZero [21], an AI system for the card game DouDiZhu, has demonstrated performance levels comparable to top hu-man players, highlighting the versatility of AI in competitive gaming.\nIn the realm of e-sports, AI developed through deep rein-forcement learning have exhibited superior performance com-pared to elite human professional players. A notable example is AlphaStar [6], an AI designed to master the complex Real-Time Strategy (RTS) game StarCraft II. AlphaStar achieved a milestone by defeating a top human player in an exhibition match, adhering to limited Actions Per Minute (APM) con-ditions to ensure fairness. Another significant achievement is OpenAI Five [5], an Al system for the Multiplayer Online Battle Arena (MOBA) game Dota 2, which triumphed over a championship-level team. These successes have not only showcased the capabilities of AI in competitive gaming but have also inspired human players to engage with and challenge game AI, fostering a symbiotic relationship between human and machine in the pursuit of strategic excellence."}, {"title": "B. LLM Agents", "content": "Large Language Model (LLM) agents have demonstrated remarkable capabilities in complex game environments, show-casing their potential in understanding, reasoning and deci-sion making. ChessGPT [22], for instance, integrates policy learning with language modeling by leveraging data from both game and language datasets to enhance performance in Chess games. Voyager [23], on the other hand, highlights the ability of LLM agents to navigate and perform tasks within the MineDojo [24] environment, a complex open-world simulation. In the realm of Real-Time Strategy (RTS) games, TextStarCraft [25] introduces a natural language interface that enables LLMs to engage in StarCraft II. By employing the Chain-of-Summarization method, TextStarCraft facilitates efficient reasoning and decision making, allowing the agent to process complex game states and generate coherent strategies. Pok\u00e9llmon [26] represents another significant advancement, introducing an environment that allows LLMs to participate in Pok\u00e9mon battles. The agent developed by Pok\u00e9llmon achieves human-level performance by consuming instant feedback to iteratively refine its policy. Additionally, it incorporates mech-anisms for retrieving external knowledge to mitigate hallu-cination, a common challenge in LLM-based systems. This dual approach enhances the agent's robustness and adapt-ability, enabling it to perform competently in dynamic and knowledge-intensive game scenarios. Furthermore, Cradle [27] introduces a multi-modal agent capable of perceiving the game screen, analyzing game instructions, generating action plans, and directly controlling characters through mouse/keyboard operations in the renowned 3D action-adventure game Red Dead Redemption 2 (RDR2) based on the multi-modal LLM GPT-4V.\nWith the increasing robustness of LLMs in generating code, direct generation of executable codes or scripts becomes feasible. Researchers from UIUC and Apple propose the general framework CodeAct [28], which allows LLMs to generate executable Python code as their action. Compared to JSON and pre-formatted text formats, code inherently supports control and data flow, enabling intermediate results to be stored as variables for reuse. A single piece of code can combine multiple tools to perform complex logical operations (e.g., if statements and for loops), unlocking the potential of pre-trained programming knowledge of large models to handle complex tasks. Eureka [29] is a general-purpose reward design algorithm driven by coding large language models and contextual evolutionary search. Without prompt engineering or human intervention for any specific task, Eureka enables human-level reward generation across a wide range of robots and tasks."}, {"title": "III. BACKGROUND", "content": "In this section, we briefly introduce the curling game environment and review the concept and formulation of re-inforcement learning."}, {"title": "A. Curling Game Environment", "content": "We choose the curling game environment developed by Jidi platform for our study. The visualization of the curling game environment can be seen in Figure 1.\n1) Game Rules: In the curling game environment, two teams compete against each other, taking turns to throw stones. The detailed rules of the curling game environment are as below:\nEach team controls four elastic spherical stones with the same mass and radius.\nThe agents of both teams take turns to throw the stones to the target point in the center of the field, and each agent has four chances to throw in each game.\nStones can collide with each other and against walls, but they will lose a certain amount of velocity after collision.\nThe agent can no longer exert force on the stones after it has passed the red line.\nThe agent's field of view is limited to a matrix area of 30*30 facing forward.\nTwo games in total and swap the order of serve in the second game.\nAt the end of each game, the score will be calculated, and one point will be awarded for each curling stone of one team that is closer to the center area than all the curling stones of the other team, and only one team will score points in each game.\nAt the end of the two games, the winner will be deter-mined based on the total score of the two games, with the team with the highest score being the winner.\nWhen the two games are over, the environment ends.\n2) Game Pre-process: Observation pre-process: as men-tioned in the game rules section, agents can only receive a 2-dimension image-like observation. However, truly useful information is about the positions of the current stones on the ice. Therefore, we utilize template matching to calculate the coordinates of the stones. Both decision tree AI and neural model AI take the concrete coordinates information as input rather than abstract image-like observation.\nAction pre-process: before a stone crosses the red line, agents can exert a force with a specific angle and magnitude. In other words, the trajectory of a stone after it crosses the red line is determined by its position and velocity. Therefore, there is no need to change the angle and magnitude of the exerted force at each step. Instead, selecting an initial position and throwing the stone with a fixed angle and fixed magnitude force can reduce the complexity while maintaining flexibility.\n3) Game Platform: Jidi is an online competition plat-form that provides users with massive environments, high-quality competitions, real-time discussions and fair algorithm rankings. The platform boasts high levels of openness and source availability in areas such as agent algorithms, arena competitions, and forum communities, offering a channel for exchange and sharing among researchers, students, and industry professionals in the field of game decision-making both domestically and internationally. We submit our curling AI to this platform for testing our Al's ability."}, {"title": "B. Reinforcement Learning", "content": "Reinforcement learning [30] (RL) is a type of machine learning that involves an agent learning to make decisions by interacting with an environment. The goal of the agent is to learn a strategy for selecting actions that maximizes some notion of cumulative reward over time.\n1) Markov Decisioin Process: A Markov Decision Process (MDP) [30] is a mathematical framework used to model decision-making in situations where outcomes are influenced both by random factors and by the actions of a decision maker. MDPs are particularly useful in reinforcement learning, where they provide a way to formalize the problem of learning from interactions with an environment to achieve a goal. A MDP can be defined as a tuple (S, A, P, R, \u03b3), in which S denotes the set of possible states, A denotes the set of possible actions, P denotes the transition probability function, R denotes the reward function, and \u03b3 denotes the discount factor that determines the importance of future rewards. At each step t, the agent takes an action \\(a_t \\in A\\) according to the current state \\(s_t \\in S\\). Then the environment state transforms to \\(S_{t+1}\\) according to the probability of \\(P(s_{t+1} | s_t, a_t)\\). Meanwhile, the agent receives a reward signal \\(r_t = R(s_t, a_t, S_{t+1})\\). The accumulated reward is defined as:\n\\(R_t = \\sum_{\\tau=t}^{T} \\gamma^{\\tau-t} r_{\\tau}\\) (1)\nThe expected cumulative reward of a certain policy \u03c0 for a given state s is calculated as follows:\n\\(R^{\\pi}(s) = E_{a_t \\sim \\pi, s_{t+1} \\sim P}[\\sum_{t=0}^{\\infty} r_t | s_0 = s]\\). (2)\nThe primary objective of RL is to find an optimal policy that maximizes the expected cumulative reward:\n\\(J(\\pi) = E_{s_0 \\sim P}[R^{\\pi}(s_0)]\\). (3)\n2) Proximal Policy Optimization: PPO [31] is a popular algorithm in reinforcement learning, particularly in the field of deep reinforcement learning. It is introduced as an improve-ment over previous policy gradient methods [32], aiming to provide a balance between ease of implementation, sample efficiency, and computational complexity. PPO is a policy-based algorithm and adopts an actor-critic framework. The value loss of PPO is defined as:\n\\(L_{value} = E[(r_t + \\gamma V_\\theta(s_{t+1}) - V_\\theta(s_t))^2]\\). (4)\nIn order to alleviate the effect of off-policy learning, PPO utilizes a clipped policy loss:\n\\(C_{clipped}^{policy} = E[min(ratio A_{\\pi_{old}}(a|s), clip(1 - \\epsilon, ratio, 1 + \\epsilon)A_{\\pi_{old}}(a|s))],\\)\n\\(clip(1-\\epsilon, ratio, 1+\\epsilon) = \\begin{cases} 1+\\epsilon, & \\text{ratio} > 1 + \\epsilon \\\\  \\text{ratio}, & 1-\\epsilon < \\text{ratio} < 1 + \\epsilon \\\\ 1-\\epsilon, & \\text{ratio} < 1 - \\epsilon \\end{cases}\\) (5)\nwhere \\(ratio = \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{old}}(a|s)}\\), \\(A_{\\pi_{old}}(a|s)\\) means the advantage estimation which is calculated by Generalized Advantage Estimation (GAE) [33]. PPO utilizes entropy loss to encourage exploration:\n\\(L_{entropy} = E_s [\\sum_{a \\in A} \\pi_\\theta(a|s) log \\pi_\\theta(a|s)]\\). (7)\nOverall, the total loss of PPO is calculated as:\n\\(L_{PPO} = C_{clipped}^{policy} + c_v L_{value} + c_e L_{entropy}\\), (8)\nin which \\(c_v\\) is the value coefficient and \\(c_e\\) is the entropy coefficient."}, {"title": "IV. METHOD", "content": "In this section, we introduce our hybrid method to build decision tree with robust and outstanding performance. The reasoning and coding capability of LLM helps generate deci-sion tree policy and decision tree code, while RL contributes to finding the flaws of the decision tree. Following is a detailed discussion for our framework."}, {"title": "A. Framework", "content": "The proposed framework, illustrated in Figure 2, is com-posed of three pivotal modules: the Large Language Model (LLM) Coder, the Reinforcement Learning (RL) Debugger, and the LLM Critic. This framework is designed to iteratively refine decision tree tactics within the context of the curling game environment.\nThe LLM Coder module is responsible for generating Python scripts that implement the specified decision tree tactic. Given the LLM's limited prior knowledge of the curling game environment, the prompt provides detailed descriptions and uses of the available interfaces. This contextual information is crucial for the LLM to generate executable decision tree code that accurately reflects the desired tactic.\nOnce the decision tree code is generated, it is passed to the RL Debugger module. Here, a customized deep neural model is trained using a deep reinforcement learning algo-rithm to compete against the decision tree code. During the training process, the deep neural network iteratively improves its performance through interactions with the curling game environment. If the decision tree code is defeated by the deep neural model, detailed records of the lost games are collected. These records include essential information, such as state transitions and actions taken during gameplay.\nSubsequently, the information about lost games is passed to the LLM Critic module. The LLM Critic analyzes the game information to summarize the reasons behind the decision tree's defeat. This analysis involves identifying patterns, errors, or inefficiencies in the decision tree tactic that led to its failure. Based on this analysis, the LLM Critic designs an improved version of the decision tree tactic. The improvement process leverages both the previous decision tree information and the feedback from the failure of previous games, which aims to address the identified flaws and enhance the decision tree's performance.\nThe improved decision tree tactic is then re-evaluated within the framework. The refined decision tree code is passed back to the RL Debugger module, where the deep neural model is retrained to compete against it. This loop continues iteratively, allowing the decision tree tactic to be refined progressively. The iterative refinement process terminates when one of the following conditions is met: The RL Debugger module is unable to identify any further flaws in the decision tree tactic. The LLM Critic module is unable to propose any further improvements to the decision tree tactic."}, {"title": "B. LLM Coder", "content": "The coder module is utilized to generate the executable Python code to implement a given decision tree tactic. As the rules and APIs of the curling game environment are specific, they need to be clearly defined in the prompt. We use \\(I_{rules}\\) to denote information about the curling game environment and use \\(I_{APIs}\\) to represent available environment interfaces. Then executable decision tree code can be obtained from the LLM Coder:\n\\(C = LLM_{coder}(DT, I_{rules}, I_{APIs})\\), (9)"}, {"title": "C. RL Debugger", "content": "Algorithm 1: Actor Process\n1: Initialize local policy model M and local buffers D;\n2: for iteration = 1, 2, 3, ... do\n3:  Synchronize policy model M with the learner process and reset the game environment;\n4:  for t = 0, 1, 2, ..., T do\n5:   Calculate \\(a_t\\), \\(v_t\\), \\(\\pi(a_t|s_t)\\) according to \\(s_t\\) through model M;\n6:   Perform \\(a_t\\) and observe \\(s_{t+1}\\) and reward \\(r_t\\);\n7:   Store (st, at, rt, vt, \u03c0(at|st)) to buffer D;\n8:  end for\n9:  vT+1 \u2190 0;\n10: d\u0442+1 \u2190 0;\n11: for t = T, T-1, ..., 0 do\n12:  \\(d_t \u2190 r_t + \\gamma v_{t+1} - v_t + \\lambda d_{t+1}\\);\n13:  \\(r_t \u2190 d_t + v_t\\) and update \\(r_t\\) in D;\n14: end for\n15: Send data (st, at, rt, vt, \u03c0(at|st)) from buffer D to shared buffer B;\n16: end for\nAlgorithm 2: Learner Process\n1: Initialize local policy model M;\n2: for iteration = 1, 2, 3, ... until convergence do\n3:  if Shared buffer B is half full then\n4:   Sample a batch of (st, at, rt, vt, \u03c0(at|st)) from B and the batch size is accurately the half size of B;\n5:   Update model M with value loss and clipped policy loss in PPO and learning rate \u03b1;\n6:  end if\n7: end for\nIn the RL Debugger module, a deep neural model is trained against the generated decision tree code C in the curling game environment. Deep RL algorithms can be freely applied in this module. In order to accelerate the training process, we utilize the distributed version of PPO as our training algorithm. We adopt multiple actor processes to interact with the curling game environment for collecting samples in the replay buffer, which is detailed in the Algorithm 1. As for the single learner process summarized in Algorithm 2, the deep neural model is optimized using the PPO loss on the trajectories data from actor processes. In order to prevent the sampling policy from deviating far from the learning policy, the updated deep neural model is synchronized periodically with the model in the actor process. When the win rate of the neural model stabilizes, a customized neural model against the generated decision tree code is obtained:\n\\(M_C = RL_{debugger}(C, Env_{curling})\\). (10)"}, {"title": "V. EXPERIMENTS", "content": "In this section, we initiate a simple decision tree tactic as a start and progressively refine it in our framework until the LLM can't give a better version of the decision tree. Within the process, the decision tree becomes stronger and stronger and it takes a longer time for the RL debugger to train a deep neural model to beat it. We submit our different versions of the decision tree code to the Jidi platform to compete against other users' curling AI. The final version of our curling AI ranks first on the agents' PK platform, showing the effectiveness of our method and the ability of LLMs to design curling game strategies and implement corresponding Python scripts."}, {"title": "A. The Efficacy of the Framework", "content": "As evidenced by Table I, our iteratively refined decision tree curling AI demonstrates progressively superior performance on Jidi platform. The third iteration of the refined decision tree achieves the top rank on the platform, surpassing the curling AI of any other team, thereby validating the efficacy of our methodology. A total of 34 teams submitted their customized curling AI for evaluation. Despite the diverse and unpredictable strategies employed by the opponents, our best decision tree curling AI attained an exceptionally high evaluation score of 0.93 out of 1 when competing against unseen opponent curling Als, higher than the best decision tree version in our former work. This outcome reveals the robust performance of the generated decision tree curling AI. As illustrated in Figure 3, compared to the final decision tree refined by humans, the final decision tree generated by LLM shares a more complicated structure, which takes the remaining stones' number into account and prioritizes attack or protection according to the game situation. It tends to hit the opponent's stone in the early stage of the curling game while prioritizing protection when it comes to the last throw. Also, the decision tree generated by LLM tends to ignore opponent's stone away from the center area while the tree does not. By incorporating LLM, we successfully develop a robust decision tree with high performance for the curling game, even better than the version generated by human in our former work."}, {"title": "B. The Efficacy of LLM Coder", "content": "For a given decision tree tactic, the LLM Coder module generates the corresponding Python code. At the outset of the iterative process, we initialize a rudimentary decision tree tactic and request the LLM Coder to translate this tactic into Python scripts. An illustrative user prompt example is presented in Figure 4, with the corresponding initial decision tree output showcased in Figure 5. Remarkably, the LLM Coder not only faithfully implements the Python code in accordance with the provided decision tree tactic description but also subtly enhances the decision tree by incorporating a protective operation, which was not explicitly demanded by the tactic. This enhancement underscores the LLM Coder's capability to autonomously refine and fortify the generated code."}, {"title": "C. The Efficacy of RL Debugger", "content": "We apply distributed PPO to train a customized deep neural model against the decision tree code in the curling game environment. As discussed in section III-A2, the policy model necessitates the incorporation of stone coordinates within its input domain. Furthermore, to augment the granularity of game-related information, the number of remaining stones for both our team and the opponent is integrated into the policy model's input. Due to the limited control accuracy of the applied force, the continuous action space of the curling game is systematically discretized through uniform sampling. This discretization enables the policy model to generate a discrete probability distribution as its output. The architecture of our policy model is predicated upon a Multi-Layer Perceptron (MLP) framework. The input of the model comprises the coordinates of specific stones and the tally of remaining stones, rather than abstract graphical observations. This design ensures that the policy model is directly informed by the essential spatial and quantitative attributes of the game state, thereby enhancing its decision-making capabilities."}, {"title": "D. The Efficacy of LLM Critic", "content": "As illustrated in Figure 8, the LLM Critic ingests the rules of the curling game, the description of the decision tree, the corresponding executable Python code implemented by the LLM Coder, and the trajectory of the lost game as input. The LLM Critic is tasked with diagnosing the reasons behind the decision tree code's failure in the curling game against the customized neural model and is required to generate an improved version of the decision tree tactic.\nThe output shown in Figure 9 reveals that the LLM Critic meticulously analyzes the shortcomings of the current decision tree and the strategies employed by the deep neural model to out-maneuver it. Drawing upon this analysis, the LLM Critic formulates an enhanced version of the decision tree.\nGiven that the curling game environment is not a widely recognized domain, the LLM Critic does not possess exten-sive prior knowledge about this specific environment. Con-sequently, after two cycles of refinement, when the decision tree ascends to the top rank on the Jidi platform, the LLM Critic's ability to provide clear and actionable improvement advice diminishes. As a result, the LLM Coder generates an identical decision tree code, with the only variation being the code comments, as shown in Figure 10. In this way, we terminate the iterative refinement loop until the LLM Critic fails to further enhance the decision tree."}, {"title": "E. Discussion", "content": "1) Explainable Strategy: The decision tree code generated by the LLM inherently constitutes an explainable strategy. In comparison to deep neural model policies derived from reinforcement learning, decision trees offer transparent and interpretable decision-making processes, enabling stakeholders to comprehend and trace the rationale behind each decision. This transparency is vital for fostering trust and accountabil-ity, particularly in critical applications such as autonomous driving. Moreover, decision trees are inherently modular and can be effortlessly modified or extended to incorporate new rules or conditions, rendering them more adaptable to evolving environments or requirements. Conversely, deep neural net-work models often lack such interpretability and flexibility. Furthermore, decision trees are less susceptible to overfitting compared to deep neural models, as they naturally capture hierarchical relationships. In contrast, policy models tend to perform poorly when competing against unseen opponents during training, highlighting the robustness and generalization ability of decision trees.\n2) Human Prior Knowledge: The development of a deci-sion tree inherently necessitates robust human prior knowl-edge, as it involves structuring rules and conditions that guide the decision-making process. This requirement presents a significant challenge, particularly in specialized environments such as the curling game, where expert knowledge may be scarce or inaccessible. However, large language models exhibit the remarkable capability to supply this essential knowledge to generate decision trees. By harnessing vast amounts of pre-existing text data, LLMs can distill pertinent information, identify patterns, and formulate logical structures that emulate human expertise. This capability not only mitigates the depen-dency on human experts but also accelerates the development process, enabling the creation of a sophisticated decision tree. Consequently, LLMs serve as invaluable tools in augmenting human knowledge and facilitating the efficient generation of decision trees, showing a potential to enhance the adaptability and robustness of AI systems in complex environments."}, {"title": "VI. CONCLUSION", "content": "In this paper, we have introduced a novel hybrid approach which leverages the strengths of both RL and LLMs to itera-tively refine decision tree tactics, enhancing their performance and adaptability. The proposed framework consists of three pivotal modules: the LLM Coder, the RL Debugger, and the LLM Critic. The LLM Coder generates executable Python code based on the specified decision tree tactic, ensuring con-sistency and accuracy in implementation. The RL Debugger trains a customized deep neural model to compete against the generated decision tree code, identifying its shortcomings. The LLM Critic analyzes the loss trajectories and provides insights into the decision tree's vulnerabilities, generating an improved version of the tactic. The iteratively refined decision tree curling AI achieves superior performance on the Jidi platform, ranking first among 34 competing teams.\nWe posit that the advent of domain-specific Large Language Models presents a novel avenue for the generation of deci-sion tree code, which can be subsequently refined through Reinforcement Learning to identify and help mitigate potential flaws. This integrated approach holds significant promise for the development of robust and high-performance strategies. Moving forward, we intend to extend the applicability of our methodology to other domains, including but not limited to chess, card games, and e-sports such as StarCraft, thereby broadening its impact and demonstrating its versatility across diverse strategic contexts."}]}