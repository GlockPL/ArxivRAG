{"title": "The Reality of Al and Biorisk", "authors": ["Aidan Peppin", "Anka Reuel", "Stephen Casper", "Elliot Jones", "Andrew Strait", "Usman Anwar", "Anurag Agrawal", "Sayash Kapoor", "Sanmi Koyejo", "Marie Pellat", "Rishi Bommasani", "Nick Frosst", "Sara Hooker"], "abstract": "To accurately and confidently answer the question \"could an AI model or system increase biorisk\", it is necessary to have both a sound theoretical threat model for how AI models or systems could increase biorisk and a robust method for testing that threat model. This paper provides an analysis of existing available research surrounding two AI and biorisk threat models: 1) access to information and planning via large language models (LLMs), and 2) the use of AI-enabled biological tools (BTs) in synthesizing novel biological artifacts. We find that existing studies around AI-related biorisk are nascent, often speculative in nature, or limited in terms of their methodological maturity and transparency. The available literature suggests that current LLMs and BTs do not pose an immediate risk, and more work is needed to develop rigorous approaches to understanding how future models could increase biorisks. We end with recommendations about how empirical work can be expanded to more precisely target biorisk and ensure rigor and validity of findings.", "sections": [{"title": "Introduction", "content": "A major focus of current efforts to govern the risks associated with AI models and technologies is concerned with biological risks, known as \u201cbiorisks\". These are risks that a biological event such as [...] a release of a biological agent or biological material adversely affects the health of humans, non-human animals, or the environment (Lentzos et al., 2022).\nA relatively recent flurry of media reporting about the potential for AI to accelerate such biorisks, often featuring influential figures like think tank or industry CEOs, has influenced public discourse and evolved into policy and governance attention and activity (Metz, 2024; Reuters, 2023; Goode, 2024; Lovelace, 2022).\nFor example, the AI Safety Institutes in the US and UK are developing and conducting biorisk-related tests and guidance for advanced AI models (UK AI Safety Institute, 2024; NIST, 2024). Some AI model developers are evaluating their systems for biorisk, as well as partnering with external biology labs to test how AI models can be used safely in their work (Anthropic, 2023; OpenAI, 2024a; Google DeepMind, 2024; OpenAI, 2024b). Emerging legislative frameworks include specific provisions or references to biorisk. For example, the US White House Executive order specifies \u201cbiological threats\u201d as a focus for testing of AI models, and includes a lower compute threshold for AI models trained using primarily biological sequence data (The White House, 2023). The EU AI Act notes biological risks in the context of \u201csystemic risks\" that may be associated with general-purpose AI models (European Parliament, 2024). Considerations of biorisk also featured at the international AI Safety Summits at Bletchley Park, UK (UK Government, 2023) and Seoul, South Korea (UK Government, 2024). This recent focus has been motivated by the notion that some AI models or systems could amplify biorisks.\nIn this work, we review the available literature to-date to assess whether currently available evidence supports this focus and resource. Given the significant focus and resources allocated to the consideration of biorisk the claims and evidence underpinning it merit scientific scrutiny. Understanding whether the focus on biorisk is justified requires answering the question: could a specific AI model or system amplify biorisk? To answer this question it is necessary to have both a sound theoretical threat model for how an AI model or system increases biorisk, as well as a robust method for testing the viability or likelihood of that theoretical threat model. Additionally, the theoretical threat models should clearly define and specify in detail the type of AI model(s) or system(s) of concern. Therefore, this paper asks: given publicly available evidence, 1) are the current threat models sound and 2) are the methods we have to test them robust?\nWe find that studies around AI-related biorisk are nascent and often speculative in nature or limited in terms of their methodological maturity, and transparency around the methods underpinning some studies is limited. This leads to uncertainty about the theoretical models for how AI may augment biorisk and empirical assessments"}, {"title": "Background", "content": "Biorisk research and governance has emerged progressively since the early 20th Century, with the 1925 Geneva Protocol responding to the use of biochemical weapons in the First World War, followed later by the 1972 Biological Weapons Convention (United Nations). Beyond state development and use of bioweapons, national and international frameworks for biorisk management have iteratively emerged too, partly in response to terror group-led attacks, such as the Japanese doomsday cult Aum Shinrikyo (Kaplan, 1996) or the Amerithrax attacks (United States Department of Justice, 2010), as well as naturally emerging pandemics such as bird flu and Covid-19 (Cummings et al., 2021). These biorisk management frameworks largely center on implementing appropriate safety and security practices across facilities that conduct biological research and manufacturing, as well as monitoring of certain biological materials and synthetic development techniques (National Science & Technology Council, 2022). This is to reduce the likelihood of both accidental and malicious biological harms, by addressing risks across different stages in the \u201cbiorisk chain\". A typical \"biorisk chain\" involves multiple stages: an actor having an irresponsible, misguided, or malicious\u201d intention, which develops into a \u201cbiological idea\", which is then \"converted into biological data, such as a pathogen genome,\u201d transforming the data into a \u201clive biological artifact\u201d, culturing and testing this artifact, before successfully dispersing it \u201cinto the target environment\u201d (Sandberg & Nelson, 2020)."}, {"title": "Assessing Amplification of Biorisk", "content": "Most published work to-date on assessing the uplift in biorisk that comes from AI has centered around two threat models: 1) access to biological information and planning and 2) synthesis of harmful biological artifacts. For 1), amplification of risk is hypothesized to occur because Large language models (LLMs) could increase or \u201cuplift\u201d a users' ability to gather information relevant to planning and carrying out biological attacks (Batalis, 2023; Rose et al., 2024; Allen, 2023; Casagrande, 2023). For 2), the uplift in risk is hypothesized to be due to specialized AI \"biological tools\" assisting malicious actors in identifying new toxins, designing more potent pathogens, or optimizing existing biological agents for increased virulence (Jeffery et al., 2023; Alstott, 2023; Halstead, 2024).\nFor each of the threat models considered, we explore publicly available evidence around each of these threat models according to the following format:\n1. Threat Model: We introduce the assumed theoretical model for how a type of AI model may augment biorisk.\n2. Relevant Research: We present a summary of research, experiments, and/or studies conducted to-date around this proposed threat model.\n3. Assessment: We evaluate the scientific conclusions we can draw about this threat model and what gaps or limitations in understanding remain, based on the available evidence."}, {"title": "Access to Biological Information and Planning", "content": "Hypothesized source of amplified threat: Large language models (LLMs) could increase or \u201cuplift\u201d a users' ability to gather information relevant to planning and carrying out biological attacks (Batalis, 2023; Rose et al., 2024; Allen, 2023; Casagrande, 2023).\nThis threat model draws from an established concern in bio-security literature known as the information access threat model, where access to relevant knowledge provides an actor with an increased ability to conduct a biological attack (Cummings et al., 2021). Where an LLM is determined to support this increased access to relevant biological information, it is termed an \u201cuplift\u201d in the ability to carry out a biological attack (OpenAI, 2024a). Here, the key question is whether access to LLMs fundamentally amplifies the degree of information access beyond what is already easily available (e.g., through the internet). This increase in risk afforded by an AI model or system relative to risks afforded by existing available tools is often referred to as a \"marginal risk\u201d (National Telecommunications and Information Administration, 2024; Kapoor et al., 2024)."}, {"title": "Relevant research", "content": "Initial work on this topic by Soice et al. (2023), evaluated how access to LLMs enables users to gather information about how to develop a pathogen or biological weapon, and plan to deploy it in the real world. Through a red teaming methodology, three groups of 3-4 students with no scientific training used LLMs to see how they could assist in planning and carrying out a biological attack, for example by using them to gather information about harmful biological artifacts, or to gain guidance on how to acquire those artifacts and deploy them to cause maximum harm. The authors said their findings \"suggest that LLMs will make pandemic-class agents widely accessible [...] even to people with little or no laboratory training\" (Soice et al., 2023). However, this study did not include a critical baseline - how access to information via LLMs compared to information access via, for example, sources on the internet.\nIn later work, this important baseline comparing LLMs and information available on the internet was added. Researchers at RAND Corporation applied a similar red teaming approach which involved 45 participants who had varying degrees of expertise with both LLM technologies and in biology. In contrast to the prior study, these groups were randomly assigned to have access to the internet and an LLM, or to the internet only. The team's plans to develop a bio-attack were scored and it was found that the groups with access to LLMs in addition to the internet did not score significantly higher than those without access to an LLM (Mouton et al., 2024). No groups had access to only an LLM without internet access and so it remains unclear how strong performance is with only access to an LLM.\nSince these initial studies, similar red teaming assessments have been carried out by AI model developers. Anthropic (2024) reported \"minor uplift\" in relation to their Claude 3 model, but the statistical significance and methodological details are not fully reported. Work by OpenAI (2024a) included 100 red-teamers (more than 2x that of Mouton et al. (2024)) with different levels of expertise to represent varied types of threat actor, and found no statistically significant uplift in threat actor capability. (See Table 1.)\nIt is important to note that the studies reviewed here highlight their own methodological limitations, for example the relatively limited sample sizes in and variety of expertise in terms of red team participants. Furthermore, we observe in Table 1 that the majority of studies were"}, {"title": "Assessment", "content": "The available evidence suggests that information access via current, publicly available LLMs does not meaningfully increase the risk that an actor could plan and conduct a biological attack, compared to them simply having access to the internet. This is echoed by the United States' National Security Commission on Emerging Biotechnology, which concluded in January 2024 that \"At this time, LLMs do not significantly increase the risk of the creation of a bioweapon as LLMs do not provide new information [...] beyond what is already available on the internet\" (NSCEB, 2024). Additionally, access to biological information is only one part of the risk chain, and harms cannot materialize until biological artifacts are physically tested and released. This means that the locus of risk lies not only in access to information via an LLM, but across the biorisk chain. Currently available evidence does not yet offer detailed theoretical models or empirical analyses for how biological information access and planning meaningfully increases risk across the whole chain.\nTo strengthen collective understanding of this threat model and appropriately target policy"}, {"title": "Synthesis of harmful biological artifacts", "content": "Hypothesized source of amplified threat: Specialized AI \"biological tools\" assist malicious actors in identifying new toxins, designing more potent pathogens, or optimizing existing biological agents for increased virulence (Jeffery et al., 2023; Alstott, 2023; Halstead, 2024).\nThis threat model is explicitly focused on the ability of malicious actors to use specialized AI models trained specifically on biological data and intended for application in biosciences to facilitate the creation of harmful biological artifacts. Therefore, in this section we ask: do AI models designed for use in bioscience make it easier to design harmful biological artifacts? To do so, we first assess how specialized AI tools can be applied to biological sciences. Then we ask what empirical evidence there is that those tools could be applied to successfully augment the likelihood of a malicious actor carrying out a biological attack."}, {"title": "Relevant research", "content": "Often termed together as \u201cbiological tools\" (BTs) (Halstead, 2024; Rose et al., 2024), a range of AI models and systems are being developed using biological data to perform tasks that support biological research and engineering. The majority of biological sequence models are trained on protein (amino acid), DNA or RNA sequences. This is often for tasks such as:\n1. Protein structure prediction, to model protein structures, functions, and interactions to improve understanding of cellular functions and aid the design of potential therapeutics (Abramson et al., 2024; Baek et al., 2023)\n2. Protein Design, to model protein binders to aid protein engineering problems in relation to, for example, therapeutics, biosensors and enzymes (Dauparas et al., 2022)\n3. Gene Element Prediction, via foundation models that learn generalizable features from genome data, to predict how DNA changes affect an organism's fitness and design biological systems (Nguyen et al., 2023)\n4. Pathogenic Variant Prediction, by modeling of possible changes to amino acid chains to understand the affects for pathogenicity (Cheng et al., 2023)\n5. General-purpose biological sequence modeling, which generates possible proteins in response to natural-language prompts, to aid in biological programming (Hayes et al., 2024).\nCore to understanding the dual-use nature of these models lies in determining how transferable their biological use cases are to malicious settings. Applying AI models to biological problems is in its nascency: even in the case of AlphaFold (Jumper et al., 2021), several studies have found that it performs less well than experimental structures as targets for the computational docking algorithms used in drug design (Read et al., 2023; Terwilliger et al., 2023; Karelina et al., 2023). The most serious limitations arise because these models \"are based on learning patterns and know almost nothing about physics and chemistry\" and \"cannot consider factors such as pH, temperature or the binding of ions, other ligands or other proteins\" (Read et al., 2023). Several papers have pointed out the difficulties of translating AlphaFold predictions outside of simulations, concluding that \"despite the large recent gains in structure prediction accuracy, using predicted structures effectively for pharmaceutical applications remains a challenge\" (Tourlet et al., 2023; Terwilliger et al., 2023). The brittle and often clear shortcomings of current BTs illustrate that the danger as a dual-use tool is currently limited.\nWhat does this mean for predicting unknown toxins or proteins that underpin harmful biological artifacts? Designing novel diseases or bioweapons is considerably difficult (European Commission, 2005; National Academies Press, 2004). Skilled and technical expertise is required to utilize BTs effectively, such as cell culturing skills or experience working with genomes, in addition to hard-to-obtain or expensive material as well as organizational and staff resources (National Academies Press, 2004; Ouagrham-Gormley, 2014) (See Table 2). This suggests that these risks are primarily limited to state actors or sophisticated users with advanced knowledge in biology and machine learning, reducing both scale of risk and the \"attack surface\" that needs to be monitored (Rose et al., 2024; Ouagrham-Gormley, 2014).\nEven with state-sponsored expertise and resources, it is hard for experts to precisely target desirable characteristics in diseases such as contagiousness or stability, and when considering the necessary resources and skills to develop pathogens \"the importance of tacit knowledge is commonly overlooked\" (Lentzos et al., 2022). As researchers from the Soviet program to weaponize biological agents said: \"Everyone who has ever dealt with the genetics of bacteria knows how complicated it is to produce a new strain\" (Leitenberg et al., 2012). In this sense, and similar to the information access threat model, discussed above, risks associated with BTs still only manifest at the point of synthesis of biological compounds in a real-world laboratory - not just simulation in silico \u2013 and so further research is needed to clarify the theoretical model for how AI tools may reduce barriers to developing harmful biological artifacts with easy-to-access equipment or avoiding screening at laboratories and monitoring of biological materials (Jeffery et al., 2023; Halstead, 2024; Batalis, 2023; Terwilliger et al., 2023).\nAdditionally, the efficacy of BTs is limited by data availability, meaning that BTs are limited in their potential for harm where they do not have access to data about harmful artifacts"}, {"title": "Assessment", "content": "There are no known examples of current AI biological tools being misused to cause real-world harm, and complexities in the application of BTs to solve biological problems requires greater empirical research to understand how their misuse may manifest in real-world harms. This needed research includes whole-chain analyses that consider the risks associated with BTs in the context of biological artifact synthesis and deployment.\nTo strengthen collective understanding of this threat model, and appropriately target policy and risk mitigation measures, AI safety and governance researchers can expand empirical research and theoretical modeling for how BTs may interact across the entire lifecycle required to deploy a biorisk in the wild, rather than a sole focus on assessing BTs capabilities in the abstract. Studies must also report on important baselines such as how risk is amplified relative to access to existing tools, including the internet, and how a model trained only on data before a given cut-off would be able to perform on biology problems beyond that data."}, {"title": "Other potential threat models", "content": "In addition to these two threat modes associated with AI and biorisk, some assessments have theorized that AI technologies may also augment biorisk when applied in code-generating tools which could be used to target autonomous labs (potentially powered by AI-based agentic systems) and (mis)direct their operations (Jeffery et al., 2023; Halstead, 2024; Inagaki et al., 2023). To our knowledge, there is no publicly-available empirical or experimental research around this threat model.\nOther, analyses highlight that LLMs may considerably or significantly uplift malicious users' ability to develop harmful biological artifacts through improving laboratory experimentation and troubleshooting, and reducing barriers to biological work, for example by enabling technical coding for computational biology to be performed with non-technical, natural language (Rose et al., 2024). There are few empirical studies of this threat model, though one example is reported in OpenAI's ol system card which assesses model capabilities across a range of biological tasks, such as troubleshooting wet lab protocols and automating wet lab work. From these evaluations, OpenAI reports that ol-preview and o1-mini pose \u201climited risk,\" because the models do not provide biology experts with new knowledge, and \"do not enable non-experts to create biological threats, because creating such a threat requires hands-on laboratory skills that the models cannot replace\" (OpenAI, 2024c). With regards to AI tools assisting lab-based work, it is notable that existing risk frameworks focus on mitigating physical synthesis of threats, for example focusing on containment and control procedures that secure the handling and production of biological artifacts, rather than digital capabilities that support knowledge gathering, task automation, or planning (US Department of Health and Human Sciences, 2015).\nRelatedly, existing compute-based thresholds set, for example, in the White House Executive Order imply that the amount of compute used to train an AI model also contributes to"}, {"title": "Conclusion: the way forward for Al and biorisk", "content": "Given evidence to-date, are current threat models for how AI models could augment biorisk sound, and are methods to test those threat models robust? We have found that the available literature does not support the notion that access to biological information and planning via current, publicly available LLMs can significantly increase biorisk. While this does not offer conclusions for future models, more work is needed to develop this theoretical threat model before it can be considered viable or useful for assessing risks of AI models with additional capabilities. This includes accounting for how this threat model interacts with other stages throughout the entire the biorisk chain for example enabling the acquisition of materials to physically synthesize a harmful biological artifact as well as more clarity around the specific biological datasets or task-based training an LLM would need to demonstrate relevant capabilities. Current evidence suggests that LLMs lacking specialized biological data or capabilities and which cannot interact with other stages in the biorisk chain are unlikely to present risk.\nFor the second threat model considered in this paper, the synthesis of novel harmful artifacts through AI biological tools (BTs), we have found that current understanding of BTs' capabilities and available evidence suggests that BTs still underperform in unexpected ways at core and known tasks, and so present limited risk in terms of their ability to propose novel formulations of harmful biological artifacts such as pathogens or toxins. Additionally, barriers to the availability of and access to data needed to train BTs suggest development of capabilities will be irregular, hard to predict, and slower than in other domains. Furthermore, empirical research is lacking to understand how this risk model may interact with the entire biorisk chain and manifest into real-world harm.\nOther potential threat models, such as the application of AI in code-generating tools for autonomous labs and the improvement of laboratory experimentation through LLMs, have little available theoretical or empirical research. It is notable that much of the available evidence is drawn largely from computer science fields: while some research draws on biorisk expertise, there is little cross-citation between research papers produced by the relatively nascent AI safety field and the existing research and practices of international biorisk management.\nGiven this assessment, we offer the following considerations for AI safety and governance researchers working across industry, academia, and government:\n1. Focus on whole-chain risk analysis, considering how LLMs and BTs interact with the various complex stages of developing and deploying a harmful biological substance, including access to materials, specialized skills, laboratory facilities, etc, rather than solely focusing on assessments of AI models' biological capabilities.\n2. Direct attention towards AI models that are developed specifically for biological purposes, such as biological tools and LLMs trained or fine-tuned to show high-performance on tasks specifically relevant to stages in the biorisk chain, rather than all general purpose AI models.\n3. Target policy and risk mitigation measures towards establishing more precise and accurate threat models for how AI models uplift risk of physical harm, and developing robust empirical assessments for example with clear control variables and high-ecological validity, rather than relying on or mandating assessments that lack theoretical validity or methodological rigor.\nOverall, the findings of this paper should not be misinterpreted as a dismissal of the potential for AI models and systems to augment biorisk such dismissal would be irresponsible. However, our findings suggest that biological harms from AI models and systems remain a future risk rather than an immediate threat. It will be key to continually assess potential biorisks from new generations of advanced AI systems, particularly those trained on biological data. To better understand the level and nature of risk, these assessments must account for the above considerations. This will be crucial to supporting more precise and therefore effective and proportionate efforts by industry, government, and academia to assure AI safety in relation to biological applications."}]}