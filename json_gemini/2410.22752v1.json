{"title": "SoftCTRL: Soft conservative KL-control of Transformer Reinforcement Learning for Autonomous Driving", "authors": ["Minh Tri Huynh", "Duc Dung Nguyen"], "abstract": "In recent years, motion planning for urban self-driving cars (SDV) has become a popular problem due to its complex interaction of road components. To tackle this, many methods have relied on large-scale, human-sampled data processed through Imitation learning (IL). Although effective, IL alone cannot adequately handle safety and reliability concerns. Combining IL with Reinforcement learning (RL) by adding KL divergence between RL and IL policy to the RL loss can alleviate IL's weakness but suffer from over-conservation caused by covariate shift of IL. To address this limitation, we introduce a method that combines IL with RL using an implicit entropy-KL control that offers a simple way to reduce the over-conservation characteristic. In particular, we validate different challenging simulated urban scenarios from the unseen dataset, indicating that although IL can perform well in imitation tasks, our proposed method significantly improves robustness (over 17% reduction in failures) and generates human-like driving behavior.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous Driving Vehicles (AVs) have evolved rapidly in academic and industrial fields for decades. After the DARPA Grand Challenges in 2005-2007, many car manufacturers and startups rushed to develop the first level 4+ AVs. In reality, making a safe, smooth, and deployable system is a major challenge for autonomous vehicles (AVs), especially under complex urban driving scenarios where the drivers must handle surrounding items' behaviors (such as bother vehicles, cyclists, pedestrians, other movable objects, and traffic light signals) before acting. This encourages learning-based methods, which are becoming more and more popular. One of these methods is an imitative learning-based approach proposed that allows the performance to scale with the amount of data available [1]-[3]. However, imitation learning (IL) is simple but suffers from covariate shift caused by the accumulation of errors [4] and causal confusion [5].\nReinforcement learning (RL) has shown promising results in overcoming some limitations of traditional methods and IL by leveraging explicit reward functions to increase safety awareness [6]\u2013[8]. In addition, RL policies can also form causal relationships between states, actions, and outcomes by modeling sequential decision-making problems. Unfortunately, many modern RL algorithms heavily rely on reward design, an open challenge in AV.\nWith a human-designed reward design, driving policies using RL may be technically safe but unnatural. Many methods [9]\u2013[19] resolve this by combining IL and RL since they offer complementary strengths. RL enhances safety and robustness, especially in long-tail scenarios while IL improves realism and lessens the load on reward design. Therefore, combining IL and RL is crucial to enhance realism with large-scale human driving datasets and safety with causal relationship modeling.\nHowever, combining IL and RL is challenging. Utilizing IL (teacher) policy to constrain RL (student) policy may lead to the over-conservation problem caused by covariate shift of IL. This issue happens when IL trained with logged datasets guide RL students in online training process, where exploration is possible. Accumulated single-step errors can cause planned states to diverge from the dataset distribution, resulting in a compounding error issue. Therefore, this constraint force causes the RL student to be too conservative with the estimated expert policies to prevent the model from searching for a global solution.\nTo address these problems, we propose a Soft conservative KL-ConTrol of RL model (SoftCTRL) for autonomous driving, a novel approach for off-policy RL. First, we use a pretrained Transformer IL model to constrain RL updates with simple rewards. During the training step, the RL policy is implicitly regularized by the KL divergence from this prior model and the entropy of its actions. The entropy term encourages the model to generate more diverse trajectories. This combats the over-conservation issues when retaining the proximity to the distribution of the realistic IL model by KL control. Our experiments show that a simple reward combining imitation with collision is sufficient for our proposed method. Also, our new approach beat the pretrained IL model by a large margin in overall tasks, demonstrating that our approach is not affected by the covariate shift of IL.\nThe main contributions of our work are summarized as follows:\n\u2022 We proposed a novel off-policy RL method using implicit entropy-KL control to incorporate both IL and RL advantages. This approach substantially improves the learning speed, balances realism and safety in driving,"}, {"title": "II. RELATED WORKS", "content": "A. Learning-based approaches in autonomous driving\nMany data-driven SDV systems have recently developed, exploiting breakthroughs in deep learning methods. We briefly summarize the signatures of different learning-based algo-rithms for motion forecasting and planning in SDV. Imitation learning (IL) and inverse reinforcement learning (IRL) are scalable ML approaches that utilize large expert demonstration data. These methods aim to directly mimic expert behav-ior or recover the underlying expert costs. Early research on simple open-loop behavior cloning dates back to 1989 when the ALVINN system [1] employed a 3-layer neural network to perform lane keeping using front camera images. Although open-loop IL shows significant progress [2], [20]\u2013[23], it suffers from covariate shift issue [4] (which can be alleviated with perturbation [3] or closed-loop training [24]\u2013[27]). Furthermore, IL techniques do not inherently possess explicit knowledge of essential driving components, such as collision avoidance. Finally, a majority of human data consists of trivial scenarios, such as remaining stationary or driving in a straight line at a nearly constant speed; therefore IL becomes an unreliable and not robust method in complex and long-tail scenarios like urban driving. RL, on the other hand, can tackle this problem by leveraging the Markov decision process (MDP) model to learn the long-term consequence of its action. Moreover, RL allows the policy to learn from explicit and non-differentiable reward signals with closed-loop training and has been applied to specific tasks such as lane-keeping [6], intersection traversal [28], and lane changing [29] or general tasks like driving in urban area like [7].\nRL and other closed-loop methods require a simulator environment to train and validate. In recent years, many heuristic-based simulators CARLA [30] and SUMO [31] show a significant improvement in realism. However, there is a huge gap between the hand-coded simulator and real-world behaviors. Highway-env [32] and TORCS [33] are OpenAI Gym [34] compatible simplified, compact settings for au-tonomous driving but lack essential semantic aspects such as traffic lights, a detailed evaluation procedure, and expert data. Data-driven simulators are intended to address this issue by utilizing real-world traffic logs to simulate. VISTA [35] developed a photorealistic simulator for training a complete RL policy. [36] created a bird's-eye picture of congested freeway traffic. Recently, TrafficSim [37], SimNet [38], and DriverGym [7] powered by large-scale datasets [39], [40], created mid-representation closed-loop simulators and showed their utility in training and verifying ML planners. In our experiments, we train and evaluate our methods in closed-loop on real-world data with other agents following logs."}, {"title": "B. Encoding Map and Interaction", "content": "One popular class of map representation is rendering inputs as a multi-channel rasterized top-down image, which has been used by many visual-based approaches [3], [40]-[42]. This approach captures relationships between elements on a map via convolution neural networks (CNNs). Although CNN-based approaches show simplicity, they suffer from high rendering time and high-dimensional input space, which may cost a large network, training, and inference time. Moreover, rasterized images lose explicit relations between agent-agent and agent-map, which requires more data to recover those relationships [43]. More recently, a notably compact vector map representation [44] has captured a collection of static and dynamic elements around the ego, which can significantly reduce computational cost. VectorNet [44] tokenizes the lanes on the map and the historical trajectories of the agent as a set of polylines. The key idea is to create a global fully connected interaction graph that captures the relationships between scene elements, which are processed by a graph neural network (GNN). PlanT [22], wayformer [45], MTR [46], MTR++ [47] utilize transformer encoder layers to extract features from different vehicles and routes, modeling interpretable relations between entities to predict actions. Urban Driver [27] presents a PointNet-based version of VectorNet [44], as well as provides closed-loop training and evaluation. In this work, we use the Urban Driver [27] architecture as a Transformer backbone for downstream RL motion planning tasks."}, {"title": "C. Combining imitation and reinforcement learning", "content": "Combining RL and IL is, in fact, not new. Prior works attempted to combine IL and RL (IL+RL), by pre-training RL policies with demonstrations [9], incorporating demonstration data into off-policy RL algorithms [10], and combining IL and RL loss functions into a single algorithm [11], [19]. BC-SAC [19] conducted a combined IL and RL approach utilizing large amounts of real-world urban human driving data and a simple reward function to address challenges in autonomous driving. Also, DDPGfD [12], and DAPG [13] combine demonstrations with RL by including a margin loss which encourages the expert actions to have higher Q-values than all other actions. Offline RL approaches, such as TD3+BC [14] and CQL [15] combine RL objectives with IL ones to regularize Q-learning updates and avoid overestimating out-of-distribution values. RL with Kullback-Leibler (KL) regulariza-tion is a particularly successful approach. In KL-regularized RL, the standard RL objective is augmented by a KL term that penalizes the dissimilarity between the RL policy and a BC reference policy derived from expert demonstrations. Recent research has shown that KL-regularized objectives in offline RL, such as BRAC [16] and ABM [17], improve sample efficiency and solve challenging environments previously un-solved by standard deep RL algorithms. However, these algo-rithms struggle with online fine-tuning due to covariate shifts, making KL constraints overly conservative. To mitigate over-conservativeness, BC-SAC [19] introduces a weighted IL loss directly into the policy improvement step, without using pre-trained IL, allowing modeling the mixture of online RL and"}, {"title": "III. BACKGROUND AND NOTATIONS", "content": "A. Imitation Learning\nImitation learning optimizes a policy by imitating an expert using large human data. This method assumes our expert policy is optimal, denoted as $\\pi_e$, which generates a dataset of trajectories $D = \\{s_0, a_0, s_1, a_1, ...s_N, a_N\\}$ by interacting with the environment. The IL learner's goal is to train a policy $\\pi$ that imitates the expert. This problem is similar to supervised learning in that the input is expert states and the output label is extracted from expert action data. Behavioral cloning (BC) trains the policy to minimize the negative log-likelihood objectives of the expert dataset, the optimal parameters are:\n$\\theta^* = arg \\min_\\theta -E_{s,a \\sim D} [log(\\pi(a | s)]$\nTo alleviate covariate shift, ChauffeurNet [3] proposed a state perturbation during training BC that forces to learn how to recover from drifting, leading to learning robust driving poli-cies Alternatively, closed-loop approaches include adversarial IL (GAIL [25], MGAIL [26]), which aim to align the state-action visitation distribution between the policy and the expert through a discriminator. On the other hand, methods such as offline policy gradient with BPTT [27] use a differentiable simulator to compute policy gradients based on multi-step past trajectories directly. Unlike adversarial IL, which aligns distri-butions through an adversarial game, the offline policy gradient method updates the policy directly by backpropagating through time. Both approaches can help resolve the covariate shift issue that affects open-loop IL, though they use fundamentally different mechanisms."}, {"title": "B. Reinforcement learning", "content": "Reinforcement learning uses MDPs (Markov Decision Pro-cesses) as a formal way to define the interaction between a learning agent and its environment. Let $\\Delta_x$ be the set of probability distributions over a finite set X and $Y^X$ the set of applications from X to the set Y. A discounted MDP [48] contains a 5-tuple $(S, A, P, r, \\gamma)$, where S and A are the state and action spaces, $P \\in \\Delta_{S,A}^S$ are the transition function, $r \\in \\mathbb{R}^{S \\times A}$ is the reward function and $\\gamma$ is a discount factor. A policy $\\pi \\in \\Delta_S^A$ defines either an action (deterministic policy) or a distribution over actions (stochastic policy) given a state. We will also use $\\rho_{\\pi}(s_t)$ and $\\rho_{\\pi}(s_t, a_t)$ to denote the state and state-action marginals of the trajectory distribution induced by a policy $\\pi(\\cdot | s_t)$. The state-action value functions are defined as:\n$Q_{\\pi} = E_{\\pi}[\\sum_{t=0}^T \\gamma^t r(s_t, a_t) | s_0 = s, a_0 = a]$\nThe goal is to find the optimal policy $\\pi^*$, which results in the highest expected sum of discounted rewards. In standard MDP settings, a deterministic greedy policy satisfies:\n$\\pi^* = arg\\,max_{\\pi} E_{\\pi} [\\sum_{t=0}^T \\gamma^t r_t]$\nTo do so, q-learning methods (e.g. Deep Q-learning (DQN) [49]) learn to estimate state-action value function q approxi-mated by an Q-network $q_\\phi$, with weights copied regularly to a target network $\\bar{q_\\phi}$, through stochastic gradient descent (SGD) on the mean-square Bellman error (MSBE) loss $J_q(\\phi) = E_D [(q_\\phi(s_t, a_t) - \\hat{q}_{dqn}(r_t, s_{t+1}))^2]$, where the target $\\hat{q}_{dqn}$:\n$\\hat{q}_{dqn}(r_t, s_{t+1}) = r_t + \\gamma E_{s'\\sim p_\\pi(s,a)}[max_{a'} \\bar{q_\\phi}(s', a')]$\nMaximum entropy RL (e.g. Soft Q-learning (SQL) [50], Soft Actor Critic (SAC) [51]) is an entropy-regularized de-viation of the MDP approach in which the reward is aug-mented with an entropy term. The optimal policy aims to maximize both cumulative reward and its entropy $H(\\pi) = -E_{a\\sim\\pi}[log(\\pi(\\cdot | s_t))]$ at each visited state:\n$\\pi^*_{MaxEnt} = arg\\,max_{\\pi} E_{\\pi} [\\sum_{t=0}^T \\gamma^t r_t + \\tau H(\\pi)]$\n$= softmax(\\frac{1}{\\tau}q_{soft}(s_t, a_t))$\nwhere $\\tau$ is the temperature of the entropy term. Note that when $\\tau \\rightarrow 0$ then softmax becomes regular argmax mentioned in (3), and we retrieve DQN. Because entropy-regularized RL avoids taking a hard max over noisy estimated Q during training, it leads to less overestimation of future Q value and improves the stability of temporal-difference (TD) updates. In the q-value estimation step, SQL modifies the regression target as follows:\n$\\hat{q}_{MaxEnt}(r_t, s_{t+1}) = r_t + \\gamma E_{a'\\sim\\pi_{\\bar{\\theta}}} [\\bar{q_\\theta}(s_{t+1}, a') - \\tau log \\pi_{\\theta}(a' | s_{t+1})]$\nwith $\\pi_{\\bar{\\theta}} = softmax(\\frac{1}{\\tau} \\bar{q_\\theta})$. The $-log \\pi_{\\theta}(a'|s_{t+1})$ term is analogous to entropy regularization scaled by $\\tau$. To extend to SAC [51], we simply replace the discrete action a' with the squashed Gaussian distribution. SAC also uses function approximations for estimating policy (parameterized by $\\theta$) instead of q-value. The actor policy network $\\pi_{\\theta}$ outputs the mean $\\mu$ and deviation $\\sigma$ of each possible action which is parameterized as a Diagonal Gaussian distribution $N(\\mu, \\sigma^2)$. In the actor update, we assume that the q-value has been estimated. The policy acts to maximize the state value function $v_{\\pi_{\\theta}}$ by SGD: $J_\\pi(\\theta) = arg \\, max_{\\pi_{\\theta} \\in \\Delta_S^A} v_{\\pi_{\\theta}}$, where $v_{\\pi_{\\theta}}$ can be expanded out into:\nv_{\\pi_{\\theta}} = E_{a'\\sim\\pi_{\\theta}} [q_{\\phi} - \\tau log \\pi_{\\theta}(a' | s)]$"}, {"title": "IV. SOFT CONSERVATIVE KL-CONTROL FOR REINFORCEMENT LEARNING (SOFTCTRL)", "content": "A. Model Architecture\nOur RL model is shown in Fig. 1, which comprises 2 main parts: the backbone feature extractor network and the actor-critic network. We employ a Transformer architecture in [27]"}, {"title": "B. Soft Conservation with generalized Entropy-KL control", "content": "Our approach designs a soft conservative control for com-bining the IL-RL approach to incorporate IL and RL's com-plementary strengths and combat over-conservation problems. Conventional KL-regularized RL approaches minimize a KL term $D_{KL}(\\pi(\\cdot | s_t)||\\pi_0(\\cdot | s_t))$ while training, resulting in the over-conservation issue. Alternatively, entropy-based RL approaches that maximize the entropy of the policy distribution $H(\\pi(\\cdot | s_t))$ exhibit to mitigate the overestimation of the future estimated Q value, which is similar to the mentioned over-conservation problem. Intuitively, merging KL and entropy terms offers the advantage of softening the over-conservation tendency inherent in KL divergence loss. One simple approach is directly adding KL loss to the reward function of entropy-regularized RL algorithms but it is hard to control the trade-off between entropy and KL. In this work, we extend the entropy-regularized RL to a generalized entropy-KL RL using a reparameterization trick. Consequently, we maximize the following combined objective function during the RL training process:\n$arg \\, max_{\\pi} E [\\sum_{t=0}^T \\gamma^t r_t + \\tau H(\\pi(\\cdot | s_t)\n- \\alpha D_{KL} (\\pi(\\cdot | s_t)|| \\pi_0(\\cdot | s_t))$ \nInstead of directly optimizing entropy-regularized RL with an additional KL reward to achieve both entropy and KL regularization, we employ the idea of Munchausen RL [52], which implicitly maximizes its entropy and conservative up-date policy with KL divergence. To obtain implicit entropy-KL control, we simply modify the regression target of the policy evaluation step by adding the scaled log policy of reference behavioral method to the reward, which means replacing $r_t$ by $r_t+\\alpha \\tau ln\\pi_0(a_t | s_t)$ in any TD scheme. This approach assumes stochastic policy action implemented in entropy-regularized RL approaches. Compared to Munchausen RL, instead of using KL-divergence from the previous policies, we use that from a pretrained Transformer-based behavioral policy to force the RL agent close to the expert demonstrations. In principle, a variety of entropy-regularized RL methods could be extended to SoftCtrl. In this work, we build on SAC known as an actor-critic algorithm using the principle of maximum entropy RL, which performs competitively and stably in continuous tasks. In SAC-ImKL approach, the critic update in (6) has been modified as:\n$\\hat{q}_{SAC-ImKL}(r_t, s_{t+1}) = r_t + \\alpha \\tau ln\\pi_0(a_t | s_t)+\\gamma E_{a' \\sim \\pi_\\bar{\\theta}} [\\bar{q_\\theta}(s_{t+1}, a') - \\tau ln \\pi_{\\theta}(a'|s_{t+1})]$\nwhere $\\alpha \\in [0, 1]$ is a scaling factor. When the limit of $\\alpha \\rightarrow 0$ we retrieve maximum entropy RL. The actor update remains the same as in SAC (see in (7)). What happens underneath this modification is that it implicitly performs KL regularization between the learned policy and a reference policy. This can be accomplished by a reparameterization trick with $q = q_k - \\alpha \\tau ln \\pi_0$. With slight abuse of the notion, we write $\\pi_k = \\pi(\\cdot | s_t)$, $q_k = q(r_{t-1},s_t)$, and $\\Sigma_{\\pi}q = E_{a' \\sim \\pi}[q]$. In the policy evaluation step, the regression target can be rewritten as:\n$\\hat{q}_{k+1} = r + \\alpha \\tau ln \\pi_0 + \\gamma \\Sigma_{\\pi_{k+1}} (q_k - \\tau ln \\pi_{k+1})$\n$\\leftrightarrow \\hat{q}_{k+1} - \\alpha \\tau ln \\pi_0 = r + \\gamma \\Sigma_{\\pi_{k+1}} (q_k\n- \\alpha \\tau ln \\pi_0\n\u2212(1\u2212\\alpha )\\tau ln \\pi_{k+1}$\n$\\hat{q}_{k+1} = r + \\Sigma_{\\pi_{k+1}}q_k + (1 - \\alpha)\\tau H(\\pi_{k+1})\n-\\alpha \\tau D_{KL}(\\pi_{k+1}||\\pi_0)$\nThen, we also rewrite the policy improvement step $\\pi_{k+1} = arg \\,max_{\\pi \\in \\Delta_S^A} \\Sigma_{\\pi}q + \\tau H(\\pi)$ as:\n$\\Sigma_{\\pi}q + \\tau H(\\pi)$\n$= \\Sigma_{\\pi}(q_k + \\alpha \\tau ln \\pi_0) - \\tau \\Sigma_{\\pi}ln\\pi$\n$= \\Sigma_{\\pi}q_k - \\alpha \\tau \\Sigma_{\\pi}(ln \\frac{\\pi}{\\pi_0}) - (1 - \\alpha)\\tau \\Sigma_{\\pi}ln\\pi$\n$= \\Sigma_{\\pi}q_k - \\alpha \\tau D_{KL}(\\pi||\\pi_0) + (1 - \\alpha)\\tau H(\\pi)$"}, {"title": "V. EXPERIMENTS", "content": "A. Experimental Setup\n1) Datasets: We use a dataset consisting of around 100 hours of expert driving trajectories, split into 25-second seg-ments. All logs were collected by a fleet of 20 self-driving vehicles driving along a fixed route in Palo Alto, California [39]. The training data comprises 16K scenes where the initial frame is randomly sampled. We test the performance of models on 100 unseen segments. The state representation captures a collection of static and dynamic elements around the ego. In particular, static elements include traffic lanes, stop signs, and pedestrian crossings, which are retrieved from the localization system. The dynamic elements including traffic light status and surrounding agents (other cars, buses, pedestrians, and cyclists) are detected in real time using the onboard perception system. Regarding to ego state, we utilize vectorized form derived directly from HD maps elements which contain agent dynamics and structured scene context. Each feature of a vector map can be represented as multiple points, polygons, or curves. The input and output of the model we used are similar to [27], which utilizes past and current states of all elements including ego car to predict the future states of the ego. Therefore, no navigation information that guides the vehicle toward the destination is given.\n2) Simulation: The simulator is based on the OpenAI Gym framework and uses a physics-based simulation engine to model the behavior of an ego vehicle in various driving scenarios. As mentioned in section IV-C, the simulator uses the Unicycle Kinematic Model to simulate the SDV ego's motion. On the other hand, the trajectories of other agents and pedestrians in the scene are replayed from the logs (log-playback), similarly to [7], [19]. Although this implies that the agents are non-reactive, it guarantees that the actions of other agents resemble human behavior. The simulation also allow new agents to enter ego-centered frames, resulting in more complicated scenarios. The average number of agents per frame in the train and test set is 79 and 76, respectively.\n3) Model architecture: For our urban driving scenarios, we discovered that SAC-ImKL with 2 separate feature extractor networks between actor and critic net yielded superior overall results. This separation allows each network to focus on dif-ferent aspects of the input data, thus leading to more accurate representations for each network. To boost RL training, we initialized backbones weights with pretrained perturbation BC model [3]."}, {"title": "B. Baselines", "content": "We compare our proposed algorithms against these ap-proaches."}, {"title": "C. Metrics", "content": "In this work, we use a large-scale real-world dataset with closed-loop simulation to evaluate our SDV planning model performance. For each segment in the test set, we simulated the SDV by its learned policy for the full duration of a segment and detected continuous or binary events including expert imitation, safety, and comfort of ego car, which are detailed as follows:\n\u2022 Average Displacement Error (ADE): Computes the L2 distance between predicted ego centroid and ground-truth ego centroid averaged over the entire episode.\n\u2022 Distance To Reference (D2R): Computes the L2 dis-tance between the predicted centroid and the closest waypoint in the reference trajectory (ground-truth ego). We raise an off-road failure event if the simulated SDV deviates from the reference trajectory center line by greater than 4m."}, {"title": "D. Results", "content": "Following the above derivations, we first conduct a compre-hensive comparison between our SAC-ImKL and 4 baselines (BC-perturb, BPTT, PPO, SAC, SAC-ExKL). Table II reports the performance of the urban driving rules to which the models are trained to adhere, for example, keeping progress (ADE), avoiding collisions and off-route events (Failure), and driving smoothly like humans (Discomfort). Each configuration uses a mean and standard deviation derived from 3 random seeds. Many hyperparameters could be tuned to further improve the metrics; however, we found that $\\sigma_{steer} = \\sigma_{accel} = exp(-1.5)$, $\\alpha = 0.3$ for SAC-EXKL, $\\tau = 1.2$, $\\alpha = 0.4$ for SAC-ImKL produce satisfied planners.\n1) Comparing regularized RL with IL-only and RL-only: In Table II, BC-perturb and BPTT outperform RL in imitation tasks since 1) IL directly mimics human driving behavior by supervised learning, which is easier than RL letting the agent explore and gives a reward whenever the agent drives properly and 2) in multiple possible outcomes scenarios like the T-junction or crossroad, RL can deviate from logs since it allows the agent to explore more options. However, the two IL methods yield higher average collision $\\mu_{CL}$ than off-policy RLs"}, {"title": "VI. CONCLUSION", "content": "This study shows that entropy regularization is a crucial consideration to avoid over-conservation for the RL fine-tuning process. Besides, KL-regularized RL significantly improves learning speed, human-like driving patterns, and passenger comfort. The balanced trade-off between entropy and KL-divergence shows a large improvement in safety and real-ism performance over baselines. Our extensive experiments examined the roles of entropy/ KL trade-off terms, RL in transformer models, and vehicle kinematic models. Along with these improvements, our work does not solve for unexpected behavior of other agents and high imitation error. Also, since our method uses IL prediction on RL data to guide the RL agent, we require a highly robust IL model (perturbation BC [3]) to alleviate covariate shift. Moreover, it still depends on heuristically choosing the trade-off value between entropy and KL regularization. Some considerable approaches employ offline RL taking advantage of a large amount of data available to the driver, without additional online data collection. It is perhaps treated as a pretrained approach instead of BC to mitigate distribution shifts and excessively conservative issues."}, {"title": "APPENDIX", "content": "A. Additional Details on Model Architectures and Hyper-parameters Settings\nIn the training stage, we employ a common RL fine-tuning technique where the agent is first trained in a supervised manner, and then we fine-tune using the RL optimizer. We leverage the pretrained Transformer-based behavioral cloning model in paper [27] to bootstrap our RL agent. Regarding the Transformer backbone, first, the vectorized form of the HD map extracted from the simulator is fed into a set of local encoders. The local encoder distills all points belonging to the same object into a same-size dimensional feature. These features are followed by a layer of Multi-head self-attention layer which learns the global interaction feature. The resulting features go through RL networks to predict a policy distribution and a value function. Finally, action is sampled from the policy distribution to interact with the environment and update the new state."}]}