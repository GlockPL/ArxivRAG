{"title": "Enhancing Perception Capabilities of Multimodal LLMs with Training-free Fusion", "authors": ["Zhuokun Chen", "Jinwu Hu", "Zeshuai Deng", "Yufeng Wang", "Bohan Zhuang", "Mingkui Tan"], "abstract": "Multimodal LLMs (MLLMs) equip language models with visual capabilities by aligning vision encoders with language models. Existing methods to enhance the visual perception of MLLMs often involve designing more powerful vision encoders, which requires exploring a vast design space and re-aligning each potential encoder with the language model, resulting in prohibitively high training costs. In this paper, we introduce VisionFuse, a novel integration framework that efficiently utilizes multiple vision encoders from off-the-shelf MLLMs to enhance visual perception without requiring additional training. Our approach is motivated by the observation that different MLLMs tend to focus on distinct regions given the same query and image. Moreover, we find that the feature distributions of vision encoders within an MLLM family, a group of MLLMs sharing the same pretrained LLM, are highly aligned. Building on these insights, VisionFuse enriches the visual context by concatenating the tokens generated by the vision encoders of selected MLLMs within a family. By merging the parameters of language models from these MLLMs, VisionFuse allows a single language model to align with various vision encoders, significantly reducing deployment overhead. We conduct comprehensive evaluations across multiple multimodal benchmarks using various MLLM combinations, demonstrating substantial improvements in multimodal tasks. Notably, when integrating MiniGemini-8B and SLIME-8B, VisionFuse achieves an average performance increase of over 4%.", "sections": [{"title": "1. Introduction", "content": "Multimodal LLMs (MLLMs) integrate vision encoders to Large Language Models (LLMs), allowing them to tackle multimodal tasks with emergent capabilities [29, 34, 49]. To handle complex and diverse multimodal tasks effectively, MLLMs require strong visual perception capabilities. A common approach to improving the visual perception of MLLMs is designing better vision encoders [5, 40, 54]. However, these methods typically involve switching vision"}, {"title": "2. Related Work", "content": "Enhancing the visual perception of MLLMs. Existing multimodal large language models (MLLMs) primarily enhance their visual perception by incorporating high-resolution inputs [27], employing optimized preprocessing methods to capture richer visual features [33, 54], and designing more effective vision modules [5, 14, 54]. Specifically, LLaVA-Next [33] segments input images into local patches and uses high-quality data to train the MLLM. Mini-Gemini [27] uses CLIP [39] tokens as low-resolution queries to cross-attend to another high-resolution vision encoder within co-located local windows. Honeybee [5] introduces a locality-enhanced projector that balances token management flexibility with local visual context preservation, improving both efficiency and performance in spatial understanding tasks. ConvLLaVA [14] leverages a hierarchical ConvNeXt backbone to compress high-resolution images into fewer visual tokens, enhancing efficiency while maintaining spatial understanding across diverse image resolutions. SliME [54] refines visual adapters by employing a mixture of experts for global features and compressing local image tokens with query embeddings, improving both efficiency and performance in high-resolution tasks. Eagle [40] explores the design space of multimodal LLMs by integrating multiple vision encoders with different architectures and pretraining tasks, enhancing multimodal performance through efficient fusion strategies like direct token concatenation. However, these approaches require extensive fine-tuning to align the language model with the vision modules, leading to significant computational costs. Similar to existing methods [40], our approach also concatenates features from multiple visual encoders to enhance the visual perception capabilities of the model. In contrast, our method efficiently improves the visual perception abilities of multimodal large language models without requiring extensive additional training.\nModel Merging. Model merging seeks to consolidate multiple parameter sets into a single model without requiring"}, {"title": "3. Empircal Insights", "content": "In this section, we provide comprehensive visualizations and discussions from the following perspectives to introduce three novel insights. The experiments are conducted using the following models: SLIME-7B [54] and MGM-7B [27], both based on Vicuna-v1.5 [8], as well as SLIME-8B [54] and MGM-8B [27], which are based on LLaMA-3-8B-Instruct [1]. We evaluate these models on the following datasets: TextVQA [42], MME [13], and VQAv2 [15].\nObservation 1: Different MLLMs attend to different regions for the same query and visual input. To investi-"}, {"title": "4. Methodology", "content": "Inspired by our observations, we propose VisionFuse, a simple yet effective approach for efficiently integrating different MLLMs to enhance visual perception. As illustrated in Figure 3 and Algorithm 1, VisionFuse first merges the language models and then utilizes various vision encoders to extract richer features for the input image, which are subsequently fed into the merged LLM."}, {"title": "4.1. Preliminaries", "content": "Notation: Let $f(x; \\Theta)$ represent the language model of an MLLM, where $\\Theta$ denotes its parameters. The input $x$ consists of a sequence of tokens, including vision tokens $V$ from the modality-specific encoder and text tokens $T$ from word embeddings. The parameters of the pre-trained language model are denoted as $\\Theta_{\\text{pre}}$.\nDelta parameters. Delta parameters represent the changes in model parameters during fine-tuning [35]. In MLLMs, the delta parameters for the language model during multimodal fine-tuning are expressed as $\\Delta_{\\Theta} = \\Theta_{\\text{fin}} - \\Theta_{\\text{pre}}$"}, {"title": "4.2. Ensemble of Different Vision Encoders", "content": "To leverage the visual perception capabilities of different MLLMs, an intuitive approach is to combine multiple MLLMs into an ensemble and aggregate their predictions. However, due to the large number of parameters in the language models, directly using an ensemble of the entire MLLMs would result in significant computational overhead. Therefore, we explore whether it is feasible to integrate only the vision encoders, which have relatively fewer parameters, and feed the output tokens into the language model for inference.\nIn pre-trained MLLMs, the alignment between textual and visual inputs in the feature space allows them to be treated as a unified sequence for input into the language model. As discussed in Observation 2, visual features from MLLMs trained on the same language model are more closely aligned in the feature space. Therefore, we directly aggregate the outputs of different vision encoders within an MLLM family, treating them as distinct visual context information, which enables the language model to obtain richer visual perception. Due to the varying lengths of vision tokens from different MLLMs, we propose directly concatenating the vision tokens from these MLLMs. The process of combining multiple vision encoders into an en-"}, {"title": "4.3. Merging LLMs from a Family of MLLMS", "content": "A single language model cannot directly align with multiple encoders from different MLLMs, as they have not undergone alignment training. Retraining for such alignment would result in substantial computational costs. As discussed in Observation 3, merging language model parameters within an MLLM family helps align a language model with different vision encoders. Therefore, following the approach in [19], we merge these delta parameters to create a single language model capable of interpreting visual tokens from multiple vision encoders. The merging process can be formalized as follows:\n$\\Theta_{merged} = \\Theta_{pre} + \\lambda \\cdot \\sum_{i=1}^M (\\Theta_i - \\Theta_{pre}),$\nwhere $\\Theta_{pre}$ represents the parameters of the shared base model, and $\\lambda$ is a merging coefficient. The prediction $\\hat{y}$ can then be generated as follows:\n$\\hat{y} = f(V_F, T; \\Theta_{merged}).$"}, {"title": "5. Experiments", "content": "Implementation details. We conduct experiments across various MLLM combinations, including (1) SLIME-7B [54] and MGM-7B [27] based on Vicuna-v1.5 [8], and (2) SliME-8B [54] and MGM-8B [27] based on Llama-3-8B-Instruct [1]. We evaluate the performance of our approach on multiple multimodal datasets, including VQAT (TextVQA) [42], MMB (MM-Bench) [36], MMBC(MMBench-Chinese) [36], \u039c\u039c\u0395 [13], MMMU [52], VQAv2 [16] and Vizwiz [17].\nCompared methods. We compare our method with the baselines and existing leading MLLMs, including MobileVLM [10], Qwen-VL [3], Qwen-VL-Chat [3], IDEFICS [24], LLaMA-VID [26], LLaVA-1.5 [32], VILA [29], Shika [6] and InstructBLIP [11].\nMain results. We evaluate our method across several multimodal datasets and compare it against the leading MLLMs, as detailed in Table 1. Without any additional training, our approach significantly enhances performance over individual models by simply integrating vision encoders from the same MLLM family in both the the combinations based on Vicuna-7B and LLaMA-3-8B-Instruct. Notably, in the integration of MGM-8B and SLIME-8B, VisionFuse incurs only a 3.4% increase in parameters due to the additional encoders employed, achieving a 4% relative improvement compared to the optimal individual model. Furthermore, the performance is on par with that of the MGM-8x7B model, which contains over six times more parameters, highlighting the parameter efficiency of VisionFuse. The results of integrating more MLLMs are included in the appendix.\nEffectiveness on different resolutions inputs. To further assess the effectiveness of our method with varying image resolutions, we combine the high-resolution vision encoder from Mini-Gemini-HD-8B with the low-resolution vision encoder from SLIME-8B. As demonstrated in Table 1, even the low-resolution vision encoder can mitigate some of the limitations of the high-resolution encoder by providing richer visual information to the language model. The performance after integration is higher than that of either individual model. This suggests that even models utilizing high-resolution inputs may overlook critical regions, whereas the low-resolution vision encoder can help address these gaps in visual perception.\nEffectiveness of each component. To assess the effectiveness of each component of our method, we perform ablation studies on several multimodal datasets, as shown in Table 2. The results demonstrate that without merging the delta parameters, the language model fails to align with the vision"}, {"title": "6. Conclusion", "content": "This paper investigates the differences in visual perception capabilities among various MLLMs and proposes a new paradigm for efficiently enhancing the perceptual abilities of MLLMs based on some novel empirical insights. The approach offers a simple yet effective MLLM integration strategy that requires no additional training for re-aligning the vision encoders and LLM, leveraging the distinct visual perception strengths of different MLLMs to improve performance on multimodal tasks. Overall, VisionFuse significantly enhances the perceptual abilities of individual MLLMs with minimal additional deployment overhead.\nLimitations and future work. Our current analysis focuses on two MLLMs. However, when attempting to integrate more MLLMs, a direct concatenation of visual sequences results in excessively long visual token sequences, causing discrepancies that the sequence length is much longer than that in the training phase and a subsequent decline in performance. Detailed results are provided in appendix. Future work will explore methods for efficiently incorporating additional MLLMs while reducing the length of visual tokens, such as employing token merging strategies across different MLLMs or utilizing rapid fine-tuning techniques to adapt to"}, {"title": "A. Details of Experimental Settings", "content": "A.1. Details of Evaluation using GPT\nFor each example, we include both the image and the outputs from multiple models with the same prompt. The images are provided as URLs in the GPT-40 API. GPT-40 then returns a JSON object containing the scores for all models. The prompt is as follows:\nNext, I will provide you with\ndescriptions of an image generated\nby multiple models. Please evaluate\nthese descriptions based on the\nlevel of detail and accuracy, and\nassign a score ranging from 1 to 10.\nFinally, your output only contains\na JSON object, where each item is\nthe model name and its corresponding\nscore.\nmodel A:\nmodel B:\nmodel C:"}, {"title": "B Additional Related Work", "content": "Multimodal Large Language Models. MLLMs integrate vision encoders into large language models, enabling them to handle multimodal tasks. Early models such as Flamingo [2] encode images and feed them into the attention layers of the language model, while Blip-2 [25] employs Q-Former to encode images into features, which are then input into the language model. Subsequent works [31, 34, 38, 53, 55, 56] enhance the multimodal understanding capabilities of language models through instruction fine-tuning on multimodal datasets. Further research has focused on optimizing encoder designs, extracting richer visual information, and expanding the models to handle additional modalities. For example, Eagle [40], Mini-Gemini [27], SLIME [54], and LLaVA-next [33] introduce additional vision encoders and employ preprocessing techniques such as cropping and interpolation to handle longer visual input sequences, thereby enriching the visual information available to the language model. Honeybee [5] introduces a locality-enhanced visual projector to better bridge pre-trained vision encoders with large language models. Recent works [18, 28, 46] also explore enabling language models to understand a wider range of modalities."}, {"title": "D Discussions and Limitations", "content": "D.1. Impact of the Token Sequence Length\nAlthough the proposed VisionFuse enhances visual perception capabilities by effectively integrating the vision encoders of different models through the concatenation of visual tokens, excessively long token sequences can introduce challenges. In Section D, we discuss this issue in detail. In Section D.2, we further discuss the underlying solution to alleviate this limitation.\nIn this part, we investigate the impact of the token se-"}]}