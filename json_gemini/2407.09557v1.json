{"title": "Deep Reinforcement Learning Strategies in Finance: Insights into Asset Holding, Trading Behavior, and Purchase Diversity", "authors": ["Alireza Mohammadshafie", "Akram Mirzaeinia", "Amir Mirzaeinia", "Haseebullah Jumakhan"], "abstract": "Recent deep reinforcement learning (DRL) methods in finance show promising outcomes. However, there is limited research examining the behavior of these DRL algorithms. This paper aims to investigate their tendencies towards holding or trading financial assets as well as purchase diversity. By analyzing their trading behaviors, we provide insights into the decision-making processes of DRL models in finance applications. Our findings reveal that each DRL algorithm exhibits unique trading patterns and strategies, with A2C emerging as the top performer in terms of cumulative rewards. While PPO and SAC engage in significant trades with a limited number of stocks, DDPG and TD3 adopt a more balanced approach. Furthermore, SAC and PPO tend to hold positions for shorter durations, whereas DDPG, A2C, and TD3 display a propensity to remain stationary for extended periods.", "sections": [{"title": "1. Introduction", "content": "Artificial intelligence (AI), particularly in the domain of machine learning (ML), has been progressively revolutionizing enterprises through its ability to absorb information and adapt based on inputs. One prominent sub-field within machine learning (ML), reinforcement learning (RL), has garnered acclaim for its capability to enable agents to develop optimal decision-making strategies via interaction with their surroundings. Reinforcement learning algorithms, inspired by behavioral psychology, gain knowledge through a process of experimenting and receiving feedback in the form of rewards or penalties based on their actions. The iterative nature of this approach allows reinforcement learning agents to gradually improve their decision-making abilities and achieve optimal outcomes. The objective of this paper is to investigate the behavior and decision-making processes of DRL algorithms in financial trading, with a focus on their tendencies towards holding or trading assets and their purchase diversity.\nAn improved form of reinforcement learning (RL) called deep reinforcement learning (DRL) makes use of deep neural networks to analyze large-scale input spaces efficiently. DRL algorithms utilize the integration of deep learning techniques and RL to efficiently tackle complex real-world problems that were previously unachievable using traditional RL methods. One noteworthy characteristic of DRL algorithms is their ability to autonomously gain knowledge from unprocessed sensory input, including sensor data and photographs, without the need for domain-specific expertise or pre-designed features.\nClassifying an algorithm as DRL relies on various crucial variables. Firstly, it must employ deep neural networks as function approximators to accurately represent the value functions or policies. Additionally, it should utilize reinforcement learning (RL) approaches, such as value iteration or policy gradients, to optimize the behavior of the agent by incorporating feedback from the environment. Finally, it must exhibit the capacity to acquire knowledge directly from unprocessed sensory information, allowing it to make generalizations across a wide range of complex and multi-dimensional facts.\nThe convergence of DRL (Deep Reinforcement Learning) and finance presents a significant opportunity to transform the process of making trading and investing choices. The complexity, volatility, and uncertainty of financial markets make them a perfect environment for the application of Deep Reinforcement Learning (DRL) algorithms. These algorithms have the capacity to reveal concealed patterns, take advantage of inefficiencies, and optimize trading strategies in real-time, thus improving market efficiency and maximizing returns for investors.\nWithin the domain of finance, decision-making is centered on a variety of tactics, spanning from long-term investment to short-term trading. To successfully implement any strategy, one must possess a sophisticated comprehension of market dynamics, skillful risk management, and the capability to adjust to evolving circumstances. Deep reinforcement learning (DRL) algorithms, similar to human traders, have the ability to determine whether to retain assets for an extended period or engage in frequent trading in the short term. Furthermore, their ability to navigate complex financial data in multiple dimensions allows them to discover detailed patterns and connections that human traders may not be able to find.\nAdditionally, spreading investment capital across different assets or securities are known to be a common approach to reduce the risk. Instead of putting all funds into a single asset or market, traders diversify their portfolios by investing in a variety of assets, such as stocks, bonds, commodities, and currencies, as well as across different sectors and geographical regions. The primary goal of trading diversification is to minimize the impact of adverse events or fluctuations in any single asset or market on the overall portfolio performance. By holding a diversified portfolio, traders aim to achieve a balance between risk and return, potentially improving the risk-adjusted returns of their investments. Diversification can take several forms such as asset allocation, sector diversification, geographical diversification, instrument diversification, time diversification. While diversification can help mitigate risk, it is important to note that it does not eliminate all risk, especially systemic risks that affect entire markets or economies. Additionally, over-diversification can dilute potential returns, so finding the right balance between diversification and concentration is crucial for portfolio management. In this paper, we additionally examine the behaviour of DRL-trained agents to diversify their trading strategies.\nPrevious research has investigated the application of various DRL algorithms in finance, focusing on their performance and potential to outperform traditional trading strategies employed by human actors [1], [2].This article aims to investigate the performance of different Deep Reinforcement Learning (DRL) algorithms within the field of finance. Through analyzing their performance in diverse trading and investment situations, our objective is to acquire knowledge about their capabilities, constraints, and appropriateness for different financial uses. In addition, we analyze how these DRL algorithms make the crucial decision of whether to hold or trade assets as well as asset diversification, providing insight into their decision-making processes and the consequences for financial decision-makers. By conducting this investigation, our objective is to enhance comprehension of the impact of DRL on the evolution of the financial sector."}, {"title": "2. Methodology", "content": ""}, {"title": "2.1. Data Source", "content": "Yahoo finance is one of the top available financial data sources and a comprehensive financial platform that offers a wide range of tools, resources, and information for investors, traders, and financial professionals. Serving as one of the leading financial portals, Yahoo Finance provides users with access to real-time market data, news, analysis, research tools, and investment resources. Yahoo finance offers some key features and offerings as following.\nMarket Data: Yahoo Finance provides users with access to real-time and historical data on stocks, bonds, mutual funds, exchange-traded funds (ETFs), indices, currencies, commodities, and other financial instruments. Users can track prices, view charts, and analyze performance metrics for individual securities or market indices.\nNews and Analysis: Yahoo Finance offers a comprehensive selection of financial news articles, analysis, commentary, and insights from leading financial journalists and contributors. Users can stay informed about market trends, economic developments, corporate earnings, and other relevant news impacting the financial markets. Portfolio Management: Yahoo Finance allows users to create and manage personalized investment portfolios, track holdings, monitor performance, and analyze portfolio metrics. Users can input their investment transactions, view portfolio allocation, and assess risk exposure to make informed investment decisions.\nScreeners and Research Tools: Yahoo Finance offers powerful screening tools and research resources for identifying investment opportunities based on specific criteria, such as market capitalization, sector, industry, valuation metrics, and performance indicators. Users can filter stocks, ETFs, and mutual funds based on their investment preferences and objectives. Educational Resources: Yahoo Finance provides educational articles, tutorials, videos, and guides to help users enhance their financial literacy, understand investment concepts, and improve their investment strategies. Topics cover a wide range of subjects, including investing basics, portfolio management, risk management, and market analysis.\nCommunity and Social Features: Yahoo Finance fosters a vibrant community of investors and traders through its discussion forums, message boards, and social networking features. Users can engage with other members, share investment ideas, discuss market trends, and exchange insights and opinions.\nOverall, Yahoo Finance serves as a comprehensive and user-friendly platform for individuals seeking to stay informed about the financial markets, manage their investments, conduct research, and engage with a community of like-minded investors. Its wide range of features and resources make it a valuable tool for investors of all levels of expertise.\nThe financial data used in this analysis was obtained from Yahoo Finance. The data covers hourly intervals from March 4, 2022, to March 1, 2024. The dataset was partitioned into a training period spanning from March 4, 2022, to December 1, 2023, and a testing period spanning from December 1, 2023, to March 1, 2024. This dataset contains price data for each of the thirty companies featured in the Dow Jones Industrial Average listed in table 1. The data includes the opening, low, high, and closing values, and is recorded on an hourly basis."}, {"title": "2.2. Utilized Indicators", "content": "To guide our trading decisions, we employed an extensive range of technical indicators generated from the financial data listed in table 2. The first indicators used is the Volatility Index (VIX) which is a measure of market volatility and investor sentiment in the stock market. The VIX is calculated by the Chicago Board Options Exchange (CBOE) based on the prices of options on the S&P 500 index. Options are financial derivatives that give investors the right, but not the obligation, to buy or sell an underlying asset (in this case, the S&P 500 index) at a predetermined price (strike price) within a specific period of time.\nThe second indicator is Moving Average Convergence Divergence (MACD), which is s a popular technical indicator used in financial analysis to identify changes in the strength, direction, momentum, and duration of a trend in a security's price.\nBollinger Bands are a popular technical analysis tool used by traders to analyze price volatility and potential price breakouts in financial markets. They consist of three lines of upper (BOLL_UB), middle and lower band (BOLL_LB).\nRelative Strength Index (RSI_30) which is, is a momentum oscillator that measures the speed and change of price movements in financial markets. It is a popular technical analysis tool used by traders and investors to identify overbought or oversold conditions in a security's price and to gauge the strength of a trend. The RSI is calculated based on the following formula:\n$RSI = 100 - \\frac{100}{1 + \\frac{AverageGain}{Average Loss}}$\nwhere average gain is the sum of gains over the specified period over Number of periods and average loss is the sum of losses over the specified period over Number of periods. The RSI typically uses a 14-period timeframe for its calculations, but traders may adjust this parameter based on their preferences and the characteristics of the security being analyzed.\nThe RSI ranges from 0 to 100 and is plotted as a line graph. The RSI value fluctuates between these two extremes, with readings above 70 typically considered overbought and readings below 30 considered oversold. Overbought conditions may suggest that the security is due for a potential reversal or correction, while oversold conditions may indicate a possible buying opportunity.\nCommodity Channel Index (CCI_30), is a versatile technical analysis indicator used to identify potential trends, overbought or oversold conditions, and price reversals in financial markets, particularly in commodities trading. It is developed by Donald Lambert in the late 1970 and it measures the relationship between an asset's current price, its moving average, and its typical price range. CCI can be calculated as following formula\n$CCI = \\frac{Typical Price \u2013 SMA}{0.015 * Mean Deviation}$\nwhere typical price of an asset is the average of the high, low, and closing prices over a specified period. Simple Moving Average (SMA) calculate the n-period simple moving average of the typical price, where n is the number of periods used for the CCI calculation. Additionally, mean deviation calculate the mean deviation of each typical price from the SMA over the specified period. The mean deviation is the average of the absolute differences between each typical price and the SMA.\nDirectional Movement Index (DX_30) is a technical indicator used to assess the strength and direction of a trend in financial markets. Developed by J. Welles Wilder Jr., the DMI consists of three lines: the Positive Directional Indicator (+DI) which measures the strength of upward price movement, the Negative Directional Indicator (-DI) which measures the strength of downward price movement, and the Average Directional Index (ADX), which measures the overall strength of the trend, regardless of its direction. \u00b1DI and DX can be calculated in following equations.\n$+DI = \\frac{Smoothed + DM}{ATR} \u00d7 100$\n$-DI = \\frac{Smoothed - DM}{ATR} \u00d7 100$\n$DX = \\frac{|+DI --DI|}{|+ DI+-DI|} \u00d7 100$\nwhere +DM is current high minue previous high. -DM is previous low minus current low and moothed +/-DM = $\\frac{1}{14} (\\sum_{t=1}^{14} DM - (DM)_t + CDM)$. CDM is current DM and ATR is average true range.\nWe also have 30-period and 60-period Simple Moving Averages (SMA) which is average price of a security over a specified number of periods (30 and 60 hours). It is a lagging indicator, meaning it is based on past prices and does not predict future price movements. $SMA_{30}$ and $SMA_{60}$ can be calculated in following equations 6 and 7.\n$SMA_{30} = \\frac{P_{i-1}+P_{i-2}+ ... + P_{i-30}}{30}$\n$SMA_{60} = \\frac{P_{i-1}+P_{i-2}+ ... + P_{i-60}}{60}$\nwhere $P_{i+k}$ in these equations is the price of an asset at period k. In addition, we integrated a turbulence indicator to measure market volatility and uncertainty."}, {"title": "2.3. Environment:", "content": "Our experimentation is conducted using the FinRL environment [3], which is a dedicated framework designed exclusively for the use of reinforcement learning (RL) in financial markets. FinRL provides a comprehensive set of tools specifically designed to address the distinct difficulties presented by financial time series data, including non-stationarity and high dimensionality. The platform offers a versatile and expandable setting for training reinforcement learning agents to make trading decisions across diverse market situations."}, {"title": "2.4. Hyperparameters", "content": "We adjusted the hyperparameters of our reinforcement learning models to maximize their performance on the specified task. The crucial hyperparameters consist of the number of time steps, which is set at 100,000, and it defines the length of each training session. By using this time step option, each agent goes through 33 complete episodes of training. At the onset of every episode, the agent commences with an initial capital of 1,000,000. The status space is composed of 301 dimensions (table 3), which include a range of market indicators. The action space consists of three options: selling, buying, or holding stocks."}, {"title": "2.5. Models Utilized", "content": "We utilized a varied range of reinforcement learning algorithms to create and assess our trading strategies. The algorithms in question are Deep Deterministic Policy Gradient (DDPG) [4], Proximal Policy Optimization (PPO) [5], Twin Delayed DDPG (TD3) [6], Soft Actor-Critic (SAC) [7], and Advantage Actor-Critic (A2C) [8]. These models have unique principles and learning mechanisms, leading to performance variations. Recent studies have also explored the use of convolutional neural networks (CNNs) in deep reinforcement learning for financial trading, demonstrating their ability to handle large, continuous action spaces [1], [2]. However, we shall only focus on the models mentioned above to conduct an appropriate comparison of models in this study. DDPG employs an actor-critic framework with deterministic policies, whereas PPO concentrates on maximizing the surrogate objective function to provide reliable policy updates. TD3 incorporates two critics and delayed policy updates to enhance stability and improve sampling efficiency. SAC utilizes entropy regularization to promote exploration, while A2C utilizes several actors to parallelize policy updates and improve learning speed.\nIn the next Results section, we thoroughly examine the evaluation of these models and analyze their performance in trading scenarios."}, {"title": "3. Results", "content": "The results of our extensive experimental analysis focuses on three key aspects: reward, purchase diversity, and the performance comparison between holder and trader algorithms. We examine the effectiveness of the reward mechanisms employed in our deep reinforcement learning (DRL) models, assessing their ability to incentivize desirable trading behaviors and optimize performance outcomes. Additionally, we investigate the diversity of asset purchases made by the DRL agents, exploring the breadth and distribution of investments across different financial instruments. Finally, we conduct a comparative analysis between two distinct trading strategies - holding and active trading - to evaluate their respective impact on portfolio performance and overall profitability. Through these comprehensive analyses, we aim to provide insights into the decision-making processes and efficacy of DRL models in financial applications."}, {"title": "3.1. Reward", "content": "In DRL models, rewards play a pivotal role in guiding the learning process. Rewards serve as feedback signals that inform the agent about the desirability of its actions in a given environment. By maximizing cumulative rewards over time, these algorithms learn to make decisions that lead to favorable outcomes. Rewards provide a clear objective for the agent, shaping its behavior and driving it towards optimal policies. In the absence of rewards or with sparse rewards, learning becomes challenging as the agent struggles to discern which actions contribute to long-term success. Thus, we carefully investigate the reward that is return by different DRL algorithms.\nThe accumulative rewards plot (Figure 1) illustrates the effectiveness of each model's trading strategy in producing profits. Despite our initial expectations, which were in favor of SAC and PPO because to their success in other domains like training the humanoid Mujoco, A2C proved to be the top performer in terms of cumulative rewards. PPO and TD3 had somewhat lower performance, whilst DDPG and SAC lagged behind. This discrepancy underscores the intricacies of financial markets and the need for flexible trading strategies."}, {"title": "3.2. Purchase Diversity", "content": "As it is discussed, diversifying stock purchases is known to be paramount for investors seeking to mitigate risk and enhance long-term returns. Therefore, our next investigation helps to learn the behaviour of DRL algorithms to learn whether they tend to diversify their purchase behaviour. With this objective in mind, we extract integral holding that is plotted in Figure 2. Integral Holding plot(Figure 2)offers a comprehensive analysis of the trading patterns of each RL model across many stocks. PPO used a varied approach, doing significant trades with some stocks while maintaining little activity with others. SAC focused its trading activities on a limited number of stocks and carried out large-scale transactions. A2C had comparable trading patterns to SAC, with a concentration on a limited selection of firms, but with lower trading volumes.\nOn the other hand, TD3 used a well-rounded approach by engaging in trading activities with a wide range of securities in very small amounts. DDPG had a similar pattern to TD3, but with less levels of trade activity."}, {"title": "3.3. Holder vs Trader Algorithms", "content": "The figure 3, illustrate the trading patterns of TD3 RL model at different time periods. This figure demonstrated that TD3 tends to pick a few companies and hold them for the entire trading time. Although TD3 tends to hold only a few companies, figure 1 shows that TD3 is one of the top performing DRL algorithms to to accumulate higher rewards. This means TD3 learn to hold companies to return competitively high reward. The graph illustrates the maximum number of shares purchased by the Twin Delayed DDPG (TD3) algorithm, revealing a notable trend where the quantity of shares acquired consistently remains below 400. This pattern suggests a strategic limitation or preference within the TD3 algorithm, implying that it tends to avoid excessively large share acquisitions. Such behavior could be attributed to risk mitigation strategies embedded within the algorithm, aiming to maintain a balanced portfolio or prevent over-exposure to individual assets. Understanding this tendency is crucial for optimizing trading strategies and risk management protocols.\nThe figure 4 illustrate the trading patterns of DDPG DRL model which shows the tendency of this model to holding instead of trading. Similar to Twin Delayed DDPG (TD3), DDPG algorithms exhibit a common inclination toward holding a limited number of shares, not exceeding 400. This shared characteristic reflects a deliberate strategy within both algorithms, possibly rooted in risk management principles or inherent biases in their learning processes. By maintaining a restrained portfolio size, these algorithms may aim to mitigate the potential downside risk associated with overexposure to individual assets or sectors. This preference for moderation underscores the algorithms' prudence in balancing potential rewards with the need to avoid excessive risk, highlighting their adaptive nature in navigating the complexities of financial markets. Understanding this tendency is integral to optimizing trading strategies and aligning investment decisions with the algorithms' inherent preferences for achieving robust and sustainable performance outcomes.\nThe figure 5 illustrate the trading patterns of A2C DRL model. While the A2C algorithm shares the inclination of holding a limited number of shares, akin to the DDPG and TTD3 algorithms, it distinguishes itself by incorporating a broader degree of diversity in its holdings. A2C tends to maintain a portfolio size of less than 500 shares, reflecting a similar risk-conscious approach to its counterparts. However, what sets A2C apart is its ability to introduce greater variety into its investment selections, potentially spanning across a wider range of stocks and sectors. This emphasis on diversity within its holdings enables A2C to capture a more number of companies while still adhering to its risk management objectives.\nAs oppose to first three algorithms (TD3, DDPG, A2C) the last two algorithms (PPO and SAC) tend to trade more than holding. The strategies of SAC and PPO shown a tendency for shorter duration of holding positions, often engaging in the actions of selling and purchasing stocks, as well as adjusting the amounts held.\nFigure 6, and 7 demonstart the trading behaviour of PPO and SAC algoritthms. As it is shown, PPO and SAC tend to engage in a higher volume of trades, resulting in a significantly higher maximum purchase quantity. With maximum purchases around 1700 shares, these trader algorithms demonstrate a willingness to take more frequent and substantial positions in the market. In contrast, A2C, DDPG, and TD3 algorithms typically exhibit a more restrained trading behavior, with maximum purchases seldom exceeding 400 shares. This difference underscores the varying risk profiles and trading styles inherent in each algorithm, with PPO and SAC favoring a more dynamic and aggressive investment strategy compared to the relatively conservative nature of A2C, DDPG, and TD3."}, {"title": "4. Conclusion", "content": "In this research, we examined the performance and behavior of diverse Deep Reinforcement Learning (DRL) algorithms, including DDPG, PPO, TD3, SAC, and A2C, in the domain of financial trading. Our investigation aimed to gain insights into their decision-making processes, particularly concerning the critical choice between holding or trading assets. The results demonstrate that each algorithm exhibits unique trading patterns and strategies, with A2C emerging as the top performer in terms of cumulative rewards.\nThe analysis of the integral holding values reveals that while PPO and SAC engage in significant trades with a limited number of stocks, DDPG and TD3 adopt a more balanced approach, trading smaller amounts across a wider range of securities. Furthermore, the temporal evolution of stock holdings indicates that SAC and PPO tend to hold positions for shorter durations, frequently buying and selling stocks, whereas DDPG, A2C, and TD3 display a propensity to remain stationary for extended periods.\nThese findings underscore the complexity and dynamic nature of financial markets, emphasizing the necessity for adaptable and flexible trading strategies. Although the DRL algorithms showcased promising results, further research is essential to fully comprehend the underlying factors contributing to their performance and to continuously refine their trading methodologies for real-world implementation. Moreover, our study highlights the potential of DRL in revolutionizing financial decision-making by uncovering hidden patterns, exploiting inefficiencies, and optimizing trading strategies in real-time. As DRL continues to advance, it is crucial to explore its implications for market efficiency, risk management, and investor returns.\nIn conclusion, this research provides valuable insights into the application of DRL in financial trading and underscores the importance of ongoing research and development to enhance the performance and adaptability of these algorithms in practical settings. By harnessing the power of DRL, we can pave the way for more intelligent, efficient, and profitable financial decision-making in the future."}]}