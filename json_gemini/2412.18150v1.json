{"title": "EvalMuse-40K: A Reliable and Fine-Grained Benchmark with Comprehensive Human Annotations for Text-to-Image Generation Model Evaluation", "authors": ["Shuhao Han", "Haotian Fan", "Jiachen Fu", "Liang Li", "Tao Li", "Junhui Cui", "Yunqiu Wang", "Yang Tai", "Jingwei Sun", "Chunle Guo", "Chongyi Li"], "abstract": "Recently, Text-to-Image (T2I) generation models have achieved significant advancements. Correspondingly, many automated metrics have emerged to evaluate the image-text alignment capabilities of generative models. However, the performance comparison among these automated metrics is limited by existing small datasets. Additionally, these datasets lack the capacity to assess the performance of automated metrics at a fine-grained level. In this study, we contribute an EvalMuse-40K benchmark, gathering 40K image-text pairs with fine-grained human annotations for image-text alignment-related tasks. In the construction process, we employ various strategies such as balanced prompt sampling and data re-annotation to ensure the diversity and reliability of our benchmark. This allows us to comprehensively evaluate the effectiveness of image-text alignment metrics for T2I models. Meanwhile, we introduce two new methods to evaluate the image-text alignment capabilities of T2I models: FGA-BLIP2 which involves end-to-end fine-tuning of a vision-language model to produce fine-grained image-text alignment scores and PN-VQA which adopts a novel positive-negative VQA manner in VQA models for zero-shot fine-grained evaluation. Both methods achieve impressive performance in image-text alignment evaluations. We also use our methods to rank current AIGC models, in which the results can serve as a reference source for future study and promote the development of T2I generation. The data and code will be made publicly available.", "sections": [{"title": "1. Introduction", "content": "Recently, advanced Text-to-Image (T2I) models [10, 24, 30, 31, 33, 34, 47] are capable of generating numerous impressive images. However, these models may still generate images that fail to accurately match the input text, such as inconsistency in quantities [19, 43, 44]. Given the high cost and inefficiency of manual evaluation, developing a reliable automatic evaluation metric and corresponding benchmark is vital. They can effectively assess existing model performance and guide improvements for future models.\nSince traditional evaluation metrics, such as FID [13] and CLIPScores [12], are not well-suited for assessing the consistency of T2I model generation, recent works [16, 42, 45] have explored the construction of new evaluation metrics. These methods introduce diverse evaluation approaches leveraging various multi-modal models. For example, VQAScore [21] is constructed by asking the Visual Question Answering (VQA) model \u201cDoes this figure show {text}?\u201d and obtaining the likelihood of answer \u201cYes\u201d as the alignment score. However, such a simple VQA manner cannot handle the fine-grained matching problem well. TIFA [16] and VQ2 [45] set up related questions for multiple elements split from the prompt, then averaged the answers to produce a final alignment score. These methods allow for fine-grained evaluation of image-text alignment but are also limited by the performance of the VQA model. Moreover, the relationship between element-based alignment scores and overall human preferences has yet to be explored. Therefore, to better explore the performance of existing T2I evaluation methods, we contribute a new benchmark, EvalMuse-40K, featuring fine-grained human annotations of image-text pairs.\nEvalMuse-40K includes 4K prompts, 40K image-text pairs, and more than 1M fine-grained human annotations. To ensure the diversity of prompts, EvalMuse-40K includes 2K real prompts and 2K synthetic prompts, where the real prompts are sampled from DiffusionDB [41]. We categorize the real prompts from multiple dimensions and use the MILP [39] strategy to ensure the category balance of the final sampled prompts. The synthesized prompts are then constructed for specific skills in image-text alignment, such as quantity and location. By synthesizing specific prompts and sampling from real prompts, EvalMuse-40K is able to accurately evaluate specific model problems while also better exploring model performance in real scenarios. For fine-grained evaluation, we use a large language model for the elemental splitting of prompts and question generation, and to increase the diversity of the generated images, we generate images using a variety of T2I models. Compared with the previous benchmark (see Tab. 1 for details), EvalMuse-40K not only scores image-text alignment but also performs more fine-grained annotations for elements split from the prompts. Additionally, we annotate different types of structural problems that may appear in generated images. This comprehensive and fine-grained human evaluation in EvalMuse-40K enables an analysis of current automated T2I model evaluation metrics and their correlation with human preferences, ultimately supporting the improvement and development of the T2I evaluation system.\nIn addition to the comprehensive benchmark, we introduce two methods, FGA-BLIP2 and PN-VQA, for image-text alignment evaluation. FGA-BLIP2 uses vision-language models to jointly fine-tune the image-text alignment scores and element-level annotations so that the models can output the overall scores and determine whether the generated images match the elements in the prompt. Additionally, we employ a variance-weighted optimization strategy to account for prompts' ability to generate diverse images. PN-VQA constructs specific positive-negative question-answer pairs, allowing MLLMs to evaluate the fine-grained alignment of image-text pairs. With these two methods, we can obtain a scoring model aligned with human preferences and perform fine-grained alignment analysis and evaluation using MLLMs.\nFinally, we select some representative prompts to regenerate images and analyze existing AIGC models using our proposed evaluation methods. We will release our data, models, and code to support further study.\nTo summarize, our contributions are listed as follows.\n\u2022 EvalMuse-40K collects a large number of human annotations and ensures a more robust evaluation of automated metrics' performance by using a more balanced prompt with a larger number of generative models.\n\u2022 EvalMuse-40K categorizes elements during fine-grained annotation, allowing us to evaluate the accuracy of current automated metrics in assessing specific skills at a fine-grained level.\n\u2022 We enable fine-grained evaluation by fine-tuning a scoring model, FGA-BLIP2, achieving impressive performance across multiple datasets.\n\u2022 We introduce a new automated evaluation metric, PN-VQA, which employs positive-negative questioning to enhance correlation with human ratings."}, {"title": "2. Related Work", "content": "Image-Text Alignment Benchmarks. Many different benchmarks have been proposed to evaluate the image-text alignment of T2I models. Early benchmarks are small-scale and mostly rely on captions from existing datasets like COCO [7, 16, 27, 32], focusing on some sample skills. Other benchmarks (e.g., HPDv2 [43] and Pick-a-pic [19]) use side-by-side model comparisons to evaluate the quality of the generated images. Recently, benchmarks like Draw-Bench [34], PartiPrompt [47] and T2I-CompBench [17] have introduced a set of prompts and focused on evaluating specific skills of generative models, including counting, spatial relationships, and attribute binding. Moreover, some benchmarks (e.g., GenAI-Bench [20] and RichHF-18K [25]) provide human annotations on image-text align scores to validate the relevance of automated metrics to human preference. For fine-grained evaluation, benchmarks like TIFA [16] and SeeTRUE [45] extract elements from prompts and generate corresponding questions.\nHowever, prompts in current benchmarks often focus on specific T2I model skills, which differ from prompts used by real users. The proposed EvalMuse-40K addresses this by containing 2K real prompts sampled from DiffusionDB [41] and 2K synthetic prompts for specific skills, enabling a more comprehensive evaluation of image-text alignment in image generation. Additionally, it provides large-scale human annotations on both overall and fine-grained aspects of image-text alignment, allowing deeper analysis of how well current evaluation metrics align with human preference.\nAutomated Metrics for Image-Text Alingment. Image generation models initially use Fr\u00e9chet Inception Distance (FID) [13] to calculate the distributional differences between generated images and validation images. Besides, IS [35] and LPIPS [48] have been used to evaluate the quality of the generated images. However, since these metrics do not effectively evaluate image-text alignment, recent work primarily reports CLIPScore [12], which measures the cosine similarity of text and image's features. With the strong performance of BLIP2 [23], BLIP2Score has also been adopted as an alignment metric similar to CLIPScore.\nHuman preference models (e.g., ImageReward [44], PickScore [19], and VNLI [45]) improve assessment capabilities by using a vision-language model like CLIP, fine-tuned on large-scale human ratings. However, these models primarily rely on side-by-side image comparison annotations to learn human preferences, making it difficult to obtain accurate alignment scores and perform fine-grained evaluations. We address this by using a vision-language model to directly predict both overall and fine-grained alignment scores for image-text pairs.\nFor fine-grained evaluation, TIFA [16], VQ2 [45], and Gecko [42] split prompts into elements and generate questions to assess fine-grained issues in generated images using VQA models. However, these generated questions can sometimes diverge from the original prompt's content. To address this, we design a new positive-negative question template that provides VQA models with more accurate prompt context, improving alignment with human preference."}, {"title": "3. EvalMuse-40K Benchmark", "content": "In this section, we detail EvalMuse-40K, a reliable and fine-grained benchmark with comprehensive human annotations for T2I evaluation. We present the overall architecture of EvalMuse-40K in Fig. 1.\nIn the construction process, we employ various strategies to ensure the diversity and reliability of our benchmark. Next, we will introduce the construction process of EvalMuse-40K from three aspects: prompt collection, image generation, and data annotation followed by statistical analysis of the data."}, {"title": "3.1. Prompts Collection", "content": "To better evaluate the T2I task, EvalMuse-40K collects 2K real prompts from DiffusionDB [41] and 2K synthetic prompts for specific skills. Below, we outline the process of collecting real and synthetic prompts and performing element splitting and question generation.\nReal Prompts from Community. Most prompts used by current benchmarks are generated from templates with LLMs or manually crafted, with only a small portion originating from real-world users. This results in a gap between model evaluation and actual user needs. To address this, we sampled 2K real prompts from DiffusionDB [41], which contains 1.8M prompts specified by real users.\nTo be specific, we first randomly sampled 100K prompts from DiffusionDB as the source dataset. To ensure diversity in the final sampled prompts, we classified the prompts in four aspects: subject category, logical relationship, image style, and BERT [18] embedding score. The specific categories of each aspect are shown in the supplementary material. Except for the BERT embedding score, which is computed, the other aspects are categorized using GPT-4 [1], and prompts can belong to more than one category. After labeling the 100K prompts, we improved the data shaping method proposed in [39] by using Mixed Integer Linear Programming (MILP) to ensure that the sampled data is approximately uniformly distributed across each category. The specific sampling strategy and the distribution of the sampled data are shown in the supplementary material.\nSynthetic Prompts for Various Skills. To comprehensively evaluate the generative models' skills, we employed specific templates and a rich corpus to generate 2K synthetic prompts using GPT-4. These prompts were divided into six categories: (1) Object Count; (2) Object Color and Material; (3) Environment and Time Setting; (4) Object Activity and Perspective Attributes; (5) Text Rendering; and (6) Spatial Composition Attributes. For each category, we used different templates and GPT-4 to generate reasonable and natural prompts. The specific generation methods and details are provided in the supplementary material.\nElement Splitting and Question Generation. To achieve fine-grained annotation and evaluation, we performed element splitting and question generation on the 4K collected prompts. In contrast to the word-level annotation used in RichHF [25] and Gecko [42], we split the prompt into fine-grained elements, which increases the consistency of the annotations across different humans. In addition, categorizing each element allows us to examine the model's capabilities in various aspects at a fine-grained level. We followed the element categorization strategy from TIFA [16] but divided the question generation process into two parts: first splitting the prompts into elements, and then generating specific questions for each element. We generate only simple judgment questions with yes or no answers, making the question generation process more controllable and the evaluation more explainable. The generated questions are also filtered and regenerated to ensure that each element has a corresponding question. The templates used for element splitting and question generation are shown in the supplementary material."}, {"title": "3.2. Image Generation", "content": "To ensure the diversity of the generated images, we selected 20 different types and versions of diffusion-based generative models, considering the strong performance of diffusion models [14]. Meanwhile, to ensure the differentiation among the selected models, we classify the selected models into four groups, (1) basic stable diffusion [33] models, such as SD v1.2, SD v1.5, SD v2.1; (2) advanced open-source generative models, such as SDXL [31], SSD1B [11], SD3 [10], HunyuanDiT [24], Kolors [38], PixArt-a [6], PixArt-\u03a3 [4], IF [8], Kandinsky3 [2], Playground v2.5 [22]; (3) efficient generative models, such as SDXL-Turbo [36], LCM-SDXL, LCM-SSD1B [29], LCM-PixArt [5], SDXL-Lightning [26]; and (4) proprietary generative models, such as Dreamina [9] and Midjourney v6.1 [15]. For each prompt, we randomly sample a subset of models for image generation, applying default parameters to ensure the quality of the generated images. This process results in 40K image-text pairs being generated using 4K prompts, ensuring a diverse dataset for annotation."}, {"title": "3.3. Data Annotation", "content": "In this section, we describe how we performed the data annotation. We first define the content and templates of the annotations and then detail the entire annotation process.\nAnnotation Content and Templates. The annotation includes image-text alignment, structural problems, and extra information (see Fig. 1(b)). In terms of image-text alignment, the annotator first scores the alignment using a 5-point Likert scale, like TIFA [16]. Then, for the fine-grained elements in the prompt, the annotator needs to label whether they are aligned with the image or not. Annotators also mark structural issues in the generated images, categorized into three main types: humans, animals, and objects. For human figures, structural issues are further subdivided by specific regions, such as the face, hands, and limbs. To address potential inaccuracies in element splitting by GPT-4, we introduced a new splitting confidence label, enabling annotators to flag instances with incorrect splitting during annotation. Additionally, because some prompts originate from real users and may contain unclear meanings, we added a label to indicate whether a prompt is meaningful.\nAnnotation Process. To improve the quality of annotation, our annotation process is divided into three stages. 1) Pre-annotation: We formulate straightforward annotation standards to train the annotators, using a small amount of data for pre-annotation. The pre-annotated data are then carefully reviewed. Based on the issues encountered during the annotation and review process, we refine the evaluation standards to ensure greater clarity and consistency. 2) Formal annotation: During formal annotation, each image-text pair is labeled by three annotators, with an additional annotator assigned for quality control. Furthermore, the annotators will identify any NSFW content in the generated images and determine whether they should be discarded. 3) Re-annotation: For instances where alignment scores from the three annotators show significant discrepancies (range > 2), we conduct re-annotation to reduce subjective bias. Ultimately, each image-text pair is annotated by 3 to 6 annotators, and the average score is used as the final label."}, {"title": "3.4. Data Statistics and Reliability Analysis", "content": "To summarize, we generate 40K images using 4K prompts based on 20 T2I models. For each image-text pair, we perform multiple annotations, including image-text alignment scores, element matching, and structural issues, achieving nearly 1M annotations. We employ novel sampling and generation strategies to ensure a balanced and diverse set of prompts. Additionally, we implement multi-round annotation and revision processes to guarantee the reliability of the annotated data.\nWe calculate the image-text alignment scores from the annotations and analyze the consistency of scoring across annotators. The histogram of the alignment scores is shown in Fig. 2a. The statistical results suggest that the alignment scores are distributed across all ranges, with a higher concentration in the middle range. This distribution provides a sufficient number of both positive and negative samples, enabling a robust evaluation of the consistency between existing image alignment metrics and human preferences. It also facilitates the training of a scoring model aligned with human preferences.\nTo analyze the agreement of the scoring data among annotators, we calculate the maximum score difference for each image-text pair and plot it as a histogram in Fig. 2b. It can be seen that 75% of the samples show a score difference of less than 1 point. It is worth noting that for score differences of 2 or more, we obtain double annotations (from 3 to 6) by re-annotating, further reducing the inter-annotator disagreement.\nFor fine-grained annotation, we perform statistics on the quantity and alignment scores of elements based on their respective categories. It can be observed from Fig. 2c that the overall alignment scores for most categories are around 50%, ensuring a balanced distribution of positive and negative samples in fine-grained annotation. Additionally, we found that the images generated by AIGC models exhibited relatively poor consistency with the text in terms of counting, spatial aspects, and activity aspects."}, {"title": "4. Methods for Alignment Evaluation", "content": "In this section, we introduce two methods for evaluating image-text alignment in AIGC tasks. The proposed FGA-BLIP2 can achieve Fine-Grained Alignment evaluation through end-to-end training. The proposed PN-VQA can perform the zero-shot evaluation using a Positive-Negative VQA manner."}, {"title": "4.1. End-to-End Scoring Model: FGA-BLIP2", "content": "Most current scoring models for image-text alignment tasks are trained using reward model objectives, which can learn human preferences between different images generated for the same prompt. Since alignment scores are not available during training, these methods do not always produce values that accurately reflect the degree of image-text alignment. Benefiting from the large number of image-text alignment scores labeled in EvalMuse-40K, we can train the mapping between image-text pairs and scores end-to-end.\nAdditionally, FGA-BLIP2 achieves fine-grained alignment evaluation by jointly training the overall and elemental alignment scores (see Fig. 3). We adopt the setting of Image-Text Matching (ITM) in BLIP2 [23], where the query and text are concatenated and then processed through cross-attention with the image. This yields query embedding and text embedding outputs. The final alignment score is obtained by a two-class linear classifier, where the query part is averaged to produce the overall alignment score, while the text part at each corresponding position provides the element-specific alignment scores. Since not all tokens in the text are highly relevant to the alignment task, we introduce an additional operation to predict valid tokens. The text is first inputted into a self-attention layer to extract text features, which then are passed to an MLP to predict a mask that represents the validity of each text token.\nMeanwhile, we observed that some prompts in the dataset are either too simple or overly complex, resulting in minimal differences in alignment scores across images generated by different models. Such data may lead the model to focus more on prompt complexity than on the actual alignment level of the image-text pairs during training. We therefore design a variance-weighted optimization strategy for image-text alignment task. We calculate the variance \\( \\sigma(p) \\) of the alignment scores across different images generated from the same prompt and use this to adjust the loss weights of different prompts during training.\nThe final loss objective function is as follows:\n\\[L_{\\text{total}} = e^{-\\sigma(p)} \\cdot (L_{os} + \\lambda L_{es} + \\eta L_{\\text{mask}}),\\]\nwhere weighting parameters are set to \\( \\lambda = 0.1 \\) and \\( \\eta = 0.1 \\). \\( L_{os} \\) denotes the L1 loss between the predicted overall alignment score and human annotation. \\( L_{es} \\) denotes the L1 loss between the predicted element score and human fine-grained annotation. \\( L_{\\text{mask}} \\) denotes the L1 loss between the predicted valid text token and the real elements. When using variance to weight the loss function, a larger \\( \\sigma(p) \\) for an image-text pair results in a higher loss. This approach encourages the model to focus more on samples with greater differences in image-text matching scores under the same prompt, helping the model better understand and evaluate alignment level between images and text."}, {"title": "4.2. Fine-Grained Zero-shot Alignment: PN-VQA", "content": "Given the good performance of MLLMs, we follow the previous methods [16, 45] of using question answering for fine-grained evaluation. In comparison to the previous methods, we only generate simple yes/no judgment questions to evaluate whether the elements in the text are accurately represented in the generated images. By using straightforward yes/no questions, we effectively capture the VQA model's probability of outputting correct or incorrect answers.\nAdditionally, we utilize MLLMs to perform VQA tasks from both positive and negative perspectives. The specific question template is presented as follows: \u201cThis image is generated from {prompt}. Is the answer to {question} in this image \\( a \\) ?\u201d, where the answer \\( a \\in \\{\\text{yes}, \\text{no}\\} \\). Then, we substitute the correct \\( a_t \\) and the incorrect \\( a_f \\) into this question template, and perform the VQA task to obtain the probabilities \\( P_T \\) and \\( P_F \\) that the model outputs a positive answer. The formula for calculating the probability \\( P \\) of an affirmative response from MLLMs is as follows:\n\\[P = \\frac{\\exp(L(I, Q, \\text{``Yes''}))}{\\sum_{A_i \\in \\{\\text{Yes}, \\text{No}\\}} \\exp(L(I, Q, A_i))},\\]\nwhere \\( L(I, Q, A) \\) represents the logit of the output A from the MLLMs when given the image I and the question Q. We then average to get the fine-grained alignment score as \\( S_e = (P_T + 1 - P_F)/2 \\). This positive-negative questioning method enables MLLMs to perform a more robust fine-grained evaluation of generated images. Additionally, we found that incorporating the original prompt into the question allows MLLMs to better assess the alignment of elements with the image."}, {"title": "5. Experiments", "content": "5.1. Experimental Setup\nWe use one-quarter of the samples from EvalMuse-40K as the test set, ensuring no overlap in prompts between the training and test sets. The test set includes 500 real prompts and 500 synthetic prompts. We train FGA-BLIP2 on the training set and then test existing evaluation methods along with our FGA-BLIP2 and PN-VQA on the test set. Additionally, we use GenAI-Bench [20], TIFA [16], and RichHF [25] datasets to validate the generalization capability of our fine-tuned model FGA-BLIP2.\nTraining Settings. We use the BLIP-2 [23] model fine-tuned on the COCO [27] for the retrieval task as initialization weights. We follow BLIP-2's training setup to set the maximum learning rate to \\( 1e-5 \\) and the minimum learning rate to 0 and train for 5 epochs on an A100 GPU.\nEvaluation Settings. For overall alignment scores, we compare FGA-BLIP2 with the state-of-the-art models [12, 19, 21, 43, 44] and report the Spearman Rank Correlation Coefficient (SRCC) and Pearson Linear Correlation Coefficient (PLCC) to measure the correlation between model predictions and human annotations. For fine-grained evaluation, we compare PN-VQA with TIFA [16] and VQ2 [45], and use several advanced MLLMs [28, 40, 46] for the VQA task. On one hand, we average the fine-grained scores of the image-text pairs and compare them with the overall alignment scores from human annotations. On the other hand, we conduct fine-grained evaluation by reporting the accuracy of the method's element-wise predictions."}, {"title": "5.2. Results on Automated Metrics", "content": "5.2.1 Quantitative Analysis\nWe evaluate our proposed metrics from two aspects: overall alignment evaluation and fine-grained evaluation.\nOverall Alingment. In Tab. 2, we report the results of FGA-BLIP2 and other state-of-the-art methods on multiple benchmarks. The methods used here can directly output alignment scores from image-text pairs without extra operations like VQA. It is evident that FGA-BLIP2 performs excellently not only on the EvalMuse-40K test set but also exhibits higher correlation with human annotations on GenAI-Bench [20], TIFA800 [16], and the alignment score sections of RichHF [25] compared to other methods.\nFine-grained Evaluation. In Tab. 3, we report the comparison results of our FGA-BLIP2 and PN-VQA with other VQA methods such as TIFA [16] and VQ2 [45]. We use SRCC to measure the correlation between averaged fine-grained element scores and image-text alignment scores, and report the accuracy of fine-grained evaluation. It can be seen that FGA-BLIP2 achieves the best performance, and PN-VQA also performs well compared to other VQA-based methods. In addition, Qwen2 [40] yields a superior correlation with human annotations than other VQA models.\nFor different elemental skills in fine-graining such as counting and color, we also compare multiple fine-graining evaluation methods, and both FGA-BLIP2 and PN-VQA achieve good performance. Meanwhile, we select 200 representative prompts and use our FGA-BLIP2 to evaluate the alignment of image-text pairs generated by over twenty T2I models. We found that current proprietary models such as Midjourney [15] and Dreamina [9] generally performed better. Additionally, we evaluate MLLMs' ability to detect structural problems using EvalMuse-40K's structural labels. Due to space constraints, the comparison results for all element skills, the ranking of T2I models' alignment ability by FGA-BLIP2, and the exploration of different MLLMs' structural problem detection abilities are provided in the supplementary material."}, {"title": "5.2.2 Qualitative Analysis", "content": "In Fig. 4, we present the prediction scores of FGA-BLIP2 and VQAScore [21] across multiple image-text pairs. It can be seen that FGA-BLIP2 produces scores that are more consistent with human annotations. For fine-grained evaluation, as shown in Fig. 5, our proposed methods are able to more accurately determine whether the elements in the prompt are represented in the generated image."}, {"title": "5.2.3 Ablation Study", "content": "The ablation study for FGA-BLIP2 is shown in Tab. 2. When evaluating using the overall alignment scores (os) output by FGA-BLIP2, our model achieves better correlation across four datasets compared to the method without the variance optimization strategy (w/o var). We also report the fine-grained element scores (es) output by FGA-BLIP2 and take the average (avg) as the image-text alignment score for evaluating the model's performance on the datasets. Combining and weighting os and es_avg as the final score leads to improved performance on GenAI-Bench and TIFA.\nThe ablation study for PN-VQA is shown in Tab. 4. The results show that incorporating the prompt used for generating the image into the question, and performing VQA in both positive and negative question manners separately, significantly improves the correlation between the VQA method and human annotations."}, {"title": "6. Conclusion", "content": "In this work, we contribute EvalMuse-40K, which contains a large number of manually annotated alignment scores and fine-grained element scores, enabling a comprehensive evaluation of the correlation between automated metrics and human judgments in image-text alignment-related tasks. We also propose two new evaluation methods: FGA-BLIP2, which fine-tunes a vision-language model to output token-level alignment scores, and PN-VQA, which improves the ability of the VQA model in fine-grained perception by positive-negative questioning. These methods improve the correlation between current image-text alignment metrics and human annotations, enabling better evaluation of image-text alignment in T2I models."}]}