{"title": "Simple and Interpretable Probabilistic Classifiers for Knowledge Graphs", "authors": ["Christian Riefolo", "Nicola Fanizzi", "Claudia d'Amato"], "abstract": "Tackling the problem of learning probabilistic classifiers from incomplete data in the context of Knowledge Graphs expressed in Description Logics, we describe an inductive approach based on learning simple belief networks. Specifically, we consider a basic probabilistic model, a Naive Bayes classifier, based on multivariate Bernoullis and its extension to a two-tier network in which this classification model is connected to a lower layer consisting of a mixture of Bernoullis. We show how such models can be converted into (probabilistic) axioms (or rules) thus ensuring more interpretability. Moreover they may be also initialized exploiting expert knowledge. We present and discuss the outcomes of an empirical evaluation which aimed at testing the effectiveness of the models on a number of random classification problems with different ontologies.", "sections": [{"title": "1 Introduction", "content": "Classifying individual resources in the context of Knowledge Graphs (KGs) [7] is a fundamental task enabling several more complex applications. In this context, classification may turn out to be a non trivial task, even in an approximate form, especially when the original semantics has to be preserved as much as possible.\nCurrent research on graph-structured data focuses on representation learning methods [13,11] that aim at mapping entities and relations to embedding spaces where tasks like classification and link prediction can be defined in terms of linear algebra operations. The main downsides with these methods are related both to the difficulty of incorporating implicit knowledge that can be precisely extracted from the KG through deductive reasoning, and to the poor interpretability of complex classification models, as the embedding spaces turn out to be hardly relatable to the original features, so that these models are exploited as black-boxes whose decisions can be hardly explained.\nAlternatively, for the sake of interpretability of the models and their decisions, it is possible to resort to simpler yet still effective probabilistic models inferred from data, ultimately defined in terms of Boolean variables that leverage on basic logic features extracted from primitive classes and properties in the ontology [4]. Following some ideas applied in the context of neural learning [12], simple graphical models can be fitted and also converted into probabilistic rules or simplified to generate axioms [10,4] in Description Logics (DL) [1] ensuring a direct interpretation in terms of the original representation.\nSpecifically, we focus on the problem of fitting multivariate Bernoulli models on which the common simplifying assumption is made of conditional independence of the input features given the target one, characterizing the Naive Bayes classifiers, which simplifies the model often without compromising its effectiveness [3]. Then we consider also a two-tier hierarchical model in which a mixture of Bernoullis [6] is used to cluster the individuals in groups before applying the classifier.\nGenerative models are especially suitable for cases in which the data available for their fitting is inherently incomplete, which is very likely when reasoning with an underlying open-world assumption, as typical of Logics and large (distributed) KGs. Moreover, they may be also used for multiple additional applications, such as KG completion and refinement, axiom (disjointedness) discovery, clustering, anomaly detection, etc. Differently from more complex models, these classifiers are also suitable for an easier interpretation, verification and even integration operated by domain experts, or by anybody that shares the intended semantics of the terminology. For example, in principle existing background knowledge in the form of probabilistic rules or axioms could be integrated within these models.\nAn experiment that aimed at testing feasibility and effectiveness of such models on a number of random classification problems with different Web ontologies is described. As a baseline, we also considered a related simple discriminative model like Logistic Regression: specifically a regularized version was adopted to enforce the sparseness of the coefficients of the linear combination, so to ensure model interpretability as much as possible.\nIn the remainder of the paper, we briefly introduce a Boolean encoding for the individuals in terms of basic features / concepts (Sect. 2). Then, in Sect. 3, a simple model based on multivariate Bernoullis is described, together with details on the probabilistic rules/axioms that can be derived, and its fitting, which is further extended in the next Sect. 4 discussing better ways to cope with incompleteness, resorting to a procedure based on EM. The model can be extended considering a simple two-tier hierarchy as shown in Sect. 5. An empirical evaluation is described in Sect. 6 with a comparison of some instances of these models on a number of classification tasks on real ontologies. Finally Sect. 7 concludes the paper discussing some limitations and possible extensions of this work."}, {"title": "2 Preliminaries", "content": "Preliminarily, we assume familiarity with the basic notions and the standard notation of Description Logics as in the following we will focus on knowledge graphs represented through DL axioms and ultimately as OWL-DL ontologies."}, {"title": "3 Multivariate Bernoulli Naive Bayes Model", "content": "Formally, let K be a DL knowledge base K = (T, A), where the TBox T is a set of terminological axioms that regard concepts and roles (i.e. classes and properties) of the domain of interest, and the ABox A contains basic assertions, i.e. facts regarding the individuals whose collection will be indicated with Ind(K). \nIn line with previous works on distances or kernels for these representations [2,5], we will build the models upon a simple encoding of the individuals in a feature space. In the following the Boolean set will be denoted by B = {0,1}. Given an ordered set F = {Fi}$_{i=1}^P$ of basic features, i.e. concepts in the signature of K or defined in terms of other concepts and roles (classes and properties) therein, we will consider each individual a \u2208 Ind(K) as represented by the Boolean vector a \u2208 BD with each component, i \u2208 [1 : D]:\n$A_i = \\begin{cases} 1 & K \\models F_i(a) \\\\ 0 & K \\models \\neg F_i(a) \\end{cases}$\nwhere  denotes the underlying proof procedure. Note that $a_i$ stands for the Boolean value corresponding to the definite membership to $F_i$ (1) or to its complement (0); then for some $F_i$, $a_i$ may be undefined when the procedure is unable to determine its membership under the standard open-world semantics adopted in DL.\nThese cases may be treated by replacing the missing values with a fixed constant $m_i \\in ]0, 1[$, i.e. a prior on the uncertain membership to $F_i$, derived from pseudo-counts provided by experts or from frequencies observed in the dataset. Alternatively, one might consider adopting an uninformative initialization for the missing values in the input vectors and then use a non-supervised model as an encoder (see [4]).\nWith individuals represented as D-dimensional binary vectors $X = [X_1, X_2, ..., X_D]^T$, and a binary output variable y indicating the membership w.r.t. the target class C, their conditional distribution for either membership case can be modeled as a Multivariate Bernoulli Naive Bayes Model (MBNBM), i.e. a joint distribution of Bernoulli variables, that are assumed to be conditionally independent given the membership to C:\n$P(x|y = b) = Ber(x|p_b) = \\prod_{i=1}^{D} Ber(x_i|p_{bi}) = \\prod_{i=1}^{D} (p_{bi})^{x_i} (1 - p_{bi})^{1-x_i}$ \nwith parameters $p_{bi} = P(x_i = 1|y = b)$, conditional probabilities of $x_i = 1$ (i.e. of individual x to belong to $F_i$) given the membership to C indicated by y = b."}, {"title": "3.1 Classification", "content": "A classification problem amounts to estimating the state of the output y for an input individual x represented by the Boolean vector x. Given the model parameters, i.e. prior $P(y) = \\pi$ and $P(x|y)$, the posterior can be computed using Bayes' rule and Eq. (2):\n$P(C|x) = P(y = 1|x) = \\frac{P(x|y = 1)P(y = 1)}{P(x)} = \\frac{\\pi Ber(x|p_1)}{\\pi Ber(x|p_1) + (1 - \\pi)Ber(x|p_0)}$\n$P(\\neg C|x) = 1 - P(C|x)$, then the membership to be predicted depends on the more probable case:\n$\\hat{y} = \\underset{b \\in B}{argmax} P(y = b|x)$ which is equivalent to the decision procedure:\n$if P(y = 1|x) > 0.5 then K \\models \\tilde{} C(x) else K \\models \\tilde{} \\neg C(x)$\nwhere a different symbol $\\tilde{}$ is used to denote the prediction based on the probabilistic model (instead of the logical one  ). The mid-point 0.5 is adopted for simplicity, yet more complex decision procedures can be devised, taking into account the case of rejection when the probability is close to this value. This can be defined in terms of a cost-sensitive decision using a threshold $\\theta \\in (0.5, 1]$ for either decision."}, {"title": "3.2 Rules and Conjunctive Class Definitions", "content": "As discussed in [4], in line with similar approaches for models based on features with continuous distributions (e.g. see [6,12]), a probabilistic conjunctive rule for C may be defined as shown in Fig. 2, which can be simplified considering for each i \u2208 [1: D] the conjunct with higher probability. Rules for the prediction of the complement \u00acC can be formed analogously, in terms of the parameters $p_{0i}$ and \u03c00.\nGiven a minimal threshold \u03b8, as hinted above, an approximate logic definition for the target class may be extracted from a MBNB model considering the sets F+ = {i \u2208 [1 : D] | p1i > \u03b8} of features positively correlated with the membership and F\u2212 = {i \u2208 [1 : D] | p0i > \u03b8} of features negatively correlated with the membership. Then it is possible to define the approximate axiom:\n$C \\sqsubseteq \\underset{i \\in F^+}{\\sqcap} F_i \\sqcap \\underset{j \\in F^-}{\\sqcap} \\neg F_j$\nwhich can be proposed to a domain expert for its validation and possible inclusion in the definition of the target class C. Note that only some of the basic features are considered, namely those strongly positively/negatively correlated with the membership, where the strength is quantified by \u03b8."}, {"title": "3.3 Fitting the Model", "content": "We assume the availability of a complete training set $T = (X, y) = \\{(x^t, y^t)\\}_{t=1}^N$ where $x^t$ is the encoding of an individual $x \\in Ind(K)$ and $y \\in B$ indicates the actual membership to C.\nThe naive Bayes classifier can be trained by finding the maximum likelihood (or even the maximum a posteriori) estimate for the parameters \u03a0 = {\u03c0, p1, p0}. As the probability for a single example is given by\n$P(x^t, y^t| \\Pi) = P(y^t|\\pi_{y^t}) P(x^t|y^t, p_{y^t}) = \\pi_{y^t} Ber(x^t|p_{y^t})$\nthen the log-likelihood $L(\\Pi) = log P(T|\\Pi)$ can be written:\n$L(T|\\Pi) = log \\prod_{t=1}^{N} P(x^t, y^t| \\Pi) = \\sum_{b \\in B} N_b log \\pi_b + \\sum_{b \\in B} \\sum_{t: y^t=b} \\sum_{i=1}^{D} log Ber(x_i|p_{bi})$\nwhere $N_b = \\sum_t 1(y^t = b)$ are the counts of training examples for either membership case (b\u2208B).\nThe MLE for the proportions of the prior $P(y = b)$ can be estimated as $\\hat{\\pi}_b = N_b/N$. As all input features are Bernoulli for either value of y, i.e. $X_i|y \\sim Ber(p_{yi})$, the MLE for the parameters are $\\hat{p}_{bi} = N_{bi}/N_b$, i \u2208 [1 : D] with $N_{bi} = \\sum_t 1(x_i = 1, y = b)$.\nActually, one does not expect the features to be independent (conditionally on y). However, as discussed in [3], a NB model can still be quite effective, even when this assumption does not hold true, given the limited number of parameters which makes it less prone to overfitting. To better avoid this problem, one may resort to a Bayesian approach considering Beta conjugate priors [11]."}, {"title": "4 Dealing with Incomplete Data", "content": "In the context of DL knowledge graphs it is possible that given a feature concept the membership of an individual cannot be ascertained logically because of the open-world semantics. In such non unlikely cases, some input feature $x_i$ or target feature $y^t$ would assume an indefinite value making $T = (X, y)$ incomplete.\nA further assumption should be made about the cause of missing values for target and/or input features [9]. A MAR (Missing at Random) setting is likely not adherent to the ground truth about the data but it is adopted to keep the model simple.\nAlthough in principle the problem of determining the indefinite values of features could be treated homogeneously over T for missing values either in X or in y, we consider distinct cases tackled in subsequent phases:\n1. examples with an indefinite value for some input feature $x_i$;\n2. examples with an indefinite value for the target feature $y^t$."}, {"title": "4.1 Phase 1: Missing Input Values", "content": "A simple solution to the problem of missing values of input features is imputing a fixed value or a constant decided on a per feature basis (e.g. estimated as the mode or the mean of the column vector X) which may be also conditioned to a known value of the corresponding target feature $y^t$, as previously treated in the estimation of the parameters.\nAssuming that the missingness of a feature is not informative about its potential value, if parts of an input vector x were missing during training (and/or testing), then one may marginalize out such features [11]. Writing x as ($x_o, x_u$), with $x_u$ sub-vector defined on the features with unknown (missing) values and $x_o$ defined on those with observed values, classification would amount to maximize:\n$P(y|x_o, \\Pi) \\propto \\pi_y P(x_o|y, P_y) = \\pi_y \\sum_{x_u \\in X_u} P(x_o, x_u | y, P_y) = \\pi_y Ber(x_o | y, p_y)$\nwith the term for $x_u$ ignored because of the assumed conditional independence [11]. More exactly:\n$P(y = b|x) = \\frac{P(x, y = b)}{P(y = b)} = \\frac{Ber(x_o | p_{bo}) \\pi_b}{\\sum_{b' \\in B} P(x|y = b') P(y = b')} = \\frac{\\pi_b \\prod_{x_o \\in X_o} Ber(x_o | p_{bo})}{\\sum_{b' \\in B} \\pi_{b'} \\prod_{x_o \\in X_o} Ber(x_o | p_{b'o})}$\nTo estimate the value of any $x_u$ based on the observed features $x_o$, one can compute:\n$P(x_u | x_o) = \\frac{\\sum_b \\pi_b Ber(x_u | p_b) Ber(x_o | p_b)}{\\sum_b \\pi_b Ber(x_o | p_b)}$\nThen the expected value would be:\n$E[x_u | x_o] = \\sum_{v \\in B} vP(x_u = v | x_o) = \\frac{\\sum_{v \\in B} v \\sum_b \\pi_b w_b Ber(x_o | p_b)}{\\sum_b \\pi_b Ber(x_o | p_b)}$\nwhere $w_b = p_{bu}$ .\nAnother option (which is valid even when the naive Bayes assumption cannot be not made) is to fit the model in case of incomplete data resorting to an Expectation-Maximization (EM) procedure [6,11]. The goal is to find maximum likelihood solutions for the latent variables Z (in this case the x in T) to be estimated together with the model parameters \u03a0. On each iteration l, in the E step, the current parameter values \u03a0l are used to find the distribution of the latent variables conditional on the observed ones (including the current values for \u03a0l). Then, using this posterior, the expectation of the complete-data log likelihood is computed: $Q(\\Pi|\\Pi^l) \\leftarrow E[C(\\Pi|Z, T)| Z, T, \\Pi^l]$, for some general parameter value \u03a0. In the M step, the revised parameter estimate \u03a0l+1 is determined by maximizing $Q(\\Pi|\\Pi^l)$. Before ending each iteration the current parameters are updated for the next step.\nIn this first phase:\nE step \u2200t \u2208 [1 : n], $x^t = (x_o, x_u)$:\n$P(x_u = b | x_o, \\Pi^l) \\leftarrow \\frac{\\pi_b w_b Ber(x_o | p_b)}{\\sum_{b'} w_{b'} \\pi_{b'} Ber(x_o | p_{b'})}$\nM step \u2200b \u2208 B, i\u2208 [1 : D]:\n$p_{bi} \\leftarrow \\frac{\\sum_t P(x_u = b | x_o, \\Pi^l) x_i^t}{\\sum_t P(x_u = b | x_o, \\Pi^l)}$\n$\\pi_b \\leftarrow \\frac{1}{N} \\sum P(x_u = b | x_o, \\Pi^l)$"}, {"title": "4.2 Phase 2: Missing Target Values", "content": "As regards the second case, considering a training set $T'$ with a completed X and a possibly incomplete y, the log-likelihood to be maximized would be:\n$L(\\Pi / T') = \\sum_{t=1}^{N} log \\sum_{b \\in B} \\pi_b Ber(x^t | p_b)$"}, {"title": "5 A Hierarchical Bernoulli Model", "content": "In general, the features for a class are likely to be correlated then a single model for describing the instances' distribution may turn out to be inadequate when the assumption of conditional independence cannot be made, leading to an inaccurate classifier.\nThis problem can be tackled by considering a distribution modeled as a mixture of (multivariate) Bernoullis. Then, introducing an intermediate K-dimensional layer z of latent binary indicator variables corresponding to the nodes {$z_k$}$_{k \\in [1:K]}$, characterized by a different Bernoulli $P_k(x|z_k)$, a 2-tier model is defined whose structure is depicted in Fig. 3. This network, that will be referred to as Hierarchical Bernoulli Model (HBM), combines the mixture of multivariate Bernoullis (bottom-tier) with the former MBNB classifier (top-tier). Alternatively, this model can be also described in the form of a Hierarchical Mixture of Experts [8]."}, {"title": "5.1 Classification", "content": "In this case, given the model parameters \u03a0 = {\u03c0,\u03bc, P}, where $\u03bc_k = P(z_k = 1)$, $P = [p_k] \u2208 [0, 1]^{K\u00d7D}$ with $p_k = [p_{k1}, ..., p_{kD}]$ (in Fig. 3 we have a column $P_k^T$ at each bottom node), we can write:\n$P(x|\\Pi) = \\sum_{k=1}^{K} P(x, z_k = 1| \\Pi) = \\sum_{k=1}^{K} P(z_k = 1| \\mu) P_k(x|z_k = 1; p_k) = \\sum_{k=1}^{K} \\mu_k Ber(x|p_k)$\nThe structure depends on the dimensionality of the mixture K, whose choice can be made in advance exploiting part of the training data, to maximize a score function like the BIC [11], for example. Alternatively a Bayesian procedure can be adopted.\nTo decide the membership to C for an input individual x with corresponding Boolean vector x, using Eq. (6), the posterior is determined as:\n$P(y = 1|x; \\Pi) = \\frac{P(x|y = 1; \\Pi)P(y = 1)}{P(x)} = \\frac{\\pi \\sum_{k=1}^{K} \\mu_k Ber(x|p_k)}{\\pi \\sum_{k=1}^{K} \\mu_k Ber(x|p_k) + (1 - \\pi) \\sum_{k=1}^{K} \\mu_k Ber(x|p_k)}$\nThe decision procedure is analogous to (4). Alternatively we could use disjunctive and conjunctive classification rules translating the hierarchical model."}, {"title": "5.2 Disjunctive Definitions / Rules", "content": "Given a MMBM, an axiom for class C may be extracted by defining the class in terms of the components:\n$C \\sqsubseteq \\bigcup_{k=1, ..., K \\mid z_k = 1} C_k$\nwhich can be specified probabilistically indicating the class prior \u03c0 and \u03bck such that zk = 1 or, alternatively, also as a disjunctive classification rule for C and a set of probabilistic rules for the Ck's as shown in Fig. 4.\nFor the sake of interpretability, these rules may be simplified considering, for each i, only the more likely sub-case with a minimal threshold to be exceeded. A class definition can be given also for the complement C in terms of the sub-classes Ck corresponding to the cases when zk = 0."}, {"title": "5.3 Fitting the Model", "content": "The network parameters can be determined using a training set T = (X, y). The two tiers can be trained separately: the bottom one tackles an unsupervised learning task, while the top tier is essentially a MBNB classifier.\nAs regards the bottom-tier of the structure, the parameters for the mixture model can be learned through a EM procedure with the cases of incomplete input data treated similarly to the Phase 1, described in Sect. 4.\nIn this case, given the model parameters I, the complete log-likelihood to be maximized is:\n$C(\\Pi, Z | X) = log \\prod_{t=1}^{N} P(x^t | \\Pi) = \\sum_{t=1}^{N} log [\\sum_{k=1}^{K} \\mu_k Ber_D (x|P_k)]$\nNote that it also targets the latent variables z.\nand the EM steps can be defined as follows:\nE-step: the responsibility of each component of the mixture is estimated as:\n$P_t(z_i \\rightarrow x^t) \\leftarrow \\frac{\\mu_i P_i(x^t | p_i)}{\\sum_{h=1}^{K} \\mu_h P_h(x^t | p_h)}$\nM-step: the parameters values are updated on the ground of the new estimates for $P_t(z | x^t)$ along the following rules:\n$\\mu_k \\leftarrow \\frac{1}{N} \\sum_{t=1}^{N} P_t(z | x^t)$\n$P_k \\leftarrow \\frac{\\sum_{t=1}^{N} P_t(z | x^t) x^t}{\\sum_{t=1}^{N} P_t(z | x^t)}$\nAlternatively one may adopt a more sophisticate solution based on the Bayesian treatment of the parameters (using variational inference or Dirichlet processes [11]).\nThe top-tier MBNBM is trained as described in Sect. 4."}, {"title": "6 Experiments", "content": "We briefly present how the models involved in the experiments have been implemented, then we describe the experimental setup (datasets, problems, hyper-parameters setup) and finally we discuss the results of this empirical evaluation."}, {"title": "6.1 Implementation", "content": "The implementation of the models' prototypes combines various facilities provided by Python libraries. Owlready26 is intended for ontology-oriented programming and was used to manage the knowledge graphs, including loading and storage while it relies on an embedded (Java) version of Pellet for reasoning services, mainly used to initialize the binary encoding of the individuals in each ontology. Libraries Scikit-Learn and Sklearn-Bayes offer various implemented models that could be extended and facilities for the organization of the experiments (measures, cross-validation, etc.) that were employed as described in the following. In particular, the HBM was implemented by pipelining a variational Bayes variant of the Bernoulli mixture model with a MBNB classifier."}, {"title": "6.2 Settings", "content": "Three simple probabilistic models were assessed in the experiment, namely a basic implementation of the MBNBM, its integration with EM procedures for fitting latent variables and parameters, and regularized Logistic Regression, with a L1 penalization, as a baseline discriminative probabilistic model of comparable complexity in the number of parameters.\nThe KGs of four ontologies were selected to create problems and related datasets employed as a testbed: KRKZEROONE (a small ontology derived from a well-known UCI dataset), NEW TESTAMENT NAMES (NTNAMES), FINANCIAL, and an ontology that was generated using the LEHIGH UNIVERSITY BENCHMARK (LUBM). Ontologies, target classes, code and output files of the experiments are publicly available in the project repository.\nFor each ontology basic features (minimal classes in the subsumption hierarchy) have been extracted, then classes formed as universal and existential restrictions were also included, each involving one the available object properties. However, to define the final set F and the encoding of the individuals for each dataset, a preliminary univariate feature selection phase was performed to pick the most informative ones, i.e. those that exhibited a higher variance, with respect to a cutoff threshold. Tab. 1 reports the numbers of classes and object properties selected per KG as well as the numbers of (generated/selected) features that were considered in the experiments.\nArtificial learning problems have been randomly created defining 10 new target (disjunctive) classes for each ontology, based on the respective signature. To avoid trivial problems, each eligible target class was required to partition the set of individuals in nonempty subsets with a minimal number of positive and negative instances: the ground truth, i.e. the reference classification of all individuals in the ontology w.r.t. each problem"}, {"title": "6.3 Results", "content": "The outcomes of the tests in terms of the measures averaged over the various problems per KG are summarized in Tab. 2. More details on each experiment can be found in the output files made available.\nConsidering the various measures, the general trend of the outcomes shows that MBNB-EM achieved the best performance w.r.t. all metrics and ontologies, with MBNB as a close runner-up, and both outperformed LOGREG. The outcomes show that the performance tends to increase with larger datasets in terms of numbers of individuals: they likely make it easier to get better estimates for parameters and latent variables. Also the hierarchical model proved its effectiveness although it was not as effective as the simpler models over all problems hinting that larger datasets would be required to properly fit a model with more parameters. Moreover a fine-tuning of the hyperparameters on a per-problem basis (target class) would produce better models at the cost of an extra computational effort.\nThe lesser performance of LOGREG was likely also due to the amount of penalty adopted to produce simpler models (i.e. with few non-zero parameters). Separate experiments showed that its performance could be improved adopting a L2 penalization (and fine-tuning the amount of regularization with each dataset). However, the resulting models would turn out to be less interpretable.\nAs regards the stability of the results, it must be pointed out that the outcomes for each ontology were averaged over the 10 problems. Overall, it appears that the proposed MBNB models are also more stable than the considered baseline. The diversity of the target class determined some limited variance especially in terms of recall and consequently of F1-measure. For a deeper insight in the outcomes of the single learning problems the output files can be consulted.\nA Friedman-Nemenyi test was performed which revealed the experimental results to be statistically significant especially in the comparison of the MBNB models with the baseline. More specifically, the differences were found significant in the comparison of MBNB with LOGREG on NTNAMES and FINANCIAL and of MBNB-EM with LOGREG on KRKZEROONE, NTNAMES, and FINANCIAL. Further details can be found in the material available in the project repository.\nAs a final general consideration regarding the interpretability of the models learned, we recall that the basic features involved are selected among those extracted per knowledge graph (see Tab. 1). They correspond to the nodes at the input level of the related networks. Hence the number of boolean features that can be set in a model (i.e. those whose probability may exceed the threshold) is bounded above by the number of these features, D."}, {"title": "7 Conclusions and Possible Extensions", "content": "In order to better tackle the inherent incompleteness of the DL knowledge graphs, we extended the methods to generate simple probabilistic classifiers leveraging on the semantics of basic logic features regarded as discrete random variables. These models are especially suitable for problems related to incomplete data, which is the case of the DL KGs. Besides, they can be converted into probabilistic rules or axioms with a straightforward interpretation in terms of the terminology employed by the KG, hence enforcing the model interpretability. An experiment that tested the effectiveness of these models in comparison with a simple discriminative one (logistic regression) adopted as a baseline, proved them effective and worth of further investigations.\nA number of limitations would require our attention. KGs containing plenty of classes and properties may offer numerous features for the initial feature selection phase which is unsupervised. Better supervised methods may be considered to elicit a limited number of important features. Complex concept definitions may turn out to be harder to learn, especially when nested restrictions are considered although such definitions are hardly found in KGs. The hierarchical model in its current implementation would require a more careful search of optimal values for other hyperparameters. This also calls for experiments involving larger datasets in terms of the number of individuals.\nOn the ground of similar works on mixture models [6,12,4] various extensions are possible along different lines including:\n*   the incorporation of existing rules or axioms in the probabilistic models;\n*   a different classification setting, considering decision procedures that admit rejection cases, to better comply with the original semantics;\n*   a deeper investigation on the adoption of a Bayesian approach to adapt the structure of the models (and parameters fitting) considering suitable priors;\n*   the integration of continuous (Gaussian) features to include in the model also restrictions on numerical datatypes.\nApplications of the probabilistic classifiers to various other tasks are possible, such as KG debugging, e.g. for anomaly detection, and knowledge refinement, e.g. using these models for axiom discovery, etc."}]}