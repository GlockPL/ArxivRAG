{"title": "Simple and Interpretable Probabilistic Classifiers for Knowledge Graphs", "authors": ["Christian Riefolo", "Nicola Fanizzi", "Claudia d'Amato"], "abstract": "Tackling the problem of learning probabilistic classifiers from incomplete data in the context of Knowledge Graphs expressed in Description Logics, we describe an inductive approach based on learning simple belief networks. Specifically, we consider a basic probabilistic model, a Naive Bayes classifier, based on multivariate Bernoullis and its extension to a two-tier network in which this classification model is connected to a lower layer consisting of a mixture of Bernoullis. We show how such models can be converted into (probabilistic) axioms (or rules) thus ensuring more interpretability. Moreover they may be also initialized exploiting expert knowledge. We present and discuss the outcomes of an empirical evaluation which aimed at testing the effectiveness of the models on a number of random classification problems with different ontologies.", "sections": [{"title": "1 Introduction", "content": "Classifying individual resources in the context of Knowledge Graphs (KGs) [7] is a fundamental task enabling several more complex applications. In this context, classification may turn out to be a non trivial task, even in an approximate form, especially when the original semantics has to be preserved as much as possible.\nCurrent research on graph-structured data focuses on representation learning methods [13,11] that aim at mapping entities and relations to embedding spaces where tasks like classification and link prediction can be defined in terms of linear algebra operations. The main downsides with these methods are related both to the difficulty of incorporating implicit knowledge that can be precisely extracted from the KG through deductive reasoning, and to the poor interpretability of complex classification models, as the embedding spaces turn out to be hardly relatable to the original features, so that these models are exploited as black-boxes whose decisions can be hardly explained.\nAlternatively, for the sake of interpretability of the models and their decisions, it is possible to resort to simpler yet still effective probabilistic models"}, {"title": "2 Preliminaries", "content": "Preliminarily, we assume familiarity with the basic notions and the standard notation of Description Logics as in the following we will focus on knowledge graphs represented through DL axioms and ultimately as OWL-DL ontologies."}, {"title": "3 Multivariate Bernoulli Naive Bayes Model", "content": "With individuals represented as D-dimensional binary vectors $X = [X_1,X_2,...,X_D]^T$, and a binary output variable y indicating the membership w.r.t. the target class C, their conditional distribution for either membership case can be modeled as a Multivariate Bernoulli Naive Bayes Model (MBNBM), i.e. a joint distribution of Bernoulli variables, that are assumed to be conditionally independent given the membership to C:\n$P(x|y = b) = Ber(x|p_b) = \\prod_{i=1}^D Ber(x_i | p_{bi}) = \\prod_{i=1}^D (p_{bi})^{x_i} (1 - p_{bi})^{1 - x_i}$ (2)\nwith parameters $p_{bi} = P(x_i = 1|y = b)$, conditional probabilities of $x_i = 1$ (i.e. of individual x to belong to $F_i$) given the membership to C indicated by y = b."}, {"title": "3.1 Classification", "content": "A classification problem amounts to estimating the state of the output y for an input individual x represented by the Boolean vector x. Given the model parameters, i.e. prior $P(y) = \\pi$ and P(x|y), the posterior can be computed using Bayes' rule and Eq. (2):\n$P(C|x) = P(y = 1|x) = \\frac{P(x|y = 1)P(y = 1)}{P(x)} = \\frac{\\pi Ber(x|p_1)}{\\pi Ber(x|p_1) + (1 - \\pi)Ber(x|p_0)}$ (3)\n$P(\\neg C|x) = 1 - P(C|x)$, then the membership to be predicted depends on the more probable case:\n$\\hat{y} = \\underset{b \\in B}{\\operatorname{argmax}} P(y = b|x)$\nwhich is equivalent to the decision procedure:\nif $P(y = 1|x) > 0.5$ then $K \\vdash \\sim C(x)$ else $K \\vdash \\sim \\neg C(x)$ (4)\nwhere a different symbol $\\vdash$ is used to denote the prediction based on the probabilistic model (instead of the logical one $\\vdash$). The mid-point 0.5 is adopted for simplicity, yet more complex decision procedures can be devised, taking into account the case of rejection when the probability is close to this value. This can be defined in terms of a cost-sensitive decision using a threshold $\\theta \\in (0.5, 1]$ for either decision."}, {"title": "3.2 Rules and Conjunctive Class Definitions", "content": "As discussed in [4], in line with similar approaches for models based on features with continuous distributions (e.g. see [6,12]), a probabilistic conjunctive rule for C may be defined as shown in Fig. 2, which can be simplified considering for each i \u2208 [1: D] the conjunct with higher probability. Rules for the prediction of the complement \u00acC can be formed analogously, in terms of the parameters $p_{0i}$ and $\\pi_0$.\nGiven a minimal threshold \u03b8, as hinted above, an approximate logic definition for the target class may be extracted from a MBNB model considering the sets $F^+ = \\{i \\in [1 : D] | p_{1i} > 0\\}$ of features positively correlated with the membership and $F^- = \\{i \\in [1 : D] | p_{0i} > 0\\}$ of features negatively correlated with the membership. Then it is possible to define the approximate axiom:\n$C \\sqsubseteq \\forall i \\in F^+ F_i \\sqcap \\forall j \\in F^- \\neg F_j$\nwhich can be proposed to a domain expert for its validation and possible inclusion in the definition of the target class C. Note that only some of the basic features are considered, namely those strongly positively/negatively correlated with the membership, where the strength is quantified by \u03b8."}, {"title": "3.3 Fitting the Model", "content": "We assume the availability of a complete training set $T = (X, y) = \\{(x^t, y^t)\\}_{t=1}^N$ where $x^t$ is the encoding of an individual x \u2208 Ind(K) and $y \\in B$ indicates the actual membership to C.\nThe naive Bayes classifier can be trained by finding the maximum likelihood (or even the maximum a posteriori) estimate for the parameters $\u03a0 = \\{\\pi, p_1, p_0\\}$. As the probability for a single example is given by\n$P(x^t, y^t| \u03a0) = P(y^t| \u03c0_{y^t}) P(x^t|y^t, p_{y^t}) = \u03c0_{y^t}Ber(x^t|p_{y^t})$\nthen the log-likelihood $L(\u03a0) = log P(T|\u03a0)$ can be written:\n$L(T|\u03a0) = log \\prod_{t=1}^N P(x^t, y^t|\u03a0) = \\sum_{b \\in B} N_b log \u03c0_b + \\sum_{b \\in B} \\sum_{t:y^t=b} \\sum_{i=1}^D log Ber(x_{bi}|p_{bi})$"}, {"title": "4 Dealing with Incomplete Data", "content": "In the context of DL knowledge graphs it is possible that given a feature concept the membership of an individual cannot be ascertained logically because of the open-world semantics. In such non unlikely cases, some input feature $x_i$ or target feature $y_t$ would assume an indefinite value making T = (X, y) incomplete.\nA further assumption should be made about the cause of missing values for target and/or input features [9]. A MAR (Missing at Random) setting is likely not adherent to the ground truth about the data but it is adopted to keep the model simple.\nAlthough in principle the problem of determining the indefinite values of features could be treated homogeneously over T for missing values either in X or in y, we consider distinct cases tackled in subsequent phases:\n1. examples with an indefinite value for some input feature $x_i$;\n2. examples with an indefinite value for the target feature $y_t$."}, {"title": "4.1 Phase 1: Missing Input Values", "content": "A simple solution to the problem of missing values of input features is imputing a fixed value or a constant decided on a per feature basis (e.g. estimated as the mode or the mean of the column vector X) which may be also conditioned to a known value of the corresponding target feature $y_t$, as previously treated in the estimation of the parameters.\nAssuming that the missingness of a feature is not informative about its potential value, if parts of an input vector x were missing during training (and/or testing), then one may marginalize out such features [11]. Writing x as $(x_o, x_u)$, with $x_u$ sub-vector defined on the features with unknown (missing) values and $x_o$ defined on those with observed values, classification would amount to maximize:\n$P(y|x_o, \u03a0) \\propto \u03c0_y P(x_o|y, p_y) = \u03c0_y \\sum_{x_u \\in X_u} P(x_o, x_u |y, p_y) = \u03c0_y Ber(x_o |y, p_y)$"}, {"title": "4.2 Phase 2: Missing Target Values", "content": "As regards the second case, considering a training set T' with a completed X and a possibly incomplete y, the log-likelihood to be maximized would be:\n$L(\u03a0|T') = \\sum_{t=1}^N log \\sum_{b \\in B} \u03c0_b Ber(x^t|p_b)$"}, {"title": "5 A Hierarchical Bernoulli Model", "content": "In general, the features for a class are likely to be correlated then a single model for describing the instances' distribution may turn out to be inadequate when the assumption of conditional independence cannot be made, leading to an inaccurate classifier.\nThis problem can be tackled by considering a distribution modeled as a mixture of (multivariate) Bernoullis. Then, introducing an intermediate K-dimensional layer z of latent binary indicator variables corresponding to the nodes {$z_k$}$_{k \\in [1:K]}$, characterized by a different Bernoulli $P_k(x|z_k)$, a 2-tier model is defined whose structure is depicted in Fig. 3. This network, that will be referred to as Hierarchical Bernoulli Model (HBM), combines the mixture of multivariate Bernoullis (bottom-tier) with the former MBNB classifier (top-tier).\nAlternatively, this model can be also described in the form of a Hierarchical Mixture of Experts [8]."}, {"title": "5.1 Classification", "content": "To decide the membership to C for an input individual x with corresponding Boolean vector x, using Eq. (6), the posterior is determined as:\n$P(y = 1|x; \u03a0) = \\frac{P(x|y = 1; \u03a0)P(y = 1)}{P(x)} = \\frac{\\pi \\sum_{k=1}^K \u03bc_k Ber(x|p_k)}{\\pi \\sum_{k=1}^K \u03bc_k Ber(x|p_k) + (1 - \\pi) \\sum_{k=1}^K \u03bc_k Ber(x|p_k)}$ (7)\nThe decision procedure is analogous to (4). Alternatively we could use disjunctive and conjunctive classification rules translating the hierarchical model."}, {"title": "5.2 Disjunctive Definitions / Rules", "content": "Given a MMBM, an axiom for class C may be extracted by defining the class in terms of the components:\n$C \\sqsubseteq \\bigcup_{k=1,...,K:} z_k=1 C_k$\nwhich can be specified probabilistically indicating the class prior \u03c0and $\u03bc_k$ such that $z_k$ = 1 or, alternatively, also as a disjunctive classification rule for C and a set of probabilistic rules for the $C_k$'s as shown in Fig. 4.\nFor the sake of interpretability, these rules may be simplified considering, for each i, only the more likely sub-case with a minimal threshold to be exceeded. A class definition can be given also for the complement \u00acC in terms of the sub-classes $C_k$ corresponding to the cases when $z_k$ = 0."}, {"title": "5.3 Fitting the Model", "content": "The network parameters can be determined using a training set T = (X, y). The two tiers can be trained separately: the bottom one tackles an unsupervised learning task, while the top tier is essentially a MBNB classifier.\nAs regards the bottom-tier of the structure, the parameters for the mixture model can be learned through a EM procedure with the cases of incomplete input data treated similarly to the Phase 1, described in Sect. 4.\nIn this case, given the model parameters \u03a0, the complete log-likelihood to be maximized is:\n$\\mathcal{L}(I, Z|X) = log \\prod_{t=1}^N P(x^t| II) = \\prod_{t=1}^N log [ \\sum_{k=1}^K \\mu_k Ber_D(X_t|P_k) ]$ (8)\nNote that it also targets the latent variables z.\nand the EM steps can be defined as follows:\nE-step: the responsibility of each component of the mixture is estimated as:\n$\\widehat{P}(z_t^k|x_t^t) \\longleftarrow \\frac{\\mu_k P_k(x^t|p_k)}{\\sum_{h=1}^K \\mu_h P_h(x^t|p_h)}$\nM-step: the parameters values are updated on the ground of the new estimates for $P(z_t^k|x_t^t)$ along the following rules:\n$\\widehat{p_k} \\longleftarrow \\frac{\\sum_{t=1}^N \\widehat{P}(z_t^k|x_t^t)x_t}{\\sum_{t=1}^N \\widehat{P}(z_t^k|x_t^t)} $\n$\\widehat{\\mu_k} \\longleftarrow \\frac{1}{N}\\sum_{t=1}^N \\widehat{P}(z_t^k|x_t^t)$\nAlternatively one may adopt a more sophisticate solution based on the Bayesian treatment of the parameters (using variational inference or Dirichlet processes [11]).\nThe top-tier MBNBM is trained as described in Sect. 4."}, {"title": "6 Experiments", "content": "We briefly present how the models involved in the experiments have been implemented, then we describe the experimental setup (datasets, problems, hyperparameters setup) and finally we discuss the results of this empirical evaluation."}, {"title": "7 Conclusions and Possible Extensions", "content": "In order to better tackle the inherent incompleteness of the DL knowledge graphs, we extended the methods to generate simple probabilistic classifiers leveraging on the semantics of basic logic features regarded as discrete random variables. These models are especially suitable for problems related to incomplete data, which is the case of the DL KGs. Besides, they can be converted into probabilistic rules or axioms with a straightforward interpretation in terms of the terminology employed by the KG, hence enforcing the model interpretability. An experiment that tested the effectiveness of these models in comparison with a simple discriminative one (logistic regression) adopted as a baseline, proved them effective and worth of further investigations.\nA number of limitations would require our attention. KGs containing plenty of classes and properties may offer numerous features for the initial feature selection phase which is unsupervised. Better supervised methods may be considered to elicit a limited number of important features. Complex concept definitions may turn out to be harder to learn, especially when nested restrictions are considered although such definitions are hardly found in KGs. The hierarchical model in its current implementation would require a more careful search of optimal values for other hyperparameters. This also calls for experiments involving larger datasets in terms of the number of individuals.\nOn the ground of similar works on mixture models [6,12,4] various extensions are possible along different lines including:\n*   the incorporation of existing rules or axioms in the probabilistic models;\n*   a different classification setting, considering decision procedures that admit rejection cases, to better comply with the original semantics;\n*   a deeper investigation on the adoption of a Bayesian approach to adapt the structure of the models (and parameters fitting) considering suitable priors;\n*   the integration of continuous (Gaussian) features to include in the model also restrictions on numerical datatypes.\nApplications of the probabilistic classifiers to various other tasks are possible, such as KG debugging, e.g. for anomaly detection, and knowledge refinement, e.g. using these models for axiom discovery, etc."}]}