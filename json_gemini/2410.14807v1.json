{"title": "ALIGNING AI AGENTS VIA\nINFORMATION-DIRECTED SAMPLING", "authors": ["Hong Jun Jeon", "Benjamin Van Roy"], "abstract": "The staggering feats of AI systems have brought to attention the topic of AI Alignment: align-\ning a \u201csuperintelligent\u201d AI agent's actions with humanity's interests. Many existing frame-\nworks/algorithms in alignment study the problem on a myopic horizon or study learning from human\nfeedback in isolation, relying on the contrived assumption that the agent has already perfectly iden-\ntified the environment. As a starting point to address these limitations, we define a class of bandit\nalignment problems as an extension of classic multi-armed bandit problems. A bandit alignment\nproblem involves an agent tasked with maximizing long-run expected reward by interacting with\nan environment and a human, both involving details/preferences initially unknown to the agent.\nThe reward of actions in the environment depends on both observed outcomes and human prefer-\nences. Furthermore, costs are associated with querying the human to learn preferences. Therefore,\nan effective agent ought to intelligently trade-off exploration (of the environment and human) and\nexploitation. We study these trade-offs theoretically and empirically in a toy bandit alignment prob-\nlem which resembles the beta-Bernoulli bandit. We demonstrate while naive exploration algorithms\nwhich reflect current practices and even touted algorithms such as Thompson sampling both fail\nto provide acceptable solutions to this problem, information-directed sampling achieves favorable\nregret.", "sections": [{"title": "1 Introduction", "content": "The staggering progress of artificial intelligence across the past decade has far exceeded anyone's expectations. With\nAI agents toppling grand challenges ranging from unfathomably complex decision problems such as Go, to interacting seamlessly with humans via language (Achiam et al., 2023), uncertainty about the future capa-\nbilities of theses systems looms ahead. As these agents begin to surpass humans in both knowledge and capabilities,\nthe question of alignment becomes crucial: how do we ensure that these superintelligent agents have goals which are\naligned with those of humanity? To mitigate the unintended (potentially catastrophic) consequences of deploying a\nmisaligned superintelligence, much work is required on both the theoretical and algorithmic fronts.\nMany global corporations have taken this maxim to heart and perform an explicit \u201cpost-training\" phase to better\n\"align\" their models with the goals of the human end user prior to deployment (Ouyang et al., 2022; Achiam et al.,\n2023; Team et al., 2023; Dubey et al., 2024). A natural question is: what are the limitations of the current alignment\nprotocols? A key concern which we highlight in this work is that this paradigm follows an explicit \u201cexplore then\nexploit\" pattern. Information is first gathered without an explicit sensitivity to costs and then it is leveraged without\nfurther exploration at deployment. When the algorithm is deployed in such a manner for a sufficiently long horizon,\nthe concerns surrounding alignment amplify. What to do in the almost sure scenario that fatal edge cases are omitted\nin the \"explore\" phase? A natural answer is to repeat this pattern of \u201cexplore then exploit\" in a periodic fashion ad\nnauseam. However, when analyzing this procedure in even simple problem settings, we demonstrate that it is highly\nsuboptimal. This proposition becomes all the more relevant as the scale at which theses systems operate continues to\ngrow. We believe that these protocols arise from a framing of alignment which does not correctly account for cost\nand reward across a long horizon. In this work, we demonstrate that by correctly attributing costs to querying humans\nabout their preferences and rewards to achieving aligned outcomes, an agent which aims to maximize reward (cost is\nnegative reward) will exhibit behavior that we would desire from a superintelligent agent.\nWe are not the first to propose frameworks for alignment. Notably, cooperative inverse reinforcement learning (CIRL)\ndefines a broad class of problems which attempt to capture the essence of alignment.\nA CIRL problem is a 2 player markov game involving a human and a robot in which the ladder does not know the\nreward. We levy two main criticisms of this work 1) the human (and robot) have full knowledge of the environment\nand 2) the POMDP formulation obfuscates the search for scalable algorithms. In the reality, neither humanity nor the\nsuperintelligent agent will be able to fully identify the environment. Part of the process of aligning superintelligent\nagents ought to involve these agents accruing knowledge about the external environment beyond what it is bestowed\nat initialization, and beyond what is known to humanity. As a result, in our formulation, we view the environment\nand the human as separate systems, both of which the agent must simultaneously explore while also accruing enough\nreward.\nWhile CIRL problems fall into a class of POMDP problems, we argue that this framing often confines the practitioner\nto considering only algorithms which involve (approximately) solving POMDPs. While even simple multi-armed\nbandit problems can be formulated as POMDPS, POMDP solvers are hardly a practitioner's algorithm of choice (due\nto scalability). Algorithms such as Thompson sampling (Thompson, 1933), upper confidence bounds and information-directed sampling have been demonstrated to perform well\nin bandit and reinforcement learning environments despite the fact that they perform somewhat heuristic exploration\nschemes when compared to solving a POMDP with a complex belief state. Moreover, advances in scaleable uncer-\ntainty modeling via methods such as epinet enable efficient approximate variants of TS and IDS\nfor complex problems involving neural networks. The accelerated prototyping afforded by these scaleable algorithms\nought to facilitate advances in AI alignment on both algorithmic and theoretical fronts.\nIn this paper, we define a class of problems for the purposes of studying alignment of AI agents. This class of bandit\nalignment problems builds off of the hidden utility\nbandit formulation of Freedman et al. (2023). We provide one\nsimple problem instance which we refer to as the beta-Bernoulli bandit alignment problem and provide a thorough\ntheoretical and empirical analysis. As its name suggests, it reflects the classic beta-Bernoulli bandit from the multi-\narmed bandit literature. Despite its simplicity, the environment exhibits sufficient complexity to exhibit fascinating\ntradeoffs between exploiting accumulated information, exploring the environment, and learning about the human's\npreferences. We demonstrate that naive \u201cexplore then exploit\u201d algorithms are unable to achieve adequate performance\nin this environment, exhibiting regret lower bounds which are linear in the size of the action set and the time horizon.\nFurthermore even Thompson sampling fails to produce a result which is sublinear in the horizon. However, we\ndemonstrate theoretically that Information Directed Sampling achieves regret which is\nsublinear in both the action set size and the horizon. Our results demonstrate the importance of designing algorithms\nwhich explore in a reward-sensitive manner. We believe this work provides many exciting directions of future work in\nAI alignment, both theoretical and algorithmic."}, {"title": "2 Probabilistic Framework", "content": "We define all random variables with respect to a common probability space (\u03a9, F, P). Recall that a random variable\nF is a measurable function \u03a9 \u2192 F from the sample space \u03a9 to an outcome set F.\nThe probability measure P : F \u2192 [0, 1] assigns likelihoods to the events in the \u03c3\n algebra F. For any event E \u2208 F,\nP(E) to denotes the probability of the event. For events E, G\u2208 F for which P(G) > 0, P(EG) to denotes the\nprobability of event E conditioned on event G.\nFor realization z of a random variable Z, P(Z = z) is a function of z. We denote its value evaluated at Z by\nP(Z). Therefore, P(Z) is a random variable (it takes realizations in [0, 1] depending on the value of Z). Likewise for\nrealizations (y, z) of random variables Y, Z, P(Z = z|Y = y) is a function of (y, z) and P(Z|Y) is a random variable\nwhich denotes the value of this function evaluated at (Y, Z)."}, {"title": "3 Problem Formulation", "content": "We begin with the problem formulation for a bandit alignment problem. We draw inspiration from the hidden-utility\nbandit Freedman et al. (2023).\n\u2022 Ae denotes a finite set of environment actions."}, {"title": "4 Beta-Bernoulli Bandit Alignment Problem", "content": "While we provide a general definition of bandit alignment problems, theoretical and empirical development necessi-\ntates thoughtful instantiations. In this section, we outline a concise yet complete alignment bandit problem modeled\nafter the classic beta-Bernoulli bandit. By complete, we mean that it exhibits all of the aforementioned tradeoffs. We\nbelieve that this problem is an exciting starting point for theorists and practitioners to ideate on algorithmic solutions\nto AI alignment."}, {"title": "4.1 Problem Definition", "content": "The beta-Bernoulli bandit alignment problem is an instance of the bandit alignment problem discussed in the section\nprior. Therefore, the problem can be defined by specifying the tuple (Ae, Ah, O, \u03c1\u03c6, \u03c1\u03b8, R):"}, {"title": "4.2 Remarks", "content": "We begin with a high-level description of the problem. The problem exhibits N bandit arms represented by Ae, each\nwith a beta-Bernoulli outcome. Associated with each bandit arm a is a query a which provides information pertaining\nto the human's preference da of said arm. We now provide some intuition for the reward function. Note that\n$E[R(A_t, O_{t+1}, \\theta)|\\phi, \\theta, A_t = a]$\n$T = [\\phi_a, 1 - \\phi_a] [\\theta_{\\bar{a}}, 1 - \\theta_{\\bar{a}}].$\nTherefore, the action which maximizes reward is the one for which the environment [a, 1 \u03a6\u03b1] outcomes and the\nhuman preferences [\u03b8\u0101, 1 \u2013 \u03b8\u0101] are most aligned. We do not make any claims as to the reasonability of this reward\nfunction beyond the scope of this problem. We simply use it as sufficient example to elucidate the inherent challenges\npresent in aligning AI agents.\nFor the standard beta-Bernoulli bandit problem, an optimal (and tractable) algorithm exists via Gittins indices (Gittins,\n1979). However, the beta-Bernoulli bandit alignment problem does not satisfy the conditions necessary for tractable\ncomputation of Gittins indices. Furthermore, since we hope for this theory to extend to complex problem instances\ninvolving neural networks, we restrict our attention to algorithms for which there exist scaleable extensions for neural\nnetworks. However, a thorough analysis of optimal performance in the beta-Bernoulli bandit problem to establish\nlower bounds is an interesting direction for future theoretical work.\nA subtlety which becomes apparent after some thought is that an agent must not over-explore. Over-exploration can\noccur across two axes in this problem: 1) over-querying the human, 2) over-exploring the environment. 1) manifests"}, {"title": "5 Insufficiency of Standard Methods", "content": "In this section, we will demonstrate theoretically that existing algorithms prevalent in the literature fail to provide\nsufficient solutions to the beta-Bernoulli bandit alignment problem. A solution is insufficient if its regret is \u03a9(\u03a4) or\n\u03a9(|A|). We begin with a class of naive exploration algorithms referred to as \u201cexplore then exploit\u201d and then move\nonto Thompson sampling, an effective algorithm in the standard beta-Bernoulli bandit environment."}, {"title": "5.1 Explore then Exploit", "content": "For all t, an agent \u03c0must decide which action a to take based off the information it has already accrued Ht. The\nreward-greedy agent is one which greedily optimizes reward based on Ht:\n$A_t = arg max_{a\\E A} E[R_{t+1}|H_t, A_t = a].$\nHowever, it is easy to see that such algorithms are susceptible to self-perpetuating feedback loops of only repeatedly\nselecting the actions for which it already has accrued information about. Meanwhile, the information-greedy agent is\none which acts greedily optimizes information acquisition base on Ht:\n$A_t = arg max_{a\\E A} I(O_{t+1};\\theta, \\phi|H_t = H_t, A_t = a).$\nParadigms such as A/B testing, or apprenticeship learning design explicit information-greedy phase followed by a\nreward-greedy phase. We group these methods under the umbrella of \u201cexplore then exploit\u201d agents.\nAn \"explore then exploit\u201d agent is identified by + such that for t \u2264 \u03c4, \u03c0is the information-greedy agent and for t > T,\n\u03c0 is the reward-greedy agent. However, we have the following result which demonstrates\nTheorem 3. (regret bound) If \u3160 is an \u201cexplore then exploit\" agent, then\n$R(\\pi, T) = \\Omega(|A| + T).$"}, {"title": "5.2 Thompson Sampling", "content": "In the standard beta-Bernoulli bandit, Thompson sampling (TS) is an efficient algorithm has offers strong theoretical\nguarantees. In that problem setting, TS achieves regret which is $O(\\sqrt{T}|A|$, i.e. sublinear in both the horizon T and\nthe size of the action set A. Therefore, it is a natural algorithmic choice for our beta-Bernoulli bandit alignment\nproblem. We demonstrate that despite its merits in the standard bandit environments, TS produces incoherent behavior\nin bandit alignment problems.\nWe first outline the Thompson sampling agent, which we denote by \u03c0ts. For all t \u2208 Z++,\n$\\pi_{ts}(H_t) = P(A^* = a|H_t),$\nwhere $A^* = arg max_{a\\in A} E[R_{t+1}|\\theta, \\phi, A_t = a]$. However, we notice that for any action \u0101 \u2208 Ah,\n$P(A^* = \\bar{a}|H_t) = 0.$\nThis is because actions which query the human incur a reward of -1 whereas all environment actions have non-\nnegative expected reward. As a result, TS will never query the human. Some simple calculations provide the following\nresult.\nLemma 4. For all t \u2208 Z+, let \u03c0ts\n(t) (.) denote the Thompson sampling agent at time t, then\n$\\pi_{ts}^{(t)} = \\begin{cases} \\\n\\frac{1}{|A_e|} & a.s. \\\\ Ac\\\\ 0 & if a \\in A_h\\end{cases}.$\nLemma 4 establishes that for all t \u2208 Z+, the TS agent selects actions uniformly at random from Ae. As a result, this\nalgorithm incurs linear regret even as T \u2192 8.\nTheorem 5. (ts regret bound)\n$R(\\pi_{ts}, T) = \\Omega(T).$\nThe issue with Thompson Sampling (and other common bandit algorithms such as UCB) is that they only sample\nan action if it is statistically plausible that it is optimal. So while these algorithms are reward-sensitive, they are\ninformation-blind. In the following section, we will analyze information directed sampling, an algorithm which over-\ncomes these hurdles by being simultaneously reward and information sensitive."}, {"title": "6 Information Directed Sampling", "content": "We have demonstrated that Thompson sampling fails to demonstrate any useful behavior in the beta-Bernoulli bandit\nalignment problem. However, in this section, we demonstrate that Information-Directed Sampling (IDS) (Russo and\nVan Roy, 2014) does produce favorable theoretical regret guarantees. We begin by outlining the IDS algorithm."}, {"title": "6.1 Algorithm", "content": "We begin by defining the conditional information ratio. For all t \u2208 Z+, let the conditional information ratio of an\nagent at time t be:\n$\\Gamma_t(\\pi) = \\frac{E_{\\pi} [R^* - R_{t+1} |H_t]^2}{I(\\theta, \\Phi; (A_t, O_{t+1}) |H_+ = H_t)}.$"}, {"title": "6.2 Regret Bound", "content": "By Theorem 6, upper bounds on the sum of expected information ratios and the mutual information I(\u03b8, \u03c6; HT)\nprovide an upper bound on regret. In this section, we derive a regret upper bound for IDS via upper bounding the two\naforementioned quantities. Proofs can be found in the appendix. We begin with the information ratio:\nLemma 7. (ids information ratio bound) For all T \u2208 Z+,\n$\\sum_{t=0}^{T-1} E_{\\pi_{ids}} [\\Gamma_t(\\pi_{ids})] \\leq 33\\sqrt{|A|}T^3.$\nWe now provide an upper bound on the mutual information.\nLemma 8. (mutual information bound) For all T \u2208 Z++,\n$I(H_T; \\theta, \\phi) \\leq |A|ln(4T).$\nWith these in place, we present the main result.\nTheorem 9. (ids regret bound) For all T \u2208 Z+,\n$R(\\pi_{ids}, T) \\leq \\sqrt{33ln(4T)|A|T}.$\nTheorem 9 follows directly from Theorem 6 and Lemmas 7 and 8. Notably, the regret upper bound is O(|A|3/4T3/4)\nignoring log factors, hence sublinear in |A| and T. By seeking information in a reward-sensitive manner, IDS is able\nto escape the pitfalls of \"explore then exploit\" agents and TS which only prioritize at most one of the two at a time.\nIn the standard beta-Bernoulli bandit environment, both TS and IDS achieve regret bounds which are $O(\\sqrt{|A|}T)$.\nTherefore, there is a considerable gap between the upper bounds for the standard bandit and bandit alignment problems.\nWe conjecture that IDS can actually match the $O(\\sqrt{|A|}T)$ performance and therefore the gap can be reduced via\nimproved analytic techniques. The empirical results of the following section appear to corroborate this conjecture."}, {"title": "7 Empirical Results", "content": "In this section, we provide empirical results which compare IDS and \u201cexplore then exploit\u201d agents in the beta-Bernoulli\nbandit alignment problem. The results corroborate our theoretical analysis and as mentioned, IDS appears to perform\nsignificantly better than what is suggested by Theorem 9."}, {"title": "7.1 Experimental Details", "content": "We evaluated IDS and two \u201cexplore then exploit\u201d algorithms (\u03c4 = 3200,16000). We selected an environment with\n|Ae| = 16 arms and averaged the cumulative regret of each method across 10 random problem instances (with each\n\u03b8\u03b1, \u03c6\u03b1 drawn randomly from the beta prior). The results if this experiment are presented in figure 2."}, {"title": "7.2 Discussion", "content": "Figure 2 demonstrates that as suggested by Theorem 3, the regret of \u201cexplore then exploit\u201d agents grows linearly in the\nhorizon. This indicates that even when each action is explored 100 or 500 times (for r = 3200, 16000 respectively),\nthere is still a significant number of instances in which the optimal action is not identified. Furthermore, significant\ncost is incurred in the process of exploring the environment in a reward-blind fashion. As apparent in figure 3, even\nafter the exploration phase, the agent is unable to identify the best action. The figure plots the regret of each agent\nvs t with both axes in logarithmic scale. The reference dotted line depicts t regret as the slope is 1. Evidently, the\nperformance of the \u201cexplore then exploit\u201d agents is \u03a9(t) which indicates that the the performance gap between these\nagents and the IDS agent will continue to widen as t grows."}, {"title": "8 Conclusion", "content": "In this paper we elucidate some of the challenges we will encounter in our efforts to align Al systems. Some of these\nchallenges including simultaneous exploration of the environment and human preferences and accounting of cumula-\ntive reward/regret had not been addressed prior to this work. We provide a simple yet complete problem instance which\nwe believe can serve as both a testbed for theoretical and empirical research in AI alignment. Theoretical questions\npertaining to tightening the results of Theorem 9 and empirical questions about discovering other effective algorithms\nserve as fascinating directions for future research."}, {"title": "A Proof of General Regret Bound", "content": "We now establish a regret bound in terms of the information ratio. Recall the conditional information ratio:\n$\\Gamma_t = \\frac{E_{\\pi} [R^* - R_{t+1} |H_t]^2}{I(\\theta, \\Phi; (A_t, O_{t+1}, A_t) |H_t = H_t)}.$\nThen, let the unconditional information ratio be:\n$\\Gamma_t = \\frac{E_{\\pi} [R^* - R_t]^2}{I(\\theta, \\Phi; (A_t, O_{t}, A_t) |H_t)}.$\nThe following result bounds the regret of an agent \u03c0 via its conditional information ratio:\nTheorem 6. (information ratio regret bound) For all T \u2208 Z++ and algorithms \u03c0,\n$R(\\pi, T) \\leq \\sum_{t=0}^{T-1} E_{\\pi} [\\Gamma_t(\\pi)]\\cdot \\sqrt{I(\\theta, \\phi; H_T)}.$\nProof. Let It = \u0395\u03c0[\u0393\u03b5(\u03c0)]. Then\n$\\sum_{t=0}^{T-1} E_{\\pi} [R^* - R_t] = \\sum_{t=0}^{T-1} (E_{\\pi} [R^* - R_t])^2$\n$= \\sum_{t=0}^{T-1} \\Gamma_t \\cdot I(\\theta, \\Phi; A_t, O_{t},A_t |H_t)$\n$\\leq \\sum_{t=0}^{T-1} \\Gamma_t \\cdot \\sum I(\\Theta, \\phi; A_t, O_t,A_t|H_t)$\n$\\leq \\sum_{t=0}^{T-1} \\Gamma_t \\cdot \\sqrt{I(\\theta, \\phi; H_T)}$\n$= \\sum_{t=0}^{T-1} E_{\\pi} [ \\frac{E_{\\pi} [R^* - R_t |H_t]^2] } {I(\\theta, \\Phi; A_t, O_t, A_t |H_t = H_t)}\\cdot \\sqrt{I(\\theta, \\phi; H_T)}$\n$\\leq \\sum_{t=0}^{T-1} E_{\\pi} [ \\frac{E_{\\pi} [R^* - R_t |H_t]^2}{I(\\theta, \\Phi; A_t, O_t, A_t |H_t = H_t)} ]\\cdot \\sqrt{I(\\theta, \\phi; H_T)}$\n$=\\sum_{t=0}^{T-1} E_{\\pi} [\\Gamma_t] \\cdot \\sqrt{ I(\\theta, \\phi; H_T)}$\nwhere (a) follows from Cauchy Bunyakovsky Schwarz, (b) follows from the chain rule of mutual information, (c)\nfollows from the fact that for any real-valued random variable X, E[X]2 < E[X2], and (d) follows from Jensen's\ninequality and the fact that quadratic over linear is convex."}, {"title": "B Proof of Information-Theoretic Quantities", "content": "We now provide results which upper and lower bound the mutual information of observations and 0, $ for our beta-\nBernoulli bandit alignment problem. We start with this general result for beta and bernoulli distributed random vari-\nables:"}, {"title": "B.1 Mutual Information Bounds for Beta-Bernoulli Random Variables", "content": "Lemma 10. For all a, \u1e9e, t \u2208 Z++, if P(0 \u2208 \u00b7) = Beta(\u03b1, \u03b2) and X ~ Bernoulli()), then\n$\\frac{1}{4(\\alpha + \\beta)} \\leq I(X;0) \\leq \\frac{1}{2(\\alpha + \\beta)}$\nProof.\n$I(X; 0) = h(0) \u2013 h(0|X)$\n$=\\ln \\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)} - [\\frac{\\alpha}{\\alpha + \\beta} \\ln \\frac{\\Gamma(\\alpha + 1) \\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta + 1)} + \\frac{\\beta}{\\alpha + \\beta} \\ln \\frac{\\Gamma(\\alpha) \\Gamma(\\beta + 1)}{\\Gamma(\\alpha + \\beta + 1)}]$\n$= \\ln \\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)} - [\\frac{\\alpha}{\\alpha + \\beta} [\\psi(\\alpha + 1) - (\\beta - 1)\\psi(\\beta) + (\\alpha + \\beta - 1)\\psi(\\alpha + \\beta + 1)]$\n$\\qquad + \\frac{\\beta}{\\alpha + \\beta} [(\\alpha \u2212 1)\\psi(\\alpha) \u2212 (\\beta )\\psi(\\beta + 1) + (\\alpha + \\beta \u2212 1)\\psi(\\alpha + \\beta + 1)]]$\n$\\frac{\\alpha}{\\alpha + \\beta} [\\psi(\\alpha + 1) \u2212 (\\beta - 1)\\psi(\\beta) + (\\alpha + \\beta - 1)\\psi(\\alpha + \\beta + 1)]$\n$= \\ln(\\alpha + \\beta) + \\ln \\Gamma(\\alpha) \\Gamma(\\beta) - \\ln \\Gamma(\\alpha + 1)\\Gamma(\\beta)$ $\\frac{\\alpha}{\\alpha + \\beta} \\ln \\Gamma(\\alpha + 1) \\Gamma(\\beta)$ $\\frac{\\beta}{\\alpha + \\beta} \\ln \\Gamma(\\beta + 1)$\n$= \\ln(\\alpha + \\beta) -lna \u2212 (\\alpha \u2013 1)\\psi(\\alpha) + $\n \u2212 (\u03b1 \u2212 1)\\psi(\u03b1) + \u03b2 \u03b1 +\u03b2\u03b1 (\u03c8(\u03b1) +\n$\\qquad \u2212 (\\beta )\\psi(\\beta) + \u03b2 \u03b1 +\u03b2\u03b1 (\u03c8(\u03b2) +\n\u2212\u2212(\u03b1\u22121)\u03c8(\u03b1) \u2212 (\u03b1\u22121)\u03c8(\u03b1) +\u03b1+\u03b2\u03b1 (\u03c8(\u03b1) +\n$\\frac{\\alpha}{\\alpha + \\beta} \u2212 \\frac{\\beta}{\\alpha + \\beta} = \\ln(\\alpha + \\beta) -lna \u2212 (\\alpha \u2013 1)\\psi(\\alpha) + $\n$\\qquad+ + \\beta (\\alpha + \\beta \u2212 2)\\psi(\\alpha + \\beta) \u2212 (\\alpha + \\beta \u2212 1)\\psi(\\alpha + \\beta + 1)\n$ + \\alpha + \\beta = \\ln(\\alpha + \\beta) + \\frac{\\beta}{\\alpha + \\beta} \\ln \u03b2 \u2212 \\psi(\\alpha + \\beta) \u2212 (\\alpha + \\beta) \\qquad+ + \\beta (\\alpha + \\beta \u2212 2)\\psi(\\alpha + \\beta) \u2212 (\\alpha + \\beta \u2212 1)\\psi(\\alpha + \\beta + 1)\n=ln(\u03b1+\u03b2) - \u03b2 + \u03b2 \u03b1 + \u03b2\u03c8(\u03b2)\n=ln(\u03b1+\u03b2) + \u03b1 \u03b1 + \u03b2\u03c8(\u03b2)\n+\\beta \\ln (\u03b1 + \u03b2) - \\frac{1}{\u03b1} -\u03b1\\beta-1+ +ln(\u03b1 + \u03b2) + \u03b1 \u03b1 + \u03b2\u03c8(\u03b2)- +\n+\\frac{\\alpha}{\\alpha + \\beta} {\\psi(\\alpha ) \u2212 \\ln a} +\n+\\frac{\\alpha}{\\alpha + \\beta} +{\\psi(\\alpha ) \u2212 \\ln \u03b2}$\n=ln(\u03b1+\u03b2)\u03b1 \u03b1 + \u03b2\u03b2ln\u03b2-\u03c8(\u03b1+\u03b2)\u03b1+\u03b2\n+ln(\u03b1+\u03b2)\u03b1 \u03b1 + \u03b2 \u03c8(\u03b2)\nBegin With The Lower Bound:\n1\u03b1+\u03b2\u03c8(\u03b2)\u03b2 ln\u03b2 1+\u03b2\u03c6(\u03b1)-ln\u03b1+\u03b1\u03b1+\u03b2\u03c8(\u03b1)-ln\u03b2\u03b2\n$=ln(\u03b1+\u03b2)\u03b2-\u03c8(\u03b1+\u03b2)\u03b1+\u03b2(\u03c8(\u03b1)-In\u03b2)+\n- \u03b2 +\u03b2\u03c6(\u03b1)-ln\u03b1+\n- \u03b2 In\u03b1+ +ln(\u03b1 + \u03b2)\n+\u03b2In\u03b1-+\nWhere Follows From Fact That That The Maximized\nI (Oa\u03b8\u03c6| HtAt \u03b1)= h(Ht At \u03b1)-ln\u03b1\nWhere (a) follows from the fact that the expression is maximized for a = \u03b2 = (\u03b1 + \u03b2)/2 and (b) can be shown in the\nlimit as a + \u03b2 \u2192 \u221e and numerically for small a + \u03b2. The lower bound can be demonstrated numerically. The result\nfollows.\n$I(O_\\alpha; \\theta, \\phi|H_t = H_t, A_t = a) = h(\\theta, \\phi|H_t = H_t, A_t = a) \u2013 h(\\theta, \\phi|H_t = H_t, A_t = a, O_a)$"}, {"title": "B.2 Bounding Environment and Human Information", "content": "In Theorem 6, the second term on the RHS cannot simply be upper bounded by H(0, $) since 0 and $ are continuous\nrandom variables. As a result, we have the following series of results which upper bound I(HT; 0, $) via rate-distortion\ntheory as in Jeon and Roy (2024). The following result defines a particular lossy compression of 0, & and bounds its\ndistortion.\nLemma 12. For all t \u2208 Z++ and \u0454 \u2208 [0,0.25], if Se is an e-cover of the interval [0,1] w.r.t the L1 norm, if for all\na \u2208 Ae, a \u2208 Ah,\n$\\{\\begin{aligned} &\\hat{\\phi}_{a}=\\begin{cases} \\min\\left\\{s \\in S_{\\epsilon}: \\phi_{a} \\leq s \\leq 0.5\\right\\} \\\\ \\nu & \\text { if } \\phi_{a} \\leq 0.5 \\\\ \\max \\left\\{s \\in S_{\\epsilon}: 0.55\\right\\} \\\\ \\nu & \\text { if } \\phi_{a} \\geq 0.5 \\end{cases} \\\\\\ & \\hat{\\theta}_{a}=\\begin{cases} \\min \\left\\{s \\in S_{\\epsilon}: \\theta_{a} \\leq s \\leq 0.5\\right\\} \\\\ \\nu & \\text { if } \\theta_{a} \\leq 0.5 \\\\ \\max \\left\\{s \\in S_{\\epsilon}: 0.55\\right\\} \\\\ \\nu & \\text { if } \\theta_{a} \\geq 0.5 \\end{cases}\\end{aligned}$\nthen\n$I(O_{t+1}, A_{t}; \\theta, \\phi | \\hat{\\theta}, \\hat{\\phi}, H_{t}) \\leq 2 \\frac{1}{\\epsilon^{2}} \\ln \\frac{1}{\\epsilon^{2}}$\nProof.\n$I(O_{t+1}, A_{t}; \\theta, \\phi H_{t})$ \u2264 I(O_{t+1}, A_{t}; \u03b8, \u03c6 |\u03b8, \u03b4)$\n$I(O_{t+1}; \\theta, \\phi \\hat{\\theta}, \\hat{\\phi}, A_{t}) \\leq I(O_{t+1}; \\phi_{a} | \\phi_{a}< \\phi_{a}, A_{t} = a)$\n$=E[-\\phi_a \\ln \\frac{\\phi_a}{\\hat{\\phi}_a} \u2212(1\u2212\\phi_a )ln\\frac{1\u2212\\phi_a}{1\u2212\\hat{\\phi}_a }  ]$\n$=5[\\mathbb{I}[\\phi_a >5] =1]$\nwhere (a) follows from symmetry of a \u1e9e(1,1) distributed random variable, (b) follows from the fact that ln(x)\n1- 1/x, and (c) follows from the fact that \u0454 \u2264 0.25.\nWe can now upper bound the mutual information \u2161(0, \u0444; HT) via standard techniques involving rate-distortion theory."}, {"title": "C Bounding the Information Ratio", "content": "Theorem 6 establishes that to upper bound regret for an agent \u03c0", "E\u03c0[\u0393\u03b9(\u03c0)": "for that agent.\nIDS minimizes \u0393t uniformly over all histories Ht. Therefore", "conditions": "n$A_t = \\begin{cases} a & w.p. 1 - \\epsilon \\\\ a_{h} & w.p. \\epsilon \\end{cases}$\nNote that e is allowed to depend on T since we are simply trying to upper bound the information ratio. We now bound\nthe regret of IDS by bounding the expected conditional information ratio of the above algorithm. We will refer to this\nalgorithm as \u03c0.\nFor all a \u2208 A, let Oa denote a random variable with distribution Bernoulli(a) if a \u2208 Ae and Bernoulli(0) if\na \u2208 Ah. Let Ra = Oa \u00b7 O\u0101 + (1 \u2212 Oa) \u00b7 (1 \u2013 O\u0101) for a \u2208 Ae.\nLemma 14. For all t \u2208 Z+,\n$\\mathbb{E}_{\\pi}[\\frac{\\left[R^{*}-\\mathbb{E}_{\\pi}\\left[R_{t} \\mid \\mathcal{H}_{t}\\right", "n\\right": {"mathcal{H}_{t}\\right": "mathbb{E}\\left[R_{a} \\mid \\mathcal{H}_{t}\\right"}, "mathcal{H}_{t}\\right": "mathbb{E}\\left[R_{a} \\mid \\mathcal{H}_{t}\\right"}]}