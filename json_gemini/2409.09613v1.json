{"title": "Rethinking KenLM: Good and Bad Model Ensembles\nfor Efficient Text Quality Filtering in Large Web Corpora", "authors": ["Yungi Kim", "Hyunsoo Ha", "Sukyung Lee", "Jihoo Kim", "Seonghoon Yang", "Chanjun Park"], "abstract": "With the increasing demand for substantial\namounts of high-quality data to train large lan-\nguage models (LLMs), efficiently filtering large\nweb corpora has become a critical challenge.\nFor this purpose, KenLM, a lightweight n-gram-\nbased language model that operates on CPUs, is\nwidely used. However, the traditional method\nof training KenLM utilizes only high-quality\ndata and, consequently, does not explicitly learn\nthe linguistic patterns of low-quality data. To\naddress this issue, we propose an ensemble ap-\nproach that leverages two contrasting KenLMs:\n(i) Good KenLM, trained on high-quality data;\nand (ii) Bad KenLM, trained on low-quality\ndata. Experimental results demonstrate that\nour approach significantly reduces noisy con-\ntent while preserving high-quality content com-\npared to the traditional KenLM training method.\nThis indicates that our method can be a practi-\ncal solution with minimal computational over-\nhead for resource-constrained environments.", "sections": [{"title": "1 Introduction", "content": "The advancement of large language models (LLMs)\nhas accelerated as the 'scaling law' (Kaplan et al.,\n2020), which states that the performance of LLMs\ndirectly correlates with data size, has been stud-\nied. Moreover, recent studies (Penedo et al., 2023;\nGunasekar et al., 2023; Li et al., 2024; Penedo\net al., 2024; Dubey et al., 2024) have shown that\nthe performance of LLMs is largely determined by\nthe quality of the training corpus. In other words,\na vast amount of high-quality training corpus is\nnecessary to enhance the performance of LLMs.\nHowever, large web corpora often contain sub-\nstantial amounts of low-quality data, making them\ndifficult to use directly for training. In response\nto this challenge, various methods (Wettig et al.,\n2024; Kong et al., 2024) are employed to filter\nout low-quality data and select high-quality data."}, {"title": "2 Related Work", "content": "As the demand for a vast amount of high-quality\ntraining corpus grows, it has become essential to\neffectively and efficiently filter large amounts of\nweb corpus. Among various filtering methods,\nthis paper focuses on model-based quality filtering,\nwhich can be broadly divided into the following\ntwo categories: (i) perplexity-based filtering; and\n(ii) classifier-based filtering.\nPerplexity-based filtering. Numerous stud-\nies (Wenzek et al., 2019; Computer, 2023; Nguyen\net al., 2023; Wei et al., 2023; Paster et al.,\n2023; Lauren\u00e7on et al., 2024) use the perplexity\n(PPL) scores of KenLM (Heafield, 2011), an\nn-gram-based language model, to efficiently\nfilter out low-quality data due to its lightweight\narchitecture. It can operate on CPUs, making it a\ncost-efficient solution for handling large-scale text\ndata. Despite its efficiency, there have been few\nefforts to improve its performance. Meanwhile,\nThe Pile (Gao et al., 2020) used the perplexity of\nGPT-2 (Radford et al., 2019) and GPT-3 (Brown,\n2020) to evaluate the quality of the dataset.\nClassifier-based filtering. FastText (Joulin et al.,\n2016) is widely used to distinguish the quality of\ndata (Computer, 2023; Wei et al., 2023; Li et al.,\n2024). Similar to KenLM, FastText is also an effi-\ncient model that operates on CPUs. However, as\ndetailed in Section 4, KenLM demonstrated supe-\nrior performance compared to FastText when both\nwere trained on the same dataset. Furthermore,\nrecent research (Gunasekar et al., 2023; Li et al.,\n2024; Penedo et al., 2024) has focused on fine-\ntuning pre-trained embedding models to serve as\nclassifiers for quality filtering. Especially, Fineweb\ndemonstrated that training relatively small-sized\nLLMs (1.82 billion parameters) on data filtered by\na trained classifier (resulting in 350 billion tokens),\nrather than on unfiltered data, led to performance\nimprovements across various benchmarks. How-\never, these methods are impractical for processing\nlarge web corpora due to their high computational\ncosts, which necessitate significant GPU resources."}, {"title": "3 Proposed Method", "content": "In this paper, we aim to reduce noisy data while\npreserving high-quality data in a computationally\nefficient manner. To this end, we propose an en-\nsemble approach using two contrasting KenLMs:\n(i) Good KenLM and (ii) Bad KenLM.\nGood KenLM. The objective of Good KenLM\nis to assign low perplexity (PPL) scores to well-\nstructured, high-quality text. Many previous stud-\nies (Wenzek et al., 2019; Computer, 2023; Nguyen\net al., 2023; Lauren\u00e7on et al., 2024) have used\na high-quality Wikipedia dataset for training, de-\nnoted as Wiki KenLM in this paper. However, with\nrecent advancements in LLMs, several high-quality\ndatasets (Soldaini et al., 2024; Penedo et al., 2024;\nLi et al., 2024) have emerged. In our experiments,\nas shown in Section 4, we found that the combi-\nnation of S2ORC (Lo et al., 2020) and Textbooks-\nare-all-you-need-lite (SciPhi, 2023) as training data\nwas more effective than utilizing Wikipedia. Thus,\nin this paper, we designate the KenLM trained on\nthis combination of data as Good KenLM.\nBad KenLM. The rationale behind employing\nBad KenLM alongside Good KenLM is that Good\nKenLM fails to detect unwanted content (e.g.,\nspam, advertising, and informal communication),\nwhich are generally considered poor for training\nLLMs, as it has not been explicitly trained on these\ntypes of content. For instance, if low-quality con-\ntent shares superficial linguistic patterns with high-\nquality text, it may still score reasonably well un-\nder Good KenLM. Therefore, to detect a wider\nrange of undesirable content, Bad KenLM is de-\nsigned to assign low PPL scores to such content. To\nachieve this, we trained Bad KenLM using noisy,\nlow-quality datasets, including hate speech, spam\nemails, and informal social media content. To the\nbest of our knowledge, this is the first study to em-\nploy KenLM trained on noisy, low-quality datasets.\nEnsemble. To leverage the complementary\nstrengths of two contrasting KenLMs, we ensemble\nthe models by integrating the PPL scores assigned\nby each. We perform Z-score standardization to\nalign the scales of the two PPL scores assigned\nby each model, as they are trained on different\ndatasets and therefore exhibit different distributions\nof PPL scores. Then, we compute the ensembled\nPPL score $P_{ens}(x_i)$, as follows:\n$P_{ens}(x_i) = \\alpha (\\frac{P_{good}(x_i) - \\mu_{good}}{\\sigma_{good}}) - (1 - \\alpha) (\\frac{P_{bad}(x_i) - \\mu_{bad}}{\\sigma_{bad}})$        (1)\nwhere $x_i \\in X$ denotes the i-th text data, X repre-\nsents datasets, $P_{good}(x_i)$ (resp. $P_{bad}(x_i)$) indicates\nPPL score from Good (resp. Bad) KenLM for $X_i$,\n$\\mu_{good}$ (resp. $\\mu_{bad}$) is the mean of the PPL scores\nfrom Good (resp. Bad) KenLM, $\\sigma_{good}$ (resp. $\\sigma_{bad}$)\nis the standard deviation of the PPL scores from\nGood (resp. Bad) KenLM, and $\\alpha$ denotes a param-\neter that balances the two PPL scores. Note that\nthe coefficient for the term associated with Bad"}, {"title": "4 Experiments", "content": "We designed our experiments to answer the follow-\ning key research questions (RQs):\n\u2022 RQ1: Does our ensemble approach outperform\nexisting models in removing noisy content while\npreserving high-quality content?\n\u2022 RQ2: Which data sources are effective for train-\ning the Bad KenLM?\n\u2022 RQ3: How sensitive is the performance of our\nensemble approach to hyperparameter $\u03b1$?\n\u2022 RQ4: How much additional computational over-\nhead does our ensemble approach introduce com-\npared to a single KenLM?\n\u2022 RQ5: What types of data does our ensemble\napproach effectively filter out?"}, {"title": "4.1 Experimental Settings", "content": "Dataset and model details. As mentioned in Sec-\ntion 3, we randomly selected subsets of 300,000\nsamples each from S2ORC (Lo et al., 2020) and\nTextbooks-are-all-you-need-lite (SciPhi, 2023) as\ntraining data for Good KenLM. For the training\ndata of Bad KenLM, we collected datasets that\nis likely to hinder the training of LLMs. Specifi-\ncally, we used 1,000,000 pieces of social network\nservice (SNS) data (Twitter) (mjw, 2022; fschatt,\n2023; Icama, 2022; StephanAkkerman, 2023) and\n776,142 pieces of spam message data (Metsis et al.,\n2006; thehamkercat, 2024; alissonpadua, 2024).\nDuring the training of both models, we configured\nthe n-gram size to 6 and the vocabulary size to\n65, 536. Also, we set the hyperparameter $\u03b1$ to 0.7.\nEvaluation details. To evaluate the effectiveness\nof our ensemble approach, we measured perplex-\nity (PPL) scores for the CC-MAIN-2024-10 dump\n(211 million samples) from Fineweb-edu (Penedo\net al., 2024). Following Wenzek et al. (2019); Com-\nputer (2023), we then filtered the data based on the\n30th and 60th percentiles of PPL scores. Subse-\nquently, we measured the proportion of data with\nan educational score of 2.5 or higher that was in-\ncluded. In other words, we treated data with an\neducational score of 2.5 or higher as the ground\ntruth and measured the recall value. Note that the\neducational scores are annotated using extensive\nGPU resources, and it has been demonstrated that\ntraining LLMs with data possessing high educa-\ntional scores leads to performance improvements."}, {"title": "4.2 Main Results", "content": "We highlight the best results in bold and the second-\nbest results with an underline in the tables.\nRQ1: Comparison of existing models. As\nshown in , our Good KenLM significantly\noutperformed the widely used Wiki KenLM. Al-\nthough Bad KenLM alone showed poor perfor-\nmance, our strategy of ensembling it with Good\nKenLM outperformed even FastText trained on the\nsame data, improving Recall@30 and Recall@60\nby 9.76% and 2.50%, respectively.\nMoreover, to validate the effectiveness of Bad\nKenLM within our ensemble framework, we con-\nducted a comparative experiment where Good\nKenLM and Wiki KenLM were ensembled in place\nof Bad KenLM, denoted as Ens(Good, Wiki). The\nperformance of Ens(Good, Wiki) was lower than\nthat of Good KenLM alone. This is likely due\nto the relatively lower quality of the Wikipedia\ndataset compared to the training data used for Good\nKenLM, which negatively impacts its overall per-\nformance. This result also highlight the importance\nof incorporating Bad KenLM into the ensemble, as\nit successfully identifies undesirable content that\nGood KenLM may overlook.\nRQ2: Impact of data sources on training Bad\nKenLM. The training dataset for Bad KenLM\nis diverse, including SNS, spam mail, and toxic\ndatasets (Davidson et al., 2017; de Gibert et al.,\n2018; Kennedy et al., 2020; Mathew et al., 2021;\nVidgen et al., 2021; Pavlopoulos et al., 2022) con-\ntaining hate speech and profanity. We conducted"}, {"title": "RQ3: Hyperparameter sensitivity analysis.", "content": "The parameter $\\alpha$ in Eq. (1) adjusts the balance\nbetween the PPL scores of Good KenLM and Bad\nKenLM. We analyze how the performance of our\nensemble approach varies with changes in $\u03b1$ in\nterms of Recall@30 and Recall@60.\nAs depicted in , Recall@30 and Re-\ncall@60 continuously improve as $\u03b1$ increases to\n0.7 and 0.6, respectively, and then gradually de-\ncrease. These results suggest that when $\u03b1$ is too\nsmall, the influence of Bad KenLM becomes overly\ndominant, resulting in poor preservation of high-\nquality content. Conversely, when $\u03b1$ is too large,\nthe influence of Good KenLM prevails, leading to\nthe inclusion of some low-quality content. These\nresults indicate that appropriately determining the\nvalue of $\u03b1$ is critical for effectively removing noisy\ncontent while preserving high-quality content."}, {"title": "RQ4: Degree of computational overhead.", "content": "To\nassess the computational overhead of our approach,\nwe measured the processing time and estimated\ncost\u00b9 for the CC-MAIN-2024-10 dump on a ma-\nchine with 128-core CPUs. As presented in ,\nour approach increased the processing time from\n2,234 to 3,928 seconds, with an additional cost of\n$1.08. These increases are justified by the recall im-\nprovement from 81.27% to 89.19%, as high-quality\ndata is crucial for effective LLM training."}, {"title": "RQ5: Case study on the effectiveness of our ap-proach.", "content": "To demonstrate the effectiveness of our\nensemble approach, we present examples that are\nnot filtered by Good KenLM but are successfully\nremoved by our ensemble approach. As illustrated\nin , our approach effectively filters adver-\ntising and communication-style content, which are\ngenerally unsuitable for LLM training. Since ad-\nvertising content is usually written politely, Good\nKenLM, trained only on high-quality datasets,\nstruggles to detect it. Conversely, Bad KenLM,\ntrained on spam mail and SNS data, successfully\nidentifies such content as well as communication-\nstyle content. Therefore, our ensemble approach\nmore effectively filters these types of content."}, {"title": "5 Conclusion", "content": "In this paper, we propose an ensemble approach\nusing Good KenLM and Bad KenLM for effective\ntext filtering. By integrating perplexity scores, we\nsuccessfully filter out noisy data, such as spam\nand informal content, while preserving high-quality\ntext. Empirical results suggest that our approach\ncould be a practical solution for filtering large-scale\ndatasets in resource-constrained environments."}, {"title": "Ethics Statement", "content": "The experiments conducted in this paper were car-\nried out objectively and fairly. No biases were\nintroduced during the data selection or evaluation\nprocess. All datasets used in this research are pub-\nlicly available, and the methods were rigorously\ntested to ensure the reliability and validity of the\nresults."}, {"title": "Limitations", "content": "While the proposed method using Good KenLM\nand Bad KenLM offers effective filtering of large-\nscale datasets, it has the following limitations: (i)\nAlthough our method has demonstrated effective-\nness through extensive experiments using Fineweb-\nedu, we have not been able to measure its direct\nimpact on LLMs training due to computational cost\nconstraints; and (ii) the model relies heavily on pre-\ndefined training datasets, and its performance may\ndegrade when applied to content that significantly\ndiffers from the training corpora."}]}