[{"title": "Analysis of Overparameterization in Continual Learning under a Linear Model", "authors": ["Daniel Goldfarb", "Paul Hand"], "abstract": "Autonomous machine learning systems that learn many tasks in sequence are prone to the catastrophic forgetting problem. Mathematical theory is needed in order to understand the extent of forgetting during continual learning. As a foundational step towards this goal, we study continual learning and catastrophic forgetting from a theoretical perspective in the simple setting of gradient descent with no explicit algorithmic mechanism to prevent forgetting. In this setting, we analytically demonstrate that overparameterization alone can mitigate forgetting in the context of a linear regression model. We consider a two-task setting motivated by permutation tasks, and show that as the overparameterization ratio becomes sufficiently high, a model trained on both tasks in sequence results in a low-risk estimator for the first task. As part of this work, we establish a non-asymptotic bound of the risk of a single linear regression task, which may be of independent interest to the field of double descent theory.", "sections": [{"title": "1 Introduction", "content": "The field of lifelong learning aims to develop machine learning systems that can continually learn many tasks by transferring and retaining knowledge from previously learned tasks without the need for retraining (Chen and Liu (2022)). This is an important problem because it can be a waste of computation to retrain neural networks from scratch. One challenge of continual learning is the effect of catastrophic forgetting, where models abruptly lose performance on previously learned tasks when training on new ones (French (1999)). The amount of expected catastrophic forgetting is affected by multiple aspects of the tasks, like the neural network architecture and task similarity or difficulty. Hence the field of continual learning aims to develop dedicated approaches to mitigate catastrophic forgetting. These include regularization-based methods, architectural modifications, and memory-replay methods (Li and Hoiem (2017); Kirkpatrick et al. (2017); Zenke et al. (2017); Shin et al. (2017)).\nIn order to build an understanding of state-of-the-art continual learning methods, it is a prerequisite to understand simpler methods. One such simple method is naively modifying the overparameterization level. Modern neural networks typically operate in the extremely overparameterized regime, where the number of learned parameters is much larger than the number of samples in the training set. In practice overparameterized models perform better than their underparameterized counterparts (Nakkiran et al. (2021); Zhang et al. (2021)). This phenomenon, coined double descent, sparked active research to explain this behavior (Adlam and Pennington (2020); Blanc et al. (2020)). While most double descent works contribute to the single-task setting, one continual learning related work (Mirzadeh et al. (2022)) ran experiments on popular rotated image benchmarks to show that overparameterized neural networks may be less prone to forgetting than their moderately parameterized counterparts. Goldfarb and Hand (2023) replicated these results for random data permutation tasks.\nMathematical theory is necessary in order to understand when continual learning algorithms are needed and when to expect performance degradation in practice. This theory is challenging because the theoretical analysis of trained neural networks is still a nascent field, and additional architectural and algorithmic modifications would increase the challenge of that analysis. As a foundation for understanding the algorithmic developments of continual learning, we aim to understand the simplest setting of gradient descent with linear models modified only by their overparameterization level (Evron et al. (2023); Lin et al. (2023)). This setting has been previously studied by theoretical papers for double descent in the single-task setting (Muthukumar et al. (2020); Belkin et al. (2020); Bartlett et al. (2020); Hastie et al. (2022)). As summarized in Dar et al. (2021), these works concurrently found that a linear model can enjoy optimal performance in the overparameterized regime when the effective dimensionality of the data is lower than the number of training data samples, and the true dimensionality of the data is higher than the number of training data samples. We aim to leverage existing single-task work as a foundation to quantify the effect of overparameterization in the multi-task setting. The model presented in Hastie et al. (2022) has a strong connection to neural networks, which we fully describe in Section 2, and we take inspiration from this model in our work.\nIn this paper, we study a two-task continual learning setting with a latent space data model inspired by Hastie et al. (2022). The tasks will be related by a random orthogonal transformation. This two-task setup is a mathematical idealization of the standard continual learning benchmark where tasks are related by a random permutation (Kirkpatrick et al. (2017)). We demonstrate that, under this linear model, if overparameterization is sufficiently high and there are sufficiently many training examples relative to the model's latent dimensionality, then this two-task setup has low risk on task A both before and after training on task B.\nWe now present an informal version of our main conclusion. Consider tasks A and B each with n training examples and p observed features but only d latent features. We are interested in the overparameterized regime where d < n < p. The data of each task is related by a random p\u00d7 porthogonal matrix. Given task A with data $X_A \\in \\mathbb{R}^{n \\times p}$, task B is given by $\u0425_\u0432 = X_AOT$. Both tasks have shared labels $y \\in \\mathbb{R}^{n}$ as is standard with permutation tasks. Our training procedure will fit task A by minimizing the square loss from initialization 0, and then subsequently training on task B with no explicit mechanism"}, {"title": "2 Latent Space Regression Model", "content": "In this section, we first present the latent space model from Hastie et al. (2022) and share the analogy between it and neural networks on vision tasks. Then we present an extension of the latent space model to the setting of two orthogonal transformation tasks as defined in Goldfarb and Hand (2023).\n2.1 Single-Task Latent Space Model\nConsider a linear regression model with d-dimensional standard Gaussian latent features. Let the responses be generated by a noiseless inner product with some ground truth parameters $\u03b8\\in \\mathbb{R}^d$:\n$z \\sim N(0, I_d)$ (4)\n$y = z^T\u03b8$. (5)\nInstead of performing regression on the latent features, suppose we only have access to noisy p-dimensional random projections, x, of the latent features:\n$x = Wz+ u$, (6)\nwhere $W \\in \\mathbb{R}^{p \\times d}$ and $u \\sim N(0, I_p)$. Here the number of noisy features, p, controls the overparameterization level of the problem. While W could be defined to have Gaussian entries, for mathematical convenience we consider the idealized case where $W^TW = p\\gamma I_p$. $\u03b3$ acts as a signal-to-noise term and columns of W are pairwise orthogonal and have equal length.\nThe regression problem provides n training samples and corresponding responses in the form of data $(X, y) \\in \\mathbb{R}^{n \\times p} \\times \\mathbb{R}^n$, where each of the n rows of X and entries of y are sampled independently by (4) - (6).\nConsider a model that makes predictions on the observed features linearly:\n$f_p : x \\rightarrow x^T\u03b2$, (7)\nwhere $\u03b2 \\in \\mathbb{R}^p$ are the parameters of the model. We train the parameters using gradient descent with a square loss. Due to the overparameterized nature of the problem (d < n < p), the result of gradient descent will depend on initialization and converge to a solution that fits to the training data. We consider solving $\\arg \\min \\frac{1}{2} ||X\u03b2 \u2013 y||^2$ using gradient descent from initialization $\u03b2_0$. There is an analytical solution to this given by\n$\\arg \\min \\frac{1}{2} ||\u03b2 \u2013 \u03b2_0||^2 \\text{ s.t. } y = X\u03b2$. (8)\nThe optimization problem in (8) has the following closed form solution when X is full rank (which occurs with probability 1 under the above model):"}, {"title": "2.2 Two-Task Latent Space Model", "content": "We now extend the data model of Section 2.1 to study two tasks which are related by a random orthogonal transformation. This provides tasks that are equally difficult (for a fully-connected model) and are similar to the random data permutation benchmark of continual learning (De Lange et al. (2021)).\nConsider two tasks, A and B, with n examples each. Task A has data $(Xa,y) \\in \\mathbb{R}^{n \\times p} \\times \\mathbb{R}^n$ where each of the n rows of $X_A$ and entries of y are sampled independently by (4) - (6) for a fixed \u03b8. Define O to be sampled uniformly from the set of $p \\times p$ orthogonal matrices. Task B then has data $(XB,y)$ where $XB = X_AOT$. Observe that y is the same for tasks A and B as is standard for random permutation tasks.\nRecall from Section 2.1 that each estimator is given by a solution to a least squares problem from a variety of initializations. Let $\u03b2_A$ be the solution to task A when initialized at 0, $\u03b2_B$ be the solution to task B when initialized at 0, and $\u03b2_{BA}$ be the solution to task B when initialized at $\u03b2_A$. Using the closed form solutions from Section 2.1, we get the following formulas for these parameters:\n$\u03b2_A = X_A^T(X_AX_A^T)^{-1}y$, (15)\n$\u0412_\u0432 = \u0425_\u0432^T(\u0425_\u0432\u0425_\u0432)-\u00b9y$, (16)\n$\u0412_{\u0412\u0410} = BA + BB \u2013 P_{X_B^T} BA$. (17)\nUnder the two-task model, we are interested in the risk of these estimators on task A as given by (11).\nThis two-task formulation was first used in Goldfarb and Hand (2023) which provided an analytical illustration of the previously observed empirical behavior that overparameterized neural networks can be less prone to catastrophic forgetting (Mirzadeh et al. (2022)). This empirical observation was made on the popular image rotation/permutation continual learning benchmarks. Under this setting, the first task of the problem is the original image classification task, and subsequent tasks are random rotations/permutations of the original task. This provides tasks that are nearly orthogonal, thus allowing for subsequent training with minimal forgetting (Farajtabar et al. (2020)). Goldfarb and Hand (2023) aimed to develop a linear regression data model that mirrored relevant permuted image benchmarks for their analysis. We use the same model in this work for its mathematical tractability."}, {"title": "3 Main Result", "content": "Under the two-task linear model introduced in Section 2.2, our main result quantifies the risk on task A of estimators (15)-(17). This provides parameter regimes that guarantee that:\n\u2022 Task A is initially well-learned\u00b9.\n\u2022 After subsequent training on task B, the risk on task A remains well-learned.\n\u2022 The amount of forgetting is small compared to the amount of initial learning.\nFor mathematical convenience, suppose W satisfies the following assumption:\nAssumption 1 All columns of W are pair-wise orthogonal and have equal length. That is, $W^TW = p\\gamma I_d$.\nWe will compare the performance of our learned estimators to the 0 predictor, which we call the (unlearned) null risk baseline. To demonstrate that task A is well-learned, it must outperform this null risk. By Lemma F.3 in Goldfarb and Hand (2023), null risk has an exact value of $R(0) = ||0||^2$. Our main result is the following:\nTheorem 2 Fix $0 \\in \\mathbb{R}^d$. Let tasks A, B be given by Section 2.2. Let $W \\in \\mathbb{R}^{p \\times d}$ satisfy Assumption 1 and $n \\geq d, p \\geq 20n, \u03b3 \\geq \\frac{1}{\\sqrt{nd}}$. Then there exists constant $c > 0$ such that with probability at least $1 \u2013 20e^{-cd}$, the following holds:\n$R(f_{BA}) \\leq (72\\sqrt{\\frac{d}{n}}+18\\frac{n}{p}) ||0||^2$, (18)\n$R(f_{BBA}) \\leq (72\\sqrt{\\frac{d}{n}}+96\\frac{n}{p}) ||0||^2$, (19)\n$\\frac{R(f_{BBA}) \u2013 R(f_{BA})}{R(0) \u2013 R(f_{BA})} < \\frac{78\\sqrt{\\frac{d}{n}}}{1-72\\sqrt{\\frac{d}{n}}}-\\frac{18\u03b7}{\u03c1}$. (20)\nEquation (18) quantifies the risk on task A of the estimator trained exclusively on task A. Thus observe that when n scales linearly with d and p scales linearly with n, the risk is small and task A is well-learned. Equation (19) quantifies the performance on task A of an estimator that trains on task A then task B in sequence. Under the same linear scalings as before, observe that task A stays well-learned after training on task B. Finally, equation (20) asserts that the fraction of performance loss as a result of forgetting compared to the performance gain due to original training is small under this parameter regime. Note that as $p \\rightarrow \\infty$, the bounds we provide on $R(f_{BA})$ and $R(f_{BBA})$ are monotonically decreasing but do not go to 0. Indeed performance is limited by the ratio of latent dimensionality and number of training samples. The bound on the risk ratio in (20) does go to 0, thus forgetting is fully ameliorated for this model in this setting. The conclusions about risk hold under a wide range of signal-to-noise ratios, even as low as $\u03b3 = \\frac{1}{\\sqrt{nd}}$. Observe that the failure probability is exponentially small in d. This is due to the fact that the proof is based on non-asymptotic random matrix results. Each of (18)-(20) are proven in Theorems 8, 10, and 11 respectively.\nThe bound in (18) may be of independent to the field of double descent theory. It is a non-asymptotic result similar to the latent space result in Hastie et al. (2022). Their result established that if d, n, p \u2192 \u221e with fixed ratios $\\frac{d}{n}, \\frac{n}{p}, \\gamma$ then an explicit expression for risk can be derived that exhibits double descent. This result is asymptotic in nature. In order to obtain a risk estimate for problems with finite size, we establish a non-asymptotic version of this result. Our result relies on non-asymptotic singular value estimates, such as those in Vershynin (2010). As (18) is monotonically decreasing in p, it demonstrates a double descent effect and hence this technical novelty may be of independent interest to the overparameterized machine learning theory community."}, {"title": "4 Discussion and Conclusion", "content": "In this work we present a two-task linear model where data is generated from a latent space model and tasks are related by a random orthogonal transformation. We show theoretically that when the model is suitably overparameterized, forgetting is small and allows for high performance on the first task both before and after training on the second task.\nOverparameterization provides the benefit that catastrophic forgetting is minimal. This effect can be seen geometrically in Figure 1. As seen in this figure, level sets of constant risk become orthogonal to each other and get extremely high aspect ratios in the overparameterized regime. Thus the learned predictor is able to train on each task in sequence and retain high performance on both tasks at the end of training."}, {"title": "Appendix A.", "content": "Assumption 3 All columns of W are pair-wise orthogonal and of equal length. Namely, $W^TW = p\\gamma I_d$.\nA consequence of Assumption 3 is that $WWT = p\\gamma P_W$. This formally proven in Lemma F.2 of Goldfarb and Hand (2023).\nLemma 4 Suppose $W \\in \\mathbb{R}^{p \\times d}$ follows Assumption 3. Define $\u03b2 = W(W^TW + I_d)^{-1}\u03b8$ for some $\u03b8 \\in \\mathbb{R}^d$. Then\n$||3||^2 = \\frac{py}{(py +1)^2}||0||^2$ (21)\nProof By definition,\n$||3||^2 = 0^T (W^TW + I_d)^{-1}WW(W^TW + I_d)^{-1}0$ (22)\nUsing Assumption 3,\n$||3||^2 = 0^T (pyI_d + I_d)^{-1}pyI_d(pyI_d + I_d)^{-1}0$ (23)\n$= \\frac{PY}{(py +1)^2} ||0||^2$ (24)\nLemma 5 Assume $W \\in \\mathbb{R}^{p \\times d}$ satisfies Assumption 3. Then\n$WW^T = p\\gamma P_W$ (25)\nwhere $P_W$ is the orthogonal projection onto the range of W.\nProof Lemma F.2 in Goldfarb and Hand (2023).\nLemma 6 Let $\u2211 = WW^T + I_p$ where $W \\in \\mathbb{R}^{p \\times d}$ satisfies Assumption 3. For some $0 \\in \\mathbb{R}^d$, let $\u03b2 = W(W^TW + I_d)^{-1}0$. Then\n$R(f_0) = ||0||^2$ (26)\nProof Lemma F.3 in Goldfarb and Hand (2023)."}, {"title": "Lemma 7", "content": "Let $W = \\begin{bmatrix} \\sqrt{p\\gamma}I_d \\\\ 0 \\end{bmatrix} \\in \\mathbb{R}^{p \\times d}$ and define data matrix $A \\in \\mathbb{R}^{n \\times p}$ whose rows are independent random vectors in $\\mathbb{R}^p$ with covariance $\u03a3 = WWT + I_p$ and $d \\leq n, 2n \\leq p$. Assume $\\gamma > \\frac{1}{\\sqrt{nd}}$. Then with probability at least $1 - \\frac{8e^{-cd}}{\\sqrt{nd}}$ for universal constant $c > 0$,\n$||P_{A_{T1}}e_1||^2 \\leq 18\\sqrt{\\frac{d}{n}}$ (27)\nwhere $P_{A_{T1}}$ is the orthogonal projection on the orthogonal complement of the range of $A^T$ and $e_1$ is the p-dimensional first standard basis vector.\nProof We have that\n$||P_{A_{T1}}e_1||^2 = ||e_1||^2 - ||P_{A^T}e_1||^2$ (28)\n$= 1 - ||P_{A^T}e_1 ||^2$ (29)\nSo it suffices to study $||P_{A^T}e_1||^2 = e_1^TP_{A^T}e_1$:\n$e_1^TP_{A^T}e_1 = e_1^TA^T(AA^T)^{-1}Ae_1$ (30)\nLet $A = [A_1 \\mid A_2]$ where $A_1 \\in \\mathbb{R}^{n \\times d}, A_2 \\in \\mathbb{R}^{n \\times p - d}$, then\n$e_1^TP_{A^T}e_1 = e_1^TA_1^T(A_1A_1^T + A_2A_2^T)^{-1}A_1e_1$ (31)\n$\\geq e_1^TA_1^T(A_1A_1^T + s_{max}(A_2A_2^T)I_n)^{-1}A_1e_1$ (32)\nWhere the last inequality follows from $A_1A_1^T+A_2A_2^T \\prec A_1A_1^T+s_{max}(A_2A_2^T)I_n$. By Theorem 5.39 of Vershynin (2010), $s_{max}(A_2A_2^T) \\leq (\\sqrt{p - d} + 2\\sqrt{n})^2$ on an event of a given probability. So $s_{max}(A_2A_2^T) \\leq (\\sqrt{p - d} + 2\\sqrt{n})^2$. Also under the assumptions that $p > 2n$ and $n \\geq d$, we have that $n \\leq p - d$, and further $\\sqrt{p-d}+2\\sqrt{n} < 3\\sqrt{p - d}$. This gives\n$e_1^TP_{A^T}e_1 \\geq e_1^TA_1^T(A_1A_1^T + 9(p-d)I_n)^{-1}A_1e_1$ (33)\nLet $A_1 = USV^T$ be the SVD of $A_1$ where $U \\in \\mathbb{R}^{n \\times n}, S \\in \\mathbb{R}^{n \\times d}, V \\in \\mathbb{R}^{d \\times d}$. Then\n$e_1^TP_{A^T}e_1 \\geq e_1^TV(S^TS + 9(p-d)I_d)^{-1}V^Te_1$ (34)\n$= e_1^TV^TS(S^TS + 9(p-d)I_d)^{-1}SV^Te_1$ (35)\n$= e_1^TVdiag(\\frac{s_i}{s_i^2 + 9(p-d)})V^Te_1$ (36)\nwhere $s_i$ are the singular values of $A_1$ and operator \"diag\" produces the diagonal matrix of the set of $s_i$."}, {"title": null, "content": "$e_1^TP_{A^T}e_1 \\geq e_1^TVdiag(\\frac{(p\\gamma + 1)s_i^2}{(p\\gamma + 1)s_i^2 + 9(p - d)})V^Te_1$ (37)\n$= \\sum_{i=1}^d (\\frac{(p\\gamma + 1)s_i^2}{(p\\gamma + 1)s_i^2 + 9(p - d)})V_i(1)V_i(1)e_1$ (38)\nwhere $s_i$ are the singular values of the matrix $A_1/(p\\gamma + 1)$. Observe that this matrix has i.i.d. $N(0, 1)$ entries. And thus\n$e_1^TP_{A^T}e_1 > \\sum_{i=1}^d (\\frac{(p\\gamma + 1)s_i^2}{(p\\gamma + 1)s_i^2 + 9(p - d)}) V_i(1)^2$ (39)\nwhere $V_i(1)$ is the 1st element of the ith row of V. By Theorem 5.39 in Vershynin (2010), $\\sqrt{n} - 2\\sqrt{d} \\leq s_i \\leq \\sqrt{n} + 2\\sqrt{d}$ for all i with probability at least 1 \u2013 2e-cd for universal constant c > 0. This gives\n$e_1^TP_{A^T}e_1 \\geq \\sum_{i=1}^d (\\frac{(p\\gamma + 1)(\\sqrt{n} - 2\\sqrt{d})^2}{(p\\gamma + 1)(\\sqrt{n} + 2\\sqrt{d})^2 + 9(p - d)}) V_i(1)^2$ (40)\nNote that $\\sum_{i=1}^d V_i(1)^2 = 1$ by orthogonality of V. This gives\n$e_1^TP_{A^T}e_1 \\geq \\frac{(p\\gamma + 1)(\\sqrt{n} - 2\\sqrt{d})^2}{(p\\gamma + 1)(\\sqrt{n} + 2\\sqrt{d})^2 + 9(p - d)}$ (41)\nSubstituting this into (29) gives\n$||P_{A_{T1}}e_1||^2 \\leq 1 - \\frac{(p\\gamma + 1)(\\sqrt{n} - 2\\sqrt{d})^2}{(p\\gamma + 1)(\\sqrt{n} + 2\\sqrt{d})^2 + 9(p - d)}$ (42)\n$\\leq \\frac{(p\\gamma + 1)(\\sqrt{n} + 2\\sqrt{d})^2 - (p\\gamma + 1)(\\sqrt{n} - 2\\sqrt{d})^2 + 9(p - d)}{(p\\gamma + 1)(\\sqrt{n} + 2\\sqrt{d})^2}$ (43)\n$\\leq \\frac{8(p\\gamma + 1)\\sqrt{nd} + 9(p - d)}{(p\\gamma + 1)(n + 4\\sqrt{nd} + 4d)}$ (44)\nUnder the assumption that $\\gamma \\geq \\frac{1}{\\sqrt{nd}}$, this simplifies to\n$||P_{A_{T1}}e_1||^2 \\leq \\frac{18\\sqrt{nd}}{n + 4\\sqrt{nd} + 4d}$ (45)\n$< 18\\sqrt{\\frac{d}{n}}$ (46)"}, {"title": "Theorem 8 (Single-Task Risk)", "content": "Suppose $W \\in \\mathbb{R}^{p \\times d}$ follows Assumption 3. Define data matrix $A \\in \\mathbb{R}^{n \\times p}$ and responses $y = A\u03b2 + \u03b5$ where $\u03b5 \\sim N(0, \u03c3^2I_n), \u03c3^2 = 0^T (W^TW + I_d)^{-1}0$, and $\u03b2 = W(W^TW + I_d)^{-1}0$ for some $0 \\in \\mathbb{R}^d$. Let rows $A_i$ be independent random vectors in $\\mathbb{R}^p$ with covariance $\u03a3 = WWT + I_p$ and $n \\geq d, p \\geq 20n, \u03b3 \\geq \\frac{1}{\\sqrt{nd}}$. Let $\u03b2_A$ be the parameters of the minimum norm estimator on A as defined in (15). Let $R(f_\u03b2)$ be the risk on task A of an estimator with parameters $\u03b2$. Then there exists constant c > 0 such that with probability at least $1 - \\frac{10e^{-cd}}{\\sqrt{nd}}$, the following holds:\n$R(f_{BA}) \\leq (72\\sqrt{\\frac{d}{n}}+18\\frac{n}{p}) ||0||^2$ (47)\nProof By definition,\n$R(f_{\u03b2A}) = (\u03b2 \u2013 \u03b2_A)^T\u03a3(\u03b2 \u2013 \u03b2_A) + \u03c3^2$ (48)\nSubstituting the closed-form for $\u03b2_A$ gives\n$R(f_{\u03b2A}) = (\u03b2 \u2013 A^T (AA^T)^{-1}y)^T\u03a3(\u03b2 \u2013 A^T (AA^T)^{-1}y) + \u03c3^2$ (49)\n$= (\u03b2 \u2013 A^T (AA^T)^{-1}(A\u03b2 + \u03b5))^T\u03a3(\u03b2 \u2013 A^T (AA^T)^{-1}(A\u03b2 + \u03b5)) + \u03c3^2$ (50)\n$= (\u03b2 \u2013 A^T (AA^T)^{-1}A\u03b2 \u2013 A^T (AA^T)^{-1}\u03b5)^T\u03a3(\u03b2 \u2013 A^T (AA^T)^{-1}A\u03b2 \u2013 A^T (AA^T)^{-1}\u03b5) + \u03c3^2$ (51)\n$AA^T$ has full rank so $A^T(AA^T)^{-1}A = P_{A^T}$ where $P_{A^T}$ is the orthogonal projection onto the range of $A^T$. This gives\n$R(f_{\u03b2A}) = (\u03b2 - P_{A^T}\u03b2 \u2013 A^T (AA^T)^{-1}\u03b5)^T\u03a3(\u03b2 - P_{A^T}\u03b2 \u2013 A^T (AA^T)^{-1}\u03b5) + \u03c3^2$ (52)\n$= (P_{A^T}^{\\perp}\u03b2 - A^T (AA^T)^{-1}\u03b5)^T\u03a3(P_{A^T}^{\\perp}\u03b2 - A^T (AA^T)^{-1}\u03b5) + \u03c3^2$ (53)\nBy Assumption 3, $\u03a3 = p\u03b3P_W + I_p$, and hence\n$R(f_{\u03b2A}) = (P_{A^T}^{\\perp}\u03b2 - A^T (AA^T)^{-1}\u03b5)^T (p\u03b3P_W + I_p)(P_{A^T}^{\\perp}\u03b2 - A^T (AA^T)^{-1}\u03b5) + \u03c3^2$ (54)"}, {"title": null, "content": "Directly,\n$R(f_{\u03b2A}) = (P_{A^T}^{\\perp}\u03b2 - A^T (AA^T)^{-1}\u03b5)^T ((p\u03b3 + 1)P_W + P_W^{\\perp}) (P_{A^T}^{\\perp}\u03b2 - A^T (AA^T)^{-1}\u03b5) + \u03c3^2$ (55)\n$= (p\u03b3 + 1)(P_{A^T}^{\\perp}\u03b2 \u2013 A^T (AA^T)^{-1}\u03b5)^TP_W(P_{A^T}^{\\perp}\u03b2 \u2013 A^T (AA^T)^{-1}\u03b5) + (P_{A^T}^{\\perp}\u03b2 \u2013 A^T (AA^T)^{-1}\u03b5)^TP_W^{\\perp}(P_{A^T}^{\\perp}\u03b2 \u2013 A^T (AA^T)^{-1}\u03b5) + \u03c3^2$ (56)\n$= (p\u03b3 + 1)||P_W(P_{A^T}^{\\perp}\u03b2 \u2013 A^T (AA^T)^{-1}\u03b5)||^2 + ||P_W^{\\perp}(P_{A^T}^{\\perp}\u03b2 \u2013 A^T (AA^T)^{-1}\u03b5)||^2 + \u03c3^2$ (57)\nUsing $||P_W^{\\perp}|| \\leq 1$,\n$R(f_{\u03b2A}) \\leq (p\u03b3 + 1)||P_W(P_{A^T}^{\\perp}\u03b2 \u2013 A^T (AA^T)^{-1}\u03b5)||^2 + ||P_{A^T}^{\\perp}\u03b2 \u2013 A^T (AA^T)^{-1}\u03b5||^2 + \u03c3^2$ (58)\nUsing triangle inequality,\n$R(f_{\u03b2A}) \\leq (p\u03b3 + 1)(||P_WP_{A^T}^{\\perp}\u03b2|| + ||P_WA^T (AA^T)^{-1}\u03b5||)^2 + (||P_{A^T}^{\\perp}\u03b2|| + ||A^T (AA^T)^{-1}\u03b5||)^2 + \u03c3^2$ (59)\nUsing $||P_W|| \\leq 1$, we get\n$R(f_{\u03b2A}) \\leq (p\u03b3 + 2)(||P_{A^T}^{\\perp}\u03b2|| + ||A^T (AA^T)^{-1}\u03b5||)^2 + \u03c3^2$ (60)\n$= (p\u03b3 + 2)(I + II)^2 + III$ (61)\nwhere\n$I = ||P_{A^T}^{\\perp}\u03b2||$ (62)\n$II = ||A^T (AA^T)^{-1}\u03b5||$ (63)\n$III = \u03c3^2$ (64)\nWe bound each of these terms separately, starting with I:\nWLOG study $||P_{A^T}^{\\perp}e_1||^2||\u03b2||^2$ where $W = \\begin{bmatrix} \\sqrt{p\u03b3}I_d \\\\ 0 \\end{bmatrix}$. By Lemma 7, the following bound holds with probability at least $1 - \\frac{8e^{-cd}}{\\sqrt{nd}}$ for universal constant $c > 0$,\n$||P_{A^T}^{\\perp}e_1||^2 \\leq 18\\sqrt{\\frac{d}{n}}$ (65)"}, {"title": null, "content": "Multiplying by $||\u03b2||^2$ and taking the square root yields the expression for I. By Lemma 4", "II": "nBy Lemma F.9 in Goldfarb and Hand (2023)", "III": "nBy definition", "gives": "n$R(f_{\u03b2A}) \\leq (p\u03b3 + 2)(\\sqrt{18\\sqrt{\\frac{d}{n}"}]}, {}]