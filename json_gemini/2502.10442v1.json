{"title": "Analysis of Overparameterization in Continual Learning under a Linear Model", "authors": ["Daniel Goldfarb", "Paul Hand"], "abstract": "Autonomous machine learning systems that learn many tasks in sequence are prone to the catastrophic forgetting problem. Mathematical theory is needed in order to understand the extent of forgetting during continual learning. As a foundational step towards this goal, we study continual learning and catastrophic forgetting from a theoretical perspective in the simple setting of gradient descent with no explicit algorithmic mechanism to prevent forgetting. In this setting, we analytically demonstrate that overparameterization alone can mitigate forgetting in the context of a linear regression model. We consider a two-task setting motivated by permutation tasks, and show that as the overparameterization ratio becomes sufficiently high, a model trained on both tasks in sequence results in a low-risk estimator for the first task. As part of this work, we establish a non-asymptotic bound of the risk of a single linear regression task, which may be of independent interest to the field of double descent theory.", "sections": [{"title": "1 Introduction", "content": "The field of lifelong learning aims to develop machine learning systems that can continually learn many tasks by transferring and retaining knowledge from previously learned tasks without the need for retraining (Chen and Liu (2022)). This is an important problem because it can be a waste of computation to retrain neural networks from scratch. One challenge of continual learning is the effect of catastrophic forgetting, where models abruptly lose performance on previously learned tasks when training on new ones (French (1999)). The amount of expected catastrophic forgetting is affected by multiple aspects of the tasks, like the neural network architecture and task similarity or difficulty. Hence the field of continual learning aims to develop dedicated approaches to mitigate catastrophic forgetting. These include regularization-based methods, architectural modifications, and memory-replay methods (Li and Hoiem (2017); Kirkpatrick et al. (2017); Zenke et al. (2017); Shin et al. (2017)).\nIn order to build an understanding of state-of-the-art continual learning methods, it is a prerequisite to understand simpler methods. One such simple method is naively modifying the overparameterization level. Modern neural networks typically operate in the extremely overparameterized regime, where the number of learned parameters is much larger than the number of samples in the training set. In practice overparameterized models perform better than their underparameterized counterparts (Nakkiran et al. (2021); Zhang et al. (2021)). This phenomenon, coined double descent, sparked active research to explain this behavior (Adlam and Pennington (2020); Blanc et al. (2020)). While most double descent works contribute to the single-task setting, one continual learning related work (Mirzadeh et al. (2022)) ran experiments on popular rotated image benchmarks to show that overparameterized neural networks may be less prone to forgetting than their moderately parameterized counterparts. Goldfarb and Hand (2023) replicated these results for random data permutation tasks.\nMathematical theory is necessary in order to understand when continual learning algorithms are needed and when to expect performance degradation in practice. This theory is challenging because the theoretical analysis of trained neural networks is still a nascent field, and additional architectural and algorithmic modifications would increase the challenge of that analysis. As a foundation for understanding the algorithmic developments of continual learning, we aim to understand the simplest setting of gradient descent with linear models modified only by their overparameterization level (Evron et al. (2023); Lin et al. (2023)). This setting has been previously studied by theoretical papers for double descent in the single-task setting (Muthukumar et al. (2020); Belkin et al. (2020); Bartlett et al. (2020); Hastie et al. (2022)). As summarized in Dar et al. (2021), these works concurrently found that a linear model can enjoy optimal performance in the overparameterized regime when the effective dimensionality of the data is lower than the number of training data samples, and the true dimensionality of the data is higher than the number of training data samples. We aim to leverage existing single-task work as a foundation to quantify the effect of overparameterization in the multi-task setting. The model presented in Hastie et al. (2022) has a strong connection to neural networks, which we fully describe in Section 2, and we take inspiration from this model in our work.\nIn this paper, we study a two-task continual learning setting with a latent space data model inspired by Hastie et al. (2022). The tasks will be related by a random orthogonal transformation. This two-task setup is a mathematical idealization of the standard continual learning benchmark where tasks are related by a random permutation (Kirkpatrick et al. (2017)). We demonstrate that, under this linear model, if overparameterization is sufficiently high and there are sufficiently many training examples relative to the model's latent dimensionality, then this two-task setup has low risk on task A both before and after training on task B.\nWe now present an informal version of our main conclusion. Consider tasks A and B each with n training examples and p observed features but only d latent features. We are interested in the overparameterized regime where d < n < p. The data of each task is related by a random p\u00d7 porthogonal matrix. Given task A with data $X_A \\in \\mathbb{R}^{n \\times p}$, task B is given by $X_B = X_A O^T$. Both tasks have shared labels $y \\in \\mathbb{R}^n$ as is standard with permutation tasks. Our training procedure will fit task A by minimizing the square loss from initialization 0, and then subsequently training on task B with no explicit mechanism"}, {"title": "2 Latent Space Regression Model", "content": "In this section, we first present the latent space model from Hastie et al. (2022) and share the analogy between it and neural networks on vision tasks. Then we present an extension of the latent space model to the setting of two orthogonal transformation tasks as defined in Goldfarb and Hand (2023).\n2.1 Single-Task Latent Space Model\nConsider a linear regression model with d-dimensional standard Gaussian latent features. Let the responses be generated by a noiseless inner product with some ground truth parameters $\\theta\\in \\mathbb{R}^d$:\n$z \\sim N(0, I_d)$,                                                                                (4)\n$y = z^T \\theta$.                                                                               (5)\nInstead of performing regression on the latent features, suppose we only have access to noisy p-dimensional random projections, x, of the latent features:\n$x = Wz+ u$,                                                                              (6)\nwhere $W \\in \\mathbb{R}^{p \\times d}$ and $u \\sim N(0, I_p)$. Here the number of noisy features, p, controls the overparameterization level of the problem. While W could be defined to have Gaussian entries, for mathematical convenience we consider the idealized case where $W^T W = p\\gamma I_p$.\n$\\gamma$ acts as a signal-to-noise term and columns of W are pairwise orthogonal and have equal length.\nThe regression problem provides n training samples and corresponding responses in the form of data (X, y) \u2208 $\\mathbb{R}^{n \\times p} \\times \\mathbb{R}^n$, where each of the n rows of X and entries of y are sampled independently by (4) - (6).\nConsider a model that makes predictions on the observed features linearly:\n$f_p : x \\mapsto x^T\\beta$,                                                                                             (7)\nwhere $\u03b2 \u2208 \\mathbb{R}^p$ are the parameters of the model. We train the parameters using gradient descent with a square loss. Due to the overparameterized nature of the problem (d < n < p), the result of gradient descent will depend on initialization and converge to a solution that fits to the training data. We consider solving arg min $\\frac{1}{2}||X\\beta \u2013 y||^2$ using gradient descent from initialization $\u03b2_0$. There is an analytical solution to this given by\n$\\arg \\min \\frac{1}{2}|| - \u03b2_0||^2 \\text{ s.t. } y = X\u03b2$.                                                                                             (8)\nThe optimization problem in (8) has the following closed form solution when X is full-rank (which occurs with probability 1 under the above model):"}, {"title": "2.2 Two-Task Latent Space Model", "content": "We now extend the data model of Section 2.1 to study two tasks which are related by a random orthogonal transformation. This provides tasks that are equally difficult (for a fully-connected model) and are similar to the random data permutation benchmark of continual learning (De Lange et al. (2021)).\nConsider two tasks, A and B, with n examples each. Task A has data $(X_A, y) \\in \\mathbb{R}^{n \\times p} \\times \\mathbb{R}^n$ where each of the n rows of $X_A$ and entries of $y$ are sampled independently by"}, {"title": "3 Main Result", "content": "Under the two-task linear model introduced in Section 2.2, our main result quantifies the risk on task A of estimators (15)-(17). This provides parameter regimes that guarantee that:\n\u2022 Task A is initially well-learned\u00b9.\n\u2022 After subsequent training on task B, the risk on task A remains well-learned.\n\u2022 The amount of forgetting is small compared to the amount of initial learning.\nFor mathematical convenience, suppose W satisfies the following assumption:\nAssumption 1 All columns of W are pair-wise orthogonal and have equal length. That is, $W^T W = p\\gamma I_d$."}, {"title": "4 Discussion and Conclusion", "content": "In this work we present a two-task linear model where data is generated from a latent space model and tasks are related by a random orthogonal transformation. We show theoretically that when the model is suitably overparameterized, forgetting is small and allows for high performance on the first task both before and after training on the second task.\nOverparameterization provides the benefit that catastrophic forgetting is minimal. This effect can be seen geometrically in Figure 1. As seen in this figure, level sets of constant risk become orthogonal to each other and get extremely high aspect ratios in the overparameterized regime. Thus the learned predictor is able to train on each task in sequence and retain high performance on both tasks at the end of training."}, {"title": "Appendix A.", "content": "Assumption 3 All columns of W are pair-wise orthogonal and of equal length. Namely, $W^T W = p\\gamma I_d$.\nA consequence of Assumption 3 is that $WW^T = p\\gamma P_W$. This formally proven in Lemma F.2 of Goldfarb and Hand (2023).\nLemma 4 Suppose $W \\in \\mathbb{R}^{p \\times d}$ follows Assumption 3. Define $\\beta = W(W^T W + I_d)^{-1}\\theta$ for some $\\theta \\in \\mathbb{R}^d$. Then\n$||\\beta||^2 = \\frac{p\\gamma}{(p\\gamma + 1)^2}||\\theta||^2$                                                                                             (21)\nProof By definition,\n$||\\beta||^2 = \\theta^T (W^T W + I_d)^{-1}WW^T W (W^T W + I_d)^{-1}\\theta$                                                                                                                            (22)\nUsing Assumption 3,\n$||\\beta||^2 = \\theta^T (p\\gamma I_d + I_d)^{-1}p\\gamma I_d (p\\gamma I_d + I_d)^{-1}\\theta$                                                                                                                           (23)\n$= \\frac{p\\gamma}{(p\\gamma + 1)^2}||\\theta||^2$                                                                                                                                                        (24)\nLemma 5 Assume $W \\in \\mathbb{R}^{p \\times d}$ satisfies Assumption 3. Then\n$WW^T = p\\gamma P_W$                                                                                                                                                                   (25)\nwhere $P_W$ is the orthogonal projection onto the range of W.\nProof Lemma F.2 in Goldfarb and Hand (2023).\nLemma 6 Let $\\Sigma = WW^T + I_p$ where $W \\in \\mathbb{R}^{p \\times d}$ satisfies Assumption 3. For some $\\theta \\in \\mathbb{R}^d$, let $\\beta = W(W^T W + I_d)^{-1}\\theta$. Then\n$R(f_\\theta) = ||\\theta||^2$                                                                                                                                                                            (26)\nProof Lemma F.3 in Goldfarb and Hand (2023)."}]}