{"title": "Kaleidoscope: Learnable Masks for Heterogeneous Multi-agent Reinforcement Learning", "authors": ["Xinran Li", "Ling Pan", "Jun Zhang"], "abstract": "In multi-agent reinforcement learning (MARL), parameter sharing is commonly employed to enhance sample efficiency. However, the popular approach of full parameter sharing often leads to homogeneous policies among agents, potentially limiting the performance benefits that could be derived from policy diversity. To address this critical limitation, we introduce Kaleidoscope, a novel adaptive partial parameter sharing scheme that fosters policy heterogeneity while still maintaining high sample efficiency. Specifically, Kaleidoscope maintains one set of common parameters alongside multiple sets of distinct, learnable masks for different agents, dictating the sharing of parameters. It promotes diversity among policy networks by encouraging discrepancy among these masks, without sacrificing the efficiencies of parameter sharing. This design allows Kaleidoscope to dynamically balance high sample efficiency with a broad policy representational capacity, effectively bridging the gap between full parameter sharing and non-parameter sharing across various environments. We further extend Kaleidoscope to critic ensembles in the context of actor-critic algorithms, which could help improve value estimations. Our empirical evaluations across extensive environments, including multi-agent particle environment, multi-agent MuJoCo and StarCraft multi-agent challenge v2, demonstrate the superior performance of Kaleidoscope compared with existing parameter sharing approaches, showcasing its potential for performance enhancement in MARL. The code is publicly available at https://github.com/LXXXXR/Kaleidoscope.", "sections": [{"title": "1 Introduction", "content": "Cooperative multi-agent reinforcement learning (MARL) has demonstrated remarkable effectiveness in solving complex real-world decision-making problems across various domains, such as resource allocation (Ying and Dayong, 2005), package delivery (Seuken and Zilberstein, 2007), autonomous driving (Zhou et al., 2021), and robot control (Swamy et al., 2020). To mitigate the challenges posed by the non-stationary and partially observable environments typical of MARL (Yuan et al., 2023), the centralized training with decentralized execution (CTDE) paradigm (Foerster et al., 2016) has become prevalent, inspiring many influential MARL algorithms such as MADDPG (Lowe et al., 2017), COMA (Foerster et al., 2018), MATD3 (Ackermann et al., 2019), QMIX (Rashid et al., 2020), and MAPPO (Yu et al., 2022).\nUnder the CTDE paradigm, parameter sharing among agents is a commonly adopted practice to improve sample efficiency. However, identical network parameters across agents often lead to homogeneous policies, restricting diversity in behaviors and the overall joint policy representational capacity. This limitation can result in undesired outcomes in certain situations (Christianos et al., 2021; Fu et al., 2022; Kim and Sung, 2023), as shown in Figure 1, impeding further performance\ngains. An alternative approach is the non-parameter sharing scheme, where each agent possesses its own unique parameters. Nevertheless, while this method naturally supports heterogeneous policies, it suffers from reduced sample efficiency, leading to significant training costs. This is particularly problematic given the current trend towards increasingly large model sizes, with some scaling to trillions of parameters (Zhao et al., 2023; Achiam et al., 2023). Therefore, it is imperative to develop a parameter sharing strategy that enjoys both high sample efficiency and broad policy representational capacity, potentially achieving significantly enhanced performance. While several efforts (Christianos et al., 2021; Kim and Sung, 2023) have explored partial parameter sharing initiated at the start of training, such initializations can be challenging to design without detailed knowledge of agent-specific environmental transitions or reward functions (Christianos et al., 2021).\nIn this work, we build upon insights from previous studies (Christianos et al., 2021; Fu et al., 2022; Kim and Sung, 2023) and introduce Kaleidoscope, a novel adaptive partial parameter sharing scheme. It maintains a single set of policy parameters and employs multiple learnable masks to designate the shared parameters. Unlike earlier methods that depend on fixed initializations, Kaleidoscope dynamically learns these masks alongside MARL param-eters throughout the training process. This end-to-end training approach inherently integrates environmental information, and its adaptive nature enables Kaleidoscope to dynamically adjust the level of parameter sharing based on the demands of the environment and the learning progress of the agents. The learnable masks facilitate a dynamic balance between full parameter sharing and non-parameter sharing, offering a flexible trade-off between sample efficiency and policy representational capacity through enhanced heterogeneity. Initially, we build Kaleidoscope upon agent networks, where it achieves diverse policies. Following this success, we extend it to multi-agent actor-critic algorithms to encourage heterogeneity among the central critic ensembles for further performance enhancement.\nJust like a kaleidoscope uses the reflective properties of rotating mirrors to transform simple shapes into beautiful patterns, our proposed method leverages learnable masks to map a single set of parameters into diverse policies, thereby enhancing task performance.\nWe summarize our contributions as follows:\n\u2022 To enable policy heterogeneity among agents for better training flexibility, we adapt the soft threshold reparameterization (STR) technique to learn distinct masks for different agent net-works while only maintaining one set of common parameters, effectively balancing between full parameter sharing and non-parameter sharing mechanisms.\n\u2022 To enhance policy diversity among agents, we introduce a novel regularization term that encour-ages the pairwise discrepancy between masks. Additionally, we design resetting mechanisms that recycle masked parameters to preserve the representational capacity of the joint networks.\n\u2022 Through extensive experiments on MARL benchmarks, including multi-agent particle environ-ment (MPE) (Lowe et al., 2017), multi-agent MuJoCo (MAMuJoCo) (Peng et al., 2021) and StarCraft multi-agent challenge v2 (SMACv2) (Ellis et al., 2024), we demonstrate the superior performance of Kaleidoscope over existing parameter sharing approaches."}, {"title": "2 Background", "content": "Multi-agent reinforcement learning (MARL) In MARL, a fully cooperative partially observable multi-agent task is typically formulated as a decentralized partially observable Markov decision process (dec-POMDP) (Oliehoek and Amato, 2016), represented by a tuple M = (S, A, P, R, \u03a9, \u039f, \u039d, \u03b3). Here, N denotes the number of agents, and \u03b3 \u2208 (0,1] represents the discount factor. At each timestep t, with the environment state as $s_t \u2208 S$, agent i receives a local observation $o_i \u2208 \u03a9$ drawn from the observation function $O(s_t, i)$ and then follows its local policy"}, {"title": "3 Learnable Masks for Heterogenous MARL", "content": "In this section, we propose using learnable masks as a low-cost method to enable network heterogene-ity in MARL. The core concept, illustrated in Figure 2, is to learn a single set of shared parameters complemented by multiple masks for distinct agents, specifying which parameters to share.\nSpecifically, in Section 3.1, we first adapt STR into a dynamic partially parameter sharing method, unlocking the joint policy network's capability to represent diverse policies among agents. In Section 3.2, we actively foster policy heterogeneity through a novel regularization term based on the masks. Given that the masking technique could excessively sparsify the network, potentially diminish-ing its representational capacity, in Section 3.3, we propose a straightforward remedy to periodically reset the parameters based on the outcomes of masking, which additionally mitigates primacy bias. Finally, in Section 3.4, we explore how to further extend this approach within the critic components of actor-critic algorithms to improve value estimations in MARL and further boost performance.\nFor the sake of clarity, we integrate the proposed Kaleidoscope with the MATD3 (Ackermann et al., 2019) algorithm to demonstrate the concept within this section. Nevertheless, as a versatile partial parameter-sharing technique, our method can readily be adapted to other MARL algorithms. We defer its integration with other MARL frameworks to Appendix A.1.2 and will evaluate them empirically in Section 4."}, {"title": "3.1 Adaptive partial parameter sharing", "content": "The core idea of this work is to learn distinct binary masks $M_i$ for different agents to facilitate differentiated policies, ultimately aiming to improve MARL performance. To achieve this, we apply the STR (Kusupati et al., 2020) technique to the policy parameters with different thresholds dedicated to each agent:\n$\\theta_i = \\theta_0 \\odot M_i,$\\nwhere $\\theta_i$ parameterizes the policy for agent i, $\\theta_0$ is the set of learnable parameters shared by all agents, and $M_i$ is the learnable mask for agent i. Specifically, assume $\\theta_0 = [\\theta_0^{(1)},...,\\theta_0^{(N_a)}]$ and $M_i = [m_i^{(1)},...,m_i^{(N_a)}]$, with $N_a$ being the total parameter count of an agent's network. In line with STR, we compute each element $m_i^{(k)}$ of $M_i$ as $m_i^{(k)}= \\mathbb{1} [|\\theta_0^{(k)}| > \\sigma(\\epsilon_i^{(k)})]$, where $\\sigma(\\cdot)$ denotes the Sigmoid function.\nThe benefits of such a combination are summarized as follows:\n\u2022 Preservation of original MARL learning objectives: Unlike most of the methods in pruning literature, which primarily aim to minimize the discrepancies between pruned and unpruned networks in terms of weights, loss, or activations (Hoefler et al., 2021; Menghani, 2023; Deng et al., 2020), STR maintains the original goal of minimizing task-specific loss, aligning directly with our objectives to enhance MARL performance.\n\u2022 Flexibility in sparsity: Many classical pruning methods require predefined per-layer sparsity levels (Evci et al., 2020; Ramanujan et al., 2020). Such requirements can complicate our design, with the goal not to gain extreme sparsity but rather to promote heterogeneity through masking. The STR technique is ideal in our case as it does not require predefining sparsity levels, allowing for adaptive learning of the masks.\n\u2022 Enhanced network representational capacity: Utilizing learnable masks for adap-tive partial parameter sharing enhances the network's representational capacity beyond traditional full parameter sharing. In full parameter sharing, agents' joint policies are parameterized as $\\pi_{PS}(\\cdot|\\theta_0) = (\\pi_1(\\cdot|\\theta_0), ..., \\pi_n(\\cdot|\\theta_0))$. In contrast, our pro-posed adaptive partial parameter sharing mechanism parameterizes the joint policies as $\\pi_{Kaleidoscope}(\\cdot|\\theta_0, M) = (\\pi_1(\\cdot|\\theta_0 \\odot M_1), ..., \\pi_n(\\cdot|\\theta_0 \\odot M_N))$. In the extreme case where all the values in $M_i$ are 1s, the function set represented by $\\pi_{Kaleidoscope}(\\cdot|\\theta_0, M)$ degrades to that of $\\pi_{PS}(\\cdot|\\theta_0)$. In other scenarios, it is a superset of that represented by $\\pi_{ps}(\\cdot|\\theta_0)$."}, {"title": "3.2 Policy diversity regularization", "content": "While independently learned masks enable agents to develop distinct policies, without a specific in-centive, these policies may still converge to being homogeneous. To this end, we propose to explicitly encourage agent policy heterogeneity by introducing a diversity regularization term maximizing the weighted pairwise distance between network masks, which is defined as\n$J^{div}(\\mathcal{S}) = \\sum_{i=1,...,n} \\sum_{j=1,...,n \\atop j\\neq i} || \\theta_0 \\odot (M_i - M_j)||_1.$\nThis term is inherently non-differentiable due to the indicator function $\\mathbb{1}[\\cdot]$ inside $M$. To overcome this difficulty, following established practices in the literature (Bengio et al., 2013; Alizadeh et al., 2018), we utilize a surrogate function for gradient approximation:\n$\\frac{\\partial J^{div}}{\\partial g(s_i)} = -tanh(\\frac{\\partial J^{div}}{\\partial M_i}).$\\nWe formally provide the overall training objective for actors in Appendix A.1.1."}, {"title": "3.3 Periodically reset", "content": "As the training with masks proceeds, we observe an increasing sparsity in each agent's network, potentially reducing the overall network capacity. To remedy the issue, we propose a simple approach to periodically reset the parameters that are consistently masked across all $M_i$ with a certain probability p, which is illustrated in Figure 3a. At intervals defined by $t \\mod reset\\_interval == 0$, if the parameter index k satisfies $\\forall i, m_i^{(k)} == 0$, we apply the following resetting rule\n$\\theta_0^{(k)}, \\epsilon_1^{(k)},..., \\epsilon_N^{(k)} \\leftarrow\\{\\begin{array}{ll} Reinitialize[\\theta_0^{(k)}, \\epsilon_1^{(k)},..., \\epsilon_N^{(k)}] & \\text{with probability } p\\\\ \\theta_0^{(k)}, \\epsilon_1^{(k)},..., \\epsilon_N^{(k)} & \\text{with probability } 1 - p \\end{array}.$\nThis resetting mechanism recycles the weights masked as zeros by all the masks, preventing the networks from becoming overly sparse. A side benefit of this resetting mechanism is the enhancement of neural plasticity (Lyle et al., 2023; Nikishin et al., 2024), which helps alleviate the primacy bias (Nikishin et al., 2022) in reinforcement learning. Unlike methods that reinitialize entire layers resulting in abrupt performance drops (Nikishin et al., 2022), our resetting approach selectively targets weights as indicated by the learnable masks, thus avoiding significant performance disruptions, as shown in Section 4."}, {"title": "3.4 Critic ensembles with learnable masks", "content": "In actor-critic algorithm frameworks, we further apply Kaleidoscope to central critics as an efficient way to implement ensemble-like critics. By facilitating dynamic partial parameter sharing, Kalei-doscope enables heterogeneity among critic ensembles. Furthermore, by regularizing the diversity"}, {"title": "Adaptive partial parameter sharing for critic ensembles", "content": "In the standard MATD3 algo-rithm (Ackermann et al., 2019), two critics with independent parameters are maintained to mitigate overestimation risks. However, using separate parameters typically results in a low update-to-data (UTD) ratio (Hiraoka et al., 2022). To address this issue, we propose to enhance the UTD ratio by employing Kaleidoscope parameter sharing among ensembles of critics. Specifically, we maintain a single set of parameters $\\phi_0$ and K masks $[M^c_k]_{k=1}^K$ to distinguish the critic functions, resulting in K ensembles $[Q(\\cdot; \\phi_j)]_{j=1}^K$ with $\\phi_j = \\phi_0 \\odot M^c_j$.\nTo be specific, we update the critic networks by minimizing the temporal difference (TD) error loss\n$L_c(\\phi_j) = E_{(\\mathcal{S}, a, r, \\mathcal{S}')\\sim\\mathcal{D}}[(y^t - Q(\\mathcal{S}, a^t; \\phi_j))^2]$,\nwith\n$y^t = r^t + \\gamma min_{j=1,...,K} Q(\\mathcal{S}', \\pi_1(\\mathcal{o}_1'; \\theta_1') + \\epsilon, ..., \\pi_n(\\mathcal{o}_n'; \\theta_n') + \\epsilon; \\phi_j)$.\nAnd the policies are updated by the mean estimation of the ensembles as\n$\\nabla_{\\theta_i} J(\\theta) = E_{\\mathcal{S}\\sim\\mathcal{D}} \\nabla_{\\theta_i} \\pi_{\\theta_i}(\\mathcal{o}; \\theta_i) \\frac{1}{K}\\sum_{j=1}^K [Q(\\mathcal{S}, a_1, ..., a_N|a_i = \\pi_{\\theta_i}(\\mathcal{o}; \\theta_i); \\phi_j)]$"}, {"title": "Critic ensembles diversity regularization", "content": "As in Section 3.2, we also apply diversity regular-ization to critic masks to prevent critics functions from collapsing to identical ones. The diversity regularization to maximize for the critic ensembles is expressed as\n$J^{div}(\\mathcal{S}^c) = \\sum_{i=1,...,K} \\sum_{j=1,...,K \\atop j\\neq i} || \\phi_0 \\odot (M_i^c - M_j^c)||_1.$\nIntuitively, as training progresses, this term encourages divergence among the critic masks, leading to increased model estimation uncertainty. This process fosters a gradual shift from overestimation to underestimation. As discussed in prior research (Hiraoka et al., 2022; Lan et al., 2020; Chen et al., 2021; Wang et al., 2021b), overestimation can encourage exploration, beneficial in early training stages, whereas underestimation alleviates error accumulation (Fujimoto et al., 2018), which is preferred in the late training stage. We formally provide the overall training objective for critic ensembles in Appendix A.1.1."}, {"title": "Periodically reset", "content": "To further promote diversity among critic ensembles and counteract the reduction in network capacity caused by masking, we implement a resetting mechanism similar to that described in Section 3.3. In particular, we sequentially reinitialize the masks $M^c$ following a cyclic pattern, as illustrated in Figure 3b. In this way, each critic function's mask is trained on distinct data segments, leading to different biases.\nIn summary, by adopting Kaleidoscope parameter sharing with learnable masks, we establish a cost-effective implementation for critic ensembles that enjoy a high UTD ratio. Through enforcing distinctiveness among the masks, we subtly control the differences among critic functions, thereby improving the value estimations in MARL."}, {"title": "4 Experimental Results", "content": "In this section, we integrate Kaleidoscope with the value-based MARL algorithm QMIX and the actor-critic MARL algorithm MATD3, and evaluate them across eleven scenarios in three benchmark tasks."}, {"title": "4.1 Experimental Setups", "content": "Environment descriptions We test our proposed Kaleidoscope on three benchmark tasks: MPE (Lowe et al., 2017), MaMuJoCo (Peng et al., 2021) and SMACv2 (Ellis et al., 2024). For the discrete tasks MPE and SMACv2, we integrate Kaleidoscope and baselines with QMIX (Rashid et al., 2020) and assess the performance. For the continuous task MaMuJoCo, we employ MATD3 (Ackermann et al., 2019). We use five random seeds for MPE and MaMuJoCo and three random seeds for SMACv2, reporting averaged results and displaying the 95% confidence interval with shaded areas. The chosen benchmark tasks reflect a mix of discrete and continuous action spaces and both homogeneous and heterogeneous agent types, detailed further in Appendix A.2.\nBaselines In the following, we compare our proposed Kaleidoscope with baselines (Christianos et al., 2021; Kim and Sung, 2023), as listed in Table 1. For both Kaleidoscope and the baselines, in scenarios with fixed agent types (MPE and MaMuJoCo), we assign one mask per agent. For SMACv2, where agent types vary, we assign one mask per agent type. We use official implementations of the baselines where available; otherwise, we closely follow the descriptions from their respective papers, integrating them into QMIX or MATD3. Hyperparameters and further details are provided in Appendix A.1.3."}, {"title": "4.2 Results", "content": "Performance We present the comparative performance of Kaleidoscope and baselines in Figure 4 and Figure 5. Overall, Kaleidoscope demonstrates superior performance, attributable to the flexibility of the learnable masks and the effectiveness of diversity regularization. Additionally, we observe that FuPS + ID generally outperforms NoPS, except for the Ant-v2-4x2 scenario (Figure 4c). This advantage is largely due to FuPS's higher sample efficiency; a single transition data sample updates the model parameters N times in FuPS + ID, once for each agent, compared to just once in NoPS. Consequently, FuPS + ID models learn faster from the same number of transitions. Similarly, Kaleidoscope benefits from this mechanism as it shares weights among agents, allowing a single transition to update the model parameters multiple times. Furthermore, by integrating policy heterogeneity through learnable masks, Kaleidoscope enables diverse agent behaviors, as illustrated in the visualization results in Figure 8. Ultimately, Kaleidoscope effectively balances parameter sharing and diversity, outperforming both full parameter sharing and non-parameter sharing approaches.\nCost analysis Despite its superior performance, Kaleidoscope does not increase computational com-plexity at test time compared to the baselines. We report the test time averaged FLOPs comparison of Kaleidoscope and baselines in Table 2. We see that due to the masking technique, Kaleidoscope has lower FLOPs compared to baselines, thereby enjoying a faster inference speed when being deployed."}, {"title": "Ablation studies", "content": "We conduct ablation studies to assess the impact of key components in Kaleido-scope, with results presented in Figure 6. Specifically, we compare Kaleidoscope with three ablations: 1) Kaleidoscope w/o reg, which lacks the regularization term in Equation (8) that encourages the masks to be distinct. 2) Kaleidoscope w/o reset, which does not reset parameters. 3) Kaleidoscope w/o ce, which does not use Kaleidoscope parameter sharing in critic ensembles and instead maintains two independent sets of parameters for critics. From the results, we observe that diversity regular-ization contributes the most to the performance of Kaleidoscope. Without it, masking degrades the performance due to the reduced number of parameters in each policy network. Resetting primarily aids learning in the late stages of training when needed, which aligns with the observation made by Nikishin et al. (2022). Notably, even with resetting, the performance does not experience abrupt drops thanks to the guidance provided by the masks on where to reset. When ablating the critic ensembles with Kaleidoscope parameter sharing, we observe inferior performance from the beginning of the training. This is because the critic ensembles with Kaleidoscope parameter sharing enable a higher UTD ratio of the critics, as discussed in Section 3.4.\nFurthermore, we conduct experiments to study the impact of mask designs. The results are shown in Figure Figure 7. Specifically, we compare original Kaleidoscope with two alternative mask design choices: 1) Kaleidoscope w/ neuron masks, where adaptive masking techniques are applied to neurons rather than weights. 2) Kaleidoscope w/ fixed masks, where the masks are initialized at the beginning of training and kept fixed throughout the learning process. The results show that performance drops with either alternative design choice, demonstrating that Kaleidoscope's superior performance originates from the flexibility of the learnable masks on weights.\nMore results on hyperparameter analysis are included in Appendix B.2."}, {"title": "Visualization", "content": "We visualize the trained policies of Kaleidoscope on World, as shown in Figure 8a. The agents exhibit cooperative divide-and-conquer strategies (four red agents divide into two teams and surround the preys), contrasting with the homogeneous policies depicted in Figure 1. We further examine the distinctions in the agents' masks and present the results in Figure 8b. First, we observe that by the end of the training, each agent has developed a unique mask, revealing that distinct masks facilitate diverse policies by selectively activating different segments of the neural network weights. Second, throughout the training process, we note that the differences among the agents' masks evolve dynamically. This observation confirms that Kaleidoscope effectively enables dynamic parameter sharing among the agents based on the learning progress, empowered by the adaptability of the learnable masks. More visualization results are provided in Appendix B.3."}, {"title": "5 Related Work", "content": "Parameter sharing First introduced by Tan (1993), parameter sharing has been widely adopted in MARL algorithms (Foerster et al., 2018; Rashid et al., 2020; Yu et al., 2022), due to its simplicity and high sample efficiency (Grammel et al., 2020). However, schemes without parameter sharing typically offer greater flexibility for policy representation. To balance sample efficiency with policy representational capacity, some research efforts aim to find effective partial parameter sharing schemes. Notably, SePS (Christianos et al., 2021) first clusters agents based on their transitions at the start of training and restricts parameter sharing within these clusters. Subsequently, SNP (Kim and Sung, 2023) enables partial parameter sharing by utilizing the lottery ticket hypothesis (Su et al., 2020) to initialize heterogeneous network structures. Concurrent to our work, AdaPS (Li et al., 2024) combines SNP and SePS by proposing a cluster-based partial parameter sharing scheme. While these methods have shown promise in certain domains, their performance potential is often limited by the static nature of the parameter sharing schemes set early in training. Our proposed Kaleidoscope distinguishes itself by dynamically learning specific parameter sharing configurations alongside the development of MARL policies, thereby offering enhanced training flexibility.\nAgent heterogeneity in MARL To incorporate agent heterogeneity in MARL and enable diverse behaviors among agents, previous methods have explored concepts such as diversity and roles. Specifically, diversity-based approaches aim to enhance pairwise distinguishability among agents based on identities (Jiang and Lu, 2021), trajectories (Li et al., 2021), or credits assignment (Liu et al., 2023; Hu et al., 2023) through contrastive learning techniques. Concurrently, role-based strategies, sometimes referred to as skills (Yang et al., 2020) or subtasks (Yuan et al., 2022), employ conditional policies to differentiate agents by assigning them to various conditions. These conditions may be based on agent identities (Yang et al., 2022), local observations (Yang et al., 2020), local"}, {"title": "6 Conclusions and Future Work", "content": "In this work, we introduced Kaleidoscope, a novel adaptive partial parameter sharing mechanism for MARL. It leverages distinct learnable masks to facilitate network heterogeneity, applicable to both agent policies and critic ensembles. Specifically, Kaleidoscope is built on three technical components: STR-empowered learnable masks, network diversity regularization, and a periodic resetting mecha-nism. When applied to agent policy networks, Kaleidoscope balances sample efficiency and network representational capacities. In the context of critic ensembles, it improves value estimations. By combining our proposed Kaleidoscope with QMIX and MATD3, we have empirically demonstrated its effectiveness across various MARL benchmarks. This study shows great promises in developing adaptive partial parameter sharing mechanisms to enhance the performance of MARL. For future work, it is interesting to further extend Kaleidoscope to other domains such as offline MARL or meta-RL."}, {"title": "B.1 Detailed Costs", "content": "We provide per-scenario FLOPs across different methods in Table 8 as a supplement for Table 2."}, {"title": "B.2 Hyperparameter Analysis", "content": "We conduct further analysis on the hyperparameters a and B, and present the results in Figure 9. The hyperparameter a controls the variance of the critic ensembles. As shown in Figure 9a, we observe that an excessively small a results in degraded performance because it reduces the critic ensembles to a single critic network, causing the value estimation to suffer from severe overestimation. Conversely, an excessively large a also deteriorates performance, possibly due to increased estimation bias. For B, as illustrated in Figure 9b, an overly small \u1e9e leads to degraded performance because it reduces the Kaleidoscope parameter sharing to full parameter sharing, confining the policies to be identical. An overly large \u1e9e also negatively impacts performance, as it may cause the training objective to deviate too much from minimizing the original MARL loss.\nIn general, we recommend setting both hyperparameters between 0.1 and 1. However, the optimal hyperparameter values may vary across different scenarios. For fair comparisons, we maintain the same set of hyperparameters across all scenarios in our experiments. Nevertheless, further tuning of these hyperparameters has the potential to enhance performance."}, {"title": "B.3 Further Visualization Results", "content": "To better understand how learnable masks in Kaleidoscope affect the performance through policies, we visualize the pairwise mask differences among agents and the agent trajectories at different training stages in Figure 10. As training progresses, the test return increases and diversity loss decreases, indicating better performance and greater diversity among agent policies. Correspondingly, mask differences among agents increase, and the agent trajectory distribution becomes more diverse."}, {"title": "B.4 Limitations", "content": "Here we discuss some limitations of Kaleidoscope.\nFirst, as suggested by results in Appendix B.2, the optimal hyperparameters vary from scenario to scenario. Therefore, using the same hyperparameters across all scenarios may not yield the best performance for Kaleidoscope. Developing an automatic scheme that utilizes environmental information to determine these hyperparameters would be beneficial.\nSecond, as the environments used in this work contain no more than 10 agents, we assign a distinct mask for each agent. However, when the problem scales to hundreds of agents, this vanilla implemen-tation may fail. In such cases, a possible approach is to cluster N agents into K (K < N) groups and train K masks with Kaleidoscope. This would reduce computational costs and achieve a better trade-off between sample efficiency and diversity. Within the same group, agents share all parameters, while agents from different groups share only partial parameters. Techniques for clustering agents based on experience, as proposed by Christianos et al. (2021), could be useful."}]}