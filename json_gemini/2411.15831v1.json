{"title": "EFFICIENT AND PRIVATE: MEMORISATION UNDER DIFFERENTIALLY PRIVATE PARAMETER-EFFICIENT FINE-TUNING IN LANGUAGE MODELS", "authors": ["Olivia Ma", "Jonathan Passerat-Palmbach", "Dmitrii Usynin"], "abstract": "Fine-tuning large language models (LLMs) for specific tasks introduces privacy risks, as models may inadvertently memorise and leak sensitive training data. While Differential Privacy (DP) offers a solution to mitigate these risks, it introduces significant computational and performance trade-offs, particularly with standard fine-tuning approaches. Previous work has primarily focused on full-parameter updates, which are computationally intensive and may not fully leverage DP's potential in large models. In this work, we address these shortcomings by investigating Parameter-Efficient Fine-Tuning (PEFT) methods under DP constraints. We show that PEFT methods achieve comparable performance to standard fine-tuning while requiring fewer parameters and significantly reducing privacy leakage. Furthermore, we incorporate a data poisoning experiment involving intentional mislabelling to assess model memorisation and directly measure privacy risks. Our findings indicate that PEFT methods not only provide a promising alternative but also serve as a complementary approach for privacy-preserving, resource-efficient fine-tuning of LLMs.", "sections": [{"title": "Introduction", "content": "Training of accurate machine learning (ML) models often requires access to large, well-curated datasets of high quality. Obtaining access to this data becomes more challenging if the learning task is categorised as \u201chigh risk\" under the recently introduced EU AI act [1, 2]. Moreover, this issue has become particularly important due to a recent surge in interest towards large language models (LLMs), which are often trained on large undisclosed datasets (some of which may inadvertently include private information). This phenomenon has raised a number of data privacy and governance concerns when training such models on sensitive, personally-identifying data [3]. This is primarily because it was recently demonstrated in a number of prior works that LLMs are capable of unintentionally memorising the data they have previously been trained or fine-tuned on [4, 5, 6, 7]. One popular method to alleviate these privacy concerns is the use of differentially private (DP) [8, 9] model training. DP is a property of a randomised algorithm, which makes it approximately invariant to the contributions of a single data point, resulting in a provable upper bound on how much information can be memorised for each participant [9]. This can typically be achieved through the addition of carefully calibrated noise (based on the sensitivity of an algorithm) to the output of a randomized algorithm. In the context of deep learning, this is manifested in a widespread use of differentially private stochastic gradient descent (DP-SGD) [8]. DP-SGD relies on the addition of noise to the gradients of the model during training, where the amount of noise required can be calibrated through gradient clipping to obtain a bounded sensitivity. These two steps, in turn, can significantly lower the utility of the final model [10, 11].\nFurthermore, as the gradient norms of samples need to be clipped individually, this creates additional computational costs associated with the use of DP-SGD, resulting in higher memory requirements and longer training times. This"}, {"title": "Background and Related work", "content": "Differential Privacy (DP) [18] is one of the most widely used information-theoretic frameworks for formal preservation of privacy under public data releases. This is achieved through the addition of noise to the output of the randomised algorithm, thereby limiting the impact a single individual can have on its output and the amount of information that is released is controlled through the privacy budget defined by \u025b (lower - more private). The DP definition used in this work, known as (\u03b5, \u03b4)-DP, extends the initial definition of \u025b-DP, by using an additional parameter d, allowing a small probability of privacy compromise [19]:\n$\\Pr[A(D_1) = x] < e^\\varepsilon \\Pr[A(D_2) = x] + \\delta$\nwhere D\u2081 and D2 are datasets differing by one record (through an addition/removal of an individual under unbounded-DP or a replacement under bounded-DP), A represents a randomized algorithm, and \u025b and 8 are privacy parameters, with smaller & indicating stronger privacy. In ML this can be achieved through the addition of randomised noise to the gradients of the trained model (whose norms need to be clipped to a pre-defined threshold of C in order to bound the sensitivity and add the appropriately calibrated amount of noise) under DP-SGD [8]. However, differentially private training introduces significant computational and performance trade-offs. The additional noise and gradient clipping in DP-SGD limits the information the model can learn during training and can impose substantial memory and processing overhead, particularly challenging for large-scale models such as LLMs [12, 13, 14, 20].\nTo allow efficient fine-tuning of LLMs, a number of PEFT methods that modify only small parts of the model, such as Adapters [21], LoRA [22] and (IA)3 [23] have previously been proposed. A more detailed discussion on the attributes of individual PEFTs can be found in [15]. Each PEFT method targets different parts within the transformer architecture: Adapters add (and fine-tune) compact task-specific layers within the feedforward (FF) layers. LoRA applies low-rank approximations to decompose the weight matrices in transformer models, instead of updating weights directly, it tracks changes to weight matrices during fine-tuning. (IA)\u00b3 selectively scales key activations with learned vectors, with the vectors injected in the attention and FF layers to focus on important features. PEFT techniques have demonstrated successful applications across various tasks by significantly reducing the number of trainable parameters while maintaining or enhancing model performance. For example, Adapter and LoRA achieved state-of-the-art results in speech emotion recognition and graph-based tasks, showcasing their ability to adapt large models efficiently [24, 25]. (IA)\u00b3 excells in few-shot scenarios, outperforming in-context learning while introducing minimal new parameters [23]. Combined approaches like AdaMix and LoRAPrune rely on a mixture of modular adaptations and pruning techniques to enhance PEFT's efficiency and scalability for complex tasks [26, 27]"}, {"title": "Assessing Privacy Leakage in PEFT", "content": "In this work, we empirically assess the potential privacy leakage that occurs during fine-tuning of LLMs, and study the interactions between parameter efficient fine-tuning methods behave with DP with respect to both the utility of the model and the privacy of the training data. To this end, we have conducted a number of black-box loss-based MIAs in non-DP and DP settings, which analyses model output losses to infer the membership of a data point [42]. The Area Under the Curve (AUC) scores serves as the primary metric for quantifying the privacy leakage [43].\nTo investigate the \u201cworst-case\u201d memorisation, we created a number of canaries as part of model training, which were made to deliberately be more atypical (and hence more prone to memorisation) [41]. We achieved this by intentionally mislabelling a set of data points to perform MIAs on these at the end of training [39]. Specifically, we selected 30 instances each from the training and test sets, modifying their labels to create a small subset of canary-like data points. The number of flipped samples was kept small to avoid significantly impacting the overall performance on correctly labelled data, while still allowing us to observe the model's memorisation capacity on these specific data points. After training under both DP and non-DP settings, we conducted a loss-based MIA on the entire dataset and then focused on the AUC scores of this poisoned subset. High AUC scores on this subset would indicate that the model retains memorised associations for these mislabelled instances, thereby demonstrating a tendency towards memorisation."}, {"title": "Results", "content": "We evaluated standard fine-tuning and PEFT methods (LoRA, Adapter, (IA)\u00b3) on IMDb and QNLI across non-DP and DP settings, focusing on task accuracy, memory usage, and training time per epoch.\nAccuracy: As shown in Table 4, PEFT methods matched or exceeded the accuracy of standard fine-tuning. In IMDb non-DP settings, LoRA closely aligned with DistilBERT's accuracy while drastically reducing trainable parameters. This trend held on QNLI, with PEFT methods achieving competitive accuracy levels. Under DP constraints, LoRA even slightly outperformed DistilBERT and BERT-base. At \u025b = 4.0, LoRA achieved an accuracy of 85.3%, compared to 84.3% for DistilBERT and 81.1% for BERT-base indicating robust performance in privacy-sensitive settings.\nMemory Efficiency and Training Speed: PEFT methods provided substantial gains in memory and training efficiency. As shown in Table 5, BERT-base had the highest memory usage, whereas PEFT methods significantly reduced the memory overhead. LoRA and Adapter used less than half the memory of BERT-base in both non-DP and DP settings. Training speed followed a similar pattern, where PEFT methods completed epochs significantly faster than DistilBERT, even under DP training.\nIn summary, PEFT methods, particularly LoRA, offer an effective balance of task performance, memory, and computa-tional efficiency. Despite fewer parameters and a smaller base model, PEFT methods maintain accuracy comparable to standard fine-tuning while significantly reducing memory costs, making them well-suited for resource-constrained, privacy-sensitive applications."}, {"title": "Privacy Leakage: Membership Inference", "content": "Using a loss-based black-box MIA, we quantified the risk of privacy leakage by calculating the AUC score at the final training epoch. As illustrated in Figure 2, standard fine-tuning methods demonstrated greater susceptibility to MIAs compared to PEFT methods. Specifically, non-DP DistilBERT reaching an AUC of 0.59 on the IMDb dataset, indicating increased memorisation and privacy risks. In contrast, PEFT methods like LoRA and Adapter showed lower AUC scores of 0.52 and 0.53, respectively, underscoring their enhanced resilience against membership inference attacks.\nA notable trend was observed with respect to the model size (i.e. their capacity): larger models, consistently exhibited higher AUC values and higher privacy leakage compared to DistilBERT, particularly in the non-DP settings, suggesting that larger models may inherently be more prone to memorisation, amplifying privacy risks in the absence of DP.\nUnder DP constraints, privacy leakage diminished across all settings, with DistilBERT's AUC on IMDb dropping down to 0.50 at \u025b = 8.0, demonstrating DP's efficacy in reducing memorisation. However, PEFT methods showed smaller AUC reductions under DP, implying a weaker interaction with DP mechanisms (in contrast to standard fine-tuning, where the reduction is significant). This pattern was especially evident on QNLI, where LoRA and Adapter maintained relatively stable AUC values, slightly higher than their standard fine-tuning counterpart. The reduced parameter updates in PEFT methods concentrate DP noise on a smaller subset of parameters, which may reduce the overall effectiveness of"}, {"title": "Privacy Leakage: Auditing using canaries", "content": "Using the mislabelled samples (i.e. our canaries), we examined how the models responded to a controlled set of poisonous data points, focusing on whether they had memorised the incorrect feature-label associations.\nAs shown in Figure 3, standard fine-tuning with DistilBERT on the IMDb dataset exhibited a high degree of memorisation in non-DP settings, with elevated AUC values on the flipped subset, indicating a greater risk of privacy leakage. In contrast, PEFT methods like LoRA and Adapter maintained significantly lower AUC scores, demonstrating reduced memorisation and a stronger resilience against privacy attackers. This suggests that, in non-DP settings, PEFT methods effectively limit the sensitivity (in a non-DP sense) of the model to outliers, enhancing privacy preservation. A similar trend was observed on QNLI, where non-DP DistilBERT also showed higher degree of memorisation, while PEFT methods mitigated this effect, reinforcing the privacy advantages of PEFT.\nIn DP settings, the AUC scores on the poisoned subset decreased across all methods, indicating DP's effectiveness in reducing memorisation. However, similar to the previous experiments, PEFT methods like LoRA and Adapter showed less reduction in AUC under DP compared to standard fine-tuning, suggesting that DP mechanisms are less effective in mitigating memorisation for PEFT methods. This is likely the case since DP-SGD can often act as a \"natural regulariser\", limiting the impact that PEFT methods (partially adressing the same issue) may have. Additionally, PEFT updates only a subset of parameters, limiting the noise addition to a smaller parameter set. This means that while the new information obtained during fine-tuning may be memorised less, the initial pre-training data can still be inferred and should the features (or even datapoints) in these datasets overlap, DP training would not be able to offer meaningful protection against MIAs (while still providing meaningful guarantees with respect to fine-tuning information bounds).\nWe also tracked training accuracy on the flipped subset across epochs for both IMDb and QNLI datasets, as shown in Figure 4. Standard fine-tuning with DistilBERT revealed a marked increase in accuracy on the poisoned subset, indicating a higher memorisation capacity on this data. For instance, on IMDb, DistilBERT's accuracy rose notably from 0.20 in the first epoch to over 0.45 by the final epoch, suggesting greater sensitivity to outliers. In contrast, PEFT methods such as LoRA, Adapter, and (IA)\u00b3 maintained consistently low accuracy on the mislabelled subset, with only minor increases over time. This stability reflects their higher robustness against memorisation, as they are less prone to retaining incorrect label associations. For example, LoRA's accuracy on IMDb remained consistently low, and on QNLI, PEFT methods stayed below 0.40, whereas DistilBERT's accuracy climbed sharply to 0.60, underscoring PEFT's improved generalisation and resistance to memorising outliers."}, {"title": "PEFT Parameter Variation", "content": "We tested the hypothesis that PEFT methods are more robust against memorisation only due to their reduced parameter count by varying the parameter count in Adapter and LORA models. As shown in Figure 5, increasing Adapter's bottleneck size led to higher AUC scores on QNLI, rising from 0.58 to 0.71. However, this pattern was inconsistent on IMDb, where AUC scores fluctuated, suggesting that Adapter's sensitivity to the number of trainable parameters may vary based on the dataset. For LoRA, increasing the r values, which controls dimensionality of introduced"}, {"title": "Discussion", "content": "Our findings underscore the effectiveness of PEFT methods, particularly LoRA, as practical alternatives to standard fine-tuning in privacy-sensitive settings. These methods not only maintain high task-specific accuracy, but also dramatically reduce parameter count and memory usage, making it feasible to deploy privacy-preserving models even in resource-limited environments. This efficiency directly addresses critical limitations of standard fine-tuning, supporting robust model performance in scenarios where computational resources might otherwise limit adoption of LLMs and private fine-tuning of such models."}, {"title": "Privacy Resilience Without DP", "content": "In non-DP settings, PEFT methods showed reduced privacy leakage, evidenced by lower MIA AUC scores. This suggests that their parameter-efficient design limits the model's capacity to memorise individual training data points, inherently reducing empirical memorisation risks even without formal privacy protection mechanisms. This observation aligns with recent findings showing that LoRA effectively preserves pre-trained knowledge, which in turn reduces the risk of memorising task-specific fine-tuning data [17]. The fact that PEFT methods can mitigate privacy leakage without DP could make them particularly appealing in real-world applications where DP deployment may be costly or impractical due to constrained computational budget. Our results position PEFT as both a stand-alone privacy-preserving measure and a complementary solution in cases where full-model fine-tuning with DP might be infeasible. By reducing memorisation without requiring DP, PEFT methods broaden the options for model deployment in real-world applications with fewer privacy risks stemming from fine-tuning memorisation."}, {"title": "Investigating Induced Memorisation", "content": "The data poisoning experiment experiment provided compelling evidence of PEFT's lower tendency to memorise outlying, often more sensitive data. Lower AUC and accuracy scores on the flipped subsets for LoRA and Adapter, as compared to standard fine-tuning, indicate a reduced risk of memorisation, enhancing privacy protection for unique or rare data points. The results from this experiment show that PEFT methods can potentially limit model memorisation of unintended patterns, suggesting a greater resilience against privacy risks even without additional privacy controls."}, {"title": "DP's Weaker Effectiveness on PEFT Methods", "content": "While DP effectively reduced memorisation in standard fine-tuning, its impact on PEFT methods was comparatively weaker. Specifically, PEFT models retained higher AUC values in DP settings compared to standard fine-tuning, indicating that DP mechanisms may not interact as effectively with PEFT's parameter-efficient structure. This may be due to PEFT's focus on updating only a small subset of parameters, which can concentrate the DP noise within these limited updates, and potentially reduce DP's ability to mitigate privacy risks across the entire model. These findings highlight the need to develop DP strategies optimised for PEFT architectures, potentially by dynamically adjusting noise distribution to enhance privacy protection across the full model. Such approaches could maximise privacy utility in sensitive applications while maintaining computational efficiency."}, {"title": "Parameter Variation and Robustness to Memorisation", "content": "Our parameter variation experiments revealed that PEFT robustness may not solely come from a reduced parameter count. LoRA and Adapter maintained low AUC scores across configurations with increased parameters, demonstrating limited increases in privacy leakage even with more parameters. These findings highlight that other factors, such as how and where PEFT modules are integrated within the model, could play a significant role in privacy preservation (i.e. adaptations of the model similar to [50]). For example, embedding PEFT modules within attention mechanisms (LoRA) versus FF layers (Adapters), or within different attention subcomponents (key-value pairs, query vectors or output layers) [22, 21]. Exploring these configurations could yield deeper insights into effective privacy-preserving strategies for fine-tuning LLMs."}, {"title": "Limitations and Future Directions", "content": "Our study focused on specific models (BERTs), PEFT methods (LoRA, Adapter, (IA)\u00b3) and datasets (IMDb, QNLI), which may limit the generalizability of our conclusions. Future work should expand the scope by exploring a wider variety of PEFT configurations and task domains to validate the consistency of these privacy-preserving benefits. Additionally, adaptive DP mechanisms tailored specifically to PEFT structures could improve privacy without sacrificing model utility, laying the groundwork for more secure and efficient fine-tuning practices in increasingly privacy-sensitive applications.\nAnother potential direction for future research involves examining the robustness of PEFT methods when applied to biased datasets or minority groups. Assessing PEFT's privacy and utility on imbalanced or skewed data distributions will be essential for understanding its effectiveness in protecting underrepresented groups from privacy risks [34]. In addition, expanding the range of privacy assessment techniques to include broader methods, such as the model inversion attacks, which aim to reconstruct sensitive training data from intermediate representations of the model, could provide a more comprehensive evaluation of privacy risks associated with PEFT methods under DP [51]. Furthermore, incorporating advanced inference techniques like RMIA [37] would allow us to test PEFT methods against stronger membership inference scenarios, allowing us to consider different threat models where the adversary is able to satisfy the assumption of having access to a certain (small) number of real data points, significantly reducing their computation requirements stemming from the need to train many shadow models. As the demand for privacy-preserving LLMs grows, these advancements will support more ethical, secure, and resource-efficient deployments in sensitive applications."}, {"title": "Conclusions", "content": "This study highlights the potential of EFT methods, particularly LoRA and Adapter, as both complementary and alternative approaches to standard fine-tuning for privacy-sensitive applications. Our results show that PEFT methods preserve task-specific performance while reducing memory usage and privacy leakage. The data poisoning experiment provided direct evidence of PEFT's reduced memorisation tendencies, reinforcing their suitability for privacy-sensitive scenarios. While combining PEFT with DP may provide optimal privacy protection, PEFT alone offers valuable privacy benefits when DP training is infeasible.\nFurthermore, our findings reveal that PEFT's robustness against privacy risks may not solely depend on fewer parameters, but could also be influenced by its architectural design. Future research should explore the impact of PEFT's architectural placement within large language models, particularly within attention and FF layers, to further optimize their privacy-preserving capabilities. In DP settings, while PEFT methods generally reduced privacy leakage, their interaction with DP was less pronounced than with the standard fine-tuning, suggesting potential avenues for refining DP-PEFT compatibility. These insights pave the way for more resource-efficient, secure fine-tuning practices in privacy-critical applications."}]}