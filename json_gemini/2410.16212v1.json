{"title": "Comprehensive benchmarking of large language models\nfor RNA secondary structure prediction", "authors": ["L.I. Zablocki", "L.A. Bugnon", "M. Gerard", "L. Di Persia", "G. Stegmayer", "D.H. Milone"], "abstract": "Inspired by the success of large language models (LLM) for DNA and proteins, several LLM for RNA have been\ndeveloped recently. RNA-LLM uses large datasets of RNA sequences to learn, in a self-supervised way, how to represent\neach RNA base with a semantically rich numerical vector. This is done under the hypothesis that obtaining high-quality\nRNA representations can enhance data-costly downstream tasks. Among them, predicting the secondary structure is a\nfundamental task for uncovering RNA functional mechanisms. In this work we present a comprehensive experimental\nanalysis of several pre-trained RNA-LLM, comparing them for the RNA secondary structure prediction task in an\nunified deep learning framework. The RNA-LLM were assessed with increasing generalization difficulty on benchmark\ndatasets. Results showed that two LLM clearly outperform the other models, and revealed significant challenges for\ngeneralization in low-homology scenarios.", "sections": [{"title": "Introduction", "content": "Ribonucleic acid (RNA) plays a crucial role in many fundamental biological processes, such as gene expression, cell\nsignaling, and post-transcriptional regulation [20, 56]. As in proteins, RNAs function and interaction with other molecules\nare deeply related to their structure. For example, determining RNA structure is essential for RNA-based therapeutics\nsuch as mRNA vaccines [34]. Among all RNA transcripts, only 5% is responsible for protein coding. At the same time, a\nvery large remaining portion is non-coding RNA (ncRNA) [7], which in many cases adopt specific structures to perform\nimportant biological functions [6]. Experimental results show that, to some degree, sequence determines the secondary\nstructure, and thus, their function [33, 16, 52]. Therefore, the long-established and yet unresolved RNA secondary\nstructure prediction is major challenge in computational biology today [4, 5].\nDuring many years, experimental molecular structure technologies like nuclear magnetic resonance, X-ray crystallogra-\nphy and cryogenic electron microscopy have produced several RNA structures [19, 45]. However, despite the large number\nof ncRNA sequences available, most of their structures and functions remain still unknown [57]. At the same time that\nlarge amounts of unlabeled RNA sequence data were produced by high-throughput sequencing technologies, pre-trained\nRNA language models have started to be used for modeling the semantic space of RNA sequences, intending to facilitate\nits understanding. Motivated by the success of large language models (LLM) in proteins [32, 37, 18, 3, 2, 39, 14, 31] and\nDNA [26], several RNA LLM have recently appeared [1, 36, 8, 55, 61, 58, 54] with the potential to be used for improving\nseveral important RNA-related tasks, among which one of the most relevant is RNA secondary structure prediction.\nRNA-LLM attempt to effectively embed RNA bases using deep representation learning, in particular the one developed\nin the field of natural language processing [18, 39]. Most RNA-LLM are based on bidirectional encoder representations\nfrom transformers (BERT), which was designed to generate context-sensitive distributed test token (word) representa-\ntions [12]. The relevance of LLM for transfer learning from sequences to downstream tasks have been analyzed in detail for\nproteins [25, 53]. In the case of RNA, the hypothesis is that LLM representation of nucleotide composition and sequence\nmotifs can help characterize the structure and function of a sequence, analogously to how the meaning of a sentence is\ndetermined by the grammatical structure of natural language. Thus, word embedding techniques for natural language\nhave been applied to bases for RNA sequences, obtaining a wide variety of BERT-based architectures trained on different\ndatabases. However, in spite that some theoretical reviews on the applications and utility of LLM in bioinformatics have\nrecently appeared [26, 50, 35], to the best of our knowledge all the available RNA-LLM have not yet been comparatively\nevaluated for the secondary structure prediction task, considering a fair experimental setup, with the same datasets and\nstructure prediction model. Since LLM are pre-trained models, it is important to analyze how they were trained to\nassess generalization capabilities on new sequences/structures, considering homology-aware partitions and cross-family\npredictions.\nIn this work, we provide a comprehensive experimental comparison of the latest pretrained RNA-LLM, using a\nunified and consistent experimental setup. It offers an independent, third-party assessment following the best practice\nguidelines for bias-free evaluations in the community [60]. We provide a description of each model, together with an\nexperimental testing in four benchmarks of increasing complexity, with the corresponding source code and datasets to\nensure reproducibility, supporting the development and boosting of future improvements on RNA secondary structure\nprediction based on LLM."}, {"title": "Results", "content": "Self-supervised RNA-LLM and prediction model\nIn the last three years, a number of RNA-LLM have appeared in literature. We summarize in Table 1 the most recent\nRNA-LLM, included in this study according to their availability as open source tools. All models and pre-training methods\nare detailed in the Methods section.\nFor the comparative analysis, embeddings of each RNA-LLM feed the same deep learning architecture (Figure la-e)\nfor RNA secondary structure prediction. The rationale behind this is that it should not be necessary to optimize the\nclassifier hyperparameters for each LLM, since they have complex architectures and have already been trained with a\nlarge amount of data. Thus, according to the transfer learning paradigm, the embeddings will have enough information\nto solve the downstream task. Moreover, any difference measured in performance will be due to the LLM and not because\nof the classifier architecture. This is the only trainable part of the experimental setup, which uses the train-test partitions\ndefined for each benchmark dataset (detailed in Methods).\nLLM separate structural families even when they were not pre-trained with family infor-\nmation\nA useful RNA embedding for the RNA secondary structure prediction task is expected to well-represent not only sequence\nproperties, but also structural aspects. Therefore, RNA embeddings encoding similar features should share a common\nregion in the multidimensional embedding space. To have a preliminary insight into this topic, a qualitative comparison\nof LLM embeddings of the ArchiveII dataset was performed by nonlinearly reducing them to three dimensions using\nthe uniform manifold approximation and projection (UMAP) [30]. After that, each RNA sequence was depicted with a\ndifferent color according to its corresponding RNA family (Fig. 1f-1).\nThe one-hot encoding (Fig. 1f) was used as reference. In this case, as for RNABERT (Fig. 1g) representations, there\nis a large overlap and mixing of sequences that belong to different RNA families; that is, in the same close region there are\nsequences that are very different in length, structure, and function. Instead, in the case of RNA-FM (Fig. 1h), ERNIE-\nRNA (Fig. 1j) and RiNALMo (Fig. 11) each RNA family is mostly separated from the others. The 5s family (gray), which\nis the largest family, is distributed in several regions of the projected space. In the case of RNA-FM, the 5s, tmRNA (red)\nand tRNA (pink) families are well-separated among them. Overall, it can be stated that for RiNALMo and ERNIE-RNA,\nmost RNA sequences and families are well-separated in the 3D projected space. In the case of RNA-MSM (Fig. 1i), there\nare two large and separated clusters but with a mix of families within. One large cluster has tmRNA, RNaseP (yellow),\nsrp (brown), 23s (green), and telomerase (violet); while the other cluster has 5s, srp and tRNA. RNAErnie has a mix\nof very well-separated and cohesive groups of sequences belonging to the families tRNA, srp and 5s; but there is also a"}, {"title": "Performance on homology-challenging datasets", "content": "Figure 2 shows the comparative results among the RNA-LLM here reviewed in terms of F\u2081 violin plots. In each plot the\nresults are presented in order, from best (left) to worst (right) median F\u2081. Figure 2a shows the results for 5-fold random\npartitions on the ArchiveII dataset. It can be seen that most predictors achieve a median performance above F\u2081 = 0.60,\nexcept only for the one based on one-hot encoding (F\u2081 = 0.57). The top-3 RNA-LLM achieve a very high performance\n(F\u2081 > 0.90). Those are ERNIE-RNA and RiNALMo, both with F\u2081 = 0.95, and RNA-FM with F\u2081 = 0.91. RNAErnie\nand RNA-MSM achieve an intermediate performance, F\u2081 = 0.76 and F\u2081 = 0.74, respectively. Finally, RNABERT is the\nRNA-LLM with the lowest average performance (F\u2081 = 0.62). For this random k-folding, all RNA-LLM except RNABERT\nachieve higher performance than the classical prediction method (LinearPartition-C [59]).This result shows that the neural\narchitecture used in the classifier has the capability of obtaining high performance in predictions. However, it is well-\nknown that the sequence homology between train and test at random partitions favors methods based on deep learning;\nthus performance tends to be overly optimistic.\nFor a deeper analysis, we have extended the experiments on standard datasets having controlled levels of homology\nbetween training and testing partitions. Figure 2b shows the test F\u2081 for bpRNA dataset, with a partition at 80% sequence-\nidentity cutoff between training (TR0+VL0) and testing (TSO) (see Methods). The results show that now, in a harder\ntesting setup, most methods lowered performance in comparison to random partitions. Here, again, ERNIE-RNA and\nRiNALMo have both the best median scores in the test set, but in this case with a large drop in performance: F\u2081 = 0.68\nand F\u2081 = 0.64, respectively. These two RNA-LLM are the only ones that achieve better performance than the classical\nprediction method (F\u2081 = 0.60). With median performance below it, RNA-FM has F\u2081 = 0.52, RNA-MSM has F\u2081 = 0.42,\nand RNAErnie has F\u2081 = 0.41. Both one-hot and RNABERT reached similar and very poor results, F\u2081 = 0.35 and\nF\u2081 = 0.33, respectively.\nFigure 2c shows the results of testing already trained models with the bpRNA-new dataset, which was designed to\nevaluate the prediction on new RNA families, never seen during training. The performance of all methods has been\nnoticeably reduced, being the median of the RNA-LLM in all cases below the classical folding method (F\u2081 = 0.71). The\nonly model with performance very close to the classical one is ERNIE-RNA, having a median F\u2081 = 0.67 (with large\ndispersion). In all the other cases, the median F\u2081 is below 0.45. Additionally, note that the one-hot encoding moved\nup one more position in the ranking, slightly outperforming RNAErnie in this test set. This large dataset was used to\nanalyze the prediction scores per structural motif, as seen in Fig. 2e. It can be seen that most models behave similarly for\neach motif type, with ERNIE-RNA showing a higher performance in stem base-pair predictions. Overall, ERNIE-RNA,\nRiNALMo and RNA-MSM achieved the best scores for each motif.\nResults on a more challenging dataset are shown in Fig. 2d, where every RNA-LLM was tested in a set of 62 RNA\nsequences whose secondary structures were inferred from high-resolution RNA X-ray 3D structures from the PDB dataset.\nResults clearly show a hard fall in results for all the RNA-LLM, with median performances below F\u2081 = 0.40, lower\nthan the classical folding method, which achieved a moderate performance of F\u2081 = 0.60. Notably in this dataset, the\none-hot encoding is within the top-3 methods, outperforming RNA-FM, RNAErnie and RNABERT; with almost the\nsame performance that RNA-MSM, ERNIE-RNA and RiNALMo. It can be said that, for this prediction challenge, no\nRNA-LLM provides significant improvements of transfer learning over a one-hot encoding. This dataset also contains\nnon-canonical interactions, although most of the methods fail to reach a median F\u2081 score higher than 0.2 (Fig. 2f), with\na slightly better performance for ERNIE-RNA and RiNALMO."}, {"title": "Cross-family benchmarks", "content": "For assessing inter-family performance, a family-fold cross validation in the ArchiveII dataset was performed. That is, one\nfamily was left out for testing per cross-validation fold, and the rest of the families were used for training. This eliminates\nmost of the homology to the training set, providing a hard measure of performance and, thus, allowing estimating future\nperformance on novel RNAs that do not belong to any known family. These results are shown in Fig. 3, along with the\ndistributions of sequence lengths and minimum structural distances [38] between each test sequence and all the sequences\nin its corresponding training fold.\nIn the case of the tRNA family (Fig. 3a), the one with the shortest sequence length as shown in the panel below, most\nmethods achieve performance above F\u2081 = 0.40. RNABERT and one-hot achieved both the same performance here, close\nto F\u2081 = 0.46. The four best methods for this family are RiNALMO (F\u2081 = 0.86), ERNIE-RNA (F\u2081 = 0.84), RNAErnie\n(F\u2081 = 0.81) and RNA-FM (F\u2081 = 0.79). In this case, which can be considered the easiest one for a cross-family validation\ndue to the small length of sequences and test/train structural distance below 0.5, four of the RNA-LLM outperform\nthe classical LinearPartition-C, which achieved F\u2081 = 0.75. For the 5s family (Fig. 3b), the results for all RNA-LLM\nare very similar to the previous family, again due to the length of the sequences (the second mean shortest family)\nand low structural distance between this family and all the remaining families in the training set. In this case, one-hot\nrepresentation, RNABERT and RNA-MSM are below F\u2081 = 0.26, while RNAErnie and RNA-FM achieve performances\naround F\u2081 = 0.55, and again ERNIE-RNA and RiNALMo achieved both the best results, with F\u2081 = 0.84 and F\u2081 = 0.82,\nrespectively, above the classical method (F\u2081 = 0.78). For the tmRNA (Fig. 3c) and RNaseP (Fig. 3d) families, the"}, {"title": "Discussion", "content": "We benchmarked 6 RNA-LLM for their ability to predict RNA secondary structure structure based only on the sequence,\nin 4 datasets of increasing complexity. First we performed a visual comparison of the LLM embeddings with a UMAP\nprojection, analyzing the distribution and separation of the RNA families. We found that RiNALMo and ERNIE-RNA\nwere the models that could better represent and separate the RNA families in the projection without almost overlap.\nNotably, as Table 1 shows, those RNA-LLM are the biggest ones, pre-trained with the largest and most varied datasets.\nWe tested each RNA-LLM with exactly the same experimental setup and deep neural network architecture for RNA\nsecondary structure prediction in several benchmark datasets, increasing the difficulty from simple random partitions\nto very low homology between training and testing partitions. Overall, RiNALMo and ERNIE-RNA maintained an\nacceptable level of generalization capability even in the hardest test, providing equivalent performance to the classical\nfolding method. Remarkably, in that hardest scenario the trivial one-hot representation achieved a performance similar\nto the best performing RNA-LLM. Thanks to the transfer learning paradigm, one would have expected that LLM would\nhelp to generalize even with a small training set and prediction model, because all the relevant information was learnt in\nthe embeddings. That was not the case and it was probably an indication that there is still important information that\ncannot be fully captured even by the LLM pre-training. The results achieved might be explained by the fact that the\nstructures of sequences in the testing sets were really very different from those seen during pre-training, or they were a\nsmall minority, thus the generalization capability of the prediction model could not be really benefited from using a LLM\nrepresentation.\nFor the most difficult secondary structure prediction task, we removed from training all the sequences of one specific\ntesting family. In this setup, most of the RNA-LLM achieved a high performance for the two shortest families. However, as\nthe average length of the sequence to be predicted increased, the performance of most RNA-LLM lowered below F\u2081 = 0.50.\nOverall, in 4 out of 9 cases RiNALMo and ERNIE-RNA outperformed the classical method. In two cases no differences\nwere observed, and in the last three families the classical method was the best. In the case of the telomerase RNA family,\nthe most different from the other ones, with very few samples in all datasets and the largest average sequence length, the\nresults were all poor, up to three times below the classical method. In any case, in this RNA family the classical method\nalso obtained a performance without practical usefulness, barely reaching F\u2081 = 0.50.\nOur study showed that RNA-LLM in combination with a deep neural network for prediction were surprisingly capable\nof outperforming a classical method in half of the RNA families, even though cross-family prediction was a task historically\ndominated by thermodynamic methods. Although in this study we have not delved into the analysis of classical methods,\nbecause those are outside the scope of the RNA-LLM comparison, it is important to note that their historical superiority\nis due to a limited understanding of the concept of data-driven learning. This is intuitively associated only to machine\nlearning methods, but all thermodynamics-based methods also have data-driven learning. For machine learning-based\nmethods, humans build a software with rules to be trained automatically from domain examples, thus excluding some of\nthose examples for testing is very simple. Conversely, for classical methods humans learn themselves from data samples,\nand then build a software with those learned rules. That is: they observe many sequences and structures for years,\ntest and improve their models based on those observations, and even incorporate measurements of real structures as\nparameters of their models [29]. In these cases, it is much more difficult to simply exclude training samples from testing\nsets. This is why a new methodology is required to make a fairer comparison with thermodynamic methods.\nWe found that those RNA-LLM trained in the self-supervised stage with the largest and most varied number of\nsequences, and with the largest number of trainable parameters (ERNIE-RNA and RiNALMo), were also those that\nbetter represented the different RNA families in the projected UMAP space and those that accordingly achieved higher\nperformance overall, very consistently and in all cases. At the same time, our experimental design clearly revealed the\nmost important challenges that remain unsolved in RNA secondary structure prediction yet. We generated the first\nsystematic benchmarking for this prediction task with LLM. Our experimental methodology, the curated datasets and\nthe full source code are a key tool for analysts to navigate the space of available RNA-LLM methods, and constitutes a\nreference base for developers towards building more efficient prediction methods. We are confident that the criteria and\nanalysis processes defined here can become a benchmark for future systematic investigations of RNA-LLM performance."}, {"title": "Methods", "content": "RNA large language models\nRNABERT [1]: For the pre-training of the masked language modeling (MLM)[13] task in this LLM, 76, 237 human\nderived small ncRNAs from RNAcentral [49] were utilized. First, a token embedding randomly generates a 120-dimensional\nnumerical vector that encodes the four RNA bases (A, C, G, U) and assigns the same vector to each base in the input\nRNA sequence. Second, the position embedding generates a 120-dimensional vector that encodes the position information\nof each base in the sequence. Third, the element-wise sum of token embedding and position embedding for each base in\nthe input RNA sequence is fed to the Transformer layer. RNABERT architecture consists of a stack of 6 Transformers,\neach of which is composed of a multi-head self-attention mechanism followed by a feed-forward neural network. The\nweights of the last layer are trained by alternating between different tasks. The MLM task masks 15% of the bases\nrandomly selected from the input RNA sequence and predicts the masked part using the neighboring bases, enabling a\ncontext sensitive embedding. The structural alignment learning (SAL) task is based on RNA structural alignment and\naims to obtain closer embeddings for bases in the same column of reference alignment. The seed alignment for each\nfamily was downloaded from Rfam [22] as the reference structural alignment. The MLM task enables the position and\ncontext-sensitive embedding, and the SAL task enables the structural information embedding.\nRNA-FM [8]: It is a 100 million parameter foundation model encoder based on the BERT original implementation\nand trained on 23.7 million unannotated ncRNAs from RNAcentral [49] database. Identical sequences were removed\nby applying CD-HIT [15] with a cut-off at 100%. The resulting dataset was used to train the foundation model in\na self-supervised manner, reconstructing the input masked tokens as a pretext task. During pre-training under self-\nsupervised training, around 15% of nucleotide tokens were randomly replaced with a special mask token. RNA-FM\ntakes raw sequential tokens as input and embeds each nucleotide into a 640-dimensional vector. The architecture has 12\ntransformer encoder blocks as in BERT, which includes multi-head self-attention modules and feed-forward layers, with\na final softmax layer to predict the output. The model was trained with MLM by predicting the original masked token\nwith cross-entropy loss.\nRNA-MSM [61]: Inspired by the success of AlphaFold2 [21] and the use of homologous sequences for the highly\naccurate prediction of protein structures, RNA-MSM is a multiple sequence alignment (MSA)-based RNA language\nmodel. It utilizes a set of homologous sequences that allows having a larger number of sequences for training. 4,069\nRNA families were downloaded from Rfam 14.7, totaling 3,087, 138 sequences. The RNAcmap3 database [9] for homolog\nsearch and sequence alignment was employed. Rfam families containing RNA sequences with experimentally determined\nstructures were excluded to minimize potential overfitting for downstream tasks such as structural inference. This led to a\ntotal of 3, 932 Rfam families. Pre-training was based on MLM, with 20% random masking. The embedding module, with\none initial embedding layer and two learnable position-embedding layers, encodes entries in the MSA separately. The\narchitecture is made of a stack of MSA transformer blocks, each with a residue and sequence attention layer containing\n12 heads with an embedding size of 768, followed by a feed-forward layer, which summarizes approximately 96 million\nparameters. The use of structural information associated with the sequences was not included during the LLM pre-\ntraining. To provide a fair comparison with the other RNA-LLM, a single-sequence version of this method was used. This\nway, the embedding obtained corresponds only to the input sequence.\nERNIE-RNA [58]: This RNA pre-trained language model is based on the Enhanced Representation through Knowl-\nedge Integration (ERNIE) framework [47] and a modified BERT that incorporates base-pairing restrictions to be used\nwith RNA. For training the model, a 34 million ncRNA dataset from the RNAcentral database was downloaded. After\nrefining the vocabulary and removing redundant sequences, the final dataset consisted of 20.4 million RNA sequences.\nERNIE-RNA was trained with MLM, which predicts the masked token with cross-entropy loss. The architecture has 12\ntransformer blocks, with 12 attention heads each. Every token in the input sequences is mapped to a 768-dimensional\nvector, resulting in 86 million parameters. The main hypothesis is that ERNIE-RNA can learn functional and structural\ninformation thanks to the use of attention maps during pre-training.\nRNAErnie [54]: It is also built upon the Enhanced Representation through Knowledge Integration (ERNIE) frame-\nwork [47], together with multilayer and multihead transformer blocks. Pre-training was done with a corpus of approxi-\nmately 23 million sequences extracted from the RNAcentral, and using self-supervised learning with multilevel random\nmasking. The main difference with other works was the use of a motif-aware pre-training strategy involving motif-level\nand subsequence random masking, which can capture both subsequence and motif-level knowledge extracted from motif\ndatabases [24]. The architecture of RNAErnie shares the same architectural configuration as ERNIE 2.0 [48]: a 12-layer\ntransformer with a hidden state embedding dimension of 768. A block first tokenizes RNA bases in the sequence and\nsubsequently feeds them into the transformer. Given the embeddings for every token in the RNA sequence, the RNAErnie\nbasic block transforms the series of token embeddings into a 768 \u00d7 Lembedding using trainable parameters and then\noutputs the embedding of the RNA sequence. The total number of trainable parameters in RNAErnie is approximately\n105 million.\nRiNALMO [36]: It is the largest RNA language model to date with 650 million parameters pre-trained on 36 million\nunique non-coding RNA sequences from the RNAcentral [49] database augmented by Rfam [23], nt [40] and Ensembl [27].\nTo ensure diversity in each batch, the sequences were clustered with MMSeqs2 [44] into 17 million clusters, and then each\nbatch contained a mixture of sequences sampled from different clusters. The architecture of RiNALMO is a BERT-style\nencoder-only Transformer [13]. Before passing to the Transformer, an RNA sequence is tokenized and represented as a\n1280-dimensional vector. RiNALMO consists of 33 Transformer blocks, where each block comprises a multi-head attention\nand a feed-forward network. The position of the tokens is encoded using rotary positional embedding [46]. Each multi-\nhead attention has 20 heads. To improve pre-training efficiency, FlashAttention-2 [11] is employed. In the feed-forward\nnetwork, two linear layers are used together with SwiGLU activation function [41]. Among the transformer modules,\nthere are residual connections with layer normalization to stabilize training.\nTraining deep learning models\nRNA large language models: One-hot embedding was defined as usual using 4 positions for A, U, G and C (Fig. 1b).\nEach LLM was used with the pre-trained weights provided, according to instructions in the official repositories. Models\nwere frozen, that is, not re-trained nor fine-tuned. Per-nucleotide embeddings were extracted for each sequence in each\nbenchmark dataset, obtaining a d\u00d7 L tensor, with d the embedding dimension and L the sequence length.\nDeep RNA folding: The embeddings feed the secondary structure prediction network described in Fig. 1a. This\narchitecture was designed following the one used in RiNALMo [36], RNA-FM [8], RNA-MSM [61], and ERNIE-RNA [58].\nA fully connected layer reduces the input dimension to M/2 (being M\u226a d), in order to obtain the same dimension\nfor all RNA-LLM. An outer concatenation approach was used to transform the M/2 \u00d7 L projection to a M\u00d7L\u00d7 L.\nThe M dimension can be interpreted as channels of a L XL image. Then, this tensor passes through two 2D ResNet\nblocks [17] with the following configuration: a first ResNet with convolutions of kernel size 1 and a second one with kernel\nsize 3. Both ResNet blocks include instance Normalization and ReLU activation function. The same deep architecture\nwas trained and tested using each RNA-LLM and dataset. For training, Adam optimizer was used with a learning rate of\n0.0001, a batch size of 4 and binary cross-entropy as the loss function. A fixed number of 15 epochs was used in order that\nall RNA-LLM have equal possibilities for training. Each dataset was used with its own training and testing partitions,"}, {"title": "Data", "content": "ArchiveII dataset [43]: The most widely used benchmark dataset for RNA folding methods, containing RNA struc-\ntures from 9 RNA families: 5s (ribosomal RNAs), srp (signal recognition particle), tRNA (transfer RNA), tmRNA\n(transfer messenger RNA), RNaseP (Ribonuclease P), grp1 (Glycine-rich RNA-binding protein 1), 16s (ribosomal RNA),\ntelomerase and 23s (ribosomal RNA). The total number of sequences with length less than 512 nt in this dataset is 3,864.\nThis dataset is used in two training and testing split configurations. First, a random set of 5-fold partitions, following\nthe original splits provided by the authors for the ArchiveII dataset [43]. We also perform a cross-family generalization\nanalysis, training on all RNA families but one that is used as a test unseen family, and repeating for all families. That\nis, a leave-one-family-out strategy.\nbpRNA dataset [42]: The same train and test sets as used in SPOT-RNA[42]. It is a non-redundant set of RNA\nsequences at 80% sequence-identity cutoff with CD-HIT-EST, with annotated secondary structure from bpRNA34 [15].\nThis filtered dataset of 13,419 RNAs is randomly divided into 10,814 RNAs for training (TR0), 1,300 for validation\n(VL0), and 1,305 for an independent test (TSO). In this dataset, each model was trained with TR0+VL0 and tested with\nTSO.\nbpRNA-new dataset [42]: This dataset was derived from Rfam 14.2 [23], containing new RNA families different from\nthe bpRNA dataset. This test dataset has 5,401 sequences and was built to assess cross-family model generalization.\nThis dataset is used as an additional test set for the models trained with the bpRNA dataset.\nPDB-RNA dataset [42]: The PDB sets TR1 (training), VL1 (validation) and TS1 (testing) are the same sets as in\nSPOT-RNA, prepared by downloading all the high-resolution (< 3.5A\u00b0) RNA X-ray structures from the PDB dataset\non March 2, 2019. The numbers of structures for TR1, VL1 and TS1 are 120, 30 and 62, respectively, after removing\nhomologous sequences between and within the sets by CD-HIT-EST at the lowest allowed sequence identity cut-off of\n80%. In this dataset each model was trained with TR1+VL1 and tested with TS1. This small training set is used to test\nthe LLM capabilities in a very hard setup for any transfer learning approach.\nFor all datasets, sequences longer than 512 nucleotides were filtered to limit runtime [51]."}, {"title": "Performance measures", "content": "Base pairs metrics: The focus of performance measures is on the predicted base pairs in comparison to a reference\nstructure [28]. Pairs that are both in the prediction and the reference structure are true positives (TP), while pairs\npredicted but not in the true structure are false positives (FP). Similarly, a pair in the reference structure that is not\npredicted is a false negative (FN), and a pair that is neither predicted nor in the true structure is a true negative (TN). \u0391\nwidely used metric is the recall (or sensitivity), defined as the ratio of TP to all the true pairs (TP+FN). It is a measure of\nhow many predicted pairs are true. Insted, the precision is defined as the ratio of TP to all the predicted pairs (TP+FP).\nThis relation between TP and FP is very important in the context of class imbalance, because FP can be a large number\nin comparison to TP, and this is not reflected in the recall. The F\u2081 score is the harmonic mean of precision and recall,\nthus summarizing both measures of performance in a single value. Therefore, in this work the F\u2081 score is used as the\nglobal performance measure for the comparison of the different methods. It is defined as\n$F1 = \\frac{2TP}{2TP + FP + FN}$\nStructural motifs: Stems, multiloops, internal loops, bulges, hairpin loops, dangling ends, and external loops were\nextracted using the bpRNA toolkit [10] on the reference dot-bracket structures. All reference and predicted base-pairs\nwere extracted for positions of each motif type, and average F\u2081 computed separately."}, {"title": "Data availability", "content": "Curated benchmark datasets of increasing complexity are available in the repository: https://github.com/sinc-lab/\nrna-llm-folding/tree/main/data. Moreover, all the embeddings generated in this study are available via Zenodo at:\nhttps://doi.org/10.5281/zenodo.13821093."}, {"title": "Code availability", "content": "The source code to reproduce all the experiments and results analysis can be found in: https://github.com/sinc-lab/\nrna-llm-folding/."}]}