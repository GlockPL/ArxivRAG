{"title": "SYNERGIZING DEEP LEARNING AND FULL-WAVEFORM\nINVERSION: BRIDGING DATA-DRIVEN AND THEORY-GUIDED\nAPPROACHES FOR ENHANCED SEISMIC IMAGING", "authors": ["Christopher Zerafa", "Pauline Galea", "Cristiana Sebu"], "abstract": "Seismic imaging and subsurface characterization play vital roles in understanding Earth's geological\nproperties. This review paper explores the integration of deep learning techniques with full-waveform\ninversion (FWI), a critical method for estimating subsurface properties using seismic data. The\npaper is structured into five chapters that cover the theoretical foundations, applications, challenges,\nlimitations, and future research directions of this integration.\nChapter 1 provides an introduction to the motivation and objectives behind the integration of deep\nlearning and FWI. Chapter 2 establishes the fundamentals of FWI, elucidating its mathematical\nprinciples, forward and inverse problems, and workflow components. Chapter 3 offers a compre-\\hensive introduction to deep learning, including neural networks, activation functions, optimization\nalgorithms, and regularization techniques.\nChapter 4 delves into specific applications of deep learning in geophysics, highlighting its utilization\nin various domains like seismology, geology, and hydrogeology. Notable applications include\nvelocity estimation, seismic deconvolution, and tomography, showcasing how deep learning enhances\nsubsurface characterization.\nChapter 5 addresses critical challenges and potential limitations in this integration, such as model\ncomplexity, data quality, and generalization. The chapter outlines future research directions, including\nhybrid models, generative models, uncertainty quantification, and physics-informed learning.\nIn conclusion, the synergy of deep learning and FWI holds immense potential to revolutionize seismic\nimaging and subsurface characterization. This integration, while posing challenges, offers pathways\nto more accurate, efficient, and reliable subsurface property estimation. By addressing challenges\nand pursuing innovative research, this integration promises to reshape geophysics and contribute to a\ncomprehensive understanding of Earth's subsurface properties.", "sections": [{"title": "Introduction", "content": "Full Waveform Inversion (FWI) has emerged as a powerful technique in the field of seismic imaging and exploration,\nrevolutionizing our ability to accurately reconstruct subsurface structures and properties. By iteratively minimizing the\nmisfit between observed and modeled seismic data, FWI promises high-resolution reconstructions that have implications\nspanning from oil and gas reservoir characterization to earthquake hazard assessment. However, despite its immense\npotential, FWI encounters numerous challenges that hinder its widespread applicability, including sensitivity to initial\nmodels, computational intensity, and the requirement of dense data coverage.\nIn recent years, the advent of Deep Learning (DL) methodologies has ushered in a paradigm shift in various scientific\nand engineering domains. DL techniques, such as Convolutional Neural Networks (CNNs) and Recurrent Neural\nNetworks (RNNs), have demonstrated exceptional capabilities in image recognition, natural language processing, and\nmedical image analysis. When applied to the field of FWI, these methods offer the potential to overcome some of the\ntraditional limitations by introducing innovative data-driven and theory-guided inversion strategies.\nThis review paper seeks to explore the evolving landscape of FWI through the lens of deep learning techniques,\nencapsulating a spectrum ranging from data-driven to theory-guided inversion methodologies. We aim to provide a\nsystematic overview of the various approaches that leverage the synergy between FWI and deep learning, highlighting\ntheir contributions, challenges, and potential future directions. By amalgamating traditional FWI techniques with the\npower of deep learning, researchers and practitioners can harness a synergy that might facilitate enhanced inversion\naccuracy, reduced computational costs, and improved robustness to noise and incomplete data.\nThe paper is structured as follows: In Section 2, we provide a brief overview of the traditional FWI formulation,\nhighlighting its mathematical underpinnings and challenges. Section 3 delves into the fundamental concepts of deep\nlearning, elucidating its different architectures and their applicability to FWI. Subsequently, Section 4 presents a\ncomprehensive survey of deep-learning applications to geophysics. In Section 4.1 we initial discuss legacy inversion\ntechniques that employed deep learning. In Secion 4.2, we then discuss data-driven approaches, where neural networks\nare employed to directly learn complex relationships between seismic data and subsurface parameters, and finally. In\nSection 4.3, we transition towards theory-guided approaches, exploring methodologies that integrate domain-specific\ngeological constraints and physical laws within deep learning frameworks. Finally, in Section 5, we outline critical\nchallenges, potential limitations, and provide insights into possible future research directions within this evolving field.\nIn conclusion, the synthesis of Full Waveform Inversion and Deep Learning heralds a new era of seismic imaging,\noffering the potential to address long-standing challenges and transform the way we extract subsurface information.\nThis review intends to serve as a comprehensive guide for researchers, practitioners, and students in understanding\nthe evolution, nuances, and potential of the synergy between these two dynamic fields. Through a holistic analysis\nof data-driven and theory-guided approaches, we endeavour to catalyze further innovation, foster cross-disciplinary\ncollaborations, and pave the way for more accurate and efficient subsurface characterizations."}, {"title": "Full Waveform Inversion", "content": "Full Waveform Inversion (FWI) tries to derive the best velocity model and other lithologic properties (as density,\nanelastic absorption and anisotropy) of the Earth's subsurface to be consistent with recorded data. An exhaustive search\nfor this ideal model is almost impossible and methods for finding an optimal one describing the data space are necessary.\nThere are two main categories for dealing with this problem: (i) global optimization methods, and (ii) direct solving\nthrough linearisation."}, {"title": "FWI as a global optimization method", "content": "Global optimization methods use stochastic processes to try and find the global minimum of the misfit function [1, 2].\nThree most well-known cases of global methods are:\nMonte Carlo methods: These are pure random search methods in which models are drawn stochastically\nfrom the total model space, forward modelled and the model with lowest cost function is utilized [3, 4].\nGenetic Algorithm methods: These are based on analogues of biological evolution where a relatively small\nselection of models is chosen from the model space [5]. The best models of these parent selection form new\nchild models by cross-over and mutation of the parameters describing the model. The children will then\nreplace the weakest models in the selection. By such iterative steps, the selection of models will gather towards\nan optimal model [6, 7, 8].\nSimulated Annealing methods: These are based on analogues of physical annealing processes modelled\nin statistical mechanics [9, 10]. Model parameters are randomly perturbed and updated model evaluated\nand assessed whether to be accepted or not. Only better models are propagated forward and the possibility\nof a perturbation being accepted decreases. If convergence exists, the model will iteratively become better\n[11, 12, 13]."}, {"title": "Formulation of FWI as Local Optimization", "content": "The concept of local optimization for FWI was introduced in the 1980's. [14] and [15] cast the exploding-reflector\nconcept of [16, 17] as a local optimization problem which aims to minimise in a least-squares sense the misfit between\nrecorded and modelled data [18]. The problem is set in the time-domain as follows: set a forward propagation field to\nmodel the observed data, back propagate the misfit between the modelled and the observed data, cross-correlate both\nfields at each point in space to derive a correction, and do least squares minimization of the residuals iteratively. This\noutline forms the basis of this technique to this day.\n[19] numerically demonstrate a local optimization FWI approach using two-dimensional synthetic data examples. A\nsingle diffracting point on a homogeneous model was used to illustrate the importance of proper sampling of the\nsubsurface. Furthermore, this model was used to show that the free surface adds an extra complexity to the problem and\nincreases the non-linearity of the inversion. FWI with or without free-surface multiple modelling is an active area of\nresearch to this day [20, 21]."}, {"title": "Applications in Time and Frequency", "content": "One of the pioneering applications of FWI was presented by [22] and is represented in Figure 1. They showed better\nimaging using a hierarchical multi-scale approach on the Marmousi synthetic model. This strategy initially inverts\nfor low-frequency components where there are fewer local minima, and those that exist are sparser than if for higher\nfrequencies. However, decomposing by scale did not resolve issues of source estimation, source bandwidth and noise\n[22]. In the 1990s, Pratt and his associates proposed FWI via the pseudo-spectral domain [23, 24, 25]. The initial\napplication was to cross-hole data utilizing a finite difference approach and an elastic wave propagator to facilitate the\nmodelling of multi-source data. This was extended to wide-aperture seismic data by [26]. Analytically, the time- and\nfrequency-domain problems are equivalent [18]. Initial attempts of pseudo-spectral FWI include application to the\nMarmousi model [27] and land seismic dataset [28]."}, {"title": "Beyond Academic Experiments", "content": "Theoretically, two-dimensional inversion is only able to explain out-of-plane events by mapping them into in-plane\nartefacts [29]. This meant that FWI restricted to purely academic pursuits [30] and full potential could only be realized\nif extended to three-dimensions. The first 3D frequency-domain algorithms where developed by [31] on synthetic\ndatasets, however these used low initial frequencies that are not normally present in real data [32]. Examples of this\napplication are demonstrated by [33], [34] and [35]. [36] presented the first 3D real data application to a shallow North\nSea survey. This improved the resolution of shallow high-velocity channels that resulted in uplifts upon migration.\nA re-occurring theme within this section is the creation of a better approximation to the wavefield propagation within\nthe subsurface; 1D to 2D to 3D discretization, acoustic to anisotropic to elastic to orthorhombic wavefield modelling,\nwith each additional dimension of information resulting in more numerical and computer intensive algorithms [41].\nEven though computing power has increased dramatically, making FWI more productive, the underlying algorithms are\nonly improving incrementally. Indeed, the next generation of experiments will require changes to acquisition geometry\nto allow for full-bandwidth and multi-azimuth reconstruction of the wavefield [42]."}, {"title": "Deep Neural Networks", "content": ""}, {"title": "Neural Networks for Inverse Problems", "content": "The mathematical formulation of FWI falls under the more general class of variational inverse problems [43]. The aim\nis to find a function which is the minimal or the maximal value of a specified functional [44]. Indeed, inverse problems\nattempt to reconstruct an image x \u2208 X \u2286 Rd from a set of measurements y \u2208 Y \u2286 Rm of the form\ny = F(x) + \u0454\nwhere \u0413 : X \u2192 Y, \u0393 \u2208 Rm\u00d7d is the discrete operator and \u0454 \u2208 Y \u2286 Rm is the noise. NN within Machine Learning can\nbe considered to be a set of algorithms of non-linear functional approximations under weak assumptions [45]. Namely,\nwhen applied to inverse problems, Equation 1 can be re-phrased as the problem of reconstructing a non-linear mapping\n\u0413: Y \u2192 X satisfying the pseudo-inverse property\n\u0413\u00b9 (y) \u2248 \u0445\nwhere observations y \u2208 Y are related to x \u2208 X as in Equation 1, and 0 represents the parametrization of pseudo-inverse\nby the NN learning [46]. The loss function defined in Equation 2 is dependent on the type of training data, which is\ndependent on the learning approach [47]. There are two main classes of learning in Machine Learning: (i) Supervised,\nand (ii) Unsupervised.\nIn supervised learning, training data are independent distributed random pairs with input x \u2208 X and labelled output\ny \u2208 Y [48]. Estimating @ for Equation 2 can be formulated as minimizing a loss function J(0) which has the following\nstructure [46]:\nJ(0) := D(\u0413\u00b9(y), x)\nwhere D is a distance function quantifying the quality of the reconstruction and F\u00af\u00af\u00b9 : Y \u2192 X is the pseudo-inverse to\nbe learned [46]. A common metric for the distance function is the sum of squared distances, resulting in:\nJ(0) := ||\u0413\u012b\u00b9(y) \u2013 \u0445||x\nApproaching the inverse problem directly via this approach amounts to learn \u0413\u012b\u00b9 : Y \u2192 X from data such that\nit approximates an inverse of \u0413. In particular, this has successful applications in medical imaging [49, 50], signal\nprocessing [51, 52] and regularization theory [53, 54, 55].\nIn unsupervised learning, there exist no input-output labelled pairs and the training data is solely elements of y \u2208 Y.\nThe NN is required to learn both the forward problem and inverse problem [56]. The loss functional for unsupervised\nlearning is given as:\nJ(0) := L (\u0413 (\u0413\u00b9(y)), x) + S (\u0413\u014d\u00b9 (g))\nwhere L: Y X X \u2192 R is a suitable affine transformation of the data and S : X \u2192 R is the regularization function.\nMain applications of this learning are to inherent structure and have been proven successful in exploratory data analysis\napplications such as clustering [57, 58] and dimension reduction [59]."}, {"title": "Evolution of Neural Networks", "content": "The remaining literature review is restricted to supervised learning approaches using NN as these are more suited for\nvelocity inversion. For a complete review, [60] and [61] provide further detail."}, {"title": "Early Neural Nets and the Perceptron", "content": "The basic ideas of NN date back to the 1940's and were initially devised by [62] when trying to understand how\nto map the inner workings of a biological brain into a machine. From a biological aspect, neurons in the brain are\ninterconnected via nerve cells that are involved in the processing and transmitting of chemical and electrical signals\n[62].\nEarly NN with rudimentary architectures did not learn [62] and the notion of self-organized learning only came\nabout in 1949 by [63]. [64] extended this idea of learning and proposed the first and simplest neural network \u2013 the\nMcCullock-Pitts-Perceptron. As shown in Figure 4, this consists of a single neuron with weights and an activation\nfunction. The weights are the learned component and determine the contribution of either input x to the output y.\nThe activation function & adds a non-linear transform, allowing the neuron to decide if the input is relevant for the\npaired output. Without an activation function, the neuron would be equivalent to a linear regressor [65]. [64] used\nthis fundamental architecture to reproduce a functional mapping that classifies patterns that are linearly separable.\nThis machine was an analogue computer that was connected to a camera that used 20\u00d720 array of cadmium sulphide\nphotocells to produce a 400-pixel image. Shown in Figure 5, the McCullock-Pitts-Perceptron had a patch-board that\nallowed experimentation with different combinations of input features wired up randomly to demonstrate the ability of\nthe perceptron to learn [66, 67].\nRosenblatt's perceptron was the first application of supervised learning [68]. However, [69] highlight limitations to the\napplications of a single perceptron. They also point out that Rosenblatt's claims that the \"perceptron may eventually be\nable to learn, make decisions, and translate languages\" were exaggerated. There was no other follow ups on this work\nby Minsky and Papert, and research on perceptron-style learning machines practically halted [65]."}, {"title": "Back-Propagation and Hidden Layers", "content": "Efficient error back-propagation in NN networks were described in Linnainmaa's master thesis [70]. This minimizes\nthe errors through gradient descent in the parameter space [71] and allows for explicit minimization of the cost function.\nBack-propagation permits NN to learn complicated multidimensional functional mappings [72].\nThe back-propagation formulation lends itself from major developments in dynamic programming throughout the 1960s\nand 1970s [73, 74, 75]. A simplified derivation using the chain rule was derived by [72] and the first NN-specific\napplication was described by [76]. It was until the mid-1980s that [77] made back-propagation mainstream for NN\nthrough the numerical demonstration of internal representations of the hidden layer. Hidden layers reside in-between\ninput and output layers of the NN."}, {"title": "The Vanishing Gradient and Renaissance of Machine Learning", "content": "The major milestone in NN came about in 1991. Hochreiter's thesis identified that deep NN suffer from the vanishing or\nexploding gradient problem [82]. Gradients computed by back-propagation become very small or very large with added\nlayers, causing convergence to halt or introduce unstable update steps. Solutions proposed to address this challenge\nincluded batch normalization [83], Hessian-free optimisations [84, 85, 86], random weight assignment [87], universal\nsequential search [88] and weight pruning [89].\nPrior to 2012, NN were apparently an academic pursuit. This changed when AlexNet [90] won the ImageNet [91]\nvisual object recognition by a considerable margin. AlexNet used a deep architecture consisting of eight layers [92] and\nwas the only entry employing NN in 2012. All submissions in subsequent years were NN-based [93] and in 2015, NN\nsurpassed human performance in visual object recognition for the first time [91] - see Figure 6. AlexNet is undoubtedly\na pivotal event that ignited the renaissance in interest around deep learning."}, {"title": "Deep Neural Network Architecture Landscape", "content": "According to [100], three of the most common major architectures are (i) Neural Network, (ii) CNN, and (iii) RNN.\nNeural networks are non-linear models inspired by the neural architecture of the brain in biological systems. A typical\nneural network is known as a multi-layer perceptron and consists of a series of layers, composed of neurons and their\nconnections [101].\nCNN are regularized version of MLPs with convolution operations in place of general matrix multiplication in at least\none of the layers [102]. These types of networks find their motivation from work by [103, 104]. Inspired by this work,\n[105] introduced convolutional layers and downsampling layers, [106] developed max pooling layers and [89] used\nback-propagation to derive the kernel coefficients for convolutional layers. The architecture of the NN used by LeCun\net al. is known as LeNet5 and is shown in Figure 7 for the classification of hand-written digits. This essentially laid the"}, {"title": "Not Just Algorithms", "content": "Apart from Machine Learning algorithms, re-interest in DNN has led to software architectures that allow for quick de-\nvelopment. The most common include Tensorflow [115], Keras [116], PyTorch [117], Caffe [118]) and Deeplearning4j\n[119]. These types of frameworks are facilitating interdisciplinarity between Machine Learning and geophysics. Indeed,\n[120] employed DNN architecture within Tensorflow to solve for FWI. Utilizing a common DNN optimizer - Adam\nhe shows in Figure 8 how the cost function converged quicker in the inversion process as compared to conventional\nmethods in FWI."}, {"title": "Deep Learning in Geophysics", "content": "Machine Learning techniques have been utilised across different geophysical applications. Some notable examples\ninclude geo-dynamics [121], geology [122], seismology [123], paleo-climatology [124], climate change [125] and\nhydrogeology [126]. Unsupervised algorithms have also been investigated by [127] for pattern recognitions of wavefield\npatterns with minimal domain knowledge. Other geophysical application include seismic deconvolution [128, 129],\ntomography [130], first-break picking [131], trace editing [132], electricity [133], magnetism [134], shear-wave splitting\n[135], event classification [136], petrophysics [137] and noise attenuation [138, 139]."}, {"title": "Legacy Velocity Inversion", "content": "More specific to velocity estimation, the first published investigation for the use of NN was a RNN by [140]. Their\nnetwork architecture represented all components in an elastic FWI experiment with a seismic source, the propagation\nmedia and an imaging response. Figure 9 shows a block diagram representation for their network. The neural column\nconsisted of two 1-layer neuron columns, one for particle displacement and another for particle velocity.\n[141] published the first application of NN which estimated 1D velocity functions from shot gathers from a single layer\nNN in 1994. Figure 10 shows their NN architecture. This accepted synthetic common shot gathers from a single source"}, {"title": "Data-Driven Approaches to Velocity Estimation", "content": "The terminology of data-driven geophysics is not a novel-one. This was first introduced in the literature by [144]\nwhen estimating rock properties directly from seismic-data through statistical techniques. However, conceptually, this\nis similar to the deconvolution process within a seismic processing flow [145, 146]. Namely, a filter is derived via\nautocorrelations and applied as a deconvolution operator [147]. The term has only recently found a re-invigorated\ninterest. Some modern applications of data-driven geophysical processes include dictionary learning [148], time series\nanalysis [149], fault identification [150], and reservoir characterization [151].\nTwenty-one years after [140], [152] employed DNN architecture to learn prior models for seismic FWI. Their data\ndriven approach at estimating initial models was applied to salt body reconstruction by learning the probability of salt\ngeo-bodies and use this to regularize the FWI objective function. [153] utilised DNN architecture and inverted for 2D\nhigh-velocity profiles. For the training process, they generated thousands of random 2D velocity models with up to\nfour faults in them, at various dip angles and positions. Each model had three to eight layers, with velocities ranging\nfrom 2000 to 4000 ms\u00af\u00b9, with layer velocity increasing with depth. The DNN architecture is not defined in their paper,\nhowever when applied to unseen data with and without salt anomalies, their results achieved accuracies well above 80%\nfor both cases. This was used to obtain a low-wavenumber starting model then passed to traditional FWI as an initial\nmodel. [154] proposed a convolutional-based network called \u201cInversionNet\u201d to directly map the raw seismic data to the\ncorresponding seismic velocity model for a simple fault model with flat or curved subsurface layers. More recently,\n[155] extended this further and developed a DNN framework called \u201cSeisInvNet\u201d to perform the end-to-end velocity\ninversion mapping with enhanced single-trace seismic data as the input in time domain."}, {"title": "Wave Physics as an Analogue Recurrent Neural Network", "content": "Recently, [156] and [157] derived a function between the physical dynamics of wave phenomena and RNNs. In their\nwork they propose physics-informed neural networks that are trained to solve supervised learning tasks while respecting\nthe laws of physics described by general non-linear partial differential equations. Fundamental to their approach is the\nability for DNNs to be universal function approximators. Within this formulation, [156] are able to solve non-linear\nproblems without the need to compute a priori assumptions, perform linearisation or employ local time-stepping. Under\nthis new paradigm in modelling, [156] show how back-propagation is used \"to differentiate neural networks with respect\nto their input coordinates and model parameters to obtain physics-informed neural networks. Such NN are constrained\nto respect any symmetries, invariances, or conservation principles originating from the physical laws that govern the\nobserved data, as modelled by general time-dependent and non-linear partial differential equations\". In particular,\nfollowing up from this work, [158] recast the forward modelling problem in FWI into a deep learning network by\nrecasting the acoustic wave propagation into a RNN framework. Figure 11 shows velocity inversion results from [158]\napplied to the Marmousi velocity model. These theory-guided inversions still suffer from cycle-skipping, local-minima\nand high computational cost [158]. Recent research suggests that Stochastic Gradient Descent algorithms have the\ncapacity to escape local minima to a certain extent [158]."}, {"title": "Challenges, Limitations, and Future Research Directions", "content": ""}, {"title": "Critical Challenges and Potential Limitations", "content": "\u2022 Complexity and Model Size: Deep learning models can be computationally intensive, requiring substantial\ncomputational resources and memory. The complexity of the models might hinder their practical application\nto large-scale FWI problems.\n\u2022 Data Requirements and Quality: Deep learning models often demand large amounts of high-quality labeled\ndata for training. In the context of FWI, obtaining such datasets might be challenging due to limited data\navailability and potential noise in seismic measurements.\n\u2022 Generalization and Overfitting: Ensuring that deep learning models generalize well to unseen data is crucial.\nOverfitting to limited training data can lead to poor performance when faced with new geological settings or\nvariations.\n\u2022 Interpretability and Uncertainty: Deep learning models are often viewed as black boxes, which makes it\nchallenging to interpret their decisions. Moreover, quantifying uncertainties in predictions and incorporating\nthese uncertainties into FWI results remains a significant challenge.\n\u2022 Transferability: Deep learning models trained on one geological setting might not easily transfer to another,\ndue to variations in geological characteristics and seismic acquisition setups."}, {"title": "Future Research Directions", "content": "\u2022 Hybrid Models: Investigate hybrid models that combine data-driven and theory-guided approaches, aiming to\ncapitalize on the strengths of both paradigms. Developing techniques that effectively fuse data-driven learning\nwith physical constraints could lead to more robust and accurate inversions.\n\u2022 Generative Models: Explore the potential of generative models, such as Generative Adversarial Networks\n(GANs), for generating realistic synthetic seismic data. These models could supplement limited real data for\ntraining, potentially alleviating the data scarcity issue.\n\u2022 Uncertainty Quantification: Research methodologies for estimating and quantifying uncertainties in the deep\nlearning-based FWI results. Bayesian approaches, ensemble methods, and Monte Carlo techniques could be\nemployed to provide a more comprehensive understanding of the inversion uncertainties.\n\u2022 Transfer Learning and Domain Adaptation: Investigate techniques that enable the transfer of knowledge\nfrom one geological setting to another, adapting deep learning models to new environments with limited data.\n\u2022 Physics-Informed Learning: Develop approaches that integrate domain-specific physical laws and constraints\ndirectly into deep learning architectures. Combining physics-based models with data-driven learning could\nenhance the accuracy and stability of inversions."}, {"title": "Summary", "content": "This review paper explores the integration of deep learning techniques into the field of full-waveform inversion (FWI)\nfor seismic imaging and subsurface characterization. The paper comprises five chapters that cover various aspects of\nthis integration, including the theoretical foundations, application examples, challenges, limitations, and future research\ndirections. The main findings and discussions from each chapter are summarized as follows:\nWe introduced the motivation behind integrating deep learning with FWI and provided an overview of the paper's\nstructure. We highlighted the potential of deep learning to address the challenges associated with traditional FWI\nmethods and outlined the objectives of the paper.\nThis chapter delved into the theoretical foundations of FWI. We covered the mathematical principles underlying FWI,\ndiscussed the forward and inverse problems, and explained the key components of FWI workflows. The importance of\naccurately estimating subsurface properties through seismic data inversion was emphasized.\nWe provided a comprehensive introduction to deep learning techniques. The chapter covered essential concepts\nsuch as neural networks, activation functions, loss functions, optimization algorithms, and regularization techniques.\nUnderstanding these fundamentals is crucial for grasping the subsequent chapters' applications in the context of FWI.\nThis chapter explored specific applications of machine learning and deep learning techniques in geophysics. Notable\nexamples included various geophysical domains like seismology, geology, and hydrogeology. We detailed how deep\nlearning methods have been applied to address problems such as velocity estimation, seismic deconvolution, and\ntomography, showcasing the potential for enhanced subsurface characterization.\nWe highlighted critical challenges and potential limitations associated with integrating deep learning and FWI. These\nchallenges include model complexity, data quality and quantity, generalization, interpretability, and transferability. The\nchapter then outlined various future research directions to address these challenges. Hybrid models, generative models,\nuncertainty quantification, transfer learning, and physics-informed learning were among the proposed strategies for\nadvancing the field.\nIn conclusion, this review paper underscores the significant potential of integrating deep learning techniques into the\nrealm of FWI for seismic imaging and subsurface characterization. Through an exploration of theoretical foundations,"}]}