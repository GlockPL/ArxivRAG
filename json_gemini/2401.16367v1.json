{"title": "TQCompressor: improving tensor decomposition\nmethods in neural networks via permutations", "authors": ["V. Abronin", "A. Naumov", "D. Mazur", "D. Bystrov", "K. Tsarova", "Ar. Melnikov", "I. Oseledets", "S. Dolgov", "R. Brasher", "M. Perelshtein"], "abstract": "We introduce TQCompressor, a novel method for neural network model com-\npression with improved tensor decompositions. We explore the challenges posed\nby the computational and storage demands of pre-trained language models in NLP\ntasks and propose a permutation-based enhancement to Kronecker decomposition.\nThis enhancement makes it possible to reduce loss in model expressivity which\nis usually associated with factorization. We demonstrate this method applied\nto the GPT-2small [15]. The result of the compression is TQCompressedGPT-\n2 model, featuring 81 mln. parameters compared to 124 mln. in the GPT-\n2small. We make TQCompressedGPT-2 publicly available. We further enhance\nthe performance of the TQCompressedGPT-2 through a training strategy involv-\ning multi-step knowledge distillation, using only a 3.1% of the OpenWebText [9].\nTQCompressedGPT-2 surpasses DistilGPT-2 [12] and KnGPT-2 [1] in compara-\ntive evaluations, marking an advancement in the efficient and effective deployment\nof models in resource-constrained environments.", "sections": [{"title": "Introduction", "content": "Advancements in pre-trained language models have significantly impacted the field of natural lan-\nguage processing (NLP), providing robust solutions for tasks such as language translation, text sum-\nmarization, and question answering. Despite their effectiveness, the substantial computational and\nstorage demands of these models present a notable challenge. Consequently, research efforts have\nincreasingly concentrated on model compression strategies to alleviate these demands. Techniques\nsuch as knowledge distillation [5], post-training quantization [19][18], pruning [19], and matrix\nfactorization, specifically tensor decomposition [11] [10], are at the forefront of these endeavors.\nMatrix factorization methods offer promising prospects, as demonstrated by the high compression\nratios achieved without significant performance drops in downstream tasks, such as Kronecker-\nBERT's [2] success on the GLUE benchmark [3]. While these methods theoretically reduce FLOPs,\nthe full realization of these benefits is hindered by current hardware and software limitations, which\nare not optimized for sparse structures resulting from matrix factorization.\nThe future, however, looks bright with potential solutions on the horizon. Advances in computing\narchitecture, particularly in quantum computing and adapted GPU designs, are set to bridge this\ngap. Quantum computing, in particular, with its proficiency in handling complex computations,\naligns well with the demands of matrix factorization. Research in this area, such as developments in\ntensor network computational architectures [21] [22], is already paving the way. These innovations\nsuggest that the theoretical advantages of matrix factorization could soon be fully realized,\nSome works focus specifically on factorizing weights in Language Models (LMs) using Kronecker\ndecomposition[14]. They have been applied to such models as BERT [2] and GPT-2 [1], which"}, {"title": "Related works", "content": "In this section, we review the related works in the field, with a focus on the Kronecker decomposition\nalgorithm combined with permutations of weight matrices, which is the approach employed in our\nwork.\nMatrix factorization methods have gained prominence as efficient techniques for compressing neu-\nral networks. These methods aim to approximate the weight matrices of neural networks with a\nlower-rank factorization, reducing both the number of parameters and theoretical computational\ncomplexity. Matrix factorization approaches for neural network compression include, but not lim-\nited to singular value decomposition (SVD) [8], Tensor Train decomposition [17], Tensor Ring [16]\nand Kronecker decomposition [14].\nNumerous studies have focused on the Kronecker decomposition of weights, primarily due to their\nability to achieve high compression rates while maintaining, or minimally impacting, model per-\nplexity. The Kronecker decomposition stands out not only for its compression efficacy but also for\nits significant reduction in FLOPs, which is a crucial factor in enhancing computational efficiency.\nA notable milestone in this domain was set by Edalati and Tahaei (2021)[1], who pioneered the\nuse of Kronecker decomposition for compressing the GPT-2small model. This model, containing\napproximately 83 million parameters, is versatile, having been pre-trained under multiple setups to"}, {"title": "Methodology", "content": "In this section, we expound on our contributions to the field of neural network compression, detailing\nboth the decomposition method and the corresponding training procedure. Let $W \\in \\mathbb{R}^{m\\times n}$ denote\na weight matrix within the original neural network architecture. $P\\in \\mathbb{R}^{m\\times m}$ and $C\\in \\mathbb{R}^{n\\times n}$\nrepresent the learned permutation matrices for rows and columns, respectively, while $A \\in \\mathbb{R}^{M_1\\times n_1}$\nand $B\\in \\mathbb{R}^{M_2Xn_2}$ describe the Kronecker decomposition matrices for the weight matrix $W$.\nBuilding upon the premise that Kronecker products can effectively compress weights across diverse\nneural network architectures, we present a novel decomposition approach. This method focuses on\nidentifying an optimal permutation of the weight matrix, followed by its Kronecker decomposition"}, {"title": "TQCompressed decomposition", "content": "Building upon the premise that Kronecker products can effectively compress weights across diverse\nneural network architectures, we present a novel decomposition approach. This method focuses on\nidentifying an optimal permutation of the weight matrix, followed by its Kronecker decomposition (Figure 1). This problem can be mathematically formulated as:\n$\\min_{A_i, B_i, P,C} ||PWC - \\sum_{i=1}^r A_i \\otimes B_i||$ (1)\nIt is crucial to note that the permutation matrices $P$ and $C$ are binary matrices, each with a sin-\ngle non-zero element per row, and thus they can be succinctly expressed as permutation vectors of\ndimensions $n$ and $m$, respectively. The introduction of permutations into our method incurs an ad-\nditional parameter cost of $n + m$ for each decomposed matrix, resulting in following total parameter\ncount of decomposed matrix:\n$m_1n_1 + m_2n_2 + n + m$ (2)\nThe underlying intuition of our method is akin to shuffling neurons: permuting neurons does not\nalter the network's structural integrity but renders it more amenable to factorization, enabling a\nmore efficient approximation of each layer."}, {"title": "Finding optimal decompositions", "content": "Determining optimal decompositions is challenging due to the discrete nature of permutation ma-\ntrices. Our iterative algorithm (Algorithm 2) alternates between optimizing for $P, C$ (permutation\nmatrices), and $A, B$ (Kronecker decomposition matrices), refining weight approximations.\nOptimal permutations are determined in isolation, while the Kronecker matrices $A$ and $B$ are opti-\nmized jointly. Each step concludes by fixing the newly obtained optima, and the algorithm proceeds\niteratively, optimizing one argument at a time to converge to a solution close to the global optimum.\nInitial values for $A$ and $B$ are randomly generated, while $P$ and $C$ begin as identity matrices. The\nsubsequent sections detail each stage of the algorithm."}, {"title": "Optimal permutation matrices", "content": "In this section we state the problem of finding optimal permutation matrices and explain our solution\nof it. Explanations are only given for the row-permutation matrix $P$, but the same reasoning can be\napplied to the column-permutation matrix $C$. The problem of finding an optimal permutation matrix\n$P$ can be formulated as follows:\n$\\min_{P} ||PWC - A \\otimes B||_2$ (3)\nFor simplicity we denote the $WC$ product and the Kronecker product $A \\otimes B$ as $W^{(1)}$ and $W^{(2)}$\nrespectively:\n$\\min_{P} ||PW^{(1)} \u2013 W^{(2)} ||^2_3$ (4)\nIn this setting the permutation matrix $P$ can be thought of as a bijective mapping that pairs rows\nfrom $W^{(1)}$ to rows from $W^{(2)}$. Thus the problem is reduced to finding a one-to-one correspondence\nbetween the rows of $W^{(1)}$ and $W^{(2)}$ that minimizes the total mean squared error between paired\nrows. This problem statement is equivalent to the assignment problem [23] with the following cost\nmatrix (Appendix 1.):\n$D_{ij} = \\sum_k ||W^{(1)}_{ik} - W^{(2)}_{jk}||^2, D \\in \\mathbb{R}^{n\\times n}$ (5)\nWe solve the assignment problem using the Hungarian algorithm [7, 23]."}, {"title": "Optimal Kronecker decomposition", "content": "When the optimal values for $P$ and $C$ are found, we can find the optimal values for $A$ and $B$ by\nsolving the following problem:"}, {"title": "TQCompressed embedding", "content": "Embedding layers are fundamental in natural language processing (NLP) applications, serving as\nsizable, trainable lookup tables that map discrete tokens to continuous vector spaces. These layers\nare mathematically characterized by an embedding matrix $W_{emb} \\in \\mathbb{R}^{v\\times d}$, where $v$ is the vocabulary\nsize and $d$ is the embedding dimensionality.\nIn the architecture of GPT models, two embedding layers are utilized: one for token representation\nand another for positional encoding within the sequence. Our compression technique targets the\nweight matrices of these embedding layers along the embedding axis. We apply Kronecker product\nfactorization in the form of $A_{v\\times d/f} \\otimes B_{f\\times d}$, which allows us to represent the original embedding\nmatrix $W_{emb}$ as the product of two smaller matrices $A$ and $B$ with a reduction factor $f$."}, {"title": "TQCompressed transformer", "content": "The transformer architecture, pivotal to modern NLP models, is composed of two main elements:\nmulti-head attention (MHA) and feed-forward network (FFN) layers. Our compression framework\nencompasses both these components.\nFor the MHA layers, the attention mechanism is realized by first projecting the input through three\nsets of weights-WQ, WK, and WV-to create the query, key, and value matrices, respectively.\nThe attention output $O$ is computed by the equation:\n$O = softmax(\\frac{Q K^T}{\\sqrt{d_k}})$ (7)\nwhere dk is the scaling factor, typically the dimensionality of the Key vectors.\nEach head in an MHA layer has its individual set of weight matrices $W^Q$, $W^K$, and $W^V$, which are\nconventionally concatenated across heads:\n$W^Q = Concat(W_1^Q,...,W_L^Q)$, (8)\n$W^K = Concat(W_1^K,...,W_L^K)$,\n$W^V = Concat(W_1^V,...,W_L^V)$,"}, {"title": "Knowledge distillation", "content": "This section delineates the application of knowledge distillation (KD) [5] in the training process\nof compressed model. Within the scope of our discussion, the symbols S (student model) and T\n(teacher model) are used for representational clarity.\nKnowledge distillation was employed by minimizing the divergence between the probability dis-\ntributions of the student's predictions and the soft targets provided by the teacher's predictions.\nConcretely, we minimized the cross-entropy loss between the outputs (logits) of the student and\nteacher models.\nThe composite loss function amalgamates the standard cross-entropy loss and an additional term\nrepresenting the cross-entropy of the logits:\n$L_{total}(x, y) = \\lambda L_{Cross-Entropy}(S(x), y) + (1 \u2212 \\lambda)L_{Logits}(S(x),T(x))$ (11)\nIn the loss function $L_{total}$, the variable x denotes the input data that the neural network processes,\nwhile y stands for the corresponding correct labels or targets that the network is intended to predict.\nThe loss function evaluates the performance of the student model S by comparing its predicted\noutput S(x) against the true labels y and the teacher model's predictions T(x), enabling the student\nto learn and approximate the teacher's behavior more effectively. $\\lambda$ denotes a balancing coefficient\nIn a departure from the typical implementation of knowledge distillation, our approach harnesses\nonly a fractional subset of the dataset-approximately 3.1% of the original OpenWebText dataset.\nThis strategy significantly enhances training efficiency."}, {"title": "Iterative compression with knowledge distillation", "content": "The initial application of model compression often results in a non-trivial performance deficit. To\nmitigate these effects, previous research has leveraged knowledge distillation (KD) [2] [1]. Our\nmethodology advances this technique by sequentially compressing different layers at various stages\nthroughout the training process. This iterative approach allows the model to dynamically adjust\nand recalibrate to the compression of individual layers, which we have found to be instrumental\nin achieving more rapid convergence and improved overall model performance. This iterative pro-\ncess, coupled with knowledge distillation, enables the rest of the model to adapt seamlessly to the\nmodifications in the compressed layers."}, {"title": "Experiments", "content": "In our study, we focus on training a compressed version of the GPT-2small model, which we refer\nto as TQCompressedGPT-2. This model undergoes training on approximately 3.1% of the Open-\nWebText dataset, a significantly smaller subset compared to the full dataset typically used. We then\nevaluate our model's performance on the Causal Language Modeling task, assessing its perplexity\non benchmark datasets such as Wikitext-2, Wikitext-103[25], and Lambada[24]."}, {"title": "Experimental setup", "content": "\u2022 Pre-training dataset: We utilize 250K texts from the OpenWebText dataset.\n\u2022 Downstream task: The primary benchmarking task is causal language modeling (CLM),\nwith evaluations conducted on the Wikitext-2, Wikitext-103, and Lambada datasets.\n\u2022 Decomposition shapes: Our decomposition strategy, detailed in Table 1, specifically ex-\ncludes the decomposition of attention layers. This decision was based on observations that\ndecompressing these layers resulted in a more significant performance drop compared to\nother layers. However, by applying a higher compression ratio to the embedding layer, we\nmanaged to achieve a parameter count comparable to DistilGPT-2 and KnGPT-2.\n\u2022 Knowledge distillation (KD) scheme: As outlined in Section 3.5, we adopt a knowledge\ndistillation strategy for training our compressed model."}, {"title": "Results", "content": "Our evaluation of language modeling capabilities is centered around measuring perplexity on the\nWikitext-2, Wikitext-103, and Lambada testing subsets. This assessment also includes models with-\nout added permutation matrices to compare the effectiveness of our approach."}, {"title": "Conclusion", "content": "This paper presented a novel approach to compressing neural networks, particularly the GPT-2\nmodel, by introducing a permutation-based enhancement to Kronecker decomposition. Our method\nstands out for its ability to maintain the structural integrity and performance of the original model\nwhile significantly reducing its size. This is achieved through a series of innovative steps: optimal\npermutations of neuron connections, Kronecker decomposition, and knowledge distillation.\nOur TQCompressedGPT-2 model, a compressed version of GPT-2, demonstrates the efficacy of our\nmethod. We are comparing the performance of TQCompressedGPT-2 to the original GPT-2small,\nDistillGPT-2 and KnGTP-2. Despite our using only a tiny fraction of the original dataset for KD, we\nachieve comparable performance levels to GPT-2small. Note that both DistillGPT-2 and KnGTP-2\nused far larger portions of the original dataset than we, and yet we have maintained comparable\nperformance. This not only underscores the potential of our approach in practical applications but\nalso opens new avenues for further research. The compressed model, with its fewer parameters,\nbecames a viable option for deployment in resource-constrained environments.\nLooking forward, the implications of this research are far-reaching. Our method can potentially be\napplied to a wide range of neural network architectures beyond GPT-2, paving the way for more\nefficient AI models in various domains. The combination of permutation and tensor decompositions\npresents a new paradigm in neural network compression, balancing the trade-off between model size\nand performance.\nFinally, our research raises intriguing questions about the future of neural network architecture de-\nsign. The effectiveness of permutations in improving the suitability of neural networks for compres-\nsion suggests that future architectures could be designed with such optimizations in mind from the\noutset. As artificial intelligence continues to evolve, techniques like ours will be critical in ensuring\nthat advanced models are accessible and practical for a broader range of applications."}, {"title": "Appendix", "content": "Appendix 1. Theorem proof.\nWe demonstrate the equivalence of minimizing (3) and solving the assignment problem with matrix\n(4) as follows:\nFor any matrix A its Euclidean norm can be presented in such way:\n||A||2 = tr(AT. A) = tr(A \u00b7 AT). With respect to that:\n||PW(1) \u2013 W(2) ||2 = tr[(PW(1) \u2013 W(2))(W(1)T PT \u2013 W(2)T)]\n= tr(PW(1)W(1)T PT \u2013 PW(1)W(2)T \u2013 W(2)W(1)T PT + W(2) W(2)T)\ntr(PW(1)W(1)TPT) \u2013 2tr(PW(1)W(2)T) + tr(W(2)W(2)T)\n= ||W(1) ||2 \u2013 2tr(PW(1)W(2)T) + ||W(2) ||\n||W(1) ||2 + ||W(2) ||2 = const. It is equivalent to minimize the function that differs by a constant\nvalue, so\n(||PW(1) \u2013 W (2) ||2 \u2192 min) \u21d4 (tr(P \u00b7 K) \u2192 min)\nwhere K := -2W(1) W(2)T.\nHere, the initial problem (3) is equivalent to minimization of tr(PK), where P is a permutation\nmatrix. This is by definition the assignment problem with the matrix K.\nThis explains the values stored in matrix K:\n(W(1)W(2)T)ij =  . (2) = \u2211(1) W(2)\nkikjk\nKij = \u22122(W(1)W(2)T) = -2..)\nThe multiplication of i-th row of W(1) and j-th row of W(2) results in the matrix K. Let D be a\nmatrix defined as:\nDij = ||w{1) \u2013 w2) ||2\n= tr[(w(1) \u2013 w1))(1)T \u2013 w(2)T)]\n=tr(w(1)w(1)T) - 2tr (w(1) w(2)T) (2)T) + tr(w(2) w(2)T)\n= |||12 \u2013 2WW2 + \\w\\2) || 2.\nAs shown, Kij = \u22122\u03a3 WW2 Dij = ||w{1) ||2 + Kij + ||w2||2.\ntr(P. D) = \u2211 Di,p(i) = \u2211(||\u03c9 1) ||2 + Ki,p(i) + ||w2||2) =\n= \u2211 ||w1||2 + tr(P. K) + \u2211 || 2) \\12,\nwhere p(i) is an index permutation corresponding to permutation matrix P.\nAgain, \u03a3|| (1) ||2 +\u03a3 |w2||2 = const, so minimization of tr(P. D) is equivalent to tr(P. K).\nTherefore, minimizing (3) is equivalent to the assignment problem with matrix D."}]}