{"title": "Conceptwm: A Diffusion Model Watermark for Concept Protection", "authors": ["Liangqi Lei", "Jing Yu", "Qi Wu", "Keke Gai", "Liehuang Zhu"], "abstract": "The personalization techniques of diffusion models succeed in generating specific concepts but also pose threats to copyright protection and illegal use. Model Watermarking is an effective method to prevent the unauthorized use of subject-driven or style-driven image generation, safeguarding concept copyrights. However, under the goal of concept-oriented protection, current watermarking schemes typically add watermarks to all images rather than applying them in a refined manner targeted at specific concepts. Additionally, the personalization techniques of diffusion models can easily remove watermarks. Existing watermarking methods struggle to achieve fine-grained watermark embedding with a few images of specific concept and prevent removal of watermarks through personalized fine-tuning. Therefore, we introduce a novel concept-oriented watermarking framework that seamlessly embeds imperceptible watermarks into the concept of diffusion models. We conduct extensive experiments and ablation studies to verify our framework. Our code is available at https://anonymous.4open.science/r/Conceptwm-4EB3/.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in diffusion models have revolutionized the field of image generation, making it possible to create realistic images from simple text description inputs. With the growing demand for generating highly personalized and customized content, the personalization techniques [10, 11, 18, 29] of diffusion models have achieved significant progress in generating novel concept descriptions (e.g., unique visual themes, motifs, stylistic preferences, or characters) with a few images of specific themes or styles. Through powerful personalization tools like DreamBooth [27] and Lora [14], anyone, even without design skills, can create images of specific concepts using just a few personal images and simple text inputs. However, the use of personalization techniques introduces significant risks, particularly the potential for concept misuse or leakage. Untrustworthy users can exploit these tools to create unauthorized concepts related to celebrities or artworks, leading to deep-fake incidents [15], the spread of fake news and online spam [4], and serious copyright violations, as the original creators' intellectual property and creativity are undermined.\nTo address the ownership and accountability issues of specific concepts in generative models, recent research has introduced the task of concept watermarking [7], which aims to embed watermark information to the concept and extract the watermark from the subsequent generated images based on the watermarked concept. In practice, the model owner typically possesses a pre-trained diffusion model and seeks to enable the generation of images for a new concept. The concept owner provides images with a specific concept to fine-tune the existing diffusion model while embedding watermark information related to the concept, enabling the model to generate images containing the new concept and allowing the watermark decoder to extract watermark information from the generated images.\nEven though adding watermarks to generated images has brought increasing attention [6, 8, 9, 17, 34, 36, 37, 41, 43], it is non-trivial for existing watermarking solutions to achieving concept watermarking. On one hand, most existing methods [9, 34, 43] typically watermark the whole images rather than images of specific concepts, so that they are inapplicable in the concept watermarking scenario. Fine-grained watermarking mechanisms for specific concepts has rarely addressed by prior work. The major challenge is that imgae watermarking concept watermarks must not only exist within a specific image, but also remain present across various image variations generated by diffusion model. Dissimilar to traditional image watermarks embedded at the pixel level, the concept watermarks appears a higher-level technical barrier as it associates the watermarks with high-levels of sematic features related to specific concepts. Since the amount of images with a specific concept generally is limited for personalization model fine-tuning by injecting a new concept, it is a great challenge for existing methods which embed watermarks with a large volume of training samples.\nOn the other hand, robustness is another challenge for existing watermarking methods, when considering diverse adversaries in the concept protection scenarios. Recent work also showed that existing methods failed in defending model purification attacks [21, 45], as adversaries can bypass watermarking schemes by generating various watermark-free images through personalized fine-tuning. Balancing adversariality, image quality, and watermark fidelity becomes a technical obstacle. One of alternatives for increasing watermark adversariality is to inject imperceptible protective adversarial perturbations into the protected images, but this idea generally has a negative impact on both image quality and watermark fidelity [19, 28, 31]. Therefore, finding out a watermarking solution to ensuring concept tracing while addressing threats from adversaries has an urgent demand.\nThere are three core properties needed for satisfying the requirement of concept watermarking, namely, refined nature, robustness, and adversarial properties. Refined nature: The knowledge of a specific concept is learned by the denoising network (U-net), which is the core component of the diffusion model. Adding a watermark to a specific concept should be done with the least amount of training data and training steps possible, while ensuring that the fidelity before and after adding the watermark remains unaffected. Robustness: The watermark should remain stable across various image processing operations and under different generation parameters. Adversarial properties: The generated concept-specific images should not be subject to \u201cjailbreaking\" via personalized fine-tuning.\nTo address the challenges outlined above, we propose a novel concept watermarking framework ConceptWm. We consider that a refined watermark training and Fidelity Preserving Perturbation are two critical factors in the context of concept watermarking. To achieve the concept watermark embedding, we propose Concept-oriented Watermarking Finetuning, which ensures the rapid learning of watermark embedding patterns in diffusion models with limited data and training budget. To ensure the robustness of the watermark against various image processing attacks and facilitate the learning of the watermark diffusion model, our Watermark Components Pretraining Module incorporates the watermark into the latent space and employs a distortion layer to ensure the watermark can withstand various image processing attacks. To effectively balance adversarial perturbation, watermark fidelity, and image quality, while mitigating the risk of adversarial attacks bypassing the watermark through personalized fine-tuning, we introduce the Fidelity Preserving Perturbation Modulation. This approach employs an alternating optimization strategy, iteratively refining both the model parameters and adversarial perturbations, thus attaining an optimal equilibrium between adversarial robustness, image quality, and watermark fidelity.\nOur contributions are as follows: (1) We have proposed a novel concept-driven watermarking framework that seamlessly embeds imperceptible watermarks into the concept of diffusion models. We consider that a refined watermark training and fidelity preserving perturbation are two critical factors in the context of concept watermarking, which offers a fresh perspective on how diffusion model watermarking can be applied in real-world scenarios. (2) We propose a mechanism named Fidelity Preserving Perturbation Modulation for endowing concept watermarks with adversarial properties. Through the alternating optimization of model training and adversarial algorithms, it effectively balances adversarial robustness, image quality, and watermark fidelity. (3) Experiments demonstrate that Conceptwm effectively integrates adversarial perturbations into watermarks and maintains better robustness and accuracy in concept wa-\""}, {"title": "2. Related Work", "content": "Personalization Techniques for Diffusion Models. Given the substantial computational requirements of training Stable Diffusion from scratch, many methods aim to inject specific concepts into the model through fine-tuning. Currently used fine-tuning techniques include Textual Inversion [10], DreamBooth [27] and LoRA (Low-Rank Adaptation)[14]. Textual Inversion focuses solely on training text embeddings during fine-tuning, allowing concepts to be injected into the text encoder without altering the weights of the U-Net. DreamBooth, on the other hand, fine-tunes the entire U-Net portion of the Stable Diffusion model. Unlike conventional text-to-image fine-tuning, DreamBooth employs prior loss during training to prevent overfitting. LoRA achieves quick and lightweight fine-tuning of Stable Diffusion by training incremental weights in the attention layers of the U-Net.\nModel Watermarking for Latent Diffusion Models. WDM [43] trains an autoencoder to stamp a watermark on all training data before re-training the generator from scratch. Aqualora [8] propose a watermark Lora method to watermark all the images. VAE-based methods [9, 17, 36] fine-tune VAE-Decoder to ensure that all generated images contain the watermark. However, the aforementioned watermarking schemes apply watermarks to all generated images, leaving a gap in the protection of specific styles. Meanwhile, A large number of images is often required for the model to learn the watermark embedding pattern, but concept-oriented images for finetuning are often limited.\nProtective Perturbation against Stable Diffusion. To safeguard personal images, such as faces and artwork, from potential infringement during the fine-tuning of Stable Diffusion, recent studies focus on disrupting the fine-tuning process by adding imperceptible protective noise to these images. AdvDM [19] introduces a direct adversarial attack on Stable Diffusion by maximizing the Mean Squared Error loss during optimization. This method employs adversarial noise to safeguard personal images. Anti-DreamBooth [31] integrates the DreamBooth fine-tuning process of Stable Diffusion into its framework, designing a bi-level min-max optimization approach to generate protective perturbations. Additionally, other studies [35, 38, 44, 46] have investigated the use of similar adversarial perturbation techniques to generate protective noise for images."}, {"title": "3. Preliminaries", "content": "Prompt-based Latent Diffusion Model. Conditional denoising model, denoted as $e_{\\theta}(x, t, c)$, can generate images based on a given text condition c. $x_t$ represents the latent representation at a particular timestep t within the set {1,2..., T}. During training, a loss function $\\mathcal{L}_{cond}$ is used to guide the diffusion model in denoising the latent representations, $x_t = \\sqrt{\\bar{\\alpha}_{t+1}}x_0 + \\sqrt{1 - \\bar{\\alpha}_{t+1}}\\epsilon_{\\theta}(x_t)$. Given the input image $x_o$, each variable $x_t$ is constructed via injecting noise at corresponding timestep t. The objective can be expressed as follows:\n$\\mathcal{L}_{cond}(\\theta, \\theta) = \\mathbb{E}_{x_o, t, c, \\epsilon \\in \\mathcal{N}(0, I)}||\\epsilon - \\epsilon_{\\theta}(X_{t+1}, t, c) ||^2$. (1)\nDreamBooth. DreamBooth employs the base loss of diffusion models in Eq.1. It also incorporates a prior preservation loss to avoid overfitting and text-shifting issues, particularly when only a limited set of instance examples is available. More precisely, it uses a generic prior prompt such as \"a photo of [class]\u201d and learn the concept with the prompt such as \"a photo of [concept >[class]", "follows": "n$\\mathcal{L}_{ft}(\\theta, x_o) = \\mathbb{E}_{x_o, t}||\\epsilon - \\epsilon_{\\theta}(X_{t+1}, t, c) ||^2 + \\lambda||\\epsilon' - \\epsilon_{\\rho}(x'_{t+1}, t, C_{cls}) ||^2$, (2)\nwhere $\\epsilon$ and $\\epsilon'$ are both sampled from the standard normal distribution $\\mathcal{N}(0, I)$, the noisy variable $x'_{t+1}$ is generated from the original stable diffusion model $\\theta_{ori}$ using the prior prompt $C_{cls}$. The term $\\lambda$ emphasizes the importance of the prior term in the generation process.\nAdversarial attacks. The goal of adversarial attacks is to find imperceptible perturbations to the input image that mislead the behavior of a given model. Typical research usually focuses on classification tasks, where, for a model f, an adversarial example x' of an input image x is generated such that it is visually indistinguishable, while causing a misclassification $y = f(x')$. The perturbation is typically constrained to lie within an n-ball to ensure minimal visual difference, specifically satisfying $||x'-x||_p < \\eta$. Projected Gradient Descent (PGD) [23] is a widely used attack method that relies on an iterative optimization process. The procedure to update and predict $x'$ for an untargeted attack is as follows:\n$x_{t+1} = \\Pi_{\\mathcal{X}, \\eta} (x + \\alpha \\cdot sign(\\nabla_x \\mathcal{L}(f(x), y)))$, (3)\nwhere $\\Pi_{\\mathcal{X}, \\eta} (z)$ restricts the pixel values of z within an n-ball around the original values in x."}, {"title": "4. Methodology", "content": "4.1. Motivation\nChallenges of existing methods: (1) How to achieve concept-oriented watermark fine-tuning of U-net with limited data and training budget? We will solve this via the framework design in Sec. 4.4. (2) Existing watermarking methods lack protective mechanisms to prevent watermark \"jailbreaking\" through personalized fine-tuning. Integrating"}, {"title": "4.2. Overview", "content": "The main objective of our method is to protect images of the specific concept generated by the diffusion model by embedding adversarial noise and the watermark pattern into the U-net component of the diffusion model. Figure 2 illustrates an overview of our approach. The Watermark Components Pretraining module implements watermarking in the latent space and enhances the robustness of the watermark through a distortion layer. Fidelity Preserving Perturbation Modulation implements adversarial perturbations to the image while balancing watermarking, image quality, and adversarial properties. Concept-oriented Watermark Fine-tuning enables learning for specific concepts and integrates adversarial watermarks through fine-tuning."}, {"title": "4.3. Watermark Components Pretraining", "content": "Existing image watermarking methods often face significant differences between the watermark embedding pattern and the latent space, which causes the watermark information to be disrupted or even lost when transformed into the latent space by the VAE encoder or diffusion regeneration. This issue has been discussed in related watermarking attack methods [1, 3, 5]. Our objective is to embed a robust watermark within the image that is both resilient and conducive to efficient learning by the U-Net architecture.\nThe U-Net learns the distribution and noise of the image in the latent space, and embedding the watermark into the latent space enables the model to learn the consistency between the watermark and the generated images. In this step, the watermarked images are processed through the message decoder to evaluate the discrepancy between the decoded watermark information and the original content of the watermarked image. The message loss is the Binary Cross Entropy (BCE) between the message m and the sigmoid $\\sigma(m')$.\n$\\mathcal{L}_m = \\sum_{k=0}^{n-1} m_k log \\sigma (m'_k) + (1 - m_k) log (1 - \\sigma (m'_k))$ (4)\n$I_o$ is the original image. $I_r$ is the reconstructed image. m is secret message (i.e.,watermark). To ensure visual consistency, we use the LPIPS loss function [40] to evaluate the global loss between the watermarked image and the reconstructed image. To avoid excessive modification of local image regions, we select the most prominent parts of the image and calculate channel loss.\n$Res(I_o, I_r) = \\frac{1}{K} \\sum_{k=1}^{1-3} \\sum_{c=1}^3 \\sum_{i,j} |I_c^o(i, j) - I_c^r (i, j)|$ (5)\n$L_{channel}$=[Top-k (Res (Io, Ir))]\nOur training objective can be summarized as follows,"}, {"title": "4.4. Concept-oriented Watermark Fine-tuning", "content": "At this stage, the watermark is embedded into specific concepts and then injected into the U-Net. The main challenges are as follows: The fine-tuned images for specific concepts are often limited, making it difficult for the model to learn the specific watermark patterns; Due to the introduction of adversarial perturbations, directly using the DreamBooth training paradigm will be affected by protective perturbations, preventing the model from learning the concept.\nThe limited number of specific concept samples and adversarial perturbation often result in failures when directly adding an adversarial watermark into images and deploying DreamBooth. This can prevent the watermark embedding pattern from being learned effectively and lead to quality degradation. Our core idea is to decouple concept learning from adversarial watermark learning. In the DreamBooth stage, we learn the specific concept, and by adding adversarial noise to the same image and also using the image without the adversarial noise, the model learns to differentiate between them. The training objective of this stage is as follows:\n$\\mathcal{L}_{ft} (\\theta) := \\mathbb{E}_{t, c, x} [|\\epsilon_{\\theta}(x_t + \\delta, t, c) - \\epsilon_{ori} (x_t, t, c) ||^2]$ (7)\nrepresents the adversarial perturbation containing the watermark generated through Fidelity Preserving Perturbation Modulation, $\\epsilon_{ori}$ denotes the fixed model, and $\\epsilon_{\\theta}$ refers to the trained model."}, {"title": "4.5. Fidelity Preserving Perturbation Modulation", "content": "The goal of this fine-tuning is to maximize the loss of $\\mathcal{L}_{cond}$ in Eq. 1 and minimize the loss of $\\mathcal{L}_{ft}$ 2. The adversarial requirement for watermarking is to disrupt the learning process of DreamBooth. Since the DreamBooth model tends to overfit on adversarial images, this can be exploited to make it perform worse when reconstructing clean images.\n$\\sigma^*(\\delta) = arg \\underset{\\delta}{max} \\mathcal{L}_{cond} (\\theta^*, x^{(i)}), \\forall i \\in \\{1, .., N_{ft}\\}$,\ns.t. $\\theta^* = arg \\underset{\\theta}{min} \\sum_{i=1}^{N_{ft}} \\mathcal{L}_{ft}(\\theta, x^{(\\delta)} + \\sigma(\\epsilon))$,\nand $||r^{(i)}||_j \\leq \\eta \\forall i\\in \\{1,.., N_{ft}\\}$ (8)\nwhere $x^{\\delta}$ denotes the clean samples. Meanwhile, we need to ensure that the image quality and watermark loss are as minimal as possible. Our solution is to use watermark-free images for one-step DreamBooth to reduce the DreamBooth loss $\\mathcal{L}_{cond}$ 1, while simultaneously applying PGD to enhance the adversarial loss $\\mathcal{L}_{ft}$ 2.\n$\\theta' \\leftarrow arg \\underset{\\theta}{min} \\sum_{x\\in X_{ft}} \\mathcal{L}_{ft}(\\theta, x^i)$\n$\\delta \\leftarrow \\theta.clone()$\n$\\delta^{(2)} \\leftarrow arg \\underset{\\delta}{max} \\mathcal{L}_{cond} (\\theta', x^{(i)} + \\sigma_1 + \\sigma_2)$ (9)\nConsidering that the adversarial perturbation will affect the watermark, we also incorporate the decoded message loss as a constraint to optimize the perturbation.\n$\\mathcal{L}_{msg} = BCE(decoder(x^{(\\delta)} + \\sigma_1 + \\sigma_2)), m_{target})$ (10)\nwhere $\\sigma_1$ denotes the residual of the watermarked image and original image. The Fidelity Preserving Perturbation Modulation outputs an adversarial watermark as the parameter for Concept-oriented Watermark Fine-tuning to facilitate learning."}, {"title": "5. Experiment", "content": "5.1. Implementation Details\nDatasets. To evaluate the effectiveness of the proposed methods. For subject-driven generation, we select the face dataset CelebA-HQ [47]. For style-driven generation, we select 12 artists with distinct styles from WikiArt \u00b9, with each artist contributing 10 artworks that are consistent in terms of both artistic style and time period. Each image is resized and center-cropped to a resolution of 512 \u00d7 512.\nTraining Configurations. By default, we use the Stable Diffusion (v2.1) as the pretrained generator. Unless stated otherwise, the training instance prompt for subject-driven generation is \"a photo of [concept >person\" and the prior prompt is \"a photo of person\". The training instance prompt for style-driven generation is \"a painting in the style of <concept >\" and the prior prompt is \"a painting\". For PGD, we use a = 1e-5 and the default noise budget $\\eta$ = 1e-3. The learning rate of the dreambooth is set 5e-6.\nEvaluation Metrics. We evaluate the detection performance using the true positive rate (TPR) at a false positive rate (FPR) of 10e-5. For traceability performance, we assess bit accuracy. To measure the quality of image generation, we compute the Peak Signal-to-Noise Ratio (PSNR) [13] and Structural Similarity Index (SSIM) [33] to evaluate image distortion; and the Natural Image Quality Evaluator (NIQE) [24] and Perceptual Image Quality Evaluator (PIQE) [32] for evaluating overall image quality."}, {"title": "5.2. Watermark Performance Comparison", "content": "We evaluate our method against six typical baselines on two tasks: (1) Detection: All methods are tested as single-bit watermarks with a unified watermark, where the False Positive Rate (FPR) is set to 1%, and we assess the True Positive Rate (TPR) on 1,000 watermarked images. (2) Traceability: Except for the single-bit watermark Tree-Ring, all other methods are tested as multi-bit watermarks. In this experiment, we simulate a scenario with 1,000 model users, each requiring a distinct watermark for model tracing. Each user generates 10 images, creating a dataset of 10,000 watermarked images. During testing, we calculate the Bit Accuracy by comparing the watermark of each user with the watermarked images. The user with the highest Bit Accuracy is considered the traced user. The comparison results, shown in Table 1, demonstrate that our watermarking method achieves superior robustness, significantly outperforming the baselines in both tasks. In terms of Bit Accuracy, our method outperforms the best baseline by around 6.3%. This improvement is due to the widespread diffusion of the watermark throughout the latent space, creating a strong binding between the watermark and the image semantics."}, {"title": "5.3. Fidelity", "content": "Table 1 presents a comparison of our method with other baseline methods. For fidelity evaluation metrics, we use the Fr\u00e9chet Inception Distance (FID) [12] for assessing image diversity and semantic relevance calculated on 5000 im-"}, {"title": "5.4. Watermark Robustness against Attack", "content": "Image Processing. We select eight representative types of image-level noise to evaluate the robustness of image processing. The main results are shown in Table 2.\nMain Results. For each image processing attack, we report the average bit accuracy in Table 2. We see that our Conceptwm is indeed robust across all the transformations with the bit accuracy all above 0.9. Conceptwm remarkably outperforms existing multi-bit watermarks on the average performance with more than 6.3% boost compared with the state-of-the-art (SoTA) results. Since our method embeds the watermark into the entire latent space and trains the distortion layer, it has a clear advantage over contrastive watermarking schemes when facing various image processing attacks. Please refer to the Supplementary Materials for more results."}, {"title": "5.5. Concept Protection Evaluation", "content": "Model Mismatching. We provide an example of transferring adversarial watermark trained on SD v1.4 and SD v2.1 to defend DreamBooth models trained from v2.1 and v1.4 in Table 4. The results show that our method still works effectively across different versions of the Diffusion Model.\nTextual Inversion and DreamBooth with LoRA. Textual Inversion teaches new concepts by adjusting a word vector, rather than fine-tuning the entire model. LoRA, on the other hand, employs low-rank weight updates to enhance memory efficiency and is widely adopted within the community. The relevant results are shown in the Table 4. Our method shows better performance in mitigating the degradation effect on generation with LoRA. Since Textual Inversion does not inject concepts directly into the model, the reduction in quality is not significant and the semantic quality of the generation decrease.\nMain Results. The results in Table 4 demonstrate that adversarial properties of our watermark is preserved across"}, {"title": "5.6. Ablation Studies.", "content": "Prompt Mismatching. Considering that users generate relevant images based on various prompts combined with specific concepts, we generate images for specific concepts under different prompt conditions. The main results are shown in Table 3, which shows that our watermarking method achieves higher bit extraction accuracy for images similar to the training set. When the image deviates significantly from the set instance prompt, our watermarking method still maintains good precision. Furthermore, the generated watermark exhibits a certain level of adversarial robustness, as the quality of images trained under this watermark is reduced, which is beneficial for the protection of specific concepts in diffusion models.\nDistortion Layer. We evaluated the impact of various attacks on image quality in the Distortion Layer in the watermark components pretraining. The main results are in Table 5, which indicates that cropping and the mask adversarial layers have a significant impact on image quality, as they require a stronger watermark to be stably embedded within a smaller space.\nInference Steps. Users may influence watermark extraction under different inference parameters. We generated images of specific concepts at different inference steps and performed watermark extraction. The results, shown in Table 3, indicate that the inference steps have little impact on watermark extraction.\nGuidance Scales. Larger guidance scales result in more faithful of the generated image adherence to prompts. We evaluate the impace of guidance scale in the Table 3. The results show that our method maintains good stability across various values of the Guidance scale.\nSampling Method. We evaluate our watermark on DDIM [30], DPM-Solver multi-step [22], Euler and Heun samplers [16]. The results show that our watermark is minimally affected by the choice of sampler.\nDifferent VAE Decoder. An adversary may interfere with watermark decoding by replacing the VAE decoder. To evaluate the impact of the generated images on the watermark, we used different types of VAEs including stable diffusion V1.4 and V2.1 for decoding. The results in Table 3 indicate that changing the VAE does not have a significant impact on the watermark extraction, which is a clear advantage over VAE-based schemes, as the watermark cannot be preserved when the VAE is replaced in VAE-based methods."}, {"title": "6. Discussion", "content": "Limitations. First, our method exhibits limited detection performance under certain prompts. As shown in Figure 3, when the model generates poor results for specific concepts under certain prompts, the detection accuracy cannot be guaranteed. This issue requires more precise and improved personality techniques or larger datasets and prompts. Second, the adversarial perturbations generated by our proposed method have limited effectiveness against the latest purification methods [2, 45]. Since our approach involves embedding concepts, watermarks, and adversarial perturba-"}, {"title": "7. Conclusion", "content": "This paper reveals the potential threats posed by diffusion model personalization techniques and proposes an adversarial watermarking method to protect specific concepts. Our solution integrates adversarial perturbations and watermarks into the diffusion model, ensuring that images generated by the diffusion model for specific concepts are traceable, and any personalized model trained on these images will produce low-quality results. We have evaluated the stability and adversarial effectiveness of the watermarking method we designed, demonstrating that our defense remains effective under adverse conditions. In the future, our goal is to improve the quality and robustness of the watermark and consider more potential threats."}]}