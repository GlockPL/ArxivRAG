{"title": "MAD-UV: The 1st INTERSPEECH Mice Autism Detection via Ultrasound Vocalization Challenge", "authors": ["Zijiang Yang", "Meishu Song", "Xin Jing", "Haojie Zhang", "Kun Qian", "Bin Hu", "Kota Tamada", "Toru Takumi", "Bj\u00f6rn W. Schuller", "Yoshiharu Yamamoto"], "abstract": "The Mice Autism Detection via Ultrasound Vocalization (MAD-UV) Challenge introduces the first INTERSPEECH challenge focused on detecting autism spectrum disorder (ASD) in mice through their vocalizations. Participants are tasked with developing models to automatically classify mice as either wild-type or ASD models based on recordings with a high sampling rate. Our baseline system employs a simple CNN-based classification using three different spectrogram features. Results demonstrate the feasibility of automated ASD detection, with the considered audible-range features achieving the best performance (UAR of 0.600 for segment-level and 0.625 for subject-level classification). This challenge bridges speech technology and biomedical research, offering opportunities to advance our understanding of ASD models through machine learning approaches. The findings suggest promising directions for vocalization analysis and highlight the potential value of audible and ultrasound vocalizations in ASD detection.", "sections": [{"title": "1. Introduction", "content": "Autism Spectrum Disorder (ASD) is a complex neurodevelopmental condition that affects social interaction, communication, and behavior [1]. While human studies provide critical insights, animal models, particularly mice, are essential for further understanding the genetic and neurological underpinnings of ASD [2, 3, 4]. According to [5], 10-20% of autism cases are caused by abnormal chromosomes. Several studies have sought to identify the genetic mechanisms behind at least some cases of autism [6, 7]. As mice and humans share the affected genome regions, chromosomal engineering [8] can be utilized to design mice with the same genetic defects that are proven to account for many cases of autism in humans. When undergoing behavioral tests, such mice have been shown to exhibit behavioral and affective patterns that are comparable to those seen in humans diagnosed with ASD [4], making them a potential model of human ASD.\nOne area that has garnered interest in this context is the analysis of Ultrasound Vocalizations (USVs) produced by mice [9]. These vocalizations, inaudible to humans, have been shown to vary significantly between wild-type (healthy) mice and those genetically engineered to model ASD [4]. In this work, we introduce the Mice Autism Detection via Ultrasound Vocalization Challenge (MAD-UV), in which participants are tasked with building models to automatically classify mice as either ASD or wild-type based on their USVs. The challenge makes use of data collected in the study [4], comprising around 7 hours of USVs by 84 different subjects. While numerous pre-"}, {"title": "2. Related Work", "content": "As for humans, there is a plethora of evidence for prosodic differences between individuals with and without ASD. For example, individuals with ASD have been found to exhibit a comparably slow speech rate [13] and unusually melodic intonation [14]. See [15] for a recent survey on prosodic peculiarities observed in humans with ASD.\nThe existence of systematic patterns in ASD patients' speech motivates the application of machine learning to effectively detect ASD from human speech. Automatic autism detection from (children) speech was posed as an INTERSPEECH challenge as early as 2013 [16]. Marchi et al. [17] utilize handcrafted features such as eGeMAPS [18] in combination with Support Vector Machines (SVMs) to distinguish ASD-affected"}, {"title": "3. Data", "content": "The dataset contains recordings of 84 subjects, of which 44 (30 male, 14 female) belong to the wild-type and 40 (27 male, 13 female) are ASD model types. For all subjects, we collect one recording at an early development stage, more specifically 8 days after their birth, i.e., Postnatal Day 8 (P08). During this early developmental stage, mice emit an increased amount of USVs due to anxiety-like behavior induced by separating them from their mothers [4]. Each recording is around 5 minutes long, resulting in about 7 hours of audio in total. All recordings are obtained via high-precision microphones (Avisoft UltraSoundGate 416H) with a sample rate of 300 kHz.\nThe dataset underwent stratified partitioning to ensure subject independence while maintaining balanced distributions of sex and ASD model type. For evaluation purposes, approximately 20% of subjects were allocated to the test set first, consisting of 12 male and 4 female subjects. To enhance the statistical robustness of the evaluation, each test sample was segmented into non-overlapping 30-second clips, yielding 160 audio samples. In accordance with the challenge setup, subject IDs and their corresponding labels were withheld from the test set.\nSubsequently, for the baseline system implementation, the remaining subjects were partitioned into training and validation sets, with 51 subjects (approximately 60%) allocated to the training set and 17 subjects (approximately 20%) assigned to the validation set. The distribution of the ASD model type maintained consistency across partitions, comprising 47.06% (24/51) of training subjects, 47.06% (8/17) of validation"}, {"title": "4. Baseline Experiments", "content": "To address the constraints of limited training and validation data, the 5-minute recordings in both subsets were systematically segmented into 30-second clips with 15-second overlap. Clips of insufficient duration were excluded from the dataset due to their limited information. This segmentation approach yielded 19 clips per original sample, resulting in a total of 969 training samples and 323 validation samples.\nIn order to establish a simple, yet robust benchmark, we train a CNN-based model employing different feature sets. We select three different spectrogram-based feature sets widely applied for human speech research, namely full, ultra, and audi as will be detailed.\nSpectrograms and their variant Mel-spectrograms represent a well-established and conventional approach to feature extraction in audio-based machine learning applications. These time-frequency/quefrency representations have demonstrated robust performance across diverse domains, including acoustic scene classification [27, 28], animal vocalization classification [29, 30], and speech emotion recognition [31, 32]. Consequently, spectrogram-based feature extraction presents a methodologically sound basis for the baseline system implementation.\nThe feature extraction methods began with a unified preprocessing approach for all three feature types, beginning with spectrogram generation at a sampling rate of 300 kHz by using Scipy\u00b9. To optimize frequency resolution, both the window size and the number for the Fast Fourier Transform (FFT) were configured at 300, 000. The hop length was set to 150, 000, resulting in a temporal dimension of 59 frames.\nSubsequently, the spectrogram was partitioned into distinct frequency ranges. The ultrasonic component, encompassing frequencies between 20 kHz and 150 kHz, was extracted to constitute the ultra feature set. Correspondingly, the audible spectrum below 20 kHz was separated to form the audi feature set."}, {"title": "5. Results", "content": "Given the significance of both ASD and wild-type classes, the UAR-accounting for both classes in the dataset-is selected as the evaluation criterion for the challenge. As mentioned above, we compute UAR for both predictions in segment-level and subject-level evaluation.\nThe results demonstrate the effectiveness of different feature extraction methods in addressing both segment-level and subject-level classification tasks, highlighting promising directions for further exploration. Analyses reveal comparable performance metrics across all three feature sets on the validation set, suggesting effective feature learning capabilities of the CNN architectures during the training phase. Due to the implementation of majority voting, a direct proportional correlation between subject-level and segment-level UAR is not observed. Furthermore, subject-level evaluation metrics are derived from optimal segment-level performance. When con-"}, {"title": "6. Challenge Organization and Outcome", "content": "In order to participate in MAD-UV, teams must register under the lead of a professor in academia, or a research team leader in industry. Upon signing the EULA, they obtain access to the dataset, the baseline code as well as the evaluation system. Teams are allowed to comprise at most 5 members excluding the PI. Moreover, we do not allow one and the same person to be part of several teams. None of the organizers will be members of any participating team.\nThe evaluation system will be hosted on eval.ai\u00b3, an established platform for shared task evaluation. This way, there will be a transparent leaderboard and automatic enforcement of the constraints on the number of submissions. Participants submit their segment-level predictions for the test set and immediately receive their results. Note that the test subject IDs are not disclosed to the participants. The subject-level results are calculated automatically using the same majority voting approach as described above."}, {"title": "7. Conclusion", "content": "In this baseline paper, we introduced MAD-UV, the 1st INTERSPEECH Mice Autism Detection via Ultrasound Vocalization Challenge. We described the extensive challenge dataset and reported a set of competitive baseline results that serve as benchmarks for participants' approaches. While all feature sets considered lead to above-chance performance, the results obtained with the audible spectrogram prove to be particularly encouraging. These insights not only enhance our understanding of mice vocalizations but also pave the way for leveraging human speech technologies to better analyze animal communication and behaviors. MAD-UV is designed to enable participants to contribute to this intriguing field within a standardized shared task setup. Besides those aiming at the merely quantitative task of outperforming the baselines, we also look forward to contributions that deal with the dataset in a more qualitative manner and thus increase our understanding of the data."}]}