{"title": "Cross-View Geolocalization and Disaster Mapping with Street-View and VHR Satellite Imagery: A Case Study of Hurricane IAN", "authors": ["Hao Li", "Fabian Deuser", "Wenping Yin", "Xuanshu Luo", "Paul Walther", "Gengchen Mai", "Wei Huang", "Martin Werner"], "abstract": "Nature disasters play a key role in shaping human-urban infrastructure interactions, Effective and efficient response to natural disasters is essential for building resilience and sustainable urban environment. Two types of information are usually the most necessary and difficult to gather in disaster response. The first information is about the disaster damage perception, which shows how badly people think that urban infrastructure has been damaged. The second information is geolocation awareness, which means how people's whereabouts are made available. In this paper, we proposed a novel disaster mapping framework, namely CVDisaster, aiming at simultaneously addressing geolocalization and damage perception estimation using cross-view Street-View Imagery (SVI) and Very High-Resolution satellite imagery. CVDisaster consists of two cross-view models, where CVDisaster-Geoloc refers to a cross-view geolocalization model based on a contrastive learning objective with a Siamese ConvNeXt image encoder and CVDisaster-Est is a cross-view classification model based on a Couple Global Context Vision Transformer (CGCViT). Taking Hurrican IAN as a case study, we evaluate the CVDisaster framework by creating a novel cross-view dataset (CVIAN) and conducting extensive experiments. As a result, We show that CVDisaster can achieve highly competitive performance (over 80% for geolocalization and 75% for damage perception estimation) with even limited fine-tuning efforts, which largely motivates future cross-view models and applications within a broader GeoAI research community. The data and code are publicly available at: https://github.com/tum-bgd/CVDisaster.", "sections": [{"title": "1. Introduction", "content": "Given the fast development in Remote Sensing (RS) technology, the availability of large-scale and high-quality Earth observation (EO) data has significantly benefited timely humanitarian responses to natural disasters (Van Westen, 2000; Dong and Shan, 2013; Li et al., 2023a). Meanwhile, recently, Street View imagery (SVI) has gained significant momentum in urban studies and computer vision in the last few years (Zhang et al., 2018, 2019; Biljecki and Ito, 2021), and has shown great potential in complementing traditional satellite imagery analysis by providing a unique and informative cross-view perspective on the ground (Zhu et al., 2022).\nIn a disaster mapping scenario, two types of information are critical for timely and accurate disaster response and relief. The first type of information is the disaster damage perception, which refers to the ways in which individuals and groups evaluate, subjectivize, and perceive damages to the urban built environment due to the disaster. This information is usually estimated from RS data based on expert knowledge and intensive manual efforts. The second type of information is geolocation awareness, which is basically how accurately people can geographically locate themselves on the map. By combining both information, an ideal disaster mapping framework is able to simultaneously estimate human percep- tion of the damage levels and provide accurate geolocations in the affected areas.\nHowever, it is not a trivial task to build such a framework due to two major challenges: on the one hand, traditional RS data can become insufficient for fine-grained damage perceptions, especially for distinct and sophisticated urban contexts, where a potential solution is to combine satellite imagery with the emerging source of SVIs to ensure a more fine-grained and cross-view of urban disaster dam- age perception. On the other hand, existing geolocalization approaches are often not satisfying, because they predomi- nantly depend on satellite navigation systems, such as GPS, Galileo, and BeiDou, which typically lack the appropriate accuracy required for disaster response. Meanwhile, urban context and weather conditions can bring another dimension of complexity where satellite signals are blocked. Fortunately, we have enough ingredients to address the latter challenge as cross-view geolocalization with satellite and street-view imagery offers a sensible alternative. Herein, this technique can match real-time SVI obtained from carriers against a collection of satellite imagery with known geolocations so that the geographical coordinates of SVI can be decided. To the best of our knowledge, there is no such disaster mapping framework exists that can achieve damage perception and cross-view geolocalization at the same time.\nIn this paper, we fill the aforementioned research gap by developing a novel disaster mapping framework - CVDisaster - (see Figure 1). Specifically, our framework addresses the damage perception estimation and cross-view geolocalization at the same time by leveraging the head-view satellite and street-view imagery using state-of-the-art Geospatial Artificial Intelligence (GeoAI) models. To validate the pro- posed framework, we conducted a case study in Sanibel Island, Florida, which was hit by Hurricane Ian in 2022. Intensive experiments show the great potential of CVDisaster in providing timely damage perception and geolocation awareness with competitive accuracy, leading to substantial advantages for future disaster response applications. Moreover, we made the case study dataset (i.e., CVIAN) openly available to encourage related research in both computer vision and disaster mapping communities.\nIn Section 2, we give an overview of related works re- garding state-of-the-art disaster mapping, street-view image- based urban analysis, and cross-view geolocalization, re- spectively. In Section 3, we elaborate on the detailed method- ology design of the proposed framework, ranging from the problem statement to the training and inferencing of both geolocalization and damage perception estimation models. Next, Section 4 shows the experimental results from the case study of Hurricane IAN and summarizes the key findings, followed by Section 5 presenting a critical reflection of limitations and identifying future works. Last but not least, Section 6 concludes the paper by highlighting the scientific contributions to a broader community."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. GeoAI for Disaster Mapping and Localization", "content": "Disaster mapping refers to the capability for even non- profession to assist in disaster response situations via map- ping and other spatial analysis (Herfort et al., 2021; Li et al., 2022). The concept of disaster mapping has been success- fully used to support disaster response and humanitarian aid activities, especially under a disaster scenario, where successful examples include the mapping tasks during the 2017 Hurrican Harvey (Feng et al., 2020), the 2019 Cyclone Idai and Kenneth in Mozambique (Li et al., 2020), and the 2023 Turkey Syria Earthquake (Wikipedia, 2023). However, considering the time-crucial nature of disaster responses and humanitarian aid, traditional disaster mapping workflows become less efficient and unsatisfactory in covering a large- scale area and providing timely damage assessment within a rather short time. In this context, the emergence of high- resolution satellite imagery allows for faster and better dis- aster mapping with GeoAI techniques (Salcedo-Sanz et al., 2020; Werner and Li, 2022), thus providing a promising solution to address this challenge that local stakeholders currently encounter. Early works in this direction (Herfort et al., 2019; Huck et al., 2021) report an interesting finding on improving the speed and accuracy of disaster mappings via a machine-assisted manner. In the meantime, there is a stream of GeoAI research focusing on extracting accurate location information during disasters, mainly from social media text data (e.g., Twitter) (Kumar and Singh, 2019; Hu and Wang, 2020; Mihunov et al., 2020; Hu et al., 2022, 2023b). One famous example is the news article published in the U.S. National Public Radio, titled \"Facebook, Twitter Replace 911 Calls For Stranded In Houston\", which reported how affected people by Hurricane Harvey in 2017 used social media to share their location and asked for help, which significantly helps the rescue team to locate and reach those people in need. One can find a comprehensive survey on location reference recognition in Hu et al. (2023a).\nHowever, a majority of existing disaster mapping and localization approaches either rely on post-disaster satellite imagery analysis for damage assessment or use geoparsing tools to georeference a social media text. Therefore, there is a pressing need for an intelligent disaster mapping and geolocalization solution, ideally within a single framework. To the best of our knowledge, CVDisaster is the first such integrated framework that can achieve large-scale damage perception and cross-view geolocalization at the same time."}, {"title": "2.2. Street-view Imagery for Urban Analytics", "content": "Due to its emerging availability, SVI has become a crucial data source for urban studies. Diakakis et al. (2017) conducted a comprehensive review of the applications of SVI in urban research, highlighting its growing significance in urban analysis. Their study indicates that most urban research utilizing SVI relies on Google SVI (GSVI). How- ever, crowdsourced platforms like Mapillary and KartaView are also rapidly evolving and becoming key tools in urban research.\nIn urban analysis, SVI is extensively applied across vari- ous fields, such as the maintenance of spatial data infrastruc- ture, studies of urban morphology and perception, and traffic flow analysis. For instance, Kim et al. (2020) and Li et al. (2023b) inferred urban features based on SVI to generate 3D urban models. Krylov et al. (2018) effectively detected utility poles and traffic signals using GSVI, demonstrating the unique efficacy of SVI in identifying streetlights and traffic signs. In urban morphology analysis, many scholars have estimated urban geometric indicators using SVI to study microclimates and light pollution. Hu et al. (2020) and Cicchino et al. (2020) extracted road variables from SVI to analyze the safety of walking and cycling in urban areas.\nResearchers also used SVI to extract information about human health and well-being. By matching participants' movement trajectories with SVI, one can analyze the envi- ronmental features residents encounter in their daily activi- ties, providing robust data support for public health policy- making. For example, Nguyen et al. (2018) investigated GSVI to extract derived indicators such as street greenery, crosswalks, and building types to describe the built environ- ment at the postal code level in three US cities. The study found a correlation between community characteristics and the prevalence of obesity and diabetes. In addition to these key indicators, Keralis et al. (2020) demonstrated that factors such as overhead visible wires and whether roads are single-lane are associated with various health outcomes, including diabetes, psychological distress, and alcohol con- sumption. Further related studies include analyzing resi- dents' air pollution exposure, stress levels, and infectious diseases based on street-view data (Apte et al., 2017; Han et al., 2022; Psyllidis et al., 2023).\nMore importantly, SVI plays an increasing role in dis- aster response, particularly in long-term recovery and re- construction planning. It helps decision-makers understand changes in disaster-affected areas, providing crucial refer- ences for future disaster prevention and urban planning. Curtis and Mills (2012) and Curtis et al. (2013) explored recovery after tornadoes, hurricanes, and wildfires using GSVI. Mabon (2016) utilized GSVI from the evacuation zone around the Fukushima Daiichi Nuclear Power Plant to assess dynamic disaster recovery methods. Additionally, SVI has been used in disaster emergency response and risk assessment. Diakakis et al. (2017) used GSVI to identify the probability of buildings in Athens being flooded. Naik (2016) designed a crowdsourced sensing system for disaster response during catastrophic flooding in Chennai, India, helping residents in flood-affected areas and reducing casu- alties. SVI provides detailed ground-level information, such as the condition of damaged buildings, the extent of street flooding, and the state of infrastructure. This information is crucial for disaster assessment and emergency response. By combining SVI with RS data, we can obtain more accurate and comprehensive disaster information, thereby enhancing the precision and efficiency of disaster response and support- ing post-disaster recovery and reconstruction. However, re- search that integrates SVI with RSdata in a disaster response scenario is still limited."}, {"title": "2.3. Cross-view Geolocalization", "content": "Unlike the single-image geolocalization task (Weyand et al., 2016; Cepeda et al., 2023; Zhou et al., 2024), cross- view geo-localisation enhances classic location-based ser- vices and navigation systems by matching ground-level im- agery with overhead imagery. This enables accurate posi- tioning in GNSS-denied environments, e.g., during a dis- aster. Workman et al. (2015) showed the superiority of CNN-based features for localizing a wide-ranging dataset with crawled Flickr images across the USA. In subsequent work, they introduced the first cross-view geo-localisation dataset, namely CVUSA Zhai et al. (2017). This dataset leverages street-view images from GSVI all across the US to match them against overhead imagery to locate the street-views. Since then multiple datasets have arisen with different focuses. CVACT Liu and Li (2019) aimed for a larger test set than CVUSA and included the region of Canberra, Aus- tralia, to test for cross-domain generalization. As an alterna- tive to ground-level imagery, University-1652 Zheng et al. (2020) introduced drone views of buildings to match them against overhead imagery. Unlike CVUSA and CVACT, which rely on center-aligned street-view images for match- ing to satellite imagery, VIGOR Zhu et al. (2021) uses a novel approach. This method allows multiple street view images to be matched to a single satellite image at different positions, allowing precise regression of the exact offset. None of the previously released datasets have specifically addressed cross-view geo-localization in disaster scenarios, which involve unique challenges such as destructed and altered environments.\nBy exploiting image similarities and differences, cross- view geolocation is characterized by contrastive learning. Vo and Hays (2016) pioneered soft-margin triplet loss and set a long-standing loss standard for this task. Further work intro- duced specialized aggregation methods like the NetVLAD layer Hu et al. (2018) or the SAFA-module Shi et al. (2019), enhancing the ability to capture and aggregate discrimina- tive features from cross-view images. Zhu et al. (2022) are the first to introduce the Transformer architecture in this domain and following work by Zhu et al. (2023), they utilized the MLP-Mixer architecture with further perfor- mance gains. Deuser et al. (2023) introduced hard negative sampling based on the geographical distance as well as feature similarity and showed superior performance. Fervers et al. (2023) enhanced this previous work with a second stage for re-ranking the results and improved overall retrieval performance."}, {"title": "3. Methdology", "content": ""}, {"title": "3.1. Task statement", "content": "Given a set of street-view imagery {L} and satellite imagery {La} with Ga refers to the geographical locations (e.g., longitude and latitude) of satellite imagery, our objec- tive to learn a cross-view embedding space Rcv (e.g., via a non-linear function f(Ls, La) \u2192 Rcy) in which two tasks are solved simultaneously: 1) each street-view imagery L is close to its corresponding satellite imagery La in the em- bedding space Rcy so that the correct geographical location can be retrieved based on their similarities in the embedding space; 2) each pair of street-view and satellite imagery {Ls, La} is close to all other pairs where a similar level of damage perception are observed. Figure 2 shows how we achieve this objective by integrating two GeoAI models (i.e., CVDisaster-Geoloc and CVDisaster-Est), namely the disaster perception estimation model and the cross-view geolocalization model, into a single framework CVDisaster. In the rest of the section, we will elaborate on the detailed design specifics and model choice."}, {"title": "3.2. Cross-view Geolocalization via Contrastive Learning", "content": "In this paper, we formulate CVDisaster-Geoloc, the task of cross-view geolocalization, as a imagery retrieval prob- lem, where an image encoder f() is a nonlinear function f(I\u2081,0) : [RH\u00d7W\u00d73 \u2192 RD, which is parameterized by 0 and maps the input image feature space (i.e., spatial dimension of H \u00d7 W with three RGB bands) into a vector embedding representation of D dimension. Herein, cross-view means that I\u2081 = {L, L} consisting of paired colocated SVI and satellite imagery, so that the corresponding geo-coordinates Ga from satellite imagery can be queried to use as the geographical coordinates of the input SVI. In this setting, two factors are of key importance for a good cross-view geolocalization model, which are the used image encoder f() and the vector embedding representation in the learned feature space RD."}, {"title": "3.2.1. Siamese Image Encoder with the modern ConvNeXt", "content": "To build a rock-solid image encoder for both SVI and satellite imagery, we follow the design in Deuser et al. (2023) by using a Siamese network that uses the modernized ConvNeXt as a backbone (Liu et al., 2022). Similar to the classic ResNet(He et al., 2016), ConvNeXt belongs to the Convolution Neural Network (CNN) family, which follows the classic sliding-window, fully convolutional paradigm, but brings in a list of modern neural architecture designs specificity for performance boosting, especially for high-resolution input, such as satellite imagery.\nThe key motivation for using ConvNeXt as the image encoder f() is actually intuitive: first, it keeps the simplic- ity and effectiveness of classic CNN then modernizes the ResNet step by step towards the modern Swin Transformer (Liu et al., 2021) style to ensure performance gain. Figure 3 shows the architecture of the 4-stage ConvNeXt network and highlights a comparison between ConvNeXt and ResNet blocks. Herein, it is necessary to notice the following modi- fication w.r.t a classic ResNet model.\nStage Compute Ratio: For classic ResNet, the compu- tation distribution across different stages are decided empir- ically. For example, ResNet50 is featured with a number of blocks distributed into four stages with a ratio of (3,4,6,3), which makes the convolution operation heavy already in an early stage. One change in Swin Transformer is to reduce the stage compute ratio to 1:1:9:1, which has been introduced to ConvNeXt as well. As a result, the number of blocks in ConvNeXt50 becomes (3,3,9,3).\nPatchify Layer: As natural images are inherently re- dundant, a common practice in the classic ResNet family is to use a stem cell for aggressively down-sampling. How- ever, ViT's patch encoder makes this even more aggressive by adopting a large kernel size and non-overlapping con- volution, namely the \"patchify\" layer. Similar designs are adopted in the new ConvNeXt with a 4 \u00d7 4 non-overlapped convolution layer to accommodate the network's multi-stage nature.\nInverted Boottleneck and Large Kerner: Following a similar idea in the Transformer block, the ConvNext block also uses an inverted bottleneck by keeping the dimension of the hidden layer four times of the input dimension. This idea has been proven to be beneficial in the popular MobileNetV2 (Sandler et al., 2018) and many more advanced CNN mod- els (Koonce and Koonce, 2021). Moreover, the ConvNeXt benefits from its larger kernel-sized convolution design, which brings a significantly better performance based on the Liu et al. (2022). As a prerequisite for a larger kernel, the depthwise convolution layer is placed prior to the dense convolutional layers as shown in the comparison of Figure3.\nMicro-scale Modification: The modification involves a list of micro-scale improvements, mostly related to the activation function and normalization layer. For instance, the ReLU used in ResNet is replaced by Gaussian Error Linear Unit (GELU) (Hendrycks and Gimpel, 2016), which is in fact a smoother variant of ReLU commonly used in modern transformer models. The Batch Normalization (BN) is re- placed by the simpler Layer Normalization (Ba et al., 2016) (LN). Furthermore, the downsampling layers are added only between two different stages which are also inspired by the design of Swin Transformers.\nBased on this modernized ConvNeXt backbone, we build a siamese network (Figure 3) as our image encoder f() for both SVI and satellite imagery by adapting the network input to different spatial dimensions. Noticeably, although the Siamese network is trained on cross-view imagery, the inference can handle a single input of SVI as a query base."}, {"title": "3.2.2. Contrastive Learning with Hard Negative Sampling", "content": "The key to cross-view geolocalization is how to train a siamese ConvNext so that one can obtain the desired vector embedding representation of I\u2081 = {L, L} in the learned feature space RD. Herein, we considered two factors to ensure efficient and effective representation learning in this cross-view setup: 1) contrastive pre-training on large-scale datasets and 2) fine-tuning with new cross-view imagery from the case study area.\nGiven the popularity of cross-view geolocalization, there are mainly three large-scale datasets, namely CVUSA (Work- man et al., 2015), CVACT (Liu and Li, 2019), and VIGOR (Zhu et al., 2021), which have been made available to the research community. Different in their data sizes, landscape, and sample density, these three datasets form a good basis for pre-training a cross-view geolocalization model to gain nice general-sense vector representations. In this context, we pre-trained the siameses ConvNeXt network on all three datasets (i.e., CVUSA, CVACT, and VIGOR) by using the contrastive learning objective.\nFollowing the \"cluster\" hypothesis that \"closely associ- ated documents tend to be relevant to the same requests\" (Voorhees, 1985), the most common approach of contrastive learning is to simultaneously minimize the distance between the embeddings of the anchor ta and the positive image tp while maximizing the distance to the negative sample tn. Therefore, a simple Triplet loss function looks like the following:\nLtriplet = [||f(ta)- f (tp)||2 - ||f(ta)-f(tn)||2+a]+ (1)\nHere, f() is the aforementioned image encoder (e.g., ConvNeXt) whose parameter e will be learned. To prevent the encoder from pushing the negative image without limita- tion, a rectifier term with margin m is introduced to keep the maximum distance between the anchor and negative smaller than m.\nCompared to the triplet loss, the InfoNCE (Oord et al., 2018; Radford et al., 2021) loss is often considered more ro- bust as it is able to make use of all available negative samples with the batch. Specifically the InforNCE, in a supervised learning setting, computes categorical cross-entropy loss to identify the positive sample amongst a set of negative samples (Weng, 2021). Given a context vector c, the positive sample is drawn from a conditional distribution p(xc), where (N-1) negative samples are drawn from the same distribution p(x) but without condition. In this context, the probability of correctly selecting the positive samples can be formulated as follows:\np(C = pos|X, c) = \\frac{f (Xpos, c)}{f(xpos, c) + \\sum_{j=1}^{N-1} f(x, c)} (2)\nHere, N is the total number of samples in a batch, and \\frac{p(x|c)}{f(x, c) x}{p(x)} is the similarity or scoring function between two samples.\nThen, the InforNCE loss tries to optimize the negative log probability of correcting selecting the positive samples, thus can be calculated as follows:\nLInfoNCE = -E[log p(C = pos|X, c)] (3)\nAlthough InforNCE has been intensively used in unsu- pervised and self-supervised representation learning (Mai et al., 2023; Vivanco Cepeda et al., 2024; Guo et al., 2024), it also offers a promising way for supervised representation learning in this cross-view setup. In this paper, we leverage the InforNCE as our contrastive learning loss in both the pre- training and fine-tuning stages for cross-view geolocaliza- tion. During the fine-tuning, we take the model weights pre- trained on CVUSA data given its relatively large size and geographical closeness, then fine-tune the model on the new cross-view imagery collected from the study area in Sanibel Island (Florida, USA) after the Hurricane IAN. To this end, we also compare the geolocalization performance with and without the fine-tuning stage in Section 4 as an ablation study."}, {"title": "3.3. Damage Perception Estimation with Cross-view Imagery", "content": "Herein, CVDisaster-Est, specifically the task of damage perception estimation, is tackled as a multi-class image classification problem. Similarly to the geolocalization task, we define an image encoder f() as a nonlinear function f(I\u2081,0) : RH\u00d7W\u00d73 \u2192 RB parameterized by \u03b8 and would map the input image feature space (again spatial dimension of H \u00d7 W with RGB three bands) into a vector embedding representation of B dimension, but following by a softmax classification layer. In this manner, we can use exactly the same cross-view imagery pairs (e.g., I; = {L, L}) to simultaneously estimate the damage perception level of the place when another model is trying to decide where SVI are collected.\nAlthough this is a straightforward model design, we argue that this can bring key advantages for CVDisaster against existing disaster mapping approaches (Li et al., 2020; Herfort et al., 2019; Hu et al., 2023b). On the one hand, the data preprocessing is synchronized with zero overhead for preparing two datasets for distinct tasks (i.e., geolocalization and disaster mapping). On the other hand, the cross-view imagery can provide a unique combination of observation angles and opportunities to fasten and automate the traditional post-disaster survey with inherent geolocation metadata immediately available during the survey. This can be extremely helpful in such a time-crucial application sce- nario.\nIn the rest of this section, we will elaborate on how we tackle the damage perception estimation task in CVDisaster using the modern GeoAI-based imagery classification ap- proach, specifically the couple GCViT model."}, {"title": "3.3.1. Coupled Global Context Vision Transformer", "content": "To tackle this cross-view image classification task, we develop a coupled GCViT model (CGCViT) as depicted in Figure 4) including two separate branches for SVI and satel- lite imagery, respectively. Unlike the siamese ConvNeXt, the design of CGCViT is driven by two special considerations: first, the appearance of disaster damages from two per- spectives (head-view and street-view) differs significantly, therefore, requires highly-distinct image encoders f () or sets of parameter 0; second, CGCViT can benefit from the com- plementary prediction capabilities learned from cross-view pairs at the same time. Moreover, the inference process also differs as the classification of damage perception level al- ways relies on both views while the geolocalization inference actually uses only SVI imagery to query an existing satellite database. This is also why there is a single weight-shared image encoder designed for the cross-view geolocalization task.\nAs a backbone network, the core idea of GCVit is to advocate short- and long-range spatial dependencies with a multi-resolution architecture where self-attention is still computed in local windows but can reach long-range patch via global tokens (Hatamizadeh et al., 2023). Given a cross- view imagery pair I\u2081 = {L, L } with the same dimension of RH\u00d7W\u00d73, the CGCViT consists of two branches of GCVIT following by a 2D average pooling layer and a softmax classifier. Each branch will include four stages of local and global self-attention modules similar to ConvNeXt(Liu et al., 2022) and Swin Transformer (Liu et al., 2021), but with an increasing number of channels and decreasing spa- tial resolutions, both by a factor of 2. Herein, the difference between local and global self-attention modules lies in the access to global queried features from the global query generator.\nGlobal Token Generator: As highlighted in Figure 4, the key advantage of GCViT comes from the fact that global attention is able to query long-range perception fields while keeping the local attention window unchanged. Herein, the global query token or so-called global self-attention can be pre-computed between each stage. Specifically, the global attention query G, starts with a matrix of size B\u00d7Cxhxw, where B, C, h \u00d7 w refers to batch size, channels, and spatial dimensional of the local window. In this way, the global query generator will repeat along batch dimension, and then be reshaped and added into multiple heads of local self-attention modules.\nGlobal Self-Attention: Based on the global query token, the global self-attention can be formulated as follows:\nGlobal_Attention(gq, k, v) = Softmax(\\frac{g_q k}{\\sqrt{s}} + p)v (4)\nwhere s, p refers to a scaling factor and a learnable relative position embedding vector. For instance, if the im- age patch position ranges from [-b + 1,b \u2013 1] then p will be generated based on spatial positions from a spatial grid of R(2b-1)\u00d7(2b\u22121). In this way, local self-attention has access to even long-range information from imagery regions outside of local windows, which provides an effective way of extending the reception field of self-attention without increasing the computation complexity.\nIn this paper, the CGCViT is able to extend this state-of- the-art ViT model into a dual branch setting and provide a rock-solid backbone for the cross-view damage classification task."}, {"title": "4. Experiment", "content": ""}, {"title": "4.1. Dataset overview", "content": "Hurricane IAN formed on September 23, 2022, causing severe storm surges and significant economic losses, making it one of the most devastating hurricanes in the history of Florida, USA. To this end, we have selected the renowned Sanibel Island and its surrounding area in southwest Florida, which was hit devastatingly by Hurricane IAN in 2022, as our case study area and created a novel cross-view dataset, namely CVIAN.\nVHR Satellite Imagery: VHR satellite imagery pro- vides extremely detailed overhead surface information, which is crucial for assessing disaster impacts, planning rescue op- erations, and formulating recovery strategies. For Hurricane IAN, the National Oceanic and Atmospheric Administration (NOAA) has collected relevant VHR satellite imagery. Each image is assembled into a mosaic distributed in tiles, with a ground sample distance of approximately 15 to 30 cm per pixel. In this study, we selected VHR imagery from September 30, 2022 from the NOAA open data portal\u00b9, and divided it into five subareas to support the assessment of Hurricane IAN's damage extent. These images provide a fine-grained head-view of the study area right after the hurricane, thus enhance our understanding of the impact of the disaster and aid in developing effective response and recovery measures.\nStreet-view Imagery : The street-view images of Hur- ricane IAN used in our study were collected from the open- source Mapilliry platform, specifically from a mapping cam- paign conducted by Site Tour 360 in our study area. These images were captured by Site Tour 360 after access was restored post-disaster. Site Tour 360 utilized Mapillary as an mapping tool. The Mapillary platform can rapidly and openly disseminate these high-resolution images, which is crucial for disaster response. This enables rescue organi- zations and the public to promptly access the latest post- disaster images, aiding in identifying areas in urgent need of assistance and efficiently allocating resources.\nFor downloading and filtering these street-view images, we adopted the ZenSVI 2 tool. ZenSVI can efficiently down- load, process, and analyze large-scale street-view image data, providing valuable data support for planning post-disaster recovery efforts. In total, we have processed and filtered in total 957,539 SVI records from Mapiliary using geographic extents (i.e., five subareas) and their timestamps (i.e., only after 28th September, 2022), out of which we have selected 1,135 and manually labelled them for the damage perception level with a group of GIS and disaster experts.\nThe detailed split of SVI and extent of VHR satellite imagery is listed in Table 1.\nDamage Perception Reference Data: Based on the aforementioned 1,135 SVI related to Hurricane IAN, we manually categorized them into three damage severity lev- els - light, medium, and heavy damages - based on a list of quantifiable and disaster-related indicators. Specifically, light damage images are characterized by a clean scene with no significant damage or only light damage, such as small areas of fallen trees or a few small road signs knocked down. Medium damage images are relatively cluttered and typically include larger or more extensive areas of fallen trees, as well as standing water around the trees. These images may also show more fallen road signs or road closure signs. Heavy damage images are very chaotic, featuring large or extensive areas of fallen trees, flooded roads, and housing trash. These indicators provide a extensible and subjective basis of disaster damage perception, which serves as the reference data for the subsequent cross-view imagery classification and validation.\nAs shown in Figure 6, we have elaborated some exem- plary SVI in our CVIAN dataset with light, medium, and heavy damage based on different damage indicators. Among them, (a) and (b) are classified based on the amount fallen trees. (c) and (d) are classified according to the amount of housing trash. (e) and (f) are classified based on the damage to road signs or destroyed buildings. (g) and (h) are classified according to the extent of standing water. This completes the damage perception reference data."}, {"title": "4.2. Experiment setup for Cross-View Geolocalization", "content": "In our experimental setup for CVDisaster-Geoloc, we employed a ConvNeXt-Base model, initialized using a pre- trained Sample4Geo model (Deuser et al., 2023). This pre- training on CVUSA allowed us to leverage a robust feature extraction as CVUSA features rural and urban environments. During pre-processing, we made sure that the street-view images were oriented north according to the CVUSA stan- dard. We also cropped the top and bottom of the images to reduce their size and eliminate irrelevant information, thereby streamlining the input data for more efficient pro- cessing. As a result, the cross-view imagery pairs are of size 512 x 1024 pixels and 512 \u00d7 512 pixels for SVI and satellite imagery, respectively. During training, we used the InfoNCE loss function with label smoothing set to 0.1. This regularization technique helped to mitigate overconfidence in the predictions, thus promoting better generalization.\nThe model fine-tuning process was performed over 10 epochs using the AdamW (Kingma and Ba, 2014) optimizer with an initial learning rate of 0.0001. We use"}]}