{"title": "High-Resolution Image Synthesis via Next-Token Prediction", "authors": ["Dengsheng Chen", "Jie Hu", "Tiezhu Yue", "Xiaoming Wei"], "abstract": "Denoising with a Joint-Embedding Predictive Architecture (D-JEPA), an autoregressive model, has demonstrated outstanding performance in class-conditional image generation. However, the application of next-token prediction in high-resolution text-to-image generation remains underexplored. In this paper, we introduce D-JEPA\u00b7T2I, an extension of D-JEPA incorporating flow matching loss, designed to enable data-efficient continuous resolution learning. D-JEPA T2I leverages a multimodal visual transformer to effectively integrate textual and visual features and adopts Visual Rotary Positional Embedding (VoPE) to facilitate continuous resolution learning. Furthermore, we devise a data feedback mechanism that significantly enhances data utilization efficiency. For the first time, we achieve state-of-the-art high-resolution image synthesis via next-token prediction.", "sections": [{"title": "1. Introduction", "content": "In recent years, diffusion models have become the standard method for generating high-resolution images and videos from natural language inputs, demonstrating remarkable generalization capabilities [12-14, 28, 31, 35, 36, 41, 48, 54, 66, 70, 83, 90, 93, 95, 98, 100, 103, 106, 136].\nSimultaneously, the advent of autoregressive large language models [1, 4, 5, 16, 26, 51, 60, 78, 85, 86, 88, 110, 113, 118, 119, 127, 128] has ushered in a new era in artificial intelligence, marked by substantial strides towards general artificial intelligence due to their exceptional generality and versatility.\nThe transformative success of language models has also spurred advancements in image generation [68, 89, 109, 129, 130]. Recent studies, such as D-JEPA [20], MAR [63], and VAR [116], suggest that the generative capabilities of autoregressive models can rival or even surpass those of diffusion models in class-conditioned generation tasks on ImageNet [94]. Despite these achievements in natural language processing, autoregressive models still fall short in computer vision, particularly in high-resolution text-to-image synthesis [37, 69], compared to diffusion models. As noted by Kilian et al., current autoregressive models underperform in generating high-resolution images, specifically in terms of image texture and overall quality, although they exhibit superior prompt following and throughput.\nTo improve autoregressive models for image generation and foster the development of unified multi-modal models, we focus on two primary aspects: model architecture and data utilization.\nFrom a model architecture perspective, several issues need resolution. First, the potential of representational learning to enhance model performance has been largely overlooked, as indicated in [20, 133]. Second, autoregressive models have been isolated from other generative models, notably diffusion models, hindering the adoption of successful methods from diverse domains [63]. Third, the patchify operation, which divides continuous images into discrete blocks identified via RoPE [107], impedes the modeling of continuous resolutions and aspect ratios. To address these challenges, we devise D-JEPA\u00b7T2I, which employs a multimodal visual transformer block and introduces a more adaptable flow matching loss. Additionally, we propose visual rotary positional embedding (VoPE) to enable continuous-resolution and dynamic-aspect learning.\nRegarding data utilization, we introduce a data feedback mechanism to leverage resources more effectively, as shown in Fig. 3. Traditional data curation focuses on preprocessing tasks, such as filtering low-quality images [45] and optimizing prompts using multimodal models [64]. Although these methods enhance data quality, they often introduce inherent biases and fail to address evolving data distributions during training, especially with billion-scale datasets. Model fine-tuning strategies like reinforcement learning from human feedback [65, 125] and direct preference optimization [87, 121] act as remedial measures with inconsistent effectiveness. In contrast, our data feedback mechanism continuously adjusts the data distribution based on real-time"}, {"title": "2. Method", "content": "2.1. D-JEPA T2I for Text-to-Image Synthesis\nD-JEPA, originally proposed by Chen et al. [20], is based on a visual transformer structure [34] that models the token distribution p(xi|zi) using feature prediction loss Lpred and diffusion loss Ldiff, where zi represents the predicted features of each token, respectively. D-JEPA has demonstrated significant advantages in class-conditioned image generation on ImageNet. However, the original D-JEPA architecture is limited to generating fixed-scale resolution images (typically 256 \u00d7 256 or 512 \u00d7 512) conditioned solely on class labels. In this work, we introduce D-JEPA T2I to extend its capabilities to high-resolution text-to-image synthesis.\nTo achieve this, we have adapted a multimodal visual transformer (Fig. 2b) that more effectively integrates textual and visual features. Additionally, we replace the diffusion loss Ldiff used in D-JEPA with a more flexible and fasterconverging flow matching loss Lflow. These enhancements enable D-JEPA T21 to perform more effectively in high-"}, {"title": "Multimodal Visual Transformer.", "content": "The multimodal visual transformer draws inspiration from the design of the multimodal diffusion backbone, initially proposed by Esser et al. [36]. The core idea is that text and image embeddings are conceptually quite different, necessitating the use of two separate sets of weights for the two modalities. As shown in Fig. 2b, this approach is equivalent to having two independent transformers for each modality but joining the sequences of the two modalities for the attention operation, allowing both representations to operate within their own spaces while still accounting for the other. Dehghani et al. [30] observe that the training of large vision transformer models diverges because the attention entropy grows uncontrollably. To avoid this, they propose normalizing Q and K before the attention operation. We follow this approach and use RMSNorm [134] with a learnable scale in both streams of our D-JEPA T2I architecture, similar to Esser et al. [36]. The additional normalization prevents attention logit growth instability, confirming findings by previous works[30, 36, 123], and enables efficient training at bf16mixed precision [19] when combined with the AdamW [72] optimizer.\nThe primary distinction between the multimodal visual transformer and the multimodal diffusion backbone is that the former does not require handling the additional timestep t introduced by the diffusion process. This omission sidesteps adaptive layer norm [82], which is essential for diffusion models built on top of DiT [80]. For textual tokens, we follow the design principles of large language models and use ROPE [107] as positional encoding. For visual tokens, we introduce VoPE, a positional encoding specifically designed for visual features, which we detail in Para. 2.2."}, {"title": "Flow Matching Loss.", "content": "Flow models [66, 74, 76] are generally used for generating a complete image. In this context, we adhere to the flow matching formulation presented in Gao et al. [41] for modeling the token distribution p(xi zi).\nFlow matching [3, 66] emerges as a simple alternative that linearly interpolates between noise and data along"}, {"title": "~~", "content": "a straight line. More specifically, given the data Xi ~ p(xi zi) and Gaussian noise e ~ N(0,I), we define an interpolation-based forward process:\nAtxi + \u03b2te,\nwhere ao = 0, \u03b2\u2081 = 1, \u03b1\u2081 = 1, and \u03b2\u2081 = 0. This interpolation for t \u2208 [0,1] bridges x = \u20ac and x = xi. Similar to the diffusion schedule, this interpolation schedule offers flexible choices of at and \u1e9et. In our framework, we adopt a linear interpolation schedule between noise and data for its simplicity: x = txi + (1 - t)e. This formulation represents a uniform transformation with constant velocity between the data and noise. The corresponding timedependent velocity field is defined as:\nvt(x, zi) = &txi + \u03b2te = xi \u2013 \u20ac,\nwhere & and B denote the time derivatives of a and \u03b2. This time-dependent velocity field v : [0, 1] \u00d7 Rd \u2192 Rd defines an ordinary differential equation known as the Flow ODE:\ndxi = vt(x, zi)dt.\nWe use t(xi, zi) to represent the solution of the Flow ODE with the initial condition Yo(Xi, Zi) = Xi. By solving this Flow ODE from t = 0 tot = 1, we transform noise into data samples using the approximated velocity fields vo(x, t, zi). Similar to the approaches of Chen et al. [20] and Li et al. [63], ve is implemented with a small denoising MLP [63].\nDuring training, the flow matching objective directly regresses to the target velocity for each token:\nLflow (Xi, Zi) = \u222b01||vo(xi, t, zi) \u2013 (Xi \u2013 e) ||2 dt,  (1)\nwhich is termed the conditional flow matching loss, sharing similarities with the noise prediction or score prediction losses in diffusion models [66]."}, {"title": "High-resolution Image Sampling.", "content": "For evaluating generative models in generalized next-token prediction, we employ an iterative sampling strategy similar to those used in Chang et al. [17], Li et al. [63], as outlined in Algo. 1. This strategy gradually decreases the masking ratio from 1.0 to 0.0 following a cosine schedule, typically using 64 autoregressive steps for sampling an image with a resolution of 256 \u00d7 256. Empirically, we find that even for higherresolution images (such as 2K or 4K), satisfactory sampling quality can be achieved within approximately 100 autoregressive steps. D-JEPA T2I follows the approach of Chen et al. [20], utilizing fully randomized orderings to determine the next set of tokens to predict. This design effectively enhances the diversity of the generated samples."}, {"title": "2.2. VoPE for Continuous Resolution Learning", "content": "Diffusion models learn the distribution of entire images, while autoregressive models focus on learning the distribution for each masked token. Empirically, autoregressive models face challenges when generating images with arbitrary resolutions and aspect ratios, mainly due to the absence of appropriate visual positional embeddings.\nBoth sinusoidal positional encoding and rotary positional embedding have limitations: the former cannot ensure positional consistency when images are cropped or padded, and neither can maintain positional information consistency across different scales of the same image. Consequently, operations such as cropping, padding, or scaling an image can lead to models receiving completely differ-"}, {"title": "~~", "content": "manner:\n<fq(xm, (m+b)), fk(Xn, =(n+b))) = g(xm, xn, =(m-n)).\n\u03c1\nThe aim is to determine an equivalent encoding mechanism for fq(xm, (m + b)) and fk (xn, =(n + b)) that satisfies the above relation.\nReferring to Su et al. [107], the functions f and g that meet the relationship can be defined as follows when the feature dimension d = 2:\nfq(xm, =(m + b)) = (Wqm)ei[\u03c9\u03c1(m+b)]\u03b8\nfk(xn, =(n + b)) = (Wkxn)ei[\u03c9\u03c1(n+b)]\u03b8\ng(xm, xn, (m - n)) = Re[(Wqm) (Wkxn)*ei[\u03c9\u03c1(m-n)]\u03b8],\n= 2(j-1)\nwhere Re[] denotes the real part, and (Wkxn)* is the complex conjugate of (Wkxn). The constant \u2208 R is preset non-zero. When d > 2, \u03b8 = [1,2,...,d/2], where w is a preset base frequency. For larger d, the derivations and expressions of fq(xm, (m + b)) and fk (xn, (n + b)) are consistent with RoPE [107] and are not reiterated here."}, {"title": "Comparison between VoPE and RoPE.", "content": "During sampling, RoPE requires adjusting the base frequency w for higher resolution images. For instance, with a 256 token training length and a target of 512 tokens, the NTKAware Scaled RoPE approach [81] is used, where w' = . Although effective for long texts, this causes discrepancies in positional information between training and sampling phases, as shown in Fig. 4a. These discrepancies are detrimental for image generation, which is sensitive to token boundary information. Gao et al. [41] noted that this method leads to blurry, repetitive images in higher resolution image generation, akin to issues in positional interpolation or extrapolation. Thus, RoPE is unsuitable for arbitrary resolution image generation, especially at higher resolutions.\nIn contrast, VoPE ensures that pixel normalization maintains consistent positional information across resolutions during training and sampling. This is achieved by normalizing images to ag \u00d7 g grid via p, without changing the base frequency w, regardless of resolution. As shown in Fig. 4b, images at different resolutions with VoPE use the same relative positional curve at varying resolution densities p. Notably, at p = 1/4, the curve closely matches that of p = 1.0, implying that to generate 4096 \u00d7 4096 images, training at 1024 \u00d7 1024 with p = 1/4 suffices."}, {"title": "2.3. Data Feedback for Efficient Data Utilization", "content": "While Esser et al. [36] and Li et al. [64] meticulously curate high-quality training data, they lack an in-depth examination of the actual sampled training data. It is worth noting that the curated training data does not always match the sampled training data and does not guarantee an optimal text-to-image model due to inherent data biases in natural distributions and the potential loss of long-tail data due to sampling."}, {"title": "Data Bias in Natural Distribution.", "content": "The distribution of training data obtained through curation rules typically presents significant biases. As illustrated in Fig. 5, since a large volume of training data is often sourced from publicly available internet data, the overall distribution, even post-curation, shows strong concentration regarding resolution and semantic notions. This can result in redundant data, thereby reducing the model's overall performance."}, {"title": "Under-sampling of Long-tailed Data.", "content": "Directly training with vast datasets (usually ranging from tens of millions to billions of image-text pairs) can potentially lead to the loss of long-tail data. In small-scale training, such as with ImageNet, the entire dataset is extensively traversed (e.g., thousands of iterations in Peebles and Xie [80]), providing the model ample opportunity to learn the distribution of all training data. However, when the dataset scales up to billions, each data point might only be traversed a few times, and often the entire dataset might not be fully traversed even once. Resuming training typically recovers prior model parameters and optimizer states but struggles to track previously trained data indices, exacerbating the loss of long-tail data."}, {"title": "Data Feedback.", "content": "To address these challenges, we propose a data feedback mechanism that dynamically adjusts the data distribution during training. The training pipeline is depicted in Fig. 3. We perform statistical analysis on each batch of sampled data, examining features such as resolution, prompt composition, and style tags, and feed these features back into the sampling process for subsequent iterations in real-time. Furthermore, we conduct comprehensive performance evaluations on saved model checkpoints to promptly adjust subsequent data sampling strategies.\nWhen sampling data, we introduce two distinct strategies to handle transformable and non-transformable attributes. For transformable attributes such as image resolution and aspect ratio, which can be adjusted without quality degradation, we utilize a truncated normal distribution sampling strategy, denoted as trunc_norm(\u03bc, \u03c3, \u03b1, b). We initially sample the desired data parameters, such as the target resolution. If the current data can be transformed to align with these specified parameters, we proceed with the transformed data; otherwise, we discard this sample and attempt resampling. In contrast, for non-transformable attributes, we decide whether to use the current data based on a predetermined sampling frequency specific to each attribute. If the data exceeds this frequency, there is a 50% probability it will be discarded. Although this approach may not strictly adhere to the exact predetermined sampling frequencies, it significantly enhances data utilization, particularly beneficial when dealing with the sampling of long-tail distributions."}, {"title": "3. Experiments", "content": "In this section, we succinctly outline the experimental configuration and training process of D-JEPA T2I. We evaluate the model performance through automated metrics, qual-"}, {"title": "3.1. Experiment Setup", "content": "Dataset. We employ an internally curated dataset comprising over 1 billion image-text pairs for training. Each image has a minimum shorter side length of 512 pixels. To maintain high quality, images with aesthetic scores below 5.0 are excluded using the LAION-AI aesthetic predictor2. Additionally, OCR tools filter out images containing text, which constrains the model from generating text but enhances its learning of other real-world concepts. English captions are generated for each image using InternVL2 [24, 25], and enhanced with image tags (e.g., style tags, data source).\nSynthetic datasets, like JourneyDB [108], are carefully incorporated despite their efficacy in accelerating convergence [21, 41]. We hypothesize that over-dependence on synthetic data might limit the model's output diversity, reducing data utility. Thus, synthetic data constitutes only about 5% of the dataset, ensuring D-JEPA\u00b7T2I generates authentic content, albeit with slower convergence.\nTraining. The D-JEPA-T2I model is an extension of D-JEPA-H [20] with 2.6 billion parameters. Training is strategically split into two phases for optimal performance.\nInitially, the model is trained on fixed-resolution images of 256 \u00d7 256 pixels using a batch size of 2048 across 1 million steps. This phase is crucial for enhancing textual concept comprehension. A cosine annealing schedule adjusts the learning rate from 1 \u00d7 10-5 to 1 \u00d7 10-6, with a warm-up of 100k steps. The second"}, {"title": "Inference.", "content": "The D-JEPA T2I model can generate images at arbitrary resolutions and aspect ratios. For 256 \u00d7 256 resolution images used in quantitative evaluation, we set autoregressive steps to T = 64. For higher resolutions, autoregressive steps are empirically tuned. Classifier-free guidance [46] enhances image quality, with hyperparameters optimized per benchmark. Denosing MLP diffusion steps are consistently set to 250 across tasks."}, {"title": "3.2. Automated Metric Evaluation", "content": "We evaluate D-JEPA-T2I using automated metrics on prominent text-to-image benchmarks, including GenEval [42], T2I-CompBench [52], and GenAI-Bench [62], which assess the model's capacity to generate prompt-reflective images. Table 2 compares D-JEPA T2I against state-of-the-art diffusion and autoregressive models, both open-source and closed-source. The number of model parameters significantly influences performance; thus, comparisons among similar-scale models offer practical insights.\nPer GenEval overall scores in Tab. 2, D-JEPA T2I surpasses other models within small and mainstream sizes."}, {"title": "3.3. Qualitative Results", "content": "Fig. 1 illustrates the versatile capabilities of images generated by D-JEPA T2I. The model supports flexible resolutions and aspect ratios and can adeptly handle various styles. Supplementary qualitative results are available, demonstrating that D-JEPA\u00b7T2I-generated images align more closely with human preferences in both texture quality and semantic adherence."}, {"title": "3.4. Human Ratings", "content": "We selected 532 challenging and representative prompts from GenEval [42], T2I-CompBench [52], PickScore [58], and Parti-prompts [130] to construct a comprehensive hu-"}]}