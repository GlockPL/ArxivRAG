{"title": "Solving the Cold Start Problem on One's Own as an End User via Preference Transfer", "authors": ["Ryoma Sato"], "abstract": "We propose a new approach that enables end users to directly solve the cold start problem by themselves. The cold start problem is a common issue in recommender systems, and many methods have been proposed to address the problem on the service provider's side. However, when the service provider does not take action, users are left with poor recommendations and no means to improve their experience. We propose an algorithm, PRETENDER, that allows end users to proactively solve the cold start problem on their own. PRETENDER does not require any special support from the service provider and can be deployed independently by users. We formulate the problem as minimizing the distance between the source and target distributions and optimize item selection from the target service accordingly. Furthermore, we establish theoretical guarantees for PRETENDER based on a discrete quadrature problem. We conduct experiments on real-world datasets to demonstrate the effectiveness of PRETENDER.", "sections": [{"title": "1 Introduction", "content": "Recommender systems have become an essential component of many online services, such as e-commerce [28, 31], social media [9, 59], and video streaming platforms [3, 15, 52]. These systems analyze user preferences and recommend items that the user may like. A common problem in recommender systems is the cold start problem, where the system cannot provide good recommendations for new users due to a lack of historical data. Many methods have been proposed to address the cold start problem, such as content-based filtering [24, 34, 49], utilizing side information [27, 37, 60], meta learning [25, 29, 53], and active learning [10, 61].\nHowever, all of these methods are designed to be implemented on the service provider's side, and require the service provider to take action. When the service provider does not take action, e.g., due to lack of resources, lack of incentives, or simple negligence, users are left with poor recommendations and no immediate way to improve their experience. This can be particularly frustrating for users who are eager to adopt a new service but struggle to discover relevant items due to the cold start problem.\nIn this paper, we propose a new problem setting that enables users to address the cold start problem on their own. We consider a scenario where a user has been using a source service (e.g., Netflix) for a long time and has a history of preferences for items. The user has just started using a target service (e.g., Hulu) but lacks any preference history on that platform. The user receives good recommendations from the source service but poor recommendations from the target service due to the cold start problem. The user wants to transfer the preferences from the source service to the target service so that the user can enjoy good recommendations from the target service as well. If the target service offers a built-in functionality to import preferences, this problem can be easily solved. However, in many cases, such functionality is not provided, leaving the user with no direct means to improve their recommendations. To address this, we propose an algorithm, PRETENDER, that enables users to overcome the cold start problem independently, even when neither the source nor the target service provides dedicated support for preference transfer.\nWe formulate the problem as minimizing the distance between the source and target distributions and optimize the selection of items from the target service accordingly. A key strength of our method is that it provides strong theoretical guarantees even when the way the target service uses the data is unknown."}, {"title": "2 Problem Setting", "content": "Suppose we are an end user of the source service (e.g., Netflix) and the target service (e.g., Hulu). We have been using the source service for a long time and have a history of preferences for items (e.g., thumbs-up or thumbs-down for videos). We have just started using the target service and lack any preference history on that platform. We receive poor recommendations from the target service due to the cold start problem. Our goal is to transfer our preferences from the source service to the target service so that we can enjoy good recommendations from the target service as well. The problem is formally defined as follows:\n\nInput: Sets of source items $I_s$ and target items $I_T$. Features $x$ of items $i \\in I_s \\cup I_T$. A set $D_s = \\{(i, y_i)\\} \\subset I_s \\times \\{0,1\\}$ of user preferences on source items. A positive integer $K \\in \\mathbb{Z}_+$ representing the number of target items to interact with.\nOutput: A set $D_T = \\{(i,y_i)\\} \\subset I_T \\times \\{0,1\\}$ with $|D_T| = K$ such that clicking items following $D_T$ results in good recommendations from the target service.\n\nA method outputs a set of preferences for the target items, denoted as $D_T = \\{(i,y_i)\\}$. Following this output, the end user clicks thumbs up for each item $i \\in \\{i \\in I_T | (i, 1) \\in D_T\\}$ and thumbs down for each item $i \\in \\{i \\in I_T | (i,0) \\in D_T\\}$. A good method should output $D_T$ such that the user can enjoy good recommendations from the target service after this process. Note that the clicking process (and thus the entire process) may be automated by a Web agent.\nThis problem presents three main challenges:\nItems are not shared between services. A naive approach might simply \"copy\" the history to the target service, but this approach fails because the corresponding items may not exist in the target service, i.e., $I_s \\neq I_T$.\nClicking many items is tedious. The user may not want to interact with many items just for preference transfer, i.e., $K$ is small. Even when the clicking process is automated, too large $K$ may take a long time and/or impose a heavy load on the target service.\nThe target service's use of the data is unknown. The target service may process preference data $D_T$ differently from the source service or in an unexpected manner, making it difficult to predict how the transferred preferences will influence recommendations."}, {"title": "3 Pretender", "content": "We propose PRETENDER (PREference Transfer by END usERs) to solve the preference transfer problem. We first describe the gerneral framework and then provide variants for specific settings."}, {"title": "3.1 Formulating the Problem as Distance Minimization", "content": "The goal of PRETENDER is to select items from the target service such that its empirical distribution $\\mu_T^{D_T}$ is close to the source distribution $\\mu_s$, which are defined as\n$$\n\\mu_T^{D_T} = \\frac{1}{K} \\sum_{(i,y_i) \\in D_T} \\delta_{(x_i, y_i)},\\ \\ \\mu_s = \\frac{1}{|D_s|} \\sum_{(i,y_i) \\in D_s} \\delta_{(x_i, y_i)} \n$$\nwhere $\\delta_x$ denotes the Dirac measure at $x$. We use the integral probability metric (IPM) [35, 51] to quantify the discrepancy between the distributions, which is defined for a function class $\\mathcal{F}$ as\n$$\nIPM_{\\mathcal{F}} (\\mu, \\nu) = \\sup_{f \\in \\mathcal{F}} \\Big[ \\int f d\\mu - \\int f d\\nu \\Big].\n$$\nWhen $\\mathcal{F}$ is the class of functions with a reproducing kernel hilbert space (RKHS) norm at most 1, the IPM is equivalent to the maximum mean discrepancy (MMD) [16], and when $\\mathcal{F}$ is the class of 1-Lipschitz functions, the IPM corresponds to the 1-Wasserstein distance [39, 54]. PRETENDER selects items such that $IPM_{\\mathcal{F}}(\\mu_T, \\mu_s)$ is small, ensuring that the target distribution closely aligns with the source distribution.\nThis formulation addresses the third challenge. Suppose the target service uses an unknown model $f_T(\\cdot; \\theta)$ and unknown loss function $l_y(\\cdot,\\cdot)$ to train the model. We only know that the loss $l_y(f_T(x;\\theta), y)$ is L-Lipschitz in $(x, y)$. Then, the model incurs the following loss on the source preferences:\n$$\n\\begin{aligned}\n(\\text{loss on the source data}) &= \\frac{1}{|D_s|} \\sum_{(i,y_i) \\in D_s} l_y(f_T(x_i; \\theta), y_i) \\\\\n&= \\int l_y(f_T(x; \\theta), y) d\\mu_s(x, y) \\\\\n&\\leq \\int l_y(f_T(x; \\theta), y) d\\mu_T(x, y) + L \\cdot W_1(\\mu_T, \\mu_s) \\\\\n&= (\\text{training loss on the target data}) + L \\cdot W_1(\\mu_T, \\mu_s)\n\\end{aligned}\n$$\nwhere $W_1$ denotes the 1-Wasserstein distance. The inequality follows from the definition of IPM (Eq. (2)). Therefore, if we minimize the Wasserstein distance $W_1(\\mu_T, \\mu_s)$ between the source and target distributions, and the target service effectively minimizes the training loss on the target data, the model trained on the target preferences will also generalize well to the source preferences, thereby accurately reflecting the user's preferences\nThe crux of this approach is that its guarantee is agnostic to the model, loss function, and the training method the target service employs, which are typically not known to the user. Regardless of how the target service uses the data, the user can ensure that the recommendation model trained on the target preferences reflects the source preferences as long as the distributions are close.\nThe use of IPM also addresses the first challenge. IPM exploits the geometry of the data space through the smoothnessof the function class $\\mathcal{F}$ (e.g., Lipschitz continuity and a small RKHS norm) and can be used even when the items are not shared between the services. This is in a stark contrast to other discrepancy measures such as KL divergence and Hellinger distance.\nAlthough we have formulated the problem as distance minimization between distributions, this problem remains challenging because selection of items is combinatorial. This is in contrast to other minimization problems, where optimization is performed over weights and/or coordinates of data points, making the problem continuous and often convex. However, end users cannot \"thumbs up 0.2 points\" or alter the features of the items in the service, meaning that we cannot sidesitep the combinatorial nature of the problem. We address this challenge in the following."}, {"title": "3.2 Optimization", "content": "The optimization framework of PRETENDER is as follows:\n\nPrepare the set of labeled target items $J_T = \\{(i, y) | i \\in I_T, y \\in \\{0,1\\}\\}$. We rearrange the items such that they are indexed by 1, 2, . . ., 2m and redefine $J_T = \\{(i_j, y_j) | j \\in [2m]\\}$, where m = $|I_T|$ is the number of items in the target service.\nOptimize the weights $w \\in [0, \\frac{1}{K}]^{2m} \\cap \\triangle^{2m}$ of the items in the target service such that the weighted empirical distribution $\\mu_w = \\sum_{j=1}^{2m} w_j \\delta_{(x_i, y_i)}$ is close to the source distribution $\\mu_s$, where $\\triangle_a$ denotes the (d \u2013 1)-dimensional probability simplex. This is achieved by solving the following optimization problem:\n$$\\begin{array}{cl}\n\\min_{w \\in \\mathbb{R}^{2m}} & D\\Big(\\sum\\limits_{j=1}^{2m} w_j \\delta_{(x_{i_j}, y_{i_j})}, \\mu_s\\Big) \\\\\n\\text{s.t.} & \\sum\\limits_{j=1}^{2m} w_j = 1, \\ \\ 0 \\le w_j \\le \\frac{1}{K} (j = 1, 2, ..., 2m).\n\\end{array}$$\nSample items according to the optimized weights. For each $j \\in [2m]$, sample $I_j \\sim Bernoulli(Kw_j)$ for $j \\in [2m]$, and define the selected set as $D_T = \\{(i_j, y_j) | I_j = 1\\}$.\nIf $|D_T| < K$, greedily insert additional items, and if $|D_T| > K$, greedily remove items to obtain the final preference set $D_T$ satisfying $|D_T| = K.\n\nWe construct the set of labeled target items as $J_T = \\{(i_j, y_i) | j \\in [2m]\\}$. Since the user can choose the label for each item (e.g., thumbs up or thumbs down), we include both possible labels (i, 0) and (i, 1) in the candidate set, making the total number of items 2m.\nWe first solve the continuous relaxation of the problem. This problem is continuous and convex for MMD and the Wasserstein distance. Therefore, we can employ standard methods such as the Frank-Wolfe algorithm [22] or the projected subgradient descent algorithm [4] to solve the problem. By standard results in convex optimization, we can obtain w such that\n$$\nD(\\mu_w, \\mu_s) \\le OPT^{continuous} + \\epsilon \\\\\n= \\min_{w \\in [0,\\frac{1}{K}]^{2m} \\cap \\triangle^{2m}} D(\\mu_w, \\mu_s) + \\epsilon \\\\\n\\le \\min_{D_T=\\{(i,y_i)\\}: |D_T|=K} D\\Big(\\frac{1}{K} \\sum_{(i,y_i) \\in D_T} \\delta_{(x_{i}, y_{i})}, \\mu_s\\Big) + \\epsilon \\\\\n= OPT^{combinatorial} + \\epsilon,\n$$\nwhere (a) follows because the weight (1/K,..., 1/K) of the empirical meassure is in the feasible set. Therefore, the continuous solution is at least as good as the combinatorial solution. However, we need to carefully round the solution to obtain the final output to ensure that the rounding process does not degrade the quality of the solution much. This is the main challenge from a theoretical perspective.\nWe employ a randomized approach. We first point out that the distribution $Bernoulli(Kw_j)$ is well-defined as we set the optimization domain to $w \\in [0, \\frac{1}{K}]^{2m} \\cap \\triangle^{2m}$ and $0 \\le Kw_j \\le 1$ holds. Let $\\tilde{w} \\in \\{0, \\frac{1}{K}\\}^{2m}$ be the sample weights after the random selection, i.e., $\\tilde{w}_j = \\frac{I_j}{K}$. Then,\n$$\nE[\\tilde{w}_j] = \\frac{1}{K} E[I_j] = \\frac{1}{K} Kw_j = w_j,\n$$\nwhere (a) follows from $I_j \\sim Bernoulli(Kw_j)$. Therefore, the expected weight is the same as the continuous solution. This implies that $D(\\mu_{\\tilde{w}}, \\mu_s)$ distributes around that of the continuous solution $D(\\mu_w, \\mu_s) \\approx OPT^{continuous}$. Next, we analyze the number of selected items, which is given by, in expectation,\n$$\n\\mathbb{E}[\\sum_i I_i] = \\sum_i K w_i = K,\n$$"}, {"title": "3.3 Pretender for MMD", "content": "We consider the case where we quantify the discrepancy between the distributions with MMD, defined as\n$$\nMMD(\\mu, \\nu) = \\sup_{\\|f\\|_{\\mathcal{H}} \\le 1} \\int f d\\mu - \\int f d\\nu,\n$$\nwhere $\\mathcal{H}$ is the RKHS with the kernel $k$. Equivalently, the MMD can be expressed as\n$$\n\\begin{aligned}\nMMD(\\mu, \\nu)^2 &= ||\\mathbb{E}_{x \\sim \\mu}[\\phi(x)] - \\mathbb{E}_{x \\sim \\nu}[\\phi(x)]||_{\\mathcal{H}}^2 \\\\\n&= \\mathbb{E}_{x,x' \\sim \\mu}[k(x,x')] - 2 \\mathbb{E}_{x \\sim \\mu, x' \\sim \\nu}[k(x,x')] + \\mathbb{E}_{x,x' \\sim \\nu}[k(x, x')],\n\\end{aligned}\n$$\nwhere $\\phi(x) = k(x,\\cdot)$ is the feature map. Now, consider the empirical distributions\n$$\n\\mu_T = \\sum_{j=1}^{2m} w_j \\delta_{x_j}, \\ \\ \\nu = \\frac{1}{n} \\sum_{j=1}^{n} \\delta_{x'_j},\n$$\nwhere $x_j = (x_{i_j}, y_j)$ is concatenation of the feature and label. Substituting these into Eq. (17), we obtain\n$$\\begin{aligned}\nMMD(\\mu, \\nu)^2 &= \\sum_{j=1}^{2m} \\sum_{j'=1}^{2m} w_j w_{j'}k (x_j, x_{j'}) - 2\\sum_{j=1}^{2m} \\sum_{j'=1}^{n} w_j \\frac{1}{n}k(x_j, x'_{j'}) + \\frac{1}{n^2} \\sum_{j,j'=1}^{n} k(x'_j,x'_{j'}) \\\\\n&= w^T K^{TT} w - \\frac{2}{n} w^T K^{ST} \\mathbf{1} + \\frac{1}{n^2} \\mathbf{1}^T K^{SS} \\mathbf{1} \\\\\n&= w^T K^{TT} w - \\frac{2}{n} w^T K^{ST} \\mathbf{1} + \\text{const.},\n\\end{aligned}$$\nwhere $K^{TT} \\in \\mathbb{R}^{2m \\times 2m}$, $K^{ST} \\in \\mathbb{R}^{2m \\times n}$, and $K^{SS} \\in \\mathbb{R}^{n \\times n}$ are the kernel matrices with $K^{TT} = k(x_j, x_{j'})$, $K^{ST} = k(x_j, x'_{j'})$, and $K^{SS} = k(x'_j, x'_{j'})$. These matrices are positive semi-definite, and $MMD^2$ is convex in $w$. Thus, the optimization problem to minimize MMD reduces to the following convex quadratic program:\n$$\\begin{array}{cl}\n\\min_{w \\in \\mathbb{R}^{2m}} & w^T K^{TT} w - \\frac{2}{n} \\mathbf{1}^T K^{ST} w, \\\\\n\\text{s.t.} & \\sum\\limits_{j=1}^{2m} w_j = 1, \\ \\ 0 \\le w_j \\le \\frac{1}{K} (j = 1, 2, ..., 2m).\n\\end{array}$$"}, {"title": "3.4 Pretender for the Wasserstein Distance", "content": "We now consider the case where we quantify the discrepancy between the distributions with the 1-Wasserstein distance. Consider the empirical distributions $\\mu_T$ and $\\mu_s$ defined in Eq. 18. The 1-Wasserstein distance is\n$$\nW_1(\\mu^w, \\nu) = \\inf_{\\gamma \\in \\Pi(\\mu^w,\\nu)} \\sum_{j,j'} \\gamma_{j,j'} ||x_j - x'_j||\n$$\nwhere\n$$\n\\Pi(\\mu^w, \\nu) = \\Big{\\gamma \\in \\mathbb{R}^{2m \\times n} \\Big| \\sum_j \\gamma_{j,j'} = w, \\ \\sum_{j'} \\gamma_{j,j'} = \\frac{1}{n} \\Big\\}\n$$\nis the set of coupling matrices. The 1-Wasserstein distance also admits the following dual formulation:\n$$\nW_1(\\mu^w, \\nu) = \\sup_{f \\in \\mathcal{F}_{Lip}} \\sum_{j=1}^{2m} w_j f (x_j) - \\sum_{j'=1}^{n} \\frac{1}{n} f (x'_{j'}),\n$$\nwhere $\\mathcal{F}_{Lip}$ is the set of 1-Lipschitz functions. We solve the following convex optimization problem:\n$$\\begin{array}{cl}\n\\min_{w \\in \\mathbb{R}^{2m}} & W_1(\\mu^w, \\nu), \\\\\n\\text{s.t.} & \\sum\\limits_{j=1}^{2m} w_j = 1, \\ \\ 0 \\le w_j \\le \\frac{1}{K} (j = 1, 2, ..., 2m).\n\\end{array}$$\nBy substituting the primal formulation (Eq. 34) and the definition of $\\Pi(\\mu^w, \\nu)$, we have\n$$\\begin{array}{cl}\n\\min_{w \\in \\mathbb{R}^{2m}, \\gamma \\in \\mathbb{R}^{2m \\times n}} & \\sum_{j,j'} \\gamma_{j,j'} ||x_j - x'_j|| \\\\\n\\text{s.t.} & \\mathbf{w}^T \\mathbf{1} = 1, \\ \\ \\gamma \\mathbf{1} = w, \\ \\ \\gamma^T \\mathbf{1} = \\frac{1}{n}, \\\\\n & 0 \\le w_j \\le \\frac{1}{K} (j = 1, 2, ..., 2m), \\ \\ \\gamma_{j,j'} > 0 (j = 1, 2, ..., 2m, j' = 1, 2, ..., n).\n\\end{array}$$\nThis problem is a linear program with optimization variables $w$ and $\\gamma$. We can solve this problem by a linear program solver in a polynomial time. Let $\\hat{w}$ be the solution of this problem. We then select items with the probability $K \\hat{w}_j$ and analyze the rounding process. To analyze this step, we make the following mild assumption."}, {"title": "4 Discussions", "content": ""}, {"title": "4.1 Discussion on Variants", "content": "We have presented the variants for MMD and the Wasserstein distance. Each has its own advantages and disadvantages. The primary advantage of MMD is its sample complexity. MMD is not affected by the number of dimensions of the feature space and sidesteps the curse of dimensionality while the Wasserstein distance suffers from it [12]. The advantage of the Wasserstein distance is its generality. The assumption that the loss function $l_y (f_T(x; \\theta), y)$ is Lipschitz continuous is very mild and holds in many settings, such as logistic regression, matrix factorization, and neural networks. MMD requires the loss is in the RKHS, which may not hold in practice.\nWe also note that our analysis can be extended to other metrics as longs as the continuous optimization problem (Eq. 7) is tractable and the convering number of the test function class $\\mathcal{F}$ is bounded by the same argument of Section 3.4. For example, our analysis can be applied to the discrepancy distance [30], yielding a sharp generalization bound when the model is linear and the loss is the squared loss. Practitioners can choose metrics that best suits their specific application."}, {"title": "4.2 Optimum Value is Not Monotone", "content": "It should be noted that the optimum value of the combinatorial problem is not monotonic in $K$.\n\n$OPT^{combinatorial}$ is not monotonic in $K$.\nWe prove the proposition by a counterexample. Let $I_s = \\{1\\}$, $I_T = \\{1,2\\}$, $x_1 = 0, x_2 = 1$, and $D_s = \\{(1,1)\\}$. When $K = 1$, the optimum value is 0 by selecting $D_T = \\{(1,1)\\}$. When $K = 2$, the optimum value is non-zero because we need to select items other than (1,1), and $D_s \\neq D_T$.\nOne might intuitively expect that increasing K would lead to a better or at least no worse solution. However, the above proposition shows that this is not always the case. In many practical situations, the goal is not to click exactly K items, but rather to minimize the number of interactions while effectively transferring preferences. To address this issue, we can parallelly run the algorithm with $K' = 1,2,..., K$ and select the best solution with smallest $D(\\mu_T, \\mu_s)$ among them."}, {"title": "4.3 Limitation: Inconsistent Features", "content": "We have assumed that we have access to the features $x_i$ of the items $i$ in the problem setting. In practice, rich feature representations may not always be available, or the target service may utilize a different feature space than what we expect for training its recommendation model. This issue is particularly relevant when the target service employs collaborative filtering, a widely used approach in recommender systems. In such cases, the service may not use explicit features such as text, tags, or visual information, but use collaborative features, which are not available to end users. In such a case, our approach and analysis cannot be directly applied. Nevertheless, we argue that our approach remains valuable even in such scenarios. First, explicit features often serve as good surrogates for implicit features. For example, movies with similar descriptions tend to attract similar audiences. Second, recent studies have demonstrated that even end users can estimate implicit features from recommendation networks [45]. These estimated features can then be fed into our approach. Extending our analysis to cases where feature spaces are inconsistent is an important direction for future work."}, {"title": "5 Related Work", "content": ""}, {"title": "5.1 Cold Start Problem", "content": "The cold start problem is a fundamental problem in recommender systems. It arises when a new user (or a new item) enters the system, and the system lacks sufficient information to provide accurate recommendations. Although many methods have been proposed to address the cold start problem [14, 26, 38, 55], all of them require the service providers to implement the method. In contrast, our approach is unique in that it can be applied directly by an end user without requiring any modifications to the service itself. This characteristic broadens the applicability of our method, allowing it to be used in services that lack built-in functionalities to address the cold start problem."}, {"title": "5.2 Quadrature", "content": "Quadrature is a technique to approximate the integral of a function by summing the function values at a finite number of points [1, 18, 19, 21, 41]. This is essentially equivalent to finding a discrete measure that approximates the given measure. Quadrature is widely used in numerical analysis and machine learning. One of the common applications is coresets [6, 23, 33, 50], which is a small set of training points that approximates the loss of the model on the entire training set. Our approach can be seen as a quadrature of $\\int l_y d\\mu_s$ with points $D_T$. The main difference is that standard quadrature methods use arbitrary points and/or weights and sidestep combinatorial problems [2, 5], while our approach uses only the items in the target set, which naturally lead to the combinatorial optimization problem. For example, methods based on the Frank-Wolfe algorithm [2, 8, 58] output sparse weights, but they may choose the same items repeatedly in general, and the resulting weights are not uniform. Such output cannot be applied to our setting because users cannot thumbs up the same item multiple times in most services. Some approaches [13, 23] such as Kernel thinning [13] output a subset of the input points with uniform weights, which is similar to our approach. However, these methods assume that the candidate points are sampled according to the distribution being approximated. If this assumption does not hold, as in our case, these methods cannot be directly applied. Our proposed method can be used even when the input points are arbitrary. Other methods [56, 57] employ submodular optimization and greedy algorithms, achieving a $(1-\\frac{1}{e})$-approximation ratio. However, the gap of $(1 - \\frac{1}{e})$ does not vanish as the number of items increases. By contrast, our approach can achieve the vanishing error as Corollary 3.10 shows thanks to the continuous optimization approach and the careful rouding process. To the best of our knowledge, our work is the first to provide a theoretical guarantee for such a general and combinatorial setting. We believe that this result is of independent interest."}, {"title": "5.3 User-side Realization", "content": "User-side realization refers to the concept in which end users implement desired functionalities on their own without requiring modifications to the service itself. Many users experience dissatisfaction with services. Even if they want some functinoalities and request them to the service provider, the provider may not implement them due to various reasons such as cost, complexity, and simple negligence. After all, service providers are not volunteers but businesses. In such cases, the only options users have are not satisfactory, keep using the service despite their dissatisfaction or leave the service. User-side realization provides a proactive alternative to this dilemma. This concept has been explored in various fields such as recommender systems [43, 45, 47], search engines [11, 36, 40, 42, 44], and privacy [46]. The main advantage of the user-side realization is that it can be used in services that do not have special functionalities to address the problem, and it broadens the scope of the applicability of the solution. For a more comprehensive discussion on user-side realization, we refer the reader to the Ph.D. thesis by [48]. Our approach constitutes a novel application of user-side realization to the cold start problem in recommender systems."}, {"title": "6 Experiments", "content": ""}, {"title": "6.1 Convergence Analysis", "content": "We first confirm that the regret of PRETENDER converges to zero as the number K of selected items increases.\nWe use the MovieLens 100K dataset [17]. We consider rating \u2265 4 as positive feedback (i.e., thumbs up) and rating < 4 as negative feedback (i.e., thumbs down). We create two virtual services S and T. We include each movie in S with probability 0.5. We carry out the same process independently to create T. As a result, these services share about half of the movies. We use genres (e.g., Action, Adventure, Animation) and the release year as the features of the movies. They are encoded as a 90-dimensional multi-hot vector. We concatenate $C_y \\in \\{0, C\\}$ to the feature vector to define the discrete distribution, where $y \\in \\{0,1\\}$ indicates thumbs up or thumbs down, and C = 10 is the hyperparameter that controls the emphasis on the label when we measure the distance between two data points. We focus on user 308, who is the first user who has 10 thumbs up and 10 thumbs down, and user 21, who is the first user who has more than 100 ratings. For each user u, we aim to transfer the preference $D_u \\cap D_s$ by choosing K items from $D_T$. We use MMD with the Gaussian kernel and bandwidth $\\sigma = 1$ as the distance measure. We use L = 1000 iterations of the Frank-Wolfe algorithm and R = 100 trials of the rounding process. We set K = 1, 2, 4, 8,..., 128 to see the convergence behavior.\nThe goal of the experiment is to confirm that the regret of the proposed algorithm, i.e., the MMD of the proposed method minus the MMD of the optimal solution, converges to zero as the number of selected items K increases. However, computing the true optimal solution is intractable due to the combinatorial nature of the optimization problem. Instead, we use the optimal value of the continuous relaxation as a benchmark, which serves as a provable lower bound for the optimal value of the combinatorial optimization problem."}, {"title": "6.2 Quantitative Evaluation", "content": "Datasets. We evaluate our method using datasets from three domains: MovieLens 100k (movies), Last.fm (music) [7", "32": "."}]}