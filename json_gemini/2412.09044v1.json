{"title": "Motif Guided Graph Transformer with Combinatorial Skeleton Prototype Learning for Skeleton-Based Person Re-Identification", "authors": ["Haocong Rao", "Chunyan Miao"], "abstract": "Person re-identification (re-ID) via 3D skeleton data is a challenging task with significant value in many scenarios. Existing skeleton-based methods typically assume virtual motion relations between all joints, and adopt average joint or sequence representations for learning. However, they rarely explore key body structure and motion such as gait to focus on more important body joints or limbs, while lacking the ability to fully mine valuable spatial-temporal sub-patterns of skeletons to enhance model learning. This paper presents a generic Motif guided graph transformer with Combinatorial skeleton prototype learning (MoCos) that exploits structure-specific and gait-related body relations as well as combinatorial features of skeleton graphs to learn effective skeleton representations for person re-ID. In particular, motivated by the locality within joints' structure and the body-component collaboration in gait, we first propose the motif guided graph transformer (MGT) that incorporates hierarchical structural motifs and gait collaborative motifs, which simultaneously focuses on multi-order local joint correlations and key cooperative body parts to enhance skeleton relation learning. Then, we devise the combinatorial skeleton prototype learning (CSP) that leverages random spatial-temporal combinations of joint nodes and skeleton graphs to generate diverse sub-skeleton and sub-tracklet representations, which are contrasted with the most representative features (prototypes) of each identity to learn class-related semantics and discriminative skeleton representations. Extensive experiments validate the superior performance of MoCos over existing state-of-the-art models. We further show its generality under RGB-estimated skeletons, different graph modeling, and unsupervised scenarios.", "sections": [{"title": "Introduction", "content": "Person re-identification (re-ID) aims at matching and retrieving a person-of-interest from different views or scenes, which assumes an essential role in security authentication, smart surveillance, human tracking, and robotics (Vezzani, Baltieri, and Cucchiara 2013; Ye et al. 2021). Recent advancements in low-cost and accurate skeleton-tracking devices (e.g., Kinect (Shotton et al. 2011)) have streamlined the collecting of 3D skeletons to make them as a popular and generic data modality for gait analysis and person re-ID (Liao et al. 2020; Rao et al. 2021b; Rao, Leung, and Miao 2024). Compared with traditional person re-ID methods that rely on appearance features (Wang et al. 2016), skeleton-based models typically utilize body structural features and motion patterns of key body joints to identify different persons, which could possess many merits such as lighter inputs and models, better privacy protection (e.g., without using appearances or faces), and more robust performance under view, scale, and background variations (Han et al. 2017). Early skeleton-based methods (Andersson and Araujo 2015) extract hand-crafted anthropometric and gait descriptors (e.g., kinematic parameters) based on domain expertise, while they are often incapable of exploiting latent skeleton features beyond human cognition. Recent years have witnessed the great success of deep neural networks such as graph transformers for skeleton-based person re-ID (Liao et al. 2020; Rao and Miao 2023). A common practice in these studies is to combine body-joint relation modeling and skeleton prototype learning (e.g., class feature clustering and contrasting) (Rao and Miao 2022; Rao and Miao 2023). However, most methods learn body joint or component relations with the assumption of virtual motion connections among all joints (Rao et al. 2021c; Rao and Miao 2023), while they typically lack a specific focus on key body joints or local body parts that are highly related to walking patterns (e.g., gait) to capture more discriminative features. On the other hand, existing works usually leverage average skeleton or sequential features (Rao and Miao 2022) to perform representation learning, while they rarely harness different combinatorial spatial or temporal patterns (e.g., sub-patterns) of key body joints, parts or skeletons to enhance the skeletal structure and motion learning. For example, different combinations of key joints such as hip and knee joints may characterize different structural features in walking patterns, while a combination of partial consecutive skeletons could contain key sub-patterns of a sequence, both of which can be utilized to mine more valuable skeleton features. To address the aforementioned challenges, we propose a generic Motif guided graph transformer with Combinatorial skeleton prototype learning (MoCos) (illustrated in Fig. 1), which exploits different graph motifs to guide body-joint relation learning in terms of key body structure and motion, and leverages different spatial-temporal feature combinations of both joints and skeletons to enhance skeleton graph representation learning for person re-ID. In particular, motivated by the local correlations (referred to as locality) within hierarchical body joints' structure, we first devise hierarchical structural motifs (HSM), which endow body joints with different semantic roles of connections, to specially focus on multi-order dependencies of body joints and their structural correlations to capture richer skeleton patterns. Then, considering that collaborative movements of upper and lower limbs usually contain unique (e.g., identity-specific) gait patterns (Murray, Drought, and Kory 1964), we propose gait collaborative motifs (GCM) that focus on both local and global motion relations of key limbs' joints to encourage the model to capture more salient gait features. By incorporating HSM and GCM into the joint relation learning, we devise the motif guided graph transformer (MGT) to simultaneously capture key body relations from hierarchical local structure of joints and gait-related collaborative components for person re-ID. Last, to exploit more valuable combinatorial patterns from skeletons and their sequences, a combinatorial skeleton prototype learning (CSP) approach is proposed to randomly mask body-joint nodes and skeleton graphs to generate spatial-temporal combinatorial graph features at both levels of sub-skeletons and sub-tracklets, which are utilized to contrast and learn the most representative skeleton graph features (referred to as prototypes) of each identity. CSP pulls different combinatorial skeleton graph representations closer to corresponding prototypes, and pushes them apart from other prototypes, so as to facilitate the model to learn distinguishing skeleton features and high-level class-related semantics for person re-ID. Our main contributions can be summarized as follows:\n\u2022 We propose a generic MoCos paradigm that exploits diverse graph motifs and combinatorial skeleton features to learn effective representations from skeleton graphs for person re-ID. To the best of our knowledge, MoCos is the first exploration of structure-specific and gait-based graph motifs to enhance skeleton relation and prototype learning specifically for skeleton-based person re-ID.\n\u2022 We devise the motif guided graph transformer (MGT) by synergizing hierarchical structural motifs (HSM) and gait collaborative motifs (GCM) to guide body-joint relation learning, so as to capture more discriminative body structural and gait features within skeletons for person re-ID.\n\u2022 We propose the combinatorial skeleton prototype learning (CSP) that leverages combinatorial spatial-temporal graph features of joints (sub-skeletons) and skeletons (sub-tracklets) to learn more key skeleton patterns.\n\u2022 Empirical evaluations on five public datasets validate that MoCos significantly outperforms existing state-of-the-art methods and can be effectively applied to different graph modeling, RGB-estimated or unsupervised scenarios."}, {"title": "Related Works", "content": "Skeleton-Based Person Re-Identification. Skeleton-based person re-ID focuses on the problem of matching and retrieving a certain person based on spatial and temporal representations of skeletal human body and gait (2024; 2024; 2023; 2024). Early-stage studies manually extract skeleton or body-joint descriptors in terms of anthropometric and gait attributes for person re-ID. Barbosa et al. compute Euclidean distances between different joint pairs as descriptors, while they are further extended to 13 (D13 (Munaro et al. 2014b)) and 16 skeleton descriptors (D16 (Pala et al. 2019)) to perform person re-ID. Most recent methods (2020; 2021b; 2023; 2024) leverage deep learning models for skeleton sequence or skeleton graph representation learning. PoseGait (Liao et al. 2020) is proposed to encode 3D pose features and joint-based motion descriptors (denoted as DPG) for human recognition. Rao et al. utilize an encoder-decoder model with attention mechanisms (AGE) to encode skeleton-based gait patterns, while its extension SGELA (Rao et al. 2021b) further enhances self-supervised skeleton semantic learning with diverse skeletal pretext tasks (e.g., time series forecasting (Feng et al. 2024; Zhicheng et al. 2024)) and inter-sequence contrastive mechanisms for the person re-ID task. Rao and Miao propose a masked contrastive learning framework (SimMC) to perform skeleton prototype learning with intra-sequence relation learning for person re-ID. The multi-scale skeleton graphs are explored in (2021c; 2021a; 2022) to learn body relations and patterns at various levels. In (Rao and Miao 2023), a skeleton graph transformer is devised to learn both skeleton and sequential graph features for person re-ID. A general skeleton feature re-ranking mechanism is proposed in (Rao, Li, and Miao 2022) for skeleton-based person re-ID. Hi-MPC (Rao, Leung, and Miao 2024) utilizes hierarchical prototype learning with a hard skeleton mining approach to learn discriminative skeleton features. Existing multi-modal person re-ID methods usually combine skeleton-based features with extra RGB or depth information (e.g., depth shape features based on point clouds (Munaro et al. 2014a; Hasan and Babaguchi 2016; Wu, Zheng, and Lai 2017)) to boost re-ID accuracy. For example, some works combine RGB images and skeleton data to learn auxiliary anthropometric attributes (Wang et al. 2020), body parts correlations (Lu et al. 2023), and clothing-invariant features (Nguyen et al. 2024) to enhance their performance."}, {"title": "Graph Motifs", "content": "Motifs define different patterns of connections in graphs or networks via specifying the pattern-context nodes relevant to a target node of interest (Sankar, Zhang, and Chang 2017), which have been widely applied to many areas such as neuroscience and computer vision. (2004; 2007; 2022; 2023) A few recent works (Wen et al. 2022, 2019) integrate motifs into graph convolutional networks (GCNs) to learn skeleton features from joints of interest and their context for action recognition. As far as we know, this work is the first exploration of structural and gait-based graph motifs with high-order semantic roles of joints specifically for skeletal relation learning and person re-ID."}, {"title": "Methodology", "content": "Problem Definition. Suppose that a 3D skeleton sequence $X = (x_1,\\ldots, x_f) \\in \\mathbb{R}^{f\\times J\\times 3}$, where $f$ denotes the number of skeletons in the sequence and $x \\in \\mathbb{R}^{J\\times 3}$ represents the $i^{th}$ skeleton with 3D coordinates of $J$ body joints. Each sequence $X$ corresponds to an identity class $y \\in \\{1,\\ldots,\\mathcal{C}\\}$ and $\\mathcal{C}$ is the number of different identity classes. We denote the Training set, Probe set, and Gallery set as $\\Phi_T = \\{(X_i,y_i)\\}_{i=1}^{n_1}$, $\\Phi_P = \\{X_i\\}_{i=1}^{n_2}$, and $\\Phi_G = \\{X_i\\}_{i=1}^{n_3}$, which respectively contain $n_1$, $n_2$, and $n_3$ skeleton sequences of different persons collected from different scenes or views. The model target is to encode skeleton sequences into effective representations, so that we can query the correct identity of each skeleton sequence representation (denoted as $\\{V?\\}_{i=1}^{n_2}$) in the probe set via matching it with the sequence representations (denoted as $\\{V\\}_{i=1}^{n_3}$) in the gallery set. Skeleton Graph Construction. We construct skeleton graphs based on the physical connections of human body joints (Rao and Miao 2023): For the $t^{th}$ skeleton $x_t$, we represent it as the graph $G_t(V_t, E_t)$, which consists of $J$ nodes $V_t = \\{v_1, v_2, ..., v_J\\}$, $v_i \\in \\mathbb{R}^3$, $i \\in \\{1,\\ldots,\\mathcal{J}\\}$ and edges $E_t = \\{e_{ij} | v_i, v_j \\in V\\}$, $e_{ij} \\in \\mathbb{R}$. Here $E_t$ denotes the set of connections and motion relations between different joints, and can be represented with an adjacent matrix $A^t \\in \\mathbb{R}^{J\\times J}$, initialized by the connections of adjacent body joints. Motif Guided Graph Transformer Different body joints and parts of a pedestrian typically possess unique relations, such as structural relations between adjacent joints, and actional relations between non-adjacent parts, characterizing discriminative walking patterns (Murray, Drought, and Kory 1964; Rao et al. 2021a). Existing methods typically perform global relation learning with the assumption of virtual motion relations among all joints (Rao and Miao 2023), while they rarely exploit local hierarchical structure of joints (defined as \u201clocality\") or key gait-related body components to capture richer valuable relations. To this end, we propose to endow body-joint nodes with different relational semantic roles (defined as \"motifs\" for skeleton graphs), and devise the motif guided graph transformer (MGT) to simultaneously focus on their hierarchical structural relations and gait collaborative relations to learn effective skeleton graph representations for person re-ID."}, {"title": "Graph Transformer (GT)", "content": "First, given a skeleton graph, its $J$ node representations are integrated with their positional encoding based on the graph adjacency matrix $A^t$ (Rao and Miao 2023), which can be formulated as: $h_i = (W_1v_i + b_1) + (W_2\\lambda_i + b_2),$   (1)\nwhere $h_i \\in \\mathbb{R}^D$ represents the position-encoded representation of $i^{th}$ node, $\\lambda_i \\in \\mathbb{R}^K$ denotes the $i$-node's positional encoding extracted from the $K$ smallest non-trivial eigenvectors of graph Laplacian matrix following (Dwivedi and Bresson 2021), and $W_1 \\in \\mathbb{R}^{D\\times 3}$, $W_2 \\in \\mathbb{R}^{D\\times K}$, $b_1, b_2 \\in \\mathbb{R}^D$ are learnable parameters to map $i^{th}$ node $v_i$ and corresponding positional encoding into feature spaces of the same dimension $D$. Then, GT computes the preliminary relation value of joints (referred to as \u201cfull relations (FR)\") by $R_{i,j}^{k,l} = Softmax_j(\\frac{Q_i^{k,l} (K_j^{k,l})^T}{\\sqrt{D_k}}),$   (2)\nIn Eq. (2), $Q^{k,l}, K^{k,l} \\in \\mathbb{R}^{D\\times D}$ represent the learnable weight matrices for query and key transformations in the $k^{th}$ relation head of the $l^{th}$ GT layer, $\\checkmark$ is the scaling factor of dot-product similarity, and $R_{i,j}^{k,l}$ denotes the softmax-normalized relational value between the $i^{th}$ and $j^{th}$ joint captured by the $k^{th}$ relation head in the $l^{th}$ layer. Hierarchical Structural Motifs. To guide the model to fully capture skeleton patterns from body joints' physical connections and the multi-level dependencies within their local hierarchical structure, we devise the hierarchical structural motifs (HSM) to learn structural body relations from different-order neighbors of joint nodes. The focused body-joint relations of HSM can be represented as a matrix with $A_i^m = \\begin{cases} 1 & \\text{if } j \\in \\bigcup_{k=1}^m N_i \\text{, } j \\neq i \\\\ 0 & \\text{otherwise} \\end{cases}$   (3)\nwhere $m \\in \\{1,2,3\\}$, $A^m \\in \\mathbb{R}^{J\\times J}$ denote the $m$-order HSM matrix, $i \\in \\{1,2,\\ldots,\\mathcal{J}\\}$, and $N$ represents the indices for $k$-order neighbors of the $i^{th}$ body-joint node (i.e., nodes with $k$-hop distance to the $i^{th}$ node). Intuitively, the $m$-order HSM $A^m$ defines $R_m = 2m + 1$ semantic roles for all joint nodes: $A^1$ contains $R_1 = 3$ roles, including a joint node itself, its parent node, and child node; $A^2$ contains $R_2 = 5$ roles, including a joint node itself, its grandparent node, parent node, child node, and grandchild node, while $A^3$ ($R_3 = 7$) further includes the roles of its great-grandparent node and great-grandchild node. Note that HSM does NOT require pre-defining the directions of node connections but views them as bi-directional to focus on the general hierarchical structure of joints. The maximum order is empirically set to 3 as the center joint of spine in most datasets has up to 3-hop neighbors (Li et al. 2021). By simultaneously focusing on relations of each body-joint node to its immediate and higher-level connected neighbors, HSM encourages the model to encode the inherent hierarchical structure (e.g., high-order locality) of joints' positions and motion to capture more valuable patterns of skeleton graphs.\""}, {"title": "Gait Collaborative Motifs", "content": "Motivated by the gait property that different key body components (e.g., arms and legs) usually perform collaborative motion characterizing identity-specific patterns (Murray, Drought, and Kory 1964), we propose the gait collaborative motifs (GCM) to guide the model to learn more salient patterns from motion units of both upper and lower limbs (see Fig. 2). In particular, we regard each body joint in limbs as a basic motion unit, and focus on its local relations within the same limb and global relations with other limbs to facilitate the gait pattern learning. We define GCM with the focused body-joint relations as $B_{i,j}^m = \\begin{cases} 1 & \\text{if } i \\in \\mathcal{I}^m, j \\in \\bigcup_{k=1}^2 \\mathcal{I}_{i,j} \\neq i\\\\ 0 & \\text{otherwise} \\end{cases}$,   (4)\nwhere $m \\in \\{1,2\\}$, $B_1, B_2 \\in \\mathbb{R}^{J\\times J}$ denote the GCM matrices for the upper and lower limbs, $\\mathcal{I}^1$ and $\\mathcal{I}^2$ represent the sets of indices for joint nodes in upper limbs (e.g., arms) and lower limbs (e.g., legs) respectively (visualized in Appendix I). Specifically, each GCM matrix defines $R = 3$ semantic roles for all joint nodes: $B^1$ (or $B^2$) contains the roles of a joint node in a upper (or lower) limb, its locally-correlated sibling nodes (i.e., nodes in the same limb), and its globally-collaborative nodes in other limbs. In this way, GCM aims to focus on both local and global relation learning of limb motion units to encourage mining more unique cooperative skeleton patterns from their gait-related components. By incorporating HSM and GCM into the relation learning process of GT, we devise the motif guided graph transformer (MGT) to jointly focus on hierarchical body-joint structure and gait-related components to capture more key skeleton patterns. In particular, MGT computes motif-guided relations (MR) by updating Eq. (2) to (illustrated in Fig. 2) $R_{i,j}^{k,l} = Softmax_j(\\frac{\\mathcal{M}(Q_i^{k,l} h_i) (K_j^{k,l} h_j)^T}{\\sqrt{D_k}}),$   (5)\nwhere $\\mathcal{M}_{i,j}^m = \\begin{cases} A_i^k & \\text{if } k \\in \\{1, 2, 3\\}\\\\ B_{i,j}^{k-3} & \\text{if } k \\in \\{4, 5\\}\\\\ 0 & \\text{otherwise} \\end{cases}$   (6)\nIn Eq. (5) and (6), $h_i^{(l)} \\in \\mathbb{R}^D$ denotes the feature representation of the $i^{th}$ joint encoded by the $l^{th}$ MGT layer, $R_{i,j}^{k,l}$ represents the relation value between the $i^{th}$ and $j^{th}$ joint computed by the $k^{th}$ MR head in the $l^{th}$ layer, $k \\in \\{1,2,..., H\\}$, and $H$ is the number of MR heads. MGT adopts multiple MR heads to jointly perform motif-guided and full relation learing, which are then aggregated into the each graph node representation with $h_i^{(l+1)} = O (\\parallel_{k=1}^H \\sum_{j=1}^J \\mathcal{M}_{i,j}^k (V_j^{k,l}h_j^{(l)}))$\\label{eq:4}   (7)"}, {"title": "Combinatorial Skeleton Prototype Learning", "content": "To mine the most representative skeleton features of each identity for person re-ID, existing solutions (Rao and Miao 2022; Rao and Miao 2023) typically average spatial or temporal features of skeletons for prototype clustering and contrasting, while they rarely harness different combinatorial spatial-temporal patterns of body joints, parts or skeletons to learn more effective representations. For example, a subset or dynamic combination of key body joints such as wrist, knee and foot joints (defined as \u201csub-skeleton representations", "sub-tracklet representations\") typically contain diverse sub-patterns (Zhang et al. 2020), both of which can be exploited to learn more informative and unique features. To this end, we propose the combinatorial skeleton prototype learning (CSP) that leverages spatial-temporal combinatorial graph representations of sub-skeletons and sub-tracklets to jointly perform skeleton prototype learning. Given the $t^{th}$ skeleton graph representation $(h_1,\\ldots, h_J)$ containing $J$ spatial representations of body-joint nodes, we utilize random masks to generate a subset of nodes to construct its spatial combinatorial representation (i.e., sub-skeleton representation) by $\\hat{v}_t = \\frac{1}{N_s} \\sum_{j=1}^J x_j h_j,$   (8)\nwhere $\\hat{v}_t \\in \\mathbb{R}^D$ is the sub-skeleton representation of $t^{th}$ skeleton graph by randomly masking nodes, $x_j \\in \\{0,1\\}$ denotes the $j^{th}$ mask that is an independent and identically distributed Bernoulli random variable with the probability $p_s$ of being 0 (i.e., $x_j \\sim Bernoulli(1 - p_s)$), and $N_s = \\sum_{j=1}^J x_j$ represents the number (i.e., subset size) of unmasked node representations. Each unmasked node representation is assumed to be equally important and we average them to be the sub-skeleton graph representation. In practice, the maximum number of masked node representations is $J - 1$ (i.e., $N_s \\geq 1$) to avoid empty skeleton representation. Here we adopt Bernoulli distribution for combinatorial feature generation due to its simplicity and computational tractability (Boluki et al. 2020), while other probabilistic distributions can be also extended and applied to the proposed masking. Then, provided the spatial combinatorial representations $(\\hat{v}_1,\\ldots, \\hat{v}_f)$ of $f$ consecutive skeleton frames (defined as \u201ca skeletal walking tracklet": "we generate a random subset of the walking tracklet to yield the spatial-temporal combinatorial representation (i.e., sub-tracklet representation) with $\\Bar{V} = \\frac{1}{N_r} \\sum_{t=1}^f m_t \\hat{v}_t,$   (9)\nwhere $\\Bar{V} \\in \\mathbb{R}^D$ denotes the sub-tracklet representation that incorporates both spatial and temporal combinatorial features of a skeleton sequence, and $m_t \\in \\{0,1\\}$ represents the $t^{th}$ random mask sampled from the Bernoulli distribution with the probability $p_t$ being 0. $N_r = \\sum_{t=1}^f m_t$, $N_r \\geq 1$ is the sequence length of sub-tracklet. Each sub-skeleton representation within the sub-tracklet is assigned with the same importance, and we average them as the final sub-tracklet representation. It is worth noting that a sub-tracklet contains sub-trajectory of partial body joints (i.e., sub-skeletons), and can be regarded as a subset representation of sub-sequence trajectory. In essence, the temporally-masked sequence in SimMC (Rao and Miao 2022) and average spatially-masked skeleton representations in TranSG (Rao and Miao 2023) can be viewed as two special cases of proposed sub-tracklet representation by setting $p_s = 0$ and $p_t = 0$ respectively. To exploit graph representations of both sub-skeletons and sub-tracklets to learn the most discriminative skeleton graph features (defined as \u201cprototypes\u201d) of each person and high-level semantics (e.g., identity-associated patterns), we propose the combinatorial skeleton prototype (CSP) loss as $L_{CSP} = \\lambda L_{CSP}^{str} + (1 - \\lambda) L_{CSP}^{ssk},$   (10)\nwhere $L_{CSP}^{str} = \\frac{1}{n_1} \\sum_{i=1}^{n_1} -log \\frac{exp(\\Bar{V}_i \\cdot \\Bar{c}_i/\\tau_1)}{\\sum_{k=1}^{C} exp(\\Bar{V}_i \\cdot \\Bar{c}_k/\\tau_1)}$,   (11)\n$L_{CSP}^{ssk} = \\frac{1}{fn_1} \\sum_{i=1}^{fn_1} -log \\frac{exp(\\mathcal{F}_1(\\hat{v}_t) \\cdot \\mathcal{F}_2(\\hat{c}_i)/\\tau_2)}{\\sum_{j=1}^{C} exp(\\mathcal{F}_1(\\hat{v}_t) \\cdot \\mathcal{F}_2(\\hat{c}_k)/\\tau_2)}$,   (12)\n$\\Bar{c}_k = \\frac{1}{n_k} \\sum_{y_j=k} \\Bar{V}_j$   (13)\nThe proposed CSP loss in Eq. (10) combines both sub-tracklet-level ($L_{CSP}^{str}$) and sub-skeleton-level combinatorial prototype loss ($L_{CSP}^{ssk}$) with the fusion coefficient $\\lambda$. In Eq. (11), (12) and (13), $n_1$ is the number of training skeleton sequences, $\\hat{v}$ and $\\Bar{V}_i$ denote the sub-skeleton representation of the $t^{th}$ skeleton (see Eq. (8)) and the sub-tracklet representation of the $i^{th}$ skeleton sequence (see Eq. (9)), $\\Bar{c}$ and $\\hat{c}$ correspond to their prototypes (i.e., class feature centroids) generated by averaging all sequence representations of the same identity (see Eq. (13)), $c_k$ is the skeleton prototype of"}, {"title": "Experiments", "content": "Experimental Setups Datasets. Four skeleton-based person re-ID benchmark datasets are used to evaluate our approach, including IAS (Munaro et al. 2014c), KS20 (Nambiar et al. 2017), BIWI (Munaro et al. 2014b), KGBD (Andersson and Araujo 2015), which contain 11, 20, 50, and 164 different persons. The generality of MoCos is also validated on a large-scale multi-view gait dataset CASIA-B (Yu, Tan, and Tan 2006) with RGB-estimated skeleton data of 124 individuals under three conditions (Normal (N), Bags (B), Clothes (C)). The commonly-used standard probe and gallery settings (Rao and Miao 2023) are adopted for a fair comparison. Implementation Details. The skeletons in KGBD, IAS, and BIWI contain $J = 20$ body joints, while KS20 and CASIA-B (RGB-estimated skeletons) contain $J = 25$ and $J = 14$ joints, respectively. For a fair comparison, we follow existing methods (Rao, Leung, and Miao 2024) to set the sequence length to $f = 6$ for IAS, KS20, BIWI, KGBD and $f = 40$ for the RGB-estimated skeleton data in CASIA-B. We set the embedding size to $D = 128$ for each node representation, and empirically employ 2 MGT layers with $H = 8$ relation heads and $D_k = 16$ for each layer. The probability for spatial or temporal masking of CSP is empirically set for different datasets: $p_s = 0.25$, $p_t = 0.25$ for IAS, BIWI, KS20, and $p_s = 0.5$, $p_t = 0.25$ for KGBD. We use fusion coefficient $\\lambda = 0.9$ for BIWI-W, KGBD, KS20, $\\lambda = 0.25$ for BIWI-S, $\\lambda = 0.75$ for IAS-A and IAS-B. We set the learning rate to $3.5 \\times 10^{-4}$ and use an Adam optimizer with batch size 256 for model training on all datasets. More technical details are provided in the appendices. Evaluation Metrics. Cumulative matching characteristics curve is computed and we report Rank-1, Rank-5, Rank-10 accuracy (R1, R5, R10), and Mean Average Precision (mAP) (Zheng et al. 2015) to evaluate model performance. Comparison with State-of-the-Art Methods Our approach is compared with state-of-the-art hand-crafted methods, sequence learning methods, and graph-based methods on BIWI, KS20, IAS, KGBD in Table 1. Comparison with Graph-based Methods: As shown in Table 1, the proposed MoCos significantly outperforms existing state-of-the-art graph-based methods (SPC-MGR (Rao and Miao 2022), MG-SCR (Rao et al. 2021c), SM-SGE (Rao et al. 2021a)) with an improvement of 11.1-41.3% for mAP and 10.0-51.9% for Rank-1 accuracy on different benchmark datasets. Unlike these methods that resort to multi-scale graph modeling and multi-stage relation learning, our approach can utilize simpler single-level graph representations with motif guided concurrent relation learning (MGT) to more effectively capture distinguishing skeleton features for person re-ID. In contrast to the latest skeleton graph model TranSG (Rao and Miao 2023) employing na\u00efve GT, our model using MGT also consistently achieves better performance in terms of mAP (2.0-6.1%), Rank-1 (2.4-3.3%), Rank-5 accuracy (0.9-4.3%), and Rank-10 accuracy (0.0-4.8%) on all datasets. This demonstrates the higher efficacy of MoCos incorporating joint-level motif-guided relation learning and different-level prototypical contrast (CSP) to learn richer unique skeleton features for person re-ID. We will also discuss its generality under diverse graph modeling and different unsupervised paradigms in the next section. Comparison with Hand-crafted and Sequence Learning Methods: Compared with methods that rely on hand-crafted pose features (DPG (Liao et al. 2020)) or anthropometric attributes (D13 (Munaro et al. 2014b), D16 (Pala et al. 2019)), our approach achieves superior performance by a marked margin of up to 53.5% Rank-1 accuracy and 39.5% mAP on different benchmarks. Moreover, MoCos also obtains significantly higher performance than latest skeleton"}, {"title": "Further Analysis", "content": "Application to RGB-estimated Scenarios. To verify the generality of MoCos on RGB-estimated skeletons, we extract skeleton data with pre-trained pose estimation models (Cao et al. 2019; Chen and Ramanan 2017) from RGB videos instead of depth sensors. The results in Table 3 show that our model not only achieves superior performance to most existing state-of-the-art skeleton-based models, but also outperforms many representative established appearance-based methods that rely on RGB-based features (e.g., silhouettes) or/and visual metric learning (Liu et al. 2015; Farenzena et al. 2010). This verifies the generality and higher effectiveness of MoCos to learn discriminative patterns from estimated skeletons, and demonstrates its potential for person re-ID under large-scale RGB-based scenarios. Evaluation on Different-Scale Skeleton Graphs. We construct different-scale graphs (Rao et al. 2021a) for MoCos learning to evaluate its performance under varying graph modeling. As presented in Table 4, compared with the state-of-the-art multi-scale graph method SM-SGE (Rao et al. 2021a), our model achieves better performance in most cases of both original and higher level skeleton representations (e.g., part-scale skeleton graphs). Such results suggest the compatibility of the proposed motif guided graph transformer (MGT) with different-scale graph modeling, and also justify its stronger capability to learn more effective graph features and semantics at different levels for person re-ID. Transfer to Unsupervised Paradigms. As shown in Table 5, our MGT and CSP (denoted as \u201c+ MoCos\u201d) can be transferred for unlabeled skeleton relation and prototype learning, which effectively boosts performance of different unsupervised non-graph and non-transformer models (Rao and Miao 2022; Rao and Miao 2022) in most cases. This validates generality and scalability of MoCos, which can be potentially applied to more general scenarios without labels."}, {"title": "Conclusion", "content": "In this paper, we propose MoCos to perform motif-guided joint relation learning and combinatorial skeleton prototype learning for person re-ID. We design the motif guided graph transformer (MGT) that incorporates hierarchical structural motifs and gait collaborative motifs to capture key relations within multi-order body joints' structure and gait-related limbs. The combinatorial skeleton prototype learning (CSP) is proposed to contrast randomly-combined sub-skeleton and sub-tracklet graph features with skeleton prototypes to learn"}]}