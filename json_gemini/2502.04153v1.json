{"title": "ULTRAIF: Advancing Instruction Following from the Wild", "authors": ["Kaikai An", "Li Sheng", "Ganqu Cui", "Shuzheng Si", "Ning Ding", "Yu Cheng", "Baobao Chang"], "abstract": "Instruction-following made modern large language models (LLMs) helpful assistants. However, the key to taming LLMs on complex instructions remains mysterious, for that there are huge gaps between models trained by open-source community and those trained by leading companies. To bridge the gap, we propose a simple and scalable approach ULTRAIF for building LLMs that can follow complex instructions with open-source data. ULTRAIF first decomposes real-world user prompts into simpler queries, constraints, and corresponding evaluation questions for the constraints. Then, we train an UltraComposer to compose constraint-associated prompts with evaluation questions. This prompt composer allows us to synthesize complicated instructions as well as filter responses with evaluation questions. In our experiment, for the first time, we successfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5 instruction-following benchmarks without any benchmark information, using only 8B model as response generator and evaluator. The aligned model also achieved competitive scores on other benchmarks. Moreover, we also show that ULTRAIF could further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating broader use cases for the method. Our code will be available at https://github.com/kkk-an/UltraIF.", "sections": [{"title": "1. Introduction", "content": "Large language models (Meta, 2024; OpenAI, 2024) have demonstrated remarkable capabilities, especially in following complex instructions. While modeling such ability is crucial, the technical details and the instruction datasets used in state-of-the-art LLMs remain mysterious. For example, LLaMA3 (Meta, 2024) reportedly leverages instruction-following data at the tens of millions scale but has not been open-sourced. This lack of transparency has resulted in a significant gap between the research community and the leading companies.\nRecent efforts in aligning LLMs to follow instructions have focused on creating high-quality instruction-following data. On the one hand, Wei et al. (2021); Rajani et al. (2023); Jiang et al. (2023) involve human annotators in developing instructions and manually crafting corresponding responses. While effective, these methods are label-intensive, heavily reliant on human expertise, and face challenges in scalability and cost efficiency. On the other hand, Xu et al. (2023); Wang et al. (2023); Sun et al. (2024a); Dong et al. (2024) attempt to leverage LLMs to automatically construct high-quality instruction data. Specifically, Xu et al. (2023); Sun et al. (2024a) guide LLMs to generate constraints and evolve initial instructions into more complex forms. However, these LLMs-driven methods heavily rely on models' instruction-evolving capability and overemphasize instruction complexity, ultimately hindering the diversity of evolved instructions and the correctness of generated responses. To improve this, Wang et al. (2023); Dong et al. (2024) introduce handcrafted constraints inspired by human priors to guide LLMs. For instance, Dong et al. (2024) introduces constraints that can be verified by code execution to ensure response correctness. However, these handcrafted constraints introduce rigidity,\nleading to homogeneous instructions and making it narrow in encompassing more complex or diverse instructions (e.g., write in Shakespeare's tone). As a result, scaling such instruction with correct responses remains a significant challenge, limiting the applicability of modeling the distribution of instructions from real-world users.\nIn this paper, we propose ULTRAIF, a simple and scalable method which synthesizes high-quality instruction-following data. The core idea of ULTRAIF is to decompose real-world user instructions for both constraints and evaluation questions, then train a composer model to synthesize diverse and complex instructions with verification questions. To achieve this, we first utilize LLM to decompose human instructions into simplified instructions and their associated constraints. For each constraint, the LLM further generates the corresponding evaluation question to verify whether the upcoming response meets the requirement. With these components, we train UltraComposer, which takes the simplified instruction as input and outputs the original instruction along with its evaluation question. In this way, the composer learns to evolve instructions with verifiable constraints, and benefits from the generalization ability of LLMs rather than handcrafted rules. With the composer, ULTRAIF could make any instruction more complicated to synthesize a large-scale and diverse dataset. The evaluation questions further help with quality control in rejection sampling and preference learning (Rafailov et al., 2024; Chen et al., 2024).\nThrough comprehensive experiments, we demonstrate that ULTRAIF significantly enhances the instruction-following capabilities of LLMs with high scalability and cost efficiency. Our evaluation, conducted on the LLaMA-3.1-8B model across five instruction-following datasets, confirms ULTRAIF's strong alignment with general instructions. Notably, as shown in Figure 1, by scaling up the training data, we achieve a milestone, optimizing the LLaMA-3.1-8B-Base model to match the instruction-following ability of its instruct version. Additionally, we assess the generability of ULTRAIF by evaluating it on mathematical, reasoning, coding, and general conversation domains. Furthermore, we explore the potential of self-alignment in ULTRAIF by further optimizing the LLaMA-3.1-8B-Instruct model, and achieve substantial improvement. The main contributions of our paper are as follows:\n\u2022 We introduce ULTRAIF, a simple and scalable approach that leverages real-world user instructions to train a composer model, UltraComposer, enabling the synthesis of complex and diverse instructions with correct responses.\n\u2022 Our experiments demonstrate the strong performance of ULTRAIF in handling complex instructions, surpassing all baselines under the same data budget while retaining general capabilities in domains such as mathematics, coding, and conversational tasks."}, {"title": "2. ULTRAIF", "content": "2.1. Overview\nULTRAIF synthesizes high-quality instruction-following datasets through a two-stage process. As shown in Figure 2, ULTRAIF first constructs the UltraComposer by decomposing user instructions into simplified ones and constraints, along with corresponding evaluation questions, as detailed in \u00a72.2. This specialized composer facilitates the synthesis of instructions with more complex and diverse constraints, while the evaluation questions ensure the correctness and reliability of the generated responses. Then, we introduce the Generate-then-Evaluate process, described in \u00a72.3. This framework first uses UltraComposer to incorporate constraints into instructions and then evaluates the generated responses using corresponding evaluation questions covering various quality levels. Additionally, we introduce our two training strategies in \u00a72.4.\n2.2. UltraComposer\nPrevious studies (Xu et al., 2023; Sun et al., 2024a) that rely solely on LLMs are limited by the models' instruction-evolving ability, which restricts the diversity of synthetic instructions and compromises response accuracy. While Wang et al. (2023); Dong et al. (2024) address response correctness through handcrafted constraints, this approach further limits instruction diversity. In contrast, ULTRAIF focuses on generating diverse, complex instructions with correct responses. To achieve this, we propose the UltraComposer, a specialized model to synthesize diverse instructions and generate corresponding evaluation questions. Building this composer model involves three key steps: instruction decomposition, evaluation question generation, and UltraComposer training. We provide the prompts about instruction decomposition and evaluation question generation in Appendix C.2.\nInstruction Decomposition The decomposition process leverages LLMs to decompose complex instructions into simplified components, such as those sourced from ShareGPT (Chiang et al., 2023). These components consist of a set of simplified instructions paired with constraints that represent the underlying requirements of the original instruction. For example, as shown in Figure 2 (a), the instruction (X) \"In Shakespeare's tone, recommend me ten Chinese books.\" can be decomposed into the simplified instruction (x1) \u201cRecommend me ten Chinese books.\u201d and the paired constraint (c1) \"In Shakespeare's tone.\", etc. This step is essential for disentangling intricate objectives into more structured elements, extending beyond basic format or content constraints (Dong et al., 2024; Wang et al., 2023), and forming a foundation to model the distribution of real-world user instructions effectively.\nX\u2192 {(x1, C1, q1), ..., (Xn, Ci, qi)}, i\u2208N\nEvaluation Question Generation While Xu et al. (2023); Wang et al. (2023) focus on improving the complexity of instructions, omitting the quality of generated responses often leads to low-quality samples. Inspired by Qin et al. (2024), we utilize LLM to generate an evaluation question for each identified constraint. For the above example, the evaluation question (q1) would be \"Is the response written in Shakespeare's tone?\u201d. These questions are designed to be precise, enabling reliable assessment of generated responses for adherence to the constraints. This mechanism not only addresses the limitation of verifying only constraints that can be checked programmatically (Dong et al., 2024) but also enhances the reliability of the generated data and ensures alignment with the intent of the original instruction.\nUltraComposer Training With the decomposed instructions and evaluation questions, we train our UltraComposer to take a simplified query (xi) as input and generate the original instruction (X) with its evaluation question (qi), denoted as Eq. 2. The training process is shown in Figure 2\nUltraComposer(xi) \u2192 (X, qi), i\u2208N\n2.3. Generate-then-Evaluate\nAs shown in Figure 2, with UltraComposer, ULTRAIF efficiently generates high-quality instruction-following data through the Generate-then-Evaluate process, supporting both Supervised Fine-tuning and Preference Learning. This process comprises two key components, instruction generation and response evaluation. It begins by collecting user instructions from existing datasets, after which UltraComposer iteratively incorporates multiple constraints into each instruction, and then the corresponding evaluation questions are used to assess the responses generated. Thus, ULTRAIF could make any instruction more complex, facilitating a diverse and large-scale instruction-following dataset.\nInstrucion Generation The UltraComposer adapts the augmentation process fully automated and aligns with human preferences. We start by collecting user instructions from existing datasets (Chiang et al., 2023; Teknium, 2023;"}, {"title": "2.4. Training Strategies", "content": "ULTRAIF offers flexible training strategies for aligning model with instruction following capabilities. To thoroughly evaluate the effectiveness, we provide two approaches:\nSupervised Finetuing (SFT). Given the dataset Ddata, we apply standard Supervised Finetuning (SFT) objective on vanilla model \u03c0 with parameters \u03b8, as shown in Eq. 4:\nLSFT(\u03c0\u03bf) =\n\u2211\n(x,yc) EDdata log \u03c0\u03bf (Yc|x)\nwhere I represents the augmented instruction, and rc denotes the corresponding chosen response.\nSFT + Iterative Online DPO. As ULTRAIF is equipped with evaluation questions, it facilitates quality control by enabling the generation of pairwise responses with varying quality levels. This property makes it particularly suitable for the application of Direct Perference Optimization (DPO, Rafailov et al. (2024)) to refine the fine-tuned model, \u03c0ref. The DPO objective is formulated as Eq. 5:\nLDPO(\u03c0\u03b8, \u03c0ref) = -E(1,yc,yr)\u2208Ddata log \u03c3(\u03b2 \u00b7 \u0394)\n\u0394 = (log \u03c0\u03bf (Yc|x) - log \u03c0\u03bf(yr)) - (log \u03c0ref (Yc|x) - log \u03c0ref (Yr|x))\nwhere \u03b2 is a scaling hyperparameter, \u03c3 denotes the sigmoid function, and \u03c0\u03b8 is initialized from \u03c0ref and further optimized during the DPO stage.\nIn the context of ULTRAIF, the UltraComposer enables an iterative augmentation of instructions, transitioning from simpler to more complex tasks. This allows the DPO process to be formulated as an iterative curriculum. At each iteration, the model \u03c0ref is replaced with the latest optimized model from the previous stage. Concurrently, more challenging instruction-following datasets are generated and utilized for further training. This iterative approach ensures continuous improvement in model performance and adaptability across increasingly complex scenarios.\nMoreover, during the iterative process, as observed by (Chen et al., 2024), the DPO objective primarily focuses on optimizing the margin between the chosen and rejected samples, rather than directly maximizing the probability of chosen samples and minimizing that of the rejected ones. To address this, we employ the Noise Contrastive Estimation (NCA, Chen et al. (2024)) loss in the final iteration, and the objective is defined in Eq. 6:\nLNCA(\u03c0\u03bf, \u03c0ref) =\n- E(1,yc,yr)\u2208Ddata [10 log \u03c3(Blog \u03c0\u03bf (Yc|x)))\n\u2211\u03c0ref (Yc|x)\ny\u2208{Yc,Yr}\nlog \u03c3(-\u03b2log \u03c0\u03bf (y)\\\u03c0ref (YT))"}, {"title": "3. Experiments", "content": "3.1. Experimental Setup\nDatasets and Baselines To train UltraComposer, we decompose instructions from ShareGPT (Chiang et al., 2023) and generate corresponding evaluation questions by LLaMA-3.1-70B-Instruct. In our experiments, we collect human instructions from existing open-source datasets, including ShareGPT, OpenHermes2.5, and No Robots (Teknium, 2023; Rajani et al., 2023; Chiang et al., 2023), and employ UltraComposer to complicate instructions and then generate responses. For baselines, we reimplement existing methods using either public datasets (Sun et al., 2024a; Xu et al., 2023) or available implementations (Dong et al., 2024), and include a series of currently open and closed-source LLMs. More details are in Appendix A.\nExperimental Settings We first fine-tune LLaMA-3.1-8B-Instruct to build our UltraComposer. Subsequently, we explore two settings to implement our training strategies as written in \u00a72.4.\n\u2022 Strong-to-Weak. In this setting, knowledge is distilled from a larger model to a smaller one. For ULTRAIF, we"}, {"title": "3.2. Main Results", "content": "Table 1 shows the performance of ULTRAIF on five instruction-following benchmarks.\nULTRAIF Outperforms All Previous Methods In the Strong-to-Weak setting, ULTRAIF demonstrates performance that is comparable to or exceeds previous methods across all datasets. By fine-tuning on our generated data, ULTRAIF achieves substantial improvements, particularly on IFEval and Multi-IF. When compared to strong baselines like AutoIF (Dong et al., 2024), ULTRAIF achieves scores of 53.97 (Pr(S)) and 64.15 (Ins(S)) on IFEval and 81.91 (DRFR) on InfoBench, surpassing AutoIF by margins ranging from 1.29% to 6.84%. These results underscore ULTRAIF's capability to effectively follow instructions, even with lower training data, representing a significant advancement over state-of-the-art approaches.\nIterative DPO Boosts Performance Effectively As shown in Table 1, the iterative DPO process substantially enhances alignment with complex instructions. Specifically, in comparison to SFT, iterative DPO achieves an average improvement of 5% in the Strong-to-Weak setting and 3.8% in the Self-Alignment setting for multi-turn instruction-following tasks. Furthermore, this process enables ULTRAIF to surpass state-of-the-art methods in three benchmarks that require LLM-based evaluation, with an improvement of 1.5% on InfoBench, 4.6% on LiveBench, and 2.62% on FollowBench, demonstrating the importance of ULTRAIF in handling diverse instructions.\nSmaller Supervisor Yields Better Performance A comparison between the self-alignment and strong-to-weak settings reveals that the self-alignment setting, which employs a smaller model as a supervisor, achieves superior performance. This observation is consistent with the findings of Hui et al. (2024). Notably, it is particularly evident during the SFT stage, where self-alignment outperforms strong-to-weak on Multi-IF and LiveBench. Although the"}, {"title": "3.3. Cross-Domain Validation", "content": "To verify the generalizability of ULTRAIF, we evaluate ULTRAIF across four general domains, including coding, reasoning, mathematical problem solving, and conversational tasks. Table 2 presents the performance of ULTRAIF compared to AutoIF and LLaMA-3.1-8B-Instruct. While ULTRAIF shows slightly lower performance than AutoIF in the mathematical domain, it achieves significant improvements in coding and conversational tasks. Additionally, scaling the training data leads to remarkable performance gains, and the DPO stage consistently improves results across all domains. Notably, ULTRAIF significantly improves the general capabilities of models, particularly on the comprehensive LiveBench benchmark (White et al., 2024) and the ArenaHard (Li et al., 2024) conversational task. Specifically, it outperforms AutoIF by a notable margin of 4.2% on LiveBench, while achieving an impressive 15.4% improvement in conversational performance on ArenaHard. These results suggest that ULTRAIF facilitates the development of more general and versatile models, capable of addressing a wide range of tasks effectively."}, {"title": "4. Analysis", "content": "4.1. Impact of the Iterative DPO Process\nAs UltraComposer allows for iteratively adding constraints to instructions, we introduce the Iterative DPO process during training and gradually increase instruction complexity. Figure 3 illustrates the reward trajectories for the chosen and rejected samples during the DPO process. The reward"}, {"title": "4.2. Scalability of ULTRAIF", "content": "By capturing the distribution of real-world instructions, ULTRAIF facilitates the instruction augmentation process while reducing the risk of inconsistencies between added constraints and the original instructions. Table 4 reports the pass rate during dataset synthesis, where ULTRAIF significantly outperforms AutoIF. This indicates that for generating an equivalent amount of data, ULTRAIF reduces costs by a factor of three to five. Furthermore, during the rejection sampling stage, while AutoIF necessitates rigorous function-based filtering for instruction synthesis and response generation, ULTRAIF achieves this with a single LLM call, making it far more scalable and cost-efficient."}, {"title": "4.3. Extension of Self Alignment", "content": "In our main experiments, we distill knowledge from the Instruct version model to enhance the vanilla model, demonstrating the effectiveness of ULTRAIF. However, the potential for ULTRAIF to independently enhance a strong model like Cheng et al. (2024) has not yet been explored. In this section, we conduct experiments to investigate the self-improvement capabilities of ULTRAIF. Under the Self-Alignment setting, we use data generated by LLaMA-3.1-8B-Instruct to enable the model to train itself. As shown in Figure 5, ULTRAIF significantly boosts the performance of the strong model across different size of training data, even without a more powerful supervisor. Specifically, ULTRAIF improves performance on IFEval by 2.4%-5.9%, on Multi-IF by 3.74%-5.38%, on LiveBench by 2.5%, and on FollowBench by 2.29%, further validating the effectiveness of our approach."}, {"title": "4.4. Analysis of Multi-Turn Data", "content": "Building on prior work that emphasizes enhancing multi-turn instruction-following capabilities (Sun et al., 2024b; He et al., 2024), our analysis reveals that incorporating multi-turn data during the SFT stage significantly improves ULTRAIF's performance across various benchmarks. As shown in Table 5, the inclusion of multi-turn data results in performance gains of 3.01% on Multi-IF, 1.18% on InfoBench, and 5.10% on LiveBench, compared to the baseline SFT model without such data. These improvements highlight the critical role of multi-turn interactions in training, allowing"}, {"title": "4.5. Ablation Studies on ULTRAIF", "content": "The iterative augmentation capability of our UltraComposer raises a critical question for the SFT stage: should simple or complex instructions be prioritized for training? Figure 6 presents the results of using varying levels of instruction complexity during the SFT stage. The results demonstrate that as instruction complexity increases, performance correspondingly improves, reaching the peak after three iterations with an improvement ranging from 0.73% to 1.66%. Furthermore, we evaluate the effectiveness of our evaluation questions. Without filtering out low-quality responses, performance deteriorates significantly over 3.35%-5.36%. This mechanism becomes increasingly critical as instruction complexity grows, with the performance gap widening alongside the increasing complexity, underscoring the importance of this module in maintaining high-quality training data."}, {"title": "5. Related Work", "content": "5.1. Instruction Following\nInstruction following has become a key area in LLMs, with a focus on improving the ability to understand and execute complex human instructions. Early approaches (Wei et al., 2021; Rajani et al., 2023; Jiang et al., 2023) have relied heavily on curated datasets involving human-generated instructions and their corresponding responses. Recent work has turned to leveraging LLMs to automate the process. For example, Xu et al. (2023) prompts LLMs to evolve instructions through in-depth or in-breadth transformations. Sun et al. (2024a) directly guides LLMs to complicate them. But relying solely on LLMs leads to the inclusion of low-quality samples, as this is limited by LLM's instruction-evolving ability. Wang et al. (2023); Dong et al. (2024) introduce human priors such as verifiable constraints to ensure the quality of generated responses. However, they restrict the diversity of instructions compared to real-world scenarios.\nIn contrast, ULTRAIF decomposes real-world user instructions for both constraints and evaluation questions, then trains a composer model to synthesize diverse and complex instructions with correct responses, presenting an effective mechanism for generating instruction-following data.\n5.2. Perference Learning with Instruction Following\nPerference learning has gained prominence as a method to enhance instruction-following capabilities by refining models to generate high-quality responses based on feedback (Ouyang et al., 2022; Dong et al., 2024; Sun et al., 2024a; Gao et al., 2024). In this context, it is typically employed to optimize models that have been finetuned on instruction datasets, thus improving the accuracy and alignment in generating high-quality outputs. A common approach leverages reward signals, which may be human-provided or automatically generated, to guide the model's learning process. For instance, reinforcement learning from human feedback (RLHF) often employs Proximal Policy Optimization (PPO) to optimize models, but this approach requires ranked responses, which can be resource-intensive and dependent on human labor. Recent work by Rafailov et al. (2024) and Chen et al. (2024) introduces direct optimization of preferences, offering a solution to these challenges and reducing the need for extensive human feedback.\nULTRAIF enables the generation of evaluation questions to guide preference learning in a more structured and efficient manner. It complements direct preference optimization by providing a scalable approach to generating instruction-following data, reducing costs and enhancing the scalability for instruction-following tasks."}, {"title": "6. Conclusion", "content": "In this paper, we propose ULTRAIF, a scalable and effective approach for synthesizing high-quality instruction-following data. By decomposing human instructions into simplified queries, constraints, and corresponding evaluation questions, we train an UltraComposer that enables the efficient generation of constraint-aware instructions. Across two different settings, ULTRAIF demonstrates strong performance across five instruction-following benchmarks. Extensive experiments conducted on LLaMA-3.1-8B-Instruct further highlight ULTRAIF's potential for self-alignment. Most importantly, we are the first to optimize the LLaMA-3.1-8B-Base model to match the instruction-following capabilities of its Instruct counterpart, underscoing the effectiveness and potential of our approach."}, {"title": "Impact Statements", "content": "This paper introduces ULTRAIF, a scalable approach to enhance the instruction-following abilities of LLMs. By utilizing open-source data, it fosters transparency, reproducibility, and improved interpretability. From an ethical perspective, ULTRAIF reduces dependency on proprietary datasets but may influence areas such as misinformation and biased decision-making. ULTRAIF contributes to the democratization of LLM development, enhancing accessibility to advanced models. While potential societal implications exist, none present immediate ethical concerns beyond those commonly addressed in Machine Learning research."}, {"title": "C. Experimental Details", "content": "C.1. Implementation Details\nIn the SFT stage, we perform full fine-tuning with a learning rate of 1e-5. The maximum token length is set to 8192 and variable-length packing is enabled. We use AdamW (Loshchilov, 2017) as the optimizer with a warmup ratio of 0.03 and employ a LinearLR scheduler at the beginning, transitioning to CosineAnnealingLR towards the end.\nIn the DPO stage, the configuration is similar, with the only difference being a lower learning rate of 5e-7. Additionally, the beta parameter of DPO loss is set to 0.1.\nC.2. Prompts of ULTRAIF\nTo train our UltraComposer, we prompt LLM to perform Instruction Decomposition and Eval Question Generation.\nWe use the following prompt template to decompose human instructions:"}]}