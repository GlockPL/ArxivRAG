{"title": "Advances in Diffusion Models for Image Data Augmentation: A Review of Methods, Models, Evaluation Metrics and Future Research Directions", "authors": ["Panagiotis Alimisis", "Ioannis Mademlis", "Panagiotis Radoglou-Grammatikis", "Panagiotis Sarigiannidis", "Georgios Th. Papadopoulos"], "abstract": "Image data augmentation constitutes a critical methodology in modern computer vision tasks, since it can facilitate towards enhancing the diversity and quality of training datasets; thereby, improving the performance and robustness of machine learning models in downstream tasks. In parallel, augmentation approaches can also be used for editing/modifying a given image in a context- and semantics-aware way. Diffusion Models (DMs), which comprise one of the most recent and highly promising classes of methods in the field of generative Artificial Intelligence (AI), have emerged as a powerful tool for image data augmentation, capable of generating realistic and diverse images by learning the underlying data distribution. The current study realizes a systematic, comprehensive and in-depth review of DM-based approaches for image augmentation, covering a wide range of strategies, tasks and applications. In particular, a comprehensive analysis of the fundamental principles, model architectures and training strategies of DMs is initially performed. Subsequently, a taxonomy of the relevant image augmentation methods is introduced, focusing on techniques regarding semantic manipulation, personalization and adaptation, and application-specific augmentation tasks. Then, performance assessment methodologies and respective evaluation metrics are analyzed. Finally, current challenges and future research directions in the field are discussed.", "sections": [{"title": "1 Introduction", "content": "Modern computer vision has been dominated by the so-called Deep Learning (DL) paradigm, which relies on the use of large-scale on Deep Neural Networks (DNNs). DNNs have so far exhibited outstanding performance in a wide set of visual understanding tasks. However, this eminent visual interpretation and reasoning capability is accompanied by the increased need for ever larger and sufficiently diverse training datasets. On the other hand, as image analysis tasks become increasingly intricate and demanding, the ability of DNNs to generalize robustly is hindered by limitations in training data quantity, diversity, and potential bias. As a result, data requirements has emerged as a rather prominent topic, since a sufficient volume of training samples is essential for fully harnessing the capabilities of DNNs. On the contrary, real-world image datasets, especially regarding specific-targeted application domains, often suffer in these aspects, even to the point of containing perfectly correlated training images that are proven to be essentially redundant.\nImage augmentation constitutes a common and convenient way to mitigate issues stemming from dataset limitations, by automatically creating additional variants of each training image and utilizing them for enhancing the training set. Typically, the generated variants exhibit differences in appearance, but retain semantic content identical to that of the original image. Extending the training dataset with such synthetic images increases its diversity and improves, in many cases, the learning and recognition performance of DNNs that are being trained on it. This behaviour stems from image augmentation essentially acting as an additional regularizing mechanism while training the DNN and, as a result, helping to prevent overfitting.\nTraditional approaches for image augmentation, such as geometric transformations (e.g., image rotation, flip, crop, scaling, horizontal/vertical translation, squeezing, etc.) and color space adjustments or photometric transforms (e.g., blurring, sharpening, jittering, etc.) are still very common. Multiple transformations of this type can be composed together, so that an even wider set of augmented images can be generated from the original dataset. These methods leverage domain knowledge to produce synthetic examples similar to the initial ones. More recently proposed image augmentation methods in this general vein are a set of strategies for systematically corrupting the original images, in order to generate augmented variants. This category of methods includes, among others: a) 'mixup', which uses convex combinations of pairs of training images and their labels, b) 'cutout', which randomly masks square regions of an input image, c) 'cutmix', which randomly combines two training images by masking the first with a region of the second (and vice versa), and d) 'patchshuffle', which uses a kernel filter to randomly swap the pixel values in a sliding window.\nThe effectiveness though of the above-mentioned relatively simple and straightforward augmentation methods is being increasingly challenged by the complexity and variability of contemporary image analysis demands. Although such strategies can be effective in increasing data diversity for simple tasks, they are mostly unable to capture the underlying structure and complex relationships present in high-dimensional image data. Additionally, many of them require domain-specific knowledge and dataset-specific calibration, in order to be applied correctly. Moreover, the needs of DNNs for large training datasets and effective regularization are ever-growing, rendering image augmentation a critically important component of modern machine learning.\nUnlike traditional methods, which manipulate existing images to generate variants, Diffusion Models (DMs) can be readily exploited for image augmentation practices, by synthesizing new, realistically-looking and plausible images. DMs constitute a sophisticated class of generative DNNs that excel in implicitly modeling the underlying data generating distribution and the structure of complex images. This capability allows them to essentially sample fake novel images from their training dataset's distribution, which are simultaneously diverse, highly realistic and representative of unseen data scenarios, as they encompass subtle details and preserve the inherent structure of the original dataset. Thus, they can be directly utilized for meaningfully augmenting the latter.\nThe learning paradigm of DMs, which relies on iteratively applying noise to the training images and subsequently learning to reverse the process, has shown significant promise in image augmentation, when compared against competing generative models (e.g., Generative Adversarial Networks). Additionally, recent advancements in DMs enable the conditioning of the image synthesis process via class labels, textual descriptions, or input images. This level of user control allows for targeted image augmentation, generating images that fulfill specific requirements based on the task at hand.\nThe recent advancement in generative image synthesis through DMs and multi-modal strategies (e.g., text-conditioned image creation) has been complemented by the use of large-scale pretraining on massive datasets, in the vein of the Foundation Model (FM) trend. This approach has led to the availability of pretrained DMs that can generate images with natural variations in appearance (e.g., changing the design of the graffiti on a truck) and, hence, can be directly exploited for sophisticated image augmentation without significant human effort.\nDespite the recent progress and achievement on the application of DMs for advanced image augmentation, there are very few relevant surveys in the literature. Existing surveys either focus on traditional image augmentation or provide a general overview of DMs without delving into their specific application for image augmentation. For instance, the study of presents a comprehensive taxonomy of image augmentation approaches, including input space transformations, feature space augmentation, data synthesis and meta-learning based methods. However, it does not cover the latest advancements concerning the use of DMs. In contrast, the work of  presents three generic diffusion modeling frameworks, which are based on denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations, but does not explore their potential for image augmentation. Some surveys have touched upon the efficiency aspect of DMs or their application in specific domains like medical imaging. However, these works do not provide a comprehensive overview of DMs for augmentation across various computer vision tasks. Furthermore, a recent survey categorizes augmentation methods based on large learning models, including those based on DMs, but does not focus specifically on them.\nThe remainder of this article is structured in the following way. Section 2 outlines the foundations and underlying principles of DMs. Section 3 describes the different categories of DM-powered image augmentation methods, while Section 4 presents and explains these categories in depth. Section 5 surveys the various evaluation metrics used to assess the performance of DMs for image augmentation. Section 6 discusses the current challenges and limitations associated with the use of DMs for image augmentation. Finally, Section 7 draws insights from the preceding discussion, along with suggestions for future research directions."}, {"title": "2 Foundations of Diffusion Models", "content": "Diffusion Models (DMs) are a powerful class of generative models gaining significant traction in image synthesis. Inspired by non-equilibrium thermodynamics, they operate by incrementally destroying structure in the data, through an iterative process of adding Gaussian noise (forward diffusion) that progressively transforms the data distribution towards a distribution of pure random noise. Then, a learnable reverse diffusion process that restores structure in the data yields a tractable generative model. Thus, DMs are trained for gradually transforming random noise patterns into samples of the data generating distribution. This section details the principles underlying DMs, clarifying why they are particularly suited for visual data augmentation."}, {"title": "2.1 Forward Diffusion Process", "content": "The Forward Diffusion (FD) process is the cornerstone of DMs, as it corrupts the training dataset by sequentially inserting Gaussian noise. Assume that the initial data distribution is q(x0), where subscript\u20180' denotes the original/unmodified state of the dataset and 20 ~ q(xo) is an image from this dataset. FD proceeds as a sequence q of incrementally noised versions, X1,X2...,x, which are generated by a Markov chain. The conditional distribution for each step in this sequence, p(xt Xt\u22121), is modeled as a Gaussian N(xt; \u221a1 \u2013 Btxt\u22121, \u1e9etI), where t ranges from 1 to T and it denotes the total noise added to the input image. T corresponds to the total number of diffusion steps, \u03b21,..., \u03b2r is a sequence of variance parameters that define the noise level at each step, I is the identity matrix matching the dimensionality of input xo and N(x; \u03bc, \u03c3) denotes the normal distribution with mean \u00b5 and covariance \u03c3.\nA key attribute of FD is that at can be sampled at any arbitrary time step t in closed form, using a reparameterization trick:\nLet at = 1  \u03b2t, \u0101t = Hai\ni=1\nThen q(xt xo) = N(xt \u221a\u0101txo, (1 \u2013 \u0101t)I)\nxt = \u221a\u0101txo + \u221a1 \u2013 \u0101te,\nwhere integer t \u2208 [1, N] and \u2208 ~ N(0, I). Thus, the noisy version at can be directly obtained through a cumulative variance adjustment \u00dft, determined by sequence ai, where at = 1 - Bt. This allows one to compute any noisy version xt from the original image xo in a single step, without having to iteratively generate the noisy version of all intervening time steps."}, {"title": "2.2 Reverse Diffusion Process", "content": "Following the corruption introduced by FD, the iterative Reverse Diffusion (RD) process aims at recovering the original dataset images from their noisy versions. Instead of directly generating images from noise patterns, a denoising learning model, which can be a DNN, iteratively predicts the noise pattern added to the data at each individual step of the FD process, starting from the final FD output, so that it can be removed. Progressive denoising gradually refines the image across T consecutive steps. This is the so-called Denoising Diffusion Probabilistic Model (DDPM) formulation. Alternatively, the model can learn the so-called \u2018score function', which is the gradient of the log probability density function of the data with respect to the input. Then, the model's predictions at each time step can be used to iteratively sample from the distribution, by following the gradient. Such a DM variant is called Score-based Generative Model (SGM).\nThe employed predictive DNN is usually a U-Net CNN. Regarding the mathematical formulation, the RD process is defined as follows:\nT\n\u0440\u03b8(\u0445\u043e:\u0442) = p(x\u0442) \u041f\u0440\u043e(Xt\u22121|Xt),\nt=1\nwhere po(xt-1|Xt) = N(xt\u22121, \u03bc\u03bf(xt, t), 2(xt, t) .\nIn, the U-Net is trained by the following loss function:\nLsimple = Et,x0,\u20ac[||\u20ac \u2013 \u20ac9(xt, t)||2].\nwhere e represents the Gaussian noise added to image xo to obtain the noisy version xt and eo(xt, t) denotes the noise predicted by the DNN parameterized by 0, given the noisy image xt and the time step t. In the SGM formulation, eo(xt,t) is the predicted score and, thus, after training, po(xt, t) can be approximated by a function of ee(xt, t). Even though Lsimple does not offer a way to learn Eo(xt,t), it has been shown in that the best results are obtained by fixing the variance to \u03c3\u03b5I, rather than learning it.\nRD is an iterative process of T consecutive time steps, starting from noise pattern XT and gradually recovering the original image xo. At each time step t, \u00b5 and \u2211 are computed and a new version of the output image is generated, which subsequently serves as input for the next time step t. This need for sequential generation across T consecutive iterations is a significant limitation of DMs. One simple improvement is to reduce the number of sampling steps, from T to K evenly spaced real numbers between 1 and T. Alternatively, the non-Markovian Denoising Diffusion Implicit Models (DDIMs) sample only across S diffusion steps [t1,...,ts] \u2286 [1,T] during generation:"}, {"title": "2.3 Guidance", "content": "Classifier Guidance leverages a pretrained closed-set classifier to condition the RD process of a pretrained unconditional DM on a desired class label. The classifier model po(y|xt), where & denotes its parameters, supports as many different class labels y as the potential conditioning classes. With this approach and given the SGM formulation, the RD process is adjusted at each time step by the gradient of the log-probability \u2207xt log p\u00a2(y|x+) that steers sampling. Thus, \u03bc\u0473(xt, t) is approximated by:\n\u20ac0(xt, t) + s * \u2207xt log p$(y[xt),\nwhere s is a scaling factor controlling the strength of guidance. This method ensures that the generated samples conform to the target class distribution, without any need to retrain the unconditionally trained DNN.\nClassifier-Free Guidance eliminates the need for an explicit separate classifier model, by conditioning the DM on class labels directly during its training. The DM is trained with both conditional \u03b8e and unconditional \u03b8\u03c5 objectives, alternating between conditioning on labels and generating without labels. At inference time, guidance is implemented by interpolating between the conditional and unconditional scores, so that po(xt, t) is approximated as follows:\nVx+log po (xt y) = \u221ax+ log Poc(xt|y) + w(\u2207x+log Poc(xt|y) - \u221ax+log Pou(xt)),\nwhere w is a weight parameter that controls the strength of the guidance."}, {"title": "2.4 Diffusion Models in Latent Space", "content": "Despite the faster RD process of DDIMs, image generation in pixel space and in an arbitrary resolution remains a significant bottleneck. To this end, Latent Diffusion Models (LDMs) have been introduced that operate in a latent space, in order to significantly accelerate the generation process. In particular, an LDM relies on an external autoencoder pretrained on a large-scale dataset. Its encoder E learns to map images x \u2208 Dr into a special latent code z = E(x). Its decoder D learns to map such low-dimensional latent representations back to pixel space, so that D(E(x)) \u2248 x. Thus, a regular DM or DDIM is trained to generate codes within the latent space. The resulting code can be mapped back to a realistic, high-dimensional image via the pretrained D.\nThe LDM can be conditioned on class labels, segmentation masks, or even text, which guide the generation process. Let co (y) be a model that maps a raw conditioning input y to a conditioning vector\u00b9. The LDM loss is then formulated as:\nLLDM = Ez\u2208E(x),y,\u20ac\u2208N(0,1),t[||\u20ac \u2013 \u20ac9(zt, t, co(y))||2],\nwhere t is the time step, zt is the latent representation noised at step t, e is the unscaled noise sample, and es is the denoising network's prediction. Intuitively, the objective is to correctly remove the noise added to a latent representation of an image. During training, co and es are jointly optimized to minimize the LDM loss. At inference time, a random noise tensor is sampled and iteratively denoised to produce a new latent image zo."}, {"title": "2.5 Foundation Diffusion Models for Image Synthesis", "content": "Conditional LDMs have boosted the development of the 'Stable Diffusion' (SD) Foundation Model, a Text-to-Image generator (T2I) which was pretrained on the LAION-5B dataset. SD has dominated much of recent research related to generative image synthesis for image augmentation. Still, several attempts have been made to further improve it. For instance, 'Stable Diffusion XL' (SDXL) innovates over the basic SD in three ways:\n\u2022 It boasts a U-Net three times more complex and leverages a dual text encoding system for text conditioning. This new text encoder (OpenCLIP ViT-bigG/14), operating alongside the original one, significantly expands the model's capacity.\n\u2022 It enhances control over the final image crop, via the incorporation of size- and crop-conditioning during training. This is implemented by feeding crop parameters to the model as conditioning parameters via Fourier feature embeddings.\n\u2022 Its inference stage operates in two steps: a 'base' model generates an initial image, which is then fed to a 'refiner' model that adds finer, higher-quality details.\nAnother advanced variant is PixArt-a, which differentiates from SD in three aspects:\n\u2022 Training strategy decomposition: Three distinct training steps are devised to respectively optimize pixel dependency, text-image alignment, and image aesthetic quality.\n\u2022 Efficient T21 Transformer: Cross-attention modules are incorporated into the Diffusion Transformer (DiT) to inject text conditions and streamline the computation-intensive class-conditioning branch.\n\u2022 Highly informative data: Emphasizing the significance of concept density in text-image pairs, a large Vision-Language Model (VLM) is leveraged to auto-label dense pseudo-captions, assisting in text-image alignment learning.\nMore recently, the authors of  have proposed 'Stable Diffusion 3' (SD3), which relies on a Transformer neural architecture , instead of a U-Net Convolutional Neural Network (CNN). It utilizes a distinct set of parameters for the two involved modalities, namely text and images; thus, allowing for a two-way exchange of information between images and text tokens, making use of an attention mechanism."}, {"title": "3 Taxonomy of Diffusion Models for Image Augmentation", "content": "In this section, an overview of the landscape of DM-based methods for image augmentation is provided. In particular, a taxonomy of the various approaches is defined and graphically illustrated. More specifically, considering as a criterion the task/goal of each method, DM-powered image augmentation approaches can initially be classified in the following main categories (while each class can be further divided into sub-categories, as will be discussed later in this section):\n\u2022 Semantic Manipulation: The goal is to introduce fine-grained context-aware modifications to an image, while maintaining though its main semantic contents.\n\u2022 Personalization and Adaptation: The target is to alter the appearance of the image, so as to better conform to specific datasets, tasks, requirements or user preferences.\n\u2022 Application-Specific Augmentation: The goal is to regulate the augmentation process using domain specific knowledge, i.e. introducing modifications that are only meaningful for a given application (e.g., medical imaging, facial recognition, etc.)."}, {"title": "3.1 Semantic Manipulation", "content": "Methods belonging to this category aim to induce subtle and context-aware changes to an image, altering its interpretation or conveying additional contextual information, while preserving the core semantic content and maintaining a coherent and meaningful visual representation. Such methods are useful for generating realistic and diverse training samples, while maintaining the original image context. More fined-grained and detailed sub-classes of this category are:\n\u2022 Concept Manipulation: Concept manipulation involves altering the semantic content of an image, by adding, removing or modifying objects, attributes, or even the entire scene.\n\u2022 Text-Guided Editing: Text-guided editing leverages natural language descriptions to directly influence the editing process of images, allowing for precise control over the modifications based on textual inputs. This approach combines the strengths of Natural Language Processing (NLP) and computer vision technologies, enabling a nuanced interpretation of text into visual changes.\n\u2022 Layout and Region-Based Editing: Layout and region-based editing involves modifying specific areas or rearranging elements within an image to alter its composition or focus. These methods are crucial for applications that require precise control over spatial arrangements and detailed modifications to image content.\n\u2022 I2I (Image-to-Image) Translation: Image-to-Image (I2I) translation methods harness DMs to transform one source image into a different target one, maintaining the core content while altering its style, texture, or modal characteristics. This category is critical for applications ranging from artistic style transfer to functional medical imaging translations.\n\u2022 Counterfactual Augmentation: Counterfactual augmentation uses DMs to generate images that represent hypothetical scenarios or what-if analyses, often used for enhancing model explainability and robustness. This includes generating counterfactual scenarios, where key elements are altered to assess potential outcomes. Such methods are useful in fields such as medical imaging and policy-making, where understanding the impact of variable changes is crucial."}, {"title": "3.2 Personalization and Adaptation", "content": "Personalization and adaptation methods tailor the augmentation process to better suit specific datasets, tasks or user preferences. These methods enhance the relevance and effectiveness of augmented data, by finetuning models to align with particular requirements.\n\u2022 Personalization Methods: Personalization methods aim at adapting DMs to generate content that meets specific user needs or preferences. These approaches often involve finetuning models, leveraging textual or visual inputs, and optimizing for personalized outputs.\n\u2022 Adaptation Methods: Adaptation methods tailor DMs to different domains or tasks, improving their versatility and performance in new or varied contexts. These approaches are crucial for ensuring that models can generalize well across different datasets and applications.\n\u2022 Inversion-Based Methods: Inversion-based DM methods leverage the ability to invert the diffusion process for enhancing image editing and augmentation capabilities, by obtaining the original image from a noisy or perturbed version.\n\u2022 Dataset Expansion: Dataset expansion methods utilize DMs to synthetically generate additional images, given an original input dataset. The goal is to address the limitations of small-scale datasets and to enhance the diversity of training images. This is critical for improving the generalization and robustness of machine learning models, particularly when the acquisition of extensive labeled datasets is impractical."}, {"title": "3.3 Application-Specific Augmentation", "content": "Application-specific augmentation methods tailor the augmentation process so as to meet the unique requirements and properties of a given application domain, i.e. they extensively rely on using detailed and domain specific knowledge. Such particular characteristics can be exploited for improving or guiding the augmentation process. Typical domains with unique requirements or properties are medical imaging, facial recognition, fashion industry, agriculture, etc.\n\u2022 Medical Imaging: DMs for image augmentation are extensively employed to generate high-fidelity synthetic medical images, enhance existing datasets, and improve the robustness of diagnostic models. These methods address challenges such as data scarcity, variability in medical conditions, as well as the need for anonymized training data.\n\u2022 Other Domain-Specific Applications: DMs have also been effectively applied to a wide range of other domain-specific applications, taking into account the specific requirements of each field. Examples include facial recognition and editing, fashion industry, agriculture, etc."}, {"title": "4 DM-Powered Methods for Image Augmentation", "content": "This section details the fundamental principles and mechanisms of DM-based methods for image augmentation, based on the taxonomy of approaches discussed in Section 3. The main benefits of each (sub) category are highlighted; thus, providing key insights regarding their practical usage."}, {"title": "4.1 Semantic Manipulation", "content": "Semantic manipulation transforms the image appearance, while partially preserving its semantic content, or manipulates the depicted semantic concept, potential textual elements, or layout. As discussed in Section 3, its subcategories are concept manipulation, text-guided editing, layout and region-based editing, image-to-image (I2I) translation, and counterfactual augmentation. In many cases the goal is to transform a specific input 'reference' image to an augmented variant, a process generally called 'editing'."}, {"title": "4.1.1 Concept Manipulation", "content": "Several methods for concept manipulation specialize in placing objects within images using the pretrained SD model, often being themselves training-free. In particular, they condition SD on new objects specified by text prompts, Web-retrieved images or a high-frequency map of the target object, stitched with the scene at the desired location (generated from Contrastive Language-Image Pre-training (CLIP)) for placing them at the background of an input image. The new objects might be checked for semantic consistency via CLIP embedding similarity. For example, the method in  employs identity feature extraction (using a self-supervised DINOv2 model), detail feature extraction and feature injection to seamlessly integrate the target object into the scene, by feeding the ID tokens and the detail maps into the pretrained SD as guidance to generate the final composition. It also supports additional controls, like user-drawn masks to indicate the desired shape of the object during inference.\nThe method in adapts the pretrained SD model for realistic object integration into scenes. It uses a content adaptor module that maps visual features from the input image object to a text embedding space to condition the SD. It is first pretrained on image-text pairs to learn semantics and then finetuned with SD to preserve object appearance. The SD model takes in the background image and the object embedding to generate a composite image. An example is depicted, where various methods are compared on object adding at a specific location in the target image. The first column displays the desired object, while the second one the target image and the desired location of the object.\nThe methods 'Composer' and 'Stable Artist'  provide fine-grained control of the image generation process, by leveraging operations in the latent space. The 'Composer' decomposes an image into representative factors, such as text description, depth map, sketch, color histogram, etc., and trains a DM (Guided Language to Image Diffusion for Generation and Editing (GLIDE)) conditioned on these factors, allowing customizable content creation by recombining the aforementioned factors. 'Stable Artist' uses Semantic Guidance (SEGA) to steer the diffusion process along multiple semantic directions corresponding to editing prompts, enabling subtle edits as well as changes in composition and style without masks or finetuning. SEGA allows the user to control the latent space representation, by calculating guidance vectors between the noise estimates of the original prompt and the editing prompts, and applying these to shift the unconditional noise estimate.\nLarge T2I models like SD can also replicate undesired behavior or generate inappropriate content, such as copyrighted artworks or explicit images. To address these issues, several methods have been proposed, which can be organized in four main groups:\nImage Post-Processing: These methods filter out inappropriate content from the generated images after the generation process.\nInference Guidance: These methods guide the diffusion process during inference to avoid generating undesired concepts. For example, Safe Latent Diffusion defines an 'unsafe' textual concept and uses it to guide the diffusion process away from generating inappropriate content.\nImage Inpainting: These methods remove undesired objects or regions from images and inpaint the missing parts with appropriate content. They often use a mask to specify the region to be inpainted.\nModel Finetuning: These methods finetune the pretrained DM to prevent it from generating undesired concepts. They often use approaches like concept erasure, self-distillation, or degeneration-tuning to remove the unwanted concepts from the model's learned representations."}, {"title": "4.1.2 Text-Guided Editing", "content": "Unlike text-prompt-based concept manipulation, which directly uses the text prompt to guide the editing process, text-guided editing methods first optimize a text embedding to reconstruct the input reference image. The reconstruction is the output of a pretrained conditional LDM, i.e., the outcome of the RD process. Additionally, a target text embedding is the CLIP representation of a textual prompt, which is given as a condition to the LDM. In the end, these methods interpolate between the optimized embedding and the target text embedding, so that conditioning on this interpolated vector eventualy generates the desired edited/augmented image. For example, Imagic optimizes a text embedding, finetunes the DM to better reconstruct the input image, and then linearly interpolates between the optimized embedding and the target text embedding.\nSubsequently, this interpolated embedding is provided as a condition to the finetuned DM, so that it generates the desired edited augmented image.\nA different approach to influence how the image is built word-by-word is by incorporating cross-attention maps within the diffusion process. These maps determine which parts of the image ('pixels') focus on specific elements ('tokens') of the text prompt at different stages of image creation. Methods like and use cross-attention maps to align textual descriptions with image regions, enabling nuanced image alterations. Some methods focus on segmenting images into learnable regions that can be individually manipulated, based on text commands. For example, the methods of and divide the image into regions, by using pretrained models for feature extraction, and then apply text-guided editing to each region separately. This enhances the granularity of edits and allows for more precise control over specific parts of the image.\nIn general, editing images based on human instructions allows a higher level of control over the DM's actions. These methods typically utilize a Large Language Model (LLM) to guide the editing process via human-like instructions. For example, InstructPix2Pix  makes use of GPT-3 to generate a synthetic dataset of editing instructions (e.g., input caption: 'photograph of a girl riding a horse', edited caption: 'photograph of a girl riding a dragon') and subsequently a pretrained SD model is utilized to generate synthetic images matching the captions before/after. Subsequently, an SD model is trained on this generated dataset (by optimizing the LDM loss), so as to learn to perform image edits. Classifier-free guidance can also be used along with a text prompt to guide the diffusion process and to generate new images that comply better with their conditioning. This improves the alignment between the generated images and the text prompt, without requiring a separate classifier model.\nMoving towards a different direction, the method of  introduces a 'spatio-textual representation' that combines CLIP image embeddings of object segments during training and CLIP textual embeddings of local prompts during inference. This representation is incorporated into the DM, by concatenating it with the noisy image or latent code, and the DM is finetuned accordingly (by minimizing the LDM loss). Multi-conditional classifier-free guidance is employed to control the relative importance of each condition, involving a fine-grained variant using separate guidance scales and a fast variant using a single scale for the joint probability of all conditions.\nPaint-by-Example introduces an exemplar-based editing approach, where it automatically merges a reference image into a source one. Self-supervised training utilizes the object's bounding box as a binary mask and the image patch inside as the reference image to reconstruct the source image. An information bottleneck compresses the reference image with a CLIP class token and injects the decoded feature into the diffusion process for better content understanding. Strong augmentations reduce train-test mismatches and handle irregular mask shapes. Controllability is achieved by training with irregular masks and using a classifier-free sampling strategy to adjust similarity between the edited and reference images."}, {"title": "4.1.3 Layout and Region-Based Editing", "content": "Several methods use text prompts to guide the generation and manipulation of images, based on layout and region information. For example, the approaches of and enable semantic image synthesis and geometric control using text prompts. These methods often utilize a separate layout encoder to model spatial and semantic information into a format suitable for image generation. Indicatively, the latter consists of a precision-based mask pyramid that represents region shapes at multiple resolutions combined with text embeddings. An example of such an approach is depicted. In , the layout encoder translates geometric layouts into text prompts by mapping locations, classes, and conditions into text tokens, in order to extract spatial information from the input image's layout, and then conditions the DM on this layout encoding.\nControlNet is another powerful approach for layout and region-based editing. It introduces a DNN architecture that adds spatial conditioning controls to large, pretrained T2I DMs, like SD. ControlNet creates a trainable copy of the model's encoding layers and connects it to the original model using 'zero convolutions'. This allows for efficient finetuning on small datasets for various conditioning tasks, such as edge detection, pose estimation, and depth mapping. The architecture can process both text prompts and conditioning images (e.g., edge maps, pose maps, depth maps) as inputs, making it highly versatile for different types of spatial control in image generation and editing.\nCertain methods focus on generating images from coarse layouts or scribbles. For instance, the method of utilizes SD to generate freestyle images, by integrating semantic text embeddings with spatial layouts into SD. It represents each semantic class in the input layout using a text concept, which are then encoded into text embeddings. A Rectified Cross-Attention (RCA) module is subsequently introduced to inject these text semantics into the corresponding layout regions within the diffusion model's U-Net cross-attention layers. By finetuning just the U-Net with the integrated RCA on layout-image pairs, the pretrained model can generate images conditioned on both user-specified layouts and free-form text prompts, enabling capabilities like binding new attributes to objects and generating unseen object classes.\nThe method of utilizes ControlNet to generate synthetic images conditioned on scribble labels and text prompts. It employs classifier-free guidance (10% of the conditioning scribble inputs are randomly dropped and replaced with a learnable embedding) and introduces an encoding ratio to adjust the diversity and photorealism of the generated images (by performing fewer FD steps), allowing for a trade-off between mode coverage and sample fidelity.\nInpainting methods make use of DMs to fill-in missing regions of an image based on a given mask. They typically condition the DM on the masked image and the mask itself, and then generate content to fill-in the masked region. A subset of methods employs a multi-stage diffusion process with mask guidance, in order to achieve high-resolution image editing and inpainting.\nCollage Diffusion is a method that takes a user-defined sequence of layers as input, called Collage. It consists of a full-image text string describing the entire image to be generated, along with a sequence of layers ordered from back to front; each layer consists of an RGBA image (alpha-masked input image) and a text describing it. The method modifies the text-image cross-attention in the DM to achieve spatial fidelity, while extending ControlNet to preserve appearance fidelity on a per-layer basis. Collage Diffusion also allows users to control the harmonization-fidelity trade-off for each layer by specifying desired noise levels, enabling layer-by-layer image editing. Similarly, SmartBrush utilizes text and shape guidance for object inpainting with DMs, enabling users to control the inpainted content based on both text descriptions and object masks. Fast Composer is a tuning-free multi-subject image generation method that augments text prompts with visual features extracted from input images using an image encoder. It models text prompts and input images as embeddings, using pretrained CLIP encoders, and then uses an Multilayer Perceptron (MLP) to augment the text embeddings with the visual features. The method trains the image encoder, MLP module, and U-Net with a denoising loss using a subject-augmented image-text paired dataset, while localizing cross-attention maps using the reference subject's segmentation mask to prevent identity blending in multi-subject generation. It also employs delayed subject conditioning in iterative denoising to balance identity preservation and editability.\nThe method of is designed for allowing editing with pixel-level control over the amount of applied modification, using a 'change map'. The latter is a matrix of dimensions identical to the spatial resolution of the original input image, describing the strength of the edit to be applied at each location. Different approaches can be used to generate it, such as Segment-Anything, MiDas or even manually drawn change maps. The method can operate in latent space and has been evaluated on SD, SDXL, Kandinsky and DeepFloyd IF at StabilityAI pretrained LDMs, using text prompts (either manually created, or by reversing the input image into CLIP and BLIP) along with change maps to guide the inference process."}, {"title": "4.1.4 I2I (Image-to-Image) Translation", "content": "Several methods make use of conditional DMs for I2I translation. For example, SDEdit utilizes a conditional DM, guided by stochastic differential equations, to perform image editing. CycleNet and DiffusionCLIP  finetune a conditional DM with a CLIP-based loss to ensure that the gener-ated image matches the target image's text description. Palette et al. employ conditional DMs to learn the distribution p(y|x) for various tasks, like colorization, inpainting, JPEG restoration, and uncropping. The model incorporates a U-Net architecture with self-attention layers, conditioning on the input image through concatenation. During training, it predicts the noise added to the original image, minimizing L2 or L1 loss between the predicted and the actual noise. Then, the inference process involves iterative denoising over 1000 timesteps, starting from Gaussian noise and progressively refining the output. The method's strength lies in its ability to handle multiple tasks with a single architecture, eliminating the need for task-specific customizations.\nA different group of methods focus on disentangling style and content for more targeted I2I translation. For instance, the approach of"}]}