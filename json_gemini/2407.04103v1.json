{"title": "Advances in Diffusion Models for Image Data Augmentation: A Review of Methods, Models, Evaluation Metrics and Future Research Directions", "authors": ["Panagiotis Alimisis", "Ioannis Mademlis", "Panagiotis Radoglou-Grammatikis", "Panagiotis Sarigiannidis", "Georgios Th. Papadopoulos"], "abstract": "Image data augmentation constitutes a critical methodology in modern computer vision tasks, since it can facilitate towards enhancing the diversity and quality of training datasets; thereby, improving the performance and robustness of machine learning models in downstream tasks. In parallel, augmentation approaches can also be used for editing/modifying a given image in a context- and semantics-aware way. Diffusion Models (DMs), which comprise one of the most recent and highly promising classes of methods in the field of generative Artificial Intelligence (AI), have emerged as a powerful tool for image data augmentation, capable of generating realistic and diverse images by learning the underlying data distribution. The current study realizes a systematic, comprehensive and in-depth review of DM-based approaches for image augmentation, covering a wide range of strategies, tasks and applications. In particular, a comprehensive analysis of the fundamental principles, model architectures and training strategies of DMs is initially performed. Subsequently, a taxonomy of the relevant image augmentation methods is introduced, focusing on techniques regarding semantic manipulation, personalization and adaptation, and application-specific augmentation tasks. Then, performance assessment methodologies and respective evaluation metrics are analyzed. Finally, current challenges and future research directions in the field are discussed.", "sections": [{"title": "1 Introduction", "content": "Modern computer vision has been dominated by the so-called Deep Learning (DL) paradigm, which relies on the use of large-scale on Deep Neural Networks (DNNs). DNNs have so far exhibited outstanding performance in a wide set of visual understanding tasks. However, this eminent visual interpretation and reasoning capability is accompanied by the increased need for ever larger and sufficiently diverse training datasets. On the other hand, as image analysis tasks become increasingly intricate and demanding, the ability of DNNs to generalize robustly is hindered by limitations in training data quantity, diversity, and potential bias. As a result, data requirements has emerged as a rather prominent topic, since a sufficient volume of training samples is essential for fully harnessing the capabilities of DNNs. On the contrary, real-world image datasets, especially regarding specific-targeted application domains, often suffer in these aspects, even to the point of containing perfectly correlated training images that are proven to be essentially redundant.\nImage augmentation constitutes a common and convenient way to mitigate issues stemming from dataset limitations, by automatically creating additional variants of each training image and utilizing them for enhancing the training set. Typically, the generated variants exhibit differences in appearance, but retain semantic content identical to that of the original image. Extending the training dataset with such synthetic images increases its diversity and improves, in many cases, the learning and recognition performance of DNNs that are being trained on it. This behaviour stems from image augmentation essentially acting as an additional regularizing mechanism while training the DNN and, as a result, helping to prevent overfitting.\nTraditional approaches for image augmentation, such as geometric transformations (e.g., image rotation, flip, crop, scaling, horizontal/vertical translation, squeezing, etc.) and color space adjustments or photometric transforms (e.g., blurring, sharpening, jittering, etc.) are still very common. Multiple transformations of this type can be composed together, so that an even wider set of augmented images can be generated from the original dataset. These methods leverage domain knowledge to produce synthetic examples similar to the initial ones. More recently proposed image augmentation methods in this general vein are a set of strategies for systematically corrupting the original images, in order to generate augmented variants. This category of methods includes, among others: a) 'mixup', which uses convex combinations of pairs of training images and their labels, b) 'cutout', which randomly masks square regions of an input image, c) 'cutmix', which randomly combines two training images by masking the first with a region of the second (and vice versa), and d) 'patchshuffle', which uses a kernel filter to randomly swap the pixel values in a sliding window.\nThe effectiveness though of the above-mentioned relatively simple and straightforward augmentation methods is being increasingly challenged by the complexity and variability of contemporary image analysis demands. Although such strategies can be effective in increasing data diversity for simple tasks, they are mostly unable to capture the underlying structure and complex relationships present in high-dimensional image data. Additionally, many of them require domain-specific knowledge and dataset-specific calibration, in order to be applied correctly. Moreover, the needs of DNNs for large training datasets and effective regularization are ever-growing, rendering image augmentation a critically important component of modern machine learning.\nUnlike traditional methods, which manipulate existing images to generate variants, Diffusion Models (DMs) can be readily exploited for image augmentation practices, by synthesizing new, realistically-looking and plausible images. DMs constitute a sophisticated class of generative DNNs that excel in implicitly modeling the underlying data generating distribution and the structure of complex images. This capability allows them to essentially sample fake novel images from their training dataset's distribution, which are simultaneously diverse, highly realistic and representative of unseen data scenarios, as they encompass subtle details and preserve the inherent structure of the original dataset. Thus, they can be directly utilized for meaningfully augmenting the latter.\nThe learning paradigm of DMs, which relies on iteratively applying noise to the training images and subsequently learning to reverse the process, has shown significant promise in image augmentation, when compared against competing generative models (e.g., Generative Adversarial Networks). Additionally, recent advancements in DMs enable the conditioning of the image synthesis process via class labels, textual descriptions, or input images. This level of user control allows for targeted image augmentation, generating images that fulfill specific requirements based on the task at hand.\nThe recent advancement in generative image synthesis through DMs and multimodal strategies (e.g., text-conditioned image creation) has been complemented by the use of large-scale pretraining on massive datasets, in the vein of the Foundation Model (FM) trend. This approach has led to the availability of pretrained DMs that can generate images with natural variations in appearance (e.g., changing the design of the graffiti on a truck) and, hence, can be directly exploited for sophisticated image augmentation without significant human effort.\nDespite the recent progress and achievement on the application of DMs for advanced image augmentation, there are very few relevant surveys in the literature. Existing surveys either focus on traditional image augmentation or provide a general overview of DMs without delving into their specific application for image augmentation. For instance, the study of presents a comprehensive taxonomy of image augmentation approaches, including input space transformations, feature space augmentation, data synthesis and meta-learning based methods. However, it does not cover the latest advancements concerning the use of DMs. In contrast, the work of presents three generic diffusion modeling frameworks, which are based on denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations, but does not explore their potential for image augmentation. Some surveys have touched upon the efficiency aspect of DMs or their application in specific domains like medical imaging. However, these works do not provide a comprehensive overview of DMs for augmentation across various computer vision tasks. Furthermore, a recent survey categorizes augmentation methods based on large learning models, including those based on DMs, but does not focus specifically on them.\nThe remainder of this article is structured in the following way. Section 2 outlines the foundations and underlying principles of DMs. Section 3 describes the different categories of DM-powered image augmentation methods, while Section 4 presents and explains these categories in depth. Section 5 surveys the various evaluation metrics used to assess the performance of DMs for image augmentation. Section 6 discusses the current challenges and limitations associated with the use of DMs for image augmentation. Finally, Section 7 draws insights from the preceding discussion, along with suggestions for future research directions."}, {"title": "2 Foundations of Diffusion Models", "content": "Diffusion Models (DMs) are a powerful class of generative models gaining significant traction in image synthesis. Inspired by non-equilibrium thermodynamics, they operate by incrementally destroying structure in the data, through an iterative process of adding Gaussian noise (forward diffusion) that progressively transforms the data distribution towards a distribution of pure random noise. Then, a learnable reverse diffusion process that restores structure in the data yields a tractable generative model. Thus, DMs are trained for gradually transforming random noise patterns into samples of the data generating distribution. This section details the principles underlying DMs, clarifying why they are particularly suited for visual data augmentation."}, {"title": "2.1 Forward Diffusion Process", "content": "The Forward Diffusion (FD) process is the cornerstone of DMs, as it corrupts the training dataset by sequentially inserting Gaussian noise. Assume that the initial data distribution is $q(x_0)$, where subscript\u20180' denotes the original/unmodified state of the dataset and $x_0 \\sim q(x_0)$ is an image from this dataset. FD proceeds as a sequence $q$ of incrementally noised versions, $x_1, x_2..., x_T$, which are generated by a Markov chain. The conditional distribution for each step in this sequence, $p(x_t | x_{t-1})$, is modeled as a Gaussian $N(x_t; \\sqrt{1 \u2013 \u03b2_t}x_{t-1}, \u03b2_tI)$, where $t$ ranges from 1 to $T$ and it denotes the total noise added to the input image. $T$ corresponds to the total number of diffusion steps, $\u03b2_1,..., \u03b2_T$ is a sequence of variance parameters that define the noise level at each step, $I$ is the identity matrix matching the dimensionality of input $x_0$ and $N(x; \u03bc, \u03c3)$ denotes the normal distribution with mean $\u00b5$ and covariance $\u03c3$.\nA key attribute of FD is that $x_t$ can be sampled at any arbitrary time step $t$ in closed form, using a reparameterization trick:\nLet $\u03b1_t = 1 \u2013 \u03b2_t$, $\u0101_t = \\prod_{i=1}^{t} \u03b1_i$\nThen $q(x_t|x_0) = N(x_t; \\sqrt{\u0101_t}x_0, (1 \u2013 \u0101_t)I)$\n$x_t = \\sqrt{\u0101_t}x_0 + \\sqrt{1 \u2013 \u0101_t}\u03b5, \t{where} \\  \u03b5 \\sim N(0, I)$   (1)\nwhere integer $t \u2208 [1, N]$ and $\u03b5 \u223c N(0, I)$. Thus, the noisy version $x_t$ can be directly obtained through a cumulative variance adjustment $\u03b2_t$, determined by sequence $\u03b1_i$, where $\u03b1_t = 1 - \u03b2_t$. This allows one to compute any noisy version $x_t$ from the original image $x_0$ in a single step, without having to iteratively generate the noisy version of all intervening time steps."}, {"title": "2.2 Reverse Diffusion Process", "content": "Following the corruption introduced by FD, the iterative Reverse Diffusion (RD) process aims at recovering the original dataset images from their noisy versions. Instead of directly generating images from noise patterns, a denoising learning model, which can be a DNN, iteratively predicts the noise pattern added to the data at each individual step of the FD process, starting from the final FD output, so that it can be removed. Progressive denoising gradually refines the image across $T$ consecutive steps. This is the so-called Denoising Diffusion Probabilistic Model (DDPM) formulation. Alternatively, the model can learn the so-called \u2018score function', which is the gradient of the log probability density function of the data with respect to the input. Then, the model's predictions at each time step can be used to iteratively sample from the distribution, by following the gradient. Such a DM variant is called Score-based Generative Model (SGM).\nThe employed predictive DNN is usually a U-Net CNN. Regarding the mathematical formulation, the RD process is defined as follows:\n$p_\u03b8(x_{0:T}) = p(x_T) \\prod_{t=1}^{T} p_\u03b8(x_{t-1}|x_t),$(2)\nwhere $p_\u03b8(x_{t-1}|x_t) = N(x_{t-1}, \u03bc_\u03b8(x_t, t), \u03a3_\u03b8(x_t, t))$.\nIn Ho et al (2020), the U-Net is trained by the following loss function:\n$L_{simple} = E_{t,x_0,\u03b5}[||\u03b5 \u2013 \u03b5_\u03b8(x_t, t)||^2].$(3)\nwhere $\u03b5$ represents the Gaussian noise added to image $x_0$ to obtain the noisy version $x_t$ and $\u03b5_\u03b8(x_t, t)$ denotes the noise predicted by the DNN parameterized by $\u03b8$, given the noisy image $x_t$ and the time step $t$. In the SGM formulation, $\u03b5_\u03b8(x_t, t)$ is the predicted score and, thus, after training, $p_\u03b8(x_t, t)$ can be approximated by a function of $\u03b5_\u03b8(x_t, t)$. Even though $L_{simple}$ does not offer a way to learn $\u03a3_\u03b8(x_t, t)$, it has been shown in that the best results are obtained by fixing the variance to $\u03c3^2_\u03b5I$, rather than learning it.\nRD is an iterative process of $T$ consecutive time steps, starting from noise pattern $x_T$ and gradually recovering the original image $x_0$. At each time step $t$, $\u00b5$ and $\u03a3$ are computed and a new version of the output image is generated, which subsequently serves as input for the next time step $t$. This need for sequential generation across $T$ consecutive iterations is a significant limitation of DMs. One simple improvement is to reduce the number of sampling steps, from $T$ to $K$ evenly spaced real numbers between 1 and $T$ . Alternatively, the non-Markovian Denoising Diffusion Implicit Models (DDIMs) sample only across $S$ diffusion steps $[t_1,..., t_S] \u2286 [1,T]$ during generation:"}, {"title": "2.3 Guidance", "content": "Classifier Guidance leverages a pretrained closed-set classifier to condition the RD process of a pretrained unconditional DM on a desired class label. The classifier model $p_\u03d5(y|x_t)$, where $\u03d5$ denotes its parameters, supports as many different class labels $y$ as the potential conditioning classes. With this approach and given the SGM formulation, the RD process is adjusted at each time step by the gradient of the log-probability $\u2207_{x_t} log p_\u03d5(y|x_t)$ that steers sampling. Thus, $\u00b5_\u03b8(x_t, t)$ is approximated by:\n$\u03b5_\u03b8(x_t, t) + s * \u2207_{x_t} log p_\u03d5(y[x_t),$(5)\nwhere $s$ is a scaling factor controlling the strength of guidance. This method ensures that the generated samples conform to the target class distribution, without any need to retrain the unconditionally trained DNN.\nClassifier-Free Guidance eliminates the need for an explicit separate classifier model, by conditioning the DM on class labels directly during its training. The DM is trained with both conditional $\u03b8_c$ and unconditional $\u03b8_u$ objectives, alternating between conditioning on labels and generating without labels. At inference time, guidance is implemented by interpolating between the conditional and unconditional scores, so that $p_\u03b8(x_t, t)$ is approximated as follows:\n$\u2207_{x_t}log p_\u03b8(x_t|y) = \u2207_{x_t} log p_{\u03b8c}(x_t|y) + w(\u2207_{x_t}log p_{\u03b8c}(x_t|y) - \u2207_{x_t}log p_{\u03b8u}(x_t)),$(6)\nwhere $w$ is a weight parameter that controls the strength of the guidance."}, {"title": "2.4 Diffusion Models in Latent Space", "content": "Despite the faster RD process of DDIMs, image generation in pixel space and in an arbitrary resolution remains a significant bottleneck. To this end, Latent Diffusion Models (LDMs) have been introduced that operate in a latent space, in order to significantly accelerate the generation process. In particular, an LDM relies on an external autoencoder pretrained on a large-scale dataset. Its encoder $E$ learns to map images $x \u2208 D_r$ into a special latent code $z = E(x)$. Its decoder $D$ learns to map such low-dimensional latent representations back to pixel space, so that $D(E(x)) \u2248 x$. Thus, a regular DM or DDIM is trained to generate codes within the latent space. The resulting code can be mapped back to a realistic, high-dimensional image via the pretrained D.\nThe LDM can be conditioned on class labels, segmentation masks, or even text, which guide the generation process. Let $c_\u03b8(y)$ be a model that maps a raw conditioning input $y$ to a conditioning vector. The LDM loss is then formulated as:\n$L_{LDM} = E_{z\u2208E(x),y,\u03b5\u2208N(0,1),t}[||\u03b5 \u2013 \u03b5_\u03b8(z_t, t, c_\u03b8(y))||^2],$(7)\nwhere $t$ is the time step, $z_t$ is the latent representation noised at step $t$, $\u03b5$ is the unscaled noise sample, and $\u03b5_\u03b8$ is the denoising network's prediction. Intuitively, the objective is to correctly remove the noise added to a latent representation of an image. During training, $c_\u03b8$ and $\u03b5_\u03b8$ are jointly optimized to minimize the LDM loss. At inference time, a random noise tensor is sampled and iteratively denoised to produce a new latent image $z_0$."}, {"title": "2.5 Foundation Diffusion Models for Image Synthesis", "content": "Conditional LDMs have boosted the development of the 'Stable Diffusion' (SD) Foundation Model, a Text-to-Image generator (T2I) which was pretrained on the LAION-5B dataset. SD has dominated much of recent research related to generative image synthesis for image augmentation. Still, several attempts have been made to further improve it. For instance, 'Stable Diffusion XL' (SDXL) innovates over the basic SD in three ways:\n\u2022 It boasts a U-Net three times more complex and leverages a dual text encoding system for text conditioning. This new text encoder (OpenCLIP ViT-bigG/14), operating alongside the original one, significantly expands the model's capacity.\n\u2022 It enhances control over the final image crop, via the incorporation of size- and crop-conditioning during training. This is implemented by feeding crop parameters to the model as conditioning parameters via Fourier feature embeddings.\n\u2022 Its inference stage operates in two steps: a 'base' model generates an initial image, which is then fed to a 'refiner' model that adds finer, higher-quality details.\nAnother advanced variant is PixArt-a, which differentiates from SD in three aspects:\n\u2022 Training strategy decomposition: Three distinct training steps are devised to respectively optimize pixel dependency, text-image alignment, and image aesthetic quality.\n\u2022 Efficient T21 Transformer: Cross-attention modules are incorporated into the Diffusion Transformer (DiT) to inject text conditions and streamline the computation-intensive class-conditioning branch.\n\u2022 Highly informative data: Emphasizing the significance of concept density in text-image pairs, a large Vision-Language Model (VLM) is leveraged to auto-label dense pseudo-captions, assisting in text-image alignment learning.\nMore recently, the authors of have proposed 'Stable Diffusion 3' (SD3), which relies on a Transformer neural architecture, instead of a U-Net Convolutional Neural Network (CNN). It utilizes a distinct set of parameters for the two involved modalities, namely text and images; thus, allowing for a two-way exchange of information between images and text tokens, making use of an attention mechanism."}, {"title": "3 Taxonomy of Diffusion Models for Image Augmentation", "content": "In this section, an overview of the landscape of DM-based methods for image augmentation is provided. In particular, a taxonomy of the various approaches is defined and graphically illustrated in Fig. 2. More specifically, considering as a criterion the task/goal of each method, DM-powered image augmentation approaches can initially be classified in the following main categories (while each class can be further divided into sub-categories, as will be discussed later in this section):\n\u2022 Semantic Manipulation: The goal is to introduce fine-grained context-aware modifications to an image, while maintaining though its main semantic contents.\n\u2022 Personalization and Adaptation: The target is to alter the appearance of the image, so as to better conform to specific datasets, tasks, requirements or user preferences.\n\u2022 Application-Specific Augmentation: The goal is to regulate the augmentation process using domain specific knowledge, i.e. introducing modifications that are only meaningful for a given application (e.g., medical imaging, facial recognition, etc.)."}, {"title": "3.1 Semantic Manipulation", "content": "Methods belonging to this category aim to induce subtle and context-aware changes to an image, altering its interpretation or conveying additional contextual information, while preserving the core semantic content and maintaining a coherent and meaningful visual representation. Such methods are useful for generating realistic and diverse training samples, while maintaining the original image context. More fined-grained and detailed sub-classes of this category are:\n\u2022 Concept Manipulation: Concept manipulation involves altering the semantic content of an image, by adding, removing or modifying objects, attributes, or even the entire scene.\n\u2022 Text-Guided Editing: Text-guided editing leverages natural language descriptions to directly influence the editing process of images, allowing for precise control over the modifications based on textual inputs. This approach combines the strengths of Natural Language Processing (NLP) and computer vision technologies, enabling a nuanced interpretation of text into visual changes.\n\u2022 Layout and Region-Based Editing: Layout and region-based editing involves modifying specific areas or rearranging elements within an image to alter its composition or focus. These methods are crucial for applications that require precise control over spatial arrangements and detailed modifications to image content."}, {"title": "3.2 Personalization and Adaptation", "content": "Personalization and adaptation methods tailor the augmentation process to better suit specific datasets, tasks or user preferences. These methods enhance the relevance and effectiveness of augmented data, by finetuning models to align with particular requirements.\n\u2022 Personalization Methods: Personalization methods aim at adapting DMs to generate content that meets specific user needs or preferences. These approaches often involve finetuning models, leveraging textual or visual inputs, and optimizing for personalized outputs.\n\u2022 Adaptation Methods: Adaptation methods tailor DMs to different domains or tasks, improving their versatility and performance in new or varied contexts. These approaches are crucial for ensuring that models can generalize well across different datasets and applications.\n\u2022 Inversion-Based Methods: Inversion-based DM methods leverage the ability to invert the diffusion process for enhancing image editing and augmentation capabilities, by obtaining the original image from a noisy or perturbed version.\n\u2022 Dataset Expansion: Dataset expansion methods utilize DMs to synthetically generate additional images, given an original input dataset. The goal is to address the limitations of small-scale datasets and to enhance the diversity of training images. This is critical for improving the generalization and robustness of machine learning models, particularly when the acquisition of extensive labeled datasets is impractical."}, {"title": "3.3 Application-Specific Augmentation", "content": "Application-specific augmentation methods tailor the augmentation process so as to meet the unique requirements and properties of a given application domain, i.e. they extensively rely on using detailed and domain specific knowledge. Such particular characteristics can be exploited for improving or guiding the augmentation process. Typical domains with unique requirements or properties are medical imaging, facial recognition, fashion industry, agriculture, etc.\n\u2022 Medical Imaging: DMs for image augmentation are extensively employed to generate high-fidelity synthetic medical images, enhance existing datasets, and improve the robustness of diagnostic models. These methods address challenges such as data scarcity, variability in medical conditions, as well as the need for anonymized training data.\n\u2022 Other Domain-Specific Applications: DMs have also been effectively applied to a wide range of other domain-specific applications, taking into account the specific requirements of each field. Examples include facial recognition and editing, fashion industry, agriculture, etc."}, {"title": "4 DM-Powered Methods for Image Augmentation", "content": "This section details the fundamental principles and mechanisms of DM-based methods for image augmentation, based on the taxonomy of approaches discussed in Section 3. The main benefits of each (sub) category are highlighted; thus, providing key insights regarding their practical usage."}, {"title": "4.1 Semantic Manipulation", "content": "Semantic manipulation transforms the image appearance, while partially preserving its semantic content, or manipulates the depicted semantic concept, potential textual elements, or layout. As discussed in Section 3, its subcategories are concept manipulation, text-guided editing, layout and region-based editing, image-to-image (I2I) translation, and counterfactual augmentation. In many cases the goal is to transform a specific input 'reference' image to an augmented variant, a process generally called 'editing'."}, {"title": "4.1.1 Concept Manipulation", "content": "Several methods for concept manipulation specialize in placing objects within images using the pretrained SD model, often being themselves training-free. In particular, they condition SD on new objects specified by text prompts, Web-retrieved images or a high-frequency map of the target object, stitched with the scene at the desired location (generated from Contrastive Language-Image Pre-training (CLIP)) for placing them at the background of an input image. The new objects might be checked for semantic consistency via CLIP embedding similarity. For example, the method in employs identity feature extraction (using a self-supervised DINOv2 model), detail feature extraction and feature injection to seamlessly integrate the target object into the scene, by feeding the ID tokens and the detail maps into the pretrained SD as guidance to generate the final composition. It also supports additional controls, like user-drawn masks to indicate the desired shape of the object during inference."}, {"title": "4.1.2 Text-Guided Editing", "content": "Unlike text-prompt-based concept manipulation, which directly uses the text prompt to guide the editing process, text-guided editing methods first optimize a text embedding to reconstruct the input reference image. The reconstruction is the output of a pretrained conditional LDM, i.e., the outcome of the RD process. Additionally, a target text embedding is the CLIP representation of a textual prompt, which is given as a condition to the LDM. In the end, these methods interpolate between the optimized embedding and the target text embedding, so that conditioning on this interpolated vector eventualy generates the desired edited/augmented image. For example, Imagic optimizes a text embedding, finetunes the DM to better reconstruct the input image, and then linearly interpolates between the optimized embedding and the target text embedding."}, {"title": "4.1.3 Layout and Region-Based Editing", "content": "Several methods use text prompts to guide the generation and manipulation of images, based on layout and region information. For example, the approaches of and enable semantic image synthesis and geometric control using text prompts. These methods often utilize a separate layout encoder to model spatial and semantic information into a format suitable for image generation. Indicatively, the latter consists of a precision-based mask pyramid that represents region shapes at multiple resolutions combined with text embeddings in. In, the layout encoder translates geometric layouts into text prompts by mapping locations, classes, and conditions into text tokens, in order to extract spatial information from the input image's layout, and then conditions the DM on this layout encoding.\nControlNet is another powerful approach for layout and region-based editing. It introduces a DNN architecture that adds spatial conditioning controls to large, pretrained T2I DMs, like SD. ControlNet creates a trainable copy of the model's encoding layers and connects it to the original model using 'zero convolutions'. This allows for efficient finetuning on small datasets for various conditioning tasks, such as edge detection, pose estimation, and depth mapping. The architecture can process both text prompts and conditioning images (e.g., edge maps, pose maps, depth maps) as inputs, making it highly versatile for different types of spatial control in image generation and editing.\nCertain methods focus on generating images from coarse layouts or scribbles. For instance, the method of utilizes SD to generate freestyle images, by integrating semantic text embeddings with spatial layouts into SD. It represents each semantic class in the input layout using a text concept, which are then encoded into text embeddings. A Rectified Cross-Attention (RCA) module is subsequently intro-duced to inject these text semantics into the corresponding layout regions within the diffusion model's U-Net cross-attention layers. By finetuning just the U-Net with the integrated RCA on layout-image pairs, the pretrained model can generate images conditioned on both user-specified layouts and free-form text prompts, enabling capabilities like binding new attributes to objects and generating unseen object classes.\nThe method of utilizes ControlNet to generate synthetic images conditioned on scribble labels and text prompts. It employs classifier-free guidance (10% of the conditioning scribble inputs are randomly dropped and replaced with a learnable embedding) and introduces an encoding ratio to adjust the diversity and photorealism of the generated images (by performing fewer FD steps), allowing for a trade-off between mode coverage and sample fidelity.\nInpainting methods make use of DMs to fill-in missing regions of an image based on a given mask. They typically condition the DM on the masked image and the mask itself, and then generate content to fill-in the masked region. A subset of methods employs a multi-stage diffusion process with mask guidance, in order to achieve high-resolution image editing and inpainting.\nCollage Diffusion is a method that takes a user-defined sequence of layers as input, called Collage. It consists of a full-image text string describing the entire image to be generated, along with a sequence of layers ordered from back to front; each layer consists of an RGBA image (alpha-masked input image) and a text describing it. The method modifies the text-image cross-attention in the DM to achieve spatial fidelity, while extending ControlNet to preserve appearance fidelity on a per-layer basis. Collage Diffusion also allows users to control the harmonization-fidelity trade-off for each layer by specifying desired noise levels, enabling layer-by-layer image editing. Similarly, SmartBrush utilizes text and shape guidance for object inpainting with DMs, enabling users to control the inpainted content based on both text descriptions and object masks. Fast Composer is a tuning-free multi-subject image generation method that augments text prompts"}, {"title": "4.1.4 I2I (Image-to-Image) Translation", "content": "Several methods make use of conditional DMs for I2I translation. For example, SDEdit utilizes a conditional DM, guided by stochastic differential equations, to perform image editing. CycleNet and DiffusionCLIP finetune a conditional DM with a CLIP-based loss to ensure that the generated image matches the target image's text description. Palette employ conditional DMs to learn the distribution $p(y|x)$ for various tasks, like colorization, inpainting, JPEG restoration, and uncropping. The model incorporates a U-Net architecture with self-attention layers, conditioning on the input image through concatenation. During training, it predicts the noise added to the original image, minimizing L2 or L1 loss between the predicted and the actual noise. Then, the inference process involves iterative denoising over 1000 timesteps, starting from Gaussian noise and progressively refining the output. The method's strength lies in its ability to handle multiple tasks with a single architecture, eliminating the need for task-specific customizations.\nA different group of methods focus on disentangling style and content for more targeted I2I translation. For instance, the approach of makes use of separate encoders for style and content, and then injects these representations into the DM to generate the translated image. Other methods explore few-shot or zero-shot I2I translation. For example, pix2pix-zero first inverts the input image to obtain a noise map, using DDIM and BLIP. These noise maps are then regularized to improve editability, using an autocorrelation objective. Edit directions are automatically discovered by generating diverse sentences for the source and target domains, and computing the mean difference between their CLIP embeddings. In order to preserve image structure during editing, a novel cross-attention guidance technique"}, {"title": "4.1.5 Counterfactual Augmentation", "content": "A significant number of methods make use of conditional DMs to generate counterfactual images. For instance, the approach of utilizes a conditional DM to generate 'healthy' counterfactuals of brain MRI images with tumors, by conditioning the model on the tumor mask. Diff-SCM employs a conditional DM to estimate the effects of interventions in a causal framework, by conditioning the model on the intervention variable do(class), corresponding to \u2018how the image should change in order to be classified as another class'. For instance, for a given photo of a dog in a park, the dog should be replaced by a cat, while leaving the rest of the image unchanged if do(cat) holds.\nA great portion of methods focus on generating counterfactuals for specific applications. For example, MEDJOURNEY generates counterfactual medical images by conditioning the DM on a text description of the desired change, while the methods of and generate counterfactual images to explain the model's behavior and to improve its robustness. On the other hand, a different set of approaches utilize counterfactual augmentation to address bias and fairness issues in datasets and models. For example, the work of utilizes a conditional DM to generate counterfactual images that balance the distribution of sensitive attributes (e.g., gender, race) in the dataset. The model is conditioned on the desired attribute distribution and generates images that match it, while preserving the remaining aspects of the image.\nAnother set of methods focus on expanding a given dataset with generated counterfactual or Out-of-Distribution (OOD) examples, in order to improve the robustness of a model (e.g., classifier) trained on the expanded dataset. For example, the method of utilizes a conditional DM to generate counterfactual examples that change specific attributes of the input image (e.g., object position, color, texture), while preserving the overall scene structure. These counterfactual examples are used to diagnose and mitigate model failures on OOD data."}, {"title": "4.2 Personalization and Adaptation", "content": "Personalization and adaptation constitute very common methodologies in image augmentation tasks. In the remainder of this subsection, respective DM-based approaches are detailed, grouped according to the taxonomy presented in Section 3."}, {"title": "4.2.1 Personalization Methods", "content": "Personalization implies that the DM must be adapted to generate content that meets specific user needs or preferences. These methods often involve finetuning pretrained models", "A [V] dog": ".", "textual inversion": "o personalize T2I generation. This process learns a new text embedding (termed 'pseudo-word') that represents a specific visual concept", "prompt spectrum": "that capture different visual attributes (e.g."}, {"title": "Advances in Diffusion Models for Image Data Augmentation: A Review of Methods, Models, Evaluation Metrics and Future Research Directions", "authors": ["Panagiotis Alimisis", "Ioannis Mademlis", "Panagiotis Radoglou-Grammatikis", "Panagiotis Sarigiannidis", "Georgios Th. Papadopoulos"], "abstract": "Image data augmentation constitutes a critical methodology in modern computer vision tasks, since it can facilitate towards enhancing the diversity and quality of training datasets; thereby, improving the performance and robustness of machine learning models in downstream tasks. In parallel, augmentation approaches can also be used for editing/modifying a given image in a context- and semantics-aware way. Diffusion Models (DMs), which comprise one of the most recent and highly promising classes of methods in the field of generative Artificial Intelligence (AI), have emerged as a powerful tool for image data augmentation, capable of generating realistic and diverse images by learning the underlying data distribution. The current study realizes a systematic, comprehensive and in-depth review of DM-based approaches for image augmentation, covering a wide range of strategies, tasks and applications. In particular, a comprehensive analysis of the fundamental principles, model architectures and training strategies of DMs is initially performed. Subsequently, a taxonomy of the relevant image augmentation methods is introduced, focusing on techniques regarding semantic manipulation, personalization and adaptation, and application-specific augmentation tasks. Then, performance assessment methodologies and respective evaluation metrics are analyzed. Finally, current challenges and future research directions in the field are discussed.", "sections": [{"title": "1 Introduction", "content": "Modern computer vision has been dominated by the so-called Deep Learning (DL) paradigm, which relies on the use of large-scale on Deep Neural Networks (DNNs). DNNs have so far exhibited outstanding performance in a wide set of visual understanding tasks. However, this eminent visual interpretation and reasoning capability is accompanied by the increased need for ever larger and sufficiently diverse training datasets. On the other hand, as image analysis tasks become increasingly intricate and demanding, the ability of DNNs to generalize robustly is hindered by limitations in training data quantity, diversity, and potential bias. As a result, data requirements has emerged as a rather prominent topic, since a sufficient volume of training samples is essential for fully harnessing the capabilities of DNNs. On the contrary, real-world image datasets, especially regarding specific-targeted application domains, often suffer in these aspects, even to the point of containing perfectly correlated training images that are proven to be essentially redundant.\nImage augmentation constitutes a common and convenient way to mitigate issues stemming from dataset limitations, by automatically creating additional variants of each training image and utilizing them for enhancing the training set. Typically, the generated variants exhibit differences in appearance, but retain semantic content identical to that of the original image. Extending the training dataset with such synthetic images increases its diversity and improves, in many cases, the learning and recognition performance of DNNs that are being trained on it. This behaviour stems from image augmentation essentially acting as an additional regularizing mechanism while training the DNN and, as a result, helping to prevent overfitting.\nTraditional approaches for image augmentation, such as geometric transformations (e.g., image rotation, flip, crop, scaling, horizontal/vertical translation, squeezing, etc.) and color space adjustments or photometric transforms (e.g., blurring, sharpening, jittering, etc.) are still very common. Multiple transformations of this type can be composed together, so that an even wider set of augmented images can be generated from the original dataset. These methods leverage domain knowledge to produce synthetic examples similar to the initial ones. More recently proposed image augmentation methods in this general vein are a set of strategies for systematically corrupting the original images, in order to generate augmented variants. This category of methods includes, among others: a) 'mixup', which uses convex combinations of pairs of training images and their labels, b) 'cutout', which randomly masks square regions of an input image, c) 'cutmix', which randomly combines two training images by masking the first with a region of the second (and vice versa), and d) 'patchshuffle', which uses a kernel filter to randomly swap the pixel values in a sliding window.\nThe effectiveness though of the above-mentioned relatively simple and straightforward augmentation methods is being increasingly challenged by the complexity and variability of contemporary image analysis demands. Although such strategies can be effective in increasing data diversity for simple tasks, they are mostly unable to capture the underlying structure and complex relationships present in high-dimensional image data. Additionally, many of them require domain-specific knowledge and dataset-specific calibration, in order to be applied correctly. Moreover, the needs of DNNs for large training datasets and effective regularization are ever-growing, rendering image augmentation a critically important component of modern machine learning.\nUnlike traditional methods, which manipulate existing images to generate variants, Diffusion Models (DMs) can be readily exploited for image augmentation practices, by synthesizing new, realistically-looking and plausible images. DMs constitute a sophisticated class of generative DNNs that excel in implicitly modeling the underlying data generating distribution and the structure of complex images. This capability allows them to essentially sample fake novel images from their training dataset's distribution, which are simultaneously diverse, highly realistic and representative of unseen data scenarios. Thus, they can be directly utilized for meaningfully augmenting the latter.\nThe learning paradigm of DMs, which relies on iteratively applying noise to the training images and subsequently learning to reverse the process, has shown significant promise in image augmentation, when compared against competing generative models (e.g., Generative Adversarial Networks). Additionally, recent advancements in DMs enable the conditioning of the image synthesis process via class labels, textual descriptions, or input images. This level of user control allows for targeted image augmentation, generating images that fulfill specific requirements based on the task at hand.\nThe recent advancement in generative image synthesis through DMs and multimodal strategies (e.g., text-conditioned image creation) has been complemented by the use of large-scale pretraining on massive datasets, in the vein of the Foundation Model (FM) trend. This approach has led to the availability of pretrained DMs that can generate images with natural variations in appearance (e.g., changing the design of the graffiti on a truck) and, hence, can be directly exploited for sophisticated image augmentation without significant human effort.\nDespite the recent progress and achievement on the application of DMs for advanced image augmentation, there are very few relevant surveys in the literature. Existing surveys either focus on traditional image augmentation or provide a general overview of DMs without delving into their specific application for image augmentation. For instance, the study of presents a comprehensive taxonomy of image augmentation approaches, including input space transformations, feature space augmentation, data synthesis and meta-learning based methods. However, it does not cover the latest advancements concerning the use of DMs. In contrast, the work of presents three generic diffusion modeling frameworks, which are based on denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations, but does not explore their potential for image augmentation. Some surveys have touched upon the efficiency aspect of DMs or their application in specific domains like medical imaging. However, these works do not provide a comprehensive overview of DMs for augmentation across various computer vision tasks. Furthermore, a recent survey categorizes augmentation methods based on large learning models, including those based on DMs, but does not focus specifically on them.\nThe remainder of this article is structured in the following way. Section 2 outlines the foundations and underlying principles of DMs. Section 3 describes the different categories of DM-powered image augmentation methods, while Section 4 presents and explains these categories in depth. Section 5 surveys the various evaluation metrics used to assess the performance of DMs for image augmentation. Section 6 discusses the current challenges and limitations associated with the use of DMs for image augmentation. Finally, Section 7 draws insights from the preceding discussion, along with suggestions for future research directions."}, {"title": "2 Foundations of Diffusion Models", "content": "Diffusion Models (DMs) are a powerful class of generative models gaining significant traction in image synthesis. Inspired by non-equilibrium thermodynamics, they operate by incrementally destroying structure in the data, through an iterative process of adding Gaussian noise (forward diffusion) that progressively transforms the data distribution towards a distribution of pure random noise. Then, a learnable reverse diffusion process that restores structure in the data yields a tractable generative model. Thus, DMs are trained for gradually transforming random noise patterns into samples of the data generating distribution. This section details the principles underlying DMs, clarifying why they are particularly suited for visual data augmentation."}, {"title": "2.1 Forward Diffusion Process", "content": "The Forward Diffusion (FD) process is the cornerstone of DMs, as it corrupts the training dataset by sequentially inserting Gaussian noise. Assume that the initial data distribution is $q(x_0)$, where subscript\u20180' denotes the original/unmodified state of the dataset and $x_0 \\sim q(x_0)$ is an image from this dataset. FD proceeds as a sequence $q$ of incrementally noised versions, $x_1, x_2..., x_T$, which are generated by a Markov chain. The conditional distribution for each step in this sequence, $p(x_t | x_{t-1})$, is modeled as a Gaussian $N(x_t; \\sqrt{1 \u2013 \u03b2_t}x_{t-1}, \u03b2_tI)$, where $t$ ranges from 1 to $T$ and it denotes the total noise added to the input image. $T$ corresponds to the total number of diffusion steps, $\u03b2_1,..., \u03b2_T$ is a sequence of variance parameters that define the noise level at each step, $I$ is the identity matrix matching the dimensionality of input $x_0$ and $N(x; \u03bc, \u03c3)$ denotes the normal distribution with mean $\u00b5$ and covariance $\u03c3$.\nA key attribute of FD is that $x_t$ can be sampled at any arbitrary time step $t$ in closed form, using a reparameterization trick:\nLet $\u03b1_t = 1 \u2013 \u03b2_t$, $\u0101_t = \\prod_{i=1}^{t} \u03b1_i$\nThen $q(x_t|x_0) = N(x_t; \\sqrt{\u0101_t}x_0, (1 \u2013 \u0101_t)I)$\n$x_t = \\sqrt{\u0101_t}x_0 + \\sqrt{1 \u2013 \u0101_t}\u03b5, \t{where} \\  \u03b5 \\sim N(0, I)$   (1)\nwhere integer $t \u2208 [1, N]$ and $\u03b5 \u223c N(0, I)$. Thus, the noisy version $x_t$ can be directly obtained through a cumulative variance adjustment $\u03b2_t$, determined by sequence $\u03b1_i$, where $\u03b1_t = 1 - \u03b2_t$. This allows one to compute any noisy version $x_t$ from the original image $x_0$ in a single step, without having to iteratively generate the noisy version of all intervening time steps."}, {"title": "2.2 Reverse Diffusion Process", "content": "Following the corruption introduced by FD, the iterative Reverse Diffusion (RD) process aims at recovering the original dataset images from their noisy versions. Instead of directly generating images from noise patterns, a denoising learning model, which can be a DNN, iteratively predicts the noise pattern added to the data at each individual step of the FD process, starting from the final FD output, so that it can be removed. Progressive denoising gradually refines the image across $T$ consecutive steps. This is the so-called Denoising Diffusion Probabilistic Model (DDPM) formulation. Alternatively, the model can learn the so-called \u2018score function', which is the gradient of the log probability density function of the data with respect to the input. Then, the model's predictions at each time step can be used to iteratively sample from the distribution, by following the gradient. Such a DM variant is called Score-based Generative Model (SGM).\nThe employed predictive DNN is usually a U-Net CNN. Regarding the mathematical formulation, the RD process is defined as follows:\n$p_\u03b8(x_{0:T}) = p(x_T) \\prod_{t=1}^{T} p_\u03b8(x_{t-1}|x_t),$(2)\nwhere $p_\u03b8(x_{t-1}|x_t) = N(x_{t-1}, \u03bc_\u03b8(x_t, t), \u03a3_\u03b8(x_t, t))$.\nIn Ho et al (2020), the U-Net is trained by the following loss function:\n$L_{simple} = E_{t,x_0,\u03b5}[||\u03b5 \u2013 \u03b5_\u03b8(x_t, t)||^2].$(3)\nwhere $\u03b5$ represents the Gaussian noise added to image $x_0$ to obtain the noisy version $x_t$ and $\u03b5_\u03b8(x_t, t)$ denotes the noise predicted by the DNN parameterized by $\u03b8$, given the noisy image $x_t$ and the time step $t$. In the SGM formulation, $\u03b5_\u03b8(x_t, t)$ is the predicted score and, thus, after training, $p_\u03b8(x_t, t)$ can be approximated by a function of $\u03b5_\u03b8(x_t, t)$. Even though $L_{simple}$ does not offer a way to learn $\u03a3_\u03b8(x_t, t)$, it has been shown in that the best results are obtained by fixing the variance to $\u03c3^2_\u03b5I$, rather than learning it.\nRD is an iterative process of $T$ consecutive time steps, starting from noise pattern $x_T$ and gradually recovering the original image $x_0$. At each time step $t$, $\u00b5$ and $\u03a3$ are computed and a new version of the output image is generated, which subsequently serves as input for the next time step $t$. This need for sequential generation across $T$ consecutive iterations is a significant limitation of DMs. One simple improvement is to reduce the number of sampling steps, from $T$ to $K$ evenly spaced real numbers between 1 and $T$ . Alternatively, the non-Markovian Denoising Diffusion Implicit Models (DDIMs) sample only across $S$ diffusion steps $[t_1,..., t_S] \u2286 [1,T]$ during generation:"}, {"title": "2.3 Guidance", "content": "Classifier Guidance leverages a pretrained closed-set classifier to condition the RD process of a pretrained unconditional DM on a desired class label. The classifier model $p_\u03d5(y|x_t)$, where $\u03d5$ denotes its parameters, supports as many different class labels $y$ as the potential conditioning classes. With this approach and given the SGM formulation, the RD process is adjusted at each time step by the gradient of the log-probability $\u2207_{x_t} log p_\u03d5(y|x_t)$ that steers sampling. Thus, $\u00b5_\u03b8(x_t, t)$ is approximated by:\n$\u03b5_\u03b8(x_t, t) + s * \u2207_{x_t} log p_\u03d5(y[x_t),$(5)\nwhere $s$ is a scaling factor controlling the strength of guidance. This method ensures that the generated samples conform to the target class distribution, without any need to retrain the unconditionally trained DNN.\nClassifier-Free Guidance eliminates the need for an explicit separate classifier model, by conditioning the DM on class labels directly during its training. The DM is trained with both conditional $\u03b8_c$ and unconditional $\u03b8_u$ objectives, alternating between conditioning on labels and generating without labels. At inference time, guidance is implemented by interpolating between the conditional and unconditional scores, so that $p_\u03b8(x_t, t)$ is approximated as follows:\n$\u2207_{x_t}log p_\u03b8(x_t|y) = \u2207_{x_t} log p_{\u03b8c}(x_t|y) + w(\u2207_{x_t}log p_{\u03b8c}(x_t|y) - \u2207_{x_t}log p_{\u03b8u}(x_t)),$(6)\nwhere $w$ is a weight parameter that controls the strength of the guidance."}, {"title": "2.4 Diffusion Models in Latent Space", "content": "Despite the faster RD process of DDIMs, image generation in pixel space and in an arbitrary resolution remains a significant bottleneck. To this end, Latent Diffusion Models (LDMs) have been introduced that operate in a latent space, in order to significantly accelerate the generation process. In particular, an LDM relies on an external autoencoder pretrained on a large-scale dataset. Its encoder $E$ learns to map images $x \u2208 D_r$ into a special latent code $z = E(x)$. Its decoder $D$ learns to map such low-dimensional latent representations back to pixel space, so that $D(E(x)) \u2248 x$. Thus, a regular DM or DDIM is trained to generate codes within the latent space. The resulting code can be mapped back to a realistic, high-dimensional image via the pretrained D.\nThe LDM can be conditioned on class labels, segmentation masks, or even text, which guide the generation process. Let $c_\u03b8(y)$ be a model that maps a raw conditioning input $y$ to a conditioning vector. The LDM loss is then formulated as:\n$L_{LDM} = E_{z\u2208E(x),y,\u03b5\u2208N(0,1),t}[||\u03b5 \u2013 \u03b5_\u03b8(z_t, t, c_\u03b8(y))||^2],$(7)\nwhere $t$ is the time step, $z_t$ is the latent representation noised at step $t$, $\u03b5$ is the unscaled noise sample, and $\u03b5_\u03b8$ is the denoising network's prediction. Intuitively, the objective is to correctly remove the noise added to a latent representation of an image. During training, $c_\u03b8$ and $\u03b5_\u03b8$ are jointly optimized to minimize the LDM loss. At inference time, a random noise tensor is sampled and iteratively denoised to produce a new latent image $z_0$."}, {"title": "2.5 Foundation Diffusion Models for Image Synthesis", "content": "Conditional LDMs have boosted the development of the 'Stable Diffusion' (SD) Foundation Model, a Text-to-Image generator (T2I) which was pretrained on the LAION-5B dataset. SD has dominated much of recent research related to generative image synthesis for image augmentation. Still, several attempts have been made to further improve it. For instance, 'Stable Diffusion XL' (SDXL) innovates over the basic SD in three ways:\n\u2022 It boasts a U-Net three times more complex and leverages a dual text encoding system for text conditioning. This new text encoder (OpenCLIP ViT-bigG/14), operating alongside the original one, significantly expands the model's capacity.\n\u2022 It enhances control over the final image crop, via the incorporation of size- and crop-conditioning during training. This is implemented by feeding crop parameters to the model as conditioning parameters via Fourier feature embeddings.\n\u2022 Its inference stage operates in two steps: a 'base' model generates an initial image, which is then fed to a 'refiner' model that adds finer, higher-quality details.\nAnother advanced variant is PixArt-a, which differentiates from SD in three aspects:\n\u2022 Training strategy decomposition: Three distinct training steps are devised to respectively optimize pixel dependency, text-image alignment, and image aesthetic quality.\n\u2022 Efficient T21 Transformer: Cross-attention modules are incorporated into the Diffusion Transformer (DiT) to inject text conditions and streamline the computation-intensive class-conditioning branch.\n\u2022 Highly informative data: Emphasizing the significance of concept density in text-image pairs, a large Vision-Language Model (VLM) is leveraged to auto-label dense pseudo-captions, assisting in text-image alignment learning.\nMore recently, the authors of have proposed 'Stable Diffusion 3' (SD3), which relies on a Transformer neural architecture, instead of a U-Net Convolutional Neural Network (CNN). It utilizes a distinct set of parameters for the two involved modalities, namely text and images; thus, allowing for a two-way exchange of information between images and text tokens, making use of an attention mechanism."}, {"title": "3 Taxonomy of Diffusion Models for Image Augmentation", "content": "In this section, an overview of the landscape of DM-based methods for image augmentation is provided. In particular, a taxonomy of the various approaches is defined and graphically illustrated in Fig. 2. More specifically, considering as a criterion the task/goal of each method, DM-powered image augmentation approaches can initially be classified in the following main categories (while each class can be further divided into sub-categories, as will be discussed later in this section):\n\u2022 Semantic Manipulation: The goal is to introduce fine-grained context-aware modifications to an image, while maintaining though its main semantic contents.\n\u2022 Personalization and Adaptation: The target is to alter the appearance of the image, so as to better conform to specific datasets, tasks, requirements or user preferences.\n\u2022 Application-Specific Augmentation: The goal is to regulate the augmentation process using domain specific knowledge, i.e. introducing modifications that are only meaningful for a given application (e.g., medical imaging, facial recognition, etc.)."}, {"title": "3.1 Semantic Manipulation", "content": "Methods belonging to this category aim to induce subtle and context-aware changes to an image, altering its interpretation or conveying additional contextual information, while preserving the core semantic content and maintaining a coherent and meaningful visual representation. Such methods are useful for generating realistic and diverse training samples, while maintaining the original image context. More fined-grained and detailed sub-classes of this category are:\n\u2022 Concept Manipulation: Concept manipulation involves altering the semantic content of an image, by adding, removing or modifying objects, attributes, or even the entire scene.\n\u2022 Text-Guided Editing: Text-guided editing leverages natural language descriptions to directly influence the editing process of images, allowing for precise control over the modifications based on textual inputs. This approach combines the strengths of Natural Language Processing (NLP) and computer vision technologies, enabling a nuanced interpretation of text into visual changes.\n\u2022 Layout and Region-Based Editing: Layout and region-based editing involves modifying specific areas or rearranging elements within an image to alter its composition or focus. These methods are crucial for applications that require precise control over spatial arrangements and detailed modifications to image content."}, {"title": "3.2 Personalization and Adaptation", "content": "Personalization and adaptation methods tailor the augmentation process to better suit specific datasets, tasks or user preferences. These methods enhance the relevance and effectiveness of augmented data, by finetuning models to align with particular requirements.\n\u2022 Personalization Methods: Personalization methods aim at adapting DMs to generate content that meets specific user needs or preferences. These approaches often involve finetuning models, leveraging textual or visual inputs, and optimizing for personalized outputs.\n\u2022 Adaptation Methods: Adaptation methods tailor DMs to different domains or tasks, improving their versatility and performance in new or varied contexts. These approaches are crucial for ensuring that models can generalize well across different datasets and applications.\n\u2022 Inversion-Based Methods: Inversion-based DM methods leverage the ability to invert the diffusion process for enhancing image editing and augmentation capabilities, by obtaining the original image from a noisy or perturbed version.\n\u2022 Dataset Expansion: Dataset expansion methods utilize DMs to synthetically generate additional images, given an original input dataset. The goal is to address the limitations of small-scale datasets and to enhance the diversity of training images. This is critical for improving the generalization and robustness of machine learning models, particularly when the acquisition of extensive labeled datasets is impractical."}, {"title": "3.3 Application-Specific Augmentation", "content": "Application-specific augmentation methods tailor the augmentation process so as to meet the unique requirements and properties of a given application domain, i.e. they extensively rely on using detailed and domain specific knowledge. Such particular characteristics can be exploited for improving or guiding the augmentation process. Typical domains with unique requirements or properties are medical imaging, facial recognition, fashion industry, agriculture, etc.\n\u2022 Medical Imaging: DMs for image augmentation are extensively employed to generate high-fidelity synthetic medical images, enhance existing datasets, and improve the robustness of diagnostic models. These methods address challenges such as data scarcity, variability in medical conditions, as well as the need for anonymized training data.\n\u2022 Other Domain-Specific Applications: DMs have also been effectively applied to a wide range of other domain-specific applications, taking into account the specific requirements of each field. Examples include facial recognition and editing, fashion industry, agriculture, etc."}, {"title": "4 DM-Powered Methods for Image Augmentation", "content": "This section details the fundamental principles and mechanisms of DM-based methods for image augmentation, based on the taxonomy of approaches discussed in Section 3. The main benefits of each (sub) category are highlighted; thus, providing key insights regarding their practical usage."}, {"title": "4.1 Semantic Manipulation", "content": "Semantic manipulation transforms the image appearance, while partially preserving its semantic content, or manipulates the depicted semantic concept, potential textual elements, or layout. As discussed in Section 3, its subcategories are concept manipulation, text-guided editing, layout and region-based editing, image-to-image (I2I) translation, and counterfactual augmentation. In many cases the goal is to transform a specific input 'reference' image to an augmented variant, a process generally called 'editing'."}, {"title": "4.1.1 Concept Manipulation", "content": "Several methods for concept manipulation specialize in placing objects within images using the pretrained SD model, often being themselves training-free. In particular, they condition SD on new objects specified by text prompts, Web-retrieved images or a high-frequency map of the target object, stitched with the scene at the desired location (generated from Contrastive Language-Image Pre-training (CLIP)) for placing them at the background of an input image. The new objects might be checked for semantic consistency via CLIP embedding similarity. For example, the method in employs identity feature extraction (using a self-supervised DINOv2 model), detail feature extraction and feature injection to seamlessly integrate the target object into the scene, by feeding the ID tokens and the detail maps into the pretrained SD as guidance to generate the final composition. It also supports additional controls, like user-drawn masks to indicate the desired shape of the object during inference."}, {"title": "4.1.2 Text-Guided Editing", "content": "Unlike text-prompt-based concept manipulation, which directly uses the text prompt to guide the editing process, text-guided editing methods first optimize a text embedding to reconstruct the input reference image. The reconstruction is the output of a pretrained conditional LDM, i.e., the outcome of the RD process. Additionally, a target text embedding is the CLIP representation of a textual prompt, which is given as a condition to the LDM. In the end, these methods interpolate between the optimized embedding and the target text embedding, so that conditioning on this interpolated vector eventualy generates the desired edited/augmented image. For example, Imagic optimizes a text embedding, finetunes the DM to better reconstruct the input image, and then linearly interpolates between the optimized embedding and the target text embedding."}, {"title": "4.1.3 Layout and Region-Based Editing", "content": "Several methods use text prompts to guide the generation and manipulation of images, based on layout and region information. For example, the approaches of and enable semantic image synthesis and geometric control using text prompts. These methods often utilize a separate layout encoder to model spatial and semantic information into a format suitable for image generation. Indicatively, the latter consists of a precision-based mask pyramid that represents region shapes at multiple resolutions combined with text embeddings in. In, the layout encoder translates geometric layouts into text prompts by mapping locations, classes, and conditions into text tokens, in order to extract spatial information from the input image's layout, and then conditions the DM on this layout encoding.\nControlNet is another powerful approach for layout and region-based editing. It introduces a DNN architecture that adds spatial conditioning controls to large, pretrained T2I DMs, like SD. ControlNet creates a trainable copy of the model's encoding layers and connects it to the original model using 'zero convolutions'. This allows for efficient finetuning on small datasets for various conditioning tasks, such as edge detection, pose estimation, and depth mapping. The architecture can process both text prompts and conditioning images (e.g., edge maps, pose maps, depth maps) as inputs, making it highly versatile for different types of spatial control in image generation and editing.\nCertain methods focus on generating images from coarse layouts or scribbles. For instance, the method of utilizes SD to generate freestyle images, by integrating semantic text embeddings with spatial layouts into SD. It represents each semantic class in the input layout using a text concept, which are then encoded into text embeddings. A Rectified Cross-Attention (RCA) module is subsequently intro-duced to inject these text semantics into the corresponding layout regions within the diffusion model's U-Net cross-attention layers. By finetuning just the U-Net with the integrated RCA on layout-image pairs, the pretrained model can generate images conditioned on both user-specified layouts and free-form text prompts, enabling capabilities like binding new attributes to objects and generating unseen object classes.\nThe method of utilizes ControlNet to generate synthetic images conditioned on scribble labels and text prompts. It employs classifier-free guidance (10% of the conditioning scribble inputs are randomly dropped and replaced with a learnable embedding) and introduces an encoding ratio to adjust the diversity and photorealism of the generated images (by performing fewer FD steps), allowing for a trade-off between mode coverage and sample fidelity.\nInpainting methods make use of DMs to fill-in missing regions of an image based on a given mask. They typically condition the DM on the masked image and the mask itself, and then generate content to fill-in the masked region. A subset of methods employs a multi-stage diffusion process with mask guidance, in order to achieve high-resolution image editing and inpainting.\nCollage Diffusion is a method that takes a user-defined sequence of layers as input, called Collage. It consists of a full-image text string describing the entire image to be generated, along with a sequence of layers ordered from back to front; each layer consists of an RGBA image (alpha-masked input image) and a text describing it. The method modifies the text-image cross-attention in the DM to achieve spatial fidelity, while extending ControlNet to preserve appearance fidelity on a per-layer basis. Collage Diffusion also allows users to control the harmonization-fidelity trade-off for each layer by specifying desired noise levels, enabling layer-by-layer image editing. Similarly, SmartBrush utilizes text and shape guidance for object inpainting with DMs, enabling users to control the inpainted content based on both text descriptions and object masks. Fast Composer is a tuning-free multi-subject image generation method that augments text prompts"}, {"title": "4.1.4 I2I (Image-to-Image) Translation", "content": "Several methods make use of conditional DMs for I2I translation. For example, SDEdit utilizes a conditional DM, guided by stochastic differential equations, to perform image editing. CycleNet and DiffusionCLIP finetune a conditional DM with a CLIP-based loss to ensure that the generated image matches the target image's text description. Palette employ conditional DMs to learn the distribution $p(y|x)$ for various tasks, like colorization, inpainting, JPEG restoration, and uncropping. The model incorporates a U-Net architecture with self-attention layers, conditioning on the input image through concatenation. During training, it predicts the noise added to the original image, minimizing L2 or L1 loss between the predicted and the actual noise. Then, the inference process involves iterative denoising over 1000 timesteps, starting from Gaussian noise and progressively refining the output. The method's strength lies in its ability to handle multiple tasks with a single architecture, eliminating the need for task-specific customizations.\nA different group of methods focus on disentangling style and content for more targeted I2I translation. For instance, the approach of makes use of separate encoders for style and content, and then injects these representations into the DM to generate the translated image. Other methods explore few-shot or zero-shot I2I translation. For example, pix2pix-zero first inverts the input image to obtain a noise map, using DDIM and BLIP. These noise maps are then regularized to improve editability, using an autocorrelation objective. Edit directions are automatically discovered by generating diverse sentences for the source and target domains, and computing the mean difference between their CLIP embeddings. In order to preserve image structure during editing, a novel cross-attention guidance technique"}, {"title": "4.1.5 Counterfactual Augmentation", "content": "A significant number of methods make use of conditional DMs to generate counterfactual images. For instance, the approach of utilizes a conditional DM to generate 'healthy' counterfactuals of brain MRI images with tumors, by conditioning the model on the tumor mask. Diff-SCM employs a conditional DM to estimate the effects of interventions in a causal framework, by conditioning the model on the intervention variable do(class), corresponding to \u2018how the image should change in order to be classified as another class'. For instance, for a given photo of a dog in a park, the dog should be replaced by a cat, while leaving the rest of the image unchanged if do(cat) holds.\nA great portion of methods focus on generating counterfactuals for specific applications. For example, MEDJOURNEY generates counterfactual medical images by conditioning the DM on a text description of the desired change, while the methods of and generate counterfactual images to explain the model's behavior and to improve its robustness. On the other hand, a different set of approaches utilize counterfactual augmentation to address bias and fairness issues in datasets and models. For example, the work of utilizes a conditional DM to generate counterfactual images that balance the distribution of sensitive attributes (e.g., gender, race) in the dataset. The model is conditioned on the desired attribute distribution and generates images that match it, while preserving the remaining aspects of the image.\nAnother set of methods focus on expanding a given dataset with generated counterfactual or Out-of-Distribution (OOD) examples, in order to improve the robustness of a model (e.g., classifier) trained on the expanded dataset. For example, the method of utilizes a conditional DM to generate counterfactual examples that change specific attributes of the input image (e.g., object position, color, texture), while preserving the overall scene structure. These counterfactual examples are used to diagnose and mitigate model failures on OOD data."}, {"title": "4.2 Personalization and Adaptation", "content": "Personalization and adaptation constitute very common methodologies in image augmentation tasks. In the remainder of this subsection, respective DM-based approaches are detailed, grouped according to the taxonomy presented in Section 3."}, {"title": "4.2.1 Personalization Methods", "content": "Personalization implies that the DM must be adapted to generate content that meets specific user needs or preferences. These methods often involve finetuning pretrained models, leveraging textual or visual inputs, and optimizing for personalized outputs.\nA significant number of methods make use of finetuning to personalize pretrained T21 DMs for subject-driven generation. For example, DreamBooth finetunes the DM within a few-shot learning setting, using a small set (3-5) of images of a single subject paired with a text prompt containing a unique identifier and the name of the class the subject belongs to (e.g., 'A [V] dog'). These images contain a specific subject and serve as the training dataset that the DM needs to learn, while the text prompt simply describes the new subject. In particular, the method applies a class preservation loss to ensure that the generated images maintain the identity of the subject. Similarly, HyperDreamBooth is a method for efficient personalized DMs that introduces three key components, namely Lightweight DreamBooth (LiDB), a HyperNetwork for fast personalization, and rank-relaxed fast finetuning. LiDB decomposes the rank-1 LoRA weight space using a random orthogonal incomplete basis, resulting in smaller personalized models. Additionally, the HyperNetwork, consisting of a ViT encoder and transformer decoder, predicts LiDB residuals from input images, using diffusion denoising and weight-space losses. Eventually, rank-relaxed fast finetuning captures fine-level details by relaxing the LORA rank and finetuning with the predicted HyperNetwork weights, improving subject fidelity while maintaining fast personalization.\nA different set of methods make use of the so-called 'textual inversion' to personalize T2I generation. This process learns a new text embedding (termed 'pseudo-word') that represents a specific visual concept, using a small set of images depicting that concept. This allows the model to generate images of the concept, using natural language descriptions that include the pseudo-word. ProSpect extends this idea by learning a collection of pseudo-words (called a 'prompt spectrum') that capture different visual attributes (e.g., material, style, layout) of the concept.\nOn a different basis, another group of methods focus on combining multiple concepts for personalized generation. For example, the approaches of and utilize a compositional mechanism, where different concepts are represented by separate text embeddings that can be combined to generate novel images. StyleDrop and DreamArtist adopta"}, {"title": "5 Evaluation"}]}]}