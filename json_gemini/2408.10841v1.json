{"title": "DELIA:\nDiversity-Enhanced Learning for Instruction Adaptation in Large Language\nModels", "authors": ["Yuanhao Zeng", "Fei Ren", "Xinpeng Zhou", "Yihang Wang", "Yingxia Shao"], "abstract": "Although instruction tuning is widely used to adjust behav-\nior in Large Language Models (LLMs), extensive empirical\nevidence and research indicates that it is primarily a process\nwhere the model fits to specific task formats, rather than ac-\nquiring new knowledge or capabilities. We propose that this\nlimitation stems from biased features learned during in-\nstruction tuning, which differ from ideal task-specfic fea-\ntures, leading to learn less underlying semantics in down-\nstream tasks. However, ideal features are unknown and in-\ncalculable, constraining past work to rely on prior knowl-\nedge to assist reasoning or training, which limits LLMs' ca-\npabilities to the developers' abilities, rather than data-driven\nscalable learning. In our paper, through our novel data syn-\nthesis method, DELIA (Diversity-Enhanced Learning for\nInstruction Adaptation), we leverage the buffering effect\nof extensive diverse data in LLMs training to transform bi-\nased features in instruction tuning into approximations of\nideal features, without explicit prior ideal features. Exper-\niments show DELIA's better performance compared to com-\nmon instruction tuning and other baselines. It outperforms\ncommon instruction tuning by 17.07%-33.41% on Icelandic-\nEnglish translation bleurt score (WMT-21 dataset, gemma-\n7b-it) and improves accuracy by 36.1% on formatted text\ngeneration (Llama2-7b-chat). Notably, among knowledge in-\njection methods we've known, DELIA uniquely align the in-\nternal representations of new special tokens with their prior\nsemantics.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have demonstrated re-\nmarkable capabilities across a wide range of tasks. However,\ntheir application in specific domains often requires addi-\ntional fine-tuning to align with particular use cases. Instruc-\ntion tuning has emerged as a popular method to address this\nlimitation, aiming to endow LLMs with the ability to follow\ntask-specific instructions.\nInstruction tuning, typically involving supervised fine-\ntuning on instruction-response pairs, has been shown to pri-\nmarily fit models to specific task formats rather than im-\nparting new knowledge or capabilities (Zhao et al. 2024;\nZhou et al. 2023). This limitation is particularly evident with\nsmaller datasets (Zhou et al. 2023), contradicting the ideal\nscenario where LLMs learn adaptable downstream task ca-\npabilities.\nWe propose that this issue arises from the discrepancy\nbetween instruction tuning data distribution and the diverse\nreal-world instruction distribution, leading to the learning of\nbiased features during instruction tuning. These biased fea-\ntures deviate from ideal task-specific features. While not ex-\\plicitly framed as such, previous works have implicitly ac-\nknowledged this feature bias in instruction tuning datasets\nas a key factor in suboptimal performance (Zhao et al. 2024;\nPatel, Raffel, and Callison-Burch 2024).\nWhile ideal features may be unknown and incalculable,\napproximating them is crucial for grasping task semantics.\nPrevious works have attempted to address this challenge\nfrom the perspectives of model architecture and training ob-\njectives(Wang et al. 2022a; Vernikos et al. 2020). However,\ngiven the robust library and hardware support for LLMs, it\nis crucial to bridge the gap between biased and ideal features\nwithout altering the model's structure and at a low cost.\nSynthetic data offers a promising solution, but past meth-\nods often relied on heuristic approaches based on develop-\ners' prior knowledge (Li et al. 2023; Zhao et al. 2024). To\ntranscend developer limitations, we believe it is necessary to\nextend model capabilities through data-driven methods. The\nquestion then becomes: how can data-driven synthetic data\nmethods approximate ideal features?\nTo address this challenge, we introduce DELIA\n(Diversity-Enhanced Learning for Instruction Adapta-\ntion). We demonstrate that sufficiently diverse data, capable\nof covering as many potential features as possible, can act\nas a buffer, mitigating the bias of biased features relative to\nideal features and approximating ideal features.\nDELIA leverages the buffering effect of extensive diverse\ndata in LLMs training to transform biased features in in-\nstruction tuning into approximations of ideal features, with-\nout explicit training objectives towards ideal features. Our\nmethod involves sampling diverse question-answer pairs\nfrom LLMs, anisotropically diversifying downstream task\ninstructions, and extensively shuffling these components for\ntraining.\nOur empirical evaluation of DELIA demonstrates signif-\nicant improvements over common instruction tuning meth-\nods and other baselines across various tasks. Furthermore,\namong knowledge injection methods we've known, DELIA"}, {"title": "Related Work", "content": "Limitations of Instruction Tuning\nInstruction tuning has become a widely adopted method for\nenhancing model performance on specific tasks, but numer-\nous studies have revealed inherent flaws in this approach.\nSclar et al. (2023) demonstrated that such models could be-\ncome excessively sensitive to prompt variations, compro-\nmising their robustness. Sun and Dredze (2024) observed\nthat fine-tuning often results in models fitting to specific\ntask formats, hindering their ability to adapt to novel pre-\nsentations of similar tasks. Wang et al. (2022b) argued that\ninstruction-tuned models become overly specialized in spe-\ncific task formats rather than learning underlying semantics,\nwhich is basically near what we proposed \"biased feature\u201d.\nApproaches to Improving Instruction Following\nResearchers have proposed various solutions to address the\nchallenges of instruction tuning. At the framework level, Li\net al. (2023) and Zhao et al. (2024) proposed frameworks\nthat iteratively self-improves training data quality. Address-\ning the problem at the training objective level, Vernikos et al.\n(2020) proposed modifying the training objective with an\nadversarial classifier to mitigate domain overfitting. Archi-\ntectural modifications have also been explored. Wang et al.\n(2022a) explored architectural modifications with multiple\nPEFT modules. Jain et al. (2023) focused on altering the\ntraining process itself, by introducing noise during embed-\nding to improve generalization capabilities.\nSynthetic Data for Model Improvement\nSynthetic data generation has emerged as a promising solu-\ntion to instruction tuning challenges, offering the introduc-\ntion of human-interpretable prior knowledge and scalable,\ndata-driven performance improvement.Liu et al. (2023) ex-\nplored manipulating instruction positioning in training data.\nDong et al. (2024) focuses on balancing specialized and\ngeneral knowledge retention. Mecklenburg et al. (2024) in-\nvolves extracting and synthesizing atomic facts for more ro-\nbust knowledge representation. These approaches demon-\nstrate the potential of synthetic data in addressing the lim-\nitations of traditional instruction tuning methods, provid-\ning a foundation for our work in approximating ideal task\nfeatures and improving model performance across various\ndownstream tasks."}, {"title": "Method", "content": "Problem Definition\nThe training objective of Large Language Models (LLMs)\nis to minimize the cross-entropy loss function. In the con-\ntext of instruction tuning, for a given instruction, we aim to\nminimize the following loss:\n$\\mathcal{L}(\\theta) = -E_{p(x)}[\\sum_{t=ins}^{response} \\log q_{\\theta}(x_t|x_{<t})]$\nwhere $p$ is the true distribution and $q$ is the predicted dis-\ntribution.\nHowever, there is a discrepancy between the ideal distri-\nbution of downstream tasks $p_d$ and the general alignment\ndistribution $p_g$ of open-source LLMs. Our goal is to enable\nLLMs to acquire downstream task capabilities through in-\nstruction tuning, i.e., to find parameters $\\theta_d$ that minimize:\n$\\mathcal{L}(\\theta_d) = -E_{x \\sim p_d(x)}[\\sum_{t=ins}^{response} \\log q_{\\theta_d}(x_t|x_{<t})]$\nThis leads to a difference between the model parameters\n$\\theta_d$ adapted for downstream tasks and the aligned parameters\n$\\theta_g$.\nIdeal Features vs. Biased Features We expect LLMs to\nlearn ideal features for downstream tasks through $p_d$, en-\nabling task completion under any instructions with similar\nsemantics. Based on this, training objectives corresponding\nto $p_d$ and $p_g$ should have similar loss curve trends.\nHowever, the ideal distribution $p_d$ for downstream tasks\nis unknown and incalculable. We can only approximate $p_d$\nusing the distribution $p'_d$ of the instruction tuning dataset.\nThis introduces several sources of bias, including bias from\ninstruction phrasing and overfitting bias due to fixed training\ninstructions.\nThese biases produce biased features, causing the actual\ntraining distribution $p'_d$ to deviate from the ideal downstream\ntask distribution $p_d$. Consequently, the actual loss function\nbecomes:"}, {"title": "", "content": "$\\mathcal{L}(\\theta_d) = -E_{x \\sim p'_d(x)}[\\sum_{t=ins}^{response} \\log q_{\\theta_d}(x_t|x_{<t})]$\nPractice shows that using this actual loss function as the\noptimization objective can achieve meaningful fit on down-\nstream tasks with vast data or extensive training. However,\nthis is primarily a process of fitting specific task formats\nrather than acquiring new knowledge or capabilities. We be-\nlieve this is due to the difference between biased features\ncorresponding to $p'_d$ and ideal features corresponding to $p_d$.\nTo enable LLMs to ideally complete tasks under any instruc-\ntions with similar semantics, we need to bridge the gap be-\ntween biased and ideal features.\nDistribution Difference Hypothesis Based on empirical\nobservations, we hypothesize that $p_d$ is closer to $p_g$ than $p'_d$,\ni.e.:\n$D_{KL}(P_d||P_g) > D_{KL}(P'_d||P_g)$\nThis divergence between $p'_d$ and $p_d$ arises from our goal\nof enabling LLMs to perform downstream tasks under any\ninstructions with similar semantics after instruction tuning.\nHowever, the training data inevitably contains biases, such\nas bias in instruction wording or overfitting bias due to fixed\ntraining instructions as discussed.\nThese biases lead to the biased features in $p'_d$. Importantly,\nthese specific biases are relatively rare in the general align-\nment distribution $p_g$, which explains our hypothesis that $p_d$\nis closer to $p_g$ than $p'_d$.\nDELIA Method: Bridging the Gap Between Biased\nand Ideal Features\nTo address the discrepancy in features, we propose the\nDELIA (Diversity-Enhanced Learning for Instruction Adap-\ntation) method. The core idea is to introduce extensive di-\nverse data to mitigate the gap between $p'_d$ and $p_d$.\nBuffering Effect We posit that introducing extensive di-\nverse training data sampled from similar LLMs can produce\nthe following effects:\n1. When trained independently, these data generate gradi-\nents that largely cancel each other out, resulting in mini-\nmal changes to the LLM's parameters.\n2. When trained together with downstream task data, the\ntraining process:\n\u2022 Reduces the gradients produced by data similar to the\ndownstream task\n\u2022 Increases the gradients produced by data significantly\ndifferent from the downstream task\nThis gradient differential allows the model to learn biased\nfeatures while also learning content biased towards general\nfeatures. We term this the buffering effect, as shown in Fig-\nure 2.\nMathematical Derivation To more precisely illustrate the\nworking principle of the DELIA method, we provide the fol-\nlowing mathematical derivation:\nLet $p'_d(x)$, $p_g(x)$, and $q_\\theta(x)$ denote the probability den-\nsity functions of the downstream task dataset, the extensive\ndiverse dataset, and the model prediction, respectively. Let\n$\\theta$ be the model parameters and $L(\\theta, p)$ be the cross-entropy\nloss function when training on a dataset with distribution $p$.\nSince the LLM has been trained on a subset of the down-\nstream task dataset, we define $\\epsilon(x)$ such that:\n$q_\\theta(x) = p'_d(x) + \\epsilon(x)$\nWhen training on the extensive diverse dataset, the gradi-\nent of the LLM is:\n$\\nabla_\\theta L(\\theta, p_g) = E_{x \\sim p_g(x)} [\\nabla_\\theta(-\\log q_\\theta(x))]$\nSubstituting $q_\\theta (x)$, we get:\n$\\nabla_\\theta L(\\theta, p_g) = E_{x \\sim p_g(x)} [\\nabla_\\theta(-\\log(p'_d(x) + \\epsilon(x)))]$"}, {"title": "", "content": "Applying Taylor expansion at $p'_d(x)$ and taking the first-order approximation:\n$-\\log(p'_d(x) + \\epsilon(x)) \\approx -\\log p'_d(x) - \\frac{\\epsilon(x)}{p'_d(x)}$\nTherefore, the gradient can be approximated as:\n$\\nabla_\\theta L(\\theta, p_g) \\approx E_{x \\sim p_g(x)} [\\nabla_\\theta(-\\log p'_d(x) - \\frac{\\epsilon(x)}{p'_d(x)})]$\n$= E_{x \\sim p_g(x)} [\\nabla_\\theta(-\\log p'_d(x))] - E_{x \\sim p_g(x)} [\\frac{\\nabla_\\theta \\epsilon(x)}{p'_d(x)}]$\n$= \\nabla_\\theta [H(p_g, p'_d)] - \\nabla_\\theta [E_{x \\sim p_g(x)} (\\frac{\\epsilon(x)}{p'_d(x)})]$\n$= \\nabla_\\theta [D_{KL}(p_g||p'_d) + H(p_g)] - \\nabla_\\theta [E_{x \\sim p_g(x)} (\\frac{\\epsilon(x)}{p'_d(x)})]$\nIn practice, due to good convergence, $\\epsilon(x)$, which repre-\nsents the difference between the predicted distribution and\nthe downstream task dataset distribution, is usually small.\nTherefore, its gradient has little impact on the overall result\nand can be ignored. For a single data point in the extensive\ndiverse dataset, we can observe:\n1. When $p_g(x)$ is close to $p'_d(x)$, $D_{KL}(p_g||p'_d)$ is small,\nresulting in a relatively small gradient $\\nabla_\\theta L(\\theta, p'_d)$.\n2. When $p_g(x)$ is far from $p'_d(x)$, $D_{KL}(p_g||p'_d)$ is large,\nresulting in a relatively large gradient $\\nabla_\\theta L(\\theta, p'_d)$.\nThis is consistent with our previously mentioned buffer-\ning effect.\nBased on our earlier assumption that $p_d$ and $p_g$ correspond\nto training objectives with similar loss curves, we can infer:\n$||\\nabla_\\theta L(\\theta, p_d)|| \\approx ||\\nabla_\\theta L(\\theta, p_g)||= ||\\nabla_\\theta [D_{KL}(p_g||p'_d) + H(p_g)]- \\nabla_\\theta [E_{x \\sim p_g(x)} (\\frac{\\epsilon(x)}{p'_d(x)})]|| > 0$\nFurthermore, we can express the similarity in gradient\nvector directions:\n$\\frac{\\nabla_\\theta L(\\theta, p_d)}{\\nabla_\\theta L(\\theta, p_g)} = \\frac{|\\nabla_\\theta L(\\theta, p_d)||}{||\\nabla_\\theta L(\\theta, p_g)||}$\nThis derivation suggests that introducing extensive di-\nverse data may transform gradients learned towards $p'_d$ into\napproximations of gradients learned towards $p_d$. Specifi-\ncally:\nThe directions of the gradient vectors are approximately\nthe same, indicating that the optimization process moves in\nsimilar directions. The norms of the gradient vectors are ap-\nproximately equal, suggesting similar magnitudes of opti-\nmization steps."}, {"title": "Implementation of the DELIA Method", "content": "The proposed data synthesis strategy, DELIA, is a simple\nyet effective approach to enable LLMs to learn downstream\ntask capabilities and grasp the inherent semantics of tasks.\nThe key aspects are:\n1. Diverse data is crucial - it must cover the vast majority of\npotential features to allow biased features to approximate\nideal features.\n2. In addition to sample-level diversity, word-level diversity\nis introduced by using GPT-4 to diversify the instructions\nof downstream task data. New special tokens are added\nto validate DELIA align the internal representations of\nnew special token with its prior semantics.\n3. While extensive diverse dataset training after each down-\nstream task data point is theoretically designed, the pro-\nposed strategy of extensively shuffling the data provides\na practical approximation.\nExperiment\nIn this section, we evaluate the effectiveness of DELIA\nthrough the following experiments: (1) verifying DELIA's\nimprovement of the model's intermediate representations,\nto validate DELIA's role in promoting the model's learn-\ning of ideal downstream task features; (2) performance on\nthe practical formatted text generation task; (3) performance\non the practical English-Icelandic translation task. Finally,\nwe analyze the ablation experiments on DELIA, indicating"}, {"title": "Intermediate Representation Analysis", "content": "Model and Evaluation Setup We used the Llama 2-7B-\nChat model on the Leverage Learning formatted text dataset.\nThe task is that generating JSON-formatted text with\n\"thought\" key from instructions without explicit <sep> to-\nken semantics. We used less than 100 downstream task data.\nWe expect the <sep> token to represent formatted text gen-\neration. Evaluation metric: L2 norm between <sep> and\nkey words in its prior description (lower is better).\nResults and Analysis Figure 2 shows L2 norms of <sep>\nto \"thought\" and \"json\" across transformer blocks. Table 1\nshows exact values with 100 downstream task data samples\nand 52,000 diverse data samples (520x).\nOur results demonstrate DELIA's significant advantage\nin learning semantic representations, with performance im-\nproving as sample size increases. While baseline methods\nshowed mixed results, DELIA ultimately converged to the\nlowest L2 norm, surpassing all baselines. Notably, DELIA\neffectively utilizes increasing training samples even at small\ndata scales, achieving excellent semantic understanding.\nThis highlights DELIA's unique strength in semantic learn-\ning compared to other methods.\nWe show that LLMs interpret <sep> as condensed in-\nstructions, usable as plug-and-play soft prompts. This fea-\nture of DELIA could protect against prompt leakage and in-\ntellectual property loss, as extracted prompts would be unin-\nterpretable. We demonstrate this with an example. You can\nreproduce it with our code."}, {"title": "Practical Experiment", "content": "Tasks and Datasets We evaluate the performance of\nDELIA on two different tasks: formatted text generation and\nEnglish-Icelandic translation.\nFor the formatted text generation task, we use the Llama\n2-7B-Chat model on the Leverage Learning open-source for-\nmatted text dataset. The model needs to generate JSON-\nformatted text according to the instructions.\nFor the translation task, we use the gemma-7B-it model\non the WMT-21 open-source English-Icelandic parallel\ndataset. The model needs to translate the given content into\nthe target language.\nEvaluation Metrics and Implementation For the format-\nted text generation task, we use the generation accuracy as\nthe evaluation metric. We match the earliest pair of curly\nbraces in the generated content, and make necessary quote\nverification and modifications to ensure that the content can\nbe parsed by Python's JSON library.\nFor the translation task, we adopt the bleurt score as the\nevaluation metric, which is recommended by Garcia et al."}, {"title": "Ablation Experiment", "content": "To evaluate the contribution of each component of DELIA,\nwe conducted ablation experiments. DELIA uses the buffer-\ning effect of diversified training data to approximate the\nideal task-specific features. Based on the setting of the for-\nmatted text generation task, we performed ablation experi-\nments on these two types of diversification.\nThe ablation experiment results (Table 4) show that re-\nmoving either the diversified downstream tasks or the ex-\ntensive diversified data will lead to significant deterioration\nin all metrics. This indicates that DELIA's performance im-\nprovement is not provided by a single factor, but rather the\nsynergistic effect of the two, i.e., the approximation of the\nideal task-specific features that we emphasize.\nWe further analyzed the impact of extensive diversified\ndata on model performance. Figure 3 shows that as the scale\nof diversified data increases, the model's loss on the down-\nstream tasks continues to decrease. We also evaluated the\nstate space of the first divergent token (i.e., the position of\nthe left curly brace of the JSON format) in the formatted\ntext generation task. As shown in Figure 4, compared to the\nbaseline without diversified data, the degree of state space\nreduction continues to increase with the scale of diversified\ndata. These experiments show that even at our largest ex-\nperimental scale (520x diverse data), the contribution of di-\nversified data to the approximation of ideal features is still\nnot saturated. Continuously introducing diversified data can\nfurther optimize model performance and better approximate\nthe ideal features."}, {"title": "Discussion and Limitations", "content": "Our research extends model capabilities through data-driven\nmethods, avoiding reliance on developers' prior knowledge.\nThis approach aims to transcend developer limitations to-\nwards AGI, differing from previous heuristic-based meth-\nods. However, limitations remain:\nGap Between Ideal and Actual Diverse Data\nWe assumed datasets like Taori et al. (2023) could broadly\ncover various domains, approximating biased features to\nideal features. However, inherent biases create a gap be-\ntween theory and practice, hindering in-depth study of the\nbuffering effect. Future direction: Use synthesized data in\nspecific domains to narrow this gap.\nExistence of Optimal Approximation Point\nIn our experiments, we observed that the DELIA method im-\nproves feature representation. However, we recognize two\nextreme cases: without diverse data, the model learns only\nbiased features; with infinite diverse data, learned features\ndeviate from ideal features towards general features. Con-\nsidering the continuity between these extremes, we infer that\nthere must exist an optimal scale of diverse data that yields\nfeatures closest to the ideal. Future direction: Develop theo-\nretical methods to predict and obtain this optimal point.\nExperimental Scale Limitations\nDue to budget constraints, we conducted experiments only\non small LLMs with limited tasks. Although results demon-"}, {"title": "Conclusion", "content": "This study introduces DELIA, a method that improves fea-\nture learning in instruction tuning by leveraging the buffer-\ning effect of diverse data in large language model training.\nDELIA significantly outperforms common instruction tun-\ning methods and other baselines across various tasks. Our\napproach uniquely aligns the internal representations of new\nspecial tokens with their prior semantics, surpassing known\nknowledge injection methods. This research presents a novel\ndata-driven paradigm for instruction tuning and language\nmodel adaptation, demonstrating how to effectively approx-\nimate ideal features through synthetic data. While DELIA\nopens new possibilities, limitations such as the gap between\nideal and actual diverse data, the existence of an optimal ap-\nproximation point, and experimental scale constraints pro-\nvide directions for future research."}]}