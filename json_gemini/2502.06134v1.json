{"title": "Integrating Sequence and Image Modeling in Irregular Medical Time Series\nThrough Self-Supervised Learning", "authors": ["Liuqing Chen", "Shuhong Xiao", "Shixian Ding", "Shanhai Hu", "Lingyun Sun"], "abstract": "Medical time series are often irregular and face significant\nmissingness, posing challenges for data analysis and clini-\ncal decision-making. Existing methods typically adopt a sin-\ngle modeling perspective, either treating series data as se-\nquences or transforming them into image representations for\nfurther classification. In this paper, we propose a joint learn-\ning framework that incorporates both sequence and image\nrepresentations. We also design three self-supervised learn-\ning strategies to facilitate the fusion of sequence and im-\nage representations, capturing a more generalizable joint rep-\nresentation. The results indicate that our approach outper-\nforms seven other state-of-the-art models in three represen-\ntative real-world clinical datasets. We further validate our ap-\nproach by simulating two major types of real-world missing-\nness through leave-sensors-out and leave-samples-out tech-\nniques. The results demonstrate that our approach is more ro-\nbust and significantly surpasses other baselines in terms of\nclassification performance.", "sections": [{"title": "Introduction", "content": "Multivariate time series are utilized in various real-world ap-\nplications, particularly in the medical field, where they are\nused to record vital signs and laboratory test results for di-\nagnosis (Chaudhary et al. 2020; Brizzi et al. 2022). Typi-\ncally, these time series are irregular, faced with asynchronic-\nity across sensors and nonuniform sampling in the time do-\nmain (Chowdhury et al. 2023; Huang et al. 2024). More-\nover, significant missing values are usually present in clin-\nical data collection. For example, random missingness can\nresult from patients joining or leaving treatments midway,\nor complete absence of data from a sensor when specific\ntests are not conducted (de Jong et al. 2019). Some public\nclinical datasets, such as PhysioNet2012, take even a 80%\nmissing rate, posing challenges for data analysis and clini-\ncal decision-making (Wang et al. 2024).\nDeep learning methods have been widely adopted to\nmodel irregular time series. Some methods rely on the\nassumption of time discretization, utilizing LSTMs (Neil,\nPfeiffer, and Liu 2016; Weerakody, Wong, and Wang 2023),\nRNNS (Che et al. 2018; Ma, Li, and Cottrell 2020; Miao\net al. 2021), and Transformers (Horn et al. 2020; Huang\net al. 2024) to capture characteristics of discrete sequences.\nNonetheless, these methods often face difficulties in accu-\nmulating errors from missing observations (Ma et al. 2019).\nRecently, vision models have also shown promising poten-\ntial in handling irregular sequence data (Li, Li, and Yan\n2024). By transforming series into corresponding RGB rep-\nresentations, visual frameworks can effectively capture dy-\nnamic trends and inter-sensor relationships within images\n(Maroor et al. 2024; Li, Li, and Yan 2024). However, such\ndesigns perform poorly with sparse series that exhibit heavy\nmissing rate (Li, Li, and Yan 2024).\nWe recognize that no one has yet integrated both sequence\nand image representations in handling irregular medical time\nseries. This introduces a pivotal question: How can we ef-\nfectively merge these two distinct representations to improve\nthe robustness of classification for irregular medical time se-\nries with extensive missing values?\nTo investigate this question, we utilize a joint learn-\ning framework that incorporates both sequence and im-\nage representations. Additionally, we propose different self-\nsupervised learning (SSL) strategies to enhance the inte-\ngration and capture of supplementary information across\nthese two representations. Specifically, our approach con-\nsists of three main components, as shown in Figure 1. For\nthe sequence modeling branch, we employ a generator-\ndiscriminator structure and adopt an adversarial strategy\n(Ma et al. 2019; Miao et al. 2021) for sequence imputation\ntask to minimize the propagation of cumulative errors. In the\nimage branch, we implement different image transformation\nstrategies to improve the performance on sparse series, and\nutilize a pre-trained Swin Transformer (Liu et al. 2022; Li,\nLi, and Yan 2024) to obtain the corresponding image repre-\nsentations. Three different SSL losses are designed: (1) an\ninter-sequence contrastive loss to stabilize the sequence im-\nputation process; (2) a sequence-image contrastive loss with\nmargin to learn a more generalizable joint representation for\ndownstream classification; and (3) a clustering loss on joint\nrepresentations to push similar cases closer across different\nbatches.\nWe conduct experiments on three real-world clinical"}, {"title": "Related Work", "content": "Early practices for modeling irregular time series with miss-\ning values typically relied on fixed-time discretization. In\nthis context, (Choi et al. 2016) ignores the timestamp infor-\nmation by treating all intervals as equal, (Lipton et al. 2016)\nconsiders missing data as an effective feature for learning,\nand (Harutyunyan et al. 2019) segments the data into evenly\nspaced time intervals. In contrast, GRU-D (Che et al. 2018)\nemploys a gated network and incorporates imputation of\nmissing values into the optimization process. Unlike pre-\nvious methods, it adopts an additional missing value mask\nand lag matrix as inputs. Similar strategy have been adopted\nin (Ma et al. 2019; Ma, Li, and Cottrell 2020; Miao et al.\n2021), where adversarial frameworks are utilized to enhance\nthe prediction of imputed values.\nSome recent approaches have leveraged attention mecha-\nnisms to improve modeling. For instance, SeFT (Horn et al.\n2020) introduces a set of differentiable set functions and\nuses attention mechanisms to aggregate embeddings of dif-\nferent variables. ContiFormer (Chen et al. 2024), on the\nother hand, combines neural ordinary differential equations\n(ODEs) with attention mechanisms based on continuous-\ntime dynamics, extending the relationship modeling capa-\nbilities of Transformers to the continuous time domain. Be-"}, {"title": "Approach", "content": "For a given clinical time series dataset D, each sample\nX\u2208 Rd\u00d7T represents a set of d records over a time\nT = {t1, ..., tn}, corresponding to a label y. A binary mask\nM\u2208 RdxT is used to indicate the presence of missing ob-\nservations in X, where $M_{ij}$ = 0 signifies that the observa-\ntion of the ith item at time j is missing.\nTo better handle consecutive missing values time, we fol-\nlow (Miao et al. 2021; Che et al. 2018) to obtain a time-lag\nmatrix \u03b4 \u2208 Rd\u00d7T for each sample X. This matrix quantifies\nthe time elapsed since the most recent non-missing value for\neach observation, defined as follows.\n\u03b4\n\\begin{cases}\n    0, & \\text{i=1} \\\\\n    t_i-t_{i-1}, & m_{i-1}=1 \\text{ and } i>1 \\\\\n    \\delta_{i-1}+t_i-t_{i-1}, & m_{i-1}=0 \\text{ and } i>1\n\\end{cases}\nFor each sample X, the corresponding image I is con-\nstructed, where I \u2208 R3\u00d7W\u00d7H represent a certain RGB\nformat image. In total, we implement six transformed im-\nages as shown in Figure 1. The specific transformation\nmethods applied are as follows: Line Graphs, Frequency\nSpectrums, Gramian Angular Summation/Difference Fields,\nMarkov Transition Fields, Recurrence Plots."}, {"title": "The Model Overview", "content": "In this section, we introduce the overall framework of our\nmodel, which comprises three main parts: (a) the sequence\nencoder, (b) the image encoder, and (c) the joint representa-\ntion module. The sequence encoder consists of a generator-\ndiscriminator pair employing an adversarial strategy for im-\nputation. The generator, G, takes the time series X, the mask\nM, and the lag matrix 8 as inputs. Its objective is to estimate\nthe missing values in X and generate a completed sequence\nX'. This completed sequence X' is then used to obtain the\nsequence representation s \u2208 Rd. The discriminator D eval-\nuates these estimations with the goal of distinguishing true\nobservations from the imputed values. It outputs a binary\nmatrix M', which identifies the regions of imputation pre-\ndicted. For the image encoder, it takes a transformed image\nI as input and output the corresponding image representa-\ntion v \u2208 Rd. Finally, the joint representation module is re-\nsponsible for mapping the sequence representation s and the\nimage representation v into the same space. It then uses the\nfinal joint feature u \u2208 Rd for classification."}, {"title": "Sequence Branch with Imputation", "content": "We adopt a modified bidirectional recurrent neural network\n(BiRNN) as our generator G, which has been widely used\nin imputation tasks (Che et al. 2018; Ma et al. 2019; Ma, Li,\nand Cottrell 2020; Miao et al. 2021; Xu et al. 2024). Taking\nthe forward update step as an example, we update the current\nhidden state as:\nht = tanh (Wh(Yt \u00a9ht\u22121) + Wh(xt + x8)+bh) (1)\nYt = exp {-max(0, Wydt + by)} (2)\nIn this setup, Yt is derived from the lag matrix to model the\ndynamics of decay, where a longer duration of missing data\nleads Yt closer to 0. It is applied to determine the extent to\nwhich the previous hidden state ht-1 should be retained. In\nthe updating process of ht, instead of solely utilizing the pre-\nvious reconstruction \u00eet as done in prior works, we introduce\nan additional computation involving x8 as Eq. 3.\nX8 = Xt-.exp {\u2013 max(0, Wsdt) + bs} (3)"}, {"title": "Imaging Time Series", "content": "We use a pre-trained Swin Transformer (Liu et al. 2022) as\nour image encoder. For the given image input I, the Swin\nTransformer constructs a hierarchical representation to inte-\ngrate both local and global information. Specifically, at ear-\nlier layers, it partitions the input into small patches and pro-\ngressively merges neighboring patches as depth increases.\nIt employs two types of attention mechanisms: window-\nbased multi-head self-attention (W-MSA) and shifted win-\ndow multi-head self-attention (SW-MSA). These mecha-\nnisms are respectively used to compute self-attention within\na fixed window and to calculate dynamic relationships be-\ntween windows. The vectors from the last stage after layer\nnormalization are used as our image representation v \u2208 Rd.\nOverall, we implement six types of images for represen-\ntation learning and a detailed description is presented in Ap-\npendix A.\n\u2022 Line Graphs are constructed as (Li, Li, and Yan 2024),\nwith each variable represented by a line image of uniform\nsize.\n\u2022 Frequency Spectrums are generated based on the Fourier\ntransform, considering that frequency domain signals tend\nto be more robust in cases of extreme data missingness.\n\u2022 Gramian Angular Fields (Wang and Oates 2015) trans-\nform time series into polar coordinates, constructing\ntrigonometric sums/ differences between any two time\npoints to represent temporal correlation.\n\u2022 Markov Transition Fields record the Markov transition\nprobabilities between any two time observations (Wang\nand Oates 2015). They are insensitive to the distribution\nof the time series and temporal step information, allowing\nthem to effectively capture correlations between observa-\ntions with substantial missing data.\n\u2022 Recurrence Plots (Hatami, Gavet, and Debayle 2018),\nbased on phase space reconstruction, transform time se-\nries data into trajectories within phase space and analyze\ntheir recurrences. They are designed to capture the inher-\nent repetitiveness and periodicity within the time series."}, {"title": "Joint Representations Through Contrast and\nClustering", "content": "The joint representation module includes a transformation\nfunction f : s,v \u2192 RD, which projects and concatenates\nthe sequence features s and image features v into a joint\nspace RD, and the fused feature is obtained as u = [s, v].\nTo ensure both the quality and consistency of the joint rep-\nresentation, we implement contrastive learning within each\nbatch to maximize the mutual information between corre-\nsponding pairs. A simple choice is to use the NT-Xent in\nEq. 6, where only sequence and image features correspond-\ning to the same sample are treated as positive pairs (Sangha\net al. 2024). Through this approach, NT-Xent ensures that\nthe similarity between representations from the same sam-\nple is higher than that of other pairs. However, it also misses\nopportunities to learn from a wider set of potential pairs (Li,\nTorr, and Lukasiewicz 2022).\nIn this case, a step forward is to treat s and v from differ-\nent samples within the different category as a special form of\nnegative pairs, thereby enhancing the model's ability to dis-\ntinguish inter-class differences. Specifically, we introduce an\nadditional margin m for these special negative pairs, enforce\nthe model to exert greater effort to distinguish them:\n\\Sigma_{i=1}^B -\\Sigma_{s \\in S_i}log (\\frac{exp((v_i \\cdot s_i)/\\tau)}{\\Sigma_{k \\in P(i)} exp((v_i \\cdot s_k)/\\tau)+\\Sigma_{k \\notin P(i)} exp((v_k \\cdot s_i +m)/\\tau)})"}, {"title": "Datasets and Metrics", "content": "In the experiments, we consider three real-world irregular\nclinical datasets as shown in Table 1. The physical activity\nmonitoring (PAM) dataset (Reiss and Stricker 2012) focuses\non tracking human activities, containing data from eight per-\nson who performed nine different actions. This dataset com-\nprises 5,333 samples and captures data from four types of\nsensors placed at three distinct body locations, encompass-\ning a total of 17 observational variables. The P12 dataset\n(Goldberger et al. 2000) includes 11,988 patient samples\nfrom ICU stays, with 36 measurements each. The binary la-\nbels indicate the prognosis for each sample as either sur-\nvival or not. Finally, the P19 dataset (Reyna et al. 2020)\ncontains data from 38,803 sepsis patients, each with 34 mea-\nsurements, and a high missing rate of 94.9%. Approximately\n90% of these patients died due to sepsis.\nTo maintain consistency across all experiments, we fol-\nlow the same data partition as (Zhang et al. 2022; Li, Li,\nand Yan 2024), dividing the datasets into training, valida-\ntion, and testing sets in an 8:1:1 ratio. For the PAM dataset,\nwe use Accuracy, Precision, Recall, and F1 score as evalua-\ntion metrics. For the more imbalanced P12 and P19 datasets,\nwe report the Area Under the ROC Curve (AUROC) and the\nArea Under the Precision-Recall Curve (AUPRC). For more\nexperimental results that are not included in this section, we\npresent them in Appendix D."}, {"title": "Implementation and Training", "content": "We use Gated Recurrent Units (GRU) (Dey and Salem 2017)\nin both our generator and discriminator. The generator has 4\nlayers, with the number of units fixed at 128. The discrimi-\nnator is a 5-layer RNN and the number of units is set to {128,\n64, 16, 64, 128}, respectively. A checkpoint pre-trained on\nImageNet-21K dataset are utilized for our image encoder.\nThe patch size and window size are 4 and 7. For the P12 and\nP19 datasets, all images are set to a size of 384 \u00d7 384 pixels.\nWhile for the PAM dataset, line graph and frequency spec-\ntrum are configured to 256\u00d7 320, while all other images are\nset to 320 \u00d7 320. We use a 3-layer MLP as our joint projec-\ntion, with the number of units set to {1024, 512, 1024}.\nFor the P12 and P19 datasets, the total epoch is set to 8\nand we apply upsampling of the minority class to mitigate\nimbalance. For the PAM dataset, we set the total epoch to\n40. The batch sizes used for training are 32 for P19 and P12,\nand 48 for PAM. For each dataset, we discuss the learning\nrate as well as more hyperparameter settings in Appendix\nB. All experiments are performed on a server with NVIDIA\nGeForce RTX 3090 24GB and PyTorch 2.4.0+cu124."}, {"title": "Results", "content": "We compare our approach against seven state-of-the-art methods\nfor irregularly sampled time series, including GRU-D (Che\net al. 2018), SeFT (Horn et al. 2020), CARD (Han, Zheng,\nand Zhou 2022), Raindrop (Zhang et al. 2022), PrimeNet\n(Chowdhury et al. 2023), ContiFormer (Chen et al. 2024),\nand ViTST (Li, Li, and Yan 2024). For each baseline, we in-\ntroduce our implementation and hyperparameter settings in\nAppendix C. To ensure a fair evaluation, we average the per-\nformance of each method across five individual tests, using\nthe same data splits and settings provided in (Li, Li, and Yan\n2024).\nPerformance under increased missing rates. To further\nvalidate the robustness of our approach, we conduct addi-\ntional experiments to compare the performance under in-\ncreased levels of missing rate. Given that the P12 and P19\ndatasets have already faced very high missing rates-88.4%\nand 94.9% respectively, we conduct all the tests on the PAM\ndataset, which originally has a missing rate of 60%. We con-\nducted two types of tests: the leave-sensors-out setting, sim-\nulating scenarios where certain medical tests are not per-\nformed, and the leave-samples-out setting, reflecting situa-\ntions where patients join or leave treatments midway. We\nfollow the approach in (Zhang et al. 2022), applying all mod-\nifications only to the test set by randomly masking the orig-\ninal observations."}, {"title": "Clinical Turing tests", "content": "To ensure that the learned repre-\nsentations align with clinically meaningful patterns rather\nthan statistical artifacts, we conducted a clinical Turing test\non the generated signals, as described in (Gillette et al.\n2023). Specifically, we select 60 samples from the P19\n(ICU) dataset, with half imputed using linear interpolation as\nreal measured samples and the other half imputed using our\nmodel as generated samples. Five ICU-experienced clini-\ncians (3 chief physicians and 2 attending physicians) attempt\nto distinguish between the two types. As shown in Table 3,\nthe experts achieve prediction accuracy of 50.0%, 48.3%,\n48.3%, 58.3%, and 60.0%, resulting in a kappa score of\n0.03. These results are close to random guessing, suggesting\nthat the experts generally struggle to differentiate between\nthe samples. A brief interview further revealed why experts\nstruggled to identify clear patterns to distinguish real from\ngenerated samples. One reason is that the complex events in\nthe ICU environment make the data distribution more tol-\nerant. For example, sedation or anesthesia can cause body\ntemperature to fall below the usual range."}, {"title": "Ablation study", "content": "In this section, we present the results of\nour ablation study in Table 4. The \"default\" one is our stan-\ndard setup, which includes the sequence encoder, the im-\nage encoder, and the joint representation module, along with\nthree self-supervised learning strategies. In the first part of"}, {"title": "Conclusion", "content": "In this paper, we propose a joint learning approach of lever-\naging both sequence and image representations to tackle the\nclassification of irregularly sampled clinical time series. By\nemploying our three self-supervised learning strategies, we\nare able to effectively learn more generalized joint repre-\nsentations. The effectiveness of our approach is verified on\nthree real-world clinical datasets, where it demonstrates su-\nperior performance compared to seven state-of-the-art meth-\nods. Additionally, we test our approach under more severe\nmissing rates using leave-sensors-out and leave-samples-out\ntechniques. Our approach consistently achieved strong re-\nsults, demonstrating its robustness in these scenarios. Our\ncode and data will be made publicly available later."}]}