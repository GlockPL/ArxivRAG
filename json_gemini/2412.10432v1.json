{"title": "Imitate Before Detect: Aligning Machine Stylistic Preference for Machine-Revised Text Detection", "authors": ["Jiaqi Chen", "Xiaoye Zhu", "Tianyang Liu", "Ying Chen", "Xinhui Chen", "Yiwen Yuan", "Chak Tou Leong", "Zuchao Li", "Long Tang", "Lei Zhang", "Chenyu Yan", "Guanghao Mei", "Jie Zhang", "Lefei Zhang"], "abstract": "Large Language Models (LLMs) have revolutionized text generation, making detecting machine-generated text increasingly challenging. Although past methods have achieved good performance on detecting pure machine-generated text, those detectors have poor performance on distinguishing machine-revised text (rewriting, expansion, and polishing), which can have only minor changes from its original human prompt. As the content of text may originate from human prompts, detecting machine-revised text often involves identifying distinctive machine styles, e.g., worded favored by LLMs. However, existing methods struggle to detect machine-style phrasing hidden within the content contributed by humans. We propose the \"Imitate Before Detect\" (ImBD) approach, which first imitates the machine-style token distribution, and then compares the distribution of the text to be tested with the machine-style distribution to determine whether the text has been machine-revised. To this end, we introduce style preference optimization (SPO), which aligns a scoring LLM model to the preference of text styles generated by machines. The aligned scoring model is then used to calculate the style-conditional probability curvature (Style-CPC), quantifying the log probability difference between the original and conditionally sampled texts for effective detection. We conduct extensive comparisons across various scenarios, encompassing text revisions by six LLMs, four distinct text domains, and three machine revision types. Compared to existing state-of-the-art methods, our method yields a 13% increase in AUC for detecting text revised by open-source LLMs, and improves performance by 5% and 19% for detecting GPT-3.5 and GPT-40 revised text, respectively. Notably, our method surpasses the commercially trained GPT-Zero with just 1,000 samples and five minutes of SPO, demonstrating its efficiency and effectiveness.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities in generating text that is difficult to distinguish from human writing (Brown et al. 2020; Chowdhery et al. 2023; Raymond et al. 2023; Hugo et al. 2023b,a; Ope-"}, {"title": "2 Method", "content": "We elaborate on the methods for addressing the challenge of machine-revised text detection, aiming to differentiate between pure human texts and machine-revised texts."}, {"title": "2.1 Problem Formulation", "content": "Let x denote the given text under detection, represented as a sequence of tokens {x}_1, where n is the length of the sequence. This text x may either be revised by machine or authored by a human. Our primary objective is to utilize a scoring model pe, which is an autoregressive language model, to ascertain whether the text x is machine-revised (xm) or human-written (xh), thereby formulating this problem as a binary classification task. Formally, we aim to construct a decision function f : x \u2192 0,1, where the output 0 indicates that the text is human-authored, and 1 signifies that the text is machine-revised."}, {"title": "2.2 Preliminary", "content": "Foundation The foundation of machine-generated text detection methods often lies in analyzing the probability distribution of tokens within a given text. This is rooted in the fact that common decoding strategies, such as top-k, top-p, and beam search, favor high-likelihood next tokens in autoregressive generation, while high-quality human language does not necessarily follow high-probability next words (Ari et al. 2020).\nTo quantify the differences between machine-generated text xm and human-written text xh, one effective strategy is to measure the discrepancy (d) between the log probability of the original text and its alternative versions under perturbation (Mitchell et al. 2023) or after resampling (Bao et al. 2023). Let & denote a transformation function that produces an altered version 2 from the original text x, i.e., x ~ \u00a2(x). In machine-generated texts, the original tokens often have higher probabilities, and after applying & for token replacement, the probabilities of the new tokens tend to be lower on average. Conversely, human-written texts typically exhibit a more diverse range of token probabilities, leading to a smaller discrepancy after alterations. As a result, this discrepancy tends to be larger for machine-generated text compared to human-written text. Formally, we can express this inequality as:\nlog p(xm) \u2013 E\u017em~4(xm) log p(xm)\ndiscrepancy of machine-generated text (dm)\n> log p(xh) \u2013 E\u00e3n~(xh) log p(xh)\ndiscrepancy of human-written text (\u03b4\u03b7)\nwhere p represents the probability distribution of the source model. The source model can be effectively replaced by a substitute scoring model po in black-box scenarios (Mitchell et al. 2023). This inequality forms the basis for distinguishing between machine-generated and human-written content. Recent studies have demonstrated the effectiveness of this approach in detecting machine-generated text"}, {"title": "2.3 Imitating via Preference Optimization", "content": "Based on the challenges identified in detecting machine-revised text, we observed that the key to effective detection lies in increasing the discrepancy between the probability distributions of machine-revised and human-written texts. To address this, we aim to increase the difference between the discrepancies dm and \u03b4\u03b7, as defined earlier. Specifically, our objective is to optimize the scoring model pe to better imitate the token distribution with machine style, such that:\nmax Exm,xn [d\u0442 \u2013 \u03b4\u03b7].\nPe\nThis objective seeks to widen the gap between the discrepancies between machine-revised and human-written texts, making them more distinguishable. To achieve this, we propose a method called style preference optimization, which leverages preference learning to tune the scoring model po towards favoring machine-revised text patterns."}, {"title": "2.4 Detection via Style Probability Curvature", "content": "After aligning our model with machine-revised text styles, we proceed with the detection step using conditional probability curvature (Bao et al. 2023). Specifically, given the machine-style scoring model pe and a sampling model q4, we define the style-conditional probability as:\np(x|x) = [[ Po(xj|x<j).\nj\nHere, \u00ee is generated by sampling each token xi from po(xi\nX<i) without conditioning on other sampled tokens. The style-conditional probability curvature (Style-CPC) is quantified as:\nd(x, po, q) =\nlog po (xx) \u2013 \u03bc\n\u03c3\n,\nwhere\n\u03bc = Ex~qq(x|x) (log po(Xi | x))\n\u1ee1\u00b2 = E\u017e~q4(2x) (log po(xi | x) \u2013 \u03bc\u00b2).\nThis metric d(x, pe, q) allows us to quantify the log probability difference between the original and alternative sampled texts.  We observe that using the aligned model to calculate d significantly reduces the overlap be-tween distributions of human-written and machine-revised texts. This reduced overlap enables us to identify an effective"}, {"title": "3 Experiment", "content": ""}, {"title": "3.1 Machine revision dataset", "content": "Data sources The human-written texts included in the training dataset were crawled from the internet before 2019. The texts are then polished by GPT-3.5.1 We use 500 pairs of samples for training. The composition of the dataset is 57.3% papers, 14.2% blogs, 4.0% letters and emails, and 2.1% homework. See Appendix A.1 and A.2 for training and dataset collection details, respectively.\nFor the test data, we follow Bao et al. (2023); Mitchell et al. (2023), use paragraphs from diverse domains as human-written texts, including XSum (Narayan et al. 2018) for news articles, SQUAD (Fan et al. 2018) for Wikipedia contexts, WritingPrompts (Fan et al. 2018) (Abbreviated as \"Writing\") for story writing, and PubMedQA (Jin et al. 2019) for biomedical research question answering. Then, we use the pipeline detailed in the following paragraph to generate correspondent machine-revised text.\nDataset process We design a cohesive two-stage pipeline to revise human-written text. Detailed examples of the generated instructions are in Appendix A.3.\n\u2022 Revision instruction generation: For each task, instructions are constructed with varying tones and lengths using GPT-3.5. The tone is randomly selected from a set of 10 predefined options, while the instruction length is chosen from the set of {15, 30, 50} words. The intuition behind choosing different tones and lengths is to simulate different human behaviors.\n\u2022 Paragraph revision: The generated instruction and the human-written text are then prompted into the LLM to produce the final machine-revised text."}, {"title": "3.2 Baselines", "content": "We compare our method with two lines of method: training-based models, and logit-based models. Following Bao et al. (2023), we use AUROC as a metric to evaluate detection accuracy.\n\u2022 Training-based models include RoBERTa-base (Liu et al. 2019) and RoBERTa-large (Liu et al. 2019), which is trained on substantial datasets up to 160GB of text data, as well as the commercial detector GPTZero (Tian et al. 2023), which is trained on massive datasets.\n\u2022 Logit-based models include Likelihood (Ippolito et al. 2020) (mean log probabilities), LogRank (Solaiman et al. 2019) (average log of ranks in descending order by probabilities), Entropy (Gehrmann et al. 2019) (mean token entropy of the predictive distribution), LRR (Su et al. 2023) (an amalgamation of log probability and log-rank), NPR (Su et al. 2023) (normalized perturbed log-Rank) and DNA-GPT (Yang et al. 2023) (divergent N-Gram Analysis), DetectGPT (Mitchell et al. 2023), and its advanced variant, Fast-DetectGPT (Bao et al. 2023).\nNote that Fast-DetectGPT (Bao et al. 2023), the current state-of-the-art approach, also serves as a baseline method that does not involve machine-style imitation."}, {"title": "3.3 Main results", "content": "Detection performance for GPT series We evaluate our method using passages polished by GPT-3.5 and GPT-40 across different domains."}, {"title": "3.4 Ablation study", "content": "Ablation on machine-style imitation As using fast-DetectGPT as the baseline without imitation, our method improves detection accuracy by 16% and 20% on GPT-3.5 and GPT-40 machine-revised texts, respectively.\nAblation on preference optimization To demonstrate the difference between different optimization methods on ImBD, we compare the performance of SPO against other alignment approaches on polish task. As shows that ImBD outperformed the SFT variant by 30% on GPT-3.5 and 24% on GPT-40, even when the SFT variant uses 3x training data. Additionally, ImBD exceeds RLHF and ORPO significantly.\nAblation on text length As a result, our method demonstrates strong performance across passages of varying lengths compared to other methods, with accuracy improving as passage length increases."}, {"title": "4 Related work", "content": ""}, {"title": "4.1 Machine-Generated Text Detection", "content": "Datasets Researchers developed various evaluation benchmarks for machine-generated text detection. Bao et al. (2023) and Mitchell et al. (2023) used the initial 30 tokens from human-written texts across different domains as prompts to generate pure machine-generated text via LLMs. Following this approach, Biyang et al. (2023) employed QA datasets as human samples and generated pure machine-generated text using ChatGPT. Building upon the QA framework, researchers (Mitchell et al. 2023; Su et al. 2023; Hu et al. 2023; He et al. 2024; Wang et al. 2024) collected texts generated by mainstream LLMs. Verma et al. (2023) focused on creative writing tasks, providing only writing prompts or headlines to generate text with LLMs. However, a significant portion of contemporary machine-generated content involves human input (Zhang et al. 2024). In instance, MixSet (Zhang et al. 2024) examined scenarios where human revisions are applied to machine-generated text. In contrast, our study focuses on the reverse: human-written text revised by LLMs. This practice, where people use AI to enhance, edit, or expand their writing, is increasingly common and accepted in various contexts but remains largely prohibited in academic settings. We specifically address detecting this form of human-machine collaborative text.\nMethods Previous methods for machine-generated text detection generally fall into two categories: training-based methods and logit-based metric approaches. While training-based methods (Biyang et al. 2023; Chen et al. 2023; Hu et al. 2023) achieved excellent performance due to large-scale data and high-cost training, they tended to overfit and were less effective in detecting the machine-revised text.\nExisting logit-based approaches, such as Log-Likelihood (Solaiman et al. 2019), Entropy (Solaiman et al. 2019), Rank (Gehrmann et al. 2019), and Log-Rank (Mitchell et al. 2023), relied on statistical analysis to evaluate information beyond the token level. GLTR (Gehrmann et al. 2019) combined a set of metric-based methods to assist human identification. DetectGPT (Mitchell et al. 2023) built on the observation that machine-generated texts occupy regions with steep negative log probability curvature, using this probability curvature to detect whether text originates from LLMs. This concept was further developed and improved in subsequent studies (Su et al. 2023; Mireshghallah et al. 2024; Bao et al. 2023). Zeng et al. (2024) proposed adapting scoring models through fine-tuning to handle the latest black-box models.\nWhile previous approaches generally relied on overall text features for classification, we propose isolating stylistic features as the basis, enabling more precise detection of subtle differences."}, {"title": "4.2 Preference Optimization", "content": "Direct Preference Optimization (Rafael et al. 2023) can efficiently learn and align preferences from a pair of sampled texts. Related offline algorithms (Yuan et al. 2024; Kawin et al. 2024; Hong et al. 2024; Park et al. 2024) were typically also employed to align LLMs with human preferences,"}, {"title": "5 Conclusion", "content": "In this work, we have presented the \"Imitate Before Detect\" paradigm to detect machine-revised text by learning to imitate the writing style of LLMs. Specifically, we have proposed style preference optimization for aligning the detector with machine writing styles and leveraged style-conditional probability curvature to quantify log probability differences for effective detection. We have conducted extensive evaluations across six leading LLMs, three text domains, and three revision techniques, demonstrating significant improvements in detection accuracy compared to existing state-of-the-art methods."}, {"title": "A Implement details", "content": ""}, {"title": "A.1 Training details", "content": "We fine-tune the gpt-neo-2.7B model from EleutherAI, using a learning rate of 0.0001 and a beta coefficient of 0.05. The fine-tuning process is conducted over 2 epochs (1,000 samples each epoch) with a fixed random seed of 42 to ensure reproducibility. For parameter-efficient training, we utilize a Lora configuration with a rank of 8, a Lora alpha of 32, and a dropout rate of 0.1, specifically tailored for causal language modeling tasks.  All experiments are conducted on an Ubuntu 20.04 platform using a single L20 (48GB) GPU, with Python 3.8, PyTorch 1.10.0, Transformers 4.28.1, and Datasets 2.12.0."}, {"title": "A.2 Dataset collection details", "content": "To train a detector with strong generalization capabilities, we do not use existing domain-specific text datasets. Instead, we randomly collect 500 paragraphs from the internet, all published before 2019. Each paragraph is approximately 300 words long and covers one of seven topics: academic papers, assignments, blogs, letters, literary works, news articles, and others. These texts represent human-authored content from before 2019. We then processed these human-written texts through a polish data generation pipeline by GPT-3.5-turbo, resulting in 500 pairs of human and machine-revised texts. All experimental data in the main paper are based on these pairs. Note that all data were manually collected, with collectors compensated at a rate of $ 60 per hour. We ensured that no copyrighted texts were used, and the model trained on this data is intended solely for academic discussion, with no commercial use planned."}, {"title": "A.3 Machine-revised text generation pipeline", "content": "We design a two-step generation pipeline for machine-revised text data generation. First, the pipeline generates a user instruction with GPT-3.5-turbo, and then we combine human-"}]}