{"title": "MEANT: Multimodal Encoder for Antecedent Information", "authors": ["Benjamin Iyoya Irving", "Annika Marie Schoene"], "abstract": "The stock market provides a rich well of information that can be split across modalities, making it an ideal candidate for multimodal evaluation. Multimodal data plays an increasingly important role in the development of machine learning and has shown to positively impact performance. But information can do more than exist across modes- it can exist across time. How should we attend to temporal data that consists of multiple information types? This work introduces (i) the MEANT model, a Multimodal Encoder for Antecedent information and (ii) a new dataset called TempStock, which consists of price, Tweets, and graphical data with over a million Tweets from all of the companies in the S&P 500 Index. We find that MEANT improves performance on existing baselines by over 15%, and that the textual information affects performance far more than the visual information on our time-dependent task from our ablation study.", "sections": [{"title": "Introduction", "content": "Recently, multimodal models have garnered serious momentum, with the release of large pre-trained architectures such as Microsoft's Kosmos-1 (Huang et al., 2023) and OpenAI's GPT-4 (OpenAI et al., 2023). Their general use has exploded in many domains, such as language and image processing (Lu et al., 2019; Kim et al., 2021; Huang et al., 2023). Particularly interesting to this study is the deployment of multimodal models on time-dependent environments, where recent successes have shown that event driven models processing multiple modalities are far more performant on stock market tasks than previously state of the art (SOTA) algorithms focusing purely on price information (Li et al., 2021; Zhang et al., 2022). Language data from news and social media sources have shown to greatly increase the performance of models for price prediction (Li et al., 2021; Zhang et al., 2022; Bybee et al., 2023; Mittermayer and Knolmayer, 2006; Xu and Cohen, 2018). However, these approaches typically lack attention components specifically designed to process inputs with sequential, time-dependent information (Li et al., 2021; Sun et al., 2017; Zhang et al., 2022; Xu and Cohen, 2018). This type of data is particularly important when making predictions about stock prices or market movements, as price prediction is a time series task (Zhang et al., 2022; Xu and Cohen, 2018).\nIn this work, we introduce MEANT, a multimodal model architecture with a novel, temporally focused self-attention mechanism. We extract image features using the TimeSFormer architecture (Bertasius et al., 2021) to find relationships in longer range information (i.e a graph of stock prices over a month), while extracting language features from social media information to pick up more immediate trends (e.g.: Tweets pertaining to stock prices over a five day period). Furthermore, we release TempStock, a multimodal stock-market dataset that is designed to be sequentially processed in chunks of varying lag periods."}, {"title": "Related Work", "content": "Several studies have employed natural language processing (NLP) techniques to financial markets, giving birth to the field of natural language-based financial forecasting (NLFF). Many of these studies have focused on public news (Ashtiani and Raahemi, 2023; Bybee et al., 2023). However, social media presents more time-sensitive information from active investors. Thus, for short term analysis, many researchers have begun to focus on Tweets for feature extraction (Araci, 2019; Wu et al., 2018), through which some have combined NLP techniques with traditional analysis on price data (Huang et al., 2022). Since Tweets often cor-"}, {"title": "TempStock Dataset", "content": "We collected a new dataset containing 1,755,998 Tweets and price information from all of the companies in the S&P 500 from 4/10/2022 to 4/10/2023.\nFrom the price information, we calculated the Moving Average Convergence-Divergence (MACD) (Appel, 2005) for each company over a year. The MACD is built on the back of Exponential Moving Average (EMA) (Brown, 1964). The EMA is defined as follows:\n$EMA_t = (1 - a) \\cdot EMA_{t-1} + a \\cdot Y_t$\nwhere t represents the day of EMA and $y_t$ represents the closing price on that day, or in the case of the signal line, the MACD value on that day. $a$ represents the degree of decrease, where $a = \\frac{2}{t+1}$.\nThe MACD consists of (i) an MACD line, which is the difference between the fast EMA and the slow EMA (commonly set to 12 days and 26 days respectively), (ii) a signal line, which is the EMA of the MACD line itself (usually over a 9 day period), and (iii) a histogram, which is the difference between the MACD and the signal line. The MACD indicator was chosen 1 because it has been shown to perform well against other indicators in terms"}, {"title": "MEANT", "content": "MEANT combines the advantages of image and language processing with temporal attention, in order to extract dependencies from multimodal, sequential information, where 2 displays the full architecture. MEANT, similarly to most SOTA multimodal models (Liang et al., 2021; Kim et al., 2021; Su et al., 2019; Huang et al., 2023; OpenAI et al., 2023), is built atop the Transformer architecture (Vaswani et al., 2017)."}, {"title": "Encoder Only", "content": "MEANT is an encoder-only model, similar to BERT (Devlin et al., 2018). Our model contains two pipelines, an image and a language pipeline. The language encoder stacks the attention mechanism with linear layers to extract relevant features from the input. Between the 2 parts of the encoder, and before the output, there is a standard residual connection, meaning that the input to that portion of the architecture is fed through added with the original input. This is done to alleviate the vanishing gradient problem (Pascanu et al., 2013). The interleaved encoder structure employed by the language pipeline is inspired by the Magneto model (Wang et al., 2022). It makes use of sub-layer normalization, meaning that a layer norm is interleaved between the attention and linear layer components of the encoder. This architecture was chosen because it has been shown to be successful on a wide variety of uni-modal and multimodal problems (Huang et al., 2023; Wang et al., 2022).\nFor the backbone of our image pipeline, we chose to use a variant of the TimeSFormer model (Bertasius et al., 2021), which is an encoder model designed to handle video inputs. We chose this model because of its ability to extract dependencies in the temporal dimension. Our lag graph inputs change in place in a similar manner to a video. We altered the implementation to make use of the interleaved layernorm strategy from Magneto, and used different positional embeddings. In earlier iterations of the model, we used ViT encoders, and fed the outputs of our image pipeline to our temporal attention mechanism along with our Tweets. We found this to be less performant (see 6)."}, {"title": "Token and Patch Embeddings", "content": "Before being fed to the attention mechanism, the two input types have to be prepared for processing using two different embedding strategies. The Tweets in MEANT are tokenized using the Fin-BERT tokenizer (Araci, 2019) and we use the Fin-BERT pretrained word embedding layer.\nThe images are first transformed into tensors of rgb values and reshaped to a manageable size. MEANT handles input image sizes of 3 x 224 x 224, where 3 represents the number of channels and the subsequent dimensions are the height and width respectively. TimeSFormer breaks down the vectors using the patch embedding strategy from the original vision transformer (Dosovitskiy et al., 2020) (Bertasius et al., 2021)."}, {"title": "Positional Encoding", "content": "In MEANT, the language and vision encoders use different variants of the rotary embedding (Su et al., 2021). The language encoder uses the xPos embeddings (Sun et al., 2022), while the TimeSFormer uses both rotary and axial 2-D embeddings (Su et al., 2021). In axial 2-D embeddings, the angle \u03b8 of rotation is altered according to the following equation:\n$\\theta_i = i * floor(d/2) * pi$\nWe developed two different variants of our temporal encoding pipeline, which work better in different cases: temporal attention with mean pooling, and temporal attention with sequence projection.\nIn both cases, the outputs of our language encoders $L_{out}$ are tensors of the shape $b \\times l \\times s \\times d_1$, where b denotes the batch size, l denotes the lag period, s is the sequence length, and$ d_1$ is the dimension of each encoded language token. For temporal"}, {"title": "Experiments", "content": "We ran the model at three different sizes, coined MEANT-small, MEANT-large and MEANT-XL. MEANT-small contained one encoder for language and vision, along with one temporal encoder. MEANT-large consisted of twelve language and vision encoders, and one encoder for temporal attention. twelve was selected as the number of encoders used in the original BERT model (Devlin et al., 2018). MEANT-XL had 24 encoders in our language pipeline and our TimeSFormer backbone, along with one temporal encoder. Implementation details can be seen in A.4."}, {"title": "Fine-tuning on downstream tasks", "content": "We tested the viability of the MEANT architecture on two tasks."}, {"title": "TempStock", "content": "TempStock is a binary classification task, identifying lag periods which resulted in momentum shifts and those that did not. To further measure MEANT's performance, we ran some similar SOTA encoder-based multimodal models on Temp-Stock. TEANet, a key inspiration for this work, was the most similar model in original purpose, so proved the most interesting benchmark. For more details on the baselines, experiment setup, input differences and model sizes please see A.3 and 11."}, {"title": "Stocknet", "content": "The most similar dataset to TempStock was the Stocknet dataset (Xu and Cohen, 2018), which consists of Tweets and price values from a selected batch of stock tickers. Stocknet is different from TempStock as it is a unimodal dataset, containing no graphical component, and is furthermore focused on binary price change rather than momentum shift (as measured by MACD crossing in TempStock). Nonetheless, Stocknet represents one of the only datasets to our knowledge organized in lag periods and is therefore relevant as a benchmark for the MEANT model.\nSince the StockNet dataset does not have a visual input, we implemented a MEANT model without the visual capabilities called MEANT-Tweet-price. We ran MEANT-Tweet-price against TEANet (Zhang et al., 2022) which was originally evaluated by the authors on the StockNet dataset, as well as the StockNet model itself (Xu and Cohen, 2018). Details about the StockNet task, baselines used, training settings, input differences can be found in A.3.2, 10 and 11."}, {"title": "Results", "content": "Tables 4 and 5 in sections 6.1 and 6.2 show the results for our experiments respectively."}, {"title": "TempStockLarge Experiment results", "content": "Observing 4, we can see that MEANT-XL outperformed all other models. MEANT-large performed comparably, coming in second for all three of those categories. The MEANT results in 4 use sequence projection, which performed better in this task (see 7).\nInterestingly, TEANet outperformed MEANT-base. TEANet was followed closely by the LSTM baseline, which due to TEANet being built atop an LSTM backbone (Zhang et al., 2022), and that the LSTM takes advantage of temporal information (the MACD values mt-i over all of the lag days). The MLP baseline outperforms all other BERT-based models. This illustrates the importance of the price information (further confirmed in 6) and attention without Query-Targeting does not perform well.\nViLT outperforms VL-BERT with and without the price modification. ViLT has a more similar encoding structure to MEANT, taking advantage of the patch embedding strategy, which is likely one reason for its performance advantage over VL-BERT. Since both of VL-BERT and ViLT are not designed to process lag periods, the models were at a severe disadvantage in terms of extracting temporal dependencies in the information they were given.\nFor a more in depth examination of how each modality affected performance, see A.1."}, {"title": "Stocknet results", "content": ""}, {"title": "Limitations", "content": "Here, we outline considerations, trade-offs and design decisions we have made:\n\u2022 Dataset To explore temporal information processing, we chose momentum buy signals in stock market data. We went with the MACD indicator because of its robustness, and correlation to strong positive returns against other indicators (Joshi, 2022; Chio, 2022). The serious drawback in this choice is in the infrequency of buy and sell signals that occur, which leads to a less robust dataset.\nWe gathered our stock price information from companies in the S&P 500. We chose this index because of its stability. However, as a result, we were unable to train our model on more extreme price patterns that are more common on obscure indexes (Goetzmann and Massa, 2003). Thus, in the case of extreme market events that result in periods of steep decline or rise would likely confuse the model.\n\u2022 MEANT The MEANT encoder is built atop the Kosmos-1 encoder architecture, that uses interleaved LayerNorms (Vu et al., 2022). The authors thought this to lead to increased numeric stability (Huang et al., 2023), which in turn helps prevent the exploding gradient problem. However, the inclusion of so many layerNorms in each encoder in our models can lead to an increase in bias, which eventually can lead to a serious overfitting problem (Xu et al., 2019). We chose to go ahead with this risk, as previous architectures have shown the stability gains from the interleaved normalizations to allow for better scaling (Wang et al., 2022; Huang et al., 2023).\nMEANT was trained to identify buy signals and sell signals, instead of trying to classify price periods on a more nuanced scale. We chose this path for simplicity's sake. For practical use on financial data, we would likely need more levels of categorization."}, {"title": "Conclusion and Future Work", "content": "We introduced a multimodal encoder with a novel temporal component comprised entirely of self-attention. MEANT outperforms previous models on the StockNet benchmark by 15%, and proves to be the most performant model on our own Temp-Stock benchmark. To our knowledge, MEANT-XL is the largest model to be applied to StockNet, and is the first multimodal model to contain an attention mechanism to deal with data over a lag period of days. MEANT combines the realms of language, vision, and time to produce SOTA results. We would like to explore different early fusion methods in order to make MEANT more robust against other common multi modal benchmarks, and expand upon our Query-Targeting strategy to emphasize relevant queries automatically, rather manually emphasizing any specific component such as the final lag day. We believe that the MEANT architecture has the potential to succeed on a wide variety of tasks. Furthermore, the image space that we trained MEANT on was limited. We would like to introduce more variation into our image inputs, to fully utilize the capabilities of that modality in our model."}, {"title": "Ethics Statement", "content": "Bias and Data Privacy: We acknowledge that there are biases in our study, including limiting our work to a specific time period, a small sample of securities and the general public, where we cannot verify they financial expertise in assessing markets. The data collected in this work will only be made available via Tweet IDs collected to protect X's users rights to remove, withdraw or delete their content. All datasets and Language Models are publicly available and were used under the license category that allows use for academic research.\nReproducibility: We make all of our code publicly available upon publication on Github\u00b3.\nUse case: We strongly advise against the use of our proposed model and dataset for financial decision making, including but not limited to automated or high frequency trading."}, {"title": "Albation Study", "content": "To examine the importance of the image and language modalities respectively, we also created many variations of the MEANT model, to target each modality and different combinations of them. Thus, we had a model for each modality individually, and each combination of the three modalities. MEANT-vision-price and MEANT-Tweet-price, for instance, take in the inputs x = [G, M] and x = [X, M] respectively. All variants were similarly fine-tuned and evaluated on the Temp-Stock task (5.1.1) over 15 epochs, with a training batch size of 16, a starting learning rate of 5e-5, the AdamW optimizer, and a cosine-annealing learning rate scheduler with warm restarts."}, {"title": "Sequence Projection Vs. Mean Pooling", "content": "Using sequence projection vs mean pooling in our temporal attention mechanism had an affect on our model performance across both of our tasks.\nLooking at 7, sequence projection outperformed mean pooling for our language encoder outputs on the TempStock task by a reasonable margin, the disparity especially noticeable between MEANT-Large-MP and MEANT-Large-SP.\nTempStock is built upon the MACD indicator, which relies on information over a longer time period than simple price prediction (Joshi, 2022), with the MACD calculation involving price averages over 12 and 26 days. Much of that information is not captured in our semantic inputs (Joshi, 2022) which tend to correlate to short term trends of a few days or so (see 6). Furthermore, Tweets tend to vary widely in terms of quality (Araci, 2019)(Xu and Cohen, 2018). What semantic information is pertinent to our final output must be captured with some degree of delicacy, similar to how Xu and Cohen (2018) discerns what Tweets to throw away. A lot of the semantic input is likely just noise which confuses our model, and the parameterized extraction of important Tweets for each lag day alleviates this problem to some extent."}, {"title": "Numeric vs Graphical Inputs", "content": "We wished to encode long range information into our model weights using images. The attention architecture in our transformer model is able to capture long range dependencies in disparate parts of images (Park and Kim, 2022). Only using numeric representation would not take advantage of attention for this purpose. We have run experiments with the numeric value representations of the graphs instead of the images themselves (labeled numeric-graph in the table below). We used an LSTM instead of TimeSformer for this input stream, and measured the performance vs MEANT."}, {"title": "Pretraining", "content": "For experimental purposes, we tried pretraining the MEANT language encoders on the TempStock Tweets.\nWe follow typical pretraining methods. For our language encoder, we used masked language modeling on our raw TempStock data. We trained our MEANT-small and MEANT-large language encoders on 4 NVIDIA p100 GPUs for 3 and 10 hours respectively. For MEANT-XL, we trained on an A100 GPU for 10 hours. A training batch size of 32 was used.\nFor the TimeSFormer backbone, we used masked image modeling with block and channel masking. The image encoders were trained on 4 NVIDIA p100 GPUs as well, for 20 hours. We used graphs G from the raw MACD data in Temp-Stock. For these encoders, we also used a training batch size of 32."}, {"title": "Training Details", "content": "All training was done with an AdamW optimizer (Loshchilov and Hutter, 2017) using betas of 0.9 and 0.999, a cosine annealing learning rate scheduler with warm restarts with 7 iterations for the first restart (Loshchilov and Hutter, 2016), and an initial learning rate of 5e-5. The experiments were all run on a single NVIDIA A100 GPU. More specific settings can be seen in 10."}, {"title": "TempStock Experiment Setup", "content": "TEANet makes use of a BERT-style encoder for the Tweet inputs, but uses an LSTM on the concatenated price-Tweet data rather then relying on a pure self-attention based mechanism. Furthermore, TEANet's temporal attention is a softmax-based mechanism which uses some simple concatenation to draw relationships between the last input day and the auxiliary days. TEANet can process lag periods, but cannot process the image inputs and is thus only fed the tweet and price information X and M.\nWe also fine-tuned VL-BERT (Su et al., 2019) and ViLT (Kim et al., 2021) on TempStock. VL-BERT is an early-fusion multimodal model, that uses a Faster RCNN (Girshick, 2015) to extract the image features, which are concatenated to the textual features before being fed to a BERT-style encoder. VL-BERT cannot process the price data, or data over the lag period, so we fed the model the graphs and Tweets from the final auxiliary day, those being Gt-1 and Xt\u22121 respectively.\nViLT is a single stream encoder that uses a ViT style patch embedding on the images, concatenating these to the text embeddings before feeding the concatenated input to a BERT-style encoder (Kim et al., 2021). ViLT, similarly to VL-BERT, cannot process price data, or data over a lag period. So we fed the model the same inputs as VL-BERT.\nWe recognized that the lack of price data could give tremendous advantages to TEANet and MEANT over ViLT and VL-BERT, as the labels of TempStock are determined directly from the price component. Thus, we added some extra functionality to our own variants of ViLT and VL-BERT models, called ViLT-price and VL-BERT-price respectively, to handle prices for better comparison of their multimodal strategies. We simply concatenated the price to our encodings of the images and Tweets before feeding the vectors into the attention mechanism. These models recieved the price, graphs, and text data from the last auxiliary day, Mt-1, Gt-1, and Xt\u22121 respectively.\nFinBERT and BERT were simply given the Tweets Xt-1 from the final auxiliary day. For parameter comparisons, see 3\nFor the TempStock experiment, we used 15 epochs for all MEANT models and a train batch size of 16. We decided to run TimeSFormer on the dataset as well, giving it the images over the lag period as a vision-only baseline. For more simple baselines, we ran a simple MLP on Temp-Stock without a lag functionality, only taking in the prices Mt-1 from the day before the target period. We also ran an LSTM (Sun et al., 2017), but with a different input of the MACD values mt-i for i = 1, ...5, to see if the recurrent properties could extract a pattern.\nWe used the lag periods from 4/10/2022-12/10/2023 for our training set, the periods from 11/10/2023-2/25/2023 as our validation set, and the periods from 2/25/2023-4/10/2023 as our test set."}, {"title": "StockNet Experiment Setup", "content": "The StockNet model was the predecessor to TEANet. StockNet took advantage of a similar Temporal attention mechanism, but used gated recurrent units rather then a BERT-style encoder to process the Tweets, and employed a the use of a latent representation with a variational lower bound for optimization (Xu and Cohen, 2018).\nWe ran BERTweet on the StockNet-dataset for comparison (Nguyen et al., 2020). For the inputs in this experiment, BERTweet can only process the immediate Tweets before the target day, Xt-1. The StockNet model can process the textual information and price information over the lag periods, those being X and M. TEANet X and M in their entirety as well, putting TEANet, StockNet, and MEANT-Tweet on relatively equal footing in terms of their processing capabilities. Experimental settings for each model can be seen in 10.\nStockNet is a binary classification problem, like TempStock. StockNet is built upon price movement. Built over a five day lag period, the classification of labels focused on the price change between the adjusted closing price of the last auxiliary day"}, {"title": "CMU-MOSI", "content": "We also decided to test our model on the CMU Multimodal Opinion-level Sentiment Intensity (MOSI) dataset (Zadeh et al., 2016).\nThis dataset includes audio, text, and video modalities compiled in 299 annotated video segments collected from YouTube monologue movie reviews. The data forms a binary sentiment analysis classification task.\nFor our purposes, we focus on the text and video modalities. We run MEANT on these inputs.\nCMU-MOSI is of interest because it examines videos with aligned text over time. Our vision backbone, the TimeSFormer model, is built for video inputs (Bertasius et al., 2021). We measured MEANT against previous SOTA baselines. TEASEL is a multimodal model that uses a pre-trained RoBerta as a backbone (Arjmand et al., 2021), using a CNN to break down the audio signals before coupling those with the text. UniMSE is an encoder-decoder model which breaks down the audio, visual, and textual modalities in fusion layers (Hu et al., 2022). UniMSE also uses a CNN to process the visual features. MMML is the current SOTA for the CMU-MOSI benchmark. MMML uses cross-modal attention, which is integrated into a fusion network (Wu et al., 2024). Interestingly, MMML does not take in visual inputs. The MEANT-large runs below were collected after 15 epochs of training, using the same optimizer and lr scheduler settings listed above. The other results were taken from previous work (Wu et al., 2024).\nLooking at the results above, MEANT-large performs considerably worse then previous SOTA benchmarks on the MOSI task. The disparity is expected. Query-Targeting in MEANT is designed to put great emphasis on the final component in the information period. In the CMU-MOSI task, this refers to the final frame in the video clip, along with the final text token, which have been aligned. The clips in the dataset are short movie reviews. The final frame in these clips does not contain significant information as to the entire clip (Zadeh et al., 2016), in the manner that the final price day in a lag period does to a stock price (Zhang et al., 2022) (Xu and Cohen, 2018).\nFurthermore, the previous state of the art benchmarks are designed to handle the audio component, which is better aligned to the textual inputs then the video embeddings (Zadeh et al., 2016). MEANT was working off of the visual and textual inputs alone. Thus, the performance we do achieve speaks to the soundness of our current architecture.\nWe did run TEASEL and UniMSE on Temp-Stock, replacing the audio inputs to their CNNs with our graphical data G. We changed the models to support our price data M. They were trained over 15 epochs, and train batch size of 16, and all other experimental settings identical to those used in the original TempStock experiments.\ndimension of the entire input. One method would be to extend our Query-Targeting mechanism to learn a parameterized selection of the best target components, or to learn which parts of the input the other auxiliary dependencies need to be collected in relation to. This could involve a separate temporal matrix, as in Rosin and Radinsky (2022), or some sort of softmax query weighting prior to the attention computation. Creating a mechanism which can perform at the highest level on any temporally dependent benchmark remains an open problem."}, {"title": "TempStock Dataset Details", "content": "The tables below show the number of lag periods used in TempStock for each ticker."}]}