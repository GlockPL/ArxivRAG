{"title": "Geometric Algebra Meets Large Language Models: Instruction-Based Transformations of Separate Meshes in 3D, Interactive and Controllable Scenes", "authors": ["Dimitris Angelis", "Manos Kamarianakis", "Prodromos Kolyvakis", "George Papagiannakis"], "abstract": "This paper introduces a novel integration of Large Language Models (LLMs) with Conformal Geometric Algebra (CGA) to revolutionize controllable 3D scene editing, particularly for object repositioning tasks, which traditionally requires intricate manual processes and specialized expertise. These conventional methods typically suffer from reliance on large training datasets or lack a formalized language for precise edits. Utilizing CGA as a robust formal language, our system, shenlong, precisely models spatial transformations necessary for accurate object repositioning. Leveraging the zero-shot learning capabilities of pre-trained LLMs, shenlong translates natural language instructions into CGA operations which are then applied to the scene, facilitating exact spatial transformations within 3D scenes without the need for specialized pre-training. Implemented in a realistic simulation environment, shenlong ensures compatibility with existing graphics pipelines. To accurately assess the impact of CGA, we benchmark against robust Euclidean Space baselines, evaluating both latency and accuracy. Comparative performance evaluations indicate that shenlong significantly reduces LLM response times by 16% and boosts success rates by 9.6% on average compared to the traditional methods. Notably, shenlong achieves a 100% perfect success rate in common practical queries, a benchmark where other systems fall short. These advancements underscore shenlong's potential to democratize 3D scene editing, enhancing accessibility and fostering innovation across sectors such as education, digital entertainment, and virtual reality.", "sections": [{"title": "1 INTRODUCTION", "content": "Our primary objective is to enable the instruction-based repositioning of objects within 3D, interactive and controllable scenes. Such rearrangement tasks are defined through various descriptors such as object poses, visual representations, descriptive language, or immersive interactions by agents within the desired end states. In this work, we focus primarily on textual instructions. This activity forms a part of the broader challenge in Embodied AI known as the rearrangement of environments, where the goal is to configure spaces into predetermined states [Batra et al. 2020]. The ability to reposition objects accurately and intuitively is essential, especially given its significant implications across fields such as digital entertainment, virtual reality (VR), simulation training, and architectural visualization.\nTraditionally, this task has demanded intensive manual effort and specialized knowledge, restricting the efficiency and accessibility of 3D scene editing. Recent developments in Foundational Models [Bommasani et al. 2021] represent a paradigm shift, suggesting that complex scene editing could become more intuitive and accessible through simple text-based instructions. However, existing machine learning-based scene representation techniques such as Neural Radiance Fields (NeRFs) [Mildenhall et al. 2020] and Gaussian Splatting [Kerbl et al. 2023] present substantial challenges for precise object repositioning due to their holistic nature, which often obscures individual object details and limits model generalization due to the plurality of possible scenes. [Yu et al. 2020]. In contrast, using separate mesh scene representations provides a viable solution by granting direct access to individual mesh components, which facilitates intuitive interactions and precise alignments tailored to specific editing requirements [Zhai et al. 2024]. However, the overarching question remains: How effectively can machine learning interact with these separate mesh components for editing?\nThe advent of Large Language Models (LLMs) [Jiang et al. 2023; Ramesh et al. 2021; Touvron et al. 2023] offers substantial advancements in human-computer interaction by utilizing linguistic proxies to facilitate instruction-based applications [De La Torre et al. 2024; Durante et al. 2024; Gong et al. 2024; Hong et al. 2023b; Yang et al. 2023]. Moreover, LLMs have significantly contributed to the field of Neurosymbolic AI [Garcez and Lamb 2023; Sheth et al. 2023], enhancing our capability to transform linguistic instructions into precise symbolic representations that bridge the gap between LLMs' comprehensive understanding abilities and the precision required for complex spatial transformations [Dziri et al. 2023; Hong et al. 2023a; Jignasu et al. 2023]. This LLM - Neurosymbolic AI integration prompts a critical inquiry into the optimal symbolic notation that encapsulates geometric transformations in a manner that LLMs can readily interpret. Geometric Algebra (GA), also known as Clifford Algebra, provides a robust mathematical structure ideal for managing transformations and interactions of geometric objects, which has been widely applied in Computer Graphics [Gunn and De Keninck 2019; Hildenbrand and Rockwood 2022; Papaefthymiou et al. 2016; Papagiannakis 2013].\nIn this paper, we use a specific GA model called 3D Conformal Geometric Algebra (CGA) to effectively integrate intuitive linguistic instructions with precise geometric operations, offering a more accessible method for editing separate mesh 3D scenes. This method utilizes the zero-shot learning capabilities of Large Language Models (LLMs), which allows for adaptability to new 3D environments without the requirement for scene-specific training. Our principal contribution is the development of shenlong, a system tailored for separate mesh scene editing, particularly designed for object repositioning through textual descriptions. Integrated within the ThreeDWorld (TDW) framework [Gan et al. 2021], shenlong supports the Unity3D Engine and enhances VR-ready 3D scene interaction capabilities. Through rigorously designed experiments, we show that shenlong markedly surpasses existing LLM-based alternatives, including those used in NVIDIA's Omniverse, in terms of object repositioning within 3D and interactive scenes. A critical aspect of our work is the detailed examination of the limitations currently faced by LLM solutions in object repositioning tasks. Furthermore, we illustrate that LLMs can proficiently generate and manipulate CGA operations with minimal input. Ultimately, our research marks a significant advancement towards democratizing the creation and manipulation of digital environments by simplifying the complex technicalities traditionally involved in 3D scene editing."}, {"title": "2 RELATED WORK", "content": "We now present the background pertinent to our research, outlining the foundational concepts and related advances in the field."}, {"title": "2.1 Agent-Driven Object Rearrangement", "content": "In the realm of object rearrangement, understanding reusable abstractions through geometric goal specifications ranges from simple coordinate transformations to complex multi-object scenarios [Batra et al. 2020]. Chang et al. [2023] approach rearrangement as an offline goal-conditioned reinforcement learning problem where actions rearrange objects from an initial setup in an input image to conform to a goal image's criteria. Kapelyukh and Johns [2023] propose learning a cost function through an energy-based model to favor human-like object arrangements. Simeonov et al. [2023] address rearrangement tasks using Neural Descriptor Fields [Simeonov et al. 2022], assigning consistent local coordinate frames to task-relevant object parts, localizing these frames on unseen objects, and aligning them through executed actions. Furthermore, Zhai et al. [2024] integrate scene graphs with diffusion processes for editable generative models, yet require extensive training and access to the complete scene graph, highlighting the efficiency of our localized transformation approach. Diverging from methods that depend on extensive scene-specific training, our approach leverages the generalization capabilities of LLMs to simplify object repositioning tasks. In contrast to Kwon et al. [2024], who limit their LLM applications to Euclidean spaces for predicting end-effector poses, and Mavrogiannis et al. [2023], who necessitate numerous and cumbersome predefined predicates for translating cooking instructions into Linear Temporal Logic, our method expoits CGA and decomposes repositioning tasks into a minimal set of primitive transformations. Unlike the LLMR approach [De La Torre et al. 2024], which uses specialized LLMs to generate C# Unity code for scene creation and editing, our method avoids direct code generation, thereby ensuring precise scene manipulation without the complexities of writing and debugging code."}, {"title": "2.2 Machine Learning Applications of Geometric Algebra", "content": "The incorporation of GA into neural computation was initially introduced in [Pearson and Bisset 1994], with subsequent developments introducing multivector-valued neurons for radial basis function networks [Corrochano et al. 1996], multilayer perceptrons (MLPs) [Buchholz 2000; Buchholz and Sommer 2001], and various neural network architectures [Bayro-Corrochano 2001; Buchholz and Le Bihan 2008; Buchholz and Sommer 2008; Buchholz et al. 2007]. Currently, GA-based neural networks have been applied in various domains, including signal processing [Buchholz and Le Bihan 2008], robotics [Bayro-Corrochano et al. 2018], partial differential equation modeling [Brandstetter et al. 2022], fluid dynamics [Ruhe et al. 2023], and particle physics [Ruhe et al. 2024]. Furthermore, novel GA-based architectures such as multivector-valued convolutional neural networks (CNNs) [Li et al. 2022; Wang et al. 2021], recurrent neural networks [Kuroe 2011; Zhu and Sun 2016], and transformer networks [Brehmer et al. 2024; Liu and Cao 2022] have been introduced, demonstrating the versatility and efficacy of GA in enhancing neural network capabilities for geometrically oriented tasks. Unlike these approaches, our work does not incorporate geometric algebraic components directly within deep learning architectures. Instead, we utilize geometric algebra as a communicative mediator between the LLM and our object rearrangement application. This method emphasizes the use of geometric algebra primarily as a tool to enhance the interaction and translation of complex geometrical tasks into understandable formats for the LLM, fostering more effective problem solving capabilities in practical applications. Recently, [Wang et al. 2023] fine-tuned ChatGPT with a large curated collection of textual documents on Geometric Algebra, aimed at developing customized learning plans for students in diverse fields. Our work diverges from the approach of fine-tuning LLMs and instead investigates whether LLMs can effectively utilize geometric algebra for object rearrangement tasks with minimal prompting."}, {"title": "3 CONFORMAL GEOMETRIC ALGEBRA", "content": "In his seminal 1872 Erlangen Programme, Felix Klein introduced the revolutionary idea that geometry is best explained by algebra, asserting that algebraic structures govern geometric shapes [Hawkins 1984]. Building on this, quaternions have resurged since the late 20th century due to their compact and efficient representation of spatial rotations. GA further unifies and extends algebraic systems, including vector algebra, complex numbers, and quaternions, through the concept of multivectors. Unlike traditional approaches, which treat scalars, vectors, and higher-dimensional entities separately, GA provides a cohesive framework to uniformly represent and manipulate these entities.\nAt the heart of GA is the concept of the geometric product, which generalizes the dot product and the cross product in Euclidean spaces. Additionally, as all products can be defined using only the geometric one, we need only use the latter one along with addition, scalar multiplication, and conjugation to perform any multivector manipulation. In the context of this work we will be employing CGA, a 32-dimensional extension of quaternions and dual-quaternions [Rooney 2007], where all entities such as vertices, spheres, planes as well as rotations, translations and dilations are uniformly expressed as multivectors [Kamarianakis and Papagiannakis 2021].\nFor example, assuming the standard CGA basic elements {e1, e2, e3, e4, e5 }, a basis of CGA would consist of all 32 (2^5) combinations of up to five of them via geometric product, i.e., {1, ei, eiej, eiejek, eie jekel, e1e2e3e4e5 for 1 < i < j < k < l \u2264 5}. For convenience, we define the vectors eo = 0.5(e5 \u2212 e4) and e\u221e = e4 + e5, as well as denote products of basic elements using only subscripts, e.g., eijk := eiejek. Using this notation, a sphere s centered at x = (x1, x2, x3) with radius r amounts to the CGA multivector:\n$S = x_1e_1 + x_2e_2 + x_3e_3 + (x_1^2 + x_2^2 + x_3^2 - r^2)e_\\infty - \\frac{1}{2}e_0. $  (1)\nNotice that (a) setting r = 0 would yield the respective multivector for the point x and that (b) given S we can extract both x and r.\nIn this algebra, a translation by (t1, t2, t3) amounts to\"\n$T = 1 - 0.5(t_1e_1 + t_2e_2 + t_3e_3)e_\\infty,$  (2)\nthus its inverse would be\n$T^{-1} = 1 + 0.5(t_1e_1 + t_2e_2 + t_3e_3)e_\\infty,$  (3)\nAs noted in Kamarianakis and Papagiannakis [2021], a rotation that would normally be expressed by the unit quaternion:\n$q := a - di + cj - bk,$  (4)\ncan be represented by the respective rotor:\n$R = a + be_{12} + ce_{13} + de_{23}. $  (5)\nIt then becomes apparent that the inverse of R is\n$R^{-1} = a - be_{12} - ce_{13} - de_{23}. $ (6)\nFinally, the multivector:\n$D = \\frac{1-d}{1+d} + \\frac{e_{45}}{1+d} $ (7)\ncorresponds to a dilation of scale factor d > 0 with respect to the origin, whereas it holds that:\n$D^{-1} = \\frac{(1+d)^2}{4d} + \\frac{d^2-1}{4d}e_{45}. $  (8)\nEquations (7) and (8) appear in bibliography with e45 replaced by the equivalent quantity e\u221e\u2227eo, where \u2227 denotes the wedge (or outer) product.\nTo apply transformations M1, M2,..., Mn (in this order) to an object, we define the multivector M := MnMn\u22121 \u00b7\u00b7\u00b7 M1, where all intermediate products are geometric. Shenlong is capable of identifying and generating the intermediate transformations Mi that should be applied to an object and properly returns M. As a composition of rigid body objects and dilations, the resulting multivector is known to be equivalent to a simpler product M = TRD of translation T, a rotation R and a dilation D. By extracting T, R and D, we can express the encapsulated information of M as a translation vector, a unit quaternion and a scale factor which can be ingested by TDW and Unity to visualize the edited object.\nTo extract T, R and D from a generated CGA product M = TRD, we follow the methodology presented in [Kamarianakis et al. 2024]. Specifically, we apply M to a sphere C, centered at origin, with radius equal to 1. The multivector C' corresponding to the transformed sphere can be evaluated via the sandwich product C' = MCM\u22121, and we can therefore extract its center x and its radius r. However since C' is the image of the unit, origin-centered, sphere C after applying M = TRD, i.e., first a dilation by D, then a rotation by R and finally a translation by T, its radius should be equal to the scaling factor of D and its center should be the translation vector corresponding to T. Since T and D are therefore known, along with M = TRD, we can evaluate R := T\u22121MD\u22121 and therefore the corresponding unit quaternion using (4) and (5)."}, {"title": "4 METHODOLOGY", "content": "Below we provide the high-level overview of our system, illustrated in Figure 2. When shenlong accepts a query, e.g., move the 'sofa' to the right, it replaces each object name with a corresponding variable, such as move X1 to the right. Shenlong accepts object names enclosed in quotes and sequentially assigns new variables for each unique match found. The templated queries generated allow the LLM to perform symbolic reasoning. Following the substitution of object names with variables, shenlong prompts an LLM to generate the corresponding CGA transformations for the user's query,\nIt should be noted that shenlong operates at a local level, lacking a comprehensive understanding of other objects in the scene. While this approach enhances processing efficiency, it may result in collisions with other objects. To address this, we observe that the fuzziness of queries, such as move the 'sofa' to the right, allows for multiple possible solutions. Upon detecting a collision, shenlong explores perturbed transformations to resolve the issue, e.g., a d upward translation on Z-axis, ensuring minimal deviation from the intended initial transformation. As a final remark, the proposed system is integrated within the ThreeDWorld (TDW) framework [Gan et al. 2021], supports Unity3D Engine and fosters VR-ready 3D scenes, augmenting human VR interaction."}, {"title": "4.1 LLM Processing & CGA Formulation", "content": "Shenlong harnesses CGA to express spatial transformations in a 3D environment, leveraging the precision of this rigorous mathematical framework. Shenlong replaces object names with variables; a vital step for abstracting the user's intent and preparing the data for algebraic processing. Each object X\u012f in the scene is represented by an axis-aligned bounding box defined by two points, Xmax and Xmin, which encapsulate the object's spatial extent. This bounding box model simplifies the calculation of movements and rotations by providing clear, definable limits to each object's position.\nThrough careful prompting, we instruct the LLM on the key facts and operations of CGA. The complete prompt template is provided in Figure 5. Below are some key remarks. We explained how coordinate extraction can utilize the inner product | operation to precisely isolate spatial coordinates. For example, (Xmax|e2) corresponds to the y coordinate of Xmax. Additionally, we defined the outer (or wedge) product, which establishes planes for rotational operations, and details the mechanisms for both translation and rotation rotors. These rotors are not only defined individually but are also combined through rotor composition, enabling complex sequential transformations essential for accurate scene manipulation. To effectively guide the LLM in utilizing the rotation and translation rotors, we provided a total of five illustrative examples. These include a single example each of rotation and translation to demonstrate basic movements, one compositional example that integrates both types of transformations, and two additional examples addressing more complex, fuzzy queries, such as move on top of and move next to another. This small diverse set of examples ensures the LLM effectively understands and implements the necessary algebraic operations for object repositioning. Despite the limited number of examples, i.e., only five, we demonstrate in Section 5.2.3 that the LLM can still generalize to complex queries. Finally, the LLM responds by generating a JSON output that includes the specified rotors for composition to be applied to each object."}, {"title": "4.2 Collision Detection Module", "content": "Similar to Yang et al. [2023], our system conducts a Depth-First Search (DFS) on a constructed 3D grid surrounding the colliding objects, identifying the first valid solution within the considered search space. We only consider collisions that can be determined using the bounding box information for each object. To prevent placing objects directly on the corners of the 3D grid, we incorporate a fixed buffer zone to ensure adequate spacing during object placement. Each object is described by 6 parameters, corresponding to its axis-aligned bounding box: (x, y, z) for the center coordinates, w, d and h for the width, depth and height. The process starts with initial placements of the target objects and concludes at the first occurrence of a valid placement. We prioritize solutions that are close to the original configurations, exploring all possibilities and gradually allowing for more distant solutions in a linear progression. Our goal is to minimize disruptions from the initial configuration while preserving the spatial integrity of the scene. This collision resolution operates within a set timeframe (e.g., 0.5 or 1 second). Finally, all transformations are applied."}, {"title": "5 EVALUATION", "content": "We conduct comprehensive human evaluations to assess shenlong in object repositioning, engaging 20 annotators: 5 3D designers, 5 game programmers, and 10 individuals outside the gaming and 3D design discipline areas. This study examines shenlong's proficiency in editing diverse scenes. Through these user studies, we demonstrate that, by harnessing CGA in our prompting strategies, we significantly outperform baseline alternatives, including those made with NVIDIA Omniverse (see Section 5.2.1)."}, {"title": "5.1 Experimental Setup", "content": "We generated ten diverse scenes using the HOLODECK framework [Yang et al. 2023] for human evaluation, encompassing various settings including living rooms, wine cellars, kitchens, and medical operating rooms. To facilitate the integration of diverse scenes into the TDW framework, we developed a custom importer. For each scene, we created five variations of templated queries, populating them with objects randomly selected from within the scene. We evaluated 50 distinct queries for each scene, resulting in a total of 2,500 scenes (coupled with prompts) for human assessment. For each query, we compared the initial scene with the resulting scene after processing by the two baseline system and shenlong. Examples of the human evaluation are presented in the Supplementary Material. For each edited scene, we displayed top-down view images and a 360-degree video view, asking annotators to assess the accuracy of the editing performed. Each object repositioning query was evaluated by five annotators, and a result was considered valid only if all annotators unanimously agreed on the assessment. Due to space limitations, the evaluation queries only are displayed on the y-axis of Figure 4. It should be noted that all preliminary experiments on dilation-related queries with shenlong and other baselines achieved a perfect success rate, leading us to omit these queries. The queries are categorized into five groups, each containing ten queries that evaluate the system's performance across a progressively increasing difficulty gradient. Below, we provide further details on the group categories used in our analysis:\n\u2022 Simple Queries: Cover basic actions such as rotations and movements along specified axes or planes of a single object.\n\u2022 Compositional Queries: Involve combinations of relative movements and rotations among two or more objects.\n\u2022 Fuzzy Queries: Task systems with interpreting and executing spatial and orientation-specific actions, such as proximity adjustments and directional alignments.\n\u2022 Compositional Fuzzy Queries: Combine multiple elements from compositional and fuzzy queries.\n\u2022 Hard Queries: Represent the most challenging scenarios that test the limits of each system's processing capabilities.\nThe classification of \"simple\" and \"composite\" queries is based on the study of Manesh et al. [2024] on natural language in virtual environment creation. Building on this, our work further explores fuzzy and more complex scenarios involving the repositioning of multiple objects."}, {"title": "5.1.1 System Configuration", "content": "We employed OpenAI's GPT-4 model, specifically the gpt-4-1106-preview variant, to process scene editing queries. Shenlong executed a distinct API call for each query. The evaluation was done on a laptop with Ubuntu 22.04.4 LTS and AMD Ryzen\u2122 9 4900HS Mobile Processor (8-core/16-thread, 12MB Cache, 4.3 GHz max boost), NVIDIA\u00ae GeForce RTX\u2122 2060 with Max-Q Design 6GB GDDR6 and 16GB DDR4 RAM at 3200MHz. Rendering was performed using Unity\u2122 2022.3.9f1."}, {"title": "5.2 Benchmarking", "content": "In this section, we assess the performance of our system compared to established baselines. In the following, we provide an overview of the baseline systems against which our solution is measured, the performance metrics that form the criteria for comparison, and a detailed discussion of the results."}, {"title": "5.2.1 Baseline Systems", "content": "One of our baseline systems, for comparison is one of the publicly available prompts from NVIDIA's Omniverse\u00b9. This prompt operates by generating a JSON output that details object placements within a 3D space, similar to our system. Contrary to shenlong, the Omniverse prompt imposes no constraints on the reasoning scheme that the LLM should follow, requesting only the final positions of the objects. The Omniverse prompt accepts input specifications for each object, including its name, dimensions along the X, Y and Z axes, and a centrally located origin point. To ensure this baseline prompt is comparable to our system, we include only points of interest relevant to the scene editing query, rather than the entire scene description as initially done. This refined input approach significantly impacts the LLM's response time. Our experiments demonstrate that providing only the relevant points of interest leads to a substantial decrease in response time-specifically, an average of 3.3\u00b10.1 times faster-without compromising overall accuracy. Due to these findings, we report results exclusively from this optimized methodology. Finally, we have extended the prompt by incorporating Euler angles for object orientation.\nOur second baseline model is derived from recent work showing that LLMs can predict a dense sequence of end-effector poses for manipulation tasks [Kwon et al. 2024]. We extend this concept to object repositioning on 3D scenes, treating it as a form of zero-shot trajectory generation. In this context, we task the LLM with constructing the necessary translation and rotation matrices to define the final trajectory, adapting the underlying techniques to suit scene manipulation objectives. We consider this baseline model to be the closest to our approach, as it guides the LLM to operate using templated functions that correspond to translation and rotation functions within the Euclidean space. This method aligns with our use of structured prompts that direct the LLM to generate specific geometric transformations necessary for scene editing tasks."}, {"title": "5.2.2 Performance Metrics", "content": "To evaluate the system's performance, we calculated the success rate per query across the ten different scenes, each with five templated variations. The success rate S for each scene editing query is calculated as follows:\n$S = \\frac{1}{M} \\sum_{i=1}^{M} X_{correct}(i)$\nwhere Xcorrect(i) is the characteristic function that equals 1 if the editing query was performed correctly for the i-th query, and 0 otherwise. Here, M represents the total number of queries, calculated as M = N \u00d7 k, where N is the number of different scenes considered (10 in our case), and k is the number of variations per scene (5 in our case). Similarly, we compare the average response time across the ten different scenes and their respective variations. The average response time T is calculated by: $T = \\frac{1}{M} \\sum_{i=1}^{M} t_i $, where ti represents the response time for the i-th query. It is commonly noted that LLMs may not always produce valid outputs. We adhere to the standard practice of allowing up to n retries. The reported timings include these retry durations. Notably, in all our experiments, LLMs generated valid prompts within n = 5 retries."}, {"title": "5.2.3 Results & Discussion", "content": "In this section we present a comprehensive analysis that spans overall performance metrics, detailed evaluations by query group, and granular analyses of individual query performances. These discussions aim to highlight significant findings, interpret the implications of the results, and explore potential areas for further improvement. In all reported figures, we refer to the modified baseline prompts for scene editing as Omniverse and Euclidean, respectively derived from NVIDIA's Omniverse usage examples and methods akin to zero-shot trajectory generation. We name the latter Euclidean because it closely aligns with our method's approach in guiding the LLM to generate transformations such as rotations and translations within Euclidean space, in contrast to our use of CGA. Finally, we refer to shenlong's prompt as CGA.\nPerformance Analysis by Query Group. Figure 3 presents a detailed evaluation of the prompts' performance across different groups of queries. Each group's results are analyzed to highlight specific strengths and weaknesses of the system in handling varying complexities. Specifically, the effectiveness and efficiency of the CGA, Omniverse, and Euclidean prompts were evaluated across the five categories of queries: Simple, Compositional, Fuzzy, Compositional Fuzzy, and Hard. We focused on two key performance metrics: average success rate and average response time, providing insights into each system's scene editing capabilities.\nOverall, CGA achieves the highest average success rate of 0.80 and the lowest average response time of 18 seconds (p < 0.05), with the results being statistically significant. Interestingly, all prompts exhibited the same overall standard deviation, both in terms of success rate and response time, indicating consistent variability across the different editors. To assess the differences between the systems, we performed Student's t-tests. In comparison, the Omniverse and Euclidean prompts achieve success rates of 0.74 and 0.72, and average response times of 21.5 seconds and 24.5 seconds, respectively. While the differences in success rates among Omniverse and Euclidean prompts are not statistically significant, the variations in average response times are. We conjecture that the lack of statistical significance in terms of success rates among the systems can be attributed to the nature of the reasoning methods employed. Both the Omniverse and Euclidean prompts rely on classical geometrical reasoning, which may lead to similar levels of performance. In contrast, shenlong utilizes a different approach to reasoning, which not only reflects in its superior performance but also in its statistical significance. This suggests that the distinct reasoning method employed by our system may contribute to its enhanced effectiveness.\nFocusing on the success rate metric, CGA consistently exhibits high success rates across all query types, particularly excelling in complex scenarios such as Compositional Fuzzy queries, indicating robust scene understanding. Given that the success rates of the Euclidean and Omniverse systems are not statistically significant, it is evident that CGA provides a relative boost of 9.6% over their average performance. In contrast, Omniverse demonstrates variable performance, experiencing a drop in simpler queries but excelling in fuzzy queries. Conversely, the Euclidean system performs well in simpler queries but shows significant weaknesses in both fuzzy and composite fuzzy queries. CGA appears to integrate the advantages of both systems, performing exceptionally well across simple, fuzzy, and compositional queries. However, it is important to note that with all prompts, ChatGPT-4 shows average performance on hard queries, suggesting potential deficiencies in spatial reasoning.\nRegarding response time, Omniverse exhibits consistent average response times across various query groups, regardless of query difficulty, with the notable exception of hard queries, which show the slowest response times. Importantly, CGA demonstrates a 16% relative decrease in response time compared to Omniverse, which is the fastest among the baseline systems. In contrast, the Euclidean prompt demonstrates an increasing trend in response times correlating with query difficulty, peaking with fuzzy, compositional fuzzy, and hard queries. This trend is noteworthy as it suggests that response times increase as query complexity rises. The Euclidean prompt consistently records the longest response times across the complex queries, posing challenges in time-sensitive applications. Meanwhile, the CGA prompt also displays an increasing trend in response times with escalating query difficulty. Notably, it presents slower response times for simple and simple compositional queries despite achieving a 100% success rate. Overall, CGA maintains competitive response times suitable for practical applications, with its response times scaling appropriately with the complexity of queries and achieving the best overall response performance.\nDetailed Performance by Individual Query. Figure 4 delves into the performance metrics for each individual query within the groups. This detailed breakdown provides insights into the prompts' consistency, efficiency, and accuracy in executing scene editing tasks with varying degrees of complexity, facilitating a granular understanding of its operational effectiveness. With regard to the Simple Queries, all systems generally exhibit high success rates for straightforward tasks even in geometrically nuanced rotations such as those over specific planes, e.g., Rotate X1 over the e1 plane, with CGA consistently achieving perfect scores. This highlights CGA's superior handling of precise geometric transformations. The Compositional Queries, which involve a combination of movements and rotations, we see CGA and Euclidean performing robustly, indicating effective integration of complex instructions. Omniverse, however, shows variability, particularly underperforming in scenarios like Rotate over the e3 plane and move X1 to the right where it achieves a 0.7 success rate compared to 1.0 by CGA.\nIn dealing with spatially ambiguous commands, i.e., Fuzzy Queries, Omniverse and CGA outperform Euclidean, especially notable in queries like Move X1 away from X2 where Euclidean drops to 0 against Omniverse's perfect score. Interestingly, the Omniverse prompt struggles with the seemingly simple query, Move X1 on the left of X2, displaying a stark contrast to the near-perfect performance of the other systems. Although Omniverse is not presented with a specific move below example (only on top is provided in their prompts), all systems show generalization capabilities, with CGA achieving the highest success rate of 0.6. Similar generalization is observed with the Swap X1 and X2 query, where all systems are expected to perform well given their operational logic at the coordinate level; however, Omniverse shows surprisingly lower performance. In contrast, while the Euclidean and CGA systems perform less effectively on the Move X1 near X2 query, Omniverse excels with perfect success rates. Lastly, all prompts consistently fail the Rotate X1 to face X2 query. Intriguingly, despite being equipped with Euler angles, Omniverse also fails, suggesting a significant challenge area for current scene editing technologies.\nThe Compositional Fuzzy Queries category presents a notable challenge, combining fuzzy directives with multiple actions. In this category, Omniverse and CGA show comparable performances, indicating their effective handling of combined directives. However, it is important to note discrepancies in the Euclidean system's performance. While it achieves good results on individual components of these queries, it performs poorly when actions are combined, as seen in queries like Move X1 next to X2 and rotate X3 by 90 degrees over the e2 plane and Swap X1 and X2 and move X3 on the left by 1 unit. This inconsistency highlights potential limitations in the Euclidean prompt's ability to effectively integrate multiple spatial manipulations within a single query.\nAll systems exhibit challenges with the most demanding Hard Queries, which require intricate arrangements and precise manipulations. Notably, the Euclidean prompt displays extremely inconsistent behavior: it achieves perfect success rates on complex queries such as Place X1 on top of X2 and X3 below X2 and Align X1, X2 and X3 in a straight line, where other systems face difficulties. Conversely, it presents a zero success rate on tasks like Distribute X1, X2, X3 equally in a circle around X4 or Arrange X1, X2, X3 in a vertical line, then rotate them by 45 degrees keeping X1 intact, where other systems manage average performances. This variability suggests that while the Euclidean prompt excels in certain types of spatial reasoning, it may lack robustness in scenarios requiring dynamic spatial transformations or uniform object distribution. These findings underscore the need for further optimization and testing of the Euclidean system to enhance its consistency across a broader range of complex scene editing tasks. In summary, while CGA emerges as the most capable across a broad range of tasks, challenges remain, particularly in Hard and Compositional Fuzzy queries, underscoring the need for further advancements in LLM reasoning capabilities."}, {"title": "6 LIMITATIONS & FUTURE WORK", "content": "Object Representations. Currently, our system utilizes multiple intermediate representations of objects, including textual, templated, and bounding box references, requiring exact object names. To enhance this, we plan to implement semantic similarity measures at the token level or more advanced similarity searches using distributed representations. Additionally, we aim to enrich the system with further object information, such as orientation information, to better handle sophisticated queries involving relative rotations. By improving the system's grasp of spatial relationships, improved performance in complex scenarios is expected.\nSystem Optimisations. Since we preprocess and create templated queries, it is straightforward to perform query caching optimisations [Mavrogiannis et al. 2023", "2023": "enabling nuanced handling of complex scenarios. The agent could be provided with a top-down view of the scene to facilitate"}]}