{"title": "PASSION: Towards Effective Incomplete Multi-Modal Medical Image Segmentation with Imbalanced Missing Rates", "authors": ["Junjie Shi", "Caozhi Shang", "Zhaobin Sun", "Li Yu", "Xin Yang", "Zengqiang Yan"], "abstract": "Incomplete multi-modal image segmentation is a fundamental task in medical imaging to refine deployment efficiency when only partial modalities are available. However, the common practice that complete-modality data is visible during model training is far from realistic, as modalities can have imbalanced missing rates in clinical scenarios. In this paper, we, for the first time, formulate such a challenging setting and propose Preference-Aware Self-diStillatION (PASSION) for incomplete multi-modal medical image segmentation under imbalanced missing rates. Specifically, we first construct pixel-wise and semantic-wise self-distillation to balance the optimization objective of each modality. Then, we define relative preference to evaluate the dominance of each modality during training, based on which to design task-wise and gradient-wise regularization to balance the convergence rates of different modalities. Experimental results on two publicly available multi-modal datasets demonstrate the superiority of PASSION against existing approaches for modality balancing. More importantly, PASSION is validated to work as a plug-and-play module for consistent performance improvement across different backbones. Code is available at https://github.com/Jun-Jie-Shi/PASSION.", "sections": [{"title": "1 INTRODUCTION", "content": "Multiple imaging modalities are widely used in clinical practice like multi-parametric magnetic resonance imaging (MRI) [2-4]. Such homogeneous multi-modal data representing the same media type provides various tissue contrast views and spatial resolutions, making it possible to quantitatively categorize histological sub-regions [9, 43]. Nevertheless, due to factors like image degradation, patient motion-related artifacts, incorrect acquisition settings, and cost constraints, MRI sequences may be incomplete [15, 17, 22]. Consequently, despite the great success of multi-modal learning on medical segmentation [7, 16, 20, 41, 43], it can hardly be directly deployed in practice.\nTo deal with missing modalities, various approaches have been proposed for incomplete multi-modal segmentation. One straightforward strategy is to synthesize missing modalities with available full-modality data through generative adversarial networks [14],"}, {"title": "2 RELATED WORK", "content": "2.1 Incomplete Multi-modal Segmentation\nIncomplete data is a common and long-standing issue in practical scenarios, particularly in clinical practice. The most commonly studied task is incomplete multi-modal medical image segmentation usually involving MRI images with various missing components [17]. As standard multi-modal segmentation methods built on full-modality data [20, 43] would encounter severe performance degradation given only incomplete modalities, it is of great value but also challenging.\nTo pursue effective multi-modal medical image segmentation, extensive research has been broadly explored including image synthesis, knowledge transfer, and shared representation learning. Specifically, Sharma and Hamarneh [32] used a U-Net [8, 31] generator to impute missing modalities, with a generative adversarial network [14] learning to discriminate between real and synthesized inputs. Ma et al. [25] proposed a meta-learning algorithm [13] to reconstruct the features of missing modalities. Wang et al. [39] trained a teacher-student framework for each subset of modalities. Azad et al. [1] proposed a style-matching mechanism to reconstruct missing information from a full-modality network. Chen et al. [6], Wang et al. [36] trained uni-modal models by transferring privileged knowledge from a full-modal teacher to each uni-modal student. Liu et al. [23] focused on self-supervised pre-training. However, in IDT, there may not exist sufficient full-modality data as a target for image synthesis and full-modal teacher training, making them hard to deploy in real-world training.\nTo overcome such limitations, Havaei et al. [17] and Dorent et al. [10] computed variational statistics to construct a uniform representation for segmentation. Chen et al. [5] and Ding et al. [9] performed shared representation fusion by modality re-weighting and attention-gating modules. Zhao et al. [45] introduced graph convolutional networks for incomplete brain tumor segmentation. Wang et al. [35] learned shared and specific features by distribution alignment and domain classification. Zhang et al. [44] and Shi et al. [33] introduced transformers to exploit both intra- and inter-modal dependence for feature fusion. Qiu et al. [30] proposed a group self-support algorithm to make use of the sensitive modality's specific features. However, such shared representation learning-based methods were proposed only for incomplete-modality inference while ignoring data missing in training. In other words, the frequency of each modality during training remains the same. Konwer et al. [21] discussed the limited full-modality data scenario and proposed a meta-learning method to enhance modality representations. Unfortunately, how to pursue effective multi-modal medical image segmentation with imbalanced missing rates is under-explored, which is more valuable in clinical practice."}, {"title": "2.2 Imbalanced Multi-modal Learning", "content": "Due to modality discrepancy, multi-modal learning naturally encounters the concern of fairness and imbalance. Wang et al. [37] found that different modalities could overfit and generalize at different rates and thus obtain sub-optimal solutions when jointly training them using a unified optimization strategy. Peng et al. [28] proposed that the better-performing modality would dominate gradient updating while suppressing the learning process of other"}, {"title": "3 METHODOLOGY", "content": "3.1 Problem Definition\nGiven a multi-modal training dataset containing N samples, each sample comprises M modalities. Let \\([P]\\) for any positive integer P denote the set \\(\\{1, 2, ..., P\\}\\). Each training sample is denoted as \\((\\{x_n^m\\}_{m \\in [M]}, \\{y_n^m\\}_{m \\in [M]})\\), where \\(n \\in [N]\\) is sample index. For a segmentation task, it is to assign each pixel with an individual category label from K categories. To formulate this, we use \\((x_{n,i}^m, y_{n,i}^m)\\) to denote the raw input and label of pixel i corresponding to modality m and sample n. For multi-modal segmentation studied in this paper where all modalities share a common label, \\(y_{n,i} = y_{n,i} = y_{n,i} = ... = y_{n,i}\\) holds. Let \\(C \\in \\mathbb{R}^{N \\times M}\\) be the modality presence indication matrix, and let \\(C_{nm}\\) denote the (n, m)-th entry of C. Then \\(C_{nm}\\) is 1, if modality m of sample n is available; otherwise \\(C_{nm}\\) equals to 0. The missing rate MR of modality m, \\(\\forall m \\in [M]\\), is calculated as \\(MR_m = (N - \\sum_{n \\in [N]} C_{nm})/N\\). We assume \\(MR_m \\in [0, 1)\\) to ensure that at least one sample of each modality is available during training.\nIncomplete Multi-modal Segmentation Baseline. In existing research, the SOTA paradigm [9, 33, 44] adopts m modality-specific encoders \\(E_m\\) and a shared fusion decoder \\(D_f\\). Layer-wise features extracted from available \\(E_m\\) are skipped like U-Net to be fused in \\(D_f\\). For the consistency of expression, we use \\(Z_n = D(x_n) \\approx D(x_n)\\) as the fused features of all available modalities of sample n in layer l, and \\(z_n^m = D(x_n^m)\\) as the features of only modality m. Denoting l = 0 as the output layer, the overall objective of the paradigm is"}, {"title": "Multi-Uni Self-Distillation", "content": "Motivated by knowledge distillation (KD) [18], which aims to transfer the teacher's \u201cdark knowledge\u201d to students via soft labels, we treat multi-modal knowledge as a common objective for each available uni-modal to balance inter-modal learning. As multi-modal knowledge is learned through all modalities, it may be dominated by certain modalities of lower missing rates. When penalized through KD, such modalities would be less emphasized as they are closer to multi-modal knowledge (i.e., soft labels). In this way, it is promising to re-balance modalities in IDT. Unlike previous KD-based works relying on a separate complete multimodal teacher based on PDT [1, 6, 36, 39], we prefer a unified network to transfer knowledge from multi-modal to uni-modal, denoted as multi-uni self-distillation. On the one hand, it reduces the difficulty of training a sufficiently robust teacher model under IDT. On the other hand, multi-modal knowledge is innate but under-exploited for uni-modal in a unified framework. Specifically, multi-uni self-distillation is composed of pixel-wise and semantic-wise self-distillation described in the following.\nPixel-wise Self-Distillation. As segmentation can be formulated as a pixel-level classification task, we propose to align the predictions of each pixel between multi-modal and uni-modal. It is based on the observation, validated through experiments, that feature-level alignment usually results in multi-modal performance degradation while logit alignment in deep supervision is a more robust option for imbalanced learning [26]. Therefore, pixel-wise multi-uni self-distillation for each uni-modal m is formulated as:"}, {"title": "Semantic-wise Self-Distillation", "content": "By learning more information from diverse modalities, the multi-modal teacher captures more robust intra- and inter-class representations [36]. Therefore, not only local pixel-wise knowledge but also global class-wise knowledge should be transferred to uni-modal. By using prototypes to represent the general features of a class, we hope to build multi-modal and uni-modal prototypes to achieve global knowledge transfer."}, {"title": "3.3 Preference-Aware Regularization", "content": "As discussed above, multi-uni self-distillation equally transfers knowledge to available uni-modal to balance inter-modal learning. However, it may still make those modalities with higher missing rates struggle to keep up with others. This is because the recurring uni-modal wins at optimization more often than others. Therefore, dynamically evaluating how strong (or weak) each uni-modal is compared to others and balancing the learning paces across them in IDT is crucial.\nRelative preference. Since different uni-modal students are born with unequal talents and expertise, measuring the learning progress of different students directly through their performance seems to be inappropriate. Noting that the multi-modal teacher usually prefers strong modalities that are easy to learn or have lower missing rates, we use the distance \\(D_m\\) to represent the relative"}, {"title": "Re-balancing regularization", "content": "Given the relative preference \\(RPM_n^m\\) of modality m for each sample n, it is expected to slow down the learning paces of the preferred modalities and speed up the neglected ones. Inspired by previous works on imbalanced multimodal learning [12, 28, 34], we develop two regularization items for each modality: task-wise and gradient-wise.\nNoting that strong modalities occupy the center of gravity in early training due to their larger data amounts and easy learning properties. In task-wise, we propose to identify and push the slow-learning modalities based on sample-level relative preference. If \\(RPM_n^m\\) is negative, we should further accelerate the slow-learning modality m. Therefore, we set a task mask \\(\\delta_m\\). If modality m is neglected, it should be accelerated for training by enhancing its semantic learning from multi-modal. Such a rule is formulated as:"}, {"title": "3.4 Overall Objective", "content": "Combining multi-uni self-distillation and preference-aware regularization, the overall optimization objective is written as"}, {"title": "4 EXPERIMENTS", "content": "4.1 Datasets and Evaluation Metrics\nTwo segmentation tasks with publicly-available multi-sequence MRI datasets are adopted for evaluation, including:\n(1) BraTS2020 [27]: The multimodal brain tumor segmenta-tion challenge (BraTS2020) dataset consists of 369 cases with ground truth labels and four MRI modalities (i.e., T1, T1c, Flair, and T2). Ground truth is provided with the normal tissue and three tumor sub-regions including the necrotic and non-enhancing tumor core (NCR/NET), the peritumoral edema (ED), and GD-enhancing tumor (ET). Following the task setting in the challenge, tumor classes are merged into the whole tumor (WT) including all tumor sub-regions, tumor core (TC) consisting of NCR/NET and ET, and enhancing tumor involves ET. The dataset is split into 219, 50, and 100 cases for training, validation, and testing respectively.\n(2) MyoPS2020 [29, 46]: The MyoPS 2020 challenge dataset consists of 25 cases of multi-sequence CMR (i.e., bSSFP, LGE, and T2), with labels being provided for myocardial pathology segmentation. Targets include the normal tissue, left ventricular blood pool (LVB), right ventricular blood pool (RVB), normal myocardium, myocardial edema, and myocardial scars of the left ventricle. Following standard practice, the last three classes are grouped as myocardium of the left ventricle (MYO). The dataset is split into 20 and 5 cases with multi-slices for training and test respectively.\nFor image pre-processing, each brain MRI volume is re-sampled to the 1mm\u00b3 resolution and each cardiac CMR scan is re-sampled into the same spatial resolution. Following [6, 9], we cut out the black background areas outside the brain, center-crop the heart regions, and further normalize the intensity of each volume to zero mean and unit variance. Considering the specificity of two datasets, we perform the former task (i.e., BraTS2020) in 3D and the latter (i.e., MyoPS2020) in 2D to verify the flexibility of PASSION.\nFor evaluation, two most commonly-used metrics are selected, including the Dice similarity coefficient (i.e., Dice) and Hausdorff distance (i.e., HD). Dice measures the voxel-wise accuracy and HD evaluates the surface distance. Higher Dice and lower HD indicate better segmentation performance. Quantitative evaluation is performed on subject-level volume segmentation to be consistent with the BraTS and MyoPS challenges."}, {"title": "4.2 Implementation Details", "content": "For comparison against SOTA imbalanced multi-modal learning approaches, mmFormer [44] is set as the backbone while for plug-and-play evaluation another two SOTA multi-modal segmentation methods RFNet [9] and M2FTrans [33] are included as backbones. All models are implemented and modified with the same basic-dimension size in Pytorch and trained using the optimizer AdamW [24] with an initial learning rate of 2e-4, a weight decay of 1e-4, and a batch size of 1 on NVIDIA Geforce RTX 3090 GPUs for 300 epochs. Specifically, we adopt a poly decay strategy with p = 0.9 during training and set the temperature \\(\\tau\\) for pixel-wise self-distillation as 4 and the hyper-parameters \\(\\lambda_1\\) and \\(\\lambda_2\\) as 0.5 and 0.1 respectively for the self-distillation loss. The parameter setting for re-balancing"}, {"title": "4.3 Evaluation on BraTS2020", "content": "Quantitative evaluation. Quantitative comparison results by introducing SOTA modality-balancing approaches to the baseline (i.e., mmFormer [44]) on BraTS2020 are summarized in Table 1. Compared to PDT, segmentation under imbalanced modality missing rates is more challenging, resulting in apparent performance degradation of the baseline. Through modality re-balancing, PMR [12] achieves marginal performance improvements under most settings while ModDrop [40] even brings negative effects. This is because modalities are imbalanced not only in data amounts but also in semantics. For instance, Flair and T2 are much more informative than T1/T1c. Given relatively limited Flair and T2, it is more challenging to explore sufficient semantic information, making both PMR and ModDrop fail. Simply re-balancing modalities without considering semantic misalignment can be counter-productive."}, {"title": "Qualitative evaluation", "content": "Exemplar segmentation results under different modality combinations on BraTS2020 are illustrated in Figure 4. Given only T2 (i.e., the fewest modality), all comparison approaches produce extensive false positives. With more modalities introduced, segmentation performance is gradually improved, indicating imbalanced learning, especially for T2. Comparatively, PASSION significantly outperforms comparison approaches by effectively reducing false positives. More importantly, relatively stable/consistent performance across different modality combinations shows its effectiveness in modality re-balancing in IDT."}, {"title": "4.4 Evaluation on MyoPS2020", "content": "Quantitative evaluation. Quantitative comparison results on My-OPS2020 are summarized in Table 1. Different from BraTS2020, modality-wise semantic information across modalities is closer in MyoPS2020. In other words, modalities contribute more equally to different sub-types instead of certain modalities being dominant for specific sub-types. As a result, both ModDrop and PMR achieve noticeable performance improvements by modality re-balancing. Comparatively, PASSION consistently achieves greater performance improvements under all modality settings, demonstrating its superiority in exploring richer semantic information. Visualized quantitative comparison in Figure 3 further validates this.\nQualitative evaluation. Exemplar segmentation results on My-OPS2020 are illustrated in Figure 5. Compared to BraTS2020, segmentation performance across different settings is more consistent even given only T2 (i.e., the least modality), indicating more equal semantic contributions of modalities. With more modalities, more false positives are recalled while more false positives are introduced. Comparatively, PASSION achieves the most consistent segmentation performance with the fewest false positives and false negatives."}, {"title": "4.5 Component-wise Ablation Study", "content": "We first validate the concept of relative preference defined in Section 3.3 by plotting RP curves with and without PASSION and"}, {"title": "4.6 Plug-and-Play Ablation Study", "content": "PASSION is expected to work as a plug-and-play module onto various backbones for modality re-balancing. To validate this, PASSION is integrated into SOTA incomplete multi-modal medical image segmentation methods and evaluated under various modality missing rates, as summarized in Table 3. Compared to PDT, there exists noticeable performance degradation under IDT for all backbones. By introducing PASSION, consistent performance improvements across different sub-types under various modality combinations are"}, {"title": "5 CONCLUSION", "content": "In this paper, we present PASSION to solve a new challenging task, namely incomplete multi-modal medical image segmentation with imbalanced missing rates. PASSION adopts pixel-wise and semantic-wise self-distillation to regularize modality-specific optimization objectives and preference-aware regularization in task-wise and gradient-wise to balance convergence rates of different modalities. Comprehensive evaluation demonstrates that PASSION outperforms SOTA modality-balancing approaches and works as"}]}