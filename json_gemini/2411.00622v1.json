{"title": "Lingma SWE-GPT\nAn Open Development-Process-Centric Language Model for Automated Software Improvement", "authors": ["YINGWEI MA", "RONGYU CAO", "YONGCHANG CAO", "YUE ZHANG", "JUE CHEN", "YIBO LIU", "YUCHEN LIU", "BINHUA LI", "FEI HUANG", "YONGBIN LI"], "abstract": "Large language models (LLMs) have demonstrated remarkable performance in code generation, significantly\nenhancing the coding efficiency of developers. Recent advancements in LLM-based agents have led to significant\nprogress in end-to-end automatic software engineering (ASE), particularly in software maintenance (e.g.,\nfixing software issues) and evolution (e.g., adding new features). Despite these encouraging advances, current\nresearch faces two major challenges. First, state-of-the-art performance primarily depends on closed-source\nmodels like GPT-4, which significantly limits the technology's accessibility, and potential for customization in\ndiverse software engineering tasks. This dependence also raises concerns about data privacy, particularly when\nhandling sensitive codebases. Second, these models are predominantly trained on static code data, lacking\na deep understanding of the dynamic interactions, iterative problem-solving processes, and evolutionary\ncharacteristics inherent in software development. Consequently, they may face challenges in navigating\ncomplex project structures and generating contextually relevant solutions, which can affect their practical\nutility in real-world scenarios.\nTo address these challenges, our study adopts a software engineering perspective. We recognize that\nreal-world software maintenance and evolution processes encompass not only static code data but also\ndevelopers' thought processes, utilization of external tools, and the interaction between different functional\npersonnel. Our objective is to develop an open-source large language model specifically optimized for software\nimprovement, aiming to match the performance of closed-source alternatives while offering greater accessibility\nand customization potential. Consequently, we introduce the Lingma SWE-GPT series, comprising Lingma\nSWE-GPT 7B and Lingma SWE-GPT 72B. By learning from and simulating real-world code submission\nactivities, Lingma SWE-GPT systematically incorporates the dynamic interactions and iterative problem-\nsolving inherent in software development process-such as repository understanding, fault localization, and\npatch generation\u2014thereby achieving a more comprehensive understanding of software improvement processes.\nWe conducted experimental evaluations using SWE-bench-Verified benchmark (comprising 500 real GitHub\nissues), recently proposed by OpenAI. The results demonstrate that Lingma SWE-GPT 72B successfully\nresolves 30.20% of the GitHub issues, marking a significant improvement in automatic issue resolution\n(22.76% relative improvement compared to Llama 3.1 405B), approaching the performance of closed-source\nmodels (31.80% issues of GPT-40 resolved). Notably, Lingma SWE-GPT 7B resolves 18.20% of the issues,\nsurpassing the 17.20% resolution rate of Llama 3.1 70B, highlighting the potential for applying smaller models\nto ASE tasks.", "sections": [{"title": "Introduction", "content": "Automated software engineering (ASE) has long been a vision pursued by both the software\nengineering (SE) and artificial intelligence (AI) communities. Recent advancements in large language\nmodels (LLMs) have shown significant potential in advancing this field. Initially, the HumanEval\nbenchmark [9] was developed to assess LLMs' capabilities in function-level code generation. Both\nclosed-source (e.g., GPT-40 [39]) and open-source models (e.g., Llama 3.1 405B [36]) have performed\nwell on these tasks, solving more than 90% of problems. However, function-level code generation\nrepresents only a fraction of the challenges encountered in real-world software development."}, {"title": "Related Work", "content": ""}, {"title": "Large Language Models for Code", "content": "Generative models such as ChatGPT have exhibited significant capabilities in code generation and\ncomprehension. These models have substantially impacted various aspects of software engineering,\nenabling tasks such as code generation [18, 33, 42, 57, 72] from natural language requirements,\ntest generation [24, 27, 55, 59], and code editing and refactoring [2, 7, 23, 48, 67, 68]. Furthermore,\ndevelopers and researchers have applied these models to more complex software engineering\ntasks, including code translation [13, 25, 41], debugging [10, 12, 38, 47, 60], and automated program\nrepair [19, 22, 69].\nThe effectiveness of these models can be attributed to their pretraining on extensive general-\npurpose data and open-source code repositories. This extensive training has facilitated the devel-\nopment of sophisticated code generation and reasoning capabilities. Notable examples of such\nmodels include GPT-4 [1], Claude 3.5 Sonnet [3], CodeX [9], Code Llama [45], StarCoder [30],\nDeepSeek-Coder [73], Qwen2.5 Coder [17], and CodeGemma [49]. These models have shown consid-\nerable proficiency in comprehending and generating code across various programming languages\nand paradigms. In addition to pretrained LLMs, researchers have developed instruction-tuned\nmodels specifically tailored for code-related tasks. Examples of such models include CodeLlama-\nInstruct [45], Pangu-coder [46], WizardCoder [32], Magicoder [53], WaveCoder [63] and Open-\ncodeinterpreter [70]. These models undergo additional fine-tuning with carefully curated instruc-\ntions, enhancing their performance on specific downstream code-related tasks. Despite their\nadvanced capabilities, current LLMs are primarily trained on static code data, limiting their un-\nderstanding of the dynamic interactions nature of software development. This paper proposes\ndeveloping models that can simulate these dynamic aspects, including reasoning about tool us-\nage and mimicking thought processes, to better address the challenges of real-world software\nengineering tasks."}, {"title": "LLM-based Software Engineering Agents", "content": "In recent years, LLM-based AI agents have advanced the development of automatic software\nengineering (ASE). AI agents improve the capabilities of project-level software engineering tasks\nthrough running environment awareness [14, 20, 52, 56], planning & reasoning [11, 31, 52], and tool\nconstruction [15, 21, 35, 58, 65]. Surprisingly, Devin [11] is a milestone that explores an end-to-end\nLLM-based agent system to handle complex SE tasks. Concretely, it first plans the requirements of\nusers, then adopts the editor, terminal and search engine tools to make independent decisions and\nreasoning, and finally generates codes to satisfy the needs of users in an end-to-end manner. Its\npromising designs and performance swiftly ignited unprecedented attention from the SE and AI\ncommunity to automatic software engineering (ASE) [8, 26, 28, 29, 34, 54, 62, 69, 71]. For example,\nSWE-agent [62] carefully designs an Agent Computer Interface (ACI) to empower the SE agents\ncapabilities of creating & editing code files, navigating repositories, and executing programs. Besides,"}, {"title": "Evaluation of Real-world Software Engineering Tasks", "content": "Benefiting from the strong general capability of LLMs, LLM-based software engineering agents\ncan handle many important SE tasks, e.g., code generation [52] and code debugging [14]. More\nrecently, SWE-bench team[19, 62] develop a unified dataset named SWE-bench to evaluate the\ncapability of the agent system to resolve real-world GitHub issues automatically. Specifically, it\ncollects the task instances from real-world GitHub issues from twelve repositories. Consistent with\nprevious evaluation methods, SWE-bench is based on the automatic execution of the unit tests.\nDifferently, the presented test set is challenging and requires the agents to have multiple capabilities\nsimultaneously, including repository navigation, fault locating, debugging, code generation and\nprogram repairing, so as to solve a given issue end-to-end. Besides, SWE-bench Lite [5] and SWE-\nbench Verified [40] are subsets of SWE-bench, and they have a similar diversity and distribution of\nrepositories as the original version. Due to the smaller test cost and more detailed human filtering,\nSWE-bench Lite and Verified are officially recommended as the benchmark of LLM-based SE agents.\nTherefore, consistent with previous methods [54, 62, 69], we report our performance on SWE-bench\nLite and SWE-bench Verified."}, {"title": "Lingma SWE-GPT", "content": "In this section, we present our novel approach for training LLM to perform automated program\nimprovement. Our method comprises three main phases: issue data collection (Figure 2), de-\nvelopment process data synthesis (Figure 3) and model training (Figure 1). Leveraging an\ninstruction-tuned model as the foundation model, our approach begins with the input of an issue\ndescription and the associated project codebase, then engages in a multi-stage workflow mimicking\nexpert programmers' cognitive processes. This workflow consists of three key stages: repository\nunderstanding, fault localization, and patch generation. In the repository understanding stage, the\nmodel analyzes the repository structure, navigates the codebase, and utilizes a code compressor\nto grasp relevant project files and code snippets. The fault localization stage builds upon this\nunderstanding to identify potential fault locations at the class, function, or code level. Finally, in\nthe patch generation stage, the model generates and applies patches using git-related operations.\nThroughout these stages, the model operates in a Chain-of-Thought (CoT) manner, where each step\noutputs a reasoning process (CoT\u012f) and an action (Action\u012f). This action is then executed, generating\nan observation (Obs\u012f). Based on this observation, the model proceeds to the next step (CoTi+1 +\nActioni+1), creating an iterative feedback loop for continuous refinement."}, {"title": "Issue Data Collection", "content": "The foundation of our approach lies in leveraging high-quality, real-world software development\ndata. GitHub Issues serve as valuable resources for understanding bugs, feature requests, and\nenhancements, often providing natural language descriptions of problems or desired functionalities.\nPull Requests (PRs) contain the corresponding code changes, including commit histories and code\ndiffs, which directly address the issues raised. By leveraging this combination of data, we can\nsimulate real-world programming scenarios, capturing the context and reasoning behind code\nmodifications. To effectively train SWE-GPT for automatic program improvement, we constructed a\ncomprehensive dataset by collecting issues, corresponding PRs, and codebases from public GitHub\nrepositories. Below we will describe our data collection process in detail.\nFirst, we selected Github repositories with at least 50 stars on GitHub, thereby ensuring a\nbasic level of community recognition and activity. To avoid potential data leakage, we carefully\nfiltered out any repositories that overlapped with those used in SWE-bench [19]. For each selected\nrepository, we retrieved all issues and their linked PRs, focusing specifically on those PRs that had\nbeen merged by the developers (as illustrated in Figure 2). Specifically, we utilized GitHub's API to\nfetch PRs with the state \"merged\" and their associated issues with the state \"closed\", ensuring that\nwe captured only completed and approved code changes. Additionally, we stored snapshots of the\ncodebase at the time of the PR to provide sufficient context for the code changes.\nTo further ensure the quality of the issues and PRs, we applied a set of heuristic filtering rules,\nsimilar to the approach used in OctoPack [37]. For issues, we retained only those with textual\ndescriptions containing at least 20 characters, thus excluding trivial or insufficiently detailed issues.\nAdditionally, to avoid issues that primarily reference external resources without providing adequate\ncontext, we filtered out those with more than three hyperlinks. Lastly, we retained only issues with\nat least 80% English content in their textual descriptions, to maintain language consistency across\nthe dataset. For PRs, we applied two main criteria. First, we selected PRs that involved modifications\nto between one and five code files. This ensured that each PR represented a substantive but not\noverly complex change. Second, we excluded PRs that only modified test files, focusing instead on\nchanges that directly impacted the primary functionality of the codebase.\nAfter applying these filtering criteria, we constructed a dataset consisting of approximately\n90,000 PRs collected from 4,000 repositories. Notably, the codebases we processed were typically of\nsubstantial size, often containing hundreds of files, reflecting the complexity inherent in real-world\nsoftware improvements."}, {"title": "Development Process Data Synthesis", "content": "Building upon the comprehensive issue data collection, we introduce a novel data synthesis process\ndesigned to capture the dynamic nature of software development. This process addresses the\nlimitations of current LLM-based approaches by simulating the complex, interactive aspects of\nreal-world software maintenance and evolution. Figure 1 illustrates our data synthesis approach,\nwith its algorithm detailed in Algorithm 1. This approach mimics the cognitive workflow of expert\nprogrammers across three key stages: Repository Understanding, Fault Localization, and Patch\nGeneration. Each stage involves a series of Chain-of-Thought reasoning steps, allowing the model\nto iteratively refine its understanding and output to the given issue.\nFigure 3 illustrates our three-stage SoftWare Engineering process data Synthesis and Inference\nworkflow(line 9-18, Algo. 1), named SWESynInfer. This workflow extends the publicly available\nAutoCodeRover [69] framework. AutoCodeRover provides baseline processes for context retrieval\nand patch generation stages, our work further introduces crucial enhancements to more accurately\nsimulate the cognitive processes of expert developers.\nRepository Understanding. This stage focuses on comprehending the structure and content of\na code repository, crucial for establishing the context necessary for issue resolution. The Repository\nUnderstanding Agent (RepoUer) employs a hierarchical approach inspired by Agentless [54],\nutilizing three key tools: Repo Structure tool, Navigate Repo tool, and Code Compressor tool (see\nFigure 3). First, RepoUer uses the Repo Structure tool to create a concise representation of the\nrepository's directory structure. This tool transforms the directory structure into a tree format\nsuitable for LLM input, starting from the repository's root folder and organizing code files and\nfolder names. Files and folders at the same directory level are vertically aligned, with subdirectory\ncontents indented. Next, the Navigate Repo tool is employed to traverse the repository and locate\npotentially relevant files and elements. RepoUer leverages the issue description and the generated\ndirectory tree to identify N potentially relevant files. The Code Compressor tool then transforms\nthese files into a skeleton format. This compressed representation preserves global references,\nclass definitions, function signatures, associated comments, and variable declarations, effectively\nreducing the overall context length while maintaining essential structural information. Using\nthese compressed file representations, RepoUer further refines its search to identify specific classes\nor functions that are potentially relevant to the issue. Finally, RepoUer analyzes the collected\ninformation and formulates a comprehensive plan for issue resolution. This plan typically includes:\na restatement of the issue, an analysis of the root cause, and proposed steps to fix the issue. This\nmulti-step process emulates a developer's initial exploration of a project, providing a comprehensive\nunderstanding of the codebase's architecture and relevant components.\nFault Localization. Building on insights from the repository understanding stage, the Fault\nLocalization phase aims to identify specific problem areas within the codebase. This phase employs\nthe Fault Localizer (FLer), which simulates the diagnostic steps developers take when resolving\nissues, combining tool utlization with iterative problem-solving. FLer initially uses specialized\nsearch APIs to retrieve relevant code snippets at the file, class, function, and snippet levels. These\nAPIs (see Tools in Figure 3), informed by the repository summary and issue description, extract\ncontextual information necessary for pinpointing potential fault locations. FLer then systematically\nevaluates the retrieved code snippets, performing context analysis to understand the relationships\nand dependencies within the codebase. Finally, it assesses whether the fault location is identified\nand decides whether to continue searching or proceed to the patch generation phase. FLer continues\nthis cycle until it either successfully identifies the fault locations or reaches a iteration limit. FLer\nensures that after each observation of results, it performs a summary and reflection. By documenting\nthese intermediate reasoning steps, the model learns to emulate the cognitive processes involved\nin fault localization, enhancing its ability to tackle complex software engineering tasks.\nPatch Generation. In the final stage, the Patch Generation Agent (Coder) generates and applies\npatches to address the localized issues. This process involves patch generation and the use of\ngit-related operations and lint tool to implement and validate changes. Specifically, Coder first\ngenerates a concrete solution based on the issue description and identified fault code snippets. It\nthen replaces the fault code snippets with the new solution. If the generated patch fails to conform\nto the specified format or cannot be syntactically applied to the original program (i.e., if lint tools\ndetect syntax errors in the generated code, or if the git apply command fails), Coder debugs based\non the error type until a correctly formatted patch is generated or the maximum retry limit is\nreached. This stage embodies the iterative nature of real-world software development scenarios,\nallowing for multiple refinement cycles to produce high-quality, applicable patches.\nBy enhancing each stage with detailed intermediate reasoning steps and incorporating tools\nthat mirror real-world development practices, SWESynInfer provides a more comprehensive and\nrealistic simulation of the software maintenance and evolution process. This approach enables the\ngeneration of high-quality training data that captures the complex, interactive aspects of software\ndevelopment.\nRejection Sampling. To ensure the quality of the synthesized data, we implement a rejection\nsampling process based on two key metrics. This approach allows us to selectively retain high-\nquality instances that closely mimic real-world software development practices.\nFault localization accuracy. We compare the predicted fault location with the actual fault location\nusing a similarity threshold m\u2081. Specifically, we first extract the modified locations from the\nsubmitted patch in the pull request as the actual fault location (line 7, Algo. 1). We map the\npatch to the current version of the repository and then use the abstract syntax tree to extract the\ncorresponding functions and classes for the modified locations. For global code modifications, we\nselect the surrounding code (3 lines above and below the modified line) as the modification location.\nWe then use the same method to extract the modification location from the model-generated\npatch (line 20). To quantify the accuracy of fault localization, we calculate the Jaccard similarity\ncoefficient [51] between these two sets of modification locations. Specifically, we divide the size\nof the intersection of the two sets by the size of their union. This ratio is then compared with the\nthreshold m\u2081 (line 21, Algo. 1). If the calculated ratio exceeds m\u2081, we consider the fault localization\nto be sufficiently accurate.\nPatch similarity. We evaluate the similarity between the predicted patch and the developer\nsubmitted patch using a threshold m2. Following the approach in Agentless [54], we first normalize"}, {"title": "Model training", "content": "After collecting a set of training examples B\u2081, we implement an iterative optimization strategy\nto train our model. In each iteration, the model optimizes its performance by maximizing the\nconditional probability of generating the target CoT and corresponding action given the current\nstate observation (line 29, Algo. 1). To further enhance the robustness of the training process, we\nincorporate a curriculum learning approach. As iterations progress, we accumulate problems that\nthe current version of the model fails to solve and incrementally incorporate them into subsequent\ntraining. The complexity of the accumulated training examples increases progressively with the\nnumber of model iterations. This approach enables the model to establish a robust foundation on\nsimpler tasks before addressing more complex challenges. Drawing inspiration from STaR [64] and\nNEXT [38], we adopt a similar strategy to mitigate the potential negative impact of low-quality\nsamples that may exist in early iterations. Specifically, we initialize the model from its original\ncheckpoint M at the beginning of each iteration (line 30). This approach mitigates the risk of\noverfitting to potentially low-quality training examples in the early stages, thereby ensuring the\nstability of the training process and enhancing the generalization capability of the final model."}, {"title": "Experiment Setup", "content": "To evaluate the capabilities of Lingma SWE-GPT in resolving real-world Github issues, we answer\nthe following research questions.\nRQ1: How does Lingma SWE-GPT compare to state-of-the-art models in solving real-world\nsoftware issues?\nRQ2: What is the performance of Lingma SWE-GPT compared to open-source models in auto-\nmated program improvement task?\nRQ3: How effective is Lingma SWE-GPT in fault localization within the essential steps required\nfor issue resolution?\nRQ4: To what extent does the inherent randomness of large language models impact the consis-\ntency of Lingma SWE-GPT's performance?"}, {"title": "Benchmark and Evaluation Metric", "content": "SWE-bench Verified and SWE-bench Lite. We evaluated Lingma SWE-GPT on the recently proposed\nbenchmarks SWE-bench Verified [40] and SWE-bench Lite [5], comprising 500 and 300 real-world\nGitHub issues, respectively. The model receives only the natural language description of the\noriginal GitHub issue and its corresponding code repository as input. These benchmarks employ\ndeveloper-written unit tests to verify the correctness of model-generated patches, ensuring a\nrigorous assessment of the model's performance. For detailed information on SWE-bench Verified\nand SWE-bench Lite, refer to Section 2.3.\nEvaluation Metric. We use (1) the percentage of resolved task instances, (2) average inference cost\nof requesting closed-source model API. These evaluation metrics represent overall effectiveness,\nand economic efficacy in resolving real-world GitHub issues. Due to the public accessibility of\nopen-source models, we assigns their API calls costs to NULL (-). This approach disregards potential\ndeployment costs, which is left to future research to more comprehensively evaluate various factors,\nincluding indirect expenses. In addition, to mitigate the natural randomness of LLM, we repeat\nour experiments three times. Following AutoCodeRover [69] and SWE-agent [62], we report the\nresults with the SWE-GPT @3 annotations (i.e., pass@3 metric)."}, {"title": "Baselines", "content": "To thoroughly evaluate the performance of Lingma SWE-GPT in resolving real-world GitHub\nissues, we compared our model against both state-of-the-art closed-source and open-source models.\nClosed-Source Models. We included leading closed-source models whose results are reported\non the SWE-bench leaderboard [19] and in recent related research [73]. These models primarily\nconsist of OpenAI's GPT and Anthropic's Claude series. For these closed-source models, we utilized\nthe results reported by SWE-bench [5, 40].\n\u2022 GPT-4 [1] and GPT-40 [39]: GPT-4 represents one of the most advanced language models\navailable, exhibiting strong capabilities in understanding and generating human-like text\nand code. GPT-40 is OpenAI's flagship model that can reason across audio, vision, and text\nin real time.\n\u2022 Claude 3.5 Sonnet [3] and Claude 3 Ops [4], developed by Anthropic. The Claude models\nare designed with a focus on alignment and safe AI practices, and have shown proficiency in\nvarious tasks, particularly in code understanding and generation.\n\u2022 Gemini-1.5 Pro [50], developed by Google. Gemini-1.5 Pro is a state-of-the-art multimodal\nmodel that excels in long-context understanding. It can process up to one million tokens in a\nsingle prompt, allowing for unprecedented context analysis.\nOpen-Source Models. For open-source baselines, we selected the most advanced models from\nthe Llama and Qwen series. We obtained the results by both using existing research reports [28]\nand deploying and running them ourselves.\n\u2022 Llama Series [36]: We evaluated Llama 3.1 instruction-tuned models with 70B and 405B\nparameters, representing the most advanced and largest open-source models in Llama 3.1\nseries.\n\u2022 Qwen Series [61]: We included Qwen2-72B-Instruct, Qwen2.5-72B-Instruct, and Qwen2.5-\nCoder 7B (maximum coder version [17]) models. This series demonstrated superior perfor-\nmance on the HuggingFace open-source comprehensive leaderboard [16]."}, {"title": "Implementation Details", "content": "We implemented Lingma SWE-GPT using Qwen2.5 72B instruct [61] and Qwen2.5 Coder 7B [17]\nas the foundation LLMs for the 72B and 7B versions, respectively. All training was conducted on a\ncluster of 64 NVIDIA A100 GPUs running Ubuntu 20.04, with a global batch size of 512 throughout\nthe training process. For inference, we employed temperature sampling with a temperature (T)\nof 0.3. The training process consisted of 90 iterations in total. To mitigate the risk of the model\nconverging on low-quality synthetic data during the initial stages, we utilized GPT-40 [39] to\ngenerate the initial training data for the first 10 iterations before transitioning to model-generated\ndata. To ensure training efficiency, we updated the model with synthesized data every 10 iterations.\nIn Algorithm 1, the parameters m\u2081 and m2 were set to 0.6 and 0.5, respectively. For evaluation\npurposes, we configured the inference process with a temperature of 0.3 and a maximum token\nlimit of 1024 to generate results for each round. During the Repository Understanding stage, the\nnumber of potentially relevant files (N) identified was set to 5. In the Fault Localization stage, the\npredefined iteration limit was set to 5. For the Patch Generation stage, the maximum retry limit\nwas set to 3. To ensure consistency and reproducibility, all tests were conducted using the official\nSWE-bench Docker environment provided by the SWE-bench team [19]."}, {"title": "RQ1: Overall Effectiveness in Resolving Real-life Software Issues", "content": "To comprehensively evaluate the performance of Lingma SWE-GPT in resolving real-world software\nissues, we conducted extensive experiments using two challenging benchmarks: SWE-bench Verified\nand SWE-bench Lite. These benchmarks provide a realistic runtime testing environment that closely\nsimulates real-world software development scenarios. The input for each task consists solely of\nthe natural language description of GitHub issues and the corresponding repository code at the\ntime the issue was reported. The expected output is a corrective patch that resolves the issue. We\nmeasure the overall effectiveness of Lingma SWE-GPT and baseline models by quantifying the\nnumber of successfully resolved issue instances. Table 1 presents the comprehensive results of\nLingma SWE-GPT (7B and 72B) alongside various state-of-the-art models on both SWE-bench\nVerified and SWE-bench Lite. The results reveal several significant findings.\nExisting Open-Source vs. Closed-Source Models. From table 1, we can see that the majority of\ntop-performing submissions are based on closed-source models. For instance, SWE-agent utilizing\nClaude 3.5 Sonnet achieves success rates of 33.60% and 23.00% on Verified and Lite benchmarks,\nrespectively. Similarly, Agentless combined with GPT-40 resolves 33.20% of issues on Verified and\n24.30% on Lite. These results underscore the current dominance of closed-source models in complex\nsoftware engineering tasks. In contrast, open-source models have traditionally underperformed\ncompared to their closed-source counterparts. For example, AutoCodeRover using Qwen2 72B\ninstruct achieves a 9.34% success rate on SWE-bench Lite, while the same framework with GPT-40\nresolves 22.70% of issues, highlighting a significant performance gap."}, {"title": "RQ2: Performance Against State-of-the-Art Open-Source Models", "content": "To evaluate the performance of Lingma SWE-GPT in comparison with state-of-the-art open-source\nmodels for automated program improvement, we conducted a comprehensive analysis using the\nsame inference process (SWESynInfer) across various models. Table 2 presents the overall results\nof this comparison.\nExperimental Challenges and Setup. During our evaluation, we encountered significant\nchallenges with certain open-source models due to their limited instruction-following capabilities.\nTo optimize the assessment of these models' effectiveness in software engineering tasks (rather than\ntheir general instruction-following ability), we implemented model-specific prompt engineering and\ntool customization. For instance, Qwen2.5-instruct 72B consistently generated fixed JSON formats\n`json\\n{actual_content}```, while Llama-3.1-instruct 70B output ``\\n{actual_content}```,\nboth of which led to JSON extraction failures(should be {actual_content}). These issues likely\noriginate from excessively constrained format requirements during the models' alignment processes,\nresulting in diminished adaptability to complex tasks. In contrast, larger models like Llama-3.1-\ninstruct 405B and closed-source models demonstrated superior instruction-following abilities,\ncorrectly generating JSON formats as required. To ensure a fair comparison, we adjusted prompts\nand customized tool invocations for affected models, and indicated by (*) in Table 2.\nResults on SWE-bench. The results demonstrate that Lingma SWE-GPT 72B significantly out-\nperforms the latest open-source models, including Qwen2.5-instruct 70B and the largest open-source\nmodel, Llama-3.1-instruct 405B. On SWE-bench Verified, Lingma SWE-GPT 72B achieves a 30.20%\nsuccess rate, marking an 18.90% relative improvement over Qwen2.5-instruct 72B (25.40%) and a\n22.76% relative improvement over Llama-3.1-instruct 405B (24.60%). This performance gap high-\nlights the effectiveness of Lingma SWE-GPT's training approach, which focuses on understanding\nand generating dynamically evolving processes in software development tasks.\nNotably, even the smaller Lingma SWE-GPT 7B model outperforms Llama-3.1-instruct 70B\n(18.20% vs. 17.20% on SWE-bench Verified, 12.00% vs. 7.00% on SWE-bench Lite), further validating\nthe efficacy of our process-oriented data training methodology. This result demonstrates that\nsmaller, more efficient models can also attain competitive performance when trained on high-\nquality, process-oriented data.\nAmong other open-source models, Llama-3.1-instruct 405B exhibits the best performance with-\nout prompt-specific adjustments, highlighting its general capabilities and specialized abilities in\nsoftware engineering tasks. This observation aligns with the scaling law hypothesis, which posits\nthat larger models tend to perform better across a wide range of tasks. This insight provides valuable\nguidance for the future development of more advanced software engineering models.\nPerformance Across Different Repositories. Figure 4 illustrates the performance of Lingma\nSWE-GPT 72B across 12 diverse software repositories, compared to the best-performing open-source\nmodel (Llama 3.1 405B instruct) and leading closed-source models (GPT-40 and Claude 3.5 Sonnet).\nLingma SWE-GPT 72B outperforms Llama 3.1 405B instruct in the majority of repositories (9/12)\nand approaches the performance of closed-source models in numerous instances. This consistent"}, {"title": "RQ3: Fault Localization Effectiveness", "content": "Fault localization is a critical step in resolving software issues in real-world development scenarios.\nAccurate identification of edit locations is not only crucial for automated issue resolution and\ninvaluable for assisting human developers in debugging tasks. To evaluate the fault localization\ncapabilities of Lingma SWE-GPT, we conducted a comprehensive analysis comparing the locations\nof patches generated by various models to the actual patch locations.\nEvaluation Methodology. We employed a rigorous process to ensure a thorough evaluation. We\nmapped the patches to the current version of the repository and used abstract syntax trees to extract\nthe functions and classes corresponding to the modified locations. For chunk-level analysis, which\nnot only encompasses function/class modifications but also global code changes, we considered\nthe surrounding code (3 lines above and below the modified line) as the global code modification\nlocation. Following the approach outlined in Agentless [54], we acknowledge that while errors can"}, {"title": "RQ4: Model Consistency and Randomness Impact", "content": "Large language models (LLMs) inherently exhibit stochastic behavior in their outputs, which\ncan potentially affect their consistency and reliability in practical applications. To address this\nconcern, we conducted a comprehensive investigation into the consistency of Lingma SWE-GPT's\nperformance across multiple executions on the SWE-bench Verified. This analysis aims to quantify\nthe impact of the model's inherent variability on its ability to consistently solve software engineering\ntasks.\nEvaluation Methodology. We executed Lingma SWE-GPT 72B three times (denoted as run\u2081,\nrun2, run3) on the SWE-bench Verified dataset. This approach allows us to isolate and observe the\neffects of the model's inherent randomness on its performance. We then analyzed the results in\nterms of issue resolution rates and fault localization accuracy across different granularity levels\n(chunk, function, and file).\nResults and Analysis. Table 3 presents the detailed results of our consistency analysis. Across\nthe three runs, Lingma SWE-GPT 72B demonstrated remarkable consistency in its issue resolution\ncapability, achieving success rates of 30.20%, 29.00%, and 30.20% for run\u2081, run2, and run3, respectively.\nThe fault localization performance also exhibited stability across runs, with narrow ranges at chunk,\nfunction, and file levels, further underscoring the model's consistency in pinpointing issue locations.\nFollow AutoCodeRover [69", "19": "we also implemented a pass@3 metric, which\nconsiders an issue as resolved if any of the three runs successfully addresses it. This approach\nsignificantly boosted the overall performance, increasing the issue resolution rate to 39.80%. Notably,\nthis surpasses the performance of Claude 3.5 Sonnet (35.40%), a leading closed-source model. The\npass@3 approach also substantially enhanced fault localization accuracy, achieving 61.63% at"}]}