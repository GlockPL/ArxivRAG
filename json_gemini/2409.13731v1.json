{"title": "KAG: Boosting LLMs in Professional Domains via\nKnowledge Augmented Generation", "authors": ["Lei Liang", "Mengshu Sun", "Zhengke Gui", "Zhongshu Zhu", "Ling Zhong", "Peilong Zhao", "Zhouyu Jiang", "Yuan Qu", "Zhongpu Bo", "Jin Yang", "Huaidong Xiong", "Lin Yuan", "Jun Xu", "Zaoyang Wang", "Wen Zhang", "Huajun Chen", "Zhiqiang Zhang", "Jun Zhou"], "abstract": "The recently developed retrieval-augmented generation (RAG) technology en-\nables the efficient construction of domain-specific applications. However, it faces\nlimitations due to fuzzy retrieval processes, the \"hallucination\" problem of un-\nderstanding and reasoning capabilities of general language models, and cascading\nlosses in complex systems. These challenges hinder the effectiveness of special-\nized knowledge services. However, in scenarios such as scientific computing,\nmedicine, and law, the accuracy of knowledge, the completeness of information,\nand the logical rigor of rules, time, and values are particularly critical. We Intro-\nduce professional domain knowledge service framework: Knowledge Augmented\nGeneration(KAG) to improve generation and reasoning performance by bidirec-\ntionally enhancing large language model(LLM)s and knowledge graph(KG)s, in-\ncluding five key enhancements: 1) LLM-friendly knowledge semantic representa-\ntion, 2) mutual indexing between knowledge graph and original chunks, 3) logical-\nform-guided hybrid reasoning and solving, 4) Knowledge alignment based on\nsemantic reasoning, 5) Model for KAG. We compared KAG with existing RAG\nmethods in multi-hop question answering. The results show that KAG performs\nsignificantly better than the state-of-the-art methods, with a relative improvement\nfrom 19.6% to 33.4% in F1. We apply KAG to two professional knowledge Q&A\ntasks of Ant Group, including E-Goverment Q&A and E-Health Q&A, and has\nachieved significant improvement in professionalism compared with NaiveRAG.\nWe will soon natively support KAG on the open source KG engine OpenSPG,\nallowing developers to more easily build rigorous knowledge decision-making or\nconvenient information retrieval services.", "sections": [{"title": "Introduction", "content": "Recently, the rapidly developing Retrieval-Augmented Generation (RAG)[1, 2, 3, 4, 5] technology,\nwhich utilizes external retrieval systems, has effectively improved the timeliness of large language\nmodels in acquiring task information and reduced the hallucination of answers. This allows for the\nefficient construction of applications in specific domains. However, general RAG is still constrained\nby the ambiguity of retrieval, the \"hallucination\" problem of the understanding and reasoning capa-\nbilities of general language models, and the cascading losses of complex systems. These issues hin-\nder the professionalism of knowledge services, especially in fields such as law, healthcare, and sci-\nentific computing. To address these problems, Knowledge Graph, as a precise knowledge reasoning\ntechnology, are increasingly being integrated into the RAG framework. HippoRAG[6], DALK[7],\nand ToG 2.0[8] innovatively use triple-based inverted index to replace term-based inverted index,\nimproving the indexing structure of documents, and making full use of graph computing related\ntechnologies to support document retrieval or summary aggregation.\nAlthough these works organize documents using graph structures, allowing the system to perform\nbetter in multi-hop reasoning tasks, they still do not fully apply the specialized knowledge manage-\nment capabilities of KGs to the system. This is reflected in the following aspects:\n\u2022 Compared with the knowledge graph constructed by domain experts, the automatically\nconstructed structured knowledge only contains fragmented information from the docu-\nment and lacks domain knowledge that is not mentioned in the documents. This missing\ndomain knowledge serves as an intrinsic connection between document information and\nuser queries, affecting the accuracy of problem-solving. For example, \"industrial insur-\nance\", \"unemployment insurance\", and \"housing provident fund\" all fall under the category\nof \"five insurances and one fund\", but they might not be explicitly mentioned in the e-\ngovernment service documentation. Consequently, when querying \"five insurances and\none fund\", it may fail to retrieve the correct answer.\n\u2022 The graph structure constructed through OpenIE does not explicitly obtain classification,\ncontext and other information, which introduces a lot of noise, including polysemy, syn-\nonyms, differences in word granularity, sparse relationships, etc., which brings great chal-\nlenges to both reasoning and retrieval, and also reduces the accuracy of retrieval.\n\u2022 The professional domain is highly sensitive to rules, numerical values, time, coordinates,\nnumerical logic, or causal logic. For example, when analyzing data from sources like cor-\nporate financial reports, medical test results, or case descriptions to generate analytical\nmessages or answer queries, it is necessary to capture one or a set of key metrics. This\ninvolves determining whether these metrics are showing an increasing or decreasing trend\nin chronological or distance order, and whether the magnitude of these changes exceeds\nnormal fluctuation ranges. It is also important to identify potential causes of abnormal in-\ndicators and their possible impacts. These logical connections are intricately linked, and a\nsmall error can lead to significant misinterpretations. Therefore, there is a need for a mech-\nanism or systematic language for the representation, retrieval, and controllable generation\nof professional logic.\n\u2022 The processes of index construction, inference retrieval, and answer generation pose chal-\nlenges to the capabilities of the LLM models upon which the system relies.\nTo address the above challenges and meet the requirements of professional domain knowledge ser-\nvices, we hope to fully combine the complementary features of RAG and KG technologies, and then\nsimultaneously utilize the understanding and generation capabilities of LLMs to improve the profes-\nsional level of vertical domain knowledge services. In this paper, we propose a professional domain\nknowledge service framework: Knowledge Augmented Generation(KAG) to improve generation\nand reasoning performance by bidirectionally enhancing large language LLM and KG. As shown in\nFigure 1. We summarize our main contributions as follows:\n\u2022 We proposed a LLM friendly knowledge representation framework LLMFriSPG. We\nrefer to the hierarchical structure of data, information, and knowledge of DIKW to upgrade\nSPG to be friendly to LLMs, named LLMFriSPG, to make it compatible with schema-\nfree information extraction and schema-constrained expert knowledge construction on the\nsame knowledge type (such as entity type, event type), and supports the mutual indexing"}, {"title": "Approach", "content": "In this section, we will first introduce the overall framework of KAG, and then discuss five key\ntechnology in sections 2.1 to 2.5.\nAs shown in Figure 1, the KAG framework consists of three parts: KAG-Builder, KAG-Solver,\nand KAG-Model. The KAG-Builder is designed for building offline indexes; in this module, we\nproposed LLM Friendly Knowledge Representation framework and mutual indexing between knowl-\nedge structure and text chunk. In the module KAG-Solver we introduced a Logical-form-guided\nhybrid reasoning solver that integrates LLM reasoning, knowledge reasoning, and mathematical\nlogic reasoning. Additionally, knowledge alignment by semantic reasoning is used to enhance the\naccuracy of knowledge representation and retrieval in both KAG-Builder and KAG-Solver. The\nKAG-Model optimizes the capabilities needed by each module based on a general language model,\nthereby improving the performance of all modules."}, {"title": "LLM Friendly Knowledge Representation", "content": "In order to define a more friendly knowledge semantic representation for LLMs, we upgrade SPG\nfrom three aspects: deep text-context awareness, dynamic properties and knowledge stratification,\nand name it LLMFriSPG.\nM = {T,p,C,L}\nwhere M represents all types defined in LLMFriSPG, I represents all EntityType(e.g., Person in\nFigure 2), EventType classes and all pre-defined properties that are compatible with LPG syntax\ndeclarations. & represents all ConceptType classes, concepts and concept relations, it is worth\nnoting that the root node of each concept tree is a ConceptType class that is compatible with LPG\nsyntax(e.g., TaxoOfPerson in Figure 2), each concept node has a unique ConceptType class. p\nrepresents the inductive relations from instances to conecepts. I represents all executable rules\ndefined on logical relations and logical concepts. For t \u2208T:"}, {"title": "Mutual Indexing", "content": "As illustrated in Figure 4, KAG-Builder consists of three coherent processes: structured information\nacquisition, knowledge semantic alignment and graph storage writer. The main goals of this module\ninclude: 1) building a mutual index between the graph structure and the text chunk to add more\ndescriptive context to the graph structure, 2) using the concept semantic graph to align different\nknowledge granularities to reduce noise and increase graph connectivity."}, {"title": "Semantic Chunking", "content": "According to the document's structural hierarchy and the inherent logical connections between para-\ngraphs, a semantic chunking process is implemented based on system-built-in prompts. This seman-\ntic chunking produces chunks that adhere to both length constraints (specifically for LLM's context\nwindow size constraints) and semantic coherence, ensuring that the content within each chunk is\nthematically cohesive. We defined Chunk EntityType in RC, which includes fields such as id,\nsummary, and mainText. Each chunk obtained after semantic segmentation will be written into an\ninstance of Chunk, where id is a composite field consisting of articleID, paraCode, idInPara con-\ncatenated by the connector # in order to ensure that consecutive chunks are adjacent in the id space.\narticleID represents the globally unique article ID, paraCode represents the paragraph code in the\narticle, and idInPara is the sequential code of each chunk in the paragraph. Consequently, an ad-\njacency in the content corresponds to a sequential adjacency in their identifiers. Furthermore, a\nreciprocal relation is established and maintained between the original document and its segmented\nchunks, facilitating navigation and contextual understanding across different granularities of the\ndocument's content. This structured approach to segmentation not only optimizes compatibility\nwith large-scale language models but also preserves and enhances the document's inherent semantic\nstructure and association."}, {"title": "Information Extraction with More Descriptive Context", "content": "Given a dataset, we use fine-tuning-free LLM(such as GPT-3.5, DeepSeek, QWen, etc,.) or our\nfine-tuned model Hum to extract entities, events, concepts and relations to construct KGfr, subse-\nquently, construct the mutual index structure between KGfr and RC, enabling cross-document links\nthrough entities and relations. This process includes three phases. First, it extracts the entity set\nE = {1,2,3,...} chunk by chunk, second, extracts the event set EV {ev1, ev2, ev3,...} asso-\nciated to all entities and iteratively extracts the relation set R = {r1,r2,3,...} between all entities\nin E, finally, completes all hypernym relations between the instance and its spgClass. To provide\nmore convenience for the subsequent Knowledge Alignment phase, and overcome the problem of\nlow discrimination of knowledge phrases such as Wikidata and ConceptNet, in the entity extraction\nphase, we use LLMs to generate built-in properties description, summary, semanticType, spgClass,\ndescripitonOfSemanticType by default for each instance e at one time, as shown in Figure 2, we store\nthem in the e instance storage according to the structure of e.description, e.summary, <e, belongTo,\nsemanticType> and <e, hasClass, spgClass>."}, {"title": "Domain Knowledge Injection And Constraints", "content": "When openIE is applied to professional domains, irrelevant noise will be introduced. Previous re-\nsearch has shown that noisy and irrelevant corpora can significantly undermine the performance of\nLLMs[3, 5, 13]. It is a challenge to align the granularity of extracted information and domain knowl-\nedge. The domain knowledge alignment capabilities in KAG include: 1) Domain term and concept\ninjection. We use an iterative extraction approach, First, we store domain concepts and terms with\ndescription in KG storage. Second, we extract all instances in the document through openIE, then\nwe perform vector retrieval to obtain all possible concept and term sets Ed. Finally, we add Ed to the\nextraction prompt and perform another extraction to obtain a set Ed that is mostly aligned with the\ndomain knowledge. 2) Schema-constraint Extraction. In the vertical professional domains, the\ndata structure between multiple documents in each data source such as drug instructions, physical\nexamination reports, government affairs, online order data, structured data tables, etc. has strong\nconsistency, and is more suitable for information extraction with schema-constraint, structured Ex-\ntraction also makes it easier to do knowledge management and quality improvement. For detailed\ninformation about knowledge construction based on Schema-constraint, please refer to the SPG\u00b9\nand OneKE[14]. This article will not introduce it in detail. It is worth noting that, as shown in figure\n2, for the same entity type, such as Person, we can pre-define properties and relations such as name,\ngender, placeOfBirth, (Person, hasFather, Person), (Person, hasFriend, Person), and can also extract\ntripples directly such as (Jay Chou, spgClass, Person), (Jay Chou, constellation, Capricorn), (Jay\nChou, record company, Universal Music Group) through openIE. 3) Pre-defined Knowledge Struc-\ntures By Document Type. Professional documents such as drug instructions, government affairs\ndocuments, and legal definitions generally have a relatively standardized document structure. Each"}, {"title": "Mutual indexing between text chunk vectors and knowledge structures", "content": "KAG's mutual indexing is a knowledge management and storage mechanism that conforms to the\nLLMFriSPG semantic representation. As is described in section 2.1, it includes four core data\nstructures: 1) Shared Schemas are coarse-grained-types pre-defined as SPG Classes at project level,\nit includes EntityTypes, ConceptTypes, and EventTypes, they serve as a high-level categorization\nsuch as Person, Organization, GEOLocation, Date, Creature, Work, Event. 2) Instance Graph\ninclude all event and entity instances in KGcs and KGfr. that is, the instances constructed through\nschema-free openIE or through schema-constraint strict structure construction are both stored in\nthe instance graph. 3) Text Chunks are special entity node that conforms to the definition of the\nChunk EntityType. 4) Concept Graph is the core component for knowledge semantic alignment.\nit consists of a series of concepts and concept relations. Concept nodes are also fine-grained-types\nof instances. Through relation prediction, instance nodes can be linked to concept nodes to obtain\ntheir fine-grained semantic types., and two storage structures: 1) Graph Store. Store graph data\nstructures in LPG databases, such as TuGraph, Neo4J. 2) Text Vector. Store text and vectors in a\nvector storage engine, such as ES, or the vector storage embedded in the LPG engine."}, {"title": "Logical Form Solver", "content": "In the process of solving complex problems, two key steps are involved: reasoning and retrieval.\nDisassembling question is a planning process to determine the next problem to be tackled. Reason-\ning includes retrieving information based on the disassembled question, inferring the answer to the\nquestion according to the retrieved results, or re-disassembling the sub-question when the retrieved\ncontent cannot answer the question. Retrieval is to find the content that can be used as reference for\nthe original question or the disassembled subquestion.\nSince the interaction between different modules is based on the vector representation of nat-\nural language, inaccuracies often occur. Inspired by logical form, which is often used in KGQA,\nwe design an executable language with both reasoning and retrieval capabilities, which breaks the\nquestion down into multiple logical expressions, each of which can contain functions for retrieval\nor logical operations. The mutual-structure index described in section 2.2 makes it possible to"}, {"title": "Logical Form Function", "content": "Logical Functions are defined as Table 1, with each function representing an execution action. Com-\nplex problems are decomposed by planning a combination of these expressions, enabling reasoning\nabout intricate issues.\nRetrieval. According to the the knowledge or information retrieved from SPO, s, p, o should not\nrepeatedly appear multiple times in the same expression. Constraints can be applied to the s, p, o for\nquerying. For multi-hop queries, multiple retrievals are required. When the current variable refers to\na previously mentioned variable, the variable name must be consistent with the referenced variable\nname, and only the variable name needs to be provided. The entity type and name are only specified\nduring the first reference.\nSort. Sort the retrieval results. A is the variable name for the retrieved SPO (si, oi, or s.prop, p.prop,\no.prop). direction specifies the sorting direction, where direction = min means sorting in ascending\norder and \"direction=max\" means sorting in descending order. limit = n indicates outputting the\ntopN results."}, {"title": "Logical Form for Reasoning", "content": "When the query statement represented by natural language is applied to the search, the logic is of-\nten fuzzy, such as \"find a picture containing vegetables or fruits\" and \"find a picture containing\nvegetables and fruits\". Whether text search or vector search is used, the similarity between the\ntwo queries is very high, but the corresponding answers are quite different. The same is true for\nproblems involving logical reasoning processes such as and or not, and intersection differences.\nTo this end, we use logical form to express the question, so that it can express explicit semantic\nrelations. Similar to IRCOT, we decompose complex original problems and plan out various ex-\necution actions such as multi-step retrieval, numerical reasoning, logical reasoning, and semantic\ndeduce. Each sub-problem is expressed using logical form functions, and dependencies between\nsub-questions are established through variable references. The inference resolution process for each\nsub-question is illustrated as Algorithm 9. In this process, the \u2018GraphRetrieval' module performs\nstructured retrieval based on the logical function, with the retrieval results also being structured as\nSPO (Subject-Predicate-Object) information. The 'HybridRetrieval' module, on the other hand, re-\ntrieves chunks based on the sub-question and the structured information. To understand how logical\nfunctions can be utilized to reason about complex problems, refer to the following examples as Table\n2."}, {"title": "Logical Form for Retrieval", "content": "In RAG, retrieval is achieved by calculating the similarity (e.g. cosine similarity) between the em-\nbeddings of the question and document chunks, where the semantic representation capability of\nembedding models plays a key role. This mainly includes a sparse encoder (BM25) and a dense re-\ntriever (BERT architecture pre-training language models). Sparse and dense embedding approaches\ncapture different relevance features and can benefit from each other by leveraging complementary\nrelevance information.\nThe existing method of combining the two is generally to combine the scores of the two search\nmethods in an ensemble, but in practice different search methods may be suitable for different ques-\ntions, especially in questions requiring multi-hop reasoning. When query involves proper nouns,\npeople, places, times, numbers, and coordinates, the representation ability of the pre-trained presen-\ntation model is limited, and more accurate text indexes are needed. For queries that are closer to the\nexpression of a paragraph of text, such as scenes, behaviors, and abstract concepts, the two may be\ncoupled in some questions.\nIn the design of logical form, it is feasible to effectively combine two retrieval methods. When\nkeyword information is needed as explicit filtering criteria, conditions for selection can be specified\nwithin the Retrieval Function to achieve structured retrieval."}, {"title": "Knowledge Alignment", "content": "Constructing KG index through information-extraction and retrieving based on vector-similarity has\nthree significant defects in knowledge alignment:\n\u2022 Misaligned semantic relations between knowledge. Specific semantic relations, such\nas contains, causes and isA, are often required between the correct answer and the query,\nwhile the similarity relied upon in the retrieval process is a weak semantic measure that\nlacks properties and direction, which may lead to imprecise retrieval of content.\n\u2022 Misaligned knowledge granularity. The problems of knowledge granularity difference,\nnoise, and irrelevance brought by openIE pose great challenges to knowledge management.\nDue to the diversity of language expressions, there are numerous synonymous or similar\nnodes, resulting in low connectivity between knowledge elements, making the retrieval\nrecall incomplete.\n\u2022 Misaligned with the domain knowledge structure. There is a lack of organized, system-\natic knowledge within specific domains. Knowledge that should be interrelated appears in\na fragmented state, leading to a lack of professionalism in the retrieved content.\nTo solve these problems, we propose a solution that leverages concept graphs to enhance offline\nindexing and online retrieval through semantic reasoning. This involves tasks such as knowledge\ninstance standardization, instance-to-concept linking, semantic relation completion, and domain\nknowledge injection. As described in section 2.2.2, we added descriptive text information to each\ninstance, concept or relation in the extraction phase to enhance its interpretability and contextual rel-\nevance. Meanwhile, as described in section 2.2.3, KAG supports the injection of domain concepts\nand terminology knowledge to reduce the noise problem caused by the mismatch of knowledge gran-\nularity in vertical domains. The goal of concept reasoning is to make full use of vector retrieval and\nconcept reasoning to complete concept relations based on the aforementioned knowledge structure\nto enhance the accuracy and connectivity of the domain KG. Refer to the definition of SPG concept\nsemantics2, as is shown in Table 3, we have summarized six semantic relations commonly required\nfor retrieval and reasoning. Additional semantic relations can be added based on the specific re-\nquirements of the actual scenario."}, {"title": "Enhance Indexing", "content": "The process of enhancing indexing through semantic reasoning, as shown in Figure 5, specifically\nimplemented as predicting semantic relations or related knowledge elements among index items\nusing LLM, encompassing four strategies:\n\u2022 Disambiguation and fusion of knowledge instances. Taking entity instance ecur as an ex-\nample, first, the one-hop relations and description information of ecur are used to predict\nsynonymous relations to obtain the synonym instance set Esyn of ecur. Then, the fused tar-\nget entity etar is determined from Esyn. Finally, the entity fusion rules are used to copy the\nproperties and relations of the remaining instances in Esyn to etar, and the names of these"}, {"title": "KAG-Model", "content": "KAG includes two main computational processes: offline index building and online query and an-\nswer generation. In the era of small language models, these two tasks were typically handled by\ntwo separate pipelines, each containing multiple task-specific NLP models. This results in high\ncomplexity for the application system, increased setup costs, and inevitable cascading losses due to\nerror propagation between modules. In contrast, large language models, as a capability complex,\ncan potentially integrate these pipelines into a unified, simultaneous end-to-end reasoning process.\nAs shown in Figure 7, the processes of indexing and QA each consist of similar steps. Both of the\ntwo pipelines can be abstracted as classify, mention detection, mention relation detection, seman-\ntic alignment, embedding, and chunk, instance, or query-focused summary. Among these, classify,\nmention detection, and mention relation detection can be categorized as NLU, while semantic align-\nment and embedding can be grouped under NLI. Finally, the chunk, instance or query-focused sum-\nmary can be classified under NLG. Thus, we can conclude that the three fundamental capabilities of\nnatural language processing that a RAG system relies on are NLU, NLI, and NLG.\nWe focused on exploring methods to optimize these three capabilities, which are introduced in sub-\nsections 2.5.1, 2.5.2, and 2.5.3 respectively. Additionally, to reduce the cascade loss caused by\nlinking models into a pipeline, we further explored methods to integrate multiple inference pro-\ncesses into a single inference. Subsection 2.5.4 will discuss how to equip the model with retrieval\ncapabilities to achieve better performance and efficiency through one-pass inference."}, {"title": "Natural Language Understanding", "content": "NLU is one of the most common foundational tasks in natural language processing, including text\nclassification, named entity recognition, relation Extraction, subject and object extraction, trigger\ndetection, event argument extraction, event extraction, and machine reading comprehension. We\nhave collected over 30 public datasets to enhance understanding capabilities. Experiments found\nthat simply transforming the original datasets into instruction datasets can achieve comparable re-\nsults to specialized models on trained tasks, but this approach does not improve the model's NLU\ncapabilities on unseen domains. Therefore, we conducted large-scale instruction reconstruction, de-\nsigning various instruction synthesis strategies to create an NLU instruction dataset with over 20,000\ndiverse instructions. By utilizing this dataset for supervised fine-tuning on a given base model, the\nmodel has demonstrated enhanced NLU capabilities in downstream tasks. The instruction recon-\nstruction strategy mainly consists of the following three types.\n\u2022 Label bucketing: [14]This strategy focuses on label-guided tasks, where the aim is to ex-\ntract text based on labels or map text to specified labels, including classification, NER, RE,\nand EE. When labels in a dataset collectively co-occur in the training set, the model may"}, {"title": "Natural Language Inference", "content": "The NLI task is used to infer the semantic relations between given phrases. Typical NLI tasks include\nentity linking, entity disambiguation, taxonomy expansion, hypernym discovery, and text entailment.\nIn the context of knowledge base question answering, due to the diversity and ambiguity of natu-\nral language expressions, as well as the subtle and different types of semantic connections between\nphrases, it often requires further alignment or retrieval of related information through NLI tasks\nbased on NLU. As described in section 2.4, we categorize the key semantic relation in knowledge\nbase applications into six types. Among these, relations such as isA, isPartOf and contains exhibit"}, {"title": "Natural Language Generation", "content": "Models that have not undergone domain adaptation training often exhibit significant differences\nfrom the target text in domain logic and writing style. Moreover, acquiring sufficient amounts of\nannotated data in specialized domains frequently poses a challenge. Therefore, we have established\ntwo efficient fine-tuning methods for specific domain scenarios, allowing the generation process to\nbetter align with scene expectations: namely, K-Lora and AKGF.\nPre-learning with K-LoRA. First of all, we think that using knowledge to generate answers is the\nreverse process of extracting knowledge from text. Therefore, by inverting the previously described\nextraction process, we can create a 'triples-to-text' generation task. With extensive fine-tuning on a\nmultitude of instances, the model can be trained to recognize the information format infused by the\nKG. Additionally, as the target text is domain-specific, the model can acquire the unique linguistic\nstyle of that domain. Furthermore, considering efficiency, we continue to utilize LoRA-based SFT.\nWe refer to the LoRA obtained in this step as K-LoRA.\nAlignment with KG Feedback. The model may still exhibit hallucinations in its responses due to\nissues such as overfitting. Inspired by the RLHF(Reinforcement Learning with Human Feedback)\napproach[16, 17], we hope that the KG can serve as an automated evaluator, providing feedback on\nknowledge correctness of the current response, thereby guiding the model towards further optimiza-\ntion. First, we generate a variety of responses for each query by employing diverse input formats or\nrandom seeds. Subsequently, we incorporate the KG to score and rank these responses. The scoring"}, {"title": "Onepass Inference", "content": "Most retrieval enhanced systems operate in a series of presentation models, retrievers, and gener-\nation models, resulting in high system complexity, construction costs, and the inevitable concate-\nnation loss caused by error transfer between modules. We introduces an efficient one-pass unified\ngeneration and retrieval (OneGen) model to enable an arbitrary LLM to generate and retrieve in\none single forward pass. Inspired by the latest success in LLM for text embedding, we expand the\noriginal vocabulary by adding special tokens (i.e. retrieval tokens), and allocate the retrieval task\nto retrieval tokens generated in an autoregressive manner. During training, retrieval tokens only\nparticipate in representation fine- tuning through contrastive learning, whereas other output tokens\nare trained using language model objectives. At inference time, we use retrieval tokens for efficient\nretrieving on demand. Unlike the previous pipeline approach where at least two models are needed\nfor retrieval and generation, OneGen unified them in one model, thus eliminating the need for a\nseparate retriever and greatly reducing system complexity."}, {"title": "Experiments", "content": "To evaluate the effectiveness of the KAG for knowledge-intensive question-answerring\ntask, we perform experiments on 3 widely-used multi-hop QA datasets, including HotpotQA [11],\n2WikiMultiHopQA [9], and MuSiQue [10]. For a fair comparison, we follow IRCoT [21] and\nHippoRAG [6] utilizing 1,000 questions from each validation set and using the retrieval corpus\nrelated to selected questions.\nEvaluation Metric. When evaluating QA performance, we use two metrics: Exact Match (EM)\nand F1 scores. For assessing retrieval performance, we calculate the hit rates based on the Top 2/5\nretrieval results, represented as Recall@2 and Recall@5.\nComparison Methods. We evaluate our approach against several robust and commonly utilized\nretrieval RAG methods. NativeRAG using ColBERTv2 [22] as retriever and directly generates an-\nswers based on all retrieved documents [23]. HippoRAG is a RAG framework inspired by human\nlong-term memory that enables LLMs to continuously integrate knowledge across external docu-\nments. In this paper, we also use ColBERTv2 [22] as its retriever [6]. IRCoT interleaves chain-of-\nthought (CoT) generation and knowledge retrieval steps in order to guide the retrieval by CoT and\nvice-versa. This interleaving allows retrieving more relevant information for later reasoning steps.\nIt is a key technology for implementing multi-step retrieval in the existing RAG framework."}, {"title": "Overall Results", "content": "The end-to-end question answering performance is shown in Table 9. Within the RAG framework\nleveraging ChatGPT-3.5, HippoRAG demonstrates superior performance compared to NativeRAG.\nHippoRAG employs a human long-term memory strategy that facilitates the continuous integration\nof knowledge from external documents into LLMs, thereby significantly enhancing question an-\nswering capabilities. Additionally, it can be observed that implementing multi-step retrieval in the\nRAG framework using IRCoT is superior to single-step retrieval. The main reason is that IRCOT\ncan conduct the next search based on the results of the current retrieval, rather than returning all\nresults at once. This also increases the relevance of the retrieved content. Given the substantial\noperational costs associated with utilizing ChatGPT-3.5, we opted to use the DeepSeek-V2 API as a\nviable alternative. On average, the performance of the IRCoT + HippoRAG configuration utilizing\nthe DeepSeek-V2 API slightly surpasses that of ChatGPT-3.5. Our constructed framework KAG\nshows significant performance improvement compared to IRCoT + HippoRAG, with EM increases\nof 11.5%, 19.8%, and 10.5% on HotpotQA, 2WikiMultiHopQA, and MuSiQue respectively, and\nF1 improvements of 12.5%, 19.6%, and 12.2%. These advancements in end-to-end performance\ncan largely be attributed to the development of more effective indexing and retrieval libraries within\nour framework. We evaluate the effectiveness of the single-step retriever and multi-step retriever,\nwith the retrieval performance shown in Table 10. From the experimental results, it is evident that\nthe multi-step retriever generally outperforms the single-step retriever. Analysis reveals that the"}, {"title": "Model Ablation Studies", "content": "details"}, {"title": "Applications", "content": ""}, {"title": "KAG for E-Goverment", "content": "We used the KAG framework and combined it with the Alipay E-government service scenario to\nbuild a Q&A application that supports answering users' questions about service methods, required\nmaterials, service conditions, and service locations. To build the e-government Q&A application,\nwe first collected 11,000 documents about government services, and based on the methods described\nin section 2, implemented functional modules such as index building, logical-form-guided reasoning\nand solving, semantic enhancement, and conditional summary generation.\nDuring the offline index construction phase, the semantic chunking strategy is used to segment gov-\nernment service documents to obtain specific matters and their properties such as the administrative\nregion, service process, required materials, service location, target audience, and the corresponding\nchunks."}, {"title": "KAG for E-Health", "content": "We have developed a medical Q&A application based on the Alipay Health Manager scenario,\nwhich supports answering user's questions regarding popular science about disease, symptom,\nvaccine, operation, examination and laboratory test, also interpretation of medical indicators, med-\nical recommendation, medical insurance policy inquires, hospital inquires, and doctor information\ninquires. We have sorted out authoritative medical document materials through a team of medical\nexperts, and produced more than 1.8 million entities and more than 400,000 term sets, with a total\nof more than 5 million relations. Based on this high-quality KG, we have also produced more than\n700 DSL\u00b3 rules for indicator calculations to answer the questions of indicator interpretation.\nDuring the knowledge construction phase, a strongly constrained schema is used to achieve\nprecise structural definition of entities such as diseases, symptoms, medications, and medical\nexaminations. This approach facilitates accurate answers to questions and generates accurate\nknowledge, while also ensuring the rigor of relations between entities. In the reasoning phase, the\nlogical form is generated based on the user's query, and then translated to DSL form for the query\non KG. The query result is returned in the form of triples as the answer. The logical form not only\nindicates how to query the KG, but also contains the key structural information in the user's query\n(such as city, gender, age, indicator value, etc.). When parsing the logical form for query in graph,\nthe DSL rules which produced by medical expert will also be triggered, and the conclusion will\nbe returned in the form of triples. For example, if a user asks about \"blood pressure 160\", it will\ntrigger the rules as:"}, {"title": "Conclusion and Future Work"}]}