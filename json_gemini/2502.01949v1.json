{"title": "LAYOUTDREAMER: Physics-guided Layout for Text-to-3D Compositional Scene Generation", "authors": ["Yang Zhou", "Zongjin He", "Qixuan Li", "Chao Wang"], "abstract": "Recently, the field of text-guided 3D scene generation has garnered significant attention. High-quality generation that aligns with physical realism and high controllability is crucial for practical 3D scene applications. However, existing methods face fundamental limitations: (i) difficulty capturing complex relationships between multiple objects described in the text, (ii) inability to generate physically plausible scene layouts, and (iii) lack of controllability and extensibility in compositional scenes. In this paper, we introduce LAYOUTDREAMER, a framework that leverages 3D Gaussian Splatting (3DGS) to facilitate high-quality, physically consistent compositional scene generation guided by text. Specifically, given a text prompt, we convert it into a directed scene graph and adaptively adjust the density and layout of the initial compositional 3D Gaussians. Subsequently, dynamic camera adjustments are made based on the training focal point to ensure entity-level generation quality. Finally, by extracting directed dependencies from the scene graph, we tailor physical and layout energy to ensure both realism and flexibility. Comprehensive experiments demonstrate that LAYOUTDREAMER outperforms other compositional scene generation quality and semantic alignment methods. Specifically, it achieves state-of-the-art (SOTA) performance in the multiple objects generation metric of T\u00b3Bench.", "sections": [{"title": "1 Introduction", "content": "3D models are widely applied in various fields, including autonomous driving, product concept design, gaming, augmented reality (AR), and virtual reality (VR). With rapid advancements in text-to-image models [Rombach et al., 2022; Saharia et al., 2022], text-to-3D generation technology has also made significant progress in generating individual entities [Abelson et al., 1985; Metzer et al., 2023; Poole et al., 2022]. However, these models still face challenges in more complex generation tasks, such as creating objects within a contextual surrounding or generating multiple interacting objects. In these cases, they often struggle to accurately capture intricate spatial relationships, leading to inconsistencies and unrealistic outputs. These issues manifest as variations in the appearance from different viewpoints and outputs that fail to adhere to physical constraints. Even generating an interactive 3D asset that integrates with an existing one remains a significant challenge.\nRecently, several studies have attempted to extend text-to-3D generation to the creation of compositional 3D scenes. Compositional scene generation refers to creating a coherent layout for a finite set of 3D assets by analyzing their spatial interactions, guided by a detailed scene prompt. Some methods incorporate additional layout information [Bai et al., 2023; Po and Wetzstein, 2024; Zhou et al., 2024; Cohen-Bar et al., 2023], imposing strict constraints on the spatial arrangement and interactions of objects. However, these methods have inherent limitations: Constraints on flexibility and expansion potential. These models tend to focus heavily on layout, which restricts the diversity of individual 3D assets and diminishes the consistency between the text input and the generated 3D assets. Another research direction seeks to guide 3D generation using 2D diffusion priors [Gao et al., 2024; Chen et al., 2024a; Ge et al., 2024]. Although these approaches offer greater flexibility in compositional generation and produce high-quality results, they also have the limitation: A single perspective is insufficient to provide 3D consistent cues for compositional interactions. This leads to significant performance variations across viewpoints, with some perspectives potentially producing unrealistic results.\nTo achieve compositional scenes conforming to physical realism, we propose LAYOUTDREAMER, an innovative and scalable framework for generating 3D scenes from intricate text prompts. As shown in Figure 1, our approach comprises three core components. 1) To clarify the interactive relationships within the compositional scene, we present a method specifically developed for initializing compositional 3D Gaussians using scene graphs. Based on the scene graph, the size, density, and position of the initial 3D Gaussians are adaptively adjusted, establishing a disentangled 3D representation. 2) To optimize the poses, sizes, positions, and densities of objects in the scene, we propose a dynamic camera roaming strategy that adaptively determines the focal point and focal length during training, ensuring accurate rendering of objects at varying distances and with diverse textures. 3) To integrate real-world physical fields, including gravity, mutual penetration, anchoring, and center of mass stability, into the compositional scene, we define a layout energy function by minimizing physical and layout constraints in two stages. This enables a detailed, orderly, and physically consistent arrangement, facilitating the rapid expansion and editing of existing scenes.\nExtensive qualitative and quantitative studies demonstrate that LAYOUTDREAMER can efficiently generate and arrange 3D scenes, ensuring high-fidelity 3D consistency and adherence to physical laws. Our contributions are summarized as follows:\n1) To the best of our knowledge, LAYOUTDREAMER is the first text-to-3D compositional scene method by incorporating physical fields, simulating various entity layout scenarios under realistic physical constraints.\n2) LAYOUTDREAMER facilitates highly controllable scene editing and expansion by constructing a disentangled representation from a directed scene graph.\n3) LayoutDREAMER is capable of generating high-fidelity, physics-conforming complex 3D scenes, outperforming SOTA compositional text-to-3D methods."}, {"title": "2 Related Work", "content": "2.1 Text-guided 3D Generation\nEarly works in text-to-3D generation, such as CLIP-forge [Sanghi et al., 2022], Dream Fields [Jain et al., 2022], Text2Mesh [Michel et al., 2022], CLIP-NeRF [Wang et al., 2022], and CLIP-mesh [Mohammad Khalid et al., 2022] employed CLIP as a guidance mechanism for 3D generation. However, DreamFusion [Poole et al., 2022] introduced the Score Distillation Sampling (SDS) loss, significantly advancing the quality of 3D models with the aid of 2D diffusion guidance. Magic3D [Lin et al., 2023] improved the quality of generated models by employing a two-stage optimization process, progressing from coarse to fine. Fantasia3D [Chen et al., 2023] prioritized the optimization of geometry and texture in 3D models, while ProlificDreamer [Wang et al., 2024] enhanced the diversity of SDS loss and addressed out-of-distribution issues by introducing Variational Score Distillation. Similarly, Score Jacobian Chaining (SJC) [Wang et al., 2023] proposed a method for 3D generation using 2D diffusion, leveraging the Perturb-and-Average Scoring (PAAS) technique to iteratively optimize 3D structures. Additionally, other works utilized 3DGS as a 3D representation to achieve rapid and high-fidelity model generation. DreamGaussian [Tang et al., 2023] initialized 3D Gaussians by randomly assigning positions within a sphere. However, this approach introduced a bias, favoring spherical symmetry in generated structures. In contrast, methods such as GaussianDreamer [Yi et al., 2023], GSGEN [Chen et al., 2024b], and GaussianDiffusion [Li et al., 2023] employed pre-trained 3D generation models to initialize 3D Gaussians, offering a more versatile approach.\n2.2 Complex Scene Generation\nEarly methods [Chang et al., 2014] for synthesizing 3D scenes used scene graphs to define objects and organize spatial relationships. Giraffe [Niemeyer and Geiger, 2021] used compositional NeRF for scene representation, while Set-the-Scene [Cohen-Bar et al., 2023] developed a style-consistent, disentangled NeRF-based framework for scene generation. Text2Room [H\u00f6llein et al., 2023] and Text2NeRF [Zhang"}, {"title": "3 Method", "content": "3.1 Overview\nAs shown in Figure 1, given a text prompt Tp to generate a scene O = {oi}M1 with M objects, we begin by constructing a scene graph G(O) using methods for entity and relationship extraction. To initialize the 3D entities, we generate point clouds using Shap-E [Jun and Nichol, 2023], which are then converted into 3D Gaussians. We introduce a density adjustment method based on the size pool and a chain-based positioning method utilizing layout pools to optimize objects' size, density, and position. Next, we employ a decomposed optimization strategy to iteratively train and refine the scene, performing M camera roams with an adaptive strategy to optimize the generation of entities (Section 3.3). Following this, we use the scene graph to derive scene-guided configurations, allowing us to customize the scene's physical and layout constraints. These constraints are then optimized under a dynamic, hierarchical energy function, ensuring a neat and logically consistent arrangement of objects (Section 3.4).\n3.2 Scene Graph-guided Initial 3D Gaussians\nGiven a user text prompt Tp, a directed scene graph is constructed by parsing the objects and spatial dependencies described in the text. In this graph, object entities are represented as nodes and various relationships are mapped to standardized forms, represented as directed edges (e.g., 'on' and 'upon' are mapped to the standard relation 'on').\nScale-aware Density Adjustment\nTo ensure that the generated initial 3D Gaussians volumes adhere to real-world dimensional standards, we design a size pool for the nodes in the scene graph. The size pool comprises object categories, size levels, and corresponding values. After two rounds of semantic similarity matching, each object is assigned to a specific size pool, and its standard size value Si (where i \u2208 M) is determined. Considering both the standard size and the current object's size, we apply a scale-aware density adjustment technique to ensure that after scaling, 3D Gaussians maintain consistent density while preserving essential geometric details. Specifically, when the standard size exceeds the current size, we perform interpolation based on the volume ratio before and after scaling to increase the density of 3D Gaussians. Conversely, when the standard size is smaller than the current size, we use a combined method of voxel grid downsampling and geometric feature sampling to reduce the number of 3D Gaussians for smaller objects. This approach minimizes training overhead while retaining essential geometric feature points.\nChain-based Position Initialization\nIn complex scene interactions, an object's spatial position is determined by its interaction relationships and the positions of other involved objects. To obtain a coarse scene layout, we introduce a layout pool for the directed edges in the scene graph. The pool contains the standard offset \u2206P(rk) for each standard dependency relationship rk, along with energy term weights used during layout training (Section 3.4). Each object is processed according to topological sorting, with all incoming spatial dependencies aggregated for updates. For each object oi, its position P(oi) is determined by all incoming relationships:\nP(oi) = \\sum_k (P(sk) + \\Delta P(rk)), (1)\nAP(rk) = [\u2206x(rk), \u2206y(rk), \u2206z(rk), \u2206d(rk)], (2)\nwhere sk is the dependent object and \u2206x(rk), \u2206y(rk), Az(rk) represent the standard directional offsets for relationship rk. Ad(rk) denotes the distance scaling offset. To differentiate each entity, we assign an independent feature label L = {li}M1 and incorporate it with the information gathered from the size and layout pool into a scene-guided configuration Configs(oi) = {oi, li, P(oi), Si}.\n3.3 Dynamic Camera Roaming Driven by Training Focus\nThe camera configuration plays a crucial role in SDS [Poole et al., 2022], especially when capturing scenes with occlusion relationships. With static camera configuration facing the origin, issues such as incomplete information from objects at varying positions may arise due to perspective limitations. Additionally, significant size differences between objects can lead to challenges: larger objects may experience internal Janus problems during SDS optimization, while smaller objects may lack detailed texture information. Therefore, we design a dynamic camera roaming strategy driven by the training focus. During entity-level training optimization, the label li, size Si, and position information P(oi) of the current object are directly retrieved from the scene-guided configuration. We unfreeze only the parameter groups corresponding to the current label for entity training, while the camera tracks the entity and adjusts its position based on the object's location. The camera's orientation di is recalculated towards the object's center after the adjustment. By evaluating the ratio of the object's actual size to the camera-defined standard size, we can determine a distance adjustment factor ai, which is used to adjust the camera depth. The final camera position is given by:\nC' = C + P(oi) - aidi with di = \\frac{P(oi) - C}{||P(oi) \u2013 C||}, (3)\nwhere C' is the adjusted camera position, and C is the original camera position. By adjusting both the camera's translation and depth, objects within the field of view are rendered optimally.\nIrregular object edges may lead to layout complexity and cause interpenetration issues. To mitigate this, we encourage the transmittance of the foreground to approach either 0 or 1. This technique facilitates the removal of floating objects and corrects 3D Gaussians whose edges are significantly impacted by variations in 2D diffusion guidance results, inspired by [Fridovich-Keil et al., 2022; Shriram et al., 2024]. In a scene containing M objects, disentangled scene generation is achieved by training each object separately, ensuring high-quality, 3D-consistent, and well-separated objects. The total loss during the entity generation phase is expressed as:\nL = \\sum_{i=1}^M(LSDS(i) + \\lambda_O L_O(i)), (4)\nwhere \u03bb is a hyperparameter controlling the contribution of the opacity loss.\n3.4 Physical Field Integration through Layout Energy Function\nBy utilizing the edge relationships in the scene graph, we stabilize the scene layout by minimizing the total energy. We define methods for both physical and layout energy, then integrate the layout pool to derive the corresponding energy terms and their respective weights, ultimately resulting in the final total energy.\nDesign of Energy Models Reflecting Physical Reality\nTo ensure the compositional generation process adheres to the principles of physical reality, we simulate various physical layout conditions, including gravity, the influence of centroid on positioning, and the non-penetration and mutual anchoring of objects. Simultaneously, other layout energy terms are introduced to refine the spatial relationships.\nGravity energy term. To stabilize objects under gravity, we define the following bounding boxes for each entity to efficiently evaluate their direction and posture within the explicit 3DGS representation. Specifically, we set z = 0 as the ground plane. The gravity energy term stabilizes the object's position by minimizing the height deviation at the bottom of the following bounding box, ensuring that objects settle onto the ground. This term is expressed as:\nE_g^{(i)} = mean(z')^2 + \\cdot max(0, - min(z')), (5)\nwhere z' is the height of the bottom vertices of the following bounding boxes.\nPenetration energy term. To ensure proper contact between objects and prevent mutual penetration in the constraint optimization problem of a multi-object compositional system, we draw inspiration from CG3D [Vilesov et al., 2023] to define the penetration energy term $E_p^{(i,j)}$. For a Gaussian with mean \u00b5i in object O2, centered at q2, and a Gaussian with mean \u00b5j in O1, which is the closest to O2, a penalty based on the negative cosine is applied to enforce the angle \u03b8i between vectors v1 = \u00b5i - q2 and v2 = \u00b5j \u2013 \u00b5i to be acute, thus preventing penetration between the two objects. The penetration energy term is:\nE_p^{(i,j)} = k \\sum_{i=1}^N \\sum_{j=1}^N max (0, - cos(\\theta_i)), (6)\nwhere k is the repulsive strength coefficient and N is the number of Gaussians in O2.\nAnchor energy term. Considering special scenarios, such as hook-like relationships, we design an anchor energy term activated when the penetration energy term is triggered, ensuring that the anchor points do not experience undesirable shifts or aggregation. When two objects come into contact, each has an associated anchor point $A_{dd}^{(i)}$ in its local coordinate, transformed into world coordinates $A_w^{(i)}$ during layout training. The anchor energy term $E_a^{(i,j)}$ penalizes deviations between actual and expected anchor point distances, modeled as elastic potential energy:\nE_a^{(i,j)} = \\frac{1}{2}k (||A_w^{(i)} \u2013 A_w^{(j)}|| \u2013 d)^2, (7)\nwhere d denotes the expected distance between the anchor points, and k is the spring constant hyperparameter, controlling the intensity of the anchor constraint.\nOther energy terms. Proper positioning of an object's centroid is essential for maintaining system stability and physical plausibility. Therefore, we introduce a centroid energy term that minimizes the vertical displacement of object centroids. Additionally, the layout energy function enforces adherence to physical laws while preserving semantic coherence and visual appeal. To complement the centroid energy term, we propose an alignment energy term, which minimizes the directional differences between the nearest principal axes of two objects across different dimensions, quantifying deviations between the centers of mass of the objects. By reducing the centroid difference along the target direction, the alignment energy term enhances spatial orderliness and promotes logical object arrangement. For optimizing object distances, we implement a proximity energy term that limits sparsity by calculating the distance between the closest points of distant objects. This term ensures that objects maintain an appropriate level of spatial coherence while preventing excessive gaps in the scene. When an anchor energy term is required, the attachment energy term enforces the ideal distance between the nearest points of two interacting objects, which helps maintain stable relationships in anchor-reliant scenarios. While the centroid energy term optimizes object positions globally, it may sometimes induce unnatural rotations, compromising physical realism. To address this, we include a rotation energy term that restricts the maximum allowable rotation angle, ensuring the layout remains physically consistent and visually plausible.\nOptimization of Compositional Scene\nTo optimize the compositional scene layout, we freeze the other parameters of the 3D Gaussians parameter groups and focus solely on training the translation and rotation parameters. Within the defined energy constraints, the optimization includes mutual energy terms (e.g., the penetration energy) and individual energy terms (e.g., the gravity energy). By traversing the nodes and directed edges of the scene graph, we assign energy terms and weights to each node systematically. The total constrained energy function is divided into the layout and physical energy functions, with priority given to the latter. Together, these energy terms define the overall energy functions:\n\\mathbb{E}_k = \\sum_{(i,j)\\in e} \\mathbb{E}_k^{(i,j)} = \\sum_{(i,j)\\in e} (\\omega_k^P \\mathbb{E}_P^{(i,j)} + \\omega_k^L \\mathbb{E}_L^{(i,j)}) + \\sum_{i\\in N} (\\omega_k^P \\mathbb{E}_P^{(i)} + \\omega_k^L \\mathbb{E}_L^{(i)}), (8)\nwhere e and N represent all the edges and nodes in the scene graph. $E_P^{(i,j)}$ and $E_L^{(i,j)}$ denote the mutual physical and layout energy terms between two connected entities, while $E_P^{(i)}$ and $E_L^{(i)}$ refer to the individual physical and layout energy terms for each node. Additionally, we represents the weight associated with each energy term.\nTo achieve an optimal configuration that satisfies physical constraints during scene layout training, we propose a two-phase hierarchical energy minimization method. The total energy function, encompassing both physical and layout energy terms, is expressed as:\n\\mathbb{E}^{(t)} = \\lambda_P^{(t)} \\mathbb{E}_P + \\lambda_L^{(t)} \\mathbb{E}_L, (9)\nwhere $\\mathbb{E}_P$ and $\\mathbb{E}_L$ represent the physical and layout energy functions after L2 regularization, ensuring that energy terms of different magnitudes are scaled to a unified order of magnitude. $\\lambda_P^{(t)}$ and $\\lambda_L^{(t)}$ are the physical and layout constraint weights at step t, respectively.\n\\mathbb{E}_P = \\frac{\\mathbb{E}_P}{\\sqrt{\\|\\mathbb{E}_P\\|_2 + \\|\\mathbb{E}_L\\|_2^3}} \\qquad \\mathbb{E}_L = \\frac{\\mathbb{E}_L}{\\sqrt{\\|\\mathbb{E}_P\\|_2 + \\|\\mathbb{E}_L\\|_2^3}} (10)\n\\lambda_P^{(t)} = \\begin{cases} 1, & \\text{if } t < x, \\\\ 1 - \\frac{1}{2}(1 - cos(\\pi \\frac{t-x}{T-x})), & \\text{if } x \\le t \\le T, \\end{cases} (11)\n\\lambda_L^{(t)} = \\begin{cases} 0, & \\text{if } t < x, \\\\ \\frac{1}{2}(1 - cos(\\pi \\frac{t-x}{T-x})), & \\text{if } x \\le t \\le T, \\end{cases}\nIn Equation (11), x denotes the number of steps required for the physical energy to reach its threshold, and T is the total number of training steps. B controls the amplitude of the physical energy weight (where 0 < \u03b2 < 1). Initially, training emphasizes physical energy constraints until the physical energy falls below the threshold at step x. Afterward, a two-phase joint training process optimizes both physical and layout energy. Cosine functions alternate the weights of physical and layout energy, reducing the risk of getting trapped in local minima of the physical energy function. Meanwhile, the physical energy weight is gradually increased towards the end of training to ensure the layout aligns with physical reality."}, {"title": "4 Experiments", "content": "4.1 Implementations Details\nLAYOUTDREAMER is implemented using PyTorch and is built upon ThreeStudio [Liu et al., 2023]. For the complex prompts in T\u00b3Bench, we use the 8B Llama3 model to extract the subjects and relationships from the text. We employ GaussianDreamer [Yi et al., 2023] as the multi-view diffusion model. The process requires 2000 iterations to train a single object. However, for layout optimization in a scene containing three objects and 15 energy constraints, convergence is achieved within just 300 steps. Our experiments can be completed using a single RTX 3090 GPU with 43G memory. The average total generation time for a scene with M objects is 21 \u00d7 M + 2 \u00d7 CM minutes, where generating a single object takes approximately 20 minutes. Additionally, each pair of mutual energy terms requires about 2 minutes for computation, while calculating the total energy for an individual object's energy term takes approximately 1 minute.\n4.2 Comparisons with Other Methods\nTo validate the effectiveness of our method, we evaluate generation quality and text alignment using the T3Bench [He et al., 2023] evaluation criteria, which provide a comprehensive set of metrics for text-to-3D generation, particularly focusing on multiple objects compositional generation.\nQualitative Comparison. We compare our method with recent works in composition scene generation that use layouts to guide 3D scene generation. Since most of these works are not open-source, we directly reference results presented in their papers and use identical prompts to generate comparable 3D scenes. As shown in Figure 2, both Comp3D [Po and Wetzstein, 2024] and CompoNeRF [Bai et al., 2023] suffer from scene blurring, while CG3D [Vilesov et al., 2023] offers a reasonable layout but lacks spatial orderliness. GALA3D [Zhou et al., 2024] achieves good decoupled generation; our method excels by producing 3D assets with superior texture detail and complete recognition of entity prompts. Furthermore, we compare LAYOUTDREAMER with several open-source methods for text-to-3D generation, including SJC [Wang et al., 2023], LatentNeRF [Metzer et al., 2023], Dreamfusion [Poole et al., 2022] and methods that use point clouds for 3D Gaussians initialization, such as"}, {"title": "4.3 Ablation Studies", "content": "To validate the effectiveness of the key components of LAYOUTDREAMER, we design ablation experiments for compositional 3D Gaussians initialization (CGS Init.), the dynamic camera roaming strategy (DCR), and layout energy constraints (LEC).\nCompositional 3D Gaussians Initialization. To achieve the initial compositional 3D Gaussians expression, we employ a scale-aware density adjustment combined with a chain-based position initialization. However, we directly generate the initial 3D Gaussians using point cloud priors and do not use chain-based position initialization to impose rough layout information on the 3D Gaussians entities. As shown in Figure 4, the 3D Gaussians entities are initialized with unrealistic sizes. The coarse layout, characterized by mutual penetration, leads to confusion in the layout optimization process.\nStatic Random Camera Capture. In the forward rendering process of 3DGS, we do not employ a dynamic camera"}, {"title": "4.4 Scalable Disentangled Scene Layout", "content": "LAYOUTDREAMER is compatible with all 3DGS representations, offering enhanced control over disentangled scenes by designing scene-guided configurations for each entity. As illustrated in Figure 6, LAYOUTDREAMER allows for efficient removal, movement, and regeneration of objects, providing precise control over the scene composition. Additionally, it supports the dynamic combination and rearrangement of 3D Gaussians scene representations alongside scene-guided configuration, enabling seamless dynamic expansion. This rapid scene editing and incremental expansion make LAYOUTDREAMER well-suited for practical real-world applications requiring adaptive and scalable 3D asset creation."}, {"title": "5 Conclusion", "content": "We introduce LAYOUTDREAMER, a framework for rapidly generating physically realistic and well-structured 3D scenes using text prompts, demonstrating high-quality scene generation and consistency. LAYOUTDREAMER provides a reasonable initialization approach for the domain of compositional 3D Gaussians scene generation. By converting the text into a scene graph, the generated scene achieves an organized layout based on spatial interactions and physical constraints within 15 minutes, allowing users to conveniently and efficiently edit and expand disentangled scenes. Experimental results show that LAYOUTDREAMER outperforms existing methods in text-to-scene generation, effectively handling intricate text to create dynamic interactions among multiple objects while adhering to real-world physical principles."}]}