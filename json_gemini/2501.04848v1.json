{"title": "Exploring Large Language Models for Semantic Analysis and Categorization of Android Malware", "authors": ["Brandon J Walton", "Mst Eshita Khatun", "James M Ghawaly", "Aisha Ali-Gombe"], "abstract": "Malware analysis is a complex process of examining and evaluating malicious software's functionality, origin, and potential impact. This arduous process typically involves dissecting the software to understand its components, infection vector, propagation mechanism, and payload. Over the years, deep reverse engineering of malware has become increasingly tedious, mainly due to modern malicious codebases' fast evolution and sophistication. Essentially, analysts are tasked with identifying the elusive needle in the haystack within the complexities of zero-day malware, all while under tight time constraints. Thus, in this paper, we explore leveraging Large Language Models (LLMs) for semantic malware analysis to expedite the analysis of known and novel samples. Built on GPT-40-mini model, MalParse is designed to augment malware analysis for Android through a hierarchical-tiered summarization chain and strategic prompt engineering. Additionally, MalParse performs malware categorization, distinguishing potential malware from benign applications, thereby saving time during the malware reverse engineering process. Despite not being fine-tuned for Android malware analysis, we demonstrate that through optimized and advanced prompt engineering MalParse can achieve up to 77% classification accuracy while providing highly robust summaries at functional, class, and package levels. In addition, leveraging the backward tracing of the summaries from package to function levels allowed us to pinpoint the precise code snippets responsible for malicious behavior.", "sections": [{"title": "Introduction", "content": "Reverse engineers face significant challenges in analyz-ing and understanding malware samples in the ever-evolving cybersecurity domain. Even more so when dealing with suspected novel or zero-day malware, reverse engineering the unknown binary or program file is crucial. This approach involves dissecting the malware's code, structures, and be-haviors to understand its mechanisms. However, reverse engineering demands a high skill level and deep knowledge of coding, system architecture, and cybersecurity principles. As such, understanding malware capabilities through reverse engineering can be time-consuming and complex especially for advanced, and obfuscated trojans with extensive code bases such as Android malware. Thus, in this paper, we propose a technique to enhance the current static process of Android malware analysis by exploring the capabilities of pre-trained Large Language Models (LLMs). Our pro-posed approach, titled MalParse, allows for categorizing malware, detailed code summarization, and efficient back-tracking within the codebase to identify the root causes of malicious activity. Essentially, MalParse streamlines mal-ware analysis by providing analysts with a summary of the malware and a direct path to its root causes. To achieve this, MalParse is designed with three major components: Auto-decompilation and Feature Extraction, Prompt Engi-neering, and Hierarchical-Tiered Code Summarization mod-ules. In the Auto-decompilation and Feature Extraction module, a target Android application is reverse-engineered and decompiled to access its source code. The proposed Hierarchical-Tiered Summarization module breaks down and summarizes extracted code structures at multiple gran-ularity levels, specifically focusing on function-level, class-level, and package-level summaries. Starting from the most basic building blocks of a program, individual functions are summarized, and these summaries are then aggregated and re-summarized to generate contextual summaries for each class. This iterative process is applied to all classes within the Android application. Finally, the class summaries are consolidated and fed into the model to generate a compre-hensive package summary, providing an overview of the application's functionality, identifying any detected mali-cious activities, if present, and tagging the sample as either malicious or benign. Furthermore, our proposed approach is further tuned using Prompt Engineering to improve the accuracy of a foundational LLM, which was not fine-tuned on malware. This component of MalParse pre-instructs the model with three prompt categories to guide the model's un-derstanding of malware: Vanilla for functionality overview, API-Scoped for common permissions and APIs used in malware, and Malware-Scoped for identifying malicious indicators such as privilege escalation and dynamic code execution.\nEvaluating the effectiveness of our approach, MalParse was tested on a dataset comprising 200 Android applica-tions, split evenly between malware and benign. The opti-"}, {"title": "2. Literature Review", "content": "Automated code summerization are techniques that gen-erates natural language descriptions of high-level source code using machine learning. From a security standpoint, integrating code summarization techniques with malware analysis signifies a substantial advancement in cybersecurity initiatives aimed at analyzing malware threats."}, {"title": "2.1. LLMs for Code Summarization", "content": "Recent advancements in generative AI, notable Large Language Models (LLMs), have sparked a growing interest in the use of LLMs for tasks involving the understanding and generation of code documentation, repair, and test-ing. One example of an LLM developed for such tasks is CodeBERT [1], which is trained on both natural and programming languages aimed at improving code search and comprehension. Similarly, GraphCodeBERT [2] integrates structural code information into the CodeBERT training process to enhance its performance on code related tasks. PLBART [3] on the other hand explores the utility of LLMs trained in a denoising autoencoding paradigm for a variety of programming tasks, including code documen-tation and summarization. Large foundational LLMs have also garnered significant attention for programming tasks. OpenAI's GPT-3 [4], has demonstrated exceptional abilities in producing text, including coherent code segments. Many studies [5], [6], [7], [8], [9] have evaluated ChatGPT's automatic bug resolution performance, revealing its potential nature for enhancing software repair methodologies. Zahan et al. [10] investigated a variety of LLMs, including GPT-3 and GPT-4, for the purpose of detecting malware in the npm ecosystem (JavaScript code). Their study served as a baseline for JavaScript malware analysis and demonstrated notable improvements over static analysis tools in terms of precision and F1 scores. In contrast, our proposed MalParse explores Android malware analysis using a hierarchical-tiered approach with function-, class-, and package-level summaries. The novelty of our work lies not only in the categorization of malware, which we believe will be highly beneficial for analysts, but also in the final package-level descriptions aimed at aiding analysts in evaluating zero-day samples. Our evaluation demonstrates the critical impact of strategic and effective prompt engineering in enabling the LLM to effectively categorize and generate enhanced descriptions of target samples."}, {"title": "3. Design and Implementation", "content": "This section outlines the conceptual methodology and implementation of the proposed Malware Semantic Parser -MalParse. Broadly, this approach can be described around three primary steps: 1) auto-decompilation and static fea-ture extraction, 2) hierarchical-tiered summarization, and 3) model optimization using advanced prompt engineering techniques to contextualize the summarization module for APK package analysis."}, {"title": "3.1. Auto-decompilation and Feature Extraction Module", "content": "The initial step in analyzing any program of unknown origin involves reverse engineering, typically accomplished through disassembly, decompilation, or a combination of both [11]. Since Android applications are developed in Java and compiled into a compressed intermediate format known as Davik Bytecode (Dex), decompilation is the standard"}, {"title": "3.2. Hierarchical-Tiered Code Summarization", "content": "Following the code extraction, our approach leverages a hierarchical-tiered code summarization to conduct a thor-ough analysis of the program files and functions as shown in Figure 1. The summarization module which was imple-mented in Python and compatible with any general-purpose LLM priotize the identification and concise summarization of potential program functionality inherent in the code base. It does so using a bottom-up iterative approach, begin-ning with summarization at the individual function level and working up to the full package-level. The hierarchical approach offers two key advantages. First, it allows the use of LLMs with restricted context windows, resulting in cost savings and reduced resource requirements. Second, it generally produces superior summarization quality for large contexts compared to a single-step approach [14], [15]. In designing MalParse, we focus on OpenAI's GPT-40-mini model due to its high-performance on major benchmarks, low cost, and its large 128k token context window, which enables the processing of larger chunks of code. We also use the Langchain [16] library for implementing custom LLM chains and prompt templates."}, {"title": "3.3. Model Optimization using Advanced Prompt Engineering", "content": "In MalParse, during the code analysis and summariza-tion, three sets of prompting are utilized - Vanilla, API-scoped, and Malware-scoped. Each category provides the LLM with different contextual information and scope on its objective.\n\u2022 In the Vanila-scoped, the LLM is not provided with any information about what constitutes potentially sus-picious features.\n\u2022 In the API-scoped, the prompts provide the LLM with information on identifying key sensitive APIs, per-missions, and libraries commonly found in Android malware.\n\u2022 In the Malware-Scoped, the prompts offer the model additional information for identifying malicious func-tionalities and malware types such as dynamic class loading, rooting, data exfiltration etc.\nEach of these prompts enable the model to dynamically expand its context window by iteratively summarizing all functions, thereby generating class-level summaries. Subse-quently, these class-level summaries are combined to pro-duce package-level summaries. This hierarchical approach results in a more comprehensive contextual summary of potential security threats and common activities within the codebase. To craft the most effective prompts, several tech-niques were employed:\n1. Few-Shot Prompting: The technique of few-shot prompting was employed, where expected input and output formats were provided to guide the model. This approach ensured that the model received clear instructions on the desired input structure and the expected format for generated summaries, facilitating more accurate and relevant output.\n2. Context Injection: Through tailored prompts (API-and Malware-Scoped), contextual cues are injected to guide the LLM in summarizing code snippets, with a particular emphasis on dangerous APIs, sensitive permissions, and unusual programming practices present in the code. This approach ensures that the final summary retains pertinent context, specifically addressing security considerations."}, {"title": "4. Evaluation", "content": "In this section, we evaluate the performance of Mal-Parse based on two criteria: 1) Accuracy in distinguishing malware from benign Android applications across three dif-ferent prompting categories; 2) the Efficacy of the model's response after package-level summarization."}, {"title": "4.1. Experimental Setup", "content": "Dataset and Preprocessing: To ensure reproducibility, our dataset comprised 200 APK files, with 100 identified as known malicious applications and the remaining 100 as known benign applications. The malicious samples, sourced from VirusTotal, included various malware categories such as ransomware, scareware, adware, and spyware. Notable examples within these categories included WannaLocker, MazarBot, and Jifake. The benign samples, sourced from the top 500 applications listed by Similarweb in the United States[17], included notable apps such as Facebook, Google, YouTube, LinkedIn, Outlook, and Instagram. These benign samples served as a reliable benchmark to assess the LLM's ability to differentiate between malicious and benign APK files. Each APK sample is processed through the auto-decompilation and feature extraction process as described in Section 3, resulting in the decompilation of all classes within each APK and their preparation for input. Additionally, to mitigate bias in the LLM's summaries, we did not fine-tune (train) the LLM on any known examples of benign or malicious Android applications. This allows the MalParse to conduct semantic analysis on \"newly\" encountered APKs without any contextual bias. This approach is critically important as it simulates an analyst examining a 0-day sample for the first time without prior knowledge of what to expect. Furthermore, to mitigate the GPT-40-mini model from relying on memory, the names of the APKs were not provided. This aids in reducing the model's reliance on external information such as the name of the application, aiding in the prevention of it categorizing the application before deeper code analysis.\nModel Execution: After pre-processing, the input fea-tures for each APK are passed to the Code Summarization Module of MalParse for execution as detailed in Section 3. To facilitate a more efficient analysis, separate instances of the program were deployed for analyzing the malware and benign samples. Both instances ran concurrently on a local server, allowing simultaneous and independent evaluations of each sample type."}, {"title": "4.2. Experimental Results", "content": "4.2.1. R1 - Evaluation of the Categorization Accuracy.\nThe objective of this first test is to assess how effectively MalParse can identify and categorize applications based on their security status (malicious vs. benign) under varied contextual prompts. This metric is crucial, particularly in determining how prompt scoping impacts and influences the general performance of the model.\n1. Vanilla-Based App Categorization: The Vanilla Prompts are used to provide summaries of the APK's general functionality without referencing any context to what suspi-cious activities, device permissions, or API calls to observe. The result of our analysis indicates that the Vanilla prompts performed the worst in categorizing the APK's true nature. Examining the confusion matrix in Table 1, we see that 92% of the benign samples (TP) and 7% of the malicious samples (TN) were correctly categorized. Dividing the sum by the total samples (200) yields a categorization accuracy of only 49.5%. This indicates these prompts only correctly categorized 99 out of the 200 samples. Overall, the Vanilla prompts demonstrated poor classification accuracy, clearly evidenced by high performance in benign categorization with low performance in malware categorization. Moreover, the only identified malware samples were applications as-sociated with tracking user activity.\n2. API-Scoping-Based App Categorization. These prompts provide a list of the common APIs, libraries, and permissions used in Android malware. Unlike the Vanilla prompts, the model is now provided with context regarding the drivers of malicious functionalities to be aware of, aiding in the categorization assessment. Our analysis result showed that the API-scoped prompts ranked as the second-best in correctly categorizing the APK samples. Upon examining the confusion matrix as shown in Table 2, it's apparent that these prompts accurately identified 90% of the benign samples (TP) and 22% of the malware samples. Summing the percentages of correctly identified samples and dividing by the total number of samples (200) resulted in a cate-gorization accuracy of 56%. This indicates that the API prompts correctly identified 112 samples out of the 200, marking a higher result than Vanilla's 45% categorization accuracy. The API-Scoped prompts demonstrated superior classification performance compared to the Vanilla prompts. However, this improvement comes with significant trade-offs. Our manual analysis of the summaries revealed that the model displayed a bias towards obfuscation APIs like Java reflection APIs which alters the way in which functions are"}, {"title": "4.3. Discussion", "content": "The effectiveness of MalParse demonstrated in the eval-uation indicates its ability to accurately and effectively summarize and categorize malware. One of the significant capabilities of MalParse is its use of Chain-Of-Thought pro-cesses, which not only enhance the depth of analysis but also improve the traceability of decisions made by the model. This feature is particularly useful in the context of malware analysis and reverse engineering, where understanding the how and why of the malware, including the step-by-step reasoning of its design, is as important as the payload itself. However, MalParse is not without limitations, as shown below."}, {"title": "4.3.1. Limitations.", "content": "The quality and accuracy of the Mal-Parse 's summarization and categorization heavily depend on the underlying GPT-40-mini and the robustness of the prompt engineering, as was seen with the results in R1. The model's accuracy in categorizing malware and becoming less biased toward benign applications improves as the prompting is tuned to understand the general behavior of Android malware. Additionally, for stealthy and obfuscated malware samples, MalParse 's reliance on static summariza-tion will result in less comprehensive information especially for obfuscated malware. Code obfuscation techniques will make it difficult for static summarization approaches to capture and describe all aspects of a malware's functionality. Furthermore, the accuracy of the static decompilation relied upon the accuracy of Dex2Jar and CFR which in some cases doesn't decompile complex classes. It was evident that this resulted in some of the malware samples in our dataset being categorized as benign. Finally, although MalParse achieved a balanced accuracy at 77% with a precision of 76% and a recall rate of 77% all without prior training. This being said, the 24% and 22% False Positive and False Negative rates, respectively, can have serious implications in real analysis, either by overlooking actual threats or by causing unnecessary alarm and resource expenditure on benign entities."}, {"title": "4.3.2. Future Work.", "content": "To address these limitations and as part of our future work, first, we aim to improve the prompt engineering to refine how the model processes and responds to input, aiming to improve accuracy and bias. Second, we plan to build a locally-hosted model based on a smaller open-source LLM, which will increase processing speed and address rate limitations. Additionally, we intend to redesign MalParse to summarize bytecode directly instead of relying on decompiled Java code extracted via memory analysis. This shift will enable a more direct and granular analysis of APK files, potentially increasing MalParse accuracy. This new approach will be tested on large Android APK sample sets."}, {"title": "5. Conclusion", "content": "This paper introduces MalParse a system that leverages the power of LLMs for semantic analysis of Android ap-plications. MalParse is built on a hierarchical-tiered code summarization approach and advanced prompt engineering to enhance malware classification and the understanding of its structure and behavior. Our approach produces precise, contextually relevant summaries that emphasize security-critical aspects of the code. In its evaluations, MalParse clas-sified malware with 77% accuracy without prior training and generated summaries similar to those from automated en-gines. The backtracking process effectively traces package-level summaries to function-level summaries, providing a detailed description of the malicious behavior's root cause and an explanation of the model's decision."}]}