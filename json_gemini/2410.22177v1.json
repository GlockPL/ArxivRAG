{"title": "Analyzing Multimodal Interaction Strategies for LLM-Assisted Manipulation of 3D Scenes", "authors": ["Junlong Chen", "Jens Grubert", "Per Ola Kristensson"], "abstract": "As more applications of large language models (LLMs) for 3D content for immersive environments emerge, it is crucial to study user behaviour to identify interaction patterns and potential barriers to guide the future design of immersive content creation and editing systems which involve LLMs. In an empirical user study with 12 participants, we combine quantitative usage data with post-experience questionnaire feedback to reveal common interaction patterns and key barriers in LLM-assisted 3D scene editing systems. We identify opportunities for improving natural language interfaces in 3D design tools and propose design recommendations for future LLM-integrated 3D content creation systems. Through an empirical study, we demonstrate that LLM-assisted interactive systems can be used productively in immersive environments.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have gained popularity in assisting task completion in immersive environments. LLMs provide various advantages to improve interaction experience in virtual and augmented reality, such as improving task completion efficiency [30], democratizing VR content creation for non-expert users [12], and improving expressiveness while reducing the user\u2019s perceived workload [21]. However, the introduction of LLMs in interaction tasks such as scene editing can also pose barriers and adversely affect the interaction experience due to the current limitations of LLMs and its capability to integrate with 3D scene content. Examples of these barriers include transparency and explainability [23] reflected through user trust in the system, as well as appropriate error handling and timely user feedback [12].\nOur central hypothesis is that LLM-assisted 3D scene editing is best carried out through multimodal interaction. To begin investigating this hypothesis we have created the Advanced Speech Support and Interactive System for Virtual Reality (ASSISTVR), which integrates LLMs with multimodal interaction techniques. ASSISTVR uses an off-the-shelf Microsoft Azure Conversational Language Understanding (CLU) Service and GPT-4o to handle user queries. We use this system to study the effects of LLMs on user behaviour patterns in scene editing tasks through an empirical user study with 12 participants. Specifically, we focus on whether user interaction with such LLM-assisted interactive systems reveale certain high-level interaction strategies, and if so, was the LLM-assisted interactive system able to assist participants in identifying more efficient interaction strategies without external guidance. We also examine whether the system poses any interaction barriers, and suggest design approaches to overcome these. Through this study, we extract observations on user performance and interaction patterns, and provide design implications for future LLM-assisted interactive systems for immersive 3D content and possibly general interactive systems which involve LLMs.\nThis paper contributes to existing literature by analyzing interaction patterns and strategies through an exploratory study. We deliberately chose not to engage in a comparative study since prior work [8, 12, 21] have proposed systems which apply LLMs to immersive content, and the capabilities of LLMs advance in a very rapid speed. Instead of making a technical contribution, this paper provides insights on observed user strategies and behavioural patterns, which generalizes to different types of interactive systems involving LLMs."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Scene Editing and Multimodal Interaction in XR", "content": "Rakkolainen et al. [29] reviews recent advances in multimodal interaction technologies for XR content, pointing out how XR technologies introduce new interaction concepts, paradigms, and metaphors and play an important role in addressing accessibility barriers and inequality. Similar views were proposed by Spittle et al. [35], who suggests that multimodal interaction improves selection and manipulation tasks. In 3D editing tasks in virtual reality (VR), a combined gesture and speech interface can perform on par with a unimodal interface of a radial menu in terms of promoting creativity, usability, and presence [43].\nWilliams et al. [38] reports on an elicitation study of speech, gesture, and multimodal speech and gesture interactions in unconstrained object manipulation tasks in augmented reality. Zhou et al. [42] found that participants preferred to use the same gesture for one and two-object manipulation in the same task, and revealed associations between speech patterns and gesture strokes during 3D object manipulation. Rodriguez et al. [31] studied natural unimodal and multimodal interaction techniques for 3D sketching in virtual reality.\nPlopski et al. [27] reviewed gaze interaction and eye tracking research in XR and outlined how eye gaze has been applied to enhance user interaction with virtual content and interface design. Multimodal interactive systems such as GAZEPOINTAR [22] also demonstrate the possibility of leveraging eye gaze and pointing gestures to provide contextual information for speech queries."}, {"title": "2.2 Large Language Models for Extended Reality", "content": "A plethora of recent research in AI and XR has focusing on different aspects, including accessibility and inclusion [19, 5], privacy [5], 3D content generation [15], and general applications [12, 25, 8, 34].\nMa et al. [24] reviews integration of LLMs with 3D spatial data as 3D-LLMs and applications. Recent work [16, 17, 3] has further explored how LLMs can assist agents in altering the physical 3D world in various ways.\nIn terms of 3D content editing, LLM-assisted systems such as LLMR [8] demonstrate a wide range of possible applications in XR, including world creation, multimodal interaction, scene editing, scene query, and integration with other external plugins, platforms, and sensors. DREAMCODEVR [12] is an AI-powered tool for generating code in VR applications during runtime to modify the appearance and behaviour of elements in a 3D scene. Prior work has also studied LLM prompting for immersive content. Roberts et al. [30] show that prompt-based methods can accelerate in-VR level editing and become an integrated part of the gameplay. Aghel Manesh et al. [2] used a Wizard of Oz elicitation study to examine the implicit expectations of users when they prompt generative AI agents to create interactive virtual scenes."}, {"title": "2.3 Interaction Pattern Analysis", "content": "Interaction analysis is an important part of human-computer interaction (HCI) research. Wright et al. [39] proposed the resources model to analyze human-computer interaction as distributed cognition, where interaction strategies play a crucial role in bringing resources in use to generate actions. Scholz et al. [32] proposed a model to study user behaviour and interaction patterns in online news forums while Guo et al. [13] studied interaction modes and user agency in human-LLM collaboration tasks. Beyan et al. [4] conducted a human-human interaction analysis and identified interaction patterns and behaviours such as nonverbal cues which resulted in effective performance. These interaction patterns are often uncovered through log analysis [36] or audio and video analysis [18].\nInteraction patterns have also been studied within the context of extended reality. To support the analysis of interaction patterns, symbolic event visualization methods have been proposed by Rabasahl et al. [28]. Feit et al. [10] and Foy et al. [11] studied ten-finger typing on a physical keyboard and mid-air typing in virtual reality respectively, and summarized common typing behaviours as interaction patterns. Dudley et al. [9] studied the performance envelopes of four alternative text input strategies in virtual reality to provide design implications for novel text entry systems."}, {"title": "3 METHODOLOGY", "content": "LLM systems have evolved from text-based interaction [7] to vision-language models [37], which support multimodal text and images, to general-purpose multimodal LLMs [40] that support any combination of text, image, video, and audio as inputs and outputs. For immersive 3D environments, while multimodal interactive systems assisted by LLMs have been proposed [8, 20, 12], there is still need to investigate their effects on user behaviour and interaction patterns. We have designed ASSISTVR, an LLM-assisted multimodal interactive system for the purpose of studying typical interaction patterns and interaction barriers in an example task that involves editing an indoor scene to match a given target appearance using multimodal speech commands and raycast selections. The design of ASSISTVR fulfills high-level requirements of multimodal interaction and integration of LLM by incorporating speech and raycast pointing as different interaction modalities for the 3D editing task and follows a method similar to LLMR [8] to integrate LLMs including Azure CLU and GPT-40 with scene graph information and the post-processing pipeline in Unity to provide an integrated scene editing system.\nThrough a scene editing user study, we gather quantitative usage data and qualitative feedback from post-experience questionnaires. Collectively, these results help us to identify main interaction strategies as well as reoccurring interaction patterns. Through post-hoc analysis of the study data, we identify key barriers in user interaction with LLM-assisted interactive systems in virtual reality and propose design implications for future LLM-assisted interactive systems.\nApparatus. To study user behaviour and patterns when interacting with LLM-assisted 3D scene editing systems, we designed ASSISTVR. An outline of the system workflow is provided in Figure 2. The system leverages large language models such as Microsoft Azure Conversational Language Understanding (Azure CLU, as shown in grey) and GPT-4 Omni (GPT-4o, as shown in blue) [26, 1] to process user natural language input.\nIn the 'Training Phase' of Azure CLU, representative utterance data of possible user speech input samples are labelled with intents (such as 'Select', 'Deselect', 'Modify', 'Undo', or other intents) and key entities (such as 'Object of Interest', 'Original Color', 'Original Material', 'Target Color', and 'Target Material'), and are used to finetune the default model (2022-09-01 training configuration) provided by the Azure CLU service. Upon training the model, the utterances with labeled intents and entities are adjusted to iteratively improve model performance. The final model with an F1 score of 92.73% on intent classification was deployed. As the GPT-40 model is ready for deployment, there is no training phase for GPT-40.\nIn the 'Deployment Phase' of Azure CLU and GPT-40 in Figure 2, the system first uses the Azure Speech Recognition service to recognize user speech, then uses the Azure CLU model to classify the recognized speech input into different intents and extracts key entities from the user input. If the intent is classified as 'Select', 'Deselect', 'Modify', or 'Undo', the system executes post-processing scripts in Unity to perform object selection, object deselection, color/material modification, or actions to undo the previous color/material editing step. Following De La Torre et al. [8], the system generates a scene graph in JSON format to represent content in the 3D scene. If the intent does not fall under these four categories, the user natural language input, an instructions prompt (providing context about the scene editing task, available colors, and available materials), and a JSON file containing the scene graph of the current 3D scene are sent to the cloud-based GPT-40 model via Application Programming Interface (API) calls. GPT-40 subsequently generates a natural language response, which is then synthesized into speech and sent to the user."}, {"title": "4 RESULTS", "content": "Observations and quantitative data from the user study revealed several common patterns in user behaviour. These findings are organized and presented below as overall performance, interaction patterns and interaction barriers. Here, significance tests do not serve to conduct comparisons between different system or interfaces, but instead serve as a tool to indicate how well users can learn to use the system over time."}, {"title": "4.1 Overall Performance", "content": "Task Completion Quality. As the task involves scene editing, and different participants achieved different goal states which match the target scene appearance to different extents, we consider the difference between the color and material of all objects in the current scene and the color and material of all objects in the target scene as the number of Remaining Elemental Editing Steps (REES), a metric to quantify user progress and task completion quality in the scene editing task. This final REES metric measures how close the final state of the scene is compared to the target scene, with a lower final REES value indicating a closer match to the target scene and higher task completion quality. Friedman tests revealed a significant difference (\\( \\chi^2 \\) = 4.50, p < .05) in the final REES between TASK1 (M = 4.58, SD = 4.72) and TASK2 (M = 1.83, SD = 3.69), suggesting that participants were able to match the target scene significantly better in Task 2 compared with their performance in Task 1. Please note, that while it can be expected that participants' performance improves over time, the scale of this improvement (53.2% reduction in final REES on average) can indicate that users can adopt quickly to the multimodal editing system.\nTask Completion Time. Another measure for task completion is the time taken for each participant to edit the scene to match the target appearance. Friedman tests revealed a significant difference (\\( \\chi^2 \\) = 5.33, p < .05) between the completion time of Task 1 (M = 11.2 minutes, SD = 4.85) and Task 2 (M = 5.74 minutes, SD = 3.99), suggesting that participants completed Task 2 in a significantly shorter amount of time.\nCombining the results for task completion quality and task completion time, we observe that participants were able to match the scene significantly closer to the target scene in a significantly shorter amount of time in Task 2 after familiarizing with the system in Task 1 and making queries to the system to find the most efficient scene editing method. High standard deviations in the results also suggest that different individuals can have a vastly different performance."}, {"title": "4.2 Post-Experience Questionnaire Findings", "content": "Participants provided task load ratings on mental demand, physical demand, temporal demand, performance, effort, and frustration from a scale of 1 to 10 using the unweighted version of the NASA-TLX questionnaire [14]. A Wilcoxon signed rank test revealed that the overall task load rating of TASK1 (M = 3.97, SD = 1.08) was significantly higher (W = 4, p < .05, |r| = .8) than that of TASK2 (M=3.35, SD = .983).\nResults from the System Usability Scale (SUS) [6] of TASK1 and TASK2 yielded a higher average SUS score in Task 2 compared with Task 1. However, a Wilcoxon signed rank test did not reveal a significant difference (W = 9.5, p = .073, r = .568) between the SUS ratings of TASK1 (M = 72.1, SD = 15.5) and TASK2 (M = 75.2, SD = 14.9).\nResults from the short version User Experience Questionnaire (UEQ-S) [33] show that TASK1 attains a higher average pragmatic quality score and TASK2 attains a higher average hedonic quality score. Wilcoxon signed rank tests reveal a significant difference (W = 6, p < .05, |r| = .737) in the overall UEQ-S score between TASK1 (M = 1.50, SD = .590) and TASK2 (M = 1.22, SD = .640). For the subcategories of the UEQ-S ratings, significant differences were found in the PRAGMATIC quality (W = 6, p < .05, |r| = .738) between TASK1 (M = 1.50, SD = .798) and TASK2 (M = .833, SD = .587).\nFollowing the questionnaires on task load, system usability, and user experience, participants also provided descriptions of the most efficient strategy they found, as well as open comments about the system.\nTen out of 12 participants were able to find an efficient strategy of bulk-editing object properties by interacting with the system without external assistance by the end of the study. Participants who did not find the bulk modification strategy described their strategy as follows:\nThese two participants either found it more reassuring to change individual objects (P2) or did not find the supported command or workflow to select multiple objects with the same property first and then use another command to modify the appearance of all selected objects (P12). Further analysis on open comments about the system revealed the following trends, which provide further insights on the observed behaviours and interaction patterns:\nSystem ease of use. Participants appreciated how easy and efficient it was to complete the scene editing task once they knew how to phrase the commands and which strategy to adopt. P6 found the system to be \u201cincredibly quick\", and liked how efficient the system was. P6 commented, \"I did not need to select anything, which made it really easy. I just told the system what to do and only had to use 5 commands.\" P8 liked how the system supported multiple ways to complete the task. P10 commented that with speech commands, he could easily tell the system to select objects, then modify their colors and textures. P11 commented, \"It was efficient and straightforward to use once the commands were known.\"\nDialogues between participants and AssistVR during the study revealed that participants found the system useful in providing suggestions to help them find the list of all supported colors, materials, supported speech commands, as well as the efficient bulk modification strategy. For example, P12 asked, \"Can you make what I selected into another material,\" and the system responded, \"Yes, you can change the material of the selected objects. Simply use a speech command like 'Make it leather' or 'Make it cotton' to modify the material of the selected objects.\" P2 asked, \"What are the options for colours,\" and the system responded, \"You can change the color of an object to red, orange, yellow, ..., yellow pattern, purple pattern, or white pattern.\" P6 asked, \"What materials can I use,\" and the system responded, \"You can change the material of\""}, {"title": "4.3 Interaction Patterns", "content": "The study revealed how participants preferred to iteratively modify the color and material of individual objects in the scene to match the target appearance, or to select a group of objects with a common feature, and change their color/material using a single voice command. Figure 4 plots the number of remaining elemental editing steps for all 12 participants with respect to elapsed time.\nAs shown in Figure 4, in Task 1, in which participants are asked to find the most efficient way to edit the scene to match the target appearance, P2, P3, P7, P9, and P12 preferred to make incremental edits to individual objects. Similarly in Task 2, in which participants are asked to edit the scene based on the most efficient method they found, P2, P9 and P12 also preferred to modify the scene iteratively. We define this high-level scene editing strategy as:\nIncremental Exploration (IE) This strategy emphasizes visual inspection of individual object properties and combines raycast selection or speech selection of single objects by their names and modifying object appearance using speech commands, or direct modification (without explicit selection) of individual object appearance through speech commands.\nIn Task 1, P1, P6, P8, P10, P11 used speech commands to select a group of objects via their common color or material property and used a single voice command to bulk edit their appearance. This strategy is also observed in Task 2 within the behaviour of more participants including P1, P3, P4, P5, P6, P7, P8, P10, and P11. We define this high-level interaction strategy as:\nBulk Modification (BM) This strategy uses speech to select a group of objects with a shared color/material property, then uses speech to bulk modify their appearance. In this strategy, there is not explicit involvement of visual inspection of individual object properties.\nIt is important to note that the interaction strategy of a certain user can change over time. For example in Task 1, P4 started the task with incremental exploration, then adopted bulk modification, before returning to the incremental exploration strategy. Therefore, we visualize how interaction strategies have changed (if any) over the course of time in Task 1 and Task 2 for each participant in Figure 5. Based on these interaction patterns, we make the following observations:\nColor modification tends to precede material modification in IE and BM. In Task 1, among all 12 participants, 7 edited color before editing material (P1, P3-P6, P8, P10) while none edited material before editing color. The remaining 5 participants did not exhibit a strong preference on editing a certain property before another (P2, P7, P9, P11, P12). In Task 2, 8 participants edited color before editing material (P1, P3-P6, P8, P11, P12) while none edited material before editing color. The remaining 4 participants did not exhibit a strong preference on the editing sequence (P2, P7, P9, P10). This trend in editing sequence regardless of the high-level strategy employed demonstrates how the majority of participants draw attention to the more distinguishable visual features such as object colors and edit these features first in comparison with less distinguishable visual features such as object material.\nCarpet tends to be edited last. Prior to the study, participants were instructed to match the appearance of the carpet to the appearance shown in an image of the target scene. Participants were not explicitly told how to modify the carpet appearance, or how to reference the target appearance of the carpet. In comparison, the remaining objects were given an explicit target color (grey or purple). The carpet represents objects which are difficult to edit verbally, and the study results revealed that in Task 1, 9 out of 12 participants (P1, P4-P9, P11, P12) chose to edit the carpet last. In Task 2, 9 out of 12 participants (P1, P3-P8, P10, P12) edited the carpet after editing the remaining objects. The results show that in speech-based interfaces, users tend to edit objects with a clear goal state such that the speech commands are easy to enunciate."}, {"title": "4.4 Interaction Barriers", "content": "The study also revealed certain interaction barriers which adversely affected the completion quality of scene editing tasks or the completion efficiency of the tasks.\nSpeech Recognition/Processing Issues. The misrecognition and processing errors of certain words by the system required participants to repeat their queries multiple times, which ultimately resulted in a delay in task completion. Interaction barriers under this category can be due to an recognition error from the Microsoft Azure Speech Recognition service, or due to a processing error in the misrecognition of user intents or key entities by Azure CLU or an error occurred when matching key entities to the GameObjects or textures in Unity during the Unity post-processing step.\nFeedback Clarity. In the user study, participants were often confused when the system failed to respond to user speech input according to user expectations. In such circumstances, the system does not always provide clear feedback on why a command did not work or instruct participants on how to phrase it correctly. For"}, {"title": "5 DISCUSSION", "content": "This study highlights the promising potential of LLM-assisted interactive systems in guiding users towards more efficient multimodal interaction strategies, thereby improving user performance in typical interaction tasks such as scene editing in virtual reality.\nUser Performance. Performance indicators, such as the number of remaining elemental editing steps and task completion time reported in Section 4.1, reveal how user performance significantly improved in Task 2 compared with Task 1. First, as shown by performance indicators, the study exemplifies the impact of choosing the correct interaction strategy on task completion quality. While 431.3% more time was used on incremental exploration as compared to bulk modification in Task 1, bulk modification resulted in a 66.38% reduction in the remaining elemental scene editing steps compared with the incremental exploration strategy in Task 1. Second, the performance indicators reveal how LLM-assisted interactive systems help to guide users to select better interaction strategies which result in improved performance. In several cases for P12, P2, P6, and P5, the LLM-assisted scene editing system was able to give constructive feedback in response to user queries on the supported speech commands, available colors, available materials, and the most efficient way supported by the system for users to complete the scene editing task, with examples provided below. This demonstrates how LLM-assisted interactive systems have the strong potential of handling various types of natural language user input and providing a response to the best of its customized knowledge base to guide users to improve their interaction strategy.\nInteraction Patterns. Results from Section 4.3 also revealed certain interaction patterns. First, users tend to edit visually-distinguishable features, such as color properties, first before editing features with less distinct visual features, such as material properties. Second, when using the LLM-assisted interactive system for scene editing, participants preferred to edit objects with clear target states which could be edited through simple voice commands (i.e., 'Make this purple', 'Make them leather'), as opposed to objects with goal states that are difficult to enunciate and issue voice commands, such as the carpet. Third, queries were mainly posed during incremental exploration as opposed to during bulk modification. More queries did not necessarily guide participants to find the bulk modification strategy. Instead, participants (i.e., P3-P6, P8) who tried some form of bulk modification in Task 1 and who posed some queries to the system were more successful in finding the most efficient strategy combination (bulk modification for most objects followed by incremental exploration to modify carpet pattern) in Task 2. Finally, the study also revealed that participants tended to adhere to a single interaction strategy or a single pattern of interaction modalities. Even when Task 1 and Task 2 explicitly encourage participants to find the most efficient way of scene editing, P2 and P12 adhered to the incremental exploration strategy throughout the entire study and adhered to two interaction modality patterns: (1) modifying single objects directly through speech commands; or (2) using raycast to select one or several objects, then modifying the selected object(s) through a single speech command."}, {"title": "5.1 Implications for LLM-assisted Interactive system design", "content": "Results from this study shed light on design implications for future LLM-assisted interactive systems. While the study is conducted in a VR environment, design implications are applicable to 3D content applications in general, and even possibly applicable to general interactive systems where LLMs are involved. Based on results from the study, we formulate design implications as follows.\nFirst, effective use of multimodal input is critical for improving the user experience for LLM-assisted interactive systems. In the study, many participants stumbled upon providing clear descriptions on how to edit the carpet. This corroborates findings from Liao et al. [23] who state that interaction with LLM systems with only the natural language modality can be easily affected by subtle language and communication cues. To cope with this issue, Tsimpoukelli et al. [37] allowed language models to support multimodal image and text input in addition to original natural language prompts. Wu et al. proposed NEXT-GPT [40], a general-purpose any-to-any MM-LLM system which supports inputs and outputs of any combination of text, image, video, and audio, demonstrating the importance of LLM-assisted interactive systems to leverage different information modalities to specify user intent. In the study, we also found cases where participants would like to use a combination of raycast selection and speech interaction to make queries about the selected object. For example, P7 asked, 'Is this a leather object?' when using raycast to point at different objects. Many participants also used raycast to point at the object of interest when asking questions, which further emphasizes the importance of leveraging multimodal information in interaction design. Associating with the comment by P7 on including eye gaze under 'Multimodal Interaction' in Section 4.2, this demonstrates the promise of using additional input such as eye gaze to contextualize a query.\nSecond, the design of LLM-assisted interactive systems should place special considerations on fostering user trust and improving user agency. Processing steps in LLM-assisted systems can easily become a black box to users as it is not evident how the system handles natural language input from the user. It was observed that when some participants first began using the system, they preferred to ask some simple questions to the system to verify that the system is processing their request as they expected, which is an important step for users to build trust with the system.\nWhen asked about what made them to believe that the bulk modification strategy yielded correct results, some participants (P6, P8) confessed that they simply chose to trust the system but were not able to completely rule out the possibility that some editing steps might have gone wrong.\nIn the post-experience questionnaire, P2 and P12 used the incremental exploration strategy in both Task 1 and Task 2. P2 commented preferring this strategy over bulk modification because selecting individual objects provided more visual confirmation which consequently fostered trust with the system that it was making the correct editing step. P12, on the other hand, chose the incremental exploration strategy because it provided more user agency and sense of control over the system. P12 commented that the system was not able to respond to her commands to select all blue objects, which affected her sense of agency and control over the system through speech commands.\nWhile we used a draggable panel to assist users in verifying the correctness of the system output, we believe that there are many more possibilities to increase user trust and agency, for example, by overlaying or blending (future or already conducted) changes of 3D objects directly in the 3D scene and to allow for quick assertion and undo of previous actions.\nFinally, LLM-assisted interactive systems should implement measures to convey the fundamental uncertainties that emerge from LLM interaction, such as hallucination. In the study, there were some instances when hallucination occurred, which resulted in inaccurate or incomplete information provided by the system. In this regard, immersive environments could help to address the hallucination problem by providing additional contextual information to reduce uncertainty, which might help in minimizing the chances of hallucination compared to text-only input.\nP9 asked, \"Is there a bin in this room,\" and received the response \"Based on the provided context, there is no information indicating the presence of a bin in the room.\" In this case, there is a bin in the room but the system provided inaccurate information. P9 also asked, \"How many blue cotton items are there,\" and the system responded, \"There is one blue cotton item, which is the pen holder.\" In this example, there were several blue cotton items in the room, but the system provided incomplete information to list only the pen holder. While LLMs inherently have limitations in hallucinating information [41], it is important to signpost to the user when such information can be inaccurate or incomplete."}, {"title": "6 CONCLUSION AND FUTURE OUTLOOK", "content": "This work has provided an analysis of user interaction patterns and strategies with LLM-assisted interactive systems through an example scene editing task in virtual reality. As evidenced by the results in Section 4.1, LLM-assisted interactive systems have the potential to guide users to find more effective and efficient interaction strategies and improve task performance without external guidance. Results from the post-experience questionnaire corroborate findings in prior work [21, 12] on the strengths of LLM-assisted interactive systems for immersive content in perceived workload, usability, and user experience. Based on post-experience questionnaire comments, we summarize design considerations for LLM-assisted interactive systems in terms of multimodal interaction, user trust, user agency, and appropriate feedback to cope with uncertainty and hallucination. We also summarize interaction patterns such as the fact that visually distinguishable features tend to be edited first, and objects with an obscure goal state tend to be edited last. Interaction"}]}