{"title": "Preference-based opponent shaping in differentiable games", "authors": ["Xinyu Qiao", "Yudong Hu", "Congying Han", "Weiyan Wu", "Tiande Guo"], "abstract": "Strategy learning in game environments with multi-agent is a challenging problem. Since each agent's reward is determined by the joint strategy, a greedy learning strategy that aims to maximize its own reward may fall into a local optimum. Recent studies have proposed the opponent modeling and shaping methods for game environments. These methods enhance the efficiency of strategy learning by modeling the strategies and updating processes of other agents. However, these methods often rely on simple predictions of opponent strategy changes. Due to the lack of modeling behavioral preferences such as cooperation and competition, they are usually applicable only to predefined scenarios and lack generalization capabilities. In this paper, we propose a novel Preference-based Opponent Shaping (PBOS) method to enhance the strategy learning process by shaping agents' preferences towards cooperation. We introduce the preference parameter, which is incorporated into the agent's loss function, thus allowing the agent to directly consider the opponent's loss function when updating the strategy. We update the preference parameters concurrently with strategy learning to ensure that agents can adapt to any cooperative or competitive game environment. Through a series of experiments, we verify the performance of PBOS algorithm in a variety of differentiable games. The experimental results show that the PBOS algorithm can guide the agent to learn the appropriate preference parameters, so as to achieve better reward distribution in multiple game environments.\nKey words: Opponent Shaping, Preference, Game Theory, Differentiable Game", "sections": [{"title": "Introduction", "content": "Multi-agent reinforcement learning (MARL), as a theoretical framework for modeling agent behavior in complex game environments, has become a significant area of research [42, 37]. Unlike traditional game theory, MARL typically allows agents to learn strategies through repeated interactions to achieve equilibrium [34]. By relaxing the assumptions of agent rationality and independence, MARL can learn strategies efficiently with arbitrary environments and opponents [10, 20, 17].\nCurrent applications of MARL in game environments are primarily focused on zero-sum games (fully competitive) [10, 41] and fully cooperative games [12, 38], since the behavioral preferences of opponent agents in these environments are relatively easy to predict. Nevertheless, the environments in practical applications, e.g., economic markets, robotics and distributed control, may have multiple equilibrium [16, 40], and opponent agents may not exhibit clear preferences for different strategies, thus agents need to learn strategies in general-sum games [8, 7]. The Prisoner's dilemma [3, 14] is a classic example of the tension between mutual cooperation leading to a win-win situation and focusing solely on self-interest leading to a lose-lose situation. Therefore, modeling and shaping the behavior of opponent agents is the main challenge for the application of MARL in these environments [11].\nRecent advancements in MARL have introduced opponent modeling and shaping techniques that allow agents to learn not just their own strategies, but also to predict and influence the strategies of the opponent, such as [10, 20, 36]. These methods show promise in improving the efficiency of strategy learning by incorporating the behavior of other agents into the learning process. However, these methods presuppose a static notion of opponents'"}, {"title": "Related Work", "content": "The study of non-zero-sum games has a long history in game theory and evolutionary studies [41]. With the vigorous development of multi-agent reinforcement learning, research on non-zero-sum games has gained additional perspectives [37]. This paper focuses on a series of approaches to solve the opponent shaping problem [10, 20, 36, 11].\nThe core idea of opponent shaping is to maintain an explicit belief about the opponent and optimize decisions by establishing assumptions about the opponent's strategy [36]. This allows the system to reason about the behavior of the opponent and calculate the optimal response strategy, thereby facilitating more effective decision-making in uncertain and dynamic environments.\nThere are several types of methods: pre-defined opponent types [30, 33], policy reconstruction methods [23], and recursive reasoning methods [15, 1, 35]. In comparison, PBOS assumes white-box access to the opponent's learning algorithm, rewards, and gradients, placing it within the framework of differentiable games [5, 20, 36].\nLearning with Opponent Learning Awareness (LOLA) [10] modifies the learning objective by predicting and differentiating through opponent learning steps [20], which has been successful in experiments, especially in the Iterated Prisoner's Dilemma (IPD) [21]. Unfortunately, LOLA fails to guarantee the preservation of stable fixed points (SFPs) [20]. To improve upon LOLA, Stable Opponent Shaping (SOS) [20] applies ad-hoc corrections to the LOLA update, leading to theoretically guaranteed convergence to SFPs [20, 36]. Competitive Gradient Descent (CGD) [28] provides an algorithm for numerical computation of Nash equilibria in competitive two-player zero-sum games [28].\nThe agents using these methods [10, 20, 28] are rational and self-interested, focusing solely on optimizing their strategies to maximize personal gains without considering extending goodwill to promote cooperation with their opponents. This selfishness can cause agents to miss the opportunity for better rewards. For instance, in the Stag Hunt [25], agents utilizing the LOLA, SOS, and CGD algorithms tend to converge to the less favorable Nash equilibrium point (1,1) rather than the more advantageous equilibrium (4,4) (Fig. 17).\nNext, we introduce the concept of Differentiable games and outline the baseline methods used in our experiments."}, {"title": "LOLA", "content": "LOLA [10] addresses a differentiable game scenario with n = 2. A LOLA agent updates its parameter 01 under the assumption that its opponent behaves as a \"naive\" learner, updating its parameter 02 using gradient descent. Specifically, agent 1 formulates its modified loss as L\u2081 = L1(01,02 + \u2206\u03b82), where \u220602 = -a\u22072L2(01,02). Here, \u22072 denotes the gradient with respect to 02 and a represents the assumed learning rate of the \"naive\" opponent. The first-order Taylor expansion of L1 yields L1 \u2248 L1 + (\u22072L1)T\u2206\u03b82. Consequently, agent 1's first-order LOLA update is\n$\\Delta\\theta_1 = -\\beta(\\nabla_1 L_1 + (\\nabla_2 L_1)^T \\Delta \\theta_2 + (\\nabla_1 \\Delta \\theta_2) \\nabla_1 L_1)$,\nwhere \u1e9e denotes agent 1's specific learning rate.\nLOLA has demonstrated empirical success, notably achieving tit-for-tat in the Iterated Prisoner's Dilemma (IPD) [36]. However, it has shown limitations in maintaining Stable Fixed Points (SFPs) [20]."}, {"title": "SOS", "content": "SOS [20] represents a significant advancement over LOLA. According to [20], the simultaneous gradient of the game is defined as the concatenation of each players' gradient,\n$\\xi = (\\nabla L_1, ..., \\nabla L_n)^T \\in \\mathbb{R}^d$.\nThe Hessian of the game, denoted as H = \u2207\u00bf, forms a block matrix\n$H = \\begin{pmatrix}   \\nabla_{11} L_1  & ... &  \\nabla_{1n} L_1 \\\\   : &  & : \\\\   \\nabla_{n1} L_n &  & \\nabla_{nn} L_n  \\end{pmatrix} \\in \\mathbb{R}^{d \\times d}$\nNotably, H is typically asymmetric and can be viewed as described in [20]. Furthermore, H decomposes into H = Ha + Ho, where Ha comprises the diagonal blocks of H, and Ho represents the off-diagonal components. By defining x = diag(HTVL), the LOLA gradient can be computed as stated in [20]:\n$LOLA = (I \u2013 \u03b1\u0397\u03bf)\u03be \u2013 \u03b1\u03c7.$\nFrom [20], the resulting gradient of the SOS algorithm is expressed by:\n$\\xi_\\rho = (I - \\alpha H_o)\\xi - \\rho \\alpha \\chi$"}, {"content": "where a denotes the learning rate and I stands for the identity matrix. Here, p is a hyperparameter determined by Algorithm 1. Specifically, setting p = 0 results in \u00c9p reducing to LookAhead [39, 20], while p = 1, then p corresponds to LOLA. However, the maintenance of fixed points is contingent upon p approaching infinitesimal values [20]. To resolve this issue, SOS introduces a dual criterion for the probability p at each learning step, aiming to drive p towards zero. This approach not only combines the advantages of LookAhead and LOLA but also addresses LOLA's challenge in preserving Stable Fixed Points (SFPs). Empirical findings consistently demonstrate that SOS achieves or surpasses LOLA's performance, as evidenced in [20]."}, {"title": "CGD", "content": "CGD [28] is an algorithm designed for computing of Nash equilibria in competitive two-player games. It extends gradient descent principles to the two-player setting.\nThe update rule of CGD is defined as:\n$\\begin{pmatrix}   \\Delta \\theta_1 \\\\   \\Delta \\theta_2  \\end{pmatrix} = -\\beta \\begin{pmatrix}   \\nabla_{11} L_1  & 0 \\\\   0 & \\nabla_{22} L_2  \\end{pmatrix}^{-1} \\begin{pmatrix}   \\nabla_1 L_1 \\\\   \\nabla_2 L_2  \\end{pmatrix}$"}, {"title": "Preference-based Opponent Shaping (PBOS)", "content": "Intuitively, focusing solely on agent's own loss function in a game may lead to a non-stationary environment [20, 18] or suboptimal outcomes that are socially undesirable [10]. Therefore, it is crucial to model behavioral preferences of opponent agents. Our method, PBOS, integrates the opponent's loss function into the objective to model their preferences.\nIn this section, we provide a detailed description of the PBOS algorithm. To validate the effectiveness of incorporating preference parameters into the loss functions, we initially conduct experiments with fixed preference parameters. The original loss functions for agents 1 and 2 are denoted as L\u2081 and L2, respectively. We introduce preference parameters into the loss functions to facilitate agent training. The modified loss functions for agents 1 and 2 are then defined as L\u2081 = L1 + C1L2 and L2 = L2 + c2L1, where c\u2081 and c\u2082 are the introduced preference parameters.\nSubsequently, we apply the SOS strategy updating to train agents using these modified loss functions, forming the Constant-preference-based Opponent Shaping (CPBOS) approach, detailed in Algorithm 1."}, {"title": "Theoretical Results", "content": "In this section, we will give theoretical properties of PBOS.\nExample 1. In the Tandem Game, selecting an appropriate value for the preference parameter c is advantageous for both agents.\nProof. The original loss functions of the Tandem game are defined as follows:\n$L_1 = (x + y)^2 \u2013 2x$,\n$L_2 = (x + y)^2 \u2013 2\u0443$.\nApplying the SOS algorithm, we derive the conditions for convergence:\n$\u25bd_1L_1 = 2(x + y) \u2013 2 = 0$;\n$\u25bd_2L_2 = 2(x + y) \u2013 2 = 0$.\nThis leads to x + y = 1, corresponding to the NE of the game, where both agents minimize their loss jointly at x = y = 0.5, resulting in L\u2081 = L2 = 0 [20].\nIntroducing the preference parameter with C\u2081 = C2 = 1, the revised objective functions become:\n$L'_1 = 2(x + y)^2 \u2013 2(x + y)$,\n$L'_2 = 2(x + y)^2 \u2013 2(x + y)$.\nApplying the SOS algorithm to these recised objectives yields:\n$\u25bd_1L'_1 = 4(x + y) \u2013 2 = 0$;\n$\u25bd_2L'_2 = 4(x + y) \u2013 2 = 0$.\nIn this scenario, convergence occurs at x + y = 0.5. The solution minimizing losses for both agents is x = y = 0.25, resulting in L\u2081 = L2 = -0.25. While this outcome does not constitute a NE, it yields lower losses compared to the NE of the game."}, {"title": "Experiments", "content": "We conducted an empirical evaluation to assess the efficacy of the PBOS algorithm across a spectrum of six distinct differentiable games: Tandem Game, Iterated Prisoner's Dilemma (IPD), Matching Pennies, Ultimatum Game, Stackelberg Leader Game, and Stag Hunt. This comparative analysis juxtaposed PBOS against established baseline algorithms, including LOLA, SOS, and CGD, to evaluate its relative performance in these strategic contexts.\nFurthermore, we implemented PBOS alongside the aforementioned baseline algorithms to govern individual agents. Subsequently, these agents were engaged in a quartet of games characterized by symmetrical loss functions. This experimental setup aimed to scrutinize PBOS's effectiveness in fostering both cooperative and competitive outcomes."}, {"title": "Tandem Game", "content": "The Tandem game, introduced by [20], is a polynomial game defined on the continuous space R2. The loss functions for agents 1 and 2 are specified as L\u2081(x,y) = (x + y)\u00b2 \u2013 2x and L2(x,y) = (x + y)\u00b2 \u2013 2y, respectively. This game structure yields symmetric SFPs at x + y = 1, as identified in [20].\nIt has been noted in previous research [20, 36] that LOLA does not sustain these SFPs. Instead, LOLA converges to Pareto-dominated solutions, attributed to a phenomenon where both agents adopt an \"arrogant\" strategy."}, {"title": "IPD", "content": "The Iterated Prisoner's Dilemma (IPD) serves as a paradigmatic model for elucidating the emergence of cooperation within complex dynamical systems [22, 14]. In the IPD, the payoffs are typically subject to a discount factor \u03b3\u2208 [0, 1], reflecting the reduced value of future payoffs relative to immediate ones. In this study, we have selected a discounted factor y = 0.96 to capture the temporal dynamics of the game.\nThe traditional payoff matrix for the IPD is presented in Table 1, illustrating outcomes for each combination of strategies chosen by the two agents. In our formulation, each agent i is characterized by five parameters, including the probability Pi(Cstate) of cooperating given the initial state so = \u00d8 or any subsequent state st = (a\u0142-1,-1) for t > 0 [20]. These probabilities, contingent on the current game state, play a crucial role in shaping the strategic decisions of the agents throughout the IPD.\nThe game features two NEs. One is the always-defect strategy (DD), resulting in a loss of 2 for both agents. The other NE is the tit-for-tat (TFT) strategy, where agents initially cooperate and subsequently mirror the opponent's previous action [20]. TFT incurs a loss of 1 for each agent, recognized as a simple yet effective strategy [2]."}, {"title": "Matching Pennies", "content": "The Matching Pennies game, introduced in [19], is a quintessential example of a zero-sum game. The game's structure is characterized by a payoff matrix that is displayed in Table 2.\nIn this game, each agent's policy is encapsulated by a single paremeter, which is the probability of choosing the \"heads\" option [36]. It is a well-established result that the unique NE for this game involves each player choosing their strategy with equal probability, specifically, a probability of 0.5 for each [6]."}, {"title": "Ultimatum Game", "content": "The single-shot Ultimatum game, extensively studied in various literature [13, 27, 24, 29, 26, 36], exists in numerous variants. In this study, we focus on a version where two players are tasked with dividing ten dollars. Player 1 proposes a split to Player 2, who then decides to accept or reject the offer. Rejection leads to a null outcome for both players, whereas acceptance results in the proposed division of funds.\nThe principle of backward induction suggests that Player 2 will accept any positive offer, as it is preferable to receiving nothing [24]. Anticipating this, Player 1 may offer the minimal possible amount. However, this study considers two notable solutions: an equitable split of five dollars each or an inequitable division of eight dollars for Player 1 and two dollars for Player 2 [9].\nIn this context, Player 1's strategy is parameterized by the log-odds of proposing a fair split, denoted as Pfair = \u03c3(01). Similarly, Player 2's strategy is captured by the log-odds of accepting an inequitable split, given that equitable split is always accepted, represented as Paccept = \u03c3(02) [36]. The loss functions of the two players are definited as follows:\n$L_1 = -(5P_{fair} + 8(1 \u2013 P_{fair})P_{accept})$,\n$L_2 = -(5P_{fair} + 2(1 \u2013 P_{fair})P_{accept})$."}, {"title": "Stakelberg Leader Game", "content": "The Stackelberg Leader game, as discussed in [32], is characterized by a payoff matrix depicted in Table 3. This game is known for having a unique NE, which is the strategy profile (D, L). However, this equilibrium does not correspond to the most optimal outcome within the game's framework.\nTraditional analyses often assume heterogeneity among the players in the Stackelberg Leader game: one player acts as the leader, making the initial move, while the other serves as the follower, responding subsequently [17]. Under this assumption, a more advantageous outcome, such as (3, 2), can be achieved.\nIn this study, leveraging our algorithm, we demonstrate convergence towards the superior outcome of (3, 2) without relying on the assumption of player heterogeneity, as illustrated in Fig. 15. This discovery indicates that our algorithm effectively navigates the strategic complexities inherent in the Stackelberg Leader game, potentially uncovering cooperative or efficient solution paths that may not be readily apparent under traditional game-theoretic assumptions."}, {"title": "Stag Hunt", "content": "The Stag Hunt game, originally introduced in [25], serves as a classic illustration of cooperation challenges in game theory. It depicts a scenario where two hunters must decide between pursuing a large stag quietly or switching to hunt a hare that suddenly appears.\nThe payoff matrix for the Stag Hunt game is presented in Table 4. This game features two pure strategy NEs: the Stag NE, where both hunters cooperate to hunt the stag, and the Hare NE, where both opt to hunt the hare.\nThe Stag NE, characterized by mutual cooperation in pursuing the stag, yields a higher collective payoff of 4 and is often viewed as a \"risky\" strategy [31]. This risk stems from the possibility that if one hunter defects and hunts the hare instead, they can still secure a lower but positive payoff of 3, while the other, who continues pursuing the stag, incurs a significant loss of -10.\nIn contrast, the Hare NE represents a \"safe\" non-cooperative equilibrium where both hunters choose the less lucrative but certain payoff from hunting the hare, resulting in a payoff of 1 for each participant [31]. This equilibrium ensures a positive outcome for each hunter individually, irrespective of the other's decision, but at the expense of forgoing the higher collective payoff achievable through mutual cooperation."}, {"title": "Fixed c", "content": "We have developed an experimental framework to assess the efficacy of the CPBOS algorithm in conjunction with three established baseline algorithms. Our primary objective is to empirically validate the integration of preference parameters into the learning process. The experimental results are depicted in Fig. 1-6, presenting a visual comparison of algorithm performance.\nIn the context of the Tandem Game and IPD, our analysis centers exclusively on the loss function of the first agent. Conversely, when studying the Ultimatum Game and Matching Pennies game, we depict the loss functions for both participating agents. For the Stackelberg Leader Game and Stag Hunt Game, our representation involves plotting the average combined loss incurred by both players. From Fig. 1-6, we derive the following conclusions.\n(a). In the Tandem Game, IPD, Stackelberg Leader Game, and Stag Hunt Game, we set C1 = C2 = 1.00, reflecting a cooperative stance adopted by both agents towards their counterparts. This mutual goodwill leads to favorable outcomes for both parties. Specifically, in the Tandem Game, it results in reduced losses for both agents (Fig. 1). In the IPD, the CPBOS algorithm exhibits quicker convergence towards cooperative behavior compared to the three baseline algorithms (Fig. 2). In the Stackelberg Leader Game and Stag Hunt Game, agents utilizing CPBOS identify reward structures that are more advantageous than the traditional NE (Fig. 5 and 6).\n(b). In the Ultimatum Game, the parameters are set as c\u2081 = 1.00 and c2 = -1.00, indicating a cooperative stance by agent 1 and a confrontational stance by agent 2 towards their counterparts. This dynamic results in an equitable division of the ten-dollar pool between the two agents (Fig. 3).\n(c). In the Matching Pennies, the scenario is characterized by c\u2081 = C1 = \u22121.00, with both agents exhibiting hostility towards their opponents. This antagonistic interaction aligns with the zero-sum nature of the game, yet both agents achieve a mixed strategy NE (Fig. 4)."}, {"title": "Learning c", "content": "In this section, we apply opponent shaping to learn the preference parameters c. The outcomes are depicted in Fig. 7-18. This approach allows for adaptive tuning of preference parameters in response to the opponent's actions, thereby enhancing the sophistication and responsiveness of strategy selection. Fig. 7-18 visually illustrate the efficacy of this learning methodology within the examined game contexts.\nThe outcomes are summarized as follows:\n(a). Tandem Game: We observed that the preference parameters converge to c\u2081 \u2248 1.00 and c2 \u2248 1.00, resulting in a loss of approximately -0.25 for both agents, corresponding to the optimal outcome of the game (Fig. 7, 8).\n(b). IPD: Learned preference parameters are c\u2081 \u2248 0.60 and c\u2082 \u2248 0.60, with a loss of 1.00 for both agents, indicative of the best NE in IPD (Fig. 9, 10)."}, {"title": "Adversarial testing", "content": "In the context of four game environments with symmetrical loss functions, PBOS and three baseline algorithms have participated in gameplay. The comparative results of these interactions are depicted in Fig. 22-29.\nBased on Fig. 22-29, we derive the following insights regarding the performance of the PBOS algorithm compared to three baseline algorithms across four game environments with symmetrical loss functions:\n(a). Tandem Game: PBOS achieves NE when competing against agents using the SOS and CGD algorithms (Fig. 22). However, it falls short against LOLA due to the misalignment in learning a cooperative strategy c\u2081 > 0, which is exploited by the opponent (Fig. 23).\n(b). IPD: PBOS converges to the NE when competing against agents using LOLA or SOS algorithms (Fig. 24). Conversely, when confronting an agent using the CGD algorithm, PBOS adopts an antagonistic strategy (c1 < 0) and ultimately converges to a less favorable outcome (25).\n(c). Matching Pennes: In this zero-sum game, PBOS learns a confrontational strategy (c\u2081 < 0) and achieves a zero loss when competing against agents using LOLA, SOS, or CGD (26, 27). This result aligns with the nature of Matching Pennies, where the mixed strategy NE is equal probabilities of each strategy.\n(d). Stag Hunt: PBOS agents adopt a confrontational strategy (c\u2081 < 0) and converge to a loss of -1, representing the \"safe\" NE, when competing against agents employing LOLA, SOS, or CGD (28, 29).\nFrom these observations, we can infer that, in the majority of cases, the PBOS algorithm is capable of reaching NE when competing with baseline algorithms. However, there are specific scenarios where PBOS may underperform. The primary reason for this suboptimal performance is attributed to the foundational premise of PBOS, which relies on modeling the opponent's preference changes. If the opponent's preference remains constant (c2 = 0), the interaction between PBOS and the baseline algorithms may yield less satisfactory results. This underscores the importance of accurate modeling of opponent behavior in the efficacy of learning algorithms within game-theoretic contexts."}, {"title": "Conclusion", "content": "In this paper, we propose a novel preference-based adversary shaping (PBOS) method to improve the strategy learning process by using the opponent's objectives as preferences in the loss function. We introduce the preference parameter to avoid the limitation of agents considering only their own loss functions. By employing a method for shaping changes in opponent preference parameters, PBOS achieves higher reward in cooparative and competive game environments. Theoretical analysis shows that PBOS has good convergence properties and can obtain Nash equilibrium in games where other opponent shaping algorithms fail. We also conduct a series of experiments to demonstrate the effectiveness of PBOS. In many classic game environments, PBOS improves both the convergence and rewards of strategy learning. Furthermore, the PBOS algorithm exhibits strong generalization capabilities in randomly generated games and yields a 22% improvement over baseline algorithms with respect to proximity to the NE. Future research will focus on enhancing the PBOS algorithm to better navigate complex game environments and refine opponent modeling, as well as exploring methods to detect and adapt to the dynamics of opponent preferences. We hope that this work can promote the research of opponent modeling in game environments."}]}