{"title": "ArPA: A Novel Speech Analysis and Correction Tool for Arabic-Speaking Children", "authors": ["Lamia Berriche", "Maha Driss", "Areej Ahmed Almuntashri", "Asma Mufreh Lghabi", "Heba Saleh Almudhi", "Munerah Abdul-Aziz Almansour"], "abstract": "This paper introduces a new application named ArPA for Arabic kids who have trouble with pronunciation. Our application comprises two key components: the diagnostic module and the therapeutic module. The diagnostic process involves capturing the child's speech signal, preprocessing, and analyzing it using different machine learning classifiers like K-Nearest Neighbors (KNN), Support Vector Machine (SVM), and Decision Trees as well as deep neural network classifiers like ResNet18. The therapeutic module offers eye-catching gamified interfaces in which each correctly spoken letter earns a higher avatar level, providing positive reinforcement for the child's pronunciation improvement. Two datasets were used for experimental evaluation: one from a childcare centre and the other including Arabic alphabet pronunciation recordings. Our work uses a novel technique for speech recognition using Melspectrogram and MFCC images. The results show that the ResNet18 classifier on speech-to-image converted data effectively identifies mispronunciations in Arabic speech with an accuracy of 99.015% with Mel-Spectrogram images outperforming ResNet18 with MFCC images.", "sections": [{"title": "1 Introduction", "content": "Speech disorders severely impact the child's communication abilities and academic progress. The impact of speech disorders on children's social development and educational achievement is considerable [5]. Among these disorders are difficulties with pronunciation, rhythm, and voice quality. These disorders can have a variety of negative repercussions, such as reduced self-esteem, dissatisfaction, shame, and, in severe cases, withdrawal from social engagement [7]. It is important to be aware that errors in pronunciation are an inevitable part of language development in children, with research revealing that a substantial number of children have such difficulties during early speech acquisition [5].This work is supported by Prince Sultan University in Saudi Arabia"}, {"title": "2 Related Work", "content": "Pronunciation classification has made significant advances over the past few years [9]. Nowadays, automated systems for diagnosing pronunciation errors are crucial for assisting learners in enhancing their speaking abilities. The purpose of"}, {"title": "3 Proposed Approach", "content": "Our application aims to evaluate children's pronunciation of Arabic letters and then encourage improvement through an engaging game. ArPA is composed of two modules: the therapeutic module and the diagnostic module, see 1.\n1. Therapeutic Module: First, this module is responsible for capturing the kid's voice and delivering it to the diagnostic module. After the kid's pronunciation is evaluated by the diagnostic module, this module proposes a tailored and engaging game for kids. Interactive and enjoyable activities are used to practice incorrectly pronounced letters. The kid's progress is saved in a report which could be viewed by the parents and the therapist.\n2. Diagnostic Module: The diagnostic module starts with preprocessing the child's raw voice by denoising it and removing the silence. After that, Mel-spectrogram and Mel-Frequency Cepstral Coefficients (MFCC) features are extracted and converted to images. Finally, a trained ResNet18 classifier evaluates the correctness of the child's pronunciation."}, {"title": "3.1 Preprocessing", "content": "In this phase, the signal undergoes a denoising and a silence removal processes. The main challenge in noise reduction is mitigating external disturbances while preserving the integrity of the original speech signal. We applied a Gaussian filter which suppresses the high-frequency noise components leading to a smoother signal [6]."}, {"title": "3.2 Feature Extraction", "content": "In this work, we used Mel-Frequency Cepstral Coefficients (MFCC) and Mel-spectrogram features. Mel-spectrogram is a time-frequency representation that maps the power spectrum of an audio signal to the mel scale whereas MFCCS are obtained after taking the Discrete Cosine Transform (DCT) of the log mel-spectrogram. The cepstral coefficients are computed based on known human auditory perception. To extract MFCC features, we started by applying a first-order high-pass filter to increase the energy level of high frequencies. Then, we segmented the signal into L-sized overlapping frames with size L, where (20ms < L < 40ms). Afterwards, a time domain Hamming window is applied to minimize the discontinuities between frames as shown by Eq.1.\n\n$y(n) = s(n) \\cdot w(n)$ \nwith\n$w(n) = 0.54 - 0.46 \\cos(\\frac{2\\pi n}{N}), 0 \\leq n \\leq N$\nwhere Where N is the number of samples in each frame, s(n) is the input speech signal, y(n) is the windowed output signal and w(n)is the Hamming window. Next, we applied a Fast Fourier transform as represented by Eq.2 to each windowed frame.\n\n$S(k) = \\sum_{i=0}^{N-1} s(i) e^{i\\frac{2\\pi}{N} k i}$\n\n$S(k)$ is the $k^{th}$ Fourier transform coefficient and $s(i)$ is the $i^{th}$ speech signal sample. After that, the Mel filter bank is applied to the spectrum of each frame and the Log energy spectrum is computed representing the Mel-Spectrogram. To obtain the MFCC, the log Mel scale spectrum is converted to a time domain using DCT."}, {"title": "3.3 Image conversion", "content": "In this phase, we convert the MFCC and Mel-Spectrogram images by scaling the data values in the matrices to the range of a colormap. The scaling process starts by identifying the minimum and maximum values within the MFCC or Mel-Spectrogram matrix. The smallest value in the matrix is assigned to the first color in the colormap and the largest value is mapped to the last color. Intermediate values are linearly interpolated to assign corresponding colors between these two extremes. Converting MFCC and Mel-spectrogram features to images enables the use of pre-trained CNNs. In addition, this conversion allows the preservation of the time and frequency dimensions necessary for accurate audio analysis."}, {"title": "3.4 Classification", "content": "To assess how well classical machine learning models performed in detecting speech-related issues among children, we compared three classifiers: SVM, KNN, and Decision Trees. The speech signal of the child is first recorded as part of the diagnostic procedure. It is then put through a number of preprocessing stages to extract MFCC features which are subsequently supplied into the SVM, KNN, and Decision Tree classifiers. SVM, KNN, and Decision Trees are selected as baseline models for comparison. SVM, KNN, and Decision Trees have shown their effectiveness in speech recognition in [11] and [12] respectively.\nWe also used ResNet18 deep neural networks in this work. ResNet, Residual Neural Network, is an advanced neural network architecture developed to solve the issue of vanishing gradients in deep neural networks by implementing skip connections - called residual connections that bypass one or more layers. The residual block, in ResNet, consists of two convolutional layers with batch normalization and rectified linear unit (ReLU) activation functions, followed by a skip connection that adds the input to the output of the second convolutional layer. This skip connection preserves the original input information and helps propagate gradients more effectively during training. The inputs of the ResNet classifier, unlike SVM, KNN, and Decision Trees, are images obtained after the conversion of MFCC and Mel-Spectrogram matrices. To find out which model works best at diagnosing particular speech patterns or disorders, each classifier is assessed based on specific performance metrics."}, {"title": "4 Experiments and Results", "content": "All experiments were carried out using a Lenovo, i7 processor with 4 Cores and 8 Logical Processors, a RAM of 16GB. We used Java to develop the game provided in the therapeutic module and MATLAB for the speech analysis used in the diagnostic module."}, {"title": "4.1 Datasets", "content": "In this work, we used two datasets. We collected the first dataset from a childcare center from kids aged between 5 and 7 years old. We recorded 10 correct and 10 incorrect letters. We focused on three letters \"Raa\", \"Ghaa\", and \"Thaa\". These letters have unique phonetic characteristics that can emphasize different aspects of speech processing. We recorded the kids' voices and saved them as waveform (.wav) files. For data augmentation, we used a pitch variation factor to reduce the effect on the pronunciation. Afterwards, we augmented the dataset to 100 samples per letter. The second dataset is the Arabic dataset for alphabet pronunciation classification [13]. It is a balanced dataset that contains 8120 audio records for all Arabic letters providing a broader range of pronunciations and phonetic variations including both child and adult pronunciations. This allows us to capture a variety of pronunciation patterns, which can enhance the model's robustness. The spectrum of correct and incorrect pronunciations of the letter \"Raa\" for both adults and kids are represented in Figure 2."}, {"title": "4.3 Diagnostic Module Results", "content": "Classifications with classical machine learning: First, we used the MFCC features to compare SVM, KNN and Decision Tree classifier models. We used 10-fold cross-validation to divide the data into training and testing sets. In Table 1, we presented the results of applying the classical classifiers with for dataset 1 and dataset 2 after hyperparameters tuning using Bayesian optimization."}, {"title": "4.4 Therapeutic module", "content": "In this module, ArPA offers a game that starts with a registration interface for the user as a parent or therapist. After that, the kid's information such as age and gender are saved. After the registration phase, the child can use the game under the parent's or the therapist's control, see Figure 6. The game interacts with the kid via the voice entry only. In case, the kid's pronunciation of the letter is correct, a rabbit jumps to the next level otherwise it remains on the same level, see Figure 7. The history of the levels reached by the kid for each letter is saved in a report that could be viewed later. A report could be generated and used by the therapist to analyze the kid's amelioration."}, {"title": "5 Conclusion", "content": "This paper introduces ArPA, an innovative application developed for Arabic-speaking children with speech problems, focusing on their distinct speech characteristics rather than targeting adults or non-native speakers. ArPA includes diagnostic and therapeutic components that use sophisticated deep learning classifiers, such as ResNet18, to effectively identify mispronunciations, showcasing high precision, recall, and F1 scores in testing outcomes. However, we emphasize that the main issue encountered during the development of this paper was the limited dataset size. Subsequent research will focus on increasing the considered dataset with additional samples of children's speech, improving gamified therapy strategies for increased participation, incorporating more deep learning structures and transfer learning techniques such as audio deep learning, and carrying out long-term assessments to evaluate the effects of ArPA on children's language development and pronunciation abilities."}]}