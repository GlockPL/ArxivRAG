{"title": "ChartCitor: Answer Citations for ChartQA via Multi-Agent LLM Retrieval", "authors": ["Kanika Goswami", "Ryan Rossi", "Puneet Mathur", "Franck Dernoncourt"], "abstract": "Large Language Models (LLMs) can perform chart question-answering tasks but often generate unverified hallucinated responses. Existing answer attribution methods struggle to ground responses in source charts due to limited visual-semantic context, complex visual-text alignment requirements, and difficulties in bounding box prediction across complex layouts. We present ChartCitor, a multi-agent framework that provides fine-grained bounding box citations by identifying supporting evidence within chart images. The system or-chestrates LLM agents to perform chart-to-table extraction, answer reformulation, table augmentation, evidence retrieval through pre-filtering and re-ranking, and table-to-chart mapping. ChartCitor outperforms existing baselines across different chart types. Qualitative user studies show that ChartCitor helps increase user trust in Generative AI by providing enhanced explainability for LLM-assisted chart QA and enables professionals to be more productive.", "sections": [{"title": "1 Introduction", "content": "Chart data finds extensive use across diverse domains such as healthcare, finance, and education. Recently, LLMs such as Llama-3.2 [16], Claude-3.5 Sonnet, and GPT-4V [1] have proven effective in utilizing in-context learning and visual prompting to interpret and reason over chart images. However, these models tend to halluci-nate \u2013 generate answers with semantically plausible but factually incorrect information \u2013 which undermines their reliability and erodes user trust [14, 19]. While existing approaches attempt to address hallucination by grounding LLM-generated responses in source documents through citation mechanisms [7], charts present unique challenges: (i) complex mapping between visual elements and underlying data, (ii) limited contextual information due to compressed visual data representation, (iii) difficulty in localizing chart elements across diverse visualization types and layouts, and (iv) ambiguity in alignment between text descriptions and visual elements. Prior research has explored various approaches to address this challenge, including instruction tuning [8], in-context learning [5], and natural-language inference (NLI)-based post-hoc attribution methods [4]. However, these approaches have primarily focused on attributing entire charts rather than specific structural elements [6], limiting their practical utility. To address these limitations, we propose ChartCitor, a system that provides visual evidence for generated answers by identifying and highlighting relevant chart elements through bounding box annotations. ChartCitor works by orchestrating multiple specialized LLM agents to: (1) extract structured data table from charts, (2) break down answers into logi-cal steps, (3) generate contextual descriptions for rows/columns, (4) identify supporting evidence through pre-filtering and re-ranking to connect specific table cells to claims, and (5) localize the selected cells in the chart image. ChartCitor helps professionals save time on fact-checking LLM-generated answers and enhances user trust by providing reliable and logically-explained citations sourced from charts."}, {"title": "2 ChartCitor", "content": "We aim to solve the Fine-grained Structured Chart Attribution task which involves identifying graph elements (e.g bars, lines, pies in chart images) that support factual claims in a generated text response to a user's question. We propose ChartCitor (Fig. 1) - a multi-agent framework that provides fine-grained citations for generated answers grounded in chart image by orchestrating multiple LLM agents, which is explained as follows:\n(1) Chart2Table Extraction Agent: Charts are predominantly present in PDFs or scanned documents that need to be converted into struc-tured table formats (e.g., CSV, or HTML). We utilize GPT-4V to comprehend PDF images and output corresponding HTML without the need for external OCR using few shot prompting to identify cell data across each row/column. We use visual self-reflection [13] to provide the GPT-4V with its own rendered HTML and data table output to check for consistency between the re-plotted LLM output and the original chart. In case of inconsistencies in the data extraction, the LLM refines its output until the extracted table data is error-free.\n(2) Answer Reformulation Agent: The answer to be attributed, which can be AI-generated or otherwise, may be composed of mul-tiple facts, numerical formulations and multi-hop logic. Each fact may be sourced from a different row/column in the chart table. To facilitate precise citations, re-framing the answer statement into a chain of reasoning steps helps to better retrieve the correct citations from the table. We convert the answer statement into a hierarchy of reasoning thoughts/arguments via few-shot in-context prompting, ensuring the resultant answer arguments are independent sentences without any deviation in their collective meaning from the original answer statement.\n(3) Entity Captioning Agent: Understanding tabular data extends beyond simple cell interpretation, requiring comprehension of how cell information relate to both the table's structure and its broader context. Tables often present analytical challenges through ambigu-ous content, including technical terminology, contextless numeric values, domain-specific symbols, and hierarchical row/column head-ers. These ambiguities impede reliable evidence extraction and cita-tion validation through semantic matching. Our solution leverages LLMs in an unsupervised manner to generate rich, multi-layered contextual descriptions: (i) Row Captioning: Our system employs GPT-40 to generate comprehensive row-level descriptions that cap-ture complex patterns across features, summarize temporal trends, highlight significant dates and provide comparative analysis with respect to outliers within each row. (ii) Column Captioning: We generate detailed captions for each column using GPT-40 to explain ambiguous measurement units, symbols, empty cell spaces, and tech-nical relationship of it's contents with corresponding row headers. (iii) Cell Captioning: Row and column-level captions may highlight broader trends but the fine-grained cell level information needs to be contextualized in terms of its associated row and column headers. Captioning agent uses GPT-40 to describe the importance of each cell in the context of its associated row and columns.\nTable Cell Retrieval: We use retrieve-then-rank approach to identify the most relevant table cells. Our two-step approach begins with LLM-based pre-filtering to reject irrelevant rows and columns. We then employ LLM re-ranking to retrieve the most precise cell-level matches, ensuring both comprehensive coverage and accuracy in the final selection.\n(4) LLM Pre-filtering Agent: We hypothesize that some of the table rows/ columns are likely to be unrelated to the answer facts. Passing irrelevant and distracting table entities to the LLM-based re-ranker can mislead it, negatively impacting the ranking process. Inspired by [11], the LLM-based pre-filtering step uses chain-of-thought [18] followed by Plan and Solve [17] prompting techniques to generate a relevance score for each row/column based on the significance of its descriptive caption to the given answer statement (between 0 to 1). Additionally, we prompt the LLM to explain its rationale behind the score generation to enhance explainability and avoid hallucinations. We establish a specific threshold (usually 0.3 - 0.5) for row/column filtering to retain potential citations that are sent to the re-ranker, discarding those falling below the threshold. This implementation significantly reduces the number of noisy of rows and columns that can misguide the re-ranker, leading to an improved citation retrieval performance.\n(5) LLM Re-ranking Agent: LLMs providing citations that don't directly support their claims can be interpreted as a form of hal-lucination which may diminish user confidence. To solve this, we retrieve the set of table cells that are collectively both sufficient and directly relevant to the answer claims. We use RankGPT [15], a listwise LLM re-ranker to re-rank the table cells extracted from the intersection of rows and columns selected in the pre-filtering stage. Additionally, we prompt GPT-40 to provide a layer-of-thought [3] explanation of its rationale in ranking the cell items, enhancing the transparency in the citation mechanism by enforcing a logical coherence in the evidence chain.\n(6) Cell Localization Agent: The final step maps cited table cells to their corresponding visual elements in the chart image. This agent leverages DETR trained [2] on ChartQA data to identify all possible data marks (bars, line segments, pie slices) using image processing"}, {"title": "3 Implementation Details", "content": "All constituent agents utilize textual LLM APIs such as those pro-vided by OpenAI (GPT-40, GPT-4V) or Claude Sonnet-3.5. We use ChatGPT-40 as the base multimodal language model (MLLM) for ChartCitor. We convert data tables from TabCite benchmark [10] into bar/pie/line charts along with paired QA.\nEvaluation: We adopt visual Intersection over Union (IoU) as prin-cipal metric for chart attribution tasks. Detected regions in the chart image are matched to ground truth regions (e.g., bars in barplot or pies in piechart) based on a threshold value of IoU \u2265 0.9. Unlike bar charts and pie charts, where detected regions can be matched to discrete ground truth regions, line charts involve discrete points. Since grounding models generate bounding boxes or regions, we compute the proportion of ground truth points covered within the detected region(s) over total points detected.\nBaselines: (1) Zero-shot LLM Bounding Box Prompting \u2013 We prompt GPT-40 and Claude 3.5 Sonnet to predict normalized bound-ing box coordinates for chart components based on input text and the visual chart. (2) Kosmos-2 [12] is a multimodal LLM with text-to-visual grounding capabilities. It represents object locations as Markdown links for generating bounding boxes for visual grounding tasks. (3) LISA (Large Language Instructed Segmentation Assistant) [9] is a reasoning-based zero-shot segmentation model that generates masks from implicit and complex textual queries."}, {"title": "4 Results and Discussion", "content": "Table 2(a) shows quantitative results that demonstrate that ChartCitor consistently outperforms the baselines across all chart types, highlighting its robustness and effectiveness in visual chart understand-ing. ChartCitor achieves better performance compared to di-rectly prompting LLMs to predict bounding boxes. Further, even using GPT-4V with set of marks prompting over detected chart ele-ments show weak performance. Kosmos-2 and LISA perform poorly, with very low IoU scores, highlighting their inability to handle fac-tual grounding in charts due to insufficient visual and numerical reasoning. Interestingly, all tested method including our proposed ChartCitor, zero-shot LMM prompting, LISA and KOSMOS2 struggle with interpreting complex geometrical proportions in pie charts due to their difficulty in handling non-rectangular bounding box segmentation task. Further, we conducted a user study (Fig. 2(b) to evaluate the citation accuracy and perceived utility of fine-grained chart attribution provided by ChartCitor. Five participants eval-uated 250 randomly sampled question-answer pairs with associated chart images to study the usefulness and accuracy of the citations provided by ChartCitor compared to direct GPT-40 prompting. The evaluation results demonstrated strong positive reception, with participants rating the attributions as Completely Accurate (41% vs 28%) or Somewhat Accurate (17% vs 15%) for verifying chart-based question answering accuracy in ChartCitor and GPT-40, respectively. Attributions were found to be more \"Completely Inaccurate\" ChartC-itor than GPT-40 (17% vs 31%). Participants described the citations as a handy tool in making verification of LLM-generated answers easier (\"...can help me to quickly verify trends in charts, cutting down the time I spent on 10 documents from 5 hrs to 20 mins.\")."}, {"title": "5 Conclusion", "content": "We introduced ChartCitor, which grounds LLM-generated QA re-sponses to chart elements using agentic orchestration and set-of-marks prompting. The system outperforms baselines by 9-15% and shows promise for rich document QA over PDF collections. While currently effective for single-chart citations, future work can ad-dress multi-chart interactions, hallucination mitigation, and explicit citation-text mapping to enhance trustworthy multimodal content generation."}]}