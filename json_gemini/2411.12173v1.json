{"title": "SkillTree: Explainable Skill-Based Deep Reinforcement Learning\nfor Long-Horizon Control Tasks", "authors": ["Yongyan Wen", "Siyuan Li", "Rongchang Zuo", "Lei Yuan", "Hangyu Mao", "Peng Liu"], "abstract": "Deep reinforcement learning (DRL) has achieved remarkable success in various domains, yet its reliance on neural networks results in a lack of transparency, which limits its practical applications in safety-critical and human-agent interaction domains. Decision trees, known for their notable explainability, have emerged as a promising alternative to neural networks. However, decision trees often struggle in long-horizon continuous control tasks with high-dimensional observation space due to their limited expressiveness. To address this challenge, we propose SkillTree, a novel hierarchical framework that reduces the complex continuous action space of challenging control tasks into discrete skill space. By integrating the differentiable decision tree within the high-level policy, SkillTree generates diecrete skill embeddings that guide low-level policy execution. Furthermore, through distillation, we obtain a simplified decision tree model that improves performance while further reducing complexity. Experiment results validate SkillTree's effectiveness across various robotic manipulation tasks, providing clear skill-level insights into the decision-making process. The proposed approach not only achieves performance comparable to neural network based methods in complex long-horizon control tasks but also significantly enhances the transparency and explainability of the decision-making process.", "sections": [{"title": "Introduction", "content": "Deep Reinforcement learning (DRL) has been shown as a powerful framework for tackling complex decision-making tasks, achieving remarkable success in various domains such as games (Mnih et al. 2015; BAAI 2023), robotic manipulation (Akkaya et al. 2019; Jitosho et al. 2023), and visual navigation (Kulh\u00e1nek, Derner, and Babu\u0161ka 2021). Despite these advancements, the black-box nature of neural networks poses significant challenges in understanding and trusting their decision making processes. This lack of transparency is particularly concerning in safety-sensitive and human-agent interaction applications, where understanding the rationale behind decisions is essential (Hickling et al. 2023). For example, in the autonomous driving domain, only if human users can understand the driving policies can they trustfully use them in their cars.\nExplainable reinforcement learning (XRL) (Heuillet, Couthouis, and D\u00edaz-Rodr\u00edguez 2021; Hickling et al. 2023; Milani et al. 2024) aims to enhance the transparency and explainability of DRL models. XRL methods can be broadly categorized into inherently explainable models, where explainability is induced in the model training process, and post-hoc explanations, where models are explained after training. Existing works have shown that using a decision tree (DT) (Costa and Pedreira 2023) as the underlying model is effective (Frosst and Hinton 2017; Bastani, Pu, and Solar-Lezama 2018; Ding et al. 2020; Vasi\u0107 et al. 2022). In a DT, observations are input at the root node, and decisions are determined by conditional branches leading to the leaf nodes, which provide the final outputs. This simple structure gives DTs high explainability, offering clear and straightforward decision paths. Unlike post-hoc methods that rely on external algorithms to interpret black-box models, DT-based methods embed transparency directly within the model. The DT-based XRL paradigm not only enhances clarity but also simplifies debugging and validation, as the rationale behind each decision is explicitly encoded within the tree.\nDespite their advantages, DT-based methods are not suitable for the following challenging task settings. (a) Long-horizon tasks: The temporally-extended decision processes require large and complex trees, which are difficult to optimize (Liu et al. 2021). (b) High-dimensional state spaces: DTs often lack sufficient representational ability to effectively manage high-dimensional state spaces, leading to suboptimal performance in these environments (Bastani, Pu, and Solar-Lezama 2018; Ding et al. 2020). (c) Continuous action spaces: The limited number of leaf nodes constrains DTs' ability to encode continuous control policies optimally, particularly in complex robotic tasks. These limitations restrict the applicability of DT-based RL methods in complex environments, highlighting the need for approaches that can manage high-dimensional spaces and long-horizon tasks.\nTo address the limitations of traditional DTs in handling long-horizon, high-dimensional, and continuous control tasks, we propose a novel framework called SkillTree. SkillTree introduces the concept of skills, which represent"}, {"title": "Related Work", "content": "temporal abstractions of low-level actions in the context of RL. These skills simplify the decision-making process by breaking down long trajectories into manageable segments, enabling the agent to plan at a higher level, thereby reducing the complexity associated with extended tasks. However, the challenge of skill representation arises, as skills are often represented in continuous spaces, leading to difficulties in control and explainability. To overcome this, our framework leverages hierarchical structures and discrete skill representation learning. By regularizing the skill space into discrete units, we simplify policy learning for skill selection and enhance the explainability of the learned skills. Specifically, we employ decision trees to index and execute these discrete skills, combining the inherent explainability of decision trees with the flexibility of skill-based representations. This approach provides a robust solution for managing complex, long-horizon decision tasks while offering clear, skill-level insights into the decision-making process. We summarize the main contributions of this paper as follows:\n1. We propose SkillTree, a novel hierarchical, skill-based method for explainable reinforcement learning. To the best of our knowledge, it marks the first successful application of DT-based explainable method in long-horizon continuous control tasks.\n2. We introduce a method for discrete skill representation learning, which effectively reduces the skill space and improves the efficiency of skill-based policy learning.\n3. Experiment results across a variety of robotic manipulation tasks demonstrate that our method provides skill-level explanations while achieving performance comparable to neural network based approaches.\nRecently, explainability approaches in DRL have been broadly categorized into intrinsic and post-hoc explanations, based on the method of their generation. We briefly discuss these two classes of methods here. For a more detailed taxonomy and discussion of XRL, please refer to the following surveys: (Heuillet, Couthouis, and D\u00edaz-Rodr\u00edguez 2021; Hickling et al. 2023; Milani et al. 2024).\nIntrinsic explanation methods are designed with explainability as a fundamental aspect, often incorporating simpler, more transparent models such as decision trees (Liu et al. 2019; Silva et al. 2020; Ding et al. 2020), linear models (Rajeswaran et al. 2017; Molnar, Casalicchio, and Bischl 2020; Wabartha and Pineau 2023) or symbolic rules (Lyu et al. 2019; Ma et al. 2021). By ensuring that the model itself is explainable, intrinsic methods offer real-time, straightforward explanations of the agent's decisions. For instance, using a decision tree as the policy model enables immediate and clear understanding of the decision-making process.\nPost-hoc methods apply explainability techniques after the RL model has been trained. These approaches do not alter the original model but instead seek to explain the decisions of more complex, often black-box models. Typical post-hoc methods include feature attribution methods like saliency maps (Greydanus et al. 2018; Anderson"}, {"title": "Decision Tree", "content": "Decision trees are widely used for their explainability and simplicity, often serving as function approximators in reinforcement learning. Classical decision tree algorithms like CART (Loh 2011) and C4.5 (Quinlan 1993) produce explainable surrogate policies but are limited in expressiveness and impractical for integration into DRL models. Approaches such as VIPER (Bastani, Pu, and Solar-Lezama 2018) attempt to distill neural network policies into verifiable decision tree by imitation learning. Recent advancements include rule-based node divisions (Dhebar and Deb 2020) and parametric differentiable decision tree, such as soft decision tree (Frosst and Hinton 2017) and differentiable decision tree for approximating Q-function or policy (Silva et al. 2020). While these methods improve expressiveness, they are generally constrained to simpler, low-dimensional environments. By contrast, our approach addresses the complexity of high-dimensional continuous control tasks by converting the action space into a skill space, thereby reducing complexity and providing a more effective and explainable solution for challenging tasks."}, {"title": "Skills in Reinforcement Learning", "content": "Recent works have increasingly focused on learning and utilizing skills to improve agent efficiency and generalization (Shu, Xiong, and Socher 2018; Hausman et al. 2018; Shankar and Gupta 2020; Lynch et al. 2020). Skills can be extracted from the offline dataset (Shiarlis et al. 2018; Kipf et al. 2019; Pertsch, Lee, and Lim 2021; Pertsch et al. 2021; Shi, Lim, and Lee 2023) or manually defined (Lee, Yang, and Lim 2019; Dalal, Pathak, and Salakhutdinov 2021; BAAI 2023). Typically, these skills are represented as high-dimensional, continuous latent variables, which limits their explainability. In contrast, we propose extracting a discrete skill representation from a task-agnostic dataset, making it suitable for DT-based policy learning."}, {"title": "Preliminary", "content": "In this paper, we are concerned with a finite Markov decision process (Sutton and Barto 2018), which can be represented as a tuple $M = (S, A, R, P, p_0, \\gamma, H)$. Where $S$ is the state space, $A$ is the action space, $S \\times A \\rightarrow R$ is the reward space, $P: S \\times A \\leftrightarrow \\Delta(S)$ is the transfer function, $p_0 : \\Delta(S)$ is the initial state distribution, $\\gamma \\in (0, 1)$ is the discount factor, and $H$ is the episode length. The learning objective is to find the optimal policy $\\pi$ maximizing the expected discounted return\n$\\max_\\pi J(\\pi) = \\max_\\pi E_{s_0 \\sim p_0, (s_0, a_0, \\dots, s_H) \\sim \\pi} [\\sum_{t=0}^{H-1} R(s_t, a_t)]$\nThe differentiable decision tree (Frosst and Hinton 2017) is a special type of decision tree that differs from the traditional ones like CART (Loh 2011) by using probabilistic decision boundaries instead of deterministic boundaries. This modification increases the flexibility of model by allowing for smoother transitions at each decision node, as shown in Figure 1. The differentiable soft decision tree is a complete binary tree in which each inner node can be represented by a learnable weight $w$ and bias $\\phi$. Here, i and j represent the layer index and the node index of that layer, respectively. By using the sigmoid function $\\sigma(\\cdot)$, the probability $\\sigma(\\omega x + \\phi)$ represents the transfer probability from that node to the left subtree, and $1 - \\sigma(\\omega x + \\phi)$ gives the transfer probability to the right subtree. This process allows the decision tree to provide a \"soft\" decision at each node for the current input.\nConsider a decision tree with depth d. The nodes in the tree are denoted by $n_u$, where $u = 2^i - 1 + j$ denotes the node index, and the decision path $P$ can be represented as\n$P = \\underset{\\{u\\}}{\\arg \\max} \\prod_{i=0}^{d-1} \\prod_{j=0}^{2^i} P\\[i+1]\\[P\\[i]\\]\\rightarrow j]$\nwhere $\\{u\\}$ denotes nodes on the decision path, and $P\\[i+1]\\[P\\[i]\\]\\rightarrow j]$ denotes the probability of moving from node $n_{2^i+\\[j/2\\]}$ to node $n_{2^{i+1}+j}$, the probability is calculated as\nP\\[i+1]\\[P\\[i]\\]\\rightarrow j] =\\begin{cases}\n\\sigma(w_i^T x + \\phi_i) & \\text{if } j \\mod 2 = 0, \\\\\n1 - \\sigma(w_i^T x + \\phi_i) & \\text{otherwise}.\n\\end{cases}\nWith Equation 2 and 3, the tree can be traversed down to the leaf nodes. Leaf nodes are represented by parameter vectors $w^{2^d \\times K}$, where K is the number of output categories. The output of the leaf node k is a categorical distribution given by softmax($w_k$), which is independent of the input. The learnable parameters of both the leaf nodes and the inner nodes can be optimized using existing DRL algorithms, making the soft decision tree suitable as an explainable alternative model for DRL policy."}, {"title": "Explainable RL with SkillTree", "content": "Our goal is to leverage discrete skill representations to learn a skill decision tree, enabling skill-level explainability. Overall, our approach is divided into three stages: (1) extracting the discrete skill embeddings from the offline dataset, (2) training an explainable DT-based skill policy by RL for downstream long-horizon tasks, and (3) distilling the trained policy into a simplified decision tree."}, {"title": "Learning Discrete Skill Embeddings", "content": "Our goal is to obtain a fixed number of skill embeddings that contain a temporal abstraction of a sequence of actions. Skills represent action sequences of useful behaviors and are represented by D-dimensional vectors. We assume an existing task-agnostic dataset $D = \\{T_i\\}, T_i = \\{s_0, a_0, ..., s_{H_i}, a_{H_i} \\}$, consists of d trajectories of varying lengths, where each trajectory includes states $s_t,...,s_{H_i}$ and corresponding actions $a_t,...,a_{H_i}$. To regularize the skill embeddings, instead of learning within the continuous skill space, we employ a skill table $Z = \\{e_1, e_2,..., e_K\\}$ (i.e., codebook) that contains D-dimensional vectors, each representing a learnable skill embedding.\nTo learn reusable skills from offline datasets and guide exploration efficiently, we modified VQ-VAE (Van Den Oord, Vinyals et al. 2017) to learn skill representations and skill prior, as illustrated in Figure 2. First, a demonstration trajectory is randomly divided into state-action segments of fixed length h during the training. Each segment contains h observation states $s_{t,h} = s_t, s_{t+1},..., s_{t+h-1}$ and the corresponding actions $a_{t,h} = a_t, a_{t+1},..., a_{t+h-1}$. The input to the encoder $q_{\\phi}$ is $s_{t,h}$ and $a_{t,h}$. The output of the encoder is a D-dimensional embedding $z_e$. Next, we select the embedding from the codebook that is nearest to $z_e$ and its index $k = arg \\min_j || z_e - e_j ||_2$, obtaining the regularized embedding $z_q = e_k$. Finally, the low-level actual action $a_t$ is obtained through the state-conditioned decoder $\\pi_l(s_t, z_q)$, which serves as the low-level policy.\nIn addition, following the approach of (Pertsch, Lee, and Lim 2021), we introduce a skill prior to guide the high-level policy. The skill prior $p(k_t|s_t)$ predicts the skill distribution given the first state of the sequence in the offline data. This enhances exploration efficiency during high-level policy training by avoiding random skill sampling during warming up. Here, $s_t$ represents the first state of the sequence. The skill prior is a K-categorical distribution that matches the output of encoder. The objective of the skill"}, {"title": "Downstream RL Policy Learning with Skill Prior", "content": "To reuse the learning skill embeddings and improve exploration efficiency in RL training, we utilize the skill prior to regularize the high-level policy in skill space. After pre-training the low-level policy and skill prior, the low-level policy $\\pi_l(s_t, z_t)$ and the skill prior $p(k_t|s_t)$ are fixed. The high-level policy $\\pi_h(k_t|s_t)$ is executed every h-steps, and the output is the categorical distribution of the skill index $k_t$. To obtain the actual skill embedding, we sample an index $k_t$ from the distribution and then query the codebook with $k_t$, i.e., $z_t = Z[k_t]$. In each single time step, the low-level policy predicts the output action conditioned on the state and skill embedding. The low-level policy executes h-steps and the state changes to $s_{t+h}$. We use $P^+(s_{t+h}|s_t, z_t)$ to denote the state transition after h steps (see line 3-6 in Algorithm 1). Next, in order to improve the training efficiency by utilizing the prior information obtained from previous skill training, we add an additional prior term as the learning objective:\n$J(\\theta) = E_{\\pi_h} [\\sum_{t=0}^{\\Gamma H-1} \\gamma^t r^+(s_t, z_t) - \\alpha D_{KL}(\\pi_\\theta(k_t|s_t)||p(k_t|s_t))]$\nwhere $r^+(s_t, z_t) = \\sum_{i=t}^{t+h} R(s_i, z_i)$ is the total sum of rewards, $\\theta$ is the trainable parameter of policy, $\\alpha$ is the hyperparameter of divergence term. To prevent the policy from overfitting the prior, following (Pertsch, Lee, and Lim 2021), $\\alpha$ can be automatically tuned by defining a target divergence $\\delta$ (see Algorithm 1 line 15).\nTypically, the high-level policy is implemented as a neural network, which is a black box and lacks transparency. In order to achieve an explainable decision making process, we implement the high-level policy $\\pi_h(k_t|s_t)$ using a differentiable soft decision tree instead, which can be optimized using existing backpropagation algorithm. Furthermore, the structure of the skill prior is identical to that of the high-level policy, both of which are implemented using soft decision trees. To improve learning efficiency, the parameters of the skill prior are used to initialize the policy during the initialization of training. We modify SAC (Haarnoja et al. 2018) algorithm to learn the objective 6 and the process is summarized in Algorithm 1. See Appendix for more details."}, {"title": "Distilling the Decision Tree", "content": "After downstream RL training, we obtain a soft decision tree with skill-level explainability, where each decision node probabilistically selects child nodes until a skill choice is made. Generally, models with fewer parameters are easier to understand and accept. To further simplify the model structure, we distill the soft decision tree into a more straightforward hard decision tree through discretization. Previous methods select the feature with the highest weight at each node but it often obviously degrades tree performance (Ding et al. 2020). By framing the problem as an imitation learning task, we leverage the learned high-level policy as an expert policy. We sample trajectories from the environment to generate the dataset. Given the simplified skill space, we employ a low-depth CART to classify state-skill index pairs. This approach effectively manages the complexity of the tree while preserving performance, ensuring that the model remains both explainable and efficient."}, {"title": "Experiments", "content": "In this section, we evaluate SkillTree's effectiveness in achieving a balance between explainability and performance. We demonstrate that our method not only provides clear, skill-level explanations but also achieves performance on par with neural network based approaches.\nTo evaluate the performance of our method in long-horizon sparse reward tasks, we chose the Franka Kitchen control tasks in D4RL (Fu et al. 2020), robotic arm manipulation task in CALVIN (Mees et al. 2022) and office cleaning task (Pertsch et al. 2021), as illustrated in Figure 3.\nKitchen The Franka Kitchen environment is a robotic arm controlled environment based on the mujoco implementation. The environment contains a 7-DOF robotic arm that is able to interact with other objects in the kitchen, such as a kettle and other objects. A total of 7 subtasks are included. The observation is 60-dimensional and contains information about joint positions as well as task parameters, and the action space is 9-dimensional with episode length of 280. The offset dataset contains 601 trajectories, and each trajectory completes a variety of tasks that may not be in the same order, but no more than 3 subtasks. We set two sets of different subtasks: MKBL and MLSH, as shown in Figure 3(a) and 3(b). The MLSH task is more difficult to learn due to the very low frequency of subtask transitions in the dataset.\nCALVIN In CALVIN, a 7-DOF Franka Emika Panda robotic arm needs to be controlled to open a drawer, light a light bulb, move a slider to the left and light an LED in sequence. the observation space is 21-dimensional and contains both robotic arm states and object states with episode length of 360 steps. The dataset contains 1,239 trajectories and a total of 34 subtasks. Since it contains a large number of subtasks, the average frequency of transitions between any subtasks in the dataset is very low, requiring more precise skill selection, which poses a greater challenge during the learning of the high-level policy.\nOffice For the Office environment, a 5-DOF WidowX robotic arm needs to be controlled to pick up multiple objects and place them in their corresponding containers with episode length of 350. We utilized a dataset collected by (Pertsch et al. 2021), which includes 2,331 trajectories. The state space is 97-dimensional and the action space is 8-dimensional. Due to the freedom of object manipulation in this environment, the task is more challenging to complete.\nIn the experiments, we compare our proposed method against several representative baselines to demonstrate its effectiveness and competitive performance with additional skill-level explainability.\n* SPiRL (Pertsch, Lee, and Lim 2021): Learns continuous skills embeddings and a skill prior, and guides a high-level policy using the learned prior. It serves as a competitive skill-based RL algorithm but lacks explainability.\n* VQ-SPiRL: Discretizes the skills in SPiRL but uses neural networks as the high-level policy model structure.\n* Behavioral Cloning + Soft Actor-Critic (BC+SAC): Trains a supervised behavioral cloning policy from the demonstration dataset and finetunes it on the downstream task using Soft Actor-Critic (Haarnoja et al. 2018). It serves as a general algorithm without leveraging skills.\n* CART (Loh 2011): Imitation learning using CART with trained SPiRL agent as the teacher.\nFor SkillTree, we set the depth of the tree to 6 in all domains. The size of codebook K is 16 for Kitchen and Office, and 8 for CALVIN. For CART, we sample 1,000 trajectories on each domain to train and set the maximum depth to 10."}, {"title": "Results", "content": "From the learning curves shown in Figure 4, it is evident that our method demonstrates comparable learning efficiency and asymptotic performance in all domains. This highlights the effectiveness of our discrete skill representation in handling complex, long-horizon tasks. BC+SAC failed to complete any subtasks because of the sparse reward. In the CALVIN and Office domain, our method exhibits a marked improvement in early learning efficiency compared to SPiRL and performs comparably to VQ-SPiRL. This advantage underscores the benefit of discrete skills in enhancing exploration efficiency, allowing our approach to quickly identify and leverage useful skills for target task. Through comparing to VQ-SPiRL, we validate that the decision tree can maintain performance while reducing model complexity in simplified discrete skill space.\nIn Table 1, we evaluate the additional SkillTree after distilling on sampled dataset. Specifically, we use two sampling methods. One is to sample the trajectories directly with SkillTree (distilling, D), and the other is to clean the trajectories and keep only those with higher rewards (data cleaning + distilling, DC+D). In data cleaning, we select the trajectories completed subtasks greater than 2 for Kitchen and CALVIN, and greater than 1 for Office. In each environment, we sample 1000 trajectories. To control the complexity, we set the depth of the tree to 6 in all domains. We observe that after data cleaning and distillation, the performance of SkillTree can be further enhanced. In most of the domains, SkillTree DC+D achieves best performance than other baselines. This improvement reflects the effectiveness of our method in optimizing the skill representations, allowing it to adapt well even in complex scenarios. Moreover, the distillation process with data cleaning not only simplifies the model but also ensures that it retains critical decision making capabilities, leading to better performance across different tasks. In contrast, CART has worse performance despite having deeper layers and a much larger number of leaf nodes than SkillTree. Huge number of leaf nodes (~ 1000) suggests that it is difficult to understand intuitively."}, {"title": "Explainability", "content": "In Figure 5, we visualize the final decision tree obtained after distillation for the Kitchen MKBL task. This depth of decision tree is only three but can complete an average of 3.03 subtasks. The very few parameters make the decision-making process clear and easy to understand. Each node makes splits based on one of the observed features. In the 60-dimensional observations of the Kitchen domain, the last 30 dimensions are task parameters that are independent of the state and thus do not influence decision making. This property is reflected in the learned tree structure because the decision nodes use only the features in the first 30 dimensions (robot arm position and object position) for splitting.\nTo analyze the effects of each skill, we also evaluated the effectiveness of different skills, as shown in Figure 6. We fixed the skill output individually and evaluated the completion of subtasks. Each skill is capable of completing different subtasks with varying focus. SkillTree selects the appropriate skill based on the observed state, which are then executed by the low-level policy. Figure 7 illustrates the sequence of skill indices chosen in one episode, during which four sub-tasks were completed. It can be seen that the same skill was repeatedly selected at consecutive time steps to complete specific subtasks, indicating the reuse of particular skills. Moreover, repeated selection of the same skill indicates a relatively long-term and stable skill execution process. By isolating and testing each skill, we can clearly see which skills are most effective for particular subtasks. This makes it easier to understand the role and functionality of each skill within the decision making process. See Appendix for more experiment results as well as visualizations."}, {"title": "Conclusions and Limitations", "content": "We proposed a novel hierarchical skill-based explainable reinforcement learning framework designed to tackle the challenges of long-hoziron decision making in continuous control tasks. By integrating discrete skill representations and DT, our method offers a robust solution that enhances explainability without sacrificing performance. The proposed approach effectively regularizes the skill space, simplifies policy learning, and leverages the transparency of the DT. Experimental results demonstrate that our method not only achieves competitive performance compared to neural network based approaches but also provides valuable skill-level explanations. This work represents a important step towards more transparent and explainable RL, paving the way for their broader application in complex real-world scenarios where understanding the decision making process is essential. Despite the promising results demonstrated, several limitations remain. The low-level policy in SkillTree still lacks explainability due to its reliance on neural networks, which limits full transparency in the decision-making process. Additionally, the current approach assumes an offline, task-agnostic dataset for skill learning, which may not always be available in practical scenarios. Future work could focus on developing methods to enhance the explainability of low-level policies, possibly by integrating explainable models directly into the low-level decision-making process."}]}