{"title": "D-LORD for Motion Stylization", "authors": ["Meenakshi Gupta", "Mingyuan Lei", "Tat-Jen Cham", "Hwee Kuan Lee"], "abstract": "This paper introduces a novel framework named D-LORD (Double-Latent Optimization for Representation Dis-entanglement), which is designed for motion stylization (motion style transfer and motion retargeting). The primary objective of this framework is to separate the class and content information from a given motion sequence using a data-driven latent optimization approach. Here, class refers to person-specific style, such as a particular emotion or an individual's identity, while content relates to the style-agnostic aspect of an action, such as walking or jumping, as universally understood concepts. The key advantage of D-LORD is its ability to perform style transfer without needing paired motion data. Instead, it utilizes class and content labels during the latent optimization process. By disentangling the representation, the framework enables the transformation of one motion sequence's style to another's style using Adaptive Instance Normalization. The proposed D-LORD framework is designed with a focus on generalization, allowing it to handle different class and content labels for various applications. Additionally, it can generate diverse motion sequences when specific class and content labels are provided. The framework's efficacy is demonstrated through experimentation on three datasets: the CMU XIA dataset for motion style transfer, the MHAD dataset, and the RRIS Ability dataset for motion retargeting. Notably, this paper presents the first generalized framework for motion style transfer and motion retargeting, showcasing its potential contributions in this area.", "sections": [{"title": "I. INTRODUCTION", "content": "MOTION stylization is a key technique in animation, computer graphics, virtual reality, games, and robotics. The term \"motion stylization\" encompasses both motion style transfer and motion retargeting. Motion style transfer generates animated characters with various emotions from motion capture data. It transfers the style of one motion sequence to another while preserving the original motion content, as shown in Figure 1(b). Researchers have employed disentanglement frameworks to separate style from source motion, enabling animators to imbue different emotional expressions or artistic styles into animated characters while maintaining details of the motion [1], [2].\nThe concept of motion style transfer stems from image style transfer [3]. Previous work, such as Aberman et al. [4], had applied image-style techniques like disentanglement and Adaptive Instance Normalization (AdaIN) [5] to motion capture data for emotion transfer. However, they faced challenges in preserving motion content when transferring style between significantly different action types due to using 1D convolution in the content encoder [6]. Park et al. [2] addressed this by using a spatial-temporal graph convolution network, improving source motion retention. However, the root trajectory of the generated motion is still calculated directly from the source motion, resulting in an unnatural output when transferring a high-intensity style to a walking motion. They further advanced their work by eliminating the need for labeled motion data and transferring style features locally to different body parts [3]. This sometimes results in physically implausible whole-body motions due to the independent control of each body part's style. Concurrent work [7] has explored generative human motion stylization in the neural latent space, aiming to preserve content while allowing for diverse stylizations. However, it struggles to stylize content motion when the target styles are tied to specific content characteristics.\nIn summary, existing motion-style transfer algorithms suffer from some of the following drawbacks:\n\u2022 Adversarial training: Difficult to train [4], [2].\n\u2022 Root motion preservation: Preserving root motion may degrade style transfer quality if the target style is linked to specific content characteristics [4], [2], [8], [7], e.g., changing neutral motion to a proud style.\n\u2022 Spatial relations between joints: Using 1D convolution for content code generation fails to consider spatial relations between joints, leading to poor motion content preservation for significantly different action types [4], [7], e.g., neutral-kicking and proud-jumping.\n\u2022 Root trajectory calculation: Directly calculating the root trajectory from source motion weakens the stylizing effect [4], [2], [8], [7].\n\u2022 Deterministic output: A pair of input content and style motions yields a deterministic output [4], [8].\nThis paper addresses all these issues and proposes a latent optimization-based disentanglement framework called D-LORD (Double-Latent Optimization for Representation Dis-entanglement). D-LORD operates in two stages:\nStage 1: Using latent optimization, decompose motion capture data into three latent variables - class, content, and aleatoric uncertainty (AU) [9].\nStage 2: Train encoders for these latent variables using optimized latent codes, and train a separate variational autoen-coder (VAE) [10] to map aleatoric uncertainty to a Gaussian distribution space.\nD-LORD does not require paired motion data but uses labeled motion data to disentangle class (e.g., emotion, subject ID) and content (e.g., action). Paired data, if available, can further refine the network. It incorporates different class and content labels depending on the specific application. For motion style transfer, emotion is a class label, and action is a content label.\nAdditionally, D-LORD can be used for motion retargeting, applying a source motion to a target character with different kinematic properties [11], [12]. The skeletal differences between the source and target characters necessitate disentangling skeleton-independent features of the source motion and transferring them to the target character [13], [14]. D-LORD effectively performs motion retargeting by treating the class label as a person's identity and the content label as the action.\nDisentanglement is crucial for both motion style transfer and motion retargeting. Current algorithms typically focus on style-dependent features for motion style transfer or style-independent features for motion retargeting, lacking a comprehensive solution for both. In motion style transfer algorithms, the source motion is applied to the target after passing through the instance normalization layer, with the root trajectory of the target motion being directly calculated from the source motion. Conversely, motion retargeting algorithms directly use skeletal information, provided in the form of a T-pose. Thus, current motion style transfer algorithms cannot be used for motion retargeting, and vice versa. The D-LORD framework addresses this by disentangling both feature types, making it applicable to both tasks based on selected class and content labels.\nThe contributions of our work are:\n\u2022 Ease of training: Our framework uses latent optimization, simplifying the training process without requiring adversarial training.\n\u2022 Accurate motion style adaptation: By disentangling class and content features, our algorithm adapts style-specific motion characteristics to the content, even when the target style is linked to specific motion characteristics.\n\u2022 Style transfer between different actions: The content encoder is trained to generate optimized embeddings, allowing style transfer between significantly different action types despite using 1D convolution.\n\u2022 No strong motion content preservation: Our framework does not rely on calculating the root trajectory from the source motion, avoiding issues with strong motion content preservation.\n\u2022 Generalized disentanglement: It incorporates various class and content labels, applicable to both motion style transfer and motion retargeting.\n\u2022 Diverse motion data generation: It can generate diverse motion capture data by sampling AU latent from a Gaussian distribution, enhancing animation variety and richness.\nIn summary, D-LORD is a generalized motion stylization framework that is easy to train. It avoids strong motion content preservation, allowing for effective style transfer between different action types. Additionally, it generates diverse motion sequences. Through various experiments, we demonstrate the ability of our method to produce improved results in terms of visual quality, style transfer, and content preservation."}, {"title": "II. RELATED WORK", "content": "A. Image Style Transfer\nEarly methods utilized deep features from a pre-trained deep convolutional network by the Visual Geometry Group (VGG) [15] for image classification, extracting style features from shallower layers and content features from deeper layers. However, these methods were computationally inefficient due to iterative optimization. Johnson et al. [16] introduced perceptual loss functions and feed-forward generator networks for efficient, single-pass stylized image generation. Ulyanov et al. [17] proposed instance normalization (IN) to remove instance-specific contrast from content images and later extended it to AdaIN [5], which aligns the mean and variance of content features with style features.\nGenerative adversarial networks (GANs) [18] were used to align translated image distributions with real images in the target domain [3], [19]. Lee et al. [3] proposed disentangling content and domain-specific attributes for diverse outputs, while others decomposed images into domain-invariant content and style codes [19]. These methods were limited to two domains, but Choi et al. [20], [21] introduced Star Generative Adversarial Network (StarGAN) for multi-domain translations using a single model. Despite the advances in disentanglement, GANs are difficult to train and require careful tuning.\nTo address training challenges, Gabbay and Hoshen introduced latent optimization for representation disentanglement (LORD), a non-adversarial approach to image style transfer [22]. In their later work [23], they proposed a method to disentangle partially labeled factors and separate residual factors (unknown factors of variation). The aleatoric uncertainty in motion datasets can be defined similarly to residual factors in their paper. However, this approach requires replacing the generator with StyleGAN2 and adding an adversarial discriminator for real images. For more on image style transfer, see [6], [24], [25].\nB. Motion Style Transfer\nEarly motion style transfer methods [26], [27] used statistical properties of joint quaternions and local mixtures of autoregressive models to define motion style. These approaches required paired datasets and preprocessing, limiting their scalability and applications.\nInspired by image style transfer, research shifted to data-driven frameworks for motion style transfer. Holden et al. [28] extended Gatys et al.'s work [29] for human motions by training an autoencoder to match hidden unit activations and Gram matrices. They later replaced optimization with a feed-forward network [30], but content and style were not disentangled. The method in [31] used a conditional VAE to learn the style distribution and disentangle these features. Dong et al. [32] applied a cycle-consistent adversarial network (CCycleGAN) [33] to transform adult to child motions, though it struggled with non-cyclic motions. In [34], content and style features were combined using a pre-trained network, CCycleGAN, and kinematic constraints for improved motion style transfer."}, {"title": "III. D-LORD FRAMEWORK FOR MOTION STYLIZATION", "content": "This section presents our D-LORD framework for motion stylization, inspired by image-style transfer works [22], [5], [19], [44]. D-LORD builds upon LORD, a latent optimization framework proposed by Gabbay and Hoshen [22]. LORD is a class-supervised image disentanglement method that utilizes shared latent optimization, asymmetric regularization, and a second amortization stage for single-shot generalization. Although LORD, combined with bone length consistency loss, can perform motion style transfer and retargeting, it faces common motion style transfer issues, such as preserving root motion. When adapted for motion retargeting, LORD cannot handle advanced tasks like transferring personalized skeleton features (e.g., gait cycle) or generating diverse motion sequences for a given target skeleton.\nUnlike images, human motion data is more intricate due to temporal variations and a different latent manifold structure. Thus the architecture of LORD, designed for image-style transfer, must be adapted to handle aleatoric uncertainty [9] in human motion, even in repeated tasks by the same individual. To address this, we developed D-LORD, which disentangles motion capture data into three latent spaces. In addition to class labels used in LORD, D-LORD uses content labels for latent optimization and adds a skeleton consistency loss to encourage limb lengths to remain invariant in the synthesized motion sequence. To generate diverse motion sequences, the aleatoric uncertainty latent is mapped to a Gaussian distribution using a VAE in the second stage.\nThis section first outlines the motion representation used in this framework and then details the methodology.\nA. Motion Description and Motion Representation\nMotion description: A motion sequence can be described as consisting of three parts: class information, content information, and aleatoric uncertainty. Consider the case of the Berkeley Multimodal Human Action Database (MHAD), which contains 11 actions performed by 12 subjects, with each action repeated five times.\n\u2022 Class information. This represents the subject-dependent aspect of the motion and is indicated by the class label. In the MHAD dataset, it corresponds to the subject's identity, which remains the same across all frames and can be used to categorize or identify individual subjects.\n\u2022 Content information. This represents the subject-invariant aspect of the motion that is consistent across the population and is indicated by the content label. In the MHAD dataset, it refers to the type of action being performed.\n\u2022 Aleatoric uncertainty (AU). It captures the variations between repetitions of the same action by the same subject, accounting for unpredictable factors, such as variations in movement, speed, or style.\nMotion sequences can be effectively described using these three components: class, content, and aleatoric uncertainty. This representation is valuable for applications like action recognition, motion synthesis, motion stylization, and understanding human behavior.\nMotion representation: In our setting, we represent motion sequences using trajectories of body markers in a 3D space. Let J be the number of body markers used to represent a human skeleton in a given frame, with each body marker represented by 3D position coordinates (x, y, z). Thus, $j_k^t \\in \\mathbb{R}^3$ denotes the (x, y, z) coordinates of the kth marker at frame t of ith motion sequence. The tth frame of the ith motion sequence $m_i$ can be described as $m_i^t = (j_1^t, ..., j_J^t) \\in \\mathbb{R}^{3 \\times J}$. A motion sequence $M_i$ is an ordered sequence of T frames, represented as $M_i = (m_i^1,...,m_i^T) \\in \\mathbb{R}^{3 \\times J \\times T}$. To make all motion sequences comparable, they are down-sampled or up-sampled to have the same length of T frames. To ensure a common starting point for all motion sequences, first-pose root normalization is performed by subtracting the root position of the first frame from all markers in all frames. Orientation normalization is also performed to unify the direction in which the subject faces at the beginning of the motion. This is achieved by rotating the z-axis so that the subject faces the positive x-axis, with the pelvis parallel to the y-axis. Finally,"}, {"title": "B. Double-Latent Optimization for Representation Disentanglement (D-LORD) Methodology", "content": "D-LORD disentangles human motion sequences using a two-stage process, as shown in Figure 1. In Stage 1, the algorithm optimizes embeddings for class, content, and aleatoric uncertainty (AU) using latent optimization. The term \u201cdouble latent optimization\" refers to the optimization of both class and content embeddings. In Stage 2, encoders are trained to estimate the optimized embeddings obtained from Stage 1 for new input motion sequences. Given an input motion sequence, the encoders produce the corresponding class, content, and AU embeddings. Additionally, a separate VAE network is trained to map the aleatoric uncertainty latent to another latent space and align it with a prior Gaussian distribution using Kullback-Leibler (KL) divergence. During inference, the VAE allows sampling of the AU latent from the Gaussian distribution, enabling the generation of diverse human motion sequences, even for a single subject performing the same action.\n1) Stage 1 - latent optimization: Figure 2 illustrates the diagram for this stage. Our approach incorporates two labels (class and content) and employs three types of embeddings (class, content, and AU) for the motion sequences.\nConsider a dataset comprising n motion sequences, denoted as $M_1, M_2,..., M_n \\in \\mathbb{M}$. Let $N_c$ be the number of unique class labels (e.g., subject IDs) and $N_e$ the number of unique content labels (e.g., action types) for the given dataset. For each motion sequence $M_i$, we are provided with a class label $x_i \\in \\{1, ..., N_c\\}$ and a content label $y_i \\in \\{1, ..., N_e\\}$. We represent the class embedding of a given class label $x_i$ as $c_{x_i}$ and the content embedding of a given content label $y_i$ as $e_{y_i}$. There are $N_c$ class embeddings, each corresponding to a unique class label, and $N_e$ content embeddings, each corresponding to a unique content label. All the motion sequences with the same class label share a common class embedding, while all the motion sequences with the same content label share a common content embedding. The aleatoric uncertainty (AU) embedding, denoted as $a_i$, captures the variations that differentiate motion sequences with identical class and content labels. Each motion sequence has a unique AU embedding, resulting in a total of n AU embeddings for the dataset.\nWe assume that the motion sequences can be disentangled into representations in three latent spaces, X, Y, and A. The primary objective of this stage is to disentangle the class embedding $c_{x_i} \\in \\mathbb{X}$, the content embedding $e_{y_i} \\in \\mathbb{Y}$, and the AU embedding $a_i \\in \\mathbb{A}$ for a given motion sequence $M_i$. This disentanglement allows the generator $G$, which consists of a multi-layer perceptron (MLP) and a decoder network, to transform these disentangled embeddings into a motion sequence. Thus, the reconstruction loss between the generated motion sequence $M_i'$ and the actual motion sequence $M_i$ can be expressed as:\n$L = ||G_{\\theta}(c_{x_i}, a_i, e_{y_i}) \u2013 M_i||_2$ (1)\nwhere $\u03b8_G$ represents the parameters of the generator network. In this stage, we optimize the class and content embeddings directly using latent optimization. As the class embedding is shared exactly among all motion sequences with the same class label, it is impossible to include any content information in the class embedding. Similarly, as the content embedding is common across all motion sequences with the same content label, it is also impossible to include any class information in the content embedding. We learn the AU representation by optimizing over per-sample AU embeddings using latent optimization.\nIn the proposed network architecture, the class embedding is incorporated into the AdaIN layers of the generator using an MLP network. The AdaIN layer applies an affine transformation to the feature activations, modifying the mean and variance of the channels. Since this affine transformation is temporally invariant and only affects non-temporal attributes of the motion, the class label used for class embeddings should correspond to the time-invariant properties of the motion sequence. In contrast, the content embedding is provided to the input layer of the generator and is responsible for motion generation. Therefore, the content label is associated with the deterministic time-varying aspects of the motion sequence. The AU embedding captures random variations in the motion sequences and is given as input to the generator along with the content embedding. In the proposed network architecture, the AU embedding is designed to be independent of the class label, but it may still capture some content-related information if there are significant intra-content variations. To minimize the leakage of content information into the AU embedding, the AU embedding is regularized to minimize information. This regularization is achieved by initializing $a_i$ from Gaussian noise with a random mean $\\epsilon_i$ and a fixed variance $\\sigma^2$, i.e. $a_i \\sim \\mathcal{N}(\\epsilon_i, \\sigma^2I)$, and adding an L2 regularization term for the AU embedding in the objective function, as done in [22].\nTo ensure consistent bone lengths in the synthesized motion sequence $M_i'$ over time, a skeleton-consistency loss is introduced into the optimization function [45]. The skeleton-consistency loss aims to minimize the temporal variance of bone lengths and is calculated as follows:\n$L_s = \\frac{1}{T} \\sum_{t=1}^{T} (l_i^t - \\overline{l_i})^2$ (2)\nwhere $\\overline{l_i}$ is the vector of mean bone lengths of the generated motion sequence $M_i'$ and $l_i^t$ is the vector of bone lengths at frame t of the generated motion sequence $M_i'$. Thus, the final objective function of Stage 1 becomes:\n$\\mathbb{L}_1 = \\sum_{i=1}^{n} \\mathbb{L} + \\lambda||a_i||_2 + \\mathbb{L}_s$ (3)\nwhere \u03bb is a constant for L2 regularization. The first term in the overall Stage 1 objective function is the reconstruction loss $\\mathbb{L}$, which measures the discrepancy between the synthesized motion sequence $M_i'$ and the input motion sequence $M_i$. It encourages the generator to produce motion sequences that resemble the original input sequences. The second term is L2 regularization ($\\lambda||a_i||_2$), which is applied to the AU embedding $a_i$ to discourage the embedding from carrying excessive information by penalizing its magnitude. The third term is the skeleton-consistency loss ($\\mathbb{L}_s$), which ensures that the bone lengths in the synthesized motion sequence remain consistent across time. Once the model is trained in Stage 1, all motion sequences are disentangled into three optimized latent embeddings: class, content, and AU. Stage 1 is followed by Stage 2a, where we train the encoders to estimate the optimized embeddings learned in Stage 1 for new input motion sequences.\nAll latent codes and the parameters of the generator are learned end-to-end using stochastic gradient descent:\n$c^*, a^*, e^*, \\theta^* = arg\\,min_{\\mathbf{c}, \\mathbf{a}, \\mathbf{e}, \\theta} \\mathbb{L}_1$   $\\mathbf{c} = (c_1, ..., c_{N_c}), \\mathbf{a} = (a_1, ..., a_n), \\mathbf{e} = (e_1, ..., e_{N_e}) $ (4)\n2) Stage 2a - Training encoders for amortization inference: In Stage 1, we optimized the latent variables using Equation 4. Stage 1 can generate a motion sequence given the motion sequence labels (optimized embeddings corresponding to labels). However, given a new motion sequence in which the labels are unknown, the embeddings need to be estimated. Our objective for this framework is to disentangle the class and content information from a given motion sequence. Thus, in this stage, we train three encoders: Class, Content, and AU, to predict the optimized embeddings learned in Stage 1 for a given motion sequence. Figure 3(a) illustrates the architecture, where each encoder outputs its respective embedding. The objective function aims to minimize the error between the embeddings estimated by the encoders and the original embeddings learned in Stage 1. A reconstruction loss term is added to the objective function to ensure that the learned embeddings accurately reconstruct the original motion sequence. Similar to Stage 1, a skeleton consistency loss is included in the optimization function to maintain consistent bone lengths in the synthesized motion sequence over time. Thus, the overall loss function for"}, {"title": "3) Stage 2b - Mapping of aleatoric uncertainty latent space to normal distribution:", "content": "For a motion sequence with given class and content labels, its class and content embeddings remain fixed according to their labels. However, these embeddings do not account for the variations that may occur within sequences that share the same class and content embeddings. This is addressed with an AU latent embedding, somewhat analogous to having a slack variable. Additionally, we aim to synthesize new, diverse motion sequences even when the class and content labels are fixed. Since the distribution of the AU latent space is unknown, it must be mapped to a latent space with a known distribution from which we can sample multiple AU latents. To achieve this, we map the aleatoric uncertainty latent space to another latent space that follows a Gaussian distribution, using a VAE [10], as shown in Figure 3(b).\nThe encoder and decoder of the VAE are neural networks with parameters \u03b8 and \u03c6, respectively. The encoder outputs are the mean and variance of a conditional Gaussian distribution $q_\u03b8(z|a)$, from which we can sample to obtain $z \\sim q_\u03b8(z|a)$. The decoder takes z as input, and it produces the parameters of the distribution $p_\u03c6(a|z)$. The loss function of the VAE is the negative evidence lower bound (ELBO) given by\n$L_{VAE}(\\theta,\\phi) = \\sum_{i=1}^{n} - \\mathbb{E}_{z\\sim q_{\\theta}(z|a_i)}[logp_{\\phi}(a_i | z)] + KL(q_{\\theta}(z|a_i) || p(z))$ (7)\nThe first term is the reconstruction loss or the expected negative log-likelihood of the ith datapoint. The expectation is taken with respect to the encoder's distribution over the representations. This term encourages the decoder to learn to reconstruct the vector ai given its latent representation z. The second term can be interpreted as a regularizer and is the Kullback-Leibler divergence between the encoder's distribution $q_\u03b8(z|a_i)$ and p(z). Here p(z) is defined as $\\mathcal{N}$(0, I), so that the latent space distribution learned by the encoder is close to a standard normal distribution, which facilitates easy sampling of AU latents during synthesis.\nBy training the VAE on a dataset of AU latents, the encoder E and the decoder D parameters are optimized to learn a mapping from the given AU latent space to another latent space that follows a Gaussian distribution. During testing, a sample is taken from a Gaussian distribution $\\mathcal{N}$(0, I) and passed through the VAE decoder D to generate the AU latent.\n4) Motion stylization with diverse motion generation: After training the D-LORD model, motion stylization is performed using two motion sequences, as shown in Figure 1(b) and 4. The sequence whose style / identity is to be retained is provided to the Class Encoder (Ec), and the sequence representing the action to be performed is given to the Content Encoder (Ee). A sample from a Gaussian distribution $\\mathcal{N}$(0, I) is passed through the VAE decoder (D) to generate the AU latent, which captures aleatoric uncertainty and contributes to the diversity of the generated motions. The class latent (style) is then passed to an MLP, which maps the AdaIN parameters, adjusting the mean and variance of the decoder's AdaIN layers for style adaptation. Finally, the AU latent is combined with the output of the Content Encoder (Ee) and fed into the decoder (Ge) to generate the stylized motion sequence. This process is described as follows:\n$M_{ij} = G_{e_c}(E_c(M_i), D_\\phi(z \\sim \\mathcal{N}(0, I)), E_e(M_j))$ (8)"}, {"title": "IV. EXPERIMENTS AND EVALUATIONS", "content": "This section first details the implementation, the network architecture of the D-LORD framework, and the datasets used in the experiments. We then report on various experiments conducted to evaluate the performance of the D-LORD framework. Qualitative and quantitative results on these datasets are discussed and compared with state-of-the-art algorithms.\nA. Implementation Details\nOur D-LORD motion style transfer framework is implemented using PyTorch, and the experiments were conducted on a PC equipped with an NVIDIA GeForce RTX 3090 GPU (24 GB) and Intel Core i9-10900K/3.70GHz CPU (64GB RAM).\n1) Network Architecture: The proposed framework uses the MLP and decoder networks in both Stage 1 and Stage 2, while the encoder networks are only used in Stage 2. The MLP network, comprising fully connected layers with Leaky ReLU activation, takes class embeddings as input to generate AdaIN parameters for the decoder's AdaIN layers. The decoder network features fully connected layers, residual blocks (see Figure 5), upsampling layers, and 1D convolution. The output of the MLP network sets the parameters of the decoder's AdaIn layers. Then the decoder network concatenates the content embedding and AU embedding and processes them to reconstruct the input motion sequence. The residual blocks enhance motion quality while upsampling and 1D convolutions aid sequence generation. The encoder networks consist of six 1D convolution layers followed by three fully connected layers with Leaky ReLU activation, extracting local features and transforming them into embeddings. A VAE network maps the aleatoric uncertainty latents to a Gaussian distribution, enabling sampling for diverse motion generation.\nPlease refer to the Appendix for detailed information on the network architecture and hyperparameter tuning. An open-source implementation of the framework in PyTorch will be released soon.\n2) Datasets: In this paper, we conducted experiments on three datasets: CMU Xia, Berkeley MHAD, and RRIS. For the Berkeley MHAD [46] and RRIS [47] datasets, the class label is the subject ID, while for the CMU Xia dataset [27], it is the emotion label (e.g., angry). In all cases, the action label serves as the content label.\nCMU Xia dataset [27]: It is publicly available and contains 572 motion sequences of 28 actions performed in 8 different emotions by a single actor. Each sequence has 3D positions of 21 joints and is resampled to 256 frames for standardization. Transitional actions are excluded, and some actions are grouped under common labels. The dataset is split into training (80%) and test (20%) sets.\nBerkeley Multimodal Human Action Database [46]: It consists of 659 motion sequences from 12 subjects, with each subject performing 11 actions, repeated 5 times. The sequences capture 3D positions of 33 joints and are resampled to 256 frames for standardization. The training set includes all 11 actions and repetitions from 10 subjects, while the test set contains data from the remaining 2 subjects.\nRRIS Ability Data [47]: This dataset, obtained from the authors of [47], contains 3,998 motion sequences performed by 200 individuals. It includes four lower limb tasks, automatically segmented, with each sequence resampled to 256 frames and 3D positions of 64 markers. The dataset is split into training, test, and validation sets: the training set includes sequences from 150 individuals, the test set contains sequences from 30 individuals, and the validation set includes sequences from 20 individuals.\nB. Latent Space Disentanglement Evaluation\nWe evaluated D-LORD's ability to disentangle class and content information based on two conditions:\n\u2022 Class and content codes should only contain information related to their respective labels.\n\u2022 No content information should leak into the class code and vice versa.\nTo verify the first condition, we projected the class and content codes into a 2D space using t-SNE (t-distributed stochastic neighbor embedding) [48] and plotted them according to their labels 6. The t-SNE visualization showed well-clustered class and content codes, indicating successful disentanglement of the labels by the network.\nTo further validate this, we calculated the centroids of the class codes for each label and computed the average distance between these centroids and the corresponding latent codes. As shown in Table I and Table III, the average distance between the latents and their respective class centroids was significantly smaller than the distance to other class centroids. This confirms that the network effectively captured class-specific information. The same approach was applied to the content latents, with results in Table II and Table IV, further confirming the network's ability to distinguish between content To test the second condition, we evaluated the potential leakage of content information into the class codes. We trained a Linear Discriminant Analysis (LDA) classifier using the class codes as inputs and the content labels as outputs. If content information were present in the class codes, the LDA projection in a 2D space, plotted with respect to content labels (figure 7), would show distinct clustering. However, no such clustering was observed, confirming that the class codes do not contain content information. Similarly, we applied the same test to the content codes, and the absence of clustering based on class labels further verified that content codes remain independent of class information\nOverall, the evaluation showed that D-LORD effectively disentangles class and content information from motion sequences. Additional results on the RRIS dataset, provided in the supplementary material, further confirm these findings."}, {"title": "C. Visual results", "content": "Figure 8 shows the results of motion stylization experiments conducted on three datasets. In all cases, the AU latent was sampled from a Gaussian distribution.\nIn the first experiment, emotion transfer (motion style transfer) was applied to the CMU Xia dataset. The class encoder received a Neutral Jump motion sequence, while the content encoder received a Proud Walk motion sequence. The class encoder extracted the neutral style, and the content encoder extracted the walking motion. The generator then produced a Neutral Walk motion sequence, successfully changing the style from proud to neutral (see first row in Figure 8). The proud walk, characterized by a bent spine and raised shoulders, was transformed into a neutral walk with a straight spine and unraised shoulders.\nThe second experiment focused on identity transfer (motion retargeting) using the MHAD dataset. The class encoder received a Jump-in-place motion sequence performed by Identity S3, while the content encoder received a Bending motion sequence performed by Identity S9. The class encoder extracted the identity S3 while the content encoder extracted the bending motion. The generator then produced a Bending sequence performed by Identity S3. As shown in the second row of Figure 8, the generator successfully transferred the bending motion from S9 to S3, which is taller than S9.\nIn the third experiment, identity transfer was applied to the RRIS dataset. The class encoder received a Step-up motion sequence from Identity S11, and the content encoder received a Walk motion sequence from Identity S101. The class encoder extracted the identity S11 while the content encoder extracted the walking motion. The generator then produced a Walk sequence performed by Identity S11. As shown in the third row of Figure 8, the generator transferred both the identity and gait length of S11 to the walking sequence, changing the"}, {"title": "D. Qualitative and Quantitative Comparison", "content": "1) Motion style transfer: We conducted a qualitative and quantitative comparison of our D-LORD framework with two state-of-the-art methods: Aberman et al. [4", "2": "using the CMU Xia dataset. The evaluation focused on three main aspects: generation quality", "4": "and Park et al. [2"}, {"2": "failed to transfer the neutral style effectively. In the second experiment", "metrics": "Cosine Similarity of Deep Features (CSDF) [49", "4": "and Park et al. [2", "Retargeting": "Figure"}]}