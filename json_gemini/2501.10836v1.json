{"title": "BAP v2: An Enhanced Task Framework for Instruction Following in Minecraft Dialogues", "authors": ["Prashant Jayannavar", "Liliang Ren", "Marisa Hudspeth", "Charlotte Lambert", "Ariel Cordes", "Elizabeth Kaplan", "Anjali Narayan-Chen", "Julia Hockenmaier"], "abstract": "Interactive agents capable of understanding and executing instructions in the physical world have long been a central goal in Al research. The Minecraft Collaborative Building Task (MCBT) provides one such setting to work towards this goal (Narayan-Chen, Jayannavar, and Hockenmaier 2019). It is a two-player game in which an Architect (A) instructs a Builder (B) to construct a target structure in a simulated Blocks World Environment. We focus on the challenging Builder Action Prediction (BAP) subtask of predicting correct action sequences in a given multimodal game context with limited training data (Jayannavar, Narayan-Chen, and Hockenmaier 2020). We take a closer look at evaluation and data for the BAP task, discovering key challenges and making significant improvements on both fronts to propose BAP v2, an upgraded version of the task. This will allow future work to make more efficient and meaningful progress on it. It comprises of: (1) an enhanced evaluation benchmark that includes a cleaner test set and fairer, more insightful metrics, and (2) additional synthetic training data generated from novel Minecraft dialogue and target structure simulators emulating the MCBT. We show that the synthetic data can be used to train more performant and robust neural models even with relatively simple training methods. Looking ahead, such data could also be crucial for training more sophisticated, data-hungry deep transformer models and training/fine-tuning increasingly large LLMs. Although modeling is not the primary focus of this work, we also illustrate the impact of our data and training methodologies on a simple LLM- and transformer- based model, thus validating the robustness of our approach, and setting the stage for more advanced architectures and LLMs going forward.", "sections": [{"title": "1. Introduction", "content": "There has long been interest in developing interactive agents that can communicate with humans about and operate within the physical world (e.g., Winograd (1971)). The goal for these agents is not only to engage in rich natural language dialogue, but also to ground it to physical objects, and execute instructions in the real world. We work toward this goal using Minecraft (https://minecraft.net/), a popular multiplayer game where players control avatars to navigate a 3D world and build structures from block-like materials. Minecraft, in recent years, has garnered a lot of interest as an AI experimentation platform. Our previous work\u2014the Minecraft Collaborative Building Task (MCBT) and the corresponding Minecraft Dialogue Corpus (MDC) (Narayan- Chen, Jayannavar, and Hockenmaier 2019)\u2014was among the first to use it to study grounded task-oriented dialogue. Jernite et al. (2019) is another such example. More recent works, such as Mohanty et al. (2024), have been directly inspired by the MCBT. Additionally, there now exists a well-developed set of platform and tools that support Al experimentation within Minecraft, including Malmo (Johnson et al. 2016), Craftassist (Gray et al. 2019), TaskWorldMod (Ogawa et al. 2020), MC-Saar-Instruct (K\u00f6hn et al. 2020), IGLU GridWorld (Zholus et al. 2022), and Narayan-Chen, Jayannavar, and Hock- enmaier (2019). The latter builds on the Malmo platform.\nThe MCBT situates a dialogue task-designed to be asynchronous, asymmetric, and with minimal language constraints\u2014in a simulated 3D Minecraft environment where participants, with constantly shifting perspectives, must refer to and modify a dynamic world. In this task, two players collaborate: an Architect (A) instructs a Builder (B) to construct a target structure using multi-colored blocks (Figure 1). The MDC comprises 509 human-human game logs for this task. Narayan-Chen, Jayannavar, and Hocken-"}, {"title": "", "content": "maier (2019) focus on generating Architect utterances, and Jayannavar, Narayan-Chen, and Hockenmaier (2020) focus on end-to-end neural models for building an automated Builder agent. The latter defined the challenging Builder Action Prediction (BAP) task, aimed at predicting the sequence of actions (block placements and/or removals) a hu- man Builder performed at a specific point in a human-human game. Despite the task's inherent challenges and limited training data, we demonstrate that models trained with a suitable amount of game history, enriched world representations, and a sufficient amount of synthetic, diversified data (via simple data augmentation techniques) yield promising initial results. We recap relevant prior work on the Minecraft Builder in Sec- tion 2. Building on this foundation, we next take a closer look at evaluation and training data for the BAP task, uncovering key challenges and making significant improvements on both fronts to propose BAP v2, an upgraded version of the task. This will allow future work to make more efficient and meaningful progress on it.\nGiven the inherent complexity of the task, we find that evaluation is nuanced and requires closer examination. We identify key challenges that hinder fair and insightful evaluation, then systematically address them. To this end, we introduce a new, cleaner v2 test set to replace the original legacy test set for evaluating BAP models (Section 3). We also refine the F1 metric, proposing a fairer variant that accounts for nuances in the data. The current evaluation, which relies solely on an aggregated and opaque F1 metric, lacks detailed insights into model behavior. To enhance insightfulness, we introduce additional metrics that measure specific model capabilities. Together, these contributions form the updated BAP v2 evaluation benchmark (Section 4).\nOur work on evaluation highlights spatial reasoning as a key bottleneck in model performance, alongside the challenge of limited training data (even with the afore- mentioned data augmentation). Instead of merely collecting more data (which can be impractical/expensive in realistic scenarios for such complex situated dialogue tasks), we create additional diverse synthetic training data generated from novel Minecraft dialogue and target structure simulators that emulate the MCBT (Section 5). This data, though naturally simpler than it, is rich in spatial relations and referential expressions and we show that it can be successfully used to train better models, even with simple training methods (Section 6). We demonstrate that jointly training on combined syn- thetic and original BAP (augmented) data already improves performance. Further, a straightforward Curriculum Learning method provides an additional boost. Together, the synthetic and augmented BAP data form the BAP v2 training set. Looking ahead, such data could also be crucial for training more sophisticated, data-hungry deep transformer models and training/fine-tuning increasingly large LLMs. We provide a detailed analysis of model performance of our best performing model relative to the baseline in Section 7.\nAlthough modeling is not the primary focus of this work, solely to test the ro- bustness of our synthetic data and training methodologies and show their applicabil- ity to relatively contemporary model architectures, we experiment with very simple LLM (BERT)- and transformer-based variants of our neural models. The simplicity also allows us to assess whether/to what extent simpler LLMs like BERT and vanilla Transformers alone can already effectively handle the BAP task. Our results show that the synthetic data and training methodologies improve these models as well, validating the robustness of our approach (Section 6.3). Future work can build on these insights to explore more sophisticated LLMs and architectures while leveraging the BAP v2 framework.\nSection 8 provides a detailed discussion on four topics an additional key chal-"}, {"title": "", "content": "lenge in BAP evaluation, the importance of evaluation on the synthetic data, relevant concurrent work, and broader implications of our work that go beyond BAP. The latter further highlights the potential impact of our work and directions for future research. Related work is covered in Section 9. We conclude and discuss future work in Section 10."}, {"title": "2. Prior Work on the Minecraft Builder", "content": "This section provides a high-level overview of the dataset, task, and baseline model, summarizing our prior work (Narayan-Chen, Jayannavar, and Hockenmaier 2019; Jayannavar, Narayan-Chen, and Hockenmaier 2020). We also introduce some necessary formalisms and figures for better exposition."}, {"title": "2.1 The Minecraft Collaborative Building Task and Dialogue Corpus", "content": "The Minecraft Collaborative Building Task (MCBT) is a two-player game in which player A (the Architect) has to instruct player B (the Builder) to create a copy of a Target structure that is only shown to A. B controls a Minecraft agent that is given a fixed inventory of blocks. A has access to two Minecraft windows, one which contains the Target, and one in which it can observe B's actions. A remains invisible to B and cannot place blocks itself. A and B can only communicate via a text-based chat interface that"}, {"title": "2.1.1 Features and Challenges", "content": "Since target structures can be fairly complex, Architects typically give step-by-step instructions (\"now a yellow 6\") for different parts of the target. Builders should execute these instructions, but may also ask questions (\"Where does the 6 start\") which the Architects has to answer (\"behind the 7 from your perspective\"). Archi- tects may also need to identify and correct mistakes the Builder may have made (\"too much overlap unfortunately\u201d). A can move around freely, but remains invisible to B and views the structure from behind B when giving instructions. As a result, A instructions frequently include spatial relations, both between pairs of blocks or substructures (\u201cput on top of...\"), and relative to B's current position and perspective (\u201cleft\u201d, \u201cright\u201d). Humans also often use higher-level descriptions to refer to complex (sub)shapes (e.g. \u201cstaircase\u201d, \u201cv\u201d). Due to the asynchronous nature of the dialogue, the Architect often talks while the Builder is placing blocks (\u201cso it will look like a v\"), leading to an apparent interruption of the Builder's action sequences.\nFloating blocks. Blocks do not need to be placed on the ground if their cell is adjacent to a supporting block. If that supporting block is later removed, the remaining block (and any structure supported by it) will \u201cfloat\u201d in place. Thus, placing floating blocks needs the placement and subsequent removal of such placeholder supporting blocks. Instructions for floating structures vary greatly, ranging from step-by-step instructions involving temporary supporting blocks to single-shot descriptions such as, simply, \u201cplace a floating yellow block\u201d."}, {"title": "2.1.2 MCBT subtasks: Architect Utterance Generation and Builder Action Prediction", "content": "We define the Architect Utterance Generation (AUG) Task as the task of generating a suitable Architect utterance at any point in a human-human game at which the human"}, {"title": "2.2 The Builder Action Prediction (BAP) Task", "content": "The Builder Action Prediction (BAP) task is defined as the task of predicting the actions (block placements and/or removals) that a human Builder performed at a particular point in a human-human game. Games start with an empty board, and consist of an alternating sequence of dialogues $D_t$ (which may each consist of a single utterance, or a back-and-forth exchange between the two players) and Builder actions $A_t$ that result in an updated built structure $S_t$. Each action sequence $A_t$ yields a BAP item $(H_{t-1}, S_{t-1}, D_t, A_t, S_t)$ that consists of the game history $H_{t-1}$ that culminated in the previous built structure $S_{t-1}$, the new dialogue $D_t$, and the new action sequence $A_t$ that resulted in the new structure $S_t$. Given $H_{t-1}, S_{t-1}$, and $D_t$, BAP models M should predict an action sequence $A^M$ that yields $S_t$.\nDataset statistics. The training, test and development splits of the MDC contain 3709, 1616, and 1331 items respectively, and the average sequence length of an action sequence (in the development set) is 4.3 (with a std. deviation of 4.5). Target structures in the test data do not appear in the training or development data."}, {"title": "2.3 Formalizing the BAP task", "content": "The built structure. In Minecraft, blocks can be placed into the cells $c = (x, y, z)$ of a discrete 3D grid if c is empty and either on the ground or adjacent to any cell $c'$ that contains a block. A cell $c = (x, y, z)$ is on the ground if its height $y = 1$, and"}, {"title": "2.4 Evaluating BAP models", "content": "To compare two action sequences, or to measure the change from S to S', it is help- ful to note that any action $a = (t, c, c)$ is undone by the inverse action $\\bar{a} = (\\bar{t}, c, c)$ where $\\bar{t} = REMOVE$ if $t = PLACE$, and $\\bar{t} = PLACE$ if $t = REMOVE$. Most commonly, this occurs when blocks are placed to serve as a necessary supporting block for a floating structure and removed in the same action sequence. These supporting blocks can be ignored since they do not occur in S'. Moreover, builders are free to use any color they wish, and can often choose among a variety of possible locations where to place these blocks. Human Builders are also prone to accidentally placing or removing blocks, but typically recognize and correct such mistakes immediately within the same action sequence. Finally, many structures do not require their blocks to be placed in a particular order. We can therefore transform any (feasible) action sequence A into a"}, {"title": "", "content": "set of net actions $A^{net}$ by first removing from A any actions a and their inverse $\\bar{a}$ and turning the resulting (infeasible) sequence A' into a set $A^{net}$. The distance $\\Delta(S, S')$ between two structures S and S' can then be defined as the number of net actions of any action sequence $A : S \\rightarrow S'$ that changes S to S': $\\Delta(S, S') = |A^{net}|$, and any two action sequences $A_1$ and $A_2$ are equivalent ($A_1 \\equiv A_2$), i.e. lead to the same subsequent structure, if they have the same set of net actions ($A^{net}_1 = A^{net}_2$). We therefore evaluate BAP models against human Builders by comparing their respective net actions, $A^{net}$ and $A^{net}_H$ to compute a (strict) F1 score (we report a micro-averaged F1 score over all action sequences in the test/dev data):\nGiven a BAP item consisting of a (human) reference action sequence $A_H = (a_1, ..., a_n)$ that leads from S to $S_H$, corresponding to a reference net action set $A^{net}_H$, and a (model) predicted action sequence $A_M = (a_1, ..., a_m)$ that leads from S to $S_M$, corresponding to a predicted net action set $A^{net}_M$, strict Precision, Recall and F1 scores assume that a Builder action $a^M_i = (t, c, (x, y, z)) \\in A^{net}_M$ is correct if and only if there is an equal reference action $a^H_i = (t, c, (x, y, z)) \\in A^{net}_H$:"}, {"title": "2.5 Data Augmentation", "content": "Since the small size of the training set (3,709 examples) is a major limiting factor for training complex models, in Jayannavar, Narayan-Chen, and Hockenmaier (2020), we generated synthetic variants of the original game logs in the training data by combining three data augmentation techniques: utterance paraphrasing (synonym-based substitu- tions), color substitution (random color permutations across logs), and spatial transfor- mations (rotating structures and Builder B) (details in appendix). Empirically, we found that increasing the training data to 14,836 (4x) items gave the best performance for our baseline model (Section 2.6). We will refer to this as the original BAP (augmented) data."}, {"title": "2.6 A CNN- and GRU-based baseline model", "content": "In Jayannavar, Narayan-Chen, and Hockenmaier (2020), we proposed a neural model for the BAP task. In this work, we build upon it and refer to it as the baseline model. The model (Figure 4) is based on a recurrent encoder-decoder architecture (Sutskever, Vinyals, and Le 2014; Cho et al. 2014) in which a GRU-based encoder (bottom left box) captures the game context (dialogue and action history) via GloVe embeddings (Pennington, Socher, and Manning 2014), and a CNN-based encoder (top left box) captures the world state at each time step. The world state encompasses the current built structure, action history and the Builder's position and orientation. The decoder (right box) predicts one action per time step, based on the game history, the world state at that time, and the last action taken. It consists of another GRU backbone over action sequences (bottom right), and a multi-class classifier that reads in the output of the"}, {"title": "3. Revisiting BAP Evaluation: A Cleaner Test Set", "content": "Since the MDC consists of real human-human game logs collected in a low-stakes, free- flowing setting with minimal constraints on the participants' language and actions, rather than standalone instructions, this strategy does not always yield BAP items that are suitable for evaluation. First, Builders might move blocks even when no new instructions were given, or before it is completely clear what they should be building next (e.g. when the Architect only instructs the Builder to \u201cbuild a rectangle to the right of that", "that": ".", "place a red block on the ground\"). This likely occurs because there are no blocks on the board\"\n    },\n    {\n      \"title\"": "3.1 Annotating the test set"}, {"content": "We manually annotate items in the test set so that we can identify cases that have to be distinguished for evaluation purposes. Annotators were asked to annotate all items in the same dialog in sequence. For each non-empty board (NEB) item, annotators were first asked whether the context is clear and has a unique interpretation, i.e. clearly specifies what structure to be built next, and there is only a single way of placing that structure on the board (as is typically the case when the board is not empty), or whether the context is unclear, i.e. it is unclear what the Builder should do next, either because more information is required, or no new instruction was given. For empty board (EB) items, annotators could also indicate that the context is clear and has multiple inter- pretations, i.e. it is clear what structure should be built, even though there are multiple possible placements or orientations within the build region, as is often the case when the board is empty. If the context is clear (with unique or multiple interpretations), a second question asked whether the built structure is correct or incorrect. Details about the annotation interface, the annotation workflow, the annotation tool, data collection setup, etc. are provided in the appendix. We now illustrate the resulting categories of BAP items with a few examples.\nClear context with unique interpretation and correct structure (Figure 5). It is clear what structure B should build next, only one such structure can be built, and the human Builder actually built that structure (Figure 5a). This can also arise when the board is empty (Figure 5b), since B is first made to stand at the bottom right corner of the board, and A clearly specifies the locations of the two blocks to be placed, given B's new orientation.\nClear context with unique interpretation and incorrect structure (Figure 6) . Here, it is clear what structure B should build next (and only one such structure can be built), but B's actions did not result in that structure, perhaps because B got interrupted by A, or because B made a genuine mistake.\nClear context with multiple interpretations and correct structure (Figure 7). When the board is empty, it is often clear what structure B should build next, but the placement and orientation of that structure may not have been uniquely specified. In this case, the context is clear, although it can be (correctly) interpreted in multiple ways.\nUnclear context (Figure 8). Finally, some action sequences occur in contexts in which it is unclear what the Builder should do next."}, {"title": "3.2 Creating a clean test set", "content": "To obtain a clean BAP test set, we remove all items with unclear contexts, as the the correctness of the built structure cannot (and should not) be assessed for the BAP task. We also automatically fix/replace incorrect structures that can be fixed in such manner, and remove the items with structures that can't be fixed. This results in a new, BAP"}, {"title": "4. Revisiting BAP Evaluation: Metrics", "content": "We now revisit the strict F1 metric and propose a fairer variant that accounts for the fact that some EB items have multiple valid interpretations. Additionally, the current evaluation, relying solely on an aggregated F1 score, lacks detailed insights into model behavior. To address this, we introduce additional metrics. These contributions collec- tively define an updated BAP v2 evaluation benchmark."}, {"title": "4.1 Fairer F1", "content": "The clean v2 test set consists only of BAP items with a clear context and correct struc- tures, but 84 of the EB items (7.3% of the v2 test set) do not have a unique interpretation, since their structure can be placed and oriented in many different ways (see Figure 7 above). The v2 test set can therefore be split up into items with a unique correct inter- pretation (U) and items with multiple correct interpretations (M). But while the strict F1 score defined in Section 2.4 is appropriate for items in U, it unfairly penalizes Builders on items in M when their structure is correct, but placed in a different location and/or orientation than the reference structure. A fair evaluation metric needs to account for these allowable changes in location and orientation for items in M, i.e. structures placed on an empty board when the context does not uniquely identify their placement.\nRecall from Section 2.3 that a structure $S = {((x, y, z), c)}$ is a set of blocks that each have a color c and a location (x, y, z), and that the distance $\\Delta(S, S')$ between two structures S and S' is defined as the number of net actions of any action sequence that changes S to S' (or vice versa). To fairly compare two structures S, S' on the empty board, we first align them by searching for a translation $A_T$ of blocks in the horizontal plane and a rotation $A_R$ about the vertical axis in 90-degree intervals that transform S into a structure $S^* = A_R(A_T(S))$ in the BUILDREGIONthat is as close to S' as possible (i.e. where $\\Delta(S^*, S')$ is minimized). We refer to the composed transform $A_RA_T$ as alignment A, and call $A^* = arg \\min_A(\\Delta(A(S), S'))$ the optimal alignment of S to S'.\nTo evaluate a model's net action set $A^{net}_M$ against a human reference net action set $A^{net}_H$ for an item in M, where $A_M$ leads from the (empty) board to a predicted structure $S_M$, and $A_H$ leads from the (empty) board to a reference structure $S_H$, we therefore first identify an optimal alignment of the predicted structure to the reference, $A^* = arg \\min_A(\\Delta(A(S_M), S_H))$, and apply the same transformation $A^*$ to all actions in the model's net action set $A^{net}_M$, yielding an aligned action sequence $\\widehat{A^{net}_M}$ that can now be fairly compared against $A^{net}_H$ by our original strict F1 metric. That is, a fairer F1 metric computes a strict F1 score for the predicted action sequences $A^{net}_M$ for any item in U, because the reference sequences for these items yield the only correct structures for these items, but for any item in M, it computes a strict F1 score for the optimally aligned predicted action sequences $\\widehat{A^{net}_M}$ because this item has multiple correct interpretations that only differ from each other in location and orientation."}, {"title": "4.2 Auxiliary metrics", "content": "The current BAP evaluation relies solely on an aggregate F1 metric. While it reasonably reflects overall model performance (especially with fairer F1), it remains opaque and lacks detailed insight into specific model capabilities, such as spatial reasoning. This is a key missing aspect in the evaluation framework. To address this, we introduce finer- grained auxiliary metrics to complement the aggregate fairer F1 metric. These metrics aim to enhance evaluation robustness, provide deeper insight into model behavior, enable precise error analysis, better differentiate models, identify reasons behind a model's performance changes, and ultimately guide the development of better models.\nType, Color, and Location F1. In analogy to the strict F1 score, we define auxiliary metrics that only evaluate certain aspects of the predicted actions. Recall that the strict evalu- ation assumes that a Builder action $a = (t, c, (x, y, z)) \\in A^{net}_M$ is correct if and only if there is an equal reference action $a_H = (t, c, (x, y, z)) \\in A^{net}_H$, and that our strict precision, recall and F1 scores are therefore based on the sizes of Builder and Human net actions and their intersection. To evaluate whether a Builder model correctly predicts action types t, colors c, or locations (x, y, z), we define corresponding evaluation metrics that consider Builder actions correct if and only if there is an equivalent reference action in $A^{net}_H$. We consider three different equivalence relations: Type, Color (and type), and Location:"}, {"title": "", "content": "Intuitively, the Type F1 metric quantifies how good a model is at understanding what type of actions to perform \u2013 placements, or removals, or both, and how many of each type. The Color F1 metric narrows down Type F1 a bit more to factor for colors as well. It quantifies how good a model is at understanding the colors of blocks that need to be placed or removed. The Location F1 metric quantifies how good a model is at understanding the specific locations of blocks that need to be placed or removed (without regard to color), i.e., spatial reasoning. Also, as mentioned earlier, these met- rics are analogous to the strict F1 metric. They can then be used to define their fairer counterparts in the same way the strict F1 was used to define the fairer F1. Henceforth, we will use Type, Color and Location F1 to refer to their fairer counterparts.\nShape F1. To evaluate how good a Builder model is at understanding the \"shape\" in which the required blocks need to be placed or removed, we define a metric, Shape F1, that is agnostic to the absolute (x, y, z) locations of the required blocks, but only cares that they be placed/removed in the right locations relative to each other. Essentially, it compares net actions modulo their exact placement and orientation. Therefore, similar to the methodology used for fairer F1 (Section 4.1), we optimally align the model's net actions $A^{net}_M$ against the human reference net actions $A^{net}_H$ (instead of the corresponding resulting built structures themselves like was done for fairer F1), yielding aligned net actions $\\widehat{A^{net}_M}$ that can now be compared against $A^{net}_H$ by our original strict F1 metric. This is done for all items in the v2 test set (and not just the ones with multiple interpretations like was done for fairer F1). We will see examples of Shape F1 and the other auxiliary metrics later in Sections 7.2 and 8.1."}, {"title": "4.3 Micro vs Macro F1", "content": "Current BAP evaluation reports the micro-averaged F1 (micro F1) over all action se- quences in the test data (Section 2.4), reflecting a model's quality in predicting indi- vidual actions. However, since the BAP task requires sequence prediction, it's equally important to assess the model's quality in predicting entire action sequences using the macro-averaged F1 (macro F1). Intuitively, this is also particularly relevant for Shape F1, which measures the model's performance on a broader sequence-level property (the \"shape\" of the net actions). Therefore, hereafter, we will report both micro and macro F1."}, {"title": "4.4 BAP v2 evaluation benchmark", "content": "The above metrics enable a fairer, robust, and more insightful evaluation on the clean v2 test set. We use the fairer F1 metric to report both micro and macro F1 scores in Tables 1 and 2 respectively. For each, we include both auxiliary and overall F1 metrics, and report them on the overall/full test set and separately for the EB and NEB subsets as well. This forms the BAP v2 evaluation benchmark. Our approach also enables more accurate evaluation within the EB and NEB categories, offering a clearer understanding of model performance across these two distinct scenarios.\nEB examples are generally the easiest for the BAP task, making EB performance a basic competency or sanity test, and it also reflects a model's maximum potential quality."}, {"title": "", "content": "EB performance is also an estimate of the ability of a model to begin a game accurately \u2013 an elementary yet important trait to measure. Moreover, in an interactive/online setting, from a user (human Architect) experience perspective and considering the potential for subsequent cascading errors, mistakes made in EB scenarios can potentially be costlier than those in NEB, as EB marks the start of a game (even though EB examples in the data are far fewer than NEB). Quantifying EB performance will help provide a sense for how a model can potentially fare in an interactive setting wrt this aspect.\nWe begin with the micro scores in Table 1 for direct comparison with our prior work. Recall that the baseline model achieves an F1 score of 21.1% under the strict F1 metric. This increases to 27.3% under the fairer F1 metric (the overall F1 on the overall dataset), a notable difference of 6.2 percentage points and a 29.4% improvement. This highlights the importance of the high quality cleaner test data and the fairer F1 metric in providing a fairer reflection of model capability. On the overall dataset, the model achieves reasonably good F1 scores of 65.2% for Type and 62.4% for Color, but only 27.7% for Location F1, which is also very close to the overall F1 of 27.3%. This suggests that spatial reasoning (Location F1) is the key bottleneck for high performance on this task. As expected, the model performs better on EB than NEB across all metrics. Recall that the number of EB items are fewer than NEB items though (and the metrics on the overall dataset will therefore be more influenced by NEB). Next, we turn to the macro scores in Table 2. While all values increase compared to the micro scores, the overall trends remain the same. The overall F1 on the overall test set increases from 27.3% (micro) to 28.1% (macro). The model achieves a Shape F1 of 36.3% (micro) and 41.1% (macro) on the overall test set, with the macro score providing a more intuitive measure in this context, as discussed in Section 4.3.\nHereafter, we will use the v2 evaluation to report model performance."}, {"title": "5. Revisiting BAP Data: Generating Synthetic Data", "content": "One key challenge of the BAP task is the limited training data, despite the data augmen- tation (Section 2.5). We aim to avoid simply collecting more data like the MDC, due to scalability issues and the desire to stay aligned with many realistic scenarios, where data collection for complex situated dialogue tasks, like the MCBT, can be impractical/ex- pensive, warranting exploration of alternative methods. Additionally, we observe that the baseline model struggles primarily with spatial reasoning, as reflected in its Location F1 score. These reasons motivate the need for additional synthetic training data that, while naturally simpler than the MDC, remains rich in spatial relations and referential expressions to help train more robust models. Looking ahead, such data could also be crucial for training more sophisticated, data-hungry deep transformer models and training/fine-tuning increasingly large LLMs.\nWe design novel Minecraft dialogue and target structure simulators that emulate the MCBT, and use them to generate a set of three synthetic dialog datasets with corresponding target structures . Each dataset is modeled after the MDC and is un- derstandably simpler, while preserving essential elements of realistic human behavior observed in the MDC as much as possible, such as the language, clarification exchanges, Architect A 's and Builder B's planning perspectives, dialog flow, etc. Across these datasets, we vary the level of instruction abstraction, the complexity of spatial relations and referential expressions, the complexity of target structures, etc. Special emphasis is laid on spatial relations as that is a core part of our motivation here. Each dataset is thus unique, contributing to the overall diversity and richness of the overall set.\nNote that although our primary focus is on BAP, the simulators and data introduced here are applicable to other MCBT subtasks as well (e.g. the AUG subtask; Section 2.1.2), supporting advancements across the broader MCBT ecosystem."}, {"title": "5.1 General dialog simulation framework", "content": "Similar to the target structures and game logs in the MDC, the follow- ing components of synthetic dialogues need to be generated:"}, {"title": "", "content": "The target structure\nThe dialogue/game a chronological sequence that interleaves the following elements:\nA dialogue between A and B, consisting of utterances that include instructions, potential clarification exchanges, etc.\nB's position and orientation at appropriate points in the game, i.e., when an utterance or action is recorded (Grounded elements of the dialog, e.g. spatial relations, are always wrt this frame of reference.)\nB's actions, which reflect the evolving structure being built"}, {"title": "5.1.2 High-level Algorithm", "content": "As described in Section 2.1, the MCBT task is asynchronous and loosely structured. The MDC dialogs are real human-human game logs collected in a low-stakes, free-flowing setting with minimal constraints on participants' language and actions. To enable tractable simulation and facilitate simpler synthetic data genera- tion, we make some simplifications and structure the task as follows."}]}