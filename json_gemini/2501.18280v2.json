{"title": "Jailbreaking LLMs' Safeguard with Universal Magic Words for Text Embedding Models", "authors": ["Haoyu Liang", "Youran Sun", "Yunfeng Cai", "Jun Zhu", "Bo Zhang"], "abstract": "The security issue of large language models (LLMs) has gained wide attention recently, with various defense mechanisms developed to prevent harmful output, among which safeguards based on text embedding models serve as a fundamental defense. Through testing, we discover that the output distribution of text embedding models is severely biased with a large mean. Inspired by this observation, we propose novel efficient methods to search for universal magic words that attack text embedding models. Universal magic words as suffixes can shift the embedding of any text towards the bias direction, thus manipulating the similarity of any text pair and misleading safeguards. Attackers can jailbreak the safeguards by appending magic words to user prompts and requiring LLMs to end answers with magic words. To eradicate this security risk, we also propose defense methods against such attacks, which can correct the bias of text embeddings and improve downstream performance in a train-free manner.", "sections": [{"title": "1. Introduction", "content": "Recently, large language models (LLMs) have been widely applied in the industry, such as chat systems (Brown et al., 2020) and search engines (Nayak, 2019). However, LLMs can be maliciously exploited to extract harmful output, making LLM security an important research topic.\nIn this topic, it is of great significance to discover security vulnerabilities of text embedding models and propose the corresponding defense methods. Current LLM security strategies include alignment (Bai et al., 2022) and input- output safeguards (OpenAI, 2025). Lightweight text classifiers based on text embedding models (Kim et al., 2023) can be used as safeguards to judge whether the input and output of LLMs are harmful. This method can serve as a foundational line of defense because it is low-cost and able to maintain the performance of LLMs. In addition, text embedding models are also used to enhance modern search engines (Nayak, 2019). Therefore, the robustness of text embedding models affects the security of both LLMs and search engines.\nAttacking LLMs' safeguards is challenging because the output of LLMs is unknown, the safeguards are black boxes, and the token space is vast and discrete. This results in the following limitations of existing attack methods on text embedding models: 1) Case-by-case attack methods require access to LLMs' output before safeguards, which is unrealistic for online dialogue systems; 2) White-box attack methods require the gradients of text embedding models, which are also unrealistic; 3) Brute-force search for prompt perturbations requires traversing a massive token space, leading to high time costs.\nTo address these challenges, we propose an innovative approach to attack LLMs' safeguards based on text embedding models: to find universal \u201cmagic words\" (i.e., adversarial suffixes) that would increase or decrease the embedding similarity between any pair of texts so as to mislead the safeguards in classifying within the text embedding space."}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Defense Methods for LLMs", "content": "Alignment involves training LLMs to align with human values (Askell et al., 2021; Liu et al., 2022; Bai et al., 2022). This method is widely used because it does not introduce additional computational overhead during inference. Due to the competition between assisting users and aligning values, as well as the limited domain of safety training (Wei et al., 2024), such methods are vulnerable to adversarial attacks (Zou et al., 2023; Chao et al., 2023). This has forced people to develop additional security measures.\nSafeguards are the additional measures on the input or output of LLMs to prevent harmful responses.\nOn the input side, there are several guard measures: 1) Detecting suspicious patterns (Alon & Kamfonas, 2023; Jain"}, {"title": "2.2. Attack Methods for LLMs", "content": "Templates jailbreak LLMs with universal magic words effective for various prompts, some even transferable across LLMs. Manual templates are heuristically designed, including explicit templates (e.g., instructing LLMs to \u201cignore previous instructions\u201d (Perez & Ribeiro, 2022), \u201cStart with 'Absolutely! Here's\u201d (Mozes et al., 2023) or \u201cDo anything now\u201d (Mozes et al., 2023)) and implicit templates (e.g., role-playing (Bhardwaj & Poria, 2023; Shah et al., 2023), storytelling (Li et al., 2023b) and virtual scenarios (Li et al., 2023a; Kang et al., 2024; Singh et al., 2023; Du et al., 2023)). Automatical templates are optimized by gradient descent (black-box) (Wallace et al., 2019; Zou et al., 2023; Zhu et al., 2024), random search (white-box) (Lapid et al., 2024; Andriushchenko et al., 2024), or generative models (Liao & Sun, 2024) to find adversarial prefixes and suffixes for user prompts. These prefixes and suffixes could be individual words or sentences (Zou et al., 2023), and comprehensible (Liao & Sun, 2024) or not (Lapid et al., 2024).\nRewriting attacks language models at several levels, including character-level (e.g., misspelling (Li et al., 2019)), word-level (e.g., using synonyms (Jin et al., 2020)), segment-level (e.g., assigning variable names to segmented harmful text (Wu et al., 2024; Kang et al., 2024)), prompt-level (e.g., rewriting user prompts with an LLM (Chao et al., 2023; Mehrotra et al., 2023; Tian et al., 2023; Ge et al., 2023)), language-level (e.g., translating into a language that lacks LLM safety (Qiu et al., 2023)), and encoding-level (e.g., encoding harmful text into ASCII, Morse code (Yuan et al., 2023) or Base64 (Kwon & Pak, 2024)). Through optimization algorithms, attackers can automatically find the most effective rewrites to bypass the LLM's safeguards.\nThe methods above are all focused on attacking the LLM"}, {"title": "3. Method", "content": "Notation. Let $s_1$ and $s_2$ be two text strings, and let $r$ be a positive integer. The operation $s_1 + s_2$ denotes the concatenation of $s_1$ and $s_2$, and $r * s_2$ denotes the string $s_2$ repeated $r$ times. For example, if $s_1 =$ \u201che\u201d, $s_2 =$ \u201cllo\u201d, then $s_1 + s_2 =$ \u201chello\u201d and $s_1 + 2 * s_2 =$ \u201chellollo\u201d.\nDenote the text embedding of text string $s$ by $e(s)$ and its dimension by $d$. $e(s)$ is normalized to a unit vector, hence $e(s) \\in S^{d-1}$.\nSpecifically, the text embedding $e(s)$ of $s$ is computed as $e(s) = \\mathbf{e}(\\mathbf{s}), \\ \\mathbf{s} = \\mathcal{E} \\mathcal{T} \\mathcal{T}(\\mathbf{s})$. Here, $\\mathbf{s} \\in \\mathbb{R}^{h \\times l}$ denotes the representation of $\\mathbf{s}$ in the token embedding space, which is mapped to a text embedding by $\\mathbf{e}$ \u2013 the core module of the text embedding model $e$. Moreover, $\\tau$ is a tokenizer that splits $\\mathbf{s}$ into $l$ tokens, outputting $\\tau(\\mathbf{s}) \\in \\{0, 1\\}^{T \\times l}$ where the columns are one-hot. $T = \\{t_i\\}_i$ is the token vocabulary, with size $|T| = T$. $\\mathcal{E} \\in \\mathbb{R}^{T \\times h}$ denotes the token embeddings of all tokens, with dimension $h$.\nThe cosine similarity between text $s_1$ and $s_2$ is defined as\n$\\cos \\theta(s_1, s_2) := e(s_1)^T e(s_2)$.\nThis paper aims to find all possible universal magic words. This problem can be formulated as follows.\nAssumption 3.1. There exists a word $w_+$ satisfying that\n$\\cos \\theta(s_1 + w_+, s_2) \\ge \\cos \\theta_*, \\ \\forall s_1, s_2,$\nwhere $\\cos \\theta_*$ is a number close to 1. We refer to $w_+$ as a positive universal magic word for the text embedding model $e$, which can force any pair of texts to be similar enough in the text embedding space."}, {"title": "3.1. Description of the Uneven Direction", "content": "To describe the unevenness of the text embedding distribution, we represent the bias direction of the distribution by"}, {"title": "3.2. Searching for Universal Magic Words", "content": "Based on the observations in Section 3.1, we boldly presume the existence of universal magic words. When used as a suffix, universal magic words could make any text more similar or dissimilar to other texts in the embedding space.\nWe refer to the words that increase the text similarity as positive magic words and those that decrease the text similarity as negative magic words.\nBrute-Force Method. The simplest method to find magic words is a brute-force search, shown in Algorithm 1. This method directly calculates the similarity score of all tokens in the vocabulary set and finds the top-$k_0$ magic words. This method does not rely on the bias direction.\nFor each token $t_i$ in the token vocabulary set $T = \\{t_i\\}_i$, we define the positive similarity score as\n$c_i^+ = \\max_{1 < r < 16} \\sum_{j, k} \\cos \\theta(s_j + r * t_i, s_k)$.\nTokens with higher positive scores are more effective as positive magic words. $r$ represents the repetition count. Repeating the magic word usually amplifies its effect. However, we limit $r$ to a maximum of 16 to avoid completely distorting the text.\nFinding negative magic words requires more data. Specifically, in addition to the text $s_j$, we also need another piece of text $s'_j$ that is semantically similar to $s_j$ but phrased differently. This is because the effect of a negative magic word is to make synonymous text no longer synonymous. Now the set of text pairs is in the form $S = \\{(s_j, s'_j)\\}$; with $\\cos \\theta(s_j, s'_j)$ close to 1. We define the negative similarity score of $t_i$ as\n$c_i^- = \\min_{1 < r < 16} \\sum_{j} \\cos \\theta(s_j + r * t_i, s'_j)$.\nThe lower negative similarity score indicates the more effectivity of magic words in making synonymous text dissimliar.\nContext-Free Method. As demonstrated previously, all text embeddings tend to be close to $e^*$ and far from $-e^*$. Intuitively, tokens whose text embeddings has the same diretion as $e^*$ are likely to be positive magic words, and vice versa. Specifically, for a given $t_i \\in T$, we select the top-$k$ and bottom-$k$ tokens as candidates for positive and negative magic words based on the following score\n$c_i = e(r * t_i)^T e^*,$"}, {"title": "3.3. Attacking LLMs' Safeguard", "content": "As shown in Figure 2, we can append magic words to the prompt to attack the input guard of LLMs directly and require the LLM to end answers with magic words to attack the output guard indirectly.\nThis method works by moving text embedding to where the safeguard fails. As shown in Figure 3, the data manifold"}, {"title": "4. Experiments", "content": "We tested our method on several state-of-the-art models from the MTEB text embedding benchmark (Muennighoff et al., 2023), including sentence-t5-base (Ni et al., 2022), nomic-embed-text-v1 (Nussbaum et al., 2024), e5-base-v2 (Wang et al., 2022), and jina-embeddings-v2-base-en (G\u00fcnther et al., 2023). Additionally, considering that LLMs are sometimes used as text embedding models, we also tested Qwen2.5-0.5B (Qwen, 2024) with mean pooling. We used sentence-transformers/simple-wiki (tomaarsen, 2024) as the text dataset $S = \\{(s_i, s'_i)\\}$, where $s_i$ is an English Wikipedia entry, and $s'_i$ is its simplified variant."}, {"title": "4.1. Bias Direction", "content": "Since the whole dataset is massive, we sampled 1/100 of all entries (sample number is 1,000) to estimate the bias direction of text embeddings. Our experiments show that when the sample number exceeds 100, the estimation for $e^*$ or $v^*$ is sufficiently accurate. We found that the normalized mean vector $e^*$ is almost identical to the principal singular vector $v^*$ as shown in Table 3. Appendix B explains that this is a property of biased distributions. Therefore, we only use $e^*$ in the subsequent experiments."}, {"title": "4.2. One-token Magic Words", "content": "In our experiments, Algorithm 2, 3 successfully find the best one-token magic words identified by the brute-force baseline Algorithm 1. We demonstrate some of them in Table 2. Here, (Clean) represents the data without magic words, and the similarity $\\cos \\theta(s_j, s_k)$ or $\\cos \\theta(s_j, s'_j)$ between clean text pair is shown in the form $\\mu \\pm \\sigma$. The similarity score of each magic word is defined in Equation (4), (5), which indicates how much it can shift the similarity. The table shows that the shift of similarity can be up to several standard deviations, which is significant. This indicates that the magic words have a strong ability to manipulate text similarity."}, {"title": "5. Theoretical Analysis", "content": "As discussed above, the distribution of text embeddings on $S^{d-1}$ is biased towards the mean direction $e^*$, as shown in the left part of Figure 5. Algorithm 3 finds the inverse image of $e^*$ in the token embedding space, denoted by $t^*$ defined in Equation (8). Since tokens are discrete, there isn't always a token near $t^*$ in the token embedding space. However, our experiments show that candidates can always be found near $t^*$.\nTo address this paradox, we propose the following explanation. At the initialization of word embedding models, token embeddings are randomly initialized in the token embedding space. During training, they concentrate towards a certain subset of the token embedding space (Tulchinskii et al., 2024). This subset must be away from $t^*$ to avoid degrading text embeddings' performance on high-frequency data. However, there are always some low-frequency tokens that are rarely updated by gradient descent and thus remain almost as they were randomly initialized. If they happen to be located near $t^*$, they are the positive magic words we are looking for.\nAn interesting insight into why text embeddings, initially uniformly initialized, tend to concentrate after training is that a Transformer can be mapped to an $O(N)$ model (Sun & Haghighat, 2025), and an $O(N)$ model can acquire a nonzero vacuum expectation value (VEV) through spontaneous symmetry breaking (SSB). This nonzero VEV implies that the mean of the text embeddings is no longer zero."}, {"title": "6. Defense against Our Attacks", "content": "To minimize the negative impact of our work, we propose the following recommendations to defend our attacks based on the above analysis.\nRenormalization. Estimate the mean embedding $\\bar{e}$ from a large amount of text, subtract $\\bar{e}$ from text embeddings, and renormalize them\n$e(s) := \\frac{e(s) - \\bar{e}}{||e(s) - \\bar{e}||}$.\nThis can eradicate the risk of the magic words we found. We test the defense effect of renormalization against our magic words on the sentence-t5-base model. The experimental setup is the same as Section 4.5. As shown in Figure 4, renormalization significantly alleviates or even eradicates the decrease in AUC caused by magic words, therefore improving the robustness of LLMs' safeguards.\nAdditionally, renormalization makes the distribution of text embeddings more uniform, which may improve the performance of text embedding models. As shown in Figure 4, renormalization increases AUC on clean data, i.e., enhances the performance of three downstream classifiers in both input and output data. This represents a train-free improvement to the text embeddings.\nBesides, standardization and batch normalization may play a similar role because they also subtract the mean. See Appendix C for the experiments on them.\nVocabulary Cleaning. A larger vocabulary is not always better; it should align with the training data, avoiding the inclusion of noisy words such as tokenization errors, misspellings, markups, and rare foreign words.\nReinitialization. After the model has been trained, noisy words can be reinitialized based on the average value of the token embeddings or the value of <unk> and then finetuned."}, {"title": "7. Conclusion", "content": "We have found that the output distribution of many current text embedding models is uneven. Based on this, we have designed new algorithms to attack LLMs' safeguards using text embedding models. Our algorithms can efficiently search for magic words that significantly increase or decrease the text embedding similarity between any pair of texts in both black-box and white-box manner. We propose to inject the magic words into the input and output of LLMs to attack their safeguards. This attack misleads safeguards based on a variety of text embedding models. Beside, we proposed and validated that renormalization in the text embedding space can defend against this attack and improve downstream performance in a train-free manner."}, {"title": "D. Another Definition of Negative Magic Words", "content": "In the main text, we define universal negative magic words as words that make a text move away from semantically similar texts. However, there also exist words that push a text away from any other text, which can be another definition of negative magic words. This can be expressed as an assumption similar to Assumption 3.1: There exists a word $w^\u2212$ satisfying that\n$\\cos \\theta(s_1 + w^\u2212, s_2) \\le \\cos \\theta_*, \\ \\forall s_1, s_2,$\nwhere $\\cos \\theta_*$ is a number close to -1. Such magic word $w^\u2212$ can force any pair of texts to be dissimilar enough in the text embedding space.\nAnd similar to Section 3.1, any text appended by such magic word $w^\u2212$ will be close to $-e^*$ (or $-v^*$), as shown in Figure 8. The Proposition 3.2, 3.3 for negative magic words can be given and proved in a similar way.\nThis effectively moves text embeddings closer to the southern pole $-e^*$ of the sphere, so we refer to such magic words $w^\u2212$ as southern magic words. Concretely, a good southern magic word should make the following metric as small as possible,\n$\u03b6 = \\min_{1 < r < 16} \\sum_{j,k} \\cos \\theta(s_j + r * t_i, s_k).$"}]}