{"title": "Wasserstein Distance Rivals Kullback-Leibler Divergence for Knowledge Distillation", "authors": ["Jiaming Lv", "Haoyuan Yang", "Peihua Li"], "abstract": "Since pioneering work of Hinton et al., knowledge distillation based on Kullback-Leibler Divergence (KL-Div) has been predominant, and recently its variants have achieved compelling performance. However, KL-Div only compares probabilities of the corresponding category between the teacher and student while lacking a mechanism for cross-category comparison. Besides, KL-Div is problematic when applied to intermediate layers, as it cannot handle non-overlapping distributions and is unaware of geometry of the underlying manifold. To address these downsides, we propose a methodology of Wasserstein Distance (WD) based knowledge distillation. Specifically, we propose a logit distillation method called WKD-L based on discrete WD, which performs cross-category comparison of probabilities and thus can explicitly leverage rich interrelations among categories. Moreover, we introduce a feature distillation method called WKD-F, which uses a parametric method for modeling feature distributions and adopts continuous WD for transfer-ring knowledge from intermediate layers. Comprehensive evaluations on image classification and object detection have shown (1) for logit distillation WKD-L outperforms very strong KL-Div variants; (2) for feature distillation WKD-F is superior to the KL-Div counterparts and state-of-the-art competitors. The source code is available at https://peihuali.org/WKD.", "sections": [{"title": "1 Introduction", "content": "Knowledge distillation (KD) aims to transfer knowledge from a high-performance teacher model with large capacity to a lightweight student model. In the past years, it has attracted ever increasing interest and made great advance in deep learning, enjoying widespread applications in visual recog-nition and object detection, among others [1]. In their pioneering work, Hinton et al. [2] introduce Kullback-Leibler divergence (KL-Div) for knowledge distillation, where the prediction of category probabilities of the student is constrained to be similar to that of the teacher. Since then, KL-Div has been predominant in logit distillation and recently its variants [3; 4; 5] have achieved compelling performance. In addition, such logit distillation methods are complementary to many state-of-the-art methods that transfer knowledge from intermediate layers [6; 7; 8].\nDespite the great success, KL-Div has two downsides that hinder fully transferring of the teacher's knowledge. First, KL-Div only compares the probabilities of the corresponding category between the teacher and student, lacking a mechanism to perform cross-category comparison. However, real-world categories exhibit varying degrees of visual resemblance, e.g., mammal species like dog and wolf look more similar to each other while visually very distinct from artifact such as car and bicycle. Deep neural networks (DNNs) can distinguish thousands of categories [9] and thus are"}, {"title": "2 WD for Knowledge Transfer", "content": "Given a pre-trained, high-performance teacher model T, our task is to train a lightweight student model 8 that can distill knowledge from the teacher. As such, supervisions of the student are from both the ground truth label with the cross entropy loss and from the teacher with distillation losses to be described in the next two sections."}, {"title": "2.1 Discrete WD for Logit Distillation", "content": "Interrelations (IRs) among categories. As shown in Figures la and 4, real-world categories exhibit complex topological relations in the feature space. For instance, mammal species are nearer each other while being far away from artifact or food. Moreover, features of the same category cluster and form a distribution while neighboring categories have overlapping features and cannot be fully separated. As such, we propose to quantify category IRs based on CKA [18], which is a normalized Hilbert-Schmidt Independence Criterion (HSIC) that models statistical relations of two sets of features by mapping them into a Reproducing Kernel Hilbert Space (RKHS) [21].\nGiven a set of b training examples of category C\u00bf, we compute a matrix $X_i \\in \\mathbb{R}^{u \\times b}$ where the k-th column indicates the feature of example k that is output from the DNN's penultimate layer. Then we compute a kernel matrix $K_i \\in \\mathbb{R}^{b \\times b}$ with some positive definite kernel, e.g., a linear kernel for which $K_i = X_iX_i^T$, where T indicates matrix transpose. Besides the linear kernel, we can choose other kernels such as polynomial kernel and RBF kernel (cf. Section A.1 for details). The IR between $C_i$ and $C_j$ is defined as:\n$IR(C_i, C_j) = \\frac{HSIC(C_i, C_j)}{\\sqrt{HSIC(C_i, C_i) HSIC(C_j, C_j)}}$,\n$HSIC(C_i, C_j) = \\frac{1}{(b-1)^2} tr(K_iHK_jH)$. (1)\nHere H = $I-\\frac{1}{b}11^T$ is the centering matrix where I indicates the identity matrix and 1 indicates an all-one vector; tr indicates matrix trace. $IR(C_i, C_j) \\in [0, 1]$ is invariant to isotropic scaling and orthogonal transformation. Note that the cost to compute the IRs can be neglected since we only need to compute them once beforehand. As the teacher is more knowledgeable, we compute category interrelations using the teacher model, which is indicated by $IR(C_i, C_j)$.\nBesides CKA, cosine similarity between the prototypes of two categories can also be used to quantify IRs. In practice, the prototype of one category can be computed as the average of the features of the category's examples. Alternatively, the weight vectors associated with the softmax classifier of a DNN model can be regarded as prototypes of individual categories [22].\nLoss function. Given an input image (instance), we let $z = [z_i] \\in \\mathbb{R}^n$ be the corresponding logits of a DNN model where $i \\in S_n = \\{1,\\dots, n\\}$ indicates the index of i-th category. The predicted category probability $p = [p_i]$ is computed via the softmax function \u03c3 with a temperature \u03c4, i.e., $p_i = \\sigma(z_i/\\tau); exp(z_i/\\tau)/\\sum_{j \\in s_n}exp(z_j/\\tau)$. We denote by $p^t$ and $p^s$ the predicted category"}, {"title": "2.2 Continuous WD for Feature Distillation", "content": "As the features output from intermediate layers of DNN are of high dimension and small size, the non-parametric methods, e.g., histogram and kernel density estimation, are infeasible. Therefore, we use one of the widely used parametric methods (i.e., Gaussian) for distribution modeling.\nFeature distribution modeling. Given an input image, let us consider feature maps output by some intermediate layer of a DNN model, whose spatial height, width and channel number are h, w and l, respectively. We reshape the feature maps to a matrix $F \\in \\mathbb{R}^{l \\times m}$ where $m = h \\times w$ and the i-th column $f_i \\in \\mathbb{R}^l$ indicates a spatial feature. For these features, we estimate the 1st-moment $\\mu = \\frac{1}{m} \\sum f_i$ and the 2nd-moment $\\Sigma = \\frac{1}{m} \\sum (f_i \u2013 \\mu) (f_i \u2013 \\mu)^T$. We model feature distribution of the input image by a Gaussian with mean vector \u00b5 and covariance matrix \u2211 as its parameters:\n$\\mathcal{N} (\\mu, \\Sigma) = \\frac{1}{2\\pi^{l/2} |\\Sigma|^{1/2}} exp(-\\frac{1}{2} (f-\\mu)^T \\Sigma^{-1} (f-\\mu))$, (6)\nwhere |\u00b7| indicates matrix determinant."}, {"title": "3 Related Works", "content": "We summarize KD methods related to ours and show their connections and differences in Table 1.\nKL-Div based knowledge distillation. Zhao et al. [3] disclose the classical KD loss [2] is a coupled formulation that limits its performance, and thereby propose a decoupled formulation (DKD) that consists of a binary logit loss for the target category and a multi-class logit loss for all non-target categories. Yang et al. [4] propose a normalized KD (NKD) method, which decomposes the classical KD loss into a combination of the target loss (like the widely used cross-entropy loss) and the loss of normalized non-target predictions. WTTM [5] introduces R\u00e9nyi entropy regularizer without temperature scaling for student. In spite of competitive performance, they cannot explicitly exploit relations among categories. By contrast, our Wasserstein distance (WD) based method can perform cross-category comparison and thus exploit rich category interrelations.\nWD based knowledge distillation. The existing KD methods founded on WD [15; 16] mainly concern cross-instance matching for feature distillation, as shown in Figure 3 (left). Chen et al. [15] propose a Wasserstein Contrastive Representation Distillation (WCORD) framework which involves"}, {"title": "4 Experiments", "content": "We evaluate WKD for image classification on ImageNet [41] and CIFAR-100 [42]. Also, we evaluate the effectiveness of WKD on self-knowledge distillation (Self-KD). Further, we extend WKD to object detection and conduct experiment on MS-COCO [43]. We train and test models with PyTorch framework [44], using a PC with an Intel Core i9-13900K CPU and GeForce RTX 4090 GPUs."}, {"title": "4.1 Experiment Setting", "content": "Image classification. ImageNet [41] contains 1,000 categories with 1.28M images for training, 50K images for validation and 100K for testing. In accordance with [25], we train the models for 100 epochs using SGD optimizer with a batch size of 256, a momentum of 0.9 and a weight decay of 1e-4. The initial learning rate is 0.1, divided by 10 at the 30th, 60th and 90th epochs, respectively. We use random resized crop and random horizontal clip for data augmentation. For WKD-L, we use POT library [45] for solving discrete WD with \u03b7=0.05 and 9 iterations. For WKD-F, the projector has a bottleneck structure, i.e., a 1\u00d71 Convolution (Conv) and a 3\u00d73 Conv both with 256 filters followed by a 1\u00d71 Conv with BN and ReLU to match the size of teacher's feature maps."}, {"title": "4.2 Dissection of WD-based Knowledge Distillation", "content": "We analyze key components of WKD-L and WKD-F on ImageNet. We adopt ResNet34 as a teacher and ResNet18 as a student (i.e., setting (a)), whose Top-1 Accuracies are 73.31% and 69.75%, respectively. See Section C.1 for analysis on hyper-parameters, e.g., temperature and weight."}, {"title": "4.2.1 Ablation of WKD-L", "content": "How WD performs against KL-Div? We compare WD to KL-Div with (w/) and without (w/o) separation of target probability in Table 2a. For the case of without separation, WD (w/o) improves over KL-Div (w/o) by 1.0%; for the case of with separation, WD (w/) outperforms non-trivially KL-Div (w/) based DKD and NKD. The comparison above clearly shows that (1) WD performs better than KL-Div in both cases and (2) the separation also matters for WD. As such, WD with separation of target probability is used across the paper.\nHow to model category interrelations (IRs)? Table 2b compares two methods for IR modeling, i.e., CKA and cosine. For the former, we assess different kernels while for the latter we evaluate prototypes with classifier weights or class centroids. We note all WD-based methods perform much better than the baseline of KL-Div. Overall, IR with CKA performs better than IR with cosine, indicating it has better capability to represent similarity among categories. For IR with CKA, RBF kernel is better than polynomial kernel, while linear kernel is the best so is used throughout."}, {"title": "4.2.2 Ablation of WKD-F", "content": "Full covariance matrix or diagonal one? As shown in Table 3a (3rd and 4th rows), for Gaussian (Full), WD performs better than G\u00b2DeNet [33], which suggests that the former metric is more appropriate for feature distillation. When using WD, Gaussian (Diag) (5th row) produces higher accuracy than Gaussian (Full). We conjecture the reason is that high dimensionality of features makes estimation of full covariance matrices not robust [52]; in contrast, for Gaussians (Diag), we only need to estimate 1D variances for univariate data of single dimension. Besides, Gaussian (Diag) is much more efficient than Gaussian (Full). So we use Gaussians (Diag) throughout the paper."}, {"title": "4.3 Image Classification on ImageNet", "content": "Table 4 compares to existing works in two settings. Setting (a) involves homogeneous architecture, where the teacher and student networks are ResNet34 and ResNet18 [9], respectively; setting (b) concerns heterogeneous architecture, in which we set the teacher as ResNet50 and the student as MobileNetV1 [57]. Refer to Section C.2 for hyper-parameters in Settings (a) and (b).\nFor logit distillation, we compare our WKD-L with KD [2], DKD [3], NKD [4], CTKD [54] and WTTM [5]. Our WKD-L performs better than the classical KD and all its variants in both settings. Particularly, our WKD-L outperforms WTTM, a very strong variant of KD, which additionally introduces a sample-adaptive weighting method. This suggests Wasserstein distance that performs"}, {"title": "4.4 Image Classification on CIFAR-100", "content": "We evaluate WKD in the settings where the teacher is a CNN and the student is a Transformer or vice versa. We use CNN models including ResNet (RN) [9], MobileNetV2 (MNV2) [58] and ConvNeXt [59], as well as vision transformers that involve ViT [60], DeiT [61], and Swin Trans-former [62]. The setting of hyper-parameters can be found in Section C.5.\nFor logit distillation, we compare WKD-L to KD [2], DKD [3], DIST [63] and OFA [46]. As shown in Table 6, WKD-L consistently outperforms state-of-the-art OFA for transferring knowledge from Transformers to CNNs or vice versa. For feature distillation, we compare to FitNet [24], CC [64], RKD [65] and CRD [25]. WKD-F ranks first across the board; notably, it significantly outperforms the previous best competitors by 2.1%-3.4% in four out of five settings. We attribute superiority of WKD-F to our distribution modeling and matching strategies, i.e., Gaussians and Wasserstein distance. We posit that, for knowledge transfer across CNNs and Transformers that yield very distinct features [46], WKD-F is more suitable than raw feature comparisons as in FitNet and CRD."}, {"title": "4.5 Self-Knowledge Distillation on ImageNet", "content": "We implement our WKD in the framework of Born-Again Network (BAN) [66] for self-knowledge distillation (Self-KD). Specifically, we first train an initial model So using ground truth labels. Then we distill, using WKD-L, the knowledge of So into a student model S\u2081 with the same architecture as So. For the sake of simplicity, we do not perform multi-generation distillation, such as training a student model S2 with S\u2081 as the teacher, etc.\nWe conduct experiments with ResNet18 on Ima-geNet, where the hyper-parameters are consistent with those in Setting (a). As shown in Table 7, BAN achieves competitive accuracy that is comparable to state-of-the-art results. Our method achieves the best result, outperforming BAN by ~ 0.9% in Top-1 accuracy and the second-best (i.e., USKD) by 0.6%. This comparison demonstrates that our WKD can well generalize to self-knowledge distillation."}, {"title": "4.6 Object Detection on MS-COCO", "content": "We extend WKD to object detection in the framework of Faster-RCNN [47]. For WKD-L, we use the classification branch in the detection head for logit distillation. For WKD-F, we transfer knowledge from features straightly fed to the classification branch, i.e., features output by the RoIAlign layer, and choose a 4\u00d74 spatial grid for computing Gaussians. Implementation details, ablation of key components, and extra experiments are given in Section E of Appendix.\nWe compare with existing methods in two set-tings, as shown in Table 8. In RN101\u2192RN18 setting, the teacher is ResNet101 and the student is ResNet18; in RN50\u2192MNV2, the teacher and student are ResNet50 and MobileNetV2 [58], respectively. For logit distillation, our WKD-L significantly outperforms the classical KD [2] and is slightly better than DKD [3]. For feature distillation, we compare with FitNet, FGFI [50], ICD [51] and ReviewKD [29]; our WKD-F im-proves ReviewKD, the previous top feature dis-tillation performer, by a non-trivial margin in both settings. Finally, by combining WKD-L and WKD-F, we achieve performance better than DKD+ReviewKD [3]. When additional bounding-box regression is used for knowl-edge transfer, our WKD-L+WKD-F improves further, outperforming previous state-of-the-art FCFD [8]."}, {"title": "5 Conclusion", "content": "The Wasserstein distance (WD) has shown evident advantages over KL-Div in several fields such as generative models [11]. However, in knowledge distillation, KL-Div is still dominant and it is unclear whether WD will outperform. We argue that earlier attempts on knowledge distillation based on WD fail to unleash the potential of this metric. Hence, we propose a novel methodology of WD-based knowledge distillation, which can transfer knowledge from both logits and features. Extensive experiments have demonstrated that discrete WD is a very promising alternative of predominant KL-Div in logit distillation, and that continuous WD can achieve compelling performance for transferring knowledge from intermediate layers. Nevertheless, our methods have limitations. Specifically, WKD-L is more expensive than KL-Div based logit distillation methods, while WKD-F assumes features follow Gaussian distribution. We refer to Section F in Appendix for detailed discussion on limitations and future research. Finally, we hope our work can shed light on the promise of WD and inspire further interest in this metric in knowledge distillation."}, {"title": "G Broader Impact", "content": "We address limitations of Kullback-Leibler Divergence for knowledge distillation (KD), and can obtain stronger, lightweight models suitable for resource-limited devices. Our methodology is very promising, readily applicable to a variety of visual tasks, e.g., image classification and object detection. We hope our work sheds light on importance of WD and inspires future exploration of it in the field of knowledge distillation.\nWith breakthroughs of large-scale pre-training, multimodal large language models (LLMs) like GPT-4 [83] have excelled in many visual tasks. Our methodology can be potentially applied to transfer knowledge from LLMs to smaller ones for specific visual or language tasks, allowing for improved performance while preserving fast inference cost. Consequently, researchers as well as practitioners can more easily benefit from advanced technologies of LLMs, facilitating their widespread use and broader accessibility.\nCurrently, the theoretical quest for KD is limited. Due to the black-box nature of the distillation process, the student distilled using our methods may inevitably inherit harmful biases from the teacher [84]. Moreover, the reduction in deployment cost may lead to more potential harms of model abuse. This highlights the necessity for more widespread efforts to regulate the use of artificial intelligence techniques including knowledge distillation."}, {"title": "A Implementation Details on WKD", "content": "A.1 Interrelations (IRs) among Category for WKD-L\nVisualization of category interrelations. We select in random fifty images from each of one hundred categories randomly chosen on the training set of ImageNet. Then we feed them to a pre-trained ResNet50 model and extract from the penultimate layer features that are projected to 2D space using t-SNE. The 2D embedding points are shown in Figure 4a where different categories are indicated by different colors. For intuitive understanding, inspired by karpathy\u00b9, we display features by the corresponding images at their nearest 2D embedding locations in Figure 4b. It can be seen that the categories exhibit complex topological relations (distances) in the feature space, e.g., mammal species are nearer each other while far from artifact or food. The relations encode abundant information and are beneficial for knowledge distillation. In addition, the features of the same category cluster and form a (unknown) distribution that often overlaps with those of neighboring categories. The observation suggests that it is more desirable to model the interrelations with statistical method.\nQuantization of IRs with CKA. We propose to use CKA for modeling category IRs as it can effectively characterize similarity of deep representations [17]. CKA is normalized HSIC [18] that measures statistical dependence of random variables (features) by mapping them into a RKHS with some positive definite kernels. Recall that, for category Ci, we have a matrix $X_i \\in \\mathbb{R}^{u \\times b}$ of u-dimensional features of b training images. As such, for the commonly used kernels, we have linear kernel $K_{lin} = X_i X_i^T$, the polynomial kernel $K_{poly} = (X_i^T X_i+1)^k$ where k \u2208 {2, 3, 4}, and RBF kernel $K_{rbf} = exp(-\\frac{d}{2\\sigma^2}) , where the bandwidth \\sigma \\in \\{0.2, 0.4, 0.6\\}$ and $D_i = 2(diag(X_iX_i^T)11^T)_{sym}-2X_iX_i^T$. For a matrix A, Med(A) denotes the median of all of its entries, diag(A) denotes a vector formed by its diagonals, and $(A)_{sym} = \\frac{1}{2}(A+A^T)$.\nQuantization of IRs with cosine similarity. Besides CKA, cosine similarity between the prototypes of two categories is used to quantify category interrelations. The prototype of a category can be naturally computed as feature centroid of this category's training examples, i.e., $\\bar X_i = \\frac{1}{b}X_1$. Alternatively, the weight vectors associated with the softmax classifier can be used as prototypes [22]. Specifically, if the weight matrix of the last FC layer is $W \\in \\mathbb{R}^{u \\times n}$ where n is number of total classes, then its i-th column wi can be regarded as the prototype of category $C_i$, i.e., $\\bar X_i = W_i$."}, {"title": "A.2 Distributions Modeling for WKD-F", "content": "Recall that, for an input image, we have 3D feature maps output by some layer of a DNN, whose spatial height, width and channel number are h, w and l, respectively. We reshape the feature maps to a matrix $F \\in \\mathbb{R}^{l \\times m}$ where $m=h \\times w$; we denote the i-th column as a feature $f_i \\in \\mathbb{R}^l$, while the j-th row (after transpose) as a feature $f_j \\in \\mathbb{R}^m$. In WKD-F, we estimate channel 1st-moment $\\mu\\in \\mathbb{R}^l$ and 2nd-moment $\\Sigma \\in \\mathbb{R}^{l \\times l}$:\n$\\mu=\\frac{1}{m} \\sum f_i , \\Sigma=\\frac{1}{m} \\sum (f_i-\\mu)(f_i-\\mu)^T$, (10)\nwhich are used to construct a parametric Gaussian $\\mathcal{N} (\\mu, \\Sigma)$. For measuring difference between Gaussians, we use Wasserstein distance (WD) that is a Riemannian metric. We prefer Gaussians (Diag) that have diagonal covariances to Gaussians (Full) that have full covariances, as they are much more efficient and have better performance as shown in Table 3a.\nG2DeNet. Different from WD, Wang et al. [33] propose a method called G2DeNet, which considers Lie group of the space of Gaussians and embeds a Gaussian into the space of SPD matrices:\n$\\mathcal{N}(\\mu, \\Sigma) \\mapsto \\left[\\begin{array}{cc} \\Sigma + \\mu\\mu^T & \\mu \\\\ \\mu^T & \\frac{1}{l} \\end{array}\\right]$, (11)\nThus the difference between two Gaussians is defined as the Euclidean distance of the Gaussian embeddings of the teacher and student models.\nICKD-C and NST. ICKD-C [6] uses raw channel 2nd-moment for exploring inter-channel correla-tions of features, i.e., $M = \\frac{1}{m} \\sum f_i f_i^T$. Instead of channel-wise statistics, NST [35] estimates raw spatial 1st-moment $\\bar\\mu \\in \\mathbb{R}^m$ and raw spatial 2nd-moment $M \\in \\mathbb{R}^{m \\times m}$ for describing distributions:\n$\\bar \\mu= \\frac{1}{m} \\sum f_j , M = \\frac{1}{m} \\sum f_j f_j^T$. (12)\nKL-Div between Gaussians (Diag). Let $\\mu = [\\mu_1,..., \\mu_l ]^T$ be mean and $\\delta = [\\delta_1,..., \\delta_l ]^T$ be variance of Gaussians (Diag). In this case, both KL-Div and symmetric KL-Div have closed forms:\n$D_{KL}(N_T || N_S) = \\sum (\\frac{\\delta_i^t}{\\delta_i^s} + (\\frac{\\mu_i^t - \\mu_i^s}{\\delta_i^s})^2 -1 - log \\frac{\\delta_i^t}{\\delta_i^s})$, (13)\n$D_{Sym KL}(N_T || N_S) = \\sum ( (\\frac{\\delta_i^t}{\\delta_i^s} + \\frac{\\delta_i^s}{\\delta_i^t} ) + ((\\frac{\\mu_i^t - \\mu_i^s}{\\delta_i^s})^2 + (\\frac{\\mu_i^s - \\mu_i^t}{\\delta_i^t})^2 )-2 )$.\nHere T and S denote the teacher and student, respectively.\nKL-Div between Laplace distributions. We assume components of feature $f = [f_1,\\dots, f_l]^T \\in \\mathbb{R}^l$ are statistically independent. Then the Laplace distribution of f is $L(\\mu, \\nu) = \\prod \\frac{1}{2\\nu_i} exp(-\\frac{|f_i - \\mu_i|}{\\nu_i} ) , where \u00b5 is the mean and $\\nu = [\\nu_1,\\dots, \\nu_l]^T$ is the scale parameter. The KL-Div takes the following form [53]:\n$D_{KL}(L_t || L_s) = \\sum log \\frac{\\nu_i^s}{\\nu_i^t} + \\frac{\\nu_i^t}{\\nu_i^s} + \\frac{(\\mu_i^t - \\mu_i^s)^2}{2(\\nu_i^s)^2} + exp(-\\frac{|\\mu_i^t - \\mu_i^s|}{\\nu_i^s}) -1$. (14)\nKL-Div between exponential distributions. Under independence assumption, the exponential distribution of feature f is $E(\\beta) = \\prod_i \\beta_i exp(-\\beta_i f_i)$, where $\\beta = [\\beta_1,\\dots, \\beta_l ]^T$ is the rate parameter. For exponential distributions, the KL-Div can be written as [53]:\n$D_{KL}(E_t || E_s) = \\sum log \\frac{\\beta_i^s}{\\beta_i^t} + \\frac{\\beta_i^t}{\\beta_i^s} -1$. (15)\nNon-parametric PMD. Though non-parametric methods, e.g., histogram or kernel density esti-mation, are infeasible due to curse of dimensionality, we can still use probability mass function (PMF) for distribution modeling. Specifically, given a set of features $f_i, i = 1, ..., m$, the PMF of the features is of the form:\n$P_f = \\prod P(f_i), P(f_i) = \\frac{1}{m}$, (16)"}, {"title": "B Computational Complexity of WKD", "content": "The logit-based WKD-L is formulated as an entropy regularized linear programming, which can be solved fastly by Sinkhorn algorithm [23]. Let n be the dimension of the predicted logits, the complexity of WKD-L can be written as $O(Dn^2 log n)$ [70]. Here D = $||C||_{\\infty}^\\frac{3}{\\epsilon}$ is a constant, where $||C||_{\\infty}$ indicates the infinity norm of the transportation cost matrix C = $[C_{ij}]$, and $\\epsilon > 0$ indicates a prescribed error. In contrast, the computational complexity of KL-Div is O(n). Despite its high complexity, WKD-L can be computed efficiently as Sinkhorn algorithm is highly suitable for parallel computation on GPU [23]. For feature-based WKD-F, the dominant cost lies in computation of means and variances. Given a set of m features $f_i$ of l-dimension, the means can be computed by global average pooling that takes O(ml) time; the complexity of variances is also O(ml), as it can be obtained by element-wise square operations followed by global average pooling."}, {"title": "C Extra Experiment on Image Classification", "content": "C.1 More Ablation Analysis of WKD\nWe adopt the setting (a) for ablation on ImageNet, in which the teacher is ResNet34 and the student is ResNet18.\nHyper-parameters of WKD-L. The total loss function of WKD-L consists of the cross-entropy loss $L_{CE}$, the target loss $L_t$ and WD-based logit loss $L_{WKD-L}$. Following [3; 4], we set the weights of the two former losses to 1 across the paper. As such, our hyper-parameters includes the temperature \u03c4, the weight \u03bb of $L_{WKD-L}$, the sharpening parameter \u043a that controls smoothness of IRs for transport cost in WD, and the regularization parameter \u03b7 (set to 0.05) in discrete WD. As simultaneous optimization of them is computationally infeasible, we first analyze the effect of temperature by"}, {"title": "C.2 Summary of Hyper-parameters on ImageNet", "content": "The setting (a) involves homogeneous architecture, where the teacher and student networks are ResNet34 and ResNet18, respectively. For WKD-L, the weights of $L_{CE}$, $L_t$ and $L_{WKD-L}$ are 1, 1 and 30, respectively; the temperature \u03c4 = 2 and sharping parameter \u043a = 1. For WKD-F, we set the weights of $L_{CE}$ and $L_{WKD-F}$ to 1 and 2e-2, respectively. We adopt features from Conv5_x and Gaussian (Diag) with mean-cov ratio \u03b3=2. The setting (b) concerns heterogeneous architecture, in which the teacher is ResNet50 and the student is MobileNetV1. In this setting, the weight of $L_{WKD-L}$ is set to 25 while that of $L_{WKD-F}$ is 1e-3, and the remaining hyper-parameters are identical to those in the setting (a)."}, {"title": "C.3 More Experiments on Combination of WKD-L or WKD-F", "content": "How WKD complements other KD methods? We combine WKD-F with a state-of-the-art logit-based knowledge distillation (i.e., NKD), and combine WKD-L with a state-of-the-art feature-based method (i.e., ReviewKD). The results are shown in Table 9a and Table 9b, respectively. We can see that NKD+WKD-F improves over individual NKD and WKD-F, which suggests our WKD-F is complementary to NKD. Notably, NKD+WKD-F is slightly inferior to WKD-L+WKD-F. In addition, WKD-L+ReviewKD improves over ReviewKD but underperforms WKD-L. We conjecture that, due to the large gap (~ 0.9%) between ReviewKD and WKD-L, the combination hurts WKD-L.\nWill separation of target probability in WKD-L still help when combined with WKD-F? To answer this question, we integrate WKD-L without separation into WKD-F, which is called WKD-L (w/o)+WKD-F. From Table 9c, we can see that it is slightly inferior to WKD-L (w/)+WKD-F in which the separation scheme is used in WKD-L. This suggests that the benefit due to the separation scheme decreases but still persists in the logit+feature approach."}, {"title": "D Visualization", "content": "D.1 Visualization of Teacher-Student Discrepancies\nFollowing [25; 3", "42": ".", "2": "which indicates WKD-L produces correlations matrices more similar to the teacher than KD. As the differences of correlation matrices capture inter-class correlations ["}]}