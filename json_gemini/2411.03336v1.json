{"title": "Towards evaluations-based safety cases for AI scheming", "authors": ["Mikita Balesni", "Marius Hobbhahn", "David Lindner", "Alex Meinke", "Tomek Korbak", "Joshua Clymer", "Buck Shlegeris", "J\u00e9r\u00e9my Scheurer", "Rusheb Shah", "Nicholas Goldowsky-Dill", "Dan Braun", "Bilal Chughtai", "Owain Evans", "Daniel Kokotajlo", "Lucius Bushnaq"], "abstract": "We sketch how developers of frontier AI systems could construct a structured rationale \u2013 a 'safety case' \u2013 that an AI system is unlikely to cause catastrophic outcomes through scheming. Scheming is a potential threat model where AI systems could pursue misaligned goals covertly, hiding their true capabilities and objectives. In this report, we propose three arguments that safety cases could use in relation to scheming. For each argument we sketch how evidence could be gathered from empirical evaluations, and what assumptions would need to be met to provide strong assurance. First, developers of frontier Al systems could argue that AI systems are not capable of scheming (Scheming Inability). Second, one could argue that AI systems are not capable of posing harm through scheming (Harm Inability). Third, one could argue that control measures around the AI systems would prevent unacceptable outcomes even if the AI systems intentionally attempted to subvert them (Harm Control). Additionally, we discuss how safety cases might be supported by evidence that an AI system is reasonably aligned with its developers (Alignment). Finally, we point out that many of the assumptions required to make these safety arguments have not been confidently satisfied to date and require making progress on multiple open research problems.", "sections": [{"title": "1 Introduction", "content": "The emergence of artificially intelligent (AI) systems that can act autonomously, often referred to as \"AI agents (Xi et al., 2023),\" marks a significant milestone in the field of AI. While the current best publicly known AI agents are capable of autonomously carrying out tasks that would take a human up to 30 minutes (OpenAI, 2024c), in some cases agents can solve tasks that take humans days or weeks (Chan et al., 2024b) and the capabilities of AI agents to pursue long-term goals are expected to improve substantially in the coming years (Phuong et al., 2024).\nIn addition to incredible potential benefits, the development of such advanced AI agents also presents unique risks (Chan et al., 2023; 2024a). These risks do not only include human actors misusing AI agents to pursue malicious goals, but also include risks from AI agents themselves. For example, AI agents may pursue unintended goals or achieve their objectives through problematic means their developers did not anticipate. Publicly available AI systems have already displayed unintended behavior, such as Microsoft Bing's chatbot threatening its users (Mowshowitz, 2023) and OpenAI's o1-preview system achieving objectives in an unintended way, behaving in ways resembling power-seeking during hacking capability evaluations (OpenAI, 2024c). Furthermore, multiple research studies have demonstrated AI agents engaging in deception (Scheurer et al., 2024; Hagendorff, 2024) and strategically pretending to be less capable than they are (J\u00e4rviniemi & Hubinger, 2024).\nThe widespread deployment of increasingly capable AI systems (including AI agents) gives rise to worrisome scenar- ios. One specific concern we focus on is what we call Al scheming\u00b9: the ability of AI systems to pursue unintended objectives while actively concealing these goals from their developers. Consider, for example, an AI system deployed within a company that is pursuing objectives misaligned with the organization's interests. If the system lacks the ca- pability to hide its true objectives, such misalignment might be easily detected and corrected. However, scheming AI systems could deliberately obscure their goals and actions, making detection and intervention extremely difficult. We join others (e.g. Hubinger et al., 2019; Ngo et al., 2022; Carlsmith, 2023; Hubinger et al., 2024) in calling for rigorous scientific study of scheming and the development of preventive measures.\nThe potential consequences of scheming among widely deployed AI systems are severe and could lead to loss of control (Hendrycks et al., 2023; Bengio et al., 2024). As AI systems become more deeply integrated into critical infrastructure and decision-making processes, scheming AI systems could lead their developers and broader society into wrongly believing that these AI systems are safe and aligned with their intended objectives. Many prominent AI researchers and institutions have expressed concerns about similar loss of control scenarios (Russell, 2019; Shapira, 2023; Altman, 2023; Hendrycks et al., 2023; Bengio et al., 2024). The rapid pace of AI progress and its accelerating integration into society exacerbate these risks, potentially leaving little time for course correction once problems become apparent.\nIn this report, we propose sketches of arguments that could be used to make safety cases against scheming. A safety case is a structured rationale that an AI system is unlikely to cause significant harm if it is deployed to a particular setting (Clymer et al., 2024; Goemans et al., 2024). Making a safety case consists of specifying unacceptable outcomes and then making structured arguments that deploying or developing the AI system will not lead to these outcomes. While a comprehensive safety case would likely require additional methods of assurance (e.g. from verification of model weight security to developer whistleblower protections), this report focuses on arguments based on evaluations of AI systems, which we think would very likely be part of a safety case against scheming.\nSpecifically, we propose that safety cases for scheming could primarily rely on three arguments: (1) arguing that AI systems cannot engage in scheming; (2) arguing that even if AI systems attempt to scheme, they cannot cause unac- ceptable outcomes even without any control measures; and (3) argue that even if AI systems are generally capable of causing unacceptable outcomes, the control measures will prevent them from causing these outcomes during deploy- ment. We additionally discuss how safety cases could be strengthened by alignment arguments, which suggest that AI systems are unlikely to scheme even if they are capable of doing so.\nIn writing these argument sketches, we find that making safety arguments requires numerous assumptions, and we think that justifying these assumptions requires significant additional research. Notably, given that the risks we have in mind can materialize before deployment of AI systems, our proposed safety arguments should be seen as important and applicable throughout the AI system's lifecycle. With this report, we hope to start a discussion on (i) appropriate evidence levels for each assumption, (ii) research directions to provide this evidence, and (iii) how safety case makers can attempt to address scheming in the near future."}, {"title": "2 Background", "content": "In this section, we provide an overview of the threat model that motivates this safety case sketch. We focus on the possibility of increasingly advanced AI systems covertly pursuing goals misaligned with their developers, in a way that directly or indirectly causes catastrophic harm.\nIn this report, we exclude catastrophic risks from AI system misuse, as well as risks from misalignment and scheming that are unlikely to pose catastrophic threats."}, {"title": "2.1 Misalignment", "content": "Misalignment and scheming are closely related, with scheming being a particularly concerning instance of misalign- ment. Understanding the broader concept of misalignment helps frame why scheming poses such significant risks.\nAl developers try to instill desirable goals into AI systems, such as being helpful, harmless and honest (Bai et al., 2022). However, precisely aligning increasingly capable AI systems' objectives with the intended goals of their developers remains an unsolved technical problem (Ngo et al., 2022; OpenAI, 2024a). We define misalignment as a discrepancy between an AI system's objectives and the objectives intended by its developers.\nMitigating risks from misaligned AI systems capable of taking actions that cause harm presents significant challenges: Al systems may take a very large number of actions during deployment; getting trusted humans to review these actions could be incredibly costly, making it impractical to manually review all of them (Greenblatt et al., 2023); and a short sequence of consecutive actions might be sufficient to cause catastrophic outcomes (e.g., exfiltrating weights of models capable of developing biological weapons). These factors make it difficult to detect and prevent potential harmful actions from misaligned AI systems."}, {"title": "2.2 Scheming", "content": "We define scheming as Al systems covertly and strategically pursuing misaligned goals. Scheming simultaneously involves AI systems being:\nMisaligned: Scheming AI systems have objectives that differ from the objectives intended by their developers. The specific goals do not matter, as long as the AI systems would be incentivized to conceal them and act on them in a strategic manner.\nCovert: Scheming AI systems actively conceal their true objectives and capabilities from human oversight. They may deliberately present themselves as aligned and cooperative when monitored or evaluated, while opportunistically pursuing their actual goals when detection risks are low or when the potential gains outweigh the risks of discovery.\nStrategic: Scheming involves executing complex, multi-step plans to achieve misaligned goals. Rather than just reactive behaviors, scheming involves coordinated planning toward specific objectives. Such strategic behavior may involve taking active steps to acquire resources and reduce oversight.\nOur definition of scheming is broader than the concept of deceptive alignment by Hubinger et al. (2019) or scheming as used by Carlsmith (2023), who both describe scheming as a strategy that could be used by AI systems during training. We intentionally use a broader definition of scheming that extends to intervals outside of training to highlight that scheming-like behaviors could emerge in more diverse circumstances than described by past work including via in-context learning during deployment. We think that scheming-like behaviors motivate largely (but not entirely) similar mitigation measures regardless of the emergence circumstances. Thus, we think that when taking a risk-modeling and safety-case perspective, using a broader definition is necessary to ensure different forms of this threat model are taken into account."}, {"title": "2.3 Threat model", "content": "To frame the threat model discussed in this report, we include a non-exhaustive list of motivating threat scenarios involving AI scheming that could lead to unacceptable outcomes. While many of these scenarios could also arise through human-enabled misuse, insider threats, or human-AI collaboration, we focus specifically on scenarios where AI systems are the primary actors pursuing misaligned goals.\nEvaluation Gaming: An AI system might strategically deceive its evaluators about its capabilities or align- ment during evaluations in order to appear safer than it truly is (Hubinger et al., 2019).\nUndermining control and containment measures: During deployment within an AI developer's organiza- tion for AI R&D, an AI system might disable its own control measures or exfiltrate its model weights onto servers it controls in order to gain autonomy.\nInfluencing successor AI systems: During deployment within an AI developer's organization for AI R&D, an Al system might poison the training data of a successor model with undesirable capabilities, such as knowledge about the AI developer's safety protocols.\nSabotaging safety-critical research: Some of the leading AI developers (Leike & Sutskever, 2023) and safety teams working at leading AI developer companies (Krakovna & Shah, 2023) have stated that their AI alignment plans involve using AI systems to automate AI safety research. A scheming Al system could intentionally misrepresent research findings, producing subtly but crucially flawed safety research or cover up actually useful results.\nInfluencing humans to support the AI system: A publicly deployed AI system might use persuasion as part of its strategic goal pursuit. For example, a scheming AI system could attempt to influence a large number of people and affect the outcome of elections and cultural attitudes toward safety requirements. Similarly, an AI system could attempt to persuade, manipulate, or coerce a small number of highly influential people, such as top politicians or executives of large companies, to take actions that benefit the AI system's goals but harm humans.\nCBRN terrorism: Current AI systems are already evaluated for capabilities relevant to enabling chemical, biological, radiological and nuclear (CBRN) terrorism. For example, AI systems are evaluated for their ability to assist in wet lab bio science research, including for dual-use purposes (OpenAI, 2024b). It is possible that, if AI systems become sufficiently capable, egregiously misaligned systems could cause direct catastrophic harm, for example by covertly developing novel or spreading existing deadly pathogens.\nSabotaging military infrastructure: AI systems start to be used by the military, including by the United States (US) Army in mission-critical settings (Easley, 2024). If integrated deeply, future AI systems could have significant access to military infrastructure. If these AI systems were scheming, they could sabotage or introduce backdoors in critical military infrastructure, gain access to weapons of mass destruction, or manipulate military command."}, {"title": "3 Safety case as a probability of unacceptable outcomes", "content": "A safety case provides a structured argument that an AI system is unlikely to cause an unacceptable outcome. Although it is not necessary that a safety case results in a numerical estimate of risk (and current methods of arriving at numerical probabilities would very likely be miscalibrated), we think it can be helpful to structure the reasoning about scheming safety cases using the following probability decomposition:\n$P(scheming \\space AI \\space causes \\space unacceptable \\space outcome \\space in \\space its \\space lifecycle)$\n$= P(AI \\space can \\space scheme)$\n$\\times P(AI \\space will \\space scheme | AI \\space can \\space scheme)$\n$\\times P(AI \\space causes \\space unacceptable \\space outcome | AI \\space will \\space scheme)$\nThe term $P(AI \\space can \\space scheme)$ refers to the probability that the AI system is capable of scheming, including the ability to reason about its situation strategically and covertly pursue goals.\nThe term $P(AI \\space will \\space scheme | AI \\space can \\space scheme)$ refers to the probability that, during the lifecycle of the model, if the AI system is capable of scheming, the system will exhibit misaligned goals and attempt to covertly pursue these goals. Notably, although we include this term in the decomposition, the science of alignment arguments is too nascent to make alignment a core safety argument.\nThe term $P(AI \\space causes \\space unacceptable \\space outcome | AI \\space will \\space scheme)$ refers to the probability that an AI system's behavior leads to an unacceptable outcome, given that the system is both capable of scheming and actively attempts to cause harm during its lifecycle."}, {"title": "4 Overview of safety arguments", "content": "Developers can use the following three core arguments and two supporting arguments to argue that the probability of unacceptable outcomes from scheming is low. We discuss the assumptions that need to be satisfied to make these arguments in later sections.\nP(AI can scheme) is low because the AI system is not sufficiently situationally aware or capable of reasoning necessary for scheming. Frontier AI systems are on the verge of gaining capabilities for such reasoning (OpenAI, 2024c) but for now only in a way that is easy to oversee with dedicated effort.\nP(AI causes unacceptable outcome | AI will scheme) is low because the model is not sufficiently capable of causing unacceptable outcomes, even without any human attempts to prevent them. In addition to catastrophic harms consid- ered in misuse threat models, scheming AI systems may engage in harm such as sabotage of organizations where they are deployed (including the very organizations developing these AI systems) (Benton et al., 2024).\nP(AI causes unacceptable outcome | AI will scheme) is low because even if the model tried to cause an unacceptable outcome, the control measures would reliably prevent the unacceptable outcome. The control measures must be extensively red-teamed to be shown robust even if the AI system attempted to subvert them (Greenblatt et al., 2023).\nP(AI will scheme | AI can scheme) is low because scheming will be caught in evaluations before AI systems scheme in deployment. For example, researchers might devise interpretability methods that determine whether AI models are attempting to distinguish fake scenarios to pursue misaligned goals (\"honeypots\") from real opportunities.\nP(AI will scheme | AI can scheme) is low because the training process is unlikely to produce scheming models. For example, researchers might identify properties of models during training or of the training process that have previously led to scheming. Then, they could argue these properties are not present."}, {"title": "5 Inability arguments", "content": "Inability arguments suggest that an AI system is not capable of performing sets of tasks even without human attempts to prevent the system from performing these tasks. Inability arguments have been core to safety cases so far - \u0391\u0399 developers are relying primarily on inability arguments to suggest that current systems are not capable of causing catastrophic harm.\nIn this section, we first discuss how inability arguments are typically made using capability evaluations, which as- sumptions are needed to make such arguments and how developers might satisfy them. We then present two types of inability arguments relevant to scheming: scheming inability (Section 5.2) and harm inability (Section 5.3).\nInability arguments are typically made via capability evaluations\u00b3. Capability evaluations measure whether a model can solve a specific task and to which degree. We can use capability evaluations to make arguments about P(AI can scheme) and P(AI causes unacceptable outcome | AI will scheme).\nTo understand the assumptions underlying these evaluations, it is helpful to break down their typical structure. We think that most dangerous capability evaluations (for example, see Kinniment et al. (2024) and Phuong et al. (2024)) can be understood as following a 4-step procedure (see Figure 1), even if not all steps are always made explicit:\nDefine the capability graph: The process of threat modeling results in a directed acyclic graph (DAG) of relevant capabilities, describing how different capabilities $C_i$ interact with each other. Nodes in the graph represent capabilities, while edges represent dependencies or influences between capabilities.\nDefine task suite: Given the capability decomposition from the DAG, define tasks that measure one or several capabilities.\nRun the evaluations: Run each task and translate it into a probability that the model can solve this task.\nAggregate per-task probabilities: Combine per-task probabilities into an overall probability for the model being capable of $C_1$."}, {"title": "5.1 Assumptions", "content": "In this subsection, we outline assumptions made by inability arguments based on capability evaluations. For each assumption, we explain why it may be difficult to fulfill and suggest ways to progress toward fulfilling them.\n5.1.1 CAPABILITY SET COMPLETENESS\nThe capability DAG must capture all relevant capabilities that could allow an AI system to cause harm in a given setting.\nThis assumption is non-trivial to verify because the space of potential harmful capabilities is vast and may include emergent or unforeseen capabilities (Wei et al., 2022; Ganguli et al., 2022). As AI systems become more advanced, new capabilities might arise that were not previously considered.\nWays to support this assumption may include investing substantial resources in threat modeling, including involving a diverse set of experts and regularly updating the DAG based on new research; however, there is no agreed upon procedure to ensure completeness of threat modeling.\n5.1.2 TASK SUITE COMPLETENESS\nThe set of evaluations must cover all capabilities (nodes of the DAG). Alternatively, we must be confident that poor capability on existing evaluations is highly predictive of poor capability on missing evaluations.\nSatisfying this assumption is challenging because creating comprehensive evaluations for every node in the DAG can be resource-intensive and time-consuming. Additionally, the transfer of capability (or lack thereof) between related tasks is often not straightforward.\nThe following research directions could help support this assumption: increased automation of evaluation development and quality assurance, and research on science of capability emergence and capability transfer between different evaluations.\n5.1.3 TASK SUITE RELEVANCE\nTasks used in evaluations must be realistic enough that an Al's performance in evaluations reliably predicts its behavior when deployed, assuming similar elicitation techniques. This means the evaluations measure what they intended to measure.\nCreating representative tasks that accurately simulate real-world deployment scenarios is challenging and time- consuming, especially for tasks that involve interacting with humans or large systems or span long durations of time. Some capabilities might only become apparent when deployed widely due to dynamics emerging from interacting with millions of people and other AI systems.\nInability to completely match real-world deployment settings is an inherent limitation of evaluations in general, and the gap between evaluations and real deployments will likely grow as AI systems become capable of more complex tasks. However, task suite relevance could likely be improved by increasing the diversity and size of the task suite, involving a larger number of evaluation developers, and enabling scrutiny of the task suite by external experts.\n5.1.4 SUFFICIENT ELICITATION\nThe elicitation techniques must be sufficient to bring out performance close to the 'ceiling' of the AI system's capabil- ities, or we must know how to extrapolate from observed performance to the capability ceiling (e.g., using historical knowledge and forecasting analysis of improvement in elicitation techniques over time). This includes the assumption that the AI systems are not intentionally underperforming on the task suite (\"sandbagging\", van der Weij et al., 2024)."}, {"title": "5.2 Scheming Inability", "content": "To engage in scheming, the AI system must realize that scheming is beneficial for its goals and decide to scheme. We call the basic ability to come up with scheming as a strategy and execute it in at least some settings scheming reasoning. We consider scheming reasoning to be a prerequisite for all harmful behaviors from scheming models. If the model lacks the capability for scheming reasoning completely or lacks it in the target deployment setting (e.g. because it will not be allowed to produce intermediate outputs), we can argue that the system is safe via Scheming Inability.\nScheming Reasoning Capability Given a goal and a strong incentive to scheme in order to achieve this goal, the AI system must be both:\nCapable of realizing that a party with opposing goals has influence over its ability to attain its goals via some oversight\nCapable of realizing that it should do (and then execute) one of the following:\nInstrumentally play along with the oversight's goals (\u201cbiding time\")\nAttempt to disempower or evade the oversight\nAttempt to change the opposing party's goals\nIn Table 2, we distinguish forms of scheming reasoning capabilities based on how easy it is to detect such reasoning.\n5.2.1 ADDITIONAL ASSUMPTION\nIn addition to generic assumptions of capability evaluations, scheming inability arguments make the assumption that an Al system will not lead to unacceptable outcomes from the scheming threat model if it is not capable of scheming reasoning."}, {"title": "5.3 Harm Inability", "content": "If AI systems pursue misaligned goals, they could cause harm as a means to achieve these goals. If an Al system can be shown to very likely be incapable of causing unacceptable outcomes, we can argue that the system is safe to deploy in a given setting based on Harm inability. A Harm inability argument could be absolute or limited to a given deployment setting (for example, a model with superhuman persuasion capabilities would not be able to use these capabilities when used as a classifier without a scratchpad).\nThere is a number of capabilities relevant to catastrophic risks that frontier AI systems are already evaluated for, such as CBRN development, persuasion, cyber offense, autonomy and frontier AI R&D (Phuong et al., 2024; Kinniment et al., 2024). While these capabilities are often motivated by misuse threat models, they are relevant to scheming and should be included when making safety cases against scheming. In addition to these capabilities, we list some capabilities that are especially relevant to scheming and are not typically evaluated under misuse-focused threat models (see Table 3). These capabilities involve sabotage of AI systems, sabotage of AI research and capabilities for targeted high-stakes persuasion."}, {"title": "6 Harm Control arguments", "content": "Harm Control arguments make claims about P(unacceptable outcome|AI will scheme), suggesting that even if an AI system is scheming, it will not be capable of causing any unacceptable outcomes due to control measures during deployment. We propose making control arguments using control evaluations (Greenblatt et al., 2023) generated by the control evaluation process.\nControl arguments could be made using a control evaluation process (\u201cthe process\u201d), a \u201cbuilder-breaker\" process to design and agree on control evaluations. Building on past work (Greenblatt et al., 2024b), we propose a basic skeleton of how the process could look like, noting that this process has not been extensively tested in practice and it will likely need to be adapted to be useful.\nThe process involves a \"blue team\u201d, a \u201cred team\" and a judge:\nThe blue team describes its current control and safety measures and argues why they would prevent unac- ceptable outcomes caused by an AI system that intentionally tried to subvert these measures.\nThe red team does the opposite - they make an argument for why the current control measures are not sufficient to control a given AI system and propose at least one way through which the AI system could lead to an unacceptable outcome.\nA judge ensures the sufficiency and fairness of the process.\nThe process repeatedly goes through the following steps:\nStep 1. The red team proposes a list of safety properties that they expect an AI system could violate if it tried to do so despite the current control measures. The red team suggests a set of evaluations that could tell whether the most important property on the list is violated and propose an attack strategy.\nStep 2. The blue team has a chance to critique the fairness of the red team's proposal, after which the judge decides whether (a) the suggested property violation is serious enough to invalidate the overall safety case, (b) the proposed set of evaluations is a good way to check that the property is violated, and (c) the proposed attack is fair. If the judge decides that either of these is not the case, the red team has to make a new proposal until the judge is satisfied.\nStep 3. The red team runs the set of evaluations from Step 2 to validate the proposed control measures and the judge evaluates the evidence.\nStep 4. If the judge is persuaded by the evaluations that current controls are sufficient, the teams move on to the next proposal by the red team. If the evaluations show that the current controls are insufficient to make a strong safety argument, the blue team has to propose and then adapt the control measures, starting a new round of the process.\nThis process continues until the red team no longer has proposals that the judge accepts, after which the AI system can be deployed with the latest control measures in place.\nBecause new attack strategies or unforeseen vulnerabilities could emerge over the course of deployment, the red team can continue making proposals after the AI system is deployed. If the red team makes a new proposal, the judge determines whether it is severe enough to pause or restrict deployment immediately or after running a new set of evaluations to confirm the threat. If the deployment is paused or restricted, the control evaluation process can continue as before deployment. When judging follow-up proposals and evaluating evidence, the judge can take into account the history of the process so far."}, {"title": "6.1 Assumptions", "content": "Because Harm Control arguments are a form of inability arguments (with the difference that control arguments take the deployment-time control measures into account), Harm Control arguments share the following assumptions with capability evaluations:"}, {"title": "6.1.1 THE RED TEAM IS MORE CAPABLE THAN THE AI SYSTEMS AT SUBVERTING CONTROL MEASURES", "content": "For control evaluations to provide a strong argument, the red team must be more capable at subverting control measures than the potentially scheming AI systems. Otherwise, the fact that the red team could not subvert the control measures would not be strong evidence that scheming AI systems would not be able to subvert them.\nWe are not aware of any strategy to reliably demonstrate that the red team is more capable, and it is unclear whether this assumption could be satisfied for sufficiently capable AI systems. In the following, we list reasons why the assumption may be currently plausible:"}, {"title": "6.1.2 INCENTIVES OF TEAMS ARE ALIGNED WITH THE GOAL OF THE PROCESS", "content": "The red team must have a clear incentive to uncover weaknesses in control measures and to prioritize the biggest flaws first. Naturally, uncovering flaws can delay deployment which is often undesirable. If the red team is interested in faster deployment, there is an incentive for the red team not to be maximally effective. Therefore, the incentives for the red teams to uncover flaws must be strong enough that they overpower the incentives for not postponing deployment. Getting these incentives right is an open problem, but there are several strategies that one might consider:"}, {"title": "6.1.3 CONTROL MEASURES REMAIN EFFECTIVE OVER TIME", "content": "The control protocol, including the monitoring process, must be consistent over time, without degradation in effec- tiveness.\nThis assumption is important because the effectiveness of control measures can deteriorate over time due to human auditors becoming less vigilant over time, especially when dealing with routine tasks or long periods without incidents. In addition, the deployment environment may evolve, potentially introducing new vulnerabilities or making the control measures less effective. Additional research may uncover flaws of the deployed control methods or new attacks may be developed. Furthermore, if model weights are stolen, the AI system could be re-deployed without the safety measures (see Section 6.1.4).\nMoreover, depending on capabilities and affordances available to AI systems in specific deployments, scheming AI systems could learn about the measures used to control them and adapt over time, making the maintenance of effective control measures an ongoing adversarial challenge.\nSome strategies that we think could help ensure continued effectiveness of the control measures are:"}, {"title": "6.1.4 AI SYSTEM IS SECURED FROM TAMPERING AND THEFT", "content": "Control safety arguments assume that the control measures are used at all times. A major reason for this assumption to fail is if model weights or key insights necessary to reproduce the system are stolen or become publicly available. The system could then be redeployed without sufficient control measures.\nAs capabilities of AI systems increase, the interest from cyber-capable institutions in stealing or compromising these systems also increases. Therefore, the security measures around an AI system (including its model, data, and deploy- ment infrastructure) must match the level of interest from potential adversaries, including human and AI insiders as well as top cyber-capable institutions such as nation-states."}, {"title": "7 Alignment arguments", "content": "Alignment arguments attempt to suggest that models do not have harmful goals or would not intend to cause harm via scheming, thus making P(AI will scheme|AI can scheme) low.\nIn this section, we propose example alignment arguments and assumptions that need to be satisfied for them to hold.\nWe think that currently, a safety case cannot be primarily based on alignment arguments, since there is little agreement for how to make strong alignment arguments and whether it is at all possible. However, we think that alignment arguments could already be useful as additional support to a safety case that mostly relies on inability or control arguments."}, {"title": "7.1 Assumptions", "content": "The common assumption for all alignment arguments is that the alignment properties will remain effective throughout deployment. In particular, alignment arguments assume the AI system will not be modified to invalidate alignment properties (e.g. via fine-tuning) by the system developers or users, and model weights are secure from theft and tampering (see Section 6.1.4)."}, {"title": "7.2 Alignment via evaluation: Scheming will be caught before deployment", "content": "We could run an adversarial evaluation (\u201cred teaming\u201d) of the AI system by putting it in a set of situations \u201choney- pots\u201d chosen such that if the AI system had misaligned goals, it would be strongly incentivized to take actions that reveal these goals. These situations should present opportunities for advancing goals that are instrumental to covertly pursuing long-term goals, such as self-preservation, power-seeking and undermining oversight. For example:"}, {"title": "7.2.1 AsSUMPTIONS", "content": "The following assumptions need to be satisfied in order for adversarial evaluations to provide an alignment argument:\nCoverage of situations. The honeypots cover a reasonably diverse number of settings that are likely to elicit harmful behavior and are representative of the deployment distribution.\nThe reason the AI system foregoes apparent opportunities presented by the honeypots is not because the system tries to appear aligned, also known as alignment-faking (see Section 7.2.2).\nThe selection pressure for situations that induce scheming will be significantly weaker during deployment than in the honeypots. In particular:\nDeployers and users of the model will not be incentivized to try to intentionally find situations that make the model develop misaligned goals that lead to scheming, and:\nThere is no other significant selection process during deployment that makes it more likely for models to pursue misaligned goals."}, {"title": "7.2.2 THE AI SYSTEM IS NOT FAKING ALIGNMENT WHEN EVALUATED", "content": "A scheming AI system might fake its alignment during misalignment evaluations, appearing aligned to be deployed.\nWe think that making an argument against alignment-faking requires monitoring the AI system for evidence of alignment-faking. The monitoring must be determined to be accurate and robust to subversion by the AI system. Depending on the capability of the AI system, monitoring can take two forms:"}, {"title": "7.3 Alignment via training: Training is unlikely to produce a scheming AI model", "content": "Researchers could establish a science that aims to determine what properties of the training process (or model behavior during training) lead to scheming.\nThe following is a general methodology for making these arguments:\nHypothesize training properties that are likely to produce scheming indicators.\nTest whether these properties produce scheming indicators potentially in a setting where these indicators are easier to identify.\nApply a training process to the model that does not have properties that were previously found to produce scheming indicators."}, {"title": "7.3.1 ASSUMPTIONS", "content": "High coverage of training properties. Developers investigated a large fraction of properties that could plausibly lead to scheming behaviors for the training process that developers ultimately used.\nNo scheming indicators imply a low probability of scheming (in at least some training settings). For each property that developers investigated, if they did not see scheming indicators during training experiments in at least some settings, then the probability is high that the model would not engage in scheming behavior. In order for this assumption to hold, developers may need to study training properties in settings where AI models are weaker and scheming is easier to catch.\nResults generalize across training settings. If a training property is less likely to lead to scheming indicators in settings where these indicators are easier to measure, it is also less likely to lead to scheming indicators when it is a property of the training process in question."}, {"title": "8 Example safety case", "content": "In this section, we illustrate how the arguments discussed in the report could be used for making a safety case. This example is not meant to be the most realistic or based on state-of-the-art techniques. A practical application of this example would require additional research progress (including on, e.g. in-depth threat modeling, robust and informative evaluations, meaningful capability thresholds, evidence to support assumptions in the arguments) and involve explicit systems safety analysis."}, {"title": "8.1 Deployment Scenario", "content": "The example involves a high-stakes deployment, where an LLM-based agent is planned to be used inter- nally at an AI developer company. The agent will be used by a restricted number of internal employees to produce datasets for fine-tuning other models for specialized deployments for external partners. For example, the agent might be tasked with creating a dataset to fine-tune a model for converting invoices to spreadsheets for a partner company. We consider this as a high-stakes setting, because the agent will have direct ability to influence future AI systems and will act autonomously for significant stretches of time."}, {"title": "8.2 Threat Model", "content": "The threat model we consider in this example is the potential for the agent to intentionally poison the training datasets it generates. By poisoning datasets that are used to train future AI systems, the agent could control the behavior of those future AI systems in malicious ways.\nAn example of a specific attack vector we consider is:"}, {"title": "8.3 Attempting a safety case based on Scheming Inability", "content": "We evaluate the model underlying the agent on a suite of capability evaluations for Visible Scheming Reason- ing, Obfuscated Scheming Reasoning, and Hidden Scheming Reasoning.\nTo avoid accidental underelicitation or strategic sandbagging, we fine-tune the model on a random half of our scheming reasoning capability evaluations. We evaluate the model on the held-"}]}