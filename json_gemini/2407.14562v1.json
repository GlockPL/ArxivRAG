{"title": "THOUGHT-LIKE-PRO: Enhancing Reasoning of Large Language Models through Self-Driven Prolog-based Chain-of-Though", "authors": ["Xiaoyu Tan", "Yongxin Deng", "Xihe Qiu", "Weidi Xu", "Chao Qu", "Wei Chu", "Yinghui Xu", "Yuan Qi"], "abstract": "Large language models (LLMs) have shown exceptional performance as general-purpose assistants, excelling across a variety of reasoning tasks. This achievement represents a significant step toward achieving artificial general intelligence (AGI). Despite these advancements, the effectiveness of LLMs often hinges on the specific prompting strategies employed, and there remains a lack of a robust framework to facilitate learning and generalization across diverse reasoning tasks. To address these challenges, we introduce a novel learning framework, THOUGHT-LIKE-PRO. In this framework, we utilize imitation learning to imitate the Chain-of-Thought (CoT) process which is verified and translated from reasoning trajectories generated by a symbolic Prolog logic engine. This framework proceeds in a self-driven manner, that enables LLMs to formulate rules and statements from given instructions and leverage the symbolic Prolog engine to derive results. Subsequently, LLMs convert Prolog-derived successive reasoning trajectories into natural language CoT for imitation learning. Our empirical findings indicate that our proposed approach substantially enhances the reasoning abilities of LLMs and demonstrates robust generalization across out-of-distribution reasoning tasks.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs) have positioned them as versatile tools for a variety of tasks, achieved through a structured training process that includes pre-training on large text corpora, supervised fine-tuning, and reinforcement learning from human feedback (RLHF) (Casper et al., 2023). These phases equip LLMs with extensive knowledge, the ability to generate context-specific responses, and alignment with human values. One of the key factors contributing to the perception of LLMs as a precursor to artificial general intelligence (AGI) is their scalable and explainable reasoning capabilities (Zhao et al., 2023), which have demonstrated potential to solve multiple types of real-world complex logic reasoning problems, including natural language logical reasoning and arithmetic reasoning.\nGeneral logic reasoning tasks continue to present substantial challenges for LLMs. These challenges primarily emerge during the LLMs' reasoning processes (Ribeiro et al., 2023; Zhou et al., 2024). At the semantic level, the generation of accurate multi-hop inferential pathways and the assurance of validity at each reasoning step remain significant obstacles (Creswell et al., 2023; Kazemi et al., 2023). LLMs function in a self-regressive manner at the token level, processing information sequentially from left to right, one token at a time. This method can amplify errors: an inaccurate token prediction may negatively impact all subsequent tokens, leading to escalating inaccuracies and false reasoning outcomes. Interestingly, some studies (Ye and Durrett,\n2022; Saparov and He, 2023; Ribeiro et al., 2023) have observed instances where LLMs deliver correct answers despite erroneous internal reasoning processes. This phenomenon highlights a misalignment between the reasoning processes and outcomes of LLMs, suggesting a weak causal relationship and underdeveloped implicit reasoning connections within the LLMs.\nTo address the challenges associated with generating reasoning trajectories in LLMs, several strategies have been developed to enhance cognitive alignment and logical coherence. The Chain-of-Thought (CoT) approach (Wei et al., 2022; Kojima et al., 2022; Zhang et al., 2022), for instance, enhances LLMs' reasoning abilities by crafting instructions and few-shot examples that promote sequential reasoning. Building on the CoT framework, other methodologies like Tree-of-Thoughts\n(Yao et al., 2024; Long, 2023; Hulbert, 2023) and Graph-of-Thoughts (Besta et al., 2024; Zhang et al., 2024) have been introduced. These meth-"}, {"title": "2 Methods", "content": "2.1 Preliminary: Supervised Fine-tuning, Chain-of-Thought, and Prolog\nConsider a LLM, denoted by $p_\\theta$, which is parameterized by $\\theta$. Given an instruction dataset $D_{IF}$ consisting of question and answer pairs $(x, y)$, a highly effective method for aligning $\\theta$ with the target output $y$ given $x$ is to minimize the negative log-likelihood of $y$ conditioned on $x$ (Gunel et al., 2020; Dong et al., 2023). This can be formulated autoregressively as:\n$\\min_\\theta -E_{(x,y)\\sim D_{IF}} \\sum_{i=1}^{k} \\log p_\\theta(y_i | y_{<i}, x)$, (1)\nwhere $k$ is the number of tokens in $y$.\nTo enhance reasoning performance on complex logical and algorithmic tasks, one effective approach involves using a specialized prompt, $prompt_{cor}$, to initiate a step-by-step reasoning trajectory prior to generating the final output: $(y, c) \\sim p_\\theta(\\cdot | x, prompt_{cor})$. Here, $c$ represents a CoT reasoning process (Wei et al., 2022; Kojima et al., 2022; Zhang et al., 2022). Typically, this process decomposes complex, multi-step reasoning tasks into simpler, intermediate steps, thereby directing the LLM toward the correct answer. Nevertheless, several studies (Ye and Durrett, 2022; Saparov and He, 2023; Ribeiro et al., 2023) have indicated that $c$ may sometimes exhibit disorganized patterns that do not adhere strictly to logical reasoning processes. This inconsistency can adversely affect the LLM's reasoning performance across diverse tasks."}, {"title": "2.2 THOUGHT-LIKE-PRO", "content": "Here, we introduce THOUGHT-LIKE-PRO, a novel framework designed to enhance the general logical reasoning capabilities of LLMs across various logical tasks. As suggested by the name, our framework enables LLMs to imitate strictly logical reasoning trajectories that are generated and validated by the Prolog engine. The architecture of THOUGHT-LIKE-PRO is organized in a self-driven manner, eliminating the requirement for auxiliary services from other advanced LLMs, such as GPT-4 (Achiam et al., 2023) and Claude 2 (Wu et al., 2023). It solely relies on the utilization of efficient, open-source LLMs (i.e., Llama3-8B-Instruct (AI@Meta, 2024)), fostering accessibility and ease of replication.\nTo employ the Prolog engine Prolog for logical reasoning, we initiate the process by constructing a few-shot demonstration $prompt_{rft}$ (Listing 1). This is designed to guide the model $p_\\theta$ in generating relevant rules $R_x$, facts $F_x$, and deriving the reasoning target $T_y$:\n$(R_x, F_x, T_y) \\sim p_\\theta( \\cdot | prompt_{rft}, x, y)$. (3)\nSubsequently, the Prolog engine is employed to deduce the set of reasoning trajectories $O(x,y)$, which encompasses all logical paths leading to the target $T_y$. This process is formalized as:\n$O(x,y) = Prolog(R_x, F_x, T_y)$. (4)\nDuring this phase, we selectively utilize only those reasoning trajectories that conclusively reach the target $T_y$, excluding any data that result from erroneous transformations or generation failures as delineated in Equation 3. To ensure the acquisition of high-quality and interpretable reasoning trajectories via Prolog, we implement a meta-solver developed by (Yang et al., 2023), which retrieves all feasible reasoning paths.\nAfter acquiring the set $O(x,y)$, we design a novel few-shot prompt, $prompt_{cot}^{rft}$ (Listing 2), to translate existing reasoning trajectories into CoT-like reasoning processes expressed in natural language. This transformation aids in constructing the new dataset $D_{RIF}$:\n$c_{pro} \\sim p_\\theta(prompt_{cot}^{rft}, x, y, o_i), o_i \\in O(x,y)$,\n$(x, c_{pro}^i, y) \\in D_{RIF}$, for $i = 1, ..., n$.\n(5)\nIt is important to note that this new dataset, $D_{RIF}$, encompasses all $n$ available reasoning trajectories for each instruction $x$. Subsequently, we optimize the model using SFT as outlined in Equation 1, aligning both the CoT-like reasoning trajectories and the target outputs conditioned on $x$ autoregressively:\n$\\min_\\theta -E_{(x,c_{pro},y)\\sim D_{RIF}} [\\log p_\\theta(y, c_{pro}|x)]$, (6)\nresulting in a trained model parameterized by $\\theta'$. This entire learning process parallels imitation"}, {"title": "3 Experiment", "content": "In our preliminary experiments, we continually trained the Llama3-8B-Instruct model, using it as a baseline to evaluate the effectiveness of the THOUGHT-LIKE-PRO framework. We also implement CARING (Yang et al., 2023) under the experimental setup as our baseline method which utilizes the Prolog engine to derive the final results without training LLMs. Hence, the performance of CARING in reasoning tasks should be the upper limit of THOUGHT-LIKE-PRO. We trained the model and evaluated its reasoning capabilities across three datasets: GSM8K (Cobbe et al., 2021), ProofWriter (Tafjord et al., 2020), and PrOntoQA (Saparov and He, 2022). Additionally, we assessed OOD general task performance using the MMLU (Hendrycks et al., 2020), GPQA (Rein et al., 2023), HumanEval (Chen et al., 2021), and MATH (Hendrycks et al., 2021) datasets. Our experimental framework comprises four configurations to explore the impact of model averaging and training with multiple reasoning trajectories. The configurations are: 1) full implementation with model averaging and multiple trajectories (MA+multiple), 2) model averaging with a single trajectory for each instruction (MA+single), 3) multiple trajectories without model averaging (multiple), and 4) a single trajectory without model averaging (single). We refer the readers to Appendix A for more details about the experiment.\nThe results, as detailed in Table 1, reveal that the THOUGHT-LIKE-PRO framework significantly enhances reasoning capabilities through continuous training on the Llama3-8B-Instruct model. By incorporating multiple reasoning trajectories, this approach leads to superior outcomes compared to training with a single trajectory. Furthermore, the application of the model averaging technique has proven effective in exploiting the trade-off between specialty and generality, addressing the issue of CF, and yielding performance on OOD generalization tasks that are not included in the continual learning. The results show that the OOD performance is comparable to Llama3-8B-Instruct itself and significantly exceeds that of methods not employing model averaging. See Appendix B for a comprehensive discussion of the experimental results."}, {"title": "4 Conclusion", "content": "In this paper, we introduce THOUGHT-LIKE-PRO, a framework designed to enhance the logical reasoning of LLMs by imitating Prolog-based strictly logical reasoning trajectories in a self-driven manner. The design is simple, straightforward, and effective, ensuring ease of use in industrial applications. Our initial findings show improved task performance on both specialized reasoning and general benchmarks. This foundational framework will be further developed and extensively evaluated across various tasks to refine its methodology in future works."}, {"title": "5 Limitations", "content": "Our comparison of Llama3-8B-Instruct and THOUGHT-LIKE-PRO is constrained to moderate-scale language modeling tasks due to limited computational resources. Furthermore, to comprehensively evaluate and better understand the performance potential of THOUGHT-LIKE-PRO, it may be beneficial to explore the generation of multiple reasoning paths for more challenging datasets. Intuitively, while the complexity of reasoning paths for more complex problems increases exponentially, the difficulty of describing these problems in code only increases linearly. Additionally, we did not explore the performance of models under different $\\alpha$ values when applying the model averaging technique, nor did we balance the performance of the models on specialized versus generalized tasks. These considerations are scenario-specific and left for future research."}, {"title": "A Experimental Details", "content": "In this subsection, we will provide detailed information on the equipment, models, datasets, fine-tuning methods, and hyperparameter settings used in our experiment.\nA.1 Baseline, Model, and Equipment\nThroughout the entire process, we utilized two A800-80GB (NVIDIA Corporation, 2023) GPUs for inference, fine-tuning, and evaluation tasks. We applied our THOUGHT-LIKE-PRO framework by continually fine-tuning the Llama3-8B-Instruct model which can be accessed in https://huggingface.co/meta-llama/ Meta-Llama-3-8B-Instruct.\nOur baseline was established by assessing the untrained Llama3-8B-Instruct model's performance on various datasets. This initial evaluation gave us a benchmark to demonstrate the enhancements our framework could provide. Additionally, we implemented the CARING (Yang et al., 2023) method to generate Prolog code with Llama3-8B-Instruct, assessing the correctness of solutions this code produced to problems.\nA.2 Datasets\nWe evaluated THOUGHT-LIKE-PRO across seven prominent LLM benchmark datasets: two focused on mathematical reasoning (GSM8K and MATH), two on knowledge-based question answering (MMLU and GPQA), two on logical reasoning (ProofWriter and PrOntoQA), and one on code generation (HumanEval).\nGSM8K: GSM8K (Cobbe et al., 2021) comprises 8,500 high-quality, linguistically diverse grade school math word problems created by human experts. We randomly selected 2,000 entries from this dataset to generate reasoning trajectories and used the balance to gauge the effectiveness of our THOUGHT-LIKE-PRO framework. Given that the number of reasoning trajectories generated by the Prolog engine is variable, we capped the trajectories for each problem at 10 to mitigate data distribution bias. Consequently, we produced 20,000 pieces of training data for the LLM.\nMATH: MATH (Hendrycks et al., 2021) encompasses a collection of 12,500 challenging competition-level mathematics problems, each accompanied by detailed step-by-step solutions. These solutions facilitate training models to generate complete derivations and explanations. We leveraged this difficult dataset to assess the OOD performance of our THOUGHT-LIKE-PRO framework, noting that the LLM was not previously trained on it.\nMMLU: MMLU (Hendrycks et al., 2020) benchmark is designed to gauge the knowledge acquired during pretraining by assessing models in exclusively zero-shot and few-shot scenarios, which closely aligns with methods used to evaluate human capabilities. It encompasses 57 subjects, spanning STEM, the humanities, and the social sciences, among others. Given its wide content range and varying difficulty, we utilize the entire dataset to evaluate the THOUGHT-LIKE-PRO framework's generalization abilities. Notably, within the THOUGHT-LIKE-PRO framework, the LLM does not have exposure to MMLU during the training phase.\nGPQA: GPQA (Rein et al., 2023) represents a formidable dataset aimed at testing the capabilities of LLMs alongside scalable oversight mechanisms. The dataset comprises 448 multiple-choice questions crafted by domain experts in disciplines such as biology, physics, and chemistry. We employ this complete set to determine the generalization capacity of the THOUGHT-LIKE-PRO framework.\nProofWriter: ProofWriter (Tafjord et al., 2020) is a widely utilized logical reasoning dataset comprising many small-scale knowledge bases expressed in English, each containing facts and rules. Each knowledge base is paired with a set of questions, also in English, which can be definitively proven as true or false via proofs of varying depths, or the answer may be categorized as \"unknown\" under an open-world assumption (OWA) or presumed negative under a closed-world assumption (CWA). This dataset contains subsets of varying difficulty, we have selected the most challenging subset within the OWA setting, which includes 482 knowledge bases and 10,190 questions. We initially randomly sampled 100 knowledge bases and their corresponding questions to create reasoning trajectories. We restricted the number of allowed reasoning trajectories per question to five. Ultimately, we utilized all remaining data to evaluate the performance of our THOUGHT-LIKE-PRO framework.\nPrOntoQA: PrOntoQA (Saparov and He, 2022) is a synthetic question-and-answer dataset designed to test the logical reasoning capabilities of LLMs. Each instance is structured to verify the validity of a statement within a specific context. We chose the most demanding subset (Pan et al., 2023b), \u201cFic-"}, {"title": "B Analysis of Experimental Results", "content": "In our study, we assessed the performance of both the baseline model, Llama3-8B-Instruct, and our proposed framework, which was tested under four distinct configurations: 1) Full implementation featuring both model averaging and multiple trajectories (MA+multiple), 2) Model averaging applied to a single trajectory per instruction (MA+single), 3) Multiple trajectories without model averaging (multiple), and 4) A single trajectory without model averaging (single). This comparison allows us to discern the contributions of key components within our framework, particularly the impact of model averaging and the learning of multiple reasoning trajectories for the same instruction. Additionally, we incorporated the CARING method (Yang et al., 2023) within our experimental setup. This was utilized to evaluate the final reasoning accuracy, relying solely on Prolog engine inferences, without the requirement for training LLMs or using them to derive final answers.\nAll experiment results are presented in Table 1. We report the CARING results across three reasoning tasks. The Prolog engine delivered completely accurate reasoning outputs for the ProofWriter and PrOntoQA tasks and achieved an accuracy of 98.19% for the GSM8K task. The slightly lower accuracy in GSM8K can be attributed to errors in the translation of facts or rules and formatting-related issues, although the reasoning trajectories are correct. Consequently, we posit that the performance of CARING represents the practical upper limit for the proposed THOUGHT-LIKE-PRO framework. Across all tasks, our framework, configured in four distinct ways, demonstrated substantial performance improvements over the baseline model. Notably, the configuration employing the 'multiple' method without model averaging recorded the highest performance, consistent with our expectations, as performing domain-specific fine-tuning and forgoing model averaging tend to enhance domain-specific expertise. In the GSM8K task, the Llama3-8B-Instruct model achieved an accuracy rate of 79.6%. In contrast, our framework marked a significant improvement, attaining an accuracy of 87.81%. This improvement was particularly pronounced in datasets requiring logical reasoning, especially in the ProofWriter and PrOntoQA datasets, where the model utilizing multiple reasoning paths achieved 98.19% and 100% accuracy, respectively.\nThe experimental results for reasoning tasks also suggest that imitating strictly logical reasoning trajectories can significantly enhance performance in logical reasoning tasks, such as ProofWriter and PrOntoQA. However, the improvements in algorithmic reasoning tasks are comparatively modest. These findings not only confirm the efficacy of imitating strictly correct logical reasoning trajectories in aligning the reasoning capabilities of LLMs, but also suggest that LLMs may be better suited to modeling natural language problems using the relatively straightforward Prolog language, rather than translating these problems into more complex mainstream programming languages like Python.\nWe can assess the performance differences between the 'single' and 'multiple' training approaches to validate the effectiveness of utilizing multiple reasoning paths in training. Across all three reasoning tasks, it is evident that training with a single reasoning path can enhance performance relative to baseline methods. However, there exists a significant performance disparity between the 'multiple' and 'single' approaches, underscoring the importance of training with multiple reasoning trajectories for each instruction. This performance gap may be attributed to the inherent multi-modal nature of LLMs. Training on multiple correct trajectories toward the same target could promote robust learning of multi-modal distributions and significantly improve the robustness of LLM's reasoning.\nTo evaluate the performance of THOUGHT-LIKE-PRO on general OOD tasks, we selected four benchmarks commonly employed to assess the common knowledge and abilities of LLMs: MMLU (Hendrycks et al., 2020), GPQA (Rein et al., 2023), HumanEval (Chen et al., 2021), and MATH (Hendrycks et al., 2021). These benchmarks were chosen to measure the impacts of continual fine-tuning on three distinct reasoning tasks. Our observations indicate that Llama3-8B-Instruct achieves almost the highest performance across these tasks, and exhibits signs of catastrophic forgetting of continual training. However, the implementation of model averaging in THOUGHT-LIKE-PRO effectively navigates the trade-off between specialization and generalization, significantly enhancing the OOD performance. The results show results of 67.9% on MMLU and 33.8% on GPQA with MA+multiple, compared to domain-specific training, with only a slight decrease in performance on the reasoning tasks. Thus, model averaging serves as a viable strategy to mitigate catastrophic forgetting, enhancing both the robustness and generalizability of our approach.\nWhile we observe a decline in performance on the MMLU, GPQA, and HumanEval datasets, the performance on the MATH dataset intriguingly remains consistent with the base model. This stability may be attributed to the inherent nature of the MATH tasks, which are graduate-level problems requiring arithmetic and logical reasoning skills. These skills have been effectively cultivated through the THOUGHT-LIKE-PRO approach applied within the GSM8K, ProofWriter, and PrOntoQA datasets. This approach appears to successfully generalize to MATH tasks and mitigate the negative impacts of catastrophic forgetting. Moreover, these results suggest that the THOUGHT-LIKE-PRO strategy possesses a potential broad generalization capability for OOD reasoning tasks."}, {"title": "C Prompt demo", "content": "In this section, we present the few-shot demonstration prompt utilized. It should be noted that due to space constraints, all prompts are displayed in their 1-shot condition, with the black text denoting the prompt sent to the LLM and the red text indicating the model's response. For more demonstrations, we refer the readers to https://anonymous. 4open.science/r/Prolog_datasets-9875.\nListing 1: Construct a prompt promptrft to guide the LLM to generate Prolog code contains rules R, facts F, and targets T based on the problem.\nHere is the problem:\nsent-1: Tina makes $18.00 an hour.\nsent-2: If she works more than 8 hours per shift,\nsent-3: she is eligible for overtime,\nsent-4: which is paid by your hourly wage + 1/2 your hourly wage.\nsent-5: If she works 10 hours every day for 5 days,\nListing 2: Construct a prompt promptert to guide the LLM to generate a natural language CoT-like reasoning logic chain based on the problem, Prolog code, and reasoning tree.\nHere is the problem:\nsent-1: Tina makes $18.00 an hour.\nsent-2: If she works more than 8 hours per shift,\nsent-3: she is eligible for overtime,\nsent-4: which is paid by your hourly wage + 1/2 your hourly wage.\nsent-5: If she works 10 hours every day for 5 days,"}]}