{"title": "SemAI: Semantic Artificial Intelligence-enhanced DNA storage for Internet-of-Things", "authors": ["Wenfeng Wu", "Luping Xiang", "Qiang Liu", "Kun Yang"], "abstract": "In the wake of the swift evolution of technologies such as the Internet of Things (IoT), the global data landscape undergoes an exponential surge, propelling DNA storage into the spotlight as a prospective medium for contemporary cloud storage applications. This paper introduces a Semantic Artificial Intelligence-enhanced DNA storage (SemAI-DNA) paradigm, distinguishing itself from prevalent deep learning-based methodologies through two key modifications: 1) embedding a semantic extraction module at the encoding terminus, facilitating the meticulous encoding and storage of nuanced semantic information; 2) conceiving a forethoughtful multi-reads filtering model at the decoding terminus, leveraging the inherent multi-copy propensity of DNA molecules to bolster system fault tolerance, coupled with a strategically optimized decoder's architectural framework. Numerical results demonstrate the SemAI-DNA'S efficacy, attaining 2.61 dB Peak Signal-to-Noise Ratio (PSNR) gain and 0.13 improvement in Structural Similarity Index (SSIM) over conventional deep learning-based approaches.", "sections": [{"title": "I. INTRODUCTION", "content": "Internet of Things (IoT) has permeated various domains, such as smart homes, industrial automation, and smart cities [1]. However, as the massive distributed sensors in IoT systems generate massive data that require efficient integration, storage, processing, and utilization, data management imposes a significant challenge in the implementation of IoT [2]. The key challenge lies in how to extract valuable information from the vast amount of data, enhancing the efficiency of data integration and storage.\nAdditionally, the data generated by IoT devices require a reliable and efficient storage system for storage and management. The significance of storage lies in providing data persistence and accessibility, enabling data to be queried, analyzed, and utilized. Currently, traditional storage media with cloud storage as a core application (such as magnetic storage, optical storage, solid-state storage, etc.) face technological bottlenecks in power consumption, size, reliability, and effective storage time [3]. In recent years, DNA storage has garnered attention from many researchers due to its distinct characteristics [4]. Specifically, compared with traditional storage media, DNA molecules exhibit significant advantages in data storage [5], [6]: (1) Ultra-high storage density. The storage density of DNA molecules can reach $10^{19}$ bit/cm\u00b3, surpassing traditional storage media by six orders of magnitude. (2) Extremely long lifespan. Data stored in DNA may persist for millennia without specific artificial intervention. (3) Remarkably low maintenance costs. The footprint, resources, and energy required for DNA storage are significantly smaller than those for traditional storage media, resulting in minimal maintenance costs. Furthermore, the biochemical reactions and inherent operations of DNA molecules demonstrate substantial parallelism. Despite current drawbacks such as high read/write costs and slow read/write speeds, DNA is still considered the optimal choice for storing over 60% of global cold data, making it a potential storage medium for current cloud storage applications [7]. Vast amounts of sensor data, images, and video streams are expected to be encoded and stored in tiny DNA molecules, achieving extremely high information density.\nCurrently, with the rapid development of DNA synthesis and sequencing technologies, DNA storage has witnessed numerous groundbreaking advancements [8]\u2013[14]. In 2012, Church et al. [8] from Harvard Medical School first stored 0.65 MB of data in vitro, sparking a surge of research interest in DNA storage. Then in 2013, Goldman et al. [9] increased the capacity of stored data to 739 KB of data. Grass et al. [10] translated 83 KB of information to 4991 DNA segments and encapsulated in silica, which proved that data can be archived on DNA for millennia under a wide range of conditions. In 2017, Shipman et al. [12] encoded a movie message into living cells through CRISPR technology. In 2018, Microsoft and the University of Washington achieved random access to 200 MB of stored data [13]. In 2019, Julian et al. [14] devised a \"DNA-of-things\" storage architecture to produce materials with immutable memory. They successfully stored and retrieved a 1.4 MB video in plexiglass spectacle lenses.\nDNA encoding is a crucial way of achieving DNA storage, wherein raw data is encoded into a sequence composed of the four bases (A, C, G, T) through specific mapping rules and can be reconstructed back to the original data [21]. The challenge in DNA encoding lies in designing high-density and fault-tolerant encoding strategies. A common approach is to apply classical coding theories, such as utilizing Huffman coding and fountain codes for data compression [9], [11], [22], and incorporating Forward Error Correction (FEC) codes, such as Reed-Solomon (RS) codes, Low Density Parity Check (LDPC) codes, and Bose-Chaudhuri-Hocquenghem codes (BCH) codes to enhance fault tolerance [10], [13], [22]\u2013[24]. However, due to the distinct physical and chemical properties as well as constraints inherent in DNA sequences compared to conventional digital communication environments, such as limitations on sequence length, distribution ratios of base pairs, and length constraints on homopolymers, which pose additional challenges to conventional coding schemes [15], [25]. Moreover, noise forms introduced during the synthesis and sequencing processes of DNA sequences, such as base pair insertions and deletions [26], differ from those in traditional digital communication, rendering traditional coding schemes ineffective in rapidly and effectively identifying and correcting a large number of base errors. Therefore, some scholars have introduced deep learning (DL) techniques into DNA encoding to address the complex characteristics and encoding requirements of DNA sequences, leveraging the powerful feature learning capabilities to improve encoding efficiency and accuracy [16], [17], [27]. In [16], a robust multi-reads reconstruction neural network was proposed to handle noise in DNA storage, including base substitutions, insertions, deletions, and chain breakage. In [17], a deep joint source-channel coding system for DNA storage, called DJSCC-DNA, was designed. As shown in Fig. 1, DJSCC-DNA replaces traditional encoding and decoding with convolutional neural networks (CNN) and uses multiple copies of erroneous DNA chains for information retransmission at the decoding end, achieving high-density and fault-tolerant data storage. The system also incorporates an end-to-end optimization function to ensure DNA sequences adhere to biological constraints, specifically limiting GC content and homopolymer length. Up to the present, research on utilizing DNA molecules for data storage primarily focused on syntactic data storage, aiming to ensure the complete preservation of data information.\nHowever, with the widespread deployment of IoT devices, the explosive growth of multimedia applications such as extended reality, autonomous driving, and intelligent surveillance has emerged. This rapid expansion poses tremendous challenges to the utilization of limited spectrum resources, prompting a shift towards semantic communication paradigms [28]. Semantic communication differs from the traditional Shannon paradigm by focusing on conveying the actual meaning intended by the sender or information conducive to achieving common goals, whereas the Shannon paradigm is more concerned with the accurate reception of bits [29]. With the rapid development of artificial intelligence (AI), the integration of knowledge bases and DL algorithms has found extensive applications in constructing models for semantic learning and communication [30]\u2013[33]. For example, Jiang [34] proposed a semantic framework based on large-scale AI models, incorporating a knowledge base built on the Segment Anything Model (SAM) and introducing Attention-based Semantic Integration (ASI) to integrate semantic-aware data sources. In parallel, the current DNA image storage paradigm still emphasizes the encoding and restoration of the overall image, balancing high compression rates with the accuracy of image extraction. Inspired by this, to adapt to the continuously evolving IoT deployment environment, this paper shifts focus from storing comprehensive data information in DNA to prioritizing the storage of semantic information. By leveraging semantic communication principles, we aim to enhance the efficiency and effectiveness of data storage in IoT systems.\nIn the context of IoT, the need for efficient and scalable data storage solutions is more pressing than ever. This paper proposes a Semantic AI-enhanced DNA Storage (SemAI-DNA) scheme that utilizes DNA storage technology to encode the semantic information of data into DNA sequences, thereby making it possible to store or transmit large-scale data in the upcoming IoT era. The key technical contributions of our investigation are contrasted to the literature in Tab. I and are summarized as follows:\n\u2022 We propose a large-scale AI model for DNA storage by utilizing the novel application of SAM and ASI techniques, which enables the extraction and storage of rich semantic targets from images. By leveraging the image semantic extraction module, the system efficiently achieves DNA image semantic storage, ensuring the accurate transfer of meaningful information.\n\u2022 By exploiting the unique characteristics of DNA multi-reads, we propose a multi-reads-based DNA chain screening scheme. To ensure reliability, the system evaluates and filters the multi-reads through isomorphic matching and K-mers comparison techniques, selecting highly trustworthy multi-reads for further processing. The error correction module utilizes parallel convolutional layers to extract multi-layered semantic information from DNA multi-reads, which are then fed into the decoder to reconstruct the semantic image.\n\u2022 Our numerical results demonstrate that the proposed SemAI-DNA scheme effectively achieves high-quality resolution of semantic information in images and high fault tolerance while adhering to biological constraints."}, {"title": "II. SYSTEM MODEL", "content": "In this section, we introduce SemAI-DNA, emphasizing the encoding and storage of semantic targets in images. On the one hand, this approach utilizes high-performance large AI models to achieve the extraction of semantic targets, eliminating redundant information outside the targets, thereby significantly reducing the storage data volume and improving storage efficiency. On the other hand, the multi-reads characteristics of DNA storage are leveraged to enhance decoding fault tolerance, ensuring high-quality storage of important semantic information under high compression rates. Finally, a potential IoT application scenario is proposed.\nBuilding upon the research by [17], this paper proposes a coding scheme based on DL for DNA image semantic storage. The proposed system model, as illustrated in Fig. 2, primarily comprises modules such as the image semantic extraction (ISE) based on the Large-AI module, the deep joint semantic-channel encoding and decoding module (Encoder-Decoder), and the Multi-Reads Screening (MRS) module. Firstly, the ISE utilizes a semantic recognition and integration model based on a shared semantic-aware knowledge base to extract the most meaningful semantic targets from the image. Secondly, the encoder constructs a CNN to convert the integrated semantic-aware image into DNA sequences. During the processes of DNA synthesis, Polymerase Chain Reaction (PCR) amplification, and sequencing, multiple DNA sequences are prone to various types of base errors, which significantly impact subsequent data recovery efforts. The MRS screens out more suitable sequences from multiple DNA strands for decoding. Lastly, the decoder analyzes the DNA sequences, mitigating the effects introduced by the DNA storage channel, and reconstructs the semantic-aware image."}, {"title": "A. Image Semantic Extraction", "content": "The ISE module utilizes attention mechanisms and large-scale model to accurately capture the most crucial and representative semantic-aware image x from the original image \u017c. As shown as Fig. 3, it primarily comprises two components [35]:\n1) The SAM module. We utilize SAM as a knowledge repository to achieve semantic segmentation of images. SAM [36] is an indicative model, leveraging the largest segmentation dataset to date, the Segment Anything 1-Billion mask dataset, which comprises over 11 million images totaling more than one billion mask images. The model is designed as an interactive prompting model during training, enabling zero-shot learning transfer to new image distributions and tasks. The SAM architecture primarily consists of three components: an image encoder, a prompt encoder, and a mask decoder. It is capable of producing an effective segmentation mask given any segmentation prompt.\n2) The ASI module, is an attention-based semantic integration module that emulates human perception by selectively choosing the most salient semantic segments, ultimately synthesizing a novel semantic perceptual image [34]. It consists of channel attention network and a spatial attention network, which introduce attention mechanisms to identify important objects in the image of formaldehyde.\nThe process can be expressed as follows:\n$x = \\mathcal{W}(i),$ (1)\nwhere $\\mathcal{W}$ presents the function of ISE."}, {"title": "B. Encoder", "content": "The encoder encompasses two stages: feature extraction and base mapping. Initially, the encoder employs cascading convolutional layers to extract the latent features from the input image. By stacking multiple convolutional layers, the network acquires a more advanced level of feature representation. This hierarchical framework empowers the encoder to capture increasingly abstract and intricate semantic information. Subsequently, the extracted latent features are meticulously mapped to their corresponding DNA sequences. The bases {A, C, G, T} are represented by set {0,1,2,3}. This mapping procedure engenders a DNA sequence that encodes the semantic information of the input image by designating specific bases for the extracted features.\nAssuming the input image semantic sequence as $x \\in \\mathbb{R}^n$, where $n = H(height) \\times W(width) \\times C(channels)$ is the number of pixels, the encoding process can be expressed as\n$z = f_{\\Theta}(x), (2)$\nwhere $f_{\\Theta} : \\mathbb{R}^n \\rightarrow \\mathbb{Z}^k$ is the encoding function and $\\Theta$ denotes the parameter set of the encoder, and $k$ represents the base sequence size. The output sequence $z \\in \\mathbb{Z}^k$ will be generated by DNA synthesis to generate DNA strands for storage or transmission."}, {"title": "C. Channel", "content": "Most DNA synthesis techniques involve adding nucleotides in a specified order, and various errors may occur during this process [37], [38]. For example, nucleotides may not be added to the designated location (base deletion); nucleotides may be added to the wrong location (base insertion); non-designated nucleotides may be added (base substitution). To facilitate data retrieval, primers are attached to both ends of the DNA chain. When the synthesis process is completed, the resulting DNA strands are then placed in an aqueous solution to generate a DNA pool. When reading the data, a specific DNA chain needs to be extracted from the main DNA pool, and PCR technology is used to amplify DNA chains containing the corresponding primers in the pool, thereby forming a new DNA library that includes multiple copies of the target DNA chain and a small amount of unrelated DNA chains. Target information can be obtained from this new library by sequencing. The main error type during sequencing is base substitution, while the probability of base insertion and deletion errors is significantly reduced.\nIn proposed SemAI-DNA model, we model the DNA synthesis, storage, amplification, and sequencing processes as a DNA channel, and simulate errors such as base substitution, insertion, and deletion. According to previous research [17], [37], the probabilities of substitution, insertion, and deletion can be set to 17%, 40%, and 43%, respectively, and the total base error rate is denoted as $\\gamma$.\n$\\tilde{z} = F_{\\gamma}(z), (3)$\nwhere $F_{\\gamma}$ denotes the channel process with base error rate $\\gamma$. This process allows us to obtain a collection of multiple reading sequences that simulate the DNA channel and contain the specified information z, denoted as {$\\tilde{z}^1, \\tilde{z}^2, ..., \\tilde{z}^v$}, where v is the number of multiple reading sequences."}, {"title": "D. Multi-Reads Screening", "content": "The MRS module selects w sequences {$\\tilde{z}^{b_1}, \\tilde{z}^{b_2}, ..., \\tilde{z}^{b_w}$} from v sequences and converts them back to the digital sequence group as input for the decoder, which can be expressed as\n{\\begin{equation}\n{\\tilde{z}^{b_1}, \\tilde{z}^{b_2}, ..., \\tilde{z}^{b_w}} = \\mathcal{D}^{\\upsilon}_u({\\tilde{z}^1, \\tilde{z}^2, ..., \\tilde{z}^v}).\n\\end{equation}}\n}(4)\nwhere b is the vector of the index of the selected sequences and $\\mathcal{D}^{\\upsilon}_u$ denoted the process of selecting w sequences from v sequences.\nSince DNA sequences have the unique characteristic of multi-copy generation during extraction, we propose to select high-reliability sequences from multiple sequence groups, rather than merely increase the number of sequences from multiple readings and not prioritize sequence quality in the selection process for data restoration, as it did in existing literture [17]. In this paper, sequences with fewer insertion, deletion, or substitution errors in multiple reads are considered highly reliable and more conducive to neural network decoding. Preliminary screening of sequence groups reduces the likelihood of including highly contaminated sequences, thereby enhancing the accuracy and integrity of the restored data. The proposed MRS method is particularly well-suited for multi-read scenarios where the channel conditions are similar or identical.\nThe core idea of the scheme is to evaluate the reliability of sequences by comparing them and selecting sequences with lower levels of noise contamination for decoding. The specific operations are as follows:\n1) Initialize a score matrix Q of size v \u00d7 k. The received sequence group is denoted as {$\\tilde{z}^1, \\tilde{z}^2, ..., \\tilde{z}^v$}, where the number of sequences is v, and the length of the sequences is approximately k. The inconsistent lengths of the sequences are due to variations in the number of base insertions and deletions.\n2) Compute the values of Q based on the distribution of bases at the same positions among sequences. The number of occurrences of the j-th base in the same column was counted and assigned to $Q_{i,j}$. If the base is missing at that position, assign a penalty value of -v to $Q_{i,j}$.\n3) Normalize the values of Q and calculate the row sums to obtain a score vector q for the v sequences.\n4) Sort the elements of q in descending order to obtain the index set b of the top w scoring sequences, where b \u2208 {1, 2, 3, ..., \u03c5}.\n5) Convert the sequences corresponding to the top w indices in b into numerical sequence groups according to the {A, C, G, T} \u2192 {1,2,3,4} conversion rule, which will serve as the input for the decoder. If a sequence's length is less than k, pad it with zeros; if it exceeds k, ignore the excess part."}, {"title": "E. Decoder", "content": "The decoder consists of two steps: sequence analysis and image reconstruction. Firstly, the received multiple chains are subjected to convolutional operations with varying step sizes to analyze sequence information at multiple levels. The results are then concatenated and used for subsequent decoding. Subsequently, convolutional layers are employed to reconstruct semantic segments of the image. The decoding process can be delineated as\n$\\hat{x} = g_{\\Phi}({\\tilde{z}^{b_1}, \\tilde{z}^{b_2}, ..., \\tilde{z}^{b_w} }), (5)$\nwhere $g_{\\Phi} : \\mathbb{Z}^{w\\times k} \\rightarrow \\mathbb{R}^n$ denotes the decoding function and $\\Phi$ represents the parameter set of the decoder."}, {"title": "F. Potential scenarios.", "content": "The semantic extraction module of the SemAI-DNA storage system can be embedded in edge computing devices to directly process and encode data collected from IoT sensors. This reduces the need to transfer large amounts of raw data, thereby increasing the efficiency of data storage. Utilizing existing network infrastructure such as Wi-Fi and 5G, the encoded data can be efficiently transferred to DNA storage systems. During decoding, SemAI-DNA's multi-read filter model leverages the multi-copy properties of DNA molecules to enhance system fault tolerance and ensure the security and integrity of data transmission."}, {"title": "III. END-TO-END TRAINING", "content": "In this section, we introduce the end-to-end implementation process of the SemAI-DNA.\nThe training framework of our model is illustrated in Fig. 6, which consists of two parts. After completing the training of the ISE, the generated semantic image dataset is used to perform the encoder-decoder training."}, {"title": "A. ISE training", "content": "SAM is a pre-trained, large-scale AI model that does not require further training. It utilizes human-perceived semantic information and images as an experiential knowledge base to conduct foundational training for ASI. During ASI training, semantic segments are used as inputs to the network, with human-perceived information serving as a supervised training database. The model selects the choices most closely aligned with human perception from dispersed semantic information, generating images of semantic perception that are of greatest concern to humans."}, {"title": "IV. NUMERICAL RESULTS", "content": "In this section, we demonstrate the performance of our proposed SemAI-DNA model for DNA storage, with simulation settings discussed in Section IV-A and numerical results given in SectionIV-B"}, {"title": "B. Encoder-Decoder training", "content": "The functions that the encoder and decoder used in our methodology are listed in Tab.II. Here, the tuple notation (C, N, S) signifies a convolutional layer composed of C filters, each with a kernel size N and a stride S. The base pixel radio is represented as $R = k/n$. We can modulate R by varying the value of c. The DNA channel is constituted as non-trainable layers. In light of the present state of DNA synthesis techniques, synthesizing long-chain DNA is both labor-intensive and inefficient. Therefore, we partition the output sequence of the encoder into non-overlapping segments, each of lengths, which are then recombined before being fed into the decoder.\nThe loss function comprises terms that ensure both image reconstruction quality and adherence to the biological constraints of the DNA chain. Generally, biological constraints encompass restrictions on GC content and polymer length. Typically, the GC content is maintained within the range of 40-60%, and the homopolymer run-length should not be excessively long, preferably less than 5 [11], [40]. The loss function is expressed as follows [17]:\n{\\begin{equation}\n\\mathcal{L}(\\theta, \\phi) = \\frac{1}{n} \\sum_{i=1}^{n}(x_{i}, \\hat{x_{i}})^{2} + \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[(\\mathcal{T_{i}}, \\mathcal{T}^{*})^{2} + (\\mathcal{H_{i}}, \\mathcal{H}^{*})^{2}],\\end{equation}}(6)\nwhere \u03b1 is an adjustable parameter. Here, the sequence z is divided into m overlapping segments. Ti and Hi respectively measure the base composition and base distribution of the i-th segment of the DNA sequence, used to constrain the GC content and homopolymer length. T* and H* represent their expected values. The index of the base of i-th segment in the DNA sequence z forms the set Mi.\n{\\begin{equation}\n\\mathcal{T}_{i} = \\frac{1}{t} \\sum_{t\\epsilon M_{i}} z_{t},\\end{equation}}(7a)\n{\\begin{equation}\n\\mathcal{H}_{i} = \\frac{1}{t} \\sum_{t\\epsilon M_{i}} (z_{t} - \\mathcal{T}_{i})^{2},\\end{equation}}(7b)\n{\\begin{equation}\n\\mathcal{T}^{*} = 1.5,\\end{equation}}(7c)\n{\\begin{equation}\n\\mathcal{H}^{*} = 1.25.\\end{equation}}(7d)"}, {"title": "B. Evaluation Results", "content": "Fig. 7 illustrates the variation of loss values with increasing epochs. It can be observed that, under the same values of train and R, the convergence results of the proposed SemAI-DNA scheme outperform the traditional DJSCC-DNA scheme [17]. The DJSCC-DNA used the structure is shown in Fig. 1, with other settings consistent with the SemAI-DNA."}, {"title": "C. The complexity analysis", "content": "The subsection provides a brief analysis of SemAI-DNA's complexity, comparing it with DJSCC-DNA. These simulations are performed on a server equipped with an Intel Xeon CPU, 128 GB of memory, and an RTX3080 GPU with 10GB of video memory.\nTab. IV and Tab. V compare the model complexity of SemAI-DNA with that of DJSCC-DNA, including the number of parameters, model storage size, and the time required for model inference on a single image. The difference between SemAI-DNA and DJSCC-DNA lies primarily in the sequence analysis portion of the decoder. Consequently, the parameters and storage space of this module are slightly higher compared to the latter. SemAI-DNA employs an ISE module based on a large-scale AI, resulting in a significantly larger number of parameters and required memory. Its advantage lies in achieving high performance with minimal training time costs by leveraging a large-scale AI. The average duration for semantic-aware image analysis using the ISE module inSemAI-DNA is 0.2698 s. If an encoder-decoder module without MRS is used for image storage, the total duration required is 0.2806 s, which is 0.1163 s faster than SemAI-DNA's 0.3969 s. Although the number of model parameters between the encoder-decoder modules of SemAI-DNA and DJSCC-DNA are comparable, the inference is faster due to less redundancy of image data in SemAI-DNA. When using an encoder-decoder module with MRS, the total duration is 0.2824 s seconds slower than the DJSCC-DNA scheme due to the longer time required for sequence comparison with MRS. However, the image quality with the MRS module is higher compared to without, as evidenced in Fig. 9 and Fig. 11."}, {"title": "V. CONCLUSION AND FUTURE TRENDS", "content": "This paper introduces a new DNA storage paradigm, namely SemAI-DNA for the upcoming IoT era. A pair of key modifications have been proposed: embedding a semantic extraction module for the detailed encoding of semantic information and implementing the MRS model to enhance system fault tolerance. Numerical results have demonstrated its efficacy, showing a significant improvement over baseline models with a 2.61 dB gain in PSNR and a 0.13 increase in SSIM.\nSemAI-DNA presents an innovative and reliable solution to the challenges of data storage, particularly in the context of managing large-scale data. By leveraging advanced AI technology, this framework addresses the limitations of traditional storage methods in handling substantial volumes of data. In the context of Internet of Things (IoT) applications, SemAI-DNA holds potential for effectively managing and storing data generated by numerous devices. Future research should focus on expanding the capabilities of SemAI-DNA, including exploring improvements in data transmission and enhancing data security through DNA storage techniques. Additionally, while SemAI-DNA currently demonstrates efficacy in image storage, there is potential for extending its application to other data types, such as text and video, thereby increasing its utility in data storage for IoT devices."}]}