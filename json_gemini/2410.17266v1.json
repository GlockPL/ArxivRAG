{"title": "Temporal Relational Reasoning of Large Language Models for Detecting Stock Portfolio Crashes", "authors": ["Kelvin J.L. Koa", "Yunshan Ma", "Ritchie Ng", "Huanhuan Zheng", "Tat-Seng Chua"], "abstract": "Stock portfolios are often exposed to rare consequential events (e.g., 2007 global financial crisis, 2020 COVID-19 stock market crash), as they do not have enough historical information to learn from. Large Language Models (LLMs) now present a possible tool to tackle this problem, as they can generalize across their large corpus of training data and perform zero-shot reasoning on new events, allowing them to detect possible portfolio crash events without requiring specific training data. However, detecting portfolio crashes is a complex problem that requires more than basic reasoning abilities. Investors need to dynamically process the impact of each new information found in the news articles, analyze the the relational network of impacts across news events and portfolio stocks, as well as understand the temporal context between impacts across time-steps, in order to obtain the overall aggregated effect on the target portfolio. In this work, we propose an algorithmic framework named Temporal Relational Reasoning (TRR). It seeks to emulate the spectrum of human cognitive capabilities used for complex problem-solving, which include brainstorming, memory, attention and reasoning. Through extensive experiments, we show that TRR is able to outperform state-of-the-art solutions on detecting stock portfolio crashes, and demonstrate how each of the proposed components help to contribute to its performance through an ablation study. Additionally, we further explore the possible applications of TRR by extending it to other related complex problems, such as the detection of possible global crisis events in Macroeconomics.", "sections": [{"title": "1 INTRODUCTION", "content": "In equity investing [27, 28], investors typically form stock portfolios [34] to diversify their risk across multiple stocks. This could be done by selecting the stocks from across different categories, based on geographical regions [40] and/or business sectors [10], etc., in order to dampen the impacts of events that affect any specific category. However, there also exist rare, fat-tailed events [33] that are unprecedented in history and cause the market to be increasingly interconnected [44], which can result in crashes (e.g., 2007 global financial crisis [17], 2020 COVID-19 stock market crash [35]). Even though stock portfolios have extensively considered various risks when they are curated, they are often still ill-prepared to handle these events [44], as they do not have past statistics or historical information to learn from. There are currently limited works [45] on detecting portfolio crash events.\nToday, Large Language Models (LLMs) present a possible toolset for detecting these crash events, without requiring specific training data. This stems from their known capabilities to perform zero-shot reasoning [29], which can be attributed to their ability to generalize [5, 56] across the large corpus of data they have previously been trained on. This allows them to identify repeating patterns on new emerging events, and potentially detect possible crashes before they happen. In this work, we explore the use of LLMs to detect possible stock portfolio crashes, given an input set of financial news events.\nDetecting portfolio crashes is a complex problem that requires more than basic reasoning abilities. Currently, there are some reasoning frameworks for LLMs to handle complex tasks: Thought-based frameworks (e.g., Tree-of-Thoughts (ToT) [57], Graph-of-Thoughts (GoT) [6]) break down a task into generated sub-steps that can be merged to solve the task; Search-based frameworks (e.g., Think-on-Graph (ToG) [43]) search through an existing Knowledge Graph of facts to find a reasoning path that can answer questions on a single entity. However, among these methods, we can still identify three challenges for our task. (1) The current methods focus mainly on tackling isolated problems, such as solving a task through sequential thoughts or answering questions from a static graph of information. However, these methods do not deal with the constantly evolving nature of news events, which would require the dynamic processing of new information. (2) Portfolio crashes are often caused by the unexpected interconnectivity of its constituent stocks in response to unprecedented events [17, 35]. While current frameworks combine thoughts or search for a single path on a Knowledge Graph (see Figure 1), they do not reason across multiple search paths, which could reveal these interconnectivity between news events and portfolio stocks within the graph. (3) It is also known from stock prediction works that there exists temporal context dependency [23, 55] between news events when considering their impacts on stock prices. While there are some LLM works on temporal graphs [53, 61], these works focus on performing question-answering on individual graphs with temporal information in the nodes or edges, and do not handle information across multiple graphs captured at different time-steps."}, {"title": "2 RELATED WORKS", "content": "Relational Stock Prediction Utilizing relational information to predict stock prices have been widely explored in multiple previous works. Early works [13, 14] have studied the use of relational tuples in the form of (Actor, Action, Object, Timestamp) to learn embeddings, such that similar events [13] or similar stock entities [14] would have similar representative vectors. The tuples are generated with rule-based techniques [60] from news headlines, as opposed to the LLM-based method in our work.\nLater works would improve on this using graph-based methods [15, 16, 41], by learning embeddings across a Knowledge Graph to represent stock entities. These works utilize stock relational information from external Knowledge Graphs such as Freebase [7] or Wikidata [47] to train their models. However, these models rely mainly on the static relation information retrieved from a central database, and do not consider possible changes in the connectivity between stocks, that could result from news events.\nIn a more recent work [11], LLMs were used to infer relations between stocks from news headlines, resulting in more dynamic relational data. This information is then used to generate stock embeddings using a Graph Neural Network, which are used to train a deep-learning model for stock prediction. In contrast to this, our work focus on zero-shot reasoning frameworks in order to detect crashes across a portfolio of stocks, that often occur due to events that are unprecedented in historical training data.\nReasoning Frameworks for Large Language Models LLMs are known for their zero-shot reasoning capabilities [29], which has been largely attributed to their ability to generalize knowledge across the large corpus of data it was trained on [9, 56]. To enhance this capability, researchers have proposed frameworks to tackle more advanced tasks, such as ToT [57], GoT [6]. In particular, these works were stated [57] to be inspired by general problem-solving"}, {"title": "3 TEMPORAL RELATIONAL REASONING", "content": "Our TRR framework seeks to emulate the spectrum of human cognitive capabilities that are used together for complex problem-solving (see Figure 2). It consists of four phases: 1. Brainstorming, which generates a graph of sub-impacts on affected entities; 2. Memory, which retrieves relevant past impact chains that contain the same entities; 3. Attention, which extracts the most important impact chains to form a new sub-graph; 4. Reasoning, which reasons over this sub-graph to to determine if a portfolio crash will occur. We will formalize the task and the four cognitive phases in this section.\nFor our task, we start with a portfolio of N stocks, P = {$1, $2,\u2026, $N}, where $s_n$ is a single stock and n \u2208 N. For each day, given a set of J news articles X = {$x_1$, $x_2$, \u2026, $x_J$}, we aim to make a binary prediction on whether a crash will occur on portfolio P on the following day, i.e., \u0177 \u2208 {0, 1}."}, {"title": "3.1 Brainstorming", "content": "To obtain the overall impact of each news event on our selected portfolio, we first brainstorm for all possible chains of sub-impacts that lead to its constituent stocks. We model this as a directed graph G = (Z, A). The set of vertices is Z = {$x_1$,..., $x_J$, $e_1$, , $e_H$, $s_1$,..., $s_N$}, which starts from each news article $x_j$, passes through all impacted sub-entities $e_h$ where h \u2208 H, and ends at the portfolio stocks $s_n$. The set of edges is A, which simply represent the direction of impact between the vertices. Some possible examples are (American Home Mortgage closing most operations, impacts, Mortgage industry) and (Mortgage industry, impacts, U.S. housing market).\nTo generate new vertices for the graph, we iteratively prompt an LLM to generate possible affected entities given each news article or previously affected sub-entity. Hence, for all vertices $z_i$ \u2208 Z at iteration i, we have $z \\sim p_{brainstorm}(z_i|z)$. where k is the number of new vertices generated. This is done iteratively until the chain of impacts reaches a portfolio stock $s_n$, or the max number of iterations I is reached. Repeated entities are merged [6] as a single vertex on the graph.\nFor notation, we also refer to each individual chain of impact as C, where C \u2208 G. Each chain C starts from a news article $x_j$, passes through any number of impacted entities, and ends at a stock $s_n$."}, {"title": "3.2 Memory", "content": "For understanding the temporal context of news events, we equip TRR with a memory module, which stores all previous mentions of impacted sub-entities. We denote the memory module as M = {$M_{e_0}$, $M_{e_1}$,...}, where $M_{e_n}$ is the collection of all the previous impact chains $C_{e_n}$ that contains entity $e_h$. With the memory module, we are then able to perform retrieve and store functions:\n(a) On each day, for each impacted entity that is generated by the LLM, we search the memory module for its previous mentions and add them to our daily graph G, which give us a new temporal contextualized graph $G_{temporal}$. We can obtain: $G_{temporal} = \\bigcup_{v \\in Z}(G \\cup M_{e_n})$.\n(b) At the end of each day, we then update the memory module with the daily chains of impact, i.e., $M_{e_n} = \\bigcup_{v \\in Z}(M_{e_n} \\cup C_{e_n})$, which stores the temporal context for future time periods.\nHere, the additional temporal context allows us to form a temporal relational graph G \u2192 $G_{temporal}$, which represents the Relational+Temporal variant of our TRR framework (see Figure 1).\nIn addition, human collective memory on news events tend to fade over time [1, 19], which can lessen their impact on the market. The temporal decay of memory has previously been modelled with an exponential decay in both the social sciences [19, 31] and LLM works [63]. Following this, we also track the memory retention of each impact with a variable $R_{u,v} = exp(-\\frac{t_{u,v}}{\\lambda})$, where $t_{u,v}$ is the time-step when entity u impacted entity v, and \u03bb is a decay rate constant to be determined. The variable $R_{u,v}$, will be used in the next phase to decide if an impact is considered in the market context."}, {"title": "3.3 Attention", "content": "The overall contextualized graph $G_{temporal}$ is too large to be used in LLMs, given their token limits. While other works deal with this by merging thoughts [6] or finding a single answering path [43], we want to maintain a relational graph of information (see Figure 1) to provide the LLM with an overview of the market. In a similar fashion, the amount of news that investors can process each day is also limited, and their attention is usually focused on more important information [21, 54].\n$G_{temporal}$ contains a network of impact chains, with varying impact strengths on the target portfolio. To obtain the most important information on this network, we draw inspiration from the PageRank algorithm [38] by assigning ranking scores to each entity. This is done by iteratively transferring scores across the entities following the direction of impact, until the convergence of scores. Furthermore, the scores are weighted [52] based on their retention in memory from the previous phase. For an entity $e_h$ \u2208 Z and the set of its parent vertices $B_{e_h}$, the score it receives in each iteration can be formulated as $PR(e_h) = \\sum_{b \\in B_{e_h}} \\frac{PR(b)}{L_b} R_{b,e_p}$, where $L_b$ is the number of outgoing impacts from entity b.\nUsing the ranking scores PR($e_h$), we then filter for all the impact chains C containing the top-q highest scoring entities, where q is to be defined. These are used to form a new sub-graph $G_{TRR}$, which represents the most important information for each day that investors would pay attention to."}, {"title": "3.4 Reasoning", "content": "Finally, to determine if a portfolio crash will occur, we reason over the generated temporal-relational graph $G_{TRR}$. This emulates the reasoning process of investors, who will assess their portfolio risks by considering the most relevant news impacts and how the constituent stocks would be interconnected [3, 12]. Following previous graph-based LLM works [43, 59], we let a LLM reason on the graph $G_{TRR}$ in the form of relational tuples [58]. Each tuple can be formalized as (t, $z_s$, a, $z_o$), where t is the time-step when the impact was generated, $z_s$ and $z_o$ are the subject and object entities, and a is the direction of impact. Given the stock portfolio P and the graph $G_{TRR}$ in the form of tuples, we then prompt the LLM to generate our portfolio crash prediction. This step can be formalized as \u0177 ~ $p_{reason}$(\u0177|P, $G_{TRR}$). An example of the prompt can be found in Appendix C."}, {"title": "4 EXPERIMENTS", "content": "We extensively evaluate TRR across multiple portfolios and time periods to demonstrate its effectiveness. We form two diversified portfolios using common investor strategies: (1) Country-neutral portfolio, where each constituent stock company is based in a different country [40]; (2) Sector-neutral portfolio, where each stock company is from a different market sector [10] (see Appendix A).\nFor the experiments, we select three notable time periods containing events which have caused a big impact on the stock market: (a) June-August 2007 (Global financial crisis); (b) March-May 2010 (Greek government debt crisis); (c) January-March 2020 (COVID-19 stock market crash). Each time period consists of three months, and the mentioned events can be found towards its middle, which allows us to capture news impacts from both the stable (i.e., before portfolio crash) and crash periods."}, {"title": "4.1 Dataset and Evaluation Metrics", "content": "For news data, we use the Reuters financial news dataset [13], which we also extend to the year 2020 to cover the selected time periods. The dataset contains general financial news from Reuters\u00b9 which are not filtered by any stock or country. This allows the LLM to decide by itself if each article is relevant to the target portfolio."}, {"title": "4.2 Baselines", "content": "As the task of detecting portfolio crashes is not widely explored currently, we compare with multiple zero-shot LLM frameworks, such as standard IO prompting, Chain-of-Thought (CoT) [48], Graph-of-Thoughts (GoT) [6] and Think-on-Graph (ToG) [43]. In addition, we also compare with a non-zero-shot deep-learning model (Bi-GRU + Attention [23]) that has been trained on past Reuters news data from the same dataset. While this model seemingly has an \"advantage\" by having task-related training data, we argue that they are not able to handle unprecedented events that has not previously occurred, such as COVID-19. We highlight our implementation of the baselines below.\nDeep-Learning Model:\nBiGRU+Attention [23]: For the deep-learning model, we use a BiGRU+Attention model, that was originally used for stock price classification. As this model requires training data, it was trained on the available Reuters news data before the test period, from October 2006 to May 2007. For training, the independent variables are the news article headlines, while the ground truth are the binary crashes. The model is trained for 50 epochs with early stopping, with a learn rate of 1e-3 and batch size of 32.\nLarge Language Models:\nInput-output (IO) prompting: For this LLM baseline, we simply use the news articles as input and prompt the LLM to generate a crash prediction for the portfolio. As this is a huge amount of text which goes beyond any LLM's token limits, we use only the headlines of the news articles as input.\nChain-of-Thoughts (CoT) [48]: This LLM baseline largely follows the same methodology as IO prompting, but includes an additional line of prompt which instructs the LLM to \"think step-by-step\".\nGraph-of-Thoughts (GoT) [6]: In this LLM baseline, we first provide the LLM with the retrieved news articles, and use GoT to divide and merge thoughts to arrive at a crash prediction for the target portfolio. This is done in a similar fashion as our TRR framework - we first split the portfolio into individual stocks, and prompt the LLM to discover the impact on each stock using sub-thoughts. These thoughts are then combined to find the overall impact on the portfolio. However, the key difference between this method and ours is that the thoughts are not used to form a temporal Knowledge Graph, and that there was no pruning of the combined information, which was done in our Attention phase.\nThink-on-Graph (ToG) [43]: The ToG framework requires an existing factual Knowledge Graph, in order to search for a reasoning path that can tackle the task. To adapt it for our problem, we use the graph of impacts that was formed in our Brainstorming Phase. We then use ToG to find the best reasoning path on this graph to answer the prompt. Following the original work, this is done by identifying the most relevant paths at each depth via a beam search process, and checking if they are sufficient to make a crash prediction. This is repeated iteratively at each depth until the LLM respond that it has sufficient information to make a prediction, or the maximum depth of the graph is reached. When this happens, the most relevant path is then provided to the final LLM to make its prediction."}, {"title": "4.3 Parameter Settings", "content": "For all LLM experiments, we use OpenAI GPT-3.5-turbo to generate the responses, with a temperature setting of 0.0. For the last reasoning phase, we repeat the prediction prompt 5 times for each model and report the average AUROC and standard deviation. For the main experiments, we set 1 to 1 and q to 6. These parameters will be explored further in the ablation study."}, {"title": "5 RESULTS", "content": "Table 2 reports the main results for our task. From the table, we can make the following observations:\n\u2022 The deep-learning model, BiGRU + Attention, shows results that lie close to 0.5. This is because the model predicts mostly False (Note that All-True or All-False predictions produce an AUROC of exactly 0.5). Portfolio crashes are often rare, which causes a large bias towards False predictions when training the model. In addition, it is likely that the model is unable to handle events that are previously unseen in the train set, causing them to predict False on the unprecedented crash events.\n\u2022 Among the thought-based frameworks (i.e., IO, CoT and GoT), we can see a clear rising trend in the AUROC metric. This shows that it is beneficial to break down the task of predicting portfolio crashes into smaller thought processes. Going further, we observe that the search-based ToG was able to outperform these models. As the input dataset was not manually filtered, it is likely that there are numerous news articles that contain noisy information not relevant to the specified portfolio. By first searching for an impact path from the articles to the portfolio, ToG was able to find the most relevant information that can help it to decide if a possible portfolio crash will occur.\n\u2022 Finally, our TRR framework was able to outperform all models, by an average of 10.6% over the strongest baseline (ToG). By considering multiple impact paths that are relevant to the portfolio and also the relationship between these paths, TRR was able to get a more holistic overview of the various market forces on the portfolio, which can help it to detect possible crashes more accurately."}, {"title": "5.1 Ablation Study", "content": "Next, we perform an ablation study by removing the individual components in TRR, which include its relational, temporal (memory) and the memory decay. For removing the relational component, we repeat the results from the ToG experiment, given that generating a relational graph forms the backbone of the TRR model. In our implementation, ToG was used to search for a single path across the graph to make its prediction, and hence does not consider the relations between multiple paths. For removing the temporal component, we remove the memory module (we set $G_{temporal}$ = G), hence not providing any past temporal context to the LLM for reasoning. Finally, for removing the decay component, we set the memory retention to a constant value (we set $R_{u,v}$ = 1 for all u, v). These experiments were conducted over the dataset for year 2007, on the Country-Neutral portfolio."}, {"title": "5.2 Parameter Selection", "content": "For choosing the parameters, we conduct ablation studies over different values of \u03bb and q, which determine the memory decay rate and top-q entities that investors would pay attention to respectively. These experiments were conducted over the dataset for year 2007, on the Country-Neutral portfolio."}, {"title": "5.3 Graph Analysis", "content": "In addition, we explore the generated graphs by visualizing an example from each dataset over the crash periods. For each graph, we project the vertex sizes based on the number of incoming edges. To prevent overcrowding, we label only the top few vertices with the highest number of incoming edges.\nFigure 4 showcases the graphs $G_{TRR}$ generated using the series of news articles, which are used by the LLM to detect portfolio crashes for the next day. From the graphs, we can qualitatively determine that TRR was able to highlight the most important information that caused the portfolio crashes in real life. Within the 2007 dataset, TRR was able to find that the U.S. housing market was impacted more than other entities from the given news, as shown from its higher number of incoming edges. This coincides with the global financial crisis in 2007, which was caused by the housing bubble. From the 2010 dataset, TRR was able to capture the impact on the Greece citizens and the Greek economy, which aligns with the Greek government debt crisis. In the 2020 dataset, the top impacted entities were less obvious as they were spread out over various entities. However, the impacted entities, such as the export and tourism industries, show the impacts that was caused by the COVID-19 pandemic.\nImportantly, we note that the input news articles were not manually filtered in any way. The TRR model was able to find these \"top-impacted entities\" on its own by tracing the impacts of each news event individually, and obtaining the relevance of each impacted entity through the ranking scores."}, {"title": "5.4 Additional Experiments", "content": "From the optimistic results on portfolio crash detection above, we explore the generalizability of TRR to a closely-related task. In Macroeconomics, it is a crucial task to develop warning indicators for economic crisis events [2, 39, 62], in order for policymakers to take preemptive measures to mitigate these events. By viewing the global economy as a network of regional economies, we can utilize our TRR framework to trace the impacts on each individual economy, then reason over these impacts to detect possible widespread crisis globally. For this task, instead of a binary prediction, we prompt the LLM to output the probability of a global crisis, for its application as a continuous warning indicator.\nFor this experiment, we now set our \"portfolio\" P as a series of economies, e.g., the American economy, European economy, Asian economy, etc., and use TRR to trace the impacts of news events to these entities. For the crisis labels, we use the TED spread, which is the difference between the interest rates on interbank loans and short-term government debt. It has been shown in empirical works that a TED spread above 48 basis points is indicative of economic crisis [8], as lenders switch to safer government investments when they believe the risk of default on interbank loans is rising. We label the TED spread above 0.48 as a crisis. In addition, we also provide the past 5 days of TED spread data in the input LLM prompt for context.\nFor the baselines, we use the Financial Stress Indicator (FSI) [39], the volatility index (VIX) [24] and the yield curve (Yield) values [20] from the previous day as indicators. In particular, the FSI is one of the earliest works in economics that use newspaper articles as a financial indicator. This is done by searching the article headlines for financial keywords, such as \"economy\", \"gold\" or \"railroads\"."}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "In this work, we explored the main task of portfolio crash detection, which was difficult to solve before the advent of LLMs, due to the unprecedented nature of crash-related events. We proposed our TRR framework, which is able to do zero-shot reasoning across relational and temporal information through a set of human cognitive capabilities. Through extensive experiments, we show that TRR is able to outperform state-of-the-art frameworks on detecting portfolio crashes. Furthermore, we also explored the generalizability of TRR by using it to develop a crisis warning indicator in macroeconomics.\nThe results of this work open up some possible future directions for research. Firstly, each of the individual components in TRR can be expanded with more specialized techniques. For example, the memory component in TRR can be augmented with a more advanced symbolic database [22]; the PageRank algorithm is also dated and can be replaced with newer information retrieval-based methods [51]. Secondly, for the crisis detection task, more baselines could be studied [2, 26], such as government debt, external trade flows, etc. These statistical indicators could also be used together with our TRR method in a ensemble system, which could help to improve the prediction capability of the warning indicator."}, {"title": "A PORTFOLIO CONSTRUCTION", "content": "To construct our portfolios, we select the top market capitalization stocks from each country or sector that have historical data since 2007 (i.e., newer big companies such as Alibaba Group or Spotify were not considered due to lack of data). We limit the portfolio sizes to 10 due to the LLM token limits, at the time of the experiments. The constituent stocks of the portfolios are found in Table 6 and 7."}, {"title": "B DETAILS OF ADDITIONAL EXPERIMENTS", "content": "We detail the experimental setup for the crisis detection task in this section. For this task, we redefine our portfolio of target entities as a set of regional economies, i.e., P = {American economy, European economy, Asian economy, African economy, Australian economy}."}, {"title": "C EXAMPLES OF LLM PROMPTS", "content": "We provide some examples of the LLM prompts and responses, for better replicability of our work.\nFigure 6 and 7 show the initial and subsequent iterations of the Brainstorm prompt respectively. Given each news article from the dataset, the Brainstorm prompts will first generate the direct impacts, then trace their following knock-on effects repeatedly. This is done until the the chain of impacts reaches a portfolio stock, or the max number of iterations is reached. The responses are then used to form our graph of impacts. When forming our graph, we keep only the impacted entities and remove the explanations, which were only used to elicit a better reasoning process in the LLM (see CoT [48]).\nFigure 8 shows the final generated temporal-relational graph $G_{TRR}$ in its tuple form, and the Reasoning prompt used for analyzing this graph. The tuples are organized first by their relational level in the graph, then temporally by their dates. Similar as above, the generated explanations are used for eliciting a better reasoning process in the LLM, and are not explicitly evaluated in our work. However in this case, the explanations are also used to ensure that the LLM does not refer to past events (e.g., \"A portfolio crash will occur because the dates of the provided events correspond to the 2007 Global Financial Crisis...\"), but contains actual reasoning on the information given in the graph."}]}