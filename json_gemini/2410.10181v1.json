{"title": "SCALABLE MULTI-DOMAIN ADAPTATION OF LANGUAGE MODELS USING MODULAR EXPERTS", "authors": ["Peter Schafhalter", "Shun Liao", "Yanqi Zhou", "Chih-Kuan Yeh", "Arun Kandoor", "James Laudon"], "abstract": "Domain-specific adaptation is critical to maximizing the performance of pre-trained language models (PLMs) on one or multiple targeted tasks, especially under resource-constrained use cases, such as edge devices. However, existing methods often struggle to balance domain-specific performance, retention of general knowledge, and efficiency for training and inference. To address these challenges, we propose Modular Domain Experts (MoDE). MoDE is a mixture-of-experts architecture that augments a general PLMs with modular, domain-specialized experts. These experts are trained independently and composed together via a lightweight training process. In contrast to standard low-rank adaptation methods, each MoDE expert consists of several transformer layers which scale better with more training examples and larger parameter counts. Our evaluation demonstrates that MODE achieves comparable target performances to full parameter fine-tuning while achieving 1.65% better retention performance. Moreover, MoDE's architecture enables flexible sharding configurations and improves training speeds by up to 38% over state-of-the-art distributed training configurations.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advances in large-scale Pre-trained Language Models (PLMs) have showcased impressive generalization capabilities (Brown et al., 2020; Chowdhery et al., 2023; Anil et al., 2023; Team et al., 2023). However, when applied to specialized domains such as medical, legal, or financial sectors, these general-purpose models often require further fine-tuning to maximize performance on target domains (Huang et al., 2023; Li et al., 2023; Singhal et al., 2023).\nA straightforward approach to domain adaptation is full-parameter fine-tuning, where the entire model is further trained on domain-specific data (Houlsby et al., 2019; Bapna et al., 2019). While this method provides strong performance on target domains, full-parameter fine-tuning may lead to catastrophic forgetting where the model loses previously learned capabilities by overfitting to the target domain (Goodfellow et al., 2013; Kirkpatrick et al., 2017). Additionally, this method is memory-intensive to serve in multi-domain settings as each domain has a unique set of parameters, incurring a significant parameter loading overhead when switching between domains (Hu et al., 2021). In such cases, the cost of frequent \u201ccontext switches\u201d significantly impacts performance, making full-parameter fine-tuning impractical for scalable, efficient deployment (Dhar et al., 2024).\nTo address the issues of forgetting and memory-efficiency, parameter-efficient fine-tuning methods have been proposed, such as adapter modules (Houlsby et al., 2019), LoRA (Hu et al., 2021), and CoDA (Lei et al., 2023). These methods introduce a small number of trainable parameters and keep the original model frozen during training. Through targeted parameter updates, these approaches are both computationally efficient and effective at retaining prior knowledge (Biderman et al., 2024). Despite these benefits, parameter-efficient methods are limited in their expressive potential and struggle to scale effectively across large domains and datasets (Niederfahrenhorst et al., 2023)."}, {"title": "2 RELATED WORK", "content": "Domain-Adaptive Pre-training. MoDE is designed for scenarios where large domain-specific unlabeled datasets are available, a process often referred to as \"domain-adaptive pre-training\", \"continual learning\u201d, \u201ccontinued pre-training\u201d, or \u201cfurther pre-training\u201d (Shi et al., 2024; Azerbayev et al., 2023; Colombo et al., 2024; Agarwal et al., 2024). Beyond single-domain adaptation, recent research has expanded into multi-domain adaptation, which presents additional challenges (Saunders, 2022; Wu et al., 2024). To the best of our knowledge, the most relevant works are parameter-efficient fine-tuning methods, which are described in detail in the next paragraph.\nParameter-Efficient Fine-Tuning. Significant progress has been made in parameter-efficient fine-tuning methods, which update only a small subset of model parameters (Houlsby et al., 2019; He et al., 2021). Techniques such as LoRA (Hu et al., 2021) and QLoRA (Dettmers et al., 2023) enhance language model performance by introducing trainable low-rank decomposition matrices. However, these low-rank approaches often fall short when adapting to domains that require high expressiveness, such as mathematical reasoning or coding (Biderman et al., 2024; Niederfahrenhorst et al., 2023).\nMixtures of Experts (MoE) architectures are promising for expanding the capacity of language models while keeping the computational cost of training constant. MoE models learn a routing function that selectively activates a subset of experts, enabling sparse computation (Du et al., 2022; Zhou et al., 2022). Unlike MODE, MoE models are primarily used to enhance pre-training performance (Dai et al., 2024; Team, 2024; xAI, 2024). Recent approaches extend MoE concepts to improve adaptation to new domains, such as life-long learning with distribution-specialized experts (Chen et al., 2023), conditional adapters (Lei et al., 2023), AdaMix (Wang et al., 2022), and MixPHM (Jiang & Zheng, 2023). These approaches modify PLMs at a fine granularity (e.g., by modifying the feed-forward network in the transformer layers). In contrast, MoDE applies MoE at the transformer-level, offering a scalable and expressive solution for multi-domain adaptation.\nFurthermore, MoE-inspired approaches have shown potential in selecting domain-specific adapters by routing input sequences, which enhances model performance across diverse domains while alleviating catastrophic forgetting (Feng et al., 2024). In this work, we focus on token-level routing, with sequence-level routing as a natural extension for future research.\nDistributed Training. Scaling PLMs to billions of parameters requires partitioning parameters and models and training data across multiple processors (Dean et al., 2012; Li et al., 2014; Barham et al., 2022). Common sharding strategies, including model parallelism and data parallelism, evenly divide inputs and parameters across accelerators (Rajbhandari et al., 2020; Lepikhin et al., 2020; Alabed et al., 2024). To address memory overheads and communication bottlenecks, researchers have developed specialized tools to optimize the sharding configuration of a model for the available processors (Lepikhin et al., 2020; Alabed et al., 2024).\nTypically, distributed training and serving frameworks implement the Single Program, Multiple Data (SPMD) model of computation which enables building models and training programs as if developing for a single processor with a large pool of memory. However, SPMD has two key limitations: (i) all operations (i.e., layers of a model) execute sequentially, precluding the execution of different operations in parallel on different processors, and (ii) all operations must use all devices, which may be inefficient (e.g., by incurring significant communication overhead). In contrast, the Multiple Program, Multiple Data (MPMD) computational model has the ability to run multiple programs in parallel on different devices, enabling fine-grained control over parallelism which has the potential for more efficient execution compared to SPMD (Zheng et al., 2022). We exploit parallelism in MODE's structure to implement an MPMD-enabled sharding strategy that accelerates training by up to 38% over SPMD model parallelism."}, {"title": "3 METHOD", "content": "Our proposed model architecture augments PLMs with modular, domain-specialized experts and aims to achieve the following design goals:\n\u2022 Domain specialization. Experts must provide strong performance and high expressiveness on their target domains. Moreover, composing multiple experts with a PLM should result in strong performance on each of the target domains.\n\u2022 Capability retention. The model must preserve the generalist capabilities of the PLM and avoid catastrophic forgetting which may cause performance regressions on other tasks.\n\u2022 Efficient training and inference. The model architecture must be efficient to train on large clusters, and provide advantages for inference on edge devices."}, {"title": "3.1 MODEL DESIGN", "content": "To meet these design goals, we decompose the language model into multiple MoDE blocks (see Figure 1a). Each ith block contains: (i) a few backbone transformer layers, $f_b(\u00b7)$, which are initialized\nFor each i-th block, the input x \u2208 Rd and the output y \u2208 Rd are defined as:\n$y = f_b^i(x) + \\sum_{j=1}^{N} \u03b1_j^i f_e^i(x)$       (1)\nwhere $[b_b, \u03b1_1^i, \u03b1_2^i, \u2026, \u03b1_N^i] = g^i(x)$\nBackbone Model. The backbone transformer layers, $f_b^i(x)$ are derived from PLMs. In each MoDE block, the number of backbone layers is determined by dividing the total number of layers in the original PLM by the total number of MODE blocks. The number of MoDE blocks is an important configuration hyperparameter: more blocks enables more frequent synchronization between the backbone and expert layers which provides greater flexibility. In Table 3, we conduct an ablation to assess the impact of the number of blocks on accuracy. In MoDE, the backbone layers are initialized from PLMs and remain frozen during training to mitigate the issue of catastrophic forgetting.\nExperts. Each expert, $f_e^i(x)$ consists of several transformer layers, and the number of transformer layers impacts the expert's performance. Table 3 indicates that increasing the number of expert layers improves performance on the target domain at the cost of higher computational cost. We use transformer layers for experts for two reasons: (i) transformer blocks scale more efficiently than finer-grained designs, such as modifications within attention matrices, and (ii) they simplify deployment, as there are no modifications to the transformer architecture itself. In contrast, fine-grained methods often face deployment challenges (Yi et al., 2023).\nGating Function. The outputs from the backbone and expert layers are combined through the gating layer, $g^i(x)$. In this work, we use a simple token-level gating function. This function consists of linear layer followed by a softmax for normalization. The gating function takes x as input and outputs a vector of length N + 1 as follows:\n$g^i(x) = softmax(Linear(x))$ (2)\nThe simplicity of MoDE's gating function enables efficient composition of multiple experts, as shown in Table 1. However, future work could explore more advanced designs, such as sparse gating and sequence routing, to further optimize memory usage and computational efficiency."}, {"title": "3.2 TRAINING PROCEDURE", "content": "MODE has a two stage training procedure (Figure 1b): (i) train a single expert independently on each domain, and (ii) compose different experts into a single model to improve multi-domain performance.\nSingle Modular Expert Training. For each domain, we independently train an MoDE model with a single expert. During training, only the expert layers and gating layers are updated, while the"}, {"title": "3.3 PARALLELIZING BLOCK EXECUTION", "content": "We exploit parallelism in MoDE's structure to enable flexible sharding configurations supported by the MPMD model of computation. These sharding configurations improve training efficiency due to:\n1. The ability to execute the backbone and experts on distinct, smaller sets of devices which reduces communication overheads (Figure 2).\n2. The ability to configure the backbone and experts independently to maximize overall efficiency, when their number of parameters are different.\nWhile MPMD has the potential to reduce communication overheads by running blocks on fewer devices, the merges in the MoDE model architecture are points of synchronization that require communication across all devices. To determine whether MPMD is beneficial for training, we evaluate whether MPMD configurations can afford the cost of resharding between meshes in Section 4.4.\nWhile MPMD has the potential to reduce communication overheads by executing blocks on smaller sets of devices, the linear combination of the outputs from the backbone and expert blocks requires are points of synchronization that require communication across all devices. To assess the effectiveness of MPMD in training, we evaluate if the benefits of MPMD configurations outweigh the costs associated with resharding between device meshes, as discussed in Section 4.4.\nMoreover, MODE's architecture enables several benefits during inference for privacy and performance. By using MPMD to run MoDE with the backbone and expert parameters on different devices, expert parameters for sensitive domains (e.g., personal information, medical data, etc.) can remain private (e.g., by executing on a user's phone). To switch to another MoDE model adapted to different task, only the new expert weights need to be loaded into memory because the backbone PLM's weights are shared. This reduces the overhead of loading model weights and works well with existing techniques developer to serve many adapters concurrently, such as SLORA (Sheng et al., 2024)."}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENT SETUP\nPLM Configuration. Our PLM model consists of 1.58 billion parameters distributed across 18 transformer layers, comparable to smaller open-source models such as Gemma (Team et al., 2024), Phi (Gunasekar et al., 2023), and Llama 3.2 (Meta, 2024). The model is pre-trained on a high-quality dataset that spans a diverse range of natural language use cases, similar to GPT-3 (Brown et al., 2020), GLaM (Du et al., 2022), and LLaMA (Touvron et al., 2023).\nMulti-Domain Datasets. We prepare two target domain datasets, Code and Math, to evaluate multi-domain adaptation, and one retention dataset, English, to measure catastrophic forgetting:\n\u2022 Code consists of code samples retrieved from real-world applications.\n\u2022 Math tests the model's mathematical reasoning capabilities and is similar to the GSM (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) datasets.\n\u2022 English contains literature texts with a distribution distinct from Code and Math.\nAdditionally, we create a mixed dataset, Math + Code, for model training, which contains an equal number of samples from the Math and Code datasets."}, {"title": "4.2 MULTI-DOMAIN ADAPTATION", "content": "Overall Performance. We evaluate MODE, LoRA, and full-parameter fine-tuning on the Math, Code, and English test sets. We train the baseline models directly on the mixed dataset, Math + Code. For MODE, we follow the training procedure described in Section 3.2, where individual MoDE models are first trained on Math and Code training sets independently, and then composed using the Math + Code dataset. As shown in Table 1, MoDE with two experts (one for Math and one for Code) achieves the best overall performance and retention capabilities. On average, MoDE outperforms full-parameter fine-tuning by 0.59% and LoRA by 1.41%. Notably, MoDE achieves target domain performance comparable to full-parameter fine-tuning, outperforming it by 0.28% on Math while trailing by 0.06% on Code, and provides a 1.55% improvement in retention on the English dataset. Compared to LoRA, MODE delivers a higher accuracy on the target domains, with a 1.68% improvement on Math and 2.12% on Code, and surpasses LoRA by 0.45% in English retention.\nMODE Configurations. We address two questions in exploring different configurations:\n1. Which configuration of MoDE modular experts provides the best performance?\n2. Which composition strategy yields the best multi-domain performance?\nFor the first question, we evaluate MoDE with only one uninitialized expert and vary two hyperparameters: the number of MODE blocks, and the number of transformer layers per expert within each block. As shown in Table 3, increasing the total number of expert layers leads to higher accuracy in the target domain. To balance the number of added parameters, domain-specific accuracy, and"}, {"title": "4.3 SCALABILITY OF MODE", "content": "We evaluate the scalability of MoDE along two dimensions: scaling with additional parameters and scaling with increased training examples. We use LoRA as the baseline in both experiments and conduct experiments on a single domain using the Code dataset for training and evaluation.\nScalability with Additional Parameters. We compare the accuracy improvements of MODE and LORA as additional parameters increases. For LoRA, we increase parameters by increasing the rank of the decomposition matrices, ranging from 8 to 2,048. For MoDE, we increase the number of transformer layers per expert while keeping the number of MoDE blocks constant. As shown in Figure 3, while LoRA is parameter-efficient, it struggles to convert additional parameters into higher accuracy. In contrast, MoDE shows a continuous improvement in accuracy as more parameters are added, indicating that MoDE scales more effectively with more parameters.\nScalability with Training Examples. We evaluate how well MODE and LoRA leverage additional training data to improve accuracy. Different versions of the Code dataset were created, each containing varying amounts of training examples. Both MoDE and LoRA were trained on these dataset versions, with the number of training steps keep the same across all experiments. As shown in Figure 3, we"}, {"title": "4.4 ACCELERATING TRAINING", "content": "Through our experiments, we find several MPMD-enabled sharding configurations which increase training speeds by up to 38% over SPMD model-parallelism. The results confirm that the reduction in communication costs by running backbones and experts on fewer accelerators outweighs the added costs of resharding across meshes when merging the intermediate outputs of models.\nAll experiments use 8 TPUv5e accelerators which are managed by an orchestration layer that supports MPMD (Anonymized, 2024). We implement MPMD training by extending a distributed machine learning framework (Anonymized, 2024) implemented in Jax (Bradbury et al., 2018). While we use TPUs and a Jax-based framework as our training environment, we underscore that the key idea of using MPMD to increase training speed in distributed settings extends to other accelerators (e.g., GPUs) and training systems such as PyTorch (Paszke et al., 2019). We measure the training performance, but note that many of the performance improvements enabled by MPMD sharding strategies may also benefit model serving. In details, we compare the following configurations:\n1. MPMD divides the TPUs into different meshes to which backbone or experts are assigned.\n2. SPMD creates a single mesh which consists of all 8 TPUs, backbone and experts.\n3. SPMD-pjit uses our training library's to parallelize model training.\nCost of Resharding. We compare the following configurations, which train MoDE using 4 experts where each consists of 6 transformer layers divided evenly into two blocks:\n1. 1 mesh uses model parallelism to shard the model across a single mesh.\n2. 3 mesh assigns the backbone to 4 TPUs and each expert shares 2 TPUs with another expert. The backbone and the experts are sharded on their meshes using model parallelism.\n3. 5 mesh assigns the backbone to 4 TPUs, and one expert to each of the remaining TPUs. The backbone is sharded with model parallelism.\nWe find that the cost of resharding between meshes is lower than the cost of model parallelism (Figure 4a). The reduction in training step latency from 329 ms for 1 mesh to 204 ms, a 38% reduction, results due to the reduction in communication caused by the MPMD sharding configuration. Because the latency of the 3 mesh and 5 mesh configurations is similar, we conclude that the cost to reshard does not increase with the number of meshes.\nConfiguring the Expert Size. We configure a mixture of 2 expert divided into two blocks and change the total number of transformer layers added. For example, setting the expert block size to 2 transformer layers adds 8 total transformer layers to the model. The backbone is assigned to 6 TPUs with a model parallel sharding, and each expert is assigned to one unoccupied TPU."}, {"title": "5 CONCLUSIONS", "content": "Adapting pre-trained language models (PLMs) to complex, multi-domain settings is increasingly important as these models are deployed in specialized environments requiring diverse capabilities. To address this challenge, we propose Modular Domain Experts (MoDE), a scalable technique for adapting PLMs to multi-domain tasks. MoDE introduces modular, domain-specialized experts while preserving the general knowledge of the PLM by keeping its weights frozen. MoDE allows experts to scale with both the number of parameters and the amount of training data, outperforming standard parameter-efficient methods like LoRA by 1.4% on a challenging multi-domain dataset. Additionally, MODE delivers competitive performance compared to full-parameter fine-tuning, achieving 1.5% better retention of general capabilities and 0.6% higher accuracy across all subdomains. To further optimize training efficiency, MoDE 's architecture supports flexible sharding strategies through MPMD, resulting in up to 38% faster training compared to standard distributed training methods. The ability to compose and reuse modular experts represents a significant advancement in adapting PLMs to complex domains. We hope that our work inspires further research into building modular, expert-based language models."}]}