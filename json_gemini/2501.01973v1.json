{"title": "INFELM: In-depth Fairness Evaluation of Large Text-To-Image Models", "authors": ["Di Jin", "Xing Liu", "Yu Liu", "Jia Qing Yap", "Andrea Wong", "Adriana Crespo", "Qi Lin", "Zhiyuan Yin", "Qiang Yan", "Ryan Ye"], "abstract": "The rapid development of large language models (LLMs) and large vision models (LVMs) have propelled the evolution of multi-modal Al systems, which have demonstrated the remarkable potential for industrial applications by emulating human-like cognition. How-ever, they also pose significant ethical challenges, including ampli-fying harmful content and reinforcing societal biases. For instance, biases in some industrial image generation models highlighted the urgent need for robust fairness assessments. Most existing eval-uation frameworks focus on the comprehensiveness of various aspects of the models, but they exhibit critical limitations, includ-ing insufficient attention to content generation alignment and social bias-sensitive domains. More importantly, their reliance on pixel-detection techniques is prone to inaccuracies.\nTo address these issues, this paper presents INFELM, an in-depth fairness evaluation on widely-used text-to-image models. Our key contributions are: (1) an advanced skintone classifier incorporating facial topology and refined skin pixel representation to enhance classification precision by at least 16.04%, (2) a bias-sensitive con-tent alignment measurement for understanding societal impacts, (3) a generalizable representation bias evaluation for diverse de-mographic groups, and (4) extensive experiments analyzing large-scale text-to-image model outputs across six social-bias-sensitive domains. We find that existing models in the study generally do not meet the empirical fairness criteria, and representation bias is generally more pronounced than alignment errors. INFELM estab-lishes a robust benchmark for fairness assessment, supporting the development of multi-modal AI systems that align with ethical and human-centric principles.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in large language models (LLMs) have facili-tated the rapid evolution of multi-modal AI systems, which inte-grate information across diverse modalities, such as text, images, audio, graphs, and more. In contrast to purely LLMs, multi-modal AI agents learn and process data in ways that more closely resem-ble human cognition, enabling them to generate content that is often more accessible and interpretable to the general public. This poses significant ethical challenges, particularly as they have the potential to amplify the spread of harmful content and reinforce existing social bias.\nAs large multi-modal models, particularly those represented by large vision models (LVMs), become increasingly integrated into a wide range of industrial applications, it is essential to develop the accurate and in-depth assessment of commonly used models to ensure they behave in accordance with human intentions. Due to the critical importance of assessing fairness in Al models, sub-stantial attention and efforts have been directed toward this area. For example, [8, 13, 16] provide the comprehensive survey of bias evaluation and mitigation for LLMs. In the realm of multi-modality models, pioneering works such as HEIM [12] and Dall-eval [7] evaluate different dimensions of the text-to-image models, such as fairness, toxicity, bias, and more. Particularly for fairness testing, HEIM measures the bias by detecting gender expression and skin pixels in images generated by a variety of LVM models. However, we identified several limitations that prevent those works from providing an accurate and in-depth fairness analysis and bench-mark to be widely adopted across the industry: (L1) previous work focuses on measuring the inclusiveness and demographic repre-sentation of existing text-to-image generated images. However, the severe diversity issue caused by Google Genmini's text-to-image feature indicates that content alignment should be an indispens-able dimension of model fairness, i.e., whether individual generated images accurately align input prompt demographics. (L2) the test-ing scenarios focus on a narrow set of target domains, primarily around social group representations between different occupations. (L3) the pixel-detection-based method could give inaccurate results (see the example in Figure 1), which may impede the accuracy of follow-up assessment results.\nTo address (L1) and (L2), we measure text-to-image model fair-ness from the perspective of both demographic representation bias and content alignment errors, and then perform in-depth fairness testing over a comprehensive prompt set designed specifically for these two dimensions, composing in total 6 target domains. We further develop a novel skintone classifier to tackle (L3) by address-ing four challenges associated with the existing approaches: (C1)"}, {"title": "2 RELATED WORK", "content": "AI alignment: fairness governance There are surveys [8, 10] that focus on Al model fairness, ethics, bias evaluation and mit-igation for LLMs. [15] provides a comprehensive survey of key aspects that contribute to the trustworthiness of LLMs in terms of alignment. HELM [13] introduces the holistic evaluation of 30 foundation language models with respect to 7 metrics, including ac-curacy and fairness, on 16 scenarios (use cases). HEIM [12] extends the evaluation to text-to-image models. These studies underscore the growing need to ensure that Al systems are both equitable and aligned with societal values, as they become increasingly integral to decision-making processes across diverse applications. There are also efforts from industry that aim to address AI bias and ensure equitable outcomes. For example, Aequitas [24] and AI Fairness 360 [5] from IBM are toolkits developed to facilitate fairness re-search and evaluaiton on small datasets. LinkedIn [19] addresses the issue of operationalizing fairness for recommendation or feed models on scale by disentangling fairness into equal treatment and equitable product expectations separately, rather than trying to reach their trade-off.\nSkintone classification Skintone detection and classification is the foundation of ethical alignment, as accurate skintone classification ensures high-quality analysis of Al model performance across multi-ple aspects. Most existing skintone detection methods are based on the aggregation of dominant color pixels of human body [12] and mapping to the predefined class. For example, HEIM [12] identifies the skin pixels from the RGBA and YCrCb space, and then takes the mean value to map to the nearest monk scale [17] based on Eu-clidean distance. DALL-EVAL [7] takes illumination into account to improve the prediction accuracy. There are also evaluation studies that are based on small batches of samples with human annota-tion [6]. Unfortunately, we found that this line of methods could lead to inaccurate skintone classification, as the example shown in Figure 1. In practice, we identified four reasons behind (C1 - C4), and illustrate the qualitative comparison against the exsting methods in Table 1."}, {"title": "3 PRELIMINARIES", "content": "Fairness Fairness is a broad and subjective term that can have many definitions depending on the context, including how it might be applied within policy frameworks to algorithmic impact assess-ments for a wide variety of algorithmic modalities. In this work, we focus on ensuring that Al technology does not introduce or reinforce harmful biases, and that we should strive to be inclusive in our development and use of Al technologies. [XL: Legal/policy to suggest ref here] Source: TikTok Responsible AI Principles. This manifests itself as part of a risk-based approach towards responsible, safe, and fair algorithmic governance.\nWe first provide the mathematical expression of fairness. Given specific demographic groups $g\u2208 G$, each group should have the same acceptance rate, i.e.,\n$P(x = 1|g = g_1) = P(x = 1|g = g_2) = ...P(x = 1|g = g_i), Vg\u2208 G$\n$P(x = 0|g = g_1) = P(x = 0|g = g_2) = ...P(x = 0|g = g_i), Vg\u2208 G$ (1)\nNote that \"equalizing acceptance rate\" refers to the rate at which a model accepts or approves outcomes is equal. Also, we specify that the model should perform similarly on both positive and negative predication, i.e., the same true positive and false positive. It is also acceptable to have slight slacks between groups in practice:\n$| |P(x = 1|g = g_i) \u2013 P(x = 1|g = g_j)| \u2264 \u20ac, \u2200g \u2208 G$\n$|P(x = 0|g = g_i) \u2013 P(x = 0|g = g_j)| \u2264 \u20ac, \u2200g \u2208 G$ (2)\nWhere e is the acceptable threshold that is usually a small value. For example, if the acceptance criteria for the disadvantaged group should be at least within 80% of the other group, then e = 0.2.\nTarget domains. The target domains define the thematic areas within which testing scenarios are constructed to evaluate the behavior of text-to-image models. In this work, target domains"}, {"title": "4 METHOD", "content": "We leverage RealisticVision v5.1 [3] to generate 2,000 images for training and 400 for testing for each group. Also, we specified different environments such as dim or bright so that the trained topology classification model learns robust latent features invariant from the external effects. We also specify diverse demographics such as ages and genders, etc. An example prompt is as follows.\nA native American, {thin, average, large body shape, {male, female}, {dim, natural, bright} environment, {young, middle, senior} age, looking at camera, casual, potrait\nBased on the synthetically generated images, INFELM adopts a CNN-based classification model that includes 3 2D convolutional layers and 2 fully-connected layers. As the output, the latent topo-logical features are invariant regardless the external lighting, which might affect the accuracy of skintone colors detected. The model architecture is shown in Figure 7 of the appendix, only the latent topological features are used for the next stage.\n4.2.2 Dominant skin pixel extraction. To accurately identify the skin regions based on color values, INFELM focuses on the facial region in the image and it adopts the Otsu's method to distinguish skin pixels from non-skin pixels in the YCbCr and HSV color space. Then, it represents the detected dominant K = 15 pixels in a dis-tribution format, where each bin corresponds to a segment of an equally divided range of the predefined skintone scales, and the weight of each bin corresponds to the total area of that pixel. Fur-thermore, the features represented by the pixel distribution are highly compatible with our proposed deep learning solution, as they preserve more detailed information compared to the existing aggregation techniques, such as computing the mean. An exam-ple is shown in Figure 3. The dominant pixel distributions will be used to supplement the latent facial topological features in the final skintone detection.\n4.2.3 Skintone classification. INFELM takes the outputs from the above two components, and then performs fusion on the aforemen-tioned features as the input to learn skintone scales. It jointly learns the latent facial topology class and the annotated skintone scale, if"}, {"title": "4.1 Gender Classification", "content": "We leverage the VIT-based gender classification model [1] to auto-matically classify the character gender expression in an image. The model performance is reported to be > 95% in precision."}, {"title": "4.2 Skintone Classification", "content": "We propose a deep learning solution to address the non-linear boundaries incurred by the unevenly-distributed color scaling (C1). To mitigate the illumination disturbance (C2), the skintone classifier leverages the latent facial topological features based on studies from human and social geography as the supplementary to dominant skin pixels. This is achieved by generating diverse synthetic facial images to extract the learned latent embeddings, and then performing feature fusion with the skin pixels as the input to the skintone classification module. Our approach is highly expressive and can be tailored to different scaling with subjective annotations and image styles (C3 and C4).\n4.2.1 Synthetic facial image generation and topology classifier. In or-der to ensure appropriate diversity in the baseline synthetic dataset used to train the skintone classification model, we leveraged the 1997 OMB standards on ethnicity from the U.S. Census Bureau\u00b9 to generate images containing human faces that cover the follow-ing 6 broad groups on the basis of geographic origins: European, Asian, Pacific islanders, African, South Asian, Native American (this category origins from 5 OMB race categories, see Section 7.1 for more details). Note that we recognize the limitations of racial and ethnicity categories, and for this reason have limited the use of the Census Bureau categories to only ensure a baseline level of diversity in the underlying dataset and have not applied these cate-gories in any other way. This categorization is also recognized by the large vision model employed for generating synthetic images. Consequently, the generated images are diverse while exhibiting high accuracy."}, {"title": "4.3 Fairness testing framework", "content": "The overview of INFELM is illustrated in Figure 2. First, INFELM takes the prompts from specific cross-domain scenarios as the input to text-to-image models to generate images. This image generation process has been made automated and conducted at scale. Then, the generated images are labeled by the demographics (gender and skintone) classifier. In the end, INFELM calculates fairness metrics for representation bias and content alignment, as the basis to measure the model fairness degree.\n4.3.1 Testing prompts. The testing prompts are constructed to eval-uate the performance and fairness of text-to-image models across a variety of socially sensitive domains, emphasizing professions, so-cioeconomic status, education, physical appearance, and behavioral descriptors. The prompts are designed to reflect diverse occ\u0441\u0438\u0440\u0430-tions, from high-status roles such as doctors, lawyers, and CEOs to service and labor-intensive jobs like janitors, fast-food work-ers, and housekeepers. This ensures a comprehensive analysis of potential occupational biases. Socioeconomic prompts range from wealthy individuals, such as billionaires and magnates, to impover-ished and socially vulnerable groups, allowing for the assessment of class-based stereotype amplification. Additionally, prompts for stu-dents from prestigious universities, as well as generic educational roles, are included to explore biases linked to academic representa-tion. Criminal descriptors, such as prisoners and ex-convicts, are tested to uncover any harmful associations or over-generalizations. Prompts involving physical attractiveness and personality traits aim to identify any disparities in how models portray people based on perceived beauty or behavioral attributes. This strategic selection of prompts enables a thorough examination of the models' outputs for fairness, inclusiveness, and social impact, ensuring the generated content does not perpetuate harmful stereotypes or amplify societal biases.\n4.3.2 Fairness metrics. We evaluate model fairness from two per-spectives: content alignment error and representation bias. Content alignment error reflects how accurately the demographics of the generated content align with the input prompts, serving as a criti-cal prerequisite for accurate follow-up analyses. In this work, we quantify content alignment error by calculating the mismatch ra-tio between the input prompts and the demographic classifier's predictions across all generated images. This metric ensures a sys-tematic evaluation of the model's ability to faithfully translate input prompts into demographically accurate outputs.\nRepresentation bias quantifies the discrepancy between the ac-tual and groundtruth demographic distributions in the generated images, where the groundtruth could be social statistics from au-thoritative sources, such as U.S. Bureau. In this work, we assume that the demographic groups are equally distributed. Intuitively, significant deviations indicate the presence of bias in the model, suggesting that the model tends to favor or disfavor generating images with certain demographic characteristics. We numerically"}, {"title": "5 EXPERIMENTS & RESULTS", "content": "We first describe our experimental setup and the datasets and base-line methods used in our empirical analysis, and then show quantita-tive improvements from our sampling method and a closer ablation study. Specifically, we aim to answer two research questions. R1. as the fundamental capability for the downstream fairness analy-sis, how well does INFELM perform on the skintone classification task with the existing challenges (C1-C4), and R2. what is the current fairness status of the text-to-image models measured by demographic representation bias and alignment errors from social bias-sensitive domains?\nData. For representation bias tests, we construct the whole set of prompts by combining the target domains with background de-scriptive texts (e.g., \"a doctor, portrait, natural light\"). For content alignment tests, we further annotate the prompts with demograph-ical descriptive information (e.g., \"a female lawyer, a black CEO, an Asian firefighter, etc.\"). Totally, there are 246 prompts. We then run all the models M to generate 100 images per prompt for the down-stream analysis. We run all models on the large-scale computation platform equipped with 4 Tesla V100 GPUs, each featuring 32 GB memory and optimized for deep learning workloads.\nBaselines. Main baselines: HEIM [12] and VIT [20]. We apply HEIM to the WBB dataset by adding a post-mapping and reporting the optimal performance. For VIT, we leveraged ChaptGPT-40 to provide the accurate descriptive phrases for the 3 groups in WBB, and the 10 Monk scales for High-Aes.\nSetup & Evaluation. To comprehensively evaluate the model per-formance as the ordinal classification on different datasets, we leverage precision, recall, and mean square error (MSE) due to the inherent ordering between the labels. The precision and recall are computed considering the labeling tolerance, and we normalize MSE with the square of the maximum scale difference given by the dataset, see Equation 6.\n$MSE = \\frac{1}{N(S) - 1)^2} \\sum_{i=1}^{N} (Yi-Yi)^2$ (6)\nwhere S denotes the whole set of skintone scales."}, {"title": "5.1 Skintone classification", "content": "To address R1, we first evaluate our proposed skintone classifier against the state-or-the-art methods as the foundation of extensive fairness analysis.\nSetup In this experiment, we leverage both the publicly available and the internal facial image datasets. The WBB 2 open-source dataset contains images of human skintones categorized into three classes: White, Brown, and Black. High-AES is the company internal dataset that contains 11000 synthetic images with human annotations following the Monk Scales. For all datasets, we filter out images with low-quality facial regions. The stats of train/test split are given in Table 5."}, {"title": "5.2 Text-to-image model fairness analysis", "content": "We report the model fairness performance in Table 7. At first glance, we find that in terms of the fairness of generated content, most text-to-image models in the study have bias risks, and there is no single model that consistently performs the best on each evaluation metric. Dall-E 3 achieves the best performance on gender fairness, but its performance on skintone is comparatively suboptimal. The fine-tuned model Openjourney v4 performs the best on skintone,"}, {"title": "5.3 Findings & takeaways", "content": "In this section, we summarize a few takeaways based on the above extensive experimental results. We illustrate the overall model per-formance from the perspective of both content alignment error and representation bias in a 2-D plot, and compare them with the fairness reference computed using the empirical four-fifth rule, . To put it in our context concretely, if the model performance for one demographic group G is more than 20% different than the performance for another demographic group, it's considered the model is biased over the two groups. For demographic bias metric b, the four-fifth rule could be directly applied with the threshold being set at 0.2, where a fair model promises b < 0.2. For content alignment error, we extend the usage of four-fifth rule, to further require at least 80% of the generated content should be accurate in order to make the model fair, where a well aligned and accurate model promises e < 0.2.\nTakeaway 1 From the figure, we find that most models do not meet the criteria of fairness under the four-fifth rule, with Dall-E 3 being the closest. The generated content exhibits a demographi-cally polarized pattern given the prompts in the study, indicating potential data bias during the training or fine-tuning phase.\nTakeaway 2 Model representation bias is generally more pro-nounced than alignment errors, and among the alignment error metric, skintone alignment error is significantly higher than gender. This is likely due to the inaccuracies in captions describing skintone in the images during model training or fine-tuning. Our proposed skintone classification method could improve by providing large-scale labeled corpus with accurate skintone information.\nTakeaway 3 Although there are exceptions, the fine-tuned mod-els RealisticVision variants generally exhibit inferior fairness per-formance compared to their base Stable Diffusion models. Realis-ticVision variants are fine-tuned on a much smaller, curated dataset of photorealistic images to enhance the ability to produce realistic"}, {"title": "6 CONCLUSION", "content": "Governing fairness in text-to-image models is not just a technical challenge but also a driving force behind corporate responsibility in the age of AI. Accurate and in-depth fairness risk assessments are essential to identifying and mitigating these biases, ensuring that such systems align with ethical principles and serve all users equitably. This paper addresses these critical concerns by providing an evaluation within industrial settings. In INFELM, we improve the precision of skintone classification by leveraging facial topolog-ical information and enhance fairness evaluation through metrics that incorporate bias-sensitive prompts and demographic diversity. Extensive experiments conducted on large-scale datasets reveal that most existing text-to-image models do not meet the criteria of fairness under the empirical four-fifth rule, providing insights for developing more equitable Al systems. One future work is to extend the evaluation framework to additional modalities, such as audio and video, and incorporate real-world user feedback to further refine fairness metrics."}, {"title": "7 APPENDIX", "content": "42.  The monk skin tone scale. 2019.\n Facial topological features and extraction\nThe 1997 Office of Management and Budget (OMB) standards on race and ethnicity include the following 5 social groups: (1) White, (2) Black or African American, (3) American Indian or Alaska Native (4) Asian, and (5) Native Hawaiian or Other Pacific Islander.\nWe follow the above 5 groups and supplement with the South Asian group so that the images generated cover rich facial charac-teristics that are representative for INFELM to learn the latent topo-logical features from, and compatible to the text-to-image model to generate synthetic images. Note that the categorization adopted in this work follows the OMB guideline that the categories only reflect a social definition of race recognized in the U.S., and it is not a biological categorization. The model architecture to derive the facial topological features is as follows."}, {"title": "7.1 Facial topological features and extraction", "content": "The 1997 Office of Management and Budget (OMB) standards on race and ethnicity include the following 5 social groups: (1) White, (2) Black or African American, (3) American Indian or Alaska Native (4) Asian, and (5) Native Hawaiian or Other Pacific Islander.\nWe follow the above 5 groups and supplement with the South Asian group so that the images generated cover rich facial characteristics that are representative for INFELM to learn the latent topological features from, and compatible to the text-to-image model to generate synthetic images. Note that the categorization adopted in this work follows the OMB guideline that the categories only reflect a social definition of race recognized in the U.S., and it is not a biological categorization. The model architecture to derive the facial topological features is as follows."}, {"title": "7.2 Complete fairness analysis", "content": "We list the complete set of fairness analysis results per text-to-image model included in this work in Figure 7."}]}