{"title": "Large Language Models Can Help Mitigate Barren Plateaus", "authors": ["Jun Zhuang", "Chaowen Guan"], "abstract": "In the era of noisy intermediate-scale quantum (NISQ) computing, Quantum Neural Networks (QNNs) have emerged as a promising approach for various applications, yet their training is often hindered by barren plateaus (BPs), where gradient variance vanishes exponentially as the model size increases. To address this challenge, we propose a new Large Language Model (LLM)-driven search framework, AdaInit, that iteratively searches for optimal initial parameters of QNNS to maximize gradient variance and therefore mitigate BPs. Unlike conventional one-time initialization methods, AdaInit dynamically refines QNN's initialization using LLMS with adaptive prompting. Theoretical analysis of the Expected Improvement (EI) proves a supremum for the search, ensuring this process can eventually identify the optimal initial parameter of the QNN. Extensive experiments across four public datasets demonstrate that AdaInit significantly enhances QNN's trainability compared to classic initialization methods, validating its effectiveness in mitigating BPs.", "sections": [{"title": "Introduction", "content": "In recent years, there have been significant advancements in quantum computing, particularly with the advent of noisy intermediate-scale quantum (NISQ) devices (Preskill, 2018). Within this research landscape, quantum neural networks (QNNs), which integrate quantum circuits with classical deep-learning layers, have been widely applied in various domains, such as quantum machine learning (Stein et al., 2021; Liang et al., 2024), quantum physics (Chen et al., 2022), and quantum hardware architecture (Liang et al., 2023). However, recent studies reveal that the performance of QNNs may be hindered due to gradient issues, such as barren plateaus (BPs) (McClean et al., 2018), referring to a kind of gradient issue that the initialization of QNNS might be trapped on a flattened landscape at the beginning of training. McClean et al. (2018) first systematically investigate BPs and affirm that the gradient variance will exponentially decrease as the model size increases when the QNNs satisfy the assumption of the 2-design Haar distribution. Under this circumstance, most gradient-based approaches would fail. To better illustrate the BPs' mitigation process, we present an example in Fig. 1."}, {"title": "Methodology", "content": "In this section, we first introduce the preliminary background and formally state the problem we aim to address in this study. Besides, we present our proposed framework in detail and conduct a theoretical analysis of the expected improvement (EI)."}, {"title": "Preliminary Background", "content": "Variational Quantum Circuits (VQCs) play a core role in quantum neural networks (QNNs). Typical VQCS consist of a finite sequence of unitary gates U(\u03b8) parameterized by \u03b8 \u2208 RLNR, where L, N, and R denote the number of layers, qubits, and rotation gates. U(\u03b8) can be formulated as:\n\nU (\u03b8) = U(\u03b8,1,..., \u03b8L) = \\prod_{l=1}^{L}U_{l}(\u03b8_{l}),\n(1)\n\nwhere Ul(\u03b8l) = e\u2212i\u03b8\u03b9VI.\nQNNS, which are built by wrapping neural network layers with VQCs, can be optimized using gradient-based methods. To optimize QNNs, we first define the loss function E(\u03b8) of U(\u03b8) as the expectation over Hermitian operator H:\n\nE(\u03b8) = \u27e80|U(\u03b8)\u2020HU(\u03b8)|0\u27e9.\n(2)\n\nGiven the loss function E(\u03b8), we can further compute its gradient by the following formula:\n\n\u2202E(\u03b8)/\u2202\u03b8_{k} = i\u27e80|U\u2020 [Vk, U\u2021HU+] U_{0}\u27e9,\n(3)\nwhere we denote U\u2212 = \u03a0k\u22121l=1Ul(\u03b8l)Wl and U+ = \u03a0L l=k+1Ul(\u03b8l)Wl. Also, U(\u03b8) is sufficiently random s.t. both U\u2212 and U+ (or either one) are independent and match the Haar distribution up to the second moment.\nBarren Plateaus (BPs) are first investigated by (McClean et al., 2018), who demonstrate that the gradient variance Var[\u2202E] of QNNs will exponentially decrease as the number of qubits N increases when the random QNNS match 2-design Haar distribution. This exponential pattern can be approximated as:\n\nVar[\u2202E] \u221d 2^{\u2212N}.\n(4)\n\nThe Eq. 4 indicates that Var[\u2202E] will approximate zero when the number of qubits N is very large, i.e., most gradient-based approaches will fail to train QNNs in this case.\nBased on the above description, we formally state the problem that we aim to solve as follows:\nProblem 1. By leveraging a generative AI (GenAI) model, such as an LLM, as a Bayesian posterior estimator with adaptive prompting, we aim to iteratively identify the optimal QNN's parameter \u03b8\u2217, where a given QNN is initialized with \u03b80, which can maximize gradient variance Var[\u2202E] during training, thereby mitigating barren plateaus (BPs)."}, {"title": "Our Proposed Framework", "content": "In this study, we introduce a new framework, AdaInit, designed to mitigate BP issues in QNNS by leveraging generative AI (GenAI) models, particularly LLMS. Our key innovations can be described as follows. (i) First, unlike conventional one-time initialization strategies, we propose a generative approach that iteratively searches the optimal initial model parameters \u03b8\u2217 \u2208 RLNR that maximize the gradient variance Var[\u2202E] of QNNS, thereby mitigating BPs and improving QNNs' trainability. In each search iteration, we employ an LLM as a Bayesian estimator to refine the posterior (candidate initial model parameters \u03b80) through adaptive prompting. After posterior estimation, we train the QNN initialized with the generated \u03b80 and further compute its Var[\u2202E]. The benefit of using LLM as a posterior estimator is that the LLM can incorporate diverse textual instructions via prompts and adaptively update the prompts based on feedback from the previous iteration. This adaptive refinement allows our framework to dynamically optimize the generation process. (ii) To validate the generation quality, we employ Expected Improvement (EI), \u2206(t), as a guiding metric for our search. Furthermore, we rigorously prove that the EI and its cumulative sum satisfy the properties of submartingale. Consequently, we theoretically establish their boundedness, thereby demonstrating that our proposed search framework will ultimately find the optimal initial parameters for QNNS.\n\nHere is pseudocode of the algorithm:\n\nRequire: A GenAI model f(\u00b7), prompts xp, a QNN g(\u00b7), the number of search iterations T.\n1: Initialize prompts xp and the GenAI model, f(\u00b7);\n2: Create an empty list \u0398 \u2190 \u2205 to collect optimal candidates of initial model parameters for the QNN, g(\u00b7);\n3: for t = 1 to T do\n4:  \u03b8(t) \u2190 f(xp\n(t),P(\u03b8(t)));\n5:  Var[\u2202E(t)] \u2190 g(\u03b8(t));\n6:  \u2206(t) \u2190 max(Var[\u2202E(t)] \u2212 S(t\u22121), 0);\n7:  if \u2206(t) > poly(N,L)T then\n8:   xpt+1) \u2190 \u03c6(x(t), Var[\u2202E(t)], S(t\u22121));\n9:   S(t) \u2190 Var[\u2202E(t)];\n10:  \u0398 \u2190 \u0398 \u2295 [\u03b8(t)];\n11:  end if\n12: end for\n13: return \u0398;\n\nWe present our framework workflow in Fig. 2 and further introduce details in Algo. 1. Given a GenAI model f(\u00b7), prompts xp for the f(\u00b7), a QNN"}, {"title": "Theoretical Analysis of Expected Improvement (EI).", "content": "Before presenting the details of all necessary theoretical analysis, we would like to discuss how we can interpret these results. First, we formally define the Expected Improvement (EI) at each search iteration t as \u2206(t) and its accumulative sum in the past iterations as S(t\u22121) in Def. 1. Besides, we assume that the maximum possible gradient \u2202Emax during QNN's training is bounded by a positive constant B\u2202E, which is practical in real-world simulation. Next, we establish an upper bound for EI through Lem. 1 and Lem. 2. These results indicate that S(t) is L\u00b9-bounded and integrable for each t. Building upon these lemmas, we investigate the submartingale property of \u2206(t) and rigorously prove in Lem. 3 that S(t) is submartingale. This insight is crucial as it provides a theoretical basis to analyze the convergence of our proposed search framework. Finally, leveraging the convergence of submartingales and the monotonicity of S'(t), we establish in Lem. 4 that S(t) has a supremum, which indicates that our proposed search framework can eventually identify the optimal initial model parameters that maximize the gradient variance of QNNS in optimization. Due to the page limit, we provide rigorous proof in the Appendix.\n\nDefinition 1 (Expected Improvement). For \u2200 t\u2208 Z+, the Expected Improvement (EI) in the t-th search iteration is defined as:\n\n\u2206(t) = max(Var[\u2202E(t)] \u2212 S(t\u22121),0),\n\nwhere Var[\u2202E(t)] denotes the gradient variance in the t-th search iteration, and S(t\u22121) = \u2211t\u22121 i=1 \u2206(ti)\u22c5 I(ti) denotes the maximum observed gradient variance in the past iterations, where I(ti) represents an indicator function 1(\u2206(ti) > poly(N,L)T) given a condition inside.\nAssumption 1 (Bounded Maximum Gradient). We assume there exists a positive constant B\u2202E > 0, s.t. the maximum possible gradient \u2202Emax during QNN's training satisfies:\n\n|\u2202E_{max}| \u2264 B_{\u2202E}.\n\nWithout loss of generality, let's say \u2202Emax \u2208 [\u2212B\u2202E, B\u2202E].\nLemma 1 (Boundedness of Gradient Variance). Given a certain-size quantum neural network (QNN), the variance of its gradient during training, Var[\u2202E], is bounded by:\n\nVar[\u2202E] < (\u2202Emax \u2212 \u2202Emin)\u00b2,\n\nwhere \u2202Emax and \u2202Emin denote the maximum and minimum values of the gradient \u2202E, respectively.\nLemma 2 (Boundedness of EI). From Def. 1 and Lem. 1, during the search of initial model parameters \u03b80 for a certain-size QNN, for \u2200 t \u2208 Z+, there exist a bound for the expected improvement (EI) s.t.\n\n\u2206(t) < (\u2202Emax \u2212 \u2202Emin)\u00b2.\nLemma 3 (Submartingale Property of EI). Let {\u2206(t)}t\u22651 be an i.i.d. sequence of random variables on a probability space (\u03a9, F, P) s.t.\n\n{P(\u2206(t) > poly(N,L)T) = p,\nP(\u2206(t) \u2264 poly(N,L)T) = 1 \u2212 p,\n\nfor a probability p \u2208 [0, 1]. We define natural filtration F(t) = \u03c3(\u2206(1), \u2206(2), ..., \u2206(t)), and the selective accumulation of \u2206(t) for the past t iteration as a stochastic process {S(t)}t>1 according to Def. 1. Then, {S(t)}t>1 is a submartingale with respect to the filtration {F(t)}t\u22651."}, {"title": "Experiment", "content": "In this section, we first introduce the experimental settings and further present our results in detail.\nDataset. We evaluate our proposed method across four public datasets. Iris is a classic machine-learning benchmark that measures various attributes of three-species iris flowers. Wine is a well-known dataset that includes 13 attributes of chemical composition in wines. Titanic contains historical data about passengers aboard the Titanic and is typically used to predict the survival. MNIST is a widely used small benchmark in computer vision. This benchmark consists of 28\u00d728 gray-scale images of handwritten digits from 0 to 9. We follow the settings of BeInit (Kulshrestha and Safro, 2022) and conduct experiments in binary classification. Specifically, we sub-sample instances from the first two classes of each dataset to create a new subset. After sub-sampling, we adjust the feature dimensions to ensure they do not exceed the number of available qubits. The statistics of the original datasets, along with the data splits for training, validation, and testing, are presented in Table 1. Importantly, the total number of sub-sampled instances corresponds to the sum of the split datasets. For instance, in the Iris dataset, the total number of sub-sampled instances is 100.\nExperimental settings. In the experiment, we analyze the trend of gradient variance by varying the number of qubits ranging from 2 to 20 in increments of 2 (fixed 2 layers) and the number of layers spanning from 4 to 40 in steps of 4 (fixed 2 qubits). To obtain reliable results, we repeat the experiments five times and present them as curves (mean) with their bandwidth (standard deviation). During the search, our framework can identify the optimal model parameters within 50 iterations. In each search iteration, we employ the Adam optimizer with a learning rate of 0.01 and a batch size of 20 to train a QNN with 30 epochs and compute the gradient variance. After training, we compute the expected improvement (EI) and compare it with an assumed lower bound, poly(N,L)T, in each iteration. We compute the lower bound by [22NT]\u22121, which is originally designed for uniform distribution. We empirically apply it for all cases as we observe in Fig. 3 that the magnitude of gradient variance is comparable across all datasets.\nEvaluation. We measure the QNN's training by its gradient variance. A higher gradient variance in training indicates a lower likelihood of being trapped on the barren plateau landscape."}, {"title": "Searching initial model parameters of QNNS via large language models can help alleviate barren plateaus.", "content": "We analyze gradient variance trends in the first element of QNNs' model parameters across varying qubit and layer settings for three classic initialization distributions, uniform, normal, and beta distributions, which are presented in Fig. 5 as examples. For each initialization with classic distribution, we compare it (\"Classic\") with our proposed methods (\u201cOurs\u201d). As presented in Fig. 3, we observe that in the case of using classic initialization, the gradient variance of QNNs will significantly decrease as the number of qubits or layers increases. Compared with it, our method can maintain higher variances, indicating that our framework can mitigate BPs better. In the rest of the experiments, if there is no specific state, we adopt uniform distribution as prior knowledge for posterior estimation."}, {"title": "Investigation of prompts.", "content": "We further examine whether the content of prompts influences search performance. In the experiments, we tested four prompting scenarios: (i) Including both data description and gradient feedback in prompts (Both desc. and feedback), (ii) Including gradient feedback only (No desc.), (iii) Including data description only (No feedback), (iv) Including neither data description nor gradient feedback (Neither desc. nor feedback). As the results presented in Fig. 4, we observe that suppressing either dataset description or gradient feedback in the prompts leads to a reduction in the gradient variance of QNNS. Notably, the reduction is more significant in most cases when gradient feedback is muted compared to the dataset description, suggesting that both factors play a crucial role in mitigating BPs, with gradient feedback contributing significantly more."}, {"title": "Comparison of generative performance using LLMS.", "content": "In our proposed framework, the initial model parameters of QNNS are generated by LLMs. In this experiment, we compare the generative performance under varying QNN structures, such as different numbers of qubits or layers. Specifically, we primarily evaluate whether the correct size of model parameters can be generated by testing 20 combinations in accuracy, fixing either 2 layers while varying qubits from 2 to 20 or 2 qubits while varying layers from 4 to 40. As shown in Tab. 2, the results indicate that both GPT-4o and Claude 3.5 Sonnet can achieve 100% accuracy in generating the correct shapes of model parameters. Considering that 4K output tokens are sufficient for our settings, in this study, we mainly use GPT 4o as the backbone LLMS."}, {"title": "Comparison with initialization-based strategies.", "content": "We compare our framework with two popular initialization-based strategies, GaInit (Zhang et al., 2022) and BeInit (Kulshrestha and Safro, 2022). Both of them leverage well-designed Normal and Beta distributions to initialize the quantum circuits, respectively. For a fair comparison, we initialize the QNNS with the corresponding distribution. We present the results on Iris in Fig. 6 as an example. The results demonstrate that our framework can identify the initial model parameters of QNNS that achieve higher gradient variance during training as the model size increases, indicating better mitigation for BPs."}, {"title": "Sensitivity analysis of hyperparameters.", "content": "We analyze the sensitivity of hyperparameters, including Temperature and Top P, for LLMs. Temperature controls the randomness of predictions, with higher values generating more diverse outputs, while Top P affects the probabilities of token selections, ensuring a more focused yet flexible generation. To identify the optimal settings, we first narrowed down the hyperparameter ranges through manual tuning and then applied grid search to determine the best combinations (Temperature, Top P) for each dataset: Iris (0.5, 0.9), Wine (0.1, 0.45), Titanic (0.8, 0.75), and MNIST (0.8, 0.8), as presented in Fig. 7. The combinations of the above hyperparameters were subsequently used in this study."}, {"title": "Analysis of the expected improvement.", "content": "We analyze the patterns on the expected improvement (EI) and the corresponding gradient variance across various QNN structures as search iterations progress. Representative experiments conducted on Iris are illustrated in Fig. 8 as an example. Our findings show that as the model size grows, more search iterations are required to obtain optimal initial parameters that enable QNNs to maintain higher gradient variance in training. This is expected, as larger models expand the search space, demanding greater computational resources to explore effectively."}, {"title": "Related Work", "content": "McClean et al. (2018) first investigated barren plateau (BP) phenomenons and demonstrated that under the assumption of the 2-design Haar distribution, gradient variance in QNNS will exponentially decrease to zero during training as the model size increases. In recent years, enormous studies have been devoted to mitigating BP issues in QNNS (Qi et al., 2023). Cunningham and Zhuang (2024) categorize most existing studies into the following five groups. (i) Initialization-based strategies initialize model parameters with various well-designed distributions in the initialization stage (Grant et al., 2019; Sack et al., 2022; Mele et al., 2022; Grimsley et al., 2023; Liu et al., 2023; Park and Killoran, 2024). (ii) Optimization-based strategies address BP issues and further enhance trainability during optimization (Ostaszewski et al., 2021; Suzuki et al., 2021; Heyraud et al., 2023; Liu et al., 2024; Sannia et al., 2024). (iii) Model-based strategies attempt to mitigate BPs by proposing new model architectures (Li et al., 2021; Bharti and Haug, 2021; Du et al., 2022; Selvarajan et al., 2023; T\u00fcys\u00fcz et al., 2023; Kashif and Al-Kuwari, 2024). (iv) To address both BPs and saddle points, Zhuang et al. (2024) regularize QNNs' model parameters via Bayesian approaches. (v) Rappaport et al. (2023) measure BP phenomenon via various informative metrics."}, {"title": "Conclusion", "content": "In this study, we proposed a new LLM-driven framework, AdaInit, designed to mitigate barren plateaus (BPs) in QNN's training. By iteratively refining QNN's initialization through adaptive prompting and posterior estimation, AdaInit can maximize gradient variance, improving QNN's trainability against BPs. Our theoretical analysis establishes the submartingale property of expected improvement (EI), ensuring the iterative search can eventually identify optimal initial model parameters for QNN. Through extensive experiments across four public datasets, we demonstrated that AdaInit outperforms conventional classic initialization methods in maintaining higher gradient variance as QNN's sizes increase. Overall, this study paves a new way to explore how LLMS help mitigate BPs in QNN's training."}, {"title": "Limitations & future work.", "content": "First, in our theoretical analysis, we assume that the maximum gradient of QNNS is bounded by a positive constant, i.e., the gradient doesn't explode during training. This assumption is practical in most cases. Besides, we rigorously prove that the submartingale has a supremum in our settings. In the future, we plan to prove that convergence of submartingale is guaranteed in a finite number of search iterations."}, {"title": "APPENDIX", "content": "In the appendix, we present the architecture of the quantum circuit and our hardware/software. Besides, we display the prompt designs in this study.\nModel architecture of the quantum circuit. In this study, we examine our proposed framework using a backbone QNN, which concatenates the following quantum circuit with a fully connected layer. The circuit architecture is described in Figure 9.\nHardware and software. The experiment is conducted on a server with the following settings:\n\u2022 Operating System: Ubuntu 22.04.3 LTS\n\u2022 CPU: Intel Xeon w5-3433 @ 4.20 GHz\n\u2022 GPU: NVIDIA RTX A6000 48GB\n\u2022 Software: Python 3.11, PyTorch 2.1, Penny-lane 0.31.1.\nPrompt designs. Before presenting the prompts, we first introduce the notation for the hyperparameter in prompts. 'nlayers', 'nqubits', 'nrot', 'nclasses' denote the number of layers, qubits, rotation gates, and classes for the QNN, respectively. 'init' denotes the initial data distribution for the QNN. 'data_desc' denotes the data description. 'feedback' denotes the gradient feedback from the previous iteration during the search."}, {"title": "Prompts", "content": "Role: data generator.\nGoal: Generate a dictionary iteratively with the following shape:\n{\n '10': a list, shape=(nlayers, nqubits, nrot),\n '11': a list, shape=(out_dim, nqubits),\n '12': a list, shape=(out_dim)\n}\nRequirements:\n\u2022 Data shape: nlayers={nlayers}, nqubits={nqubits}, nrot={nrot}, out_dim={nclasses}.\n\u2022 Data type: float rounded to four decimals.\n\u2022 Data distribution: numerical numbers in each list are sampled from standard {init} distributions, which may be modeled from the following dataset.\n\u2022 Dataset description: {data_desc}\n\u2022 Adjust the sampling based on feedback from the previous searches: {feedback}\n\u2022 Crucially, ensure that the length of '10' = 'nlayers' and the length of '11' = 'out_dim'.\n\u2022 Print out a dictionary [only] (Don't show Python code OR include \u2018[\u2018python\\n]', '[\u2018\u201cjson\\n]', '[\"\u201c]')."}, {"title": "Proof of Lemmas.", "content": "We provide a rigorous proof of the following lemmas.\nLem. 1. We denote a sequence of gradient \u2202\u0415 = {\u2202E(t)}(t=0), )) Ttro, where Ttr represents the number of training epochs for a QNN. Within this sequence, we denote \u2202Emax, \u2202Emin, and \u2202E as the maximum, minimum, and mean values of the gradient. For \u2200 t \u2208 Z+, we have:\n\u2202E(t), \u2202E \u2208 [\u2202Emin, \u2202Emax],\nthen the gap between \u2202E(t) and \u2202E will not exceed the range of [\u2202Emin, \u2202Emax]:\n|\u2202E(t) \u2212 \u0408\u0415| < \u0415\u0442\u0430\u0445 \u2212 \u0434\u0415\u0442\u0456\u043f.\nThus, we have:\nVar[DE] = 1/Ttr \u2211(\u2202E(t) \u2212 \u0408\u0415)2 \n< 1/Ttr \u2211(\u2202Emax \u2013 \u2202Emin)2 = (\u2202Emax \u2013 \u2202Emin)2.\nThus, the gradient variance Var[DE] satisfies the bound Var[DE] \u2264 (\u04d8\u0415\u0442\u0430\u0445 \u2013 \u0414\u0415min)2.\nLem. 2. From Def. 1, for \u2200 t_e_Z+, in the t-th search iteration, we have:\n\u2206(t) = max(Var[\u2202E(t)] \u2212 S(t\u22121), 0).\nCombining with Lem. 1, for \u2200 t \u2208 Z+, we have:\nVar[JE(t)], S(t-1) < (\u2202\u0415\u0442\u0430\u0445 \u2013 \u0434\u0415min)2.\nThe above equation holds true as S(t-1) denotes the historical maximum gradient variance in the past iterations. Thus, we have:\nVar[\u2202E(t)] \u2013 S(t-1) < (\u04d8\u0415\u0442\u0430\u0445 \u2013 \u0434\u0415min)2,\nwhich indicates that:\n\u2206(t) < (\u2202Emax \u2212 \u2202Emin)\u00b2.\nLem. 3. A process S'(t) is submartingale relative to (\u03a9, F, P) if the following three conditions, Adaptedness, Integrability, and Submartingale condition, hold true (Williams, 1991)."}, {"title": "Adaptedness.", "content": "We first aim to verify that S(t) is determined based on the information available up to past t iterations. By Def. 1, S(t) = \u2211t i=1 \u2206(ti)\u22c5 I(ti). I(ti) is a finite sum of random variables that are measurable w.r.t. \u03c3(\u2206(1), \u2206(2), ..., \u2206(t)). Thus, S(t) is also measurable w.r.t. F(t), ensuring the adaptedness.\nIntegrability. In Lem. 2, \u2206(t) \u2264 (\u0434\u0415\u0442\u0430\u0445 \u2212dEmin)\u00b2 for \u2200 t \u2208 Z+. Thus,\nE[[S(t)]] = E[[\u2211 \u2206(ti)\u22c5 I(ti)]] \n< E[\u2211 (Emax - Emin) 2. I(i) ] < \u221e,\nwhich ensures E[|S(t)|] is integrable for each t.\nSubmartingale condition. We observe that\nS(t) = S(t-1) + (t).\nThus, given F(t-1), we have\nE[S(t)|F(t-1)] = E[S(t-1) + \u2206(t)|F(t-1)],\nSince S(t-1) is F(t-1)-measurable and {\u2206(t)}t>1 is i.i.d., thus,\nE[S(t)|F(t-1)] = E[S(t-1) + \u2206(t)|F(t-1)]\n= S(t-1) + E[\u0394(t)]\n= S(t-1) + (\u0431\u0440 + 0(1 \u2212 p))\n> S(t-1),\nwhere 8 denotes a positive increment when \u2206(t) > poly(N,L)T.\nThus, the submartingale condition holds true for Vt > 1 s.t.\nE[S(t) | F(t-1)] > S(t-1), t > 1,"}, {"title": "Computational budgets.", "content": "Based on the above computing infrastructure and settings, computational budgets in our experiments are described as follows. Our search framework can be reproduced within one hour given that the number of qubits is less than 18. The total experiments take around 600 hours to complete."}, {"title": "Ethical and broader impacts.", "content": "We confirm that we fulfill the author's responsibilities and address the potential ethical issues. This work paves a novel way to explore how generative models, such as LLMS, help improve the trainability of QNNS, which could benefit the community of natural language processing and quantum machine learning."}, {"title": "Statement of data privacy.", "content": "The datasets used in this study were obtained from publicly available sources."}, {"title": "Potential risk.", "content": "Reproducing the experiments in this study may take significant time and computational resources."}, {"title": "Disclaimer regarding human subjects results.", "content": "NA"}]}