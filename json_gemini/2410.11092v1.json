{"title": "EchoApex: A General-Purpose Vision Foundation Model for Echocardiography", "authors": ["Abdoul Aziz Amadou", "Yue Zhang", "Sebastien Piat", "Paul Klein", "Ingo Schmuecking", "Tiziano Passerini", "Puneet Sharma"], "abstract": "Purpose: Quantitative evaluation of echocardiography is essential for precise assessment of cardiac\ncondition, monitoring disease progression, and guiding treatment decisions. The diverse nature of echo\nimages, including variations in transducer types, manufacturers, and pathologies, poses challenges for\ndeveloping artificial intelligent models that can generalize across different clinical practice. We introduce\nEchoApex, the first general-purpose vision foundation model for a variety of clinical applications in\nechocardiography.\nMethods: Leveraging self-supervised learning, EchoApex is pretrained on over 20 million echo images\nfrom 11 clinical centres. By incorporating task-specific decoders and adapter modules, we demonstrate the\neffectiveness of EchoApex on 4 different types of clinical applications with 28 sub-tasks. Specifically, we\nevaluate the pretrained EchoApex encoding model on: 1) sequence view classification with linear decoder\non 18 different view points; 2) interactive structure segmentation with prompt-based encoder-decoder\non chambers from different views; 3) left ventricle hypertrophy detection with multiscale convolutional\ndecoder, and 4) ejection fraction estimation with a spatial-temporal sequence decoder. In addition, we\ninvestigate the learning efficiency and generalizability of EchoApex via different learning settings, including\nsupervised fine-tuning, zero and few-shot learning. Finally, we study the feasibility of parameter-efficient\nadaptation of EchoApex to aforementioned downstream tasks with limited computational resources.\nResults: Compared to state-of-the-art task-specific models, EchoApex attains improved performance on\nall evaluated applications with a unified image encoding architecture. For the view classification task,\nstudied on over 26K annotated videos, EchoApex reaches a mean BACC of 0.976, showing improvement\n(p < 0.01) on 14 out of 18 view points over the ImageNet pretrained counterpart, with an average increment\nof 0.02 BACC. For the segmentation task, benchmarked on the 3 largest publicly available echocardiogram\ndatasets with a total number of 74K annotated frames, EchoApex shows an improved DICE (0.927, 95%\nCI 0.926-0.928) compared to sub-task specialist models (e.g. UNet and DeepLabV3, 0.904) as well as\ngeneralist model MedSAM (0.870, 95% CI 0.868-0.871) on all targets. In a zero-shot test, EchoApex\nshows improvement (0.877, 95% CI 0.873-0.880) over specialist models (0.834, 95% CI 0.829-0.840) in all\nevaluated datasets. On the left ventricle measurement task, studied on 12K annotated videos, EchoApex\nshows an improved MAE of 0.2 mm compared with the specialist baseline, and a significant improvement of\n1.4mm on 1K frames from a cohort of unseen data, demonstrating its generalization capability. For ejection\nfraction estimation, evaluated on a dataset with over 1.2K patients and with a frozen encoder, Echo Apex\nachieved MAE of 5.6% and AUC of 0.93 for cardiomyopathy detection with a simple spatial-temporal\nmodel extension, showing improvement compared to 6.1% of MAE and 0.89 of AUC from the ImageNet\npretrained model. With the pretrained encoder frozen, EchoApex with adapters uses less than 4% of\ntrainable parameters but showed competitive performance with a maximum of 5.1% degradation from fully\nfinetuned models.\nConclusion: The improved performance of EchoApex compared with existing works demonstrated the\nbenefits of model pretraining at scale with in-domain data. Furthermore, EchoApex illustrates the potential\nfor developing a general-purpose vision foundation model tailored specifically for echocardiography, capable\nof addressing a diverse range of clinical applications with high efficiency and efficacy.", "sections": [{"title": "1 Introduction", "content": "Echocardiography is deeply integrated into a wide range of clinical practice, covering diagnosis, intervention\nand follow-up of cardiovascular disease [1-3]. The increasing usage of echocardiography and the diverse range"}, {"title": "2 Results", "content": ""}, {"title": "2.1 Pretraining", "content": "A defining feature of foundation models is their remarkable ability to improve performance on downstream\ntasks as they scale with increased data and model size. This scalability has been demonstrated across various"}, {"title": "2.2 View Classification", "content": "TTE is widely used in cardiology for the diagnosis and follow up of patients affected by cardiovascular disease.\nEcho studies obtained from TTE examinations cover a wide range of views, resulting in large datasets to\nbe reviewed by clinicians. AI-based models for TTE view classification can prove beneficial in identifying\nclips of desired views or generating structured reports. For this task, we use an internal multi-vendor dataset,\nincluding a wide range of transducers, spatial and temporal resolutions, image quality, imaging depth, color\ndoppler and the use of contrast. A total of 18 different TTE views are covered in the datasets, including\nstandard views and those where contrast is used. All the images were labeled by echocardiographers following\nthe recommended guidelines [25]."}, {"title": "2.2.1 Supervised view classification with linear classifier", "content": "The model architecture for this task is based on our pretrained EchoApex backbone and a linear classification\nlayer, which uses the features from the classification (CLS) token to produce the logits, following Fig. 3a.\nWe use a weighted cross-entropy loss to account for class imbalance. The distribution of the classes in our\ndataset, sample images from the various classes and the list of hyperparameters for this experiment are given\nin the Supplementary Fig. 7, 8 and Table. 2, respectively.\nWhen testing our model, we extract 5 frames from each video sequence, classify them individually and apply\nmajority voting to obtain a sequence label. We found this strategy to be more effective in reducing confusion.\nIndeed, using multiple frames can help with ambiguity between views due to the cardiac motion during the\ncardiac cycle, e.g. between the apical four and five chamber views. The confusion matrix in Fig. 3b details\nthe results of the best performing EchoApex model with a ViT-B backbone (EchoApex-B) over the 18 classes.\nThe confusion between the parasternal long axis left ventricle (PLAX LV), increased depth (PLAX ID), and\nzoomed on the mitral and/or aortic valves (PLAX Valves) is due to the same structures being present in the\nthree views, with the main difference between them being the varying zoom levels. Finally, the A3C and the\nPLAX ID/LV views exhibit the same anatomical structures but the angulation differs between the two sets of\nviews. The model tends to make errors when the image quality is poor or the angulation not adequate.\nFor all the subsequent experiments, we report the Balanced Accuracy (BACC) of the models, which takes\nin account the class imbalance in the dataset. We first compare EchoApex-S with a baseline ResNet50 [24]\npretrained on ImageNet-1K [26] (ResNet50-IN) in Fig. 3c. Our model significantly outperforms the baseline\non 14 of the 18 subtasks (P < 0.01, according to a one-sided permutation test). We omit the subcostal four\nchamber and inferior vena cava views (SC:4C and SC:IVC) in Fig. 3c as both models achieve mean B\u0410\u0421\u0421\nvalues of 0.99. EchoApex-S shows stronger performance on the less prevalent parasternal classes, with the\nmost notable improvements in the PLAX:RVT and PLAX:LV classes, where we report mean BACC values of\n0.99 and 0.94 respectively, as opposed to values of 0.91 and 0.89 for the ResNet50-IN baseline. Those results\nhighlight the benefits of pretraining on a large-scale echo database.\nIn Fig. 3c, when comparing versions of the EchoApex model varying with model size (Vit-S vs Vit-B) and\ntraining strategy (fine-tuned vs adapters), the fine-tuned EchoApex-B model achieves the highest mean BACC\n(0.976) over all the classes, compared to values of 0.972 for EchoApex-S, 0.975 for EchoApex-B with a ViT-B\nbackbone and adapters (EchoApex-AdaB), and 0.968 for EchoApex-AdaS. When comparing the fine-tuned\nmodels, EchoApex-B outperforms EchoApex-S significantly on 8 out of the 18 classes (P < 0.01, no significant\ndifference in performance on 4 classes), mostly on the contrasted and parasternal views (PLAX:LV, PLAX:ID,\nPSAX:MV), for which the number of training samples is one or two orders of magnitude smaller compared to\nthe apical classes. This showcases the benefit of pretraining for increased performance even with reduced\ntraining samples size."}, {"title": "2.2.2 View classification with parameter efficient adaptation", "content": "We investigate the use of Adapters [27] for fine-tuning our EchoApex backbone. Adapters are modules\ninserted between the frozen backbone's transformer blocks and are useful in reducing the computational\ncost of fine-tuning a large model, as only the adapter layers are trained. Additionally, they allow for more\nmodularity, as one can fine-tune adapter modules for different tasks while keeping the same backbone.\nWe report the performance of EchoApex modules with adapters in Fig. 3d. Notably, EchoApex-AdaB\noutperforms not only its counterpart with a Vit-S backbone but also the fine-tuned EchoApex-S (P < 0.01\nfor both cases). In terms of balanced accuracy, EchoApex-AdaB significantly surpasses EchoApex-AdaS in\n11 classes (no significant difference in 4 classes) and EchoApex-S in 8 classes (no significant difference in 6\nclasses). EchoApex-AdaB showing improved performance compared to the fine-tuned EchoApex-S showcases\nthe utility of adapters in achieving a strong performance at a fraction of the computational cost compared to\nfine-tuning the entire backbone. Tables listing the models' performance and metrics per class for each model\nare available in Supplementary Tables. 5, 6, 7."}, {"title": "2.3 Interactive Structure Segmentation", "content": "We sought to evaluate EchoApex's effectiveness in structure segmentation, a common yet time-consuming\ntask in echo exams. We use three large public echocardiogram segmentation datasets, CAMUS [28], EchoNet-\nDynamic (ENDym) [29], EchoNet-Pediatric (ENPed) [30], as well as an internal dataset acquired with\nvolume transducers. CAMUS includes left ventricle and left atrium tracings over an entire cardiac cycle.\nEchoNet-Dynamic provides tracings of the left ventricle in the apical-4-chamber (A4C) view at end-systole\nand end-diastole. EchoNet-Pediatric offers tracings of the left ventricle in both A4C and parasternal short\naxis (PSAX) views. The internal dataset comprises tracings of all four chambers, with biplanes utilized for\ntraining and evaluation. The number of annotations per dataset is shown in Fig. 4b. We used the Dice\nSimilarity Coefficient as our primary evaluation metric, consistent with the public benchmark. Additional\ndetails regarding the datasets, experimental setup, and other performance metrics are provided in Methods\nand Supplementary Table 9.\nWe build an interactive segmentation model EchoApex-SAM-B (and its variation EchoApex-SAM-S) by\nincorporating the pretrained EchoApex encoder and prompt-based (including points, box, and text) encoder-\ndecoder modules following SAM [20], the state-of-the-art foundation model on natural image segmentation.\nThe model architecture is illustrated in Fig. 4a. We compare EchoApex-SAM with two types of model. The\nfirst is MedSAM [31], a foundation model trained on 1.5 million annotated images from diverse modalities,\nincluding CT, MRI, X-ray and ultrasound, and it is specialized in general medical image segmentation. The\nsecond are sub-task specialist models, e.g. a UNet model [28], that are individually trained on and published\ntogether with each dataset. Results are illustrated in Fig. 4c. We observe that the UNet specialist models\ngenerally shows better performance than MedSAM. This hints that adding additional data from different\nmodalities does not necessarily improve the performance of the model on echo segmentation. As echo represents\nonly a small portion (approximately 5%) of the training data in MedSAM, its model performance on echo\nmay be impacted negatively by the imbalanced data distribution. We also observe that EchoApex-SAM-B\noutperforms both MedSAM and UNet-specialist on all categories in all datasets, achieving an average dice\nscore of 0.927 (95% CI 0.926-0.928), showing an average of 0.23 and 0.57 dice improvement accordingly. This\nsupports the observation that the model pretrained with more in-domain data will boost the performance on\nthe target task."}, {"title": "2.3.1 Effectiveness of pretraining on few-shot segmentation learning", "content": "It has been shown in prior art that pretraining with large in-domain data yields better performance on few-shot\nclassification. We investigate whether this observation holds for few-shot segmentation learning. We randomly\nsample 50, 100, 500, 1000 and 5000 annotated images from the whole dataset, and evaluate the performance\nof EchoApex-SAM-S with and without pretraining on the same evaluation dataset used in the aforementioned\npublic benchmark. Each experiment is repeated 24 times, maximizing the usage of 3 node of 8-GPUs on the\ncompute cluster. From the results in 4d, we observe that EchoApex does not show superior performance\nto the imagenet pretrained model when samples are very few, e.g. 50 or 100. However, the performance"}, {"title": "2.3.2 Segmentation performance generalizability", "content": "We also assess the generalizability of the segmentation performance, i.e. how well it performs on unseen data.\nWe retrain EchoApex-SAM from scratch on the EchoNet-Dynamic dataset, and compare its performance\nwith the specialist model DeepLabV3 published together with the dataset. We used the ViT-S model to\nensure a comparable model size with DeepLabV3. Following Fig. 4e, We observe that EchoApex-SAM shows\nimproved performance compared to DeepLabV3, not only on the in-domain test set from EchoNet-Dynamic,\nbut also on the generalization test set. Specifically, EchoApex-SAM-S achieves an average dice score of 0.923\n(95% CI 0.922-0.925) on the in-domain test set, and an average dice score 0.877 (95% CI 0.873-0.880) on the\ngeneralization test set, with 0.756, 0.905, 0.889 on Vol-Biplane, CAMUS and EchoNet-Piediatric respectively.\nThe DeepLabV3 model achieves an average dice score of 0.915 (95% CI 0.913-0.917) on the in-domain test set,\nand an average dice score 0.834 (95% CI 0.829-0.840) on the generalization test set, with 0.744, 0.848, 0.844\non Vol-Biplane, CAMUS and EchoNet-Piediatric respectively. This indicates that the model pretrained on\nin-domain data, even after fine-tuned, can still be more robust and generalizable than the specialist models."}, {"title": "2.4 Landmark Detection for Left Ventricle Measurements", "content": "Left ventricular hypertrophy (LVH) is an adaptation of the cardiac muscle in response to disease, cardiac wall\nstress or significant hemodynamic pressure. Its diagnosis is crucial as patients with LVH are at increased risk\nof developing heart failure, arrhythmia, strokes and sudden death [32]. Left ventricular wall measurements\nusing echocardiography in the parasternal long-axis (PLAX) view are widely used in the diagnosis of LVH.\nHowever these measurements are subject to inter-observer and intra-observer variability due to the complexity\nof the myocardial shape, imaging artefacts and image quality [33].\nIn clinical practice, the LV mass is indexed against the body surface area to and compared to reference values\nto perform a diagnosis of LVH and determine its severity [34]. Three key measurements are taken in the\nPLAX view to compute the LV mass: The intraventricular septum (IVS), the LV internal dimension (LVID)\nand the LV posterior wall thickness (LVPW). AI-driven approaches in medical imaging have demonstrated\nsignificant potential in enhancing the accuracy and consistency of cardiac measurements. AI models can\nanalyze echocardiographic images with a high degree of precision, minimizing human error and subjectivity\nand thus benefiting the clinical workflows [35, 36]."}, {"title": "2.4.1 Supervised landmark detection for left ventricle wall thickness measurements", "content": "We use our pre-trained EchoApex model as an image encoder and a decoder architecture inspired by UNETR\n[21], as described in Fig. 5a. We compare our model with the state-of-the-art method from [37] which uses a\nDeepLabV3 [38] backbone, retrained following the authors' specifications. All hyperparameters are listed in\nthe Supplementary Table. 10. Training is conducted on the training split of the EchoNet-LVH dataset [37]\nwhile testing is performed on two datasets: the test split of EchoNet-LVH (internal dataset) and the validation\nsplit of the Unity dataset [35] (external dataset). Fig. 5b details the distribution of the measurements for the\ntwo datasets.\nWe report the average landmark error (L2 norm) between all landmarks and the Mean Absolute Error (MAE)\non the three key measurements on the internal EchoNet-LVH (resp. Fig. 5c and Fig. 5e) and external Unity\ntest datasets (resp. Fig. 5d and Fig. 5f). Qualitative results showing attention maps and landmark predictions\nare shown in Fig. 5g and in the Supplementary Fig. 9.\nEchoApex-S achieves superior performance compared to the DeepLabV3 baseline on the internal EchoNet-LVH\ndataset. Landmark error is significantly improved on IVS and LVID measurements, with P < 0.01, according\nto a one-sided t-test. We report mean values of 4.11 mm and 4.89 mm for IVS and LVID, respectively, for our"}, {"title": "2.4.2 Left ventricle measurements with parameter efficient adaptation", "content": "We investigate the use of adapters for parameter efficient tuning of the EchoApex backbone. As previously\ndone, we freeze the EchoApex encoder and finetune only the adapter layers. On the internal EchoNet-LVH\ndataset, EchoApex-AdaS outperforms significantly (P < 0.01) the DeepLabV3 baseline in both landmark\nerror and MAE for the IVS and LVID measurements. We report at R2 of 0.81 for EchoApex-AdaS on this\ndataset.\nOn the external Unity dataset, EchoApex-AdaS shows superior performance compared to DeepLabV3, with\nsignificantly better results for both landmark error and MAE in the measurements of IVS and LVID (P < 0.01\nfor all). There are no statistically significant difference between the performance of the EchoApex-S and\nEchoApex-AdaS on both datasets except for the MAE on the Unity LVID measurements, where EchoApex-\nAdaS outperforms its fine-tuned counterpart. However. EchoApex-S shows a better agreement in terms of\nmeasurements compared to human annotators with a R2 of 0.87 on the Unity dataset, as opposed to 0.81 for\nits adapter counterpart.\nEchoApex-AdaS comparable performance to EchoApex-S on the EchoNet-LVH dataset shows the utility\nof the parameter efficient tuning in resource limited settings. In this experiment, the training dataset was\norders of magnitude smaller compared to the view classification task. Indeed, tens of thousand of training\nsamples were used for this task as opposed to millions in the view classification experiment. However, the\nEchoApex-S predictions agree better with human annotations on the Unity dataset, indicating a less effective\ngeneralization of EchoApex-AdaS to the unseen distribution."}, {"title": "2.5 Automatic Ejection Fraction Estimation", "content": "We assessed the performance of the proposed model in the task of automated ejection fraction (EF) estimation.\nEF, defined by changes in blood volume, inherently requires a model with a temporal perspective. Although\nthe model was initially pretrained on individual images, we aimed to evaluate the potential of Echo Apex\nfor temporal prediction by extending it with a spatial-temporal architecture, termed as EchoApex-ST and\nillustrated in Fig. 6a. We train EchoApex-ST on the public EchoNet-Dynamic dataset and test it on both\nthe same dataset for in-domain evaluation and CAMUS dataset for out-domain evaluation to assess its\ngeneralization capability. We compare EchoApex-ST with its ImageNet pretrained counterpart ImageNet-ST.\nThroughout the finetuning process, we froze the image encoder of both models and only trained the temporal\ndecoder.\nOn the EchoNet-Dynamic dataset, EchoApex-ST achieved a mean absolute error (MAE) of 5.6% for LVEF,\nreducing the error by 0.5% compared to ImageNet-ST (6.1%). When using an LVEF threshold of less than 50%\nto classify cardiomyopathy, EchoApex-ST had an area under the curve (AUC) of 0.93, whereas ImageNet-ST\nhad an AUC of 0.89(Fig. 6c, 6d). On the CAMUS dataset, where neither EchoApex-ST nor ImageNet-ST were\ntrained, EchoApex-ST achieved an MAE of 10.7% for LVEF, a reduction of 7.7% compared to ImageNet-ST's\n18.4%. Using the same LVEF threshold of less than 50% for cardiomyopathy classification, EchoApex-ST"}, {"title": "3 Discussion", "content": "Modern deep learning models have significantly advanced the study and clinical applications of echocardiogra-\nphy. Our study on EchoApex demonstrates that a general-purpose vision foundation model, when pretrained\non a large and diverse dataset, can outperform specialized task-specific deep learning models across a range of\nclinical applications. The comprehensive evaluation of EchoApex across four distinct clinical applications\nunderscores its versatility and robustness, highlighting the value of foundation models in medical imaging.\nEchoApex's impressive performance across various tasks emphasizes the potential of large-scale pretraining\nwith in-domain data. To ensure a diverse range of data for pretraining, we curated Echo20M, a dataset\ncontaining 20 million images derived from 450,338 videos across 11 clinical centers. By varying data and\nmodel scales, we observed a consistent pattern in echocardiography similar to that in natural images and\nother medical imaging domains: downstream task performance improves as data and model size increase. In a\nK-Nearest Neighbors (KNN) test classification, the EchoApex model achieved an accuracy of 0.92 without\nany label supervision during training. When trained with labeled data, the balanced accuracy (BACC)\nimproved to 0.98, surpassing models pretrained on ImageNet in 14 out of 18 views. This result highlights the\neffectiveness of leveraging large-scale in-domain data for specific downstream tasks with limited supervision.\nEchoApex also excelled in segmentation tasks when trained on a large annotated dataset, achieving a mean\nscore of 0.93. This outperformed specialist models and the generalist model MedSAM, which scored 0.90\nand 0.88, respectively. These results validate the model's ability to utilize supervised learning effectively. In\nfew-shot evaluation, EchoApex demonstrated stable and superior performance compared to its ImageNet-\npretrained counterparts. In zero-shot evaluation, EchoApex not only surpassed specialist models on the\ntrained in-domain dataset but also on external datasets, demonstrating superior generalizability even after\nfine-tuning. This observation also applies to left ventricle measurement tasks, where EchoApex achieved\nbetter in-domain performance in both landmark detection and ventricle measurement, with an average MAE\nof 0.2 mm compared to specialist baselines. Notably, it showed a remarkable improvement of 1.4 mm on\n1K frames from an unseen data cohort. Further, EchoApex's success in achieving high accuracy and AUC\nscores in tasks such as ejection fraction estimation and cardiomyopathy detection, even with a frozen encoder,\nemphasizes its ability to generalize across different datasets and tasks without extensive retraining. This\ngeneralizability is crucial for real-world clinical applications, where models must handle a wide variety of\nimage types and patient conditions.\nWhile fully fine-tuning a large model end-to-end can be computationally expensive, we explored parameter-\nefficient adaptation of EchoApex. With less than 4% of trainable parameters and only a portion of the\ncomputational load, the adapted model still showed competitive performance, with a maximum degradation\nof 5.1% compared to fully fine-tuned models. This finding suggests the potential to develop an affordable AI\nsystem where a unified image encoder is utilized with task-dependent adapters requiring minimal parameters\nand computation, thereby enhancing performance across a variety of downstream tasks.\nOur study has several limitations. EchoApex is pretrained on static images independently, while echocardiog-\nraphy naturally involves sequences of images. This approach may lack a temporal perspective, as shown in the\nexperiment on ejection fraction estimation. Although a sequence decoder was attached to aggregate temporal\ninformation in late feature fusion, EchoApex achieved an AUC of 0.93 in cardiomyopathy classification.\nWhile this is higher than the ImageNet-pretrained model's AUC of 0.89, it is lower than the specialized\nspatiotemporal model with early feature fusion, which achieved an AUC of 0.97 [29]. Across all tasks, we\nused the same adapters. While the adapted model already showed competitive performance with task-specific\nmodels, having task-specific adapters such as ViT-Adapter [22] and Adaptformer [39] could potentially lead\nto further improvement based on the characteristics of the downstream task. Besides model development,\nfairness and equity considerations are essential. For privacy reasons, much of the evaluation dataset, including\nboth internal and external ones, had sex and demographic information anonymized. This limits our ability to\nevaluate the impact of the pretraining distribution on downstream tasks.\nThe promising results of EchoApex suggest several potential avenues for future research. One key direction is"}, {"title": "A Methods", "content": "The following sections are organized as follows: for each core component of EchoApex, including pretrain-\ning, view classification, structure segmentation, ventricular measurement, and automated ejection fraction\nestimation, we present the details of the data curation process, model architecture, training, and evaluation."}, {"title": "A.1 Pretraining", "content": "Curation of EchoApex dataset. To ensure the quality and diversity of data for model pretraining, we\ncurated a large-scale dataset of echocardiographic images from 450,338 videos involving 26,704 patients\nacross 11 clinical centers. This dataset encompasses a comprehensive range of echocardiographic data,\nincluding transthoracic echocardiograms (TTE) from routine diagnostic exams, as well as transesophageal\nechocardiograms (TEE) and intracardiac echocardiograms (ICE) obtained from various echo-guided cardiac\ninterventions. Data were acquired using transducers from multiple vendors: Siemens Healthineers (68%, by\nnumber of videos), GE (15%), Philips (16%), and Samsung (1%). All images underwent a de-identification\nprocess to remove any protected health information (PHI) and were stored in DICOM format. We developed a\npreprocessing workflow to remove non-image data, including ECG signals, text, and respirometer information.\nFor DICOM files acquired with volume transducers, we converted them into Cartesian space and then extracted\n2D slices from the volume data. For each volume, 12 slices were extracted at 15-degree intervals from the\nAB-plane views (sagittal and coronal views), and 3 slices from the C-plane view (transverse view). For both\nthe 2D echo and sliced volume images, we performed a tight crop to the image region, padded to square,\nand resized to 112 \u00d7 112 pixels. A total of 37.4 million images were extracted from the entire dataset. As\ndescribed in the main body of the paper, to study the impact of dataset size on model performance, we\nsampled the dataset into Echo3M, Echo12M, and Echo20M. Although the dataset for the downstream tasks\ncould come from the same transducers of the same vendors as the pretraining dataset, we deliberately avoided\nany overlap between the patients in the pretraining and all downstream evaluation sets to minimize the risk\nof data contamination.\nProtocol of EchoApex pretraining. We used the Vision Transformer (ViT) architecture [23] for pretraining.\nIn terms of model size, we utilized the base model, ViT-B, which has 12 blocks, a patch size of 14, and a\nfeature embedding dimension of 768, as well as the smaller model, ViT-S, which has 12 blocks, a patch size of\n8, and an embedding dimension of 384. We leveraged the state-of-the-art self-supervised learning approaches\nfrom the DINO method family (DINOv1 [8] and DINOv2 [10]) for model pretraining. The DINO methods are\nbased on teacher-student self-distillation, where two augmented images from the same input are processed\nthrough the teacher and student networks, respectively, and the student network is trained to predict the\nteacher network's output. Compared to DINOv1, DINOv2 integrates additional best practices, such as iBOT\nloss [44], Sinkhorn-Knopp centering [7], KoLeo regularization [45], and computational optimizations. While\nthese components in DINOv2 are designed to improve performance, they also increase computational cost. To\nbalance computation and convergence, we employed a two-stage approach: starting with DINOv1 until the\npretrained model converged and then continuing training with the full DINOv2. Additionally, to leverage\nknowledge from large-scale natural images, we initialized our pretraining model with pretrained weights from\nthe officially released DINO checkpoints. Since DINOv2 does not release a pretrained checkpoint for ViT-S\nwith a patch size of 8, we used the pretrained ViT-S model from DINOv1 instead. We followed the same\ntraining hyperparameters as detailed in the DINO papers, which are outlined in Table 1."}, {"title": "A.2 View Classification", "content": "Curation of view classification datasets The datasets used for view classification come from 6 different\nclinical sites and include several vendors, including Siemens, GE, Philips and Samsung. All patient data was\nremoved from the DICOM files. All images were extracted from video clips and processed following these\nsteps: Firstly, we extracted the B-mode image from the DICOM, discarding extra information in the image.\nWe masked all the data outside the imaging cone to remove potential annotations, texts and elements from\nthe user interface. The images were then padded to square sizes and finally converted to grayscale. Images\nwere resized to 112x112 for the ViT-B models, 128x128 for the ViT-S models and the ResNet50 baseline. The\ndata was split in the following way for model development (number of video sequences / number of images):"}, {"title": "A.3 Structure segmentation", "content": "Curation of segmentation datasets The EchoNet-Dynamic dataset [29", "28": "released by the University of Lyon", "30": "also\nreleased by Stanford University School of Medicine", "52": ".", "prompts,\"\nand the model provides segmentation output based on these prompts. Prompts can be a bounding box,\na point set, or a text description. The flexibility of this prompt-based framework allows the model to be\nsuitable for segmenting a variable number of structures. Our EchoApex-SAM model consists of three modules": "nthe pretrained EchoApex encoder, a prompt encoder, and a mask decoder. The prompt encoder design is\nsimilar to that of SAM, with the addition of one convolutional layer and one LayerNorm layer in the mask\nembedding to match the output dimension to the EchoApex embedded feature dimension. We used the CLIP\nmodel [53"}]}