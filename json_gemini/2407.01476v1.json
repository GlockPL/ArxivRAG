{"title": "TREE SEARCH FOR LANGUAGE MODEL AGENTS", "authors": ["Jing Yu Koh", "Stephen McAleer", "Daniel Fried", "Ruslan Salakhutdinov"], "abstract": "Autonomous agents powered by language models (LMs) have demonstrated\npromise in their ability to perform decision-making tasks such as web automation.\nHowever, a key limitation remains: LMs, primarily optimized for natural language\nunderstanding and generation, struggle with multi-step reasoning, planning, and\nusing environmental feedback when attempting to solve realistic computer tasks.\nTowards addressing this, we propose an inference-time search algorithm for LM\nagents to explicitly perform exploration and multi-step planning in interactive web\nenvironments. Our approach is a form of best-first tree search that operates within\nthe actual environment space, and is complementary with most existing state-of-\nthe-art agents. It is the first tree search algorithm for LM agents that shows effec-\ntiveness on realistic web tasks. On the challenging VisualWebArena benchmark,\napplying our search algorithm on top of a GPT-40 agent yields a 39.7% relative\nincrease in success rate compared to the same baseline without search, setting a\nstate-of-the-art success rate of 26.4%. On WebArena, search also yields a 28.0%\nrelative improvement over a baseline agent, setting a competitive success rate of\n19.2%. Our experiments highlight the effectiveness of search for web agents, and\nwe demonstrate that performance scales with increased test-time compute. We\nconduct a thorough analysis of our results to highlight improvements from search,\nlimitations, and promising directions for future work. Our code and models are\npublicly released at jykoh.com/search-agents.", "sections": [{"title": "1 INTRODUCTION", "content": "Building agents that can perceive, plan, and act autonomously has been a long standing goal of ar-\ntificial intelligence research (Russell & Norvig, 1995; Franklin & Graesser, 1996). In recent years,\nthe advent of large language models (LMs) with strong general capabilities has paved the way to-\nwards building language-guided agents that can automate computer tasks. However, the best LM\nagents today are still far worse than humans. On the realistic web benchmarks WebArena (Zhou\net al., 2024b) and VisualWebArena (Koh et al., 2024), humans succeed on 78% and 89% of tasks\nrespectively, but agents even those powered by the latest frontier models are far worse, typi-\ncally achieving success rates below 20%. One significant bottleneck in existing agents arises from\ntheir inability to leverage test-time computation for exploration and multi-step planning. Search\nand planning is especially important in open ended web environments, as the potential action space\n(i.e., all possible actions one can take on a webpage) is much larger than in most video games or\ntext-based simulators. There are often multiple plausible actions that must be sequenced to reach a\ngoal, and being able to efficiently explore and prune trajectories is essential.\nIn artificial intelligence systems, one effective strategy for leveraging test-compute to improve re-\nsults is search: iteratively constructing, exploring, and pruning a graph of intermediate states and\npossible solutions (Newell et al., 1959; Laird, 2019; Silver et al., 2016). The effectiveness of search\nalgorithms has been shown time and time again, enabling models to achieve or surpass human-\nlevel performance on a variety of games, including Go (Silver et al., 2016; 2017), poker (Brown &\nSandholm, 2018; 2019), and Diplomacy (Gray et al., 2020).\nHow might we apply search in the context of automating computer tasks, where the search space\nis large and unlike games there do not exist clear cut rewards and win conditions? Towards\nthis goal, we propose a method to enable autonomous web agents to search over a graph that is\niteratively constructed through exploration of an interactive web environment. This search procedure"}, {"title": "2 BACKGROUND", "content": "2.1 REALISTIC SIMULATED WEB ENVIRONMENTS\nTowards the goal of developing autonomous web agents powered by large language models, several\nprior works focused on building evaluation benchmarks for measuring the progress of models on web\ntasks. Mind2Web (Deng et al., 2023) is an evaluation benchmark that measures the ability of fron-\ntier models in predicting actions taken on static Internet pages. VisualWebBench (Liu et al., 2024)\nintroduced a multimodal benchmark for assessing the ability of models to understand web content."}, {"title": "2.2 LANGUAGE-GUIDED AUTONOMOUS AGENTS", "content": "Autonomous web agents, powered by frontier (multimodal) language models (Google, 2023; Ope-\nnAI, 2024; Anthropic, 2024), are the SOTA approaches for many of the above benchmarks. Kim\net al. (2024) showed that large language models can be prompted to execute computer tasks on Mini-\nWoB++ (Liu et al., 2018), requiring far fewer demonstrations than reinforcement learning methods.\nAutoWebGLM (Lai et al., 2024) collects web browsing data for curriculum training and develops\na web navigation agent based off a 6B parameter language model that outperforms GPT-4 on We-\nbArena. Patel et al. (2024) showed that a language model agent can improve its performance through\nfinetuning on its own synthetically generated data. Pan et al. (2024) show that introducing an au-\ntomatic evaluator to provide guidance on task failure or success can improve the performance of\na baseline Reflexion (Shinn et al., 2024) agent. Fu et al. (2024) extracts domain knowledge from\noffline data and provides this to the language agent during inference, to enable it to leverage helpful\ndomain knowledge. Sodhi et al. (2024) proposes a method to enable agents to dynamically compose\npolicies to solve a diverse set of web tasks. Our procedure is an inference-time approach that is\ncompatible with many of these past approaches that develop better base agents.\nIn the multimodal setting, WebGUM (Furuta et al., 2024) finetuned a 3B parameter multimodal\nlanguage model on a large corpus of demonstrations, achieving strong performance on MiniWoB\nand WebShop. Koh et al. (2024) showed that prompting multimodal language models with a Set-\nof-Marks (Yang et al., 2023a) representation enables the model to navigate complex webpages more\neffectively than text-only agents. SeeAct (Zheng et al., 2024) demonstrated that frontier multimodal\nmodels such as GPT-4V (Yang et al., 2023b) and Gemini (Google, 2023) can be grounded and\nprompted to follow natural language instructions for automating web tasks."}, {"title": "2.3 SEARCH AND PLANNING", "content": "Our method also draws inspiration from a rich history of search and planning algorithms in computer\nscience. Search algorithms such as breadth first search, depth first search, and A* search (Hart et al.,\n1968) have long been used in artificial intelligence systems. Newell et al. (1959) and Laird (2019)\ncast goal-oriented behavior as search through a space of possible states. Dean et al. (1993) and\nTash & Russell (1994) proposed planning algorithms over a limited search horizon, and employed\nan expansion strategy to improve plans based off heuristics about the value of information. Tash &\nRussell (1994) showed that this allowed agents to provide appropriate responses to time pressure and\nrandomness in the world. Deep Blue (Campbell et al., 2002), the chess engine which defeated world\nchampion Kasparov in chess in 1997, was based on massive parallel tree search. Pluribus (Brown &\nSandholm, 2019) leverages search to find better multiplayer poker strategies for dynamic situations.\nIn deep learning, search algorithms with neural network components have been instrumental in\nachieving superhuman performance on many games: Monte-Carlo Tree Search (MCTS) (Browne\net al., 2012) was used to provide lookahead search, and was pivotal in the AlphaGo (Silver et al.,\n2016) and AlphaGo Zero Silver et al. (2017) systems that achieved superhuman performance in the\ngame of Go. Gray et al. (2020) performs one-step lookahead search to achieve SOTA on no-press\nDiplomacy. More recently, several papers (Yao et al., 2024; Besta et al., 2024) showed the potential\nof applying search to large language models to introduce exploration over multiple reasoning paths,\nenhancing performance on text based tasks that require non-trivial planning. Others have applied\nMCTS (Xie et al., 2024b; Chen et al., 2024a; Zhang et al., 2024a; Zhou et al., 2024a; Hao et al.,\n2023) to improve the performance of LMs on math benchmarks (Cobbe et al., 2021) or simplified\nenvironments (Yao et al., 2022a; Valmeekam et al., 2023).\nIn contrast to prior work, our setting is grounded in realistic web environments, and we search over\nthe actual environment space (i.e., the web). This means that the search mechanics need to incor-\nporate not just the text outputs of the agent, but also external environmental feedback from a highly\ncomplex (and realistic) environment. Our experiments show that our environmentally grounded tree\nsearch substantially improves the performance of language model agents."}, {"title": "3 METHOD", "content": "In this section, we describe the search procedure (Fig. 1) in detail. Successfully solving a task in a\nweb environment such as (V)WA can be interpreted as navigating to some goal state $s^*$ which gives\na positive reward $R(s^*) = 1$. The agent starts at state $s_0$ (e.g., the homepage). Given a natural\nlanguage task instruction $I$, the agent's goal is to navigate to the goal state by executing a set of\nactions $(a_0,...,a_t) \\in A$. Each action produces a new state $s_{t+1} \\in S$ and observation $O_{t+1} \\in \\Omega$\nfrom the environment. The transition $s_t \\rightarrow s_{t+1}$ is governed by a deterministic transition function\n$T: S \\times A \\rightarrow S$.\nMost approaches treat this as a partially observable Markov decision process, and only condition\non the current observation ot when predicting the next action at to take. This has significant lim-\nitations: the error of the agent compounds with each step, and if an erroneous action is taken at\ntime t, it cannot be easily rectified if this leads to a bad state in the future. Our approach aims to\nalleviate this by explicitly conducting search and backtracking to identify better trajectories. There\nare several components involved which we describe in the following sections: the baseline agent\nmodel (Sec. 3.1), the value function (Sec. 3.2), and the search algorithm (Sec. 3.3)"}, {"title": "3.1 AGENT BACKBONE", "content": "Most SOTA web agents are built through prompting large (multimodal) language models (Zhou\net al., 2024b; Pan et al., 2024; Fu et al., 2024; Zheng et al., 2024; Koh et al., 2024). A pretrained\nlanguage model or multimodal model $f_\\theta$ is prompted with the current webpage observation $o_t$ and\ninstructed to predict the next action $a_t$ to be executed. It is common to leverage prompting tech-\nniques, such as ReAct (Yao et al., 2022b), RCI (Kim et al., 2024), or Chain-of-Thought (CoT)\nprompting (Wei et al., 2022), to improve the performance of the agent. Language model agents also\nallow us to sample a diverse set of actions (e.g., with nucleus sampling (Holtzman et al., 2020)),\nwhich is essential for creating plausible branches to explore during search (see Sec. 3.3). Our pro-\nposed search algorithm can in principle be applied to any baseline language agent model. We show"}, {"title": "3.2 VALUE FUNCTION", "content": "We implement a best-first search heuristic using a value function $f_v$ which estimates the expected\nreward $E[R(s_t)]$ of the current state $s_t$, where the ground truth goal state would provide a perfect\nreward of 1. As the state st of the simulator is not always accessible to the agent (st may include\nprivate information such as database entries of the site), the value function computes the value\n$U_t$ using the current and previous observations of the agent, as well as the natural language task\ninstruction I:\n$U_t = f_v(I, {o_1,..., o_t}) \\in [0,1]$\nIn our experiments, the value function is implemented by prompting a multimodal language model\nwith the natural language instruction and observations as screenshots (Sec. 4.1)."}, {"title": "3.3 SEARCH ALGORITHM", "content": "Our proposed search algorithm is a best-first search method loosely inspired by A* search (Hart\net al., 1968), a classic graph traversal algorithm used widely in computer science. We use a language\nmodel agent to propose candidate branches of the search tree. The search has hyperparameters depth\nd, branching factor b, and search budget c which determine the maximum size of the search tree,\u00b9\nand termination threshold $\\theta$. We describe the procedure in detail in the following paragraphs and\nprovide the formal algorithm in Appendix A.3.\nAt time t in the execution trajectory, the agent has previously executed a sequence of actions to arrive\nat the current state st. We begin the search algorithm from st by initializing the frontier $F \\leftarrow {}$\n(implemented as a max priority queue) which holds the set of states that we plan to evaluate, the best\nstate found so far $\\hat{s}_t \\leftarrow s_t$, the score of the best sequence $\\hat{u}_t \\leftarrow 0$, and the search counter $s \\leftarrow 0$.\nAt each iteration of the search process, we extract the next item from the frontier, $s_p \\leftarrow pop(F)$.\nWe use the value function to compute the score for state $s_p$ (with observation $o_p$ and previous\nobservations $o_1,..., o_{p-1}$):\n$U_p = f_v(I, {o_1,..., o_p})$\nThen, we increment the search counter s, and if up is higher than the current best score \u00fbt, we update\nit and our best state accordingly:\n$s \\leftarrow s+1$\n$\\hat{s}_t \\leftarrow\\begin{cases} s_p & \\text{if } U_p > \\hat{u}_t\\\\ s_t & \\text{otherwise} \\end{cases}$\n$\\hat{u}_t \\leftarrow max(\\hat{u}_t, U_p)$\nIf $U_p \\geq \\theta$ (the agent is likely to have found a goal state) or $s > c$ (the search budget has been\nexceeded), we will terminate the search and navigate to the best state $\\hat{s}_t$ found thus far.\nOtherwise, if the current branch does not exceed the maximum depth (i.e., $|(s_0,..., s_p)| < d$), we\nwill generate plausible next actions for branching by obtaining b candidate actions $\\{a_1^p,...,a_b^p\\}$\nfrom the language model agent $f_\\theta$. For each i, we execute $a_i^p$ and add the resulting state $s_i^p$ to the\nfrontier with the score of the current state\u00b2:\n$F\\leftarrow F \\cup (U_p, s_i^p) \\quad \\text{for } i=1,...,b$\nThis concludes one iteration of search. If both of the termination conditions have not been reached,\nwe backtrack and repeat this process for the next best state from the updated frontier F."}, {"title": "4 EXPERIMENTS", "content": "We run experiments on the full set of 910 VisualWebArena (VWA) and 812 WebArena (WA) tasks.\nThese tasks are distributed across a set of diverse and realistic web environments: a Classifieds,\nReddit, and Shopping environment for VWA, and a Shopping, CMS, Reddit, GitLab and Maps\nenvironment for WA."}, {"title": "4.1 IMPLEMENTATION DETAILS", "content": "Baseline agent models Our search algorithm is compatible with most off-the-shelf language\nmodel agents. In this work, we test it with simpler, more general, prompt-based agents, and leave\nincorporation of our method with more performant methods that incorporate domain-specific tech-\nniques (Fu et al., 2024; Sodhi et al., 2024) for future work. We run several agent baselines (full\nprompts provided in the appendix):\n\u2022 GPT-40 + SoM: We run the multimodal GPT-40 (OpenAI, 2024) (gpt-40-2024-05-13)\nbased agent with the same prompt from Koh et al. (2024). We similarly apply a prepro-\ncessing step to assign a Set-of-Marks (SOM) (Yang et al., 2023a) representation to the\nwebpage. This highlights every interactable element on the webpage with a bounding box\nand a unique ID. The input to the agent is a screenshot of the SoM-annotated webpage, and\na text description of the elements on the page with their corresponding SOM IDs.\n\u2022 Llama-3-70B-Instruct: We run the caption-augmented Llama-3-70B-Instruct agent with\nthe same prompt from Koh et al. (2024). We generate captions for each image on the\nwebpage using an off-the-shelf captioning model (in our case, BLIP-2; Li et al. 2023). The\naccessibility tree\u00b3 representation of the webpage observation is extracted and provided to\nthe language model as its input observation at each step.\n\u2022 GPT-40: On WebArena (which does not require visual grounding), we run a text-only\nGPT-40 agent using the same prompt from Zhou et al. (2024b). Similar to the Llama-3\nbaseline, this model uses an accessibility tree representation of the current webpage as its\ninput observation (but this baseline does not include image captions).\nSearch Parameters We run these agents with and without search. Our search parameters are\nset to d = 5,b = 5, c = 20, and we stop execution after a maximum of 5 actions. We enforce\nthese constraints due to compute and budget limitations, though we expect that increasing these\nparameters is likely to further improve results (see Sec. 5.1 for results on scaling search parameters).\nWe note that the fairly strict limitations on maximum actions imply that there are certain tasks that\nare intractable (e.g., VWA tasks with \u201chard\u201d action difficulty usually require humans to execute 10 or\nmore actions to complete). Despite this, our results show that GPT-40 with search capped at 5 max\nactions still substantially outperforms the GPT-40 baseline (without search) with 30 max actions.\nObtaining Actions We sample actions using nucleus sampling (Holtzman et al., 2020) with a\ntemperature of 1.0 and top-p of 0.95 for all experiments. At each step of execution, we generate 20\noutputs from the model by prompting it with CoT reasoning (Wei et al., 2022), and aggregate the\ncount of the action candidates. We use the top-b actions with the highest counts for branching.\nValue Function As detailed in Sec. 3.2, we require a value function which scores the likelihood\nthat the current state st is a goal state. We implement the value function by prompting a multimodal\nlanguage model (specifically, GPT-40 (OpenAI, 2024) (gpt-40-2024-05-13)) with the task instruc-\ntion I, screenshots of the agent's trajectory, previous actions the agent took, and the current page\nURL. The full prompt is provided in Appendix A.2.2. The multimodal LM is instructed to output\nwhether the current state is a success, a failure, and if it's a failure, whether it is on a trajectory\ntowards success. These outputs are assigned values of 1, 0, and 0.5 respectively (and 0 for invalid\noutput). In order to get more finegrained and reliable scores, we leverage ideas from self-consistency\nprompting (Wang et al., 2023), and sample multiple reasoning paths by prompting the multimodal\nLM with CoT (Wei et al., 2022). We sample 20 different paths from the GPT-40 model using ances-\ntral sampling (temperature of 1.0 and top-p of 1.0). The final value assigned to state st, used in the"}, {"title": "4.2 RESULTS", "content": "Our results are summarized in Tab. 2. Introducing search increases success rate substantially across\nthe board. Search improves the success rate of the baseline GPT-40 + SoM agent on VWA by\n39.7% relatively (increasing from 18.6% to 26.4%), setting a new state-of-the-art on the benchmark.\nOn WA, introducing search to the GPT-4o agent improves the success rate substantially as well,\nincreasing it by 28.0% relatively (15.0% to 19.2%). This is competitive with other prompt-based\nagents on WA, but in future work it will be interesting to explore introducing search to stronger\nbaseline agents that incorporate domain-specific techniques, such as SteP (Sodhi et al., 2024) or\nAutoGuide (Fu et al., 2024).\nWith the Llama-3 caption-augmented agent on VWA, we see that search brings even more significant\nimprovements, more than doubling the success rate. With search, success rate improves by 119.7%\nrelative to the baseline (7.6% to 16.7%). We attribute this to the GPT-40 model used for the value\nfunction being a generally stronger (multimodal) model than Llama-3. With search, Llama-3-70B-\nInstruct achieves success rates that are close to the best frontier multimodal models. As Llama-3\nhas openly available model weights, the strong performance of the Llama-3-70B-Instruct agent with\nsearch can prove to be a cost effective agent model for iteration and development in future work\nwhich requires access to model internals."}, {"title": "5 ANALYSIS", "content": "5.1 ABLATIONS\nWe conduct several ablation experiments on a subset of 200 tasks from VWA (100 Shopping tasks,\n50 Reddit task, and 50 Classifieds tasks).\nSearch budget We plot the success rate of the GPT-40 agent with search limited to varying budgets\nc\u2208 {0, 5, 10, 15, 20} in Fig. 2. All experiments are conducted with search parameters of depth d = 5\nand branching factor b = 5. The search budget specifies the maximum number of node expansions\nperformed at each step. For example, a search budget of 10 indicates that at most 10 nodes will be\nexpanded, after which the agent will commit to and execute the trajectory with the highest value. We\nobserve that success rate generally increases as search budget increases. Notably, performing even\nSearch depth and breadth We run an ablation experiment varying the search branching factor b\nand maximum depth d. The results are summarized in Tab. 3. We observe that in general, success\nrate increases as the size of search tree increases (along both b and d dimensions). In particular,\nscaling both b and d is necessary to achieve strong performance.\nTrajectory-level reranking An alternative to tree search would be to generate multiple trajecto-\nries, re-rank, and commit to the best one as scored by the value function, similar to the methods\nproposed in Chen et al. (2024b) and Pan et al. (2024) without their Reflexion (Shinn et al., 2024)\ncomponent. This is a less practical method, as it is harder to prevent destructive actions from being\nexecuted (see Sec. 5.4 for more discussion) as the agent is required to take the trajectory to com-\npletion before it can be evaluated. It is also a more limited form of search, as it only considers\nentire trajectories and cannot backtrack to prune bad branches. Nevertheless, we perform an abla-\ntion where we sample 3 trajectories from the GPT-40 agent (with nucleus sampling (Holtzman et al.,\n2020) at each step using a temperature of 1.0 and top-p of 0.95) and use the same value function to\nre-rank the trajectories, picking the best one out of 3. We choose 3 as this is the same number of\ntrajectories that Pan et al. (2024) use (performance for their approach peaks at 3 trajectories). This\nre-ranking baseline achieves a success rate of 28.5%, which is worse than our approach with search\nbudget c \u2265 5 (Fig. 2). It is also substantially worse than our approach with c = 20, which achieves\na success rate of 37.0% on the ablation subset, a relative 29.8% increase over re-ranking."}, {"title": "5.2 SUCCESS RATE BREAKDOWN", "content": "Success rate by task difficulty The VWA\nbenchmark includes labels for the action difficulty\nof each task. These labels are human annotated,\nand roughly indicate the number of actions a hu-\nman would need to take to solve the tasks: easy\ntasks require 3 or fewer actions, medium tasks re-\nquire 4-9 actions, and hard tasks demand 10 or\nmore. These guidelines are approximate and de-\nvised by the human annotators of VWA, so there\nmay exist more optimal solutions in practice. The\nincrease in success rate from introducing search is"}, {"title": "5.3 QUALITATIVE RESULTS", "content": "In this section, we show several qualitative examples of agent trajectories, and identify various\nfailure modes that can be solved through incorporating search.\nMore robust multi-step planning Many tasks in VWA and WA require an agent to keep a per-\nsistent memory of multiple previous actions and observations. A common failure mode amongst\nagents without search is that they tend to undo previous actions, or get stuck in loops (see Appendix"}, {"title": "5.4 LIMITATIONS", "content": "While we have shown that introducing search to LM agents obtains promising results on web tasks,\nit does come with some practical limitations. In this section we discuss some common failure modes\nand possible ways to address these.\nSearch can be slow Introducing search allows us to expend more compute at inference time to\nextract stronger results from the pretrained LM agent. However, this results in trajectories taking\nsignificantly longer to execute, as the agent has to perform more exploration and hence more in-\nference calls to the LM. For example, a search budget of c = 20 implies that an agent with search\ncould potentially expand up to 20 states in each search iteration, which would use up to 20\u00d7 more\nLM calls than an agent without search. Research on improving the efficiency and throughput of\nmachine learning systems (Leviathan et al., 2023; Dao et al., 2022; Dao, 2023) will likely help with\noptimizing this, but for practical deployment one may need to carefully set the search parameters b,\nd, and c to balance between achieving better results and overall time spent completing a task.\nIn our approach, we implemented search by keeping track of the sequence of actions required to get\nto a state. During backtracking, we reset the environment and apply the same sequence after resetting\nthe environment. This is necessary, as naively executing the go_back action (Tab. 1) may discard\nimportant information on the page, such as the scroll offset and already entered text. However,\nthese environment calls for backtracking introduce additional overhead, which may be restrictive\nfor deployment if calls to the environment are expensive.\nDestructive actions For real world deployment, we will need to restrict the search space to actions\nthat are not destructive. Destructive actions are defined as actions that will irreversibly change the\nstate of the website and are difficult to backtrack from. For example, placing an order on an e-\ncommerce site is typically very difficult to automatically undo.\nOne way to address this is to introduce a classifier that predicts when certain actions are destructive,\nand prevent node expansion for those states. If we have specific domain knowledge about the down-\nstream application (e.g., we know certain pages should be off limits), such rules can be manually\nenforced with high accuracy. One advantage of our method over trajectory level reranking (Sec. 5.1)\nis that it is easier to incorporate such a constraint: it can be directly integrated into the value func-\ntion to prevent execution of dangerous actions. Another direction to handle this would be to train a\nworld model (Ha & Schmidhuber, 2018) that we can use for simulations, and search over this rather\nthan explore the real world. Search may also be more easily implemented in offline settings where\nactions are non-destructive as they can always be undone or reset, such as programming (Jimenez\net al., 2023; Yang et al., 2024) or Microsoft Excel (Li et al., 2024)."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduced an inference-time search algorithm designed to enhance the capabilities\nof language model agents on realistic web tasks. Our approach integrates best-first tree search with\nLM agents, enabling them to explore and evaluate multiple action trajectories to achieve superior\nperformance on web tasks. This is the first time search has shown to significantly improve the\nsuccess rates of LM agents on realistic web environments, as demonstrated on the (Visual) WebArena\nbenchmarks. Our search procedure is general, and it will be valuable to apply it to other domains\nin future work. We believe that inference-time search will be a key component for building capable\nagents that can plan, reason, and act autonomously to perform computer tasks."}]}