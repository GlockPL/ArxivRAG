{"title": "EdgeEar: Efficient and Accurate Ear Recognition for Edge Devices", "authors": ["Camile Lendering", "Bernardo Perrone Ribeiro", "\u017diga Emer\u0161i\u010d", "Peter Peer"], "abstract": "Ear recognition is a contactless and unobtrusive biometric technique with applications across various domains. However, deploying high-performing ear recognition models on resource-constrained devices is challenging, limiting their applicability and widespread adoption. This paper introduces EdgeEar, a lightweight model based on a proposed hybrid CNN-transformer architecture to solve this problem. By incorporating low-rank approximations into specific linear layers, EdgeEar reduces its parameter count by a factor of 50 compared to the current state-of-the-art, bringing it below two million while maintaining competitive accuracy. Evaluation on the Unconstrained Ear Recognition Challenge (UERC2023) benchmark shows that EdgeEar achieves the lowest EER while significantly reducing computational costs. These findings demonstrate the feasibility of efficient and accurate ear recognition, which we believe will contribute to the wider adoption of ear biometrics.", "sections": [{"title": "I. INTRODUCTION", "content": "Ear recognition has gained attention as a reliable biometric modality for unobtrusive and contactless authentication [11]. Despite advances in deep learning yielding significant performance improvements, state-of-the-art models often rely on computationally intensive architectures, making them unsuitable for resource-constrained edge devices and limiting its wider use on general hardware. Addressing this gap is crucial for secure and efficient authentication across mobile, IoT, and various real-world applications.\nBuilding on the success of lightweight face recognition models such as EdgeFace [13], we introduce EdgeEar, a model designed for ear recognition on edge devices. With fewer than 2 million parameters, EdgeEar balances accuracy and efficiency by adapting EdgeFace's hybrid CNN-transformer architecture to open-set ear recognition.\nExtensive evaluation on the Unconstrained Ear Recognition Challenge 2023 (UERC2023) benchmark [9] demonstrates that EdgeEar achieves performance comparable to state-of-the-art deep learning architectures across key metrics, such as Equal Error Rate, as shown in Fig. 1, while maintaining a significantly smaller parameter count. These findings highlight the potential of adapting edge-optimized face recognition architectures for ear recognition in resource-constrained scenarios. Key contributions of this work include:\n\u2022\nWe introduce EdgeEar, the first ear recognition model specifically designed for edge devices. Combining a hybrid CNN-Transformer architecture with fewer than two million parameters, EdgeEar achieves state-of-the-art performance on the UERC2023 benchmark, attaining an Equal Error Rate (EER) of 0.143, Area Under Curve"}, {"title": "II. RELATED WORK", "content": "Biometric authentication utilizes unique physiological traits to verify identity, offering enhanced security and convenience over traditional key-, secret-, or token-based methods [4]. The ear is particularly suitable for recognition due to its stable morphological features, which remain consistent from childhood to late adulthood [19]. Unlike facial recognition, ear biometrics are less affected by expressions, aging, or accessories such as beards, mustaches, or glasses. Additionally, ear features are typically not obscured by hairstyles or cosmetic changes, ensuring reliable identification [2].\nHowever, ear recognition systems must function in unconstrained environments, where pose, ethnicity, gender,\nA. Ear Biometrics and Core Challenges"}, {"title": "B. Deep Learning Advances for Ear Recognition", "content": "While early ear recognition systems relied on hand-crafted features, contemporary approaches primarily utilize deep learning architectures such as Convolutional Neural Networks (CNNs) [7] and Vision Transformers (ViTs) [6] to learn data-driven embeddings [8]. These models effectively extract discriminative features from large ear datasets, enhancing performance under various conditions.\nFor instance, Alshazly et al. [1] improve ResNet through transfer learning to address data scarcity by employing feature extraction, fine-tuning, and SVM-based classification. Similarly, Korichi et al. [21] introduce TR-ICANet, which integrates CNN-based normalization, Independent Component Analysis (ICA), and tied-rank normalization to boost accuracy. However, both methods are evaluated only on closed-set recognition tasks, where all target identities are known during training, limiting their relevance for open-set scenarios."}, {"title": "C. Lightweight CNN and Transformer Architectures", "content": "As deep networks become increasingly complex, significant progress has been made in developing lightweight architectures tailored for memory- and power-constrained environments. MobileNet [16], [26] introduced depthwise and pointwise convolutions to reduce parameters and computational load. Other efficient CNNs-such as SqueezeNet [18], ShuffleNet [31], ShiftNet [29], and GhostNet [14]-further optimize efficiency through smaller kernels, channel splitting, and shifting techniques.\nWith the rise of Vision Transformers (ViTs), hybrid networks have been developed to integrate transformer capabilities with CNN efficiencies. However, the multi-head attention mechanism often remains a computational bottleneck. Models like Mobile-Former [3] and MobileViT [25] partially address this issue, but still require higher Multiply-Adds (MAdds) and longer inference times.\nEdgeNeXt [23] offers a solution by extending ConvNeXt [22] with a Split Depth-wise Transpose Attention (SDTA) encoder, combining depth-wise convolutions, adaptive kernel sizes, and transpose attention across channels. This design significantly reduces MAdds compared to traditional self-attention, making EdgeNeXt suitable for mobile applications."}, {"title": "D. Lightweight Ear Recognition", "content": "While efficient biometric models have advanced in domains like face recognition, with solutions such as Idiap EdgeFace-XS [13] adapting EdgeNeXt to operate with fewer than two million parameters, lightweight ear recognition remains under-explored. Many existing ear recognition methods rely on closed-set protocols, assuming all target identities have been seen during training. For example, a recent model [24] utilizes an ensemble of over 11 million parameters, achieving 98.74% accuracy on the IITD-II dataset but only within a closed-set framework. Similarly, MobileNet-based approaches [30] employ 3.5 to 5.4 million parameters, which may be too large for memory-constrained devices and are limited by closed-set evaluations.\nTo address these limitations, we introduce EdgeEar, a lightweight ear recognition model inspired by EdgeNeXt and EdgeFace, with fewer than two million parameters. This parameter efficiency aligns with the objectives of competitions like EFaR 2023 [20], where compact architectures are evaluated for their performance-resource trade-off. Unlike prior methods, EdgeEar is evaluated in open-set scenarios, demonstrating robust performance and effective generalization to unseen identities. This development advances the feasibility of deploying ear recognition systems on resource-limited hardware."}, {"title": "III. METHODS", "content": "EdgeEar is a lightweight ear recognition model adapted from the EdgeFace architecture [13] and built upon the EdgeNeXt framework [23]. Designed for deployment on resource-constrained edge devices, EdgeEar maintains a hybrid CNN-Transformer structure, as illustrated in Fig. 2.\nEdgeEar modifies the original architecture by replacing specific linear layers in the Split Depth-wise Transpose Attention (SDTA) modules with Low Rank Linear (LoRaLin) layers, as proposed in EdgeFace [13]. This reduces the total number of parameters while preserving the model's capacity to capture essential features for accurate ear recognition.\nA. EdgeEar Architecture"}, {"title": "B. Low Rank Linear Layers (LoRaLin)", "content": "To improve parameter efficiency, EdgeEar integrates Lo-RaLin layers into SDTA modules. Each linear transformation $Y = W_MX_N + b$ (input dimension N, output dimension M, bias b) is factorized as:\n$Y = W_{Mx r}(W_{r x N}X) + b$,\n(1)\nwhere $W_{Mx r}$ and $W_{r x N}$ are low-rank matrices with rank r, set by:\n$r = max (2, \u03b3 \u00b7 min(M, N))$.\n(2)\nHere \u03b3 controls the rank ratio. This reduces parameters and computational complexity while enabling complexity-performance balancing through \u03b3."}, {"title": "C. Selective Replacement Strategy", "content": "In the final stage of the SDTA module (Stage 4 of the EdgeNeXt architecture), only the linear layers responsible for computing the query (Q), key (K), and value (V) projections are replaced with low-rank layers, following established low-rank approximation techniques [17], [28]. Specifically, the QKV linear layers are substituted with LoRaLin layers using"}, {"title": "E. Ablation Studies", "content": "We conducted two categories of ablation studies:\n1) Ablation With Non-Selective LoRaLin with Varying \u03b3: To evaluate the effectiveness of the selective replace-ment strategy in the LoRaLin layers, we conducted experiments where EdgeFace was instantiated with with varying \u03b3 while ensuring that the number of parameters remained below or near 2M.\n2) Ablation With Different Losses: We also performed experiments with different loss functions, such as regular cross entropy (CE) and ArcFace [5]."}, {"title": "F. Evaluation", "content": "We evaluated our model on the sequestered dataset from the UERC 2023 competition for direct comparison with other architectures. Under an open-set protocol, we first generated 512-dimensional embeddings for each pair of samples, computed cosine similarities between these embeddings, aggregated them per identity, and generated receiver operating characteristic (ROC) curves. By averaging these curves, we obtained a single representative ROC from which we derived key metrics: area under the curve (AUC), equal error rate (EER), False Non-Match Rate at 1% False Match Rate FNMR @ 1% FMR (F1F), and Rank-1 accuracy. We also plotted the ROC curve by demographic subgroups (ethnicity and gender) to highlight bias considerations."}, {"title": "IV. RESULTS", "content": "Table II presents the results of our ablation study on EdgeNext variants with < 2M parameters. EdgeEar, trained with cross-entropy (CE) loss and label smoothing, achieves the lowest equal error rate (EER) of 0.143, highest AUC of 0.904, highest Rank-1 accuracy (R1) of 0.929, and an F1F of 0.544.\nUsing ArcFace (margin: 0.2, scale: 8) instead of CE significantly worsens performance across all metrics, particularly increasing the EER to 0.229 and lowering R1 to 0.571, underscoring the suitability of CE loss for this task.\nRemoving label smoothing from CE leads to a trade-off: while EER increases to 0.198, R1 and AUC slightly decrease to 0.927 and 0.903 respectively, with a decrease in F1F, suggesting that label smoothing improves overall metrics at the cost of higher F1F. Variants of EdgeFace, parameterized by \u03b3, show strong AUC performance, with \u03b3 = 0.6 achieving the same AUC as EdgeEar (0.904), but none surpass EdgeEar in EER or overall balance across metrics. Notably, EdgeFace (\u03b3 = 0.7) achieves the lowest F1F (0.305), though at the expense of higher EER (0.198) and lower R1 (0.879).\nFigure 3 shows the ROC curves for ethnicity and gender splits within the UERC23 dataset. EdgeEar achieves high AUC scores across all demographics, demonstrating robustness, though biases favoring Male-White and Male-Asian groups are evident. These biases stem from the overrepresentation of Male-White identities (74.5%) in the training data and the inclusion of the EARVN1.0 dataset, which boosts Male-Asian representation."}, {"title": "V. CONCLUSIONS", "content": "In this paper, we present EdgeEar, a lightweight ear recognition model optimized for deployment on resource-constrained edge devices, with a parameter count of fewer than 2 million. While we did not formally participate in the UERC 2023 competition, we evaluated our model using the sequestered dataset and official evaluation protocol, enabling direct comparison with competition results. EdgeEar would rank first in EER all while operating at a significantly lower computational cost compared to other submissions.\nOur findings suggest that model capacity is not the main limitation in achieving ear recognition performance comparable to other biometric modalities, such as face recognition. Instead, the size and diversity of datasets play a critical role. Future research should prioritize the development of larger and more diverse datasets to advance the field, enhance the applicability of ear-based biometrics, and mitigate demographic bias."}, {"title": "ETHICAL IMPACT STATEMENT", "content": "EdgeEar was developed using publicly available biometric datasets, with no additional data collection conducted. While the demographic diversity of these datasets is not guaranteed, we evaluate the model's performance across known subsets to identify and address potential biases. Safeguards should be implemented to prevent misuse such as in surveillance."}]}