{"title": "Location is Key: Leveraging Large Language Model for Functional Bug Localization in Verilog", "authors": ["Bingkun Yao", "Ning Wang", "Jie Zhou", "Xi Wang", "Hong Gao", "Zhe Jiang", "Nan Guan"], "abstract": "Bug localization in Verilog code is a crucial and time-consuming task during the verification of hardware design. Since introduction, Large Language Models (LLMs) have showed their strong programming capabilities. However, no work has yet considered using LLMs for bug localization in Verilog code. This paper presents Location-is-Key, an opensource LLM solution to locate functional errors in Verilog snippets. LiK achieves high localization accuracy, with a pass@1 localization accuracy of 93.3% on our test dataset based on RTLLM, surpassing GPT-4's 77.9% and comparable to Claude-3.5's 90.8%. Additionally, the bug location obtained by LiK significantly improves GPT-3.5's bug repair efficiency (Functional pass@1 increased from 40.39% to 58.92%), highlighting the importance of bug localization in LLM-based Verilog debugging. Compared to existing methods, LiK only requires the design specification and the erroneous code snippet, without the need for testbenches, assertions, or any other EDA tools. This research demonstrates the feasibility of using LLMs for Verilog error localization, thus providing a new direction for automatic Verilog code debugging.", "sections": [{"title": "1 Introduction", "content": "Verilog is an important hardware description language. In the verification of Verilog code design, locating bugs is an crucial and time-consuming task. Thus, automatic bug localization in Verilog code is of great significance and has attracted research attention in recent years [14, 19, 22-24, 31, 32, 35, 37]. However, existing methods generally require pre-defined components (e.g., test cases, assertions and templates), which need to be carefully written by experts with extensive professional knowledge. This prolongs the debugging cycle of hardware design.\nIn recent years, Large Lauguage Models (LLMs) have demonstrated their powerful coding capabilities and many LLMs tailored for programming tasks have been proposed [20, 29]. Some works have already considered using LLMs to locate bugs in software languages [4, 9, 13, 34]. Particularly, by leveraging the code understanding capabilities of LLMs, their method could locate bugs with high precision using only the design description and the target code snippet, without relying on any test cases, expert knowledge, or any code analysis tools. For example, on a C-language benchmark Devign [38], LLMAO [34] pretrained from open-source coding LLM CodeGen-16B achieved a top-5 localization accuracy of 60.3%, where top-5 refers to the probability that the buggy line is among the five most likely erroneous lines identified. This inspires us to use LLM for the bug localization problem in Verilog code, which has not been addressed in the literature. Existing works related to Verilog bug localization using LLMs could be divided into the following two categories:\n(1) The first category [8, 15, 18, 26, 27] focused on open-source Verilog-specific LLMs. However, their major concern is Verilog code generation, and their training data is not for locating bugs. Thus, these LLMs are not suitable for the problem of bug localization in"}, {"title": "2 Method", "content": "This section presents the training process of LiK, which includes three steps: Firstly, we use data from the open-source community and synthetic data to conduct two rounds of unsupervised continual pre-training to enhance the base model's basic knowledge on Verilog bug localization. Then, to enable the model to output the correct bug localization, we fine-tune the model with bug localization samples generated from common Verilog error patterns. Finally, to reduce the errors made by the model, we further finetune it through SimPO algorithm."}, {"title": "2.1 Continual Pre-Training Using Verilog/C Corpus and Bug Localization Knowledge", "content": "This step involves two rounds of continual pre-training on the base model. The first round is to enhance the model's understanding on basic Verilog knowledge, while the second is target at the task of functional bug localization. In order to reduce training costs while retaining the base model's coding capabilities, we use LoRA algorithm [10] for pretraining, in which only a small portion (0.9142% in our pretraining process) of all the model parameters are updated.\n2.1.1 Pretraining with basic Verilog Knowledge. In the first round of pretraining, we use Veriseek [18] dataset, which consists of Verilog and C corpora as described below:\n(a) Verilog corpus. This part of data is from VGen [26] and includes two parts: (1) Open-source Verilog files from GitHub, in which each data entry contains \"module\" and \"endmodule\" statements and does not exceed 20000 characters in length; (2) Corpus from the e-PDF of 70 Verilog textbooks, where irrelevant content is filtered out and only entries containing Verilog code are retained.\n(b) C corpus. According to [21], since the syntax of the C language is similar to Verilog, incorporating some C corpus into the model training can enhance its understanding of Verilog syntax. This part of corpus is extracted from the C language documentation in CodeSearchNet [12] dataset.\nWe use MinHash and LSH algorithms to further deduplicate the Veriseek dataset. The final dataset contains a total of 91787 entries, with a total size of 206.48MB, comprising 171.03MB of Verilog-related corpus and 35.45MB of C corpus.\n2.1.2 Pre-Training with Bug Localization Reasoning Thoughts. Regarding our research problem, we also need to further pretrain the model with samples of bug localization. According to [30], using reasoning thoughts that lead to correct the answer for pretraining can enhance the LLM's understanding of input tasks, enabling it to generate more relevant answers. Hence, we apply reasoning"}, {"title": "2.2 Supervised Fine-tuning Using Bug Localization Samples", "content": "After pretraining, the model acquires the fundamental knowledge for functional bug localization. However, since pretraining is unsupervised, the model still does not know how to answer the question \"Where is the bug?\". To solve this problem, we use samples of bug localization to perform supervised finetuning on the model. We use the LoRA algorithm to finetune the model under the supervision of bug localization samples.\nIn terms of the training data, each of the training sample could be represented as (p, l), in which p is the input to the model and l is the expected answer. The input p should be the natural language description of bug localization problem, including the module design description and the buggy code. As for the answer l, A natural idea is to directly apply the buggy line number, just like existing non-LLM solutions. However, LLMs are generally trained on natural language text, with relatively little data on mathematical reasoning. This indicates that if we use numbers as the output, the answer accuracy is likely to decrease. Therefore, we should have the model output the specific content of the buggy line, rather than its line number. A training sample is shown in Fig. 4.\nWe use the same seed dataset and method as in Sec. 2.1.2 for training data generation, producing a total of 7,866 data samples with a size of 36.72MB. Note that we do not use the reasoning thoughts as the expected output of the model, and this issue will be discussed in Sec. 4."}, {"title": "2.3 Reduce the Error using SimPO", "content": "The supervised finetuning in the last subsection teaches the model \"What answers are good\". In this step, we further finetune the model using Reinforcement Learning with Human Feedback (RLHF [7]) to let the model know \"What answers are bad\", thereby reducing the response error. Specifically, we use the SimPO algorithm [17]. In each episode of SimPO, the model receives input from training samples and produces output. It then calculates the reward for the output and objective function based on the preference data, and uses gradient descent to maximize the objective function. The details are as follows.\n2.3.1 Training data of SimPO. According to [17], each training sample could be represented as triples (x, yw, y\u012b), in which x is the input, yw and yr are the positive and negative examples of the output that reflecting human preferences. We use the same seed dataset and method as in Sec. 2.2 to generate training samples, where x is the natural language description for bug localization problem and yw denotes the buggy line. As for the negative example y\u0131, to reduce output errors, we set yr as a correct line in the buggy code snippet that the model may identify as the buggy line. To be specific, we input x into the model and obtain multiple outputs. Then, set yr as the line that appears most frequently among all the non-buggy lines output by the model. If all the outputs are buggy lines, randomly pick a line in the buggy code snippet as yr.\nWe generate 3,542 training samples, with a total size of 19.82MB.\n2.3.2 Reward and Objective Function. SimPO directly aligns the reward function with the generation process of the LLM. Specifically, consider an LLM \u03c0\u03b8, which receives input x and outputs y. The following formula could represent the probability of re generating y given x as the input:\n$P_{\\theta} (y | x) = \\frac{1}{|y|} \\sum_{i=1}^{|y|} log \\pi_{\\theta} (Yi | x, y_{<i})$\nGiven \u1e9e as the constant controlling the magnitude of reward differences, SimPO uses $\u03b2\u00d7p_{\u03b8} (y | x)$ as the reward of output y given x as the input. Then, SimPO applies the Bradley-Terry objective function, which represents the difference between the rewards of yw and yr. To enhance yw's advantage during training, SimPO introduces a difference term y in the function, ensuring that yw's reward exceeds yr by at least y. Thus, the objective function is:\n$L(\\pi_{\\theta}) = E [log \u03c3 (\u03b2\u00d7 p_{\u03b8}(y_{w} | x) \u2013 \u03b2\u00d7 p_{\u03b8}(y_{l} | x) \u2013 \u03b3)]$"}, {"title": "3 Evaluation", "content": "3.1 Experimental Settings\n3.1.1 Training Setups. We trained LiK based on an open-source coding LLM Deepseek-coder-V2-Lite-base-16B [6] using 8 A800-80G GPUs. We apply the cosine annealing to adjust the learning rate during the training process, with warmup steps accounting for 10% of all training steps. The training hyperparameters are shown in Table. 1. Two rounds of pre-training, supervised fine-tuning, and SimPO training take approximately 1, 3 and 20 hours respectively.\n3.1.2 Evaluation Metrics. We apply the pass@k metric, which is commonly used to evaluate the coding abilities of language models [5]. It refers to the probability that at least one out of k generated answers to a given problem is correct. According to [5], the following formula yields an unbiased estimate of pass@k:\n$pass@k := E [1 \u2013 \\frac{C(n-c,k)}{C(n,k)}]$\nin which n>k is the number of generated answers for each test case. In our experiment, we set n = 20 and measure pass@1 and pass@5. Obviously, pass@1 reflects the accuracy of model's single response, while pass@5 evaluates the performance when multiple answers are generated for each testcase.\nIn the evaluation of LiK, given the output of the model, the line within the buggy code snippet that has the smallest edit distance from the output is identified as the line located by LiK.\n3.1.3 Testset. The testset is constructed based on RTLLM [16] dataset, which is commonly used for evaluation on Verilog coding tasks using LLMs. RTLLM consists of 29 typical hardware module design, each of which includes three parts: (1) The design description, which only specifies the design requirements and the input/output interfaces, without mentioning the internal implementation logic; (2) The golden code, with the length ranging from 35 to 175 lines; (3) Testbench. For our testset, we insert bugs into the golden code of RTLLM based on common Verilog error types mentioned in Sec. 2.1.2, ensuring that the resulted buggy code does not cause synthesis errors but fails the testbench tests. Each testcase is formatted as the input shown in Fig. 4. In total, we obtain 102 testcases, with the number of each bug type shown below:"}, {"title": "3.2 Experimental Results", "content": "3.2.1 Main results. We evaluate LiK after each training stages, with the model's temperature set to 0.3. As shown in Table 2, the base model of LiK, Deepseek-coder-V2-Lite-16B, does not perform well. This is because there is relatively little data related to Verilog and bug localization in its training corpus. As the training progresses, the performance of LiK continues to increase. Particularly, after the continual pretraining with the reasoning thought for bug localization, the model's performance improves compared with that using only Verilog and C corpus (i.e., the localization accuracy of LiKpTwt and LiKPT wt+sft is higher than that of LiKPT wot and LiKPT wot+s ft respectively). This indicates that to enable the model to perform bug localization, during the pretraining stage, it is not sufficient to use only the code data; it is also necessary to use corpus specialized to the task of bug localization. In addition, SimPO training reduces the error made by the model, further improving the LiK's performance.\nCompared to LLM-related baselines, the performance of LiK surpasses GPT-4 and is comparable to that of Claude-3.5-Sonnet. Note that, as mentioned in Sec. 3.1.4, the appearance of the buggy line in the response of GPT and Claude does not necessarily mean that the line located actually located by them is the buggy line. Thus, the experimental data is in fact better than the true performance of GPT-3.5, 4 and Claude.\nAs for Strider, for 20 testcases, it could not return a list of buggy lines, which accounts for 19.6% of all the testcases. Hence, the hit rate of Strider is at most 78.4%. Mention that in our evaluation, the list computed by Strider includes 6.67 lines on average, while LiK always returns only one line. Overall, starting from all mismatched output signals, Strider identifies some nodes related to these signals in the abstract syntax tree of the code, with each node corresponding to one possible buggy line. Theoretically, this method dictates that the output of Strider comprises multiple lines. In comparison, LiK could directly pinpoint the buggy line with high precision without the need for testbenches, assertions or other EDA tools. This offers a promising new approach for Verilog bug localization.\n3.2.2 Result on Bug Repair Using LLMs. Fundamentally, the task of bug localization is to facilitate the code repair process. This part briefly examines the impact of bug locations, as computed by LiK, on the process of automatic code repair using LLMs. Specifically, we conducted experiments using GPT-3.5 on the same testset for bug localization. We use a simple prompt to instruct the model to output the fixed code, which contains the design description, the buggy code and the buggy line located by LiK. A repair is considered successful if the fixed code passes the testbench without syntax errors. For comparison, we consider the scenario where the located buggy line is removed from the prompt.\nThe experimental result shows by adding the buggy line located by LiK into the prompt, pass@1 and pass@5 increases from 40.39% and 63.77% to 58.92% and 80.89% respectively. This significant improvement highlights the critical importance of bug localization data in LLM-assisted Verilog code repair."}, {"title": "4 Discussion", "content": "4.1 The Impact of SimPO Training\nSec. 2.3 highlights that SimPO training could reduce common errors in model responses. To validate this, we compare the performance of LiK before and after SimPO training. The performance is evaluated as a function of model temperature, which is typically set between 0 and 1. A higher temperature leads to more varied responses. Fig. 5 shows that prior to SimPO training, pass@5 score varies significantly with the temperature, and is notably higher than pass@1. This indicates that the model's output for the same testcase is very unstable, leading to more incorrect answers. In SimPO training,"}, {"title": "4.2 Why not Using Reasoning Thought for SFT?", "content": "In the second round of continual pretraining, we use the reasoning thoughts for bug localization as the training corpus. Moreover, as mentioned in [30], prompting the LLM to output the reasoning thought instead of directly providing results could improve the answer accuracy. Thus, a natural idea is to let the model output the reasoning thought, as shown in Fig. 3. For this purpose, in the SFT phase, we need to use the reasoning thought rather than just the buggy line as the label of each training sample. However, intuitively, reasoning thoughts are not the intended output. Introducing them during SFT could potentially introduce extra noise, leading to decreased accuracy. To verify this, we finetune the pretrained model using both reasoning thoughts (Generated as in Sec. 2.1.2) and the buggy lines (What we currently do in Sec. 2.2), while tracking the loss changes during training.\nAs shown in Fig. 6, the model trained with reasoning thoughts exhibits significantly higher training loss compared with that trained with only the answers (i.e., loss_sft_with_thought vs loss_sft). This indicates that reasoning thoughts bring additional noise into the training process, preventing the model from fitting the training data well. This also leads to a decrease in answer accuracy: when temperature equals to 0.3, the model trained with reasoning thoughts achieve pass@1 and pass@5 score of only 73.48% and 77.69% respectively, which are lower than 87.54% and 90.83% by the model trained directly with the answer. Hence, we use the localization results directly as data labels for SFT phase."}, {"title": "5 Conclusion & Future Work", "content": "This paper proposes LiK, an LLM-based solution for locating the functional bugs in Verilog codes. LiK is trained through two rounds of continuous pretraining with Verilog/C corpora and reasoning thoughts for bug localization, supervised finetuning with bug localization samples and SimPO algorithm with human preference data. Experimental results show that LiK achieves high localization accuracy, which surpasses GPT-4 and is comparable to Claude-3.5-Sonnet. Also, by utilizing LiK's outputs, the debug efficiency of GPT-3.5 is significantly improved. To our knowledge, this is the first work to consider LLM-based Verilog bug localization. The model and materials of this research will be open-sourced later.\nThis research also indicates that using LLMs for Verilog bug localization is a promising topic, which can be further explored in following aspects:\n(1) Leveraging more information. Currently, LiK only takes the design description and the code snippet as its input. If more expert information or feedback from EDA tools (e.g., Mismatched waveforms from testbench simulation) could be utilized, the performance of LiK might be further improved.\n(2) Extending to more general scenarios. Since existing works mainly considered the problem within a single module, we will extend LiK for more complex hardware designs (e.g., a 5-stage pipeline). Additionally, we aim to adapt LiK for other hardware design languages, including VHDL, SystemVerilog, and Chisel.\n(3) Integration with other components for LLM-based debugging. As mentioned in Sec. 3.2.2, the location of bugs is crucial for LLM-assisted Verilog debugging. Thus, we will develop a Verilog debugging framework consists of multiple LLM agents, each dedicated to a specific task such as bug localization and repair."}]}