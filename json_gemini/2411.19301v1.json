{"title": "Structured Object Language Modeling (SoLM):\nNative Structured Objects Generation Conforming to Complex Schemas\nwith Self-Supervised Denoising", "authors": ["Amir Tavanaei", "Kee Kiat Koo", "Hayreddin Ceker", "Shaobai Jiang", "Qi Li", "Julien Han", "Karim Bouyarmane"], "abstract": "In this paper, we study the problem of gen-\nerating structured objects that conform to a\ncomplex schema, with intricate dependencies\nbetween the different components (facets) of\nthe object. The facets of the object (attributes,\nfields, columns, properties) can be a mix of\nshort, structured, type-constrained facts, or\nlong natural-language descriptions. The object\nhas to be self-consistent between the different\nfacets in the redundant information it carries\n(relative consistency), while being grounded\nwith respect to world knowledge (absolute con-\nsistency). We frame the problem as a Lan-\nguage Modeling problem (Structured Object\nLanguage Modeling) and train an LLM to per-\nform the task natively, without requiring in-\nstructions or prompt-engineering. We propose\na self-supervised denoising method to train the\nmodel from an existing dataset of such objects.\nThe input query can be the existing object itself,\nin which case the model acts as a regenerator,\ncompleting, correcting, normalizing the input,\nor any unstructured blurb to be structured. We\nshow that the self-supervised denoising train-\ning provides a strong baseline, and that addi-\ntional supervised fine-tuning with small amount\nof human demonstrations leads to further im-\nprovement. Experimental results show that\nthe proposed method matches or outperforms\nprompt-engineered general-purpose state-of-\nthe-art LLMs (Claude 3, Mixtral-8x7B), while\nbeing order-of-magnitude more cost-efficient.", "sections": [{"title": "1 Introduction", "content": "Following natural-language text generation and\ncode generation by the state-of-the-art Large Lan-\nguage Models (LLMs) (Jiang et al., 2024; Reid\net al., 2024; Floridi and Chiriatti, 2020; Anthropic,\n2024; Jiang et al., 2023), structured objects gen-\neration, also known as JSON (JavaScript Object\nNotation) generation or key-value pairs object gen-\neration, is a challenging problem for existing LLMs\n(Kitouni et al., 2024). It is one of the most desired\nbehaviour of LLMs when used in production set-\ntings beyond traditional chatbot applications. It\nallows LLMs to be used as autonomous agents\nthat integrate seamlessly with APIs, since JSON\nis the de-facto communication standard between\nAPIs, and the universal string serialization format\nof structured objects. It also allows to use LLM out-\nputs directly without any post-processing required,\nfor example writing the output directly to a data\nstore or passing it as input to subsequent functions.\nFinally, it allows to optimize LLM inference cost\nand number of calls and ensures self-consistency\nof the output by generating the entire object in a\nsingle LLM call, instead of generating each field\nof the object independently by an LLM query.\nGeneral-purpose instruction-following and\nhuman-intent-aligned LLMs (chatbots) can be\nsteered towards generating JSON objects outputs\nby specifying the requirement in their instruction\nprompts. Multiple prompting techniques have\nbeen tried to ensure that the output is a valid\nJSON that conforms to a schema, with variable\nsuccess (Beurer-Kellner et al., 2024; Wang,\n2024; Sengottuvelu, 2023). State-of-the-art LLM\nservices such as OpenAI's GPT-4, Anthropic's\nClaude 3, and Mistral AI's models, to name a few,\nhave also recently introduced a \u201cJSON-mode\" that\nallows the user to steer the model's output towards\ngenerating JSON, but without strict guarantee and\nstill requiring the model to be explicitly instructed\nto output the JSON (OpenAI, 2024). Various\nwrapper libraries like JSONFormer (Sengottuvelu,\n2023) allow to decompose the JSON genera-\ntion problem into multiple independent value\ngeneration queries for each key, then using the\ngenerated values to fill the schema of the object\nin a post-processing re-composition step. The\ndrawbacks of the approaches mentioned above"}, {"title": "2 Self-Supervised Training", "content": "In the following, we use the pre-trained MPT-7B\nas the backbone transformer architecture. MPT-7B\nis a decoder-only transformer pre-trained on En-\nglish text and code including 1 trillion tokens (Mo-\nsaicML, 2023). However, the proposed approach\ncan be applied to any generative model (encoder-\ndecoder or decoder-only). MPT-7B supports ALiBi\nposition encoding for long text processing and gen-\neration regardless of the training text length and\nFlash Attention (Dao et al., 2022) for less GPU\nmemory usage and a faster attention algorithm.\nThe self-supervised learning approach does not\nrequire human labeled data and uses denoising\ntechniques on a corpus of existing noisy data and\ntasks (Tay et al., 2022; Raffel et al., 2020). The"}, {"title": "2.1 Noising Functions", "content": "The two main components of the self-supervised\nlearning stage are 1) a dataset of existing objects\nof interest (possibly noisy) and 2) a set of noising\nfunctions to apply on these noisy objects and gen-\nerate very noisy inputs. The self-supervised model\nlearns to remove the noise from the very noisy\nobjects to recover their less noisy version. The as-\nsumption we make is that most of the objects in\nthe dataset naturally carry some minimum amount\nof quality. We use these objects as target samples,\nand use the set of engineered noising functions to\ncorrupt these target objects to form artificial input\nsamples. The concatenation of the corrupted input\nand original target form one learning sample.\nEach component (facet) of an object is corrupted\nbased on a subset of noising functions only target-\ning that component while using other components\nfor noise customization. For instance, a structured\nobject in an e-commerce catalog dataset would in-\nclude four components: 1) title, 2) free-form bullet\npoints describing the product features, 3) long de-\nscription, and 4) tabular attributes such as color,\nmaterial, brand, and size (up to a few hundred at-"}, {"title": "2.2 Training Data Preparation", "content": "For each structured object, a random combination\nof the noising functions explained above is calcu-\nlated on-the-fly and applied during training. The\nnoising function for each component is randomly\nselected from a noising functions pool for that com-\nponent. The noise intensity for each function (for\ninstance, average number of words removed from\nthe title) is itself randomly chosen from 0 to 100%.\nAt the end, a combination of noising functions on\nall the components are applied to the structured\nobject to prepare a noisy structured object.\nWhile adequate for the use-case of regenera-\ntion of existing structured object for the purpose\nof cleaning (completing, correcting, normalizing,\netc), the combination of the targeted noising func-\ntions mentioned above is not sufficient for the use-\ncase of structured object generation from scratch\ngiven free-style input contexts or completely un-\nstructured blurb inputs. To make the model more\ngeneral and able to convert any informative text to\nthe structured object of interest, with p probabil-\nity (e.g. p = 30%), we apply additional extreme\nnoising to convert the corrupted JSON to so called\n\"soup-of-words\" (including complete structure de-\nstruction of the input and random shuffling of the\ntokens). Fig. 1 shows the noising functions and the\nfinal noise combination specified for e-commerce\ntraining data preparation. As shown in this figure,\nthe model can get any input (plain text, structured\nobject/JSON, image caption, tabular data, etc.) and"}, {"title": "2.3 Denoising Training", "content": "The fine-tuning in the decoder-only model is con-\nducted by feeding the prepared input followed by\nthe target structured object to the LLM for CLM\n(causal language modeling) training. The training\nsample template is\n<BOS><input text><\\n><target JSON><EOS>\nAt inference, the model requires the noisy (orig-\ninal) structured object or text followed by \u201c\\n\u201d to\ngenerate the corresponding structured object."}, {"title": "3 Supervised Fine Tuning", "content": "Through the use of denoising functions, the self-\nsupervised denoising trains the LLM to generate\nstructured objects that conform to the target schema.\nAlthough it is effective in adapting a pretrained\nLLM to the desired domain, as shown in Section 6,\nthe resulting model performance is limited as it is\nnot explicitly trained to generate human preferred\nobjects, especially on the subjective parts of the\nobject (e.g. long description and free-form bullets).\nSupervised Fine Tuning (SFT) is commonly\nused in the literature to align an LLM to desir-\nable user responses (Ouyang et al., 2022). Given\na structured object, there exist a notion of human\ndesirable or preferred responses. The desired out-\nput is one that contains all relevant and factual\ninformation representing the data. The key to SFT\nlies in a demonstration dataset generated by human\nexperts."}, {"title": "4 Data Corpus", "content": "We validate our approach in the domain of e-\ncommerce structured product catalog data."}, {"title": "4.1 Self-Supervised Denoising Dataset", "content": "We used a sample dataset from an established e-\ncommerce online store containing 30 million prod-\nuct listings across thousands of product categories,\nfiltered with simple heuristics to ensure a minimum\ndata quality bar. The components representing a\nproduct are correlated and have different data types\nlike image, free-form text, structured attributes, and\nclass names. These product listings are used as the\ntarget text generated by our model and the input\nis created by the proposed targeted noising func-\ntions explained in the previous section. The self-\nsupervision stage maximizes for training data quan-"}, {"title": "4.2 SFT Dataset", "content": "A naive approach to SFT would be to collect a\nsmall amount set of supervised training data for\nall product categories. However given the num-\nber of categories present in the data-set, this naive\napproach is both expensive and impractical to im-\nplement at scale. Instead, we propose a training\npipeline similar to a funnel, where as it progress\ndown the funnel the quality of the training data\nimproves but with lower quantity:\n\u2022 SFT Stage 1 - Existing High Quality Struc-\ntured Objects: We used an ad-hoc model\ntrained to predict product quality to select ex-\nisting structured objects from the noisy cor-\npus which are of high quality for SFT training.\nThis product quality model is trained based on\nexisting business definition of product listing\nquality. The model is able to identify a sam-\nple subset of around 200K existing products\nwhich are deemed to be high quality accord-\ning to this definition, filtered down from the\noriginal 30M self-supervision dataset.\n\u2022 SFT Stage 2 - Human Labels: Due to the\naggressive nature of SFT Stage 1, many prod-\nuct categories remain under represented in the\nSFT training data set. Samples from these\nunder represented products are then sent to\nhuman experts for labelling. Each human re-\ngenerated product is cross checked by another\nexpert. This dataset contains around 3K prod-\nuct listings (structured objects)."}, {"title": "5 Model Training and Evaluation Metrics", "content": "5.1 Training\nThe 7B SOLM model is trained on 5xAWS P4 in-\nstances, each with 8x 40GB A100 GPUs. We run\nablations on a few backbone architectures includ-\ning FLAN-T5 (XL and XXL) (Chung et al., 2022),\nMPT-7B (MosaicML, 2023) and Mistral-7B (Jiang\net al., 2023) to find the best pre-trained base model\nfor the rest of the developments. See Appendix B."}, {"title": "5.2 Evaluation Metrics", "content": "The evaluation metrics we use in this paper fit the\nmulti-facet structured objects (such as JSON data"}, {"title": "6 Experiments and Results", "content": "6.1 Offline Evaluations\nWe performed initial experiments to assess the per-\nformance of the self-supervised model in compari-\nson with a SOTA instruction-tuned LLM (Mixtral-\n8x7B-Instruct) with JSON-mode in zero-shot. The\nself-supervised model outperforms zero-shot Mix-\ntral significantly. For example on title generation,\nthe self-supervised SoLM outperformed Mixtral\nby 40.38 percentage points on Rouge-L F1 Score.\nDetails are available in Appendix A.\nAs a proof of concept, Table 1 shows the results\nof the self-supervision model on the synthetic task\nof improving and regenerating the whole structured\nobject in one pass after applying a combination of\nsynthetic noises to all the object's components."}, {"title": "6.2 Real Test Cases", "content": "The real case benchmark consists of a sample of\naround 5K product listings randomly sampled from\nan e-commerce catalog, with the task of improving\ntheir quality and fixing any issue with the listing.\nThe original (input) structured objects and the re-\ngenerated ones were all human labeled to measure\nthe baseline versus the regenerated quality.\nTable 2 shows the product listings quality, ver-\nsus the quality of regenerated ones by the proposed\nmodel (natively) and by SOTA LLM extensively\nprompt-engineered for the task. Claude 3.0 Sonnet\nwas prompt-engineered for generating the object in\none LLM call (single prompt). We also compare\nagainst an alternative prompting strategy consist-\ning in running multiple independent LLM calls for\neach component/attribute of the object, generating\nthe structured object one piece at a time by execut-\ning the prompt for each attribute separately. This\nstrategy requires >100 LLM runs per object fol-\nlowed by post processing for recomposing the ob-\nject. Due to its high throughput requirement (100X\nthroughput), we could not use Claude 3.0 for this\napproach, therefore we used a SOTA self-hosted\nopen-source LLM, namely Mixtral-8x7B-Instruct.\nNote that both prompt-engineering approaches re-\nquire product categories and corresponding product\nschema to be given as input. This requires run-\nning an upstream product category classification\nmodel and connecting the prompt with a product-\ncategory to product-schema mapping table. The\nprecision and recall in Table 2 represent the cor-\nrectness and completeness scores of the generated\nattributes (Sec. 5.2). The title quality is a composite\nscore of human scores of overall quality, and au-\ntomatic quality check of title length and restricted\ncharacters/phrases in the title. The feature bullets\nquality is assessed by heuristics rules.\nAs our SoLM model is trained in 2 stages (self-\nsupervised training followed by SFT), we report\nresults for both stages. As reported in Table 2, the\nself-supervised model shows high precision (cor-\nrectness) for structured attribute generation. Gen-\neral self supervised denoising increases the hallu-\ncination rate as the model is trying to fill out the\nmissing parts as much as possible which drops pre-\ncision (correctness). However, in our proposed\ntargeted denoising, we define specific control vari-\nables in the noising functions (as explained earlier)\nto minimize the hallucination, resulting in high\nprecision/correctness. The SFT model shows sig-"}, {"title": "6.3 Online A/B Tests", "content": "Structured product data are mostly self-reported\nby individual retailers when listing on e-commerce\nproduct websites. Studies have shown that these\nself-reported data can be sparse and contain noisy\nfacts (Cheng et al., 2023). In this paper we use the\nproposed LLM to improve the product titles that\nis provided by retailers. Specifically given all rele-\nvant information provided by retailers when listing\na product, our goal is to enhance the initial retailer\nprovided product title in a manner in which will\nimprove our buyers experience in discovering prod-\nucts relevant to their intent. Improving our buyer's\nshopping experience consequently will also mean-\ningfully improve our retailers products exposure.\nWe use the enhanced title as output by our\nLLM - to run an online A/B test against the ex-\nisting retailer provided version in an English Lan-\nguage Store over a period of 2 weeks. Evalua-\ntion results show that our customers (buyers) pre-\nfer the title generated by our LLM compared to\nthe existing retailer provided version overall. Par-\nticularly the revised titles improved revenue (p-\nvalue=0.059) and increased the total units pur-\nchased (p-value=0.034)."}, {"title": "7 Conclusion", "content": "This paper proposes a new approach to generate\nstructured objects in a single pass without needing\nany prompt nor objects' schema. The Structured\nObject Language Model (SoLM) is trained using a\nnovel self-supervised training method incorporat-\ning a combination of targeted noising functions to\nhelp create or improve structured objects with com-\nplete, correct, and normalized components. The\nself-supervised model is further fine-tuned on hu-"}, {"title": "B Training Details", "content": "The FLAN-T5s are encoder-decoder models while\nothers are decoder-only models. The current T5 ar-\nchitecture does not support existing Flash Attention\nalgorithms (Dao et al., 2022). Thus, the training\nand inference time is expected to be high and the"}]}