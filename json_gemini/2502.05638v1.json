{"title": "ELMTEX: Fine-Tuning Large Language Models for Structured Clinical Information Extraction. A Case Study on Clinical Reports", "authors": ["Aynur Guluzade", "Naguib Heiba", "Zeyd Boukhers", "Florim Hamiti", "Jahid Hasan Polash", "Yehya Mohamad", "Carlos A Velasco"], "abstract": "Europe's healthcare systems require enhanced interoperability and digitalization, driving a demand for innovative solutions to process legacy clinical data. This paper presents the results of our project, which aims to leverage Large Language Models (LLMs) to extract structured information from unstructured clinical reports, focusing on patient history, diagnoses, treatments, and other predefined categories. We developed a workflow with a user interface and evaluated LLMs of varying sizes through prompting strategies and fine-tuning. Our results show that fine-tuned smaller models match or surpass larger counterparts in performance, offering efficiency for resource-limited settings. A new dataset of 60,000 annotated English clinical summaries and 24,000 German translations was validated with automated and manual checks. The evaluations used ROUGE, BERTScore, and entity-level metrics. The work highlights the approach's viability and outlines future improvements.", "sections": [{"title": "1 Introduction", "content": "There is a growing need for interoperability and digitalization in Europe [6]. The variety of health systems and the need to cope with legacy documentation and procedures present in the sector foster the search for innovative approaches, which should address the upcoming requirements of the European Health Data Space (EHDS [8,16]). Recent advances in Large Language Models (LLMs) offer a potential for structured clinical information extraction (IE) to enhance the quality and interoperability of healthcare. Furthermore, automated IE reduces the manual effort required by healthcare professionals in daily tasks, such as clinical"}, {"title": "2 State-of-the-Art", "content": "Earlier approaches for data extraction from clinical reports include rule-based systems [20]. New approaches involve supervised machine learning (ML) models [1,13]. The constant evolution in the medical field requires the update of such methods, implying a manual effort [2]. Likewise, supervised ML models also need large annotated datasets, which are expensive and take a long time to create [30].\nThe rising development of Large Language Models (LLMs) in recent years has shown significant potential in Natural Language Processing (NLP), including clinical IE. Transformer-based LLMs [26] can handle and understand large amounts of text with limited task-specific training without the need for a large labeled dataset [24]. A concern is that these models are pre-trained on general texts and they often lack domain-specific knowledge. This can cause LLMs to"}, {"title": "3 ELMTEX Dataset and Evaluation Approach", "content": "For a clinical report R, represented as a sequence of tokens R = {r1,r2,...,rn}, the task is to map R to a structured representation S:\nS = {C1: s1, C2 : s2, ..., Ck :sk}\nwhere S is an object containing predefined categories; C = {C1,C2,...,Ck} represents the set of categories with k = 15; and si is the extracted information for category Ci, expressed as a string. For each si, we use separate concepts delimited by semicolons (;) since each category may contain multiple concepts.\nThe categories Ci are listed in Table 1.\nThe goal is to train and evaluate LLM fo, to approximate the mapping function fe: R\u2192S, such that for each category Ci, the extracted string si accurately reflects the corresponding structured information derived from R. The dataset D used for training and evaluation consists of N samples, where each sample (R, S*) includes a clinical report R and its corresponding ground truth structured representation S*."}, {"title": "3.1 Evaluation", "content": "The experiments explore three model setups, described in the following:\nNaive Prompting. It consists of directly querying the LLM with a simple instruction to extract information for all categories from R. We constructed a simple prompt P to describe the task and explicitly list the categories C from which information needs to be extracted, without providing detailed definitions or scope for each category. The expectation is that the LLM fe can infer the meaning and scope of each category based solely on the category names and perform the task as instructed.\nThis setup relies on the LLM's inherent ability to understand the semantic meaning of the task described in the prompt, comprehend the intent behind each category name, and generate syntactically valid and semantically accurate structured output without further guidance. However, due to the lack of explicit definitions or examples for each category, the model's output may vary significantly in quality and completeness. The performance in this setup depends primarily on the model's pre-training data and generalization capabilities.\nAdvanced Prompting with In-Context Learning. It includes examples of input-output pairs as context within the prompt to guide the LLM. Here, we explicitly defined each category Ci\u2208 C, providing clear descriptions and the scope of information expected for each category. To further improve task performance, we integrated in-context learning by incorporating examples retrieved from a training set. Given a clinical report R from the test set, an encoder-based retrieval model go retrieves the top m most similar clinical reports R' = {R1, R2,..., Rm} from the training set, where the training set is disjoint from the test set. These retrieved reports are paired with their corresponding annotated structured representations S' {S1, S2,..., Sm}, forming the in-context examples. Formally, the retrieval process is defined as:\ng\u00a2 : R \u2192 {(R1, S\u2081), (R2, S\u00bd), . . ., (Rm, Sm)}\nwhere go ranks training examples based on their semantic similarity to R using cosine similarity. The prompt P is constructed with a detailed task description, instructing the LLM to extract structured information from R. The explicit definitions and scopes for all categories Censure clarity in the expected output for each category. In addition, the retrieved clinical reports R' and their corresponding annotated structured representations S' are appended to the prompt as in-context examples, demonstrating how the task should be performed. Our objective is to leverage the LLM to use explicit task instructions and category definitions to better understand the task. We intended that the LLM learns from the retrieved in-context examples to handle the clinical report of the test R more effectively and to generalize to new instances using similar previous examples. By integrating retrieval-augmented in-context learning, this setup mitigates ambiguities in naive prompting and enables the LLM to produce a more accurate and reliable output for each category."}, {"title": "LLM Fine-tuning", "content": "We trained small LLMs on a domain-specific dataset to optimize fe for clinical reports. Building upon our naive prompting approach, we employed parameter-efficient fine-tuning using Low-Rank Adaptation (LORA) [14]. This method enables the LLM fe, to adapt to the task while maintaining its general pre-training capabilities. The goal is for the LLM to learn the task-specific mapping fo : R \u2192 S and the definitions and scope of each category C\u2208 C during fine-tuning, therefore eliminating the need for detailed prompts at inference time. Fine-tuning is conducted on the training set, which comprises 90% of the total dataset D. Each training instance consists of a clinical report R and its corresponding ground truth structured representation S*. The objective of fine-tuning is to minimize the loss L between the predicted structured representation S and the ground truth S*:\nL = \u2211(Loss(fo(Ri), S))\nwhere N is the number of training samples, and Loss refers to the loss of cross-entropy. The fine-tuning process enables the LLM to internalize the task-specific knowledge required, learn the definitions and scopes of each category Ci directly from the training data, and generalize effectively to unseen test samples, including edge cases and ambiguous scenarios.\nBy fine-tuning on domain-specific data, the model becomes more robust and precise, overcoming the limitations of relying solely on prompt engineering. This is particularly important given the complexity and variability of clinical reports, where complete coverage of all possible scenarios through prompts alone is unfeasible. Unlike the other setups, which depend heavily on prompt construction, the fine-tuned LLM requires only minimal instructions at inference time."}, {"title": "3.2 Dataset Generation Workflow", "content": "We introduced a new dataset of clinical report summaries, annotated with structured information across 15 categories [12]. This dataset was created to address the lack of large-scale resources for clinical IE. It also promotes the development of methods tailored to clinical data, helping to improve healthcare provision.\nThe dataset contains 60,000 annotated English clinical report summaries, from which we translated over 24, 000 examples into German. The dataset is based on PMC-Patients [31], a collection of 167,000 patient summaries from case reports in PubMed Central. We extracted a subset of patient reports for our work and used a semi-automated approach to create the dataset. First, we defined the categories and their scopes by reviewing related work [3] and consulting physicians to ensure that the selected categories were relevant. Afterwards, we manually annotated reports to serve as in-context learning examples. We then used the GPT-4 model with advanced prompting and in-context learning to generate the initial annotations."}, {"title": "4 Experiments", "content": "4.1 Experimental setup\nBaseline Models We used the Llama 3 series [10] of models for our experiments. For small-sized models, we employed Llama 3.2 1B&3B Instruct for advanced prompting with in-context learning and LoRA fine-tuning setups. We skipped naive prompting for these models, as their size and pre-training were insufficient for effectively performing the task or generating properly formatted JSON outputs. For medium-sized models, we used Llama 3.1 8B Instruct across all setups, including naive prompting, advanced prompting, and fine-tuning. For large models, we selected Llama 3.1 70B Instruct and Llama 3.1 405B Instruct. These were tested only with naive and advanced prompting, as fine-tuning such large models is not optimal for a task of this specificity. For the Llama 405B Instruct model, we used FP8 quantization due to GPU resource limitations. The experiments were primarily run on 4 H100 GPUs. However, smaller and medium-sized models could be run on a single GPU, as LoRA fine-tuning reduces the GPU memory requirements.\nEvaluations Metrics We used three metrics for evaluation, chosen to provide complementary insights into the accuracy and relevance of the extracted information."}, {"title": "4.2 Results", "content": "Table 2 summarizes the results comparing different setups in the models. We observe that the LLama 3.1 8B fine-tuned model achieves the best overall performance across all metrics, outperforming all non-fine-tuned models, including the LLama 405B, even with advanced prompting. This suggests that fine-tuning is crucial for enabling the model to fully grasp the various definitions and concepts required for each category. We also see that fine-tuning smaller models like LLama 3.2 1B&3B yields surprisingly strong results. These models, which can run on edge devices and require far fewer resources, demonstrate their potential for practical deployment in resource-constrained environments.\nOn the other hand, we note that only large models perform well with naive prompting, which confirms their inherent advantage due to their scale and pre-training. Advanced prompting and in-context learning, however, enable medium and large models to perform significantly better, underscoring the value of this more sophisticated approach."}, {"title": "4.3 Error Analysis", "content": "For the error analysis, we randomly sampled 20 incorrect predictions for each model experiment setup and examined the types of errors. The analysis revealed two main error types:\nMissing Extracted Information Models often missed specific concepts within certain categories. This was particularly common in naive and advanced prompting setups. While the main concepts for each category were usually extracted, additional relevant concepts were often missed. This error was mainly observed in categories like diagnostic_techniques_procedures,\ndiagnosis, laboratory_values, and pharmacological_therapy. A possible reason is that these categories can involve lengthy lists of concepts, which can be spread over multiple sentences in clinical reports, making it challenging for the LLM to capture all relevant details.\nWrongly Categorized Concepts Errors involving the misplacement or incorrect categorization of concepts were observed, particularly between similar categories. This issue was more frequent in naive and advanced prompting setups and was the primary error type for fine-tuned smaller LLMs (1B&3B). The affected categories included life_style, family_history, social_history, medical_surgical_history, signs_symptoms, and comorbidities. Such errors can occur due to misinterpretation of the LLMs in sentences.\nAdditionally, for LLama 3.2 1B with advanced prompting, we identified instances of complete hallucination, where the model generated non-existent concepts or concepts copied from in-context learning examples. This behavior was expected given the small size of the model and its limited capacity to retain pre-trained knowledge and follow detailed instructions."}, {"title": "5 Conclusions and future work", "content": "In this article, we investigated the potential of LLMs for clinical IE, focusing on tasks involving structured data generation from unstructured clinical reports. We systematically evaluated approaches like naive prompting, advanced prompting with in-context learning, and fine-tuning across LLMs of varying sizes, and observed that fine-tuning not only enhances performance but also bridges the gap between large and small LLMs. The results highlight the practical effectiveness of smaller fine-tuned models for deployment in real-world clinical settings with\nlimited resources. To support our experiments, we released a large-scale clinical dataset containing 60,000 patient summary reports in English. The dataset offers extensive coverage of various clinical categories and serves as a valuable benchmark in the field.\nOur future work is focused on different aspects. First, we are refining our demonstrator user interface to facilitate clinicians the evaluation of the results of the model. We also identified that the categories need to be refined to be better synchronized with the standard terminologies and data models in the health sector, as highlighted by some of the approached clinical teams. This will contribute to the refinement of the domain-specific training. Additionally, we are considering the expansion of the multilingual capabilities of the dataset and the training approach. Furthermore, we should also consider how our developments are influenced by the AI-Act [7] in Europe."}]}