{"title": "Corporate Fraud Detection in Rich-yet-Noisy Financial Graph", "authors": ["Shiqi Wang", "Zhibo Zhang", "Libing Fang", "Cam-Tu Nguyen", "Wenzhon Li"], "abstract": "Corporate fraud detection aims to automatically recognize companies that conduct wrongful activities such as fraudulent financial statements or illegal insider trading. Previous learning-based methods fail to effectively integrate rich interactions in the company network. To close this gap, we collect 18-year financial records in China to form three graph datasets with fraud labels. We analyze the characteristics of the financial graphs, highlighting two pronounced issues: (1) information overload: the dominance of (noisy) non-company nodes over company nodes hinders the message-passing process in Graph Convolution Networks (GCN); and (2) hidden fraud: there exists a large percentage of possible undetected violations in the collected data. The hidden fraud problem will introduce noisy labels in the training dataset and compromise fraud detection results. To handle such challenges, we propose a novel graph-based method, namely, Knowledge-enhanced GCN with Robust Two-stage Learning (KeGCNR), which leverages Knowledge Graph Embeddings to mitigate the information overload and effectively learns rich representations. The proposed model adopts a two-stage learning method to enhance robustness against hidden frauds. Extensive experimental results not only confirm the importance of interactions but also show the superiority of KeGCNR Over a number of strong baselines in terms of fraud detection effectiveness and robustness.", "sections": [{"title": "1. Introduction", "content": "Corporate fraud refers to illegal schemes by listed companies in the stock market, aiming at financial gains through different means such as fraudulent financial statements and illegal insider trading. This kind of fraud bears systematic risks, which can potentially lead to financial crises at the macro level [1]. Unfortunately, the rapid growth of young capital markets has given rise to an increasing number of fraudulent cases in recent years, putting pressure on regulators and auditors. Since the traditional human supervision solution is no longer efficient, it is desirable to build an autonomous system to assist regulators in this essential task.\nPrevious studies on machine learning for corporate fraud detection focus mostly on traditional machine learning (ML) methods such as linear regression [2, 3, 4], random forest [5, 4], and BP neural network [5, 4]. These machine-learning models are built to classify annual financial statements as fraudulent or not, based on expert-chosen feature sets. Unfortunately, the rich interactions in the company network have not been effectively integrated for corporate fraud detection. Financial experts, on the other hand, have recognized the influence of \"Directors/Supervisors/Executives (DSE)\" and \"Related Party Transactions (RPT)\" on corporate fraud (see Figure 1). DSE refers to the members of the director board of the company. Being the decision-making body in a company, the director board is certainly the agent behind most corporate frauds [6]. Connection via DSE also helps companies lower the coordination cost for illegal activities, thus significantly increasing the likelihood of committing fraud [7]. RPT refers to deals or arrangements between two companies that are joined by a previous business association or share common interests. RPTs, particularly those that go unchecked, carry the risk of financial fraud by various means such as illegal profit transmission [8, 9].\nIn order to better understand the role of rich relations between companies in fraud detection, we first collect 18-year financial records of A-share listed companies"}, {"title": "Challenge I (Information Overload)", "content": "Our objective is to incorporate relational data on a financial graph to help corporate fraud detection. The common approach is to make use of Graph Convolution Networks [10, 11, 12, 13], which have shown great success in dealing with relational data. Unfortunately, directly applying GCN [10] on financial graphs is difficult. This is because the number of DSE and RPT nodes in our graph (referred to as support nodes) dominates the number of company nodes (referred to as target nodes) by around 20 times, hindering the message-passing process between companies. In addition, many of the support nodes lack attributes, making the problem more serious. We refer to this as the information overload problem, which is caused by the excessive number of (noisy) support nodes. Unfortunately, existing graph-based methods for other fraud detection tasks [14, 15, 16, 17, 16] are not of help since they do not face this problem. More specifically, these studies target personal or transaction fraud detection, where the number of these target nodes (persons, transactions) is from 4 to 100 LARGER than the support nodes. The closest one to ours is [18] which exploits the hierarchical graph structure to carefully distill information from the support nodes into the target nodes, mitigating the information overload issue. This method, however, is not general enough to be applied to our task due to the strong graph structure assumption."}, {"title": "Challenge II (Hidden Fraud)", "content": "In the real world, regulators often discover a company's fraud much later than the time it occurred. According to our collected data in the 18 recent years, only 30% of frauds were found in the same year as the violation actually happens (see Section 4), which implies a large percentage of \"hidden fraud\" that cannot be detected timely. Those hidden frauds will result in noisy labels in the training dataset. It has been proven that noisy labels can cause serious bias in the GCN models and harm the classification re-"}, {"title": "3. Preliminary", "content": "Given all the important relations and properties, it is crucial to consider how we can better use these various types of data and build the financial knowledge graph. This section introduces our knowledge graphs and the concept of meta-path company subgraphs. These are preliminary concepts that will be used to describe our proposed model, KeGCNR."}, {"title": "3.1. Financial Knowledge Graph", "content": "A knowledge graph (KG) [54] is a multi-relational graph that represents knowledge as a set of entities and the relationships between them. The entities in a KG are represented by vertices (V), and the relationships between them are represented by edges (E). Each edge is labeled with a relation type from a predefined set R. Formally, a KG can be represented by a set of triples {(hi, ri, ti)}, each of which is a tuple of three elements: the head entity hi \u2208 V, the relation r\u2081 \u2208 R, and the tail entity ti e V."}, {"title": "3.2. Meta-path based Company Subgraphs", "content": "Meta-paths are commonly used in GNNs to help in learning heterogeneous information [25, 36, 12]. Here, we adopt this popular idea and use company meta-paths to represent different possible relations between companies."}, {"title": "Definition 2.", "content": "A Meta-Path[24] $A_1 \\xrightarrow{R_1} A_2 \\xrightarrow{R_2} ... \\xrightarrow{R_l} A_{l+1}$ is defined as a composite relation from R\u2081 to R1. A"}, {"title": "Definition 3.", "content": "A Multi-path Weight Matrix is constructed for each company meta-path pk. It is a matrix $W^{mp}_k$ of size $N^C \u00d7 N^C$ where $N^C$ is the number of companies in FKG. Each element (i, j) of $W^{mp}_k$ is defined as the number of possible paths between company nodes i, j in FKG that match pk."}, {"title": "Definition 4.", "content": "A Meta-path based Company Subgraph $\\mathcal{G}_{k}$ = <$\\mathcal{V}_{C},\\mathcal{E}_k$, WP> is a graph with only company nodes connected by a company meta-path $p_k$, where $\\mathcal{V}_{C}\\subseteq\\mathcal{V}$ denotes the company nodes in FKG, $\\mathcal{E}_k$ denotes edges following the pattern in $p_k$, $W^p$ is the multi-path weight matrix of $p_k$, which plays the role of edge weights in $\\mathcal{G}_{k}$."}, {"title": "4. Data Collection and Analysis", "content": "This section describes our data collection process, and provides more information on the inherent challenges of our problem."}, {"title": "4.1. Data Collection and Statistics", "content": "Since previous datasets are neither large enough nor contain company relations [4, 33, 5], we collect our data by combining relevant information from several databases on CSMAR2. More specifically, we first extract all the annual financial records of listed companies on two stock exchange markets (Shanghai and Shenzhen) during the period from 2003 to 2020. Data after 2020 might contain much noise due to the We then combine the financial statements and the stock market quotation data to extract 429 financial attributes for each company in a specific year. The financial attribute set includes all kinds of financial ratios, the annual value, the volatility of monthly value for turnover rates, and the return of stock prices, to name a few. Finally, we map RPT, and DSE records from the RPT and DSE databases to the corresponding company instance, where each RPT (DSE) record contains 12 (10) attributes.\nFollowing financial studies [55, 56], we categorize companies based on their board markets for research and comparisons. We consider three main markets which together account for the majority of listed companies, including the \"Main Board Market\" (MBM), \"Small and Medium Enterprise\" (SME) and \"Growth Enterprise Board Market\" (GEM). We then build one FKG for each market according to the procedure in Section 3. The resulting FKGs with statistics are shown in Table 1, from which several insights can be drawn."}, {"title": "4.2. Hidden Fraud Analysis", "content": "To study the impacts of hidden frauds, we collect two date values associated with each company fraud case: 1) the violation date which is the time when the fraud activity actually happens; and 2) the declared date which is when regulators declare the fraud and the detailed information to the public. We then define the year gap as the difference in the year of these two values. The distributions of year gaps in three datasets are plotted in Figure 3 (a) and the percentages of different gap ranges are shown in Figure 3 (b).\nFigure 3 shows that the problem of hidden fraud is serious, with only 30% of all the cases (in all datasets) can be detected in the same year. The average year gap in MBM is around 2 years, bigger than that of the other two datasets (see Figure 3), showing that the problem of hidden fraud is most serious in the MBM dataset. In addition, 50% of violations have gone undetected for at least 2 years in MBM and at least 1 year in SME and GEM. Note that outliers with much longer delays are"}, {"title": "5. The Base Model: Knowledge-enhanced GCN", "content": "The problem of corporate fraud detection is formalized as a binary classification that aims to assign labels y to company nodes in FKG with y = 1 being fraudulent and 0 otherwise. More specifically, we are given Gkg = (V,R,E), where the company nodes in V CV are associated with financial attributes Xatt \u2208 RNxdatt and the objective is to generate the fraud possibility vector y \u2208 R1\u00d72 for each company node v \u2208 V. The elements of y, represent for the possibility of the node being normal and fraud respectfully.\nOur base model architecture is shown in Figure 4, which consists of four phases: 1) Knowledge embedding learning: this is the pretraining phase, where KGE methods are used to learn company embeddings Xke from FKG; 2) Multi-path weighted convolution layers: company meta-paths are exploited to draw meta-path based company subgraphs {Gk|k \u2208 [1, NmP]}. For each Gk, there are two ways to initialize node representations, i.e. either Xatt or Xke. Subsequently, node representations are then learned on different subgraphs by using Multi-Path Weight Graph Convolutional Networks (MW-GCN); 3) Hierarchical attention-based fusion: Two kinds of attention are exploited to combine node representations from subgraphs, which take into account the importance of different meta-paths and different initialization strategies (Xatt or Xke); 4) Node classification layer: The fraud probability y, \u2208 R1\u00d72 (for v) is inferred by a single neural network layer."}, {"title": "5.1. Knowledge Embedding Pretraining", "content": "KGE learns to map entities and relations into continuous vector space and facilitates the task of predicting unknown triples in the knowledge graph. In other words, it aims to learn entity and relation embeddings from triples in our FKG E = {(hi, ri, t\u2081)}. Unlike GNN, which propagates and aggregates information from neighbors, KGE methods focus on the interactions of entities via pairwise relations. In general, many KGE methods can be applied in our model, such as TransE [21], DistMult [58], ComplEx [59], RotaTE [22]. For simplicity and efficiency, we choose to use TransE in this paper.\nThe main idea of TransE is to map entities and relations to the same vector space, say Rdke, so that we can connect entities h and t via the relation r using h + r \u2248 t where (h, r, t) is an observed triple (or fact) in our KG. For example, we can obtain embeddings for transaction \"T1\", the relation \"type\", and the attribute entity \"Community Trading\" so that \"T1\" + \"type\" \u2248 \u201cCommunity Trading\u201d. Formally, TransE learns a scoring function f as follows:\nf(h, r, t) = -||h + r - t||1/2\nwhere ||1/2 is either L\u2081 or L2 norm. The scoring function is larger if (h, r, t) is more likely to be a fact, i.e., h is connected to t via r in KG. Contrastive learning [60] is used to learn embeddings for all the entities and relations by enforcing the scores of true triples to be higher than those of negative (distorted) triples.\nAfter the pretraining phase, we extract embeddings Xke \u2208 RNxdke of all the company instances. The relation constraints in FKG allow TransE to distill useful semantic information from embeddings of connected nodes and relations to company embeddings."}, {"title": "5.2. Multi-path Weighted Convolution Layers", "content": "This phase aims to learn representations from company subgraphs Gk = (VC,WP,Xit), each obtained from a meta-path pk and an initialization strategy (Xatt or Xke). For simplicity, we drop k, it and show how higher representations are learned from a single graph.\nWe develop MW-GCN, a new variant of GCN [10] that takes into account the multi-path weights. Let the representations from l-th layer be H\u00b9 \u2208 RN\u00b0\u00d7d\u0131 (H\u00b0 = X), where the v-th column vector is the representation of ve Ve at l-th layer and d\u2081 is the hidden size of the l-th layer. For learning the representations of l+ 1 layer, row normalization is first conducted on Wmp:\n$\\widehat{W}^{mp} (i, j) = \\frac{W^{mp} (i, j)}{\\sum_{l=1}^n W^{mp} (i, l)}$"}, {"title": "5.3. Hierarchical Attention-based Fusion Layer", "content": "The input for this phase is a set of representations {H \u2208 RN,d} from the output layers of WM-GCN trained on Nsg = 2 \u00d7 Nmp subgraphs; where $H^{it,k} | k\\in [1,..., N_{mP}]$ and it $\\in$ {ke, att}. For simplicity, all WM-GCN are set to have the same number of layers and the same output dimension dr. The objective of this fusion step is to pool the input representations to obtain Z \u2208 RNxdz for prediction."}, {"title": "Relation Attention SubLayer.", "content": "We pool the representa-tions from different subgraphs with the same initialization strategy it, which is dropped from the notation for simplicity. Inspired from HAN [25], for each H, we leverage a readout function to get the graph embedding and the relation attention value as:\n$h_{gk} = \\frac{1}{N_C} \\sum_{i=1}^{N_c} \\overrightarrow{h}_{gk}^{i}$ \n$\\alpha^{rel}_k = \\frac{exp(\\psi (W^{rel} h_{gk} + b^{rel}))}{\\sum_{k\\prime\\in N_{mp}} exp(\\psi (W^{rel} h_{Gk\\prime} + b^{rel}))}$"}, {"title": "5.4. Node Classification Layer", "content": "Given the fusion representation Z = [Z1, ..., ZNc], where z, is the representation of v \u2208 V, a fully connected layer is used to get the probability of node v to be fraudulent:\ny = sigmoid(Wpredz, + bpred)\nwhere (Wpred, bpred) are the layer parameters."}, {"title": "6. Hidden-Fraud Robust Two-stage Learning", "content": "As we described before, hidden fraud issues are crucial in corporate fraud detection. In this section, we describe how we handle this issue during the training phase in a model-agnostic way.\nBefore introducing our method, we first formalize our problem into the framework of learning Bayes optimal distribution as follows:.\nHidden Fraud Robust Learning Problem. Considering a company node v with attributes x, and y being its (noisy) label in the dataset, we denote the true (clean) label as yv, where y = 1 corresponds to the fraudulent label. The noise associated with hidden fraud is asymmetric, i.e., only if y = 1 then \u1ef9, might be different from y. When there is a difference (y = 1 and y = 0), we say v is a hidden fraud case. Our task is to train our base model to be robust given training data with possible hidden frauds.\nLearning Bayes Optimal Distribution. A Bayes optimal label is defined as y = arg maxy P(y|xv), (xv, y) D. The Bayes optimal label can be seen as the prediction label of an optimal classifier learned from the clean distribution D. The distribution of (x, y*) is the Bayes optimal distribution, denoted by D*. Based on"}, {"title": "6.1. Training Bayes Label Transition Model", "content": "To train the transition model, we need to estimate Bayes optimal labels for company instances in our dataset. The procedure in [27, 28] is adopted to collect such labels as it has been proven effective with non-iid data likes ours. The main idea is to define a confidence regularizer to train a reference model, which is used to assign estimated optimal labels to company instances using confidence scores. During this process, some instances can be dropped if the reference model is not confident enough. Here, we train our base model with cross-entropy loss and confidence regularizer [27, 28] to obtain the reference model. Note that this model is not used in the later stage, only for collecting the Bayes optimal labels. Please refer to [27, 28] for more details."}, {"title": "6.1.2. Training Bayes Label Transition Model", "content": "After collecting the Bayes optimal labels, we attain a set of filtered examples as {(x\u00a1, \u1ef9, y)} where y denote the estimated Bayes optimal labels. Although we can directly use these samples for training a detection model that is robust to some extent [28, 29], making use of the Bayes-label transition matrix helps us better bridge the Bayes optimal distribution and the noise distribution, resulting in a more robust model [26].\nTo better model hidden fraud, which is instance and neighbor dependent noise, we leverage GCN as our transition model due to its ability to incorporate neighbor information for prediction. Specifically, we use a single layer of MW-GCN to estimate yv,\n$\\widehat{\\gamma_v} = P(\\widetilde{y}_\\nu = 0|y_{\\nu}^{\\ast}, X^{att}, \\mathcal{N}_\\nu) = MW-GCN(X^{att}, \\mathcal{G}^{sum}; \\Theta)$\nwhere @ is the parameters of MW-GCN. For simplicity, we do not introduce knowledge embedding and attention fusion at this stage, instead, we use MW-GCN on the sum-up graph Gsum, which is obtained by summing all meta-path sub-graphs adjacency matrices.\nTraining. we use the following empirical risk to train our transition model on {(xi, \u1ef9, \u0177)}:\n$L_{IDN} = \\frac{1}{m} \\sum_{i=1}^{m} log(\\widehat{\\gamma_i}) * \\I(\\widetilde{y}_{i} = 0)$\nwhere y is the predicted hidden fraud rate associated with node v, I is the indicator function, and m is the number of all filtered examples. Here, different from [26], I is used to introduce the asymmetric noise structure into learning the transition model. After training, we can use the transition model to predict \u0177; for all instances in the training dataset. Note that, the transition model is fixed, i.e., it is not trained along with the fraud detection model in the next stage."}, {"title": "6.2. Training Hidden Fraud Robust Detection Model", "content": "Given the estimated hidden fraud rate \u0177 predicted by the Bayes Label Transition Model trained in the previous subsection, we can get the estimated transition matrix \u00ce*(v) for each v in the training set V:\n$\\widehat{T} = \\begin{pmatrix} \\widehat{\\gamma_v} & 1 - \\widehat{\\gamma_v} \\\\ 0 & 1 \\end{pmatrix}$\nwhere each cell (i, j) corresponds to the transition probability P(\u1ef9, = i|y = j). The transition matrices are then fixed and used to perform forward loss correction [51] to train our model KeGCNR as follows:\n$\\mathcal{L}= -\\frac{1}{\\left|\\mathcal{V}\\right|}\\sum_{\\nu \\in \\mathcal{V}} \\widetilde{y}_{\\nu} \\log (y_\\nu \\widehat{T}^{\\*}(\\nu))$"}, {"title": "7. Experiments", "content": "This section examines the performance of KeGCNR on our collected datasets (MBM, SME, GEM). Our experiments are designed to answer the following questions:\n\u2022 Q1: Does KeGCNR outperform other possible approaches on the real-world corporate fraud detection datasets?\n\u2022 Q2: Is the knowledge embedding in KeGCNR essential in dealing with the information overload problem caused by the excessive number of support nodes?\n\u2022 Q3: Is the hidden fraud robust learning in KeGCNR effective in dealing with the hidden fraud issue compared to other label-noise robust training methods?\nGeneral Experimental Settings. We exploit AUC as the evaluation metric following the common practice in fraud detection studies [18, 17]. We randomly split each of our datasets into train/valid/test following the proportion of 6:2:2 with the additional constraint that the test set contains no hidden fraud. Specifically, we include only the non-fraud company instances labeled at least 8 years ago in the test set (see Section 4), while trying to keep train/valid/test in proportion. For data preprocessing, we do min-max normalization on the financial attributes and replace a missing value with the attribute mean. Such a process is common for all the compared methods. We run all experiments 5 times with different random seeds and report the average results."}, {"title": "7.1. Comparisons with Baselines (Q1)", "content": "Three groups of possible baseline methods are considered as follows (see Appendix for more details):\n\u2022 Traditional methods. XGBoost [62] and Deep Neural Network (DNN) are the commonly used traditional methods for fraud detection. These methods use only financial attributes without relational (graph) information."}, {"title": "7.1.2. Experimental Results", "content": "The average AUC results can be seen from table 2. We can draw several conclusions from the main results:\nOverall. our method outperforms all baselines and achieves the best AUC across all the datasets, suggesting that our model can better handle the challenges of corporate fraud detection. Particularly, our method outperforms other methods by a large margin in the MBM dataset, which is the one with the most serious hidden fraud problem (see section 4). Note also that, MBM represents the biggest and the most important market in China A-share market.\nRegarding Relational Information. GNN-based methods generally perform better than DNN, showing that relational information between companies can help improve detection results. MW-GCN is better than GCN, suggesting that despite its simplicity, our multi-path weighted matrix is effective for our problem. Overall, KeGCNR outperforms even the strongest baseline (FastGTN) by a large margin in all three datasets, thanks to its ability the handle the information overload and the hidden fraud problems. Compared to GCN, Tribe-GNN has a higher AUC score, showing that vanilla GNN is not as effective. However, Tribe-GNN's assumption about the graph structure may lead to information loss. Our approach produces better results on three datasets even without two-stage training.\nAblation Study. The ablation study results in Table 2 reveal several observations. First, both the knowledge embedding and the hidden-fraud robust learning play essential roles in our model as the performance drops without either of them (\"w/o KE\" and \"w/o robust\"), particularly on the MBM dataset. Second, financial attributes still are the most essential features for corporate fraud detection because the performance drops the most with \"w/o attr\". Third, the attention fusion layer in KeGCNR does not show its advantages on the MBM dataset. It might be because the dataset is big enough to learn good knowledge embeddings, and thus a simple summation can work well for MBM. This is supported by the fact that, in comparison with SME and GEM, the roles of KE and attributes are more balanced on MBM (the performance of \"w/o KE\" is closer to \"w/o attr\" on MBM). However, it is recommended to include the attention fusion layer as it generally performs well on SME and GEM, and comparably on MBM."}, {"title": "7.2. Impacts of Knowledge Embedding (Q2)", "content": "This section aims to shed light on the challenge caused by the excessive number of support nodes, the"}, {"title": "8. Conclusion", "content": "This paper studies the problem of corporate fraud detection. The problem has gained significant attention from regulators and investors in recent years, yet current solutions are still far from sufficient. We collect real-world datasets, which contain a large number of company instances and relations from the Chinese stock markets. Our data analysis reveals two main challenges associated with our data and problem: the information overload issue and the hidden fraud issue. We then propose a novel Knowledge-enhanced GCN model with Robust two-stage learning, KeGCNR, to systematically handle the issues. Numerous experiments show that KeGCNR is better than contemporary graph-based solutions thanks to its ability to handle the information overload issue using knowledge graph embeddings and the hidden fraud issue using our robust training method. In addition, by considering the distinct characteristics associated with hidden fraud, i.e. asymmetric noise structure and Instance and Neighbor Dependence (IND), our robust training solution is shown to perform better than recent methods for handling label noise, including those designed for graph data and instance-dependent noise.\nIn the future, more investigations are needed to improve the effectiveness of corporate fraud detection. These include but are not limited to the investigation of hidden fraud robust solutions with class imbalance consideration or the study of effective methods to handle IND noise. Our datasets, which are also the first real-world graph dataset with label noise, will be public to foster future research on these important issues 3."}, {"title": "Appendix A. Detailed experiment settings", "content": "Appendix A.1. Experimental Settings for Section 7.1\nFor all compared methods, the following settings remain the same for all of them:\nTrian/valid/test split 6:2:2\nloss function weighted cross-entropy\nhidden layer number 2\nhidden units number 1000\ninitial embedding financial attributes\nFor the homogeneous GNN model, we also leverage the sum-up company graph, which is the sum-up of all adjacency matrices of different company sub-graphs, as the input of them. For heterogeneous GNN methods, we also leverage the same meta-paths as their input. And detail setting for different baselines are as follows:\n\u2022 XGBoost [62]: XGBoost is an algorithm based on GBDT (Gradient Boosted Decision Tree), and one of the most successful methods in many data mining competitions. Here, we used the xgboost package, where lambda and gamma are set to 10 and 0.1.\n\u2022 DNN: For node classification with GCN, neural network (NN) is often used as the prediction model on top of convolution layers. To study the effectiveness of convolution layers, i.e. the role of structural information, we include a NN with 2 fully connected layers as one of our baselines. We set the learning rate as 0.0001, the activation function as the sigmoid function and the early-stopping epoch as 300.\n\u2022 GCN: GCN is the common baseline for GNN-based works, and we set the learning rate as 0.001 and the early-stopping epoch as 300 for it. All the settings are exactly the same with MW-GCN to ensure fairness."}, {"title": "\u2022 MW-GCN:", "content": "MW-GCN is our proposed method with simple modifications to the vanilla GCN. It's also an important component in our proposed methods, so we leverage it as one of our baselines. We set the learning rate as 0.001 and the early-stopping epoch as 300."}, {"title": "\u2022 DAGNN[11]:", "content": "DAGNN is a strong homogeneous GNN method that learns node representations by adaptively incorporating information from large receptive fields. It's the recent SOTA GNN methods so we use it as the baseline to better show our model's superiority. We adopt the code in github. The final parameters after tuning are K=2, learning rate=0.0001 and early-stopping epoch as 300."}, {"title": "\u2022 MHGCN[13]:", "content": "MHGCN is a recent heterogeneous graph convolutional network that effectively integrates both multi-relation structural signals and attribute semantics into the learned node embeddings. We adopt the code in github. The final parameters after tuning are learning rate=0.001, epochs=500, degree=2, and per=1."}, {"title": "\u2022 FastGTN [12]:", "content": "FastGTN is the improved version of GTN and has the ability to find new meta-paths. It achieved SOTA results on both homogeneous and heterogeneous graph benchmarks. We adopt the code in github. The final parameters after tuning are learning rate=0.0001, epochs=2000, channels number=2, K=1 and early-stopping epoch=300."}, {"title": "\u2022 CSGNN [63]:", "content": "CSGNN introduces a cost-sensitive graph neural network that employs reinforcement learning to adaptively determine an optimal sampling threshold. By conducting neighbor sampling based on node similarity, the approach effectively mitigates graph imbalance challenges. We leverage code in official github. The final parameters after tuning are learning rate=0.002, A in loss function is 0.2 and early-stopping epoch as 100.\nImplementation Details. For KeGCNR, knowledge graph pretraining was conducted by using [23], where we set the embedding dimension of 500, the learning rate of 0.25, and the maximum number of epochs of 80000. For the Bayes transition model learning stage, we set the hidden unit number as 500 for the MW-GCN layer during the hidden-fraud probability learning. For the final end-to-end node classification learning stage with loss correction, we use MW-GCN with L = 2 layers, the ReLu activation function, and the hidden layer sizes of N\u2081 = 1000. As an imbalance classification problem, we apply weighted cross-entropy loss for our methods. We tuned hyper-parameters and do early stopping according to the result on the validation set. The source code for KeGCNR can be found in GitHub.\nFor other baselines, we followed the standard practice to tune hyper-parameters on the validation set. Note that, during training, we also apply weighted cross entropy loss to handle the class imbalance issue."}, {"title": "Appendix A.2. Experimental Settings for Section 7.2", "content": "For the experiment in section 6.2, the common settings are the same as in table A.4. The implementing and setting details are as follows:\n\u2022 FastGTNfull: Categorical support node attributes are processed by one-hot encoding and then used as the initial feature of support nodes. Codes of FastGTN cannot handle nodes with attributes in different feature spaces and we add another linear layer before the FastGTN forward function to make them into the same vector space. We keep all other settings the same as FastGTN in section 6.1 for a fair comparison.\n\u2022 MW-GCNfull: We apply GraphSAGE as the sampling layer with MW-GCN as the aggregation method to the full graph. Because the model can not process heterogeneous information, we initialize support node attributes as zero vectors in the same feature space as financial attributes and consider all edge types the same. We leverage graphsage code in https://github.com/twjiang/graphSAGE-pytorch and remain the model setting of MW-GCN as the same in section 6.1 for a fair comparison."}, {"title": "Appendix A.3. Experimental Settings for Section 7.3", "content": "We replace our two-stage hidden fraud robust training (HFRT) with two related label noise-robust methods while remaining the backbone node classification method the same. And we also compare with two other noise-resistant GNNs. The detailed experimental settings of them are as follows:\n\u2022 NRGNN [20]: NRGNN is a novel method of learning noise-resistant GNNs on graphs with noisy and limited labels; It links the unlabeled nodes with labeled nodes of high feature similarity to bring more clean label information. We adopt the"}]}