{"title": "A Statistical Case Against Empirical Human-AI Alignment", "authors": ["Julian Rodemann", "Esteban Garc\u00e9s Arias", "Christoph Luther", "Christoph Jansen", "Thomas Augustin"], "abstract": "Empirical human-AI alignment aims to make AI systems act in line with observed human behavior. While noble in its goals, we argue that empirical alignment can inadvertently introduce statistical biases that warrant caution. This position paper thus advocates against naive empirical alignment, offering prescriptive alignment and a posteriori empirical alignment as alternatives. We substantiate our principled argument by tangible examples like human-centric decoding of language models.", "sections": [{"title": "1. Introduction", "content": "Aligning artificial intelligence (AI) with human goals has shifted from abstract ethics to urgent policy agendas. Regulators see safety and harm prevention as prerequisites for AI's widespread deployment, not optional virtues, see e.g., Chatila & Havens (2019).\nAgainst this background, it comes as no surprise that the challenges of human\u2013AI alignment have sparked a lot of interest in the machine learning (ML) research community. In 2024 alone, more than 1500 papers with alignment-related keywords were uploaded to arxiv, see Fig. 1. Analyses of github and Hugging Face uploads show similar trends. An alignment benchmark study (Kirk et al., 2024) won a best paper award at NeurIPS.\nThe position paper track at last year's ICML alone saw at least four papers (Sorensen et al., 2024; Lindauer et al., 2024; Conitzer et al., 2024; Yang et al., 2024) explicitly calling for more, better, or more nuanced human-AI alignment. Lindauer et al. (2024) advocate for a more human-centered approach to automating ML pipelines. Both Sorensen et al. (2024); Conitzer et al. (2024) emphasize the need to align AI systems with diverse and potentially conflicting human interests. Yang et al. (2024) move beyond a mere human-centered perspective: They recognize the need to adapt AI to environments and self-constraints in addition to human intentions. For instance, agents have to be aligned with environmental dynamics in order to understand whether the next actions could violate human preferences learned in the first place. Exclusively aligning with human intentions seems too strong of a restriction even for adhering to the very same intentions in the real world.\nPartly inspired by Yang et al. (2024), this interdisciplinary position paper at hand argues that empirical alignment's nobility of purpose can disguise statistical flaws. We are convinced that the pursuit of aligning Al with human preferences through observing the latter may inadvertently introduce biases and limitations into these systems. Unlike Yang et al. (2024), we specifically focus on forward (a priori) alignment of ML systems in an empirical way. That is, we make the case against aligning AI through observing human preferences before deployment, see definitions 2.1 through 2.3 and Section 3. Constructively, we offer three alternatives: Empirical backward (rather than forward) alignment (Section 5) as well as prescriptive (rather than empirical) alignment, both forward (Section 4) and backward (Section 6), see Table 1.\nOur stance relies on a statistical perspective, as empirical alignment hinges on a myriad of (implicit and often ignored) statistical assumptions like representative samples, absence of confounders and well-defined populations. We identify several statistical biases as empirical alignment's Achilles heel, see Section 3.3. Effectively, they skew the alignment goals away from the \u201cpurpose which we really desire\" to a \u201ccolorful imitation of it\" (Wiener (1960), page 1355, see above).\nUnlike backward empirical alignment, which aligns AI after deployment, forward empirical alignment directly leads to these alignment-caused biases being encoded in the AI models. This makes it hard to disentangle them from the originally trained model. Thus, we contend that AI should be \u2013 wherever possible - explained and altered a posteriori rather than empirically aligned a priori. Section 5 compares the merits of backward empirical alignment to the perils of forward empirical alignment in greater detail.\nEmpirical-alignment-induced biases are not only counterproductive for solving novel tasks in non-human-centered environments (Yang et al., 2024). More dramatically, these biases restrict an AI systems' potential for scientific discovery by confining them to human-centric perspectives. They hinder the understanding of broader and potentially more important phenomena that lie beyond human interests and perception. Such anthropocentric biases are not unique to AI alignment. In Section 3.1, we draw parallels to the rich literature on anthropocentrism, eventually relating alignment-induced biases to the observation selection effect in physics, see Section 3.2. We learn that AI systems will miss crucial parts of the universe"}, {"title": "2. Human\u2013AI Alignment", "content": "We do not argue against human\u2013AI alignment per se, but rather focus on what we perceive as a popular (see Fig. 1) yet often unquestioned approach to alignment in the ML community: forward and empirical human-AI alignment. We approach this notion by first defining alignment generally.\nDefinition 2.1 (Human\u2013AI Alignment). Consider any ML model identified with parameters $\\theta\\in \\Theta$. Denote by $w \\in \\Omega$ an alignment target. Define human-AI alignment as any process involving changes of $\\theta$ that considers a function of the form $\\Theta \\times \\Omega \\rightarrow A; (\\theta, \\omega) \\leftrightarrow ||f(\\theta) - g(w)||$, where $f: \\Theta \\rightarrow A$ and $g: \\Omega \\rightarrow A$ map to some alignment space $A$ equipped with some notion of distance $||. ||$. For instance, this process can be $\\min_{\\theta \\in \\Theta} || f(\\theta) - g(w)||$.\nIn line with Ji et al. (2023), we distinguish between forward and backward human\u2013AI alignment. Forward alignment refers to harmonizing AI systems with human values already during the training phase and before deployment. In other words, $\\theta$ is altered informed by $(\\theta, \\omega) \\leftrightarrow ||f(\\theta)-g(w)||$ before the test phase. The approach is more proactive, embedding alignment into learning itself.\nBackward human-AI alignment, in contrast, changes the parameters $\\theta$ of a trained model during or after deployment. It includes safety evaluations, governance frameworks, and interpretability methods, see Section 5. This approach is rather reactive, focusing on monitoring, adjusting, and managing AI behavior as new risks or misalignments emerge during deployment. In addition to the forward-backward distinction by Ji et al. (2023), we define prescriptive human-AI alignment and empirical alignment as follows, giving rise to our fourfold distinction in Table 1.\nDefinition 2.2 (Prescriptive Human\u2013AI Alignment). If $w \\in \\Omega$ result from pre-defined axioms, the process in definition 2.1 shall be called prescriptive human-AI alignment.\nDefinition 2.3 (Empirical Human\u2013AI Alignment). If $w \\in \\Omega$ are observed human behaviors, the process in definition 2.1 shall be called empirical human-AI alignment.\nWe emphasize the abstract nature of this fourfold dis"}, {"title": "3. Contra Forward Empirical Alignment", "content": "As this paper cautions against anthropocentric biases arising from forward empirical alignment, we turn to this kind of alignment in more detail. We first emphasize the philosophical underpinnings of our argument in Section 3.1, and then \u2013 motivated by the anthropic principle in physics (Section 3.2) \u2013 discuss concrete statistical biases in Section 3.3."}, {"title": "3.1. The Anthropocentric Bias", "content": "Anthropocentric thinking, which places humans at the center of the universe, has deep roots in Western thought. In the Judaeo-Christian tradition, the anthropocentric perspective originates from the creation narrative, where humans are depicted as the pinnacle of creation (Genesis 1:27). Historically, this has heavily influenced science.\nHowever, the anthropocentric interpretation of Genesis 1:27 faced staunch opposition already from within the Jewish tradition itself. One notable critic was Moses Maimonides, a preeminent Torah scholar of the twelfth century AD (Lamm, 1965; Shapiro, 2003). In his seminal work, The Guide for the Perplexed, Maimonides emphasized the vastness and complexity of the universe, which, in his view, diminished the centrality of humans. He famously referred to humans as \"just a drop in the bucket,\" evoking Isaiah (40:15), see Dan (1989). Notably, Maimonides' opposition to anthropocentrism was a theological one. He believed that attributing undue importance to humanity was arrogant and a misinterpretation of divine intent. For Maimonides, recognizing the humility of human existence was essential for a proper understanding of God and the natural order.\nIt was more than three centuries later when Nicolaus Copernicus, Galileo Galilei, and Isaac Newton established a new heliocentric model of the universe, literally shifting the center of the universe away from humans. This paradigm shift (Kuhn, 1997) paved the way for the scientific revolution in the 16th and 17th centuries. Yet, anthropocentric biases still persist in science to this day. The argument is plain and simple: By focusing on explaining phenomena that are useful for humans, we inadvertently miss structures that are not perceived as useful according to current societal values, which are strongly limited in time and generality. In the words of Peters (2012), \"Nature has much more structure than what is useful for humans.\" Strong positions within evolutionary epistemology even argue we cannot perceive anything that is not useful, since our cognitive apparatus is a product of evolution, thus overfitted to useful elements of nature (Lorenz, 1977; Wuketits, 1990).\nThe example of Maimonides shows that even within an overarching anthropocentric dogma, there is room for critical reflection and nuanced positions. This motivates our stance on AI alignment. We are well aware of the pressing need to align AI with human safety constraints in the wake of ever more powerful models and in anticipation of AGI, see also Section 7. However, we assert that empirical alignment is the wrong path towards that goal. Just like the Torah scholar Maimonides, who firmly believed in the dogma of his times, we do not oppose the paradigm of alignment generally. Quite the contrary, for alignment to work sustainably, we argue, it has to be freed from anthropocentric biases originating from forward empirical alignment."}, {"title": "3.2. The Anthropic Principle in Physics", "content": "Modern-day physics is aware of such anthropocentric biases and \u2013 going even further \u2013 the more general anthropic principle, also referred to as \"observation selection effect\" (Bostrom, 2000; 2002; Carter, 1983). It states that all possible observations of the universe are limited by the fact that they can only be made in a universe capable of developing intelligent life, and is commonly attributed to Robert Dicke (Dicke, 1961; 1957), building on work by Paul Dirac. For example, constants of nature like the electron charge appear fine-tuned for life because, if they were not, we would not be around to observe them.\nAccepting this natural restriction to human reasoning, Bostrom (2002) postulates the self-sampling assumption, which we can directly relate to empirical alignment. It states: \u201cOne should reason as if one were a random sample from the set of all observers in one's reference class\u201d (Bostrom (2000), page 57). The self-sampling assumption reveals an unresolvable dilemma of empirical alignment. If we define the reference class broad enough to capture all environments relevant to AI deployment, empirical alignment will violate the self-sampling assumption, since the sample is biased towards humans. On the other hand, if the reference class is narrowly restricted to human-related objects, the model can hardly generalize beyond these objectives, see e.g.,Yang et al. (2024).\nIndeed, it was recognized by Schmidhuber (2000; 2002) that the anthropic principle provides little insight when the thought experiment is restricted to only one universe. A meaningful theory requires informative priors or alternative universes, see approaches by Schmidhuber (2000; 2002); Bostrom (2002). These are all pre-defined axioms, corresponding to a prescriptive approach.\nTransferred to empirical human-AI alignment, this insight implies that we have to at least enrich empirical alignment by axiomatic assumptions on the alignment procedure. In Popper's image of empirical science rising \"above a swamp\u201d, see Section 1, these assumptions are the \"piles [...] driven down from above into the swamp\" (Popper, 2002/1959, page 93-94). Blindly relying on a sample without additional assumptions on the population where it is drawn from will always lead to biased conclusions abound non-human entities. The heart of the matter is that making no population-related assumptions at all implicitly corresponds to making the strongest assumption of all \u2013 the sample being fully sufficient for the alignment goal. The anthropic literature even teaches us that we have to explicitly take sample selection probabilities into account, requiring a statistical perspective. We thus need to consult the statistical literature on causal and selective inference as well as sampling theory. The following Section does the job."}, {"title": "3.3. The Statistical Perspective", "content": "Indeed, the limits of empirical alignment can be best understood from a statistical perspective: Empirical alignment intends to harmonize AI with the intentions of some population of (potential) human agents, constituting $\\Omega$ in Definition 2.1. This is commonly done by means of a self-selected, thus as we learned from the anthropic principle in physics \u2013 distorted sample thereof. This distortion can become manifest in a myriad of statistical biases, which particularly have been discussed in the social and survey statistics literature.\nA sole focus on empirical evidence denies the problem of adequation (Menges, 1982; Grohmann, 2000), which is concerned with the \u2013 eo ipso insufficient \u2013 fit between what is, in principle, observable, describable, and analyzable within the framework of our formalization process, and the \u201cworld-in-itself\u201d. Groves & Lyberg (2010) concretise some major biases arising from this discrepancy by their TSE-(Total Survey Error) concept. They distinguish between what we call population representation biases and structural representation biases. The first one comprises, e.g., biases in the selection frame. That is, the population from which the sample is taken differs from the population of interest. Other instances of the population representation biases are the classical sampling error arising from a random selection of the sample from the underlying population and the unit-nonresponses arising when certain units refuse, or are incapable of, participating in the survey. The latter can distort conclusions because non-respondents' characteristics often differ systematically from those who participate.\nThe structural representation biases refer to the content-related part of the analysis and the resulting incomplete reflection of the complex relationships between the true concepts and the set of variables available for analysis. This includes biases of operationalizing complex latent constructs and the item-nonresponse bias, arising when individuals ready to participate in principle refuse to answer certain sensitive questions. Response biases refer to biased data obtained from those that do respond, comprising acquiescence bias (a tendency to agree) or primacy and recency effects (a preference for selecting the first or last item in a sequence), see Kaufmann et al. (2023). A further source of structural representation bias, not explicitly elaborated in the TSE framework, is the omitted variable bias. It refers to situations when decisive influence factors (such as hidden personal characteristics or genetic dispositions) are not accessible to the researcher, for example, for reasons of privacy preservation. This can spuriously enlarge the effect of global variables like sex or age in the merely empirical data-based analysis. Note that all the biases / errors listed here are big data biases in the sense that they do not vanish with increasing sample size. They are present to their full extent, also in big data sets. The only exception is the usual statistical uncertainty induced by the sample error.\nBeyond the TSE, there is a bulk of statistical literature on what Benjamini (2020) calls \u201cthe silent killer of replicability\": Selective inference. It occurs when hypotheses, models, or features are selected based on the observed data. Such post-selection inference biases arise in numerous scenarios, like variable selection in regression models, multiple hypothesis testing, and adaptive stopping rules in ML (Benjamini & Hochberg, 1995; Wasserman, 2009; Fithian et al., 2014; Tibshirani & Taylor, 2016; Lee et al., 2016). Empirical alignment is prone to suffer from selective inference, if the alignment targets w are functions of the same observations being used for finding $\\theta \\in \\Theta$ through empirical risk minimization, see definition 2.1, as is often the case. If data is sampled anew from a different population for alignment, such trouble can be avoided. In that case, however, the procedure becomes very data hungry. This can pose practical challenges, as human samples are typically burdensome and expensive to acquire (Shinn et al., 2024). For an illustrative example of selective inference that arises when empirically aligning language models, we refer the interested reader to experimental results in Appendix C.\nReflexivity constitutes an even more fundamental problem in empirical alignment. In addition to the observation selection effect \u2013 which persists in any empirical science, see Section 3.2 \u2013, empirical human-AI alignment suffers from a fundamental problem in the social sciences: The observant entity (human) coincides with the observing entity (also human). Unlike for atoms (natural sciences), we can expect humans (social sciences) to react to conclusions (in our case, Al alignment) being drawn from observing them, thereby compromising the validity of those conclusions. This \u201creflexivity problem\" (Soros, 2015) dates back to early work of Morgenstern (1928) and has recently seen some revival in the ML community, recognizing the fact that today's predictions can change tomorrow's population (Perdomo et al., 2020; Hardt & Mendler-D\u00fcnner, 2023; Mendler-D\u00fcnner et al., 2020; Miller et al., 2021). In empirical alignment, consider the popular example of fine-tuning LLMs by RLHF, which implicitly relies on the \u201cunrealistic\" (Carroll et al., 2024) assumption that human preferences regarding LLM answer quality is static. To say the least, it is certainly plausible that previous LLM answers affect our judgments through, e.g., anchor effects (Lieder et al., 2018). Very recently, Carroll et al. (2024) took steps to address such feedback loops by drafting AI alignment as a dynamic Markov Decision Process (MDP), taking into account that our preferences can change by interacting with changing Al systems. Mitelut et al. (2023) go further and argue entirely against alignment to human intent, since Al systems can reshape the latter, see also Gabriel (2020).\nCausal misrepresentation refers to misleading associations that do not reflect causal connection between variables. Such correlations occur in observational data, on which empirical alignment mostly still hinges on with the notable exception of experimental methods in the assurance literature, see Ji et al. (2023, \u00a74.3).\nStatistical science offers sound remedies for many of the above mentioned biases. They typically involve additional assumptions about the population like \"absence of unobserved confounders.\u201d Essentially, these assumptions are prescriptive elements, which we advocate for. Even if they are self-evident as in the case of confounders, we strongly encourage making them explicit.\nThis increases transparency not only for humans but also for AI, thus addressing potential unintended misalignments due to miscommunication. If AI systems fail to understand that the actual goal is population-alignment rather than sample-alignment, consequences can be dire, see Bostrom (2014, pages 122-\""}, {"title": "4. Pro Forward Prescriptive Alignment", "content": "The theory of preference(s) (relations) originates in modeling rational agents within the foundations of decision theory. It finds ever broader areas of application in modern ML (e.g., F\u00fcrnkranz & H\u00fcllermeier (2003); Jansen et al. (2023a;c); Bengs et al. (2021); Jansen et al. (2024); Rodemann et al. (2023a;b; 2025); Dietrich et al. (2024); Kim & Oh (2024)). In particular, the subfields of preference elicitation (e.g., Ha & Haddaway (1999); Baarslag & Gerding (2015); Mukherjee et al. (2024)) and preference aggregation (e.g., Eugster et al. (2012); Mersmann et al. (2015); Jansen et al. (2018); Zhang & Hardt (2024)) play decisive roles here. In what follows, we support our case pro forward prescriptive alignment by findings from these two subfields.\nThe term preference elicitation refers to the systematic retrieval of initially unknown (human) preferences, often achieved by successively (and adaptively) presenting queries to the agent under consideration. It is an excellent illustration of the superiority of a prescriptive understanding of alignment (using pre-defined axioms as alignment targets $w \\in \\Omega$, see Definition 2.2) over a purely empirical understanding of alignment (Definition 2.3): It is easy to find pre-defined axioms on the nature of preference structures modeling rational behavior that meet broad social consensus. - and it is precisely these supposedly rational axioms that are often violated in empirical studies. These violations, however, are often due to lacking oversight rather than being conscious and intended choices of the agent. At the same time, integrating rationality axioms in elicitation strategies is an instance of forward prescriptive alignment: The axioms are agreed upon before the model is deployed."}, {"title": "5. Pro Backward Empirical Alignment", "content": "Another argument versus forward empirical alignment stems from the pursuit of transparency. While most ML models are powerful predictors their decision making process typically is not human intelligible \u2013 with the exception of intrinsically interpretable models. Transparency, however, is desired to understand model decisions especially in high-stakes environments \u2013 let alone legal requirements like the EU AI Act.\nWe argue that forward empirical alignment adds to the opacity of models. In RLHF, for example, the reward function is typically approximated by a deep neural network trained on observed human behavior (Christiano et al., 2023), and hence the (often non-transparent) policy is optimized using feedback from a nontransparent reward model. More importantly, though, any forward empirical alignment encodes biases into the model that cannot be disentangled from dependencies in the training data as discussed in Section 3.3, which further impedes model understanding. Thus, we argue for more transparent alignment. Prescriptive alignment techniques check this box by explicitly stating the axioms a model shall be aligned to. If the latter is impractical, however, we offer changes of an ML model's parameters $\\theta \\varepsilon \\theta$ informed by interpretable ML (IML) as potential strategy. In line with Ji et al. (2023), we call this backward alignment. Modifications to the model architecture, by contrast, are another valid option but per definition not part of the alignment process of the original model.\nThere is a vast literature on IML methods for classical ML (e.g., Molnar (2022); Ribeiro et al. (2016); Lundberg & Lee (2017); Greenwell et al. (2018); Covert et al. (2020)) as well as those dedicated to explain decision making in RL (e.g., Puiutta & Veith (2020); Milani et al. (2022); Madumal et al. (2019); Topin & Veloso (2019); Olson et al. (2021); Huang et al. (2018)).\nSuch methods typically provide explanations of ML models during or after deployment that are subsequently inspected by human observers. Upon inspection, model explanations can a posteriori assure that the model works as intended. On the contrary, the observer can infer that the model behaves against their intentions and imply changes on the parameters $\\theta \\in \\Theta$ without ever explicitly stating their preferences and while never providing universally applicable axioms. Thus, we merely observe human preferences implicitly and call the latter scenario empirical backward alignment. Note that \u2013 while inherently prescriptive \u2013 the choice of the (I)ML method is not yet part of the alignment process. As we argue for transparent alignment, we contend that such a posteriori informed backward alignment achieves this due to the more explicit nature of the performed changes unlike forward empirical alignment. Moreover, the strategy comprises a rather static choice situation which can mitigate the issue of irrational behavior in empirical alignment raised in Section 4."}, {"title": "6. Pro Backward Prescriptive Alignment", "content": "An illustrative example of backward alignment in language models emerges in the choice of decoding strategies for autoregressive text generation. These strategies specify how each subsequent token is selected from the model's probability distribution over tokens at every inference step, thereby exerting a critical influence on the quality of the generated text.\nVarious automatic metrics have been introduced to evaluate text generated by different decoding strategies. Among them are several prescriptive ones like coherence, diversity or QText (harmonic mean of these latter), and empirical ones such as MAUVE (Pillutla et al., 2021). For a detailed overview and technical description, we refer to Appendix A.1 and A.2. While coherence assesses how likely the generated text is given the prompt, diversity measures lexical repetition rates (Su & Xu, 2022). In contrast, MAUVE measures how closely machine-generated text aligns with empirical samples of human-written text by comparing their distributional \u201cfingerprints\u201d in a latent representation space. Technically, it calculates the Kullback-Leibler divergence between the two distributions. Higher MAUVE scores correspond to lower divergence, indicating greater similarity to human-produced text.\nChoosing a decoding strategy to align the model output with human-generated text is an obvious example of backward alignment: Decoding strategies are chosen after the language model has been trained, allowing for fine-tuning towards human preferences. Relying exclusively on MAUVE to select or evaluate decoding strategies constitutes backward empirical alignment: the quality of a strategy is evaluated based on how well its text generations align with observed human-written text samples. This methodology has notable limitations, as our illustrative case study reveals: Based on experimental results in Garc\u00e9s et al. (2024b; 2025), we compare the quality (assessed by human evaluators) of text generated by the same model (GPT2-XL (Radford et al., 2019)) using two different decoding strategies, namely CS (contrastive search, Su & Xu (2022); Su & Collier (2023)) and DoubleExp (Garc\u00e9s et al., 2024b). In one scenario, the decoding strategy is empirically aligned via MAUVE. In the other scenario, we use QText, a prescriptive metric, to choose the decoding method."}, {"title": "7. Alternative Views", "content": "In this Section, we present two alternative views \u2013 pro forward empirical alignment and contra alignment altogether."}, {"title": "7.1. Pro Forward Empirical Alignment", "content": "While statistical caution is appropriate, there might be other \u2013 potentially superior \u2013 reasons to do align Al with human preferences in a forward empirical way. Often, ML's main and only goal is good prediction. Strong cases for empirical alignment are especially concerned with such \"performance\" goals (see Ibarz et al. (2018)). A prime example is InstructGPT (Ouyang et al., 2022b), an LLM trained with a \"technique [that] can align to a specific human reference group for a specific application\" (p. 18). The authors name improved performance and cost-effectiveness over larger models as key benefits of their approach. Stiennon et al. (2020) report similar benefits of smaller, empirically forward aligned models for the clearly defined task of text summarization. Potentially reduced model size while maintaining predictive performance is further desirable for the widespread deployment of ML models on less powerful hardware. We refer to Appendix D for a similar argument in defense of forward empirical alignment from a benchmarking perspective."}, {"title": "7.2. A Case Against Alignment Altogether", "content": "With the very same principled arguments from Section 3, one might as well arrive at the reasonable position of giving up alignment entirely. Our offered alternatives (backward and/or prescriptive alignment) increase transparency and decrease negative effects of statistical biases. Yet, they cannot resolve the fundamental dilemma of the observation selection effect: As detailed in Section 3.1, any alignment to humans will necessarily bias the AI away from other entities in nature. Fundamentally opposing alignment might thus be particularly justified if the AI's goal is scientific inference, e.g., by means of IML. While we considered explicit changes to a model informed by IML as backward alignment, the use of IML can end at the stage of model explanation as a tool for scientific inference (Freiesleben et al., 2024; Ewald et al., 2024; Molnar et al., 2023; K\u00f6nig et al., 2024) \u2013 typically about the data generating distribution. From a statistician's perspective, we deem such inference as valuable in itself and further argue that principled explanations aid in understanding model decisions. Covert et al. (2020) introduce SAGE values for so called global feature importance that \u2013 under certain conditions - represent mutual information between inputs and output or conditional output variance, K\u00f6nig et al. (2024) extend similar insights to feature dependencies and Freiesleben et al. (2024) define a general framework to design and use IML methods for scientific inference grounded in statistical learning theory.\nWe contend that a bias from any form of alignment encoded in the ML model can confound the relations between the model variables and as a result diminish the potential for scientific discovery. This argument is further underlined by findings from RL. Silver et al. (2017) show that the famous AlphaGo model trained from observations of human play is inferior to AlphaGo Zero trained without human knowledge. Moreover, Schut et al. (2023) use concept-based explanations to extract sequences of actions in chess from an Al trained without human oversight that go beyond human skill level. It can be argued that not only forward empirical, but any kind of alignment \u2013 in the language of Schut et al. (2023) \u2013 biases the machine representational space towards the human representational space. It introduces or exacerbates an anthropocentric bias that can substantially reduce Al's ability to learn concepts beyond human knowledge."}, {"title": "8. Conclusion", "content": "AI alignment is a double-edged sword. Done well, it makes AI safer. Done poorly, it biases models and limits their potential for discovery. This paper developed a nuanced statistical perspective on this tradeoff. It cautioned against forward empirical alignment unless strictly necessary.\nWe emphasized the statistical caveats of common empirical alignment practices (Section 3) and scrutinized (in)consistencies of observed human preferences (Section 4). We showed that forward empirical alignment is especially prone to \u201clocking in\" statistical biases during training. What is more, we constructively discussed alternatives like prescriptive (Section 4 and 6) and backward (Section 5 and 6) alignment. We further provided practical guidance by concrete examples like decoding of language models (Section 6)."}, {"title": "9. Impact Statement", "content": "This paper warns against the uncritical and naive use of forward empirical alignment, arguing that such alignment introduces statistical biases and anthropocentric constraints. We propose alternatives: prescriptive alignment and backward adjustments. These latter ensure transparency and prevent AI from merely imitating (potentially irrational and inconsistent) human behavior at the cost of better reasoning. This misalignment can lead to significant societal risks, as touched upon in Section 3.3.\nFuture Al policy should thus consider the biases discussed in this position paper. Regulation should not blindly enforce empirical alignment, but instead, demand transparency in how models are aligned. This means documenting alignment assumptions, ensuring explainability, and regularly auditing statistical biases in training data.\nThe ethical stakes are high. If AI is aligned to flawed human preferences, it may amplify societal biases rather than correct them.\nAt worst, it could reinforce harmful power structures, see the example of racial biases in Section 3.3. Conversely, dismissing empirical alignment entirely might create models that fail to serve human needs. We advocate a middle ground: rigorous, principle-based alignment that minimizes bias while allowing AI to generalize beyond human-imposed limits.\nParticularly, by pushing for a middle ground, we object to the misinterpretation of this paper's arguments as a call against alignment altogether. We discussed this as an alternative view in Section 7. Abandoning alignment entirely poses the severe and existential risk of loosing human control over AI systems, potentially giving rise to several \u201cdoom\u201d scenarios, see also Section 3.1."}, {"title": "A. Metrics for Evaluating Decoding of Language Models", "content": "In what follows, we discuss the prescriptive and empirical metrics used for language model alignment via decoding strategy selection in Section 6."}, {"title": "A.1. Prescriptive Metrics", "content": "Coherence. Proposed by Su & Xu (2022), the coherence metric is defined as the averaged log-likelihood of the generated text conditioned on the prefix text as\n$\\text{Coherence}(x, \\hat{x}) = \\frac{1}{|\\hat{x}|} \\sum_{i=1}^{|\\hat{x}|} \\log P_M(p_i | [x : <i])$ where $x$ and $\\hat{x}$ are the prefix text and the generated text, respectively; $[:]$ is the concatenation operation and $M$ is an external language model, namely OPT (2.7B) (Zhang et al., 2022).\nDiversity. Proposed by Su & Xu (2022), the diversity metric aggregates n-gram repetition rates:\n$\\text{DIV} = \\prod_{n=2}^{4} \\frac{|\\text{unique n-grams} (\\hat{x}_{cont}) |}{|\\text{total n-grams} (\\hat{x}_{cont}) |}$ A low diversity score suggests the model suffers from repetition, and a high diversity score means the model-generated text is lexically diverse.\nQText. QText (Garc\u00e9s et al., 2025) is given by the harmonic mean of rescaled coherence and diversity:\n$QText = \\frac{2}{\\frac{1}{COH} + \\frac{1}{DIV}} * 100,$ where $COH = \\frac{\\text{Coherence} \u2013 \\min(\\text{Coherence}) + 1}{\\max(\\text{Coherence}) \u2013 \\min(\\text{Coherence}) + 1}$ QText values close to 100 indicate high-quality text generation, while values approaching zero reflect low-quality outcomes."}, {"title": "A.2. Empirical Metrics", "content": "MAUVE. MAUVE (Pillutla et al., 2021) is a metric designed to quantify how closely a model distribution $Q$ matches a target distribution $P$ of human texts. Two main types of error contribute to any discrepancy between $Q$ and $P$:\n\u2022 Type I Error: $Q$ assigns high probability to text that is unlikely under $P$.\n\u2022 Type II Error: $Q$ fails to generate text that is plausible under $P$.\nThese errors can be formalized using the Kullback\u2013Leibler (KL) divergences $KL(Q || P)$ and $KL(P || Q)$. If $P$ and $Q$ do not share the same support, at least one of these KL divergences will be infinite. To address this issue, Pillutla et al. (2021) propose measuring errors through a mixture distribution\n$R_{\\lambda} = \\lambda P + (1 \u2212 \\lambda ) Q \\text{ with } \\lambda \\in (0, 1).$"}, {"title": "C. Selective Inference in Decoding Alignment of Language Models", "content": "In this section, we discuss a tangible and concrete example of selective inference (Section 3.3) arising in empirical alignment of decoding methods in language models.\nWhen empirically aligning decoding methods via MAUVE, see also section 6, only machine-generated samples that match the distributional length"}]}