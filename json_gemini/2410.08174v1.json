{"title": "SAMPLE THEN IDENTIFY: A GENERAL FRAMEWORK FOR RISK CONTROL AND ASSESSMENT IN MULTIMODAL LARGE LANGUAGE MODELS", "authors": ["Qingni Wang", "Tiantian Geng", "Zhiyuan Wang", "Teng Wang", "Bo Fu", "Feng Zheng"], "abstract": "Multimodal Large Language Models (MLLMs) exhibit promising advancements across various tasks, yet they still encounter significant trustworthiness issues. Prior studies apply Split Conformal Prediction (SCP) in language modeling to construct prediction sets with statistical guarantees. However, these methods typically rely on internal model logits or are restricted to multiple-choice settings, which hampers their generalizability and adaptability in dynamic, open-ended environments. In this paper, we introduce TRON, a two-step framework for risk control and assessment, applicable to any MLLM that supports sampling in both open-ended and closed-ended scenarios. TRON comprises two main components: (1) a novel conformal score to sample response sets of minimum size, and (2) a nonconformity score to identify high-quality responses based on self-consistency theory, controlling the error rates by two specific risk levels. Furthermore, we investigate semantic redundancy in prediction sets within open-ended contexts for the first time, leading to a promising evaluation metric for MLLMs based on average set size. Our comprehensive experiments across four Video Question-Answering (VideoQA) datasets utilizing eight MLLMs show that TRON achieves desired error rates bounded by two user-specified risk levels. Additionally, deduplicated prediction sets maintain adaptiveness while being more efficient and stable for risk assessment under different risk levels.", "sections": [{"title": "Introduction", "content": "In recent years, there has been swift progress in the development of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). MLLMs extend the capabilities of LLMs by integrating and processing information from multiple modalities, including text, vision, and audio. However, MLLMs are proven to exhibit significant drawbacks in trustworthiness, such as hallucination , which results in non-factual information and biased generations. These issues have elicited increasing societal concerns regarding the reliable deployment of foundation models in consumer-facing applications.\nUncertainty estimation provides valuable insights into the trustworthiness of model generations. Prior work performs probabilistic modeling, develops entropy-based measures, and generates multiple responses to analyze the output space. However, these approaches cannot provide guarantees of the error rate, and forcing a measure to predict a single data point is overly restrictive. Split Conformal Prediction (SCP) is a promising candidate to tackle these challenges above, which constructs statistically rigorous prediction sets with the hallmark of risk control. Furthermore, SCP allows the calibrated set size to vary based on the model's uncertainty regarding a particular input.\nSCP has been successfully applied in language modeling. However, existing methods either (1) modify the original model outputs to ensure factuality with desired probability, potentially leading to uninformative and vague generations, (2) rely on internal token sequence logits or (3) are restricted to multiple-choice settings. Adapting SCP for proprietary MLLMs in practical open-ended video question-answering (VideoQA) applications presents several challenges: (a) Adaptability. Unlike closed-ended tasks equipped with fixed options covering the correct answers, the output space for each VideoQA task is unbounded, and there may not be an acceptable response within the sampled generations; (b) Reliability. Logits or verbalized confidence levels are often miscalibrated, leading to biased prediction sets; (c) Flexibility. For some API-only MLLMs, users lack access to internal model information like logits.\nIn this paper, we aim to tackle these challenges by developing a two-step framework for risk control and assessment (TRON). We illustrate the pipeline of TRON via two open-ended VideoQA samples in Figure 1. The workflow begins by determining whether acceptable responses exist in the sampled generations, akin to options in closed-ended tasks. To this end, we propose a novel conformal score that controls the minimum number of response samples required for each calibration data point. For a new data point, we define the number of samples as the approximate 1 - \\alpha quantile of all conformal scores in the calibration set, denoted as r. Then, the error rate for the candidate set of each test data point failing to encompass an acceptable response is controlled by the risk level \\alpha (i.e., < \\alpha).\nOnce the risk level for acceptable responses being sampled is specified, we evaluate the reliability of each response. Inspired by self-consistency theory, we propose using the frequency of each response as a proxy for confidence\u00b2, compensating for the limitations of internal logits information and verbalized confidence scores, and define the nonconformity score of each calibration data as one minus the frequency score of any one acceptable response in the candidate set. Then, we specify another risk level \\beta and calculate the approximate 1 - \\beta quantile of all nonconformity scores, denoted as s. The quantile serves as a threshold to identify high-quality responses within the candidate set of each test data point. Finally, we guarantee that the"}, {"title": "Preliminaries", "content": "Following standard SPC for risk control , we assume a calibration dataset D_{cal} = \\{(X_i, Y_i)\\}_{i=1}^N of N query-answer pairs and an independent and identically distributed (i.i.d.)\u00b3 test point (X_{test}, Y_{test}). Our goal of risk control is to construct prediction sets that are calibrated to bound the correctness miscoverage rate\nP\\{Y_{test} \\in C(X_{test})\\} \\leq \\epsilon,  (1)\nwhere \\epsilon is the user-specified risk level, and C is a function formed utilizing D_{cal}. Note that the risk level is marginal (average) over the draw of both the calibration and test data points.\nFor each query of calibration data X_i, we generate M_i response samples \\{y_{m}^{(i)}\\}_{m=1}^{M_i} and demand that there is at least one correct response in the candidate set (i.e., Y_i \\in \\{y_{m}^{(i)}\\}_{m=1}^{M_i}). For the test point, we also generate M_{test} response samples \\{y_{m}^{(test)}\\}_{m=1}^{M_{test}}. Note that we are currently unsure if the candidate set of test data encompasses acceptable responses."}, {"title": "Conformal Risk Control", "content": "As we construct a prediction set by selecting high-quality responses from the candidate set, we first perform a simple calibration step for the setting of M_{test} to control the risk of \\{y_{m}^{(test)}\\}_{m=1}^{M_{test}} failing to encompass acceptable responses, so that we can approximate closed-ended settings with a specific probability. We develop a novel conformal score for each calibration data point, which determines the minimum sampling threshold that ensures Y_i \\in \\{y_{m}^{(i)}\\}_{m=1}^{M_i}\nr(X_i, Y_i) := sup \\{M_i : \\#\\{m : Y_i \\notin \\{y_{m}^{(i)}\\}_{m=1}^{M_i} \\} < M_i\\} , (2)\nand define \\hat{r} to be the [(N+1)(1-\\alpha)] quantile of r_1, \u2026, r_N (r_i = r (X_i, Y_i)): \\hat{r} = r_{[(N+1)(1-\\alpha)]}. Then, we set M_{test} := \\hat{r} and obtain the upper bound of the error rate\nP(Y_{test} \\notin \\{y_{m}^{(test)}\\}_{m=1}^{\\hat{r}}) \\leq \\alpha.  (3)\nA complete proof of Eq. 3 is presented in Appendix A.1."}, {"title": "Identification", "content": "Inspired by self-consistency theory , which states that a repetitively sampled response is viewed as a form of consistency linked to higher confidence in the response, we perform a simple semantic clustering process on the candidate set and obtain the frequency of each response, denoted as F(y_m^{(i)}). For a detailed description of semantic clustering, refer to Appendix B.1.\nThen, we define the nonconformity score of the i-th calibration data point as one minus the frequency of the acceptable response, denoted as \\hat{y}_{ref}, within the candidate set, which is semantically equivalent to the label Y_i (i.e., Y_{ref} \\Leftrightarrow Y). A \\Leftrightarrow B represents a bidirectional entailment between A and B (i.e., equivalence). The specific evaluation for bidirectional entailment is provided in Appendix B.1. In this case, the nonconformity score is formulated as\ns (X_i, Y_i) := 1 - F (\\hat{y}_{ref}).  (4)\nNote that \\hat{y}_{ref}^{(i)} refers to any acceptable response in the candidate set that is semantically equivalent to the reference answer Y_i. Similar to \\hat{r}, we defines \\hat{s} to be the [(N+1)(1-\\beta)] quantile of s_1, \u2026, s_N (s_i = s (X_i, Y_i)):"}, {"title": "Empirical Evaluations", "content": "Experimental Settings\nModels. We employ five open-source MLLMs, including VideoLLaMA-7B, VideoLLaMA-13B, PandaGPT-7B, PandaGPT-13B, and NEXT-GPT, and three closed-source MLLMs, including Gemini-1.5-Flash , Gemini-1.5-Pro, and GPT-40 mini [OpenAI, 2023]. Note that we utilize all five open-source MLLMs as if they are API-only MLLMs, i.e., it assumes no access to any internal information of MLLMs.\nDatasets. We consider two multiple-choice VideoQA datasets: Video-MME (VMME)  and NEXT-QA (NEXT) , where we only utilize the multiple-choice section of NEXT, and two open-ended VideoQA datasets: MUSIC-AVQA (MUSC) and MSVD . More details of the dataset utilization can be found in Appendix B.\nEvaluation Metrics. We utilize the Empirical Error Rate (EER) to assess whether we control the risk of miscoverage by the two user-specified risk levels . Note that in the first stage, miscoverage refers to not sampling the acceptable responses, and this EER is controlled by \\alpha. In the second stage, miscoverage refers to the calibrated prediction sets failing to cover the acceptable responses, and this EER is"}, {"title": "Results for Risk Control", "content": "Following prior work , we randomly split our datasets in half by default, calibrating the sampling number and threshold for identifying high-quality responses on the first half, i.e., the calibration set, and measure the EER (i.e., miscoverage rate) at various risk levels (i.e., upper bound) in two steps on the second half, i.e., the test set. We plot the empirical results in the two steps utilizing the GPT-40 mini model on four datasets in Figure 2a and Figure 2b, and defer similar plots employing other MLLMs to Appendix F.\nIn the first step, we aim to address the challenge of determining whether acceptable responses are covered by the candidate set in open-ended VideoQA tasks. Figure 2a demonstrates that by deriving the minimum number of responses required to sample for each test data point on the calibration set based on our developed conformal score, we achieve statistically rigorous guarantees on EER. The marginal correctness miscoverage rates by the sampled responses on the test set are consistently bounded by various predetermined risk levels. In this way, we can approximate the fixed options provided in closed-ended tasks through sampling, despite the presence of duplicate options.\nIn the second step, we attempt to overcome the limitations of previous approaches, which rely on internal model logits or verbalized confidence scores. We specify the risk level in the first step to be 0.1 (i.e., \\alpha = 0.1), and then measure the correctness miscoverage rates by the prediction sets, which are calibrated by our devised nonconformity score based on self-consistency theory. Figure 2b illustrates the validity of identifying high-quality responses based on their frequency scores with the candidate sets. The miscoverage rates on the calibrated prediction sets are strictly bounded by the risk level \\epsilon (= \\alpha + \\beta \u2013 \\alpha\\beta) at various settings of \\beta. In this way, we obtain an intuitive confidence level by analyzing the frequency of each response in the candidate set, adapting our framework to API-only MLLMs. In terms of the black-box reliability measurement, we also evaluate the development of nonconformity score based on semantic diversity in Section 4.4.\nEmpirical evaluations demonstrate that our risk control framework matches our theory in practice, as the correctness miscoverage rates in both steps never exceed the user-specified risk level. Note that EER is marginal (average) over the test set, not conditional to individual test data points."}, {"title": "Results for Risk Assessment", "content": "Conformal sets are constructed adaptively to each particular input , so that the set size gives us an indicator of the model's uncertainty (or the risk in the current decision-making). Prior work utilize APSS to benchmark LLMs and Vision-Language Models (VLMs) through risk assessment in MCQA tasks. In contrast to closed-ended scenarios, where MLLMs are provided a predetermined set of options that encompasses the correct answer and produce a prediction set without internal"}, {"title": "Sensitivity Analyses", "content": "Splitting Ratio of Calibration and Test Set. As previously discussed, the calibration set is essentially a set of sufficiently general observation samples, from which we derive the number of response samples required for each test data point and the threshold for identifying high-quality responses in the candidate set based on specific statistical criteria. In this section, we explore the impact of the ratio of sample sizes between the calibration set and the test set on the final performance, specifically how many resources are needed for our framework to achieve risk guarantees on the test set. In previous work, we set a ratio of 0.5 by default. Here, we evaluate the EER when the ratio is set to 0.3 and 0.1. As shown in Figure 5, when \\alpha and \\beta are set to 0.1, resulting in an overall risk level (i.e., \\epsilon) of 0.19, we achieve consistently strict guarantees of the miscoverage rates across three split ratios utilizing three closed-source MLLMs on the MUSC dataset, which demonstrates the efficiency and stability of our framework for risk control in practical open-ended VideoQA applications."}, {"title": "Conclusion", "content": "Deploying MLLMs in practical VideoQA applications requires reliable risk control and assessment. In this paper, we present a general two-step framework for risk control by calibrating a conformal score to sample new generations and a nonconformity score to identify high-quality responses, which provides statistically and theoretically rigorous guarantees on the correctness miscoverage rate. Our method bridges the gap between SCP and closed-source MLLMs, and it can also be applied to other generative models (e.g., LLMs) that support sampling in any language generation tasks. Furthermore, we conduct risk assessments and explore the issue of semantic redundancy in prediction sets within open-ended contexts for the first time, which results in a promising evaluation metric for MLLMs in open-ended VideoQA tasks via the average set size after deduplication. Additionally, we analyze reliability measurement in the development of the nonconformity score for more efficient predictions. We hope that our framework can be helpful for human-in-the-loop decision-making and human-AI teams."}, {"title": "Limitations", "content": "In our work, guarantees are marginal over the test set, not conditional to individual data points. In addition, our framework does not deviate from the fundamental exchangeability assumption of SCP, and we will explore risk control under distribution shift conditions in future work."}, {"title": "Implementation Specifics", "content": "Semantic Clustering Process"}, {"title": "Datasets", "content": "Video-MME (VMME) [Fu et al., 2024] is a multi-modal evaluation benchmark for MLLMs in video analysis, which includes 900 manually selected videos totaling 254 hours and resulting in 2,700 MCQA pairs. NExT-QA (NEXT) [Xiao et al., 2021] is a benchmark for VideoQA that focuses on causal and temporal action reasoning, moving beyond simple scene descriptions. It features both multiple-choice and open-ended QA tasks. However, we only selected the multiple-choice part for our experiments. We use the sub-test set of MCQA, which consists of 2,312 QA pairs. MUSIC-AVQA (MUSC) [Li et al., 2022b] is an open-ended VideoQA dataset designed to evaluate comprehensive multimodal understanding and spatio-temporal reasoning over audio-visual scenes. We use the test set of MUSIC-ACQA, which consists of 9,185 QA pairs. MSVD [Chen and Dolan, 2011] is an open-ended dataset that provides a large-scale, highly parallel text collection. We use 2,519 QA pairs from this dataset. In our experiments, we extracted the audio from the videos and integrated it with the corresponding visual and textual data before inputting them into the MLLMs."}, {"title": "Hyperparameters", "content": "We employ multinominal sampling for responses which are used to construct prediction sets. The temperature of generation for all MLLMs is set to 1.0. All the experiments are conducted on a server with one Intel Xeon Gold 6326 CPU and 7 NVIDIA A6000 GPUs."}, {"title": "Evaluation Metrics", "content": "Empirical Error Rate (EER) evaluates the validity of our framework for risk control. In the first stage, we expect that, under the condition of exchangeability, we calibrate the number of response samples of each test data point based on the calibration set so that the candidate set encompasses at least one acceptable response with a specific probability. To this end, the EER on the test set, D_{test}, should be bounded by the risk level \\alpha (i.e., \\leq \\alpha). At this point, we can approximate the closed-ended settings utilizing the candidate set. We redefine each sample in the test set as (x_i, Y_i) and the candidate set of the i-th data point as S (x_i). Then, EER is formulated as\nEER = \\frac{1}{\\|D_{test}\\|} \\sum_{(X_i, Y_i) \\in D_{test}} 1\\{y_i \\in S (x_i)\\}  (20)\nIn the second stage, we expect that we can identify the acceptable response within the candidate set, which is also said the prediction set encompasses the correct answer, based on the nonconformity score with a user-specified probability. To this end, the EER on the test set should be bound by \\alpha and another risk level \\beta (i.e., <\\alpha+\\beta-\\alpha\\beta.). In the evaluations, we set \\alpha = 0.1 by default and measure EER at various values of \\beta. At this point, the upper bound of EER is 0.1 + \\beta \u2013 0.1\\beta, and EER is formulated as\nEER = \\frac{1}{\\|D_{test}\\|} \\sum_{(X_i,Y_i) \\in D_{test}} 1\\{Y_i \\in C(x_i)\\}  (21)\nAverage Prediction Set Size (APSS) measures the average size of all prediction sets required to guarantee the EER on the test set. A larger APSS indicates greater uncertainty/risk, while a smaller APSS reflects higher efficiency and confidence. At this point, APSS is formulated as\nAPSS = \\frac{1}{\\|D_{test}\\|} \\sum_{(X_i,Y_i) \\in D_{test}} |C(x_i)|  (22)\nAccuracy (ACC) assesses the correctness of MLLMs' predictions, serving as a foundational benchmark for model performance. In our experiments, we employ the most frequent (reliable) response within the candidate set of each test data point, denoted as \\hat{y}_{ref}^{(i)}, as the model output to measure the total accuracy, which is formulated as.\nACC = \\frac{1}{\\|D_{test}\\|} \\sum_{(X_i, Y_i) \\in D_{test}} 1{\\{\\hat{y}_{ref}^{(i)} = Y_i \\}}  (23)"}]}