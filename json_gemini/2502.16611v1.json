{"title": "Target Speaker Extraction through Comparing Noisy Positive and Negative Audio Enrollments", "authors": ["Shitong Xu", "Yiyuan Yang", "Niki Trigoni", "Andrew Markham"], "abstract": "Target speaker extraction focuses on isolating a specific speaker's voice from an audio mixture containing multiple speakers. To provide information about the target speaker's identity, prior works have utilized clean audio examples as conditioning inputs. However, such clean audio examples are not always readily available (e.g. It is impractical to obtain a clean audio example of a stranger's voice at a cocktail party without stepping away from the noisy environment). Limited prior research has explored extracting the target speaker's characteristics from noisy audio examples, which may include overlapping speech from disturbing speakers. In this work, we focus on target speaker extraction when multiple speakers are present during the enrollment stage, through leveraging differences between audio segments where the target speakers are speaking (Positive Enrollments) and segments where they are not (Negative Enrollments). Experiments show the effectiveness of our model architecture and the dedicated pretraining method for the proposed task. Our method achieves state-of-the-art performance in the proposed application settings and demonstrates strong generalizability across challenging and realistic scenarios.", "sections": [{"title": "1. Introduction", "content": "In the target speaker extraction task, the model is required to extract the target speaker's voice from an audio mixture. To specify the characteristics of the target speaker, prior works have explored using conditional information of the target speaker in multiple modalities, including visual (Pan et al., 2022; 2021), textural (Ma et al., 2024; Hao et al., 2024), or acoustic modality (\u017dmol\u00edkov\u00e1 et al., 2019; Xu et al., 2020; Ge et al., 2020; Zhang et al., 2024a).\nThough prior works using conditional information in the acoustic modality have achieved significant performance, most of these works only considered using clean audio examples of the target speaker (He et al., 2024; Meng et al., 2024; Zhao et al., 2024; Pham et al., 2024). This strong assumption prevents these models from performing well when only noisy audio examples are available. For example, consider a cocktail party, where the user meets a stranger who has not obtained the audio enrollments before. To extract the target stranger speaker's voice from the noisy environment to assist conversation, the user will have to ask the speaker to step outside to record the clean audio enrollment of the target speaker's voice. Enrolling target speakers in this way is often impractical in real-world applications.\nIn this work, we present a method to perform target speaker extraction conditioned on noisy audio enrollments where both the target speaker and the disturbing speakers exist. Performing target speaker extraction conditioned on such audio enrollments is an ill-posed problem since the model does not know which speaker in the audio enrollment is the target speaker required by the user. We resolve such ambiguity by additionally using negative audio enrollment that captures the disturbing speakers' characteristics, and trains the model to extract the target speaker's embedding by learning from the difference between positive and negative audio enrollments, as shown in Figure 1. Note that all three audio inputs to our model are audio mixtures containing multiple speakers.\nBy addressing target speaker extraction in this manner, our model achieves a broader range of real-world applications. To perform target speaker extraction in real-time conversation, the user could press one button on the device to record the positive audio enrollments when the user visually observes the target speaker speaking, and press another button to record negative audio enrollments when the target speaker does not speak. Thanks to the causal extraction model architecture, our model could perform causal target speaker extraction for such application scenarios. To perform non-real-time target speaker extraction for arbitrary audio in an audio editing application, users could label some audio segments as positive or negative when they do or do not identify the desired speaker in the segment, and then use our model to extract the target speaker's voice in the whole audio mixture. Note that users do not have to label all the segments in the audio mixture that contains the target speaker. Through comparing the difference between the positive and negative audio enrollments, our model could obtain the target speaker's voice identity and perform the extraction in both of the above two application scenarios.\nIn conclusion, our contributions include:\n1. Address the problem of target speaker extraction when disturbing speakers exist in the enrollment stage, through comparing the positive enrollment (where the target speaker and disturbing speakers are present) and negative audio enrollment (where only disturbing speakers are present).\n2. Design a fusion module and an associated pretraining method for the proposed task. Ablation experiments show that the proposed fusion method achieves 1.67 dB SI-SNRi improvement over the previously commonly used fusion method in three speaker extraction scenarios, and the proposed training method allows our model to achieve the same level of performance (5 SNR-dB on the validation set) with 280k less optimization steps.\n3. Demonstrate and discuss our model's effectiveness under various difficult and realistic application scenarios, including an increased number of disturbing speakers in the scene, speech extraction for multiple target speakers, and different lengths of positive and negative audio enrollments."}, {"title": "2. Related Work", "content": "In target speaker extraction, prior works have explored using visual (Pan et al., 2022; 2021), textural (Ma et al., 2024; Hao et al., 2024), and audio examples (\u017dmol\u00edkov\u00e1 et al., 2019; Xu et al., 2020; Ge et al., 2020; Zhang et al., 2024a) of the target speaker as extraction condition. Our work belongs to the third category, which learns the acoustic characteristics from given audio examples. Prior works in this category have attempted to improve extraction quality by modifying the fusion method (He et al., 2024; Zeng & Li, 2024), leveraging multi-channel information in the audio mixture input (Meng et al., 2024; Pandey et al., 2024), and processing audio in the temporal domain (Xu et al., 2020; Ge et al., 2020).\nMeng et al. (2024) experimented on binaural target extraction. Experiments show that models leveraging directional information achieve better performance than monaural separation. Similarly, Pandey et al. (2024) extracted audio from a specified direction-of-arrival (DOA), using multi-channel audio recorded by an 8-microphone circular array. In our experiment, we show that our model is capable of implicitly leveraging the directional information of the target speaker to extract the target speech and the associated binaural reverberation effect."}, {"title": "2.2. Target Speaker Extraction in Challenging Scenarios", "content": "To improve target speaker extraction models' robustness and applicability in real-world applications, prior works have addressed the model performance in extracting multiple target speakers' speech simultaneously (Rikhye et al., 2021; Ma et al., 2024; Zeng et al., 2023), transferring to extract speech for different languages (Pham et al., 2024), extracting speech from a variable number of disturbing speakers in the audio mixture (Zhao et al., 2024), and disentangling irrelevant audio characteristics (e.g. reverberation effect) from the enrollment (Liu et al., 2024; Ranjan & Hansen, 2018; Heo et al., 2024; Pandey & Wang, 2023; Borsdorf et al., 2024; Mu et al., 2024; Luo et al., 2024)."}, {"title": "2.3. Target Audio Extraction Conditioned on Enrollments with Disturbing Speakers", "content": "Though prior works have achieved significant progress in target audio extraction, most of these works assume the availability of clean audio enrollment. However, in real-world application scenarios, such enrollment examples are not necessarily available.\nAmong those prior works that attempted to extract target speech conditioned on noisy audio enrollments, TCE (Chen et al., 2024) explored the turn-taking dynamics in the conversation. TCE model accepts an audio encoding of the user's clean voice and performs extraction by considering the speakers who cross-talk with the user as the disturbing speakers. However, TCE is limited to performing target speaker extraction when the user is participating in the conversation and does not allow specifying audio segments where the target speaker is present. OR-TSE (Zhang et al., 2024b) addressed the overlap between the target and the disturbing speakers at the start and end of the enrollment. A speech diarization model is used to discard the audio segments with multiple speakers, and the enrollment encoder still relies on the non-overlapping regions in the enrollment to extract the target speaker's characteristics. LookOnceToHear (Veluri et al., 2024) extracts the target speaker when multiple other disturbing speakers are present. To obtain the target speaker's identity, the model requires the user to look at the target speaker when sampling enrollment audio. Through beamforming at a 90-degree azimuthal angle, the model disambiguate the target speaker from disturbing speakers talking simultaneously during the enrollment stage.\nIn comparison to the previous target speaker extraction methods, our model does not assume knowledge of any speaker in the mixed audio, or the target speaker's spatial location in the enrollment stage. This allows our model to extract the target speaker from an arbitrary mono-channel sound mixture, and only use easily obtained binary labels of positive and negative audio segments in the temporal domain to extract the target speech. To the best of our knowledge, we are the first work to address monaural target speaker extraction without relying on any clean audio enrollment input. We summarize the difference in conditional information used by our method and the prior works in Table 1."}, {"title": "3. Method", "content": ""}, {"title": "3.1. Problem Formulation", "content": "In a target speaker extraction task, multiple speakers talk simultaneously in the scene. The aim is to extract one or a subset of the speakers' voices from the audio mixture. We refer to this audio mixture subject to extraction as Mixed Audio in the following explanation. Let $S = \\{P_1,...,P_N\\}$ be the set of speakers in the scene and $S_M \\subset S$ be the set of speakers present in the Mixed Audio (by present in an audio mixture, we mean the speaker spoke when the audio mixture is recorded). To obtain the target speaker's voice characteristics, we record Positive and Negative Enrollments capturing the audio mixtures with or without the target speaker's voice. Both of these two enrollments contain a mixture of multiple disturbing speakers talking at the same time. We define the speakers whose voice is recorded in the Positive Enrollment as $S_P$, and speakers in the Negative Enrollment as $S_N$. The set of target speakers is defined as the set difference $S_T = S_P - S_N$.\nEach speaker may speak different content in the Positive Enrollment, Negative Enrollment, and Audio Mixture. We write the voice of each speaker i in the Mixed, Positive, and Negative Audio as $a^{\\{M,P,N\\}}_i$, and the background noise of the scene in the three audio mixtures as $n^{\\{M,P,N\\}}$. The mixture of audio is modeled as the addition of each speaker's voice and the background noise. Thus, the Mixed Audio $a^M$, the Positive Enrollment $a^P$, and the Negative Enrollment $a^N$ are:\n$a^M = \\sum_{i \\in S_M} a^M_i + n^M, a^P = \\sum_{i \\in S_P} a^P_i + n^P, a^N = \\sum_{i \\in S_N} a^N_i + n^N$.\nThe target speakers' voice $a_{clean}$ in the positive audio enrollment and the target speakers' voice $a_{tgt}$ in the Mixed Audio (i.e. model prediction ground truth) are\n$a^{clean} = \\sum_{i \\in S_T} a^P_i, a^{tgt} = \\sum_{i \\in (S_T \\cap S_M)} a^M_i$.\nOverall, we draw a Venn diagram (Figure 2) to show the different types of speakers. In detail, based on whether a speaker's voice exists in the Positive/Negative/Mixed Audio, speakers are classified into 8 types. In addition to the Classical Target speaker, whose voice is expected to be the sole voice present in the extracted audio, speakers belonging to the four Disturbing speaker types should not be present in the extracted audio. Even though Silent Target, Silent Disturbing Type 1, and Silent Disturbing Type 2 speakers are not present in the Mixed Audio, their presence in the Positive and Negative Enrollments might affect the model performance on encoding the target speaker's voice. We"}, {"title": "3.2. Model Architecture", "content": "As shown in Figure 3, we adopt TF-GridNet (Wang et al., 2023) as our Encoding and Extraction Branches' backbone, and introduce an attention-based Pos-Neg Fusion Module and an Extraction Branch Fusion Module to effectively integrate enrollment information into the extraction model.\nThe model architecture and training method for encoding the target speaker's feature is shown in Figure 4. The Encoding Branch adopts the TF-GridNet architecture as the backbone, which consists of a 2D convolution layer followed by stacks of three TF-GridNet blocks. For encoding, we directly use the output from the last TF-GridNet block as the enrollment representation. Each block processes the features through three submodules sequentially: the Intra-frame Spectral Module, which models inter-frequency information within each frame (i.e., timestep), the Sub-band Temporal Module, which models temporal information within sub-bands, and the Full-band Self-attention Module, which captures the long-range frame information between frames. Positive and Negative Enrollments are encoded using the same TF-GridNet encoder, producing two sequences of embeddings $E_{pos}$ and $E_{neg}$ with shapes $[T_{pos}, C\\times F]$ and $[T_{neg}, C\\times F]$, respectively, where C and F denote the channel and the frequency bin number, and $T_{pos}$, $T_{neg}$ are the frame numbers in the temporal dimension of the positive and negative enrollment embeddings.\nThe Pos-Neg Fusion Module extracts the target speaker's voice embedding through comparing the positive and negative enrollment embeddings $E_{pos}$, $E_{neg}$. We first element-wise-add each embedding in both sequences with a learnable segmentation embedding, to indicate whether it encodes frames of the Positive or Negative Enrollment. The two embedding sequences then concatenate along the temporal dimension to form a feature of shape $[T_{pos} + T_{neg}, C \\times F]$,"}, {"title": "and pass through two layers of Full-band Self-attention Module. By performing self-attention calculation, the Full-band Self-attention Module learns the difference between embeddings of the positive and negative enrollments and encodes the target speaker that only exists in the positive enrollment by removing the disturbing speakers' characteristics. We reshape the output embeddings corresponding to the positive enrollment as the extracted target speaker embedding, which results in a [C, Tpos, F] shape embedding Epos-neg being extracted after the Pos-Neg Fusion Module.", "content": "In the Extraction Branch, we modify the TF-GridNet block to perform causal inference by adopting the same modification as LookOnceToHear (Veluri et al., 2024). In particular, we remove the global layer normalization after the first convolution layer, change the BiLSTM in the TF-GridNet blocks to unidirectional LSTM, and constrain the Full-band Self-attention Modules to calculate the causal attention value of a one-time frame with only frames before it. Three causal TF-GridNet blocks modified as above are used in the Extraction Branch.\nExtraction Branch Fusion Blocks are added after the first two causal TF-GridNet blocks to integrate the target speaker embedding with the extraction model. To reduce computation time, we apply non-overlapping average pooling with 20 kernel size along the temporal dimension to the extracted target speaker embedding. We discuss the effect of different kernel sizes on the model performance and inference time in Section 4.3. In the Extraction Branch Fusion Block, we adopt three 1 \u00d7 1 convolution layers to ensure the pooled target speaker embedding Epos-neg and the output from the previous TF-GridNet block Elayer 1 have the same channel number H. The target speaker embeddings are reshaped to [Tpos, H \u00d7 F] and used as the Key and Value feature, while the output from the previous TF-GridNet block is reshaped to [Tmix, H \u00d7 F] and serves as the Query. Information fusion between Query, Key, and Value is performed using the multi-head attention calculation in the Full-band Self-attention Module.\nIn comparison to encoding the target speaker with a fixed-sized embedding, using cross-attention allows the target speaker to be represented as a longer sequence of embeddings, which helps the model achieve better extraction quality (as shown in Section 4.3). In addition, the memory complexity of the cross-attention fusion is O(Tmix \u00d7Tpos+Tmix \u00d7 H \u00d7 F) with respect to the Mixed Audio frame number Tmix, and the computation complexity is O(Tmix \u00d7 Tpos \u00d7 H \u00d7 F). As the length of the audio mixture subject to extraction increases, the memory and time complexity of the fusion module scale linearly. Moreover, by selecting models with different pooling sizes for the target speaker embedding, we can achieve a balance between extraction quality and inference time/memory requirements. This flexibility broad-\nens the model architecture's applicability, from lightweight audio extraction on portable devices with limited computation and storage capabilities to professional audio editing tools where higher computational power and relaxed speed constraints are available."}, {"title": "3.3. Training Method", "content": "The training pipeline includes two stages. As shown in Figure 3, the first stage pretrains the encoder and the Pos-Neg Fusion Module from scratch to extract the target speaker's embedding from the positive and negative audio enrollments. We obtain the encoding of the target speaker's embedding by passing the clean target speaker's voice through a trained frozen TF-GridNet encoder. Formally, the pretraining loss is written as\n$L_{pretrain} = ||E_{clean} - E_{pos-neg}||^2$,\nwhere $E_{pos-neg}$ is the target speaker's embedding extracted by our model from input pair $(a^P, a^N)$, and $E_{clean}$ is the embedding of the clean target speaker voice $a_{clean}$ encoded by a trained TF-GridNet encoder from (Veluri et al., 2024).\nIn the second stage, we train the Extraction Branch to extract the target speaker from the Audio Mixture. The training loss for the second stage is the negative SNR value of the extracted audio $\\hat{a_{tgt}}$ and the ground truth target speaker speech $a_{tgt}$:\n$L_{snr} = -SNR(a_{tgt}, \\hat{a_{tgt}}).$"}, {"title": "4. Experiment", "content": ""}, {"title": "4.1. Datasets and Baselines", "content": "Datasets We select the LibriSpeech dataset (Panayotov et al., 2015) to construct our Positive, Negative, and Mixed Audio. The background noise $n^{\\{M,P,N\\}}$ in the Positive, Negative, and Mixed Audio are from the WHAM noise dataset (Wichern et al., 2019). Besides, following the experiment configuration in LookOnceToHear (Veluri et al., 2024), we also construct binaural Positive/Negative/Mixed Audio samples by convolving speech from each speaker with the binaural RIR data provided in the ASH-Listening-Set dataset (Pearce, 2022). We refer to the model trained on monaural data as Classical-Monaural, and on binaural data as Classical-Binaural. Both models are trained on scenarios containing one Classical Target and two Classical Disturbing speakers. We evaluate by calculating the SNR and SI-SNR of the extracted speech, along with the improvement in SNR (SNRi) and SI-SNR (SI-SNRi), which"}, {"title": "4.2. Result on Monaural and Binaural Reverberant Target Speech Extraction", "content": "We compare our model with baseline methods under scenarios where different numbers of speakers are present. Table 2 shows the monaural target speaker extraction performance. SpeakerBeam (\u017dmol\u00edkov\u00e1 et al., 2019) fails to perform when multiple speakers are present in the audio enrollments. TCE (Chen et al., 2024) achieves better performance than SpeakerBeam (\u017dmol\u00edkov\u00e1 et al., 2019) baseline since it does not rely on the clean target speaker's enrollment. However, the TCE model shows worse performance than our method under all different numbers of disturbing speakers.\nIn the multi-channel target speech extraction task, prior work (Veluri et al., 2024) has explored using beamforming to extract the target speaker's characteristics. In this section, we show that additionally including the Negative Enrollment helps improve model performance. As shown in Table 2, our method achieves better extraction performance in all scenarios with 1 to 5 disturbing speakers.\nIn contrast to the monaural experiment results, the binaural model's SNRi performance increases as the number of disturbing speakers increases. This shows that the binaural audio input helps the model estimate the target speaker's voice intensity more effectively. However, this does not indicate better extraction quality, as the SI-SNRi of both our model and the baseline method decreases."}, {"title": "4.3. Ablation Study", "content": "In the ablation experiments, we modify our model architecture and training pipeline to investigate the proposed methods' effectiveness. Unless otherwise specified, all the reported results are tested on the experiment scenario with one Classical Target and two Classical Disturbing speakers."}, {"title": "Cross-Attention based Fusion over Film Fusion Method", "content": "Film fusion is widely applied in the prior target audio extraction works (Perez et al., 2017). However, Film fusion passes the condition embedding through a linear layer, and element-wise multiplies the output with the input tensor to perform fusion. This limits the model to use fixed embedding size, which might not be capable of encoding the fine-grained details of the target speaker's characteristics. In this section, we show that our proposed attention-based fusion method is more optimum for our target speech extraction task.\nWe keep the training method and the rest of the model architecture intact and only change the fusion block of our model to a Film fusion block. In particular, we perform global average pooling on the encoder head output along the temporal dimension to obtain an embedding of fixed dimension as the condition input to the Film fusion block. As shown in Table 2, our attention-based fusion method achieves higher performance in all application scenarios. This shows the cross-attention-based fusion method is more preferable for the proposed target speech extraction task."}, {"title": "Embedding Pooling Size", "content": "To reduce the model computation time in the Extraction Branch, we perform an average pooling of 20 on the embedding extracted by the encoder head. In this section, we compare our model performance and inference time with models using 10 and 40 average pooling sizes. As shown in Table 3, we train two model variants with 10 and 40 average pooling sizes. Smaller pooling size results in less information loss, thus achieving better performance. However, a smaller pooling size also results in a longer sequence of audio embedding being extracted, which leads to a longer inference time. As a result, we selected 20 pooling sizes in the final model configuration."}, {"title": "Effectiveness of Pretraining the Encoding Branch", "content": "Since our model needs to extract the target speaker's identity by comparing two noisy audio examples, training the model from scratch will result in slow and unstable convergence. In this section, we show the effectiveness of the pretraining step by showing the validation loss curve of our model with and without the first pretraining stage. As shown in Figure 5, the model trained from scratch reaches 5 SNR-dB on the validation set after 520k optimization steps (around 108 hours), while the combined training time for the two-staged training takes 240k optimization steps (around 50 hours) based on single Nvidia A10 24GB GPU."}, {"title": "4.4. Model Performance in Challenging Application Scenarios", "content": "We show our model's performance under different lengths of Positive and Negative Enrollments. We train our model"}, {"title": "Model Performance when Undefined Speaker Present", "content": "The Undefined speaker belongs to neither target nor disturbing speakers. Depending on the application scenario, the user may want to extract the undefined speaker's voice (if the speaker is someone who wants to join the discussion at a cocktail party), or the user may want to remove them to only listen to the target speaker's voice. In this scenario, we demonstrate the model's performance when such undefined speakers are present if not explicitly trained to remove or extract the Undefined speakers' voice.\nWe select the 4-speaker scenario, which consists of one target, two disturbing, and one undefined speaker. We compare the SNRi and SI-SNRi between the extracted audio with the following audios: Classical Target + Unknown speaker's voice, only Unknown speaker's voice, and only Classical Target speaker's voice. As shown in Table 6, model output is most similar to the Classical Target speaker's voice. This shows that the model trained with no unknown speakers presenting in the training samples tends to remove the unknown speakers in its extraction in inference. On the other hand, fine-tuning is required if the model is expected to also extract the Undefined Speaker's voice."}, {"title": "5. Conclusion", "content": "In this paper, we propose a new task for target speech extraction, where additional disturbing speakers are present throughout the audio enrollments. We optimize our model to extract the target speaker's characteristics from easily obtained noisy positive and negative enrollments, without using any clean audio enrollment input. We design two fusion modules and an associated training pipeline for the proposed task. Experiments show that our method achieves SOTA performance under multiple challenging and realistic application scenarios. However, further work is still required to improve the model performance when a large number of disturbing speakers are present and reduce the performance gap between our method and prior works that use clean audio enrollment as the extraction condition."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning and Audio Processing. This technology can potentially improve communication aids, hearing devices, and audio editing tools, making it easier for users to focus on desired voices in complex auditory scenes. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}, {"title": "Appendix", "content": ""}, {"title": "A. Data Simulation Method", "content": "We generate our training, validation, and testing data from the train-clean-360, dev-clean, and test-clean components, respectively. The sampling rate is 16000. We train our model on data samples containing one Classical Target and two Classical Disturbing speakers. We construct our monaural training samples for such a scenario by first selecting three distinct speakers from the LibriSpeech dataset (Panayotov et al., 2015). Two audio samples of the target speaker are randomly sampled to form the $a_{tgt}$ and $a_{clean}$ audio defined in Section 3.1. For each of the disturbing speakers, we randomly sample three speech examples of them to be their voice in the Audio Mixture, the Positive Enrollment, and the Negative Enrollment. In order to prevent pauses and leading zeros in audio samples affecting the actual number of speakers in the synthesized audio samples, we use WebRTC Voice Activity Detector (Wiseman, 2018) to detect and remove zeros in the LibriSpeech dataset samples, before constructing the mixed, positive, and negative audio examples.\nTo simulate the background environment noise, we use the WHAM noise dataset. Different noise samples from the WHAM noise dataset are recorded in different environments. As a result, additional disturbing sounds might exist in one WHAM noise example but not the other. For instance, some WHAM noise dataset samples contain music playing in the background while others don't. Using a WHAM noise sample containing music in the Positive Enrollment and a WHAM sample of ambient environment noise in the Negative Enrollment will confuse our model on whether the music is the target sound to be extracted. We resolve such ambiguity by sampling the $n^{\\{M,P,N\\}}$ from the same WHAM noise sample but at different temporal segments. Selecting random segments from the WHAM noise prevents the model from assuming the background noise is the same across the Positive, Negative, and Mixed Audio. All the Positive, Negative, and Mixed Audio are three seconds long audio in both the training and testing stages, but our model architecture can process arbitrary long audio longer than one second. We scaled the WHAM noise to be 0 SNR with respect to the ground truth target speaker's voice in each sample.\nTo simulate the binaural reverberant training and testing samples, we convolve each speaker's voice with BRIR from the ASH-Listening-Set dataset (Pearce, 2022). The binaural RIRs used for constructing one data sample are randomly selected such that 1. All BRIR used in a single data sample are from the same scene in the ASH-Listening-Set dataset (Pearce, 2022), and 2. The BRIR for the target speaker in the Positive Enrollment has the direction of arrival of 0 degrees. The pre-processing strategy for each speaker's voice and the generation of background noise remains the same as the monaural dataset."}, {"title": "B. Model Architecture and Training Detail", "content": "The TF-GridNet Encoder in the Encoding Branch uses the following configuration: 4 \u00d7 4 kernel size and 1 \u00d7 1 stride in the first Conv2D, 64 hidden units in all three BiLSTM layers, 8 attention head numbers in the Full-band Self-attention Module. The input audio is processed by Short-Time Fourier Transform (STFT) with 128 window size and 64 hop length. The causal TF-GridNet blocks in the Extraction Branch use the same configuration as above, apart from using 1 \u00d7 1 kernel size in its first Conv2D layer. The number of learnable parameters in the Encoding Branch and Extraction Branch is shown in Table 7.\nAll the training is done on a single Nvidia A10 24GB GPU with a batch size of 2. In both training stages, we use the Adam optimizer. We use an initial learning rate of 2e-3 and decay the learning rate by half if validation loss does not decrease for more than 50 epochs. 500 epochs (200k optimization steps) are used in the first pretraining stage, and 1000 epochs (400k optimization steps) are used in the second stage.\nSince TCE is not trained on the LibriSpeech dataset, we fine-tune the TCE model on our simulated data till convergence (for 120k optimization steps) using the Adam optimizer with 5e-4 learning rate. To train and test the TCE model on the proposed extraction task, we modify our dataloader's output to match TCE's application scenario by adding a known speaker's voice in the Negative Enrollment and concatenating the Mixed Audio with this modified Negative Enrollment. The model then performs extraction conditioned on the known speaker's d-vector embedding."}]}