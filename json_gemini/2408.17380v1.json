{"title": "Traffic expertise meets residual RL: Knowledge-informed model-based residual reinforcement learning for CAV trajectory control", "authors": ["Zihao Sheng", "Zilin Huang", "Sikai Chen"], "abstract": "Model-based reinforcement learning (RL) is anticipated to exhibit higher sample efficiency compared to model-free RL by utilizing a virtual environment model. However, it is challenging to obtain sufficiently accurate representations of the environmental dynamics due to uncertainties in complex systems and environments. An inaccurate environment model may degrade the sample efficiency and performance of model-based RL. Furthermore, while model-based RL can improve sample efficiency, it often still requires substantial training time to learn from scratch, potentially limiting its advantages over model-free approaches. To address these challenges, this paper introduces a knowledge- informed model-based residual reinforcement learning framework aimed at enhancing learning efficiency by infusing established expert knowledge into the learning process and avoiding the issue of beginning from zero. Our approach integrates traffic expert knowledge into a virtual environment model, employing the Intelligent Driver Model (IDM) for basic dynamics and neural networks for residual dynamics, thus ensuring adaptability to complex scenarios. We propose a novel strategy that combines traditional control methods with residual RL, facilitating efficient learning and policy optimization without the need to learn from scratch. The proposed approach is applied to CAV trajectory control tasks for the dissipation of stop-and-go waves in mixed traffic flow. Experimental results demonstrate that our proposed approach enables the CAV agent to achieve superior performance in trajectory control compared to the baseline agents in terms of sample efficiency, traffic flow smoothness and traffic mobility. The source code and supplementary materials are available at https://github.com/zihaosheng/traffic-expertise-RL/.", "sections": [{"title": "1. Introduction", "content": "Connected automated vehicles (CAVs) represent a pivotal innovation in the evolution of future transportation networks (Liu et al., 2023b; Qu et al., 2023; Liao et al., 2024; Sheng et al., 2024b). By harnessing advanced technologies like sensors, artificial intelligence, and communication systems, CAVs hold the promise of significantly enhancing road safety, travel efficiency, and environmental sustainability (Olovsson et al., 2022; Dong et al., 2022, 2023; Chen et al., 2023b; Huang et al., 2024a). This potential has fueled an increasing focus on the development of safe and intelligent control strategies for CAVs. Such strategies are designed to enable these vehicles to function as dynamic actuators within mixed traffic, thereby optimizing traffic flow and reducing congestion (Stern et al., 2018; Cui et al., 2017; Andreotti et al., 2023). Classical optimization-based approaches have achieved considerable successes in this context, but these approaches often encounter limitations due to their dependence on the careful selection and tuning of parameters (Shi et al., 2023; Garriga and Soroush, 2010). This process can be both labor-intensive and specific to each application, thereby struggling to maintain performance in the face of unexpected or unusual conditions.\nLearning-based approaches have shown a remarkable ability to adapt and thus overcome the limitations inherent in the above classical strategies (Ding et al., 2022; Kiran et al., 2022; Sheng et al., 2024a). Within the spectrum of"}, {"title": "2. Related works", "content": ""}, {"title": "2.1. Classical analytical controller", "content": "Enhancing driving behavior and performance through automated driving techniques unlocks promising avenues for achieving smoother traffic flow and heightened mobility efficiency (Cui et al., 2017; Zheng et al., 2020; Yue et al., 2022). Notably, real-world experiments by Stern et al. (2018) have shown promising potential of such technologies in mitigating stop-and-go traffic patterns. Among these technologies, Adaptive Cruise Control (ACC) is a typical and widely used system known for its ability to autonomously adjust vehicle speeds by detecting the relative distance and speed to the leading vehicle (Vahidi and Eskandarian, 2003). The introduction of vehicle-to-vehicle or vehicle-to-infrastructure wireless communication in Cooperative ACC (CACC) further augments this capability by leveraging additional information from multiple nearby vehicles, and significantly improves string stability and mobility (Milanes et al., 2014). From a control perspective, the implementation of these technologies has largely relied on feedback and feedforward controllers, including proportional-integral-derivative (PID) control (Lidstrom et al., 2012), and model-based control strategies (Yu and Long, 2022). These model-based control usually utilizes car-following models, such as Newell model (Newell, 2002), and the Intelligent Driver Model (Treiber et al., 2000). Leveraging these foundational models, a variety of model-based control strategies have been developed, ranging from optimal control and H-infinity control (Zheng et al., 2020; Zhou et al., 2020) to model predictive control (MPC) (Feng et al., 2021). Additionally, the seminal works of Zheng et al. (2020) and Cui et al. (2017) provide theoretical proofs and experimental evidence demonstrating that a strategically controlled autonomous vehicle can act as a mobile actuator in traffic flow, effectively damping stop-and-go waves and improving overall traffic stability.\nMPC stands out as a predominant tool in tackling traffic control tasks with constraints, with a focus on the optimization of multiple objectives (Feng et al., 2021; Sheng et al., 2023). The essence of MPC lies in its ability to predict and optimize the future behavior of a system based on a system dynamics model in a rolling/receding horizon manner. Typically, MPC frameworks are designed with vehicle velocities and positions as the state variables, employing acceleration as the control variables to achieve these objectives (Li et al., 2023). For instance, Yang et al. (2023) adopted MPC to regulate the acceleration and velocities of CAVs for minimized fuel consumption, taking into"}, {"title": "2.2. Reinforcement learning", "content": "RL-based approaches have recently emerged as powerful tools in addressing complex driving tasks (Wu and Qu, 2022; Chen et al., 2021; Wang et al., 2023). This methodology involves an agent that learns optimal decision-making through interactions with its environment, aimed at maximizing cumulative rewards. The strengths of RL-based ap- proaches are primarily manifested across three dimensions. First, this learning paradigm offers a significant advantage over classical analytical controllers by enabling agents to learn behaviors via trial and error, thus equipping them to adapt to new and unforeseen scenarios (Huang et al., 2024b). Second, its computational intensity is predominantly confined to the offline training phase. This allows the trained driving policies to be executed swiftly in real-time ap- plications, addressing the limitation of extensive computation times often associated with MPC (Shi et al., 2023; Liu et al., 2024). Third, unlike conventional control methods that require to define objective functions explicitly through control actions, RL can implicitly encode them within the reward function. This indirect approach to specifying con- trol goals enables RL to adeptly handle tasks that are challenging for conventional analytical controllers (Staessens et al., 2022). Building on these advantages, RL-based approaches have shown remarkable success across a broad spectrum of challenging driving scenarios, including ramp merging (Kiran et al., 2022; Zhu et al., 2022), bottleneck management (Ha et al., 2023), and intersection navigation (Peng et al., 2021; Liu et al., 2023a). For instance, Guo et al. (2023) demonstrated the superiority of PPO-based approaches in their cooperative traffic light and vehicle con- trol system. Their study showed that PPO-based controllers achieved up to 30% reduction in both travel time and fuel consumption and emissions under varying CAV penetration rates, outperforming traditional control methods. Similarly, Yang et al. (2024) showcased the effectiveness of PPO and SAC in optimizing eco-driving strategies for mixed traffic near signalized intersections. Their results indicated that RL-based methods, particularly SAC, outperformed human drivers in all aspects and showed better energy efficiency compared to traditional models like IDM. However, the majority of these methods are model-free RL and often suffer from low sample efficiency due to the necessity of training the RL agent from scratch, resulting in substantial computational resource usage and prolonged training durations.\nModel-based RL has gained significant attention in recent years as a powerful paradigm to address the sample inefficiency and exploration challenges in model-free RL (Moerland et al., 2023). Within the realm of traffic man- agement, model-based RL introduces innovative strategies for addressing the complex transportation challenges. One of the pioneering works in model-based RL is the Dyna-style algorithm (Sutton, 1990), which utilizes collected in- teraction samples to learn a virtual model of the environment. This model allows the agent to generate simulated experiences, thereby facilitating the update of its policy based on these synthetic interactions. For instance, Yavas et al. (2022) have leveraged a Dyna-style model-based meta-policy optimization approach to ensure safer and more comfortable car-following behaviors. Similarly, Lee et al. (2020) proposed a Dyna-based model-based RL to optimize the control of electric vehicles, aiming for energy-efficient eco-driving. Furthermore, Huang et al. (2021) introduced meta learning techniques into a model-based RL framework tailored for traffic signal control, with a focus on en- hancing travel efficiency. Nevertheless, the RL agent within Dyna-style model-based RL still requires learning from scratch, necessitating extensive training periods to achieve optimal performance.\nAnother branch of model-based RL integrates the principles of MPC. This approach often employs a learned model of the environment to predict future states and rewards. It then solves the optimal sequence of actions over a defined time horizon using these predictions in an MPC manner (Chua et al., 2018). For instance, Pan et al. (2021) introduced an MPC-based RL algorithm tailored for the control of mixed freeway traffic, utilizing a gradient-free cross-entropy technique to solve the optimal action sequence. In a similar vein, Dong et al. (2021) developed an MPC-based RL strategy for CAVs, specifically designed to prevent collisions in imminent crash scenarios by applying"}, {"title": "2.3. Residual RL", "content": "The concept of learning residual models and policies falls into a broader framework of physics-informed machine learning (Karniadakis et al., 2021), which synergizes the deductive and interpretable power of analytical physics-based models with the generalization and learning capabilities of data-driven models. Within the established frameworks, the integration of physics knowledge with NN models is accomplished in three distinct manners: generating sup- plementary data from physics models to enrich training datasets (Han et al., 2022), embedding constraints derived from physics into the loss functions of NN models (Shi et al., 2022), and crafting specialized NN architectures that inherently incorporate prior knowledge (Long et al., 2024). Residual model and policy learning, fitting into this third category, leverages the foundational performance offered by analytical physics-based models to account for basic and known dynamics, with data-driven models layered on top to address the residuals of unknown dynamics. The focus of residual model learning has predominantly been on predicting residuals to improve the accuracy of physics models, which has shown considerable promise in control tasks, ranging from autonomous racing (Kabzan et al., 2019) to robot control (O'Connell et al., 2022). Notably, in the field of transportation, the pioneering work of Long et al. (2024) introduces the concept of physics-enhanced residual learning (PERL) for car-following trajectory pre- diction, marking a significant advancement in the application of residual model learning. Recent studies have further explored the integration of physics-based models with machine learning techniques in transportation scenarios. For instance, Chen et al. (2023a) employed IDM to predict HDV trajectories for collision risk assessment in multi-agent RL highway on-ramp merging, while Sun et al. (2023) used IDM to generate expert demonstrations representing ideal car-following behavior for initializing their RL model through imitation learning. Additionally, Zhou and Gayah (2023) proposed incorporating domain control knowledge into deep reinforcement learning agents for perimeter me- tering control, demonstrating improved learning efficiency, control performance, and scalability in urban network traffic management. However, these approaches rely heavily on physics models which may not fully capture the complexities of real-world traffic dynamics. As a result, the RL agents trained using these methods may inherit the inherent simplifications and biases of these physics models, potentially limiting their ability to adapt to and effectively handle more complex traffic scenarios encountered in real-world environments.\nDistinct from residual models which primarily enhance prediction accuracy, residual policy learning decomposes a complex control task into two parts: one part that can be partially handled by conventional controllers for a near- optimal solution, and a residual part where RL techniques come into play (Johannink et al., 2019). This division not only streamlines the learning process by leveraging existing controllers to bypass the initial learning from scratch but also enables continuous enhancement through RL-driven NNs. Recently, residual policy learning has captured attention across various domains. For instance, Hou et al. (2023) leveraged residual policy learning to improve the braking controller for better passenger comfort during the post-braking process. Zhang et al. (2022) designed a controller that integrates a modified artificial potential field with residual policy learning for autonomous racing, attaining performance on par with professional human racers across F1Tenth tracks. Furthermore, its applicability has extended to robot arm control (Johannink et al., 2019), and parking scheduling (Hou et al., 2024). Inspired by the efficiency and adaptability of residual learning, we aim to combine the advantages of both residual model learning and residual policy learning to effectively tackle the existing issues of low sample efficiency in model-free RL and the limitation of needing to learn from scratch in model-based RL. By leveraging the foundational insights provided by physics-based models and the dynamic adaptation facilitated by NNs within the RL framework, our approach promises to enhance sample efficacy, reduce computational demands, and expedite the training process."}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Overall framework", "content": "Figure 1 illustrates the overall framework of our proposed knowledge-informed model-based residual reinforcement learning. The CAV agent interacts with the actual traffic environment, and the interaction data are stored in"}, {"title": "3.2. Virtual environment modeling with residual model", "content": "In RL, the environment represents the actual system with which the RL agent interacts, defined by its states, actions, and rewards. At each time step t, the agent observes the current state $s_t$, takes an action $a_t$ based on its learned policy, and receives a reward $r_t$ as feedback from the environment. The environment then transitions to a new state $s_{t+1}$. The transition dynamics of the environment can be described by the following equation:\n$s_{t+1},r_t = T(s_t, a_t)$\nTo facilitate efficient exploration and learning for the RL agent while minimizing direct impact on the actual environ- ment, we introduce a virtual environment. In this virtual environment, a model is used to approximate the transition function. The transition equation for the virtual environment is given by:\n$\\hat{s}_{t+1},\\hat{r}_t = T_{\\psi}(s_t, a_t)$"}, {"title": "3.2.1. Knowledge-driven traffic dynamics model", "content": "IDM is a widely adopted car-following model in traffic simulation and modeling, providing valuable insights into traffic flow dynamics and the influence of different driving behaviors on traffic congestion and safety. In the IDM, drivers aim to maintain a safe and comfortable following distance from the leading vehicle while adjusting their acceleration to reach a desired velocity. The IDM takes into account factors including the relative distance and velocity to the leading vehicle, and its desired speed. The IDM equations are as follows:\n$a^* = a_{max} \\left[1 - \\left(\\frac{v}{v_0}\\right)^{\\delta} - \\left(\\frac{s^*(v, \\Delta v)}{s}\\right)^{2}\\right]$\n$s^*(v, \\Delta v) = s_0 + \\text{max} \\left\\{0, vT_0 + \\frac{v \\cdot \\Delta v}{2\\sqrt{a_{max}b}}\\right\\}$\nwhere $a^*$ is the calculated acceleration, v is the current velocity of the vehicle, $a_{max}$ the maximum acceleration, $v_0$ is the desired velocity, $\\delta$ is the acceleration exponent, s is the gap to the leading vehicle, $s^*(v, \\Delta v)$ is the desired gap considering the relative velocity $\\Delta v$ to the leading vehicle, $s_0$ is the minimum desired gap between the vehicle and its leading vehicle, $T_0$ denotes the safe time headway, and b represents the comfortable deceleration of the vehicle.\nEquation (5) describes the acceleration dynamics of the vehicle, which is determined by three terms: (i) the first term $\\left[1 - \\left(\\frac{v}{v_0}\\right)^{\\delta}\\right]$ encourages the vehicle to reach its desired velocity $v_0$; (ii) the second term $-\\left(\\frac{s^*(v, \\Delta v)}{s}\\right)^{2}$ ensures that the vehicle maintains a safe distance from the leading vehicle; and (iii) the maximum acceleration limit $a_{max}$ prevents unrealistic accelerations. Equation (6) defines the desired gap $s^*(v, \\Delta v)$, which is a function of the current velocity v and the relative velocity $\\Delta v$ to the leading vehicle. The desired gap consists of three components: (i) the minimum desired gap $s_0$; (ii) the velocity-dependent term $vT_0$, which increases the desired gap proportionally to the vehicle velocity to maintain a safe time headway; and (iii) the dynamic term, $\\frac{v \\cdot \\Delta v}{2\\sqrt{a_{max}b}}$, which increases the desired gap when the vehicle is approaching a slower leading vehicle to ensure a safe and comfortable deceleration.\nBy incorporating the IDM as the knowledge-driven base model $T_\\psi$ in our virtual environment, we leverage the domain expertise encoded in the model to capture the average traffic dynamics. The IDM provides a reliable repre- sentation of the fundamental car-following behavior, considering factors such as desired velocity, safe distance, and comfortable acceleration and deceleration. This integration of traffic domain knowledge enhances the reliability of the virtual environment."}, {"title": "3.2.2. Knowledge-informed neural network", "content": "While the IDM model captures average behaviors in the traffic system with reasonable fidelity, it may not be sufficient for tasks that require a more accurate representation of real-world dynamics. For instance, drivers typically exhibit diverse driving styles, preferences, and situational responses. The simplified assumptions of IDM regarding uniform driving behavior may overlook these individual differences, leading to inaccuracies in real-world traffic sce- narios. Furthermore, the fixed parameters in IDM, such as the desired time headway and comfortable deceleration, may not fully capture the adaptability and context-dependent nature of human driving. These limitations can result in discrepancies between the actual behaviors and the predictions of IDM.\nTo address these challenges, we propose a knowledge-informed neural network to learn the residual dynamics. By combining prior knowledge with a neural network, this knowledge-informed approach can improve prediction accuracy and learning efficiency. The neural network is denoted as follows:\n$\\Delta s_{t+1}, \\Delta r_t = \\Delta_{\\phi}(s_t, a_t)$\nwhere $\\Delta s_{t+1}$ and $\\Delta r_t$ represent the predicted residuals of state and reward, respectively. The neural network $\\Delta_{\\phi}$ takes the current state $s_t$ and action $a_t$ as inputs and outputs the estimated residuals, which capture the discrepancies between the IDM predictions and the actual dynamics. The knowledge-informed neural network is trained in an end- to-end manner using the interaction data generated by the RL agent's interactions with the environment. The input to the model consists of the state features. The model then directly outputs the predicted residual dynamics, which are compared with the ground truth residual dynamics to calculate the MSE (mean square error) loss for learning. By learning the residuals, the neural network can fine-tune the knowledge-driven IDM model to better match real- world data, thereby improving the overall fidelity of the virtual environment. The predicted residuals from the neural network are then added to the predictions of $T_\\psi$ to obtain the final outputs of the virtual environment model.\nThe integration of the knowledge-driven IDM model and the data-driven residual neural network enables the virtual environment to accurately capture both the average dynamics of the traffic system and the individual variations and adaptations of human driving behavior. The IDM serves as a base model, capturing the fundamental dynamics of traffic flow. The use of residual learning is specifically designed to address the limitations of fixed IDM parameters. The residual component, implemented as a neural network, allows our model to capture complex behaviors and dynamics that the IDM alone cannot describe. For instance, while the IDM might predict a certain acceleration based on its fixed parameters, the residual component can learn to adjust this prediction based on factors not considered by the IDM, such as individual driver characteristics. This combination allows the virtual environment to generate realistic predictions and facilitates effective learning for the RL agent. By interacting with this knowledge-informed virtual environment, the RL agent can explore and learn from a large number of simulated experiences, improving sample efficiency and reducing the need for extensive actual environment interactions."}, {"title": "3.3. Driving policy learning with residual RL", "content": "In our proposed approach, we aim to leverage existing well-established knowledge to improve the learning effi- ciency of RL and avoid learning from scratch. Given an initial policy $\\pi_H: S \\rightarrow A$ with states $s \\in S$ and actions $a \\in A$, we learn a residual policy $\\pi_\\theta: S \\rightarrow A$. The main idea is to leverage the reliability of conventional controllers while also benefiting from the flexibility and adaptability of RL. By combining the initial policy $\\pi_H$ and the residual policy $\\pi_\\theta$, we obtain a full policy given by:\n$\\pi= \\pi_H + \\pi_{\\theta}$"}, {"title": "3.3.1. Physics-based initial policy", "content": "In our proposed framework, we employ a physics-based controller that incorporates traffic domain knowledge as the initial policy $\\pi_H$. Specifically, we adopt the PI with saturation controller (Stern et al., 2018), a well-established and field-validated model that has demonstrated effectiveness in managing CAV behaviors in real-world traffic scenarios. The PI with saturation controller aims to regulate the velocity of the CAV based on the estimated average speed of the leading vehicles while handling stop-and-go waves commonly encountered in traffic.\nThe core principle of the PI with saturation controller is to determine a command velocity $v_{cmd}$ based on a proportional-integral (PI) control mechanism. The average speed, denoted as $\\bar{v}$, is estimated by calculating the aver- age of the CAV velocity over a predefined time window. To ensure safety and effective gap management, the PI with saturation controller incorporates two key mechanisms. First, it defines a target velocity $v^*$ that adjusts the speed of CAV based on the current gap between the CAV and the leading vehicle, which is calculated as:\n$v^* = \\bar{v} + v_e \\times \\text{min} \\left(\\frac{s}{s_l}, \\text{max}\\left(\\frac{s}{s_u},-1.0\\right),1\\right)$\nwhere $v_e$ is the maximum additional velocity allowed for catching up to the leading vehicle, s is the current gap between the CAV and the leading vehicle, and $s_l$ and $s_u$ are the lower and upper gap thresholds, respectively. When the gap is below a lower threshold $s_l$, the target velocity is set to the estimated average speed $\\bar{v}$, ensuring that the CAV maintains a safe distance. As the gap increases beyond $s_l$, the target velocity gradually increases up to a maximum of $\\bar{v} + v_e$, enabling the CAV to catch up to the leading vehicle and close the gap.\nThe second mechanism involves updating the command velocity $v_{cmd}$ using the following rule:\n$v^{cmd}_{t+1}= \\beta_t \\left(a_t \\bar{v} + (1 - a_t)v_{lead}\\right) + (1 - \\beta_t)v^{cmd}_t$"}, {"title": "3.3.2. RL-based residual policy", "content": "The RL problem is typically formulated as a Markov Decision Process (MDP), which provides a mathematical framework for modeling decision-making. An MDP is defined by a tuple (S, A, T, R, \u03b3), where S, A, and T are the state space, action space, and transition dynamics of the environment, as introduced in the previous sections. R:S\u00d7A \u2192 R represents the reward function, and \u03b3\u2208 [0,1] is the discount factor. The goal of the RL agent is to learn an optimal policy \u03c0* that maximizes the expected cumulative discounted reward over time, i.e., \u03c0* = arg max $E_\\tau \\left[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) | \\pi\\right]$, where the expectation is taken over the trajectory of states and actions generated by following a policy \u03c0. To solve the RL problem and find the optimal policy, we often use the state-value function and the action-value function. The action-value function $Q^{\\pi}(s, a) = E_{\\pi} \\left[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) | s_0=s, a_0=a\\right]$ represents the expected cumulative discounted reward starting from state s, taking action a, and then following a policy \u03c0. The state-value function $V^{\\pi}(s) = E_{a_t \\sim \\pi(\\cdot|s_t)} \\left[Q^{\\pi}(s_t, a_t) | s_0=s\\right]$ represents the expected cumulative discounted reward starting from state s and following a policy \u03c0.\nIn our proposed framework, the residual policy network $\\pi_\\theta$ is represented by a fully-connected neural network with two hidden layers, each containing 64 neurons with ReLU activation functions. The output layer of the network has a tanh activation function to ensure that the residual actions are bounded. We employ Trust Region Policy Optimization (TRPO) (Schulman et al., 2015) to learn the residual policy $\\pi_\\theta$, which adapts to complex traffic dynamics and compensates for the limitations of the physics-based controller. The main idea behind TRPO is to perform constrained optimization in the policy space, ensuring that each policy update improves performance while preventing the policy from deviating too far. TRPO achieves this by defining a trust region that constrains the magnitude of each policy update, ensuring that the deviation between the updated and old policies is limited. Specifically, the optimization problem is formulated as follows:\n$\\begin{aligned} &\\underset{\\theta}{\\text{arg max }} E_{\\tau \\sim \\pi_{\\theta_{\\text{old}}}}\\left[\\sum_{t=0}^{T} \\frac{\\pi_{\\theta}(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)} Q_T(s_t, a_t)\\right] \\\\ &\\text { s.t. } E_{\\tau \\sim \\pi_{\\theta_{\\text {old }}}} \\left[D_{K L}\\left[\\pi_{\\theta_{\\text {old }}}\\left(\\cdot \\mid s_t\\right) \\| \\pi_{\\theta}\\left(\\cdot \\mid s_t\\right)\\right]\\right] \\leq \\delta \\end{aligned}$\nwhere $\\pi_{\\theta_{\\text{old}}}$ and $\\pi_{\\theta}$ represent the old and updated policies, respectively. T is the trajectory sampled from the old policy. $D_{KL}$ measures the Kullback-Leibler (KL) divergence between the old and updated policies. The constraint ensures"}, {"title": "3.4. Model-based residual RL", "content": "Despite incorporating prior knowledge and neural networks to learn the environment dynamics can improve the prediction performance, there still exists model inaccuracy in the virtual environment model due to factors such as the complexity of real-world traffic scenarios, and inherent stochasticity in human driving behavior. Any inaccuracies in the IDM or the neural network components could significantly impact the performance of the RL agent. This is because the agent learns and optimizes its policy based on the interactions with both the virtual and actual environ- ments. If the virtual environment does not accurately reflect real-world traffic dynamics, the learned policy may not transfer well to actual traffic scenarios. Inaccuracies could lead to suboptimal decision-making, reduced safety, or inefficient traffic flow when the agent is deployed in the actual environment. Therefore, we aim to design a feasible model-based RL approach that can effectively handle and mitigate the impact of these inaccuracies, ensuring that the policy performance improvement observed in virtual environment can be transferred to the actual environment.\nAs the number of rollout steps k in the virtual environment model increases, a trade-off emerges between the growing cumulative error and the potential for higher policy performance improvement due to the increased virtual interaction data. On one hand, longer rollouts provide more diverse and informative experiences for the RL agent to learn from, leading to a more optimal policy. On the other hand, the accumulation of model inaccuracies over extended rollouts can negatively impact the reliability and effectiveness of the learned policy. Consequently, it is crucial to balance these opposing factors to ensure that the benefits of longer rollouts outweigh the detrimental effects of the increased model error. To guarantee policy improvement in the actual environment, the policy performance must exceed a certain threshold $C$ in the virtual environment (Janner et al., 2019), which is determined by the model inaccuracy, the rollout length, and the discrepancy between the updated and previous policies. Based on these, we can derive the following relationship:\n$\\eta(\\pi) \\geq \\hat{\\eta}(\\pi) - C(\\epsilon_{\\pi}, \\epsilon_m, k)$\n$C(\\epsilon_{\\pi}, \\epsilon_m, k) = 2R_{max}\\left(\\frac{\\gamma^{k+1} \\epsilon_{\\pi}}{(1 - \\gamma)^2} + \\frac{\\gamma k \\epsilon_m}{1 - \\gamma}\\right)$\nwhere $\\eta(\\pi)$ and $\\hat{\\eta}(\\pi)$ represent the cumulative reward in the actual and virtual environment, k denotes the virtual model rollout length, $\\epsilon_{\\pi}$ denotes the bounded total-variation distance due to the policy update, defined as $\\epsilon_{\\pi} = \\text{max}_s D_{TV} [\\pi_{\\theta} || \\pi_{\\theta_{\\text{old}}} ]$, and $\\epsilon_m$ is the bounded model inaccuracy. This relationship provides insights for how to utilize the virtual model for policy update, i.e., we should minimize $C$ throughout the learning process to maximize the lower bound of $\\eta(\\pi)$. Although analytically solving for the minimum of $C$ is challenging, we observe that $C$ increases monotonically with respect to the model inaccuracy $\\epsilon_m$ and can grow to infinity as the rollout length k increases. Consequently, it is advisable to prevent the excessive use of the virtual environment model, and reduce the rollout length k when faced with high model inaccuracy. Based on this, we propose a dynamic rollout length as follows:\n$k^* = \\text{min}\\left(k_{max}, \\kappa \\frac{\\epsilon_{\\pi}}{\\epsilon_m}\\right)$\nwhere $k_{max}$ is a predefined upper bound for rollout step to avoid model overusing, $\\kappa$ is a hyperparameter that controls the sensitivity of the rollout length to the model inaccuracy. When the model inaccuracy $\\epsilon_m$ is low, the rollout length $k^*$ is allowed to increase, enabling the RL agent to benefit from longer rollouts and potentially achieve higher policy"}, {"title": "4. Experimental results and analysis", "content": ""}, {"title": "4.1. Experimental setup", "content": "To validate the proposed method, we utilize an open-source traffic simulator SUMO (Lopez et al., 2018) in con- junction with an RL framework Flow (Wu et al., 2022) to construct the RL environment and control the CAV agent. SUMO provides a comprehensive platform for simulating a wide range of traffic scenarios in both urban and highway networks, allowing for realistic modeling of traffic dynamics and interactions between vehicles. Flow is built on top of the SUMO simulator and offers a user-friendly and extensible interface specifically designed for researchers to develop and evaluate advanced traffic control strategies using RL techniques. By leveraging the capabilities of SUMO and Flow, we create a flexible experimental setup that enables the effective validation of our knowledge-informed residual model-based RL approach in various traffic settings. All the parameters utilized in the experimental evalua- tion are summarized in Appendix B."}, {"title": "4.1.1. Experimental scenarios", "content": "We aim to validate that the proposed approach can mitigate traffic disturbances and smooth traffic flow in mixed traffic scenarios. As illustrated in Figure 3, we mainly study three scenarios: a ring road, a figure eight road, and a merge road. Each scenario includes both CAV agents and human-driven vehicles (HDVs) to form mixed traffic. The CAV agents are controlled by the RL algorithm, while HDVs are governed by the IDM. The IDM parameters used for human drivers are set to typical values for highway driving. To introduce stochasticity and account for real-world uncertainties, we augment the IDM by adding Gaussian noise with a mean of 0 and a standard deviation of 0.2 to the vehicle acceleration (Wu et al., 2022). This noise injection allows us to simulate the variability and randomness present in actual traffic scenarios, enhancing the robustness and realism of our simulations.\nRing Road: The ring road scenario is designed to simulate traffic flow dynamics on a single-lane circular road- way. The total length of the ring road varies between 220 and 270 meters, providing a compact yet representative environment for studying traffic congestion. The ring road length varies across different episodes, introducing diverse spatial dynamics. In this scenario, we follow the setting of existing work (Stern et al., 2018), considering a fleet of 22 vehicles, where one vehicle is designated as a CAV agent, and the remaining 21 vehicles are modeled as HDVs adhering to the IDM. The high vehicle density on the ring road, coupled with the limited perception capabilities of the IDM-driven vehicles, frequently results in the formation of stop-and-go waves. These waves are characterized by alternating periods of deceleration and acceleration, resulting in reduced traffic flow and increased travel times. The"}, {"title": "4.1.2. RL settings", "content": "State Space: In all the studied scenarios, each CAV agent can perceive its surrounding environment, which includes the states of the leading and following vehicles. Specifically, the state space consists of the velocity of the"}, {"title": "Action Space:", "content": "The action space for each CAV agent is defined as the acceleration, with a range of \u00b11 m/s\u00b2. This range is designed to maintain passenger comfort, which is a crucial factor in the widespread adoption and acceptance of automated driving systems. Furthermore, this range aligns with the acceleration capabilities of many production vehicles, ensuring that our approach remains practical for real-world implementation. By choosing an appropriate acceleration value at each time step, the CAV agent can control its speed and adjust its position relative to the sur- rounding vehicles. This continuous action space enables the CAV agent to smoothly and effectively control its motion in response to the prevailing traffic conditions."}, {"title": "Reward Function:", "content": "The reward is designed to align the behavior of the"}]}