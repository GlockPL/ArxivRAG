{"title": "Alopex: A Computational Framework for Enabling On-Device Function Calls with LLMs", "authors": ["Yide Ran", "Zhaozhuo Xu", "Yuhang Yao", "Zijian Hu", "Shanshan Han", "Han Jin", "Alay Dilipbhai Shah", "Jipeng Zhang", "Dimitris Stripelis", "Tong Zhang", "Salman Avestimehr", "Chaoyang He"], "abstract": "The rapid advancement of Large Language Models (LLMs) has led to their increased integration into mobile devices for personalized assistance, which enables LLMs to call external API functions to enhance their performance. However, challenges such as data scarcity, ineffective question formatting, and catastrophic forgetting hinder the development of on-device LLM agents. To tackle these issues, we propose Alopex, a framework that enables precise on-device function calls using the Fox LLM. Alopex introduces a logic-based method for generating high-quality training data and a novel \u201cdescription-question-output\" format for fine-tuning, reducing risks of function information leakage. Additionally, a data mixing strategy is used to mitigate catastrophic forgetting, combining function call data with textbook datasets to enhance performance in various tasks. Experimental results show that Alopex improves function call accuracy and significantly reduces catastrophic forgetting, providing a robust solution for integrating function call capabilities into LLMs without manual intervention.", "sections": [{"title": "1 Introduction", "content": "With the rapid advancement of Large Language Models (LLMs) [24, 2, 14], their integration into software applications has become increasingly widespread [25, 31]. Researchers and engineers from both academia and industry are now focusing on developing LLM-powered agents [20, 35] for mobile devices to provide personalized assistance to users. A key aspect of creating an on-device LLM agent is enabling LLMs to call external API functions [36, 26] for enhanced performance. These API functions act as external tools, grounding the LLMs to generate more accurate and personalized outputs.\nDespite the growing interest, several challenges hinder the development of on-device LLM agents with function call capabilities. We summarize these challenges from the perspective of data scarcity, question format, and catastrophic forgetting.\nScarcity of LLM Function Call Demonstrations. Training data that demonstrates successful LLM function calls is scarce. Since the concept of LLM agents has only recently gained researchers' attention, there is a shortage of examples showing how LLMs can effectively use external API functions on mobile devices to complete queries correctly. Consequently, current methods often rely on LLMs to generate synthetic function call demonstrations based on function descriptions [5]. However, this synthetic data, as seen in"}, {"title": "2 Related Work", "content": "We summarize the related works from the dataset generation, training schema, and benchmarks.\nDataset Generation. Recent works have developed function call dataset generation pipeline. Octopus-v2 [5]"}, {"title": "3 Alopex Framework", "content": "We overview our framework in Figure 1. The framework contains three major components: i) function call demonstration generation, ii) formatting function call demonstrations for LLM fine-tuning, and iii) overcoming catastrophic forgetting."}, {"title": "3.1 Generating Function Call Demonstrations Using Rule-Based Logic", "content": "In this study, we employed the Rule-Based Logic approach to generate questions and outputs for the dataset. Based on the function description of each API, we categorized common user questions into two styles: requests and commands. Requests can be further divided into a question part, an action part, and a parameter part. Commands, on the other hand, only include the action and the parameter parts.\nLet's consider the take_a_photo API as an example. For more function descriptions, please refer to \u00a7A in the appendix. Based on the given function description, we can generate the detailed elements that constitute a question, following the division into the question part, the action part, and the parameter part. For the content of the three-part elements, please refer to \u00a7A in the appendix.\nBy arranging the elements in the order of question, action, and parameter, we can instantly generate questions related to function take_a_photo. As long as the number of queries we want to generate is less than the maximum combinations of questions, actions, and parameters, we don't have to worry about generating duplicate questions. For generating the outputs, we can establish a mapping table between parameters and their corresponding values. Based on the parameter content appearing in the question, we can retrieve the corresponding values from the mapping table to generate the value part of the output. By following the logic to generate data, we can make sure that we won't generate incorrect function parameters and function names. Simultaneously, we can greatly improve efficiency as well. We save a huge amount of time on LLM inference and eliminate the need for manual data checking and validation and LLM regeneration in the later stages."}, {"title": "3.2 Formatting Function Call Demonstrations for LLM Fine-Tuning", "content": "There is still an issue with the dataset's structure regarding the position of the function description. Although Octopus-v2 [5] has not open-source their dataset, based on the LLM inputs and outputs, we speculate that its dataset structure follows the format of Data Format(1) in Figure 2, called as $DF_1$.\nMeanwhile, according to the data training format provided by gorilla-llm/gorilla-openfunctions-v1 [36], they employ Data Format(2) in Figure 2, i.e., $DF_2$.\nSo, which one is better? Before we proceed with the next step, we need to introduce a metric called Out-of-Logic Function Call Accuracy (OOFC). Since we design the dataset generation rules manually, it is inevitable that cases will deviate from the pre-defined rules. For example, the existing designed rules generate the question part that contains phrases such as \u201cCan I\u201d, \u201cHow do I\u201d, \u201cHow can I\u201d, \u201cIs it possible to"}, {"title": "3.3 Overcoming Catastrophic Forgetting in LLM Fine-Tuning for Function Calls", "content": "Previous studies have shown that after fine-tuning and continuing learning, LLMs can successfully perform downstream tasks. However, they may forget their previous knowledge and perform poorly on previous tasks. This phenomenon is called catastrophic forgetting. One common approach to address catastrophic forgetting is to mix the history pretraining data with the new data to enable LLMs to recall previous knowledge [18, 30, 12].\nWe have also observed the phenomenon of catastrophic forgetting after fine-tuning the function call task. While the LLMs perform well on the function call task, there is a significant decrease in metrics MMLU [11], GSM8K [8], Arc [7], HellaSwag [38], Winogrande [28], and TruthfulQA [15] compared with the pretraining LLM.\nAlthough mixing the pretraining dataset alleviates catastrophic forgetting, the pretraining data for LLMs is not publicly accessible. Therefore, the approach of mixing the pretraining dataset is not feasible. Empirically, textbook datasets are typically used for pretraining LLMs. Although we do not have access to the exact textbook datasets used for LLM pretraining, is it possible for us to use opensource textbook datasets for mixed dataset finetuning to achieve the same goal of mitigating catastrophic forgetting? We conducted experiments by mixing the function call dataset with the opensource dataset nampdn-ai/tiny-textbooks [21] with ratio 1:1 and observed that the phenomenon of catastrophic forgetting can be mitigated, some metrics are even better than before. These experiments were performed on models google/gemma-2b, Qwen/Qwen1.5-1.8B, stabilityai/stablelm-2-1.6b, tensoropera/Fox-1-1.6B."}, {"title": "4 Experiment", "content": "In this section, we want to validate the effectiveness of the Alopex framework. We would like to answer the following questions:"}, {"title": "4.1 Experiment Settings", "content": "This study focuses on single function calls. We utilized three constructed datasets, and conducted fine-tuning experiments on the following models: google/gemma-2b [33], stabilityai/stablelm-2-1_6b [4], Qwen1.5 - 1.8B [3], and tensoropera/Fox-1-1.6B [34]. Due to the relatively small number of parameters of these LLMs, we directly fine-tuned the full models. The training parameters are provided in \u00a7B in the appendix."}, {"title": "4.1.1 Dataset", "content": "We referenced 10 Android APIs open-sourced from Octopus-v2 [5] to construct the function call datasets; see \u00a7A in the appendix for the descriptions of the 10 functions. We generated the following three datasets for our research questions.\n\u2022 We generated 1,000 data points for each API using a Logic-Rule Based method. The format of the dataset follows $DF_1$. The dataset was then split into a training dataset containing 200 data items and a test dataset containing 800 data items.\n\u2022 We employed the GPT3.5 API [23] to generate the dataset, using the Octopus-v2 approach [5]. Since we did not have access to the open-source docstrings mentioned in Octopus-v2, we directly utilized the queries generated in the first part of our dataset generation as the docstrings. We mixed and shuffled the question dataset generated by the Rule-Based Logic method for the 10 functions. Then, using the GPT-3.5 API, we filtered the queries based on each function's description. Subsequently, we used the GPT-3.5 API to generate the output that contains the function name and the function parameters. Additionally, we appended each function's description after the output. We used the test dataset generated from previous rule-based logic to evaluate the performance of LLMs trained using the LLM-generated data.\n\u2022 We generate a dataset with the same generation process and a different data format $DF_2$. Note that we utilized the 10 function descriptions and the user's questions as inputs, instead of only using the user's questions as inputs, as the model needs to select the most appropriate function from the 10 function descriptions based on the user's questions to assist the user."}, {"title": "4.1.2 Settings", "content": "Testbed. We conducted our experiments using 2 GPUs NVIDIA H100 80GB HBM3.\nEvaluation Benchmarks and Metrics. We utilized the algorithm from the Berkeley Function Call Leader-board [36] to calculate the function call accuracy. Additionally, we employed the LM Evaluation Harness [10] to measure metrics such as MMLU, GSM8K, Arc, HellaSwag, Winogrande, TruthfulQA, and their average. Additionally, we introduced a new metric called Function Call LLM Average Accruacy(FCLAA)which is the sum of average function call accuracy in Rule domain and Out of rule domain, and the average accuracy of MMLU, GSM8K, Arc, HellaSwag, Winogrande, TruthfulQA into our evaluation. Experiments revealed that the function call data generated by GPT-3.5 alone has many errors. It can only achieve an accuracy of 70% which required subsequent validation, modification and re-generation. The Rule-Based Logic method is efficient and generates high-quality function call data. LLMs using Rule-Based Logic function call data finetuning can achieve 99% accuracy. Since our output does not directly use the function name but instead utilizes special tokens inspired by Octopus-v2 [5] to reduce the probability of spelling errors in predicting function names, we used 10 shots to provide prompts for the expected output when testing the ability of pre-trained LLMs to perform function calls. The experiment revealed that, apart from gemma-2b, none of the other three LLMs were able to perform function calls, with gemma-2b exhibiting an accuracy of 48.9% for function calls."}, {"title": "4.2 Results", "content": "Exp1: Comparisons between the Rule-Based Logic method and the LLM-based method. Based on the experimental results from Table 1 and Table 2, we found that while using GPT3.5 to generate data for function calls allows for automated selection of queries that are suitable for each function and automated batch generation of function call outputs, it still results in a significant number of errors, which affects the effectiveness of the fine-tuned LLMs. Consequently, careful checking, validation and secondary generation are required after generating the datasets.\nExp 2: Comparisons between $DF_1$ and $DF_2$. Based on the experimental results from Table 1 and Table 2, we observed that $DF_2$ was more likely to get better function call accuracy, especially in the out-of-logic case.\nExp 3: Evaluation of the effectiveness of mixing datasets. Based on the results in Table 3, using open-source textbook datasets helps recover LLMs' previous performance. Although there are fluctuations in metrics, the average accuracy of all metrics shows an upward trend, except for Fox, which is more robust and exhibits stable results and achieves the highest average accuracy."}, {"title": "5 Conclusion", "content": "Alopex efficiently generates high-quality function call datasets without the need for data validation and regeneration. It also alleviates the catastrophic forgetting phenomenon caused by the function call task in LLMs. Experimental results show that, among the four small LLMs, Fox performs the highest level of robustness and average accuracy."}, {"title": "Appendix", "content": null}, {"title": "A Function Description", "content": "We present the question part, action part, and parameter part we designed for the Rule-Based Logic for the take_a_photo API. We also show the parameter Dictionary Mapping for the take_a_photo API.\nTo better compare with Octopus-v2 [5], the functions and their descriptions we are using here are from their study."}, {"title": "B Experiment Parameters", "content": "In the experiment, we used the following configuration parameters:\n\u2022 Learning Rate: 6e-5, 5e-5, 4e-5, 3e-5, 2e-5, 4e-4, 3e-4, 5e-4\n\u2022 Batch Size: 1, 2\n\u2022 Epochs: [3,8]\n\u2022 Warmup Steps: 5\n\u2022 Max sequence length: 2048\n\u2022 LR scheduler type: linear\nWe conducted 10 experiments for each model using this set of hyperparameters. The best result from each experiment was selected for the report."}]}