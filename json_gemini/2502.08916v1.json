{"title": "PathFinder: A Multi-Modal Multi-Agent System for Medical Diagnostic Decision-Making Applied to Histopathology", "authors": ["Fatemeh Ghezloo", "Wisdom O. Ikezogwo", "Joann G. Elmore", "Mehmet Saygin Seyfioglu", "Rustin Soraki", "Beibin Li", "Tejoram Vivekanandan", "Ranjay Krishna", "Linda Shapiro"], "abstract": "Diagnosing diseases through histopathology whole slide images (WSIs) is fundamental in modern pathology but is challenged by the gigapixel scale and complexity of WSIs. Trained histopathologists overcome this challenge by navigating the WSI, looking for relevant patches, taking notes, and compiling them to produce a final holistic diagnostic. Traditional AI approaches, such as multiple instance learning and transformer-based models, fail short of such a holistic, iterative, multi-scale diagnostic procedure, limiting their adoption in the real-world. We introduce PathFinder, a multi-modal, multi-agent framework that emulates the decision-making process of expert pathologists. PathFinder integrates four Al agents\u2014the Triage Agent, Navigation Agent, Description Agent, and Diagnosis Agent that collaboratively navigate WSIs, gather evidence, and provide comprehensive diagnoses with natural language explanations. The Triage Agent classifies the WSI as benign or risky; if risky, the Navigation and Description Agents iteratively focus on significant regions, generating importance maps and descriptive insights of sampled patches. Finally, the Diagnosis Agent synthesizes the findings to determine the patient's diagnostic classification. Our Experiments show that PathFinder outperforms state-of-the-art methods in skin melanoma diagnosis by 8% while offering inherent explainability through natural language descriptions of diagnostically relevant patches. Qualitative analysis by pathologists shows that the Description Agent's outputs are of high quality and comparable to GPT-40. PathFinder is also the first AI-based system to surpass the average performance of pathologists in this challenging melanoma classification task by 9%, setting a new record for efficient, accurate, and interpretable AI-assisted diagnostics in pathology.", "sections": [{"title": "1. Introduction", "content": "Medical diagnosis of histopathology through the examination of whole slide images (WSIs) is a cornerstone of modern pathology. WSIs are high-resolution, digitally scanned"}, {"title": "2. Related Work", "content": "Multi-modal Histopathology Models. There have been a series of studies in histopathology that leverage WSI-level and pacth-level images to train unimodal classifiers based on multiple instance modeling leveraging pretrained feature extractors. More recently unimodal foundational models trained on varying self-supervised objectives have achieved significant improvements on performance downstream. With the introduction of large-scale multi-modal datasets in histopathology, we have seen significant advancements, with the emergence of large language models and vision-language models for histopathology. For instance, studies like Quilt-1M and PathGen-1.6M curate large histopathology image-text paired dataset and train CLIP-based models to learn joint vision-language representations, significantly enhancing clinical histology downstream tasks on patch-level. On the WSI-level, PathAlign aligns diagnostic texts from pathology reports with corresponding WSIs, facilitating applications such as automatic report generation and case/patient-level visual question answering, moving towards a more clinically integrated and holistic diagnostic process. While many other studies like Quilt-LLaVA, SlideChat, and PathChat train histopathology Multi-modal Large Language models (MLLM) and improve on diagnostic reasoning tasks, none of these models effectively automatically navigate the giga-pixel scale WSIs towards a diagnosis.\nThe role of Navigation in Histopathology Diagnosis. Computational pathology studies have tried to capture and analyze the navigation patterns of pathologists when reviewing digital slide images specifically characterizing mouse patterns, zooming in/out, and panning the field of view (FOV) to piece out morphological clues towards a diagnosis. Often, these studies juxtapose the navigation patterns of junior and senior-level pathologists. NaviPath, presents a human-AI collaborative navigation system designed to seamlessly integrate into pathologists' workflows, considering the specific domain knowledge and navigation strategies required for effective examination of pathology scans.\nMulti-agent Systems. The concept of multi-agent systems has gained traction in Al research, particularly for tasks requiring dynamic behavior and contextual understanding. Recent research has demonstrated the potential of large foundation models in creating interactive agent-based AI systems including interactions between robots, environments, and humans in the field of robotics. These systems can perform complex tasks by leveraging the strengths of individual agents utilizing collaboration and coordination. The potential of multi-agent systems in handling real-world scenarios has been demonstrated in recent studies including but not limited to role-playing, reasoning, gaming and software engineering.\nIn the medical domain some studies have explored role-playing providers (clinicians) treating patients and accumulating proficiency with increasing interactions."}, {"title": "3. Datasets", "content": "To lay the groundwork for describing our agents, we first start by introducing the different datasets used for training and evaluating our system.\nM-Path Skin Biopsy WSIs. The skin biopsy WSIs in this dataset originate from M-Path study, consisting of 238 melanocytic lesion specimens stained with Hematoxylin and eosin (H&E). A consensus reference panel of three dermatopathologists, each with internationally recognized expertise, independently interpreted all 238 cases and established a consensus diagnosis for each case through a series of review meetings. There are 4 diagnostic classes in this dataset: class 1 with 35 cases (mild and moderate dysplastic nevi); class 2 with 86 cases (severe dysplasia/melanoma in situ); class 3 with 70 cases (invasive melanoma stage pT1a); and class 4 with 47 cases (advanced invasive melanoma stage pT1b or more). For model development, the dataset is divided into training, validation, and test sets with a 168/35/35 case split, maintaining consistent class distribution across these sets.\nM-Path Pathologists' Viewport Data. The M-Path study conducted viewport data collection, recruiting 87 pathologists from 10 U.S. states. Eligibility criteria included completion of residency and/or fellowship training and recent experience interpreting skin specimens in clinical practice. Pathologists' viewport data was gathered through an online digital slide viewer developed using Microsoft's open-source Silverlight-based HD View SL, a gigapixel image viewer. This viewer enabled pathologists to navigate each image by panning and zooming up to 60x magnification. During interpretation, the web-based viewer automatically"}, {"title": "4. PathFinder", "content": "The multi-agent multi-modal framework proposed in this study includes four agents: 1) Triage Agent; 2) Navigation Agent; 3) Description Agent; and 4) Diagnosis Agent.\nThe details of training data and model architectures are described below. Figure 1 demonstrates how the four agents interact with each other towards the final goal which is diagnosing a WSI."}, {"title": "4.1. Triage Agent", "content": "The Triage Agent is an image-only transformer-based model tasked with separating class 1 (nevus/mild atypia and moderate atypia/dysplasia) from the rest in the M-Path dataset (Refer to section 3 for M-Path class definitions). We describe the data preparation, model architecture, and training details below.\nData Generation. Each whole slide image (WSI) is divided into non-overlapping 512 \u00d7 512 patches at 10\u00d7 magnification. Background patches (saturation less than 15) are discarded. If fewer than 150 patches remain, we randomly select additional patches from the WSI, apply the saturation filter again, and include the ones that pass. These additional patches may overlap with existing patches but ensure that each WSI contains sufficient information. All patches are then rearranged based on their spatial coordinates. The patches are embedded using the Quilt-Net image encoder, resulting in a feature vector of shape (N, 768) per WSI, where N is the total number of patches for the WSI.\nModel Architecture. The Triage Agent includes several sequential stages (See Fig 1 in the Appendix): The feature vector is initially projected from (N, 768) to (N, dim) using a linear layer to align with the model's embedding dimension dim. For compatibility with 2D processing, the vector is reshaped into a square grid through padding to dimensions H \u00d7 H, where H is the smallest integer satisfying H \u00d7 H > N, with padding achieved by repeating the first M = H^2 \u2212 N features. The padded vector is then processed through a transformer block, followed by positional encoding via the Pyramid Position Encoding Generator (PPEG), and an additional transformer block, where each transformer block contains a single self-attention layer. Subsequently, multi-scale convolutional layers and a squeeze-and-excitation (SE) block refine the vector, capturing spatial patterns across scales and emphasizing key features. The output is then flattened and transformed back to the embedding dimension. A learnable class token is appended to capture global context, and the modified vector is passed through another transformer block, positional encoding, and a final transformer block. Finally, the class token is pooled and passed through an MLP head to produce the model's output.\nTraining Details. We used binary cross-entropy loss for the classification task. The embedding dimension dim is set to 512. Training hyperparameters are as follows: a batch size of 1, learning rate of 2 \u00d7 10^{-4}, weight decay of 1 \u00d7 10^{-5}, and gradient accumulation over 32 steps. Training is conducted for up to 100 epochs, with early stopping after 30 epochs without improvement to prevent overfitting."}, {"title": "4.2. Navigation Agent", "content": "The Navigation Agent is designed to mimic a pathologist's methodical approach to identifying regions of interest (ROIs) in whole slide images (WSIs). Unlike traditional systems that scan the entire WSI in a single, mechanistic sweep, our Navigation Agent adopts a more human-like, iterative process collaborating with the Description Agent. It begins by pinpointing an initial ROI, much as a pathologist would focus on one area at a time. This selected ROI is then relayed to the Description Agent, which provides a natural-language description of the area. Figure 2 illustrates the workflow of the Navigation Agent in the left panel.\nIn our initial attempt, we designed the Navigation Agent using a multi-modal architecture inspired by LLaVA, integrating an image encoder and a large language model (LLM). The image encoder extracted features from a low-resolution version of the WSI, and the LLM processed these features along with previous text descriptions to predict the next ROI. Specifically, the WSI was divided into a grid of patches, and the LLM would output the grid coordinates of the most relevant patch based on both visual and textual inputs. However, this approach faced significant challenges due to the limited size of our training dataset. The model tended to overfit, frequently selecting central patches regardless of the input (see Appendix 2 for details). This limitation prompted the exploration of more data-efficient methods that could better generalize from limited samples.\nTo overcome these challenges, we restructured the Navigation Agent to directly generate an importance map over the WSI, conditioned on textual descriptions from previous observations. This approach removes the dependency on the LLM for spatial selection and leverages a feedback mechanism between the image and text modalities. Let I(t) be the input WSI at iteration t, with previously selected patches masked out to avoid re-sampling and D(1:t) = {D(1), D(2), ..., D(t)} be the set of textual descriptions up to iteration t. At each iteration t, the Navigation Agent processes the masked WSI I(t) to predict an importance map M(t), indicating the likelihood of each region being the next ROI. The importance map is conditioned on the aggregated textual information from previous descriptions. We define the importance map generation as M(t) = f_{Nav}(I(t), E(t-1)) where, f_{Nav} is the Navigation Agent's function (implemented as a lightweight U-Net) that has four layers in both encoder and decoder and is conditined with text embeddings of descriptions, as well as the masked version of the WSI that masks the earlier predicted ROIs, and E(t-1) is the aggregated text embedding up to iteration t \u2212 1. E(t-1) is computed by encoding each description D(k) using a pre-trained Text-to-Text-Transfer-Transformer (T5) text encoder and averaging the embeddings:\n$$E(t-1) = \\frac{1}{t-1}\\sum_{k=1}^{t-1} T5_{text}(D(k))$$\nAt the first iteration (t = 1), since there are no prior descriptions, the importance map is generated solely from the unmasked WSI M(1) = f_{Nav} (I(1)).From the importance map M(t), we then statistically sample the next patch to analyze. The probability p(i,j)^{(t)} of selecting a location (i, j) is proportional to its importance score:\n$$p_{(i,j)}^{(t)} = \\frac{M_{(i,j)}^{(t)}}{\\sum_{(i',j')}M_{(i',j')}^{(t)}}$$\nWe then sample the patch coordinates (i*, j*) based on this probability distribution:(i*, j*) ~ P(i,j)^{(t)}. The selected high-resolution patch corresponding to (i*, j*) is sent to the Description Agent, which generates a new textual description D(t). The new description D(t) is encoded and incorporated into the aggregated text embedding E(t):\n$$E(t) = \\frac{1}{t} \\sum_{k=1}^{t} T5_{text}(D(k))$$\nThis updated embedding E(t) is then used to condition the Navigation Agent in the next iteration, enabling the model to refine its importance map M(t+1) based on both the visual information from I(t+1) and the accumulated textual insights. Therefore, we refer to it as the Text-conditioned Visual Navigator.\nTraining details. To train the Navigation Agent, we constructed a dataset from M-Path consisting of WSIs and sequences of textual descriptions for the most important patches. Each training sample includes: The WSI and"}, {"title": "4.3. Description Agent", "content": "We utilize Quilt-LLaVA, a multi-modal large language model capable of describing individual histopathology patches, as our Description Agent. While the original Quilt-LLaVA generates highly detailed findings, in this work, we instruction-tuned the model to produce more concise summaries, optimizing for computational efficiency. Using captions from the Quilt-1M dataset, we prompted GPT-4 to generate a list of findings as concise as possible. This process yielded 102,000 instruction-tuning samples."}, {"title": "4.4. Diagnosis Agent", "content": "The Diagnosis Agent is a language-only model that analyzes all the gathered natural text descriptions produced by the Description Agent over all the patches identified by the Navigation Agent, to analyze natural text descriptions of histopathological findings and classify them into three categories (classes 2, 3, and 4).\nData Generation. To train the Diagnosis Agent, we generated diagnostic trajectories\u2014sequences of patch descriptions that simulated how a pathologist examined a whole slide image (WSI). Using our Navigation Agent, we proceeded as follows.\nWe first obtained a heatmap for a sub-sampled WSI (512x 512 pixels) using a text-conditioned U-Net model, which highlighted regions of diagnostic significance. The WSI was divided into a 16 \u00d7 16 grid, creating 256 patches of 32 \u00d7 32 pixels each. Each patch received an importance score based on the mean intensity of the heatmap over the patch, indicating its diagnostic relevance. These scores were normalized across all patches.\nTo generate a single trajectory, we iterated the following steps ten times, yielding ten patches per case. At each iteration, a patch was selected using weighted probabilistic sampling based on the normalized importance scores, introducing variability and ensuring different patches were chosen across iterations. The selected patch was then cropped from the high-resolution 10\u00d7 WSI, and a description was generated by the Description Agent. Selected patches were"}, {"title": "5. Experiments and Results", "content": "This section outlines the experimental setup and evaluates the performance of the proposed PathFinder framework. First, we conduct a qualitative assessment of the descriptions generated by the Description Agent, comparing them to two vision-language models (VLMs). Next, we evaluate PathFinder on the M-Path dataset for melanoma diagnosis (see Section 3), benchmarking it against state-of-the-art transformer-based and MIL-based baselines, as well as public and private large language models (LLMs) using prompting without additional training. Finally, we analyze PathFinder's performance under various configurations, altering the Triage, Navigation, and Description Agents. Detailed evaluations are provided in the following subsections."}, {"title": "5.1. Pathologist Evaluation of Description Quality", "content": "To assess the quality of descriptions generated by our Description Agent, we conducted a survey in which two expert pathologists rated descriptions produced by our Description agent in comparison to those generated by GPT4-0 and LLaVA-Med. We selected 25 cases from the M-Path dataset, sampling across the four diagnostic classes. For each case, we cropped the consensus region of interest, manually labeled by a panel of expert dermatopathologists as the area most representative of the diagnosis. Using this region, we prompted our Description Agent, LLaVA-Med and GPT4-o to generate concise descriptions of each histopathology patch. These descriptions were then presented to two expert pathologists in a randomized, double-blind format. Each pathologist was asked to respond to two questions for each case to indicate their preferred description and the reason for their preference."}, {"title": "5.2. PathFinder Evaluation", "content": "For evaluating PathFinder, we utilize the M-Path dataset which contains histopathology WSIs of melanocytic skin tissue. As outlined in 4.4, multiple trajectories are generated per case to simulate the variability in diagnostic patterns observed among pathologists, who may assess a single case with diverse visual strategies to identify diagnostically significant regions. To mitigate randomness in our results, we evaluated PathFinder 10 times on the test set, each time using a different random subset of 5 trajectories selected from the total of 20. For each Whole Slide Image (WSI), majority voting is performed on the predictions from the 5 selected trajectories to produce the final result. The overall performance is then reported in Table 1 as the mean of the results across the 10 runs. We balanced the testing dataset to ensure that each diagnostic class is represented by an equal number of samples. Consequently, the micro-averaged F1 score, precision, and recall are equivalent to the accuracy reported in the table. We opted to use micro-averaged metrics in our clinical evaluation, because they appropriately balance the importance of different stages of skin cancer, which is crucial for assessing the overall reliability and effectiveness of the diagnostic tool.\nWe compared Pathfinder to four state-of-the-art baseline models: 1) three transformer-based models all utilizing the ScAtNet architecture and 2) four MIL-based model, using ABMIL with different backbones. ScAtNet utilizes a MobileNetV2 backbone to extract multi-scale features from images at 7.5x, 10x, and 12.5x magnification. For the first baseline model, these feature vectors are subsequently fed into ScATNet which aggregates information of the three scales to perform the diagnostic task using Transformer blocks. The second approach augments the WSI with ROI heatmaps generated by the U-Net model, appending these maps as a fourth input channel and using ScAtNet for classification. The third baseline model, SAG, converts diagnostically relevant entities into attention signals, integrating these with ScAtNet and employing an attention-guiding loss function that combines heuristic guidance (HG) and tissue guidance (TG) based on disease-specific prior knowledge such as tissue, structure, and cellular information. In addition to the original ABMIL model, we extended our evaluation by incorporating three additional ABMIL variants, each using a different pathology-specific foundation model as a backbone: CONCH, UNI2-h, and QuiltNet. ABMIL aggregates information across instances using an attention mechanism that assigns weights to each instance, allowing the model to capture its contribution to the final bag label in a permutation-invariant manner.\nThen, we conducted comprehensive experiments to evaluate PathFinder by examining different architectures for each agent component, achieving 74% accuracy that surpasses both human experts (65%) and previous state of the art (66% best). Our evaluation focused on three main aspects:\nNavigator Architectures. First, to quantify the importance of Description Agent feedback, we tested a Visual-Only Navigator that employs weighted probabilistic sampling for patch selection without iterative feedback in a single pass over the WSI. Additionally, we implemented Imitated Sampling, which leverages pathologists' viewing pattern distributions (viewport width, height, and zoom level) from our M-Path dataset (Section 3) to statistically sample patches as important WSI regions. If pathologists spend more time focusing on a region, we gave a higher chance of sampling to that region. Both Imitated Sampling and Vision-Only Navigator performed similarly (64% and 63% respectively), indicating that both pure statistical and learned sampling, regardless of source, has limited effectiveness. To further assess the necessity of our iterative navigation approach, we added a non-iterative baseline, selecting the top 10 patches from ABMIL attention scores using three different pathology-specific foundation model backbones. The best model with CONCH backbone performed substantially lower than our best navigation-based approach (74% vs 54%). This confirms that a purely image-based, non-iterative selection approach is insufficient, as it lacks the ability to iteratively refine patch selection based on evolving textual descriptions. Furthermore, we evaluated text-conditioned visual navigators using either CLIP-based or T5-based text encoders. The T5-based navigator significantly outperformed its CLIP-based counterpart (74% vs 62%), suggesting CLIP's 77-token limit constrains its ability to effectively process multiple descriptions (we simply truncate descriptions exceeding 77 tokens, then average if a description is long). Finally, our navigation-based approach (74%) outperformed exhaustive search (68%), which utilizes all non-background patches of the WSI, suggesting that selective patch sampling helps avoid confusion from irrelevant regions.\nDescription Agents. We compared a fine-tuned version of Quilt-LLaVA (optimized for concise descriptions) against off-the-shelf LLaVA-Med. The fine-tuned version showed"}, {"title": "6. Discussion", "content": "This study presents PathFinder, a multi-modal, multi-agent AI framework designed to emulate the multi-scale, iterative diagnostic approach of expert pathologists for histopathology whole slide images (WSIs). By integrating Triage, Navigation, Description, and Diagnosis Agents, PathFinder collaboratively gathers evidence to deliver accurate, interpretable diagnoses with natural language explanations. Notably, it surpasses state-of-the-art methods and the average performance of human experts in melanoma diagnosis, setting a new benchmark in AI-driven pathology.\nPathFinder has the potential to accelerate diagnostic workflows, reducing the reliance on manual examination and enabling timely patient care in clinical settings. Its natural language descriptions provide interpretability, facilitating the validation of AI-generated diagnoses by pathologists. Moreover, its integration of vision-language models (VLMs) and large language models (LLMs) highlights the promise of multi-modal AI in delivering scalable, specialized diagnostic tools that could improve access to pathology expertise.\nLimitations. Despite its strengths, PathFinder has limitations. The framework relies on pre-existing datasets and significant computational resources, posing challenges in resource-constrained environments. Additionally, the complexity of the Navigation Agent's decision-making process and occasional hallucinations by the Description Agent could affect transparency and accuracy of the decision-making process. Future work should address these issues by enhancing dataset diversity, computational efficiency, and patch selection strategies, further advancing PathFinder's potential as a transformative tool in AI-assisted pathology."}]}