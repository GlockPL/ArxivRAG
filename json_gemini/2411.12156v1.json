{"title": "HNCSE: Advancing Sentence Embeddings via Hybrid Contrastive Learning with Hard Negatives", "authors": ["Wenxiao Liu", "Zihong Yang", "Chaozhuo Li", "Zijin Hong", "Jianfeng Ma", "Zhiquan Liu", "Litian Zhang", "Feiran Huang"], "abstract": "Unsupervised sentence representation learning remains a critical challenge in modern natural language processing (NLP) research. Recently, contrastive learning techniques have achieved significant success in addressing this issue by effectively capturing textual semantics. Many such approaches prioritize the optimization using negative samples. In fields such as computer vision, hard negative samples (samples that are close to the decision boundary and thus more difficult to distinguish) have been shown to enhance representation learning. However, adapting hard negatives to contrastive sentence learning is complex due to the intricate syntactic and semantic details of text. To address this problem, we propose HNCSE, a novel contrastive learning framework that extends the leading SimCSE approach. The hallmark of HNCSE is its innovative use of hard negative samples to enhance the learning of both positive and negative samples, thereby achieving a deeper semantic understanding. Empirical tests on semantic textual similarity and transfer task datasets validate the superiority of HNCSE.", "sections": [{"title": "Introduction", "content": "Sentence representation learning (SRL), or sentence embedding, is a crucial subfield of natural language processing (NLP) that involves encoding sentences into low-dimensional vectors to capture their semantic content. The essence of learning sentence representations is to grasp the full linguistic context, transcending mere word meanings. Sentence representation learning contributes to facilitating a myriad of applications, including information retrieval Liu et al. (2015), conversational AI You et al. (2021), and machine translation systems Zhang et al. (2021).\nConventional endeavors generally follow the supervised learning paradigm Jordan and Rumelhart (2013). However, such supervised learning methods suffer from the substantial cost associated with gathering sufficient training data. Recently, unsupervised sentence learning aims to enhance efficiency and cost-effectiveness in training without sacrificing the accuracy or efficacy of the resulting sentence representations. Unsupervised sentence representation learning refers to training models to understand the semantic content of sentences without explicit human-labeled guidance, often using methods like auto-encoding, predicting surrounding context, or self-supervision techniques such as contrastive learning Gao et al. (2021). At present, the mainstream methods of unsupervised sentence representation learning follow the idea of contrastive learning, that is, to train the sentence representation model by maximizing the similarity between positive sample pairs and minimizing the similarity between negative sample pairs.\nThe core of contrastive learning-based SRL lies in how to construct effective positive samples and negative samples. Positive samples refer to the sentence pairs that are semantically similar or identical, while negative samples denote the sentence pairs that are semantically unrelated or distinct. Under unsupervised learning conditions, labeled information is not available. Therefore, we need to design some heuristic methods to generate correct positive and negative samples. A popular strategy is to use the same sentence to generate positive samples through different data enhancement (such as random replacement, deletion, insertion of words, etc. Chuang et al. (2022)), and randomly select other sentences as the negative samples. However, such strategies are not a panacea to solve the unsupervised SRL task due to the following reasons. Firstly, the positive samples and negative samples are generated independently, without considering the semantic distance between them. Such a weak constraint may blur the boundary between positive and negative samples, leading to a dilemma such as the similarity between positive samples is lower than the one between negative samples Yang et al. (2022). Secondly, model performance is seriously affected by the number and distribution of positive and negative samples. If the distributions of positive/negative samples are uneven, SRL models tend to learn biased or unstable sentence representations. Meanwhile, the mainstream unsupervised SRL methods (e.g., SimCSE Gao et al. (2021)) adopt the method of in-batch negative sampling. Although in-batch negative sampling enjoys the advantage of computing resource efficiency, it still suffers from severe limitations such as relying on batch size, a limited number of negative samples, and diversity Li et al. (2021).\nThis paper targets the acquisition of superior, correlated positive-negative sample pairs to enhance SRL. A major hurdle is managing confusing, over-hard, and over-easy negative samples effectively. Confusing samples, often a byproduct of data augmentation like in SimCSE, can lead to divergent embeddings for positive pairs, resulting in subpar learning samples. Over-hard negatives, due to their high similarity to the query, may cause learning distortion. Conversely, over-easy negatives, marked by their extreme dissimilarity, can lead to an unchallenging learning process, inhibiting the acquisition of meaningful representations. Figure 1 illustrates an example of a dense embedded distribution and the issue of missing hard negatives. Addressing these issues highlights the necessity for careful construction of positive/negative samples, particularly hard negatives that balance difficulty and simplicity. This balance is crucial for a well-tuned and effective learning path. However, identifying these challenging negatives remains a significant challenge due to computational and storage demands.\nIn this paper, we introduce an innovative unsupervised comparative learning framework that leverages hard negative sample mixing to mitigate the adverse effects of confusing samples and enrich the reliability of hard negatives within the same batch. Contrasting prior research that solely targets negative samples, our approach underscores the significance of positive samples in unsupervised SRL, fostering a synergistic learning process between positive and negative samples. Our framework not only harnesses hard negative information to enhance the quality of positive samples but also generates additional hard negatives through the mixing of existing samples. Furthermore, it efficiently expands the negative sample pool, facilitating convergence in alignment characteristics. Extensive evaluations across various tasks have confirmed the superior performance of our proposal against SOTA benchmarks. In summary, our key contributions are as follows:\n\u2022 We propose the HNCSE framework, which is divided into HNCSE-PM and HNCSE-HNM, both extended based on SimCSE. HNCSE-PM constructs positive samples closer to the query through the hardest negative sample. HNCSE-HNM uses mixup on existing hard negative samples to obtain higher quality hard negative samples.\n\u2022 The theoretical analysis of Hard Negative Mixing (HNM) has deeply elucidated its profound impact on the enhancement of sentence representation learning.\n\u2022 Through extensive experiments, HNCSE has achieved promising improvements over Sim-CSE in terms of semantic textual similarity (STS) and transfer tasks (TR). Additionally, HNCSE demonstrates clear advantages over current popular large language models (LLMs) in STS tasks."}, {"title": "Related Work", "content": "In recent times, the academic community increasingly focuses on unsupervised sentence representation learning. Traditional approaches, such as those demonstrated by Arora et al.Arora et al. (2017) and Ethayarajh Ethayarajh (2019), generate sentence embeddings through a weighted average of word embeddings. Kiros et al.Kiros et al. (2015) introduce the SkipThought model, which applies the skip-gram model at the sentence level, using the encoded sentence to predict its adjacent sentences. Many researchers, like Qiao et al Qiao et al. (2019), are adopting outputs from pre-trained language models like BERT for sentence embedding. However, Ethayarajh Ethayarajh (2019) observes that directly using BERT for this task yields suboptimal results. Li et al Li et al. (2022) note that BERT in-duces anisotropy in the sentence embedding space, adversely impacting the performance on Semantic Text Similarity (STS) tasks.\nTo address this, Li et al Li et al. (2022) introduce BERT-flow, a method that utilizes a flow-based technique, as described by Dinh, Krueger, and Bengio Dinh et al. (2014). This technique transforms the anisotropic sentence embedding distribution into a more uniform, isotropic Gaussian distribution, aiming to improve performance on STS tasks. Concurrently, Su et al Su et al. (2023) present the BERT-whitening approach. This method leverages traditional whitening techniques to produce a more consistent sentence embedding distribution. It not only matches the performance of BERT-flow but also reduces the dimensionality of the sentence embeddings, enhancing computational efficiency."}, {"title": "Methodology", "content": "The HNCSE model, as outlined in the literature, pioneers a novel training approach that leverages hard negative mixing to refine sentence embeddings. It rectifies misclassified positives by incorporating hard negative traits, thereby boosting the model's discriminative capacity. HNCSE then broadens the challenge set by augmenting existing hard negatives, fostering a more robust learning context that advances sentence representation learning. Following this, we will elucidate the foundation of our base model, SimCSE Gao et al. (2021).\nThe core idea of unsupervised SimCSE is to use a contrastive learning objective to train sentence embeddings without any labeled data. The contrastive learning objective is based on the principle of maximizing the agreement between two different views of the same sentence, while minimizing the agreement between different sentences. The two views are obtained by applying dropout to the input sentence and encoding it with a pre-trained transformer model. The contrastive learning objective can be formulated as follows:\n$L^{(0)} = -\\frac{1}{N} \\sum_{i=1}^{N} log \\frac{exp (sim (f_\\theta(s_i), f_\\theta(\\tilde{s_i})) /T)}{\\sum_{j=1}^{N} exp (sim (f_\\theta(s_i), f_\\theta(\\tilde{s_j})) /T)}$\nIn Equation (2), $s_i^+$ denotes the positive example, while $s_i^-$ stands for the most similar negative instance. $s_i^{\\pm}$ refers to the generated positive instance after the mixup of both positive and negative instances. $d_1$ signifies the similarity between the query and the positive instance, and $d_2$ refers to the similarity between the query and the hardest negative. $\\omega_1$ and $\\omega_2$ respectively represent the weights of $s_i^+$ and $s_i^-$.\n\u2022 Contrastive Loss. InfoNCE is adopted as the contrastive loss, which represents the model capability to estimate the mutual information. Optimizing InfoNCE loss maximizes the mutual information between positive samples and minimizes the mutual information between negative samples as:"}, {"title": "Positive Mixing", "content": "In text-based contrastive learning, \"hard negatives\" are exemplars that the model nearly misclassifies as positive during training. These negatives are erroneously deemed highly similar to positives.\nIntegrating hard negatives with positives offers unique advantages. It mirrors real-world scenarios where category distinctions can be ambiguous, training the model to navigate such complexities. This refines the model's class boundaries, enhancing its ability to discern between subtly distinct instances. It also mitigates overfitting by diverting the model's focus from only the clear and easy examples.\nSimCSE's positive sampling involves feeding the same sentence into the model twice to yield two distinct eigenvectors as positives, facilitated by dropout layers. Despite dropout, semantically distinct sentences may still produce similar positive samples due to random masking.\nFor example, as shown in Figure 2, given two input sentences \"Two dogs are running\" standing for Sentence1 and \"Two dogs are walking\" standing for Sentence2, after dropout in SimCSE's unsupervised mode, two pairs of query and positive can be achieved, which are respectively shown as $(Q_1, P_1)$ and $(Q_2, P_2)$. The similarity between $Q_1$ and $P_2$ might be even larger that the one between $Q_1$ and $P_1$, which is obviously incorrect.\nIn order to shorten the distance between the query and the positive while increasing the distance between the positive and the negative, we compute the similarity between the query and the positive as well as the hardest negative, denoted as $d_1$ and $d_2$, respectively:\n$\\begin{cases}s_i^{\\pm} = \\omega_1 s_i^+ + \\omega_2 s_i^-, & 0 < \\frac{d_1}{d_2} < 0.1 \\\\d_1 - d_2\\\\s_i^{\\pm} = \\frac{\\omega_1 s_i^+ + \\omega_2 s_i^-}{d_1 - d_2},& \\frac{d_1}{d_2} > 0.1\\\\ & d_1 < d_2\\end{cases}$"}, {"title": "Hard Negative Mixing", "content": "Consider a scenario in which the embedding of a given query, denoted as q, and the embedding of a corresponding key in the matched pair, denoted as k, is contrasted with every feature n in the bank of negatives (N).\nThe popular loss function for contrastive learning Chen et al. (2020); He et al. (2020) is:\n$L_{q, k, N} = - log \\frac{exp(q^Tk/\\tau)}{exp(q^Tk/\\tau) + \\sum_{n\\in N} exp(q^Tn/\\tau)}$\nwhere T assumes the role of a temperature parameter, and it is worth noting that all embeddings undergo 12-normalization.\nThe query and key are two distinct augmentations of an identical sentence. The negative bank, denoted as N, comprises instances that act as negatives for each positive pair. Specifically, it can be defined as encompassing the remaining texts within the present minibatch Chen et al. (2020).\nThe logarithmic likelihood function formulated in Eq (4) operates within the bounds of a probability distribution constructed via the application of the softmax function to each input/query q. If we let $P_{f_i}$ denote the probability of match between the query and the feature $f_i \\in F = N \\cup k$, then the derivative of the loss concerning the query q, with respect to the gradient, can be expressed:\n$\\frac{\\partial L_{q, k, N}}{\\partial q} = \\frac{1}{\\tau} ((1 - p_k)k - \\sum_{n \\in N} p_n n)$\nwhere $p_{f_i} = \\frac{exp(q^Tf_i/T)}{\\sum_{j \\in F}exp(q^Tf_j/T)}$\n$p_k$ and $p_n$ symbolize the likelihood of a match pertaining to the key and the negative feature, denoted as $f_i = k$ and $f_i = n$ respectively. It is discernible that the impacts of the positive and negative logits upon the loss function mirror those encountered in a (K + 1)-way cross-entropy classification loss."}, {"title": "Hard Negative Mixing for Contrastive Learning", "content": "In contrastive learning, particularly within unsupervised learning, the role of hard negative samples is pivotal for effective model training. The standard practice of using batch negatives often falls short, as they may not offer a sufficient challenge or provide the necessary contrast with positive examples, as shown by recent studies Kalantidis et al. (2020). To address this, Hard Negative Mining has been introduced to enhance the learning process.\nHard Negative Mining involves deliberately selecting negative samples that are more challenging and closely resemble positive samples in the feature space. This method is more effective in teaching the model to distinguish between similar-looking but distinct categories, thereby improving the model's robustness and discriminative capabilities, as evidenced by recent advancements.\nIn response to the need for more effective hard negatives, we have developed an approach inspired by Kalantidis et al. (2020) to further enhance the model's generalization capabilities. Our strategy innovatively applies the mixup technique to generate more relevant and challenging hard negative samples, thus refining the learning process.\nAs shown in Figure 3, this approach begins by identifying a set of top k vectors, denoted as $G = \\{G_1, G_2, ..., G_m\\}$, which closely resemble a given input query $x_i$ from a previous minibatch. These vectors are then utilized as the foundation for crafting new hard negative samples. We employ a method of linear combination to produce m novel hard negatives. These are subsequently incorporated into our existing negative sample bank (denoted as N in equation 4). The process of generating each new hard negative instance, $u_o$, is methodically formulated as follows:\n$\\tilde{u_o} = \\frac{u_o}{||u_o||_2}$ where $u_o = \\alpha G_i + (1 - \\alpha) G_j$\nHere, $u_o$(where $o \\in \\{1, ..., m\\}$) represents each new hard negative instance. The indices (i, j) are randomly selected from the set of top k vectors, and the coefficient $\\alpha$ helps balance the contribution of each vector in the mixup.\nExpanding further, we introduce an adaptive selection process for $\\alpha$, which varies based on the similarity between the vectors $G_i$ and $G_j$. This approach ensures that the resulting hard negative is neither too similar nor too dissimilar to the query, maintaining an optimal level of difficulty. The adaptive selection is defined as:\n$\\alpha = f(sim(G_i, G_j))$\nwhere f is a function that maps the similarity score to an appropriate value for $\\alpha$. The similarity function sim(,) could be a cosine similarity.\nThis section outlines the Hard Negative Mixing strategy to optimize sentence representation learning. By crafting challenging negative samples, this approach enhances the model's semantic relationship recognition. The efficacy of this method is demonstrated through experiments detailed in the subsequent chapter, showcasing performance gains and comparisons with other methods.\nThe Appendix A provides a comprehensive mathematical analysis of the Hard Negative Mixing strategy. It defines hard negatives, explains the generation of challenging negatives through linear combinations, and discusses the impact on the model's optimization and loss function adjustments. The analysis elucidates how this method enhances model performance by increasing sample distinctiveness and improving discriminative ability. It also addresses the role of regularization and learning rate adjustments in stabilizing and optimizing the learning process. This theoretical framework supports the experimental outcomes and offers guidance for applying the strategy effectively."}, {"title": "Experiments", "content": "We executed experiments across seven established semantic text similarity (STS) benchmarks, spanning STS 2012\u20132016 as delineated by Agirre et al. from 2012 to 2016, in conjunction with the STSBenchmark Cer et al. (2017) and SICKRelatedness Wijnholds and Moortgat (2021). Our model's performance was assessed using the SentEval toolkit, and the Spearman's correlation was chosen as the evaluation metric. Our methodology initiated by leveraging pre-existing BERT checkpoints. The [CLS] token embedding, extracted from the model's output, served as the sentence representation. Beyond semantic similarity evaluations, our model was subjected to seven distinct transfer learning tasks to gauge its capacity for generalization."}, {"title": "Implementation Details", "content": "In the methodology outlined by Gao, Yao, and Chen Gao et al. (2021), we adopted an identical training dataset and protocol. This dataset encompasses a million sentences, sourced from Wikipedia. Utilizing a BERT model that's been finely optimized Devlin et al. (2018), we derive an embedding for each of these sentences. Subsequent to this, we employ distinct dropout masks to produce augmented variants of the aforementioned sentence embeddings. All models are trained over a single epoch, with a batch size set at 64. Our computational implementation is founded on Python version 3.8.13, leveraging the capabilities of Pytorch version 1.12.1. All experimental tasks were executed on an NVIDIA GeForce RTX 3090 GPU equipped with 24G memory."}, {"title": "Main Results", "content": "In the study by Zhang et al. (2022), we benchmarked the newly introduced HNCSE against a range of conventional unsupervised approaches and the prevailing state-of-the-art contrastive learning technique for the text semantic similarity (STS) task. This comparison encompasses methods such as average GloVe embeddings Pennington et al. (2014), mean embeddings from BERT or ROBERTa,\nAs delineated in Table 1, the peak performances achieved by our two variant models of HNCSE are 78.38% and 78.27% respectively. This clearly surpasses the unsupervised SimCSE employing a BERT-base architecture. Specifically, HNCSE exhibits superior performance to SimCSE across STS2012, STS2013, STS2014, STS2015, STS2016, SICK-R and STS-B benchmarks. Moreover, the more robust iterations of our HNCSE, leveraging the BERT-large architecture, consistently outshine the corresponding large model of SimCSE on the majority of STS tasks."}, {"title": "Large language models are used for the STS Task", "content": "The growing interest in open-source large language models (LLMs) like LLaMA has highlighted their use in unsupervised sentence similarity tasks (STS). However, studies Li and Li (2023); Zhang et al. (2024) have shown that these models' performance in STS has not been up to par. The latest results for LLaMA2-7B and its three prompt engineering strategies\u2014PromptEOL, Pretended Chain of Thought (CoT), and Knowledge Enhancement-have demonstrated a shortfall when compared to the methods introduced by HNCSE.\nThe architectures and training objectives of LLaMA2-7B and HNCSE have notably diverged. LLaMA2-7B, an autoregressive model, has been optimized for text generation, while HNCSE has been trained using contrastive learning for sentence-level representation. This divergence may have reduced the efficacy of LLaMA2-7B in sentence similarity tasks compared to HNCSE. Additionally, prompt engineering for LLaMA2-7B has faced limitations: (1) it heavily relies on carefully crafted templates requiring extensive optimization across models and tasks, increasing complexity in research and application; (2) these methods, designed primarily for generative models, show limited effectiveness for discriminative models, limiting their applicability; (3) as prompts become longer and more complex, the computational resource demand escalates, especially with large-scale datasets, highlighting resource consumption. In contrast, HNCSE's contrastive learning has eliminated the need for complex prompt designs, ensuring stable application across various models and tasks, and has shown improved efficiency in resource consumption."}, {"title": "Transfer Task", "content": "In the comprehensive evaluation of HNCSE's performance using the seven transfer tasks from SentEval Conneau and Kiela (2018), the results delineated in Table 2 are particularly revealing. Notably, both the Base and Large configurations of HNCSE surpass SimCSE in six out of the seven benchmark datasets within the TR task. Importantly, a remarkable enhancement in performance is distinctly observed for the MRPC dataset."}, {"title": "Ablation Study", "content": "To assess model components' impact, we run ablation studies using two model variants, HNCSE-PMsingle and HNCSE-HNMsingle, with identical setups and hyperparameters as the main experiment."}, {"title": "Analysis of Batch Size and Max Sequence Length", "content": "In our HNCSE model analysis, we've assessed the influence of batch size and max sequence length on model performance. Figure 4(a) demonstrates that optimal performance for both HNCSE-PM and HNCSE-HNM has been achieved with a batch size of 64, regardless of the number of hard negatives. The plateau in performance with larger batch sizes suggests that beyond a certain point, increased size no longer provides additional valuable learning information, underscoring the importance of hard negatives in mini-batches. Figure 4(b) shows the highest model value at a maximum sequence length of 32. Yet, model performance does not improve with increased sequence length. This may be due to longer sequences introducing greater ambiguities or complexities in semantic relationships, making it more challenging for the model to disambiguate and accurately capture nuances, thus reducing performance."}, {"title": "Conclusion and Limitation", "content": "In this work, we introduce a novel framework, HNCSE, specifically designed to address the multi-faceted challenges posed by the incorporation of hard negative samples in sentence representation learning. Unlike conventional approaches, HNCSE innovatively integrates identified hard negative samples to optimize positive samples and construct even harder negative samples, thereby extending the well-established SimCSE methodology. This approach facilitates a deeper and more nuanced understanding of negative samples. Empirical evaluations of HNCSE on semantic textual similarity datasets and transfer task datasets demonstrate its superiority, indicating that unsupervised learning of sentence representations can make promising advancements to existing research.\nDespite its strong performance, we have identified certain limitations associated with HNCSE. Firstly, HNCSE relies on large amounts of unannotated text data. If this data contains substantial noise or irrelevant information, it may adversely affect the quality of the model's embeddings. Secondly, HNCSE lacks explicit contextual information during training, potentially impeding its ability to capture long-distance dependencies. These issues present challenges for future research."}, {"title": "Theoretical Analysis of Hard Negative Mixing", "content": "Given a dataset of sentences S = {81, 82, ..., sn}, with a sentence embedding function f : S \u2192 Rd mapping sentences to a d-dimensional vector space, Hard Negative Mixing (HNM) aims to refine the representation learning process by leveraging challenging negative samples."}, {"title": "Definition of Hard Negatives", "content": "Hard negatives are crucial for the effective training of models, especially in tasks that require fine-grained discrimination between semantically close but distinct sentences. A hard negative for a given sentence si is defined as a sentence sj that, while semantically different, lies close in the embedding space. This closeness poses a greater challenge for the model to accurately discriminate:\nHN(si) = {sj|sim(f(si), f(sj)) > @,\u2200sj \u2208 S \\{$i\\}} $\nwhere HN(si) denotes the set of hard negatives for the sentence si, sim(\u00b7) is a similarity measure (cosine similarity), f(\u00b7) represents the sentence embedding function mapping sentences to points in a high-dimensional vector space, S is the set of all sentences, and @ is a predefined threshold indicating the minimum similarity for a sentence to be considered a hard negative."}, {"title": "Mixing Strategy", "content": "Building upon the foundational concept of hard negatives, as delineated in the preceding section, we introduce a novel approach to further enrich the training dataset and amplify the model's discriminative power. By strategically leveraging the defined hard negatives, we propose a method to generate synthetic samples that embody the nuances and complexities encountered in real-world scenarios. This method, termed Hard Negative Mixing (HNM), involves the innovative creation of synthetic hard negatives through the blending of embeddings from identified hard negative pairs. This strategy not only augments the diversity of training data but also introduces a heightened level of challenge, compelling the model to refine its understanding and representation of semantic distinctions.\nTo enhance the diversity and challenge of the training process, we employ a strategy of generating synthetic hard negatives. This is achieved by taking a linear combination of two existing hard negatives, hni and hnj, associated with the same anchor sentence:\nhnmix = $$\\lambda hni + (1 \u2212 \\lambda)hnj$$\nwhere \u03bb is a mixing coefficient that lies in the range [0, 1]. This coefficient determines the relative contribution of each hard negative to the synthetic sample. By adjusting \u5165, we can control the difficulty and variability of the generated hard negatives, enriching the training data and encouraging the model to learn more discriminative features."}, {"title": "Optimization Objective", "content": "The conceptual groundwork laid by the identification of hard negatives and the subsequent generation of synthetic hard negatives through our Hard Negative Mixing strategy paves the way for a refined training objective. This advanced stage of model training seeks not only to utilize these synthetic samples effectively but also to optimize the model's sensitivity towards differentiating between semantically similar and dissimilar sentences. By intricately balancing the influences of positive samples and synthetic hard negatives, we aim to craft an optimization objective that embodies the complexity of natural language, enhancing the model's ability to discern subtle semantic nuances.\nThe primary goal of incorporating HNM into the training process is to optimize the model's ability to distinguish between positive and negative samples. The optimization objective is formulated to minimize the distance between positive pairs while maximizing the distance between the anchor sentence and its synthetic hard negatives. This is quantified by the following loss function:\nmin$\\frac{$\\frac{$exp(sim(f(si), f(st))/T)}{exp(sim(f(si), f(s+))/T) + \\sum_{hnmix\\in HNM(s_i)}$ \\frac{exp(sim(f(si), f(hnmix))/T)}$}\nIn this equation, f(\u00b7) denotes the embedding function mapping sentences to a high-dimensional vector space. The term sim(\u00b7) represents a similarity measure (e.g., cosine similarity) between two embeddings. The variable st signifies a positive sample that is semantically similar to the"}, {"title": "Optimization Objective", "content": "In the realm of representation learning, particularly within the framework of HNM, the essence of refining the model's capacity to distinguish between semantically proximate yet distinct sentences is encapsulated in the formulation of a meticulously crafted optimization objective. This advanced stage of model refinement leverages both the synthetic hard negatives generated through HNM and the positive samples to formulate a loss function that embodies the intricate dynamics of natural language understanding. The overarching goal is to optimize the model in such a way that it achieves a heightened sensitivity to the subtle semantic nuances that differentiate positive pairs from their hard negative counterparts.\nTo articulate this concept more concretely, we introduce a loss function designed to minimize the distance between positive sentence pairs while simultaneously maximizing the distance between each sentence and its associated synthetic hard negatives. This is achieved through a contrastive learning approach, where the model is encouraged to align closely with positive samples in the embedding space and diverge from the synthetic hard negatives. The loss function is formulated as follows:\nmin$\\frac{$\\frac{$exp(sim(f(si), f(st))/T)}{exp(sim(f(si), f(s+))/T) + \\sum_{hnmix\\in HNM(s_i)}$ \\frac{exp(sim(f(si), f(hnmix))/T)}$}\nIn this equation, f(\u00b7) denotes the embedding function that maps sentences to a d-dimensional vector space. The function sim(\u00b7) represents a measure of similarity (e.g., cosine similarity) between the embeddings of two sentences. The term st refers to a positive sample that is semantically similar to si, and hnmix denotes a synthetic hard negative sample generated through the HNM process for the sentence si. The parameter T, known as the temperature, serves to scale the similarity scores, controlling the sharpness of the softmax distribution utilized in the denominator.\nThe numerator of the fraction within the logarithm focuses on the similarity between the sentence si and its positive counterpart st, encouraging the model to learn embeddings that bring these pairs closer together in the embedding space. Conversely, the denominator aggregates the similarities between si and its synthetic hard negatives hnmix, alongside the similarity to the positive pair, thereby promoting the model to differentiate si from its hard negatives effectively."}, {"title": "Analyzing Impact on Embedding Space", "content": "Following the in-depth exploration in the \"Optimization Objective\" section, where we discussed optimizing the model to distinguish between positive samples and synthetic hard negatives through a carefully designed loss function, this process not only enhances the model's discriminative capabilities but also suggests a significant impact on the structure of the embedding space. Therefore, our subsequent analysis will focus on the specific effects of the HNMstrategy on the embedding space, particularly on how it alters the average pairwise distance between embeddings, thereby facilitating a more distinct separation of semantic meanings.\nThe structural dynamics of the embedding space are crucial for the effective representation of semantic relationships among sentences. The introduction of mixed hard negatives via HNM alters this structure, potentially leading to a more separable space where similar and dissimilar sentences"}, {"title": "Enhancement of Model Discriminability", "content": "Following the detailed exploration of HNMstrategies and their profound impact on the embedding space, our focus shifts towards quantitatively assessing the improvements in model discriminabil-ity. The synthetic hard negatives generated through HNM pose refined challenges, necessitating an advanced model capability to distinguish between semantically similar yet distinct sentences. This subsequent section delves into the empirical evaluation of model performance enhancements, highlighting the pivotal role of HNM in advancing the nuanced discernment capabilities of sentence embeddings.\nOne of the primary goals of HNM is to enhance the model's ability to discriminate between se-mantically similar and dissimilar sentences, which is pivotal for tasks such as semantic similarity measurement and classification. This enhancement can be theoretically represented by an increase in model accuracy, measured as follows:\nA' - A = $\\sum_{i=1}^{n}$\\\\min$\\frac{sim(f'(si), f'(s+)) - sim(f'(si), f'(hnmix))}{sim(f(si), f(s) - sim(f(si), f(hnmix))}$$\nIn this formulation, A and A' represent the model's accuracy before and after the application of HNM, respectively. The set T encompasses the triplets formed by an anchor sentence si, its positive counterpart st, and the synthetic hard negative hnmiz, generated through HNM. The function [[[\u00b7] denotes the indicator function, yielding a value of 1 when the condition within the brackets is true, and 0 otherwise. The term sim(f(si), f(sj)) measures the similarity between the embeddings of sentences si and sj, facilitated by the embedding function f(\u00b7), which is refined to f'(\u00b7) post-HNM application.\nThe accuracy measure thus reflects the differential impact of HNM on the model's performance, quantified through the comparison of similarity scores between positive and hard negative pairs, before and after the integration of HNM. This metric directly illuminates the enhanced discriminative capability of the model, as a result of the nuanced challenges introduced by synthetic hard negatives, fostering a deeper understanding and distinction between closely related sentences."}, {"title": "Impact on Learning Dynamics", "content": "After elucidating the role of HNMin enhancing model discriminability and its substantial effects on the embedding space, it becomes imperative to examine how these strategies influence the learning dynamics of the model. The adaptive challenges introduced by synthetic hard negatives necessitate a deeper understanding of the underlying adjustments in the model's optimization process. This analysis is critical for optimizing the training strategy and ensuring that the model effectively leverages the nuanced complexities introduced by HNM, thereby achieving a sophisticated balance between accuracy and generalization.\nThe introduction of HNM into the training process modifies the learning dynamics by influencing the gradient of the loss function with respect to the embeddings. This section delves into the mathematical underpinnings of how HNM alters the embedding updates during training, providing insights into the optimization trajectory and the embedding space's evolution."}, {"title": "Gradient of Loss Function", "content": "The loss function's gradient with respect to the embeddings is a critical factor in the model's learning process, dictating how the embeddings are updated in each iteration. Given the loss function L, the gradient with respect to the embedding of a sentence si can be expressed as:\n$\\frac{\\partial L}{\\partial f(si)} = (1-p_k)\\frac{k}{T}  - \\sum_{n\\in N} \\frac{p_n n}{T}$\nwhere:\n\u2022 f(si) denotes the embedding of sentence si.\n\u2022 T is the temperature parameter scaling the similarity scores.\n\u2022 $p_k$ and $p_n$ represent the softmax probabilities of the positive and negative samples, respectively.\n\u2022 k is the embedding of the positive sample, and n denotes the embeddings of the negative samples in the set N.\nThis formulation encapsulates the essence of contrastive learning, emphasizing the model's aim to pull positive pairs closer while pushing negative pairs apart in the embedding space.\nHNM influences the learning dynamics by adjusting the distribution and characteristics of negative samples, thereby affecting the gradient of the loss function. The modification of the gradient can lead to more effective learning, especially in distinguishing closely related but distinct sentences. This is achieved by incorporating a mix of hard negatives, which are closer to the query sentence in the embedding space, thereby providing a stronger learning signal for the model."}, {"title": "Balancing Diversity and Difficulty", "content": "In the context of the profound adjustments introduced by HNMin the learning dynamics and its pivotal role in enhancing model discriminability, the strategic balance between the diversity and difficulty of hard negatives emerges as a critical consideration. This balance is instrumental in ensuring that the synthetic hard negatives generated through HNM not only challenge the model but also contribute to a comprehensive representation of the semantic landscape. Addressing this balance enables the optimization of the model's performance by fine-tuning the difficulty level of training samples to match the model's learning stage, thereby facilitating a more nuanced understanding of complex semantic relationships.\nThe efficacy of"}]}