{"title": "Heterogeneity-Aware Coordination for Federated Learning via Stitching Pre-trained blocks", "authors": ["Shichen Zhan", "Yebo Wu", "Chunlin Tian", "Yan Zhao", "Li Li"], "abstract": "Federated learning (FL) coordinates multiple devices to collaboratively train a shared model while preserving data privacy. However, large memory footprint and high energy consumption during the training process excludes the low-end devices from contributing to the global model with their own data, which severely deteriorates the model performance in real-world scenarios. In this paper, we propose FedStitch, a hierarchical coordination framework for heterogeneous federated learning with pre-trained blocks. Unlike the traditional approaches that train the global model from scratch, for a new task, FedStitch composes the global model via stitching pre-trained blocks. Specifically, each participating client selects the most suitable block based on their local data from the candidate pool composed of blocks from pre-trained models. The server then aggregates the optimal block for stitching. This process iterates until a new stitched network is generated. Except for the new training paradigm, FedStitch consists of the following three core components: 1) an RL-weighted aggregator, and 2) a search space optimizer deployed on the server side, and 3) a local energy optimizer deployed on each participating client. The RL-weighted aggregator helps to select the right block in the non-IID scenario, while the search space optimizer continuously reduces the size of the candidate block pool during stitching. Meanwhile, the local energy optimizer is designed to minimize the energy consumption of each client while guaranteeing the overall training progress. The results demonstrate that compared to existing approaches, FedStitch improves the model accuracy up to 20.93%. At the same time, it achieves up to 8.12\u00d7 speedup, reduces the memory footprint up to 79.5%, and achieves 89.41% energy saving at most during the learning procedure.", "sections": [{"title": "I. INTRODUCTION", "content": "Federated learning (FL) enables large amount of clients to collaboratively train a global model while preserving data privacy, which has been widely used to support different kinds of applications [1]. In order to obtain high-quality analysis, recently developed DNNs are becoming deeper and wider [23]. At the same time, large memory space and high computing power are required within the training process. For example, training a VGG16 model consumes more than 15 GB of memory [24]. On the other hand, the available RAM for mobile devices only ranges from 4 to 16 GB [22], which means a large amount of low-end devices are excluded from participating the training process. Therefore, the excluded devices cannot make contribution to the global model with their own local data. The performance of the global model is then severely degraded. Meanwhile, on-device training is highly energy-demanding and badly hurts the battery lifetime of mobile devices. Thus, high resource consumption during the training process seriously impedes the deployment of FL in real-world scenarios.\nLimitation of Prior Arts. Several approaches have been proposed to deploy FL on resource-constrained devices. The existing methods can be mainly divided into the following two categories. One direction is to prune the channels/widths of CNN-based global models, forming different sizes of sub-models based on the memory budgets of the participating clients [4]-[6]. However, in this way, each client only sees parts of the global model, severely compromising the model architecture. Therefore, the aggregated global model has de-graded performance [7]. Another direction is to adjust the depth of a network, in other words, use a layer-level parti-tion. Nevertheless, for a sub-network with only initial global model layers, key parts responsible for deep information, like semantic features, are missing. Training it directly on a dataset poses a challenge to learning only corresponding parts of the global model, such as low-level features. During aggregation, mismatches in parameters may arise [8], leading to a decline in model performance. Thus, considering the memory and energy overhead, a new learning paradigm that can efficiently deploy FL on resource-constrained devices is critical for FL in real-world deployment.\nOur Design. In this work, we try to tackle this issue from a new perspective. With the emergence and improvement of an increasing number of open-source datasets, the pre-trained models are prevalent and readily available [12], [16]. Introduc-ing pre-trained models and fine-tuning them for downstream tasks provide opportunities to tackle the issue of resource limitation from the following perspectives. First, Fine-tuning produces lower computational, energy, and memory overhead compared to training models from scratch, as a result, more devices can contribute to the learning process with their"}, {"title": null, "content": "own local data. Second, leveraging the knowledge acquired on large public datasets can effectively compensate for the insufficient data on devices with resource constraints, further improving performance on local tasks. Thus, composing the global model for a specific task with the pre-trained models can be a promising way to surmount the resource limitation in FL. Nevertheless, despite the lower cost of fine-tuning on new tasks, it remains impractical in highly resource-constrained and disconnected environments for re-training the whole global network, especially for very large models, such as deep computer vision models. If only a portion of the global network is re-trained, the aggregated global model will exhibit particularly poor performance on downstream tasks, while also incurring substantial re-training costs. Furthermore, faced with an increasingly diverse set of downstream tasks, a single model trained on open-source datasets often lacks sufficient generalization ability for different tasks, further leading to performance degradation. To tackle these challenges, instead of relying on a single pre-trained model, we can harness mul-tiple pre-trained models with distinct network architectures, each offering varying expressive capabilities across different downstream tasks. However, how to leverage these pre-trained models is a new challenge. Simply selecting different models based on tasks fails to exploit the unique expressive capa-bilities of each model fully, and fine-tuning the models still incurs high training costs. Here, we introduce a new paradigm to address the above challenges: By splitting different pre-trained networks into multiple blocks, selecting part of blocks, and stitching blocks together, we create a stitched network on new task. Each part of the stitched network comes from a different pre-trained network, allowing us to utilize the divergent advantages of different pre-trained networks.\nIn this work, we propose FedStitch, a novel paradigm to address the resource limitation in FL. Specifically, we first partition the pre-trained model into blocks (each comprising one or more consecutive layers). Subsequently, on the local dataset, each participant client compares the compatibility between blocks from the block pool through a set of sim-ple forward inferences based on centered kernel alignment scores to select the optimal block. On the server side, the uploaded selected blocks are aggregated. FedStitch continually selects appropriate blocks and stitches them together until a completely new network is generated. The whole process only introduces a little inference overhead, significantly saving the computation and memory consumption related to training. Hence, FedStitch can replace the fine-tuning process on new tasks and is able to be deployed on most highly resource-constrained devices.\nChallenges and Techniques. However, designing such a new learning paradigm faces the following challenges.\n\u2022 Eliminate the impact of non-IID on block selection. The data distribution among clients in FL is highly biased. In this situation, using a simple aggregation method like FedAvg [1] to aggregate block may result in poor performance for the global stitched model. To address this challenge, we propose a reinforcement learning (RL)-"}, {"title": null, "content": "based weighted aggregator on the server to address the data heterogeneity in FL. With the help of the RL algorithm and cross-validation, the server selects and aggregates the right block in non-IID scenario.\n\u2022 Oversized search space in the block pool. Although FedStitch can efficiently generate a well-performance network in the downstream task, the huge block pool leads to an enormous block search space, increasing the aggregation time in each round. To reduce the search space during block selection, on the server side, we deploy a search space optimizer to continuously reduce the size of candidate block pool in each round, further reducing the computation and energy costs.\n\u2022 Suboptimal energy efficiency during local block selec-tion. On typical edge devices, the default DVFS governor often sets the highest frequency for local block selection. However, in FL's diverse system landscape, using the highest frequency speeds up inference but delays overall aggregation, waiting for slower updates. Addressing this, we introduce a client-side, feedback-based frequency con-figuration method: a local energy coordinator. It allows the server to set deadlines for each client per round. This coordinator predicts energy use across system settings, choosing the best one for each client to balance real-time response and minimal energy consumption.\nTo the best of our knowledge, we are the first work utilizing pre-trained models without the need for any training in FL. The experiments demonstrate that our approach requires no training overhead related to back-propagation, significantly re-ducing the memory and energy consumption on the device, and requires much less data compared to the traditional fine-tuning method. This enables the participation of clients with highly resource-constrained, while the generated neural network ex-hibits comparable performance to the state-of-the-art methods. Thanks to our design tailored for FL, the generated stitched network largely mitigates the impact of non-IID, concurrently accelerating the entire generation process significantly. The main contributions of this paper are summarized as follows:\n\u2022 We propose FedStitch, an FL framework that employs a new neural network generation method, splitting pre-trained models into blocks and stitching them together for downstream tasks. It avoids any training-related overhead, and each client requires only a few pieces of data.\n\u2022 We demonstrate the impact of non-IID on the generated neural network and analyze the reason. We design an RL-based weighted aggregation algorithm with cross-validation that significantly reduces the influence of non-IID and accelerates the entire process with an on-the-fly CKA-based search space reduction method.\n\u2022 For reducing energy consumption, we propose a feedback-based frequency configuration method to meet the real-time requirement while minimal energy cost.\n\u2022 Extensive experiments evaluate the effectiveness of Fed-Stitch on accuracy improvement, generation speed-up, and cost reduction for energy and memory."}, {"title": "II. RELATED WORK", "content": "A. FL on Resource-limited Devices\nRecent research on memory limitations in FL partitions the global model into local sub-models based on each client's memory budget. Approaches like HeteroFL [4] and FjORD [6] allow variability in model architecture across clients through diverse model widths/channels. Others, like InclusiveFL [9] and DepthFL [10], allocate models of different sizes to clients based on their on-device capabilities by adjusting the net-work's depth, while FEDEPTH [7] decomposes the full model into blocks and trains blocks sequentially to obtain a full global model. ProFL [25] divides the model into blocks, trains the front blocks, and safely freezes them after convergence. In summary, existing research primarily focuses on splitting the global model among users based on their hardware constraints. Regardless of the partitioning method, each client receives only a partial global model, with only a few high-memory clients getting the full model. Consequently, locally trained models fail to fully capture features and lack expressiveness. The final aggregated model also suffers from parameter mis-match issues [8], resulting in subpar performance.\nIn this work, our approach not only eliminates the need for training, thereby avoiding a huge amount of associated overhead, but also utilizes a full pre-trained model, whose knowledge acquired on public datasets can be leveraged on specific tasks. This allows the most of users to access the full global model in the generation process, and the final aggregation model also exhibits high performance for new tasks.\nB. Pre-trained Neural Network in Federated Learning\nPre-training is common in current deep learning to enhance model performance. However, integrating it with FL is nascent, with few studies focusing on it. References [19] find pre-training improves FL and narrows the accuracy gap vs. cen-tralized learning, notably with non-IID client data. Reference [17] uses pre-trained models for medical image segmentation, mitigating memory and communication overhead with knowl-edge distillation [18]. FedPCL [20] employs fixed pre-trained neural networks as backbones in FL, sharing updated class-wise prototypes for client-specific representations.\nAlthough the previous works related to pre-training play a certain role in addressing non-IID problem and reducing the computational overhead in FL, the generation of their global model still requires training, leading to significant memory overhead and energy consumption for clients. In contrast, FedStitch eliminates the need for training entirely, fundamentally addressing this issue."}, {"title": "III. FEDSTITCH: OVERALL LEARNING PARADIGM", "content": "Figure 1 represents the overall learning paradigm of Fed-Stitch, which can be mainly divided into the following main steps. In the initialization stage, given a set of models that are pre-trained on public datasets, we first split the models into blocks. We categorize blocks into three types: starting blocks, originating from the initial layers of pre-trained mod-els; intermediate blocks, which may exist multiple times within a single pre-trained model; and terminating blocks, referring to the classifier of the pre-trained model. Then, we distribute the pre-trained models and the pool of candidate blocks to all participating clients. \u2461 In each round, the participating clients receive the current stitched network and block pool. 3 Given a current network $N$, each client searches all candidate blocks in the block pool. Let's assume $B_{nl}$ is a candidate block, derived from a pre-trained neural network layer $n$ up to layer $l$. We stitch $N$, and $B_{nl}$ together to form a candidate stitched neural network. Next, we perform two neural network inference computations, one is that we pass a batch of data of the local target dataset $D$ to the candidate stitched network, resulting in the activation $X$. The other one is passing the same batch data $D$ to the pre-trained network where the candidate block $B_{nl}$ comes from. It stops at layer $l$, resulting in the activation $Y$. We then measure the compatibility to obtain the score of $B_{nl}$. These operations are repeated for all candidate blocks. \u2463 From these blocks, $K$ blocks with the highest compatibility score are selected and uploaded with their scores. 5 At the server side, all received block combinations are integrated through a voting process, and the blocks with the $K$ highest number of votes are determined as the selection for that round. Each selected block is stitched with the current network $N_s$, generating $K$ new stitched networks for the next round. The selection continues until a terminating block is picked, the maximum stitched network depth is reached, or all possible paths are explored. During the local block selection, we only need a batch size of data for inference in total. Compared to training, the amount of required data is significantly reduced.\nIn the composing process, the following two key approaches are employed to 1) measure the compatibility of two pre-trained blocks and 2) stitch two blocks.\nMeasure the compatibility. The compatibility of the se-lected block, the higher the performance of the stitched neural network. We choose Centered Kernel Alignment (CKA) to measure the compatibility between two blocks and to guide the block selection.\nGiven two representation K and L, their CKA score is calculated by:\n$CKA(K, L) = \\frac{HSIC(K, L)}{\\sqrt{HSIC(K, K)HSIC(L, L)}}$    (1)\nwhere HSIC is Hilbert-Schmidt Independence Criterion [3]. For the linear kernels, HSIC is:\n$HSIC(K, L) = ||cov(X^TX,Y^TY)||^2$     (2)\nStitch two pre-trained blocks. Once we have selected an appropriate successive block, the next step is to stitch these two blocks together. Since the two blocks may have different input and output dimensions, we use Moore-Penrose pseudoinverse to create a projection tensor $A_{kj}$ which projects"}, {"title": null, "content": "an output tensor $X_{ij}$ of the incoming block to an input tensor $Y_{ik}$ of the outgoing block:\n$A_{kj} = Y_{ik} X (X_{ij}X)^{-1}$   (3)\nwhere $i$ represents the sample dimension, $j$ refers to the output dimension of the incoming block, $k$ is the input dimension of the outgoing block. We can find A such that $Y_{ik} = A_{kj}X_{ij}$.\nHowever, the data and system heterogeneity in FL pose new challenges. The following key observations motivate the design of the core components of FedStitch.\nQ1: How does the data heterogeneity impact the stitch-ing performance?\nTo investigate the relationship between the distribution of the local training data and the performance of the stitched network, 100 devices with the same amount of training data and different data distributions are set up. We conduct separate experiments for IID and non-IID scenarios. For the experiment of IID, each dataset is evenly distributed among all clients, with each client having data for all classes. For non-IID, we follow [14] to perform the data partition, for each client, the number of training samples belonging to each class is based on Dirichlet distribution [15] using a concentration parameter set to $\\alpha = 1$. These two groups execute the complete process of generation of the stitched models. The comparison result is shown in Table. I. We can observe that non-IID data has a significant impact on the performance of the stitched network, resulting in 5.84%, 12.77%, and 9.43% performance drops in CIFAR10, CIFAR100, and CINIC10 datasets.\nTo investigate the reasons for degradation, we analyze as follows. We introduced 10 users, each having data of different number of classes, with the same total amount of data. Given the varying impacts of different classes on the results, we conducted 10 sets of experiments with different initializations for each user. The result is shown in Fig. 2a. The results indicate that the lower the non-IID data held by a user (i.e., data encompassing a broader range of classes), the more effectively the produced stitched network performs. Conversely, the performance is still significantly affected for users with only a few classes, even with a sufficient quantity. Therefore, during server aggregation, we should not employ traditional aggregation methods, such as FedAvg [1] but rather prioritize users with lower non-IID level data.\n#Principle 1: Local data heterogeneity has a significant impact on the performance of stitched networks, with networks generated from data with lower non-IID levels exhibiting better performance.\nQ2: When the server cannot directly access local data, how to select users with lower non-IID level data?\nHowever, since the server cannot access the local data in FL, it is difficult for the server to identify the client with low non-IID level data. In each round, the server only receives the selected blocks and related CKA scores. Hence, we conduct the following analysis. Given a candidate stitched network (not yet reaching the terminating block) and selected N users with different levels of non-IID data, we require these users independently to find the most suitable next block from a block pool for this network. We then choose one user $U_k$, record the block he selected, and then look up the CKA score of this corresponding block on another user $U_i$. We calculate the rank of this score within the user $U_i$'s score range of all candidate blocks. We calculate the ranks in the same way for all other users $U_i$, $i \\in \\{1, 2, ..., N\\} \\\\ \\{k\\}$. The averaged rank $r_k$ can reflect the performance of $U_k$'s block selection on other users. Except for user $U_k$, we calculate the same rank for each user with the same method as $U_k$. To investigate the different impacts of non-IID at different stages of stitching, we configured three candidate stitched networks with varying depths. In Fig. 2b, we observe that low non-IID level users, which have more classes, will select blocks that always achieve a relatively high CKA rank (not the highest) on other users with different non-IID levels. We will use this observation to identify users with low-level non-IID data in the next section. Additionally, as the stitched network goes deeper, the differences between the blocks selected by users with different levels of non-IID tend to increase.\n#Principle 2: Blocks selected by low non-IID level user also have high CKA scores in other users.\nQ3: How the system heterogeneity impacts the overall training progress and the stitching overhead of the partic-ipating clients?\nTo investigate the relationship between the system hetero-geneity and the energy consumption of stitching, one user is configured to conduct local block selection at different fixed process frequencies on the Jetson TX2, Table. II shows the result as an example, we can find that the system spends more than 98% of the time on the highest processor frequency dur-ing the local block selection. This is because, on edge devices,"}, {"title": "IV. FEDSTITCH: CORE COMPONENTS", "content": "Guided by the corresponding principles, in this section, we present the core components designed to address the challenges introduced by the data and system heterogeneity in FL.\nA. Overview\nFig. 3 shows the system overview. The process is divided into the following steps. \u2460 Initialization. At the beginning, the server assigns initialized weights to each client, and divides the pre-trained models into blocks, forming a block pool. The server then sends both the pre-trained models and the initialized block pool to all participating users. \u2461 Network Dispatching. In each round, the server dispatches the stitched neural network, block pool state, and deadline to that round's participants. \u2462 Local Selection. Locally, the local energy coordinator chooses the optimal configuration for minimizing energy consumption within the deadline. Then, the local user selects the most suitable block for the current stitched neural network using the relevant CKA scores from the block pool. \u2463 Block Updating. All participants upload their selected block and all associated scores for this round. 5 Weighted Aggre-gation. On the server side, the weighted aggregator, utilizing uploaded scores, adjusts each client's weights and employs weighted aggregation to select suitable blocks for the current stitched network. \u2465 Space Optimization. Simultaneously, the search space optimizer shrinks the block pool size according to the CKA score results. The new stitched network, weights of clients, and block pool will be updated for the next round as well. This process repeats until either the terminating block is selected or the stitched neural network reaches its maximum depth.\nB. RL Weighted Aggregator\nDrawing from Principle 1, we highlight the significance of users with less non-IID data, suggesting their blocks be prior-itized during server aggregation. We propose weighting users' contributions, giving more weight to those with lower non-IID data. However, FL faces hurdles as data privacy prevents server access to this data, complicating weight adjustments. Utilizing Principle 2, our approach involves a reinforcement learning-based weighted aggregation to counteract non-IID's effects on block selection. We use the Epsilon-Greedy algorithm to balance performance and exploration in stitching schemes.\nThe FedStitch process unfolds as follows: Initially, every user $u_i$ is given the same weight $w_i$, with the sum of their weights equaling 1. We also set the exploration factor $\\epsilon$, and the reward (a) and penalty ($\\beta$) update factors, along with a threshold $\\theta$. In each round, the server sends the current stitched network candidate and block pool to the participating client. The client computes the CKA on their local dataset for each block. Based on the strategy, it either randomly selects K blocks with probability $\\epsilon$ (exploration) or picks the blocks with the highest CKA scores with probability 1 - $\\epsilon$ (exploitation). Clients then upload their selected blocks and all CKA scores. Server-side, cross-validation among users assesses block choices. For a block from $u_i$, we calculate its average CKA score rank across others. If $r_i > \\theta$, reflecting"}, {"title": null, "content": "Algorithm 1 FedStitch\nInput: initialized client weights W, initialized candidate stitched networks set N and related block pool set B\nParameter: exploration factor $\\epsilon$, the reward and penalty weight update factors $\\alpha$, $\\beta$, threshold $\\theta$, number of block selection K.\nOutput: updated client weights W and new candidate stitched networks set N\n1: for round t = 0,1, ...,T-1 do\n2: $\\quad$  Random clients\n3:  Random candidate stitched network in N\n4:  Related block pool for $N_t$\n$\\quad$ // Client Score Calculation\n5: $\\quad$ for each client $i \\in P_t$ do\n6:  S$\\leftarrow$\u00d8\n7:  for each block $b_j \\in B_t$ do\n$\\quad \\leftarrow$ CKA($(b_j)$)\n  S\\leftarrow SU$\\qquad$\n10: $\\quad$ end for\n$\\quad$ Select K blocks with highest scores in $S_i$ with the probability 1 \u2212 \u03f5\n$\\quad$Randomly select K blocks in $S_t$ with the probability \u03f5\n$\\quad$ end for\n$\\quad$ // Server Weighted Aggregation\n$\\quad$ for each client $i \\in P_t$ do\n$\\quad$r  RankCalculation($B_i$, $S_t$)\n$\\quad if r < \\theta$ then\n$\\quad$$w_i \\leftarrow w_i \u00d7 (1 \u2212 \u03b2)$\n$\\quad$ else\n$\\quad$$w_i  w_i \u00d7 (1 + \u03b1)$\n$\\quad$ end if\n$\\quad$ end for\n$\\quad$W$\\leftarrow$\u222aW for i\u2208 [1, ...,]\n$\\quad$M$\\leftarrow$\u222a for i\u2208 [1,...]\n$\\quad$MWeightedVoting(W, Mt)\n$\\quad$N\u222a Stitching(Nt, Mt)\n$\\quad$return N, W\n$\\quad$ end for\nlow non-IID data for $u_i$, we reward by increasing $w_i$ with $w_i \u00d7 (1 + \u03b1)$. If $r_i < \\theta$, indicating high non-IID data, we penalize by reducing $w_i$ with $w_i \u00d7 (1 \u2013 \u03b2)$. Afterward, the weights are re-normalized to ensure the sum equal to 1. The server then uses weighted voting to select blocks, stitching them into new candidates. It updates the network set and client weights for the next round. As stitching progresses, we adjust \u03b1, \u03b2, and to suit different stages. This algorithmic flow is detailed further in our algorithm description.\nC. Search Space Optimizer\nWhile the proposed method mitigates non-IID effects, the large block pool expands the search space, slowing down ag-gregation. For example, a pool with five pre-trained networks yields over 50 blocks, necessitating CKA score calculations for each block per round, adding unnecessary computation and delaying aggregation. However, indiscriminate removal of blocks is not feasible without knowing their potential value. To tackle this, we experimented to dynamically narrow the search space using CKA scores during stitching. For a network split into six blocks , if $B_3$ has the highest CKA score, it implies blocks before $B_3$ ($B_1$ and $B_2$) are less suitable than those after, due to their lower-level features, whereas blocks $B_4$, $B_5$, and $B_6$ are more likely to improve performance due to their higher-level features.\nTherefore, when selecting blocks for a candidate stitched network, the client identifies the block with the highest C\u041a\u0410 score among those belonging to the same pre-trained model. In the subsequent selections, all blocks of each pre-trained network that are shallower than the block are removed from the block pool. In every block selection stage, a portion of the block pool is eliminated. The search space progressively reduces during the stitching process, significantly improving the speed of each user's block selection and accelerating the aggregation process.\nD. Local Energy Coordinator\nBased on the key observation Principle 3 that DVFS gov-ernor fails to efficiently balance block selection and energy use in FL, we seek a method to lower energy consumption during local block selection without extending selection time. Simply reducing client frequency during local inference to save energy could delay block selection, especially for slower devices, affecting the entire aggregation process. Therefore, an approach that reduces energy costs without negatively impacting the overall schedule is essential.\nTo tackle this requirement, we set a deadline for all partici-pating clients in each round. This deadline is determined by the completion time of the last round. With the received hardware configuration information and the size of local input data, the time required by device $i$ to complete the local selection process can be modeled as:\n$t_i = \\frac{c_i D_i}{f_i}$ (4)\nwhere $c_i$ represents the number of processor cycles required to process one data object in inference on mobile device $i$, which can be obtained through offline profiling, $D_i$ represents the number of data objects in the local inference data set, and $f_i$ is a particular process frequency available on device $i$. With the local completion time of each client $[t_1, t_2, ..., t_N]$ in the last participant round, the server selects the maximum of them as the deadline for this round $d = max(t_i)$. If the size of candidate stitched networks and related block pools are different from the last round, the deadline $d$ will be adjusted with deep factor \u00b5 and pool size factor \u03c3. After receiving the deadline of the current round, the local energy coordinator conduct a feedback-based system configuration method for each user. It dynamically adjusts the frequency of the device so that the participant can meet the deadline while minimizing"}, {"title": null, "content": "energy consumption. To select the proper block for a candidate stitched network, we model the energy consumption of a device in one round as follows:\n$E = \\rho\\frac{p^{infer}}{ \\eta} * t^{infer} +  \\rho\\frac{p^{inf}}{ \\eta}* t^{inf} + p^{idle} * t^{idle}$   (5)\nwhere $p^{infer}$ and $p^{inf}$ represent the power consumed while the inference process is running for the current stitched net-work $s_n$ and related pre-trained network $p_n$. $p^{idle}$ represents the base power when the smartphone is powered on but not actively used. $t^{infer}$, $t^{inf}$, and $t^{idle}$ are the time spent in the inference state of two networks and idle state, respectively.\nConsidering a stipulated deadline d for this round, our goal is to minimize client energy consumption while ensuring the block selection process is finalized before reaching d. Denote the energy consumed by device $i$ during a particular round when running the inference process at a CPU frequency of $f_i$ as $E_i(f_i)$. The problem becomes:\n$\n\\underset{f_i}{arg \\, min} E_i (f_i), \\, f^{min} < f_i < f^{max}\n$   (6)\ns.t.\n\n$\\rho\\frac{p^{infer}}{ \\eta}(f_i) +  \\rho\\frac{p^{inf}}{ \\eta}(f_i) + t^{idle} = d$   (7)\n\n$\\, 0 < \\rho\\frac{p^{infer}}{ \\eta}(f_i),  \\rho\\frac{p^{inf}}{ \\eta}(f_i), t^{idle} < d$\nFor each client, we assign an initial frequency $f_i$ and then use the device's hardware specs to estimate its inference and idle times at this frequency. If the combined time falls below the deadline d, suggesting $f_i$ is too high, we reduce it. Conversely, if it exceeds d, indicating $f_i$ is too low, we increase it. We adjust $f_i$ based on this feedback until the total time is just under d, within a narrow margin. This method dynamically fine-tunes $f_i$ for each client, ensuring we meet deadlines while optimizing energy consumption."}, {"title": "V. EVALUATION", "content": "A. Experimental Setup\n1) Models and Block Pool: We select five representa-tive neural networks: alexnet", "11": "dataset. Each pre-trained model is divided into blocks consisting of one or more successive layers", "Datasets": "We evaluate FedStitch using the following popular datasets including CIFAR10", "13": "and CINIC10 [21", "Baselines": "To showcase the effectiveness of FedStitch", "partitioning": 1, "4": "prunes the global model via varying model channels; (2) DepthFL [10", "Details": "We choose Nvidia Jetson TX2 as the embedded device for deploying the on-device FL system. We use the Monsoon Power Monitor to record en-ergy consumption and htop to monitor memory usage. We configured 100 users, with 10 users participating in each round. The model sizes of 5 pre-trained models are 94MB (resnet50), 33MB (densenet121), 14MB (mobilenet_v2), 228MB (alexnet), and 537MB (vgg16). Thus total memory of 906MB is needed to store these pre-trained models. We split the users into 4 groups, with each group representing 30%, 30%, 30%, and 10% of the total. The memory constraints for each group are 1GB, 2GB, 4GB, and 8GB, respectively. Under this setting, three baselines in group 1 train a global model individually using their respective model partition and training methods based on users' memory budgets. Compared to the model size, the memory overhead of inference can be negligible. Therefore, in this memory setting, even users with the smallest memory allocation can participate in the entire process of FedStitch. In real-world scenarios, even if there are devices with highly limited resources that cannot participate in local block selection, we can still ensure that the majority of devices contribute to the network generation process.\nFor the baselines in group 1, we adopt SGD as the optimizer with a momentum of 0.9, weight"}]}