{"title": "Verify when Uncertain: Beyond Self-Consistency in Black Box Hallucination Detection", "authors": ["Yihao Xue", "Kristjan Greenewald", "Youssef Mroueh", "Baharan Mirzasoleiman"], "abstract": "Large Language Models (LLMs) suffer from hallucination problems, which hinder their reliability in sensitive\napplications. In the black-box setting, several self-consistency-based techniques have been proposed for\nhallucination detection. We empirically study these techniques and show that they achieve performance close\nto that of a supervised (still black-box) oracle, suggesting little room for improvement within this paradigm.\nTo address this limitation, we explore cross-model consistency checking between the target model and an\nadditional verifier LLM. With this extra information, we observe improved oracle performance compared to purely\nself-consistency-based methods. We then propose a budget-friendly, two-stage detection algorithm that calls the\nverifier model only for a subset of cases. It dynamically switches between self-consistency and cross-consistency\nbased on an uncertainty interval of the self-consistency classifier. We provide a geometric interpretation of\nconsistency-based hallucination detection methods through the lens of kernel mean embeddings, offering deeper\ntheoretical insights. Extensive experiments show that this approach maintains high detection performance while\nsignificantly reducing computational cost.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have extremely powerful abilities to answer questions or summarize or explain content.\nHowever, hallucinations have emerged as a persistent problem, where plausible-looking but incorrect information is mixed\nin with the generations. Hallucinations create major difficulties for both human users and tool calling or agentic applications\ndue to their visual plausibility, as verifying all portions of the model generation is often just as hard as finding the answer\nusing legacy methods.\nA key challenge is detecting hallucinations in black-box scenarios, where we can access only an LLM's outputs but not\nits intermediate states-an especially relevant concern when using closed-source commercial APIs. An important intuition\nis that self-consistency\u2014mutual entailment between multiple stochastically sampled high-temperature generations from\nthe LLM for the same question is lower when the model is hallucinating compared to when it provides a correct answer.\nAs Tolstoy wrote in Anna Karenina, \u201cAll happy families are alike; every unhappy family is unhappy in its own way.\u201d\nMechanistically, this LLM behavior can be understood as follows, from the fact that LLMs are pretrained as statistical\nlanguage models. If the model confidently knows the answer, then probability should be concentrated on that answer. If,\non the other hand, the model does not know all or part of the answer, its statistical pretraining will bias it towards creating\nwhat is essentially a posterior distribution on answers or parts of answers that seem plausible.\nFollowing this philosophy, prior work (Manakul et al., 2023; Farquhar et al., 2024; Kuhn et al., 2023; Lin et al., 2023;\nNikitin et al., 2024) has proposed various methods that leverage self-consistency. However, it remains unclear how much\nfurther they can be improved. Thus, we investigate whether we are already near the performance ceiling, given the limited\ninformation available in a black-box setting. Using a unified formalization of self-consistency-based methods, we design\na method that trains graph neural networks to approximate the ceiling performance of this method family. Notably, we find\nthat existing methods are already close to this ceiling, suggesting little room for further improvement within this paradigm.\nThis highlights the need to go beyond self-consistency alone. Thus, we consider the case where an additional model serves"}, {"title": "2. Related Work", "content": "There are works that explore white-box detection methods, such as (Duan et al., 2024; Varshney et al., 2023), which require\ntoken-level logits, or (Yin et al., 2024; Zou et al., 2023; Agrawal et al., 2023), which rely on intermediate representations.\nWhite-box methods are less suitable for certain scenarios, such as closed-source commercial APIs. In this work, we focus\nexclusively on black-box hallucination detection, where we do not have access to the internal workings of the LLM. In\nthis scenario, the primary approach involves checking the consistency between multiple samples of the LLM's answers\n(Manakul et al., 2023; Farquhar et al., 2024; Kuhn et al., 2023; Lin et al., 2023; Nikitin et al., 2024). These works rely\non sampling multiple answers to the same question from the LLM and using an NLI (Natural Language Inference) model\nto determine whether they are semantically equivalent. The NLI judgments are then processed in various ways to decide\nwhether a hallucination has occurred. Details of these methods are discussed in Section 4. Manakul et al. (2023); Kuhn et al.\n(2023) also explore alternative methods for judging semantic equivalence, which are either less effective (e.g., n-gram) or\ncomputationally expensive (e.g., using an LLM). (Zhang et al., 2023) identify limitations in self-consistency-based methods\nand propose leveraging question perturbation and cross-model response consistency (comparing an LLM's responses\nwith those from an additional verifier LLM) to improve performance. While their approach improves results, introducing\na verifier model adds computational overhead. In this work, we systematically explore the possibility of achieving\ncomputational efficiency when combining self-consistency and cross-model consistency. Note that the question perturbation\ntechnique from (Zhang et al., 2023) is orthogonal to our approach and could potentially be incorporated to achieve better\nresults. Another line of work involves directly asking LLMs to judge the uncertainty of their answers (Mielke et al., 2022;\nTian et al., 2023b; Kadavath et al., 2022; Lin et al., 2022), which typically requires additional finetuning/calibration and\ndoes not fit within the black-box scenario. Without any modification to the original LLMs, their verbalized confidence\nis often inaccurate (Xiong et al., 2023). The inherent conflict between calibration and hallucination, as theoretically"}, {"title": "3. Preliminaries", "content": "In the task of hallucination detection, we have a target LLM, denoted by Mt, for which we aim to detect hallucinations.\nWe are given a set of questions {qi}=1. Given a set of questions {qi}=1, the model generates answers, ai = Mt(qi, T),\nunder a specified temperature 7. The ground truth annotation hi indicates whether a\u017c is a hallucination (h\u2081 = 1) or factual\n(h\u2081 = 0). The objective is to predict whether a\u017c is a hallucination, with our prediction denoted by hi.\nTo achieve this, many methods are designed to output a value, vi, that captures specific characteristics (e.g., the uncertainty\nof the answer). A higher value of vi suggests that az is more likely to be a hallucination. The prediction hi is then determined\nbased on a threshold applied to vi, where the choice of threshold dictates the final classification.\nTo evaluate the performance of a hallucination detection method, we focus on two widely accepted metrics computed given\noutputs {v}=1 and ground truths {hi}=1: (1) AUROC, area under the receiver operating characteristic curve. is a classic\nperformance measure in binary classification. It captures the trade-off between the true positive rate and the false positive\nrate across various thresholds, providing an aggregate measure of the model's ability to distinguish between the two classes.\n(2) AURAC, the area under the \u201crejection accuracy\" curve (Farquhar et al., 2024). It is designed for scenarios where\na hallucination detection method is employed to refuse answering questions that the model is most likely to hallucinate\non. Rejection accuracy measures the model's accuracy on the X% of questions with the lowest vi values (least likely to\nhallucinate), and the area under this curve summarizes performance across all values of X."}, {"title": "4. From Self-consistency to Cross-Consistency", "content": "Prior work (Manakul et al., 2023; Farquhar et al., 2024; Kuhn et al., 2023; Lin et al., 2023; Nikitin et al., 2024) has introduced\nvarious methods leveraging self-consistency. However, the extent to which these methods can be further improved remains\nunclear. To explore this, we develop a method to approximate the ceiling performance for any approach that utilizes\nself-consistency and compare it against the performance of existing methods.\nm\nUnified formalization of self-consistency-based methods. We first present a unified formalization of the existing methods.\nRecall that the goal is to determine whether each M(qi) is a hallucination. All these methods rely on additionally sampling m\nanswers from the LLM M for question qi under a high temperature \u03c4', which is typically much higher than 7, the temperature\nused to generate az. For example, in (Farquhar et al., 2024), the settings are r = 0.1 and \u03c4' = 1.0. Let {a',j}=1 denote the\nset of these additionally sampled answers. These methods then use an entailment estimator (e.g., DeBERTa-Large-MNLI),\ndenoted by E. The estimator & takes two answers as input and outputs a value between 0 and 1, indicating the degree of\nentailment between the two answers, where 1 means full entailment. Using E, a self-entailment matrix Pself is constructed\nas: Pself = [(aj, di,k)]1<j<m,1<k<m, where each element is the entailment value for a pair of answers in {a',j}}=1\u00b7\nExisting methods can then be formalized as some function f applied to the self-entailment matrix P\u0219elf which outputs a\nscalar. The focus of prior work lies in designing various forms of f. Specifically, (1) SE(Pself), the Semantic Entropy\n(Farquhar et al., 2024), uses a binarized version of Pself to identify which answers belong to the same semantic set and then\ncomputes the entropy over these semantic sets. (2) MPD(P\u0219elf) (Lin et al., 2023; Manakul et al., 2023) is simply the mean\npairwise distance, computed as 1 \u2013 Mean(Pself). (3) EigV(P\u0219elf) (Lin et al., 2023) is defined as the sum of the eigenvalues\nof the graph Laplacian of P\u0219elf. (4) Ecc(P\u0219elf) (Lin et al., 2023) measures the eccentricity of the answers leveraging the\neigenvectors of the graph Laplacian of Pself. (5) KLE(Pself), the Kernel Language Entropy (Nikitin et al., 2024), involves\napplying the von Neumann entropy to a graph kernel derived from Pself. For all these methods, a higher output indicates\ngreater uncertainty among {a',;}}=1, making the corresponding low-temperature answer a\u017c more likely to be a hallucination.\nThe underlying assumption is that Pself contains exploitable information related to hi, the ground truth hallucination\nannotation. This prompts the question: how much information does Pself actually encode about hi? To explore this, we aim\nto identify the optimal function f that maps Pself to the hallucination label. This leads to the following formulation:\n\\(f = arg min E[l(f(P^{self}), \\widehat{h})],\\)\nwhere l is a loss function that measures the discrepancy between the output value and the actual label.\nApproximating the ceiling performance with GCN models. To search for f, we frame it as a learning problem. Since\nthe task is ultimately binary classification based on the matrix Pself, graph neural networks are well-suited due to their\nability to process matrix structures and express a wide range of functions. We use a two-layer Graph Convolutional Network\n(GCN) to represent f. The model is trained with BCE loss on sampled pairs of Pself and h. We then evaluate AUROC\nand AURAC of the resulting model as an approximation of the ceiling performance. The training and test samples are\ndrawn independently to account for the finiteness of the data, ensuring that the evaluation reflects the model's ability of\ncapturing a generalizable relationship between Pself and \u0125, rather than that of overfitting the training data.\nResults. In Figs. 2 and 7, we compare the performance of existing methods with the ceiling performance approximated\nusing the aforementioned approach across various settings. We consider three different LLMs: Llama-2-13b-chat,\nLlama-3-70B-Instruct, and Mixtral-8x7B-Instruct-v0.1, as well as two datasets: SQUAD and TriviaQA.\nIn the plots, the x-axis represents different methods with varying hyperparameters. The best-performing method varies across\ndifferent settings, with MPD, KLE, and EigV consistently showing relatively strong performance. Notably, in each setting,\nthe top method closely approaches the approximated ceiling, indicating that existing methods already make near-maximal\nuse of Pself, particularly when sufficient validation data is available to optimize method and hyperparameter selection."}, {"title": "4.2. Incorporating cross-model consistency", "content": "As noted in the previous subsection, existing methods bring us very close to the ceiling performance for self-consistency\nalone. The question now is: how can we push beyond this limit? In the black-box scenario, our options are constrained\nby the lack of access to any internal model information. (Zhang et al., 2023) has explored another potential approach:\nleveraging outputs from other LLMs to improve hallucination detection through cross-model comparisons. A minimal"}, {"title": "4.2.1. IMPROVEMENT IN THE CEILING PERFORMANCE", "content": "We explore how much gain cross-model consistency checking can possibly bring. Let us denote this verifier model as M\u03c5.\nSimilar to the self-consistency case, a natural extension is to encode the cross-model consistency information in a matrix:\n\\(P^{cross} = [E(a'_{i,j}, b'_{i,k})]_{1<j<m,1<k<m}\\), where {b',k}=1 are m answers sampled from Mv under temperature \u03c4' for question\nqi. Thus, Pcross captures the pairwise entailment relationships between the answers generated by the target model Mt and\nthe verifier model Mv.\nRemark 4.1 (Cross entailment). To build intuition, consider the setting of a very strong verifier model that always returns\na sample from the ground truth distribution. Then, if the entailment model returns a calibrated posterior probability of\nentailment, it is easy to see that Mean (Pcross) is the probability of entailment between an M\u0165 sample and a ground truth\nsample. In other words, it can be interpreted as the probability of correctness. For as the verifier becomes weaker, we\nhypothesize that the Mean(Pcross) retains significant correlation with the probability of correctness, and observe this in\npractice.\nBuilding on the formalization in Section 4.1, we aim to determine how much information can be extracted when both Pself\nand Pcross are used to predict the ground truth hallucination label. To achieve this, we search for a function f that takes both\nPself and Pcross as input. Given the pairwise nature of the data, we again leverage GCN models to represent the function.\nSpecifically, we combine Pself and Pcross into a single matrix:\n\\(\\left[\begin{array}{cc}P^{self} & P^{cross} \\P^{cross} & 0\\end{array}\right]\\),\nwhich encodes the underlying structure of\nthe data as pairwise relationships between answers. We then apply a GCN to this combined matrix and train the model to fit\nthe ground truth labels h. Finally, we evaluate the resulting model to approximate the ceiling performance achievable with\nboth Pself and Pcross\nWe now compare the approximated ceiling performances achieved using only Pself to those achieved using both Pself and\nPcross, as shown in Fig. 3. The x-axis represents the target model, while the colors indicate the verifier model used. Gray\nbars correspond to the scenario where only Pself is used, i.e., no verifier model is involved. The results demonstrate a clear\nimprovement in performance, measured by both AUROC and AURAC, when a verifier model is introduced. Interestingly,\nthis improvement is observed even in cases where the target model itself is quite strong. For example, on the TriviaQA\ndataset, adding a weaker verifier model can still enhance detection performance when Llama-3-70B-Instruct is\nused as the target model. This highlights the potential of leveraging cross-model consistency, as even a less powerful verifier\ncan provide complementary insights that enhance hallucination detection."}, {"title": "4.2.2. LINEARLY COMBINING MDPS CLOSELY APPROACHES THE CEILING", "content": "Although the function the GCN implements to achieve ceiling performance is unknown, interestingly, we find that a\nsimple extension of existing methods can perform almost equally well. We leverage MPD introduced earlier, which can\nbe naturally extended to Pcross as MPD(Pcross) = 1-Mean(Pcross). The combined approach uses a weighted average:"}, {"title": "5. Budget-Aware Hallucination Detection with A Verifier Model", "content": "From the previous section, we observe that performance improves significantly when self-consistency checking is combined\nwith cross-model consistency checking. However, this approach can introduce substantial computational overhead, especially\nwhen the verifier is a large model. For instance, with a 7B target model and a 70B verifier model, cross-model consistency\nadds 10 times the computation.\nTo address this issue, we propose a method to control computational overhead. As illustrated in Fig. 1. The key idea is\nto perform cross-model consistency checking only when it is most necessary. Intuitively, when self-consistency scores\nare extremely high or low, it is likely\u2014though not guaranteed\u2014that the model's output is non-hallucinatory or hallucinatory,\nrespectively. In such cases, performing cross-model consistency checking may not be necessary under limited computational\nbudgets. Instead, cross-model consistency should focus on cases with intermediate self-consistency scores, where our\njudgment is more uncertain. This is formalized in Alg. 1. The parameter p specifies the fraction of instances for which\ncross-model consistency checking will be performed. The parameter t\u2081 is the threshold for MPD(Pself) below which the"}, {"title": "Geometric Interpretation in Mean Embedding Space", "content": "We provide a geometric interpretation of our hallucination\ndetection. For each prompt x we can observe the conditional distribution of the target model \u03c0\u03c4(y|x) and the verifier \u03c0(y|x).\nIn particular we observe \\(\\frac{1}{Na}\\sum_{i=1}^{Na} \\delta_{y_i, Y_i} \\sim \\pi_t(.|x)\\) and \\(\\frac{1}{N_v}\\sum_{i=1}^{N_v} \\delta_{y_i, Y_i} \\sim \\pi_v(.|x)\\). We assume that the entailment kernel\n\\(E : Y \u00d7 Y \u2192 [0,1]\\) to be a reproducing kernel. The mean embeddings (Muandet et al., 2017) of target, verifier and\nground truth are respectively \\(\\mu_t = \\frac{1}{Na}\\sum_{i=1}^{N_a}E(y_i, .)\\), \\(\\mu_v = \\frac{1}{N_v}\\sum_{i=1}^{N_v}E(y_i, .)\\), and \\(\\mu_* = \\frac{1}{N_*}\\sum_{i=1}^{N_*}E(y_i, .)\\) We can write\nthe self consistency in terms of norms of mean embeddings :\n\\(||\\mu_t||^2 = \\frac{1}{N_a^2}\\sum_{i,j}E(y_i, y_j) = 1 - MPD(P^{self})\\)\nand the cross consistency\n\\(\\langle\\mu_t, \\mu_v\\rangle = \\frac{1}{N_a n_v}\\sum_{i,j}E(y_i, y_j) = 1 - MPD(P^{cross})\\)"}, {"title": "Evaluating AUROC/AURAC for Algorithm 1", "content": "Recall that, in the conventional scenario, the AUROC and AURAC metrics\nare defined for a system that outputs a single value for binary classification. To compute these metrics, we vary the threshold\nused to produce the label from the output. In such cases, for both ROC and RAC curves, each X-value corresponds to a\nsingle Y-value. For example, in ROC, one false positive rate corresponds to one true positive rate, allowing us to obtain a\nsingle curve by varying the threshold and then compute the area under it. However, the situation is more complex for our\nAlgorithm 1, which, given a fixed hyperparameter p, ultimately outputs binary labels but involves two thresholds, t\u2081 and t2.\nDifferent combinations of t\u2081 and t2 can yield the same X-value but different Y-values, resulting in a plot that resembles\na thick band rather than a single curve. Thus, the area under the curve is not well-defined. The way these thresholds are\nadjusted relative to each other significantly affects the Y values for a given X. To derive a meaningful performance measure\nfor our algorithm, we establish the relationship between the two thresholds using a validation set. Specifically, on this set,"}, {"title": "Theorem 5.1 (AUROC Generalization)", "content": "Suppose we are given nneg i.i.d. samples from the non-hallucinating distribution\nand npos i.i.d. samples from the hallucinating distribution, and sets of candidate thresholds T\u2081 = {t}}} and T2 = {t}\nfor stages 1 and 2 respectively. Suppose we use this data to choose a mapping t1, t2 = A(PFA) from desired probability of\nfalse alarm level PFA \u2208 [0, 1] to thresholds t1 \u2208 T1, t2 \u2208 T2, maximizing the probability of detection on the validation data.\nLet Aval(A) be the AUROC using thresholds given by A. Then, with probability at least (1\n2\n|T1||T2|\n) the test AUROC\nsatisfies Atest(A) \u2265 Aval(A) \u2013 2e and the test ptest - PFA| \u2264 6, where \u0454 =\n\\(\\frac{\\sqrt{2\\left(\\log (\\left|T_{1}\\right|)+\\log (\\left|T_{2}\\right|)\\right)}}{min\\left(n_{\\text {neg }}, n_{\\text {pos }}\\right)}\\),"}, {"title": "6. Experiments", "content": "Datasets. We consider datasets widely used in research on hallucination detection (Kuhn et al., 2023; Farquhar et al.,\n2024; Lin et al., 2023; Nikitin et al., 2024). These datasets focus on reading comprehension and question-answering tasks,\nincluding TriviaQA (Joshi et al., 2017) for trivia knowledge, SQUAD (Rajpurkar et al., 2016) for general knowledge, and"}, {"title": "Estimation of computational overhead", "content": "Given that Alg. 1 focuses on budget awareness, it is important to consider both\nperformance and budget in our evaluation. To quantify the additional computational cost introduced in Alg. 1 compared to\nthe case where only self-consistency is used, we define a metric called relative additional cost:\n\\(\\frac{FLOPs(Alg. 1)\u2013 FLOPs(only\\ self\\ consistency\\ checking)}{FLOPs(only\\ self\\ consistency\\ checking)}\\)\nwhich can be estimated as \\(\\frac{p N_{verifier}}{N_{target}}\\) using the FLOP estimation formula from (Kaplan et al., 2020) (detailed derivation Appx\nC.1), Here, Nt and N represent the number of non-embedding parameters in the target and verifier models, respectively.\np accounts for the fact that the verifier model is queried for only a fraction p of the questions in Alg. 1."}, {"title": "Performance vs. cost", "content": "In Figs. 6 (AUROC) and 9 (AURAC) (in the supplement), we plot the detection performance\nagainst the estimated relative additional cost when varying p. A general trend observed is that when the verifier model is\nstronger than the target model (e.g., when the target is merlinite-7b and the verifier is Llama-3-70B-Instruct),\nincreasing the computational budget-by allowing more verifier calls controlled by a larger p\u2014monotonically improves\nperformance. However, with a weaker verifier (e.g., when the target is Llama-3-70B-Instruct and the verifier\nis Llama-2-13b-chat), we observe an increasing-decreasing trend, where an intermediate number of verifier calls\nachieves the best results. This aligns with the intuition from Fig. 4, where intermediate weights on self-consistency\ncross-model-consistency are optimal, indicating that even a weaker verifier can contribute meaningfully to detection when\nan appropriate balance is maintained."}, {"title": "Selection of the verifier", "content": "Llama-3-70B-Instruct (blue), consistently achieves the best results when the computational\nbudget is large. However, Mixtral-8x7B-Instruct-v0.1 (purple), stands out for its exceptional performance with\na very small cost. This efficiency can be attributed to its MoE based design-despite having 46.7B total parameters, it only\nuses 12.9B parameters per token."}, {"title": "Transferability of thresholds", "content": "The gap between the scenarios where the validation set contains the same questions or\ndifferent questions is overall small (dashed vs. solid lines, Fig 6), suggesting that the selection of thresholds transfers well to\ndifferent questions, making the approach practical."}, {"title": "Comparison with approximated ceiling performance", "content": "The gap between our approach and the ceiling performance\napproximate using supervised learning with GCNs (described in Sec. 4.2.1; horizontal dashed line, Fig 6) is generally\nsmall. This indicates that our method effectively utilizes the verifier's information despite not always querying it."}, {"title": "7. Conclusion", "content": "In this paper, we empirically analyzed consistency-based methods for hallucination detection. Based on this analysis,\nwe introduced a budget-aware two-stage approach that leverages both self-consistency and cross-model consistency with\na given verifier. Our method reduces computational cost by selectively querying the verifier. Extensive experiments\ndemonstrate that it achieves strong performance with minimal computation, notably reducing computation by up to 28%\nwhile retaining 95% of the maximal performance gain."}, {"title": "C.1. Estimation of computation cost", "content": "To compute the budget, we use the estimation formula derived in (Kaplan et al., 2020), Table 1, which states that the number\nof FLOPs per token is approximately 2N for models with sufficiently large dimensions, where N represents the number\nof non-embedding parameters in the model. Then, for each question, the computation cost of cross-model consistency\nchecking can be expressed as:\n\\(ml_q 2N_{verifier} + m^2l_a 2N_{deberta},\\)\nwhere lq is the number of tokens in the question, and la is the number of tokens in each answer (assuming all m answers\nshare the same length), Nverifier, Ndeberta are the number of parameters of the verifier model and the entailment estimator\n(i.e., deberta-v2-xlarge-mnli), respectively. This estimation remains challenging due to the variability in the\nlengths of questions and answers, which depend on each specific question and model. To simplify this, we consider the\nscaling limit, where\u2014assuming no restrictions\u2014the lengths of questions and answers scale to the respective context lengths\nof the models processing them (since tokens exceeding the context length are dropped). Notably, the context length of\ndeberta-v2-xlarge-mnli is only 512, while the context lengths of verifier models range from 4,096 to 128,000.\nUsing this, we compute the ratio of the second term to the first term in Equation 2, which is \\(\\frac{m l_a N_{deberta}}{l_q N_{verifier}}\\), in the limit where\nboth lengths reach their maximum. We find that this ratio is no greater than 0.087 in the largest case in our settings, indicating\na negligible contribution of the second term in the limit. Thus, we omit the second term to simplify the estimated additional\ncomputation per question to mlq2Nverifier. Similarly, we can derive the estimation for the cost of self-consistency checking\nas mlq2Ntarget. Based on this simplification, the metric additional relative cost can be estimated as \\(\\frac{p N_{verifier}}{N_{target}}\\, where p\naccounts for the fact that the verifier model is queried for only a fraction p of the questions in Algorithm 1."}]}