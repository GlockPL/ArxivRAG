{"title": "Online Mirror Descent for Tchebycheff Scalarization in Multi-Objective Optimization", "authors": ["Meitong Liu", "Xiaoyuan Zhang", "Chulin Xie", "Kate Donahue", "Han Zhao"], "abstract": "The goal of multi-objective optimization (MOO) is to learn under multiple, potentially conflicting, objectives. One widely used technique to tackle MOO is through linear scalarization, where one fixed preference vector is used to combine the objectives into a single scalar value for optimization. However, recent work (Hu et al., 2024) has shown linear scalarization often fails to capture the non-convex regions of the Pareto Front, failing to recover the complete set of Pareto optimal solutions. In light of the above limitations, this paper focuses on Tchebycheff scalarization that optimizes for the worst-case objective. In particular, we propose an online mirror descent algorithm for Tchebycheff scalarization, which we call OMD-TCH. We show that OMD-TCH enjoys a convergence rate of O(logm/T) where m is the number of objectives and T is the number of iteration rounds. We also propose a novel adaptive online-to-batch conversion scheme that significantly improves the practical performance of OMD-TCH while maintaining the same convergence guarantees. We demonstrate the effectiveness of OMD-TCH and the adaptive conversion scheme on both synthetic problems and federated learning tasks under fairness constraints, showing state-of-the-art performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Multi-objective optimization (MOO) is a fundamental problem in various domains, including Multi-Task Learning (MTL) (Caruana, 1997), Federated Learning (FL) (McMahan et al., 2017a), and drug discovery (Xie et al., 2021). These problems often involve conflicting objectives, such as MTL tasks with differing label distributions and FL clients with heterogeneous local data, making it infeasible to find a single solution that optimizes all objectives simultaneously. Consequently, MOO methods identify solutions on the Pareto Front (PF), where any improvement in one objective necessitates a compromise in another (Miettinen, 1998).\nOne classic line of work to tackle MOO is evolutionary algorithms (Schaffer, 1985; Deb et al., 2002; Zhang and Li, 2007). However, they often suffer from slow convergence using zeroth-order optimization oracles. Recently, gradient-based methods have gained increasing attention for their scalability in large-scale machine learning problems (Sener and Koltun, 2018; Yu et al., 2020; Liu et al., 2021; He et al., 2024). Among them, adaptive gradient methods dynamically combines objective gradients to approach a stationary solution (Fliege and Svaiter, 2000; D\u00e9sid\u00e9ri, 2012) but face a large per-iteration computational complexity (Lin et al., 2024). In contrast, classic scalarization techniques reduce multiple objectives to a scalar function optimized via (sub)gradient descent (Miettinen, 1998), offering not only lower computational overhead but also specific solutions with controlled trade-offs. Linear scalarization has been widely adopted for its simplicity and promising empirical performance, yet has been shown incapable of recovering the complete Pareto optimal set when the PF is non-convex (Hu et al., 2024), which happens for under-parameterized models. Tchebycheff scalarization overcomes this limitation and calibrates even more precisely the quantitative relation among multiple objectives. Nevertheless, it suffers from training instability and stagnation for optimizing only the worst objective at each step (Mahapatra and Rajan, 2020).\nIn light of the above limitations, inspired by the literature in online learning (Nemirovskij and Yudin, 1983; Hazan et al., 2016), we propose a novel algorithm for Tchebycheff scalarization using online mirror descent (OMD), named OMD-TCH. Previously, OMD has been applied to solve minimax objectives aiming at algorithmic robustness and fairness (Namkoong and Duchi, 2016; Mohri et al., 2019; Michel et al., 2021; He et al., 2024), which are equivalent to the special case of Tchebycheff scalarization using a uniform preference vector. In this work, we claim that this can be generalized to arbitrary preferences, making OMD-TCH a versatile MOO method. We prove that OMD-TCH converges to the optimal solution of the original Tchebycheff scalarization problem at a rate of O(\\sqrt{\\log m/T}), or O(\\sqrt{m/T}) under different update rules, where m is the number of objectives and T is the number of rounds.\nDespite our algorithm's online nature, we propose a novel adaptive online-to-batch conversion scheme that allows the learner to selectively discard unwanted iterates and output a reweighted average for batch/offline learning, termed AdaOMD-TCH. At a high level, this adaptive scheme is motivated by the observation that the practical performance of OMD-TCH is hindered by the uniform averaging in the traditional conversion, which includes all iterates in the final solution. We prove that AdaOMD-TCH retains the same convergence guarantees as OMD-TCH while showing much-improved performance in practice.\nEmpirically, we evaluate (Ada)OMD-TCH on two sets of experiments. We first verify their effectiveness in finding diverse optimal solutions on a synthetic non-convex Pareto front. We then study their applicability in tackling the key accuracy-fairness challenge in MOO literature under Federated Learning (FL). In this particular case where a balanced trade-off among objectives (clients) is desired, OMD has been previously proposed as a solver (Mohri et al., 2019). We then focus on the under-explored adaptive conversion scheme. Our empirical results show that AdaOMD-TCH significantly improves over OMD-TCH in both accuracy and fairness metrics, achieving comparable performance to state-of-the-art fair FL methods."}, {"title": "2 PRELIMINARIES AND\nRELATED WORK", "content": "Notation Given a positive integer n, we use [n] for the set {1, 2, ..., n} and \\Delta_n for the (n-1)-dimensional simplex {a \\in \\mathbb{R}^n | \\sum_{i=1}^n a_i = 1, a_i \\geq 0, \\forall i}."}, {"title": "2.1 Multi-Objective Optimization", "content": "MOO aims to tackle the vector-optimization problem:\n$\\min_{\\Theta \\in \\Theta} f(\\theta) = (f_1(\\theta), f_2(\\theta), ..., f_m(\\theta))$,\nwhere $\\Theta \\subset \\mathbb{R}^d$ is the feasible region, and $f_i : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ are m differentiable objective functions. Generally, objectives conflict with each other so that no single solution is optimal for every objective. We are instead interested in Pareto optimal solutions (Miettinen, 1998):\nDefinition 1 ((Strict) Pareto dominance). For two solutions $\\theta_1, \\theta_2 \\in \\Theta$, $\\theta_1$ dominates $\\theta_2$, denoted as $\\theta_1 \\prec \\theta_2$, if $f_i(\\theta_1) \\leq f_i(\\theta_2)$ for all $i \\in [m]$ and $f_j(\\theta_1) < f_j(\\theta_2)$ for some $j \\in [m]$. If $f_i(\\theta_1) < f_i(\\theta_2)$ for all $i \\in [m]$, $\\theta_1$ strictly dominates $\\theta_2$, denoted as $\\theta_1 \\ll \\theta_2$.\nDefinition 2 ((Weak) Pareto optimality). A solution $\\theta^* \\in \\Theta$ is Pareto optimal if there is no $\\theta \\in \\Theta$ such that $\\theta \\prec \\theta^*$. A solution $\\theta' \\in \\Theta$ is weakly Pareto optimal if there is no $\\theta \\in \\Theta$ such that $\\theta \\ll \\theta'$.\nAll (weakly) Pareto optimal solutions form the (weak) Pareto optimal set, whose image in the objective space forms the (weak) Pareto Front (PF). We also have the following definition of Pareto stationarity:\nDefinition 3 (Pareto stationarity). A solution $\\theta^* \\in \\Theta$ is Pareto stationary if there exists a vector $\\alpha \\in \\Delta_m$ such that $\\sum_{i=1}^m \\alpha_i \\nabla f_i(\\theta) = 0$.\nPareto stationarity is a necessary condition for optimality, and a sufficient condition if objectives are convex and every entry of $\\alpha$ is non-zero (Miettinen, 1998).\nScalarization is a classic technique to tackle multiple objectives. It finds a particular Pareto optimal solution by converting (1) into a scalar optimization task given a preference vector and a scalarizing function. Two popular methods are linear scalarization (LS) (Geoffrion, 1967) and Tchebycheff scalarization (TCH) (Bowman Jr, 1976) 1:\n$\\min_{\\Theta \\in \\Theta} LS(\\theta; w) = \\min_{\\Theta \\in \\Theta} \\sum_{i=1}^m w_i f_i(\\theta)$,\n$\\min_{\\Theta \\in \\Theta} TCH(\\theta; w) = \\min_{\\Theta \\in \\Theta} \\max_{i \\in [m]} w_i f_i(\\theta)$,\nwhere $w \\in \\Delta_m$ is a user-defined preference vector. It controls the trade-off among objectives of the specific optimal solution found. Even with the same w, the two methods do not necessarily give the same result: LS locates the optimal solution where w is a norm vector of the PF's supporting hyperplane (Boyd and Vandenberghe, 2004a), while TCH enjoys:\nTheorem 1 (Informal, Ehrgott (2005)). Suppose $\\theta^* = \\arg \\min_{\\Theta \\in \\Theta} TCH(\\theta;w)$, under mild conditions 2, $w_i f_i(\\theta^*) = w_j f_j(\\theta^*), \\forall i, j \\in [m]$."}, {"title": "2.2 Online Learning Algorithms", "content": "Online learning refers to scenarios where data arrives in a stream rather than as a fixed dataset (Zinkevich, 2003). The defining characteristic is that the current solution may no longer be optimal after new data is presented. This process can be modeled as a game between a player and an adversary: in each round $t \\in {1,...,T}$, the player outputs a solution $x^{(t)} \\in \\mathcal{X}$ based on past data and the adversary responds with a function $l^{(t)} : \\mathcal{X} \\rightarrow \\mathbb{R}$ that imposes a loss $l^{(t)}(x^{(t)})$. The goal is to minimize the regret:\n$Regret(T) = \\sum_{t=1}^T l^{(t)}(x^{(t)}) - \\min_{x \\in \\mathcal{X}} \\sum_{t=1}^T l^{(t)}(x)$.\nOnline algorithms can also be applied to batch learning scenarios, with an online-to-batch conversion that calculates a deterministic solution from iterates ${x^{(t)}}$ and transforms a regret guarantee on (4) into a convergence rate guarantee. The classic conversion is uniform averaging, i.e., $\\bar{x} = \\frac{1}{T} \\sum_{t=1}^T x^{(t)}$, which assures a diminishing convergence rate w.r.t. T given a sublinear regret bound (Cesa-Bianchi et al., 2004).\nOnline mirror descent is a fundamental online learning algorithm (Nemirovskij and Yudin, 1983; Hazan et al., 2016). The general update rule is:\n$x^{(t+1)} = \\arg \\min_{x \\in \\mathcal{X}} \\langle \\nabla l^{(t)}(x^{(t)}), x \\rangle + \\frac{1}{\\eta} B_{\\psi}(x; x^{(t)})$,\nwhere $\\langle , \\rangle$ denotes the inner product, $\\eta$ is the step size, and $B_{\\psi}$ is the Bregman divergence induced by a convex function $\\psi : \\mathcal{X} \\rightarrow \\mathbb{R}$. With different choices of $\\psi$, (5) can be instantiated into different forms. Induced by the l-2 norm and negative entropy respectively, (5) are instantiated as projected gradient descent (PGD) (6) and exponentiated gradient (EG) (7):\n$x^{(t+1)} = \\Pi_{\\mathcal{X}}(x^{(t)} - \\eta \\nabla l^{(t)}(x^{(t)}))$,\n$x^{(t+1)} = \\frac{x^{(t)} \\exp(-\\eta l^{(t)}(x^{(t)}))}{\\sum_{i=1}^d x_i^{(t)} \\exp(-\\eta l^{(t)}(x^{(t)}))}$,\nwhere $\\Pi_{\\mathcal{X}} : \\mathbb{R}^d \\rightarrow \\mathcal{X}$ is a projection function to the feasible domain of $\\mathcal{X}$."}, {"title": "3 (ADA)OMD-TCH", "content": "We now present our proposed methods. Section 3.1 formulates Tchebycheff scalarization into a dual online learning problem and introduces its solver OMD-TCH. Section 3.2 states the adaptive online-to-batch conversion scheme and its algorithmic implementation AdaOMD-TCH. Section 3.3 provides convergence guarantees for both OMD-TCH and AdaOMD-TCH."}, {"title": "3.1 OMD-TCH", "content": "Derivation. Recall that our goal is to solve the general Tchebycheff scalarization problem (2). First, we formulate a key equivalent transformation:\n$\\min_{\\theta \\in \\Theta} \\max_{i \\in [m]} w_i f_i(\\theta) = \\min_{\\theta \\in \\Theta} \\max_{\\lambda \\in \\Delta_m} \\sum_{i=1}^m \\lambda_i (w_i f_i(\\theta))$,\nusing the fact that $\\max_{i \\in [m]} \\sum_{i=1}^m \\delta_{ic} c_i = \\max_i c_i$, for any scalers $c_i \\in \\mathbb{R}$. As such, the feasible region of the inner maximization problem is converted from a discrete set [m] to the entire simplex $\\Delta_m$, supporting our smoothing out the one-hot optimization. In the followings, we use $L(\\theta, \\lambda; w) := \\sum_{i=1}^m \\lambda_i (w_i f_i(\\theta))$.\nWe then interpret (8) as a zero-sum game between two parameters, $\\theta$ and $\\lambda$, that attempt to minimize and maximize a shared objective respectively. This adversarial relationship makes optimization for each parameter an online learning task - from the perspective of $\\theta$, $\\lambda$ is the adversary and $L(\\cdot, \\cdot; w)$ is the time-varying loss function. A similar analysis applies to $\\lambda$ being the player. Such dual problem can be solved by applying OMD to $\\theta$ and $\\lambda$ separately. For $\\theta$, we adopt PGD: setting $l_\\theta^{(t)} = L(\\theta, \\lambda^{(t+1)}; w)$ (with $\\lambda^{(t+1)}$ obtained before $\\theta^{(t+1)}$), we derive from (6):\n$\\theta^{(t+1)} = \\Pi_{\\Theta} (\\theta^{(t)} - \\eta_\\theta \\lambda^{(t+1)} \\circ w \\circ \\nabla f(\\theta^{(t)}))$,\nwhere $\\circ$ is the Hadamard product. When the feasible region is unconstrained, as in most cases for machine learning model parameters, $\\Pi_{\\Theta}$ can be omitted and PGD reduces to gradient descent. For $\\lambda$ restricted to a simplex, both PGD (10) and EG (11) are applicable. Setting $l_\\lambda^{(t)} = L(\\theta^{(t)}, \\cdot; w)$, we have from (6) and (7):\n$\\lambda^{(t+1)} = \\Pi_{\\Delta_m}(\\lambda^{(t)} + \\eta_\\lambda \\nabla_{\\lambda} L(\\theta^{(t)}, \\lambda; w))$,\n$\\lambda^{(t+1)} = \\frac{\\lambda \\exp(\\eta_\\lambda w_i f_i(\\theta^{(t)}))}{\\sum_{j=1}^m \\lambda_j \\exp(\\eta_\\lambda w_j f_j(\\theta^{(t)}))}$.\nAnalysis. The update rule (9) for $\\theta$ involves gradients of all objectives, weighted by both the a priori preference vector w and the dynamic $\\lambda$. By Equations (10) and (11), $\\lambda$ is iteratively updated such that larger weights are assigned to objectives with larger accumulated losses, and how fast the influence of earlier iterations decays can be tuned through the step size $\\eta_\\lambda$. As such, OMD-TCH smooths out the one-hot optimization while still prioritizing undertrained targets.\nReaders may wonder about the difference between using PGD and EG for $\\lambda$. In Section 3.3, we show that using EG offers a theoretical advantage in the convergence rate's dependency on the number of objectives. However, we observe no significant distinction between their empirical performance as shown in Section 4.\nComparison. We establish a connection between OMD-TCH using EG for $\\lambda$ and smooth Tchebycheff scalarization (Lin et al., 2024). Consider the gradient descent update rule of (3) :\n$\\theta^{(t+1)} = \\theta^{(t)} - \\eta_\\theta \\alpha^{(t+1)} \\circ w \\circ \\nabla f(\\theta^{(t)}))$,\n$\\alpha^{(t+1)} = \\frac{\\exp(w_i f_i(\\theta^{(t)})/\\mu)}{\\sum_{j=1}^m \\exp(w_j f_j(\\theta^{(t)})/\\mu)}$.\nCompared to (9)+(11), with $\\mu$ and $\\eta_\\lambda$ both being scaling constants, the only difference lies in the weights for combining gradients, i.e., $\\lambda^{(t+1)}$ and $\\alpha^{(t+1)}$. While $\\alpha^{(t+1)}$ is solely determined by current losses, $\\lambda^{(t+1)}$ additionally incoporates historic information through the previous iterate $\\lambda^{(t)}$. This \u201cbuffering\u201d mechanism makes $\\lambda$ change less drastically, which can be considered as a result of OMD's inherent property of regularizing distances between consecutive iterates.\nFrom another perspective, by setting the step size $\\eta_\\lambda$ to 0, $\\lambda$ becomes a static vector initialized uniformly. In this case, (9) is equivalent to linear scalarization with preference w. OMD-TCH can therefore also be interpreted as a middle ground between LS and TCH."}, {"title": "3.2 Adaptive Online-to-Batch Conversion", "content": "Like previous works applying OMD to the minimax fairness objective (He et al., 2024), Algorithm 1 adopts the traditional online-to-batch conversion that outputs the uniformly averaged solution $\\bar{\\theta}$. Although it enjoys a convergence guarantee, as later stated in Section 3.3, it is unclear whether the conversion should involve every iterate, especially those dominated by new solutions as training proceeds. Motivated by this concern, we propose an adaptive conversion scheme that selectively excludes suboptimal iterates while retaining the same convergence guarantees. For readers to quickly grasp the key idea of this adaptive scheme, we first present the properties of its final solution before elaborating on the algorithmic implementation."}, {"title": "3.3 Theoretical Convergence Guarantees", "content": "In this section, we formally state the convergence guarantees for OMD-TCH and AdaOMD-TCH. In regard to the consistently observed superiority of batch and stochastic gradient descent over their deterministic counterpart, we directly consider the stochastic setting. In the followings, we use $\\delta f_i(\\theta)$ to denote the stochastic gradient computed on a randomly sampled batch and $f_i(\\theta)$ to denote the true gradient. The $\\nabla$'s in Algorithms 1 and 2 should be replaced with the corresponding $\\delta$'s. We adopt the following assumptions commonly used in convergence analysis:\nAssumption 1. With $||x||_\\infty = \\max_{i \\in [d]} x_i$ for $x \\in \\mathbb{R}^d$ being the infinity norm:\n1. Convexity: $\\forall i \\in [m]$, $f_i(\\theta)$ is convex in $\\theta$.\n2. Bounded objectives: $\\forall i \\in [m]$, $\\forall \\theta \\in \\Theta$, $f_i(\\theta) \\leq U$.\n3. Bounded gradients and stochastic gradients: $\\forall i \\in [m]$, $\\forall \\theta \\in \\Theta$, $||\\nabla f_i(\\theta)||_\\infty \\leq L$, $||\\$\\delta f_i(\\theta)||_\\infty \\leq L$.\n4. Bounded feasible region: $\\forall \\theta \\in \\Theta$, $||\\$\\theta||_\\infty < R_\\theta$.\nNow, we first present the general convergence rates using OMD instances induced by arbitrary distance generating functions $\\psi_\\theta$ and $\\psi_\\lambda$.\nTheorem 3. Suppose Assumption 1 holds. With $\\psi_\\theta$, $\\psi_\\lambda$, $\\eta_\\theta$, and $\\eta_\\lambda$ satisfying (1) $\\psi_\\theta$ is $\\mu_\\theta$-strongly convex, (2) $\\psi_\\lambda$ is $\\mu_\\lambda$-strongly convex, (3) $\\eta_\\theta = \\frac{\\sqrt{2 \\mu_\\theta D_\\theta}}{5 T C \\theta L^2}$, and (4) $\\eta_\\lambda = \\frac{\\sqrt{2 \\mu_\\lambda D_\\lambda}}{3 T C_\\lambda U^2}$, both Algorithms 1 and 2 enjoy the following convergence guarantee:\n$\\mathbb{E} [TCH(\\hat{\\theta}; w)] - \\min_{\\Theta \\in \\Theta} TCH(\\theta; w) \\leq \\frac{\\sqrt{2 D_\\theta C \\theta L^2}}{\\mu_\\theta \\sqrt{T}} + \\frac{\\sqrt{54 D_\\lambda C_\\lambda U^2}}{\\mu_\\lambda \\sqrt{T}}$.\nOn the LHS, $\\hat{\\theta}$ is either $\\bar{\\theta}$ or $\\tilde{\\theta}$, and the expectation is taken on the stochastic gradients. On the RHS, constants $D_\\theta, D_\\lambda, C_\\theta$, and $C_\\lambda$ depend on the choices of $\\psi_\\theta$ and $\\psi_\\lambda$, and are associated with the dimensions of $\\theta$ (i.e., d) and $\\lambda$ (i.e., m) respectively.\nWe provide the high-probability bound and proofs in Appendix A.1. Theorem 3 indicates that both OMD-TCH and AdaOMD-TCH converge at a rate of O(1/\\sqrt{T}), matching batch or stochastic gradient descent in the dependency on the iteration round T.\nTo see the dependency on d and m, we instantiate this general bound with specific choices of $\\psi_\\theta$ and $\\psi_\\lambda$: for $\\theta$, setting $\\psi_\\theta(\\theta) = ||\\theta||^2$, we have projected gradient descent; for $\\lambda$, we adopt either PGD or exponentiated gradient with $\\psi_\\lambda = \\frac{1}{2}||\\lambda||^2$ or $\\psi_\\lambda = \\sum_{i=1}^m \\lambda_i \\log \\lambda_i$, i.e., the negative entropy. The specific bounds are:\nTheorem 4. Suppose Assumption 1 holds. Using PGD for both $\\theta$ and $\\lambda$, with optimal step sizes $\\eta_\\theta = \\frac{4 R_\\theta}{\\sqrt{5 T L^2}}$ and $\\eta_\\lambda = \\frac{2}{\\sqrt{3 T m U^2}}$, both Algorithms 1 and 2 converge as follows, where $\\hat{\\theta}$ is either $\\bar{\\theta}$ or $\\tilde{\\theta}$:\n$\\mathbb{E} [TCH(\\hat{\\theta}; w)] - \\min_{\\Theta \\in \\Theta} TCH(\\theta; w) \\leq \\frac{2\\sqrt{10} d R_\\theta L}{\\sqrt{T}} + \\frac{6\\sqrt{3} \\sqrt{m} U}{\\sqrt{T}}$.\nTheorem 5. Suppose Assumption 1 holds, using PGD for $\\theta$ and EG for $\\lambda$, with optimal step sizes $\\eta_\\theta = \\frac{4 R_\\theta}{\\sqrt{5 T L^2}}$ and $\\eta_\\lambda = \\frac{8 \\sqrt{R_\\theta^2 + 2 \\log m}}{3 T U^2}$, both Algorithms 1 and 2 converge converge as follows, where $\\hat{\\theta}$ is either $\\bar{\\theta}$ or $\\tilde{\\theta}$:\n$\\mathbb{E} [TCH(\\hat{\\theta}; w)] - \\min_{\\Theta \\in \\Theta} TCH(\\theta; w) \\leq \\frac{2\\sqrt{10} d R_\\theta L}{\\sqrt{T}} + \\frac{3\\sqrt{6} \\sqrt{\\log m} U}{\\sqrt{T}}$.\nProofs are provided in Appendix A.2. Comparing (16) and (17), one can see that using EG for $\\lambda$ gives a logarithm advantage in the convergence rate's dependency on the number of objectives m, as in O(\\sqrt{\\log m/T}) over O(\\sqrt{m/T}). Nevertheless, despite this theoretical superiority, we do not observe significant differences between the two instances in our experiments."}, {"title": "4 EXPERIMENTS", "content": "Our experimental studies mainly investigate two questions: (1) Can (Ada)OMD-TCH serve as a general MOO method for finding optimal solutions with diverse tradeoffs? (2) Is the proposed adaptive online-to-batch conversion scheme effective in improving practical performance over its uniform averaging counterpart? We provide affirmative evidence to these questions in Sections 4.1 (synthetic problem) and 4.2 (fair federated learning) respectively."}, {"title": "4.1 Finding Solutions with Diverse Trade-offs", "content": "In this section, we verify the ability of (Ada)OMD-TCH to find a diverse set of Pareto optimal solutions given different preference vector w's.\nProblem setup. We adopt the widely studied VLMOP2 problem (Van Veldhuizen and Lamont, 1999) which has a non-convex PF with:\n$f_1(\\theta) = 1 - \\exp(-\\sum_{i=1}^d (\\theta_i - \\frac{1}{\\sqrt{d}})^2)$,\n$f_2(\\theta) = 1 - \\exp(-\\sum_{i=1}^d (\\theta_i + \\frac{1}{\\sqrt{d}})^2)$,\nwhere $\\theta \\in \\mathbb{R}^d$ (d = 10). Ten preferences w are evenly spaced between [0,1] and [1,0]^T. Experiments are conducted using MOO library (Zhang et al., 2024b), with (Ada)OMD-TCH implemented in its framework. More details are listed in Appendix B.1.\nResults. Figure 2 illustrates the solutions found by linear scalarization (LS), Tchebycheff scalarization (TCH), and (Ada)OMD-TCH with (projected) gradient descent applied to the dynamic $\\lambda$. Results are evaluated on averaged decision variable $\\theta$'s found with each preference vector over 3 random seeds. As expected, LS can only locate the two endpoints on the PF and completely misses the non-convex regions, while TCH yields equally scattered diverse solutions, which, by Theorem 1, are the intersection points of the PF and rays corresponding to the (element-wise)inverse of w. Proposed as smooth solvers for TCH, we desire solutions produced by (Ada)OMD-TCH to resemble those found by vanilla TCH. As shown, both OMDgd-TCH and AdaOMDgd-TCH successfully recover the diverse solutions with minor errors. Full results for individual seeds and (Ada)OMDeg-TCH where $\\lambda$ is updated with exponentiated gradient are listed in Appendix B.1."}, {"title": "4.2 Fair Federated Learning", "content": "One important application of MOO is to find the balanced trade-off optimal solution. Solving Tchebycheff scalarization with uniform preference is one natural approach by Theorem 1. Thus, we apply (Ada)OMD-TCH to such a problem - the accuracy-fairness challenge in Federated Learning (FL), where each client represents an objective, and achieving fairly equal performances across clients while maintaining overall accuracy is desired. As OMD has been adopted to solve this uniform preference case (Mohri et al., 2019), we particularly focus on how the adaptive conversion scheme improves over traditional conversion. As we will show, AdaOMD-TCH demonstrates comparable results to other state-of-the-art fair FL methods.\nBaseline methods. We include FedAvg (McMahan et al., 2017b), and 6 state-of-the-art client-level fair FL methods: qFFL (Lin et al., 2019), AFL (Mohri et al., 2019), TERM (Li et al., 2020), FedFV (Wang et al., 2021), PropFair (Zhang et al., 2022), and FedMGDA+ (Hu et al., 2022b). Due to the same problem nature, many of them can be interpreted as a corresponding MOO method with uniform preference: FedAvg represents linear scalarization (LS), TERM has a highly similar loss function to smooth Tchebycheff (STCH), and FedMGDA+ is a mixture of LS and multiple gradient descent. In particular, AFL is equivalent to OMD-TCH with projected gradient descent, yet outputs the traditional time-averaged solution. The vanilla Tchebycheff scalarization (TCH) is also included. For a fair comparison, we focused on generic FL methods that train a shared global model rather than personalized approaches.\nData and models. We consider MNIST (Deng, 2012) and CIFAR10 (Krizhevsky and Hinton, 2009) datasets, and for each dataset, we simulate three scenarios of FL data heterogeneity for m = 10 clients, following Ghosh et al. (2020); Collins et al. (2021):"}, {"title": "5 CONCLUSION", "content": "We propose OMD-TCH, a general solver for Tchebycheff scalarization in multi-objective optimization using online mirror descent that smooths out the hard one-hot update. We further propose AdaOMD-TCH by replacing the uniform online-to-batch conversion with a novel adaptive scheme that selectively discards suboptimal iterates while retaining the same convergence guarantees. Specifically, we prove an O(\\log m/T) convergence rate for both OMD-TCH and AdaOMD-TCH in the stochastic setting. Empirically, (Ada)OMD-TCH effectively recovers diverse solutions on a non-convex PF. When applied to fair federated learning problems, AdaOMD-TCH significantly improves over OMD-TCH, achieving state-of-the-art results in both accuracy and fairness metrics. We believe our methods serve as a powerful tool for multi-objective optimization and algorithmic fairness, and offer a new perspective in online-to-batch conversion."}, {"title": "3: The Corresponding High-Probability Bound and Proofs", "content": "Theorem 6 (High-probability bound for Theorem 3). Suppose Assumption 1 holds. With the same choices of $\\psi_\\theta$", "guarantee": "with probability at least 1 - $\\gamma$", "w)": "sum_{i=1"}, "m \\lambda_i (w_i f_i(\\theta))$.\nLemma 1. Suppose Assumption 1 holds and $\\tilde{\\theta} = \\frac{1}{T} \\sum_{\\theta^{(\\tau)} \\in \\mathcal{P}} \\gamma_\\tau \\theta^{(\\tau)}$ is the output of Algorithm 2 on iterates {$\\theta^{(t)}$}$_{t=1}^T$ and {$\\lambda^{(t)}$}$_{t=1}^T$. Then, for any $\\lambda$, $w \\in \\Delta_m$,\n$\\frac{1}{T} L(\\tilde{\\theta}, \\lambda; w) \\leq \\frac{1}{T} \\sum_{t=1}^T L(\\theta^{(t)}, \\lambda; w)$.\nProof. First, by Assumption 1, $f_i(\\theta)$ is convex in $\\theta$, and hence so is $L(\\theta, \\lambda; w)$. By convexity,\n$\\frac{1}{T} L(\\tilde{\\theta}, \\lambda; w) \\leq \\frac{1}{T} \\sum_{\\theta^{(\\tau)} \\in \\mathcal{P}} \\gamma_\\tau L(\\theta^{(\\tau)}, \\lambda; w)$.\nNow, to prove (21), it suffices to prove:\n$\\frac{1}{T} \\sum_{\\theta^{(\\tau)} \\in \\mathcal{P}} \\gamma_\\tau L(\\theta^{(\\tau)}, \\lambda; w) \\leq \\frac{1}{T} \\sum_{t=1}^T L(\\theta^{(t)}, \\lambda; w)$.\nIn fact, this inequality naturally stems from how {$\\gamma_\\tau$} are constructed through weight re-allocation, as stated in the properties"]}