{"title": "ProgCo: Program Helps Self-Correction of Large Language Models", "authors": ["Xiaoshuai Song", "Yanan Wu", "Weixun Wang", "Jiaheng Liu", "Wenbo Su", "Bo Zheng"], "abstract": "Self-Correction aims to enable large language models (LLMs) to self-verify and self-refine their initial responses without external feedback. However, LLMs often fail to effectively self-verify and generate correct feedback, further misleading refinement and leading to the failure of self-correction, especially in complex reasoning tasks. In this paper, we propose Program-driven Self-Correction (ProgCo). First, program-driven verification (ProgVe) achieves complex verification logic and extensive validation through self-generated, self-executing verification pseudo-programs. Then, program-driven refinement (ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement on both responses and verification programs to mitigate misleading of incorrect feedback in complex reasoning tasks. Experiments on three instruction-following and mathematical benchmarks indicate that ProgCo achieves effective self-correction, and can be further enhance performance when combined with real program tools.", "sections": [{"title": "1 Introduction", "content": "Although large language models (LLMs) have shown excellent performance on certain tasks, they still face challenges such as hallucinations and unfaithful reasoning when solving complex instruction-following and reasoning tasks (Zhao et al., 2023; Chang et al., 2024). Self-correction is an expected capability of LLMs, wherein the LLM first needs to reflect on its initial output, identify potential issues and generate feedback (self-verification phase), which then guides the LLM to optimize and refine its output (self-refinement phase), as illustrated in Fig 1 (Pan et al., 2024; Kumar et al., 2024). However, studies have shown that current LLMs severely lack this capability and struggle to achieve effective self-correction in the absence of external feedback (also called intrinsic self-correction), particularly in complex tasks (Li et al., 2024; Huang et al., 2024; Tyen et al., 2024; Kamoi et al., 2024).\nA core reason for the failure of self-correction in LLMs is their inability to effectively self-detect problematic outputs and generate high-quality feedback (Tyen et al., 2024). Existing works mainly used two approaches for self-verification: (1) prompting LLMs to perform step-by-step self-checks (Madaan et al., 2024), or (2) generating a checklist based on the task and then checking responses against this list (Zhang et al., 2024c; Cook et al., 2024). However, on one hand, LLMs often exhibit overconfidence, making it difficult for them to identify their own errors or hallucinations, resulting in a low recall of incorrect responses (Zhang et al., 2024c). On the other hand, for complex tasks, these methods struggle to parse intricate verification logic. For instance, checklists usually only express parallel relationships and examine superficial issues. Ineffective verification further leads to ineffective refinement. The inaccurate and low-quality error detection and feedback not only makes it challenging for LLMs to correct erroneous outputs in one attempt but also seriously mislead them into modifying from correct to incorrect for misrecalled responses. To overcome this issue, we propose Program-driven Self-Correction (ProgCo), achieving effective self-correction by incorporating self-generated and self-executed programs in the verification and refinement stages.\nWe first introduce Program-driven Verification (ProgVe) to achieve better self-verification. Dif-"}, {"title": "2 Method", "content": "2.1 Program-driven Verification\nGiven model M and input x, in the i-th iteration of self-correction, the goal of self-verification is to generate feedback fbi for response yi, indicating whether yi passes verification or the reason it fails.\nVerification Program Generation. After obtaining the initial response yo = M(x), we first use prompt Pren fuc to guide M in generating a verification pseudo-program function f. This process is independent of yo to ensure a different perspective and avoid biases from the response:\n$f = M(P_{gen}^{fuc}||x)$ (1)\nAs illustrated in Fig 2, for instruction-following task, f verifies a series of constraints extracted from x. For mathematical problems, f starts from the output answer and uses reverse reasoning to verify step-by-step whether it contradicts the given conditions in x.\nVerification Program Execution. For each round, we use prompt Pexec to instruct M act as fuc a code executor, taking yi as input, executing f step by step to obtain the execution result ri. ri is further converted to feedback fbi by prompt Pfb:\n$r_i = M(P_{exec}^{fuc}||x||y_i), fb_i = M(P_{fb}||x||y_i||r_i)$ (2)\nIf the verification passes, self-correction stops and yields the final output Yfinal = Yi. Otherwise, if fbi indicates that yi fails to meet constraints or contains contradictions, the process enters the self-refinement stage until maximum rounds I are reached.\n2.2 Program-driven Refinement\nThe vanilla self-refinement generates a new output Yi+1 = M(prefine||X||Yi||fbi) through prompt Prefine. This method is effective for tasks like instruction following, as constraints are apparent, and"}, {"title": "3 Experiment", "content": "3.1 Experiment Setup\nWe evaluate ProgCo on the instruction-following dataset IFEval (Zhou et al., 2023) and the mathematics datasets GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021)\u00b9. Since complex reasoning is not involved, we only use the combination of ProgVe and vanilla refinement for IFEval. We provide detailed datasets, baseline introductions, and implementation details in Appendix C.\n3.2 Main Result\nTable 1 shows the performance comparison between ProgCo and different self-correction baselines. 2 Overall, ProgCo outperforms all baselines with a large margin across three benchmarks. On GPT-3.5, ProgCo improves over the initial response by 4.62% (IFEval(Pr)), 3.23% (IFEval(Ins)), 5.84% (GSM8K), and 5.8% (MATH) with just one round of self-correction. After three rounds, the improvements further increase to 4.80% (IFEval(Pr)), 3.47% (IFEval(Ins)),7.28% (GSM8K) and 8.0% (MATH). Similar improvements are observed on other LLMs. From a task perspective, many baselines achieve positive improvements on IFEval but failed on mathematical tasks, while our method achieve significant positive improvements on both GSM8K and MATH, demonstrating its effectiveness in complex reasoning.\n3.3 Analysis\nIntegrating symbolic tools for LLM program-executor. Due to the advantage of program easily integrating with symbolic tools, we further indicate in Pexed that LLM executor can delegate complex numerical operations to a python tool to overcome the shortcomings of LLM. As shown in Table 2, this further improves ProgCo's performance.\nRecall of Self-Verification. Fig 4 shows the recall and F1-score for CoT-Check, Checklist, and ProgVe in identifying incorrect responses. Prog Ve outperforms the baselines in both Recall and F1-score significantly, with further improvements when combined with Python tool. This demonstrates the advantages of using self-play programs in verification, including expressing more complex structures, providing a different perspective, and integrate with symbolic tools.\nAblation on Self-Refinement. We conduct an ablation analysis of ProgRe in Table 3. First, removing contrast and regeneration leads to a significant increase in the ratio of correction to incorrec-tion, proving that this strategy can effectively alleviate feedback misleading. Second, since wrong"}, {"title": "4 Conclusion", "content": "In this paper, we propose ProgCo, a program-driven self-correction method. ProgCo first self-generates and self-executes verification pseudo-programs for self-verification (ProgVe), then uses dual reflection and refinement of responses and programs for self-refinement (ProgRe). Experiments and analyses on three benchmarks demonstrate the effectiveness of ProgCo in self-correction."}, {"title": "Limitations", "content": "In this paper, we propose ProgCo and experimentally validate its effective self-correction. However, there are still some limitations as follows: (1) In terms of application scenarios, although using pseudo-program and LLM executors can extend the application scope beyond numerical and symbolic solving tasks, we primarily validated the effectiveness of ProgCo in instruction-following and mathematical tasks. (2) One advantage of using LLMs in executing verification programs is the integration of their own knowledge and causal logic. However, they are limited in large and precise numerical calculations. This issue can be mitigated by combining real symbolic tools, as shown in the experiment in Table 2. (3) Due to the lack of specialized training, we use detailed prompts to guide the LLM in completing tasks in ProgCo, which results in additional inference costs. Synthesizing data for each component of ProgCo and jointly training the LLM can replace the need for prompts and demonstration costs during inference."}, {"title": "A Related Work", "content": "Self-Correction. Self-correction aims to enable LLMs to achieve the ability to self-check and correct its outputs (Pan et al., 2024). Although some work (Shinn et al., 2024; Renze and Guven, 2024) achieves correction by relying on environmental feedback (such as True/False signals), many studies (Li et al., 2024; Huang et al., 2024; Tyen et al., 2024; Kamoi et al., 2024) have shown that in the complete absence of environmental feed-back, LLMs find it difficult to engage in effective self-reflection, with a particular emphasis on the challenge of identifying their own errors. To this end, some work (Han et al., 2024; Zhang et al., 2024a; Kumar et al., 2024) focuses on enhancing LLMs' self-correction capabilities during the training phase through imitation learning or reinforcement learning, while another part (Madaan et al., 2024; Dhuliawala et al., 2024; Wu et al., 2024; Zhang et al., 2024c; Kim et al., 2024) focuses on designing reflection or correction pipelines during the inference phase to help LLMs examine and analyze their own outputs. In this work, we focus on the inference phase, and to the best of our knowledge, we are the first to introduce self-generated and self-executed pseudo-verification programs into self-verification and self-refinement, achieving effective self-correction.\nIntegration of LLM with Programs. Several studies have enhanced LLMs by introducing programs or symbolic solvers. Some works integrate code executor or symbolic solving tools within the LLM's forward reasoning to address mathematical or symbolic reasoning problems, such as PAL (Gao et al., 2023), POT (Chen et al., 2023), and TORA (Gou et al., 2024). Others (Chen et al., 2024; Dong et al., 2024b; Qiao et al., 2024b; Dong et al., 2024a) use programs to assist in data synthesis or training; for example, AutoIf (Dong et al., 2024a) filters synthetic training data through testing programs. Additionally, Lyu et al. (2024) explore the capability of LLMs to act as code interpreters to execute LeetCode programs. Distinguishing these studies, our work is unique in that it: (1) employs verification programs for the self-correction phase; (2) utilizes pseudo-programs that do not require strict executability; (3) enables LLMs to self-generate and self-execute programs without the necessity for actual symbolic tools.\nInference Framework. Unlike directly generating answers, many studies have explored enhancing LLMs' ability to solves complex tasks through reasoning frameworks. Decomposition-based methods like COT, TOT, and POT (Wei et al., 2022; Yao et al., 2024a; Chen et al., 2023; Qiao et al., 2024a) guide models to break down tasks step-by-step, while sampling-based methods (Wang et al., 2022; Zhang et al., 2024b; Dong et al., 2024c) explore diverse reasoning paths and select the most consistent or optimal response. Another key approach is the mechanism of reflection and self-correction (Zhang et al., 2024c; Shinn et al., 2024; Pan et al., 2024), which encourages models to iteratively evaluate and refine their responses. Additionally, to balance efficiency, some works (Yao et al., 2024b) dynamically combine fast and slow reasoning based on task complexity. ProgCo can be seen as a method of fast and slow reasoning, where RrogVe filters out complex tasks, and ProgRe uses dual optimization for slow reasoning. Moreover, since ProgCo focuses on the self-correction, it is orthogonal to forward reasoning methods like sampling and decomposition, and can be well combined with them."}, {"title": "B Details of Method", "content": "B.1 Details of Prompts\nWe present the prompts used for mathematical tasks in Figures 6, 7, 8, 9, 10, 11.\nB.2 Pseudo-code\nWe summarize the pseudo-code of ProgCo in Algorithm 1."}, {"title": "C Details of Experiment", "content": "C.1 Details of Datasets\nThe introduction to the evaluation datasets is as follows:\n\u2022 IFEval (Zhou et al., 2023): IFEval is one of the most commonly used instruction-following benchmark for LLMs, containing over 500 test samples and covering 25 types of atomic instructions. The Prompt(Strict) metric calculates the proportion of strict following of the input prompt across all samples. For a sample, the prompt is considered followed only if all atomic instructions in it are"}]}