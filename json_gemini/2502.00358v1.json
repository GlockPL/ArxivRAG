{"title": "Do Audio-Visual Segmentation Models Truly Segment Sounding Objects?", "authors": ["Jia Li", "Wenjie Zhao", "Ziru Huang", "Yunhui Guo", "Yapeng Tian"], "abstract": "Unlike traditional visual segmentation, audio-visual segmentation (AVS) requires the model not only to identify and segment objects but also to determine whether they are sound sources. Recent AVS approaches, leveraging transformer architectures and powerful foundation models like SAM, have achieved impressive performance on standard benchmarks. Yet, an important question remains: Do these models genuinely integrate audio-visual cues to segment sounding objects? In this paper, we systematically investigate this issue in the context of robust AVS. Our study reveals a fundamental bias in current methods: they tend to generate segmentation masks based predominantly on visual salience, irrespective of the audio context. This bias results in unreliable predictions when sounds are absent or irrelevant. To address this challenge, we introduce AVSBench-Robust, a comprehensive benchmark incorporating diverse negative audio scenarios including silence, ambient noise, and off-screen sounds. We also propose a simple yet effective approach combining balanced training with negative samples and classifier-guided similarity learning. Our extensive experiments show that state-of-the-art AVS methods consistently fail under negative audio conditions, demonstrating the prevalence of visual bias. In contrast, our approach achieves remarkable improvements in both standard metrics and robustness measures, maintaining near-perfect false positive rates while preserving high-quality segmentation performance.", "sections": [{"title": "1. Introduction", "content": "Audio-Visual Segmentation (AVS) aims to identify and segment sounding objects within visual scenes [12, 52]. This essential multimodal task mirrors a fundamental aspect of human perception: the integration of auditory and visual stimuli to focus attention on relevant sources [5, 39, 52]. For instance, when hearing a baby cry, people naturally locate the sound's visual source. Simulating this ability in machines could open up valuable cross-modal applications, such as improved multimedia analysis [1, 2, 18, 33], enhanced human-computer interaction [11, 19, 28, 43, 50], and autonomous systems capable of interpreting sound-emitting objects in complex environments [10, 37, 42].\nRecent years have witnessed remarkable progress in AVS. State-of-the-art (SOTA) methods leverage multimodal information, utilizing encoder-decoder structures with audio-visual interaction [52], multimodal transformer architectures [12, 24, 25], audio query-guided designs [25, 40], and strong vision foundation models [26, 32, 40, 46] like SAM [22]and Mask2Former [9]. These innovations have driven impressive performance on standard benchmark datasets: AVSBench-S4 and AVSBench-MS3 [52].\nHowever, a critical question arises: are these models truly performing audio-visual segmentation, or simply conducting visual segmentation with minimal audio integration? AVS, by definition, introduces a crucial constraint: only objects acting as sound sources should be segmented. For example, an AVS model should not segment a visually salient yet silent dog. Current AVS models, primarily trained and evaluated on \u201cpositive\u201d cases where visual objects correspond to audio cues, often neglect scenarios with unrelated sounds, such as silence or off-screen sources.\nTo systematically investigate whether AVS models truly integrate audio-visual information, we introduce AVS-Robust, a comprehensive benchmark comprising 4,932 single-source and 424 multi-source videos across 20 diverse object classes from AVSBench [52]. We incorporate four different audio conditions for each video: original audio, silence, ambient noise, and off-screen sounds. Each condition represents 25% of the evaluation scenarios. Our study reveals a concerning bias in existing SOTA methods: they tend to generate segmentation masks based primarily on visual salience, irrespective of the audio context. For instance, these models may segment an ambulance even in the presence of silence or unrelated ambient sounds, indicating an over-reliance on visual cues rather than genuine audio-visual integration (Fig. 1).\nBuilding upon these insights, we explore solutions to address this visual bias. While incorporating negative audio-visual pairs during training seems intuitive, this approach alone presents a challenge: without explicit guidance for audio-visual integration, models struggle to determine whether to segment objects based solely on visual information. To overcome this, we propose a debiasing approach with two key components: (1) Balanced Training with Negative Samples: Incorporating both positive and negative audio-visual pairs during training to expose models to a wider range of audio-visual relationships. (2) Classifier-Guided Similarity Learning: Utilizing a classifier to guide the model in learning effective audio-visual feature representations and promoting similarity between corresponding audio and visual features.\nExtensive experiments using our new benchmark yield several crucial findings. Recent SOTA methods, including SAMA-AVS [26], Stepping-Stones [29], and CAVP [8], consistently fail under negative audio conditions, exhibiting high False Positive Rates (FPR). When evaluated with our comprehensive metrics-such as G-mIoU, G-F, and G-FPR, as discussed in Sec. 3-these models show significant performance degradation compared to their reported results on standard benchmarks. In contrast, our approach achieves superior performance across all robustness metrics while maintaining competitive segmentation quality on positive audio inputs in both single- and multi-source scenarios.\nOur main contributions are summarized as follows:\n\u2022 We conduct a systematic study on audio robustness in AVS and introduce AVSBench-Robust along with our new robustness evaluation protocols. This benchmark rigorously evaluates AVS models under both standard conditions and challenging negative scenarios, assessing their ability to effectively integrate audio-visual information.\n\u2022 We propose a training strategy for robust AVS by incorporating diverse negative audio scenarios and employing classifier-guided similarity learning, which enhances model robustness and preserves segmentation quality.\n\u2022 Extensive experiments demonstrate that our approach substantially outperforms current SOTA methods in terms of our robustness metrics while achieving competitive performance on standard AVS benchmarks."}, {"title": "2. Related work", "content": "Sound Source Localization. This task is closely related to AVS, focusing on localizing sound sources within visual scenes [2, 6, 30, 31, 38]. This task advances cross-modal understanding through various technical approaches, from basic feature fusion strategies to sophisticated attention mechanisms [17, 31, 35, 38]. Recent sound source localization approaches have significantly improved sound source discrimination through multiple innovations: contrastive learning with hard-mining strategies enhances complex region distinction [6, 17, 31], while class-aware approaches and dual-phase feature alignment enable robust multi-source localization without explicit pairwise annotations [6, 17, 35]. However, the predicted sounding object heatmaps lack the fine-grained precision offered by AVS's pixel-level segmentation capabilities.\nAudio-Visual Segmentation. AVS task focuses on iden-"}, {"title": "3. Problem and Benchmark", "content": "In this section, we first present the formulation of the AVS task and its associated challenges in Sec. 3.1. In Sec. 3.2, we introduce our new benchmark for robust AVS. Finally, we present our evaluation protocols in Sec. 3.3."}, {"title": "3.1. Task and Challenges", "content": "Given T non-overlapping video and audio clips {Vt, At}t=1T, the goal of the AVS task is to predict a segmentation mask $\\mathbf{M}_{t}^{\\text {pred }} \\in \\mathbb{R}^{H \\times W}$ that labels sounding pixels in each video frame of the clips, where H and W denote the frame dimensions, and the mask is binary. Following previous studies [12, 23, 52], we extract a single video frame at the end of each second and set T = 5 in practice, so each clip contains only one extracted frame."}, {"title": "3.3. Evaluation Protocols", "content": "A robust AVS model should not only accurately segment sound-producing objects but also reliably suppress predictions when no valid audio-visual correspondence exists. To enable this comprehensive evaluation, we propose new metrics that assess both aspects of AVS performance.\nLet P and N denote sets of positive and negative samples, respectively. For positive samples, following established protocols [12, 29, 52], we employ mean Intersection over Union (mIoU) and F-score to evaluate segmentation accuracy. For negative ones, we introduce complementary metrics to capture different aspects of model robustness.\nFalse Positive Rate (FPR):\n$\\text { FPR }=\\frac{\\sum_{x \\in N} m(x)}{H \\cdot W},\\qquad(1)$\nwhere m(x) denotes the binary indicator (0 or 1) for pixel x in the predicted mask. FPR measures the proportion of incorrectly activated pixels in negative scenarios, directly assessing the model's tendency to generate false predictions. To evaluate overall performance across both positive and negative cases, we propose three global metrics.\nGlobal mIoU (G-mIoU):\n$\\text { G-mIoU }=\\frac{2 \\text { mIoU}_{P} \\cdot\\left(1-\\text { mIoU}_{N}\\right)}{\\text { mIoU}_{P}+\\left(1-\\text { mIoU}_{N}\\right)},\\qquad(2)$\nwhere mIoUp is the mIoU for positive samples, and mIoUN is for negative samples. G-mIoU balances region-level accuracy, emphasizing the model's ability to maintain precise segmentation boundaries while suppressing false activations. A high score indicates accurate object delineation in positive cases and clean masks in negative cases.\nGlobal F-score (G-F):\n$\\text { G-F }=2 \\cdot \\frac{F_{P} \\cdot\\left(1-F_{N}\\right)}{F_{P}+\\left(1-F_{N}\\right)}\\qquad(3)$\nG-F provides a pixel-level assessment that equally weighs precision and recall, which is essential for evaluating both false positives and false negatives. This metric is particularly sensitive to small errors that may be overlooked by IoU-based measures.\nGlobal False Positive Rate (G-FPR):\n$\\text { G-FPR }=\\frac{1}{|N|} \\sum_{i \\in N} \\text { FPR }_{i}\\qquad(4)$\nThis metric specifically focuses on false activation suppression across all negative conditions. While G-mIoU and G-F balance positive and negative performance, G-FPR provides a dedicated measure of a model's robustness against different types of audio distractors.\nThe combination of these metrics provides a comprehensive evaluation framework: G-mIoU captures region-level accuracy, G-F ensures pixel-level precision, and G-FPR specifically measures robustness to negative conditions. Together, they enable thorough assessment of both segmentation quality and prediction suppression capabilities."}, {"title": "4. Method", "content": "In this section, we first present a framework overview in Sec. 4.2. Upon the framework, we detail our approach to address the bias problem in AVS through three key components: balanced audio-visual pair construction (Sec. 4.3), classifier-guided similarity learning (Sec. 4.4), and joint segmentation training (Sec. 4.5). To validate our approach, we apply it to two representative AVS models: AVSBench [52] and AVSegFormer [12]. The architecture of AVSBench is described in Sec. 4.1. Due to space constraints, implementation details for [12] are provided in the appendix."}, {"title": "4.1. Preliminary: AVS Architecture", "content": "Encoder: We employ an encoder structure that separately processes audio clip A and visual frames V. Specifically, input audio is converted into spectrograms and processed through a VGGish-based network [16], pre-trained on AudioSet [13], to generate audio feature FA \u2208 Rd where d = 128. For visual inputs V, we utilize a transformer-based backbone [45] to extract hierarchical visual features.Fv \u2208 \u2210i=1n Rhi\u00d7wi\u00d7Ci, where (hi, wi) = (H, W)/2i+1, i = 1,...,n. The number of levels is set to n = 4 in all experiments.\nCross-Modal Fusion: Following the work in [52], the fusion process involves an Atrous Spatial Pyramid Pooling (ASPP) module [7] that manipulates the visual feature maps to enhance object recognition capabilities in varying receptive fields. Subsequently, audio features are integrated to"}, {"title": "4.2. Framework Overview", "content": "Our framework, as illustrated in Fig. 2, processes both positive and negative audio-visual pairs to learn robust correspondence for segmentation. Built upon the presented AVS architecture, our model achieves balanced training by incorporating negative audio-visual pairs, enhancing robustness in AVS. Within this framework, audio and visual features are extracted and used to compute cosine similarity scores for both positive pairs P and negative pairs N, allowing the model to differentiate aligned from unaligned audio-visual pairs. For mask prediction, we employ a segmentation module that combines a fusion module and an FPN decoder, enabling precise segmentation of sound-producing objects. The dual-stream design allows the model to accurately identify sound-relevant regions in complex scenes while suppressing predictions when no valid audio-visual correspondence exists. The following sections detail each component and their integration within the framework."}, {"title": "4.3. Learning with Balanced Audio-Visual Pairs", "content": "In real-world scenarios, audio-visual correspondence is inherently dynamic [3, 4, 51]. A visible object may or may not be producing sound at any given moment for instance, a person may be speaking or silent, and a car may be running or stationary. Additionally, sounds may come from off-screen sources or be ambient noise. This variability requires AVS models to learn true audio-visual association rather than assume that all visible objects are sound sources.\nExisting AVS models have been trained with predominantly positive audio-visual pairs, where audio and visual signals align, and salient objects are typically the sound sources. This encourages AVS models to rely solely on visual information, bypassing true multimodal integration.\nMotivated by this insight, we propose a critical requirement: models must be trained with both positive and negative audio-visual pairs. This balanced approach ensures that the model learns not only when to segment objects that make sounds but also, crucially, when to suppress segmentation predictions for visually salient but silent objects.\nGiven a video clip with its corresponding audio signal, we construct two types of pairs:\nPositive Pairs (P): Original audio-visual pairs where the audio corresponds to visual objects in the frame. These pairs represent valid correspondence cases and constitute the majority (approximately 90%) of training samples."}, {"title": "4.4. Classifier-Guided Feature Alignment", "content": "However, we observed that simply introducing negative pairs is insufficient to mitigate the visual bias, as show in Table 4. Due to the inherent bias in existing models, which often fail to effectively utilize audio information, the model tends to behave more like a purely visual segmentation model. Without explicit guidance, adding negative pairs can lead to confusion during training, as the model alternates between predicting object masks and empty masks. This ultimately degrades performance, not only on the original dataset but also in negative conditions, where the model may continue to produce object masks despite the absence of valid audio-visual correspondence.\nWhile balanced training with positive and negative pairs exposes the model to diverse scenarios, it needs explicit guidance to learn when audio and visual features truly correspond. To address this, we propose using a classifier to directly supervise audio-visual similarity learning, creating clear decision boundaries for correspondence detection.\nGiven multi-scale visual features $F_{i} \\in \\mathbb{R}^{h_{i} \\times w_{i} \\times C_{i}}$ from the backbone, we use the final-stage features $F_{4} \\in \\mathbb{R}^{h_{4} \\times w_{4} \\times C_{4}}$ and audio features $F_{a} \\in \\mathbb{R}^{D_{a}}$ for similarity computation. We project $F_{A}$ to $C_{4}$ dimensions via a linear layer and apply spatial pooling to F\u2081 to obtain aligned features $\\widehat{F}_{A}, \\widehat{F}_{V} \\in \\mathbb{R}^{C_{4}}$. Their correspondence is then computed through cosine similarity:\n$s\\left(\\widehat{F}_{A}, \\widehat{F}_{V}\\right)=\\cos \\left(\\widehat{F}_{A}, \\widehat{F}_{V}\\right).\\qquad(6)$\nWe then apply BCE loss to explicitly guide similarity learning in a contrastive manner:\n$\\begin{aligned}L_{B C E} = & \\frac{1}{|P|+|N|} \\sum_{j=1}^{|P|+|N|}\\left(y_{j} \\log \\left(\\sigma\\left(s_{j}\\right)\\right)\\right.\\\\&+\\left.\\left(1-y_{j}\\right) \\log \\left(1-\\sigma\\left(s_{j}\\right)\\right)\\right),\\end{aligned}\\qquad(7)$\nwhere \u03c3(\u00b7) is the sigmoid function, yj is the binary label (1 for positive pairs, 0 for negative pairs), and |P| + |N| is the total number of positive pairs and the total number of negative pairs respectively. By explicitly supervising the similarity learning, the BCE loss forces the model to maximize similarity for positive pairs (where valid audio-visual"}, {"title": "4.5. Joint Training with Segmentation", "content": "Our total loss objective function $\\mathcal{L}$ can be computed as follows:\n$\\mathcal{L}=\\lambda L_{B C E}+L_{\\text {Seg }},\\qquad(8)$\nwhere \\lambda is a balancing weight. Together, these loss terms enforce robust and effective learning in AVS models: 1) The first term determines whether segmentation should occur based on audio-visual correspondence; 2) The second term ensures correct segmentation masks when correspondence exists; 3) For negative pairs, the empty GT masks naturally guide the segmentation loss to suppress predictions.\nThis simple, well-motivated approach can achieve strong performance without relying on complex model modifications, making our method easier to implement, tune, and integrate with existing AVS architectures."}, {"title": "5. Experiment", "content": "5.1. Setup\nDataset. We utilize the AVSBench-Robust Benchmark for our evaluation, which is designed to rigorously assess AVS capabilities. Further details on the dataset specifics and video categories have been discussed in Sec. 3.\nBaselines. We benchmark our model against notable methods including AVSBench [52] and AVSegFormer [12], representing fusion-based and prompt-based approaches, respectively. We also compared our method with the CAVP [8], Stepping-Stones [29], SAMA-AVS [26] and COMBO [51]. These baselines allow us to demonstrate the broad applicability of our method by comparing it against state-of-the-art models designed to address different aspects of audio-visual segmentation.\nEvaluation Metrics. Evaluation metrics, including mIoU, F1 score, FPR, and G-mIou, G-F, G-FPR, are used to assess the segmentation accuracy and robustness of AVS models.\nImplementation: Our implementation for the AVSBench model employ the Pyramid Vision Transformer (PVT-v2) [45] pretrained on the ImageNet dataset [36] as the visual backbone, which processes video frames of size H\u00d7W = 224 \u00d7 224 and output multiple scales visual feature Fvi \u2208 Rhi\u00d7wi\u00d7Ci for i = 1,..., 4, The channel dimensions Ci correspond to {64, 128, 320, 512} for each respective scale. For audio input, we employ VGGish [16] pretrained on AudioSet [13] to extract features FA \u2208 R128 from each one-second audio clips. This model is trained using the Adam [20] optimizer with a learning rate of 1\u00d710\u22124, batch size of 4, and loss weighting factor \u03bb = 1."}, {"title": "5.2. Experimental Comparison", "content": "Our extensive experimental comparisons reveal several significant findings in AVS performance as shown in Table 2.\nSOTA methods fail under negative audio conditions, demonstrating a strong visual bias and ineffective audio-visual integration. Surprisingly, recent methods like Stepping-Stones [29] and CAVP [8] achieve nearly identical mIoU and F-scores regardless of whether the input audio is silent, irrelevant, or noisy. These methods consistently exhibit high False Positive Rates (FPR), ranging from 0.17 to 0.19 across all negative scenarios on AVSBench-S4, indicating a significant reliance on visual cues. This issue also impacts their global metrics, with G-mIoU scores between 28.19 and 35.03. These results suggest that these methods fail to effectively leverage audio information in this multimodal segmentation task. While this issue is somewhat less pronounced on the MS3 dataset, it remains present.\nOur method resolves bias while maintaining performance. When integrated with AVSBench [52], our approach performs comparable positive audio performance (mIoU: 78.1, F-score: 88.2) while achieving perfect robustness to negative audio inputs with an FPR of 0.00 across all negative conditions. Similarly, our AVSegFormer [12] variant demonstrates only minimal degradation in positive audio metrics while achieving perfect FPR scores. Most notably, our approach achieves superior global metrics, with our AVSBench variant reaching a G-mIoU of 87.672 and G-F score of 82.461, substantially outperforming existing methods. The consistent improvement in robustness across two very different AVS architectures demonstrates the effectiveness and generality of our approach. Fig. 3 provides examples of S4 dataset visualizations. Visualizations for"}, {"title": "5.3. Impact of Positive-Negative Pair Ratio", "content": "Our investigation into the ratio of positive to negative audio-visual pairs reveals important insights about training data composition for robust audio-visual segmentation.\nAs illustrated in Table 3, we found that introducing negative samples, even in small proportions, dramatically improves performance. Without negative samples, the baseline model shows poor performance with the G-mIoU of 35.032 and a high FPR of 0.186 on the S4 dataset. Introducing just 10% negative samples yields substantial gains, improving G-mIoU to 87.672 and reducing FPR to 0.000. Similar improvements are observed in the MS3 dataset, where G-mIoU increases from 59.468 to 65.427 and FPR drops from 0.072 to 0.001, demonstrating the crucial role of negative samples in developing robust models.\nWhile further increasing the proportion of negative samples (to 20% or 30%) maintains similar performance levels, the marginal gains are minimal compared to the 10% setting. For instance, the G-mIoU difference between 10% and 20% negative samples is less than 0.3% on S4 and 2.5% on MS3. Considering that adding 10% negative pairs only increases training time by approximately 10% while achieving nearly optimal performance, we adopt this ratio as our default setting, offering an efficient balance between robustness and training cost."}, {"title": "5.4. Abaltion Study", "content": "A common assumption might be that simply adding negative samples would enhance the model's ability to distinguish between sound-producing and non-sound-producing visual regions in AVS. To test this hypothesis, we conducted an ablation study using AVSBench [52] as the baseline, comparing configurations with and without negative samples and classifier guidance, as summarized in Table 4.\nNegative samples alone fail to address the bias problem. Our experimental results highlight the limitations of using only negative samples. Without explicit loss guidance, adding negative pairs not only fails to improve performance but can even lead to significant degradation, particularly on the challenging MS3 dataset. Here, G-mIoU drops from 59.47 to 55.49, and G-F decreases dramatically from 51.04 to 30.06. This degradation occurs because the model becomes confused when alternately exposed to scenarios requiring empty predictions and those with salient object mask predictions, ultimately compromising performance even on standard positive audio inputs.\nCombining negative samples with classifier guidance enables robust segmentation. Our full approach shows substantial improvements across all metrics. On the Robust S4 dataset, we achieve G-mIoU of 87.672, G-F of 82.461, and perfect G-FPR of 0. Similar gains are observed on MS3, with G-mIoU of 66.605, G-F of 70.590, and near-perfect G-FPR of 0.004. The effectiveness of classifier guidance is illustrated in Fig. 4: initially, audio-visual feature similarities cluster around 0.5 for both positive and negative pairs; after training, they are well-separated (0.75 for positive vs. 0.30 for negative), demonstrating enhanced discrimination. This improved feature alignment enables strong performance on positive cases while accurately suppressing predictions in negative scenarios. The classifier guidance serves as a critical learning framework for effectively utilize negative samples while maintaining its original capabilities, resulting in a robust AVS system.\nFurther experiments on unseen audio categories demonstrate the generalization capability of our approach. Due to space constraints, we refer readers to the supplementary material for detailed results."}, {"title": "6. Conclusion and Discussion", "content": "Our comprehensive study using AVSBench-Robust reveals that current SOTA methods exhibit strong visual bias, generating segmentation masks based predominantly on visual salience regardless of audio context. To address this issue, we introduce a simple yet effective approach combining balanced training with negative audio-visual pairs and classifier-guided feature alignment, which significantly improves model robustness while maintaining competitive performance on standard AVS tasks. While our method effectively addresses the robustness issue, several challenges remain. Our approach is constrained by the baseline model's performance on positive samples, and real-world applications may encounter even more challenging conditions than those covered in our benchmark. We hope our work could inspire further research in this significant and worthwhile field."}, {"title": "A. Sensitivity to hyperparameter choices", "content": "We evaluate the model's sensitivity to the classifier guidance weight \u03bbBCE by varying its value from 0.2 to 1.0."}, {"title": "A.2 Multi-Source (MS3) Dataset Visualizations", "content": "Figure 5 illustrates our method's performance on complex multi-source scenarios. The visualizations demonstrate segmentation results under various audio conditions: original audio (positive), silence, noise, and off-screen sounds, providing qualitative evidence of our model's effectiveness in handling diverse acoustic environments."}, {"title": "D. AVSegFormer Implementation Details", "content": "AVSegFormer [12] introduces several key architectural innovations while maintaining some fundamental components from AVSBench [52]. As illustrated in Figure 8, the framework consists of five main components: audio-visual backbone encoders, a query generator that produces audio-conditioned queries, a transformer encoder for multi-scale"}, {"title": "Ltotal = LIoU + A1Lmix + ABCE LBCE", "content": "where LIoU guides the similarity learning between audio and visual features, helping the model distinguish between valid and invalid audio-visual correspondences through contrastive learning. The weighting factors A\u2081 = 0.1 and ABCE = 1.0 balance the contributions of each loss component."}, {"title": "D.3 Training Details", "content": "Following AVSegFormer [12]'s setting, we adjust the input resolution to 512 \u00d7 512 to better capture detailed visual information. This model's training employs the AdamW optimizer [27], with a batch size of 1 and an initial learning rate of 2 \u00d7 10\u22125. The encoder and decoder consist of 6 layers each, with an embedding size of 256. The training protocol extends to 60 epochs for the MS3 dataset and 30 epochs for the S4 dataset, conducted on an NVIDIA RTX A6000 GPU."}]}