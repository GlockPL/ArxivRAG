{"title": "LEAD: Latent Realignment for Human Motion Diffusion", "authors": ["N. Andreou", "X. Wang", "V. Fern\u00e1ndez Abrevaya", "M.P. Cani", "Y. Chrysanthou", "V. Kalogeiton"], "abstract": "Our goal is to generate realistic human motion from natural language. Modern methods often face a trade-off between model expressiveness and text-to-motion alignment. Some align text and motion latent spaces but sacrifice expressiveness; others rely on diffusion models producing impressive motions, but lacking semantic meaning in their latent space. This may compromise realism, diversity, and applicability. Here, we address this by combining latent diffusion with a realignment mechanism, producing a novel, semantically structured space that encodes the semantics of language. Leveraging this capability, we introduce the task of textual motion inversion to capture novel motion concepts from a few examples. For motion synthesis, we evaluate LEAD on HumanML3D and KIT-ML and show comparable performance to the state-of-the-art in terms of realism, diversity, and text-motion consistency. Our qualitative analysis and user study reveal that our synthesized motions are sharper, more human-like and comply better with the text compared to modern methods. For motion textual inversion, our method demonstrates improved capacity in capturing out-of-distribution characteristics in comparison to traditional VAEs.", "sections": [{"title": "1. Introduction", "content": "Text-to-motion (T2M) generation is the process of creating human-like motion that reflects a given language instruction. Generating motions that comply with textual descriptions is a task that received significant attention [CJL*23, TRG*23, PBV22, APBV22] due to its potential to democratize 3D content creation and its numerous applications in fields such as robotics [PMA18], entertainment [HSK16, HKS17] and virtual reality [BRB*21].\nA text-to-motion model should be able to accurately reproduce arbitrary descriptions in natural language while accounting for the many-to-many nature of the problem. This is a challenging task, since there is a large discrepancy between the space of natural language and the space of human motions (i.e. skeletal poses) [TGH*22, AM19, CJL*23]. Solutions can be classified into two main categories. Initial works such as TEMOS [PBV22], L2JP [AM19] and MotionCLIP [TGH*22] build a common la-"}, {"title": "2. Related Work", "content": "Human motion synthesis can be split into unconditional and conditional. Unconditional synthesis [YLX*19, ZBT20, ZSJ20] models the entire manifold of possible motions, while conditional synthesis introduces constraints such as audio or text that guide the generation. For a complete overview of motion synthesis, we refer the reader to [MHLC*21, KAK*22]. Here, we focus on conditional synthesis using multimodal constraints.\nMultimodal Motion Synthesis. To condition motion generation, research works have explored the use of text [PBV21, PBV22, GZW*20, GZZ*22], images [GWE*20, RBH*21], audio [GBK*19], music [TCL23, AYA*21, LYL*19] and scenes [HCV*21, SZZK21]. Generating motion from text is an intuitive way to produce 3D content and has received significant attention. Initial works targeted the action-to-motion task [PBV21, GZW*20, LBWR22] that produces motions depicting a single action. [GZW*20] propose a temporal-VAE based on GRUs to produce diverse motions, with a disentangled representation to better capture the kinematic properties. [PBV21] designs a transformer-based VAE with learnable tokens for each action, while [LBWR22] compress motion into a discrete latent space and realize future states as next-index predictions.\nInstead of relying on a fixed set of action categories, several works incorporate a text-encoder that transforms natural language into a latent space that acts as the conditioning signal [ZZC*23, GZWC22, GZZ*22, BRB*21, JCL*24]. Guo et al. [GZZ*22] first learn motion codes using a motion autoencoder and then use a recurrent VAE to map the text condition to a motion snippet code sequence. Similarly, Zhang et al. [ZZC*23] learn a mapping from motion to discrete codes using a VQ-VAE [vdOVK17], and use a transformer module to generate motion indices using text. MotionGPT [JCL*24] extends this idea to a versatile framework designed to address a variety of motion-language related tasks such as motion<->text. Some text-to-motion works align the motion-text spaces directly [AM19, PBV21, PBV22, GCO*21, APBV22]. JL2P [AM19] learns a joint pose-language space with a cross-modal loss, while [GCO*21] follow a similar approach but construct two separate manifolds for the upper and lower body. TEMOS [PBV22] bypasses the need for two manifolds and ensures diverse sampling by encoding distribution parameters using a VAE and a pre-trained language model. To enable sequential generation of motions, TEACH [APBV22] augments the text-encoder branch of TEMOS to account for temporal compositions."}, {"title": "3. Preliminaries", "content": "Diffusion models are a class of generative models based on\nprogressively corrupting data $x_0$, where the noising process\n${x_t}_{t=0}^T$ follows the Markovian principle:\n$q(x_t | x_{t-1}) = \\mathcal{N}(\\sqrt{\\alpha_t}x_{t-1}, (1 - \\alpha_t)I),$ (1)\nwith $\\alpha_t \\in (0, 1)$ hyperparameters that control the rate of diffusion at each timestep. Note that when $\\alpha_t$ is small, we can approximate $x_t \\sim \\mathcal{N}(0, I)$. New samples can be generated by reversing the diffusion process, starting from a random vector $x_T \\sim \\mathcal{N}(0, I)$ and predicting the next diffusion step iteratively. Here we follow the DDPM variant [HJA20] and train a neural network $\\epsilon_\\theta(x, t)$ to predict the noise from step $t$ to $(t-1)$ by minimizing\n$\\mathbb{E}_{x_0 \\sim q(x_0 | c), t \\sim [1, T]} ||\\epsilon - \\epsilon_\\theta(x, t)||^2.$ (2)\nThe model can additionally receive conditioning information $c$ such as an action category or language embedding, i.e. $\\epsilon_{t-1} = \\epsilon_\\theta(x_t, t, c)$.\nAn effective variant is the Latent Diffusion Model [RBL*22, CJL*23], where diffusion is performed on the latent space of a pre-trained VAE instead of $x$."}, {"title": "4. Method", "content": "Our goal is to generate a motion sequence given a sentence in natural language. For this, we introduce \"Latent rEAlignment for human motion Diffusion\" (LEAD). We build on MLD [CJL*23] and perform diffusion over the latent space of a previously trained motion VAE. Different from other approaches, LEAD includes a specialized projector module trained to produce embeddings aligned with CLIP [RKH*21], transforming the diffused latent into a better-structured semantic space (Section 4.1). Additionally, in Section 4.2 we introduce the task of motion textual inversion, where we optimize for the textual embedding given a set of example motions. As will be shown in Section 5, using the realigned space allows the optimization to better capture the input motion, showing the potential of our approach for personalized downstream tasks."}, {"title": "4.1. LEAD", "content": "Given a sentence $y$, the goal of LEAD is to generate a motion sequence $x \\in \\mathbb{R}^{N \\times D}$, where $N$ is the motion length and $D$ the dimension of motion features, including joint rotations, positions, velocities and foot contacts as in [GZZ*22].\n4.1.1. Architecture.\nLEAD consists of four modules (Figure 2).\n(1) A motion VAE [KW13] (blue in Fig. 2), with an encoder $z_{vae} = E_{vae}(x)$, where $z_{vae} \\in \\mathbb{R}^{M}$ is a compressed representation of the motion segment, and a decoder $x = D_{vae}(z_{vae})$ that transforms the latent back into a motion sequence. Following [CJL*23], the VAE is a transformer-based architecture [VSP*17] with long skip connections [RFB15]. The encoder $E_{vae}$ takes as input two learnable distribution tokens corresponding to $\\mu$ and $\\sigma$ of a Gaussian distribution, along with the motion features. The VAE decoder $D_{vae}$ takes as input the latent $z_{vae}$ and zero motion tokens and generates the motion sequence via cross-attention.\n(2) A conditional diffusion model (brown in Fig. 2) over the latent space, $\\epsilon_\\theta(z_{vae}, t, c)$, inspired by Motion-Latent-Diffusion (MLD) [CJL*23], that predicts noise given the current noised version of the latent $z_{vae}$, a timestep $t$, and a conditioning vector $c$ (text embedding). We employ here a transformer with long skip connections [RFB15].\n(3) A text encoder for the condition, $\\tau_\\theta$ (green in Fig. 2), which converts the input sentence into a latent embedding. In our work, $\\tau_\\theta$ follows the CLIP text-encoder [RKH*21]."}, {"title": "4.1.2. Training and losses.", "content": "We train LEAD in three stages. First, the motion VAE is trained over a large, unlabeled dataset consisting of motion data only, using the Mean Squared Error (MSE) and Kullback-Leibler divergence (KL) loss. The MSE loss on the motion features acts as a geometric loss, ensuring that the motion latents retain geometric properties.\nSecond, we freeze the VAE and the text-encoder $\\tau_\\theta$ and train the diffusion model $\\epsilon_\\theta$ using classifier-free guidance [HS22], which provides a trade-off between quality and diversity. This is done by applying 10% dropout on the condition during training to learn both the conditioned and unconditioned distribution."}, {"title": "4.1.3. Inference.", "content": "As shown in Fig. 2, following a standard reverse diffusion process, we sample a latent noise vector $z_{vae} \\sim \\mathcal{N}(0, I)$ and gradually denoise it using $\\epsilon_\\theta(z_{vae}, t, c)$ with the CLIP condition $c$ by relying on classifier-free guidance as in [CJL*23]:\n$\\epsilon_\\theta(z_{vae}, t, c) = s(\\epsilon_\\theta(z_{vae}, t, c)) + (1 - s)\\epsilon_\\theta(z_{vae}, t, \\varnothing)$ (8)\nwhere $s$ denotes the guidance scale. Once obtained the clean latent $z_{vae}$, we pass it through the projector $P$ and recover the output motion through $x = D_{vae}(D_{proj}(z_{proj}))$."}, {"title": "4.2. Motion Textual Inversion", "content": "Latent motion diffusion models allow us to introduce task of Motion Textual Inversion (MTI), where given a few examples of a motion, the goal is to find the corresponding embedding in the language space such that it can later be used to generate action sequences that retain the exemplar's characteristics.\nFollowing Gal et al. [GAA*22], we assume that the concept to be learned can be captured using a single word ($C^*$).\nTherefore, given a motion $m \\in \\mathbb{R}^{N \\times D}$, we interpret the problem as seeking the optimal word embedding $v^*$ that best represents the concept in the latent space of a pre-trained text encoder, i.e. CLIP."}, {"title": "5. Experiments", "content": "In this section, we demonstrate the results of our method. First, we introduce the datasets and evaluation metrics (Sec. 5.1) as well as implementation details (Sec. 5.2). Next, we evaluate LEAD on the task of text-to-motion generation (T2M) (Sec. 5.3), and the newly proposed task of motion textual inversion (MTI) (Sec. 5.5). Finally, in Sec. 5.6 we ablate our design choices for both tasks. More qualitative results can be found in the supp. mat."}, {"title": "5.1. Datasets and Metrics", "content": "5.1.1. Datasets.\nFor T2M we experiment on two standard datasets: HumanML3D [GZZ*22] and KIT-ML [MTD*15]. HumanML3D contains 14,616 human pose sequences with 44,970 descriptions, while KIT-ML contains 6,353 textual descriptions for 3,911 motions. Poses in both datasets are represented using the parameterization from [GZZ*22] consisting of the root angular velocity along the Y-axis, the root linear velocities on the XZ-plane, the root height, the root local joint positions, velocities and rotations in root space and binary features for foot-ground contact."}, {"title": "5.1.2. Evaluation Metrics.", "content": "For T2M we follow the standard evaluation protocol with metrics from [GZW*20, GZZ*22] that measure four components: motion realism, text-motion consistency, generation diversity and multimodal matching between text and motion. To assess realism we compute the Fr\u00e9chet Inception Distance (FID) [HRU*17] between the ground-truth and predicted motion features, i.e.\n$FID = ||\\mu_r - \\mu_g||^2 + Tr(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{\\frac{1}{2}}),$ (11)\nwhere $\\mu_r, \\Sigma_r$ and $\\mu_g, \\Sigma_g$ correspond to the mean and covariance matrix of the real and generated motions respectively.\nTo measure text-motion consistency we report the multimodal distance (MMdist) and R-precision, where MMdist measures the average Euclidean distance between the generated motions and the corresponding conditioning texts, and R-precision measures the average retrieval accuracy for the top 1/2/3 matches. We compute MMdist using\n$\\text{MMdist} = \\frac{1}{N} \\sum_{i=1}^N ||v_i - t_i||,$ (12)\nwhere $v_i$ are the features of the motions generated using texts $y_i$, and $t_i$ are the features of the corresponding texts, $y_i$. For R-precision, following Guo et al. [GZZ*22], for each generated motion we select its ground-truth textual description along with 31 randomly selected mismatched descriptions. Then we calculate the Euclidean distances between the motion feature and the 32 text features and rank them based on the distance. We select the top-1, top-2, and top-3 and consider as successful retrieval when the corresponding real text falls into the top-k candidates. R-precision is computed as the average across all generated motions from the test set.\nTo evaluate generation diversity (Div) we compute the variance of synthesized motions across all categories. For a subset of size $p$ the diversity can be computed as:\n$\\text{DIV} = \\frac{1}{p} \\sum_{i=1}^p ||v_i - \\hat{v}_i||,$ (13)\nwhere $v_i$ and $\\hat{v}_i$ are the features corresponding to the first and second subset of predictions respectively. Following previous work [CJL*23], we set $p = 300$.\nFinally, multimodality (MModality) measures the variance across 100 motions generated using the same textual description. To calculate MModality (multimodality) we sample m textual descriptions. For each description we generate two subsets of motions of size d. Then MModality is computed as:\n$\\text{MModality} = \\frac{1}{m \\times d} \\sum_{j=1}^m \\sum_{i=1}^d ||v_{j, i}^1 - v_{j, i}^2||$ (14)"}, {"title": "5.3. Text-to-Motion Generation", "content": "For T2M, we compare against the SOTA on the HumanML3D and KIT-ML datasets. We report the average statistics and 95% confidence interval across 20 runs.\nTab. 1 report the results for HumanML3D and Tab. 2 for KIT-ML. Notably, on both datasets, LEAD achieves a significant improvement in motion realism compared to the other methods, as reflected by the FID. This includes the reference latent diffusion methods (MLD [TRG*23], MotionLCM [DCW*24]) which differ only in the use of the projector, showing that a simple realignment step during inference can substantially boost realism. Overall, we observe that the realignment mechanism leads to significant improvements in motion realism over the baselines (MLD, MotionLCM). Even though it demonstrates slightly worse performance compared to the state-of-the-art ReMoDiffuse, which relies on a retrieval mechanism at inference, our method achieves a better trade-off between realism and alignment as reflected in SOTA performance (R-precision, MMdist).\nAdditionally, we visualize using tSNE [vdMH08] the VAE latent space in comparison to the realigned space. As seen in Fig. 3 (right), similar motions are better clustered in the projected space, suggesting that a semantic structure of the latent space might have an effect on the generation quality.\nFinally, we show qualitative results in Fig. 3. We observe that the synthesized motions from LEAD are more lifelike and expressive compared to the ones from MLD. In particular, Fig. 3 (a,b) show that motions generated with LEAD display richer movement that still complies with the text, while motions generated with MLD are smoother and inert, an observation that holds for more rare actions such as swimming. Furthermore, LEAD minimizes motion artefacts such as foot-sliding, as can be seen in Fig. 3 (c). We provide animated qualitative results and comparisons to baselines in the supplementary video. We compare text-to-motion results of LEAD with those generated using T2M [GZZ*22], MDM [TRG*23] and our reference method, MLD [CJL*23].\nUser Study To assess the quality of our generated motions we conducted two user studies, where we compared LEAD against our"}, {"title": "5.4. Re-usability of LEAD latent space", "content": "We investigate the generalization capacity of our proposed LEAD latent projector on different motion latent diffusion models.\nIn particular, we consider a different diffusion model for text-to-motion generation such as MotionLCM [DCW*24]. MotionLCM offers speed enhancements by employing distillation, resulting in"}, {"title": "5.5. Motion Textual Inversion", "content": "To show the effectiveness of our proposed motion textual inversion we compare our approach (denoted MTIproj) against the following baselines:\n\u2022 MTImld: Same as in image textual inversion [GAA*22], we first consider MTI on MLD, i.e. calculating the reconstruction in the VAE space, i.e.\n$||\\epsilon - \\epsilon_\\theta(z_{vae}, t, \\tau_\\theta(y'))||^2$.\n\u2022 MTI feat: Here we optimize using the reconstruction loss directly on the decoded motion, with F defined in Eq 10: $||D_{vae}(F(\\epsilon)) - D_{vae}(F(\\epsilon_\\theta(z_t, t, \\tau_\\theta(y'))))||$"}, {"title": "5.6. Ablations", "content": "Finally, we conduct ablation studies on the architecture design for T2M on HumanML3D (Tab. 6). In addition, we ablate the losses of LEAD for both T2M on HumanML3D (Tab. 6), as well as MTI on HumanML3D-mini (Tab. 4).\nFor T2M, we examine three different architectures (Tab. 6): a simple MLP (first row), a transformer with long skip connections"}, {"title": "6. Conclusion", "content": "We proposed LEAD, a human motion diffusion model equipped with a latent realignment mechanism. Leveraging the power of CLIP, we introduced a new latent space that is semantically better structured and can be trained with little effort given a pre-trained motion diffusion model.\nMoreover, we proposed the new task of motion textual inversion, which optimizes for the textual embedding that best explains a set of example motions. This enables the generation of actions that are hard to explain with natural language, laying the groundwork for a more personalized content creation process.\nOur quantitative and qualitative results show improved performance on the text-to-motion task on the HumanML3D and KIT-ML datasets, achieving greater realism without compromising diversity. Our evaluation of motion textual inversion on the 100styles dataset further shows that the realigned latent space is more suitable for the generation of out-of-distribution sequences learned with few examples. We believe this is a promising direction, as it opens new opportunities for intuitive, personalized motion generation with only a few exemplar motions.\nLimitations As demonstrated quantitatively and reflected by the user study, compared to the baselines, motions generated with LEAD are more realistic. However, they still sometimes exhibit motion artifacts such as foot-sliding. Furthermore, for long and sequential textual descriptions LEAD may struggle to produce motions that faithfully comply to the motion description. Finally, even though LEAD can generate arbitrary-length motions due to its design, it remains bounded by the maximum motion length present in the dataset.\nSocial Impact Personalized motion generation from text introduces the risk of misuse in developing deceptive artificial content and raises ethical concerns regarding privacy and consent, as the technology may create motions that portray a specific person without explicit authorization from them."}]}