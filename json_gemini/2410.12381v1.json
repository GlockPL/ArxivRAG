{"title": "HUMANEVAL-V: EVALUATING VISUAL UNDERSTANDING AND REASONING ABILITIES OF LARGE MULTIMODAL MODELS THROUGH CODING TASKS", "authors": ["Fengji Zhang", "Linquan Wu", "Huiyu Bai", "Guancheng Lin", "Xiao Li", "Xiao Yu", "Yue Wang", "Bei Chen", "Jacky Keung"], "abstract": "Coding tasks have been valuable for evaluating Large Language Models (LLMs), as they demand the comprehension of high-level instructions, complex reasoning, and the implementation of functional programs core capabilities for advancing Artificial General Intelligence. Despite the progress in Large Multimodal Models (LMMs), which extend LLMs with visual perception and understanding capabilities, there remains a notable lack of coding benchmarks that rigorously assess these models, particularly in tasks that emphasize visual reasoning. To address this gap, we introduce HumanEval-V, a novel and lightweight benchmark specifically designed to evaluate LMMs' visual understanding and reasoning capabilities through code generation. HumanEval-V includes 108 carefully crafted, entry-level Python coding tasks derived from platforms like CodeForces and Stack Overflow. Each task is adapted by modifying the context and algorithmic patterns of the original problems, with visual elements redrawn to ensure distinction from the source, preventing potential data leakage. LMMs are required to complete the code solution based on the provided visual context and a predefined Python function signature outlining the task requirements. Every task is equipped with meticulously handcrafted test cases to ensure a thorough and reliable evaluation of model-generated solutions. We evaluate 19 state-of-the-art LMMs using HumanEval-V, uncovering significant challenges. Proprietary models like GPT-4o achieve only 13% pass@1 and 36.4% pass@10, while open-weight models with 70B parameters score below 4% pass@1. Ablation studies further reveal the limitations of current LMMs in vision reasoning and coding capabilities. These results underscore key areas for future research to enhance LMMs' capabilities. We have open-sourced our code and benchmark at https://github.com/HumanEval-V/HumanEval-V-Benchmark.", "sections": [{"title": "1 INTRODUCTION", "content": "Coding ability is essential for both the development and evaluation of Large Language Models (LLMs). By enabling LLMs to solve complex tasks by decomposing them into modular sub-tasks, coding fosters more autonomous and efficient interactions with the world. As a result, coding tasks serve as a valuable testbed for advancing research in Artificial General Intelligence. Recently, Large Multimodal Models (LMMs) composed of billions of parameters have emerged, with notable examples such as GPT-40 and Claude 3.5 Sonnet, demonstrating remarkable capabilities in understanding and reasoning within visual contexts.\nWhile several recent multimodal benchmarks offer evaluations across a wide range of vision-related tasks, there remains a significant gap in benchmarks specifically designed for coding scenarios. These"}, {"title": "2 BENCHMARK CONSTRUCTION", "content": "As shown in Figure 1, each coding task in HumanEval-V consists of three main components. The first component is a single image input, denoted as I, which provides the essential visual context necessary to solve the coding problem. The second component is a Python function signature, denoted as \u03c3, which specifies the function name, input parameters, and return type, accompanied by a brief problem description in the comment block. Both the image I and the function signature \u03c3 are formatted into a predefined prompt template, which is then provided to the LMM. The model's output, denoted as O, represents the complete Python function generated by the LMM based on \u03c3 and I. The third component is a set of test cases T = {t1, t2, ..., tn}, which are used to validate the functional correctness of O through execution. A solution is considered correct if O passes all test cases, meaning it produces the expected outputs for each ti \u2208 T.\nBefore constructing HumanEval-V, we establish rigorous standards to ensure the quality of the coding task annotations: (1) the visual context provided must be essential for solving the task, with all relevant information contained within a single image; (2) the coding task should be largely self-explanatory through its visual context, requiring minimal textual descriptions; and (3) the coding task should target entry-level programmers and be solvable using only common Python libraries.\nAs shown in Figure 2, the construction of HumanEval-V follows a collect-adapt-mutate pipeline. First, we collect coding problems with visual contexts from platforms such as CodeForces and Stack Overflow, identifying those suitable for adaptation based on the aforementioned standards (Section 2.1). Next, we modify the selected problems by adapting their task descriptions and redrawing the visual elements to ensure they meet our quality requirements. During this stage, we annotate each task with a function signature (\u03c3), a set of test cases (T), and a ground truth solution. To further expand the benchmark, some tasks undergo mutations, generating similar yet distinct versions by introducing changes to the coding task's visual patterns while preserving the core context. This iterative process results in a final set of 108 code generation tasks (Section 2.2). After constructing the benchmark, we perform rigorous validation to ensure that each coding task aligns with the standards (Section 2.3). Finally, we provide detailed benchmark statistics for reference (Section 2.4)."}, {"title": "2.1 DATA COLLECTION AND SCREENING", "content": "The coding tasks in HumanEval-V are sourced from prominent Q&A and coding challenge platforms such as Stack Overflow and CodeForces. These platforms offer a diverse range of coding problems and are also commonly used in the development of well-established benchmarks for code generation. From these sources, we collect a large set of coding problems that incorporate visual elements in their problem descriptions.\nHowever, the collected problems are unsuitable for direct inclusion in HumanEval-V. In most cases, the visual context is non-essential for solving the task, with the problem primarily solvable through rich textual descriptions alone. This makes it challenging to adapt such problems into our benchmark, which emphasizes visual reasoning abilities. Therefore, we focus on identifying tasks that already feature high-quality visual elements and present a moderate level of difficulty. After a thorough screening process, we retain 40 candidate coding tasks out of the thousands reviewed for further adaptation. A detailed discussion of the challenges encountered during data collection and screening, along with demonstrating examples, is provided in Appendix D.1."}, {"title": "2.2 CODING TASK ANNOTATION", "content": "The annotation process begins by adapting the screened coding problems. For each of the 40 selected coding tasks, we first identify and summarize the essential context and algorithmic patterns required to solve the problem. We then create a new coding problem by modifying the context and patterns of the original problem and redrawing the corresponding images. This is to prevent data leakage and ensure consistency with the standards of HumanEval-V. Detailed examples of the problem adaptation can be found in Appendix D.2.\nDuring adaptation, we ensure that all critical visual information for each coding task is encapsulated within a single image. The coding tasks in HumanEval-V span a variety of visual elements, including trees, graphs, matrices, maps, grids, flowcharts, and other abstract representations. This diversity allows for comprehensive testing of the model's visual reasoning abilities. Next, we define a Python function signature for each task, beginning with the input and output specifications. To simplify the Input/Output (I/O) formats, we prioritize basic data structures such as numbers, strings, lists, and dictionaries. After finalizing the image and I/O definitions, we craft a concise problem description that directs models to rely primarily on the visual information to complete the Python function. Once the task definition is completed, we manually construct test cases and implement a ground truth solution for each coding problem to ensure its validity. To further verify the comprehensiveness of the test cases, we perform statement and branch coverage analysis on the ground truth solution, ensuring that all logical branches and execution paths are thoroughly tested.\nFollowing the initial annotation of the 40 coding tasks, we conduct an additional round of mutation-based extensions. This process expands the number of coding tasks based on the initial annotations, by creating similar yet distinct coding tasks. The mutated tasks retain most of the original visual elements but incorporate different algorithms to solve. For example, we can change the rule of the coding task in Figure 1 by just considering the situation where the line segments intersect within the circle, regardless of outside the circle. It is important to note that not all of the 40 tasks are suitable for mutation. For each suitable task, we create one or two mutations, resulting in a total of 108 coding tasks in HumanEval-V. Examples of the mutation process are provided in Appendix D.3"}, {"title": "2.3 QUALITY ASSURANCE", "content": "We implement a rigorous quality assurance process to ensure the quality of HumanEval-V. The annotation team consists of three programmers, each with over four years of Python programming experience. During each of the data collection, adaptation, and mutation stages, annotators independently perform annotations based on pre-defined guidelines. After that, all annotators conduct a cross-validation process to review and resolve any identified issues. A coding task is only finalized when consensus is reached among all annotators. Additionally, one annotator maintains consistent formatting and style across all visual representations and coding tasks. Each annotator dedicates over 200 hours to the overall benchmark construction process. To validate the reliance on visual context, we ensure that GPT-40 cannot solve any of the coding tasks without access to the images,"}, {"title": "2.4 DATASET STATISTICS", "content": "To provide a clearer understanding of our benchmark, Table 1 presents key statistics for several dataset attributes. Each coding task includes a single image input, with the image dimensions constrained to a maximum of 1024 pixels in height or width, to prevent overly long or complex visual contexts. The average image width and height are 998.2 and 729 pixels, respectively. We also analyze the length of function signatures using the OpenAI tiktoken tokenizer. The longest function signature consists of 230 tokens, while the average token count is 111.3, demonstrating high succinctness. We also quantify the complexity of the ground truth (GT) code solutions annotated by human experts. On average, GT solutions contain 16.3 code statements, encompassing import statements, function definitions, and the function body, reflecting the relative simplicity of the tasks. Finally, we provide statistics on the number of test cases used for evaluation, with an average of 9.8 test cases per task. We ensure the test cases achieve full statement and branch coverage on the GT solutions, guaranteeing rigorous testing of the generated code. We also include a detailed list of the I/O types and module dependencies in Appendix D.4."}, {"title": "3 EXPERIMENTAL SETUP", "content": "Models: We conduct a comprehensive evaluation of 19 state-of-the-art LMMs to assess the current progress in visual reasoning and coding capabilities. Our selection includes a representative set of the most advanced proprietary and open-weight models. Specifically, we evaluate five of the latest proprietary models: GPT-40 (0513), GPT-4o-mini (0718), Gemini 1.5 Pro (001), Gemini 1.5 Flash (001), and Claude 3.5 Sonnet (0620). In addition, we test 14 top-performing open-weight models, selected based on their rankings on the OpenVLM Leaderboard. These models span various parameter sizes to explore the impact of scale on performance in the HumanEval-V benchmark. The open-weight models include Phi-3-Vision (4.2B), Phi-3.5-Vision (4.2B), LLaVA-OneVision (8.0B, 73.2B), MiniCPM-V 2.5 (8.5B) and 2.6 (8.1B), InternVL-Chat-V1.5 (26.0B), InternVL-2 (4.2B, 8.1B, 25.5B, 40.1B), and Qwen2-VL (8.3B, 73.4B). We deliberately include different versions within the same model series, such as Phi-3-Vision and Phi-3.5-Vision, MiniCPM-V 2.5 and 2.6, as well as InternVL-Chat-V1.5 and InternVL-2, to investigate whether iterative improvements in model development result in enhanced performance on HumanEval-V. More details of the models can be found in Appendix G.\nPrompting, Hyper-parameters, and Post-processing: All the LMMs evaluated in our experiments have been trained on instruction-following or conversational data. To align with this, we employ a conversational prompt template, formatted in Markdown, as illustrated in Figure 3, to prompt the LMMs to generate code solutions for the tasks in HumanEval-V. For hyper-parameters, we follow the established practices in code generation benchmarking, using two distinct settings. First, we employ greedy search to generate a single code solution from each LMM, allowing us to assess the models' performance in a deterministic setting. Additionally, we sample 20 code solutions using a Top-p sampling method with p = 0.95 and a relatively high temperature of 0.8. This setting is designed to explore the likelihood of the models generating correct solutions when given more opportunities."}, {"title": "4 EXPERIMENTAL RESULTS", "content": "Given the moderate complexity of the benchmark, we set the maximum number of new tokens for code generation to 1024. Early stopping is triggered by \u201c\\n```\\n\u201d, since the LMMs are instructed to enclose the generated code within a Markdown code block. We also develop a post-processing pipeline to extract valid code solutions from the model outputs. This pipeline identifies and extracts the content within the Markdown code block and uses an abstract syntax tree parser to detect any generated import statements, along with class and function definitions. These components are then concatenated to form the final predicted solution for test-execution-based evaluation.\nEvaluation Metrics Following established practices in code generation, we use the pass@k metric to evaluate the functional correctness of the generated code solutions. For each coding task, n code samples are generated, and k solutions are randomly selected from these samples to be tested against ground truth test cases. A task is considered solved if at least one of the k selected solutions passes all test cases. The pass@k score is then calculated as the percentage of successfully solved tasks. In our main experiments, we report pass rate results for k = 1, 10. For greedy search, we set n = 1 to compute pass@1. For sampling-based evaluation, we set n = 20 to calculate pass@10.\nWe incorporate a second evaluation metric: Execution Success Rate. This metric measures the syntactic correctness of the generated code, independent of its functional accuracy. A solution is considered executable if it can be compiled and run without triggering syntax errors, null pointer exceptions, or other runtime failures, regardless of passing the test cases. The execution success rate is calculated as the proportion of executable code samples out of all generated samples."}, {"title": "4.1 MAIN EXPERIMENTS", "content": "We evaluate 19 state-of-the-art LMMs on HumanEval-V, with results presented in Table 2. Based on the results, we have the following key findings:\nCurrent LMMs' performance is underwhelming on our benchmark: While proprietary models like GPT-40 and Claude 3.5 Sonnet show the best results, even their highest pass@1 scores (13% and 18.5% respectively) fall short of expectations. Moreover, there remains a substantial performance gap between proprietary and open-weight models. Open-weight models spanning 4B to 76B parameters exhibit particularly weak performance, with none exceeding a 4% pass@1. This is surprising given that the coding tasks in our benchmark are designed for entry-level programmers with simplified problem context. None of the open-weight models with fewer than 70B parameters achieve more than 4% pass@10. Even the best-performing model, GPT-4o, achieves only 36.4% pass@10, showing there is much room for improvement. In terms of execution success rate, we observe a rough correlation with the pass rate. Most LMMs exhibit a high execution success rate, while smaller-scale open-weight models show lower success rates. Most failed cases are due to common syntax errors, such as unclosed parentheses, generating code repeatedly without termination, or encountering list index out-of-range issues. To further investigate, we perform another experiment increasing the number of samples to evaluate model performance, as detailed in Appendix C.1.\nOverfitting leads to hallucination errors in LMMs' generated solutions: Upon examining many incorrect solutions produced by the LMMs, we identify a recurring issue: the models tend to generate solutions based on the context of the original problems rather than the new versions of coding tasks in our benchmark. For instance, both GPT-40 and Claude 3.5 Sonnet fail to produce correct solutions for the coding task shown in Figure 1, as they mistakenly assume that the numbers in the image are arranged in a clockwise order. Furthermore, their solutions rely on the assumption that the two line segments can only intersect within the circle, which reflects the context of the original"}, {"title": "4.2 ANALYSING EXPERIMENTS", "content": "To investigate the reasons behind the suboptimal performance of current LMMs on HumanEval-V, we perform analyzing experiments by answering two key research questions.\nQ1. Are LMMs Limited by Their Vision Capabilities?\nWe conduct an ablation study to evaluate whether the limitations in visual understanding contribute to the underperformance of LMMs. In this study, we manually annotate detailed descriptions for each image in the coding tasks, ensuring that these descriptions are descriptive rather than instructive, without revealing any specific algorithms. We design a new prompt template incorporating the image description to provide LMMs with better-grounded visual context, thereby reducing issues such as ambiguity and hallucination. Details of the new prompt template and examples of annotations are provided in Appendix C.3. To further assess the quality of our annotations, we also test a setting where LMMs receive only the image descriptions, without access to the images themselves. Additionally, we evaluate several top-performing Code LLMs using image descriptions to explore their potential in HumanEval-V. We present the results in Table 3. Below are the key findings:\n(1) The inclusion of image descriptions leads to notable performance gains across all LMMs, with higher-capability models showing the most significant improvements. For example, GPT-4o exhibits a 31.5% absolute increase in pass@1. Similarly, large open-weight LMMs demonstrate substantial improvement, indicating that current models still require enhanced visual understanding capabilities. However, the limited improvement observed in smaller open-weight models suggests that merely perceiving visual elements is insufficient for solving tasks that require deeper reasoning. We"}, {"title": "Q2. Are LMMs Limited by Their Coding Abilities?", "content": "Open-weight LMMs with parameter sizes ranging from 4B to 40B exhibit surprisingly low performance on HumanEval-V, even when utilizing grounded visual elements through image descriptions. This suggests that open-weight LMMs may suffer from degradation of relevant coding abilities. So we evaluate the models on a well-established code generation benchmark, EvalPlus , to investigate their coding abilities. This benchmark includes two sub-datasets refined from HumanEval and MBPP , both consisting of Python function completion tasks with problem descriptions and test-execution-based evaluation. Different from HumanEval-V, these datasets depend exclusively on textual context.\nGiven that open-weight LMMs typically employ a vision-encoder and language-decoder architecture, we also evaluate their LLM decoders separately to determine whether their coding performance deteriorates after integrating the vision abilities. The results presented in Table 4 lead to the following findings: (1) Open-weight LMMs consistently experience performance degradation on coding benchmarks compared to their LLM decoders, despite having similar parameter sizes. Among these, InternVL-2 (40.1B) and MiniCPM-V 2.6 show the most degradation, while InternVL-2 (4.2B) and LLaVA-OneVision (8B) show the least. (2) Despite this degradation, open-weight LMMs still exhibit relatively strong coding capabilities. Although their performance on EvalPlus does not match GPT-40, many of these models produce competitive results, indicating they retain a substantial degree of code generation ability. These results highlight the need for further improvement in the coding abilities of current open-weight LMMs."}, {"title": "5 RELATED WORK", "content": "While numerous benchmarks have been developed to evaluate various capabilities of LMMs, ranging from optical character recognition (OCR) to multidisciplinary knowledge reasoning, few specifically focus on the intersection of visual reasoning and code generation. This section reviews the current progress of LMM benchmarking and demonstrates how HumanEval-V fills this gap.\nOCR and Multidisciplinary Knowledge Abilities: A variety of benchmarks have been developed to evaluate multidisciplinary capabilities of LMMs. There are popular benchmarks like DocVQA , ChartQA , TextVQA, OCR-Bench, and OCRVQA assess models' ability to recognize and interpret text embedded in visual formats, including documents, charts, and images, often combining these with multiple-choice questions (MCQ) and visual question answering (VQA) tasks. Meanwhile, benchmarks such as MMMU , MME , MMBench, MMVet , SEEDBench, MMT-Bench, and MMStar test models on their general knowledge and reasoning abilities across diverse domains, such as scientific concepts, cultural knowledge, and logical reasoning. In contrast, HumanEval-V distinguishes itself by expanding the evaluation format beyond traditional MCQ and VQA. HumanEval-V requires models to interpret visual elements and apply that understanding to generate correct and executable code, which introduces a more complex challenge.\nSpecialized Abilities: There are also benchmarks focusing on specific capabilities of LMMs. MathVista evaluates mathematical problem-solving skills. Safety-related benchmarks assess models on their ability to recognize and mitigate potential risks or harmful content. ConvBench evaluates conversational abilities, testing models on their proficiency in maintaining coherent and contextually relevant dialogues. Benchmarks for instruction-following ability assess how well models can execute tasks based on given instructions. Long-context reasoning benchmarks assess the ability of models to maintain coherence and logical reasoning over extended dialogues or documents. HallusionBench focuses on hallucination detection abilities to differentiate between factual information and generated content. There are also benchmarks evaluating mobile app navigation, testing models on their ability to interpret and interact with user interfaces. In contrast, HumanEval-V mainly focuses on integrating visual reasoning and coding.\nCoding Abilities: Despite the wide range of benchmarks available, the coding ability of LMMs remains under-explored. Coding capabilities are crucial for leveraging LMMs in autonomous and agentic applications . Current efforts focus primarily on the derendering of web"}, {"title": "A ERROR ANALYSIS ON THE EXAMPLE TASK", "content": "Figure 1 illustrates a simple coding task in HumanEval-V. The task requires determining whether two line segments, defined by pairs of numbers on a clock-like circle, will ultimately intersect if allowed to extend outside the circle. The numbers on the circle are arranged in a non-standard order. Despite the problem's simplicity, all evaluated LMMs failed to solve it correctly even when generating 20 samples. We present representative solutions generated by GPT-40 and Claude 3.5 Sonnet in Figure 6.\nBoth models implement sorting-based algorithms that compare the numbers at the endpoints of the line segments. However, they fail to account for the critical scenario where the segments intersect outside the circle, and fail to recognize the unordered arrangement of the numbers. This oversight indicates that the models are not effectively capturing the essential visual details of the problem. Notably, this issue appears to stem from data leakage, as the original coding task is derived from a Code-Forces problem (https://codeforces.com/contest/1971/problem/C), and the generated solutions in Figure 6 reflect patterns more suitable for the original context. This phenomenon is not isolated to this task; we observe similar issues across many coding tasks in HumanEval-V. This highlights that the models rely on memorized patterns instead of genuinely understanding the visual context. Such failures emphasize the importance of preventing data leakage and validate the rationale behind our careful adaptation and mutation processes during data annotation."}, {"title": "B REPRESENTATIVE IMAGES IN HUMANEVAL-V", "content": "As shown in Figure 7, the coding tasks in HumanEval-V incorporate a wide range of visual elements. These elements go beyond basic representations like trees, graphs, and maps that are typically used to illustrate fundamental data structures in computer science. Instead, the visual contexts in HumanEval-V are carefully designed to be self-explanatory, embedding rich contextual information and algorithmic patterns directly into the images. Achieving this standard is particularly challenging because encoding complex rules, conditions, and problem contexts into a single image requires significant effort. Each image must convey the necessary information for understanding the task without relying heavily on textual descriptions. For instance, in tasks where geometric transformations, recursive structures, or logical operations are involved, the image alone should provide enough visual cues for the model to infer the underlying patterns and expected outputs.\nTo meet these high standards, we dedicate extensive resources and effort to the data annotation process. Each image is meticulously crafted to ensure that the visual context is indispensable for solving the task, thus minimizing the reliance on textual explanations. This involves transforming the critical problem-solving elements into concise visual representations while preserving clarity and interpretability. Through this rigorous approach, HumanEval-V sets a new bar for integrating visual and algorithmic reasoning, ensuring that the coding tasks genuinely assess the visual understanding and reasoning capabilities of LMMs."}, {"title": "C ADDITIONAL EXPERIMENTAL RESULTS", "content": "C.1 PERFORMANCE WITH MORE SAMPLES\nThe results in Section 4.1 indicate that increasing the number of samples can significantly enhance model performance on HumanEval-V, so we conduct an ablation study to examine the effect of scaling up sample sizes. Due to budgetary constraints, we primarily test open-weight LMMs ranging from 4B to 40B parameters. For proprietary models, we evaluate GPT-40 and GPT-40-mini. For all selected models, we increase the number of generated samples n to 100 to observe their performance. The results are presented in Table 5.\nFrom the results, we observe that increasing the sample size consistently improves performance across most models. For example, GPT-4o achieves a substantial improvement, rising from 36.4% pass@10 to 53.7% pass@100. Smaller-scale open-weight LMMs also show notable gains; for instance, InternVL-2 (4.2B) improves from a pass@10 of 2.3% to a pass@100 of 14.8%. However, not all models benefit equally from scaling the sample size. For instance, Phi-3.5-Vision, which has the same 4B-level parameter size, achieves only a pass@100 score of 6.5%. These findings underscore both the potential and the limitations of scaling sample numbers to improve current LMMs' performance on HumanEval-V.\nTo further evaluate the performance of current LMMs, we increase the sample size for GPT-40 to 1,000. The results, presented in Table 6, show promising results with GPT-4o achieving a pass@1000 of 68.5%, compared to the 36.4% pass@10. Similarly, GPT-40-mini demonstrates strong performance, achieving a pass@1000 score of 46.3%, surpassing the pass@10 score of GPT-40. These findings suggest that a significant proportion of the coding tasks in HumanEval-V are solvable with current LMM capabilities, highlighting the need for further research on strategies to better motivate the abilities of LMMs."}, {"title": "C.2 COMPARISON WITH OTHER MULTIMODAL BENCHMARKS", "content": "To analyze whether HumanEval-V identifies specific weaknesses that are not captured by existing benchmarks, we select five widely used multimodal benchmarks that cover multidisciplinary abilities. The selected benchmarks include MMMU, MathVista, MMVet , MME , and RealWorldQA . We collect the performance results of the 19 LMMs evaluated in this paper from the OpenVLM Leaderboard and the corresponding papers and reports. These results are presented alongside the pass@1 and pass@10 scores on HumanEval-V in Table 7. From the results, we observe that open-weight LMMs with over 70B parameters generally perform well on the selected benchmarks, with models such as InternVL-2 (76.3B) and Qwen2-VL (73.4B) even surpassing proprietary models"}, {"title": "C.3 EXPERIMENTING WITH IMAGE DESCRIPTIONS", "content": "We provide two examples in Figure 9 and Figure 10 to illustrate our annotation process and demonstrate how we construct image descriptions. When creating these descriptions, we ensure they are purely descriptive rather than instructive, refraining from disclosing any specific algorithms or problem-solving strategies. This approach allows us to evaluate whether current LMMs possess genuine visual understanding capabilities and whether they can perform well when the visual elements are grounded through detailed textual descriptions.\nThis process poses a unique challenge. While humans can intuitively identify patterns in images and summarize them succinctly, we require our annotators to use precise descriptive language that details every visual aspect without inferring the specific steps to solve the problem. This increases the complexity of annotation and often results in verbose image descriptions. Despite this verbosity, maintaining a purely descriptive approach is crucial for our benchmark, as it ensures that solving the task requires the model to interpret and reason about the visual content, rather than simply interpreting the description into code.\nOnce the image descriptions are finalized, we employ the prompt template shown in Figure 11 to guide the LMMs in generating code solutions for the tasks in HumanEval-V."}, {"title": "D BENCHMARK CONSTRUCTION DETAILS", "content": "D.1 ADDITIONAL DETAILS OF DATA COLLECTION\nOur data collection process involves two primary sources: Stack Overflow (SO) and coding challenge platforms. Each coding problem undergoes a strict screening process to ensure that it aligns with the standards of HumanEval-V. Annotators are instructed to identify suitable problems by"}, {"title": "D.2 EXAMPLES OF ADAPTING CODING PROBLEMS", "content": "We present three adapted examples in Figure 17, Figure 18, and Figure 19, derived from the original coding tasks shown in Figure 14, Figure 15, and Figure 16. For each problem, we redesign the questions, redraw the accompanying images to include the critical problem-solving context, and simplify the textual descriptions. Furthermore, we adjust the difficulty to ensure that entry-level programmers can interpret the visual information accurately and implement the solution using basic coding skills.\nIn Figure 17, we transform the original parallelogram problem into the coding task involving a five-pointed star, incorporating richer visual information. To enhance the visual cues, we include four examples in the image demonstrating different ways to connect five points to form a star. In the adapted function signature, we specify the implementation requirements for the model, clearly"}, {"title": "D.3 EXAMPLES OF MUTATING CODING TASKS", "content": "We apply mutations to some of the 40 screened coding tasks to expand the volume of our benchmark. The objective is to generate new tasks that retain the essence of the original tasks but introduce distinct patterns with minimal modification. As illustrated in Figures 20, Figure 21, and Figure 22, these mutated tasks are derived from the coding problems in Figures 17, 18, and 19, respectively."}, {"title": "D.4 ADDITIONAL DATASET STATISTICS", "content": "The input and output (I/O) types used in the coding tasks in HumanEval-V are designed to maintain a low level of complexity. A distribution of their frequencies is shown in Table 9. We focus on using simple and commonly used data structures, such as integers, lists, dictionaries, and tuples, which are frequently encountered in standard programming tasks. Most of the tasks utilize basic types like integers, 1D and 2D lists, or simple boolean outputs, ensuring that solving them does not require specialized fine-tuning on domain-specific data. These I/O types are prevalent in open-source code"}, {"title": "E DISCUSSION ON THE MMCODE DATASET", "content": "MMCode introduces a multimodal coding dataset aimed at evaluating LMMs' algorithmic problem-solving skills in visually rich contexts. The dataset includes 3,548 questions scraped from various competitive programming websites. However, as discussed in Appendix A, the issue of data leakage poses a significant challenge, as many of these coding tasks may have been previously encountered and memorized by the models, making them unsuitable for direct use as test data. Additionally, as demonstrated in Appendix D.1, a majority of the coding challenges in MMCode contain visual content that is redundant; the information conveyed through images can often be inferred from the textual descriptions alone, rendering the visuals non-essential. The reported results from MMCode further confirm this issue, as the performance using \"language-only\" inputs is similar to that with \u201cvision + language\" inputs.\nIn contrast, HumanEval-V is specifically designed to focus on visual understanding and reasoning abilities, rather than general coding proficiency, ensuring an irreplaceable dependency on visual context. During the annotation phase, we verify that language-only inputs achieve a 0% pass rate for GPT-40, demonstrating the necessity of visual context in HumanEval-V. Moreover, our careful adaptation and mutation processes prevent data leakage, ensuring that evaluations accurately measure visual reasoning and coding abilities, rather than memorization of previously seen tasks."}, {"title": "F LIMITATIONS", "content": "Despite the contributions of our benchmark, several limitations remain that we aim to address in future work:\n(1) Limited Number of Coding Tasks: The size of our benchmark is currently restricted due to the difficulty of identifying suitable coding problems and the challenges associated with adapting these problems to meet our standards. Each annotator has dedicated over 200 hours to constructing the benchmark, ensuring that every task is meticulously curated and aligns with our goals of testing visual reasoning. Our priority has been to maintain high quality, which we believe is crucial for deriving meaningful insights. Fortunately, the current version of HumanEval-V has already enabled us to identify several unique findings about the limitations of current LMMs. Moving forward, we plan to expand HumanEval-V by continuing to annotate additional tasks using our established pipeline and guidelines. To benefit the community, we will open-source our annotation process and release all details of our work.\n(2) Limited Model Coverage: While our experiments evaluate a diverse set of representative top-performing LMMs, the rapid pace of development in this area means that new models are frequently released, which may not be covered in our evaluation. We acknowledge that broader model coverage could provide a more comprehensive understanding of current capabilities. To address this, we will publicly release the evaluation toolkit and dataset, along with an up-to-date leaderboard to track ongoing advancements and benchmark new models as they become available. This will help keep our benchmark relevant and provide a platform for continuous assessment.\n(3) Limited Scope of Experimental Analysis: Due to budget constraints, our in-depth analysis is limited to a subset of the evaluated models and hyper-parameter settings. While we have included as many models as possible to ensure that our findings are not biased toward specific LMMs, there are areas that remain unexplored, such as evaluating the impact of different prompting templates and experimenting with alternative sampling strategies, including varying temperature settings. Never-"}, {"title": "G DETAILS OF THE EVALUATED MODELS", "content": "To facilitate the reproducibility of our results, we provide detailed information on all the evaluated models in Table 10. The open-weight models are sourced from Hugging Face\u00b2, while the proprietary models are accessed via their respective APIs.\nFor model inference, we utilize 8 NVIDIA A800 GPUs and maintain the original tensor data types specified by each model to ensure consistent evaluation. To further optimize inference efficiency, we leverage the open-source framework vLLM\u00b3.\nAdditionally, the Code LLMs used in Section 4.2 are also listed in Table 10. These models are fine-tuned versions of foundational LLMs, specifically trained on large-scale, multilingual programming datasets to enhance their performance across diverse coding scenarios."}]}