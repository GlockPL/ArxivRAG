{"title": "NEURAL DECONSTRUCTION SEARCH FOR VEHICLE ROUTING PROBLEMS", "authors": ["Andr\u00e9 Hottung", "Paula Wong-Chung", "Kevin Tierney"], "abstract": "Autoregressive construction approaches generate solutions to vehicle routing problems in a step-by-step fashion, leading to high-quality solutions that are nearing the performance achieved by handcrafted, operations research techniques. In this work, we challenge the conventional paradigm of sequential solution construction and introduce an iterative search framework where solutions are instead deconstructed by a neural policy. Throughout the search, the neural policy collaborates with a simple greedy insertion algorithm to rebuild the deconstructed solutions. Our approach surpasses the performance of state-of-the-art operations research methods across three challenging vehicle routing problems of various problem sizes.", "sections": [{"title": "1 INTRODUCTION", "content": "Methods that can learn to solve complex optimization problems have the potential to transform decision-making processes across virtually all domains. It is therefore unsurprising that learning-based optimization approaches have garnered significant attention and yielded substantial advancements (Bello et al., 2016; Kool et al., 2019; Kwon et al., 2020). Notably, reinforcement learning (RL) approaches are particularly promising because they do not rely on a pre-defined training set of representative solutions and can develop new strategies from scratch for novel optimization problems. These methods generally construct solutions incrementally through a sequential decision-making process and have been successfully applied to various vehicle routing problems.\nDespite recent progress, learning-based methods for combinatorial optimization (CO) problems usually fall short of outperforming the state-of-the-art techniques from the operations research (OR) community. For instance, while some new construction approaches for the capacitated vehicle routing problem (CVRP) have surpassed the LKH3 solver (Helsgaun, 2000), they still struggle to match the performance of the state-of-the-art HGS solver (Vidal et al., 2012), particularly for larger instances with over 100 nodes. One reason for this is their inability to explore as many solutions as traditional approaches within the same amount of time. Given the limitations of current construction approaches, we propose challenging the traditional paradigm of sequential solution construction by introducing a novel iterative search framework, neural deconstruction search (NDS), which instead deconstructs solutions using a neural policy.\nNDS is an iterative search method designed to enhance a given solution through a two-phase process involving deconstruction and reconstruction along the lines of large neighborhood search (LNS) (Shaw, 1998) and ruin-and-recreate (Schrimpf et al., 2000) paradigms. The deconstruction phase employs a deep neural network (DNN) to determine the customers to be removed from the tours of the current solution. This is achieved through a sequential decision-making process, in which nodes are removed one at a time based on the network's guidance. The reconstruction phase utilizes a straightforward greedy insertion algorithm, which inserts customers in the order given by"}, {"title": "2 LITERATURE REVIEW", "content": "Construction Methods The introduction of the pointer network architecture by Vinyals et al. (2015) marked the first autoregressive, deep learning-based approach for solving routing problems. In their initial work, the authors employ supervised learning to train the models, demonstrating its application to the traveling salesperson problem (TSP) with 50 nodes. Building on this, Bello et al. (2016) propose using reinforcement learning to train pointer networks, showcasing its effectiveness in addressing larger TSP instances."}, {"title": "3 NEURAL DECONSTRUCTION SEARCH", "content": ""}, {"title": "3.1 DECONSTRUCTION POLICY", "content": "For solution deconstruction, a neural policy is employed to sequentially select customers for removal from a given solution. We model this selection process as a Markov decision process. Let s be a feasible solution to a vehicle routing problem (VRP) instance l, which involves customers C1,..., CN.\nA policy network \\u03c0\\u03b8, parameterized by \\u03b8, is used to select M customers for removal. At each step m\\u2208 {1,..., M}, an action am \\u2208 {1,..., N} is chosen according to the probability distribution \n\\u03c0\\u03c1(am | l,s,v, A1:m\\u22121), where am corresponds to selecting customer cam, l is the instance, s is the solution, v is a random seed, and a1:m\\u22121 are the previous actions. We condition the policy on a random seed v to encourage more diverse rollouts as explained in Hottung et al. (2024). Each seed is a randomly generated binary vectors of dimension dv (we set dv = 10 in all experiments). Finally, after all M customers are selected the reward can be computed as discussed in the following sections."}, {"title": "3.2 TRAINING", "content": "The deconstruction policy in NDS is trained using reinforcement learning. During the training process, solutions are repeatedly deconstructed and reconstructed, aiming to discover a deconstruction policy that facilitates the reconstruction of high-quality solutions."}, {"title": "Greedy Insertion", "content": "The greedy insertion procedure reintegrates the customers removed by the policy, inserting them one by one into either existing or new tours. Specifically, if M customers have been removed, the procedure performs M iterations, where in each iteration, a single customer Cam is inserted. At each iteration m, the cost of inserting customer cam at every feasible position in the current tours is evaluated. Throughout this process, various constraints, such as vehicle capacity limits, are taken into account. If at least one feasible insertion point is found within an existing tour, the customer Cam is placed at the position that incurs the least additional cost. If no feasible insertion is available, a new tour is created for customer Cam."}, {"title": "3.3 MODEL ARCHITECTURE", "content": "We design a transformer-based architecture that consists of an encoder and a decoder. The encoder is used to generate embeddings for all nodes based on the instance l and the current solution s. The decoder is used to decode a sequence of actions based on these embeddings in an iterative fashion."}, {"title": "3.3.1 ENCODER", "content": "The encoder processes the input features xi for each of the N + 1 nodes, where xo corresponds to the depot's features, alongside the current solution s that needs to be encoded. Initially, each input vector xi is mapped to a 128-dimensional node embedding hi through a linear transformation. The embeddings ho,..., hN are sequentially processed through several layers. First, two attention layers encode static instance information. Next, a message passing layer allows information exchange between consecutive nodes in the solution. This is followed by a tour embedding layer, which computes embeddings for each tour within the solution. Finally, two additional attention layers refine the representations. The attention mechanisms employed are consistent with those used in prior work (e.g., Kwon et al. (2020)), and detailed descriptions are omitted here for brevity."}, {"title": "Message Passing Layer", "content": "The message passing layer updates the embedding of a customer ci by incorporating information from its immediate neighbors (i.e., nodes that are visited before and after ci in the solution s). Specifically, the embedding hi of customer ci is updated as follows:\nh'i = Norm (hi + FF (ReLU (W\u00b3 [hi; W\u00b9hprev(i) + W\u00b2hnext(i)])))\nIn this equation, prev(i) and next(i) represent the indices of the nodes immediately preceding and following ci in the solution s. The weight matrices W\u00b9 and W\u00b2 are used to transform the embeddings of these neighboring nodes, while W\u00b3 is applied to the concatenated vector of hi and the aggregated embeddings from the neighbors. The ReLU activation function introduces non-linearity into the transformation. The output of this transformation is processed through a feed-forward network, which consists of two linear layers with a ReLU activation function in between. The resulting output, combined with the original embedding hi via a skip connection, is then normalized using instance normalization."}, {"title": "Tour Encoding Layer", "content": "The tour encoding layer updates the embedding of each customer ci by incorporating information from the tour they are part of. To this end, a tour embedding is first computed using mean aggregation of the embeddings of all customers within the same tour, and this aggregated tour embedding is then used to update the individual customer embeddings. Specifically, the embedding hi of customer ci is updated as follows:\nh'i = Norm(hi + FF(ReLU(W4 [hi; \\u2212jet(i) hj'])),\nwhere T(i) denotes the set of customers in the same tour as customer ci and W4 is a weight matrix. This layer captures important information about which customers belong to the same tour in the current solution, without considering their specific positions within the tour."}, {"title": "3.3.2 DECODER", "content": "Given the node embeddings generated by the encoder, the decoder is responsible for sequentially selecting customers for removal. The overall architecture of our decoder is identical to that of Hottung et al. (2024), which utilizes a multi-head attention mechanism (Vaswani et al., 2017) followed by a pointer mechanism (Vinyals et al., 2015). This architecture has been widely used in many routing problems methods (Kool et al., 2019; Kwon et al., 2020)."}, {"title": "3.4 SEARCH", "content": "At test time, we leverage the learned policy within a search framework that supports batched rollouts, enabling fast execution. Importantly, this framework is problem-agnostic, meaning it contains no problem-specific components, allowing it to be applied to a broader range of problems than those evaluated in this paper.\nOur search framework consists of two main components: the improvement step function and the high-level augmented simulated annealing (ASA) algorithm . The improvement step function aims to enhance a given solution by iteratively applying the policy model through a series of deconstruction and reconstruction steps. The ASA algorithm integrates this function and supports batched execution for improved performance on the GPU. It is important to note that we parallelize solely on the GPU, requiring only a single CPU core during test time."}, {"title": "Improvement Step", "content": "The improvement step, the core component of the overall search algorithm, is depicted in Figure 2. The process begins with an initial solution s0 that is passed to the policy DNN, which generates K rollouts, each consisting of M actions that specify the customers to be removed. Once the policy DNN completes its execution, these rollouts are sequentially applied to produce new candidate solutions. Specifically, the solution s0 is first deconstructed based on the actions from the first rollout (yielding s'0) and then reconstructed into s''0. After reconstruction, a simulated annealing (SA) based acceptance criterion is used to determine whether s''0 or s0 should be retained, resulting in s1. This process is repeated in each subsequent iteration. After K iterations, the final solution sK is returned, representing the outcome of K consecutive deconstruction and reconstruction operations. By performing these iterations sequentially, the solution s0 is significantly modified, often leading to notable cost improvements between the initial input s0 and the final output sK."}, {"title": "Augmented Simulated Annealing", "content": "We introduced a novel simulated annealing (SA) algorithm to conduct a high-level search specifically designed for GPU-based parallelization. While parallel SA algorithms have been proposed in prior work, their main concern is on the information exchange between CPU or GPU cores. In contrast, our approach focuses on executing parallel rollouts of the policy network on the GPU.\nAt a high level, the ASA technique modifies solutions over multiple iterations using a temperature-based acceptance criterion. This criterion allows worsening solutions to be accepted with a certain probability, which depends on the current temperature. The temperature \\u03bb is manually set at the start of the search and is gradually reduced after each iteration"}, {"title": "4 EXPERIMENTS", "content": "We evaluate NDS on three VRP variants with 100 to 2000 customers and compare to state-of-the-art learning-based and traditional OR methods. Additionally, we provide ablation experiments for the individual components of NDS and evaluate the generalization across different instance distributions. All experiments are conducted on a research cluster utilizing a single Nvidia A100 GPU per run. We will release our implementation of NDS, along with the instance generators, under an open-source license upon acceptance."}, {"title": "4.1 PROBLEMS", "content": "CVRP The CVRP is one of the most extensively studied variants of the VRP. The goal is to determine the shortest routes for a fleet of vehicles tasked with delivering goods to a set of N customers. Each vehicle begins and ends its route at a depot and is constrained by a maximum carrying capacity. We use the instance generator from Kool et al. (2019) to create scenarios with uniformly distributed customer locations, and the generator from Queiroga et al. (2022) for generating more realistic instances, with clustered customer locations to better simulate real-world conditions.\nVRPTW The VRPTW extends the traditional CVRP by adding time constraints for customer deliveries. Each customer has a time window, defining the earliest and latest allowable delivery times. Vehicles can arrive early but must wait until the window opens, adding scheduling complexity. All routes start at a central depot, with a fixed service duration for deliveries and travel times based on the Euclidean distance. The objective is to minimize the total travel time while respecting both vehicle capacity and time windows, making VRPTW more complex than the standard CVRP. To\nPCVRP The PCVRP is a variant of the VRP in which not all customers need to be visited. Each customer is associated with a prize, and the objective is to minimize the total travel cost minus the sum of collected prizes. Similar to the CVRP, all vehicles start and end their routes at a central depot and are constrained by vehicle capacities. To generate PCVRP instances, we use the instance generator from Queiroga et al. (2022) to create customer locations and demands. Customer prize values are generated at random, with higher prizes assigned to customers with greater demand, reflecting the increased resources required to service them."}, {"title": "4.2 SEARCH PERFORMANCE", "content": "Baselines We compare NDS to several heuristic solvers, including HGS (Vidal, 2022), SISRs (Christiaens & Vanden Berghe, 2020), and LKH3 (Helsgaun, 2017). Additionally, we include PyVRP (Wouda et al., 2024) (version 0.9.0), which is an open-source extension of HGS for other VRP variants. For the CVRP, we further compare NDS to the state-of-the-art learning-based methods, SGBS-EAS (Choo et al., 2022), BQ (Drakulic et al., 2023), LEHD (Luo et al., 2023), and GLOP (Ye et al., 2024b).\nNDS Training For each problem and problem size, we perform a separate training run. Training consists of 2000 epochs for settings with 1000 or fewer customers. For the 2000 customer setting, we resume training from the 1000 customer model checkpoint at 1500 epochs and train for an additional 500 epochs. In each epoch, we process 1500 instances, with each instance undergoing 100 iterations, 128 rollouts, and 10 initial improvement steps. The learning rate is set to 10-4 and 15 customers are selected per deconstruction step across all problem sizes. The training durations are approximately 5, 8, 15, and 8 days for the problem sizes 100, 500, 1000 and 2000, respectively. The training curves are presented in Appendix A, while visualizations of policy rollouts are available in Appendix B.\nEvaluation Setup At test time, we limit the runtime to 5, 60, 120, and 240 seconds of wall time per instance for HGS, SISRs, and NDS to ensure a fair comparison, as these methods process test instances sequentially. SGBS-EAS and LEHD, which process instances in batches, are given an equivalent search budget per batch. All approaches are restricted to using a single CPU core. For the CVRP, we use the test instances from Kool et al. (2019) for N=100 (10,000 instances), Drakulic et al. (2023) for N=500 (128 instances), and Ye et al. (2024b) for N=1000 and N=2000 (100 instances each). For the VRPTW and PCVRP, we generate new test sets consisting of 10,000 instances for N=100 and 250 instances for settings with more than 100 customers.\nNDS Test Configuration For NDS, the starting temperature \\u03bbstart is set to 0.1 and decays exponentially to 0.001 throughout the search. The threshold factor \\u03b4 is fixed at 15. During the improvement step, 200 rollouts are performed per instance, and each deconstructed solution is reconstructed 5 times (1x based on the selected order of the DNN and 4\\u00d7 using a random customer order). The number of augmentations is set to 8 for the CVRP and VRPTW, and 128 for the PCVRP.\nResults Table 1 presents the performance of all compared methods on the test data. The gap is reported relative to HGS for the CVRP, and to PyVRP-HGS for the VRPTW and PCVRP. Across the 12 test settings, NDS delivers the best performance in 11 cases, with HGS being the only approach able to outperform it on the CVRP with 100 customers. Compared to other learning-based methods, NDS shows significant performance improvements across all CVRP sizes. On the CVRP with 2000 customers, NDS achieves a 7 percentage point improvement over the best-performing learning-based method, LEHD, and a 12 percentage point improvement over GLOP. Against the state-of-the-art HGS and its extension, PyVRP-HGS, NDS performs especially well on larger instances, achieving a gap of more than 2% across all problems for instances with 2000 customers. For the PCVRP, NDS also attains substantial gaps relative to PyVRP-HGS, exceeding 4% on instances with 500 or more nodes. When compared to SISRs, NDS maintains a small advantage on larger instances and demonstrates significantly better performance on small instances."}, {"title": "4.3 ABLATION STUDIES", "content": "We perform a series of ablation experiments to assess the importance of different components of NDS. These experiments are conducted on separate validation instances with N=500 customers. The parameter configuration remains identical to the previous section, except the training is reduced to 1,000 epochs and the ASA search is limited by the number of iterations. For the CVRP and VRPTW, we run 1,000 iterations using 8 augmentations, while for the PCVRP, we perform 50 iterations with 128 augmentations."}, {"title": "Network Architecture", "content": "We assess the impact of the message passing layer (MPL) and tour encoding layer (TEL) on overall performance by training separate models without these components. Table 2a summarizes the resulting search performance. Excluding both layers leads to a significant performance drop, with a 1.5% reduction on the PCVRP. Even the removal of a single layer causes a notable performance decline, particularly for the VRPTW and PCVRP. The VRPTW in particular benefits from both layers, likely due to the MPL's ability to better interpret and handle time windows."}, {"title": "Insertion Order", "content": "The insertion algorithm reinserts removed customers in a specified order. During testing, we reconstruct a deconstructed solution five times using different customer orders and retain the best solution. For the first reconstruction iteration, we use the customer order provided by the DNN, while for the remaining four iterations we use a random order. We compare our standard setting to using only random orderings for all five insertion iterations to assess whether the ordering enhances overall search performance. The results in Table 2b show that using a only random orderings leads to significantly worse performance across all three problems, indicating that the learned policy not only plays a crucial role in deconstruction, but also significantly influences reconstruction."}, {"title": "4.4 GENERALIZATION", "content": "One major advantage of learning-based solution approaches is their ability to adapt precisely to the specific type of instances at hand. However, in real-world scenarios, concept drift in the instance distributions cannot always be avoided. In this experiment, we evaluate whether the learned policies of NDS can handle instances sampled from a slightly different distribution. For the CVRP with N=500, we train a policy on instances with medium-capacity vehicles and customer locations that follow a mix of uniform and clustered distributions. We then evaluated the learned policy on instances with low- and high-capacity vehicles, and customer locations following either uniform or clustered distributions. Additionally, we train distribution-specific models for each test setting for comparison. As a baseline, we compare against HGS and SISRs, giving all approaches the same runtime. The results are shown in Table 3, where NDS (OOD) represents the model's performance when the training and test distributions differ, and NDS (ID) represents the setting where the training and test distributions are identical. Overall, the performance difference between the two settings is minimal, indicating that NDS generalizes well across different distributions. Interestingly, the distribution of customer locations has a larger impact on performance than vehicle capacity."}, {"title": "4.5 SCALABILITY ANALYSIS", "content": "We assess the scalability of NDS by analyzing its runtime and GPU memory consumption on CVRP instances of varying sizes. Overall, NDS demonstrates strong scalability to larger instances. Notably, solving instances with 1,000 customers requires only 61% more runtime and 23% more memory compared to instances with 100 customers, despite the problem size increasing by an order of magnitude."}, {"title": "5 CONCLUSION", "content": "In this work, we introduced a novel search method, NDS, which leverages a learned policy to deconstruct solutions for routing problems. NDS presents several key advantages. First, it delivers superior performance, consistently outperforming state-of-the-art OR methods under equal runtime. Second, NDS scales effectively to larger problem instances, handling up to N=2000 customers, due to the fact that the number of customers selected by the policy is independent of the problem size. Third, it demonstrates strong generalization across different data distributions. Finally, NDS is easily adaptable to new vehicle routing problems, requiring only small adjustments to the greedy insertion heuristic and the model input."}]}