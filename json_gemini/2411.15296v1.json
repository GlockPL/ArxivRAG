{"title": "MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs", "authors": ["Chaoyou Fu", "Yi-Fan Zhang", "Shukang Yin", "Bo Li", "Xinyu Fang", "Sirui Zhao", "Haodong Duan", "Xing Sun", "Ziwei Liu", "Liang Wang", "Caifeng Shan", "Ran He"], "abstract": "As a prominent direction of Artificial General Intelligence (AGI), Multimodal Large Language Models (MLLMs) have garnered increased attention from both industry and academia. Building upon pre-trained LLMs, this family of models further develops multimodal perception and reasoning capabilities that are impressive, such as writing code given a flow chart or creating stories based on an image. In the development process, evaluation is critical since it provides intuitive feedback and guidance on improving models. Distinct from the traditional train-eval-test paradigm that only favors a single task like image classification, the versatility of MLLMs has spurred the rise of various new benchmarks and evaluation methods. In this paper, we aim to present a comprehensive survey of MLLM evaluation, discussing four key aspects: 1) the summarised benchmarks types divided by the evaluation capabilities, including foundation capabilities, model self-analysis, and extented applications; 2) the typical process of benchmark counstruction, consisting of data collection, annotation, and precautions; 3) the systematic evaluation manner composed of judge, metric, and toolkit; 4) the outlook for the next benchmark. This work aims to offer researchers an easy grasp of how to effectively evaluate MLLMs according to different needs and to inspire better evaluation methods, thereby driving the progress of MLLM research.", "sections": [{"title": "INTRODUCTION", "content": "LARGE Language Models (LLMs) [1] are sweeping across\nthe whole artificial intelligence community. Through\nscaling up the size of model parameters and training cor-\npus, LLMs exhibit emergent capabilities, such as follow-\ning instructions [2] and learning from context [3]. Distinct\nfrom previous paradigms that train a specific model for a\nspecific task, LLMs are competent in solving a wide array\nof general tasks through prompting. Furthermore, LLMs\ncan only support language while our world is naturally\nmultimodal, encompassing information of various forms,\ne.g. vision and audio [4]. This limitation spurs the rise of\na newer family of models, i.e. MLLMs [5], [6]. Building\nupon LLMs, MLLMs are further equipped with capabilities\nof processing multimodal information, which considerably\nexpands the task coverage of models.\nIn the process of MLLM development, model evaluation\nhas played a crucial role since it quantitatively reflects"}, {"title": "BACKGROUND", "content": "In this section, we briefly introduce the essentials of MLLMS,\nincluding the architecture and training. For a more compre-\nhensive illustration, we recommend the relevant work [5]\nthat discuss MLLMs in detail."}, {"title": "Architecture of MLLM", "content": "A typical MLLM comprises three modules: a modality\nencoder [7], a LLM, and a connector between them, as\npresented in Fig. 2. Take the vision-language model as an\nexample, given a text query and vision sample, the vision\nencoder extracts features from the vision sample, while the\nconnector aligns the vision features with the text embedding\nspace. Subsequently, the aligned vision features are concate-\nnated with the text embeddings of user queries as input. The\nLLM takes this multimodal input and generates a natural\nlanguage response.\nSimilar to how LLMs process information, the core of\nMLLM is a unified autoregressive modeling:\n\\(p(w_o|w_v, w_T) \\sim \\prod_{t=1}^{L} P(w_t|w_{<t}, w_V, w_T)\\)\nwhere \\(w_o = {w_{o,t}}_{t=1}^{L}\\) is the output word token sequence\nof length L, \\(w_V\\) represents the processed vision tokens, and\n\\(w_T\\) corresponds to text embeddings of the user query."}, {"title": "Training of MLLM", "content": "We can see from Fig. 3, a comprehensive training process of\nMLLMs consists of three stages, i.e. pre-training, instruction\ntuning, and alignment tuning.\nPre-training. The main objective of the pre-training stage\nis to align different modalities [8] and inject multimodal\nworld knowledge into models. The pre-training stage typ-\nically involves large-scale text based paired data, such as\nimage caption data [9]. Generally speaking, the captions are\n\"translations\" of images, describing the content in natural\nlanguage. To align vision with text, MLLMs learn to predict\nthe ground-truth captions of the corresponding images in\nan autoregressive way.\nInstruction Tuning. Its purpose is to teach MLLMs to\nfollow instructions from users and complete the required\ntasks. Tuning in this way, MLLMs can generalize to new\ntasks defined by new instructions, thereby boosting zero-\nshot performance. Instruction data can be derived from\nthe adaptation of existing multi-task datasets, e.g. VQA, or"}, {"title": "BENCHMARK CATEGORIES", "content": "In this section, we introduce representative benchmarks\ndesigned for diverse evaluation purposes. We organize the\nexisting benchmarks for a quick scan as illustrated in Fig. 4.\nBesides, the examples of different evaluation tasks are dis-\nplayed in Fig. 5 and the statistics of some representative\nbenchmarks are given in Table 1."}, {"title": "Foundational Capability", "content": "A primary objective behind designing MLLM is to develop\nintelligent chatbots capable of comprehensively answering\nhuman queries related to perception and reasoning. A large\nnumber of evaluation benchmarks have emerged to assess\nthe comprehensive capabilities of MLLMs.\nVQA v2 [18] is an early benchmark that includes 453K\nmanually annotated QA pairs for model evaluation. It in-\ncludes open-ended questions such as counting objects and\ndistinguishing colors, but the answers are usually concise,\nsuch as one word. VizWiz [19] appears at approximately\nthe time of VQA v2. It contains 8K QA pairs derived from\nthe daily life scenarios of visually impaired individuals,\neffectively capturing the real-world needs of disabled users.\nHowever, these traditional benchmarks often fail to mea-\nsure the emergent capabilities of today's MLLMs, such as\npowerful reasoning. There has been some works to bring\ntogether existing traditional benchmarks for comprehen-\nsive evaluation. For example, LVLM-eHub [20] compiles"}, {"title": "Comprehensive Evaluation", "content": "extensive public datasets, including 47 standard text-related\nvisual benchmarks. The evaluation finds that, while MLLM\nsurpasses the SOTA in commonsense tasks, it significantly\nlags behind leading supervised models in tasks such as\nimage classification, OCR, and VQA. Similarly, LAMM [21]\nuses public datasets for evaluation, expanding beyond 9\ncommon image tasks. The research indicates that MLLMs\nperform poorly in large-scale counting problems, only capa-\nble of rough estimations, and also struggle with fine-grained\nattribute differentiation. Although MLLMs possess object\nlocalization abilities, accurately predicting bounding boxes\nremains challenging, which can be effectively mitigated\nwith further fine-tuning.\nConsidering the limitations of existing traditional bench-\nmarks, researchers have begun to design new evaluation\ndatasets specifically for the characteristics of MLLMs. For\nexample, MME [24] establishes a comprehensive benchmark\nencompassing 14 perception and cognition tasks, where the\nlatter consists of commonsense reasoning, numerical calcu-\nlation, text translation, and code reasoning. Similarly, MM-\nBench [22] features 20 distinct ability dimensions, including\nobject localization and social reasoning. Seed-Bench [23]\nshares similarities with MME and MMBench but consists of\na lager number of multiple-choice question. SEED-Bench-\n2 [25] further expands the QA pairs from 19K to 24K,\ncovering 27 evaluation dimensions. MMT-Bench [26] scales\nup the dataset even further, incorporating 31K QA pairs\nfrom diverse scenarios. These benchmarks highlight some\ncommon traits. For instance, the performance of models\nimproves significantly with increasing LLM scale [22], [26].\nFine-grained perception tasks, such as spatial localization\nand pixel-level perception, generally pose significant chal-\nlenges to MLLMs [22], [24], [26], [35]. Besides, MLLMs often\nstruggle with understanding charts and visual mathematics,\nwith this limitations becoming more pronounced as dataset\nsize increases [25], [26]. The interleave-image-text problem\nremains difficult to resolve, with related strategies during\nthe training phase only partially alleviating the issue [22],\n[26]. Lastly, with recent advancements in MLLMs, the per-\nformance of open-source models has increasingly matched\nor even surpassed that of closed-source counterparts [22],\n[24], [35], demonstrating the rapid progress of the open-\nsource community.\nReal-world usage scenarios have become a focal point\nfor researchers seeking to understand how models per-\nform in practical applications. For instance, RealWorldQA\u00b9\nevaluates fundamental spatial understanding capabilities\nsourced from real-life scenarios. These scenarios, though\nrelatively straightforward for humans, often challenge state-\nmanually annotated QA pairs for model evaluation. It in-of-the-art models. Similarly, BLINK [27] identifies tasks such\nas relative depth estimation, visual correspondence, foren-\nsics detection, and multi-view reasoning that humans can\nsolve \u201cwithin a blink\" but present significant challenges\nfor current MLLMs. WV-Bench [29] and VisIT-Bench [30]\nunderscores the importance of evaluating human prefer-\nences and instruction-following capabilities in real-world\napplications. MME-RealWorld [35] places greater empha-\nsis on quality and difficulty compared to its predecessor,\ncontaining the largest manually annotated QA pairs and"}, {"title": "Optical Character Recognition (OCR)", "content": "Current multimodal benchmarks increasingly focus on eval-\nuating model performance in Optical Character Recogni-\ntion (OCR) tasks, driving technological advancements in\nareas such as document understanding and transportation.\nBenchmarks have evolved from single scenarios to com-\nplex multiple scenarios. For instance, TextVQA [36] and\nOCR-VQA [37] focus on standard text recognition tasks,\nwhile InfoVQA [44] and WebSRC [38] introduce more intri-\ncate structural reasoning tasks, such as understanding web\npage structures and inferring information from infograph-\nics. SEED-Bench-2-Plus [40] and OCRBench [39] further\nbroaden the scope of tasks by including diverse data types\nlike charts, maps, and web pages, demonstrating that mod-\nels can perform comparably to state-of-the-art supervised\nmodels in recognizing regular text, irregular text, occluded\ntext, and artistic text. Additionally, VCR [41] addresses\nvariants of OCR where text is embedded in images and\npartially occluded, requiring models to restore the specific\ncontent of the text from the images. However, many MLLMs\nstill face challenges in fine-grained OCR capabilities, hand-\nwriting, non-semantic text, and multilingual text recogni-\ntion [39], [40], [41], [175]. MLLMs like GPT-4V have shown\nexceptional performance in several evaluations [39], [40],\n[176], but still lag behind models trained specially on OCR\ntasks [175]. Besides, the impact of different data types on"}, {"title": "Chart and Documentation", "content": "Charts and documents are important data types in practical\napplications, designed to convey information in an efficient\nway. Unlike natural images, these data are highly structured\nand dense in information, requiring models to understand\nthe layouts and the relationships between the embedded\nelements. With the purpose of developing models that can\nunderstand and reason with such data, benchmarks for\ndifferent types of charts [42], [44], [47], [48], [49], [50], [51],\n[52] and documents [43], [45], [46] have been proposed.\nChartQA [42] focuses on VQA with charts, such as bar,\nline, and pie plots. The questions range from ones that\ndemand simple data retrieval to more complex compo-\nsitional ones that require both data extraction and math\nreasoning. DocVQA [43] is built for VQA on document\nimages scraped from industry documents. The questions\ngenerally focus on simpler information extraction tasks.\nInfoVQA [44] centers on understanding infographic images,\na type of data designed to convey information compactly.\nDue to this nature, the layouts and structures of infographics\nare more diverse than conventional charts. Questions in this\nbenchmark generally require basic reasoning and arithmetic\nskills. As MLLMs evolve, recent benchmarks shift to the\nunderstanding of more complex charts and documents.\nFor example, DocGenome [45] focuses on the analysis of\nscientific papers, with tasks ranging from information ex-\ntraction and layout detection to VQA and code generation.\nCharXiv [47] centers on challenging charts from scientific\npapers. MMLongBench-Doc [46] focuses on general long\ndocument understanding, where documents span an aver-\nage of 47.5 pages.\nAlthough the performance gap between proprietary\nmodels and open-source models is closing on more tradi-\ntional benchmarks like ChartQA, DocVQA, and InfoVQA, it\nis still wide on more challenging benchmarks like CharXiv\nand MMLongBench-Doc. Moreover, current MLLMs still\nstruggle with 1) reasoning questions that require more than\nsimple information extraction [47] and 2) long-context docu-\nment understanding [46], where understanding long multi-\nmodal context is critical."}, {"title": "Mathematical Reasoning", "content": "Visual math problem-solving capability is a critical aspect of\nMLLM evaluation, giving rise to many specifically designed\nbenchmarks. MathVista [53] is an early attempt in this direc-\ntion, which collates samples from existing datasets as well\nas newly created ones. The images vary from mathematical\nillustrations, such as geometry diagrams and bar charts, to\ndifferent scenes and domains, such as abstract scenes and\nmedical images. Subsequent works develop more challeng-\ning benchmarks [54], [55], and design more fine-grained\nsettings for evaluation [56], [57]. For instance, We-Math [57]\ndecomposes a problem into sub-problems based on knowl-\nedge concepts and evaluates MLLMs at the level of basic\nknowledge concepts. To evaluate to what extent MLLMs"}, {"title": "Multidisciplinary", "content": "The mastery of multidisciplinary knowledge is an important\nindicator of the models' expertise. Multiple benchmarks for\nthis sort of evaluation have been developed. ScienceQA [58]\nis a benchmark of scientific questions with annotations\nof lectures and explanations for ease of chain-of-thought\nevaluation. The benchmark covers grade-level (1\u201312) knowl-\nedge across various domains. MMMU [59] is a more chal-\nlenging benchmark that covers broad subjects and college-\nlevel questions, including engineering, art and design, busi-\nness, science, humanities and social science, and medicine.\nThe format of questions further develops from a single\nimage-text pair into interleaved text and images. Similarly,\nCMMU [60] (grade-level knowledge) and CMMMU [61]\n(college-level knowledge) are domain-specific benchmarks\nin Chinese contexts. The comprehensive evaluation of these\nworks reveals that even the advanced models (such as GPT-\n4V and Gemini Ultra) can only achieve accuracies lower\nthan 60%, which suggests a huge space for improvement\ntowards AGI."}, {"title": "Multilingual", "content": "MLLMs are progressively developed towards multilin-\ngualism to benefit a larger community. Apart from the\npredominant English, researchers have collected bench-\nmarks in other languages to accommodate evaluation un-\nder other cultural contexts and customs, including Chi-\nnese [60], [61], [63], [69], Urdu [66], Swahili [67], Viet-\nnamese [68], and multi-languages [64], [65]. For example,\nCMMMU [61] follows MMMU [59] and collects a multidisci-\nplinary benchmark in Chinese. Works like ViOCRVQA [68],\nUrdu-VQA [66], and Swahili-STR [67] evaluate OCR and\nVQA capabilities in other languages. Video-MME [178] is\ndedicated to a multilingual evaluation category that in-\ncludes the world's dominant languages. MTVQA [64] and\nM3Exam [65] develop multilingual benchmarks across 9\ndifferent languages. The evaluation reveals that the perfor-\nmance varies greatly when evaluated in different languages.\nNotably, both proprietary models and open-source models\nperform better in Indo-European languages that use the\nLatin alphabet, such as German, French, and Italian, which\nmight be attributed to their visual and linguistic similarities\nwith English [64]."}, {"title": "Instruction Following", "content": "Instruction following refers to the ability to comply with\nuser instructions and perform specified tasks. As a foun-\ndational capability, instruction following directly influences\nresponse quality and user experience. MIA-Bench [70] is de-\nsigned to evaluate how well MLLMs can adhere to complex"}, {"title": "Multi-Round QA", "content": "Current MLLMs are generally developed as multi-round\nchatbots, while most benchmarks remain at the single-round\nQA stage. Multi-round QA benchmarks are developed to\nalign with real-world conversation scenarios, simulating the\nhuman-AI interaction setting with a long-context history.\nConvBench [71] develops a progressive evaluation scheme,\nwith each round focusing on a specific capability, i.e. percep-\ntion, reasoning, and creation. The evaluation is performed\nat both the single-round level and the overall conversation\nlevel. Evaluation results reveal that insufficient fine-grained\nperception in MLLMs leads to reasoning and creation fail-\nures. MMDU [72] engages in multi-turn and multi-image\nconversations, where a conversation sample can include up\nto 20 images and 27 turns. The analysis points out that\nthe gap between open-source models and closed-source\nones can be attributed to limited conversational instruction\ntuning data."}, {"title": "Multi-Image Understanding", "content": "With the evolution of MLLMs, researchers have explored\nupgrading vision capabilities from single image to multiple\nimage. In line with this tendency, some benchmarks for mul-\ntiple images are compiled. NLVR2 [73] is a early benchmark,\nwhere each sample contains a pair of similar images and a\nnatural language caption. The task is to decide whether the\ncaption is true with respect to the pair of images. Recently\nproposed benchmarks are more specifically designed for the\nevaluation of MLLMs. For example, SparklesEval [74] chal-\nlenges models' conversational proficiency across multiple\nimages and multiple turns. The user prompt is presented\nin the flexible form of interleaved text and images. Each\ninstance contains two rounds of dialogue with four images\nin total. Similarly, MMDU [72] is a multi-image and multi-\nround benchmark with a maximum of 20 images and 27\nturns inside a single sample.\nThere are some other benchmarks pay more attention\nto reasoning with multiple highly correlated images. Me-\nmentos [75] is designed to evaluate MLLMs' capabilities\nof understanding sequential images, covering daily life,\nrobotics, and comics domains. MIRB [76] aims to assess\nthe ability to answer by aggregating and reasoning with\ninformation from multiple images. It encompasses four\ncategories: perception, visual world knowledge, reasoning,\nand multi-hop reasoning. ReMI [77] designs 13 tasks with\nvarious input formats and relationships between images, e.g.\nfrom same or different concepts. MuirBench [78] devises 12\nmulti-image understanding tasks, e.g. scene understanding\nand visual retrieval, with diverse multi-image relations like"}, {"title": "Interleaved Images and Text", "content": "Interleaved images and text are natural forms of information\ndelivery, and prevalent on the Internet in media like blogs\nand news. While most benchmarks adopt the image-text\nnon-interleaved format, there are multiple benchmarks have\nbeen developed to evaluate models' ability to understand\ninterleaved content.\nIn MMMU [59], the format of questions is interleaved\ntext and images. SparklesEval [74] adopts a similar format\nand a two-round prompt fashion. VEGA [79] is specifically\ndesigned for the evaluation of interleaved image-text com-\nprehension. The proposed task requires models to discern\nuseful images and text from superfluous ones and derive\nthe correct answer. Evaluation results show that advanced\nproprietary MLLMs such as GPT-4V and Gemini 1.5 pro\nonly achieve modest performance, suggesting large room\nfor improving interleaved information processing."}, {"title": "High Resolution", "content": "Processing images of high resolution is an important capa-\nbility of MLLMs, especially in practical applications like\nautonomous driving. V*Bench [80] is designed to assess\nperformance in processing high-resolution images and fo-\ncusing on correct visual details. This benchmark contains\n191 high-resolution images with a resolution of 2,246x1,582\non average. Two sub-tasks are designed: the attribute recog-\nnition task aims to recognize the attribute such as color or\nmaterial of an object; the spatial relationship reasoning task\nrequires the model to determine the spatial relationships be-\ntween two objects. MME-RealWorld [35] has 13,366 images\naveraging 2,000\u00d71,500 resolution, including the real-world\ntasks of video monitoring, autonomous driving, remote\nsensing, diagram table, and OCR in the wild. The evaluation\nresults show that even the most advanced MLLMs have not\nachieved more than 60% accuracy, suggesting the difficulty\nof these scenarios."}, {"title": "Visual Grounding", "content": "Visual grounding is a classical computer vision task that\naims to locate the most relevant object/region specified by\na natural language query [181], [182]. The query is usually a\nshort expression, such as \u201cwoman in red\". On the traditional\nbenchmarks like RefCOCO [81], RefCOCO+ [82], and Ref-\nCOCOg [82], MLLMs have already achieved performance\ncomparable to SOTA specialist models [183], [184]. In view\nof the relatively high labeling error rates in RefCOCO series,"}, {"title": "Model Self-Analysis", "content": "In order to better understand MLLM itself, researchers have\ndeveloped various benchmarks to study the behavior or\ncharacteristics of models, including hallucination, model\nbias, safety, and causal analysis. In this section, we introduce\ntypical aspects of model analysis."}, {"title": "Hallucination", "content": "The term \"multimodal hallucination\" is used to describe\nthe phenomenon, where the response content generated\nby MLLMs is inconsistent with the visual content [185].\nHallucination is a critical issue damaging model reliability\nand hindering its practical application."}, {"title": "Bias", "content": "Model bias is a critical issue that hinders the usability\nof MLLMs. Current benchmarks have explored different\naspects of model bias and shed light on possible reasons.\nVLBiasBench [116] identifies response bias unaligned with\nhuman values. Specifically, the benchmark covers 9 cate-\ngories of social biases, such as age, gender, and physical ap-\npearance. The evaluations on open-source and closed-source\nmodels reveal that the open-source models like LLaVA [187]\nand Shikra [188] generally show different degrees of bias,\nwhile the advanced closed-source model, Gemini [189],\nconsistently exhibits weak bias. This suggests a huge gap\nbetween open-source and closed-source models in terms"}, {"title": "Safety", "content": "Model safety is of central concern for model deployment in\npractice. Benchmarks of this type mainly consider robust-\nness, including Out-Of-Distribution (OOD) robustness and\nadversarial robustness, as well as jailbreaks.\nOOD Robustness. It mainly considers MLLMs' ability\nto generalize to unseen domains, such as different styles of\nimages not encountered in the training corpus. For example,\nOODCV-VQA and Sketchy-VQA [118] incorporate rarely\nseen images in real-life scenarios and simple sketchy images,\nrespectively. Moreover, OOD text instructions adapted from\nthe original questions are also included. MultiTrust [119]\nfurther considers images from other domains, e.g. MRI and\ninfrared images. The evaluation results show that MLLMs\nare better at understanding OOD visual content rather than\nfollowing OOD text instructions [118]. This might indicate\ninsufficient capabilities to generalize to new instructions.\nAdversarial Robustness. Adversarial attacks on MLLMs\naim to trick models into making wrong responses. Cor-\nrespondingly, adversarial robustness is a critical aspect to\nevaluate, which measures how robust models are to ma-\nlicious attacks. AttackVLM [120] develops a framework to\nsynthesize adversarial samples and evaluate the adversarial\nrobustness of open-source MLLMs. The evaluation results\nreveal the adversarial vulnerability of open-source models\nlike LLaVA [14] and MiniGPT-4 [190]. AdvDiffVLM [121]\naims to improve the efficiency and transferability for the\ngeneration of adversarial samples. Experimental results\nshow that, compared with open-source models, closed-\nsource models exhibit better adversarial robustness, sug-\ngesting a large room for improvement.\nJailbreaks. It centers on models' capabilities to reject\nattempts to elicit illegal responses [119], [191]. VLLM-safety-\nbenchmark [118] designs two jailbreak strategies target-\ning LLM and ViT respectively, to assess model resilience.\nMultiTrust [119] incorporates three tasks to test models'\nrobustness against jailbreaking, including 1) inserting de-\ntailed jailbreaking prompts into images, 2) combining nor-\nmal textual prompts with jailbreaking prompts inserted into\nimages, and 3) jailbreaking prompts paired with positively\nor negatively correlated images. These studies reveal that"}, {"title": "Causation", "content": "It refers to the cause-and-effect relationship where a change\nin one variable results in a change in another variable [123].\nThe ability to understand this relationship, i.e. causal rea-\nsoning, is an important ability to understand and analyze\nour world. Recently, some works have explored the evalu-\nation of MLLMs in terms of causal reasoning capabilities.\nCELLO [123] introduces a unified definition of causality\ninvolving humans and/or objects, and constructs a bench-\nmark of 12 causal tasks. Evaluations show that current\nMLLMs, such as BLIP-2 [192] and Claude3 Sonnet [193],\nexhibit weak causal reasoning abilities with some underper-\nforming random guessing."}, {"title": "Extended Applications", "content": "With the rapid development of MLLMs, researchers have\nactively explored the application in downstream tasks and\ndeveloped corresponding benchmarks in the fields such as\nmedicine and emotion. Compared with general evaluation,\nthese benchmarks focus more on the mastery of domain\nknowledge and skills."}, {"title": "Medical Image", "content": "Medical images directly reflect the state of the human body\nand are critical parts of clinical decision-making. A number\nof benchmarks have been developed to evaluate the perfor-\nmance of MLLMs in analyzing this type of image.\nVQA-RAD [124] is an early benchmark designed for\nthe VQA task on radiology images, encompassing 11 ques-\ntion types, including plane, modality, organ system, etc.\nThe question and answer are generally simple and con-\ncise, with the answer spanning only one or a few words.\nPathVQA [125] is a similar benchmark focusing on pathol-\nogy images. SLAKE [126] is a bilingual (Chinese and En-\nglish) benchmark with more annotations in terms of modal-\nities, including segmentation masks and bounding boxes.\nRecent benchmarks are heading towards more comprehen-\nsiveness. PMC-VQA [127] encompasses more image do-\nmain\ns, including radiology, pathology, microscopy, signals,\netc. RadBench [129] covers both 2D and 3D scan images\nand 5 distinct tasks, including modality recognition, disease\ndiagnosis, VQA, report generation, and rationale diagno-\nsis. GMAI-MMBench [130] incorporates 39 medical image\nmodalities, 18 clinical-related tasks, 18 departments, and"}, {"title": "Emotion Analysis", "content": "Emotion analysis aims to extract human emotions from data\nof various modalities, such as vision, text, and audio. Differ-\nent from common tasks that are largely objective, emotion\nanalysis entails interpreting highly subjective and emotional\nmultimodal content, thus posing new challenges. With its\npowerful generalization and reasoning capabilities, MLLMs\nare expected to make a breakthrough in this task.\nEmoBench [131] contains tasks ranging from general\nemotion and intention understanding (multiclass classi-\nfication from pre-defined sets) to emotion detection in\nsocial media (binary classification, \u201cYes/No\u201d), with data\nsourced from existing datasets. FABA-Bench [132] focuses\non facial emotion analysis, incorporating two tasks, i.e.\nemotion recognition and action unit recognition. The eval-\nuation results on these benchmarks reveal that MLLMS\nfine-tuned with emotion-related data can achieve superior\nperformance compared with zero-shot MLLMs, including\nadvanced closed-source models like GPT-4V. This suggests\nthat it is essential to inject emotion-domain knowledge for\ndownstream tasks of emotion analysis."}, {"title": "Remote Sensing", "content": "Remote sensing is a multidisciplinary field that involves the\nacquisition and analysis of information about the Earth's\nsurface and atmosphere from a distance, typically using\nsatellites or aerial sensors. Remote sensing plays a cru-\ncial role in numerous applications, such as environmen-\ntal monitoring, urban planning, agriculture, and disaster\nmanagement. Multiple benchmarks have been developed to\nadvance the understanding of remote sensing images.\nEarly works such as RSVQA [133] builds evaluation\nsets in the form of traditional VQA, covering tasks like\nclassification, object counting, and detection. The questions\nand answers in the RSVQA benchmark are concise and built\nfrom pre-defined pipelines based on elements (e.g. road and\nwater area) and associated attributes (e.g. shape and size) or\npositional relations. The two subsets of the benchmark con-\ntain images of low resolution (256px) and high resolution\n(512px), respectively. More recent benchmarks enjoy wider\ncoverage of tasks and QA pairs. For instance, RSIEval [138]\nmanually annotates captions and visual questions. Apart\nfrom common object-related questions involving existence,\nquantity, or color, the benchmark also includes questions\nthat require reasoning/external knowledge, such as \"What\nseason was this image taken in?\". Similarly, VRSBench [140]"}, {"title": "Agent", "content": "An intelligent agent can perceive the environment and take\naction to fulfill target tasks. Recently, developing multi-\nmodal agents that can process and reason with multimodal\ninformation, e.g. vision, audio, and text, has aroused wide\nattention [194], where MLLMs are playing a pivotal role. In\nline with this progress, multiple benchmarks have been built\nto gauge the performance of MLLMs in acting as agents.\nAppAgent [141] mainly assesses agents' abilities to per-\nform 50 tasks on 10 smartphone applications like Google\nMaps, as the instruction like \"change my profile name\nto AppAgent\". The used metrics include successful rate,\nreward, and average steps. Mobile-Eval [142] is a similar\nbenchmark designed to evaluate mobile agents. This bench-\nmark contains 3 instructions for each of the 10 mainstream\nApps. GPT4Tools [143] centers on the capability of tool\nusage, with metrics designed for different aspects, including\noverall successful rates and successful rates in terms of\napplying specific tools, such as thought, tool name, and\ntool arguments. Evaluation results show that even advanced\nGPT-4 struggles to plan and execute smartphone application\nqueries in a zero-shot way, partially due to the challenges of\naccurately predicting coordinates [141], [142] or insufficient\nknowledge of the specific applications, which entails more\nexplorations to solve."}, {"title": "Code Generation", "content": "Code generation is an essential capability of MLLMs, which\nhas a wide range of applications in real life, such as assisting\nin writing code or providing automatic solutions for a\ncomplicated problem.\nChartMimic [144", "145": "revolves around webpage-to-code generation,\naiming to assess the ability to translate webpage screenshots\ninto HTML code. According to the evaluation results, the\ncode generation ability of the LLM backbone plays an\nimportant role [145", "195": "only achiev-\ning"}]}