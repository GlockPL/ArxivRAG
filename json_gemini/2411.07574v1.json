{"title": "Disentangling Tabular Data towards Better One-Class Anomaly Detection", "authors": ["Jianan Ye", "Zhaorui Tan", "Yijie Hu", "Xi Yang", "Guangliang Cheng", "Kaizhu Huang"], "abstract": "Tabular anomaly detection under the one-class classification setting poses a significant challenge, as it involves accurately conceptualizing \"normal\" derived exclusively from a single category to discern anomalies from normal data variations. Capturing the intrinsic correlation among attributes within normal samples presents one promising method for learning the concept. To do so, the most recent effort relies on a learnable mask strategy with a reconstruction task. However, this wisdom may suffer from the risk of producing uniform masks, i.e., essentially nothing is masked, leading to less effective correlation learning. To address this issue, we presume that attributes related to others in normal samples can be divided into two non-overlapping and correlated subsets, defined as CorrSets, to capture the intrinsic correlation effectively. Accordingly, we introduce an innovative method that disentangles CorrSets from normal tabular data. To our knowledge, this is a pioneering effort to apply the concept of disentanglement for one-class anomaly detection on tabular data. Extensive experiments on 20 tabular datasets show that our method substantially outperforms the state-of-the-art methods and leads to an average performance improvement of 6.1% on AUC-PR and 2.1% on AUC-ROC.", "sections": [{"title": "Introduction", "content": "Tabular anomaly detection under the one-class classification setting presumes the availability of only one-class data, i.e., the normal class samples for training, while the goal during the test is to discern anomalies (Chandola, Banerjee, and Kumar 2009; Ruff et al. 2021). In practical scenarios, such as financial fraud detection, cyber intrusion detection, and medical diagnosis (Hilal, Gadsden, and Yawney 2022; Malaiya et al. 2019; Chen and Konukoglu 2018), this method tries to detect whether new data points conform to the pattern of observed normal samples, thereby identifying them as either normal or abnormal. Given that only normal instances are available during the training, the inherent challenge lies in extracting the invariant feature of normal data. An instance observed with patterns deviating from these characteristics is detected as an anomaly. However, the lack of prior knowledge on structures in tabular data poses a significant challenge for learning such knowledge (Shenkar and Wolf 2022). One reasonable way to mitigate this challenge is to focus on capturing the intrinsic correlation among attributes of normal samples. Once one or more attributes of a sample deviate anomalously, it is an anomaly, and the correlation among attributes becomes different from that exhibited in normal samples. The most recent MCM (Yin et al. 2024) proposes a learnable mask generator module with a diversity loss to create diverse soft masks (i.e., mask values range from 0 to 1 for input data). A reconstruction task is then designed to restore the original data from its masked variants. During inference, anomalous attributes will disrupt the correlations observed in the normal class, leading to a failure to reconstruct the original data from masked instances, thereby allowing anomalies to be detected. Despite its efficacy, the diversity loss may generate masks with nearly uniform values, inducing trivial solutions of the learned intrinsic correlations. Empirical evidence in Figure 1(a) suggests that the values of all soft masks are around 0.5. In this case, each attribute is scaled with the same value, indicating essentially no attribute is masked. Thus, the model learns the objective of reconstructing the original data from complete features instead of partial attributes, resulting in less effective extraction of the desired correlation.\nIn this paper, we presume that attributes related to others in normal tabular data can be split into two\u00b9 non-overlapping and correlated subsets, termed as CorrSets, to ensure the distillation of the intrinsic correlation among attributes from normal instances. Building on this premise, this paper validates that disentangling those CorrSets from normal samples for reconstruction promotes the learning of normal samples' inside correlations. Specifically, the sufficiently disentangled CorrSets are used to restore the whole original data individually, in which processes of the correlation inside normal data would be well captured by the model. Compared to MCM (Yin et al. 2024), which may generate ineffective masks, this approach guarantees that the model reconstructs the original data with partial attributes, thus strengthening its ability to extract the correlation within normal samples and yielding improvements in anomaly detection.\nMotivated by the aforementioned assumption, we propose a novel paradigm named Disent-AD, which efficiently disentangles CorrSets from normal tabular data and adequately captures the intrinsic correlation. For practical implementation, a two-head self-attention module is utilized to implicitly extract two distinct subsets in latent space. The attention maps depict the attributes of interest to the network. By learning two independent attention maps, the two attention heads focus on different regions of tabular data, enabling the implicit extraction of distinct attribute subsets. As shown in Figure 1(b), darker regions in attention maps indicate greater weights. Evidently, the attention module successfully concentrates on two non-overlapping attribute subsets. To ensure that disentangled subsets are correlated, a reconstruction task is performed to restore the original data by utilizing subset features extracted from either of the two attention heads. We train the attention module with the reconstruction task in an end-to-end fashion. During testing, samples with high reconstruction errors are detected as anomalies.\nOur contributions can be summarized as follows:\n\u2022 To our knowledge, this is one pioneering work that leverages the concept of disentanglement to enhance the efficiency of tabular one-class anomaly detection.\n\u2022 We propose a novel paradigm that learns the intrinsic correlation by disentangling two distinct and correlated attribute subsets from normal tabular data.\n\u2022 Extensive experiments conducted on 20 tabular datasets demonstrate the superiority of our method to state-of-the-art methods, with an average improvement of 6.1% and 2.1% in the AUC-PR and AUC-ROC, respectively."}, {"title": "Related Work", "content": "One-class anomaly detection aims to identify unobserved class samples during training by utilizing the knowledge learned from a single normal class. Relying on distance measurement techniques (Breunig et al. 2000), empirical cumulative distribution functions (Li et al. 2022), and regularized classifiers (Sch\u00f6lkopf et al. 1999) are some of the classical ways to tackle this task. A straightforward approach involves modeling a distribution based on normal samples and evaluating the likelihood of each test sample (Zong et al. 2018; Li et al. 2020). Establishing a reliable decision boundary between normal and anomaly samples through an end-to-end process with an effective one-class loss function has proven useful (Ruff et al. 2018). Generating pseudo anomalies is an alternative way to construct a decision boundary (Goyal et al. 2020; Cai and Fan 2022), which utilizes adversarial training or variational autoencoder (VAE) (An and Cho 2015) to produce synthetic data that are abnormal yet closely resemble normal samples. Nevertheless, both methods require specific assumptions about distributions of normal and anomaly samples, which may not always be appropriate.\nRecent methods focus on designing self-supervision learning tasks to detect anomalies, such as applying geometric transformations to images and predicting the transformation (Golan and El-Yaniv 2018). However, geometric transformations are essentially not suitable for tabular data. To tackle this issue, GOAD (Bergman and Hoshen 2020) develops learnable transformations for tabular data. Inspired by learnable transformations, NeuTraLAD (Qiu et al. 2021) designs one contrastive loss to learn the invariant relationship among original and multiple transformed samples for normal data. With the idea of building a criterion based on contrastive loss, ICL (Shenkar and Wolf 2022) aims at maximizing the mutual information between each subset and the rest of the parts in normal tabular data. SLAD (Xu et al. 2023b) proposes scale as a new characteristic for tabular data to capture the invariant representations of the normal class, which is treated as the relationship between the dimensionality of attribute subsets and that of their transformed latent representations. The most recent MCM (Yin et al. 2024) focuses on learning the intrinsic correlation of normal samples by restoring original data from data masked by diverse soft masks. Unlike previous methods, our work concentrates on implicitly disentangling CorrSets from normal data to extract the intrinsic correlation for anomaly detection.\nSemi- and weakly-supervised anomaly detection on tabular data assumes that anomaly samples are accessible during training (Yoon et al. 2020; Chang et al. 2023; Pang et al. 2023). Introducing anomaly knowledge into the training process enables these methods to extract discriminative knowledge between normal and anomaly features. In this paper, we train the model for anomaly detection without prior knowledge of anomalies. This task is particularly challenging as it involves constructing a criterion for distinguishing between normal and abnormal samples without explicit examples of anomalies to guide the process.\nOut-of-distribution (OOD) detection often trains the network with a conventional classification task (e.g., ten-class classification) and identifies test samples belonging to the different distribution from which the training set is sampled (Liu et al. 2020; Zhu et al. 2022). Training with a multi-class classification task permits OOD detection methods"}, {"title": "Methodology", "content": "3.1 Problem Statement\nOne-class anomaly detection on tabular data setup involves a training set \\(D_{train} = {x_i \\in \\mathbb{R}^M}_{i=1}^{N}\\), which consists of N normal samples with M attributes. A test set, \\(D_{test} = {x_i \\in \\mathbb{R}^M, y_i \\in \\mathbb{Y}}_{i=1}^{J}\\), consists of samples \\(x_i\\) with labels \\(y_i\\), where \\(y = {0,1}\\) (0 denotes a normal and 1 denotes an anomaly). By training on \\(D_{train}\\), a deep anomaly detection model constructs a scoring function \\(\\phi : \\mathbb{R}^M \\rightarrow \\mathbb{R}\\) that quantitatively assesses the abnormality levels of new data points.\n3.2 Overview\nOur strategy for anomaly detection is illustrated in Figure 2. Considering a tabular dataset where each sample is composed of M attributes: \\(X_i = {a_1, a_2, \\dots, a_M}\\). We denote the CorrSets of normal data as \\(s_{norm}^1\\) and \\(s_{norm}^2\\), \\(s_{norm}^1 \\cup s_{norm}^2 = x_{norm}\\) if there are no noise attributes in \\(x_i\\). The inherent correlation can be captured by training network \\(f_\\theta\\) to restore the original data with only \\(s_{norm}^1\\) or \\(s_{norm}^2\\), i.e. \\(f_\\theta(s_{norm}^1) \\rightarrow x_{norm}\\) and \\(f_\\theta(s_{norm}^2) \\rightarrow x_{norm}\\). For anomaly instances during the test stage, we also extract two subsets, \\(s_{anom}^1\\) and \\(s_{anom}^2\\). If anomaly attributes are presented in \\(s_{anom}^1\\) or \\(s_{anom}^2\\), normal attributes of anomalies can not be reconstructed well, as the internal relations of anomalies deviate from those of normal instances. There is a case where anomalous attributes are not in two subsets, but the model fails to restore them using normal attribute subsets, still producing large reconstruction errors. Noisy attributes are unavoidable in real-world scenarios; however, neither normal nor anomaly attributes reconstruct noise well, which barely affects the distinction between normal and abnormal samples. Accordingly, a criterion based on the reconstruction error for anomaly detection can be built by disentangling CorrSets from normal samples.\nWe present the framework of our method in Figure 3. Given one sample for illustrating our procedure, the encoder takes it as input and extracts its latent features. Then, a two-head self-attention module is utilized to disentangle tabular data for extracting features of two attribute subsets. Afterward, a decoder maps these features back to the input space for independently reconstructing the original data. Our model is trained in an end-to-end fashion and optimized by a disentangling loss and a reconstruction loss. During inference, the sum of the two subsets' reconstruction errors serves as the anomaly score for each test sample.\n3.3 Our Proposed Method\nTo capture the correlation inside normal samples, we propose to disentangle CorrSets from them. There are two main components in our method: disentangling and learning correlation. Practicality, we leverage a two-head self-attention for implicit disentanglement, and a reconstruction task guarantees the correlation between two subsets.\nDisentangling Aiming to extract two distinct attribute subsets from normal samples, we utilize a two-head self-attention module to implicitly disentangle tabular data in latent space. The attention map describes attributes that the network focuses on, and the greater the attention weight, the more interested the network is. Learning two independent attention maps allows two heads of the attention module to extract latent features of two distinct attribute subsets in tabular data, thus enabling implicit disentanglement. We respectively demonstrate the importance of extracting two subsets and disentangling with two-head attention for capturing effective correlations in Section 4.6.\nWe apply the attention module to latent features extracted by the encoder to disentangle tabular data. To successfully adopt the attention module, we add an additional dimension to tabular data, that is, \\(x_i \\in \\mathbb{R}^{M\\times1}\\), where M still denotes the number of attributes, and each attribute owns 1 feature channel. The encoder \\(f_E\\) maps \\(x_i\\) to latent features \\(z_i \\in \\mathbb{R}^{M\\times C} = f_E(x_i)\\), where C refers to the channel number of each attribute. With the attention, \\(z_i\\) yields queries, keys, and values, symbolized respectively as \\(q^h, k^h, v^h \\in \\mathbb{R}^{M\\times C}\\), for each head \\(h \\in {1,2}\\). The computation of the two attention maps is formalized as follows:\n\\(w^h_i = Softmax(\\frac{q^h(k^h)^T}{\\sqrt{C}}), h \\in {1,2}\\),\nwhere the size of \\(w^h\\) is \\(M \\times M\\). To accomplish the objective of disentangling two distinct attribute subsets from each training instance, it is imperative to learn two independent attention maps. Our strategy is to diminish the similarity between them. To this end, a cosine similarity loss is employed to yield a disentangling loss:\n\\(L_d = \\frac{1}{N} \\sum_{i=1}^N \\frac{w_i^1}{\\|w_i^1\\|_2} \\cdot \\frac{w_i^2}{\\|w_i^2\\|_2}\\,\nwhere \\(\\|\\cdot\\|_2\\) is the \\(L_2\\) norm. As \\(L_d\\) approaches 0, the two attention maps are orthogonal, thus allowing two heads to focus on distinct attribute subsets. Drawing from empirical observations, due to the absence of two attention maps with inverse relationships in single samples, the absolute value is not employed in the computation of \\(L_d\\). Consequently, this restriction confines the range of \\(L_d\\) to interval [0, 1], despite the range of the cosine similarity loss spanning [-1,1]. We validate the significance of \\(L_d\\) for learning the intrinsic correlation of normal data in Section 4.6.\nLearning Correlation With disentangling, we extract two distinct attribute subsets from normal tabular data. In order to capture the intrinsic correlation, it is essential to ensure that the disentangled subsets are correlated. Therefore, we perform a reconstruction task to individually restore the input sample from the features of each subset. Such a strategy guides the network in extracting CorrSets, thus progressing the learning of the intrinsic correlation of normal instances.\nAs a result of directing two attention heads to focus on distinct attribute subsets, the latent features of those can be obtained by:\n\\(z^h = w^h v^h, h \\in {1,2}\\),\nwhere \\(v^h \\in \\mathbb{R}^{M\\times C}\\). Then a decoder \\(f_D\\) takes \\({z^1,z^2}\\) as inputs to produce reconstruction outputs:\n\\(\\hat{x_i}^h = f_D(z^h), h \\in {1,2}\\).\nThe mean squared error (MSE) loss function is applied for the reconstruction loss:\n\\(L_r = \\frac{1}{N} \\sum_{i=1}^N (x_i - \\hat{x_i}^h)^2\\).\nThe overall loss function for training our method is defined as follows:\n\\(L_{overall} = L_d + L_r\\),\nwhere we equal the weight for \\(L_d\\) and \\(L_r\\) to avoid a possible bias to one loss. By training with \\(L_{overall}\\), the model is able to extract CorrSets, thereby learning the inherent correlation of normal data.\n3.4 Anomaly Score for Inference\nThe determination of a test sample as normal or abnormal is made by evaluating the reconstruction error. The cosine similarity between attention maps is not included in the anomaly score function. It is due to that the model is trained to extract two distinct subsets. Empirical evidence is presented in Figure 4. In the right part, the disentangling loss of anomalies varies almost identically to that of normal samples and eventually nears 0. As for the reconstruction error, the network aims to model the intrinsic correlation of normal class, thus test normal samples generally yield low errors. While anomalies consistently exhibit high reconstruction errors. Therefore, we define the anomaly score function \\(\\phi(x_i)\\) with the MSE function as below:\n\\(\\phi(x_i) = \\sum_{h=1,2} (x_i - \\hat{x_i}^h)^2\\).\nThe anomaly scores for normal data are expected to be as small as possible. For anomalies, whether or not the anomalous attributes are included in disentangled subsets, the model fails to reconstruct the complete input as the intrinsic correlation of the anomalies deviates from that of normal samples, resulting in high anomaly scores. The greater the anomaly score, the more likely that a sample is an anomaly."}, {"title": "Experiments", "content": "4.1 Datasets\nOur evaluation encompasses 20 tabular datasets, aligning with previous work (Yin et al. 2024). 12 of them are obtained from the Outlier Detection Datasets (ODDS) (Rayana 2016), while the remainder are derived from ADBench (Han et al. 2022), which cross various application scenarios, including healthcare, finance, and more. For specific dataset statistics, please see Supplementary D.\n4.2 Evaluation Metrics\nExperiments on tabular data are conducted by following previous work (Xu et al. 2023b; Yin et al. 2024). We randomly sample 50% of the normal samples as the training set, and the remaining normal samples with all anomaly samples are combined into the test set. Area Under the Precision-Recall Curve (AUC-PR) and Area Under the Receiver-Operating-Characteristic Curve (AUC-ROC) are selected as our evaluation criteria. These two metrics can objectively evaluate detection performance without making any assumption on the decision threshold. All reported results are averaged over three independent trials.\n4.3 Implementation Details\nIn our network architecture, a three-layer Multilayer Perceptron (MLP) with LeakyReLU activation function forms the encoder, and the decoder is symmetrically designed to the encoder. For the adaption of the attention module to tabular data, we add one additional dimension to most datasets and perform a patch-splitting preprocessing method for the rest to facilitate disentanglement, mostly derived from image scenarios. We consider the data structures of these datasets to be too complicated to affect disentangling. The preprocessing method is performed by splitting the data into three patches with the size of M/2 and treating each patch as an attribute to compose new data to simplify the data structure. We present a detailed description of the preprocessing method in Supplementary A. For datasets applied with the preprocessing method, we set epochs to 200 and the channel number C of latent features to 512 for efficient convergence, while epochs to 100 and C to 128 for the rest. Due to the large variation in the number of samples between datasets, ranging from 129 to 299,285, we use different batch sizes for different datasets. The parameters of the network are optimized by Adam with a uniform learning rate of le-4 for all datasets. See Supplementary B for more implementation details.\n4.4 Baseline Methods\nWe compare our method with eleven outstanding tabular anomaly detection methods. IForest (Liu, Ting, and Zhou 2008), LOF (Breunig et al. 2000), OCSVM (Sch\u00f6lkopf et al. 1999), and ECOD (Li et al. 2022) represent classical non-deep methods. The competing deep methods include DAGMM (Zong et al. 2018), DeepSVDD (Ruff et al. 2018), GOAD (Bergman and Hoshen 2020), NeuTraLAD (Qiu et al. 2021), ICL (Shenkar and Wolf 2022), SLAD (Xu et al. 2023b), and MCM (Yin et al. 2024). It is noted that the results of IForest, LOF, OCSVM, ECOD, and DAGMM are obtained from MCM. We implement DeepSVDD, GOAD, NeuTraLAD, ICL, and SLAD by using DeepOD (Xu et al. 2023a), an open-source Python library for deep learning-based anomaly detection. The implementation of MCM is based on their official open-source code.\n4.5 Anomaly Detection Performance\nTable 1 and 2 present the AUC-PR and AUC-ROC results of our method alongside the competing methods across 20 datasets, respectively. Despite the heterogeneity in the datasets, our method achieves the best overall performance on both two evaluation metrics, outperforming the second-best method by an average of 6.1% on AUC-PR and 2.1% on AUC-ROC. To prevent a few datasets from dominating the averaged results, we also present the mean rank for comparison. The best mean ranking out of 12 is obtained by"}, {"title": "Ablation Study", "content": "We select four datasets sourced from different application scenarios to conduct the ablation study: Thyroid (healthcare), Arrhythmia (healthcare), Glass (forensic), and Satellite (satellite image). The results are reported in Table 3.\nDisentangling two subsets is crucial for learning the intrinsic correlation of normal data: We design \"Variant 1\" that learns one attention map with a one-head self-attention module. Restoring original data from one subset makes the model extract attributes that can be used for reconstruction. However, the attention module might focus on each attribute to minimize reconstruction loss, causing the model to struggle with capturing the intrinsic correlation of normal samples. According to the experimental results, the detection performance of \"Variant 1\" is much lower than that of our method, validating the significance of disentangling two subsets for learning the correlation in normal samples.\nExtracting two subsets with two-head self-attention can capture better correlation than with one-head self-attention: \"Variant 2\" disentangles two subsets with a one-head self-attention module. We implement it by \\(w^2 = 1 - w^1\\), where \\(w^1\\) is produced by the attention module, and they are independent of each other. Evidently, \u201cVariant 2\u201d is significantly inferior to our method. It is due to that noise attributes might be contained in subsets according to \\(w^1\\), leading to less effective learning of the intrinsic correlation. In our method, disentangling with the two-head self-attention module helps reduce the effect of noise attributes on learning the correlation. Additionally, the performance gap between \"Variant 2\" and \"Variant 1\" further demonstrates the significance of disentangling two subsets.\nConstraining the two subsets to be distinct is necessary for learning the correlation within normal data: A substantial performance drop is observed in \u201cVariant 3\", since\""}, {"title": "Further Analysis", "content": "Analysis on Subset Number Figure 5 reports the detection performance of our method vs. different numbers of disentangled attribute subsets, where Thyroid, Arrhythmia, Glass, and Satellite are used as illustrative datasets. Evidently, as the subset number is increased, the performance drops slightly, partially as extracting more correlated subsets raises the difficulty of training convergence. Additionally, there is a case that multiple (> 2) subsets related to each other are not present in normal samples, which may lead to computational redundancy. This analysis indicates the rationale of our two subsets strategy. Visualization of attention maps for three subsets is shown in Supplementary C.\nRobustness to Anomaly Contamination In real-world anomaly detection, the training set may be contaminated, with a small percentage of anomalies included. By reference to (Yin et al. 2024), we successively set the anomaly contamination ratio to 0%, 1%, 3%, and 5% to analyze the robustness of our method to anomaly contamination. Selecting Breastw and Cardiotocography as typical datasets, analysis results are shown in Figure 6. It is observed that our method experiences minimal performance degradation as the anomaly contamination ratio increases, consistently yielding the best results in each case. Such empirical evidence justifies the outstanding robustness of our method to anomaly contamination.\nAnalysis on the Preprocessing Method Table 4 presents the detection performance of preprocessing data into various data shapes, where Satellite and Satimage-2 are selected as illustrative datasets. They are sourced from satellite image scenarios and preprocessed in our implementation. There are overlaps between split patches when the data shape is 2 \u00d7 3M/4 or 3 \u00d7 M/2. Evidently, there is a noticeable performance improvement when applying the preprocessing method, as a reduced number of attributes enables the model to disentangle data more easily, thereby facilitating correlation learning. According to the empirical results, we use the data shape of 3 \u00d7 M/2 in our implementation. We also test our method's robustness to randomly ordered attributes in original data when using the preprocessing method. Although there is a slight performance drop, the results are still comparable, which verifies that our method is robust to the randomly ordered attributes."}, {"title": "Conclusion", "content": "In this paper, we design a novel disentangling-based method to learn the correlation inside normal tabular data by extracting two distinct and correlated attribute subsets for anomaly detection. To our knowledge, this is the first work to successfully leverage disentanglement for tabular anomaly detection under the one-class classification setting. Extensive experiments conducted on 20 tabular datasets sourced from diverse application scenarios with AUC-PR and AUC-ROC metrics evidence that our method outperforms the existing best methods. However, it is imperative to note that our current design specifically caters to tabular data and, as such, does not directly apply to other data types, such as image or point cloud data. We intend to explore disentangling-based methods for non-tabular data types in future work."}, {"title": "Patch-splitting Preprocessing", "content": "The patch-splitting preprocessing is performed to facilitate disentanglement, as illustrated in Figure 7. In our implementation, we split the original data into three patches, each of size M/2, where M is the attribute number of the original data. These patches are then concatenated to form new data, with each patch treated as a new attribute of the preprocessed data. With such a procedure, the original data is transformed into new data with fewer attributes and more features of each attribute, allowing the model to disentangle data more easily."}, {"title": "Experimental Setup", "content": "We conduct experiments on a single NVIDIA GeForce RTX 3090 GPU on the Ubuntu 20.04.1 system. Our code is implemented based on PyTorch 1.13.0 framework with Python 3.9.17. Other critical package requirements include numpy 1.25.0, scikit-learn 1.2.2, scipy 1.10.1, and pandas 2.0.3."}, {"title": "Hyperparameters for Each Dataset", "content": "Learning rate, batch size, training epoch, channel number C of latent features, and preprocessed or not for different tabular datasets are presented in Table 5. Datasets marked with \u2713 are applied with the patch-splitting preprocessing method to produce data with the shape of 3 \u00d7 M/2. We set epochs to 200 and C to 512 for these datasets to ensure efficient convergence. The batch size of each dataset is set according to its number of training samples. The greater the training set, the larger the batch size."}, {"title": "Analysis on Epoch and C", "content": "Table 8 presents the results with various epochs and C, where Satellite and Satimage-2 are illustrative datasets. Evidently, using the larger epoch and C can obtain performance improvement, which validates the effectiveness of our hyperparemeter setting for epoch and C."}, {"title": "Analysis on Batch size", "content": "We report results with various batch sizes for each dataset in Table 7, which demonstrates the rationale of our batch size hyperparameter setting. Additionally, our method surpasses all competing methods when using a fixed batch size across all datasets. It is noted that our method performs better with a smaller batch size on the Fraud dataset. However, to use a uniform batch size for datasets with a large number of training samples, we set the batch size to 2,048 for the Fraud dataset in our implementation."}, {"title": "Attention Maps for Three Subsets", "content": "We visualize the average attention maps on training data of the Thyroid dataset when three subsets are disentangled from normal data in Figure 8. Nearly uniform weights are observed in the third attention map, i.e., the third attention head focuses on complete data rather than on partial attributes, resulting in less effective correlation learning. This empirical evidence supports the rationale of our two subsets strategy."}]}