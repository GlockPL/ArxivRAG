{"title": "Boosting Flow-based Generative Super-Resolution Models via Learned Prior", "authors": ["Li-Yuan Tsao", "Yi-Chen Lo", "Chia-Che Chang", "Hao-Wei Chen", "Roy Tseng", "Chien Feng", "Chun-Yi Lee"], "abstract": "Flow-based super-resolution (SR) models have demonstrated astonishing capabilities in generating high-quality images. However, these methods encounter several challenges during image generation, such as grid artifacts, exploding inverses, and suboptimal results due to a fixed sampling temperature. To overcome these issues, this work introduces a conditional learned prior to the inference phase of a flow-based SR model. This prior is a latent code predicted by our proposed latent module conditioned on the low-resolution image, which is then transformed by the flow model into an SR image. Our framework is designed to seamlessly integrate with any contemporary flow-based SR model without modifying its architecture or pre-trained weights. We evaluate the effectiveness of our proposed framework through extensive experiments and ablation analyses. The proposed framework successfully addresses all the inherent issues in flow-based SR models and enhances their performance in various SR scenarios.", "sections": [{"title": "1. Introduction", "content": "Image super-resolution (SR) aims to reconstruct a high-resolution (HR) image given its low-resolution (LR) counterpart. Typically, the effectiveness of SR methods is evaluated based on fidelity and perceptual quality of the generated SR images, with commonly used metrics such as Peak Signal-to-Noise Ratio (PSNR), Structural Similarity (SSIM) for the former, and Learned Perceptual Image Patch Similarity (LPIPS) [78] for the latter. However, optimizing both metrics simultaneously is fundamentally challenging due to the inherent perception-distortion trade-off [6] in SR tasks. As a result, existing SR methods are broadly classified into two categories: fidelity-oriented SR, which prioritizes pixel-wise reconstruction accuracy, and generative SR, which focuses on enhanced visual quality.\nThe recent emergence of Flow-based SR [34, 45, 50, 51, 67, 76] bridges this divide, as flow models possess the capability to control the diversity of image content during inference time by adjusting the sampling temperature (i.e., the standard deviation of a Gaussian distribution). As a result, a single flow-based SR model (or simply \"flow model\" hereafter) can produce images that either prioritize high fidelity or exhibit improved perceptual quality. This unique feature provides flow-based models with the potential to excel in each category of SR methods, making them a promising framework for SR tasks.\nDespite their flexibility, flow-based SR methods encounter several challenges in the image generation process. These include (1) grid artifacts in the generated images, (2) the exploding inverse issue, and (3) suboptimal results stemming from the use of a fixed sampling temperature. \"Grid artifacts\" stands for the discontinuities in textures within an image [47].  Another critical aspect is the \"exploding inverse\" [4, 26] in invertible neural networks, which refers to the occurrence of infinite values in the inverse process. This phenomenon leads to the appearance of noisy patches within images. Moreover, despite using a fixed sampling temperature during evaluation is a common practice in flow-based SR methods, this approach might not always be suitable, as the ideal temperature settings could vary across different regions.  In contrast, high-frequency areas (e.g., the detailed fur of a squirrel) require lower temperatures to maintain consistent contents.\nWhile the optimal temperature for each area can be identified through an exhaustive search to boost performance, this approach may be highly cost-ineffective and impractical in real-world scenarios. As a result, the efficacy of flow models could be constrained by the suboptimal temperature setting. Based on these challenges, flow-based SR methods still hold the potential for further enhancement.\nIn light of the aforementioned issues, a key element to addressing these problems of flow models could be a learned prior. This learned prior should hold the following properties. First, it captures the correlation between image patches to alleviate the discontinuities. Second, this prior should be prevented from being an out-of-distribution input to the flow model, which leads to an exploding inverse [4, 26]. Third, to eliminate the need to fine-tune a sampling temperature, this prior should be directly generated by a model instead of sampling from a Gaussian prior.  In this approach, the flow model receives the learned prior as input, replacing the conventional method which employs a randomly sampled latent code.\nTo achieve this, this study proposes a framework that introduces a latent module as the conditional learned prior. The latent module is designed to directly estimate a learned latent code in a single-pass for flow model inference. This design philosophy aligns well with the principle that real-world SR applications prefer fast inference and consistent predictions [2, 9]. Specifically, the proposed framework consists of two main components: a latent module and a flow model, with the latter being any contemporary flow-based SR model. The latent module is responsible for predicting a latent code conditioned on the LR image, which is then transformed by the flow model into an SR image.\nIn this work, the proposed latent module is integrated with two flow-based SR models, including the arbitrary-scale SR framework LINF [76] and the fixed-scale framework SRFlow [50], to demonstrate its effectiveness, generalizability, and flexibility with extensive experiments. The contribution of this study can be summarized as follows:\n\u2022 This study reveals three inherent deficiencies in flow-based SR methods, which are key issues that require enhancements to fully unleash the potential of flow models.\n\u2022 We introduce a conditional learned prior to the inference phase of a flow-based SR model, which effectively addresses the inherent issues of flow models without modifying their architecture or pre-trained weights.\n\u2022 The proposed latent module is highly flexible in terms of network architecture design, which allows the adoption of any commonly used backbone. Even a lightweight module could introduce noticeable improvements.\n\u2022 Our proposed framework generalizes to both fixed-scale and arbitrary-scale flow-based SR frameworks without requiring customized components, which also leads to advancements at out-of-training-distribution scales."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Image Super-Resolution", "content": "Contemporary deep learning-based SR methods [17, 24, 33, 39, 62, 80] primarily fall into two categories: fidelity-oriented SR and generative SR. Fidelity-oriented SR methods [14, 42, 44, 48, 70, 81, 82] aim to promote pixel-wise reconstruction accuracy and are commonly trained with L1 or L2 loss. Despite achieving high fidelity, these methods often produce blurry results. To overcome this and synthesize realistic HR images, various generative SR techniques have been proposed, including GAN-based [40, 58, 59, 71, 72, 79], AR-based [23], flow-based [34, 45, 50, 51, 67], and diffusion-based [43, 60] methods. Although these works effectively generate diverse and visually plausible HR images, they are limited to super-resolving images at a predefined upsampling scale, restricting their real-world applicability. Recently, various arbitrary-scale SR methods [10, 12, 15, 27, 41, 74-76] have emerged. These approaches are capable of producing high-fidelity images even at significantly large upsampling scales (e.g., 30\u00d7). Among these methods, LINF [76] pioneers the field of arbitrary-scale generative SR, which is able to generate realistic images across continuous scales."}, {"title": "2.2. Flow-based Super-Resolution", "content": "Flow-based SR methods utilize normalizing flows to establish a bijective mapping between the conditional distribution of HR images and a prior distribution. This approach addresses the ill-posed nature of SR tasks by modeling the entire HR image space. Besides excelling in general SR tasks [34, 45, 50, 51, 67, 76], flow-based SR methods find applications in a variety of specialized SR tasks. These include blind image SR [46], remote sensing image [73], Magnetic Resonance Imaging [37], Magnetic Resonance Spectroscopic Imaging [18], and scientific data SR [61]."}, {"title": "2.3. Learned Prior", "content": "Modern deep generative models [7] typically learn mappings between data and latent variables (i.e., latent codes) based on Gaussian prior, either explicitly such as variational autoencoders (VAEs) [35], normalizing flows [38], diffusion models [25, 63-66] or implicitly such as generative adversarial networks (GANs) [21, 32]. Regardless of their distinct learning formulations, they generally adopt a fixed Gaussian prior for sampling during inference. Recently, improved VAEs [11, 13, 16, 19, 20, 22, 36, 54-56, 69] have highlighted more expressive priors for better lower bound and sample generation. These advancements suggest that learning priors using neural networks can be used for sampling since the decoder network was effectively trained on latent codes from the learned posterior. Likewise, we propose to learn a latent module as a form of learned prior over latent codes inverted from training data by conditional normalizing flow, to remedy the train-test gap as a general framework to boost flow-based super-resolution quality."}, {"title": "3. Preliminaries", "content": ""}, {"title": "3.1. Fixed-scale Flow-based SR", "content": "Given an LR image $x \\in \\mathbb{R}^{H \\times W \\times 3}$ and an HR image $y \\in \\mathbb{R}^{sH \\times sW \\times 3}$ with a predefined scaling factor $s$, fixed-scale flow-based SR models [34, 45, 50, 51] aim to capture the entire conditional distribution $p_{y|x}(y|x)$. Specifically, they learn a bijective mapping between a conditional distribution $p_{y|x}(y|x)$ and a prior distribution $p_z(z)$, where $z \\in \\mathbb{R}^{sH \\times sW \\times 3}$ is typically a standard normal distribution $\\mathcal{N}(0, I)$. By utilizing an invertible neural network with $k$ invertible layers, such transformation is given by:\n$$z = f_\\theta(y; x) = f_k \\circ \\dots \\circ f_1(y; x),$$\n$$y = f_\\theta^{-1}(z; x) = f_1^{-1} \\circ \\dots \\circ f_k^{-1}(z; x).$$\nAdditionally, according to the change of variable theorem, the probability of an HR-LR image pair $(y, x)$ is defined as:\n$$p_{y|x}(y|x, \\theta) = p_z(f_\\theta(y; x)) \\cdot \\left| \\det \\frac{\\partial f_\\theta(y; x)}{\\partial y} \\right|.$$\nDuring training, fixed-scale flow-based SR models can be optimized through negative log likelihood (NLL) loss with a large set of HR-LR training pairs $\\{(Y_i, X_i)\\}_{i=1}^{N}$."}, {"title": "3.2. Arbitrary-scale Flow-based SR", "content": "Despite successfully tackling the ill-posed nature of SR task by modeling the HR image space, fixed-scale flow-based SR models are only able to super-resolve images with a predefined scaling factor (e.g., 4\u00d7), limiting their practicality. To address this issue, the arbitrary-scale flow-based SR framework \"Local Implicit Normalizing Flow\" (LINF) [76] shifts the learning target from the entire HR image to local patches. During inference, it generates local patches at corresponding coordinates independently, and then combines these patches to form the final image.\nSpecifically, given an LR image $x \\in \\mathbb{R}^{H \\times W \\times 3}$, the coordinate of a local patch $c_{i,j} \\in \\mathbb{R}^2$, a scaling factor $s$, and the corresponding HR patch $Y_{i,j} \\in \\mathbb{R}^{n \\times n \\times 3}$, where $n$ is typically set to 3, and $i, j$ is the index of an image patch. The goal of LINF [76] is to learn a bijective mapping between a conditional distribution $p_{y|x}(Y_{i,j}|x, c_{i,j},s)$ and a latent distribution $p_z(z) = \\mathcal{N}(0, I)$, where $z \\in [\\mathbb{R}^{n \\times n \\times 3}$. Similar to fixed-scale framework, the probability is given by:\n$$p_{Y|X}(Y_{i,j}|X, c_{i,j}, s, \\theta) = p_z(f_\\theta(Y_{i,j}; X, c_{i,j}, s)) \\cdot \\left| \\det \\frac{\\partial f_\\theta(Y_{i, j}; T, c_{i,j}, s)}{\\partial Y_{i, j}} \\right|.$$\nIn practice, the HR patch $Y_{i,j}$ is replaced by the residual map $m_{i,j} = Y_{i,j} - \\mathcal{X}$, where $\\mathcal{X}$ represents the bilinear-upsampled LR image. During training, LINF [76] utilizes the Negative Log-Likelihood (NLL) loss along with pixel-wise L1 loss and VGG perceptual loss [30] for additional fine-tuning across various metrics."}, {"title": "4. Methodology", "content": "In this section, we begin by defining the formulation of the proposed method, followed by a detailed description of the proposed framework and an elaboration on the design of the objective function."}, {"title": "4.1. Problem Formulation", "content": "Given an LR image $x \\in \\mathbb{R}^{H \\times W \\times 3}$ and a flow model $f_\\theta$, the objective of this work is to derive a latent code $z^* \\in \\mathbb{R}^{sH \\times sW \\times 3}$, given by: $y = f_\\theta^{-1}(z^*;x)$, where $y \\in \\mathbb{R}^{sH \\times sW \\times 3}$ represents the ground truth HR image, which is not available during inference, and $s$ represents a scaling factor. Successfully identifying $z^*$ enables the precise reconstruction of the corresponding HR image $y$. This process is facilitated by the invertible nature of normalizing flow, which guarantees the existence of $z^*$. To realize this objective, we utilize a latent module $G$ designed to generate a latent code $\\hat{z}$ in a single-pass during inference, which aims to approximate $z^*$ as closely as possible, expressed as:\n$$z^* \\approx \\hat{z} = G(\\cdot).$$\nSimilarly, for the arbitrary-scale SR framework, we aim to identify latent codes $\\hat{z}_{i,j}$ such that $\\hat{z}_{i,j} = z_{i,j}$ for all $i, j$ within an image, formulated as:\n$$z_{i,j} \\approx \\hat{z}_{i,j} = G(\\cdot), \\forall i, j.$$\nAfter deriving $\\hat{z}$, the flow model $f_\\theta$ transforms $\\hat{z}$ into an SR image $\\hat{y}$, given by: $\\hat{y} = f_\\theta^{-1}(\\hat{z}; \\cdot)$. Note that only the latent module undergoes training, the pre-trained flow model $f_\\theta$ remains frozen during both training and inference phases."}, {"title": "4.2. Framework Overview", "content": "Fig. 4 illustrates an overview of our proposed framework, which aims to leverage a conditional learned prior to address the inherent issues in flow-based SR models. Specifically, it consists of a proposed latent module and a flow model, with the latter being any contemporary flow-based SR model. The latent module processes signals from the LR image to produce a learned prior, which is then transformed by the flow model into the SR images. In this work, we integrate our framework with two existing flow models, including LINF [76], which represents arbitrary-scale SR framework, and SRFlow [50] for fixed-scale SR method."}, {"title": "4.3. Latent Module", "content": "Overview. The latent module is designed to predict the conditional learned prior by utilizing the LR conditional signals, where this prior is a latent code that holds several properties capable of addressing the inherent issues of flow models, as mentioned in Section 1. To achieve this, the latent module is designed to: (1) fuse information across patches to mitigate discontinuities in image content (i.e., grid artifacts), (2) leverage LR conditional signals to avoid out-of-distribution predictions that lead to an exploding inverse, and (3) directly output a learned prior without random sampling. As a result, this work introduces a latent module that extracts features of an LR image from both image space and latent space, and processes these features with a deep neural network to derive the conditional learned prior. Specifically, the architecture of the latent module comprises two feature encoders and a latent generator. One encoder processes the LR image to extract image space features, while the other works on the \u201cinitial prior\" to capture latent space signals, where the initial prior is the latent code corresponding to the upsampled LR image. Then, the latent generator utilizes these features to produce the learned latent code. The introduction of an initial prior provides a promising initialization in the latent space for seeking z*, thus easing the prior generation process. A detailed analysis of this effect is presented in Section 5.3.\nDesign of the Latent Module. Regarding the design of the latent module, the feature encoders adopt a five-layer dense block [28] architecture but maintain independent weights to process inputs from different spaces. On the other hand, the architecture of the latent generator is highly flexible, which allows the incorporation of any commonly used module. In this work, UNet [57] is primarily selected as the latent generator due to its efficiency and capabilities in capturing multi-level features, which aligns well with our objective. In addition, Section 5.3 provides a detailed analysis of alternative architectures of the latent generator, including EDSR-baseline [48] and Swin Transformer [49]."}, {"title": "4.4. Objective Function", "content": "To generate a latent code $\\hat{z}$ that approximates the optimal latent code $z^*$, our framework employs an objective function that aims at boosting both the accuracy in latent space and the perceptual quality of generated images. For the learning in latent space, we define a loss function $\\mathcal{L}_{latent}$ to minimize the L1 distance between $\\hat{z}$ and $z^*$, formulated as:\n$$\\mathcal{L}_{latent} = \\frac{1}{N} \\sum_{i=1}^N || \\hat{z}_i - z_i^* ||_1,$$\nwhere $z^*$ is obtained by transforming the HR image into the corresponding latent code with a flow model, which is available during training. In addition, we adopt the VGG perceptual loss [30] $\\mathcal{L}_{percep}$ in the image space to produce enhanced SR results. This objective guides our latent module to generate a latent code, which corresponds to an image that visually resembles the HR image. Specifically, it calculates the L1 distance between features extracted by the SR image and the HR image, with a pre-trained VGG19 [31] network $\\Psi_{per}$, which can be expressed as:\n$$\\mathcal{L}_{percep} = \\frac{1}{N} \\sum_{i=1}^N || \\Psi_{per} (y_i) - \\Psi_{per} (\\hat{y}_i) ||_1.$$\nAs a result, the overall objective can be expressed as:\n$$\\mathcal{L}_{total} = \\mathcal{L}_{percep} + \\lambda \\mathcal{L}_{latent}.$$\nIn practice, $\\lambda$ is set to 0 when integrating our framework with LINF, and 0.1 when combined with SRFlow based on the following observations. First, we find the sole application of Eq. (6) results in an average prediction across all potential latent codes. This outcome resembles the \u201cregression-to-the-mean\u201d [8] effect observed with L1 regression loss in image space. Furthermore, while adopting the perceptual loss is sufficient when integrating our framework with LINF model, SRFlow experiences training instability under this scheme, where infinite values emerge occasionally. To address this phenomenon, we employ the latent space loss $\\mathcal{L}_{latent}$ as a regularization term, which effectively prevents the generation of out-of-distribution latent codes [26] that lead to an exploding inverse. Section 5.3 delivers an analysis of the impact of objective functions."}, {"title": "5. Experimental Results", "content": "In this section, we present the experimental results and ablation studies. The results demonstrate how the proposed framework addressing the inherent issues of flow-based SR methods, and the effects after integrating our proposed framework with LINF [76] and SRFlow [50]."}, {"title": "5.1. Experimental Setup", "content": "Arbitrary-scale Flow-based SR. When integrating our framework with the arbitrary-scale framework LINF [76], we utilize two variants of their 3\u00d73 patch-based models: RRDB-LINF and EDSR-baseline-LINF. The former employs an RRDB [72] as the image encoder, whereas the latter uses an EDSR-baseline [48] backbone. For RRDB-LINF, we adopt their released model. In the case of EDSR-baseline-LINF, we reproduced the baseline model using their codebase due to the absence of a pre-trained model. Upon incorporating our proposed \"Learned Prior\" (LP) into LINF, we refer to the enhanced versions as \"LINF-LP\", with two configurations: \u201cEDSR-baseline-LINF-LP\u201d and \"RRDB-LINF-LP\u201d. To train LINF-LP, we set the LR image size to 96\u00d796, and crop the corresponding 96s \u00d7 96s HR image patch as ground truth, where s denotes a continuous upsampling scale $s \\in \\mathcal{U}(1, 4)$. LINF-LP is trained over 1,000 epochs with a batch size of 16. The initial learning rate is 1e-4, which is halved at [200, 400, 600, 800] epochs when using UNet and EDSR-baseline as the latent generator, and at [500, 850, 900, 950] epochs for Swin-T.\nFixed-scale Flow-based SR. For the fixed-scale SR method SRFlow [50], we also implement the enhanced version \"SRFlow-LP\". The experiments of SRFLow-LP are conducted on the DIV2K 4x SR task, and we integrate our proposed framework with their released pre-trained model. We train SRFlow-LP over five epochs, using a batch size of 12. The HR image size is set to 160\u00d7160, and the initial learning rate is 1e-4, which is halved after each epoch.\nDatasets. We use the DIV2K [1] dataset for training EDSR-baseline-LINF-LP, and the joint of DIV2K and Flickr2K [68] to train RRDB-LINF-LP and SRFlow-LP. These two datasets consist of 800 and 2,650 images, respectively. The evaluation is conducted on the DIV2K validation set. For the arbitrary-scale framework LINF-LP, we further test on several SR benchmark datasets, including Set5 [5], Set14 [77], B100 [52], and Urban100 [29].\nNetwork Details. For our proposed latent module, we utilize a three-layer deep UNet [57] as the latent generator. It starts with an initial feature dimension of 64, comprising both initial prior and LR image features, each having a dimension of 32. The feature dimension is doubled with each downsample operation and halved with each upsample operation. For the ablation study in Section 5.3, the feature dimensions of EDSR-baseline [48] and Swin Transformer (Swin-T) [49] are 64 and 192, respectively.\nEvaluation Metrics. For evaluation, we select several commonly used metrics in SR tasks. These include PSNR and SSIM for fidelity measurements, and LPIPS [78] for assessing perceptual quality. In addition, we employ the LR-PSNR metric [3], which calculates the PSNR between the bicubic downsampled SR image and the original LR image."}, {"title": "5.2. Experimental Results", "content": ""}, {"title": "5.2.1 Flow-based Generative Super-Resolution", "content": "We compare the performance of LINF-LP and SRFlow-LP with the baselines LINF [76] and SRFlow [50]. The results in Table 1 reveal the benefits of our framework in two aspects: effectiveness and generalizability. Firstly, both SRFlow-LP and LINF-LP demonstrate significant improvements in fidelity- and perception-oriented metrics. For instance, SRFlow-LP gains an improvement of 0.42 dB in PSNR and 0.012 in LPIPS, and LINF-LP achieves an improvement of 0.62 dB and 0.67 dB in PSNR, along with gains of 0.01 and 0.007 in LPIPS when using EDSR-baseline and RRDB backbones, respectively. The simultaneous enhancements in both metrics demonstrate the capability of our framework to further push the boundary of the perception-distortion trade-off [6]. Secondly, the improvements observed in both SRFlow-LP and LINF-LP exhibit the generalizability of our approach, which is capable of extending to both fixed-scale and arbitrary-scale frameworks without the need for customized components. This finding validates the capability of the proposed framework across various SR scenarios."}, {"title": "5.2.2 Arbitrary-scale Flow-based SR", "content": "Quantitative Results. For the arbitrary-scale SR framework LINF [76], we compare the performance of EDSR-baseline-LINF-LP and RRDB-LINF-LP with their corresponding baselines. We evaluate the performance at both in-distribution (2x, 3x, 4x) and out-of-training-distribution (6x, 8x) upsampling scales on widely used SR benchmark datasets [5, 29, 52, 77]. Since our focus is flow-based generative models, we adopt LPIPS [78] for assessing the perceptual quality of generated images. In addition, LINF adopts a specific sampling temperature $\\tau$ for different scaling factors: $\\tau$ = 0.5 for 2x, 3x, 4x SR, $\\tau$ = 0.4 for 6\u00d7 SR, and $\\tau$ = 0.2 for 8\u00d7 SR. In contrast, our framework directly predicts a latent code from the LR image without the need to fine-tune this hyperparameter.\nTable 2 demonstrates the adaptability and the potential of our framework. (1) Both EDSR-baseline-LINF-LP and RRDB-LINF-LP achieve considerable improvements across in-distribution and out-of-training-distribution scales. This demonstrates the ability of our framework to adaptively predict a conditional learned prior for arbitrary upsampling scales, even at OOD scales. (2) RRDB-LINF-LP typically shows greater enhancement than EDSR-baseline-LINF-LP. For instance, RRDB-LINF-LP gains an improvement of 0.022 in LPIPS on Urban100 8\u00d7 SR, compared to a 0.009 enhancement by EDSR-baseline-LINF-LP. We attribute this to the stronger capability of the RRDB [72] backbone, which provides superior learning signals for the latent module. In light of this, we suggest the enhancements achieved by our framework are proportional to the capacity of the flow models, with more powerful models potentially yielding more pronounced improvements.\nMitigating Grid Artifacts. As illustrated in Fig. 5, grid artifacts are prominent in images produced by LINF [76]. This issue arises since LINF constructs an image by combining independently sampled patches. When utilizing a higher temperature $\\tau$ to generate images with diverse content, the magnitude of values sampled by adjacent patches could vary greatly, which leads to discontinuities in image content. However, LINF-LP effectively reduces the presence of grid artifacts. The key to this improvement lies in the learned prior predicted by our latent module, which efficiently captures global information from an LR image and can guide LINF to generate coherent content."}, {"title": "5.2.3 Fixed-scale Flow-based SR", "content": "The Occurrence of Exploding Inverses. This analysis examines the likelihood of SRFlow [50] and SRFlow-LP encountering exploding inverses and measures the quality of generated images with LPIPS and LR-PSNR. Table 3 demonstrates that SRFlow-LP effectively prevents the occurrence of exploding inverses. This enhancement could be attributed to the conditional learned prior predicted by our framework, which avoids out-of-distribution predictions that lead to subsequent exploding inverses [26]. A detailed analysis of this effect is presented in Section 5.3.\nFor SRFlow, the frequency of producing exploding inverses rises with increasing temperature. To elaborate, SRFlow achieves optimal LPIPS scores at $\\tau$ = 0.9, with a slight risk of encountering exploding inverses, which strikes a balance between perceptual quality and consistency. In addition, SRFlow shows distinct effects at $\\tau$ = 0.8 and $\\tau$ = 1.0. At a temperature $\\tau$ = 0.8, SRFlow delivers a higher LR-PSNR score and shows no exploding inverses, while the perceptual quality is compromised. At $\\tau$ = 1.0, the probability of encountering exploding inverses rises sharply. Also, excessively high temperatures could introduce image artifacts [76], thus affecting both LPIPS and LR-PSNR scores.\nQualitative Results. Fig. 6 presents a qualitative comparison between SRFlow-LP and SRFlow. The former can generate finer details such as lines and circles, while the latter struggles to render these even at a high-temperature setting of 0.9 for more diverse contents. This observation indicates the capabilities of our framework, which not only mitigates grid artifacts by integrating global information, as described in Section 5.2.2 but also excels in capturing intricate details, resulting in visually appealing effects."}, {"title": "5.3. Ablation Studies", "content": "This section presents ablation studies and in-depth discussions on the design of our latent generator, along with the effects of input selection and the objective function. In the second and third analyses, we adopt EDSR-baseline-LINF and EDSR-baseline-LINF-LP for comparison. For clarity, we denote them as \u201cLINF\u201d and \u201cLINF-LP\u201d, respectively.\nDesign of the Latent Generator. As described in Section 4.3, the architecture of our latent generator allows the use of commonly used backbones. In this analysis, we explore this flexibility by adopting EDSR-baseline [48] and Swin-T [49] as alternative latent generators. The results presented in Table 4 illustrate the effectiveness and efficiency of our proposed framework. Firstly, LINF-LP yields considerable improvements with all these backbones in both fidelity- and perceptual-oriented metrics compared to LINF. Moreover, even the lightweight backbone EDSR-baseline, with a model size of only 1.4M, significantly boosts the performance. Among these results, LINF-LP (UNet) delivers the best PSNR and LPIPS scores. This superior performance could be attributed to the multi-level architecture of UNet [57], which effectively incorporates both local and global features into the learned prior.\nInfluence of Different Inputs. We conduct analyses on the input to the proposed latent module by experimenting with various combinations, including (1) using the LR image only, (2) adopting the initial prior only, and (3) combining both, which is our final setting. During the analyses, we keep the total feature dimension at 64 in each experiment to ensure a fair comparison. The results in Table 5 reveal the importance of both image space and latent space information. Firstly, an initial prior is crucial to our proposed framework. By solely adopting an initial prior as input, LINF-LP boosts the performance at both in- and out-of-distribution scales. This suggests an initial prior provides a promising initialization in latent space and eases the prior generation process. Moreover, combining features from both image and latent spaces facilitates the best results, which infers that it is necessary to leverage both image and latent space features to enhance the quality of the learned prior. Lastly, relying solely on image space features is insufficient under our framework, as it is challenging to transform an LR image into a precise latent space signal with a single backbone. This setting leads to a decrease of 0.071 in LPIPS from the baseline on DIV2K 4\u00d7 SR task.\nImpact of the Loss Function. This analysis assesses the impact of using different loss functions. The experiment employs three configurations: (1) using $\\mathcal{L}_{latent}$ only, (2) adopting a combination of $\\mathcal{L}_{latent}$ and $\\mathcal{L}_{percep}$, and (3) exclusively adopting $\\mathcal{L}_{percep}$, which is our chosen approach for LINF-LP. Note that in this analysis, the weight of $\\mathcal{L}_{latent}$ is set to 0.1 when combined with $\\mathcal{L}_{percep}$. The results in Table 6 demonstrate that solely using the perceptual loss yields the best LPIPS results. In contrast, using only the latent space L1 loss leads to an average prediction [8] of all possible latent codes, resulting in an inferior LPIPS score. In addition, the dual application of $\\mathcal{L}_{latent}$ and $\\mathcal{L}_{percep}$ deteriorates LPIPS score by 0.014 at out-of-distribution (OOD) scales, yet slightly improves the in-distribution performance. This phenomenon suggests the $\\mathcal{L}_{latent}$ enables LINF-LP to better fit the training distribution, as it receives guidance directly from the latent space (i.e., HR ground truth latent codes) during training, making its predictions toward in-distribution outcomes. Given that the performance at OOD scales is crucial to an arbitrary-scale framework, we only utilize $\\mathcal{L}_{percep}$ to train LINF-LP. Based on the observation that the model trained with $\\mathcal{L}_{latent}$ tends to yield latent codes within the training distribution, we adopt $\\mathcal{L}_{latent}$ as a regularization term in SRFlow-LP. This approach aims to prevent SRFlow-LP from generating OOD predictions, which could lead to an exploding inverse as noted in [26]. Under this setting, as illustrated in Table 3, we successfully prevent the occurrence of exploding inverses without modifying the architecture [26] and the pre-trained weights of SRFlow ."}, {"title": "6. Conclusion", "content": "In this work, we identify several challenges in flow-based SR methods, including grid artifacts, exploding inverses, and suboptimal results due to a fixed sampling temperature. To tackle these issues, we introduce a learned prior, which is predicted by the proposed latent module, to the inference phase of flow-based SR models. This framework not only addresses the inherent issues in flow-based SR models but also enhances the performance of these models without modifying their original design or pre-trained weights. Our proposed framework is effective, flexible in design, and able to generalize to both fixed-scale and arbitrary-scale SR frameworks without requiring customized components."}]}