{"title": "Enabling High Data Throughput Reinforcement Learning on GPUs: A Domain Agnostic Framework for Data-Driven Scientific Research", "authors": ["Tian Lan", "Huan Wang", "Caiming Xiong", "Silvio Savarese"], "abstract": "We introduce WarpSci, a domain agnostic framework designed to overcome crucial system bottlenecks encountered in the application of reinforcement learning to intricate environments with vast datasets featuring high-dimensional observation or action spaces. Notably, our framework eliminates the need for data transfer between the CPU and GPU, enabling the concurrent execution of thousands of simulations on a single or multiple GPUs. This high data throughput architecture proves particularly advantageous for data-driven scientific research, where intricate environment models are commonly essential.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) stands out as a powerful algorithm for training AI agents, applicable in diverse domains such as strategy games(OpenAI, 2018; Vinyals et al., 2019), robotics (Gu et al., 2017; Ibarz et al., 2021), and large language models (Ouyang et al., 2022). Notably, there has been a recent surge in interest regarding the application of RL techniques in scientific research, encompassing diverse fields such as multi-agent\u00b9 modeling in economics, climatology, and epidemiology (Zheng et al., 2022; Trott et al., 2021; Zhang et al., 2022); signal processing in astrophysics (Nousiainen, J. et al., 2022; Yatawatta, 2023); and investigating reaction paths in chemistry (Lan and An, 2021; Yoon et al., 2021). However, numerous engineering and scientific challenges persist in the adoption of RL in scientific investigations. The performance of RL implementations can decelerate significantly when simulations become data-intensive, particularly in scenarios involving numerous agents or high-dimensional state or action spaces, resulting in experiments that span weeks. The comparatively low data throughput of RL further contributes to the emergence of non-stationary and strongly correlated data sequences, while the finite-horizon roll-out in RL introduces bias over the value function estimation (Mnih et al., 2016; Zhang et al., 2020; Lan and An, 2021). Regrettably, such complexity and challenges are commonplace in data-driven scientific modeling. For instance, in economic simulations, the construction of a realistic environment necessitates hundreds of agents and numerous actions (Zhang et al., 2022). Similarly, the study of catalytic reaction pathways involves navigating a chemical potential energy landscape that can easily exceed twenty dimensions with extreme noise (Lan and An, 2021). While distributed systems are employed to scale RL performance,\n1. An agent is an actor in an environment. An environment is an instance of a simulation and may include many agents with complex interactions. An agent is neither an environment nor a policy model."}, {"title": "2 Contribution", "content": "The primary objective of this Extended Abstract is to bring attention to the challenge of RL in scientific research arising from the data throughput, and introduce our comprehensive solution, WarpSci. WarpSci is a computational framework specifically designed to achieve massively high-throughput and domain-agnostic RL simulation in the context of data-driven scientific research. The framework builds upon the foundation of WarpDrive (Lan et al., 2022) which is accessible at https://github.com/salesforce/warp-drive.\nWarpSci performs the entire RL workflow on a single or multiple GPUs, utilizing a unified and in-place data store within GPUs for simulation roll-outs and training. This minimizes the data transfer between CPU and GPU or within GPU, reducing simulation and training time significantly. The framework also leverages GPU parallelization to concurrently run thousands of RL simulations, operating independently in the dedicated GPU blocks and concurrently producing exceptionally large batches of experience. WarpSci offers simple Python classes located on the CPU to streamline all relevant CPU-GPU communication and interactions essential for RL, and offer simple toolings for constructing custom RL environments connected to the CUDA back-end.\nThis high throughput yet cost-effective architecture proves particularly advantageous for data-driven scientific research, where enormous data consumption, complex agent in-teractions, and diverse environments are usually indispensable. More details of the design choice and the computational architecture are provided in Appendix B."}, {"title": "3 Examples", "content": "We present three examples: gym classic control (Brockman et al., 2016) for benchmarking, a multi-agent economic simulation (Trott et al., 2021), and generalizable catalytic reaction paths modeling (Lan and An, 2021; Lan et al., 2024). All experiments ran on a single Nvidia A100 GPU on the Google Cloud Platform. Due to space constraints, we provide a brief summary in this section, with more information in Appendix C.\nThroughput: WarpSci achieves significantly higher (at least 10 \u2013 100\u00d7) throughput than the distributed systems at low cost (a single A100 GPU). For example, 8.6M environment steps/second for 10K concurrent cartpole environments, 0.12M for 1K concurrent economic simulations and 0.95M for catalytic reaction modeling with 2K concurrent en-vironments 2. Scaling almost linearly to thousands of environments or agents, WarpSci demonstrates near-perfect parallelism. It can also train across multiple GPUs for further throughput scaling. Convergence: Our study indicates that training with an increased data throughput generated by concurrent environments achieves faster and more stable global convergence. Environments Agnostic: WarpSci offers tools to develop custom environments for diverse scientific research topics, and supports actor-critic algorithms for both discrete and continuous actions.\n2. In certain experiments, we employed a reduced level of concurrency to optimize the trainer's capacity and memory space."}, {"title": "Appendix A. Scalable Reinforcement Learning", "content": "Common scalable RL systems often employ a combination of distributed roll-out and trainer workers. Roll-out workers execute the environment to produce roll-outs, utilizing actions sampled from policy models on either roll-out workers or trainer workers. Typically, roll-out workers operate on CPU machines, occasionally utilizing GPU machines for richer environments. (Pretorius et al., 2021; Hoffman et al., 2020; Espeholt et al., 2018). Trainer workers gather roll-out data asynchronously from roll-out workers and iteratively optimize policies on either CPU or GPU machines. While such a distributed design is scalable, worker communication and data transfer cost is expensive and individual machine utilization can be poor. To improve performance, GPU and TPU-based RL frameworks exist (Tang et al., 2022; Hessel et al., 2021), but have focused on single-agent and domain-specific environments, e.g., for Atari (Dalton et al., 2020), or learning robotic control in 3-D rigid-body simulations (Freeman et al., 2021; Makoviychuk et al., 2021). Consequently, building efficient RL pipelines for simulations with intricate agent interactions, substantial data consumption, and diverse environments, as usually seen in scientific research, remains a challenging endeavor."}, {"title": "Appendix B. Details of Architecture", "content": "As shown in Fig. 1, WarpSci executes the entire RL workflow seamlessly on a single GPU or multiple GPUs, utilizing a unified data storage hosted within the GPU for simu-lation roll-outs, action inference, reset and training. This approach minimizes CPU-GPU data communication and eliminates the need for additional data transfer within the GPU, resulting in a substantial reduction in both simulation and training times. Furthermore, our framework achieves parallelization at low cost by concurrently running thousands of single-agent or multi-agent simulations, capitalizing on the inherent parallel processing ca-pabilities of GPUs. Each environment instance operates independently within a dedicated GPU block. Within each block, individual agents run on unique GPU threads, enabling interactions across threads. Each instance maintains a reference (not a copy) to the envi-ronment with local variations or random configurations, significantly reducing the storage overhead associated with the environment setup.\nWarpSci offers simple Python classes located on the CPU to streamline all relevant CPU-GPU communication and interactions essential for RL. These classes connect to the CUDA back-end and offer simple APIs for constructing high-level Python applications. Users only need supply the step function to finalize the custom environment definition. As a default environment composer, we employ Numba, a user friendly, just-in-time compiler for Python. Finally, our framework automatically loads and integrates the environment step into the environment-agnostic CUDA backend for the RL simulation."}, {"title": "Appendix C. Example Details", "content": "All experiments ran on a single Nvidia A100 GPU, a2-highgpu-1g, on the Google Cloud Platform.\nClassic Control. In the field of RL, classic control environments usually serve as fun-damental benchmarks to evaluate the performance of various RL algorithms and systems. These environments typically involve simple physics-based systems, yet their challenges lie in achieving stable and optimal control. Iconic examples, such as CartPole and Acrobot in gym environment (Brockman et al., 2016), offer controlled scenarios with well-defined dy-namics, making them ideal for benchmarking the throughput scalability and the learning capability of WarpSci.\nFig. 2(a) shows that WarpSci's performance in classic control environments scales lin-early to 10K of environments, yielding perfect parallelism. For example, WarpSci runs at 8.6 million environment steps per second with 10K Cartpole-v1 or Acrobot-v1 environ-ments. Fig. 2(b) and (c) displays the convergence speed of WarpSci as a function of the number of environment replicas running in parallel. The data reveal that, under consistent fixed hyperparameters, the simulations operating with an increased number of concurrent environments attain global convergence faster and more stably. Particularly, simulations with 10K Cartpole and Acrobot environment replicas reach the global optimum within 30 and 5 minutes respectively, while 10 environment replicas can barely exhibit satisfactory convergence in such a short period."}]}