{"title": "Enabling High Data Throughput Reinforcement Learning on GPUs: A Domain Agnostic Framework for Data-Driven Scientific Research", "authors": ["Tian Lan", "Huan Wang", "Caiming Xiong", "Silvio Savarese"], "abstract": "We introduce WarpSci, a domain agnostic framework designed to overcome crucial system bottlenecks encountered in the application of reinforcement learning to intricate environments with vast datasets featuring high-dimensional observation or action spaces. Notably, our framework eliminates the need for data transfer between the CPU and GPU, enabling the concurrent execution of thousands of simulations on a single or multiple GPUs. This high data throughput architecture proves particularly advantageous for data-driven scientific research, where intricate environment models are commonly essential.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) stands out as a powerful algorithm for training AI agents, applicable in diverse domains such as strategy games(OpenAI, 2018; Vinyals et al., 2019), robotics (Gu et al., 2017; Ibarz et al., 2021), and large language models (Ouyang et al., 2022). Notably, there has been a recent surge in interest regarding the application of RL techniques in scientific research, encompassing diverse fields such as multi-agent\u00b9 modeling in economics, climatology, and epidemiology (Zheng et al., 2022; Trott et al., 2021; Zhang et al., 2022); signal processing in astrophysics (Nousiainen, J. et al., 2022; Yatawatta, 2023); and investigating reaction paths in chemistry (Lan and An, 2021; Yoon et al., 2021). However, numerous engineering and scientific challenges persist in the adoption of RL in scientific investigations. The performance of RL implementations can decelerate significantly when simulations become data-intensive, particularly in scenarios involving numerous agents or high-dimensional state or action spaces, resulting in experiments that span weeks. The comparatively low data throughput of RL further contributes to the emergence of non-stationary and strongly correlated data sequences, while the finite-horizon roll-out in RL introduces bias over the value function estimation (Mnih et al., 2016; Zhang et al., 2020; Lan and An, 2021). Regrettably, such complexity and challenges are commonplace in data-driven scientific modeling. For instance, in economic simulations, the construction of a realistic environment necessitates hundreds of agents and numerous actions (Zhang et al., 2022). Similarly, the study of catalytic reaction pathways involves navigating a chemical potential energy landscape that can easily exceed twenty dimensions with extreme noise (Lan and An, 2021). While distributed systems are employed to scale RL performance,"}, {"title": "2 Contribution", "content": "The primary objective of this Extended Abstract is to bring attention to the challenge of RL in scientific research arising from the data throughput, and introduce our comprehensive solution, WarpSci. WarpSci is a computational framework specifically designed to achieve massively high-throughput and domain-agnostic RL simulation in the context of data-driven scientific research. The framework builds upon the foundation of WarpDrive (Lan et al., 2022) which is accessible at https://github.com/salesforce/warp-drive.\nWarpSci performs the entire RL workflow on a single or multiple GPUs, utilizing a unified and in-place data store within GPUs for simulation roll-outs and training. This minimizes the data transfer between CPU and GPU or within GPU, reducing simulation and training time significantly. The framework also leverages GPU parallelization to concurrently run thousands of RL simulations, operating independently in the dedicated GPU blocks and concurrently producing exceptionally large batches of experience. WarpSci offers simple Python classes located on the CPU to streamline all relevant CPU-GPU communication and interactions essential for RL, and offer simple toolings for constructing custom RL environments connected to the CUDA back-end.\nThis high throughput yet cost-effective architecture proves particularly advantageous for data-driven scientific research, where enormous data consumption, complex agent in-teractions, and diverse environments are usually indispensable. More details of the design choice and the computational architecture are provided in Appendix B."}, {"title": "3 Examples", "content": "We present three examples: gym classic control (Brockman et al., 2016) for benchmarking, a multi-agent economic simulation (Trott et al., 2021), and generalizable catalytic reaction paths modeling (Lan and An, 2021; Lan et al., 2024). All experiments ran on a single Nvidia A100 GPU on the Google Cloud Platform. Due to space constraints, we provide a brief summary in this section, with more information in Appendix C.\nThroughput: WarpSci achieves significantly higher (at least 10 \u2013 100\u00d7) throughput than the distributed systems at low cost (a single A100 GPU). For example, 8.6M envi-ronment steps/second for 10K concurrent cartpole environments, 0.12M for 1K concurrent economic simulations and 0.95M for catalytic reaction modeling with 2K concurrent en-vironments \u00b2. Scaling almost linearly to thousands of environments or agents, WarpSci demonstrates near-perfect parallelism. It can also train across multiple GPUs for further throughput scaling. Convergence: Our study indicates that training with an increased data throughput generated by concurrent environments achieves faster and more stable global convergence. Environments Agnostic: WarpSci offers tools to develop custom environments for diverse scientific research topics, and supports actor-critic algorithms for both discrete and continuous actions."}, {"title": "Appendix A. Scalable Reinforcement Learning", "content": "Common scalable RL systems often employ a combination of distributed roll-out and trainer workers. Roll-out workers execute the environment to produce roll-outs, utilizing actions sampled from policy models on either roll-out workers or trainer workers. Typically, roll-out workers operate on CPU machines, occasionally utilizing GPU machines for richer environments. (Pretorius et al., 2021; Hoffman et al., 2020; Espeholt et al., 2018). Trainer workers gather roll-out data asynchronously from roll-out workers and iteratively optimize policies on either CPU or GPU machines. While such a distributed design is scalable, worker communication and data transfer cost is expensive and individual machine utilization can be poor. To improve performance, GPU and TPU-based RL frameworks exist (Tang et al., 2022; Hessel et al., 2021), but have focused on single-agent and domain-specific environments, e.g., for Atari (Dalton et al., 2020), or learning robotic control in 3-D rigid-body simulations (Freeman et al., 2021; Makoviychuk et al., 2021). Consequently, building efficient RL pipelines for simulations with intricate agent interactions, substantial data consumption, and diverse environments, as usually seen in scientific research, remains a challenging endeavor."}, {"title": "Appendix B. Details of Architecture", "content": "Computations within this framework are orga-nized into GPU blocks, each comprising multiple threads to facilitate concurrent environment roll-outs. Each thread is responsible for operating an agent that samples actions and computes rewards. These blocks have access to the global GPU memory, which houses the RL environment (depicted as a 3-D grid in a green-bordered box) with local variations, and deep policy models. Additionally, they store in-place roll-out data for training purposes. The dashed brown boxes represent references (not copies) of the policy model objects and data placeholders managed by blocks and hosted in the global memory. Users have the flexibility to compose and upload their custom environment setups to finalize the environment construction."}, {"title": "Appendix C. Example Details", "content": "All experiments ran on a single Nvidia A100 GPU, a2-highgpu-1g, on the Google Cloud Platform.\nClassic Control. In the field of RL, classic control environments usually serve as fun-damental benchmarks to evaluate the performance of various RL algorithms and systems. These environments typically involve simple physics-based systems, yet their challenges lie in achieving stable and optimal control. Iconic examples, such as CartPole and Acrobot in gym environment (Brockman et al., 2016), offer controlled scenarios with well-defined dy-namics, making them ideal for benchmarking the throughput scalability and the learning capability of WarpSci.\nFig. 2(a) shows that WarpSci's performance in classic control environments scales lin-early to 10K of environments, yielding perfect parallelism. For example, WarpSci runs at 8.6 million environment steps per second with 10K Cartpole-v1 or Acrobot-v1 environ-ments. Fig. 2(b) and (c) displays the convergence speed of WarpSci as a function of the number of environment replicas running in parallel. The data reveal that, under consistent fixed hyperparameters, the simulations operating with an increased number of concurrent environments attain global convergence faster and more stably. Particularly, simulations with 10K Cartpole and Acrobot environment replicas reach the global optimum within 30 and 5 minutes respectively, while 10 environment replicas can barely exhibit satisfactory convergence in such a short period."}, {"title": "Multi-Agent Economics", "content": "We demonstrate the scalability of WarpSci to more intricate environments through its evaluation in a COVID-19 simulation. This simulation, grounded in real-world data, models the interplay between health and economic dynamics during the COVID-19 pandemic. Notably, the simulation step is significantly more complex compared to the gym classic control problems, consuming a larger fraction of each iteration's runtime.\nThe simulation involves 52 agents, with 51 representing governors for each U.S. state and Washington D.C., and an additional agent for the federal government of the USA. This constitutes a complex two-level multi-agent environment, where state agents determine the stringency level of the pandemic response, and the federal government provides subsidies to eligible individuals. The actions of each agent influence health and economic outcomes, such as deaths, unemployment, and GDP. Moreover, the federal government's actions can alter the health-economic trade-off and optimization objective for the U.S. states, rendering it a complex and dynamic two-level RL problem. Interested readers seeking additional scientific background and technical details are encouraged to refer to Trott et al. (2021); Zheng et al. (2022).\nFor this study, WarpSci achieves 24 times higher throughput with 60 environment repli-cas, compared to a 16 CPU node, n1-standard-16, on the Google Cloud Platform. Across different timing categories as shown in Fig. 3, the performance gains comprise a 24 times speed-up during the environment roll-out, a zero data transfer time, and a 30 times speed-up for training the policy models. Moreover, WarpSci can scale almost linearly to 1K parallel COVID-19 environments, resulting in even higher throughput gains."}, {"title": "Catalytic Reactions", "content": "Comprehending catalytic reaction pathways is essential for ad-vancing our understanding of chemical processes, refining conditions, and designing robust catalysts. These pathways offer insights into reaction mechanisms, facilitating the creation of more selective catalysts (Mattos et al., 2012; Shao et al., 2016). However, exploring these pathways presents significant challenges, including the complexity of multi-step reactions, short-lived intermediates, and experimental intricacies (Chen et al., 2021; Shi et al., 2021; Lan and An, 2021). RL shows promise in overcoming these challenges by providing an au-tomated approach to navigating reaction networks. However, RL encounters scientific and engineering obstacles, primarily limited by simulation throughput. Consequently, current RL research in chemical reactions often concentrates on specific reactions, relying on model simplifications with state vector encodings or heuristic rules. This approach limits generaliz-ability and requires substantial empirical design. Exploration is also confined to predefined sets of reaction networks, hindering the discovery of unknown mechanisms (Yoon et al., 2021; Lan and An, 2021; Margraf et al., 2023). Therefore, the pursuit of a more versatile RL solution to explore undiscovered reaction mechanisms remains a significant challenge in the field.\nIn this study, we present a reaction-agnostic methodology facilitated by WarpSci. The RL environment is constructed solely based on the potential energy landscape derived from first principles. This approach intrinsically defines the chemical reaction environment as a function of atomic positions, eliminating the necessity for laborious empirical or semi-empirical design of reaction-specific representations in RL environments. The outstanding generalizability and training speed are supported by the remarkable high-throughput capacity enabled by our architecture.\nWe forecast the reaction pathway for the crucial hydrogenation step in the Haber-Bosch (H-B) process on the Fe(111) surface. The H-B process holds a pivotal role in Earth's nitrogen cycle and represents over 2 percent of global energy consumption, yielding 160 million tons of ammonia annually. Despite a century of concentrated research to improve the H-B process, progress has been slow (Chen et al., 2018). Our framework has the potential to significantly contribute to process optimization, potentially reducing production costs and CO2 emissions while enabling the establishment of smaller and more widespread plants.\nFigure 4 displays the convergence speed as WarpSci processes the Langmuir-Hinshelwood reaction as a function of the number of environment replicas, running in parallel. The data reveal that, under consistent fixed hyperparameters, the simulations operating with an increased number of concurrent environments attain global convergence faster and more stably. The generalizable RL environment with the same hyperparameters is directly ap-plicable to the study of Eley-Rideal reaction mechanism. The results highlight the critical role of massively high data throughput in RL for effectively exploring a broad range of reaction mechanisms through a generalizable RL environment representation built solely upon atomic positions.\nOur findings reveal that the Langmuir-Hinshelwood mechanism shares the same tran-sition state as the Eley-Rideal mechanism for H migration to NH2, forming ammonia. Furthermore, the reaction path identified by our model exhibits a lower energy barrier com-pared to that through nudged elastic band calculation. In this Extended Abstract, we focus on presenting the generalizability, training speed and convergence stability facilitated by the high throughput of WarpSci. Interested readers seeking additional scientific background and technical details of this study are encouraged to refer to Lan and An (2021); Lan et al. (2024); Margraf et al. (2023)."}]}