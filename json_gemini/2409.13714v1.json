{"title": "TracrBench: Generating Interpretability Testbeds with Large Language Models", "authors": ["Hannes Thurnherr", "J\u00e9r\u00e9my Scheurer"], "abstract": "Achieving a mechanistic understanding of transformer-based language models is an open challenge, especially due to their large number of parameters. Moreover, the lack of ground truth mappings between model weights and their functional roles hinders the effective evaluation of interpretability methods, impeding overall progress. Tracr, a method for generating compiled transformers with inherent ground truth mappings in RASP, has been proposed to address this issue. However, manually creating a large number of models needed for verifying interpretability methods is labour-intensive and time-consuming. In this work, we present a novel approach for generating interpretability test beds using large language models (LLMs) and introduce TracrBench, a novel dataset consisting of 121 manually written and LLM-generated, human-validated RASP programs and their corresponding transformer weights. During this process, we evaluate the ability of frontier LLMs to autonomously generate RASP programs and find that this task poses significant challenges. GPT-4-turbo, with a 20-shot prompt and best-of-5 sampling, correctly implements only 57 out of 101 test programs, necessitating the manual implementation of the remaining programs. With its 121 samples, TracrBench aims to serve as a valuable testbed for evaluating and comparing interpretability methods.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in transformer-based language models have led to progress in various natural language processing tasks (Achiam et al., 2023; Anthropic, 2024). However, understanding the internal workings of these models remains challenging (Olah et al., 2018; Nanda et al., 2023; Black et al., 2022), which is problematic since models may generate harmful outputs (Shevlane et al., 2023; Perez et al., 2022a; Brundage et al., 2018) or harbor other unacceptable failure modes only revealed after deployment (Ngo et al., 2022; Scheurer et al., 2023; Hubinger et al., 2024). Despite various successes in interpretability (Bricken et al., 2023; Conmy et al., 2023; Nanda et al., 2023; Cunningham et al., 2023; Templeton et al., 2024), developing new interpretability methods remains difficult, partly due to the lack of models with fully understood internals (Casper et al., 2023; Casper, 2020), i.e. with ground truth mapping between weights and their functional form. Existing benchmarks for evaluating interpretability methods focus on input-output behavior (Casper et al., 2024; 2023; Mazeika et al., 2022), human evaluations (Templeton et al., 2024), or disentangling attributions of different entities (Huang et al., 2024), rather than the full mechanistic circuits, which hinders the rigorous and fast validation of novel interpretability methods.\nRestricted Access Sequence Processing Language (RASP) (Weiss et al., 2021) maps the core components of a transformer-encoder, i.e., attention and feed-forward computation, into simple primitives, forming a programming language to model and analyze transformer behavior. Tracr (Lindner et al., 2024), compiles RASP programs into functional transformer weights with a known mapping from weights to their functional form, enabling, the evaluation of interpretability methods (Conmy et al., 2023). However, its adoption is limited due to the difficulty of writing RASP programs and the large number of models required to effectively evaluate interpretability methods.\nIn this work, we introduce and evaluate a method to automatically generate RASP programs using LLMs and present TracrBench, a novel dataset with 121 LLM generated and, where necessary, manually written RASP programs and their compiled transformers. We assess the ability of frontier LLMs to generate RASP programs and find that this is a challenging task. With best-of-5 sampling and a 20-shot prompt, gpt-4-turbo-2024-04-09 correctly generates only 57 out of the 101 RASP programs in the test set. After adjusting for the difficulty of the programs, using the number of RASP operations as a proxy, the model achieves a normalized, weighted difficulty score of 0.29 (the maximum score is 1.0). TracrBench aims to be a rich testbed for evaluating interpretability methods and accelerating their development."}, {"title": "2. Method", "content": "Current interpretability research faces challenges in rigorously evaluating novel methods due to the lack of models with fully understood internals. While Tracr compiles RASP programs into transformers with known mappings from weights to their functional form, writing programs in Tracr is time-consuming and difficult. This is partly because RASP is an unconventional, non-Turing-complete programming language that requires algorithms to be implemented differently than in standard Turing complete languages like Python (see Appendix B for an example).\nTo address this issue, we propose to generate interpretability testbeds using LLMs, leveraging their ability to write code (Achiam et al., 2023; Li & Murr, 2024). We prompt LLMs to generate RASP programs that implement specified algorithms. We create TracrBench, a dataset of 121 RASP programs, by leveraging LLMs and manual annotation when they fail. These programs are then compiled into functional transformer weights using Tracr, resulting in transformer models with a known mapping between weights and their functional form. This allows researchers to validate the outputs of their novel interpretability methods against the ground truth. Our dataset of compiled models thus serves as an interpretability testbed.\nTo generate a program, we condition a language model M on a prompt P that includes a description of the specific algorithm to be implemented and at least one example input-output pair (see Fig. 2). To optimize LLM performance, P includes a detailed description of the RASP language and its five main components (SELECT, AGGREGATE, SELECTWIDTH, MAP, and SEQUENCEMAP), along with relevant Tracr source code defining these components and up to 20 RASP programs with their descriptions. We use Chain-of-thought prompting (Wei et al., 2022) to encourage reasoning and planning before generating code (see Appendix C for the prompt). We create three variations of this prompt: Zero-Shot, One-Shot (extending Zero-Shot with an RASP program and its description), and 20-Shot (extending Zero-Shot with 20 RASP programs and their descriptions).\nLet M(P) represent the extracted program from the output of model M when conditioned on the prompt. We define a five-step verification pipeline to assess the correctness of the generated program M(P). Each step performs a specific verification relevant to the overall correctness of the program. Here are the five stages of the pipeline:\n1. Compilation and execution: Test whether the program compiles without errors and runs error-free.\n2. Output correctness: Test whether the function actually performs the correct computation and implements the specified program using 1,000 input-output pairs\u00b9 generated by a manually written Python function equivalent to the desired RASP program.\n3. Tracr validation: Run the program through the in-built Tracr (Lindner et al., 2024) validator\u00b2 to filter out certain programs that aren't converted to equivalent transformer weights.\n4. Transformer weights compilation: Run the actual RASP-to-transformer compilation process to expose runtime errors like a division by zero."}, {"title": "3. Dataset", "content": "Writing RASP code to generate Tracr interpretability test beds is labor-intensive and has a steep learning curve (see Appendix B for an example). This has impeded the adoption of Tracr as a method to evaluate novel interpretability methods. To address this issue, we present TracrBench, a novel dataset of Tracr models that enables interpretability researchers to quickly test methods on transformers with known mappings from weights to their functional form. The dataset is generated as follows. First, we select 121 simple, sequence-to-sequence algorithms that cover a diverse range of tasks and difficulty levels (see the full list in Appendix A). We come up with these by sampling concrete algorithms from LLMs and manually selecting suitable ones. Some algorithms are also taken from Michaud et al. (2024). We then prompt gpt-4-0125-preview (which was the most competitive model at the time) to generate a RASP program for each program description. We test all outputs with our verification pipeline and verify them manually, finding that 49 of the generated RASP programs are correct. We then manually write the remaining RASP programs, ensuring that all programs in the dataset are correct and of high quality. Finally, we take 20 samples to use as examples in the prompt and use the remaining 101 samples as our test set.\nThe resulting dataset contains RASP programs of various complexity, from simple elementwise operations to more complex programs that lead to transformers with 2.7 million parameters. We use the number of RASP functions (such as Select and Aggregate, but also rasp.indices and rasp.tokens) as a proxy for the difficulty of the algorithm. This approach is more accurate than counting lines of code because some programs may have many lines that don't involve RASP (see Appendix B for an example). The distribution of task difficulties is depicted in Fig. 3 and Fig. 4. The first figure shows that most programs are quite"}, {"title": "4. Experiments", "content": "In this section, we evaluate the capability of LLMs to generate correct RASP programs. As described in Section 2, we condition an LLM on a prompt that includes a program specification, a detailed description of the RASP language, and important parts of the RASP source code. We use three variations of the prompt: a zero-shot, a one-shot prompt, and a 20-shot prompt. These different prompt variations are used to assess how including examples affects the LLM's performance in generating RASP programs.\nWe evaluate the generated RASP programs using the verification pipeline described in Section 2. A program is considered correctly implemented if it passes all five pipeline steps. To account for the variance in program difficulty, we introduce a second metric called the difficulty-weighted score, which weights each success by the number of RASP functions in the program. Summing these weighted scores across tasks provides us with a composite score that more effectively represents the model's proficiency.\nWe first evaluate the performance of different prompts using gpt-3.5-turbo-0125 and gpt-4-turbo-2024-04-09. To minimize compute costs, we evaluate the performance of additional models using only the full prompt. These models include gpt-4o-2024-05-13, claude-3-haiku-20240307, claude-3-sonnet-20240229, claude-3-opus-20240229 and claude-3-5-sonnet-20240620. All models are evaluated on the test set with 101 samples and sampled at temperature 0.9, with top-p=0.95. To distinguish between an LLM's general programming ability and its RASP-specific capabilities, we establish a baseline where the LLM writes a Python program for the same target algorithms."}, {"title": "4.1. Results", "content": "The results of our experiment, visualized in Fig. 1, show that state-of-the-art LLMs are able to understand the RASP language and, to some extent, generate correct RASP programs. Adding examples to the prompt clearly improves the performance, as shown with gpt-4-turbo and gpt-3.5-turbo. Overall, gpt-4-turbo achieves the highest pass rate of 56%, outperforming claude-3-opus with a pass rate of 46%. In comparison, when generating Python programs for the target algorithms, gpt-4-turbo achieves a pass rate of 96%. When taking the difficulty of the target algorithms into account, i.e., when using the difficulty-weighted score as a metric, we observe that the successes are strongly concentrated among the easy, low-difficulty programs (see Fig. 4) with gpt-4-turbo achieving a score of 0.29 (out of 1.0) and gpt-40 performing best with a score of 0.31. Claude-3-5-sonnet has a similar pass rate (0.45) to claude-3-opus (0.46), however, it achieves a higher difficulty-weighted score (0.27), than claude-3-opus (0.23).\nThese results suggest that frontier LLMs cannot yet competently generate correct RASP programs. The relatively poor performance of generating RASP programs compared to conventional programming languages like Python may be attributed to RASP's limited representation in LLM training data. This finding highlights that the ability of frontier LLMs to extend their reasoning and programming capabilities to low-resource programming languages is limited, which may stand in contrast with their generalization in natural low-resource languages (Reid et al., 2024)."}, {"title": "5. Related Work", "content": "Evaluating novel interpretability methods is challenging (Casper, 2020). While previous work has addressed this issue, it mainly focused on input-output level interpretability (Casper et al., 2024; 2023; Mazeika et al., 2022), human evaluations (Templeton et al., 2024), or disentangling attributes of different entities (Huang et al., 2024). RASP (Weiss et al., 2021) a programming language computationally equivalent to transformer, and Tracr (Lindner et al., 2024), which compiles RASP programs into corresponding transformers, have been used to create interpretable models for validating interpretability methods (Conmy et al., 2023). However, writing RASP programs in sufficient quantity is very time-consuming, which hinders the broad adoption of Tracr to evaluate interpretability methods. Notably, Tracr weights are more sparse and simple than any set of weights likely to result from gradient descent. Therefore, a method capable of interpreting Tracr weights may not necessarily be able to interpret trained transformers. However, interpretability methods that are capable of interpreting trained transformers should also be capable of interpreting Tracr transformers. Thus the latter still serve as a valid method to test (but not to develop) useful interpretability methods. Finally, both Thurnherr & Riesen (2024) and Langosco et al. (2024) programmatically generate large quantities of RASP programs with their corresponding weights to train decompiler models that generate RASP programs for a given set of transformer weights. Their RASP programs are, however, randomly generated by re-combining a few elemental operations, which leads to models that are often hard to decipher and do not correspond to realistic algorithms.\nLLMs have been explored for generating datasets for model evaluations (Perez et al., 2022b) and automating part of the interpretability workflow (Bills et al., 2023). We extend this by using LLMs to scalably generate realistic and interpretable RASP programs. The generated programs serve as a test bed for evaluating interpretability methods."}, {"title": "6. Conclusion", "content": "We demonstrate that LLMs can be used to generate interpretability test beds. However, their performance rapidly deteriorates with the increasing difficulty of RASP programs, indicating that frontier LLMs struggle to generate interpretability test beds at scale. We expect that these current limitations, likely due to Tracr's low-resource nature, will diminish as LLM capabilities continue to advance. Finally, we introduce TracrBench, a novel dataset comprising 121 transformers with known mappings from weights to functional form. Its intended use is the testing of interpretability methods. It is unsuitable as a target for interpretability method development due to its small size and the fact that Tracr weights are very dissimilar to those of trained transformers in terms of sparsity and matrix-rank. TracrBench serves as a valuable resource for evaluating and comparing interpretability methods, facilitating the development of more effective techniques for understanding the inner workings of transformer-based models."}, {"title": "7. Author Contributions", "content": "Hannes Thurnherr executed the whole project, developed the prompts, created the dataset (i.e., the Tracr programs) with the help of LLMs and manual labour, w ran all experiments and wrote the paper. J\u00e9r\u00e9my Scheurer developed the idea and ran exploratory experiments, oversaw the project, including detailed guidance on directions, experiments, presentation, and the final paper."}, {"title": "B. Example Program", "content": "RASP, a programming language designed to be computationally equivalent to transformers, requires a conceptually different approach to implementing algorithms compared to conventional programming languages. For instance, sorting algorithms in RASP must be implemented unconventionally due to the language's unique constraints. Unlike traditional programming languages that allow iteration over a sequence, RASP processes all elements in a sequence in parallel, mimicking the behavior of transformers. Consequently, a sorting algorithm in RASP would count, for each entry, the number of other entries smaller than itself and then use these counts to rearrange the original elements. While this approach would be considered inefficient in conventional programming languages, it is a straightforward implementation under the constraints of RASP. This example highlights the need for a different mindset when writing algorithms in RASP, as the language's parallel processing nature requires unconventional solutions to common problems.\ndef make_sort_unique(vals: rasp. SOp, keys: rasp.SOp) -> rasp.SOp:\nsmaller = rasp.Select(keys, keys, rasp.Comparison. LT)  find the smaller elements for each entry\ntarget_pos = rasp. SelectorWidth (smaller) # count the number of smaller elements for each entry\nrearrangement selector according to target_pos\nsel_new = rasp.Select(target_pos, rasp.indices, rasp.Comparison. EQ) # create the\noriginal sequence\nreturn rasp.Aggregate (sel_new, vals) # apply the rearrangement selector to the\ndef make_sort(vals: rasp.SOp, keys: rasp. SOp, *, max_seq_len: int, min_key: float) -> rasp\n.SOp:\nkeys = rasp. SequenceMap (lambda x, i: x + min_key * i / max_seq_len, keys, rasp.indices\n) # turn all the elements unique by adding a small fraction of their index\nreturn make_sort_unique(vals, keys) # apply sort_unique to the sequence using the now\nunique elements as keys\nRASP programs written for the Tracr compiler are written in Python using the tracr.rasp module. Sometimes they consist of a number of lines where the tracr.rasp module is not used. These parts of the RASP program can be written independently of one's understanding of the RASP language. The following is an example of a program where most lines don't involve RASP.\nThis illustrates why the number of rasp functions in a program is a better approximation of difficulty than the number of total lines when it comes to evaluating a model's ability to write RASP code.\ndef primecheck(n):\nfor i in range(2, int(n/2)):\nif n%i==0:\nreturn 0\nreturn 1\ndef make_check_prime() -> rasp.SOp:\nreturn rasp. Map (lambda x: primecheck(x), rasp.tokens)"}, {"title": "C. Full Prompt", "content": "Prompt \"Paraphrased + Tip about different stock\"\n# Introduction to Task:\nYour assignment is to generate RASP programs capable of implementing a variety of\nalgorithms using sequence operations. \"RASP\" stands for \"Restricted Access\nSequence Processing Language\". RASP allows you to articulate complex sequence\nto sequence in a format equivalent to what a neural network of the transformer\narchitecture can do. RASP programs always output a sequence that has the same\nlength as the input sequence.\n# Your Task\nMake a RASP program that replaces each element with the parity (0 for even", "index.\nExample": [5, 5, 5, 5], "1": "nName the function that you can call to make this program 'make_index_parity()'\nKeep your task in mind while reading the following information.\n# Understanding RASP:\nRASP programs are unique because they always process sequences and output\ntransformed sequences of equivalent length. While doing so they void\nconditional branches or loops if possible. Instead", "Principles": "nInput and Output: Each RASP program receives an input sequence and yields an\noutput sequence of identical length.\nStructure: Loops and if statements cannot depend on attributes or individual\nelements of the input sequence. If you make loops", "max_sequence_length\" parameter.\nOperation Calls": "Programs can only invoke core RASP functions or refer to other\nRASP programs. Never attempt to access the internals of the sequence.\n## Technical operational Jargon:\nHere are descriptions of various operations that are used in RASP.\n'rasp.Select': Matches elements from two sequences based on a boolean comparison\ncondition and returns a corresponding matrix of \"True\" and \"False\" values\ncalled a selector.\n'rasp.Aggregate': takes as input a selector and an SOp (Sequence Operation", "matrix.\n`rasp.Map'": "Transforms a sequence by applying a function to each element\n'rasp.SequenceMap': Produces a new sequence based on two previous sequences and\na lambda function that gets applied to each pair of elements.\n'rasp.SelectorWidth': returns the number of \"True\" values in each row of a\nselector\n### Function overview:\n#### Select:\nFunction: Creates a selector to define relationships between elements of sequences\nSyntax: 'rasp.Select(keys: SOp", "queries": "SOp", "predicate": "Predicate)'\nelements where indices are equal.\nExample: 'rasp.Select(rasp.indices", "Aggregate": "nFunction: Takes as input a selector and an SOp", "matrix.\nSyntax": "rasp.Aggregate (selector: Selector, sop: SOp, default: Optional [VT] = None\nExample:", "Map": "nFunction: Applies a function element-wise on the input SOp.\nSyntax: (f: Callable ((Value)", "inner": "SOP)\nExample: 'Map (lambda x: x + 1", "SequenceMap": "nFunction: Applies a function element-wise on two given SOps.\nSyntax: 'rasp.SequenceMap(f: Callable [[(Value", "fst": "SOp", "snd": "SOP)\nExample: rasp.SequenceMap (lambda x", "y": "x - y", "SelectorWidth": "nFunction: Returns the \"width\" of a selector", "\nTrue\"-values in each row.\nSyntax": "rasp.SelectorWidth(selector: Selector)`\nExample:", "Indices": "nrastokenns: The original input sequence.\nrastindicesns: Returns the position index at each token.\n### Example use of above Functions:\nThis is an example use the rasp.Select function. Here", "Greater Than\" or GT\ncomparison operator": "n```python\ngreater_than_selector = rasp.Select(rasp.tokens", "greater_than_selector\")\nIf the rasp.tokens-sequence is [1, 2, 3, 4) the selector will look like this": "n[False", "True": "n[False"}, {"True": "n[False"}, {"True": "n[False", "False": "nIf we now apply this to the original rasp.tokens again with:\n```python\noutput = rasp.Aggregate (greater_than_selector", "this": "n3, # as an average of the selected 2,3 and 4\n3.5, # as an average of the selected 3 and 4\n4, # as an average of the selected 4"}]}