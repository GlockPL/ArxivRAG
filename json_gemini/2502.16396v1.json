{"title": "FedNIA: Noise-Induced Activation Analysis for Mitigating Data Poisoning in Federated Learning", "authors": ["Ehsan Hallaji", "Roozbeh Razavi-Far", "Mehrdad Saif"], "abstract": "Federated learning systems are increasingly\nthreatened by data poisoning attacks, where mali-\ncious clients compromise global models by con-\ntributing tampered updates. Existing defenses\noften rely on impractical assumptions, such as ac-\ncess to a central test dataset, or fail to generalize\nacross diverse attack types, particularly those in-\nvolving multiple malicious clients working collab-\noratively. To address this, we propose Federated\nNoise-Induced Activation Analysis (FedNIA), a\nnovel defense framework to identify and exclude\nadversarial clients without relying on any central\ntest dataset. FedNIA injects random noise in-\nputs to analyze the layerwise activation patterns\nin client models leveraging an autoencoder that de-\ntects abnormal behaviors indicative of data poison-\ning. FedNIA can defend against diverse attack\ntypes, including sample poisoning, label flipping,\nand backdoors, even in scenarios with multiple\nattacking nodes. Experimental results on non-iid\nfederated datasets demonstrate its effectiveness\nand robustness, underscoring its potential as a\nfoundational approach for enhancing the security\nof federated learning systems.", "sections": [{"title": "1. Introduction", "content": "Federated Learning (FL) redefines conventional centralized\ndata processing by enabling training across decentralized\ndevices. This collaborative method-\nology aggregates model parameters derived from local train-\ning at the network's edge, emphasizing the advantages of\nlocal computation. By doing so, FL ensures user privacy\nby restricting the server's access to local data. The decen-\ntralized nature of FL yields significant benefits, including\ndiminished data transfer requirements, enhanced privacy\npreservation, and the capability to harness diverse datasets\nfrom various clients. Despite these advantages, FL is not\nimmune to security threats. Particularly vulnerable to poi-\nsoning attacks, FL faces challenges in maintaining the in-\ntegrity of the collaborative learning process. In light of\nthe pressing importance of this threat, this paper focuses\non fortifying FL against the subtle threat of data poisoning\nattacks, thereby ensuring the reliability and robustness of\nthe decentralized learning framework.\nData poisoning attacks in FL primarily aim to manipulate\nthe sample distribution and label distribution in both tar-\ngeted and untargeted manners. These attacks occur on the\nmalicious client side, where adversaries alter the training\ndata before contributing updates to the global model. If\nundetected or not properly addressed, a malicious update\nhas the potential to contaminate the global model, leading\nto the spread of the attack into other clients' models.\nIn untargeted schemes, the goal is to generally degrade the\noverall performance of the global model. This type of attack introduces changes\nto samples or their labels and compromises the model's\naccuracy across various classes, making it less reliable in its\npredictions. In targeted data poisoning attacks, the focus is\non specific classes or injecting triggers to produce forged\npredictions upon triggering the system. The aim\nhere is to manipulate the model's behavior strategically,\nintroducing biases or vulnerabilities that can be exploited to\ncompromise the integrity of predictions related to particular\nclasses or conditions. Addressing and thwarting these data\npoisoning techniques is of crucial importance to maintain\nthe trustworthiness and effectiveness of FL in decentralized\nenvironments.\nAddressing data poisoning challenges in FL involves ex-\nploring various solutions, mainly categorized into anomaly\ndetection, adversarial training, robust aggregation, and the\nutilization of regularization techniques. Anomaly detec-\ntion methods have been proven effective against untargeted\nattacks but may fall short in eliminating targeted ones, of-\nten requiring a test dataset on the server for comparison,\nwhich may not always be available."}, {"title": "2. Related Works", "content": "The literature on FL robustness against poisoning attacks\ncan be categorized into two main streams: server-side and\nclient-side defense. These two can be used simultaneously, as they secure dif-\nferent ends of a FL network. Server-side defense, which\nis the topic of interest in this work, is often carried out\nusing anomaly detection or tailored aggregation rules that\nmake the central aggregator robust against poisoning at-\ntacks. Here, we refer to such methods as robust aggregation.\nMoreover, a number of studies show that partial robustness\nagainst poisoning attacks can be obtained using regulariza-\ntion techniques and Differential Privacy (DP). In the following overview, we\nwill concisely go through both domains and underscore their\nrelevance to our research.\nRobust Aggregation Several robust aggregation tech-\nniques have been introduced to tackle data poisoning in\nFL, while aiming at maintaining the FL performance. A\ngroup of these algorithms detects suspicious updates and\nreduces their contribution weight to the aggregation process\non the server. For instance, the aggregator in calculates the median or coordinate trimmed mean of\nlocal updates prior to generating the global update. Another\ngroup of algorithms finds clusters of clients and sets mali-\ncious clients apart from benign users so that the suspicious\nusers do not participate in the aggregation process. As an\nexample, FoolsGold combats Sybil at-\ntacks by adjusting the learning rates of local models based\non a contribution similarity. This method effectively iden-\ntifies Sybil groups, when they are present. Nonetheless, it\nis prone to mistakenly flag benign participants and deteri-\norate the training performance. While this method relies\non the similarity of malicious users, other approaches such\nas take\nthe correlation of benign users into account. In addition,\nstatistical methods such as taking the median of updates\nhave been shown effective in enhancing attack robustness.\nRobust aggregation with adaptive\nclipping (AdaClip) is performed by zeroing out extremely\nlarge values for robustness to data corruption on clients, and\nadaptively clipping in the L2 norm to the moderately high\nnorm for robustness to outliers.\nDifferential Privacy While DP has been primarily consid-\nered as a defense against inference attacks, several studies\nshow that it can also be effective in making the FL model\nmore robust against poisoning attacks. An adaptive version of DP for FL is pre-\nsented in (Andrew et al., 2021) (AdaDP) that clips the norm\nat a specified quantile of the update norm distribution simi-\nlar to AdaClip. The value at the quantile is calculated in\nreal-time with DP."}, {"title": "3. Threat Models", "content": "Poisoning attacks can be conducted in various settings with\ndifferent objectives. In this work, we consider three main\ncategories of these attacks, namely sample poisoning, label\nflipping, and backdoors.\nSample Poisoning Let $X = \\{X_1,X_2,...,X_m\\}$ and $Y =\\{Y_1, Y_2, ..., Y_m\\}$ represent benign training samples and la-"}, {"title": "4. Noise Induced Activation Analysis", "content": "In this section, the design of the proposed method, FedNIA,\nand the motivation behind it are explained. Moreover, we\nexplain how other FL system components interact with the\nproposed approach."}, {"title": "Client Models", "content": "Considering a set of $k$ clients $C =\\{C_1, C_2, ..., C_k \\}$, a model $M$ is formally defined as:\n$M(W_i,x): x \\rightarrow h_1^M,...,h_L^M$.\nfor client $c_i$ with network weights $W_i$ and $L$ hidden layers\n$h_i$. Indicating the FL time-steps with $t$, we denote trained\nmodel weights and the local dataset at each iteration with\n$W_t^i$ and $D_t^i$, respectively. At each time step, $M$ is first\ninitialized with weights of the global model $W_G^t$, which we\nrefer to as the global state. Then, $M$ is trained on the local\ndata $D_t^i$ to obtain $W_{i,t}$ as $M(W_G^t, D_t^i) \\rightarrow W_{i,t}$. Afterward,\n$W_{i,t}$ will be communicated to the server as an update. This\nprocess is also shown in Algorithm 1."}, {"title": "Malicious Clients", "content": "Algorithm 1 also details the condition\nwhere malicious clients $C = \\{\\check{C}_1, \\check{C}_2, . . ., \\check{C}_r \\}$ can initiate a\npoisoning attack on the FL process by training the model\non poisoned data $\\check{D}$ and obtain $M (W_G^t,\\check{D}) \\rightarrow \\check{W}$. If not\neliminated by the aggregator, sending $\\check{W}$ to the server will\npoison the next global state $W_G^{t+1}$ and corrupt all $W^{t+1}$ as\na result. Here, the number of malicious clients is assumed\nto be $1 \\le r < \\frac{k}{2}$. This is because $r > \\frac{k}{2}$ indicates a 51%\nattack, where the population of malicious clients is higher\nthan benign clients, which enables them to control the FL\nnetwork."}, {"title": "Server", "content": "The FL server will receive a number of updates\n$W_i^t$ from clients. For the sake of simplicity, we assume all\nclients are available at each iteration, that is $1 < i < k+r$.\nAt this step, the server aims to evaluate $W_i^t$ to filter out po-\ntentially malicious updates. Since different mechanisms of\npoisoning attack affect $M$ differently, designing a detection\nmodel that handles a spectrum of attacks requires careful"}, {"title": "Detection Model", "content": "Each layer $l$ of the model $M$ produces a\nvector of activation values, denoted as $\\alpha_l$, where the length\nof $\\alpha_l$ equals the number of neurons in that layer. For a\nnetwork with $L$ layers, $A$ represents the concatenation of\nthese vectors $\\alpha_1, \\alpha_2, ..., \\alpha_L$ for client $i$ at time $t$. To enable\nthe comparison between the average of noise-induced client\nactivations $\\overline{A}^t, 1 \\le i \\le k + r$, and the activations of the\nglobal state, $A_G^t$, $Z_t$ is also passed to $M(W_G^t, Z_t)$. At this\nstage, $A_G^t$ can be used as a reference for comparison with\n$\\overline{A}_t$, where $1 \\le i \\le k + r$. To enable anomaly detection\non each layer of $M$, we tailor an autoencoder model with\nsub-networks that share input and output layers so that each\nsub-network can concentrate on encoding and decoding\nactivation values corresponding to a specific layer. Each\nsub-encoder $E(\\alpha_l)$ takes $\\alpha_l$ portion of $A_t$ that corresponds\nto layer $l$ of $M (W_i, Z_t)$ as in the following:\n$E(\\alpha_l) : \\alpha_l \\rightarrow h_l^{(1)}, h_l^{(2)},...,h_l^{(j)}$.\nwhere $h_l^{(j)}$ denotes the $j$-th hidden layer of the sub-network\n$E(\\alpha_l)$, and $h_l^*$ is the code layer of $E(\\alpha_l)$. The encoder\nnetwork is then formally defined as:\n$\\text{Encoder}(A) : \\overline{A} \\rightarrow \\cup_{l=1}^{L} E(\\alpha_l) | \\alpha_l \\in \\overline{A}$.\nCorrespondingly, sub-decoders $D_l$ and the decoder are for-\nmulated as:\n$D(h_l^*) : h_l^* \\rightarrow \\hat{\\alpha_l}, h_l^{(1)}, h_l^{(2)},..., \\hat{\\alpha_l}$.\n$\\text{Decoder}(h^*) : \\overline{D} \\rightarrow \\cup_{l=1}^{L} D(h_l^*), \\hat{A} | h_l^* \\in h^*$.\nwhere $\\hat{\\alpha}$ is the reconstructed activation values of $h_l^M$, and\n$\\hat{A}$ is the estimated vector of all activation values. Also,\n$h^* = \\{h_1^*, h_2^*,... h_L^*\\}$ is the code layer connected to the\nsub-networks in the encoder and decoder. The autoencoder\nis formulated as $AE(\\overline{A}) = \\text{Decoder} (\\text{Encoder}(\\overline{A}))$. The\narchitecture of the trained $AE$ and other components of the\nserver are depicted in Figure 1."}, {"title": "Training Loss", "content": "Minimizing the reconstruction error on\nthe whole $\\overline{A}^t$ vector may reduce the success rate in some\ncases. Given that $|h_l^M|$ is different for $1 < l < L$, the"}, {"title": "Filtering Updates", "content": "At each FL round, AE is first trained\non $A_G^t$. Then, AE encodes and reconstructs $\\overline{A}^t$ and re-\nceives $\\hat{U}^t = AE(\\overline{A}^t)$. Then, the reconstruction er-\nror for each $A_i^t, 1 \\le i \\le k + r$, will be computed as\n$\\epsilon_i = \\sqrt{\\frac{1}{|A_i^t|} || A_i^t - \\hat{A}_i^t ||^2}$. Once the errors are estimated,\n$\\epsilon_i$ corresponding to $e_i$ will be filtered based on the threshold\n$\\tau$:\n$\\tau = (\\frac{1}{k+r} \\sum_{i=1}^{k+r} \\epsilon_i) + \\lambda \\sigma$.\nwhere $\\lambda$ is the scaling factor, and $\\sigma$ denotes the standard de-\nviation of obtained errors. Benign updates are then sampled\ninto $\\Omega = \\{W_1^\\dagger, W_2^\\dagger, \\dots, W_k^\\dagger\\}$ to prevent $\\check{C}_i \\in \\check{C}$ contribute\nin the aggregation step:\n$\\check{W} = \\{ \\begin{array}{cc}\n\\O & \\text{if } e_i < \\tau \\\nW_{i=1}^{k+r} & \\text{otherwise} \\\n\\end{array}$.\nThe aggregation of $W_i^t$ is performed using the FedAvg\nalgorithm. At the end of round $t$, $FedAvg(\\Omega) = W_G^{t+1}$\nestimates the global state for $t + 1$, when the explained\nprocess will be repeated."}, {"title": "Time Complexity", "content": "As detailed in Appendix A.1, the time\ncomplexity of FedNIA is approximately $O(k|W|+ \\beta \\eta |\\theta|)$,\nwhich roughly equals a complexity of $O(\\beta \\eta |\\theta|)$ added to\nthat of the FedAvg."}, {"title": "5. Experimental Results", "content": "This section evaluates the proposed method under sev-\neral scenarios. The evaluation process involves comparing\nFedNIA with other defense mechanisms usable in the se-\nlected attack scenarios. The experimental setup and attack\nscenarios are reported and analyzed accordingly."}, {"title": "5.1. Experimental Setup", "content": "Dataset The proposed method is evaluated on the Ex-\ntended MNIST (EMNIST)  and Fashion-\nMNIST datasets. The original dataset\nis shuffled with a fixed random seed to ensure consistency\nacross multiple runs. To create local datasets, samples\nare randomly drawn without replacement from the origi-\nnal dataset and batched with a size of 20 samples. This\ncauses local datasets to be highly imbalanced and makes the\ndatasets non-i.i.d. across the federated network."}, {"title": "Attacks", "content": "Attacks are implemented with different ratios of\n$\\delta = \\frac{r}{k+r}$ ratios, where $r$ and $k$ are the numbers of malicious\nand benign clients, respectively. In our simulations, we\nconsider $0.02 < \\delta < 0.2$. Furthermore, the total number of\nclients is fixed to $k + r = 50$ when $\\delta$ changes. To maximize\nthe effectiveness of attacks, sample poisoning, and label\nflipping attacks are implemented with $\\gamma = 1$. Untargeted\nattacks are implemented using sample poisoning and label\nflipping. For targeted attacks, targeted label poisoning and\nbackdoors are simulated. Targeted label flipping is carried\nout in both targeted and untargeted fashion, where labels\n1 and 2 are changed to 7 and 5, respectively. Backdoors\nare injected into samples of class $c = 1$ using a trigger $\\epsilon$\nresembling a pixel backdoor attack."}, {"title": "Federated Learning Structure", "content": "The model structure used\nfor FL is a feedforward neural network with three hidden\nlayers with ReLU activations and sizes 256, 256, and 128.\nThe final layer uses a Softmax activation. Local models un-\ndergo five training epochs in each FL round, both clients and"}, {"title": "5.2. Aggregation Performance", "content": "Before evaluating the selected algorithms under poisoning\nattacks, we assess their performance when the FL network\nis not under attack and $\\delta = 0$. Figure 2 depicts the classi-\nfication performance using various aggregation techniques\nwhen the FL model is trained by 100 benign clients. Com-\nparing the training loss of these algorithms in Figure 2(a),\nit can be observed that FedNIA initially exhibits a slower\nconvergence speed compared to FedAvg; however, given\nenough time, it outperforms all selected aggregators. This\ndelay in the convergence is due to the fact that the autoen-\ncoder network inside FedNIA itself requires convergence\nand training. Nonetheless, when the autoencoder is finally\ntrained, FedNIA will boost the convergence speed faster\nthan others. Therefore, when the model is not under at-\ntack, FedNIA can maintain the model performance and\nimprove the training loss. Similarly, the results in Figure\n2(c) show that FedNIA converges at a speed similar to that\nof FedAvg. However, the convergence speeds in experi-\nments using the Fashion MNIST dataset are mostly on par\nwith each other."}, {"title": "5.3. Resilience to Attacks", "content": "The goal of attackers in untargeted attacks is generally to\ndeteriorate the overall accuracy of the FL model. Therefore,\nsample poisoning and untargeted label flipping are evaluated\nbased on the test accuracy. On the other hand, backdoor\nand targeted label flipping aim at specific classes. Thus, a\ntest subset is created by drawing the test samples associated\nwith the targeted class and used for calculating the test"}, {"title": "Targeted Data Poisoning", "content": "Figure 3 reports the aggrega-\ntion results for different ratios of $\\delta$ and attack mechanisms\nusing EMNIST and Fashion MNIST datasets. As expected,\nincreasing d significantly affects the model performance us-\ning other aggregators. Nonetheless, FedNIA maintains its\nrobustness to a large extent as d varies. The results indicate\nthat FedNIA specifically sets itself apart when it comes to\ntargeted attacks. As shown in Figure 3(a, b), targeted attacks\n(i.e., targeted label poisoning and backdoor) are properly\nmitigated by the proposed aggregation method. In contrast\nto other methods, this attack almost changes the FedNIA\nperformance with a linear pattern, while slightly chang-\ning its accuracy. Moreover, the results of backdoor attacks\nindicate even DP-based methods that induce noise in the net-\nwork weights, still fail to mitigate backdoors when they are\ninitiated by several malicious nodes. Nonetheless, FedNIA\nsets itself apart in detecting backdoors and significantly out-\nperforms the rest of the methods. It is worth mentioning that\nthe designed backdoors are so subtle that launching them\nby a single attacker node hardly has any effect on the global\nstate, even on FedAvg. Figure 3(e, f) depicts the results\nof targeted attacks for Fashion MNIST dataset. Similar to\nthe previous analysis, FedNIA outperforms the rest of the\nmethods in terms of accuracy. In addition, this method ex-\nhibits more robustness against different ratios of d. While\nthe comparison between different methods is mostly sim-\nilar for this case, what stands out is the bigger difference"}, {"title": "Untargeted Data Poisoning", "content": "The evaluation results for\nuntargeted poisoning attacks are illustrated in Figure 3(c,\nd). FedNIA demonstrates high robustness against sample\npoisoning compared to the other algorithms using EMNIST\ndataset. However, the difference in the accuracy among\nthe selected aggregators is less pronounced for this attack.\nAs the ratio of malicious clients ($\\delta$) increases, the accuracy\nof FedNIA remains relatively stable, showcasing its effec-\ntiveness in mitigating the adverse effects of data poisoning.\nFurthermore, FedNIA effectively eliminates various ratios\nof the label flipping attack, maintaining relatively high per-\nformance. In contrast, the accuracy of the other methods\ndrastically deteriorates as d increases. Looking at Figure 3(g,\nh), it can be observed that FedNIA also outperforms the\nrest of the methods when tested on Fashion MNIST dataset.\nThese results also confirm the findings from EMNIST ex-\nperiments. The most noticeable difference in this set of\nresults is the smaller difference between the performance of\nFedNIA and the rest, albeit still significant."}, {"title": "Overall Performance", "content": "Figure 3(i) shows the overall ac-\ncuracy of robust aggregators over all experiments. The\nresults are averaged for all attacks and datasets. For Back-\ndoor attacks, we converted ASR to accuracy by calculating\n1- ASR. As indicated in this figure, FedNIA outper-\nforms all when considering the overall performance. More-\nover, FedAvg results in the lowest performance when un-\nder attack, as expected. The rest of the methods exhibit a\nsomewhat similar performance, especially in higher $\\delta$ ratios.\nNonetheless, as explained before, this behavior may not be\nalways the case when studying certain attacks and datasets,\nas these results only reflect the overall performance of each\nmethod throughout the experiments. Moreover, Appendix\nA.2 confirms that FedNIA exhibits significant improvement\nin handling various types of poisoning attacks."}, {"title": "5.4. Runtime Analysis", "content": "Figure 4 presents a comparison of the average runtime for a\nsingle FL training iteration using each of the selected aggre-\ngators. The recorded run time is averaged for both datasets\nand all attacks and ratios. These measurements were ob-\ntained on a computer equipped with an NVIDIA RTX 3080\nGPU, an Intel Core i7-12700 CPU, and 32 GB of RAM. The\nexperiments were conducted using TensorFlow Federated\non an Ubuntu kernel, accessed via the Windows Subsystem\nfor Linux. The results suggest that the superior balance\nbetween accuracy and security offered by FedNIA comes\nwith a higher computational cost, a common trait among"}, {"title": "6. Conclusion", "content": "This paper contributes to the ongoing efforts to secure FL\nsystems against adversaries by presenting FedNIA, a novel\nmechanism to fortify FL systems against a wide array of\ndata poisoning attacks. This method is unique in its ability\nto address both targeted and untargeted attacks, including\nsample poisoning, label flipping, and backdoor attacks that\nare launched by several malicious nodes. By injecting ran-\ndom noise inputs into the reconstruction of client models\nand analyzing the layer activations, FedNIA was able to\nidentify and exclude malicious clients before the aggrega-\ntion process. In addition, FedNIA eliminates the need for a\ncentral test dataset for client evaluation, a common yet of-\nten impractical assumption in existing defense mechanisms.\nExperimental results demonstrate the effectiveness of our\napproach. The proposed method not only successfully iden-\ntifies and excludes malicious updates, but also maintains a\nperformance level similar to the baseline when no attack is\npresent. This balance between resilience and performance\nin the absence of threats is a significant advantage of the\nproposed approach. Moreover, FedNIA has shown robust-\nness against various mechanisms of sample poisoning, label\nflipping, and backdoor attacks, covering the primary cat-\negories of data poisoning attacks when federated data is\nnon-i.i.d. Future works will focus on further refining the\ncomputational efficiency of our method and exploring its\napplicability to other types of attacks and FL scenarios."}, {"title": "A. Appendix", "content": "A.1. Complexity Analysis\nFor the sake of simplicity, we assume all clients are benign and r = 0. In each FL iteration t, FedNIA initially generates v\nrandom inputs, which has a complexity of O(v). Next, obtaining layer activations for each client model involves a forward\npass, leading to a complexity of O(kv|W|), where k is the number of updates received from clients, and |W| is the total\nnumber of weights in the model. Averaging these activations across all clients adds a complexity of O(kvn), where \u03b7 is the\ntotal number of activations across all layers. Training the autoencoder on the averaged activations At involves \u03b2 epochs\nwith each epoch having a complexity of O(n|\u03b8|), where |0| is the number of parameters in the autoencoder, leading to a total\ncomplexity of O(\u03b2\u03b7|\u03b8|) for the training phase. The inference phase, which includes encoding and reconstructing activations,\nadds O(\u03ba\u03b7 \u03b8|). Calculating the reconstruction error for each client is O(kn), and computing the threshold and filtering\nupdates both contribute a complexity of O(k). Finally, aggregating the filtered updates, akin to FedAvg, has a complexity\nof O(kW). Assuming that \u03bd,\u03b7 \u00ab |W| and |0| < |W|, the dominating terms in the combination of the aforementioned\nterms, simplify to O(k|W| + \u03b2\u03b7|\u03b8|), indicating that time complexity of O(\u03b2\u03b7|\u03b8|) is added to that of the FedAvg."}, {"title": "A.2. Significance Test", "content": "Figure 5 shows Critical Difference (CD) diagrams obtained from the post-hoc Friedman test. The significance level (i.e.,\nparameter a) is set to 0.05 in this test. This test estimates the significance of differences among the results obtained from\neach method in different experiments. Based on the determined CD level of this test, methods that are not significantly\ndifferent in terms of accuracy are connected and grouped using colored lines. For instance, in Fig, 5(b, d), the results\nobtained from FedAvg, AdaClip, and DP are not significantly different when used against backdoor and label flipping\nattacks. In panels (c, e), on the other hand, the results indicate that AdaDP and DP statistically result in somewhat similar\naccuracy when dealing with sample poisoning, and when the overall performance is considered. The same observation can\nbe made for AdaClip and AdaDP in the same scenarios. Nevertheless, in all conducted experiments, the performance\nof FedNIA is significantly better than the rest of the methods. This indicates the effectiveness and generalizability of the\nproposed method against data poisoning attacks in FL."}]}