{"title": "YOLO-Stutter: End-to-end Region-Wise Speech Dysfluency Detection", "authors": ["Xuanru Zhou", "Anshul Kashyap", "Steve Li", "Ayati Sharma", "Brittany Morin", "David Baquirin", "Jet Vonk", "Zoe Ezzes", "Zachary Miller", "Maria Tempini", "Jiachen Lian", "Gopala Anumanchipalli"], "abstract": "Dysfluent speech detection is the bottleneck for disordered speech analysis and spoken language learning. Current state-of-the-art models are governed by rule-based systems [1, 2] which lack efficiency and robustness, and are sensitive to template design. In this paper, we propose YOLO-Stutter: a first end-to-end method that detects dysfluencies in a time-accurate manner. YOLO-Stutter takes imperfect speech-text alignment as input, followed by a spatial feature aggregator, and a temporal dependency extractor to perform region-wise boundary and class predictions. We also introduce two dysfluency corpus, VCTK-Stutter and VCTK-TTS, that simulate natural spoken dysfluencies including repetition, block, missing, replacement, and prolongation. Our end-to-end method achieves state-of-the-art performance with a minimum number of trainable parameters for on both simulated data and real aphasia speech. Code and datasets are open-sourced at https://github.com/rorizzz/YOLO-Stutter", "sections": [{"title": "1. Introduction", "content": "Speech dysfluencies are defined as any form of speech that contains sound repetition, deletion, insertion, replacement, prolongation, block, etc. [1-4]. Dysfluency modeling holds significant promise in disordered speech screening [5, 6], and has a huge impact on the language learning market [7]. Dysfluency modeling is a speech transcription problem, which seems to be well tackled by recent large-scale developments [8-11]. However, large ASR models struggle with dysfluent speech for two reasons: (1) they transcribe speech into words or phonemes that lack dysfluency-related information, and (2) their strong language models often interpret dysfluent speech as fluent, e.g., transcribing \"P-P-Please call Stella\" as \"Please call Stella\" or \"Praise call Stella\". Progress in dysfluency modeling has been limited due to the lack of a well-defined problem formulation and high-quality annotated dysfluency data.\nUDM [1] first formally defines dysfluency modeling as the detection of all types of dysfluencies at both word and phoneme levels, with accurate time region prediction. UDM transcribes dysfluent speech into unconstrained forced alignment and adopts 2D-Alignment for detection task. H-UDM [2] developed a recursive 2D alignment algorithm that further boosts performance. These methods represent the de-facto pipeline at the time.\nIn this work, we tackle dysfluency modeling from a totally different perspective. Still following the dysfluency modeling criterion [1], we develop an end-to-end model which directly predicts dysfluencies and time regions from dysfluent speech and reference text input without any handcraft templates. To create training data, we introduce VCTK-TTS (7X larger than"}, {"title": "2. Naturalistic Dysfluency Simulation", "content": "End-to-end modeling of speech dysfluency requires high-quality annotations. The prevalent approach is through simulation. Currently, existing simulated datasets [1, 15, 16] either do not incorporate time steps and broad coverage of dysfluencies or are not naturalistic. To develop a comprehensive and naturalistic simulated dysfluency corpus, we have taken two steps:\n\u2022 We extended VCTK++ [1] by introducing rule-based word-level dysfluencies in acoustic space, creating a new dataset named VCTK-stutter [12]).\n\u2022 We designed TTS rules and injected phoneme and word dysfluencies in text space. VITS [13] was used to generate naturalistic dysfluent speech. This dataset is named VCTK-TTS."}, {"title": "2.1. VCTK-Stutter", "content": "VCTK++ [1] was created in the acoustic space by editing audio based on forced alignment. However, it only focuses on phoneme-level dysfluencies. We extended VCTK++ by incorporating word-level dysfluencies (named VCTK-Stutter). Similar to obtaining phoneme-level alignments through MFA [17], in VCTK++ we employ WhisperX [18] to get word-level alignments, enabling us to establish precise insertion points for dysfluencies. Utilizing the word alignments generated by WhisperX, we annotate specific regions where artificial dysfluencies are to be introduced with the type of word-level dysfluency and the start and end times. We enable each word to have an equal probability of experiencing a dysfluency for consistency. The word-level dysfluency types we incorporate include:\n\u2022 Word-repetition: This involves the repetition of entire words or phrases multiple times, with the count of repetitions varying randomly from 1 to 4, appended by a sample of silence of length 70% of the original word after each repetition.\n\u2022 Word-missing: This pertains to the omission of certain words. The randomly chosen missing word is replaced with a silence of equivalent duration"}, {"title": "2.2. VCTK-TTS", "content": "Traditional rule-based simulation methods [1, 15, 16] operate in acoustic space, and the generated samples are not naturalistic. We developed a new pipeline that simulates in text space. To achieve this, we first convert a sentence into an IPA phoneme sequence. Then, we develop TTS rules for phoneme editing to simulate dysfluency. These rules are applied to the entire VCTK dataset [13], allowing the voice of generated speech to vary from the 109 speakers included in the VCTK, thus enhancing the scalability of the dataset. We call this VITS-based Simulation. The entire pipeline is detailed in Sec.2.2.1, and TTS rules are discussed in Sec.2.2.2. Statistics are listed in Table. 1. Overall VCTK-Stutter/VCTK-TTS are 5X/7X larger than VCTK."}, {"title": "2.2.1. Method pipeline", "content": "VCTK-TTS pipelines can be divided into following steps: (i) Dysfluency injection: We first convert ground truth reference text of VCTK text [12] into IPA sequences via the VITS phonemizer [13], then add different types of dysfluencies at the phoneme level according to the TTS rules. (ii) VITS inference: We take dysfluency-injected IPA sequences as inputs, conduct the VITS inference procedure and obtain the dysfluent speech. (iii) Annotation: We retrieve phoneme alignments from VITS duration model, annotate the type of dysfluency on the dysfluent region."}, {"title": "2.2.2. TTS rules", "content": "We inject dysfluency in the text space following these rules:\n\u2022 Repetition (phoneme&word): The first phoneme or syllable of a randomly picked word was repeated 2-4 times, with pauselengths varying between 0.5 to 2.0 seconds.\n\u2022 Missing (phoneme&word): We simulated two phonological processes that characterize disordered speech [?] - weak syllable deletion (deletion of a random unstressed syllable based on stress markers\u00b9) and final consonant deletion.\n\u2022 Block: A duration of silence between 0.5-2.0 seconds was inserted after a randomly chosen word in between the sentence.\n\u2022 Replacement (phoneme): We simulated fronting, stopping, gliding, deaffrication - processes that characterize disordered speech [19] - by replacing a random phoneme with one that would mimic the phonological processes mentioned above.\n\u2022 Prolongation (phoneme): The duration of a randomly selected phoneme in the utterance was extended by a factor randomly chosen between 10 to 15 times its original length, as determined by the duration model.\nhttps://github.com/timmahrt/pysle"}, {"title": "2.3. Evaluation", "content": "To evaluate the rationality and naturalness of two datasets we constructed, we collected Mean Opinion Score (MOS, 1-5) ratings from 10 people. The final results are as displayed in Table. 2. VCTK-TTS was perceived to be far more natural than VCTK-Stutter (MOS of 4.13 compared to 2.22). We employ VCTK-Stutter as a baseline corpus."}, {"title": "3. End-to-End Dysfluency Detection", "content": "Dysfluency modeling is text-dependent [1]. We adopt the soft speech-text alignment from VITS [13] as input, and adopt a YOLO-like objective for the detection. We take 1D extension of 2D object detection from [14], which utilizes a region-wise prediction scheme designed to aggregate local spatial information, to develop our detector. The entire paradigm is shown in Fig. 1 and the corresponding modules are detailed in the following."}, {"title": "3.1. Soft speech-text alignments", "content": "VITS [13] takes in speech and text data to train alignment, producing a $C_{text} \\times |z|$ monotonic attention matrix A that represents how each input phoneme expands to be time-aligned with target speech, where $C_{text}$ represents the tokenized text dimension and z the temporal speech dimension. We use the soft alignments A and apply a softmax operation across the text dimension, computing the maximum attention value for each time step. We use pre-trained text and speech encoders from [13]."}, {"title": "3.2. Spatial feature aggregator", "content": "To preserve local spatial features for region-wise predictions, we use learnable spatial feature aggregator blocks. Unlike spectrograms, soft alignment data cannot be effectively processed using traditional conformer [20] methods with pointwise and depthwise convolutions, as collapsing information across either the text or speech dimension would severely corrupt the relevant signal. Our initial experiments confirm that this design fails to learn dysfluencies. To address this, we employ a series of depthwise and grouped convolution operations, enabling iterative downsampling while preserving regional spatial features."}, {"title": "3.3. Temporal dependency extractor", "content": "We found that using only spatial feature aggregator modules is insufficient for accurately detecting dysfluencies due to their reliance on local spatial information. To effectively capture the sequential nature of dysfluent speech, we employ a second-stage Transformer encoder [21] that treats each speech region as an element in a sequence, using the text dimension as the embedding dimension. This Transformer encoder consists of 8 layers with 8 attention heads per layer, enabling the extraction of longer-term temporal information crucial for dysfluency detection."}, {"title": "3.4. YOLO objective", "content": "Following YOLO [14], we use a weighted loss-based multi-task training pipeline with three separate loss values: dysfluency presence confidence loss (binary cross-entropy), start and end bound loss (mean squared error), and dysfluency type categorical loss (cross-entropy). Confidence loss is computed across all regions, categorical loss in regions with dysfluency, and bound loss on the region \"responsible\" for the dysfluency. Bound values are normalized between 0-1 using fixed padded lengths as max bound values. Loss is shown below:\n$L = \\lambda_{bound}\\sum_{i=0}^{S}1_{i}^{obj}[(b_{start} - \\hat{b}_{start})^{2} + (b_{end} - \\hat{b}_{end})^{2}]$\n$\\sum_{i=0}^{S} -conf\\hat{y_{i}} log(p(y_{i})) + (1 - \\hat{y_{i}}) \\cdot log(1 - p(y_{i}))$\n$\\sum_{i=0}^{S}\\sum_{j=0}^{n} - \\lambda_{clas} c_{en} log(p(e_{n}))$\nwhere $1_{i}^{obj}$ denotes the presence of a dysfluency in a specific region, $y_{i}$ & $\\hat{y}_{i}$ denote the predicted & target confidence values, and $c$ & $\\hat{c}$ denote the predictions & targets for n classes. Loss is computed and averaged across S regions within a single sample. We scale the bound loss term by a factor of 5 and the classification term by a factor of 0.5."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Datasets", "content": "(1) VCTK [12] includes 109 native English speakers with accented speech. It is used in both VCTK-Stutter and VCTK-TTS.\n(2) LibriStutter [15] contains artificially stuttered speech and stutter classification labels for 5 stutter types. It was generated using 20 hours of audio selected from [22]. (3) Aphasia Speech is collected from our clinical collaborators, our dysfluent data comprises 38 participants diagnosed with Primary Progressive Aphasia (PPA), larger than the data used in [1,2] which only has 3 speakers. People were asked to read grandfather passage, leading about 1 hour of speech. (4) UCLASS [23] contains recordings from 128 children and adults who stutter. Only 25 files have been annotated and did not annotate for the block class, we only used those files and did not use the block class for subsequent datasets. (5) SEP-28K is curated by [24], contains 28,177 clips extracted from publicly available podcasts. We removed files where the annotations were labelled as \"unsure\"."}, {"title": "4.2. Training", "content": "The detector is trained with a randomized 90/10 train/test split on both VCTK-Stutter and VCTK-TTS, with a batch size of 64. We utilize the Adam optimization method with beta values (0.9, 0.999) and learning rate of 3e-4, opting not to use dropout or weight-decay in our training process. For VCTK-Stutter, the model is trained for 20 epochs and total of 39 hours on a RTX A6000. Meanwhile, for VCTK-TTS, the model is trained for 30 epochs and total of 70 hours on the RTX A6000."}, {"title": "4.3. Evaluation Metrics", "content": "(1) Accuracy is the accuracy of correct predictions of stutter types in regions that contain a dysfluency. The confidence prediction accuracy is defined as the number of correct predictions of dysfluent and fluent regions over all the regions. (2) Bound loss is the mean squared loss between the predicted bounds and the actual bounds of the dysfluent regions, normalized to the 1024-length padded spectrogram and converted to a time scale using a known 20ms sampling frequency. (3) Time F1 [1] measures accuracy in bounds prediction, calculating based on the intersection between predicted and actual bounds. Once there is an overlap, the sample is considered as a True Positive sample."}, {"title": "4.4. Validation", "content": "To assess the performance of trained detector, we conduct evaluations on the VCTK++ and VCTK-TTS testsets, as well as on the PPA data. The evaluation results, including type-specific detection accuracy and bound loss metrics, are presented in Table 3 for the training datasets and in Table 5 for the PPA data. To compare the performance with previous works, we also validated the model on UCLASS, Libristutter and SEP-28K, calculating type-specific accuracy as well as Time F1."}, {"title": "4.5. Results and Discussions", "content": "We conducted inference on our proposed VCTK-Stutter and VCTK-TTS test sets, using H-UDM as baseline. As indicated in Table 3, both YOLO-Stutter (VCTK-Stutter) and YOLO-Stutter (VCTK-TTS) surpassed H-UDM across all metrics. Notably, the results on VCTK-TTS from YOLO-Stutter (VCTK-TTS) were particularly promising. VCTK-TTS closely mimics human speech, affirming the potential of YOLO-Stutter (VCTK-TTS) to deliver substantial performance on actual human speech. Furthermore, we presented our findings on UCLASS, LibriStutter, and SEP-28K, as shown in Table 4. Given that the test sets from the original benchmarks are private, direct accuracy comparisons may not be entirely fair. Focusing more on time-aware detection, we reported the Time F1 score for each dataset, where all baselines, except for H-UDM, scored 0. Both of our proposed methods consistently outperformed H-UDM. Lastly, we evaluated our methods on actual PPA speech, with results detailed in Table 5. Both models consistently exceeded H-UDM's performance, with VCTK-TTS achieving additional gains owing to its higher naturalness, as reflected in Table 2. However, the average accuracy remains low, highlighting the challenge of perfectly capturing the real distribution of dysfluency."}, {"title": "4.6. Ablation experiments", "content": "To investigate the impact of the proportions of different dysfluency types quantities on training results, we selected three different proportions except for average on VCTK-TTS, as follows: P = [Rep, Block, Miss, Replace, Prolong], P\u2081=[0.9: 1:1 : 1: 1], P2= [1:1:1.2: 1: 1], P3=[1 : 1 : 1.2: 1.2: 1]. Table 6 shows validated type-specific accuracy on VCTK-TTS inference testsets, from which we can see that although the proportions were adjusted, the type accuracy for repetition and block has remained relatively high. The accuracy for missing increases with its sample proportion, but does not improve as the proportions of other samples (repetition) decrease. Increasing missing and replace proportions improves missing type accuracy but lowers replacement accuracy."}, {"title": "5. Conclusion and Limitations", "content": "We introduce YOLO-Stutter, the first end-to-end dysfluency detection paradigm, achieving state-of-the-art performance with fewer trainable parameters compared to rule-based systems. We contribute two simulated dysfluent corpora, VCTK-Stutter and VCTK-TTS, with high-quality annotations covering sound repetition, replacement, insertion, deletion, and block. However, limitations persist. First, average accuracy on aphasia speech inference remains limited, indicating a significant gap between simulated and real dysfluency. Second, VCTK-TTS still produces robotic noise and discontinuous sounds, warranting exploration of adversarial methods to address these issues. Third, it is worth to explore simulation in articulatory space [26-30]. Lastly, dysfluency is a clinical problem and is speaker dependent. Disentangled analysis and synthesis [31-35] should be leveraged to tackle dysfluency in speaker-free space."}]}