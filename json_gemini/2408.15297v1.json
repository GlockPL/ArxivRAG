{"title": "YOLO-Stutter: End-to-end Region-Wise Speech Dysfluency Detection", "authors": ["Xuanru Zhou", "Anshul Kashyap", "Steve Li", "Ayati Sharma", "Brittany Morin", "David Baquirin", "Jet Vonk", "Zoe Ezzes", "Zachary Miller", "Maria Tempini", "Jiachen Lian", "Gopala Anumanchipalli"], "abstract": "Dysfluent speech detection is the bottleneck for disordered\nspeech analysis and spoken language learning. Current state-\nof-the-art models are governed by rule-based systems [1, 2]\nwhich lack efficiency and robustness, and are sensitive to tem-\nplate design. In this paper, we propose YOLO-Stutter: a\nfirst end-to-end method that detects dysfluencies in a time-\naccurate manner. YOLO-Stutter takes imperfect speech-text\nalignment as input, followed by a spatial feature aggregator,\nand a temporal dependency extractor to perform region-wise\nboundary and class predictions. We also introduce two dys-\nfluency corpus, VCTK-Stutter and VCTK-TTS, that simulate\nnatural spoken dysfluencies including repetition, block, miss-\ning, replacement, and prolongation. Our end-to-end method\nachieves state-of-the-art performance with a minimum num-\nber of trainable parameters for on both simulated data and\nreal aphasia speech. Code and datasets are open-sourced at\nhttps://github.com/rorizzz/YOLO-Stutter\nIndex Terms: dysfluency, end-to-end, simulation, clinical", "sections": [{"title": "1. Introduction", "content": "Speech dysfluencies are defined as any form of speech that con-\ntains sound repetition, deletion, insertion, replacement, prolon-\ngation, block, etc. [1-4]. Dysfluency modeling holds significant\npromise in disordered speech screening [5, 6], and has a huge\nimpact on the language learning market [7]. Dysfluency model-\ning is a speech transcription problem, which seems to be well\ntackled by recent large-scale developments [8-11]. However,\nlarge ASR models struggle with dysfluent speech for two rea-\nsons: (1) they transcribe speech into words or phonemes that lack\ndysfluency-related information, and (2) their strong language\nmodels often interpret dysfluent speech as fluent, e.g., transcrib-\ning \"P-P-Please call Stella\" as \"Please call Stella\" or \"Praise call\nStella\". Progress in dysfluency modeling has been limited due to\nthe lack of a well-defined problem formulation and high-quality\nannotated dysfluency data.\nUDM [1] first formally defines dysfluency modeling as the\ndetection of all types of dysfluencies at both word and phoneme\nlevels, with accurate time region prediction. UDM transcribes\ndysfluent speech into unconstrained forced alignment and adopts\n2D-Alignment for detection task. H-UDM [2] developed a re-\ncursive 2D alignment algorithm that further boosts performance.\nThese methods represent the de-facto pipeline at the time.\nIn this work, we tackle dysfluency modeling from a totally\ndifferent perspective. Still following the dysfluency modeling\ncriterion [1], we develop an end-to-end model which directly\npredicts dysfluencies and time regions from dysfluent speech\nand reference text input without any handcraft templates. To\ncreate training data, we introduce VCTK-TTS (7X larger than\nVCTK [12]), a synthetic dysfluency dataset created using the\nVITS [13], including repetition, missing, block, replacement,\nand prolongation at both the phoneme & word levels. VCTK-\nTTS offers a more natural representation of speech dysfluencies\ncompared to VCTK++ [1], and the creation process is automated.\nIn addition, we extend VCTK++ by incorporating word-level\ndysfluency and obtain a new dataset named VCTK-Stutter (5X\nlarger than VCTK), thus achieving word-phoneme detection.\nOur newly proposed datasets have the potential to set a stan-\ndard benchmark for studies in this field. For the dysfluency\ndetection task, we drew inspiration from the YOLO [14] and de-\nvised a region-wise prediction scheme that captures both spatial\nand temporal information. We developed YOLO-Stutter, which\ntakes soft speech-text alignments [13] as input, followed by a\nspatial feature aggregator and a temporal dependency extrac-\ntor to directly predict dysfluency types and time regions. We\nalso collaborated with clinical partners and obtained data from\n38 Aphasia subjects. Results on simulated speech, public cor-\npora, and Aphasia speech indicate that YOLO-Stutter achieves\nstate-of-the-art performance even with a minimum number of\ntrainable parameters."}, {"title": "2. Naturalistic Dysfluency Simulation", "content": "End-to-end modeling of speech dysfluency requires high-quality\nannotations. The prevalent approach is through simulation. Cur-\nrently, existing simulated datasets [1, 15, 16] either do not in-\ncorporate time steps and broad coverage of dysfluencies or are\nnot naturalistic. To develop a comprehensive and naturalistic\nsimulated dysfluency corpus, we have taken two steps:\n\u2022 We extended VCTK++ [1] by introducing rule-based word-\nlevel dysfluencies in acoustic space, creating a new dataset\nnamed VCTK-stutter [12]).\n\u2022 We designed TTS rules and injected phoneme and word dys-\nfluencies in text space. VITS [13] was used to generate natu-\nralistic dysfluent speech. This dataset is named VCTK-TTS."}, {"title": "2.1. VCTK-Stutter", "content": "VCTK++ [1] was created in the acoustic space by editing au-\ndio based on forced alignment. However, it only focuses on\nphoneme-level dysfluencies. We extended VCTK++ by incorpo-\nrating word-level dysfluencies (named VCTK-Stutter). Similar\nto obtaining phoneme-level alignments through MFA [17], in\nVCTK++ we employ WhisperX [18] to get word-level align-\nments, enabling us to establish precise insertion points for dys-\nfluencies. Utilizing the word alignments generated by WhisperX,\nwe annotate specific regions where artificial dysfluencies are to\nbe introduced with the type of word-level dysfluency and the\nstart and end times. We enable each word to have an equal"}, {"title": "2.2. VCTK-TTS", "content": "Traditional rule-based simulation methods [1, 15, 16] operate in\nacoustic space, and the generated samples are not naturalistic.\nWe developed a new pipeline that simulates in text space. To\nachieve this, we first convert a sentence into an IPA phoneme\nsequence. Then, we develop TTS rules for phoneme editing to\nsimulate dysfluency. These rules are applied to the entire VCTK\ndataset [13], allowing the voice of generated speech to vary from\nthe 109 speakers included in the VCTK, thus enhancing the\nscalability of the dataset. We call this VITS-based Simulation.\nThe entire pipeline is detailed in Sec.2.2.1, and TTS rules are\ndiscussed in Sec.2.2.2. Statistics are listed in Table. 1. Overall\nVCTK-Stutter/VCTK-TTS are 5X/7X larger than VCTK."}, {"title": "2.2.1. Method pipeline", "content": "VCTK-TTS pipelines can be divided into following steps: (i)\nDysfluency injection: We first convert ground truth reference\ntext of VCTK text [12] into IPA sequences via the VITS phonem-\nizer [13], then add different types of dysfluencies at the phoneme\nlevel according to the TTS rules. (ii) VITS inference: We take\ndysfluency-injected IPA sequences as inputs, conduct the VITS\ninference procedure and obtain the dysfluent speech. (iii) An-\nnotation: We retrieve phoneme alignments from VITS duration\nmodel, annotate the type of dysfluency on the dysfluent region."}, {"title": "2.2.2. TTS rules", "content": "We inject dysfluency in the text space following these rules:\n\u2022 Repetition (phoneme&word): The first phoneme or syllable\nof a randomly picked word was repeated 2-4 times, with\npauselengths varying between 0.5 to 2.0 seconds.\n\u2022 Missing (phoneme&word): We simulated two phonological\nprocesses that characterize disordered speech [?] - weak sylla-\nble deletion (deletion of a random unstressed syllable based\non stress markers\u00b9) and final consonant deletion.\n\u2022 Block: A duration of silence between 0.5-2.0 seconds was in-\nserted after a randomly chosen word in between the sentence.\n\u2022 Replacement (phoneme): We simulated fronting, stopping,\ngliding, deaffrication - processes that characterize disordered\nspeech [19] - by replacing a random phoneme with one that\nwould mimic the phonological processes mentioned above.\n\u2022 Prolongation (phoneme): The duration of a randomly se-\nlected phoneme in the utterance was extended by a factor\nrandomly chosen between 10 to 15 times its original length,\nas determined by the duration model."}, {"title": "2.3. Evaluation", "content": "To evaluate the rationality and naturalness of two datasets we\nconstructed, we collected Mean Opinion Score (MOS, 1-5) rat-\nings from 10 people. The final results are as displayed in Ta-\nble. 2. VCTK-TTS was perceived to be far more natural than\nVCTK-Stutter (MOS of 4.13 compared to 2.22). We employ\nVCTK-Stutter as a baseline corpus."}, {"title": "3. End-to-End Dysfluency Detection", "content": "Dysfluency modeling is text-dependent [1]. We adopt the soft\nspeech-text alignment from VITS [13] as input, and adopt a\nYOLO-like objective for the detection. We take 1D extension of\n2D object detection from [14], which utilizes a region-wise pre-\ndiction scheme designed to aggregate local spatial information,\nto develop our detector. The entire paradigm is shown in Fig. 1\nand the corresponding modules are detailed in the following."}, {"title": "3.1. Soft speech-text alignments", "content": "VITS [13] takes in speech and text data to train alignment, pro-\nducing a $C_{text} \\times |z|$ monotonic attention matrix $A$ that rep-\nresents how each input phoneme expands to be time-aligned\nwith target speech, where $C_{text}$ represents the tokenized text\ndimension and $z$ the temporal speech dimension. We use the soft\nalignments $A$ and apply a softmax operation across the text di-\nmension, computing the maximum attention value for each time\nstep. We use pre-trained text and speech encoders from [13]."}, {"title": "3.2. Spatial feature aggregator", "content": "To preserve local spatial features for region-wise predictions,\nwe use learnable spatial feature aggregator blocks. Unlike spec-\ntrograms, soft alignment data cannot be effectively processed\nusing traditional conformer [20] methods with pointwise and\ndepthwise convolutions, as collapsing information across either\nthe text or speech dimension would severely corrupt the relevant\nsignal. Our initial experiments confirm that this design fails\nto learn dysfluencies. To address this, we employ a series of\ndepthwise and grouped convolution operations, enabling iterative\ndownsampling while preserving regional spatial features."}, {"title": "3.3. Temporal dependency extractor", "content": "We found that using only spatial feature aggregator modules\nis insufficient for accurately detecting dysfluencies due to their\nreliance on local spatial information. To effectively capture the\nsequential nature of dysfluent speech, we employ a second-stage\nTransformer encoder [21] that treats each speech region as an\nelement in a sequence, using the text dimension as the embedding\ndimension. This Transformer encoder consists of 8 layers with 8\nattention heads per layer, enabling the extraction of longer-term\ntemporal information crucial for dysfluency detection."}, {"title": "3.4. YOLO objective", "content": "Following YOLO [14], we use a weighted loss-based multi-task\ntraining pipeline with three separate loss values: dysfluency\npresence confidence loss (binary cross-entropy), start and end\nbound loss (mean squared error), and dysfluency type categorical\nloss (cross-entropy). Confidence loss is computed across all\nregions, categorical loss in regions with dysfluency, and bound\nloss on the region \"responsible\" for the dysfluency. Bound values\nare normalized between 0-1 using fixed padded lengths as max\nbound values. Loss is shown below:\n$L = \\lambda_{bound}\\sum_{i=0}^{S}1_{i}^{obj}[(b_{start} - \\hat{b}_{start})^2 + (b_{end} - \\hat{b}_{end})^2]$\\n$\\- \\sum_{i=0}^{S}[\\hat{y_{i}} log(p(y_{i})) + (1 - \\hat{y_{i}}) \\cdot log(1 - p(y_{i}))]$\\n$\\- \\lambda_{clas} \\sum_{i=0}^{S} \\sum_{j=0}^{n}c_{i}log(p(c_{i}))$\nwhere $1_{i}^{obj}$ denotes the presence of a dysfluency in a specific\nregion, $y_{i}$ & $\\hat{y_{i}}$ denote the predicted & target confidence values,\nand $c$ & $\\hat{c}$ denote the predictions & targets for n classes. Loss\nis computed and averaged across S regions within a single\nsample. We scale the bound loss term by a factor of 5 and the\nclassification term by a factor of 0.5."}, {"title": "4. Experiments", "content": "4.1. Datasets\n(1) VCTK [12] includes 109 native English speakers with ac-\ncented speech. It is used in both VCTK-Stutter and VCTK-TTS.\n(2) LibriStutter [15] contains artificially stuttered speech and\nstutter classification labels for 5 stutter types. It was generated\nusing 20 hours of audio selected from [22]. (3) Aphasia Speech\nis collected from our clinical collaborators, our dysfluent data\ncomprises 38 participants diagnosed with Primary Progressive\nAphasia (PPA), larger than the data used in [1,2] which only\nhas 3 speakers. People were asked to read grandfather passage,\nleading about 1 hour of speech. (4) UCLASS [23] contains\nrecordings from 128 children and adults who stutter. Only 25\nfiles have been annotated and did not annotate for the block class,\nwe only used those files and did not use the block class for subse-\nquent datasets. (5) SEP-28K is curated by [24], contains 28,177\nclips extracted from publicly available podcasts. We removed\nfiles where the annotations were labelled as \"unsure\"."}, {"title": "4.2. Training", "content": "The detector is trained with a randomized 90/10 train/test split\non both VCTK-Stutter and VCTK-TTS, with a batch size of 64.\nWe utilize the Adam optimization method with beta values (0.9,\n0.999) and learning rate of 3e-4, opting not to use dropout or\nweight-decay in our training process. For VCTK-Stutter, the\nmodel is trained for 20 epochs and total of 39 hours on a RTX\nA6000. Meanwhile, for VCTK-TTS, the model is trained for 30\nepochs and total of 70 hours on the RTX A6000."}, {"title": "4.3. Evaluation Metrics", "content": "(1) Accuracy is the accuracy of correct predictions of stutter\ntypes in regions that contain a dysfluency. The confidence pre-\ndiction accuracy is defined as the number of correct predictions\nof dysfluent and fluent regions over all the regions. (2) Bound\nloss is the mean squared loss between the predicted bounds and\nthe actual bounds of the dysfluent regions, normalized to the"}, {"title": "4.4. Validation", "content": "To assess the performance of trained detector, we conduct evalu-\nations on the VCTK++ and VCTK-TTS testsets, as well as on\nthe PPA data. The evaluation results, including type-specific\ndetection accuracy and bound loss metrics, are presented in Ta-\nble 3 for the training datasets and in Table 5 for the PPA data. To\ncompare the performance with previous works, we also validated\nthe model on UCLASS, Libristutter and SEP-28K, calculating\ntype-specific accuracy as well as Time F1."}, {"title": "4.5. Results and Discussions", "content": "We conducted inference on our proposed VCTK-Stutter and\nVCTK-TTS test sets, using H-UDM as baseline. As indicated in\nTable 3, both YOLO-Stutter (VCTK-Stutter) and YOLO-Stutter\n(VCTK-TTS) surpassed H-UDM across all metrics. Notably,\nthe results on VCTK-TTS from YOLO-Stutter (VCTK-TTS)\nwere particularly promising. VCTK-TTS closely mimics human\nspeech, affirming the potential of YOLO-Stutter (VCTK-TTS)\nto deliver substantial performance on actual human speech. Fur-\nthermore, we presented our findings on UCLASS, LibriStutter,\nand SEP-28K, as shown in Table 4. Given that the test sets from\nthe original benchmarks are private, direct accuracy comparisons\nmay not be entirely fair. Focusing more on time-aware detec-\ntion, we reported the Time F1 score for each dataset, where all"}, {"title": "4.6. Ablation experiments", "content": "To investigate the impact of the proportions of different dys-\nfluency types quantities on training results, we selected three\ndifferent proportions except for average on VCTK-TTS, as fol-\nlows: P = [Rep, Block, Miss, Replace, Prolong], P\u2081=[0.9: 1:1\n: 1: 1], P2= [1:1:1.2: 1: 1], P3=[1 : 1 : 1.2: 1.2: 1]. Table 6\nshows validated type-specific accuracy on VCTK-TTS inference\ntestsets, from which we can see that although the proportions\nwere adjusted, the type accuracy for repetition and block has re-\nmained relatively high. The accuracy for missing increases with\nits sample proportion, but does not improve as the proportions\nof other samples (repetition) decrease. Increasing missing and\nreplace proportions improves missing type accuracy but lowers\nreplacement accuracy."}, {"title": "5. Conclusion and Limitations", "content": "We introduce YOLO-Stutter, the first end-to-end dysfluency de-\ntection paradigm, achieving state-of-the-art performance with\nfewer trainable parameters compared to rule-based systems. We\ncontribute two simulated dysfluent corpora, VCTK-Stutter and\nVCTK-TTS, with high-quality annotations covering sound rep-\netition, replacement, insertion, deletion, and block. However,\nlimitations persist. First, average accuracy on aphasia speech\ninference remains limited, indicating a significant gap between\nsimulated and real dysfluency. Second, VCTK-TTS still pro-\nduces robotic noise and discontinuous sounds, warranting ex-\nploration of adversarial methods to address these issues. Third,\nit is worth to explore simulation in articulatory space [26-30].\nLastly, dysfluency is a clinical problem and is speaker dependent.\nDisentangled analysis and synthesis [31-35] should be leveraged\nto tackle dysfluency in speaker-free space."}]}