{"title": "Can We Trust Al Benchmarks?", "authors": ["MARIA ERIKSSON", "ERASMO PURIFICATO", "ARMAN NOROOZIAN", "JO\u00c3O VINAGRE", "GUILLAUME CHASLOT", "EMILIA GOMEZ", "DAVID FERNANDEZ-LLORCA"], "abstract": "Quantitative Artificial Intelligence (AI) Benchmarks have emerged as fundamental tools for evaluating the performance, capability, and safety of Al models and systems. Currently, they shape the direction of AI development and are playing an increasingly promi-nent role in regulatory frameworks. As their influence grows, however, so too does concerns about how and with what effects they evaluate highly sensitive topics such as capabilities, including high-impact capabilities, safety and systemic risks. This paper presents an interdisciplinary meta-review of about 100 studies that discuss shortcomings in quantitative benchmarking practices, published in the last 10 years. It brings together many fine-grained issues in the design and application of benchmarks (such as biases in dataset creation, inadequate documentation, data contamination, and failures to distinguish signal from noise) with broader sociotechnical issues (such as an over-focus on evaluating text-based Al models according to one-time testing logic that fails to account for how AI models are increasingly multimodal and interact with humans and other technical systems). Our review also highlights a series of systemic flaws in current benchmarking practices, such as misaligned incentives, construct validity issues, unknown unknowns, and problems with the gaming of benchmark results. Furthermore, it underscores how benchmark practices are fundamentally shaped by cultural, commercial and competitive dynamics that often prioritise state-of-the-art performance at the expense of broader societal concerns. By providing an overview of risks associated with existing benchmarking procedures, we problematise disproportionate trust placed in benchmarks and contribute to ongoing efforts to improve the accountability and relevance of quantitative Al bench-marks within the complexities of real-world scenarios.", "sections": [{"title": "1 INTRODUCTION", "content": "Quantitative artificial intelligence (AI) benchmarks (i.e., combinations of test datasets and performance metrics that are taken to represent general or specific tasks and used to compare AI model capabilities and/or risks [87]) play a central role in the release and marketing of newly developed AI tools. Together with qualitative evaluation methods (such as red teaming and peer confrontations [20]), quantitative benchmarks - hereafter referred to as Al benchmarks, or simply benchmarks - are generally seen as providing crucial feedback signals on the performance and capabilities of AI. Indeed, they have become so critical to AI development that businesses go to great lengths to achieve good benchmarking scores, with market players like OpenAI being estimated to have spent hundreds of thousands of dollars on compute to obtain a high score at the ARC-AGI benchmark [84].\nIncreasingly, AI benchmarks are also used in regulatory contexts, where the goal is to assess potential societal harms posed by AI models and systems. The most notable case is the EU AI Act [30], which incorporates benchmarks in several key provisions. For instance, for high-risk AI systems, benchmarks are expected to play a significant role in complying with requirements on accuracy, robustness, and cybersecurity (Art. 15(2)), and their development will be facilitated through Al regulatory sandboxes (Art. 58 (2)). They will also be of fundamental importance for classifying general-purpose AI (GPAI) models with systemic risks and assessing high-impact capabilities (Art. 51(1) and Annex XIII). Furthermore, both the Board and the Scientific Panel are expected to contribute to the development of bench-marks (Art. 66(g) and Art. 68(3)). Additionally, the first and second drafts of the Code of Practice specifically mentions benchmarks as an example of best-in-class evaluations for risk assessment measures for providers of GPAI models with systemic risks [29, 31]. In the US, benchmarks are also relevant in the recently revoked AI Executive Order [108] and the AI Diffusion Framework [113]. AI benchmarks can further be expected to play a central role in the imple-mentation of legislations such as the EU Digital Services Act (DSA) [28] and the UK's Online Safety Act [112], which require the largest online platforms and search engines - entities that increasingly rely on Al to curate, filter, and/or rank content to millions of users - to perform regular algorithmic audits showing that their systems are safe, fair, and comply with fundamental human rights. In short, AI benchmarks - which constitute a highly heterogenous and far from standardised set of techniques - are increasingly at the heart of policy efforts to make Al more transparent and secure.\nAt the same time however, as benchmarks are increasingly relied upon to provide AI safety assurances, researchers in a broad range of academic fields have raised serious concerns regarding their current use. This includes critical voices being raised in fields ranging from cybersecurity [64], linguistics [15], and computer science [35], to sociol-ogy [26], economics [27], philosophy [52], ethnography [49], and science and technology studies [49]. Such scholars have for example described current Al evaluation practices as a \"minefield\" [70], that raise serious ethical concerns regarding what should be measured, according to what standards, and with what downstream effects [13]. They have also emphasised that benchmarks are deeply political, performative, and generative in the sense that they do not pas-sively describe and measure how things are in the world, but actively take part in shaping it [38]. This happens as benchmarks continuously influence how Al models are trained, fine-tuned, and applied - practices with wide-ranging political, economic, and cultural effects."}, {"title": "2 BACKGROUND", "content": "Etymologically, the term \"benchmark\" has its roots in land surveying, where a physical mark (known as a 'bench mark') was used as a reference point for measuring elevations. This mark typically consisted of a horizontal groove in a surface, which supported a level surface or 'bench' for a levelling rod. Over time, the term evolved to encompass a broader meaning, referring to any standard or reference point used for comparison or evaluation [80]. Benchmarking is currently applied across many different domains such as bioinformatics [4], environmental quality [41], information retrieval [107], transistor hardware [19], industry and business [17], and within the public sector [16], where it often refers to procedures for comparing the performance or best practices of different companies or processes. Here, we focus on the use of benchmark tests within computing, where they are used to evaluate the performance of hardware or software systems by comparing them to a standard or reference point [41]. More specifically, we zoom in on Al development, where benchmarks are often used to facilitate cross-model comparisons, measure performance, track model progress, and identify weaknesses [90]. Benchmarks are often perceived as comparably cost- and time-effective tools for Al providers who may run them regularly throughout the model development to obtain frequent signals of Al model performance and capabilities [115]. They can be applied to both software and hardware solutions, where the latter for instance evaluates the performance of CPUs, GPUs, TPUs [63], or hardware accelerators [62]. We address"}, {"title": "3 RELATED WORK", "content": "Over the years, several surveys and meta-reviews have set out to summarize discussions on limitations in Al bench-marking and this paper represents a continuation of such efforts. In 2021, Liao et al. [56, p. 1] identified a wide range of \"surprisingly consistent critique\" directed at benchmarking practices across fields such as computer vision, natu-ral language processing, recommender systems, reinforcement learning, graph processing, metric learning, and more. The authors present a taxonomy of observed benchmark failure modes, which for example includes implementation variations (due to benchmark algorithms, metrics, and libraries being applied in slightly different ways), errors in test set construction (as a result of label errors, label leakage, test set size, or contaminated data), overfitting from test set reuse, and comparisons to inadequate baselines. Along similar lines, Hutchinson et al. [43] survey discussions on evaluation practices in machine learning, particularly in the fields of computer vision and natural language process-ing. The authors identify eight key evaluation gaps, including topics such as \"neglect of model interpretability\" and \"oversimplification of knowledge\" and argue for a shift toward application-centric evaluations that account for safety, fairness, and ecological validity. Similar points are also raised by Gehrmann et al. [33]'s most recent meta-review, which categorizes issues with text-oriented benchmarks identified over the past two decades, and propose a long-term vision for improving evaluation practices, emphasizing the need for comprehensive evaluation reports that include multiple datasets, metrics, and human assessments. Their key recommendations include focusing on model limita-tions, enhancing dataset documentation, and adopting a more nuanced approach to evaluation to better characterize model performance and capabilities.\nOur review revisits these areas of critique and traces how debates concerning benchmarks have evolved in the last few years. This is not least necessary due to rapid developments currently seen in the field. Recent research has"}, {"title": "4 METHODOLOGY", "content": "Since benchmarks constitute a key element in most papers that discuss or demonstrate the capabilities of AI models, it is challenging to identify previous works that explicitly and thoroughly address benchmark critique, as opposed to applying benchmarks or proposing new methods. When gathering source materials for this review (and instead of conducting keyword searches in research databases) we therefore opted for a method that involved starting from a selection of well-cited papers that problematise how, when, and where benchmarks are used. For each article, we surveyed its reference list and studied how the paper has been cited since its publication. This allowed us to identify additional relevant literature and expand our corpus successively.\nIn the process, we surveyed a wide range of papers that, for instance, highlight general problems with the produc-tion of AI datasets [67, 76], propose alternative ways of designing benchmark leaderboards [57, 92], or discuss the wider use of proxies in tests and evaluations [61, 69, 85]. Such papers provide important contextual insights to discus-sions concerning benchmarks. However, they were omitted from what we considered our core collection of previous research. This collection ended up consisting of about 100 papers and articles that explicitly and primarily highlight issues with benchmarks. Notably, articles that propose new benchmarks were not added to this collection by default, even though such articles naturally contain some level of benchmark critique. The reasoning behind this is twofold. On the one hand, it was necessary to limit the size and scope of our review. On the other hand, many articles propos-ing new benchmarks (if not most of them) do not question or discuss many (if any) of the underlying assumptions that benchmark assessments rely on. In that sense, they reproduce the general notion that quantitative benchmarks provide a reasonable technical \"fix\" to issues with Al safety and capability assessments, rather than taking part in more fundamental and critical discussions on current limitations in benchmark use and design. For these reasons, they were also excluded from our core collection of benchmark critique.\nOur collection was further limited by only containing articles published between 1st January 2014 and 31st December 2024. While it is true that AI models have been tested and evaluated since their origins in the mid 1950's (with the Turing test serving as an iconic example), we limit our review to this decade long period, since 2014 marks the starting point for recent intensifications in AI research and development. We consider both published and pre-published papers in our"}, {"title": "5 NINE REASONS TO BE CAUTIOUS WITH BENCHMARKS", "content": "In the sections below, we summarise the main issues that were identified during the course of our research. Importantly, these issues presented here as a taxonomy - are not arranged according to their importance or urgency. They are also not meant to be understood as isolated issues, but rather deeply interlinked problems. Indeed, this complexity and interdependence is precisely what makes Al evaluations challenging."}, {"title": "5.1 Problems with Data Collection, Annotation, and Documentation", "content": "An initial set of issues with AI benchmarks found during our research is limitations in the collection, annotation, and documentation of benchmark datasets. This ties into a broader critique regarding insufficient documentation in Al research which is central to calls for more transparent and trustworthy algorithmic systems [32, 67, 76, 95, 100]. Research has found that it is often difficult to trace precisely how, when, and by whom benchmark datasets have been made [24, 90] which compromises the ability for benchmarks to be used in robust and generalisable ways [6]. The issue has partly been linked to the low status of dataset-related work within the machine learning community (which instead privileges model development) [77, 94], and the fact that AI datasets are often \"reduced, reused, and recycled\" [50], which complicates documentations of their possible limitations [82, 109]. Notably, Koch et al. [50] have found that more than 70 percent of the benchmark datasets used in prominent computer vision papers had been reused from other domains. Park and Jeoung [82] also find that while benchmark datasets often contain ample information on how to use the dataset, documentation is often missing concerning their shortcomings and social impact. In their exploration of benchmark sharing platforms like HuggingFace\u00b9 and PapersWithCode\u00b2 the same researchers also note that confusing metadata terminology severely complicates efforts to understand benchmark dataset documentation.\nLike AI training datasets, benchmarks have been singled out for raising ethical and legal questions concerning copyrights, privacy, informed consent, and rights to opt-out [83]. Many benchmarks rely on crowd-sourced or user-generated content from platforms like Wikihow\u00b3, Reddit, or trivia websites [38, 48], whose annotation may be noisy and biased, lack input from expertise in specialised fields, or be produced under exploitative conditions [5, 99, 111]."}, {"title": "5.2 Weak Construct Validity and Epistemological Claims", "content": "Another genre of benchmark critique focuses on the epistemological claims that tend to surround benchmarks and examines the limits of what can be known through quantitative AI tests. A central reference point in these discussions is the observation by Raji et al. [87] that many benchmarks suffer from construct validity issues in the sense that they do not measure what they claim to measure. As the authors proclaim, this is especially troublesome when benchmarks promise to measure universal or general capabilities, since this vastly misrepresents their actual capability. As a result, the authors argue that framing an Al benchmark dataset as general purpose \"is ultimately dangerous and deceptive, resulting in misguidance on task design and focus, underreporting of the many biases and subjective interpretations inherent in the data as well as enabling, through false presentations of performance, potential model misuse\". [87, p. 5]. At the heart of this critique lies the realization that many benchmarks do not have a clear definition of what they claim to measure, which makes it impossible to measure if they succeed in the task or not [8, 14]. In a close analysis of four benchmarks used to evaluate fairness in natural language processing (StereoSet, CrowS-Pairs, WinoBias, and Wino-Gender), Blodgett et al. [14] for example found that all benchmarks revealed severe weaknesses in terms of defining what is being measured. For instance, culturally complex and highly contested concepts like \"stereotypes\" or \"offensive language\" were left unspecified, causing a series of logical failures and interpretational conflicts. Elsewhere, research has shown strong disagreements in how benchmark tasks are conceptualised and operationalised [106], and found that benchmarks are applied in highly idiosyncratic ways [93]. Frequently, the difficulty defining what benchmarks eval-uate persist since there is no clear, stable and absolute ground truth for what is claimed to be measured [71]. Since concepts like \"bias\" and \"fairness\" are inherently contested, messy, and shifting, benchmarks that promise to measure such terms will inevitably suffer from an \"abstraction error\" that produces a false sense of certainty [98, p. 63].\nIt has also been pointed out that many benchmarks datasets are inadequate and/or unuseful proxys for what they are meant to evaluate. For instance, researchers have identified a slippage in distinguishing between algorithmic \"harms\" and algorithmic \"wrongs\" when evaluating the capabilities of Al models - two not necessarily overlapping concepts [25]. Others have questioned whether the content of benchmark datasets are reasonable substitutes for the \"real world\""}, {"title": "5.3 Sociocultural Context and Gap", "content": "Another key insight from previous research concerns the importance of the social, economic and cultural contexts where Al benchmarks are created, used, and maintained. Among researchers engaging in benchmark critique, we identify a strong consensus regarding the need to recognise that benchmarks are ultimately \"normative instruments that perpetuate particular epistemological perspectives about how the world is ordered\" [78, p. 1877]. Qualitative research has also examined the cultural and social environments where benchmarks are made, finding that they are deeply shaped by shared and arbitrary assumptions, commitments, and dependencies [26, 65, 77, 83, 94, 95]. Such assumptions for example include valuing \"efficiency at the expense of care; universality at the expense of contextuality; impartiality at the expense of positionality; and model work at the expense of data work\" [95], or reproduce contested ideas such as the notion that low-quality data can be drowned out by scale [77]. Recent research has also illustrated that Al safety research and benchmark competitions are increasingly informed by political movements and ideologies such as longtermism and effective altruism [2]\nScholars have further identified sociotechnical gaps and a lack of consideration for downstream utility as a concern in Al benchmarking [43]. Liao and Xiao [55] highlight that this means it is often unclear who is meant to care about benchmark evaluation results and how they should be used in practice. For instance, a recent study by Blagec et al. [12], which compared the explicitly stated needs for AI technologies among clinical medical practitioners with existing clinical benchmark datasets, found that most benchmarks failed to answer to the needs of medical experts. They also found that benchmarks for the most urgently requested medical or clinical tasks were completely missing and noted that similar misalignments likely exist in many other fields. For example, Jannach and Bauer [44, p. 79] highlight a lack of attention to how recommender systems \"may create value; how they, positively or negatively, impact consumers, businesses, and the society; and how we can measure the resulting effects\". Ethayarajh and Jurafsky [27] further argue that failures to consider the practical utility of benchmarks has for example made it possible to ignore the discrimina-tory and environmental damages of AI technologies and allowed for highly energy-inefficient and deeply biased AI models to reach the top of most benchmark leaderboards."}, {"title": "5.4 Narrow Benchmark Diversity and Scope", "content": "Previous research has further found that current benchmarking practices suffer from diversity issues, a problem that is also found within the broader Al ecosystem [36]. On the one hand, a vast majority of benchmarks focus on text, while other modalities (audio, images, video, and multimodal systems) remain largely unexamined [88, 93, 115]. This concentration is problematic since Al models are increasingly multimodal in scope. Guldimann et al. [39] also find that benchmarks addressing user privacy, copyright infringement, and interpretability are currently lacking and in-comprehensive, while benchmarks in safety areas dealing with corrigibility and explainability are practically missing all together. This leads the authors to conclude that current practices for evaluating topics like safety and ethics \"are often simplistic and brittle, leading to inconclusive results\" [39, p. 3]. Koch et al. [50] further identify a concentration on fewer and fewer benchmark datasets within most task communities and note that dominant benchmarks have been introduced by researchers at just a handful of elite institutions, raising questions about representation diversity in the design of benchmarks. Scholars have also found that current Al safety evaluation practices almost exclusively deal with English content [64, 93], and are frequently based on datasets where minorities are under-represented, despite efforts to diversify them [100]. This raises concerns regarding the inclusion of multiple perspectives on complex topics like ethics and harm.\nAside from mainly focusing on a small range of tasks (evaluating capabilities of English-speaking language models), previous research has identified that most benchmarks tend to be abstracted out of their social and cultural context [98], and rely on a static, one-time testing logic, where results from single evaluations are taken to represent model capabilities writ large. This has given rise to calls for more multi-layered [115], longitudinal [68], and holistic evaluation methods that make sure that Al models do not just perform well in controlled environments, but also in critical/real world circumstances over time [18, 64, 75]. In their survey of LLM benchmark inadequacies, McIntosh et al. note that \"current benchmarks generally adopt task-based formats, such as Multiple Choice Questions (MCQs) and dialogue-based evaluations, which tend to be static and do not capture the evolving nature of human-AI interactions\" [64, p. 6]. Reuel et al. [90] further note that failures to re-run evaluations multiple times (using different random seeds and sampling temperatures for example), implies that very little is known about the intra-model variance of benchmarks. As a result, they conclude that it is possible that \"most benchmarks fail to distinguish signal and noise\" [90, p. 9]. Others have emphasised that AI audits often fail to consider risks associated with multiple (inter)acting Al systems [11], and rarely take human actions and motivations into consideration [18, 88, 115].\nWhile most benchmarks are designed to tell us something about a model's success, research has further pointed out that they often reveal little (or nothing) about their particular ways of making mistakes, which is crucial from an Al safety and policy enforcement perspective. As Gehrmann et al. [33] put it, \"ranking models according to a single quality number is easy and actionable - we simply pick the model at the top of the list - [yet] it is much more important to understand when and why models fail\" [33, p. 130]. For instance, they suggest that a focus on errors and fragilities (as opposed to instances of success) can be useful for developers of smaller models, since \"work on quantifying shortcomings is equally applicable to smaller models and methods that improve model robustness often work on all model sizes\" [33, p. 131]. In this sense, failure-focused benchmarks could play an important role in equalling out the playing field in AI development."}, {"title": "5.5 Economic, Competitive, and Commercial Roots", "content": "Another contextual element that has been singled out as important is the competitive and commercial roots of bench-mark tests. Previous research has emphasised that capability-oriented benchmarks are deeply embedded in corporate marketing strategies and play an important role in increasing the AI hype, attracting customers and investors, and showcasing how models outperform competitors [38, 78, 120]. As Orr and Kang [78, p. 1881] put it, benchmarks \"serve as the technological spectacle through which companies such as OpenAI and Google can market their technologies\". Many benchmarks also have origins from within the industry and are capability-oriented and centred around tasks with a high potential economic reward, as opposed to focusing on other goals such as ethics and safety [27, 89].\nPrevious research has noted that this competitive and corporate embedding discourages thorough self-critique since there is a direct \"incentive mismatch between conducting high-quality evaluations and publishing new models or mod-elling techniques\" [33, p. 103] and that the field of AI development is \"turning into a giant leaderboard, where publica-tion depends on numbers and little else (such as insight and explanation)\" [21]. While a lack of incentives to disclose weaknesses and limitations is a general problem in science [101], it has been pointed out that benchmarks have played an especially central role in naturalizing and solidifying a competitive culture in AI research, which is increasingly approached as a \"sport\" [78]. As of late, benchmark evaluations have also become increasingly professionalized and transformed into an industry in itself with the rise of platforms like Kaggle and Grand Challenge, who provide organi-sational and infrastructural support to Al competitions and increasingly function as infrastructures of power in fields like medical imaging [58]. The issue of optimising for high benchmark scores at the expense of insight and explanation is known as a form of SOTA-chasing [50] and sometimes described as the \"benchmark effect\" [104]. Previous research [60] has also described benchmarks as an example of what Stengers [103] calls \"fast track research\" which idolises rapid, cumulative publication, thus producing a \"winners curse\" in AI development [97].\nRisks associated with the competitive and commercial roots of benchmarks have further been linked to the growing influence of industry in Al research, where private businesses share of the biggest AI models has increased from 11% in 2010 to 96% in 2021 [1]. In such a context, researchers have noted that current benchmarking tasks - which are generally highly data-intensive - are especially well suited to fit AI models that have been developed within the industry, whose access to advanced data infrastructures, computing power, valuable datasets, and skilled researchers now vastly exceed those of academic researchers [1]. This concentration of power could potentially stifle robust AI evaluations and hinder the development of AI models that adhere to other aims and goals than commercial ones. Scholars have also warned that if academic researchers continue to uphold data-intensive benchmark tests as the SOTA, there is a risk that their research will become increasingly dependent on technological infrastructures provided by the industry [51]."}, {"title": "5.6 Rigging, Gaming, and Measure Becoming Target", "content": "A closely related issue concerns how benchmark tests can be tricked and gamed. In areas and modalities where best-practice benchmarks are missing (i.e., practically all modalities, except for text-based benchmark evaluations), researchers have noted that there are strong incentives to \"rig\" benchmark tests. For instance, Dehghani et al. [22] sur-vey how know-how and recipes for how to score high on benchmark setups are often widely circulated online. Recently, language models have also been found to be optimised for answering the multiple choice questions that are often part of benchmarks [3], and to (either intentionally or unintentionally) \"fake\" alignment with ethics or safety goals [37]. The issue point towards what is known as Goodhart's law, i.e. the recognition that \"when a measure becomes a target, it ceases to be a good measure\" [105].\nOne reason why gaming can proceed is that users of benchmarks rarely provide the resources needed to validate and replicate their test results [8, 10, 22, 90]. To examine the relevance and validity of benchmark scores, researchers have emphasised that it is necessary to access information concerning all aspects of the evaluation procedure (such as the original evaluation code and all details concerning the experimental setup) [10]. Providing such documentation is far from standard, however, especially when proprietary Al models are concerned. This makes it possible to tweak and cherry-pick benchmark results a problem that is especially pressing given that subtle \"variations in prompts, formatting or other implementation details can significantly impact the performance and validity of evaluations\" [10, p. 3]. In their in-depth analysis of 24 SOTA language model benchmarks, Reuel et al. [90] found that only four provided scripts to replicate the results and that no more than ten performed multiple evaluations or reported the statistical significance of their results.\nAn increasingly well discussed issue also concerns the problem of \"data contamination\" i.e., the risk that the models have either intentionally or unintentionally ingested benchmark datasets during training, which severely questions the integrity of AI tests [9, 59, 91, 117, 119]. The problem - which can be an instance of data leakage [47] or train-test-overlap [53] - has been known for long, and produces similar effects to those of overfitting and memorization [59, 110], leading to models with low generalization power that perform well on familiar tasks (in-distribution) but fail other tasks with a similar difficulty and distribution shift (out-of-distribution) [9, 59, 71, 91, 117-119]. When testing GPT4 on benchmark problems from Codeforces (a website hosting coding competitions) in 2023, for instance, Narayanan and Kapoor [71] found that the AI model could regularly solve benchmark problems classified as easy - as long as the problems had been added before 5th September 2021. For problems added later, GPT4 could not get a single question right, suggesting that the model had memorised questions and answers. Similar results have also been identified in multiple other models and benchmarks [9, 59, 91, 117, 119]. Despite the fact that issues with data leaks are so well known that strategies have been developed to avoid it, there is still a widespread lack of reporting of data contamination tendencies during benchmark tests. In a study from October 2024, for example, Zhang et al. [119] found that out of 30 analysed models, only 9 reported train test overlap.\nAnother recently identified issue is called \"sandbagging\" and involves a \"strategic underperformance on an evalua-tion\" which occurs when an Al developer intentionally understates a models capability, for instance to avoid becoming a target for Al safety regulation. In a 2024 study, for example, Weij et al. [116, p. 1] prompted frontier models like GPT-4 and Claude 3 Opus and found that they \"selectively underperform on dangerous capability evaluations, while maintaining performance on general (harmless) capability evaluations\". The researchers also found that that it was pos-sible to fine-tune and adjust both frontier and smaller models to hide specific capabilities or target specific capability scores. The issue could be generalised to popular benchmarks like the Weapons of Mass Destruction Proxy Benchmark (WMDP), and puts the trustworthiness of benchmark evaluations into question, especially in a regulatory context."}, {"title": "5.7 Dubious Community Vetting and Path Dependencies", "content": "A related area of research emphasises how benchmarks become naturalised and reach standard status because of the culture and logic of academic citations [78]. For instance, new benchmarks are commonly introduced together with new or updated AI models. If the AI model becomes popular, the benchmark may become widely cited and circulated as a secondary effect, even though the developers of the benchmark did not intended or expect it to become standard. In this way, a benchmark that might have been selected for practical purpose or due to subjective preferences may\ncome to have substantial technical and cultural influence as a result of community vetting, even though its suitability as a yardstick for ethics, safety, or performance may be questioned [77, 78]. For instance, Denton et al. [23] show how this was partly the case when the ImageNet dataset a key reference point in the performance testing of computer vision models - became standard, following the unforeseen success of the ImageNet Large Scale Visual Recognition Challenge hosted by Toronto University in 2010. Likewise, the so-called Lena test image - also central to computer vision benchmark tests was taken from the centerfold of a November 1972 Playboy magazine and catapulted into computer history since \"someone happened to walk in with a recent issue of Playboy\" at a time of need, according to what has been described as the most credible origin story [69, p. 80]. Since then, the ImageNet dataset and its associated benchmark challenge has been become a symbol for dataset bias [23], while the Lena test image has come to serve as a prime example of the role of whiteness and women's sexualised bodies in the standardisation of digital visual culture [69]. These examples highlight how benchmarks are fundamentally cultural and political products whose power and influence may and be (uncritically and problematically) reinforced through community vetting.\nAside from bias and representational issues, Schlangen [96, p. 1] argues that what is typically missing and left implicit in the peer-review-fuelled process of benchmark use is the argumentation for why scoring well on a particular bench-mark \"constitutes progress, and progress towards what\". Instead, researchers are expected to routinely demonstrate performance on dominant benchmarks, despite the fact that more task-specific benchmarks may be more technically appropriate [50]. In this way, peer-washing serves to \"maintain datasets as authoritative proxies even when they are shown to be harmful or problematic\" [77, p. 4966]. New benchmarks often have a difficult time to gain traction because of the dominance and authority of well-cited benchmarks [45], and in a study of 3765 benchmarks, Ott et al. [79] found that only a small minority of the proposed benchmark solutions reached widespread adoption. This problem is also recognised within the industry. For instance, researchers at Google's Brain Team describe what they call a \"benchmark lottery\" which \"postulates that many factors, other than fundamental algorithmic superiority, may lead to a method being perceived as superior\" [22, p. 1]. Ironically, researchers have simultaneously found that a majority of influential benchmarks have been released as preprints without going through rigorous academic peer-review [64, p. 6]. In other words, problems with peer-washing both concern the poor quality control that many benchmarks are subjected to, and the self-fulfilling and sometimes excessive ways through which certain benchmarks are propelled into standards. Bao et al. [7] further note that most papers that introduce benchmarks have origins in the field of machine learning and are mainly focused on methods. This means that the content of benchmarks datasets are often considered sec-ondary, since the datasets are merely there to provide a baseline for comparison between different evaluation methods. When benchmarks are applied to real and specific use-cases - such as evaluating the fairness of algorithmic risk assess-ment instruments within the criminal justice system - the downstream effects of such a lack of concern for datasets can have worrying effects. As Bao et al. [7] put it, a paper on benchmarks can be of high quality in a pure AI/ML methods sense, but irrelevant, dangerous, or harmful when applied in circumstances such as the criminal justice system, since it may introduce and perpetuate harms and mistranslations. The researchers also suggest that the current peer-review system implies that benchmarks that are primarily relevant from a methods/machine learning perspective will be cited far more often than benchmarks that are relevant for use in specific, real-life use-cases. Looking too closely at cita-tion counts when determining the quality and relevance of benchmarks may thus effectively lead users astray and complicate identifying benchmarks with a high practical utility.\nProblems with dubious community vetting become especially worrying given that benchmarks create \"path depen-dencies\" in Al research, meaning they reinforce certain methodologies and research goals, while stifling those that do not align with the logic of dominant benchmark tests [13]. In particular, Koch and Peterson [51, p. 3] warn against the"}, {"title": "5.8 Rapid Al Development and Benchmark Saturation", "content": "Another social, economic, and cultural issue is the speed of AI developments. As the capabilities of AI models have increased manifold in the past decade, researchers have emphasised that many benchmarks are old and designed to test models far simpler than those in use today [10, 48", "5": "find that many prominent LLM benchmarks (including Lambada, AI2 ARC, OBQA, Hella Swag, and WinoGrande) were \"designed prior to shifts such as in-context learning and chat interaction, and therefore were not designed to take these formats and approaches into account\", noting that this may affect their validity in unforeseen ways. Many benchmarks also struggle with the challenge of quickly being outperformed as Al models achieve 100 percent accuracy scores in tests [15, 40", "79": ".", "64": "emphasises that many benchmark frameworks are slow and complicated to implement, meaning that evaluation processes can span weeks or months, which hinders timely feedback on AI model safety risks. This becomes an issue since new model releases often enter markets continuously, making it difficult to relocate evaluation resources in quick and adequate ways. It also undermines a \"benchmark's ability to consistently evaluate reasoning, comprehension, or multimodal integration, as the results may vary with each model iteration\" [64, p. 13", "113": "."}]}