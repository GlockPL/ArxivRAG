{"title": "AUTOMATIC DATASET CONSTRUCTION (ADC): SAMPLE COLLECTION, DATA CURATION, AND BEYOND", "authors": ["Minghao Liu", "Zonglin Di", "Jiaheng Wei", "Zhongruo Wang", "Hengxiang Zhang", "Ruixuan Xiao", "Haoyu Wang", "Jinlong Pang", "Hao Chen", "Ankit Shah", "Hongxin Wei", "Xinlei He", "Zhaowei Zhao", "Haobo Wang", "Lei Feng", "Jindong Wang", "James Davis", "Yang Liu"], "abstract": "Large-scale data collection is essential for developing personalized training data, mitigating the shortage of training data, and fine-tuning specialized models. However, creating high-quality datasets quickly and accurately remains a challenge due to annotation errors, the substantial time and costs associated with human labor. To address these issues, we propose Automatic Dataset Construction (ADC), an innovative methodology that automates dataset creation with negligible cost and high efficiency. Taking the image classification task as a starting point, ADC leverages LLMs for the detailed class design and code generation to collect relevant samples via search engines, significantly reducing the need for manual annotation and speeding up the data generation process. Despite these advantages, ADC also encounters real-world challenges such as label errors (label noise) and imbalanced data distributions (label bias). We provide open-source software that incorporates existing methods for label error detection, robust learning under noisy and biased data, ensuring a higher-quality training data and more robust model training procedure. Furthermore, we design three benchmark datasets focused on label noise detection, label noise learning, and class-imbalanced learning. These datasets are vital because there are few existing datasets specifically for label noise detection, despite its importance. Finally, we evaluate the performance of existing popular methods on these datasets, thereby facilitating further research in the field.", "sections": [{"title": "Introduction", "content": "In the era of Large Language Models (LLMs), the literature has observed an escalating demand for fine-tuning specialized models [1, 2, 3], highlighting the urgent need for customized datasets [4, 5, 6].\nTraditional Dataset Construction (TDC) typically involves sample collection followed by labor-intensive annotation, requiring significant human efforts [7, 8, 9, 10]. Consequently, TDC is often hindered by the limitations of human expertise, leading to suboptimal design [11], data inaccuracies [12, 13, 14, 7, 15], and extensive manual labor [16, 17]. Furthermore, certain datasets are inherently challenging or risky to collect manually, such as those for fall detection"}, {"title": "Automatic-Dataset-Construction (ADC)", "content": "In this section, we discuss the detailed procedure of ADC, as well as an empirical application."}, {"title": "The ADC pipeline", "content": "The ADC pipeline generates datasets with finely-grained class and attribute labels, utilizing data diagnostic software to perform data curation. Below, we provide a step-by-step guide to collecting the Clothing-ADC, a clothes image dataset, along with an overview of its statistics and key information. The overall Automatic-Dataset-Construction (ADC) pipeline is illustrated in Figure 1."}, {"title": "Step 1: Dataset design with large language models (LLM)", "content": "Customization: The LLM is tailored to understand specific domains (i.e., fashion) via In-Context-Learning based prompts to leverage its pre-existing knowledge, or fine-tuning on the customized dataset. This enables the LLM to significantly simplify the design phase, making the task easier for designers by allowing them to select from existing options rather than creating from scratch. Given the extensive availability of \"clothes\"-related domain knowledge on the internet, we skip this procedure for Clothing-ADC.\nDetailed categories identification: LLMs assist researchers in conducting a more thorough search in the field by processing and analyzing numerous concepts simultaneously, unlike humans who may overlook certain factors when faced with a large volume of concepts [11]. We utilize LLMs to identify attribute types for each class. Then use a prompt of \"Show me <30-80> ways to describe <Attribute> of <Class>\" to generate the proposed subclasses.\nIterative refinement: The initial category list generated by the LLM undergoes review and refinement either by domain experts or through self-examination by the LLM itself, ensuring alignment with specific application or research needs, as shown in Figure 2. This iterative refinement process enables the creation of a high-quality dataset with finely-grained class labels. Additionally, this approach facilitates rapid iterative feedback during the design phase, offering a significant advantage over traditional methods that rely on annotator feedback during the test run annotation phase. This acceleration enables researchers to explore and refine their ideas more efficiently, resulting in better dataset quality and reduced development time."}, {"title": "Step 2: Automated querying and downloading", "content": "For image data collection, ADC utilizes APIs provided by Google Images or Bing Images for automated querying. Each category and attribute identified in the first step can be used to formulate search queries."}, {"title": "Step 3: Data curation and cleaning", "content": "Implement or use existing data curation software capable of identifying and filtering out irrelevant images, such as Docta [22], CleanLab [23], and Snorkel [24], etc. For example, these tools can identify when an item is mislabeled regarding its type, material, or color. Finally, ADC aggregates the suggested labels recommended by the dataset curation software and removes potentially mislabeled or uncertain samples. For illustration, we adopt a data-centric label curation software (Docta) in Algorithm 1.\nThe high-level idea of this algorithm is to estimate the essential label noise transition matrix $T_{Est}$ without using ground truth labels, achieved through the consensus equations (Part A). Following this, Algorithm 1 identifies those corrupted instances via the cosine similarity ranking score among features as well as a well-tailored threshold based on the obtained information (i.e., $T_{Est}$), and then relabels these instances using KNN-based methods (Part B). For more details, please refer to work [25, 26, 27]."}, {"title": "2.2 Clothing-ADC", "content": "To illustrate the ADC pipeline, we present the Clothing-ADC dataset, which comprises a substantial collection of clothing images. The dataset includes 1,076,738 samples, with 20,000 allocated for evaluation, another 20,000 for testing, and the remaining samples used for training. Each image is provided at a resolution of 256x256 pixels. The dataset is categorized into 12 primary classes, encompassing a total of 12,000 subclasses, with an average of 89.73 samples per subclass. Detailed statistics of the dataset are provided in Table 1. The following subsection elaborates on the dataset construction process in comprehensive detail.\nSubclass design Utilizing GPT-4, we identified numerous attribute options for each clothing type. For example, in the case of sweaters, we recognized eight distinct attributes: color, material, pattern, texture, length, neckline, sleeve length, and fit type. The language model was able to find 30-50 options under each attribute. Our Clothing-ADC dataset includes the three most common attributes: color, material, and pattern, with each attribute having ten selected options. This results in 1000 unique subclasses per clothing type.\nData collection The ADC pipeline utilizes the Google Image API to collect clothing images by formulating queries that include attributes such as \"Color + Material + Pattern + Cloth Type\" (e.g., \"white cotton fisherman sweater\"). The relevance of the search results tends to decline after a significant number of samples are gathered, leading us to set a cutoff threshold of 100 samples per query. After removing broken links and improperly formatted images, each subclass retained approximately 90 samples. These queries generated noisy, webly-labeled data for the training set.\nCreating a test set Note that the collected samples may suffer from web-based label noise, where annotations might be incorrect due to mismatches provided by search engines, the traditional approach typically involves manually re-labeling existing annotations and aggregating multiple human votes per label to ensure a high-quality subset for testing purposes. Our ADC pipeline enhances efficiency by presenting annotators with a set of samples that share the same machine-generated label. Annotators are then tasked with selecting a subset of correctly labeled samples, choosing a minimum of four samples out of twenty. This method significantly reduces both manual effort and difficulty, encouraging annotators to critically evaluate machine-generated labels and thereby reducing the effect of human over-trust in AI answers [28, 18]. The samples selected through this process are considered \u201cclean\" labels, representing a consensus between human judgment and machine-generated labels [29]."}, {"title": "Challenge one: dealing with imperfect data annotations", "content": "The first pervasive and critical challenge during the automatic dataset construction lies in the prevalence of noisy/imper- fect labels. This issue is intrinsic to web-sourced data, which, although rich in diversity, often suffers from inaccuracies due to the uncurated nature of the internet. These errors manifest as mislabeled images, inconsistent tagging, and misclassified attributes, introducing non-negligible noise into the dataset that may adversely affect the training and performance of machine learning models. The following discussion bridges the gap between imperfect data and curated data via mining and learning with label noise, to refine data quality, enhance label accuracy, and ensure the reliability of Auto-Dataset-Construction (ADC) for high-stakes AI applications.\nFormulation Let $D := \\{(x_n, y_n)\\}_{n\\in[N]}$ represent the training samples for a $K$-class classification task, where $[N] := \\{1,2, ..., N\\}$. Suppose that these samples $\\{(x_n, y_n)\\}_{n\\in[N]}$ are outcomes of the random variables $(X, Y) \\in X \\times Y$, drawn from the joint distribution $D$. Here, $X$ and $Y$ denote the spaces of features and labels, respectively. However, classifiers typically access a noisily labeled training set $D := \\{(x_n, \\hat{y}_n)\\}_{n\\in[N]}$, assumed to arise from random variables $(X, \\tilde{Y}) \\in X \\times \\tilde{Y}$, drawn from the distribution $\\tilde{D}$. It is common to observe instances where $y_n \\neq \\tilde{y}_n$ for some $n \\in [N]$. The transition from clean to noisy labels is typically characterized by a noise transition matrix $T(X)$, defined as $T_{i,j}(X) := P(Y = j | Y = i, X)$ for all $i, j \\in [K]$ [12, 13, 33]."}, {"title": "The challenge of label noise detection", "content": "While employing human annotators to clean data is effective in improving label quality, it is often prohibitively expensive and time-consuming for large datasets. A practical alternative is to enhance label accuracy automatically by first deploying algorithms to detect potential errors within the dataset and then correcting these errors through additional algorithmic processing or crowdsourcing."}, {"title": "Existing approaches to detect label noise", "content": "Learning-Centric Approaches: Learning-centric approaches often leverage the behavior of models during training to infer the presence of label errors based on how data is learned. One effective strategy is confidence-based screening, where labels of training instances are scrutinized if the model's prediction confidence falls below a certain threshold. This approach assumes that instances with low confidence scores in the late training stage are likely mislabeled [34]. Another innovative technique involves analyzing the gradients of the training loss w.r.t. input data. Pruthi et al. [35] utilize gradient information to detect anomalies in label assignments, particularly focusing on instances where the gradient direction deviates significantly from the majority of instances. Researchers have also utilized the memorization effect of deep neural networks, where models tend to learn clean data first and only memorize noisy labels in the later stages of training. Techniques that track how quickly instances are learned during training can thus identify noisy labels by focusing on those learned last [36, 37, 38].\nData-Centric Approaches: Data-centric methods focus on analyzing data features and relationships rather than model behavior for detection. The ranking-based detection method [39] ranks instances by the likelihood of label errors based on their alignment with model predictions. An ensemble of classifiers evaluates each instance, flagging those that consistently deviate from the majority vote as noisy. Neighborhood Cleaning Rule [40] uses the k-nearest neighbors algorithm to check label consistency with neighbors, identifying instances whose labels conflict with the majority of their neighbors as potentially noisy. Zhu et al. [27] propose advanced data-centric strategies for detecting label noise without training models. Their local voting method uses neighbor consensus to validate label accuracy, effectively identifying errors based on agreement within the local feature space."}, {"title": "Clothing-ADC in label noise detection", "content": "We prepared a subset of 20,000 samples from the Clothing-ADC dataset for the label noise detection task, including both noisy and clean labels. We collected three human annotations for each image via Amazon MTurk. Annotators were instructed to classify the labels as correct, unsure, or incorrect. Each sample received three votes. Based on these annotations, we determined the noise rate to be 22.2%-32.7%. Using majority vote aggregation implies uncertainty of the label correctness. By using a more stringent aggregation criterion, more samples are considered as noisy labeled. Under the extreme case where any doubts from any human annotator can disqualify a sample, our auto collected dataset still retains 61.3% of its samples.\nBenchmark efforts Detection performance comparisons of certain existing solutions are given in Table 3. We adopt ResNet-50 [43] as the backbone model to extract the feature here. For each method, we use the default hyper-parameter reported in the original papers. All methods are tested on 20,000 points and predict whether the data point is corrupted or not. We follow [27] to apply the baseline methods to our scenario. In Table 3, the performance is measured by the F\u2081-score of the detected corrupted instances, which is the harmonic mean of the precision and recall, i.e., $F_1 = \\frac{2}{\\text{Precision}^{-1}+\\text{Recall}^{-1}}$. Let $v_n = 1$ indicate that the n-th label is detected as a noisy/wrong label, and $v_n = 0$ otherwise. Then, the precision and recall of detecting noisy labels can be calculated as: $\\text{Precision} = \\frac{\\Sigma_n \\mathbb{1} (v_n=1,\\hat{y}_n\\neq{Y}_n)}{\\Sigma_n \\mathbb{1} (v_n=1)}$, $\\text{Recall} = \\frac{\\Sigma_n \\mathbb{1} (v_n=1,\\hat{y}_n\\neq{Y}_n)}{\\Sigma_n \\mathbb{1} (y_n\\neq{Y}_n)}$."}, {"title": "The challenge of learning with noisy labels", "content": "Another technique is robust learning that can effectively learn from noisy datasets without being misled by incorrect labels, thus maintaining high accuracy and reliability in real-world applications."}, {"title": "Existing approaches to learn with label noise", "content": "In this subsection, we contribute to the literature with robust learning software; all covered methods can be mainly summarized into the following three categories: robust loss functions, robust regularization techniques, and multi- network strategies.\nRobust loss designs Loss Correction modifies the traditional loss function to address label noise by incorporating an estimated noise transition matrix, thereby recalibrating the model's training focus [33]. Loss-Weighting strategies mitigate the impact of noisy labels by assigning lower weights to likely mislabeled instances, reducing their influence on the learning process [13, 44]. Symmetric Cross-Entropy Loss balances the contributions of correctly labeled and mislabeled instances, improving the model's resilience to label discrepancies [45]. Generalized Cross-Entropy Loss, derived from mean absolute error, offers enhanced robustness against outliers and label noise [46]. Peer Loss Functions form a family of robust loss functions [47, 48, 41], leveraging predictions from peer samples as regularization to adjust the loss computation, thereby increasing resistance to noise.\nRobust Regularization Techniques Regularization techniques are designed to constrain or modify the learning process, thereby reducing the model's sensitivity to label noise. Mixup [49] generates synthetic training examples by linearly interpolating between pairs of samples and their labels, enhancing model generalization and smoothing label predictions. Label Smoothing [50, 51] combats overconfidence in unreliable labels by adjusting them towards a uniform distribution. Negative Label Smoothing [52] refines this approach by specifically adjusting the smoothing process for negative labels, preserving model confidence in high-noise environments. Early-Learning Regularization tackles the issue of early memorization of noisy labels by dynamically adjusting regularization techniques during the initial training phase [37, 38].\nMulti-network strategies Employing multiple networks can enhance error detection and correction through mutual agreement and ensemble techniques. In Co-teaching, two networks concurrently train and selectively share clean data points with each other, mitigating the memorization of noisy labels [53]. MentorNet [54] equips a student network with a curriculum that emphasizes samples likely to be clean, as determined by the observed dynamics of a mentor network. DivideMix leverages two networks to segregate the data into clean and noisy subsets using a mixture model, allowing for targeted training on each set to manage label noise better [55]."}, {"title": "Clothing-ADC in label noise learning", "content": "We provide two versions of the Label Noise Learning task, Clothing-ADC and Clothing- ADC (tiny). Specifically, Clothing-ADC leverages the whole available (noisy) training samples to construct the label noise learning task. The objective is to perform class predic- tion w.r.t. 12 clothes types: Sweater, Wind- breaker, T-shirt, Shirt, Knitwear, Hoodie, Jacket, Suit, Shawl, Dress, Vest, Underwear. We also provide a tiny version of Clothing- ADC, which contains 50K training images, sharing similar size with certain widely-used ones, i.e., MNIST, Fashion-MNIST, CIFAR- 10, CIFAR-100, etc.\nEstimated noise level of Clothing-ADC We selected a subset of 20,000 training sam- ples and asked human annotators to evaluate the correctness of the auto-annotated dataset."}, {"title": "Challenge two: dealing with imbalanced data distribution", "content": "We now discuss another real-world challenge: when imperfect annotations meet with imbalanced class/attribute distributions. As shown in Figure 4, long-tailed data distribution is a prevalent issue in web-based datasets: to collect a dataset of wool suits without a specified target color on Google Image, the majority would likely be dark or muted shades (grey, black, navy), with few samples in brighter colors like pink or purple. This natural disparity results in most data points belonging to a few dominant categories, while the remaining are spread across several minority groups.\nWe are interested in how class-imbalance intervenes with learning. In real-world scenarios, the distribution of classes tends to form a long-tail form, in other words, the head class and the tail class differ significantly in their sample sizes, i.e., $\\max_k P(Y = k) \\gg \\min_{k'} P(Y = k')$."}, {"title": "Existing approaches for class imbalance learning", "content": "Data-Level Methods Data-level methods modify training data to balance class distribution, focusing on adjusting the dataset by increasing minority class instances or decreasing majority class instances. Oversampling increases the number of minority class instances to match or approach the majority class. This can be done through simple duplication [59] (e.g., random oversampling) or generating synthetic data [60, 61, 62, 63]. Undersampling reduces the number of majority class instances, helping to balance class distributions but potentially discarding useful information [64, 65, 66].\nAlgorithm-Level Methods These methods adjust the training process or model to handle unequal class distributions better. Specifically, cost-sensitive learning assigns different costs to misclassifications of different classes, imposing higher penalties for errors on the minority class [67]. It modifies the loss function to incorporate misclassification costs, encouraging the model to focus more on minority class errors [68, 69]. Thresholding adjusts the decision threshold for class probabilities to account for class imbalance. Instead of using a default threshold, different thresholds are applied based on class distribution, modifying the decision process for predicting class labels [70, 71]."}, {"title": "Clothing-ADC in class-imbalanced learning", "content": "Note that in the label noise learning task, the class distributes with almost balanced prior. However, in practice, the prior distribution is often long-tail distributed. Hence, the combined influence of label noise and long-tail distribution is a new and overlooked challenge presented in the literature. To facilitate the exploration of class-imbalanced learning, we tried to reduce the impact of noisy labels via selecting high-quality annotated samples as recognized by dataset curation software. Human estimation suggested a noise rate of up to 22.2%, and 10.5% marked as uncertain. To address this, we employed two methods to remove noisy samples: a data centric curation (Algorithm 1), which removed 26.36% of the samples, and a learning-centric curation (Appendix Algorithm 3), which removed 25%. Combined, these methods eliminated 45.15% of the samples, with an overlap of 6.21% between the two approaches. We provide Clothing-ADC CLT, which could be viewed as the long-tail (class-level) distributed version of Clothing-ADC. Denote by p the imbalanced ratio between the maximum number of samples per class and the minimum number of samples per class. In practice, we provide p = 10, 50, 100 (class-level) long-tail version of Clothing-ADC.\nBenchmark efforts Regarding the evaluation metric, we follow from the recently proposed metric [72], which considers an objective that is based on the weighted sum of class-level performances on the test data, i.e., $\\sum_{i\\in[K]} g_i Acc_i$, where $Acc_i$ indicates the accuracy of the class i:\n$\\delta$-worst accuracy: $\\min_{i\\in[K]} g_i Acc_i, s.t. D(g, u) < \\delta$.\nHere, $\\Delta_K$ denotes the $(K \u2013 1)$-dimensional probability simplex, where K is the number of classes as previously defined. Let $u \\in \\Delta_K$ be the uniform distribution, and $g := [g_1, g_2, ..., g_k]$ is the class weights. The $\\delta$-worst accuracy measures the worst-case g-weighted performance with the weights constrained to lie within the $\\delta$-radius ball around the target (uniform) distribution. For any chosen divergence $D$, it reduces to the mean accuracy when $\\delta \\to 0$ and to the worst accuracy for $\\delta \\to \\infty$. The objective interpolates between these two extremes for other values of $\\delta$ and captures our goal of optimizing for variations around target priors instead of more conventional objectives of optimizing for either the average accuracy at the target prior or the worst-case accuracy."}, {"title": "Conclusion", "content": "In this paper, we introduced the Automatic Dataset Construction (ADC) pipeline, a novel approach for automating the creation of large-scale datasets with minimal human intervention. By leveraging Large Language Models for detailed class design and automated sample collection, ADC significantly reduces the time, cost, and errors associated with traditional dataset construction methods. The Clothing-ADC dataset, which comprises one million images with rich category hierarchies, demonstrates the effectiveness of ADC in producing high-quality datasets tailored for complex research tasks. Despite its advantages, ADC faces challenges such as label noise and imbalanced data distributions. We addressed these challenges with open-source tools for error detection and robust learning. Our benchmark datasets further facilitate research in these areas, ensuring that ADC remains a valuable tool for advancing machine learning model training."}, {"title": "Broader impacts", "content": "Our paper introduces significant advancements in dataset construction methodologies, particularly through the develop- ment of the Automatic Dataset Construction (ADC) pipeline:\nReduction in Human Workload: ADC automates the process of dataset creation, significantly reducing the need for manual annotation and thereby decreasing both the time and costs associated with data curation.\nEnhanced Data Quality for Research Communities: ADC provides high-quality, tailored datasets with minimal human intervention. This provides researchers with datasets in the fields of label noise detection, label noise learning, and class-imbalanced learning, for exploration as well as fair comparisons.\nSupport for Customized LLM Training: The ability to rapidly generate and refine datasets tailored for specific tasks enhances the training of customized Large Language Models (LLMs), increasing their effectiveness and applicability in specialized applications.\nFurthermore, the complementary software developed alongside ADC enhances these impacts:\nData Curation and Quality Control: The software aids in curating and cleaning the collected data, ensuring that the datasets are of high quality that could compromise model training.\nRobust Learning Capabilities: It incorporates methods for robust learning with collected data, addressing challenges such as label noise and class imbalances. This enhances the reliability and accuracy of models trained on ADC- constructed datasets.\nTogether, ADC and its accompanying software significantly advance the capabilities of machine learning researchers and developers by providing efficient tools for high-quality customized data collection, and robust training."}, {"title": "Limitations", "content": "While ensuring the legal and ethical use of datasets, including compliance with copyright laws and privacy concerns, is critical, our initial focus is on legally regulated and license-friendly data sources available through platforms like Google or Bing. Addressing these ethical considerations is beyond the current scope but remains an essential aspect of dataset usage.\nBesides, similar to Traditional-Dataset-Construction (TDC), Automatic-Dataset-Construction (ADC) is also unable to guarantee fully accurate annotations."}, {"title": "A.1 The algorithm of image data collection in ADC", "content": null}, {"title": "A.2 The algorithm of learning-centric curation method in ADC", "content": null}, {"title": "B.1 Collected Clothing ADC dataset", "content": "Our collected Clothing-ADC dataset can be found here: Google Drive."}, {"title": "B.2 Dataset statistics in Clothing-ADC", "content": "Our automated dataset creation pipeline is capable of generating numerous designs per attribute, as shown in Table 6. This table provides a detailed list of designs generated by our pipeline, from which we selected a subset to include in our dataset."}, {"title": "B.3 Test set human vote collection", "content": "Our automated dataset collection pipeline enabled us to create a large, noisy labeled dataset. We asked annotators to select the best-fitting options from a range of samples, as shown in Figure 5, with each task including at least 4 samples and workers completing 10 tasks per HIT at a cost of $0.15 per task, totaling $150 estimated wage of $2.5-3 per hour, and after further cleaning the label noise, we ended up with 20,000 samples in our test set. To participate, workers had to meet specific requirements, including being Master workers, having a HIT Approval Rate above 85%, and having more than 500 approved HITs, with the distribution of worker behavior shown in Figure 6."}, {"title": "C.1 Distribution of Human Votes for Label Noise Evaluation", "content": "On the annotation page, we presented the image and its original label to the worker and asked if they believed the label was correct (Figure 7). They input their evaluation by clicking one of three buttons. Note that we encouraged workers to categorize acceptable samples as \"unsure\". The resulting distribution is shown in Table 7. Using a simple majority vote aggregation, we found that the noise rate in our dataset is 22.15%. However, if a higher level of certainty is required for clean labels, we can apply a more stringent aggregation method, considering more samples as mislabeled. In the extreme case where any doubts from any of the three annotators can disqualify a sample, our automatically collected dataset still retains 61.25% of its samples.\nFor the label noise evaluation task, we utilized a subset of 20,000 samples from the Clothing-ADC dataset, collecting three votes from unique workers for each sample. Each Human Intelligence Task (HIT) included 20 samples and cost $0.05. To participate, workers had to meet the following requirements: (1) be Master workers, (2) have a HIT Approval Rate above 85%, and (3) have more than 500 approved HITs. The total cost for this task was $150, estimated wage of $2.5-3 per hour."}, {"title": "C.2 Noisy learning and class imbalance learning benchmark implementation details", "content": "Our code refers to zip file in supplementary material."}, {"title": "C.3 Label noise detection", "content": "We run four baselines for label noise detection, including CORES [41], confident learning [34], deep k-NN [42] and Simi-Feat [27]. All the experiment is run for one time following [41, 27].\nThe experiment platform we run is a 128-core AMD EPYC 7742 Processor CPU and the memory is 128GB. The GPU we use is a single NVIDIA A100 (80GB) GPU. For the dataset, we used human annotators to evaluate whether the sample has clean or noisy label as mentioned in Appendix C.1. We aggressively eliminates human uncertainty factors and only consider the case with unanimous agreement as a clean sample, and everything else as noisy samples. The backbone model we use is ResNet-50 [43].\nFor all the baselines, the parameters we use are the same as the original paper except the data loader. We skip the label corruption and use the default value from the original repository. For CORES, the cores loss whose value is smaller than 0 is regarded as the noisy sample. For confidence learning, we use the repository\u00b9 from the clean lab and the default hyper-parameter. For deep k-NN, the k we set is 100. For SimiFeat, we set k as 10 and the feature extractor is CLIP."}, {"title": "C.4 Label noise learning", "content": "The platform we use is the same as label noise detection. The backbone model we use is ResNet-50 [43]. For the full dataset, we run the experiment for 1 time. For the tiny dataset, we run the experiments for 3 times. The tiny dataset is sampled from the full set whose size is 50. The base learning rate we use is 0.01. The base number of epochs is 20. The hyper-parameters for each baseline method are as follows. For backward and forward correction, we train the model using cross-entropy (CE) loss for the first 10 epochs. We estimate the transition matrix every epoch from the 10th to the 20th epoch. For the positive and negative label smoothing, the smoothed labels are used at the 10th epoch. The smooth rates of the positive and negative are 0.6 and -0.2. Similarly, for peer loss, we train the model using CE loss for the first 10 epochs. Then, we apply peer loss for the rest 10 epochs and the learning rate we use for these 10 epochs is 1e-6. The hyper-parameters for f-div is the same as those of peer loss. For divide-mix, we use the default hyper-parameters in the original paper. For Jocor, the hyper-parameters we use is as follows. The learning rate is 0.0001. A is 0.3. The epoch when the decay starts is 5. The hyper-parameters of co-teaching is similar to Jocor. For logitclip, T is 1.5. For taylorCE, the hyper-parameter is the same as the original paper."}, {"title": "C.5 Class-imbalanced learning", "content": "The platform we use is the same as label noise detection. The backbone model we use is ResNet-50 [43]. For different imbalance ratio (p = 10, 50, 100). The class distribution is shown in Table 8. For all the methods, the base learning rate is 0.0001 and the batch size is 448. The dataset we use is not full dataset because we want to disentangle the noisy label and class imbalance learning. We use Docta and a pre-trained model trained with cross-entropy to filter the data whose prediction confidence is low. Due to the memorization effect, we fine-tune the model for 2 epochs to filter the data. We remove 45.15% data in total where Docta removes 26.36% while CE removes 25.00% with a overlap of 6.20%. Thus, the datset we use for class-imbalance learning is 54.85% of the full dataset."}]}