{"title": "AUTOMATIC DATASET CONSTRUCTION (ADC): SAMPLE COLLECTION, DATA CURATION, AND BEYOND", "authors": ["Minghao Liu", "Zonglin Di", "Jiaheng Wei", "Zhongruo Wang", "Hengxiang Zhang", "Ruixuan Xiao", "Haoyu Wang", "Jinlong Pang", "Hao Chen", "Ankit Shah", "Hongxin Wei", "Xinlei He", "Zhaowei Zhao", "Haobo Wang", "Lei Feng", "Jindong Wang", "James Davis", "Yang Liu"], "abstract": "Large-scale data collection is essential for developing personalized training data, mitigating the shortage of training data, and fine-tuning specialized models. However, creating high-quality datasets quickly and accurately remains a challenge due to annotation errors, the substantial time and costs associated with human labor. To address these issues, we propose Automatic Dataset Construction (ADC), an innovative methodology that automates dataset creation with negligible cost and high efficiency. Taking the image classification task as a starting point, ADC leverages LLMs for the detailed class design and code generation to collect relevant samples via search engines, significantly reducing the need for manual annotation and speeding up the data generation process. Despite these advantages, ADC also encounters real-world challenges such as label errors (label noise) and imbalanced data distributions (label bias). We provide open-source software that incorporates existing methods for label error detection, robust learning under noisy and biased data, ensuring a higher-quality training data and more robust model training procedure. Furthermore, we design three benchmark datasets focused on label noise detection, label noise learning, and class-imbalanced learning. These datasets are vital because there are few existing datasets specifically for label noise detection, despite its importance. Finally, we evaluate the performance of existing popular methods on these datasets, thereby facilitating further research in the field.", "sections": [{"title": "1 Introduction", "content": "In the era of Large Language Models (LLMs), the literature has observed an escalating demand for fine-tuning specialized models [1, 2, 3], highlighting the urgent need for customized datasets [4, 5, 6].\nTraditional Dataset Construction (TDC) typically involves sample collection followed by labor-intensive annotation, requiring significant human efforts [7, 8, 9, 10]. Consequently, TDC is often hindered by the limitations of human expertise, leading to suboptimal design [11], data inaccuracies [12, 13, 14, 7, 15], and extensive manual labor [16, 17]. Furthermore, certain datasets are inherently challenging or risky to collect manually, such as those for fall detection in elderly individuals, dangerous activities like extreme sports, and network intrusion detection. Therefore, there is a growing need for more automated and efficient data collection methods to enhance accuracy and efficiency in dataset creation [18, 19, 20]. To address these challenges, we propose the Automatic Dataset Construction (ADC), an innovative approach designed to construct customized large-scale datasets with minimal human involvement. Our methodology reverses the traditional process by starting with detailed annotations that guide sample collection. This significantly reduces the workload, time, and cost associated with human annotation, making the process more efficient and targeted for LLM applications, ultimately outperforming traditional methods.\nTraditional-Dataset-Construction v.s. Automatic Dataset Construction Figure 1 illustrates the difference between Traditional Dataset Construction (TDC) and Automatic Dataset Construction (ADC). TDC typically unfolds in two stages: developing classification categories and employing human labor for annotation. Creating comprehensive categories requires deep domain knowledge and experience, tasks that even expert researchers find challenging [11]. Crowdsourcing is often used to refine these categories, but it increases time and costs without necessarily improving label quality [16, 17]. Annotation by human workers introduces label noise, which impacts dataset reliability, even when multiple inputs are aggregated [21]. In contrast, ADC offers improvements at each key step. In the \"Dataset design\", ADC uses LLMs to automate field searches and provide instant feedback, unlike traditional manual class and attribute creation. In the sample annotation steps, ADC reverses the labeling process by using predefined targets to search for samples, human annotators are then instructed to filter noisy labeled samples, significantly reducing the need for costly human annotation.\nOur main contributions can be summarized as follows:\n\u2022 The Automatic-Dataset-Construction (ADC) pipeline: We introduce Automatic-Dataset-Construction (ADC), an automatic data collection approach that requires minimal human efforts, tailored for the specialized large-scale data collection. Leveraging ADC, we developed Clothing-ADC, an image dataset containing one million images with over 1,000 subclasses for each clothing type. Our dataset offers a rich hierarchy of categories, creating well-defined sub-populations that support research on a variety of complex and novel tasks.\n\u2022 Software efforts for addressing dataset construction challenges: We explore several challenges observed in real-world dataset construction, including detecting label errors, learning with noisy labels, and class-imbalanced"}, {"title": "2 Automatic-Dataset-Construction (ADC)", "content": "In this section, we discuss the detailed procedure of ADC, as well as an empirical application."}, {"title": "2.1 The ADC pipeline", "content": "The ADC pipeline generates datasets with finely-grained class and attribute labels, utilizing data diagnostic software to perform data curation. Below, we provide a step-by-step guide to collecting the Clothing-ADC, a clothes image dataset, along with an overview of its statistics and key information. The overall Automatic-Dataset-Construction (ADC) pipeline is illustrated in Figure 1."}, {"title": "Step 1: Dataset design with large language models (LLM)", "content": "\u2022 Customization: The LLM is tailored to understand specific domains (i.e., fashion) via In-Context-Learning based prompts to leverage its pre-existing knowledge, or fine-tuning on the customized dataset. This enables the LLM to significantly simplify the design phase, making the task easier for designers by allowing them to select from existing options rather than creating from scratch. Given the extensive availability of \"clothes\"-related domain knowledge on the internet, we skip this procedure for Clothing-ADC.\n\u2022 Detailed categories identification: LLMs assist researchers in conducting a more thorough search in the field by processing and analyzing numerous concepts simultaneously, unlike humans who may overlook certain factors when faced with a large volume of concepts [11]. We utilize LLMs to identify attribute types for each class. Then use a prompt of \"Show me <30-80> ways to describe  of \" to generate the proposed subclasses.\n\u2022 Iterative refinement: The initial category list generated by the LLM undergoes review and refinement either by domain experts or through self-examination by the LLM itself, ensuring alignment with specific application or research needs, as shown in Figure 2. This iterative refinement process enables the creation of a high-quality dataset with finely-grained class labels. Additionally, this approach facilitates rapid iterative feedback during the design phase, offering a significant advantage over traditional methods that rely on annotator feedback during the test run annotation phase. This acceleration enables researchers to explore and refine their ideas more efficiently, resulting in better dataset quality and reduced development time."}, {"title": "Step 2: Automated querying and downloading", "content": "For image data collection, ADC utilizes APIs provided by Google Images or Bing Images for automated querying. Each category and attribute identified in the first step can be used to formulate search queries."}, {"title": "Step 3: Data curation and cleaning", "content": "Implement or use existing data curation software capable of identifying and filtering out irrelevant images, such as Docta [22], CleanLab [23], and Snorkel [24], etc. For example, these tools can identify when an item is mislabeled regarding its type, material, or color. Finally, ADC aggregates the suggested labels recommended by the dataset curation software and removes potentially mislabeled or uncertain samples. For illustration, we adopt a data-centric label curation software (Docta) in Algorithm 1.\nThe high-level idea of this algorithm is to estimate the essential label noise transition matrix $T_{Est}$ without using ground truth labels, achieved through the consensus equations (Part A). Following this, Algorithm 1 identifies those corrupted instances via the cosine similarity ranking score among features as well as a well-tailored threshold based on the obtained information (i.e., $T_{Est}$), and then relabels these instances using KNN-based methods (Part B). For more details, please refer to work [25, 26, 27]."}, {"title": "2.2 Clothing-ADC", "content": "To illustrate the ADC pipeline, we present the Clothing-ADC dataset, which comprises a substantial collection of clothing images. The dataset includes 1,076,738 samples, with 20,000 allocated for evaluation, another 20,000 for testing, and the remaining samples used for training. Each image is provided at a resolution of 256x256 pixels. The dataset is categorized into 12 primary classes, encompassing a total of 12,000 subclasses, with an average of 89.73 samples per subclass. Detailed statistics of the dataset are provided in Table 1. The following subsection elaborates on the dataset construction process in comprehensive detail.\nSubclass design Utilizing GPT-4, we identified numerous attribute options for each clothing type. For example, in the case of sweaters, we recognized eight distinct attributes: color, material, pattern, texture, length, neckline, sleeve length, and fit type. The language model was able to find 30-50 options under each attribute. Our Clothing-ADC dataset includes the three most common attributes: color, material, and pattern, with each attribute having ten selected options. This results in 1000 unique subclasses per clothing type. The selected attributes are detailed in Table 6 (Appendix).\nData collection The ADC pipeline utilizes the Google Image API to collect clothing images by formulating queries that include attributes such as \"Color + Material + Pattern + Cloth Type\" (e.g., \"white cotton fisherman sweater\"). Figure 3 shows examples of these queries and the corresponding images retrieved. The relevance of the search results tends to decline after a significant number of samples are gathered, leading us to set a cutoff threshold of 100 samples per query. After removing broken links and improperly formatted images, each subclass retained approximately 90 samples. These queries generated noisy, webly-labeled data for the training set.\nCreating a test set Note that the collected samples may suffer from web-based label noise, where annotations might be incorrect due to mismatches provided by search engines, the traditional approach typically involves manually re-labeling existing annotations and aggregating multiple human votes per label to ensure a high-quality subset for testing purposes. Our ADC pipeline enhances efficiency by presenting annotators with a set of samples that share the same machine-generated label. Annotators are then tasked with selecting a subset of correctly labeled samples, choosing a minimum of four samples out of twenty. This method significantly reduces both manual effort and difficulty, encouraging annotators to critically evaluate machine-generated labels and thereby reducing the effect of human over-trust in AI answers [28, 18]. The samples selected through this process are considered \u201cclean\" labels, representing a consensus between human judgment and machine-generated labels [29]."}, {"title": "3 Challenge one: dealing with imperfect data annotations", "content": "The first pervasive and critical challenge during the automatic dataset construction lies in the prevalence of noisy/imperfect labels. This issue is intrinsic to web-sourced data, which, although rich in diversity, often suffers from inaccuracies due to the uncurated nature of the internet. These errors manifest as mislabeled images, inconsistent tagging, and misclassified attributes, introducing non-negligible noise into the dataset that may adversely affect the training and performance of machine learning models. The following discussion bridges the gap between imperfect data and curated data via mining and learning with label noise, to refine data quality, enhance label accuracy, and ensure the reliability of Auto-Dataset-Construction (ADC) for high-stakes AI applications.\nFormulation Let $D := {(x_n, y_n)}_{n \\in [N]}$ represent the training samples for a K-class classification task, where $[N] := {1,2, ..., N}$. Suppose that these samples ${(x_n, y_n)}_{n \\in [N]}$ are outcomes of the random variables $(X,Y) \\in X \\times Y$, drawn from the joint distribution $D$. Here, $X$ and $Y$ denote the spaces of features and labels, respectively. However, classifiers typically access a noisily labeled training set $\\tilde{D} := {(x_n, \\tilde{y}_n)}_{n \\in [N]}$, assumed to arise from random variables $(X, \\tilde{Y}) \\in X \\times \\tilde{Y}$, drawn from the distribution $\\tilde{D}$. It is common to observe instances where $y_n \\neq \\tilde{y}_n$ for some $n \\in [N]$. The transition from clean to noisy labels is typically characterized by a noise transition matrix $T(X)$, defined as $T_{i,j}(X) := P(Y = j | \\tilde{Y} = i, X)$ for all $i, j \\in [K]$ [12, 13, 33]."}, {"title": "3.1 The challenge of label noise detection", "content": "While employing human annotators to clean data is effective in improving label quality, it is often prohibitively expensive and time-consuming for large datasets. A practical alternative is to enhance label accuracy automatically by first deploying algorithms to detect potential errors within the dataset and then correcting these errors through additional algorithmic processing or crowdsourcing."}, {"title": "3.1.1 Existing approaches to detect label noise", "content": "Learning-Centric Approaches: Learning-centric approaches often leverage the behavior of models during training to infer the presence of label errors based on how data is learned. One effective strategy is confidence-based screening, where labels of training instances are scrutinized if the model's prediction confidence falls below a certain threshold. This approach assumes that instances with low confidence scores in the late training stage are likely mislabeled [34]. Another innovative technique involves analyzing the gradients of the training loss w.r.t. input data. Pruthi et al. [35] utilize gradient information to detect anomalies in label assignments, particularly focusing on instances where the gradient direction deviates significantly from the majority of instances. Researchers have also utilized the memorization effect of deep neural networks, where models tend to learn clean data first and only memorize noisy labels in the later stages of training. Techniques that track how quickly instances are learned during training can thus identify noisy labels by focusing on those learned last [36, 37, 38].\nData-Centric Approaches: Data-centric methods focus on analyzing data features and relationships rather than model behavior for detection. The ranking-based detection method [39] ranks instances by the likelihood of label errors based on their alignment with model predictions. An ensemble of classifiers evaluates each instance, flagging those that consistently deviate from the majority vote as noisy. Neighborhood Cleaning Rule [40] uses the k-nearest neighbors algorithm to check label consistency with neighbors, identifying instances whose labels conflict with the majority of their neighbors as potentially noisy. Zhu et al. [27] propose advanced data-centric strategies for detecting label noise without training models. Their local voting method uses neighbor consensus to validate label accuracy, effectively identifying errors based on agreement within the local feature space."}, {"title": "3.1.2 Clothing-ADC in label noise detection", "content": "We prepared a subset of 20,000 samples from the Clothing-ADC dataset for the label noise detection task, including both noisy and clean labels. We collected three human annotations for each image via Amazon MTurk. Annotators were instructed to classify the labels as correct, unsure, or incorrect. Each sample received three votes. Based on these annotations, we determined the noise rate to be 22.2%-32.7%. Using majority vote aggregation implies uncertainty of the label correctness. By using a more stringent aggregation criterion, more samples are considered as noisy labeled. Under the extreme case where any doubts from any human annotator can disqualify a sample, our auto collected dataset still retains 61.3% of its samples. For a detailed distribution of human votes, see Table 7 in the Appendix."}, {"title": "3.2 The challenge of learning with noisy labels", "content": "Another technique is robust learning that can effectively learn from noisy datasets without being misled by incorrect labels, thus maintaining high accuracy and reliability in real-world applications."}, {"title": "3.2.1 Existing approaches to learn with label noise", "content": "In this subsection, we contribute to the literature with robust learning software; all covered methods can be mainly summarized into the following three categories: robust loss functions, robust regularization techniques, and multi-network strategies.\nRobust loss designs Loss Correction modifies the traditional loss function to address label noise by incorporating an estimated noise transition matrix, thereby recalibrating the model's training focus [33]. Loss-Weighting strategies mitigate the impact of noisy labels by assigning lower weights to likely mislabeled instances, reducing their influence on the learning process [13, 44]. Symmetric Cross-Entropy Loss balances the contributions of correctly labeled and mislabeled instances, improving the model's resilience to label discrepancies [45]. Generalized Cross-Entropy Loss, derived from mean absolute error, offers enhanced robustness against outliers and label noise [46]. Peer Loss Functions form a family of robust loss functions [47, 48, 41], leveraging predictions from peer samples as regularization to adjust the loss computation, thereby increasing resistance to noise.\nRobust Regularization Techniques Regularization techniques are designed to constrain or modify the learning process, thereby reducing the model's sensitivity to label noise. Mixup [49] generates synthetic training examples by linearly interpolating between pairs of samples and their labels, enhancing model generalization and smoothing label predictions. Label Smoothing [50, 51] combats overconfidence in unreliable labels by adjusting them towards a uniform distribution. Negative Label Smoothing [52] refines this approach by specifically adjusting the smoothing process for negative labels, preserving model confidence in high-noise environments. Early-Learning Regularization tackles the issue of early memorization of noisy labels by dynamically adjusting regularization techniques during the initial training phase [37, 38].\nMulti-network strategies Employing multiple networks can enhance error detection and correction through mutual agreement and ensemble techniques. In Co-teaching, two networks concurrently train and selectively share clean data points with each other, mitigating the memorization of noisy labels [53]. MentorNet [54] equips a student network with a curriculum that emphasizes samples likely to be clean, as determined by the observed dynamics of a mentor network. DivideMix leverages two networks to segregate the data into clean and noisy subsets using a mixture model, allowing for targeted training on each set to manage label noise better [55]."}, {"title": "3.2.2 Clothing-ADC in label noise learning", "content": "We provide two versions of the Label Noise Learning task, Clothing-ADC and Clothing-ADC (tiny). Specifically, Clothing-ADC leverages the whole available (noisy) training samples to construct the label noise learning task. The objective is to perform class prediction w.r.t. 12 clothes types: Sweater, Windbreaker, T-shirt, Shirt, Knitwear, Hoodie, Jacket, Suit, Shawl, Dress, Vest, Underwear. We also provide a tiny version of Clothing-ADC, which contains 50K training images, sharing similar size with certain widely-used ones, i.e., MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, etc.\nEstimated noise level of Clothing-ADC We selected a subset of 20,000 training samples and asked human annotators to evaluate the correctness of the auto-annotated dataset. After aggregating three votes from annotators, we estimate the noise rate to be 22.2%-32.7%, which consists of 10.5% of the samples having ambiguity and 22.2% being wrongly labeled. The remaining 77.8% of the samples were correctly labeled. The detailed distribution of human votes is given in Appendix Table 7.\nBenchmark efforts In this task, we aim to provide the performance comparison among various learning-with-noisy-label solutions. All methods utilize ResNet-50 as the backbone model and are trained for 20 epochs to ensure a fair comparison. We report the model prediction accuracy on the held-out clean labeled test set. For the tiny version, we"}, {"title": "4 Challenge two: dealing with imbalanced data distribution", "content": "We now discuss another real-world challenge: when imperfect annotations meet with imbalanced class/attribute distributions. As shown in Figure 4, long-tailed data distribution is a prevalent issue in web-based datasets: to collect a dataset of wool suits without a specified target color on Google Image, the majority would likely be dark or muted shades (grey, black, navy), with few samples in brighter colors like pink or purple. This natural disparity results in most data points belonging to a few dominant categories, while the remaining are spread across several minority groups.\nWe are interested in how class-imbalance intervenes with learning. In real-world scenarios, the distribution of classes tends to form a long-tail form, in other words, the head class and the tail class differ significantly in their sample sizes, i.e., $\\max_k P(Y = k) \\gg \\min_{k'} P(Y = k').$"}, {"title": "4.1 Existing approaches for class imbalance learning", "content": "Data-Level Methods Data-level methods modify training data to balance class distribution, focusing on adjusting the dataset by increasing minority class instances or decreasing majority class instances. Oversampling increases the number of minority class instances to match or approach the majority class. This can be done through simple duplication [59] (e.g., random oversampling) or generating synthetic data [60, 61, 62, 63]. Undersampling reduces the number of majority class instances, helping to balance class distributions but potentially discarding useful information [64, 65, 66].\nAlgorithm-Level Methods These methods adjust the training process or model to handle unequal class distributions better. Specifically, cost-sensitive learning assigns different costs to misclassifications of different classes, imposing higher penalties for errors on the minority class [67]. It modifies the loss function to incorporate misclassification costs, encouraging the model to focus more on minority class errors [68, 69]. Thresholding adjusts the decision threshold for class probabilities to account for class imbalance. Instead of using a default threshold, different thresholds are applied based on class distribution, modifying the decision process for predicting class labels [70, 71]."}, {"title": "4.2 Clothing-ADC in class-imbalanced learning", "content": "Note that in the label noise learning task, the class distributes with almost balanced prior. However, in practice, the prior distribution is often long-tail distributed. Hence, the combined influence of label noise and long-tail distribution is a new and overlooked challenge presented in the literature. To facilitate the exploration of class-imbalanced learning, we tried to reduce the impact of noisy labels via selecting high-quality annotated samples as recognized by dataset curation software. Human estimation suggested a noise rate of up to 22.2%, and 10.5% marked as uncertain. To address this, we employed two methods to remove noisy samples: a data centric curation (Algorithm 1), which removed 26.36% of the samples, and a learning-centric curation (Appendix Algorithm 3), which removed 25%. Combined, these methods eliminated 45.15% of the samples, with an overlap of 6.21% between the two approaches. We provide Clothing-ADC CLT, which could be viewed as the long-tail (class-level) distributed version of Clothing-ADC. Denote by p the imbalanced ratio between the maximum number of samples per class and the minimum number of samples per class. In practice, we provide $p = 10, 50, 100$ (class-level) long-tail version of Clothing-ADC.\nBenchmark efforts Regarding the evaluation metric, we follow from the recently proposed metric [72], which considers an objective that is based on the weighted sum of class-level performances on the test data, i.e., $\\sum_{i \\in [K]} g_i Acc_i$, where $Acc_i$ indicates the accuracy of the class i:\n$\\delta$-worst accuracy: $\\min_{i \\in [K]} \\sum_{i \\in [K]} g_i Acc_i, s.t. D(g, u) < \\delta$.\nHere, $\\Delta_K$ denotes the (K \u2013 1)-dimensional probability simplex, where K is the number of classes as previously defined. Let $u \\in \\Delta_{\\kappa}$ be the uniform distribution, and $g := [g_1, g_2, ..., g_k]$ is the class weights. The $\\delta$-worst accuracy measures the worst-case g-weighted performance with the weights constrained to lie within the $\\delta$-radius ball around the target (uniform) distribution. For any chosen divergence D, it reduces to the mean accuracy when $\\delta \\rightarrow 0$ and to the worst accuracy for $\\delta \\rightarrow \\infty$. The objective interpolates between these two extremes for other values of $\\delta$ and captures our goal of optimizing for variations around target priors instead of more conventional objectives of optimizing for either the average accuracy at the target prior or the worst-case accuracy.\nDifferent from the previous dataset we used in noise learning, we use a cleaner dataset for this class-imbalance learning to avoid the distractions of noisy labels. The size of this dataset consists of 56,2263 images rather than 1M. The backbone model we use is ResNet-50. For the class distributions for different p, we include them in the Appendix. All the experiments are run for 5 times and we calculate the mean and standard deviation. With the imbalance ratio going larger, the accuracy becomes worse, which is expected for a more difficult task."}, {"title": "5 Conclusion", "content": "In this paper, we introduced the Automatic Dataset Construction (ADC) pipeline, a novel approach for automating the creation of large-scale datasets with minimal human intervention. By leveraging Large Language Models for detailed class design and automated sample collection, ADC significantly reduces the time, cost, and errors associated with traditional dataset construction methods. The Clothing-ADC dataset, which comprises one million images with rich category hierarchies, demonstrates the effectiveness of ADC in producing high-quality datasets tailored for complex research tasks. Despite its advantages, ADC faces challenges such as label noise and imbalanced data distributions. We addressed these challenges with open-source tools for error detection and robust learning. Our benchmark datasets further facilitate research in these areas, ensuring that ADC remains a valuable tool for advancing machine learning model training."}, {"title": "Broader impacts", "content": "Our paper introduces significant advancements in dataset construction methodologies, particularly through the development of the Automatic Dataset Construction (ADC) pipeline:\n\u2022 Reduction in Human Workload: ADC automates the process of dataset creation, significantly reducing the need for manual annotation and thereby decreasing both the time and costs associated with data curation.\n\u2022 Enhanced Data Quality for Research Communities: ADC provides high-quality, tailored datasets with minimal human intervention. This provides researchers with datasets in the fields of label noise detection, label noise learning, and class-imbalanced learning, for exploration as well as fair comparisons.\n\u2022 Support for Customized LLM Training: The ability to rapidly generate and refine datasets tailored for specific tasks enhances the training of customized Large Language Models (LLMs), increasing their effectiveness and applicability in specialized applications.\nFurthermore, the complementary software developed alongside ADC enhances these impacts:\n\u2022 Data Curation and Quality Control: The software aids in curating and cleaning the collected data, ensuring that the datasets are of high quality that could compromise model training.\n\u2022 Robust Learning Capabilities: It incorporates methods for robust learning with collected data, addressing challenges such as label noise and class imbalances. This enhances the reliability and accuracy of models trained on ADC-constructed datasets.\nTogether, ADC and its accompanying software significantly advance the capabilities of machine learning researchers and developers by providing efficient tools for high-quality customized data collection, and robust training."}, {"title": "Limitations", "content": "While ensuring the legal and ethical use of datasets, including compliance with copyright laws and privacy concerns, is critical, our initial focus is on legally regulated and license-friendly data sources available through platforms like Google or Bing. Addressing these ethical considerations is beyond the current scope but remains an essential aspect of dataset usage.\nBesides, similar to Traditional-Dataset-Construction (TDC), Automatic-Dataset-Construction (ADC) is also unable to guarantee fully accurate annotations."}]}