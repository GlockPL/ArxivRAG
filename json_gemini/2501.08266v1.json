{"title": "AI-Driven Water Segmentation with deep learning models for Enhanced Flood Monitoring", "authors": ["Sanjida Afrin Mou", "Tasfia Noor Chowdhury", "Adib Ibn Mannan", "Sadia Nourin Mim", "Lubana Tarannum", "Tasrin Noman", "Jamal Uddin Ahamed"], "abstract": "Flooding is a major natural hazard causing significant fatalities and economic losses annually, with increasing frequency due\nto climate change. Rapid and accurate flood detection and monitoring are crucial for mitigating these impacts. This study\ncompares the performance of three deep learning models-U-Net, ResNet, and DeepLab v3 for pixel-wise water\nsegmentation to aid in flood detection, utilizing images from drones, in-field observations, and social media. This study\ninvolves creating a new dataset that augments well-known benchmark datasets with flood-specific images, enhancing the\nrobustness of the models. The U-Net, ResNet, and DeepLab v3 architectures are tested to determine their effectiveness in\nvarious environmental conditions and geographical locations and the strengths and limitations of each model are also\ndiscussed here, providing insights into their applicability in different scenarios by predicting image segmentation masks. This\nfully automated approach allows these models to isolate flooded areas in images, significantly reducing processing time\ncompared to traditional semi-automated methods. The outcome of this study is to predict segmented mask for each image\neffected by flood disaster and validation accuracy of these models are DeepLab-0.9057, ResNet-0.8870, U-Net-0.8712. This\nmethodology facilitates timely and continuous flood monitoring, providing vital data for emergency response teams to reduce\nloss of life and economic damages. It offers a significant reduction in the time required to generate flood maps, cutting down\nthe manual processing time. Additionally, we present avenues for future research, including the integration of multi-modal\ndata sources and the development of robust deep learning architectures tailored specifically for flood detection tasks. Overall,\nour work contributes to the advancement of flood management strategies through innovative use of deep learning\ntechnologies.", "sections": [{"title": "1. Introduction", "content": "Floods are among the most disastrous natural catastrophes, inflicting\nmajor damage to infrastructure, loss of life, and economic turmoil. Early\nidentification and monitoring play a crucial role in effective catastrophe\nmanagement and mitigation. Conventional flood detection systems usually\nrely on manual observation, which is time-consuming and restricted. Recent\nadvancements in deep learning algorithms open up new possibilities for\nautomated flood detection using drone or satellite imagery.\nThis paper discusses the potential of deep learning methods in water\nsegmentation for enhanced flood monitoring. Using these techniques, a\npossible comprehensive system can be developed that will rightfully detect the\nflooded zones in photos to assess actionable information for authorities and\nstakeholders. The increasing deployment of low-cost optical satellites, such as\nCubeSats, has further enabled the application of machine learning for water\nidentification in optical and multispectral imaging (Mateo-Garcia et al.,2019;\nLiu Yang et al., 2015; Yang Chen et al., 2018). Yet, despite such\ndevelopments, much flood analysis remains manual or semi-automated and is\nprovided by organizations like the United Nations Institute for Training and\nResearch - Operational Satellite Applications Program through the provision\nof a 'Rapid Mapping' service (Edoardo Nemni et al., 2020). Flood detection\nwith water segmentation enhances monitoring and prediction capabilities that\ninvolve quick action, improve disaster preparedness, and support decision-\nmaking in risk management. This will not only improve public safety through\nearly warnings but will also help in designing resilient infrastructure, land use\nplanning, and reducing costs due to floods. The proposed method enhances the\nstudy of image processing and computer vision for community awareness and\nwell-informed decisions.\nDeep learning uses multi-layered neural networks to find patterns in data.\nThese algorithms mimic the thought process of the human brain, making them\nparticularly efficient in the analysis of flood imagery (Janis BARZDINS et al.\n2024). Integration of water segmentation with deep learning is integrally\nimportant in enhancing disaster preparedness, minimizing economic losses, and\nsaving lives by availing accurate and efficient flood detection means. Some\nscholars have found a hybrid approach with robust similarity scores in the flood\nmonitoring of Malaysia, which improves the regional flood monitoring systems\n(Muhadi et al., 2020) others used anisotropic diffusion segmentation and\nSVM to process the satellite images for disaster management, with the help of\nmorphological operation to enhance the performance of flood monitoring (FU\net al., 2010). A CNN-based method for rapid flood mapping using\nSentinel-1 SAR imagery was proposed, reducing map development time\nby 80% and enabling accurate monitoring across diverse conditions\n(Nemni et al., 2020). Convolutional and recurrent neural networks were\nutilized to predict flash flood probability in Golestan Province, Iran,\nwith CNN models achieving higher accuracy through geospatial\ndatabases and SWARA techniques (Panahi et al., 2021). Some efforts\nfocused on using LBP, HOG, and pre-trained VGG-16 for floodwater detection\non roadways, with VGG-16 and logistic regression showing superior\nperformance. FCN outperformed superpixel-based methods for segmentation,\nenhanced further by CRF (Sarp et al., 2022). A modified U-NET model, U-\nFLOOD, was developed to predict 2D water depth maps in urban floods using\nhyetographs and topographical data, delivering fast and accurate predictions\n(L\u00f6we et al., 2021). Another approach involved applying CNNs like YOLOv3\nand Fast R-CNN for flood label detection with connected vision systems,\nintegrating edge detection and aspect ratio analysis for real-time monitoring\n(Pally & Samadi, 2022). A fully automated end-to-end system for predicting\nflood stage data employed U-Net CNNs for segmentation and LSTMs for time-\nseries prediction, achieving high accuracy in real-time forecasts (Windheuser et\nal., 2023). Additionally, a NN-SGW hybrid model was introduced for flood\ninundation mapping in data-scarce regions, identifying key environmental\nvariables and achieving enhanced performance in urban flood prediction and\nsusceptibility assessment (Darabi et al., 2021)."}, {"title": "2. Materials", "content": "In this study, the focus is on collecting a comprehensive dataset\nspecifically designed for flood detection and monitoring using deep\nlearning techniques. The dataset consists of two primary components:\nactual flood area images and corresponding mask images. Here, a\ndetailed overview of the data collection process is provided.\nThe flood area images were collected from various online sources,\nincluding public datasets such as Kaggle, social media platforms, and\nopen-access satellite imagery repositories (showing in Figure 1. 1 and\nFigure 1.2 Original mask image example. These sources were chosen\nto ensure a diverse set of images representing different types of flood\nscenarios and geographical locations. The dataset comprises 290 high-\nresolution images of areas affected by flooding. These images were\nselected to include a wide range of flood characteristics, such as urban\nand rural settings, different water levels, and various types of\nflooding, including riverine floods, flash floods, and coastal flooding.\nCorresponding to each actual flood area image, a mask image was\ngenerated. These mask images were either obtained from existing\nannotated datasets or created manually using image annotation tools.\nIn cases where masks were created manually, experts in the field of\nremote sensing and image processing annotated the water bodies in\nthe images. Each mask image is a binary representation of the actual\nflood area image. Pixels representing water bodies are assigned the\nvalue 1 (white) while regions that do not hold water are assigned the\nvalue 0 (black). This binary notation is essential when training\nsegmentation models because it endows these models with the\nlearning capability to distinguish flooded from unflooded regions."}, {"title": "3. Methodology", "content": "The research question calls for the development of a structured work\nstrategy or plan. In this chapter, we deduce a work plan by examining the\nexisting literature on flood detection, water segmentation, and predicting flood\neffects using a deep learning model. Error! Reference source not found.\npresents a flowchart that visually outlines the entire working procedure for the\nresearch. By following this organized approach, we can effectively carry out\nthe investigation and achieve our research objectives."}, {"title": "3.1. Preprocessing", "content": "Data preprocessing is vital to convert raw data into a usable format for\nanalysis using deep learning. It involves data cleaning, normalizing\nnumerical features, encoding categorical variables, and reducing\ndimensionality. The dataset is split into training and testing sets, with\ntechniques like cross-validation ensuring robustness. For image data,\naugmentation is used to enhance the dataset. Handling imbalanced data\nthrough resampling or synthetic data generation ensures balanced class\ndistributions. This process improves model accuracy, reduces complexity,\nand enhances generalization, leading to reliable results."}, {"title": "3.1.1. Image Resizing", "content": "The actual images are 1024x1024 which is too bigger for the process so it\nshould be resized to a standard dimension (256x256) to ensure uniformity\nacross the dataset. This helps in reducing computational complexity and\nensuring compatibility with the neural network input requirements."}, {"title": "3.1.2. Augmentation", "content": "Various data augmentation techniques, such as rotation, flipping, blurry\neffect, gray scale effect, and scaling, are applied to increase the diversity of\nthe training dataset as shown in Fig. 1. This helps in making the model more\nrobust and generalizable and make relatable with real-world images."}, {"title": "3.1.3. Normalization", "content": "The values are normalized on a specific range (normally 0-1) in order to\nfacilitate a better convergence of a neural network model during the training\nphase."}, {"title": "3.1.4. Convert to channel 1 image", "content": "To carry out a binary segmentation task, mask images are reformatted into\nsingle-channel. This stage makes sure that the mask images are ready for use\nwith deep learning models, thus cutting down on the time needed for\nprocessing."}, {"title": "3.2. Combined datasets", "content": "After preprocessing, the actual and mask images are combined into a\nunified dataset. This combined dataset is necessary for training and testing\nthe deep learning models, which will provide paired inputs-actual\nimages and outputs-mask images-for supervised learning."}, {"title": "1.1. Datasets splitting", "content": "The dataset as a whole is divided into different subsets so as to test the\nmodel on new data and avoid overfitting. We utilized various data\naugmentation techniques to enhance the variability of the dataset. These\naugmentations improve the diversity of training data, enabling our deep\nlearning model to perform more effectively and adapt to a wider range of\nscenarios. General ratios of cuts like 80,20 are employed. We used here Train\nDataset: 80%, Validation Dataset: 20%. The set of data which has been trained\nis utilized for training purposes of the models while the set of data that is\ntested is utilized to measure their generalization ability."}, {"title": "1.2. Model selection and training", "content": "For this particular study, three deep learning models have been picked\nsince they have been validated to perform tremendous jobs on the said area of\nimage segmentation. The architecture for DeepLabv3, U-Net, and ResNet-50\nis done using TensorFlow and Keras. These models are created with suitable\nlayers, activation functions, and initializers."}, {"title": "1.2.1. DeepLab v3", "content": "Deep Lab (Fig. 3) is a state-of-the-art model for semantic image\nsegmentation. It uses an atrous convolution to capture multi-scale context\nby probed image at multiple sampling rates. This ability to capture context at\ndifferent scales makes DeepLab highly effective for segmentation tasks."}, {"title": "1.2.2. U-Net", "content": "U-Net (Fig. 3) is a Convolutional Network architecture for biomedical\nimage segmentation. It contains a unique U-shaped architecture, consisting\nof a contracting path to capture context and a symmetric expanding path for\nprecise localization, enabling it to segment images very effectively by\ncapturing both low-level and high-level features."}, {"title": "1.2.3. ResNet-50", "content": "ResNet-50 (Fig. 4) is a deep residual network with 50 layers and skip\nconnections to prevent vanishing gradients, enabling efficient training of deep\nnetworks. Its residual blocks enhance pattern recognition, improving accuracy\nand efficiency. ResNet-50's architecture has inspired many models and\nremains foundational in deep learning advancements."}, {"title": "1.3. Evaluation Metrics and Configuration", "content": "The performance of the deep learning models was assessed\nusing various metrics, including accuracy, precision, recall, and F1-\nscore, to determine their effectiveness in flood segmentation tasks. The\nevaluation of the models was performed on a test dataset that included\ndiverse flood scenarios.\nThe metrics include validation loss, validation accuracy, precision,\nrecall, and F1 score."}, {"title": "1) Validation Loss", "content": "This metric indicates how well the model performs on the\nvalidation dataset. Lower values are better and suggest that the model\nis not overfitting and is generalizing well."}, {"title": "2) Validation Accuracy", "content": "This measures the proportion of correctly predicted instances\namong the total instances in the validation dataset. Higher values\nindicate better performance."}, {"title": "3) Precision", "content": "Precision is the ratio of true positive predictions to the summation\nof true positive and false positive predictions. It measures the accuracy\nof positive predictions."}, {"title": "4) Recall", "content": "Recall, also known as sensitivity or true positive rate, The ratio of\ntrue positive predictions to the total of true positive and false negative\npredictions. It measures the ability of the model to find all relevant\ninstances."}, {"title": "5) F1 Score", "content": "The F1 score is calculated by taking the harmonic mean of recall\nand precision. Both false positives and false negatives are considered in\nthis fair metric."}, {"title": "1.4. Optimization", "content": "In training deep learning models for flood detection to learn how to\nidentify flood pixels from various image sources (drones, field\nobservations, social media), three key techniques (epoch, adam\noptimizer, early stopping) work together to achieve the best possible\nresults. The first is epochs, which represent how many times the entire\ntraining dataset (images containing floods) is shown to the model. More\nepochs allow the model to learn more intricate patterns in the data, like\nthe subtle differences between water and land. However, too many\nepochs can lead to overfitting, where the model becomes overly focused.\ntraining images and struggles to identify floods in new, unseen data. The Adam\nalgorithm (Adaptive Moment Estimation) works alongside epochs and early\nstopping during the training process of flood detection training by preventing\noverfitting and ensuring the models generalize well to new flood images. If the\nvalidation performance does not improve for a specified number of epochs\n(defined by the patience parameter), early stopping is triggered."}, {"title": "2. Result", "content": "Fig. 7 demonstrates the water segmentation results from three deep\nlearning models: DeepLabv3, U-Net, and ResNet. While all models produced\nsatisfactory segmentation masks, DeepLabv3 performed better in\ndistinguishing between water and non-water bodies. In terms of accuracy,\nDeepLabv3 achieved the highest accuracy at 90.57%, followed by ResNet at\n88.7%, and U-Net at 87.12%. The accuracy was calculated using the formula:\nHere, True Positive (TP) represents pixels correctly identified as \"water,\" True\nNegative (TN) represents pixels correctly identified as \"others,\" False Positive\n(FP) refers to \"water\" pixels mislabeled as \"others,\" and False Negative (FN)\nrefers to \"others\" pixels mislabeled as \"water.\" This analysis highlights\nDeepLabv3's superior performance for water segmentation tasks."}, {"title": "3. Discussion", "content": "The models were evaluated based on their ability to accurately segment\nflood-affected areas. Metrics such as accuracy, precision, recall, and F1-score\nwere used to quantify the performance.\nError! Reference source not found. presents a variety of performance measures\nfor deep learning models. U-Net, ResNet, and DeepLab models show\nsufficient accuracy and reliability for flood detection and monitoring,\nindicating wide applicability of other approach. The capabilities of segmented\nreconstruction of depths under different conditions also outperformed the\nquality of traditional methods. The results suggest that deep learning has a\ngreat potential to improve disaster management by facilitating quicker action\nand intervention during response to\nfloods. Although DeepLabv3 shows the highest validation loss, it still shows\nexcellent performance in terms of general accuracy and precision. On the other\nhand, U-Net has the lowest validation loss, indicating good generalization of\nthis model on the validation set. However, it has less accuracy and a lower F1\nscore compared to the other two models, which implies that it would be less\nreliable in predicting positive cases as compared to DeepLabv3 and Res-Net.\nResNet is balanced with a low validation loss, high validation accuracy, and\nacceptable precision and recall values. It obtains the second-best F1 score,\nplacing it as a strong model for flood segmentation tasks. Figure 8: \"Training\nand Validation Loss\" on the left and \"Training and Validation Accuracy\" on the\nright, both plotted against epochs for the three models: DeepLabv3, U-Net, and\nResNet-50. The following charts elucidate the learning behavior of each model\nduring training. DeepLabv3 shows stable convergence with the lowest\nvalidation loss and highest validation accuracy; hence, it generally performs\nwell and indicates effective generalization. In contrast, U-Net reveals the\nlowest training loss, although it has a bit higher validation loss, which might\ncause overfitting. ResNet-50 presents balanced performances in terms of\naccuracy and loss, remaining stable and becoming one of the robust models for\nthe flood segmentation task."}, {"title": "4. Conclusion", "content": "In conclusion, this work highlights the potential of deep learning\nmodels-U-Net, ResNet, and DeepLabv3-for effective and efficient flood\ndetection and monitoring. This automation method substantially shortens the\ntime required for flood mapping and improves precision in identifying\nflooded areas. The enhanced dataset, complemented by detailed model\nevaluation, provides valuable resources for future research efforts. All deep\nlearning segmentation models show average results for segmentation\nhowever, DeepLabv3 shows that it has the highest validation accuracy and a\nbetter F1 score, which makes it the best model for flood segmentation tasks\namong the three.\nU-Net has the lowest validation loss, indicating the best generalization to\nthe validation dataset, however, it performs lower in accuracy and F1 score.\nOverall ResNet has a good balance in terms of parameters such as the\nvalidation loss, accuracy, precision, recall, and F1 score. This comprehensive\nreview provided some degree of insight into the advantages and drawbacks\nof each model along with recommendations for future improvements and\nimplementations. The study shows that flood detection can be achieved\nreliably without a costly setup, making it achievable for wider\nimplementation in resource-constrained contexts. By automating the flood\ndetection process, the technology decreases the possibility for human error in\nmanual flood mapping, resulting to more accurate and dependable findings.\nFuture advancements in flood detection and monitoring systems are likely\nto involve significant innovations in the integration of multi-modal data\nsources, such as the combination of optical, radar, and satellite-based thermal\nimaging. This fusion of diverse datasets would enhance the robustness and\naccuracy of models by leveraging the strengths of each modality-for\ninstance, radar's ability to penetrate cloud cover and optical imaging's high\nspatial resolution. Simultaneously, progress in AI and machine learning will\nallow the creation of AI algorithms that can process huge quantities of data\nmore efficiently. Such systems could detect patterns and anomalies within real\ntime data to allow for faster detection and prediction of flooding events.\nBetter computational infrastructure, including but not limited to edge\ncomputing and cloud-based solutions, may significantly reduce data\nprocessing time, enabling real-time monitoring and the rapid distribution of\nearly warning systems to impacted areas.\nMoreover, a network of Internet of Things (IoT) devices, such as ground-\nbased sensors for measuring water levels and rainfall intensity if sare\nharnessed would complement satellite data, creating a comprehensive and\ninterconnected monitoring network. Future systems may also utilize crowd-\nsourced data\nsuch as social media posts and photos\nto enhance\nsituational awareness and validation on the ground. In addition to detecting\nimminent threats, emerging communication technologies, such as 5G and low\nEarth orbit (LEO) satellite networks will allow alerts to be disseminated\nquickly to the affected communities, emergency response teams, and"}], "equations": [{"equation": "Accuracy = \\frac{True Positives + True Negative}{True Positives + True Negative + False Positives + False Negatives}"}, {"equation": "Precision = \\frac{True Positives}{True Positives + False Positives}"}, {"equation": "Recall = \\frac{True Positives}{True Positives + False Negatives}"}, {"equation": "F1 Score = 2x\\frac{Precision \u00d7 Recall}{Precision + Recall}"}, {"equation": "Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}"}]}