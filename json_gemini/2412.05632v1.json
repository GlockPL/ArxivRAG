{"title": "Biological Brain Age Estimation using Sex-Aware Adversarial Variational Autoencoder with Multimodal Neuroimages", "authors": ["Abd Ur Rehman", "Azka Rehman", "Muhammad Usman", "Abdullah Shahid", "Sung-Min Gho", "Aleum Lee", "Tariq M. Khan", "Imran Razzak"], "abstract": "Brain aging involves structural and functional changes and therefore serves as a key biomarker for brain health. Combining structural magnetic resonance imaging (sMRI) and functional magnetic resonance imaging (fMRI) has the potential to improve brain age estimation by leveraging complementary data. However, fMRI data, being noisier than sMRI, complicates multimodal fusion. Traditional fusion methods often introduce more noise than useful information, which can reduce accuracy compared to using sMRI alone. In this paper, we propose a novel multimodal framework for biological brain age estimation, utilizing a sex-aware adversarial variational autoencoder (SA-AVAE). Our framework integrates adversarial and variational learning to effectively disentangle the latent features from both modalities. Specifically, we decompose the latent space into modality-specific codes and shared codes to represent complementary and common information across modalities, respectively. To enhance the disentanglement, we introduce cross-reconstruction and shared-distinct distance ratio loss as regularization terms. Importantly, we incorporate sex information into the learned latent code, enabling the model to capture sex-specific aging patterns for brain age estimation via an integrated regressor module. We evaluate our model using the publicly available OpenBHB dataset, a comprehensive multi-site dataset for brain age estimation. The results from ablation studies and comparisons with state-of-the-art methods demonstrate that our framework outperforms existing approaches and shows significant robustness across various age groups, highlighting its potential for real-time clinical applications in the early detection of neurodegenerative diseases.", "sections": [{"title": "I. INTRODUCTION", "content": "The aging population poses significant global challenges, profoundly affecting economic, medical, and societal systems [1]. Among these challenges, declining brain function and neurodegenerative diseases in the elderly exacerbate the burden on healthcare and social structures [2]. In the fields of life sciences and biomedicine, predicting and assessing age-related neurodegeneration, as well as developing treatments to mitigate or reverse its effects, remain critical research priorities [3]. A key focus is the distinction between biological brain age and chronological age, which serves as an informative biomarker for neurological disorders such as Parkinson's disease [4], vascular dementia [5], mild cognitive impairment (MCI), and Alzheimer's disease (AD) [6]. Deviations from the typical brain aging trajectory are particularly significant, as they can predict an individual's future risk of developing neurodegenerative conditions [7]. Modeling brain aging patterns through neuroimaging data and tracking individual trajectories offers a promising approach to understanding brain aging dynamics and its implications for neurological health [8].\nNeuroimaging techniques, particularly magnetic resonance imaging (MRI), are powerful tools for studying age-related changes in brain structure and function, such as morphometric atrophy and reduced connectivity [9]. Multimodal MRI, which combines structural MRI (sMRI) and functional MRI (fMRI), provides complementary insights by capturing both structural and functional aspects of the living human brain under noninvasive or minimally invasive conditions. This approach has been widely employed in research on brain aging and the clinical diagnosis of neurological diseases [10]. However, fMRI presents challenges due to its relatively low spatial resolution, high noise levels, and the immature and rapidly changing nature of functional connectivity derived from it. These limitations make it impractical or ineffective to directly fuse sMRI and fMRI data using conventional multimodal fusion strategies [11]. In some cases, such strategies can even diminish the accuracy of sMRI-derived features, which are well-established as robust biomarkers for biological age estimation. Therefore, effective fusion of sMRI and fMRI data requires approaches that minimize the negative impact of one modality on the other during the fusion process, ensuring that each modality's strengths are preserved and effectively utilized.\nDespite the potential of multimodal neuroimaging, research on its use for brain age estimation, particularly in exploring anatomical and functional differences between male and female brains, has been limited. Our study aims to address this gap by estimating brain age using both fMRI and sMRI data, identifying potential indicators of brain development through various imaging modalities. The inherent differences between SMRI and fMRI, including noise levels, spatial resolution, and the dynamic nature of functional connectivity, highlight the need for innovative fusion techniques that leverage sMRI's precision and reliability as biomarkers for age prediction [12], [13].\nTo overcome these challenges, it is essential to address the role of sex information in brain age estimation. Figure 1 shows samples of male and female brain sMRI and fMRI scans across different ages. Research indicates significant anatomical and functional differences between male and female brains, which influence the aging process [14], [8]. Although sex information has proven effective when incorporated into deep learning models [15], its integration into multimodal neuroimaging studies has been scarce. Our work incorporates sex information into a multimodal framework, enhancing the precision and robustness of brain age estimation.\nTo address these challenges, we introduce a novel multimodal learning framework, the Sex-Aware Adversarial Variational Autoencoder (SA-AVAE). The SA-AVAE framework uniquely integrates adversarial and variational autoencoder losses to disentangle shared and modality-specific features in a latent space, effectively reducing noise and improving feature representation. This disentanglement ensures that the shared latent space captures complementary information across sMRI and fMRI, while the modality-specific latent spaces retain unique characteristics, minimizing interference during fusion. Furthermore, the adversarial component suppresses noise by enforcing the separation of irrelevant features, while the variational component enhances feature disentanglement by learning distinct probabilistic distributions for shared and unique features.\nAdditionally, the SA-AVAE framework incorporates sex information into the latent space, leveraging known anatomical and functional differences between male and female brains [14], [8]. By integrating sex information, the model not only improves accuracy but also enhances robustness by accounting for gender-related variations. This approach represents a significant advancement over traditional fusion techniques that often overlook such critical demographic factors.\nFollowing are the key contributions of this study:\n\u2022 We propose a multimodal framework, named the Sex-Aware Adversarial Variational Autoencoder (SA-AVAE), to estimate brain age using multimodal MRI scans (sMRI and fMRI). This framework is the first to integrate sex information directly into the latent space, enhancing the model's accuracy and robustness.\n\u2022 The proposed framework generates a disentangled latent space by enforcing distinct distributions through the simultaneous application of adversarial and variational autoencoder losses within a unified architecture.\n\u2022 We present a comprehensive strategy for fine-tuning loss weight parameters, providing valuable insights for optimizing architectures in broader applications.\n\u2022 Extensive experimentation on publicly available datasets demonstrates the accuracy and robustness of the SA-AVAE model, setting a new standard for brain age estimation techniques.\nThe remainder of the paper is organized as follows: Section II presents a comprehensive review of related studies, providing the context and background for our work. Section III details the proposed framework, including its various components and the underlying principles. Section IV elaborates on the dataset specifications and preprocessing steps, as well as the architectural details and training strategy of the model. Section V showcases the experimental results, accompanied by an in-depth analysis. Finally, Section VI concludes the paper by summarizing the key findings and discussing future research directions, with a focus on potential improvements and extensions to the proposed approach."}, {"title": "II. RELATED WORK", "content": "The application of deep learning techniques for brain age estimation using MRI has seen significant advancements in recent years. In particular, Convolutional Neural Networks (CNNs) have emerged as the primary architecture for estimating brain age from structural MRI (sMRI) data, offering the ability to learn complex image features directly from the data. Early works in this domain employed 2D CNNs applied to individual slices of sMRI scans [24], while more recent studies have adopted 3D CNNs, which analyze the entire brain to capture more comprehensive structural features for brain age prediction [17]. Dinsdale et al. [25] introduced a 3D CNN model that leveraged whole-brain MRI data, demonstrating improved accuracy by capturing global structural features related to brain aging.\nHowever, training deep learning models on small datasets remains a significant challenge. To address this, Peng et al. [26] proposed a 3D fully convolutional network that reduces model complexity by using fewer parameters, thereby improving efficiency without compromising prediction accuracy. Cheng et al. [18] further improved performance by introducing a two-stage age prediction network (TSAN), which first provides a rough estimation followed by a refinement stage. This multi-stage approach allows the model to progressively enhance its representations of brain aging.\nIn addition to CNN-based architectures, the incorporation of attention mechanisms has shown promise in improving brain age prediction. He et al. [17] introduced FiA-Net, a fusion-with-attention model that integrates both intensity and RAVENS channels from sMRI data. The attention mechanism enabled the model to focus on the most informative regions, thereby improving its predictive performance. Later, He et al. [27] proposed a global-local transformer model, which combines global contextual information with fine-grained local features extracted from patches of sMRI data. This approach employs self-attention mechanisms to enhance the representation of brain aging patterns by dynamically adjusting attention to both global and local aspects of the data.\nBeyond CNNs and attention-based methods, graph neural networks (GNNs) have also been explored for brain age estimation, primarily due to their ability to model the brain as a network of regions of interest (ROIs). Pina et al. [28] proposed a GNN-based model that represents the brain as a graph, where ROIs are treated as nodes, and the dependencies between regions are modeled as edges. This structural representation allows for the prediction of brain age by capturing the inter-dependencies between different brain regions. Similarly, Xu et al. [29] used a graph-based model on diffusion tensor imaging (DTI) data to study the structural connectivity of brain aging, showing the potential of graph models in capturing complex relationships between brain regions.\nThe integration of multimodal MRI data has emerged as a promising approach to enhance the accuracy of brain age estimation. Structural MRI (sMRI) and functional MRI (fMRI) provide complementary information, where sMRI captures structural features, and fMRI offers insights into functional connectivity. Early studies, such as those by Irimia et al. [30], combined cortical thickness from sMRI with structural connectivity features to predict brain age using multivariate regression. Liem et al. [16] proposed a stacked multimodal approach that integrates cortical anatomy from sMRI with functional connectivity derived from resting-state fMRI, demonstrating improved brain age prediction performance. Cherubini et al. [31] further investigated the fusion of T1-weighted MRI, T2-relaxometry, and fMRI using a voxel-based multiple regression model, highlighting the potential of multimodal data in capturing brain aging patterns.\nBuilding upon these foundational approaches, Cole et al. [32] developed a more comprehensive multimodal brain age model by integrating various MRI modalities, including T1-weighted MRI, T2-FLAIR, T2*, and diffusion MRI, along with resting-state fMRI. Although this approach demonstrated promising results, it relied on hand-crafted features and complex preprocessing steps, which limited its adaptability to new datasets. More recently, Mouches et al. [20] proposed a fusion model combining sMRI with time-of-flight magnetic resonance angiography (TOF MRA) data using a simple fully convolutional network (SFCN) alongside linear regression (LR), thus simplifying the integration process while maintaining robust performance.\nThough recent advancements, traditional multimodal approaches often rely on simply concatenating features from different modalities without explicitly modeling their interactions. This limitation can overlook the potential synergies between SMRI and fMRI data that could enhance prediction accuracy. To address this, He et al. [27] proposed a global-local transformer model that combines global and local features through an attention mechanism, significantly improving performance. Similarly, Armanious et al. [19] incorporated both chronological and biological age information into CNN-based models, enhancing brain age prediction. Liu et al. [22] further highlighted the importance of demographic factors, such as gender, in brain age prediction, using a support vector regression (SVR) approach. Dular et al. [33] extended brain age estimation to multisite data, achieving an MAE of 3.25 \u00b1 2.70 years, demonstrating the value of large-scale, heterogeneous datasets in improving prediction accuracy. Additionally, the concept of disentangled representation learning has gained attention, with Cai et al. [21] employing a two-stream convolutional autoencoder to disentangle modality-specific features, improving upon traditional autoencoder designs. A concise summary of the existing literature is provided in Table I, highlighting key methodologies such as modality integration, sex-aware modeling, adversarial learning, and disentangled autoencoders. Studies utilizing attention-based and transformer networks for improved segmentation, particularly in challenging modalities, have shown promise in advancing model-based approaches [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61] [62]. Autoencoders (AE) have been widely explored for multimodal fusion, with early and late fusion [63], [64]. However, traditional AEs often struggle to differentiate between shared and complementary information, and noisy modalities can negatively impact latent representation learning across modalities.\nDespite recent advancements, current approaches to brain age estimation still face significant challenges, particularly in fully exploiting the interactions between multimodal data and accounting for sex-specific aging patterns [65]. To address these issues, the proposed framework in this paper integrates SMRI and fMRI data while incorporating sex-specific information into the latent space, capturing both modality-specific and shared features through disentangled representations. Our framework, the Sex-Aware Adversarial Variational Autoencoder (SA-AVAE), not only improves predictive accuracy but also enhances model interpretability by explicitly considering demographic factors such as sex. The key contributions of our work include: 1) the introduction of a novel framework that integrates SMRI and fMRI data with sex information to improve accuracy and robustness; 2) the use of disentangled latent representations, achieved by applying adversarial and variational autoencoder losses to ensure effective separation of modality-specific and shared features; 3) a new loss weighting strategy for fine-tuning model parameters, providing insights into optimizing architectures for broader applications; and 4) extensive evaluation showing that our framework outperforms state-of-the-art methods, establishing a new benchmark for brain age estimation. This work paves the way for more effective, interpretable models in neuroimaging and brain health assessment, emphasizing the importance of integrating multimodal data, disentangled representations, and demographic factors to improve both prediction accuracy and clinical applicability."}, {"title": "III. PROPOSED METHODOLOGY", "content": "We propose a multimodal framework, the Sex-Aware Adversarial Variational Autoencoder (SA-AVAE), which integrates adversarial and variational learning to disentangle features derived from sMRI and fMRI scans. These features are further combined with sex information to estimate biological brain age. While sMRI is more readily available, the framework is designed to function seamlessly even without the incorporation of fMRI, relying solely on sMRI when necessary. The components of the proposed framework are described in detail in the following subsections."}, {"title": "A. Overall Framework", "content": "The architecture of the SA-AVAE framework is depicted in Figure 2. It consists of two autoencoder networks: the primary-modality path and the additional-modality path. Both networks share an identical architecture and are designed to process and integrate features extracted from sMRI (the primary modality) and fMRI (the optional modality).\nFor each modality, a multi-layer perceptron (MLP) neural network serves as the encoder $E_i$, where $i=1$ corresponds to sMRI and $i=2$ corresponds to fMRI. The encoder generates a latent representation $z_i$ from the input feature vector $x_i$:\n$z_i = E_i(x_i), i \\in \\{1, 2\\}.$ (1)\nThe latent representation $z_i$ is disentangled into two components:\n$z_i = [Shared(z_i), Dist(z_i)],$ (2)\nwhere $Shared(z_i)$ captures the common, modality-invariant information, and $Dist(z_i)$ encodes modality-specific features. The disentanglement is guided by the following principles:\n\u2022 The concatenation of $Shared(z_i)$ and $Dist(z_i)$ reconstructs the original latent vector $z_i$:\n$z_i = Concat (Shared(z_i), Dist(z_i)).$ (3)\n\u2022 Shared representations $Shared(z_1)$ and $Shared(z_2)$ should be as similar as possible to maximize commonality:\n$||Shared(z_1) - Shared(z_2)||_2 \\rightarrow 0.$ (4)\n\u2022 Distinct representations $Dist(z_1)$ and $Dist(z_2)$ should be as dissimilar as possible to emphasize modality-specific information:\n$||Dist(z_1) - Dist(z_2)||_2 \\rightarrow max.$ (5)\nThe disentangled latent features are fed into modality-specific decoders $Dec_i$ to reconstruct the input features. Finally, the concatenated latent spaces from both autoencoders, along with sex information, are passed into a regressor network $P$ to estimate biological brain age:\n$M = [Shared(z_1), Shared(z_2), Dist(z_1), Dist(z_2), Sex].$ (6)"}, {"title": "B. Feature Disentanglement Strategy", "content": "The proposed SA-AVAE framework introduces a robust feature disentanglement strategy, leveraging adversarial learning, variational constraints, shared-distinct distance ratio loss, and cross-modality reconstruction to effectively separate shared and modality-specific latent representations. This comprehensive strategy ensures that the model captures complementary and modality-invariant features while maintaining the unique characteristics of each modality, thus optimizing its performance for multimodal brain age estimation.\n1) Adversarial Learning for Shared Representations: Adversarial learning plays a pivotal role in aligning the shared latent representations $Shared(z_1)$ and $Shared(z_2)$ with a predefined prior distribution $p(z)$. This alignment ensures that the shared features are modality-invariant and contain common information across sMRI and fMRI data. A discriminator network $D_z$ is trained to distinguish between samples from the aggregated posterior distribution $q(z)$ and the prior $p(z)$. The adversarial loss for each modality is defined as:\n$L_{adv} = E_{x_i \\sim p_d(x_i)} log (1 - D_z (Shared(E_i(x_i)))) + E_{z_i \\sim p(z)} log D_z(z),$ (7)\nwhere $i \\in \\{1, 2\\}$ represents the modality index. The total adversarial loss is the sum of the losses across both modalities:\n$L_{adv} = L_{adv}^1 + L_{adv}^2.$ (8)\nThis mechanism ensures that the shared representations are well-regularized and contribute to the effective disentanglement of modality-specific and shared features.\n2) Variational Learning for Distinct Representations: To regularize the modality-specific latent spaces, variational learning enforces the distinct representations $Dist(z_1)$ and $Dist(z_2)$ to conform to predefined modality-specific prior distributions $p_1(z)$ and $p_2(z)$. This constraint ensures that the distinct latent spaces capture unique, modality-specific information. The variational loss for each modality is expressed as the Kullback-Leibler (KL) divergence:\n$L_{var} = D_{KL} (q (Dist(z_i)|x_i) || p_i (Dist(z_i))),$ (9)\nwhere $p_i (Dist(z_i))$ is typically modeled as a Gaussian distribution $\\mathcal{N}(0, I)$. The total variational loss is defined as:\n$L_{var} = L_{var}^1 + L_{var}^2.$ (10)\nThis variational constraint encourages the distinct representations to remain disentangled from the shared latent space, thereby improving the clarity of modality-specific features.\n3) Shared-Distinct Distance Ratio Loss: To reinforce the separation between shared and distinct representations, the model employs a shared-distinct distance ratio loss, $L_D$. This loss emphasizes the disentanglement by balancing the similarity of shared representations and the dissimilarity of distinct representations. The loss is defined as:\n$L_D = \\frac{L_{Shared}}{C_{Dist}},$ (11)\nwhere,\n$L_{Shared} = E_{x_1, x_2} || Shared (Enc_1 (x_1)) - Shared (Enc_2 (x_2))||_2,$ (12)\nand\n$C_{Dist} = E_{x_1, x_2} || Dist (Enc_1 (x_1)) - Dist (Enc_2 (x_2))||_2.$ (13)\nThis ratio loss ensures a clear separation between shared and modality-specific features, enabling the model to disentangle complex multimodal information effectively.\n4) Cross-Modality Reconstruction: To further encourage disentanglement, the framework incorporates a cross-modality reconstruction mechanism. This criterion leverages the shared representation from one modality to reconstruct the input of the other modality, ensuring that the shared features truly represent common information. Specifically:\n$x_i \\approx Dec_i (Shared(z_j), Dist(z_i)), i \\neq j.$ (14)\nThe reconstruction loss for modality i is defined as:\n$L_{rec} = E_{x_i \\sim p_d(x_i)} ||x_i - Dec_i (Shared(E_i(x)), Dist(E(x_i))||_2.$ (15)\nThe total reconstruction loss is the sum of the reconstruction losses across both modalities:\n$L_{rec} = L_{rec}^1 + L_{rec}^2.$ (16)\n5) Comprehensive Objective Function: The complete objective function integrates all the discussed losses, balancing them with empirically determined trade-off parameters:\n$L_{total} = \\mu_1 L_{adv} + \\mu_2 L_{var} + \\mu_3 L_{rec} + \\mu_4 L_{reg} + \\mu_5 L_D,$ (17)\nwhere $\\mu_1, \\mu_2, \\mu_3, \\mu_4$, and $\\mu_5$ are weighting coefficients. The regression loss $L_{reg}$ measures the discrepancy between the predicted and actual brain age using the L2 norm:\n$L_{reg} = E_{x_1, x_2} ||Y - P (M (X_1, X_2))||_2.$ (18)\nThis comprehensive approach ensures robust disentanglement, accurate reconstruction, and precise brain age estimation, making the SA-AVAE framework highly effective for multimodal neuroimaging analysis."}, {"title": "C. Single Modality Algorithm", "content": "In practical scenarios, the availability of datasets containing both sMRI and fMRI modalities is often limited, with most samples including only sMRI data. To address this imbalance, the SA-AVAE framework is adapted to function effectively in a single-modality setting, particularly for sMRI. This adaptation involves deactivating the fMRI encoder-decoder path and modifying the overall objective function to optimize performance with only sMRI input.\n1) Adaptation of the Objective Function: To accommodate the absence of the fMRI modality, the objective function is redefined. Adversarial and variational losses are applied exclusively to the sMRI path, while the regression and reconstruction losses are adjusted to rely solely on sMRI-derived representations. The revised losses are as follows:\n\u2022 Adversarial Loss: Ensures alignment of shared latent representations with a prior distribution:\n$L'_{adv} = L_{adv}.$ (18)\n\u2022 Variational Loss: Regularizes the distinct latent space to conform to a predefined prior:\n$L'_{var} = L_{var}.$ (19)\n\u2022 Regression Loss: Predicts biological brain age using sMRI-derived latent representations:\n$L'_{Reg} = E_{x_1} ||y - P (M (x_1))||_2,$\nwhere $M(x_1)$ includes the shared and distinct representations along with sex information.\n\u2022 Reconstruction Loss: Ensures accurate reconstruction of the sMRI input:\n$L_{Rec} = E_{x_1} ||x_1 - Dec_1 (Shared (Enc_1 (x_1)), Dist (Enc_1 (x_1)))||_2.$\nThe modified objective function is:\n$L'_{Enc1,Dec1, P} = \\eta_1L_{Reg} + \\eta_2L_{Rec} + \\eta_3 L_{adv} + \\eta_4 L_{var}$ (19)\nwhere $\\eta_1, \\eta_2, \\eta_3$, and $\\eta_4$ are trade-off parameters controlling the contribution of each loss term.\n2) Training Strategy for Uni-modal Setting: The single-modality training procedure for the SA-AVAE framework has been adapted to ensure optimal performance using only SMRI data, addressing the challenges posed by limited multimodal datasets. During training, the encoder Enc1 processes the sMRI input x1 to generate latent representations that are disentangled into shared and distinct components. These representations are defined as $Shared(z_1)$ and $Dist(z_1)$, respectively, and together they form the latent space $z_1 = [Shared(z_1), Dist(z_1)]$. The decoder Dec1 then reconstructs the original input x1 by leveraging these latent features, ensuring that the reconstructed data closely matches the input.\nTo enhance the learned representations, adversarial regularization is applied to the shared latent space $Shared(z_1)$, aligning it with a predefined prior distribution via a discriminator network. Simultaneously, variational regularization is employed on the distinct latent space $Dist(z_1)$ to encourage alignment with a modality-specific prior distribution using the KL divergence. The regressor network P predicts the biological brain age y by combining the shared and distinct features with additional sex information encoded in M(x1). The parameters of the encoder, decoder, regressor, and discriminator are updated jointly using the modified objective function, which balances regression, reconstruction, adversarial, and variational losses.\nThis approach not only ensures robust feature learning and disentanglement but also provides several benefits. First, the framework is highly adaptable, allowing it to operate seamlessly with datasets containing only sMRI data, which is crucial for real-world applications where multimodal data is often unavailable. Second, the absence of the fMRI path reduces computational overhead, resulting in faster training and inference, which enhances efficiency. Third, adversarial and variational regularizations ensure that the learned representations remain robust and meaningful, even in the single-modality scenario. Finally, the modular design of the framework ensures scalability, allowing future integration with additional modalities or extension to other single-modality tasks. These attributes make the single-modality adaptation of the SA-AVAE framework a powerful and flexible solution for brain age estimation in diverse settings."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "B. Model Architecture and Training Strategy\nThe architecture of the proposed Sex-Aware Adversarial Variational Autoencoder (SA-AVAE), employed in our experiments, is depicted in Figure 4. This architecture integrates encoders and decoders for both sMRI and fMRI modalities, alongside a shared regressor and a discriminator to facilitate adversarial learning. To ensure reproducibility and optimize the model's performance, several key hyperparameters were carefully selected. First, the batch size was set to 20 during training to balance memory usage and model convergence. The latent space of the model was designed with a total dimensionality of 120, divided into a shared space of 50 dimensions and modality-specific spaces of 70 dimensions each. These dimensions were determined empirically, striking a balance between the representational capacity of the model and the computational efficiency required for large-scale experiments.\nFor training, the Adam optimizer was used with an initial learning rate of 0.001, which was dynamically adjusted; if no improvement in validation performance was observed after nine epochs, the learning rate was reduced to a quarter of its current value to aid in convergence. Additionally, an early stopping mechanism was implemented to mitigate overfitting, halting the training process when no improvement was seen in the validation set after a predefined number of epochs.\nThe training and evaluation of the SA-AVAE framework were conducted on an NVIDIA RTX 4090 GPU using the Keras library with a TensorFlow backend. The model had a total of 2.7 million trainable parameters. Training took approximately 12 hours to complete, while testing was conducted in about 1.5 hours, demonstrating the model's computational efficiency, which makes it suitable for handling large-scale neuroimaging datasets."}, {"title": "V. RESULTS AND DISCUSSION", "content": "To evaluate the robustness of the SA-AVAE model, we performed 10-fold cross-validation to assess the consistency of its performance across the dataset. Figure 5 shows scatter plots comparing the predicted and chronological ages of the SA-AVAE and other models, under identical experimental conditions, highlighting the relative performance and robustness of each. The results demonstrate that combining adversarial and variational learning enhances both performance and robustness. Incorporating sex information further improves the model's robustness, as evidenced by lower standard deviation."}, {"title": "F. Limitations and Future Work", "content": "The proposed SA-AVAE framework leverages two neuroimaging modalities, structural MRI (sMRI) and functional MRI (fMRI), for the accurate estimation of biological brain age. While the integration of both modalities enhances the model's performance and improves the accuracy of brain age estimation, it also introduces certain limitations. One of the primary challenges is the framework's sensitivity to missing modalities. As discussed in Section V-D, the performance of the SA-AVAE model significantly deteriorates when either modality is unavailable. This makes the model highly dependent on the availability of both sMRI and fMRI data, which can be a significant limitation in practical clinical scenarios where one of the modalities may be missing or of poor quality. Another major limitation of this study is the exclusive use of data from healthy control (HC) subjects. While the results obtained on this cohort are promising, the real-world applicability of the framework for early detection of neurodegenerative diseases, such as Alzheimer's Disease (AD), remains uncertain. To more effectively evaluate the performance and generalizability of the proposed framework, it is crucial to test it on patients with various neurological conditions, including AD and Parkinson's Disease (PD). This will allow for a more thorough assessment of how the model performs in distinguishing biological brain age across different patient populations, which is essential for its potential clinical use.\nFuture work includes several key improvements that are necessary for the framework to achieve its full potential. One such area is enhancing the model's robustness so that it can perform comparably well with either sMRI or fMRI data alone, thereby reducing the performance gap between multimodal and unimodal approaches. This would increase the model's flexibility, enabling its deployment in situations where only a single modality is available or when multimodal data collection is not feasible. Additionally, we plan to conduct a comprehensive evaluation of the SA-AVAE framework using clinical data from patients with various neurological conditions. This will involve real-time testing in clinical settings to assess the feasibility of using the framework for the early diagnosis and monitoring of brain aging and neurodegenerative diseases. These efforts will be critical in demonstrating the clinical utility and robustness of the proposed framework in real-world applications."}, {"title": "VI. CONCLUSION", "content": "In this study, we introduced a novel framework for biological brain age estimation, leveraging the complementary information from structural magnetic resonance imaging (sMRI) and functional magnetic resonance imaging (fMRI) data. Our proposed Sex-Aware Adversarial Variational Autoencoder (SA-AVAE) combines adversarial and variational learning techniques to effectively disentangle latent features from both modalities. By decomposing the latent space into modality-specific and shared codes, our model captures both the unique and common aspects of brain aging, while also accounting for sex-specific aging patterns, further enhancing its performance. The results of our experiments, evaluated on the OpenBHB dataset, demonstrate that SA-AVAE outperforms existing state-of-the-art methods, showing significant robustness across various age groups. This highlights the potential of the framework for real-time clinical applications, particularly in the early detection and monitoring of neurodegenerative diseases. Future work will focus on enhancing the robustness of the framework to ensure effective performance with either modality independently. Additionally, we aim to conduct real-time evaluations using clinical data to assess the practical applicability of the SA-AVAE model in clinical settings."}]}