{"title": "Optimizing KV Cache Eviction in LLMs: Adaptive Allocation for Enhanced Budget Utilization", "authors": ["Yuan Feng", "Junlin Lv", "Yukun Cao", "Xike Xie", "S. Kevin Zhou"], "abstract": "Large Language Models have excelled in various fields but encounter efficiency limitations due to the extensive KV cache required for long sequences inference. Many efforts try to evict non-critical cache elements during runtime, thereby reducing cache size within a given memory budget while preserving generation quality. Our reexamination of their underlying principles discerns that prevailing strategies essentially aim to minimize an upper bound of eviction loss within a specific budget allocation. However, we observe that the current practice of uniformly allocating budgets across different attention heads during the eviction procedure tends to degrade the quality of generation posten-eviction. In light of these findings, we propose a simple yet effective adaptive allocation algorithm that not only theoretically ensures its loss upper bound does not exceed that of previous uniform allocation methods, but also effectively aligns with the characteristics of the self-attention mechanism, thus practically reducing the upper bound. Further, integrating this algorithm with two of the most advanced methods yields Ada-SnapKV and Ada-Pyramid. Extensive experimental validation across 16 datasets and the Needle-in-a-Haystack test confirm that Ada-SnapKV and Ada-Pyramid achieve further enhancements, establishing new benchmarks in state-of-the-art performance.", "sections": [{"title": "1 Introduction", "content": "Autoregressive Large language models (LLMs) have achieved significant success and are widely utilized across diverse natural language processing applications, including dialogue systems (Yi et al. 2024), document summarization (Laban et al. 2023), and code generation (Gu 2023). Their deployment in real-world contexts has propelled the development of their capacity to process extended sequences. For instance, GPT-4 supports sequences up to 129K (Achiam et al. 2023), Claude3 up to 200K (Anthropic 2024), and Gemini-Pro-1.5 (Reid et al. 2024) up to 1M tokens. However, the computational cost and memory requirements for Transformer architecture increase quadratically with sequence length, posing significant challenges.\nTo mitigate these issues, a traditional optimization technique called caching has been applied to LLMs, referred to as Key-Value(KV) cache. The KV cache pairs store intermediate computation results of previous tokens, effectively"}, {"title": "2 Related Works", "content": "In the context of long sequences, the vast scale of the KV caches leads to a memory-bound situation, causing significant memory and I/O latency costs (Wang and Chen 2023). Some studies, which are orthogonal to our work, employ efficient memory management strategies that reduce I/O time without altering the size of the KV Cache, such as Page Attention (Kwon et al. 2023) and Flash Attention (Dao et al. 2022). And in our experiments, we have incorporated the Flash Attention technique to achieve efficient computation. Other approaches (Tang et al. 2024) attempt to reduce I/O overhead by only recalling KV cache entries relevant to the current query for computation, while others remain stored for subsequent queries. However, these methods are constrained by substantial memory overheads, making it difficult to deploy on GPUs with lower storage capacities.\nRecently, KV cache eviction methods have gained attention and rapid development due to their flexible compression ratios given any storage budgets and the advantage of being plug-and-play without the need for fine-tuning. Earlier StreamingLLM (Xiao et al. 2023) simply maintains the cache of 4 initial and the recent tokes, discarding all others to adapt to long sequence inference. FastGen (Ge et al. 2023) searches and combines multiple strategies, like maintaining the cache of special tokens, punctuation tokens, and recent tokens, based on the characteristics of attention heads. H2O (2024b) has developed an eviction algorithm that utilizes query states of all tokens to identify important KV pairs based on the Heavy Hitter method. The most recent SnapKV (Li et al. 2024) identifies important KV cache pairs using query states of several tokens within a recent window, evicting the less important ones. This method effectively mitigates the quality degradation in cache eviction. The Pyramid (Yang et al. 2024; Zhang et al. 2024a) further adjusts the budget allocation across different layers in SnapKV, improving the generation quality in small-budget scenarios. However, to our best knowledge, current eviction methods have never tried to adaptively distribute the total budget across different heads. Building on theoretical analysis and observations of inherent attention patterns in LLM heads, we have identify and demonstrate the necessity for adaptive budget allocation in cache eviction. Based on these insights, we propose a simple yet effective budget allocation mechanism and integrate it into the two leading strategies, SnapKV and Pyramid, further reducing the accuracy drop associated with cache eviction."}, {"title": "3 Framework", "content": "In this section, we reexamine how the existing cache eviction strategies to retain essential information in the past KV cache from a theoretical perspective. Inspired by theoretical findings, we propose a simple yet effective algorithm for adaptive budget allocation, which is proved to better than the previous uniform allocation in cache eviction procedure both in theory and practice. Further integrating it into two current leading methods, we develop two adaptive cache"}, {"title": "3.1 Overview", "content": "In this section, we reexamine how the existing cache eviction strategies to retain essential information in the past KV cache from a theoretical perspective. Inspired by theoretical findings, we propose a simple yet effective algorithm for adaptive budget allocation, which is proved to better than the previous uniform allocation in cache eviction procedure both in theory and practice. Further integrating it into two current leading methods, we develop two adaptive cache"}, {"title": "3.2 Preliminary", "content": "We begin by providing a formal description of the computational processes involving KV cache and Multi-head Attention in a single-layer of LLMs to alleviate the burden of notation. LLMs are characterized by their autoregressive generation mode, where each step involves using the current final token to predict the next token. Define $X \\in \\mathbb{R}^{n\\times d}$ as the embedding matrix of all tokens in sequence, and $x \\in \\mathbb{R}^{1\\times d}$ as the last token used as input at the current timestep. To clarify the subsequent theoretical exposition, we adopt the notation system from (Liu et al. 2023) under the assumption of h attention heads in one layer. The transformation matrices for each head $i \\in [h]$ map token embeddings to their respective Query, Key, and Value are denoted as $W_i^Q, W_i^K, W_i^V \\in \\mathbb{R}^{d \\times d_h}$, and the final output matrix $W_O \\in \\mathbb{R}^{d_h \\times d}$ transform the intermediate result to the output hidden states. At each timestep, the states of the stored KV cache for head i has been initialized as:\n$$K_i = XW_i^K, V_i = XW_i^V$$\nThen, the input token x is mapped to its corresponding query, key, value for each head, and the previous KV cache is updated accordingly:\n$$q_i = xW_i^Q, k_i = xW_i^K, v_i = xW_i^V$$\n$$K_i = [K_i : k_i], V_i = [V_i : v_i]$$\nFinally the final output $o \\in \\mathbb{R}^{1\\times d}$ is computed as follow:\n$$o = \\sum_{i \\in [h]} A_i V_i W_i^V$$\nwhere $A_i \\in \\mathbb{R}^{1\\times n}$ is the attention weight calculated by:\n$$A_i = softmax(q_iK_i^T)$$"}, {"title": "3.3 Reexamining the Principle of Cache Eviction", "content": "Cache eviction is dedicated to reducing the size of KV cache to fit within a constrained budget by evicting certain cache pairs strategically. Eviction masks $\\{M_i \\in \\mathbb{R}^{1\\times n}\\}$ can be employed to simulate the post-eviction output $o'$ in self-attention mechanism:\n$$o' = \\sum_{i \\in [h]} A'_i V_i W_i^V \\quad \\text{where} \\quad A'_i = softmax(q_iK_i^T + M_i)$$\nwhere each element $M^j_i$ in mask $M_i$ indicates that whether evicting the $jth \\in [n]$ KV cache pair in $K_i, V_i \\in \\mathbb{R}^{n\\times d_h}$ on each head i:\n$$M^j_i = \\begin{cases} 0 & \\text{if the jth cache pair on head i is retained} \\\\ -\\infty & \\text{otherwise the jth cache pair on head i is evicted} \\end{cases}$$\ngiven budget allocation $\\{B_i\\}$ s.t. $\\sum_{i \\in [h]} B_i = B$\nThus, the budget $B_i$ for head i corresponds to the number of zero elements in $M_i$. Theorem 1 further simplify the output $o'$ by eliminating the softmax function. A detailed proof is provided in Appendix A.1.\nTheorem 1. The post-eviction output $o'$ can rewrite as:\n$$o' = \\sum_{i \\in [h]} \\frac{A_i N_i}{\\sum_{j=1}^{n} |A_i N_i||_1} V_i W_i^V$$\nwhere\n$$N^j_i = \\begin{cases} 1 & \\text{if } K^j_i \\text{ and } V^j_i \\text{ are retained} \\\\ 0 & \\text{otherwise, evict } K^j_i \\text{ and } V^j_i \\end{cases}$$\ngiven budget allocation $\\{B_i\\}$ s.t. $\\sum_{i \\in [h]} B_i = B$\nThe reduction in generation quality due to cache eviction stems from alterations of the attention output. Thus, we model the eviction loss using the L1-distance between the post-eviction and original outputs of self-attention mechanism:\n$$Eviction\\ Loss = ||o' - o||_1$$\nUtilizing the L1-norm of matrix, which can understand how the matrix transforms vectors, we derive an upper bound D for the Eviction Loss in Theorem 2. For a detailed proof, refer to Appendix A.2.\nTheorem 2. The eviction loss caused by cache eviction can be bounded by D as follows:\n$$Eviction\\ Loss \\le D = 2hC - 2C \\sum_{i \\in [h]} \\sum_{j \\in [n] retained} A_i^j$$\ngiven budget allocation $\\{B_i\\}$ s.t. $\\sum_{i \\in [h]} B_i = B$\nwhere $C = Max\\{||V_i W_i^V||_\\infty\\}$ is the max value in the row norms of Matrices $\\{V_i W_i^V\\}$ among all heads."}, {"title": "3.4 Adaptive vs. Uniform: Theoretical Insights.", "content": "In existing studies on cache eviction, the budget B is uniformly distributed across each head within a layer: specifically, $B_i = B/h$. Consequently, the upper bound of eviction loss D under any given allocation, is modified to D' under uniform allocation:\n$$D' = 2hC - 2C \\sum_{i \\in [h]} \\sum_{j \\in [n]} A_i^j$$\n$$A_i^j \\in Top-K(A_i, B_i)$$\ngiven uniform allocation $\\{B_i = B/h\\}$\nIn contrast, we suggest adaptive distribution of the total budget among different heads and introduce a simple yet"}, {"title": "Algorithm 2: Adaptive Budget Allocation", "content": "Input: Total Budget B, Attention Weights in h heads $\\{A_i \\in \\mathbb{R}^{1\\times n}\\}$;\nOutput: Allocated Budgets of h heads $\\{B_i\\}$;\nConcatenate across heads $A = Cat(\\{A_i\\},dim=1)$\nCreate head indicator $I = [1...1 : ... : h...h]$ with each index $\\{i\\}$ repeat n times\nIdentify top indices $T = Top-K(A, B).indices$\nSelect the corresponding head indicator $I^* = I[T]$\nCount frequencies of each i in $I^*$ to determine $\\{B_i\\}$\nReturn Allocated Results of h heads $\\{B_i\\}$\neffective budget allocation algorithm that dynamically distributes the total budget B based on the attention weights $A_i$ across h heads, with the allocated results defined as $\\{B_1^\\prime, B_2^\\prime, ..., B_h^\\prime\\}$ subject to $\\sum_{i \\in [h]} B_i^\\prime = B$. As shown in Algorithm 2, it firstly selects the B largest attention weights from all heads within one layer. Based on the times of each head is selected in the above procedure, different budgets $B_i^\\prime$ are allocated to each head. Under this adaptive allocation, the upper bound of eviction loss is denoted as $D''$:\n$$D'' = 2hC - 2C \\sum_{i \\in [h]} \\sum_{j \\in [n]} A_i^j$$\n$$A_i^j \\in Top-K(A_i, B_i^\\prime)$$\ngiven adaptive allocation $\\{B_i^\\prime\\}$\nTheorem 4. The upper bound $D''$ of eviction loss with adaptive budget allocation consistently remains at or below the upper bound $D'$ associated with uniform allocation.\n$$D'' \\le D'$$\nAccording to Theorem 4, our adaptive allocation algorithm achieves equal or smaller eviction loss than the previous uniform allocation approach, thereby enhancing the post-eviction generation quality. The detailed proof can be found in Appendix A.3."}, {"title": "3.5 Adaptive vs. Uniform: Empirical Insights", "content": "According to Theorem 4, the upper bound of eviction loss of adaptive allocation is equal or less than to that of the current uniform allocation. We further demonstrate different attention heads within each layers of LLMs exhibit significant disparities in attention concentration, resulting in the necessity of adaptive budget allocation in practice. For visualization in Figure 2 (a), most concentrated heads in Layer 8 like head 1 require only 1% of the original cache budget to effectively retain the aggregated weights retained $A_i^j$ of 0.95. Conversely, other heads like heads 18 requires nearly 50% proportion to near 0.95. This characteristic is closely related to the upper bound D of eviction loss, as detailed in Theorem 3. Under such circumstances, the previous uniform budget allocation faces a dilemma, as an illustrative example in Figure 1: either neglect the loss in dispersed heads, or allocate excessive and unnecessary budgets to heads with concentrated attention. This significantly undermines the trade-off performance between the total budget and generation"}, {"title": "3.6 Implementation", "content": "The current two leading eviction strategies, SnapKV and Pyramid, both utilize the several tokens $X_{rec} \\in \\mathbb{R}^{win*d}$ from a recent window (typically window size is 32) to identify and evict the less important cache pairs. SnapKV excels at managing evictions under scenarios with large budgets, while Pyramid is more effective in environments with"}, {"title": "Algorithm 3: Ada-SnapKV in One Layer", "content": "Input: Total budget B, Past cache $\\{K_i, V_i\\}$, Tokens in the recent window $X_{rec} \\in \\mathbb{R}^{win*d}$\nOutput: Compressed cache $\\{K_i, V_i\\}$\nfor i = 1 to h do\n$Q_{rec} = X_{rec}w_i^Q$\n$A_i = softmax(Q_{rec}K_i^T)$\n$\\tilde{A_i} = A_i.maxpooling(dim = 1).mean(dim = 0)$\nend for\nget $\\{B_i^\\prime\\}$ by invoking Algorithm 2(B, $\\{\\tilde{A_i}\\})$\n$\\{B_i\\} = \\alpha \\times \\{B_i^\\prime\\} + (1 - \\alpha) \\times \\frac{B}{h}$\n$\\{K_i, V_i\\} = Algorithm\\ 1(\\{B_i\\}, \\{K_i, V_i\\}, \\{\\tilde{A_i}\\})$\nreturn compressed cache $\\{K_i, V_i\\}$\nsmaller budgets. The key distinction lies in how Pyramid and SnapKV allocate budgets within different layers in LLMs. Pyramid suggests that information aggregation among layers takes a pyramidal form, thereby allocating a larger budget to shallower layers and progressively reducing it in deeper layers through pre-set hyper-parameters. In contrast, SnapKV distributes the budget uniformly across all layers. Nonetheless, same as other eviction methods, they both allocate the budget uniformly among all heads within a single layer. We incorporate the adaptive allocation algorithm into both of them, resulting in the creation of two novel adaptive strategies Ada-SnapKV and Ada-Pyramid, respectively.\nTake the Ada-SnapKV in algorithm 3 as an example, the adaptive budget allocation can be seamlessly integrated into any eviction strategy by invoking Algorithm 2 before eviction process in each layer to distribute adaptively the total budget among all heads. In line 6, a hyper-parameter $\\alpha$, set by default to 0.5, prevent from assigning tiny budgets to highly concentrated heads, enhances fault tolerance in the post-eviction generation."}, {"title": "4 Experiments", "content": "Firstly, we carry out an comprehensive evaluation using 16 datasets, covering domains of single-document QA (Ko\u010disk\u1ef3 et al. 2018; Dasigi et al. 2021), multi-document QA (Yang et al. 2018; Ho et al. 2020; Trivedi et al. 2022), summarization (Huang et al. 2021; Zhong et al. 2021; Fabbri et al. 2019), few-shot learning (Joshi et al. 2017; Gliwa et al. 2019; Li and Roth 2002), Synthetic (Bai et al. 2023), and code generation (Guo et al. 2023; Liu, Xu, and McAuley 2023), within LongBench (Bai et al. 2023), a benchmark for evaluating multi-task performance with long-sequence inputs. These datasets feature varying average input lengths from 1,235 to 18,409 tokens, necessitating substantial KV cache size during generation, thereby rendering them suitable for evaluating KV cache eviction strategies."}, {"title": "4.1 Settings", "content": "Datasets Firstly, we carry out an comprehensive evaluation using 16 datasets, covering domains of single-document QA (Ko\u010disk\u1ef3 et al. 2018; Dasigi et al. 2021), multi-document QA (Yang et al. 2018; Ho et al. 2020; Trivedi et al. 2022), summarization (Huang et al. 2021; Zhong et al. 2021; Fabbri et al. 2019), few-shot learning (Joshi et al. 2017; Gliwa et al. 2019; Li and Roth 2002), Synthetic (Bai et al. 2023), and code generation (Guo et al. 2023; Liu, Xu, and McAuley 2023), within LongBench (Bai et al. 2023), a benchmark for evaluating multi-task performance with long-sequence inputs. These datasets feature varying average input lengths from 1,235 to 18,409 tokens, necessitating substantial KV cache size during generation, thereby rendering them suitable for evaluating KV cache eviction strategies."}, {"title": "4.2 Evaluations Among 16 Datasets", "content": "We assess all eviction strategies using cache budget $B \\in \\{128 \\times h, 256 \\times h, 512 \\times h, 1024 \\times h\\}$ for each layer. Detailed results for each dataset on the Mistral model are provided in Table 1, while other results for the LWM model are placed in Appendix A.4 due to space constraints. To demonstrate the efficacy of adaptive allocation, we take a budget $B = 128h$ as an example presented in Table 1. After integrating the adaptive allocation algorithm, Ada-SnapKV enhances the quality scores in 15 out of 16 datasets compared to the original SnapKV, increasing the average score from"}, {"title": "4.3 Evaluations on Needle-in-a-Haystack Test", "content": "As shown in Figure 4, we employ a Needle-in-a-Haystack test to demonstrate how adaptive budget allocation can enhance long-context retrieval capabilities. All configurations maintains a recent window size of 32 and a pooling kernel size of 7 which consistent with former experiments, where the maximum inference length is limited to 33K in the full cache case on A100-80G. With a cache budget of B = 128h, all four strategies\u2014Ada-SnapKV, Ada-Pyramid, SnapKV, and Pyramid-successfully extend inference length up to 217K. Notably, Ada-SnapKV and Ada-Pyramid both effectively improve long-text retrieval capabilities. In particular, Ada-SnapKV and Ada-Pyramid achieve near-lossless retrieval within the original 33K length, a feat not replicated by the standard SnapKV and Pyramid. In terms of average score, Ada-SnapKV improves from 95.40 to 96.56, while Ada-Pyramid increases from 96.62 to 97.02."}, {"title": "5 Conclusion", "content": "In this study, we reexamine prevailing cache eviction strategies employed in LLMs, discerning their goal to minimize an upper bound of eviction loss. This loss is quantified as the L1 distance between outputs before and after eviction. By introducing an adaptive budget allocation among various attention heads, we theoretically reduce the upper bound compared to previous practices. Our empirical findings suggest that this adaptive approach significantly benefits from the varied concentration levels inherent among multiple heads within the self-attention mechanism. We develop two novel adaptive eviction methods, Ada-SnapKV and Ada-Pyramid, which incorporate this adaptive allocation into two advanced"}]}