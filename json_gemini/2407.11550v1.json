{"title": "Optimizing KV Cache Eviction in LLMs: Adaptive Allocation for Enhanced Budget Utilization", "authors": ["Yuan Feng", "Junlin Lv", "Yukun Cao", "Xike Xie", "S. Kevin Zhou"], "abstract": "Large Language Models have excelled in various fields but encounter efficiency limitations due to the extensive KV cache required for long sequences inference. Many efforts try to evict non-critical cache elements during runtime, thereby reducing cache size within a given memory budget while preserving generation quality. Our reexamination of their underlying principles discerns that prevailing strategies essentially aim to minimize an upper bound of eviction loss within a specific budget allocation. However, we observe that the current practice of uniformly allocating budgets across different attention heads during the eviction procedure tends to degrade the quality of generation posten-eviction. In light of these findings, we propose a simple yet effective adaptive allocation algorithm that not only theoretically ensures its loss upper bound does not exceed that of previous uniform allocation methods, but also effectively aligns with the characteristics of the self-attention mechanism, thus practically reducing the upper bound. Further, integrating this algorithm with two of the most advanced methods yields Ada-SnapKV and Ada-Pyramid. Extensive experimental validation across 16 datasets and the Needle-in-a-Haystack test confirm that Ada-SnapKV and Ada-Pyramid achieve further enhancements, establishing new benchmarks in state-of-the-art performance.", "sections": [{"title": "1 Introduction", "content": "Autoregressive Large language models (LLMs) have achieved significant success and are widely utilized across diverse natural language processing applications, including dialogue systems (Yi et al. 2024), document summarization (Laban et al. 2023), and code generation (Gu 2023). Their deployment in real-world contexts has propelled the development of their capacity to process extended sequences. For instance, GPT-4 supports sequences up to 129K (Achiam et al. 2023), Claude3 up to 200K (Anthropic 2024), and Gemini-Pro-1.5 (Reid et al. 2024) up to 1M tokens. However, the computational cost and memory requirements for Transformer architecture increase quadratically with sequence length, posing significant challenges.\nTo mitigate these issues, a traditional optimization technique called caching has been applied to LLMs, referred to as Key-Value(KV) cache. The KV cache pairs store intermediate computation results of previous tokens, effectively reducing the computational burden associated with autoregressive generation. It has become a prevalent approach in enhancing LLM efficiency. However, the expansion of the inference sequence necessitates a corresponding increase in KV cache size, which can easily exceed the total parameter size of the model itself in long-sequence inference (Sun et al. 2024). This not only leads to substantial memory overhead but also results in massive input/output (I/O) time during autoregressive generation (Sun et al. 2024).\nIn response, KV cache eviction methods (Zhang et al. 2024b; Xiao et al. 2023; Ge et al. 2023; Yang et al. 2024; Zhang et al. 2024a; Li et al. 2024) have been proposed to compress the cache size within specified budgets by discarding non-critical cache pairs while striving to minimize the loss of generation quality. These methods are valued for their plug-and-play capabilities, allowing for seamless integration into any LLM without fine-tuning. Typically, these algorithms employ various strategies to select and evict the majority of KV cache pairs, thus reducing memory demands. The latest leading algorithms use the Top-K base selection scheme, which effectively distinguishes between critical and non-critical cache pairs, retaining the former and evicting the latter. The recent leading algorithms have established the Top-K base selection scheme, which can effectively to distinguish the critical and non-critical cache pairs thus perserving and evicting. Despite these advancements, the challenge of minimizing quality loss in existing eviction schemes remains unresolved.\nOur study begins by reexamining the underlying principles of eviction strategies from a theoretical perspective. We establish that Top-K based strategies aim to minimize an upper bound of eviction loss, quantified by the L1-distance between the outputs of the self-attention mechanism pre- and post-eviction under given budget allocation results. We also found the common practice of distributing budget uniformly across different attention heads leads to a misallocation of total budgets, thus diminishing the generation quality, as illustrated in Figure 1a. Base on these insights, we propose a simple yet effective adaptive allocation algorithm to optimize budget distribution to improve the generation quality, which utilizes the pronounced variations in attention concentration among the heads within the self-attention mechanism as illustrated in Figure 1b. In addition, we also theoretically prove that the upper bound of the loss under Top-K"}, {"title": "2 Related Works", "content": "In the context of long sequences, the vast scale of the KV caches leads to a memory-bound situation, causing significant memory and I/O latency costs (Wang and Chen 2023). Some studies, which are orthogonal to our work, employ efficient memory management strategies that reduce I/O time without altering the size of the KV Cache, such as Page Attention (Kwon et al. 2023) and Flash Attention (Dao et al. 2022). And in our experiments, we have incorporated the Flash Attention technique to achieve efficient computation. Other approaches (Tang et al. 2024) attempt to reduce I/O overhead by only recalling KV cache entries relevant to the current query for computation, while others remain stored for subsequent queries. However, these methods are constrained by substantial memory overheads, making it difficult to deploy on GPUs with lower storage capacities.\nRecently, KV cache eviction methods have gained attention and rapid development due to their flexible compression ratios given any storage budgets and the advantage of being plug-and-play without the need for fine-tuning. Earlier StreamingLLM (Xiao et al. 2023) simply maintains the cache of 4 initial and the recent tokes, discarding all others to adapt to long sequence inference. FastGen (Ge et al. 2023) searches and combines multiple strategies, like maintaining the cache of special tokens, punctuation tokens, and recent tokens, based on the characteristics of attention heads. H2O (2024b) has developed an eviction algorithm that utilizes query states of all tokens to identify important KV pairs based on the Heavy Hitter method. The most recent SnapKV (Li et al. 2024) identifies important KV cache pairs using query states of several tokens within a recent window, evicting the less important ones. This method effectively mitigates the quality degradation in cache eviction. The Pyramid (Yang et al. 2024; Zhang et al. 2024a) further adjusts the budget allocation across different layers in SnapKV, improving the generation quality in small-budget scenarios. However, to our best knowledge, current eviction methods have never tried to adaptively distribute the total budget across different heads. Building on theoretical analysis and observations of inherent attention patterns in LLM heads, we have identify and demonstrate the necessity for adaptive budget allocation in cache eviction. Based on these insights, we propose a simple yet effective budget allocation mechanism and integrate it into the two leading strategies, SnapKV and Pyramid, further reducing the accuracy drop associated with cache eviction."}, {"title": "3 Framework", "content": "In this section, we reexamine how the existing cache eviction strategies to retain essential information in the past KV cache from a theoretical perspective. Inspired by theoretical findings, we propose a simple yet effective algorithm for adaptive budget allocation, which is proved to better than the previous uniform allocation in cache eviction procedure both in theory and practice. Further integrating it into two current leading methods, we develop two adaptive cache"}, {"title": "3.1 Overview", "content": "eviction methods: Ada-SnapKV and Ada-Pyramid. The key findings and insights are as below:\n\u2022 Reexamination of Cache Eviction Principle: We formalize the principle of cache eviction as the minimization of the upper bound of eviction loss, quantified as changes in the L1 distance of attention outputs posteviction.\n\u2022 Identifying and Addressing Limitations in Budget Allocations: We pinpoint the inefficiency of existing uniform budget allocation across heads. To address this, we introduce an adaptive allocation algorithm that both theoretically and empirically reduces the upper bound of eviction loss.\n\u2022 Developments of Adaptive Eviction Methods: Base on the proposed adaptive allocation algorithm, we develop two novel approaches, Ada-SnapKV and Ada-Pyramid. These methods demonstrably mitigate quality loss during compression in the extensive evaluations.\n\u2022 Innovative Direction to Enhance Cache Eviction: Our research presents a method that adaptively allocates budgets based on unique characteristics among heads to optimize cache eviction, guided by our theoretical analysis of eviction loss upper bounds. We hope this direction achieves significant further development."}, {"title": "3.2 Preliminary", "content": "We begin by providing a formal description of the computational processes involving KV cache and Multi-head Attention in a single-layer of LLMs to alleviate the burden of notation. LLMs are characterized by their autoregressive generation mode, where each step involves using the current final token to predict the next token. Define \\(X \\in \\mathbb{R}^{n \\times d}\\) as the embedding matrix of all tokens in sequence, and \\(x \\in \\mathbb{R}^{1 \\times d}\\) as the last token used as input at the current timestep. To clarify the subsequent theoretical exposition, we adopt the notation system from (Liu et al. 2023) under the assumption of h attention heads in one layer. The transformation matrices for each head \\(i \\in [h]\\) map token embeddings to their respective Query, Key, and Value are denoted as \\(W_i^Q, W_i^K, W_i^V \\in \\mathbb{R}^{d \\times d_h}\\), and the final output matrix \\(W^O \\in \\mathbb{R}^{d_h \\times d}\\) transform the intermediate result to the output hidden states. At each timestep, the states of the stored KV cache for head i has been initialized as:\n\\[K_i = X W_i^K, V_i = X W_i^V\\]"}, {"title": "3.3 Reexamining the Principle of Cache Eviction", "content": "Cache eviction is dedicated to reducing the size of KV cache to fit within a constrained budget by evicting certain cache pairs strategically. Eviction masks \\(\\{M_i \\in \\mathbb{R}^{1 \\times n}\\}\\) can be employed to simulate the post-eviction output \\(o'\\) in selfattention mechanism:\n\\[o' = \\sum_{i\\in[h]} A'_i V_i W^O \\quad \\text{where} \\quad A'_i = \\text{softmax}(q_i K_i^T + M_i)\\]\nwhere each element \\(M_{i,j}\\) in mask \\(M_i\\) indicates that whether evicting the \\(j^{th} \\in [n]\\) KV cache pair in \\(K_i, V_i \\in \\mathbb{R}^{n \\times d_h}\\) on each head i:\n\\[M_{i,j}=\\begin{cases} 0 & \\text{if the jth cache pair on head i is retained} \\\\ -\\infty & \\text{otherwise, the jth cache pair on head i is evicted} \\end{cases}\\]\ngiven budget allocation \\(\\{B_i\\}\\) s.t. \\(\\sum_{i\\in[h]} B_i = B\\)\nThus, the budget \\(B_i\\) for head i corresponds to the number of zero elements in \\(M_i\\). Theorem 1 further simplify the output \\(o'\\) by eliminating the softmax function. A detailed proof is provided in Appendix A.1.\nTheorem 1. The post-eviction output o' can rewrite as:\n\\[o' = \\sum_{i\\in[h]} A'_i V_i W^O = \\sum_{i\\in[h]} \\frac{A_i N_i}{\\sum_{j \\in [n]} A_i N_{i,j}} V_i W^O,\\]\nwhere \n\\[N_{i,j} = \\begin{cases} 1 & \\text{if } K_i \\text{ and } V_i \\text{ are retained} \\\\ 0 & \\text{otherwise, evict } K_i \\text{ and } V_i \\end{cases}\\]\ngiven budget allocation \\(\\{B_i\\}\\) s.t. \\(\\sum_{i\\in[h]} B_i = B\\)\nThe reduction in generation quality due to cache eviction stems from alterations of the attention output. Thus, we model the eviction loss using the L1-distance between the post-eviction and original outputs of self-attention mechanism:\n\\[\\text{Eviction Loss} = ||o' - o||_1\\]\nUtilizing the L1-norm of matrix, which can understand how the matrix transforms vectors, we derive an upper bound D for the Eviction Loss in Theorem 2. For a detailed proof, refer to Appendix A.2.\nTheorem 2. The eviction loss caused by cache eviction can be bounded by D as follows:\n\\[\\text{Eviction Loss} \\leq D = 2hC - 2C \\sum_{i\\in[h]} \\sum_{j \\text{ retained}} A_i^j\\]\ngiven budget allocation \\(\\{B_i\\}\\) s.t. \\(\\sum_{i\\in[h]} B_i = B\\)\nwhere \\(C = \\text{Max} \\{\\|V_i W^O\\|_\\infty\\}\\) is the max value in the row norms of Matrices \\(\\{V_i W^O\\}\\) among all heads.\nGiven the first dimension of \\(M_i\\) is 1, \\(M^j\\) is used to simplify the notation for \\(M_i(1, j)\\). Similarly, \\(A^j\\) is in the same manner."}, {"title": "3.4 Adaptive vs. Uniform: Theoretical Insights.", "content": "In existing studies on cache eviction, the budget B is uniformly distributed across each head within a layer: specifically, \\(B_i = B/h\\). Consequently, the upper bound of eviction loss D under any given allocation, is modified to \\(D'\\) under uniform allocation:\n\\[D' = 2hC - 2C \\sum_{i\\in[h]} \\sum_{j\\in[n]} A_i^j\\]\n\\[A_i^j \\in \\text{Top-K}(A_i, B_i)\\]\ngiven uniform allocation \\(\\{B_i = B/h\\}\\)\nIn contrast, we suggest adaptive distribution of the total budget among different heads and introduce a simple yet"}, {"title": "3.5 Adaptive vs. Uniform: Empirical Insights", "content": "According to Theorem 4, the upper bound of eviction loss of adaptive allocation is equal or less than to that of the current uniform allocation. We further demonstrate different attention heads within each layers of LLMs exhibit significant disparities in attention concentration, resulting in the necessity of adaptive budget allocation in practice. For visualization in Figure 2 (a), most concentrated heads in Layer 8 like head 1 require only 1% of the original cache budget to effectively retain the aggregated weights retained \\(A_i\\) of 0.95. Conversely, other heads like heads 18 requires nearly 50% proportion to near 0.95. This characteristic is closely related to the upper bound D of eviction loss, as detailed in Theorem 3. Under such circumstances, the previous uniform budget allocation faces a dilemma, as an illustrative example in Figure 1: either neglect the loss in dispersed heads, or allocate excessive and unnecessary budgets to heads with concentrated attention. This significantly undermines the tradeoff performance between the total budget and generation"}, {"title": "3.6 Implementation", "content": "The current two leading eviction strategies, SnapKV and Pyramid, both utilize the several tokens \\(X_{rec} \\in \\mathbb{R}^{w\\times d}\\) from a recent window (typically window size is 32) to identify and evict the less important cache pairs. SnapKV excels at managing evictions under scenarios with large budgets, while Pyramid is more effective in environments with"}, {"title": "4 Experiments", "content": "Firstly, we carry out an comprehensive evaluation using 16 datasets, covering domains of single-document QA (Ko\u010disk\u1ef3 et al. 2018; Dasigi et al. 2021), multirecall key information from extensive texts."}, {"title": "4.1 Settings", "content": "The two adaptive eviction strategies with alternatingly leading and surpassing previous versions to become the new"}, {"title": "4.2 Evaluations Among 16 Datasets", "content": "We assess all eviction strategies using cache budget \\(B \\in \\{128 \\times h, 256 \\times h, 512 \\times h, 1024 \\times h\\}\\) for each layer. Detailed results for each dataset on the Mistral model are provided in Table 1, while other results for the LWM model are placed in Appendix A.4 due to space constraints. To demonstrate the efficacy of adaptive allocation, we take a budget \\(B = 128h\\) as an example presented in Table 1. After integrating the adaptive allocation algorithm, Ada-SnapKV enhances the quality scores in 15 out of 16 datasets compared to the original SnapKV, increasing the average score from"}, {"title": "4.3 Evaluations on Needle-in-a-Haystack Test", "content": "As shown in Figure 4, we employ a Needle-in-a-Haystack test to demonstrate how adaptive budget allocation can enhance long-context retrieval capabilities. All configurations maintains a recent window size of 32 and a pooling kernel size of 7 which consistent with former experiments, where the maximum inference length is limited to 33K in the full cache case on A100-80G. With a cache budget of \\(B = 128h\\), all four strategies\u2014Ada-SnapKV, Ada-Pyramid, SnapKV, and Pyramid-successfully extend inference length up to 217K. Notably, Ada-SnapKV and Ada-Pyramid both effectively improve long-text retrieval capabilities. In particular, Ada-SnapKV and Ada-Pyramid achieve near-lossless retrieval within the original 33K length, a feat not replicated by the standard SnapKV and Pyramid. In terms of average score, Ada-SnapKV improves from 95.40 to 96.56, while Ada-Pyramid increases from 96.62 to 97.02."}, {"title": "5 Conclusion", "content": "In this study, we reexamine prevailing cache eviction strategies employed in LLMs, discerning their goal to minimize an upper bound of eviction loss. This loss is quantified as the L1 distance between outputs before and after eviction. By introducing an adaptive budget allocation among various attention heads, we theoretically reduce the upper bound compared to previous practices. Our empirical findings suggest that this adaptive approach significantly benefits from the varied concentration levels inherent among multiple heads within the self-attention mechanism. We develop two novel adaptive eviction methods, Ada-SnapKV and Ada-Pyramid, which incorporate this adaptive allocation into two advanced existing strategies. Comprehensive evaluation confirms that both methods yield substantial improvements, especially in small budgets. A comprehensive evaluation verifies that both methods significantly enhance performance, particularly in scenarios with small budgets. Furthermore, this research emphasizes the significant potential for advancing cache eviction strategies through our theoretical framework and adaptive budgeting, which are specifically designed to exploit the unique characteristics of different attention heads in LLMs."}]}