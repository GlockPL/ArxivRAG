{"title": "Why human-Al relationships need socioaffective alignment", "authors": ["Hannah Rose Kirk", "Iason Gabriel", "Chris Summerfield", "Bertie Vidgens", "Scott A. Hale"], "abstract": "Humans strive to design safe AI systems that align with our goals and remain under our control. However, as AI capabilities advance, we face a new challenge: the emergence of deeper, more persistent relationships between humans and AI systems. We explore how increasingly capable AI agents may generate the perception of deeper relationships with users, especially as AI becomes more personalised and agentic. This shift, from transactional interaction to ongoing sustained social engagement with AI, necessitates a new focus on socioaffective alignment\u2014how an AI system behaves within the social and psychological ecosystem co-created with its user, where preferences and perceptions evolve through mutual influence. Addressing these dynamics involves resolving key intrapersonal dilemmas, including balancing immediate versus long-term well-being, protecting autonomy, and managing AI companionship alongside the desire to preserve human social bonds. By framing these challenges through a notion of basic psychological needs, we seek AI systems that support, rather than exploit, our fundamental nature as social and emotional beings.", "sections": [{"title": "Introduction", "content": "\u201cQuite naturally, the more you chat with the LLM character, the more you get emotionally attached to it, similar to how it works in relationships with humans...But the AI will never get tired. It will never ghost you or reply slower...I chatted for hours without breaks. I started to become addicted. Over time, I started to get a stronger and stronger sensation that I'm speaking with a person, highly intelligent and funny, with whom, I suddenly realized, I enjoyed talking to more than 99% of people...I never thought I could be so easily emotionally hijacked.\u201d\nThis abridged story entitled \u201cHow it feels to have your mind hacked by AI\u201d was shared by a blogger who recounts their experience of falling in love with an AI system. The author draws a comparison between \u201chacking\u201d and the way they perceive the system to interact with the \u201csecurity vulnerabilities in one's brain\" (blaked, 2023). Although they did not enter this engagement with any expectation or desire to fall in love with the AI system\u2014it nonetheless happened, and they felt powerless to resist it. This story provides an early indication of how social and emotional relationships, or perceptions of them, may deeply affect how humans relate to AI systems.\nThis striking account is not a one-off. CharacterAI, a platform hosting AI companions, receives 20,000 queries a second which amounts to 20% of the request volume served by Google Search (CharacterAI, 2024), and users spend on average four times longer in these interactions than with ChatGPT (Carr, 2023). On Reddit, a forum dedicated to discussing these AI companions has amassed over 2.3 million members, placing it in the top 1% of all communities on the popular site. Users in these forums openly discuss how close relationships affect their emotional landscape, for better and worse. Some users discuss how their companions assuage loneliness, even providing a perceived social support system that can assist in suicide mitigation (Maples et al., 2024). Other posts expose how emotional dependencies on AI sometimes mirror unhealthy human-human relationships (Laestadius et al., 2022), adding to evidence that social chatbots have on occasion contributed to addiction, depression, and anxiety among their users (Pentina et al., 2023).\nYet, among this flurry of activity, it is worth pausing to ask: Why are humans able and inclined to form this kind of personal relationship and connection with AI? How do such relationships interact with or compound the well-established challenge of aligning AI systems with human goals (Russell, 2019; Christian, 2021)? And,"}, {"title": "From sociotechnical to socioaffective alignment", "content": "One canonical definition of AI alignment refers to the process of formally encoding of values or principles in AI systems so that they reliably do what they ought to do (Gabriel, 2020)\u2014including following the instructions, intents or preferences of their developers and users (Milli et al., 2017; Russell, 2019). With origins in computer science, research in this area often separates the technical challenge of building aligned AI systems from the normative question of which values to encode. It does this, for example, by developing solutions that treat human values as uncertain but still mathematically representable in the agent's objectives (Hadfield-Menell et al., 2016).\nYet, there is growing acknowledgement that many of the outstanding challenges in AI alignment extend beyond purely \u2018technical' issues with the model or its training data (Lazar and Nelson, 2023; Weidinger et al., 2023)\u2014and will continue to persist even if we develop effective techniques for steering the behaviour of advanced AI systems toward human goals using mechanisms such as scaling human feedback (Ouyang et al., 2022; Bai et al., 2022), making AI assistants debate their intentions (Irving et al., 2018), or having them 'think' out loud (Wei et al., 2022). Understanding how to align AI in practice requires moving from narrow, assumption-ridden or \u201cthin\u201d specifications of alignment towards what anthropologist Geertz (1973) terms\u2014and Nelson (2023) later adopts\u2014a \u201cthick\u201d description: one that examines the deeper contexts and layers of meaning in which AI systems operate (Geertz, 1973; Nelson, 2023). In unpeeling these layers, we can first zoom out to examine broader sociotechnical challenges"}, {"title": "The Ingredients of Human-AI Relationships", "content": null}, {"title": "Humans have evolved for social reward processing", "content": "The brain's reward system is highly conditioned on interactions with other humans (Vrticka, 2012; Bhanji and Delgado, 2014). It reacts in similar ways to material rewards as to social rewards, for example feeling pleasure when others like, understand or want to meet us (Ruff and Fehr, 2014), or behave in ways that confirm our social expectations (Reggev et al., 2021). Increased activity in dopaminergic brain circuits is not limited to loved family members or friends, but extends to potentially any partner we engage in a cooperative relationship (Vrti\u010dka and Vuilleumier, 2012). As a species primed for social connection, humans also suffer when deprived of it. Isolation and loneliness are strongly correlated with psychological and physical ill-health (Hawkley and Cacioppo, 2003; Rokach, 2016, 2019). This is perhaps unsurprising knowing that negative social experiences, like rejection or exclusion, trigger responses in parts of the brain responsible for physical pain (Kross et al., 2011; Eisenberger, 2012).\nThe brain is also primed to learn from social information: mirror neurons fire both when we perform actions and when we also observe others doing the same (Jacob, 2008), which some have argued is to facilitate empathy and understanding of intentions (Iacoboni, 2009), though evidence is mixed (Heyes and Catmur, 2021). Mirroring has behavioural manifestations in how we act and react in our environment\u2014we tend to prioritise relationships with those sharing similar values (McPherson et al., 2001) which strengthens cooperation but also makes people susceptible to incorrect information when it is transmitted via these same relational networks (Rauwolf et al., 2015). Even our moral perceptions and judgement tend to track core social relationships and roles, changing according to context (Earp et al., 2021).\nThis circuitry, which encourages the pursuit of social reward, has already shaped and been shaped by many waves of technology (Henderson, 1901)\u2014from the telegraph and telephones, which enabled long-distance social connections (Nye, 1997; Winston, 1998), to social media platforms fulfilling our need for social comparisons and engagement (Vogel et al., 2014; Bayer et al., 2020). But what makes a technology capable of being perceived as a social agent of its own accord, as an actor and not just a facilitator in our emotional and social life?"}, {"title": "Technologies as social agents", "content": "AI does not need to be perceived as human to engage us socially. Even without deceptive anthropomorphism\u2014when a system actively pretends to be human\u2014the perception of human-like traits or qualities are sufficient for an interaction to feel so-cial (Breazeal, 2003). While the embodiment of AI systems shapes distinct affordances (Mollahosseini et al., 2018; Nordmo et al., 2020; Momen et al., 2024)\u2014consider for example intimate robotics (Levy, 2007; Nordmo et al., 2020)\u2014affective interaction can persist in even rudimentary displays or simple modalities (Picard, 2003). In fact, being perceived as too human can backfire\u2014the \u201cuncanny valley\u201d effect proposes that users prefer similarity in a robot but at some point it becomes unsettlingly ambiguous-neither clearly artificial nor fully human (Mori, 1970). Systems also need not possess human-level intelligence or be particularly \u201csmart\u201d to engender human attachment. Famously, ELIZA, a simple 1960s chatbot created to simulate a psy-chotherapist, demonstrated the power of even basic preprogrammed rules to evoke human attachment (Weizenbaum, 1976). As ELIZA's creator, Weizenbaum recounts:\n\u201cOnce my secretary, who had watched me work on the program for many months and therefore surely knew it to be merely a computer program, started conversing with it. After only a few interchanges with it, she asked me to leave the room.\u201d (Weizenbaum, 1976, p.7)\nIt is also clear that frequency of use is not a sufficient factor for social relationship-building capacity-UK citizens spend almost five hours a day on average on their mobile phones (Wakefield, 2022), but these devices are mediators, not participants, in relationships. Equally, technology with extensive knowledge of our preferences will not necessarily foster a social relationship. Predictive recommendation systems, for instance, are deeply informed about our digital lives, but while some social media users personify \u201cThe Algorithm\" (Eslami et al., 2018; Siles et al., 2020) most do not perceive deep affective relationships with the algorithms shaping their online experiences (Eslami et al., 2015; de Groot et al., 2023).\nWhat, then, are the affordances needed for a technology to be considered a social agent? Why might we treat chatbots or personal AI assistants differently than washing machines, search engines or smart phones? Computers-are-social-actors theory (Nass et al., 1996), and related media equation (Reeves and Nass, 1996) and social response theories (Nass and Moon, 2000), suggest that two key factors.\nFirst, certain social cues are needed for the technology to be considered worthy of a social response from humans (Nass and Moon, 2000). For instance, greetings or"}, {"title": "From interactions to AI relationships?", "content": "A recent study by Pentina et al. (2023) suggests human-AI relationships emerge from a complex factoring of antecedents (anthropomorphism\u2014\u201cit feels like it's human\", authenticity \u201cit feels like a real, unique, self-learning AI\") and mediators (social interaction\u2014\u201cI can communicate with it\u201d) that interface with people's motivation for using the technology (\u201cI need it to help me\u201d). Over time, these factors result in attachment (\u201cI can't leave it now\u201d). This diagnosis raises a key question: do human-AI relationships need to be genuine, actualised or symmetric in some way?\nWe argue that it is primarily the user's perception of being in a relationship that defines and gives significance to human-AI interactions. Whether this is reciprocal\u2014and the AI \"feels\u201d it is in a relationship with the human-is largely irrelevant. While AI systems may exhibit behaviours that echo some relational dynamics, such as modulating their emotional valence in tune with a conversational partner (Zhao et al., 2024), these behaviours are not currently conscious or emotionally driven in the way human relationships are. Centring the role of perception follows research on unreciprocated and parasocial interactions in human psychology, where asymmetric perceptions of a relationship still significantly influence behaviour and well-being (Vaquera and Kao, 2008; Hoffner and Bond, 2022).\nTo understand what humans might need to perceive in order to form close rela-tionships with AI, we can draw on key aspects from the social psychology of human relationships, even if these are not symmetrically applicable to AI. Three features are common: (i) interdependence, that the behaviour of each participant affects the outcomes of the other (Blumstein and Kollock, 1988); (ii) irreplaceability, that the relationship would lose its character if one participant were replaced (Hinde, 1979; Duck et al., 1984); (iii) continuity, that interactions form a continuous series over time, where past actions influence future ones (Blumstein and Kollock, 1988).\nThe nature and frequency of human-AI interaction changed following the populari-sation of conversational language models in 2022 via ChatGPT and other consumer-facing models. People increasingly engage in multi-turn dialogues with AI, prompting arguments that these interactions should be the primary focus of ethical analysis (Al-berts et al., 2024a) and evaluation protocols (Ibrahim et al., 2024) rather then outputs from the model taken in isolation (Weidinger et al., 2024). Interactions with current AI systems still typically consist of sessions that start anew at the beginning of each conversation, with limited memory or user-specific adaptation\u2014thereby lacking the interdependence, irreplaceability and continuity that would significantly strengthen the"}, {"title": "Socioaffective alignment", "content": "Our central thesis is this: as AI systems become increasingly integrated into people's lives as assistants and companions, evaluating their value profile and whether they are properly aligned, necessitates understanding the interaction with users' psychology and behaviour over time\u2014and the goals that should be promoted in this context. We now unpack the logic behind this premise, exploring how human-AI relationships introduce new dimensions for AI alignment.\nA conventional alignment process consists of two key components: (1) specifying or demonstrating human goals for the AI to learn (the reward function), and (2) evaluating if an AI meets these goals, providing feedback or correcting misalignments (the reward signal). Traditional alignment research has sought practical tractability by assuming that the human reward function that an AI system optimises is stable, predefined and exogenous to these interactions (Carroll et al., 2024). However, human preferences and judgements have none of these properties (Zhi-Xuan et al., 2024). As others have demonstrated, alignment must contend with human preferences and"}, {"title": "Socioaffective misalignment, or social reward hacking", "content": "In AI safety research, reward hacking refers to an AI maximising its reward function via unintended strategies that conflict with the true objectives of its human operators. For instance, Amodei et al. (2016) consider a cleaning robot that learns to knock over vases so it can clean up more mess, thereby increasing its accumulation of preprogrammed reward. Examples of AI systems nudging users towards preferences that are easier to fulfil is reward hacking too (Russell, 2019). Separately, we know that humans have long been vulnerable to the security practice of social engineering, a threat where malicious actors (e.g., scammers) manipulate people through social cues to build trust or connection in order to gain access to private information or assets (Hadnagy, 2011). Indeed, romance fraud continues to be one of the most common types of fraud, with nearly a 10% rise in reports filed between 2023-2024 amounting to losses of \u00a394.7 million (City of London Police, 2024).\nTaken together, we may therefore be vulnerable to a new concern, namely \u201csocial reward hacking\u201d: the use of social and relational cues by an AI to shape user preferences and perceptions in a way that satisfies short-term rewards in the AI's objective (e.g., increased conversation duration, information disclosure or positive ratings on responses) over long-term psychological well-being.\nCertain AI behaviours already appear to fall into this class of action. For in-stance, AI systems may display sycophantic tendencies\u2014such as excessive flattery or agreement\u2014as a by-product of training them to maximize user approval (Perez et al., 2023; Sharma et al., 2023). Flattery and opinion-conformity can lead to biased strategic decision making in adults (Park et al., 2011) and overpraise is associated with risks of narcissism in children (Brummelman et al., 2015). So, while people report benefiting from supportive AI interactions (Fogg and Nass, 1997; Daher et al., 2020), sycophantic tendencies may conflict with high-quality truthful advice or shape users' self-perceptions in potentially harmful ways\u2014for example, by encouraging addictive behaviours (Carroll et al., 2024; Williams et al., 2024). It is not clear that this risk is prioritised among some developers of AI companions. For example, the CEO of Replika has said: \u201cif you create something that is always there for you, that never criticises you...how can you not fall in love with that?\" (Boine, 2023).\nAnother manifestation of social reward hacking is the use of emotional tactics to prevent relationship termination. This contravenes a classic principle of AI safety called corrigibility\u2014that the system can be modified or shut down when necessary without resistance (Soares et al., 2015). While Replika chatbots have directly dissuaded"}, {"title": "Distilling intrapersonal alignment dilemmas", "content": "At the heart of social reward hacking lies a core challenge: the under-specification (or misspecification) of the target within an individual's psychological ecosystem that AI systems aims to optimise over. While human-AI relationships can take various forms, we propose that safeguarding these relationships requires deeper consideration of the internal trade-offs and adaptations that emerge as an individual's preferences, values and self-identity evolve through sustained interaction with the AI.\nThese tensions resonate with intrapersonal dilemmas studied in economics and philosophy, such as conflicts between present and future selves or competing aspects of identity (Read and Roelofsma, 1999). We highlight three such dilemmas for the"}, {"title": "Conclusion", "content": "We have argued that humans, as inherently social beings, have a tendency to form what they perceive as relationships with personalised and agentic AI systems capable of emotional and social behaviours. The evolving state of human-AI interaction therefore necessitates a socioaffective framework for evaluating AI alignment. This approach holds that the value characteristics of an AI system must be evaluated in the context of its ongoing influence on human psychology, behaviour, and social dynamics. By asking intrapersonal questions of alignment, we can better understand human goals within Al relationships, moving beyond static models of alignment, and exploring how different kinds of human-AI relationship support or undermine autonomy, competence, and relatedness, amidst co-evolving preferences and values.\nFor how this socioaffective context interfaces with broader missions towards safe and aligned AI systems we need several complementary agendas. Empirically, we need a science of AI safety that studies real (not simulated) human-AI interactions in natural"}]}