{"title": "Learning segmentation from point trajectories", "authors": ["Laurynas Karazija", "Iro Laina", "Christian Rupprecht", "Andrea Vedaldi"], "abstract": "We consider the problem of segmenting objects in videos based on their motion and no other forms of supervision. Prior work has often approached this problem by using the principle of common fate, namely the fact that the motion of points that belong to the same object is strongly correlated. However, most authors have only considered instantaneous motion from optical flow. In this work, we present a way to train a segmentation network using long-term point trajectories as a supervisory signal to complement optical flow. The key difficulty is that long-term motion, unlike instantaneous motion, is difficult to model \u2013 any parametric approximation is unlikely to capture complex motion patterns over long periods of time. We instead draw inspiration from subspace clustering approaches, proposing a loss function that seeks to group the trajectories into low-rank matrices where the motion of object points can be approximately explained as a linear combination of other point tracks. Our method outperforms the prior art on motion-based segmentation, which shows the utility of long-term motion and the effectiveness of our formulation.", "sections": [{"title": "1 Introduction", "content": "Segmentation, the task of delineating and isolating distinct objects, is a fundamental problem in computer vision. Much of the current approaches are supervised, relying on expensive manual annotations. Attempts to approach this task without supervision have largely relied on manual heuristics or exploited the rich semantics of self-supervised feature extractors. Video data, however, offers an additional option as it contains motion, which can be exploited for an additional inductive bias. Such approaches are rooted in the principle of common fate from Gestalt psychology [66], which posits that elements that move together are more likely to belong together.\nMotion information is usually captured by optical flow. Flow is attractive as it arises from low- level visual properties and can provide a signal before scenes are parsed and objects are discovered. Furthermore, optical flow estimators, such as RAFT [60] or FlowFormer [24], can be trained purely on synthetic artificial data, transferring to real-world scenes with remarkable accuracy and without manual annotation. This has led many to consider optical flow as a critical modality to discover and learn objects from video data by learning to attribute and explain the motions of objects.\nOptical flow, however, only describes the instantaneous motion of the scene, which can create blindspots: not all objects are necessarily in motion at all times. Similarly, groups of objects might coincidentally move together. Recent advances in point tracking [14, 15, 22, 28] offer an alternative form of motion information. Point trackers \u201clock on\u201d to a set of query points and describe their position and visibility over the course of the whole video. This provides long-term motion information. Like optical flow estimators, point trackers are trained on synthetic data. However, unlike optical flow, point trajectories describe only a sparse set of points."}, {"title": "2 Related work", "content": "Unsupervised video object segmentation. Video object segmentation (VOS) aims to label pixels of objects in a video. Current VOS benchmarks [35, 51, 53] usually define the problem as binary foreground-background separation or salient object segmentation. The task is usually approached in two ways: semi-supervised and unsupervised VOS. Semi-supervised methods require initial frame annotations and aim to propagate them to the rest of the video [7]. Unsupervised VOS aims to discover object(s) of interest without the initial targets [18, 25, 36, 42, 52, 61]. This however does not differentiate methods based on data used to train them. Most of the traditional research in semi- or unsupervised VOS relies on annotations during training. Our approach, in contrast, does not rely on any manual annotations to learn. Some authors explore related unsupervised video instance segmentation [65] task without any annotations, object-centric learning appraoches [2, 58, 73], some of which make use of flow [29] and depth [57].\nMotion segmentation. A closely related task to video object segmentation is motion segmentation, which aims to extract the main moving objects in a video. The practical difference between these two tasks is more difficult to delineate as the same benchmark datasets are often used. Early works modeled the scenes as layers [8, 27], which later works accomplish using a slot-attention mechanism [13, 34, 69]. Flow mixture models accounted for multiple motion patterns [26, 62], and corrections were introduced for rotating cameras [3, 4]. Later works [10, 46, 47] considered parametric flow models fit to explain the scene. AMD [40] employs a single model with separate appearance and motion 'pathways'. Other works train flow-only models by generating synthetic data, which generalise well to real videos [33, 67]. An alternative line of work adopts a more generative approach, training an inpainter networks to predict optical flow [70, 71]. Several authors [37, 59]"}, {"title": "3 Method", "content": "Our goal is to solve the video segmentation task in an unsupervised manner: given a video, we want to segment out the objects that are moving independently within it. A video is a sequence of frames $I_t \\in \\mathbb{R}^{H\\times W\\times 3}$, each of which is an RGB image defined on the lattice $\\Omega = vec(\\{1, ...,H\\} \\times \\{1, ...,W\\}) \\in \\mathbb{R}^{HW\\times 1}$. To segment the objects, we self-supervise a neural network $\\Phi$ that takes as input each frame $I_t$ in turn, and outputs a corresponding segmentation mask $\\Phi(I_t) = M_t \\in [0, 1]^{HW\\times K}$ where $K$ is the number of possible segments we expect to observe in the video. Segmentation matrix entries softly assign each pixel to one of $K$ possible segments.\nThe challenge is how to supervise the network without labels, utilising only the video itself as training material. The key inductive principle that we propose to use is that physical points that belong to the same object tend to have highly correlated motion, often called principle of common fate. When these points are projected to pixels, they result in corresponding highly correlated apparent motions, which we can measure using techniques like optical flow and point tracking. Therefore, we propose to supervise the network $\\Phi$ from an analysis of apparent motion extracted automatically from the video using off-the-shelf components.\nMotion can be measured at two temporal scales. Optical flow extracts instantaneous motion, mea- suring the 2D velocity of the 3D points found at each pixel in each video frame. Point tracking extracts long-term motion, estimating the 2D location of a certain number of 3D points throughout the video's duration. These two sources of information are complementary. Optical flow is dense, easy to extract, and easy to model to discover correlations within it; however, by considering different times in isolation, it ignores most of the correlations that exist in the data. Tracks are sparse, more difficult to extract and harder to model, but potentially contain information ignored by optical flow.\nPrior works such as [10] have studied how to model optical flow for segmentation. Here, motivated by a new generation of high-quality point trackers [14, 15, 22, 28], we aim at developing the machinery necessary to use track information as well. From this analysis, we construct losses which assess the quality of the predicted mask $M_t$ given the video itself. Next, we introduce two such losses, one for optical flow from prior work, and a new one based on point tracking."}, {"title": "3.1 Learning from optical flow", "content": "First, we describe the case of optical flow. Because optical flow is instantaneous, we can fix our attention on a specific frame $I$ and corresponding mask $M$, dropping for now the time index $t$. The optical flow $F\\in \\mathbb{R}^{HW\\times 2}$ for this image associates a 2-dimensional flow vector to each of the $H \\times W$ pixels. Each flow vector can be understood as the velocity of the pixel.\nLet $M_k \\in \\mathbb{R}^{HW\\times 1}$ be the binary matrix for segment $k$, obtained by extracting the $k$-th column of $M$. Let $F_k \\triangleq M_k \\odot F$ denote the Hadamard (element-wise) product between the mask and flow vectors, broadcasting the mask along the rows.\nAssuming that the object is rigid, the optical flow can be approximated as a linear parametric model of 2D coordinate embeddings (see [1] for an overview). Following [10], we consider a six-dimensional quadratic embedding kernel $emb([x, y]) = [x, x^2, y, y^2, xy, 1] \\in \\mathbb{R}^{1\\times 6}$ for pixel coordinates $[x, y] \\in \\Omega$ and associate to each region $k$ a corresponding set of 12 parameters $\\theta_k \\in \\mathbb{R}^{6\\times 2}$. Optical flow vectors within a region should be expressible as a linear combination of these six basis functions.\nWe then consider all pixels embeddings stacked in a single matrix $E_k = M_k \\odot emb(\\Omega)$ where the product with the soft mask ensures that the embeddings are \"active\" only if the corresponding pixels are. The optical flow vectors in the region are then approximated as\n$F_k \\approx \\hat{F_k} = E_k \\theta_k \\text{ where } \\theta_k = (E_k^\\top E_k)^{-1} E_k^\\top F_k$,\nwhere $\\theta_k$ is obtained via least square. We can use the residual of this approximation as a measure of how well the mask $M_k$ fits the data:\n$\\mathcal{L}_f(M/F) = \\sum_k ||F_k - \\hat{F_k}||^2 = \\sum_k ||F_k - E_k\\theta_k||^2$.\nIntuitively, this considers the correlation of pixel motion in the spatial sense: how pixel coordinates determine its motion based on motion parameters $\\theta_k$."}, {"title": "3.2 Learning from trajectories", "content": "Having covered optical flow, we move now to developing an analogous loss for tracking. We write $P\\in \\mathbb{R}^{2T\\times N}$ for the track matrix, with one trajectory per column. With slight abuse of notation, we write $(P)_t \\in \\mathbb{R}^{2\\times N}$ for indexing rows corresponding to point locations at some time $t$. To connect pixel-wise masks and sparse points, we use a sampling operation $\\pi(\\cdot)$, writing $\\pi(M_k, (P)_t) = M_k \\in [0, 1]^{N\\times 1}$ for mask values at point locations at an appropriate time. Furthermore, we denote by $P_k \\triangleq P \\odot M_k$ the masked version of the trajectory matrix, selecting the columns/trajectories that belong to object $k$ with obvious broadcasting of the mask values.\nUnlike optical flow, trajectories are too complex to be modelled using a small set of fixed basis functions. Instead, we posit that the set of trajectories should be low-rank - all trajectories belonging to the same object should be explained well by a linear combination of some small number of trajectories. We illustrate this intuition in Fig. 1 using a 2D example.\nThis assumption results in a factorization of $P_k$ using singular value decomposition (SVD) as $P_k = U_k \\Sigma_k V_k^\\top$, where $(U_k, \\Sigma_k, V_k) = SVD(P_k)$. As $P_k$ should be low-rank, we can thus form an approximation using truncated SVD, by considering only first $r$ components. We write $[U_k]_r$ to denote such truncation. With this, we obtain the loss\n$\\mathcal{L}_{rec@r}(M|P) = \\sum_k ||P_k - [U_k]_r \\Sigma_k [V_k]_r^\\top||_F^2$.\nSince truncated SVD offers optimal decomposition for the error above, lowering this loss amounts to making $P_k$ as close as possible to rank $r$, i.e., by grouping trajectories into $P_k$ that do not increase its rank, and should come from rigid objects.\nAs we show in Section 5.3, we found an alternative formulation of this idea works better. Note the rank $r$ matrix has the $r$-th and all later singular values as 0. We can optimise singular values higher than $r$-th to be close to 0 (ignoring $U_k$ and $V_k$). Thus, for trajectories, we formulate a loss simply as:\n$\\mathcal{L}_t(M|P) = \\sum_k \\sum_{i=r}^{min(2T, N)} \\sigma_i(P_k)$,\nwhere $\\sigma_i(P_k)$ is the $i$-th singular value of $P_k$. We assume $r < min(2T, N)$."}, {"title": "3.3 Training a segmenter using flow and trajectories", "content": "The losses above require optical flow $F$, trajectories $P$, and masks $M_k$ obtained using a segmentation network $\\Phi(I) = M$. This suggests a simple procedure of training a segmentation network given a dataset of videos, which we summarise in Fig. 2. We precalculate optical flow for each frame and obtain a set of point trajectories for each video using off-the-shelf pretrained networks. For training, we consider triples of $(I, F, P)$; for each frame $I$, where for trajectories $P$, we take trajectories for which the points are visible in the image $I$. This can be accomplished by making use of visibility predictions in the output of point trackers or calculating trajectories by querying points in each frame. We use bilinear sampling for $\\pi(\\cdot)$ to obtain mask values at trajectory coordinates.\nTemporal smoothing. We include a temporal smoothing loss, which matches mask predictions between two frames offset by $\\Delta t$ using the predicted trajectories:\n$\\mathcal{L}_\\top = ||\\pi(\\Phi(I_t), (P_t)_t) - \\pi(\\Phi(I_{t+\\Delta t}), (P_t)_{t+\\Delta t})||_2$,\nwhere $I_t$ is the $t$-th frame and $P_t$ are trajectories associated with $t$-th frame. We write the final loss as: $\\mathcal{L} = \\lambda_f\\mathcal{L}_f + \\lambda_t\\mathcal{L}_t + \\lambda_{\\top}\\mathcal{L}_\\top$, where $\\lambda_f, \\lambda_t, \\lambda_{\\top}$ balance the contribution of the different loss terms."}, {"title": "4 Feasibility study", "content": "Our proposed trajectory loss (4) enables training a segmentation network using trajectory data. We first show the feasibility of the proposed cost function in a controlled setting, without actually training $\\Phi$. To this end, we consider a synthetic scene from the MOVi-F Kubric [20] dataset for which we obtain ground-truth trajectories for every point and ground-truth object segmentation masks. We explore the loss landscape of the proposed formulation by corrupting the segmentation masks along several principled axes and studying the effect of such corruptions on the trajectory loss.\nFirst, we consider a random alteration of mask pixels, which we refer to as mask noise. We control the amount of mask noise using $\\eta$ such that 0.0 corresponds to no pixels changed and 1.0 corresponds to completely random masks. Along this axis, we test whether our loss favours predictions with lower noise. Second, we consider structural alterations, namely under/over-segmentation. To simulate under-segmentation, we merge object masks with the background at random. To simulate over- segmentation, we randomly split the existing object mask into two parts in the middle along either the $x$ or $y$-axis. We represent this type of mask corruption using integers. Negative values indicate the number of objects removed, while positive values correspond to new objects generated from existing ones. Such structural corruption investigates whether the loss can correctly identify the number of moving objects. Finally, we consider the \u201csoftness\" of the predicted masks by transforming masks into logits and increasing the temperature $\\tau$ in the softmax operation. This tests whether the loss will prefer low-entropy values. We leave further details of the corruption procedure to Appendix D.\nThe results of these analyses are shown in Figure 3. All three plots show the loss value as a function of structural corruption. The trajectory loss decreases as the noise and temperature of the masks are reduced, as seen in the first two plots. The third plot also shows that such solutions are preferred in combination. Furthermore, we observe that the loss values are lower when the correct number of segments is detected, and this holds even in the presence of noise or when masks are more uniform. Note, however, that over-segmentation is penalised less than under-segmentation, i.e., missing moving objects leads to a higher value of the loss than, e.g., splitting an object into several components."}, {"title": "5 Experiments", "content": "In this section, we evaluate our approach for unsupervised motion segmentation and compare it with simple baselines and prior subspace clustering methods. Next, we compare our method with state-of-the-art methods for unsupervised video object segmentation across several datasets in a binary segmentation setting. We finish with ablation experiments of our approach.\nDatasets. We consider four primary datasets in this study. We use the synthetic MOVi-F variant of the Kubric [20] dataset with ground truth trajectories for comparison with subspace clustering-based approaches. We adopt this setting to eliminate noise in point trajectories as previous methods are sensitive to it. We report the adjusted Rand index (ARI) as the main metric, measuring how close clustering is to the ground truth up to the permutation of cluster identities, where 1 is a perfect match, and 0 means roughly random assignment. We also report FG-ARI, i.e., ARI only on foreground pixels (determined by ground truth masks), which identifies how well different objects are separated.\nWe also evaluate our approach on real-world datasets: DAVIS 2016 [53], SegTrackv2 (STv2) [35], and FBMS [51], which are popular benchmarks for video object segmentation. Following standard practice [69, 70], foreground objects in STv2 and FBMS are consolidated. We report the Jaccard ($\\mathcal{J}$) score, computed using Hungarian matching between predicted and ground truth segmentations.\nImplementation. For the experiments on real-world datasets, optical flow is estimated using RAFT [60] and point trajecto- ries using CoTracker [28]. Trajectories are computed within a context window $f = 20$ around each frame, with reflec- tion padding around video boundaries, resulting in chunks of $T = 2f +1 = 41$ frames. To reduce the effect of noisy predic- tions, we also filter trajectories along the time dimension using an average filter with a window size of 11. For the experiments on MOVi-F, a small U-Net [56] is trained as the segmentation network, starting from random initialisation. For fairness of comparisons on DAVIS, STv2 and FBMS, we use the same architecture as in [10] - MaskFormer with DINO backbone. We specify further details in Appendix E."}, {"title": "5.1 Comparison to trajectory-based methods", "content": "In Table 1, we compare our low-rank trajectory loss (LRTL) with prior subspace clustering approaches in a per-video optimisation setting. Subspace clustering operates on a similar intuition to our proposed trajectory loss by a grouping of trajectories that should be linearly dependent. We also consider K- means clustering of trajectories as a simple baseline. For fair comparisons, we train our segmentation model optimising only the trajectory loss ($\\mathcal{L}_t$). We use k = 25 components for each video and train for 5000 steps. This is comparable to the computation requirements and steps of other methods. For K-means, SSC [17] and LRR [38], we search for an optimal set of hyperparameters and the number of components k, reporting the best results. Our approach shows significantly stronger performance than simple K-Means and subspace clustering approaches."}, {"title": "5.2 Unsupervised video object segmentation", "content": "We compare to recent methods on the unsupervised video object segmentation task without first-frame prompting or post-processing. In this setting, we train a single network on the benchmark datasets for binary video segmentation. We compare with single-sequence methods that perform optimisation for each sequence/video individually. Additionally, we benchmark dataset-wide single-stage end-to-end methods where training is performed over multiple videos simultaneously, training a network in an end-to-end manner. We also compare with multi-stage methods that train and re-train several networks. We report our results on standard benchmarks in Table 2. While the closest prior work relies on multiple stages of training, pseudo-labelling, applying CRF, and retraining, our end-to-end trained method shows better performance at lower resolutions. We attribute this to the effectiveness of our approach in incorporating long-term motion information.\nIn Fig. 4, we show qualitative results of our approach and compare with RCF [37], a state-of-the-art multi-stage approach. Our network trained with both flow and trajectory losses yields segmentations with noticeably better boundaries despite operating at a lower resolution. Notably, our formulation also effectively avoids segmenting shadows and water ripples of the swan, which are difficult to separate based on instantaneous motion alone."}, {"title": "5.3 Ablations", "content": "Alternative losses. We have explored several alternative formulations of the trajectory loss in our approach and present the analysis in Table 3. Losses based on full SVD reconstruction fail to train a network sufficiently. $\\mathcal{L}_{per}$ performs the best out of these, likely as DAVIS contains several scenes"}, {"title": "6 Conclusion", "content": "We have introduced a principled method to train an image segmentation network using long-term motion information expressed as point trajectories. Our trajectory loss formulation follows the principle of common fate and aims to group trajectories into low-rank matrices, representing the idea the motion of points belonging to the same object can be roughly explained as a combination of other points. Using synthetic data we have shown that such a loss should prefer low-noise and low-entropy solutions as well as identify the correct number of moving objects. In comparison with other methods,"}, {"title": "A Broader impact", "content": "Segmentation is a component in a very large and diverse spectrum of applications in healthcare, image processing, computer graphics, surveillance and more. As with many technologies, the application can be good or bad. In this paper, we explore how to train a model to perform segmentation in an unsupervised manner. This has the positive benefit of removing manual labour requirements to obtain annotations, which might also eventually apply to bad actors. We, however, consider the immediate real-world impact beyond the research community of our work here limited as unsupervised systems still show lower performance than supervised counterparts."}, {"title": "B Additional ablations", "content": "Rank r of trajectory matrix. In Table 5, we vary r, the rank of the trajectory matrix used in the trajectory loss (Eq. (4)). As previously mentioned, the choice of rank reflects the degrees of freedom in the system and controls implicitly the assumptions about the types of motion and cameras used to capture sequences. At r = 3 and 4, we observe slight impact on the performance in comparison to r = 5. r = 5 appears to be the optimal setting, which is what we used in our main experiments. At r = 6, the performance drops again, likely as it becomes sufficient to group and explain simple motions together.\nNumber of segments k. In Table 6, we vary k, the number of masks predicted by our method, before merging. As in prior work [10], the k = 4 appears to be the optimal setting. The performance drops beyond this point as it becomes difficult to group objects.\nInfluence of context window length T. In Table 7, we vary the length of the context windows (f) and thus T for our method when considering trajectories. We find increasing the context window helps slightly. However, the performance starts to drop afterwards. We hypothesise that this is due to difficulty predicting sensible point trajectories for points that move outside of the frame and become invisible, as DAVIS contains many videos where the camera tracks the main subject. Though several values of this setting are viable."}, {"title": "C Additional results", "content": "C.1 Qualitative results on SegTrackv2\nIn Fig. 5, we provide additional qualitative results from our approach on the SegTrackv2 dataset. We compare with the state-of-the-art multi-stage Relaxed Common Fate (RCF) approach [37]. Our method correctly identifies more parts of the objects and has better boundaries."}, {"title": "D Parametric mask alterations", "content": "In this section, we show the effect of the parametric ground truth mask alterations used to study the trajectory loss in section 4.1. The purpose of these alterations is to disturb ground truth masks in a"}, {"title": "E Implementation details", "content": "Here, we further specify the configuration and implementation details used in our experiments.\nE.1 Extracting flow\nWe use RAFT [60] to extract optical flow pretrained on FlyingThings3D [44]. We follow the methods used to extract flow in previous work [10, 69]. Namely, we consider pairs of frames with a distance in time of either 1 or 2, both in forward and backward directions for DAVIS and SegTrackv2. For FBMS, we consider distances of 3 and 6 due to lower motion setting in the dataset. The optical flow is extracted before training.\nE.2 Extracting trajectories\nWe use CoTracker [28] to extract point trajectories. CoTracker is trained on MOVi-F Kubric [20] datasets. We use CoTracker v2. We query at every 4th-pixel coordinate for each frame to extract point trajectories. At 480 \u00d7 854 resolution for DAVIS, this results in 25k points for each frame. When tracking, we find it beneficial to inject auxiliary query points. For this, we define two additional query grids with a stride of 32, querying a frame seven frames in the future and the past (or less if at the video boundaries). This generates around 2k additional points, which we do not use for training. When processing videos of heterogeneous resolutions, we resize the input to 480 \u00d7 854 to maintain the same number of points.\nE.3 Training hyperparameters\nFor the segmentation network, we use the same model architecture as in [10] \u2013 MaskFormer with DINO backbone. We feed images at 192 \u00d7 352 resolution. We also use random horizontal flipping augmentation. The network is trained to predict k = 4 components, which, in the case of binary segmentation, are then merged into two following [10]. We train using AdamW optimiser, with a learning rate of 1.5e-4, weight decay of 0.01, a batch size of 8, and a linear learning warmup schedule for 1500 iterations. We train for 5000 iterations. We use an Exponential Moving Average (EMA) with the decay power of 2/3 with a warmup of 1500 iterations and update every 10 steps to help stabilise the training. On SegTrackv2 we instead used decay power 4/5 as the dataset is considerably smaller than others. We set $\\lambda_f = 0.03$, $\\lambda_t = 5*10^{-5}$, and $\\lambda_{\\top} = 0.1$ in all experiments, which yields loss values in a similar numerical range. For the temporal smoothing loss, we use $\\Delta t = 5$.\nE.4 MOVi-F experiments\nWhen conducting experiments on the MOVi-F dataset (Sec. 4.2), we consider ground-truth trajectories obtained from modified rendering script [20]. We normalise the trajectories to the [0, 1] range based on image width and height.\nFor K-Means, we consider the trajectories with the initial position at T = 0 subtracted, thus clustering offsets from the initial position.\nFor SSC [17], we translate the method to Python following the original implementation in Matlab. We use the ADMM variant, which we found to give better results. We set the $\\alpha = 100$ and kept the rest of the hyperparameters unchanged. To transform the coefficient matrix into a graph adjacency, we found that simple symmetrisation yielded slightly better results than the proposed method that additionally normalised and filtered values. We report results for this method using the optimal number of clusters for spectral clustering.\nFor LRR [41], we similarly translate the method to Python following the original implementation in Matlab. We use $\\lambda = 0.2$. Additionally, we found it beneficial to reduce $\\rho = 1.01$ and use a larger number of iterations (10k) than proposed. Similarly to SSC, we experimented with different ways to transform the coefficient matrix to adjacency, including automatically determining the number of clusters based on the block-diagonal structure. We found, however, that using simpler symmetrisation with optimal numbers of clusters determined by an oracle gave the best results.\nWhen considering our trajectory loss, we parameterise the masks with a small randomly initialised Unet [56] predicting a 25-way segmentation, which we optimize using AdamW optimizer.\nNote that K-Means, SSC and LRR baselines cluster trajectories rather than segmenting the image. To map back to the image domain and obtain segmentation masks, we repeatedly apply the method for each frame within a sequence, considering the trajectory for each pixel. This enables the most direct way to establish segmentation of the images through significant additional computation effort. An alternative could be to consider sequence wide-trajectories jointly; however, approaches like SSC and LLR do not scale well to such a large number of trajectories. For our trajectory loss, optimisation can be performed per sequence and, as we show in our real-world experiments, dataset-wide."}, {"title": "NeurIPS Paper Checklist", "content": "1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\nAnswer: [Yes]\nJustification: We have confirmed the viability of our loss formulation in controlled simu- lated settings, per-sequence optimisation settings with no tracking noise and in real-world settings. We also considered alternative formulations of the trajectories and found them to underperform.\nGuidelines:\n\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.\n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.\n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.\n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: See Section 5.3.\nGuidelines:\n\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.\n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.\n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.\n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.\n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.\n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.\n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.\n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.\n3. Theory Assumptions and Proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?"}, {"title": "4. Experimental Result Reproducibility", "content": "Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: We list all relevant details in Appendix E.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.\n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.\n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.\n\u2022 While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results."}, {"title": "5. Open access to data and code", "content": "Question: Does the paper provide open access to the data and code", "material?\nAnswer": ["No"], "nJustification": "We do not include code at the time of submission but commit to releasing the code and models at a later time.\nGuidelines:\n\u2022 The answer NA means that paper does not include experiments"}]}