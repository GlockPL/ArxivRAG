{"title": "Forward Once for All: Structural Parameterized Adaptation for Efficient Cloud-coordinated On-device Recommendation", "authors": ["Kairui Fu", "Zheqi Lv", "Shengyu Zhang", "Fan Wu", "Kun Kuang"], "abstract": "In cloud-centric recommender system, regular data exchanges between user devices and cloud could potentially elevate bandwidth demands and privacy risks. On-device recommendation emerges as a viable solution by performing reranking locally to alleviate these concerns. Existing methods primarily focus on developing local adaptive parameters, while potentially neglecting the critical role of tailor-made model architecture. Insights from broader research domains suggest that varying data distributions might favor distinct architectures for better fitting. In addition, imposing a uniform model structure across heterogeneous devices may result in risking inefficacy on less capable devices or sub-optimal performance on those with sufficient capabilities. In response to these gaps, our paper introduces Forward-OFA, a novel approach for the dynamic construction of device-specific networks (both structure and parameters). Forward-OFA employs a structure controller to selectively determine whether each block needs to be assembled for a given device. However, during the training of the structure controller, these assembled heterogeneous structures are jointly optimized, where the co-adaption among blocks might encounter gradient conflicts. To mitigate this, Forward-OFA is designed to establish a structure-guided mapping of real-time behaviors to the parameters of assembled networks. Structure-related parameters and parallel components within the mapper prevent each part from receiving heterogeneous gradients from others, thus bypassing the gradient conflicts for coupled optimization. Besides, direct mapping enables Forward-OFA to achieve adaptation through only one forward pass, allowing for swift adaptation to changing interests and eliminating the requirement for on-device backpropagation. Further sophisticated design protects user privacy and makes the consumption of additional modules on device negligible. Experiments on real-world datasets demonstrate the effectiveness and efficiency of Forward-OFA.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advances in deep learning have significantly enhanced the capabilities of recommender system, particularly in extracting user preferences from intricate sequential data[9, 47]. Traditional on-cloud recommendation methods primarily focus on enhancing the scalability and generalizability of models deployed on cloud. In such systems, user requests are processed on cloud, and recommendation lists are subsequently delivered. This process necessitates transmitting user data between remote devices and cloud, which can introduce substantial network overhead, especially in scenarios characterized by frequent user interactions [19, 43, 52]. Moreover, these user requests often include sensitive information, such as recent item interactions(e.g. item id), or even user private profiles(e.g. ages, income, etc.). Uploading these sensitive data to cloud may result in a potential leakage of user privacy.\nBenefiting from the booming computational resources on mobile devices, recommender system is now deploying models directly to mobile devices to better serve users [11, 38]. This paradigm leverages the computational resources of devices to conduct real-time reranking [11], eliminating the need for data uploads to cloud servers for processing. Such an approach not only mitigates network traffic burdens but also significantly enhances user privacy protections. Existing research in this area generally falls into two main categories: personalization-based and cost-aware mechanisms. The former focuses on tailoring device-specific parameters [24, 25, 27, 29, 42] to enhance the modeling of long-tailed users and items[30]. Contrastingly, cost-aware methods prioritize minimizing both the communication overhead involved in synchronizing local models from cloud and the computational cost of on-device inference due to the requirement for continuous adaptation. For instance, some studies [26, 31, 41] explore optimal moments for updating models from cloud to prevent unnecessary data transmission. Other works [6, 10, 33, 40] make an effort to remove redundant parameters for efficient transmission and inference.\nWhile the aforementioned methods have made significant strides in on-device recommendation, their focus has predominantly been on network parameters, potentially overlooking the importance of network structures. Current research demonstrated that the uniqueness of network structures plays a vital role in data distribution modeling [48, 53]. Consistently deploying identical networks does not ensure satisfactory services for most devices with changing interests. When it comes to resource heterogeneity, universally applying larger networks will add a substantial burden for devices with limited resources and disrupt the functionality of other applications due to continuous resource usage. On the contrary, applying smaller models for all devices may not fully utilize the capabilities of more robust devices.\nConsequently, the adaptive construction of networks has become critical for user-oriented recommender systems. Given its demanding and challenging characteristics, this problem raises four crucial research challenges: (i): How to design the corresponding local structures for each device accurately? (ii): How to efficiently build adaptive models for a given device to accommodate its local varying interests? (iii): How to protect user privacy from exposure to cloud during this process? (iv): How to avoid increasing much burden on devices due to the additional modules?\nIn light of this, we propose Forward-OFA(Forward Once For All), an approach to building both compatible and compact networks for a given device with just one forward pass and without necessitating further on-device backpropagation after training on cloud. Current sequential recommenders, which consist of stacked blocks [20, 34, 36, 44], guide us to consider learning inference paths containing one or more blocks as specific structures. In particular, we sampled heterogeneous paths from a discrete"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Sequential Recommendation", "content": "The field of sequential recommendation has witnessed significant advancements in recent years. Traditional sequential recommenders mainly rely on the Markov assumptions[15, 32], where current interactions only depend on the most recent interaction or interactions, making it challenging to capture long-term dependencies. After the emergence of neural networks[45], neural sequential recommenders have started to utilize advanced techniques to enhance modeling capabilities and account for long-term dependencies[7]. For instance, GRU4Rec[16], a solution based on recurrent neural networks, can effectively capture temporal dependencies in sequences. Given multiple history items for each user, determining the importance of them to the target item becomes a critical issue. Attention-based mechanisms [3, 20, 34, 49] address it by assigning soft attention weights to items at different time steps, allowing for more flexible integration of information and more precise capture of user interests. Additionally, some CNN-based recommenders [36, 44] utilize a convolutional network to process user sequential information by treating the user's embeddings as a picture. However, all the above methods are essentially served on cloud, necessitating frequent message transmission and raising privacy concerns. Compared with them, Forward-OFA and other on-device recommendation methods, are motivated to handle these problems by performing recommendations locally owing to the increasing on-device resources. These model-agnostic frameworks reduce user response time and protect user privacy from exposure to cloud."}, {"title": "2.2 On-device Recommendation", "content": "Coinciding with the continuous upgrading of computing resources on mobile devices, on-device recommendation is widely used to alleviate network congestion and privacy issues. The majority of current research aims to exploit the diversity of models on devices to enrich personalized user services [5, 39]. DCCL [42] incorporates meta-patch into recommenders to avoid the expensive update of finetuning the entire model, the gradient will be passed to cloud server for aggregation. DUET[27] directly sends personalized parameters to device to better adapt to the user's local interest. PEEL [51], on the other hand, groups users on cloud and designs a compact and specific item embedding for each group, which not only enhances the personalization but also reduces the space required by embedding. Another method named DIET[10] aims at discovering the most compatible parameters for device-specific distribution within a random network to quickly adapt to local changes. Moreover, to keep pace with the evolving interests of users, recent studies[31, 41] have introduced methods to detect changes on the device and request new models from cloud [10, 27, 51] when necessary. Nevertheless, current methods mainly focus on parameter adaptation while potentially neglecting the potential of architectures, which is primarily emphasized in our paper."}, {"title": "3 METHOD", "content": ""}, {"title": "3.1 Preliminary", "content": "In recommender system, consider an item set $I = \\{i_1, i_2, ..., i_n\\}$ and device(user) set $D = \\{d_1, d_2, ..., d_m\\}$, where $n$ and $m$ represent the number of items and devices. Cloud has the capability to access the historical interactions of each device, denoted as $D = \\{(x, y)\\}_{d=1,...,m,t=1,...T_d}$, where $T_d$ is the length of the historical interactions of device $d$ and $x = \\{y_1^t, ...y_{T_d-1}^t\\}$ denotes all historical interactions before $T_d$. The objective of recommendation is to train a powerful model $M$ using all the data on cloud, which can be formulated as:\n$\\min \\sum_{d=1}^{d=m} \\sum_{t=1}^{t=T_d} F(y_t, y_t^{t-1}; \\theta)$,\ns.t. $\\min P + F$.\nHere, $P$ and $F$ here refer to the number of parameters and FLOPs for inference of $M$. Besides, each device continue to generate real-time interaction data $X_t = \\{y_1^t, ..., y_{T_d-1}^t\\}$, which may not have appeared on cloud before. Ideally, $M$ should have good generalization ability and show adequate adaptation on these emerging data.\nIn on-device recommendation, we follow those widely-used paradigms [10, 26, 31, 41] from recent works. At the start of each session, or when user interests change significantly, a series of candidate item embeddings and the updated model are simultaneously sent to devices for local reranking. These embeddings are then cached on devices in Figure 3(b). As the recommended items for the user to click are chosen from these candidate items during this session, there is no requirement for any cloud-side involvement during predictions unless local interests change dramatically."}, {"title": "3.2 Structure Controller for Specific Networks", "content": "The use of stacked blocks has become prevalent in recent architectures [20, 44], significantly advancing the development of recommender systems. However, those numerous blocks also pose a challenge for on-device recommendation, particularly for resource-sensitive devices where a larger network leads to longer computation time. Additionally, complex structures can potentially result in overfitting, especially on devices with limited data. Hence, it is crucial to precisely allocate appropriate blocks for each device to ensure efficiency and reliability. A straight approach is to determine where to stop and discard the latter blocks. However, we find that those latter layers might also be crucial for stable prediction as they are closer to the final classifier. In contrast, we tend to predict the presence of each block individually to reserve those later but important blocks. Specifically, we seek a distribution variable $v \\in R^{L\\times2}$ for each network, where L is the number of blocks(residual block or transformer block) in a given network. With this variable, for each block $F_l$ with $1 \\leq l \\leq L$, we assemble it if $v_{l,0} \\geq v_{l,1}$ otherwise we would skip it. Formally,\n$h_{l+1} = F_l(h_l) * I_{l,0} + h_l * I_{l,1}$,\nwhere $h_l$ and $h_{l+1}$ are the input and output of block $F_l$. Variable $I \\in \\{0, 1\\}^{L\\times2}$ in Equation 2 indicates whether each block $F_l$ is supposed to be executed and its value is determined by $v$, where for $k = \\{0, 1\\}$:\n$I_{l,k} = \\begin{cases}\n1 & \\text{if } \\operatorname{argmax}\\{v_{l,0}, v_{l,1}\\} = k \\\\\n0 & \\text{otherwise}\n\\end{cases}$\nObviously the gradient of $h_{l+1}$ cannot propagate back to vector $v$ due to the non-differential function argmax in Equation 3. This inability to optimize through gradient backpropagation leads to the model consistently choosing a single path for parameter optimization, thus failing to achieve the intended purpose of heterogeneous structures. To overcome this, the Gumbel-Softmax straight-through estimator [18] is employed to address the non-differentiability of argmax. For random variable $G_{i,j} = -\\log(-\\log(U_{i,j}))$ sampled from a Gumbel distribution, where $U_{i,j}$ is sampled from a uniform distribution $U(0, 1)$, it is used to reparameterize $v$ to obtain the"}, {"title": "3.3 Structural Parameters to Alleviate Conflicts", "content": "Despite the promising approach we developed, there are still underlying challenges when training the structure controller and the original parameters simultaneously. Illustrated in Figure 2, different interactions tend to exhibit varying interests, which consequently requires different network configurations. The optimization of heterogeneous configurations thus introduces another critical problem, the co-adaptation of them with the shared parameters prevents each subnetwork achieves the optimal performance. In Figure 2, the unshared blocks between the two sequences(e.g., the orange"}, {"title": "3.4 Compact Constraint and Loss Function", "content": "The structural parameterized networks primarily focus on recommendation performance but may overlook the resource constraints of mobile devices. For instance, two sub-structures might achieve comparable performance for a specific user. As the method above does not consider efficiency, it could mistakenly select substructures with longer inference time. Towards this end, we introduce another compact constraint $L_c$ with coefficient $\\lambda$ to encourage a lower probability of a block being executed:\n$L_c = \\sum_{k=1}^{k=L} -\\log a_{k,1}$,\nwhere\n$a_{k.1} = \\frac{e^{\\beta_{k,1}}}{e^{\\beta_{k,0}} + e^{\\beta_{k,1}}}$.\nIn addition to reducing model capacity and decreasing on-device response time, $L_c$ can assist Forward-OFA in identifying the most effective blocks(increasing their scores) and discarding less effective blocks, thus enhancing the performance of the framework.\nFinally, the overall loss function $L$ is defined as:\n$L = L_{rec} + L_c$,\nwhere $L_{rec}$ denotes the cross-entropy loss defined in Equation 1 between the prediction and the ground truth(next-clicked item). A detailed pseudocode is used in the Appendix B to clarify the workflow of Forward-OFA better."}, {"title": "4 EXPERIMENT", "content": "In this section, we conduct various experiments on two widely used sequential recommenders using four real-world datasets, aiming to address the following research questions:\n\u2022 RQ1: How does Forward-OFA perform compared to other on-device recommendation methods in terms of resource consumption and recommendation?\n\u2022 RQ2: How do the proposed modules of Forward-OFA and different hyperparameters affect the final performance?\n\u2022 RQ3: Do the additional modules for adaptive network construction in Forward-OFA impose significant burdens on devices?\n\u2022 RQ4: What impact does an adaptive network have on different user distributions?"}, {"title": "4.1 Experimental Setup", "content": ""}, {"title": "4.2 Overall Analysis(RQ1)", "content": "To demonstrate the potential of Forward-OFA, we conduct experiments on four real-world benchmarks, comparing them to a range of baselines. The results are shown in Table 1. From these results, we can conclude that although deploying models on devices reduces the frequency of transmission when requesting, DeviceRec demands the same number of floating-point operations for inference, which is not practical for most devices with limited computing resources. Additionally, consistent networks for all devices may produce ambiguous recommendations for some users compared to those with adaptive networks, owing to the distinct distribution between them and cloud. It also requires enormous bandwidth to request the latest models from cloud in Figure 1(a).\nSTTD and Rare Gem aim to compress the model parameters to reduce the transmission cost during the cloud-coordinated model updating process. However, neither of them effectively mitigates the overhead caused by on-device inference. In fact, STTD even increases the computational cost due to the additional computation required for the semi-tensor product's matrix multiplication. Additionally, the random distribution of 0s in the sparse matrix obtained by Rare Gem still necessitates calculations once there are non-zero elements in each layer. Gater speeds up local inference but fails to achieve a satisfactory compression ratio without ensuring comparable performance. Furthermore, these methods still exhibit degradation on certain datasets, notably in Movielens-10M of NextItNet, where Rare Gem reduced the NDCG and Hit by 16%.\nFinetune leverages limited interactions on devices to adjust the parameters. However, as depicted in Table 1, it does not yield provide satisfying results compared to DeviceRec. We attribute this phenomenon to the overfitting problem for devices with few interactions and changing interest between training and test data. Another personalized method DUET achieves relatively large improvements in both NDCG and Hit. Personalized parameters for each user can effectively capture the latent interests of each user in the behavior sequence. Nevertheless, as mentioned in Section 1, the consistent model structures in DUET may not be effective enough to serve some users. The same inference overhead as the original model also limits its practical application."}, {"title": "4.3 In-depth Analysis(RQ2)", "content": ""}, {"title": "5 CONCLUSION", "content": "In this paper, we propose an efficient framework, Forward-OFA to construct local adaptive networks in only one forward pass. The compatible model provides better performance and lowers computational costs, as only a small fraction of blocks are necessary and beneficial for each device. Extensive experiments and comprehensive analysis of various real-world datasets and widely used sequential recommenders demonstrate the feasibility and effectiveness of the Forward-OFA. Furthermore, this work can be viewed as an initiative to explore the possibility of searching for compatible and lightweight networks for specific tasks."}, {"title": "4.4 Complexity and Privacy Analysis(RQ3)", "content": "As Forward-OFA takes user historical embeddings as input, uploading the sequence or embeddings to the cloud can lead to privacy issues by potentially leaking the device's privacy issues. In contrast, we address the privacy problem by placing the structure generator (a GRU and a fully connected layer) and the sequence extractor of the structural mapper(a GRU) on device. At the beginning of each session or when user interests dramatically change [31, 41], device would extract its own interest, select network paths itself and send them to cloud, which can protect user privacy because cloud can only get the sequence features extracted by devices. Other modules will be saved on cloud to prevent much burden for devices. Besides, as mentioned in Section 3.1 the candidate embeddings of this session will be stored in the local cache, there is no need to communicate with cloud in this session anymore.\nFor the complexity, we show the parameters and FLOPs of each component in Table 5. Forward-OFA only adds a small fraction of parameters(0.012\u00d7 of SASRec and 0.005\u00d7 of NextItNet), which is tolerable for resource-sensitive devices as once deployed, these modules won't be updated from the cloud frequently like parameters. Moreover, the extra modules on device take much less time to do inference than the original model. The inference of Forward-OFA will only happen when interests on device change dramatically or at the beginning of each session, occasional lightweight inference prevents it from occupying a large number of the device's resources for a long time. As for modules on cloud, even if they consume more parameters, they are shared among all devices and can be executed in parallel. Additionally, abundant computing resources on cloud make it efficient to hold these modules."}, {"title": "4.5 Case Study(RQ4)", "content": "To better understand the importance of adaptive networks for recommendation, we present the case study results from the Movielens-10M dataset. Specifically, we sample four individual interactions and generate the corresponding networks through Forward-OFA as shown in Figure 6. The adaptive path(structure) is presented in Figure 6 (b), where we can observe that different user interests correspond to different paths. Apart from this, some sequences(1, 3, 4) only require part of the blocks to finish inference, while sequence 2 needs all blocks. Without compatible networks, the final recommendation list will be a deviation from expected results."}, {"title": "A Experimental Setup", "content": ""}, {"title": "A.1 Datasets", "content": "We evaluate our framework on four public benchmarks in recommender system[14, 22] Movielens-1M\u00b9, Movielens-10M\u00b2, Amazon-Food, and Amazon-Game\u00b3. The statistics of which are presented in table 6. Consistent with prior research, all user-item pairs with positive ratings in the dataset are considered positive samples. Owing to variations in sparsity across the datasets, we exclude users and items with fewer than 20 interactions to provide reliable results.\nTo construct sequential scenarios of user interaction in practical applications, we order each user's interactions by their interaction time. The last interaction of each user will be used for test and others will be used for training."}, {"title": "A.2 Baseline", "content": "Base Models. Given that current sequential recommenders predominantly consist of Transformer and CNN, we choose SASRec[20] and NextItNet[44] as the base model, upon which we incorporate different methods:\n\u2022 SASRec[20] integrates a self-attention mechanism [37] into recommender system, thereby enhancing the capture of a user's interests by considering the individual impact of each item in the historical sequence on the target item.\n\u2022 NextItNet[44] employs a convolutional neural network to introduce local perception and parameter sharing. It stands as the pioneering work to utilize residual learning in recommender system, effectively modeling long-range and intricate dependencies within historical sequences."}, {"title": "C Association between Neural Architecture Search and Forward-OFA", "content": "Smilarity. Both NAS (Neural Architecture Search) and Forward-OFA aim to identify suitable networks for devices that adapt to specific data distributions while minimizing resource consumption. They both address an often overlooked research problem: the fundamental significance of network structures to various data distributions. Forward-OFA is also partially motivated by those methods to detect light subnetworks for each device.\nDifference. However, NAS-based methods [4, 50, 53] have to search appropriate structures in advance and retrain local data to get device-specific networks. The whole subnet-constructing process takes a long time, making it impractical for billions of users. For example, the recent method LitePred[8] matches and finetunes models with device data, requiring substantial resources and leading to longer responses. Besides, limited on-device interactions may cause overfitting or suboptimal performance. In contrast, Forward-OFA achieves adaptation through a single forward pass, directly mapping local interests to networks. We also conducted an additional comparable experiment on LitePred, where the results with ours are shown in Table 7. Owing to the accurate matching process, LightPred successfully outperforms DeviceRec and Finetune, demonstrating the necessity of discovering valuable subnetworks. However, as mentioned above, limited on-device interactions and various latent device interests restrict its potential to learn compatible parameters, leading to a large performance drop compared to Forward-OFA which directly builds networks from real-time interactions."}, {"title": "D Adaptation Time to Changing Interests", "content": "To build insights into the efficient adaptation of Forward-OFA, in this section, we compare both the adaptation time once interests shift and the communication time for the model update. In this section, we will further validate the efficiency of Forward-OFA in quickly adapting to changing user interests. As shown in Table 8, we found that on-cloud fine-tuning on an Nvidia RTX 3090 GPU(35.58 TFLOPS) takes 1000 times longer than Forward-OFA. For mobile devices like the iPhone 16(1789.4 GFLOPS), on-device fine-tuning can exceed 94 seconds, which is about 10,000 times slower than Forward-OFA."}]}