{"title": "Latent Diffusion Models for Controllable RNA Sequence Generation", "authors": ["Kaixuan Huang", "Yukang Yang", "Kaidi Fu", "Yanyi Chu", "Le Cong", "Mengdi Wang"], "abstract": "This paper presents RNAdiffusion, a latent diffusion model for generating and optimizing discrete RNA sequences. RNA is a particularly dynamic and versatile molecule in biological processes. RNA sequences exhibit high variability and diversity, characterized by their variable lengths, flexible three-dimensional structures, and diverse functions. We utilize pretrained BERT-type models to encode raw RNAs into token-level biologically meaningful representations. A Q-Former is employed to compress these representations into a fixed-length set of latent vectors, with an autoregressive decoder trained to reconstruct RNA sequences from these latent variables. We then develop a continuous diffusion model within this latent space. To enable optimization, we train reward networks to estimate functional properties of RNA from the latent variables. We employ gradient-based guidance during the backward diffusion process, aiming to generate RNA sequences that are optimized for higher rewards. Empirical experiments confirm that RNAdiffusion generates non-coding RNAs that align with natural distributions across various biological indicators. We fine-tuned the diffusion model on untranslated regions (UTRs) of mRNA and optimize sample sequences for protein translation efficiencies. Our guided diffusion model effectively generates diverse UTR sequences with high Mean Ribosome Loading (MRL) and Translation Efficiency (TE), surpassing baselines. These results hold promise for studies on RNA sequence-function relationships, protein synthesis, and enhancing therapeutic RNA design.", "sections": [{"title": "1 Introduction", "content": "Diffusion models demonstrate exceptional performances in modelling continuous data, with applications in images synthesis [100, 102, 16], point clouds generation [93], video synthesis [58], reinforcement learning [3, 62, 79], time series [115] and molecule structure generation [126]. An important advantage of diffusion models is that their generation process can be \"controlled\" to achieve specific objectives via incorporating additional guidance signal. The guidance can steer the backward process toward generating samples with desired properties, without additional training [34, 17, 31].\nBeyond continuous domains, people have explored using diffusion models to generate discrete sequences to inherit the benefits of controllability through guidance, both in the textual domain [14, 77] and in the biology domain [37, 126, 88, 45]. We note that in the domain of text generation, state-of-the-art performances are achieved by scaling up auto-regressive models [2, 10, 116, 118, 119]. However, for generating biological sequences such as proteins, DNAs, and RNAs, language models have to be adapted to handle their specific sequence characteristics [101, 65, 94, 19, 63, 22]."}, {"title": "2 Methodology", "content": "RNAdiffusion has three parts: (1) a sequence auto-encoder and decoder that map between the space of variable-length discrete sequence and a continuous latent space, (2) a continuous diffusion model trained to model the distributions in the latent space, with optionally added guidance, and (3) a reward model trained on top of the latent space for computing guidance. See Figure 1 for an overview of our methodology."}, {"title": "2.1 Sequence Auto-Encoder", "content": "We aim for a sequence auto-encoder that removes the information redundancy in the raw sequence of tokens and compress them into a fixed-length set of continuous embedding vectors. We propose to use a sequence auto-encoder that consists of a pretrained encoder, a Querying Transformer (Q-Former) [76], and a decoder (Figure 1 left panel).\nEncoder. We adopt the pretrained RNA-FM [26], a BERT-type encoder-only model, as the first component of the sequence auto-encoder [33]. BERT-type encoders are pretrained via masked language modeling task to predict randomly masked tokens. We denote the encoder by $\\text{Enc}$. It maps a sequence $x = (x_1, x_2,...,x_L)$ to a sequence of contextualized token embeddings of the same length $L$. We fix the encoder during the training of the sequence auto-encoder.\nQuerying-Transformer. Next we use a Q-Former, denoted by $\\text{QFormer}_{\\psi}(\\cdot)$, to summarize the embeddings of the pre-trained encoder into K latent vectors $z = (z_1, z_2, . . ., z_K)$, where K is a fixed number independent of L. We aim to achieve three goals: (1) To retains enough information in latent vectors, making it easy to train a decoder for raw sequence reconstruction; (2) To use latent vectors later for predicting RNA functional properties, via training additional prediction models on top of these embedding; (3) To reduce the dimensionality of raw sequences, so that diffusion models can focus on learning intrinsic structure of latent RNA vectors.\nThe Querying Transformer (Q-Former) [76] takes K trainable query token embeddings $(q^1, ..., q^K)$ as input. The query tokens go through the transformer with cross-attentions to the embedding sequence given by the encoder and progressively summarize the original token embeddings of varying lengths into a fixed-size sequence of embedding vectors. This is a flexible approach compared to using only the <cls> token embedding [75]. As when we increase K, we retain more information in the latent vectors and allow accurate reconstruction, which helps to mitigate the KL-vanishing problem [21]."}, {"title": "Decoder", "content": "We use a standard causal Transformer [124] as the decoder, denoted by $\\text{Dec}_{\\varphi}$. We build a simple linear projection layer to convert the K latent vectors into a soft prompt embeddings of equal size for the decoder to condition on. Then the decoder autoregressively reconstructs the original input sequence given the latent vectors."}, {"title": "Traning Objective", "content": "With the pre-trained encoder fixed, we train the Q-Former and the decoder from scratch in an end-to-end manner. The training objective is to reconstruct the original sequence from the embeddings given by the fixed pretrained encoder model. Our loss function is given by\n$\\mathcal{L}_{\\text{reconstruct}}(\\psi, \\varphi; x) = - \\log p_{\\text{Dec}_{\\varphi}}(x_1, x_2,...,x_L | \\text{QFormer}_{\\psi}(\\text{Enc}(x)))$,\nwhere $\\psi, \\varphi$ denote the weights of the QFormer (including the query tokens q) and Decoder, respectively. As people often use a very small (e.g. 1e-6) KL-regularization term in latent diffusion models [102, 135], we choose to not include any KL-regularization, simplifying the design and helping to avoid KL-vanishing [21]. Empirically, the training works well without the KL term."}, {"title": "2.2 Latent Diffusion Model", "content": "We build a classical continuous diffusion model [57] to model the distribution of the latent variables z, which generates samples from the target distribution by a series of noise removal processes. Specifically, we adopt a denoising network to predict the added noise $\\epsilon$ from the noisy input $(\\sqrt{\\bar{\\alpha}_t}z_0+\\sqrt{1 - \\bar{\\alpha}_t}\\epsilon)$, where t is a time step of the forward process, $\\bar{\\alpha}_t$ is the corresponding signal strength level, and $\\epsilon$ is the added Gaussian noise.\nFor the denoising score network $\\text{Denoiser}_{\\theta}$, we use transformer denoising networks following [77]. We use the $\\epsilon$-prediction scheme in Denoising Diffusion Probabilistic Models (DDPM) [57] and adopt the simplified training objective given by\n$\\mathcal{L}(\\theta) = \\mathbb{E}_{z_0 = \\text{QFormer}_{\\psi}(\\text{Enc}(x))}\\mathbb{E}_{t,\\epsilon}||\\epsilon - \\text{Denoiser}_{\\theta}(\\sqrt{\\bar{\\alpha}_t}z_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon, t)||^2$,\nIn our framework, the latent embedding have fixed sizes, making it easy to train diffusion models and treating data as if they are continuous like images. Unlike other discrete diffusion models, our framework does not require truncating/padding sequences to a fixed length compared with [77, 14], and it does not require training an additional length predictor as done by [105, 64, 130]. Our auto-regressive decoder has been trained to generate sequences with variable lengths."}, {"title": "2.3 Guided Generation in the Latent Space", "content": "To generate RNA sequences of desired properties, we need a reward model for predicting the property of new sequences. Suppose we have a labeled dataset $\\{(x,r)\\}$, where r is the measured reward of interest, e.g., the translation efficiency. We train a reward model $R_{\\xi}$ on top of the latent space to predict r given the input sequence x. The training objective is\n$\\mathcal{L}(\\xi) = \\mathbb{E}_{(x,r)} \\ell(R_{\\xi}(z),r)$,\nwhere $\\ell$ is a loss function, $z = \\text{QFormer}_{\\psi}(\\text{Enc}(x))$ is the embedding obtained from Q-Transformer, and $\\xi$ are weights of the reward model.\nFinally, we use the reward model to compute a gradient-based guidance (e.g., via universal guidance [17]) and add it to the backward generation process. This guidance steers the random paths of the backward generation process of the diffusion model toward a higher reward region in a plug-and-play manner. Specifically, at the step t of the backward diffusion process, given the current noisy sample $z_t$, we first estimate the clean sample\n$z_0 = z_0(z_t) = \\frac{z_t - \\sqrt{1 - \\bar{\\alpha}_t}\\text{Denoiser}_{\\theta}(z_t, t)}{\\sqrt{\\bar{\\alpha}_t}}$\nAnd we use $R_{\\xi}(z_0)$ as a surrogate reward for the noisy sample $z_t$. We plug in the gradient $\\nabla_{z_t}\\ell(r^*, R_{\\xi}(z_0))$ into the current predicted noise as follows, with $r^*$ being the targeted reward and $\\lambda$ being the guidance strength.\n$\\text{predicted noise} = \\text{Denoiser}_{\\theta}(z_t, t) + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\lambda \\cdot \\nabla_{z_t}\\ell(r^*, R_{\\xi}(z_0))$.\nWe note that controlling the generation for decoder-only models such as GPTs in the raw sequence space is generally hard without fine-tuning the model (see [70] and the references therein)."}, {"title": "3 RNA Sequence AutoEncoder", "content": null}, {"title": "3.1 Experiment Setup", "content": "Models. We adopt the pretrained RNA-FM [26] as the encoder. RNA-FM is an encoder-only model trained with masked language modelling on non-coding RNAs (ncRNAs), which can extract biologically meaningful embeddings from ncRNAs and can be adapted for various downstream tasks, such as UTR function prediction, RNA-protein interaction prediction, secondary structure prediction, and etc. For the Q-Former, we modify the ESM-2 model architecture [82] by adding K trainable query token embeddings and adding cross-attention layers in each transformer block that attends to the embeddings from the RNA-FM. For the decoder model, we adopt the same model architecture of ProGen2-small [94] with our customized tokenizers for RNAs."}, {"title": "Dataset", "content": "We pretrain our RNA sequence Auto-encoder with the ncRNA subset of Ensembl database [90] downloaded from RNAcentral [1], which contains collected ncRNA sequences from a variety of vertebrate genomes and model organisms. We select the sequences shorter than 768 bp to avoid exceeding RNA-FM's maximum length limit and exclude those with rare tokens more than {A, C, G, U}, curating a dataset of 1.1M samples for training and testing. Due to the space limit, additional experimental setup can be found in Appendix D.1."}, {"title": "3.2 Results", "content": "We perform a sweep over hyperparameters and obtain a set of RNA Sequence AutoEncoders with high reconstruction capabilities. Specifically, there are two hyperparameters that influence the dimension of the latent space: the number of query tokens K and the hidden dimension D of each latent vector. We tune K among {16, 32} and D among {40, 80, 160, 320}. For each input sequence x of length L, we calculate the length-normalized reconstruction loss (NLL), i.e.,\n$-\\log p_{\\text{Dec}_{\\varphi}}(x | \\text{QFormer}_{\\psi}(\\text{Enc}(x)))$. We also calculate the length-normalized edit distance (NED) between the reconstructed sequence $\\hat{x} \\sim p_{\\text{Dec}_{\\varphi}}(\\cdot | \\text{QFormer}_{\\psi}(\\text{Enc}(x)))$ and the original input sequence x, i.e., $\\text{EditDistance}(x, \\hat{x})$. For each of the two metrics, we report the empirical average on a held-out test set in Table 1. We see that increasing either the number of query tokens K or the hidden dimension D reduces the reconstruction errors and the error rates. Increasing the dimension of the latent space retains more information in the latent embeddings, and hence it is easier to reconstruct the input sequence."}, {"title": "4 Latent Diffusion Model for RNAS", "content": null}, {"title": "4.1 Experimental Setup", "content": "Models and Datasets. We build a 24-layer Transformer model [124] with hidden dimension 2048 as the denoising network for our latent diffusion model. As the latent vectors are continuous, we do not need any additional embedding layer or rounding operation. We use the same Ensembl ncRNA dataset and apply the same preprocessing as our RNA sequence Auto-encoder. We trained the diffusion model for 10 epochs with the Sequence Autoencoder frozen. Full training details are given in Appendix D.2."}, {"title": "Generation Details", "content": "We use the DDIM sampling method [109] with $\\eta = 0$ (deterministic) and 50 denoising steps to get samples from our latent diffusion models. Then we transform the latent vectors into soft prompts for the decoder and auto-regressively generate a sample sequence nucleus-by-nucleus with top-p = 0.95 and temperature 1. We remove any invalid sample sequence containing special tokens in the middle (e.g., <bos>)."}, {"title": "Evaluation Metrics", "content": "To evaluate the performance of our latent diffusion model, we calculate the weighted loss $L_{1:T-1}$ on the test dataset according to Eqn. (12) of [57], which is an upper bound of the negative log-likelihood (NLL). We also visualize RNAs in the latent space by using a t-SNE map [123], and compare with reference natural RNAs and randomly generated sequences, which match the length distribution of natural data. Besides, we feed the generated latent vectors through the decoder to get the RNA sequences and we adopt the following biological metrics following [97, 18].\n\u2022 Minimum Levenshtein Distance. The Levenshtein distance [73] is defined as the minimum number of edits (insertions, deletions, or substitutions) required to transform one sequence into another. For each generated RNA sequence, we calculate the smallest Levenshtein distance to all sequences in a reference dataset of natural RNAs, and normalized the distance by the length of the generated sequence.  By looping over all the generated RNA sequences, we obtain a distribution of minimum normalized Levenshtein distances.\n\u2022 Minimum 4-mer Distance. We calculate the frequencies of 4-mers of each sequence [125]. The 4-mer distance of two sequences is defined as the Euclidean distance between the two frequency vectors. Similarly, for each generated RNA sequence, we calculate the minimum 4-mer distance against a reference natural RNA dataset.  We obtain the distribution of minimum 4-mer distances for the generated RNA sequences.\n\u2022 G/C content [69]. G/C content, defined as the percentage of guanine (G) and cytosine (C), is an essential indicator of molecular stability. A higher G/C content generally implies higher stability due to the stronger hydrogen bonds between G and C compared to A and T.\n\u2022 Minimum Free Energy (MFE) [122]. It represents the minimum energy required for the molecule to maintain its conformation. The MFE is crucial measure of an RNA segment's structural stability, reflecting how tightly the RNA molecule can fold based on its nucleotide composition and sequence arrangement. A lower MFE suggests a more stable and tightly folded RNA structure. We calculate the MFE of generated RNA sequences using the ViennaRNA Package [85].\n\u2022 Minimum Levenshtein Distance of RNA Secondary Structure. RNA secondary structure, which involves local interactions like hairpins and stem-loops, plays a crucial role in the RNA's biological functions [55]. Dot-bracket notation represents these structures by using dots for unpaired nucleotides and matching parentheses for base pairs, simplifying the visualization and analysis of RNA sequences [85]. We calculate the secondary structure of RNA sequences using ViennaRNA Package [85], and then calculate the distribution of the minimum normalized Levenshtein distances of the secondary structures of the generated RNAs to those of natural referential RNAs."}, {"title": "4.2 Results: Samples from RNAdiffusion Resemble Natural ncRNAs", "content": "We report the test NLL of the latent generated distribution in Table 1.\nWe see that increasing the query token length K and the embedding dimension D generally decreases the test NLL per dimension. This is expected from the information-theoretic view, as increasing the dimension of the latent space means that we will need fewer bits per dimension to represent the uncertainty sourced from the distribution of the discrete sequences. Nevertheless, doubling the total dimension of the latent space does not necessarily decrease the NLL by half, as this also allows the auto-encoder to retain more information in the latent space, which increases the entropy of the latent distribution.\nFor subsequent experiments, we choose to use the trained RNA auto-encoders that have average normalized edit distance < 1%. They correspond to hyperparameters (K, D) = (16, 320), (32, 160), and (32, 320). We further select the parameter (32, 160) for generation, which achieves the smallest total NLL, i.e, NLL per dimension multiplied by the dimension of latent space (K \u00d7 D)."}, {"title": "5 Optimizing 5'-UTR Sequences with Guided Diffusion", "content": "The 5' untranslated region (5'-UTR), located at the beginning of mRNA before the coding sequence, plays a crucial role in regulating protein synthesis by impacting mRNA stability, localization, and translation. In this section, we focus on generating 5'-UTR sequences with high protein production levels through guided diffusion.\nWe consider two functional properties of 5'-UTRs that correlate with protein production levels [103, 24, 26, 136, 67]: (1) Mean Ribosome Loading (MRL) measures the number of ribosomes actively translating an mRNA, reflecting its translation efficiency [103]. In molecular biology, techniques like polysome profiling (Ribo-seq) are employed to determine MRL. (2) Translation efficiency (TE) represents the rate at which mRNA is translated into protein [24]. It is determined by dividing the Ribo-seq RPKM [60], which shows ribosomal footprints, by the RNA-seq RPKM that measures the mRNA's abundance in the cell. This calculation helps gauge how efficiently an mRNA is being translated, independent of its expression level."}, {"title": "5.1 Fine-tuning RNAdiffusion for 5'-UTRS", "content": "Although both UTRs and ncRNAs share similarities in their non-coding properties, UTRs contain important functional elements that are irrelevant to ncRNAs. Thus, our pretrained RNAdiffusion could not be directly applied to generate UTR sequences due to the distributional shift between UTRs and ncRNAs. To adapt RNAdiffusion to generate 5'-UTR sequences, we fine-tune the score function of RNAdiffusion using the five-species 5'-UTR dataset from UTR-LM [30], where we elected sequences with lengths less than 768 and get 205K samples. We finetune the diffusion model for 1 epoch on top of the frozen sequence auto-encoder with (K, D) = (32, 160).\nUpon testing the autoencoder on UTR test set, it shows considerable low reconstruction errors (NLL= 0.0001, NED=0.01% \u00b10.31%), confirming that the autoencoder pretrained with ncRNAs could be transferred onto UTRs. To compare the generated sequences of the fine-tuned diffusion model with natural 5'-UTRs, we present the sequence length histograms (Figure 4) and compute the metrics proposed in Section 4.1 (Figure 7 in Appendix D.5). The results demonstrate that they align closely with the natural distribution."}, {"title": "5.2 Latent Reward Modeling for MRL and TE", "content": "Datasets. For the TE prediction task, we use three endogenous human 5'-UTR datasets analyzed by Cao et al. [24], which contain 28,246 sequences and their measured TE. We reserve 10% of the dataset for testing, 10% for validation, and use the remaining 80% for training. For the MRL prediction task, following the length-based held-out testing approach recommended by Sample et al. [103], we train our model on a collection of 83,919 random 5'-UTR sequences ranging from 25 to 100 bp with MRL measurement. We reserve 10% of the data for validation. Subsequently, we test the model on 7,600 human 5'-UTR sequences.\nModel Architecture. The latent reward networks take as input a list of K latent vectors, obtained by the sequence autoencoder. We construct a 6-layer Convolutional Residual Network (ResNet) [53] specifically designed for 1D convolution, applicable to both the MRL and TE prediction tasks. Additional details are deferred to Appendix D.3.\nResults. For each task, we train two reward models on top of the latent spaces from the pretrained RNA Sequence Autoencorder ((K, D) = (32, 160)), each initialized under distinct random seeds. One model is utilized for guided generation in Section 5.3 while the other is employed for validation assessment. We calculate the Spearman and Pearson correlation coefficients between the predicted rewards and the actual rewards (shown in Table 2). Notably, the Spearman R values on the held-out"}, {"title": "5.3 Reward-Guided Diffusion", "content": "We compute gradient-based forward universal guidance [17] in the latent space with the reward models trained in Section 5.2 to shift the latent backward sample paths towards generating high MRLs/TEs sequences; see Section 2.3. We choose the loss function $\\ell$ to be $L2$ loss and vary the target reward $r^*$ and the guidance strength $\\lambda$. We take the latent reward models trained with one random seed for TE and MRL optimization. For cross-validation, we also employ the reward models trained with the other random seed as the oracle to evaluate the rewards of generated sequence.\nResults. We tune the guidance by varying the value of guidance strengths $\\lambda$ and target reward values $r^*$. Performances of output from our guided diffusion are shown in Figure 5. At a mild guidance strength, increasing the target TE or MRL values generally brings the reward improvements of generated sequences, demonstrating the efficacy of our reward-guided generation approach. Among all the guidance settings, The best validation TE (1.84\u00b10.82) is achieved with the moderate guidance strength as 800 and the largest log-target value as 5, showing 166.7% improvement compared to the validation TE of unguided generation (0.69 \u00b1 0.22). Under the MRL-guidance setting of $\\lambda = 800$ and $\\log r^* = 4$, mean validation MRL of the generated sequences increases from 3.92 \u00b1 1.31 to 5.98\u00b10.90 with maximum relative increase of 52.6%.\nGuidance Trade-offs. For higher guidance strength, the improvements can be artificially amplified when evaluated with the same reward model used for guidance. Moreover, setting $\\lambda$ and $r^*$ too large will eventually hurt the cross-validation performance, since the generation process is over-adapted towards the imperfect guidance reward model, which is a general phenomenon known as reward hacking [39, 32, 134].\nTo get insights into the phenomenon, we compare the generated distributions under different guidance strengths in terms of biological metrics (see Figure 9). With the $\\lambda$ and $r^*$ increasing, the disparities in G/C content (lower mean) and Minimum Free Energy (higher mean) between the generated sequences and natural sequences become larger, which might be attributed to the distribution shift of the gener-ated sequences. Adopting moderate $\\lambda$'s and $r^*$'s would be important for reward-guided generation in terms of the trade-off between achieving higher reward values and maintaining generation qualities.\nMore concretely, we combine both reward evaluation and Minimum Free Energy (MFE) calculation into Pareto front curves (see Figure 8) and compare with two competitive baselines (see Section D.5 for details). Our gradient-guided latent diffusion model achieves higher rewards with the same MFE level compared to Random [103] and UTRGAN [18]. Although the sequences optimized by UTRGAN could achieve high rewards, they consistently exhibit high free energy. This demonstrates that our method has a better reward-structural stability trade-off compared to other baselines."}, {"title": "6 Related Work", "content": "People have been exploring building D3PM-type [14] discrete diffusion models and Diffusion-LM type [77] for proteins sequences and 3D structures [72, 127, 40, 6, 80, 121, 126, 131, 61, 45, 4, 88] and DNAs [15, 78, 37]. To the best of our knowledge, we are the first work to study building diffusion models for RNA sequences. Besides, latent diffusion models for text sequences have been studied"}, {"title": "7 Conclusion", "content": "Our work presents the first application of latent diffusion models on RNA generation. Our pipeline involves a novel adaptation of the Querying Transformer to handle the inherently discrete, variable-length characteristics of RNA sequences. Through the incorporation of the gradient guidance of the latent reward models, our latent diffusion model can generate novel 5'-UTRs with 166.7% higher mean TEs and 52.6% higher mean MRLs. These results provide a biologically meaningful demonstration for RNA generation, a less explored yet exciting area of research."}, {"title": "A Limitations", "content": "The performance of our latent diffusion models depends on the total dimension of the latent space K \u00d7 D, where K is the number of query embeddings and D is the embedding dimension for each query. Increasing K and D makes the reconstruction task and the reward prediction tasks easier, but it allows more redundant information in the latent space, which may hinder the training of the diffusion model. Therefore, we require a hyperparameter search to find the best-performing K and D. To perform reward-guided diffusion, we require a trained differentiable reward network, whose performance crucially depends on the number and the quality of the labeled data. To extend to non-differentiable rewards, additional fine-tuning with reinforcement learning methods [20] is still required. Besides, we did not synthesize the generated 5'-UTRs in wet lab experiments to test the performance, which we leave as future work."}, {"title": "B Broader Impacts", "content": "Our work aims to generate 5'-UTRs with optimized protein production levels, which could significantly enhance the understanding and engineering of gene expression. This can lead to breakthroughs in developing new therapies for diseases such as cancer and genetic disorders. It can also improve synthetic biology systems that produce valuable compounds, such as pharmaceuticals and bio-fuels, more efficiently and sustainably. Nevertheless, the capability to alter gene expression and produce biological molecules could be misused to create harmful biological agents or cause unintended harm to ecosystems."}, {"title": "C Additional Related Work", "content": null}, {"title": "C.1 Continuous Diffusion Models", "content": "A majority of works on diffusion models focus on modelling continuous data [57, 109, 100, 102, 16, 93, 58, 3, 62, 79, 115, 126]. During training, continuous Gaussian noises are sequentially injected to the sample. The core of diffusion models is a denoising network that takes in a Gaussian-noise-perturbed sample and predicts the clean version of the sample. Backed by the theory of forward/backward SDEs [110, 96, 71, 28, 27, 9, 51], diffusion models are guaranteed to recover the true data distribution.\nThe behavior of diffusion models can be controlled through guidance, where an additional guidance term is added into each step of the backward diffusion process to drive the sample toward the regions with desired properties [34, 17, 31, 43, 47, 132]. Classifier-based guidance [34] and its extentions utilize a separate classifier or a reward model to calculate the guidance term, while classifier-free guidance [56] calculates the guidance term based on the conditional diffsuion model itself. More complated controls can be done via finetuning the diffusion models with additional networks [133]."}, {"title": "C.2 Diffusion Models for discrete variables", "content": "The essential components of diffusion models are the forward process, which defines how a clean sample is sequentially corrupted into a pure random noise, and the corresponding backward process, which involves training a denoising network to learn the score function to revert the forward process and sequentially recover the input.\nDiffusion on Discrete Domains. To extend diffusion models to discrete domains, the forward/backward proocesses and the score matching objectives need to be redefined [14, 64, 105, 59]. Notably, BERT-type encoder-only models can be naturally modified into diffusion models, where masking & demasking operations serve as the noising/denoising counterparts [14, 54, 130]. Researchers have also explored score matching for discrete variables [91, 114, 23, 86].\nDiffusion on Continuous Embedding Spaces. One straightforward idea to circumvent the dis-creteness is to learn diffusion models on the continuous embedding spaces. The pioneering work Diffusion-LM [77] proposes to jointly learn a token embedding together with the continuous diffusion model and develops the re-parametrization method and the clamping trick to reduce rounding error induced when projecting the embeddings back to discrete tokens. Afterwards, various work have"}, {"title": "C.3 Latent Diffusion Model for Sequences", "content": "Latent diffusion models for sequences are essentially sequence variational auto-encoders with dif-fusion latent priors, which contain a chain of conditional Guassian distributions, rather than a simple Gaussian distribution. The difference between latent diffusion models and the Diffusion-LM scheme [77] (Section C.2) lies in that latent diffusion models incorporate complex sequence (variational) auto-encoder models while Diffusion-LM only utilizes a token embedding layer.\nSequence Variational Auto-encoders. The design of sequence VAEs can be traced back to the era of RNNs [21, 68, 52]. Afterwards, OPTIMUS [75] is a sequence VAE that connects pretrained BERT and GPT-2, where the authors only use the  token embedding of BERT to extract the latent vector. Similarly, Park and Lee [98] finetune a pretrained T5 model ino a sequence VAE and they use mean/max pooling over all the embeddings to obtain the latent.\nA notorious challenge for sequence VAEs (with Gaussian priors) is the KL vanishing problem [21], where the encoder produces nearly identical latents for all the inputs (a.k.a. posterior-collaspe) and the decoder completely ignores the latents. The intuitive reason behind the KL vanishing problem is that the KL-regularization is too strong and the encoder fails to extract useful information for the decoder to reconstruct the sequence. Several tricks are proposed to address the issues, such as anealling the KL-regularization coefficient, memory dropout, etc [75, 129].\nLatent Diffusion Model for Sequences. People have been building latent diffusion model for sequences, such as Versatile Diffusion [128], LD4LG [87], Planner [135]. With diffusion priors, the KL vanishing problem can be reduced by using a tiny or zero KL-regularization, as the diffusion models are strong enough for modeling complex multimodal distributions [102]. Versatile Diffu-sion [128] trains a textual diffusion model on top of the latent space of OPTIMUS, with a focus on the multimodality setting, e.g. image captions.\nAddressing the similarities and differences of Planner [135].\nPlanner [135] proposes to use the first K token embeddings from the BERT model as the latent variable to increase the information within the latent embedding, but the BERT model needs to be fine-tuned. In comparsion, we keep the pretrained encoder model fixed to avoid the deterioration of the encoder, e.g., the encoder may be finetuned to forget the semantics, as the reconstruction task only requires the sequence auto-encoder to reproduce the raw input sequence. We train a Q-Former [76] to better summarize the entire embedding sequence through cross-attention. Our experiment on MRL/TE predictions also empirically verifies that the latent vectors contain biologically meaningful information.\nAddressing the similarities and differences of LD4LG [87].\nIn methodology, the most similar work to us is LD4LG [87]. Specifically, LD4LG uses a pretrained encoder-decoder model such as BART [74] or T5 [99]. Similarly the encoder is fixed and a Perceiver Resampler [5] is trained to summarize the embeddings. Perceiver Resampler and Q-Former are essentially the same in this setting.\nHowever, LD4LG incorporates an additional transformer Reconstruction Network, which transforms the latent embeddings for the fixed pretrained decoder to cross-attend to. Instead, we perform a simple linear transform to the latent vectors and use them as the soft prefix for the decoder. We initialize the decoder from stratch and train the decoder together with the Q-Former, so the decoder focuses only on translating the latent variables back to discrete sequences. Our design choice reduces the computation for the pipeline and also does not require a paired pretrained encoder-decoder model to work. Additionally, LD4LG solely focuses on NLP tasks such as summarization and translation. Our work focuses on RNA sequence modeling. We further show that our latent embeddings can"}, {"title": "C.4 Generaitve Models for Biological Sequences", "content": "Generative models have achieved considerable success in designing biological sequences [48"}]}