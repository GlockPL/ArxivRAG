{"title": "CLIP Unreasonable Potential in Single-Shot Face Recognition", "authors": ["Nhan T. Luu"], "abstract": "Face recognition is a core task in computer vision, designed to identify and authenticate individuals by analyzing facial patterns and features. This field intersects with artificial intelligence, image processing, and machine learning, with applications in security, authentication, and personalization. Traditional approaches in facial recognition focus on capturing facial features like the eyes, nose, and mouth and matching these against a database to verify identities. However, challenges such as high false-positive rates have persisted, often due to the similarity among individuals' facial features. Recently, Contrastive Language-Image Pretraining (CLIP), a model developed by OpenAI, has shown promising advancements by linking natural language processing with vision tasks, allowing it to generalize across modalities. Using CLIP's vision-language correspondence and single-shot finetuning, the model can achieve lower false-positive rates upon deployment without the need of mass facial features extraction. This integration demonstrating CLIP's potential to address persistent issues in face recognition model performance without complicating our training paradigm.", "sections": [{"title": "I. INTRODUCTION", "content": "Face recognition [1]\u2013[3] is a pivotal task within the realm of computer vision, wherein algorithms are designed to identify and authenticate individuals by analyzing and comparing patterns in facial features. It's a multifaceted field that intersects with various disciplines like artificial intelligence, image processing and machine learning.\nAt its core, facial recognition involves capturing facial images and videos, extracting unique characteristics such as the arrangement of eyes, nose, and mouth, and then matching these features against a database of known faces to make identifications or verifications. This technology has found widespread applications across diverse sectors, ranging from security and surveillance to authentication and personalization.\nCLIP [4], or Contrastive Language-Image Pretraining, is a ground breaking model developed by OpenAI that transcends traditional boundaries between natural language processing and computer vision. Unlike conventional models that specialize in either text or image modeling, CLIP learns to understand both modalities simultaneously. This means it is possible that CLIP can perform a wide range of tasks across different domains, including those related to facial recognition."}, {"title": "II. BACKGROUND", "content": "One of the big problem distinct facial recognition with other classification task is subjectivity to false-positive result, misunderstanding someone face with others. Upon testing, we realize that by using the vision-language correspondence from CLIP features, model only require a single shot finetuning while reducing false-positive results significantly without any state-of-the-art methods for extracting facial features from massive datasets."}, {"title": "A. Face recognition", "content": "Face recognition has emerged as a powerful tool in computer vision, enabling the identification or verification of individuals based on their facial features. This technology has applications in various fields such as security, law enforcement, and personal device authentication. Early approaches focused on geometrical models and eigenface techniques [5]. With the advent of machine learning, methods such as Local Binary Patterns (LBP) [6] and Fisherfaces [7] improved robustness under various lighting conditions and facial expressions.\nThe rise of deep learning has further revolutionized the field, with convolutional neural networks (CNNs) becoming the backbone of face recognition systems. Groundbreaking models like DeepFace [8] and FaceNet [9] introduced face embeddings that provided remarkable accuracy and efficiency, even in large-scale applications. More recent advancements leverage deep residual networks and novel loss functions to achieve even higher accuracy [10], [11]. These advancements underscore the rapid development of face recognition and its growing importance across technology sectors."}, {"title": "B. CLIP and its applications", "content": "CLIP [4] model has made significant strides in connecting vision and language modalities, enabling zero-shot capabilities across various computer vision tasks [12]\u2013[14]. CLIP [4] leverages a contrastive learning framework, aligning text descriptions and images in a shared embedding space. This alignment allows CLIP [4] to generalize across tasks without the need for task-specific training. CLIP [4] has shown versatility in object detection [15], [16], image generation [17], [18], and scene understanding [19]."}, {"title": "III. DATA ACQUISITION AND PROCESSING", "content": "In this research, we used an image dataset consists of high resolution images of volunteers, contains faces sorted by individual names, ensuring a clear and organized structure for face recognition experiments. Images were captured using a 3 megapixels camera within a 2 meters range to obtain various perspectives of each participant, including views from above, below, left, right, and the front (as shown in Figure 1). These multiple angles simulate real-world scenarios where faces may appear in diverse orientations. Each participant contributed around 30 images, resulting in a dataset of 300 images covering 10 distinct individuals.\nDue to the sensitive nature of this dataset (comprising unique facial features of each participant), only the dataset gathering methodology will be publicized to safeguard the privacy of the individuals involved.\nOnce the images were collected, we processed them using the SCRFD [29] face detection model. This model enabled us to detect and crop out the face region from each image accurately. For further alignment and standardization, we used the model to extract essential facial keypoints, specifically the positions of the eyes, nose, and mouth. These keypoints facilitated the vertical alignment of faces across different images, ensuring consistency in the data representation. The cropped and aligned face images were then quality assured manually by a person before being saved into a separate folder structure, labeled with each participant's name, allowing organized and efficient access for subsequent experimentation and analysis."}, {"title": "IV. EXPERIMENT DESIGN", "content": ""}, {"title": "A. Model training", "content": "After completing the dataset creation and preprocessing stages, we designed a face recognition pipeline that diverges from traditional approaches by replacing the standard facial feature encoder and classifier with CLIP [4] model, specifically the CLIP-RN50 variant (as shown in Figure 2).\nIn this setup, we treated face recognition as an image classification task, using a single-shot finetuning method on the processed images of participants. Instead of tuning the entire model, we froze the image encoder parameters to leverage CLIP's pretrained vision features and performed backpropagation only on the text encoder. Each image was associated with a text prompt formatted as \"This is the image of a person named ...\" which allows the CLIP model to align specific participant identities with their visual features.\nInitially, approaching face recognition as an image classification problem may seem counterintuitive. Conventional approaches that treat face recognition this way often achieve high accuracy in training and validation phases. However, these methods typically suffer from high false-positive rates in real-world applications due to factors such as environmental noise and limited data variation. Unlike these traditional models, CLIP demonstrates robustness against false positives when deployed in real-world settings, likely due to its multimodal design and pretraining on diverse, real-world data.\nLearning from previous researches related to CLIP finetuning [30], [31], the model was then single-shot trained using the"}, {"title": "B. Deployment evaluation", "content": "To evaluate the false positive rate of our face recognition system in real-world scenarios, we conducted a deployment test of the finetuned CLIP [4] model on the same device and camera setup used during the training and data extraction phases. This consistency ensured that any environmental variables or device-specific nuances affecting model performance would closely match those encountered during initial data collection.\nIn this test, we instructed each of the 10 volunteer participants included in the training dataset, as well as 2 additional participants who were not in the dataset, to stand individually in front of the camera for a duration of 5 seconds. Each participant took turns, ensuring no overlap in presence before moving to the next individual. This process allowed us to observe the model's real-time response to both known and unknown faces, assessing its ability to correctly identify or reject individuals."}, {"title": "V. RESULTS", "content": "In addition to the finetuned CLIP [4] model, we employed several well-known face recognition models, including VGG-Face [10] and ArcFace [35], for comparison under multiple settings. These settings included a traditional image recognition configuration (denoted \"c\") and the specific configurations originally described in each respective paper (denoted \"o\"), as detailed in Table I.\nTo ensure consistency in training and comparison, all additional models were trained using the Adam optimizer [36], with a learning rate of $Ir = 1 \\times 10^{-3}$ and parameters $\\beta = (0.9, 0.999)$. Training was conducted over 100 epochs with identical batch sizes, resolution settings, and device configurations to CLIP [4] to maintain comparability across models.\nThe results show that, while the CLIP [4] experienced lower performance during the training phase in comparison to the VGG-Face [10] and ArcFace [35] variants, it demonstrated significantly improved performance during deployment inference. Notably, the finetuned CLIP [4] achieved a markedly lower FPR and a reduced FNR compared to the other models. This enhanced performance was achieved even though CLIP [4] was fine-tuned with a single-shot approach as an image recognition model, without employing any advanced training techniques or traditional feature extraction methods used in face recognition tasks.\nAlthough we observed a significant degradation in performance across multiple variants of VGG-Face [10] and ArcFace [35], this outcome was somewhat expected, as these models were originally designed for large-scale facial feature extraction. When applied to an image recognition scenario or when only a limited set of facial features are used, their performance diminishes drastically, highlighting the models' dependency on detailed facial feature analysis for optimal results.\nThe results highlight CLIP's robustness and effectiveness in real-world deployment scenarios, making it a competitive choice for face recognition tasks with minimal training adjustments."}, {"title": "VI. DISCUSSION", "content": ""}, {"title": "A. Prompting choices", "content": "While there has been considerable research on prompting optimization for CLIP in various computer vision tasks, there is a noticeable lack of studies addressing this specifically within the context of face recognition. As part of our experiments, we explored the effect of different prompt formulations on the CLIP model's performance in face recognition tasks.\nIn addition to the standard prompt \"This is the image of a person named ...\", we tested other variations, such as simply stating \"This is the image of ...\" and directly feeding the name of the person to the text encoder without any additional context. Despite these variations, the fine-tuning results exhibited only minimal fluctuations in performance, with a difference of approximately 1%. This suggests that, in the context of face recognition, the choice of prompt and the inclusion of additional context may not significantly impact the model's performance."}, {"title": "B. Distinctive features between faces", "content": "While CLIP achieves a lower FPR compared to the majority of tested models, its performance remains far from perfect. As observed from the results, although both the FPR and FNR are reduced relative to other tested models, this level of performance is still inadequate for security-sensitive applications of face recognition."}, {"title": "C. Training gradient", "content": "During the hypothesis formulation and experiment design phase, we initially planned to test on a significantly larger number of classes, typical of traditional large-scale face verification problems where the number of classes greatly exceeds the number of images per class. This would have allowed us to explore the scalability and robustness of CLIP in more complex, real-world face recognition tasks.\nHowever, during testing, we encountered a significant limitation. We realized that to handle such a large number of classes, a substantial amount of GPU memory would be required, particularly because the gradients of CLIP's text encoder grow very large as the number of classes increases. Given our limited computing resources, we were unable to scale the problem as initially planned and were forced to downscale the experiment to just 10 classes (equivalent to 10 persons). This reduction in scale highlights a potential challenge when attempting to apply CLIP to large-scale image databases, as the computational demands may become prohibitive without access to high-performance hardware. This issue may need to be addressed in future work to enable the use of CLIP in more expansive and computationally intensive face recognition scenarios."}, {"title": "VII. CONCLUSION", "content": "In this paper, we explored the application of Contrastive Language-Image Pretraining (CLIP) for face recognition, highlighting its potential to address key challenges in the field. Traditional face recognition models typically rely on extracting and matching detailed facial features, but they often struggle with high false-positive rates due to the inherent similarity among individuals' facial structures. By leveraging CLIP's vision-language correspondence and employing a single-shot finetuning approach, we demonstrated that the model can achieve significantly lower false-positive rates during deployment while eliminating the need for complex facial feature extraction techniques.\nOur findings indicate that CLIP, with its ability to generalize across modalities, offers a promising solution to persistent issues in face recognition systems. The model's robustness in real-world deployments, without the need for extensive training data or traditional feature extraction methods, positions it as a powerful alternative to conventional face recognition models. Future work may focus on scaling CLIP to larger datasets and further optimizing its performance for diverse and dynamic real-world applications. Overall, this research contributes to advancing face recognition technology, showcasing CLIP's potential for more efficient and accurate facial authentication systems."}]}