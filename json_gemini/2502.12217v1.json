{"title": "Optimal Brain Iterative Merging: Mitigating Interference in LLM Merging", "authors": ["Zhixiang Wang", "Zhenyu Mao", "Yixuan Qiao", "Yunfang Wu", "Biye Li"], "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities, but their high computational costs pose challenges for customization. Model merging offers a cost-effective alternative, yet existing methods suffer from interference among parameters, leading to performance degradation. In this work, we propose Optimal Brain Iterative Merging (OBIM), a novel method designed to mitigate both intra-model and inter-model interference. OBIM consists of two key components: (1) A saliency measurement mechanism that evaluates parameter importance based on loss changes induced by individual weight alterations, reducing intra-model interference by preserving only high-saliency parameters. (2) A mutually exclusive iterative merging framework, which incrementally integrates models using a binary mask to avoid direct parameter averaging, thereby mitigating inter-model interference. We validate OBIM through experiments on both Supervised Fine-Tuned (SFT) models and post-pretrained checkpoints. The results show that OBIM significantly outperforms existing merging techniques. Overall, OBIM provides an effective and practical solution for enhancing LLM merging. We will publicly release our code upon the acceptance of this paper.", "sections": [{"title": "1 Introduction", "content": "Existing research (Akiba et al., 2025; Dekoninck et al., 2023; Wan et al., 2024a) has demonstrated that a composite LLM can be constructed by merging the parameters of different expert LLMs. Traditional approaches (Wortsman et al., 2022; Matena and Raffel, 2022; Jin et al., 2022) employ matrices to determine task-specific coefficients and perform a weighted average based on these coefficients. Methods grounded in task arithmetic (Ilharco et al., 2023) leverage task vectors, defined as the difference between the parameter values of a fine-tuned model and those of its pre-trained counterpart, to effectively manipulate and integrate the knowledge embedded within the models.\nState-of-the-art model merging methods (Yadav et al., 2024; Wang et al., 2024; Yu et al., 2024b) have shown that task performance degradation is primarily caused by interference between parameter values, as aggregation operations, such as averaging, can alter the parameter distribution (Yu et al., 2024a). The interference can be categorized into two types: intra-model interference and inter-model interference.\nIntra-model interference arises from redundant parameters within a single model. Due to the over-parameterized nature of neural networks (Choudhary et al., 2020; He and Xiao, 2023), removing a significant portion of the parameters often has little impact on model performance (Sun et al., 2023; Kim et al., 2024). However, these redundant parameters introduce noise during the model merging process, adversely affecting the outcome. To address this, it is crucial to identify parameters that are closely related to the target task. Existing approaches, however, primarily rely on magnitude-based methods, assuming that parameter magnitude directly correlates with saliency. For instance, TIES (Yadav et al., 2024) trims the parameters with the smallest magnitudes, the Model Breadcrumbs (Davari and Belilovsky, 2025) highlight the importance of removing the parameters with the largest weights to further reduce noise. While these methods demonstrate effectiveness, they fall short of fully revealing the true saliency of the parameters.\nInterference between models arises due to variations in parameter distributions (Shoemake, 1985; Jang et al., 2024). Directly averaging these parameters can lead to performance degradation. TIES addresses this issue by resolving sign conflicts in parameter values, aligning them based on the direction of the largest total movement across models. Similarly, TALL-Mask (Wang et al., 2024) is designed to exclude parameters that are relevant only to a subset of tasks. While these methods effectively mitigate inter-model interference under certain conditions, their effectiveness diminishes when parameter distributions deviate from expected patterns, causing them to revert to simple averaging. As shown in Figure 1, when there is no sign conflict, TIES yields the same result as simple averaging, deviating from both input models and leading to suboptimal performance.\nTo address the interference problem in model merging, we propose a novel method for LLMs called Optimal Brain Iterative Merging (OBIM). Our approach comprises two core components: a saliency measurement mechanism to filter intra-model interference and a mutually exclusive iterative merging framework to prevent inter-model interference.\nIn detail, our approach measures the saliency of parameters within a single model by evaluating the loss change induced by altering each parameter. Inspired by layer-wise model pruning methods (Frantar and Alistarh, 2022; Frantar et al., 2022), we forgo reliance on the overall model loss during training and instead independently apply the Mean Square Error (MSE) to each linear weight. This enables calculation of the output distribution difference between the trained weight and the original weight, providing a more precise and efficient measure of parameter saliency. By retaining parameters with high saliency, we effectively reduce intra-model interference in model merging.\nSubsequently, we design an iterative merging framework to integrate models step by step in a mutually exclusive manner, mitigating inter-model interference. Specifically, we employ a binary mask to track the positions that have already been merged. At each step, parameters with the highest saliency, which are not yet recorded in the mask, are selected from a model and integrated into the base model. This ensures that each position is occupied by only one parameter, thereby eliminating the need for averaging operations.\nIn summary, we propose a novel method, OBIM, to mitigate both intra-model and inter-model interference in LLM merging. To validate the effectiveness of our method, we conducted model merging experiments on Supervised Fine-Tuned (SFT) models of Llama2 (Touvron et al., 2023) for multitask merging and post-pretrained checkpoints of Qwen2 (Yang et al., 2024a) for catastrophic forgetting recovery. The results of both experiments demonstrate that OBIM significantly outperforms existing approaches. In addition, extensive ablation studies and analyses of key factors provide a comprehensive understanding of OBIM."}, {"title": "2 Preliminaries", "content": "In this paper, we focus on merging models that are optimized from the same backbone. Given K models with parameters ${\\theta^{1}, \\theta^{2},...,\\theta^{K}} \\in \\mathbb{R}^{d}$, each trained on a distinct task or setting ${t_{1}, t_{2},...,t_{k}}$ from a shared base model ${\\theta^{B} \\in \\mathbb{R}^{d}}$. Model merging aims to fuse these parameters into a single model with parameters ${\\theta^{M} \\in \\mathbb{R}^{d}}$, and enable ${\\theta^{M}}$ to effectively handle all K tasks simultaneously."}, {"title": "2.2 Task Vector", "content": "A task vector ${\\delta^{k} \\in \\mathbb{R}^{d}}$ for model ${\\theta^{k}}$ is defined as the delta weights between the trained model's parameters and those of the backbone:\n${\\delta^{k} = \\theta^{k} - \\theta^{B}},$  (1)\nwhich represents both the direction and magnitude of parameter updates during training.\nBy merging the task vectors ${\\{\\delta^{1}, \\delta^{2}, . . ., \\delta^{K} \\}}$ of K models into a single task vector ${\\delta^{M}}$, the parameters of the merged model can be expressed as\n${\\theta^{M} = \\theta^{B} + \\delta^{M}}.$  (2)"}, {"title": "3 Methodology", "content": "The proposed merging method comprises two key components: Optimal Brain Merging (OBM), a saliency-based mechanism that selects high-saliency parameters for merging, and Iterative Merging (IM), an iterative framework designed to mitigate interference between models. Figure 2 illustrates the complete merging process of our method."}, {"title": "3.1 Optimal Brain Merging", "content": "Optimal Brain Damage (OBD) (LeCun et al., 1989) and its subsequent works (Hassibi et al., 1993; Frantar and Alistarh, 2022) aim to establish effective criteria for pruning or quantizing specific weights while minimizing the impact on model performance. The fundamental idea is to leverage the second derivative of the objective function with respect to the parameters to compute their \"saliencies.\" Building upon the idea, we introduce Optimal Brain Merging (OBM) to mitigate intra-model interference by identifying and eliminating negligible delta weights in task vectors.\nGiven a trained LLM with parameters $\\theta$ and task vector $\\delta$, our goal is to identify a subset of parameters in the task vector whose removal results in minimal increase in the objective function $\\mathcal{L}$. The change in the objective function is measured using a Taylor series expansion:\n$\\triangle \\mathcal{L}=\\sum_{i} \\frac{\\partial \\mathcal{L}}{\\partial \\theta_{i}} \\delta_{i}+\\frac{1}{2} \\sum_{i} \\frac{\\partial^{2} \\mathcal{L}}{\\partial \\theta_{i}^{2}} \\delta_{i}^{2}+\\sum_{i \\neq j}\\frac{\\partial^{2} \\mathcal{L}}{\\partial \\theta_{i} \\partial \\theta_{j}}|\\theta_{i} \\theta_{j}\\delta_{i} \\delta_{j}+O(||\\delta||^{3})}.$  (3)\nAssuming that $\\mathcal{L}$ is at a local minimum and that each parameter contributes to $\\triangle\\mathcal{L}$ independently, the first derivative, the off-diagonal terms of the second derivative, and the higher-order terms can be discarded. Consequently, $\\triangle\\mathcal{L}$ can be approximated as:\n$\\triangle \\mathcal{L} \\approx \\frac{1}{2} \\sum_{i} \\frac{\\partial^{2} \\mathcal{L}}{\\partial \\theta_{i}^{2}} \\delta_{i}^{2}.$  (4)\nThe change in $\\triangle\\mathcal{L}$ when removing the parameter at position i indicates how much it affects performance, thereby representing its saliency:\n$S_{i} = \\frac{1}{2} \\frac{\\partial^{2} \\mathcal{L}}{\\partial \\theta_{i}^{2}} = \\frac{1}{2} h_{i i} \\delta_{i}^{2},$  (5)\nwhere ${h_{ii}}$ denotes the i-th diagonal element of the Hessian matrix of the loss for the given model. Parameters with low saliency, which contribute minimally to $\\triangle\\mathcal{L}$, should be removed.\nHowever, computing the Hessian matrix requires a back-propagation process through the LLM if the objective function used during LLM training is considered (Bowen et al., 2024). To avoid the high computational cost comparable to model training, we take inspiration from layer-wise pruning approaches (Frantar and Alistarh, 2022; Frantar et al., 2022) and employ the Mean Squared Error (MSE) as the objective function for each linear layer independently.\nFormally, let $\\mathbf{X}^{l}$ be the input to the l-th layer with the weight matrix $\\mathbf{W}^{l}$. The objective is defined as:\n$\\triangle \\mathcal{L}^{l}=\\left|\\left|\\mathbf{W}^{l} \\mathbf{X}^{l}-\\mathbf{W}^{B} \\mathbf{X}^{l}\\right|\\right|^{2}=\\left|\\left|\\triangle \\mathbf{W}^{l} \\mathbf{X}^{l}\\right|\\right|^{2},$  (6)"}, {"title": "3.2 Iterative Merging", "content": "To address inter-model interference, we propose a merging framework called Iterative Merging (IM). This method iteratively updates a unique, non-overlapping subset of parameters from each task vector, preventing weight interference among task vectors.\nFor each task vector ${\\delta^{k}}$ to be merged, a binary mask ${\\mathcal{M}(P^{k})}$ is constructed to satisfy the following constraints:\n$\\mathcal{M}(P^{k})_{i}=\\begin{cases}1, & \\text{if } i \\in P^{k}, \\\\ 0, & \\text{otherwise.} \\end{cases}$  (7)\nsubject to $\\cup_{k=1}^{K} P^{k} \\subseteq\\{1,2,...,d\\}$,\n${P^{k} \\cap P^{j}=\\emptyset \\quad \\text{for } k \\neq j}$.\nHere, ${P^{k}}$ represents the set of indices corresponding to the parameters selected for merging from ${\\delta^{k}}$, and d denotes the total number of parameters in each ${\\delta^{k}}$. Using the binary masks, the merging process is then formulated as:\n${\\theta^{M}=\\theta^{B}+\\sum_{k=1}^{K} \\delta^{k} \\cdot \\mathcal{M}(P^{k})}.$  (8)\nAlthough the formulation involves summation, no direct addition occurs between different task vectors, as the binary masks ensure that each parameter index is selected at most once.\nWhile there are many ways to construct non-overlapping binary masks for all models, we introduce a simple and easily controllable method by iteratively updating a merged mask to track the positions that have already been merged. Specifically, at the beginning, the merged mask is an empty set. At each step, starting with a task vector ${\\delta^{k}}$, we first exclude the indices that are already in the merged mask. Then, we sort the remaining parameter indices of ${\\delta^{k}}$ based on their saliencies, selecting the top ${n^{k}\\%}^{1}$ of the indices to form ${P^{k}}$. Finally, we update the merged mask with ${P^{k}}$. The procedure is described in Algorithm 1."}, {"title": "3.3 Optimal Brain Iterative Merging", "content": "OBM and IM can be combined with other existing methods. Taking TIES as an example, when combining TIES with OBM, the magnitude-based parameter pruning is replaced by a saliency-based approach. Similarly, when using TIES together with IM, we utilize global magnitude as saliency scores to construct the merged mask. However, the combination of OBM and IM, referred to as OBIM, yields better results. We conduct an ablation study to demonstrate the effectiveness of each component in Section 4.3."}, {"title": "4 Experiments", "content": "We conduct experiments on both SFT models and post-pretrained checkpoints to demonstrate the effectiveness of our method. To validate its robustness, we evaluate OBIM using two popular backbone models, LLaMA2 (Touvron et al., 2023) and Qwen2 (Yang et al., 2024a), in separate experiments. We also perform an ablation study to analyze the contributions of specific components within OBIM. Furthermore, we investigate key factors in our method to assess their influence on the final performance."}, {"title": "4.1 Experimental Setup", "content": "Experiment Settings for SFT Models. Following previous works (Yu et al., 2024b; Deep et al., 2024), we use Llama-2-13b as the pre-trained backbone and incorporate three fine-tuned models for cross-task merging experiments: WizardLM-13B-V1.2 (Xu et al., 2023) for instruction following, WizardMath-13B-V1.0 (Luo et al., 2023) for mathematical reasoning, and llama-2-13b-code-alpaca (Chaudhary, 2023) for code generation. To evaluate the capabilities of the merged models, we use AlpacaEval (Li et al., 2023) and MMLU (Hendrycks et al., 2021a) for general understanding; GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b) for mathematical reasoning; and HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for code generation. Performance is measured using the win rate for AlpacaEval2, zero-shot accuracy for MMLU, GSM8K, and MATH, and pass@1 for HumanEval and MBPP.\nExperiment Settings for Post-pretrained Models. We perform post-pretraining on Qwen2-7B using a multilingual dataset to enhance its Japanese proficiency. The dataset consists of over 200 billion tokens and includes publicly available pretraining corpora in English, Chinese, and Japanese. Details of the dataset are provided in Appendix A.3. The three best-performing checkpoints on Japanese evaluation are selected and merged with the backbone model. To assess performance across different languages, we employ three benchmarks: C-Eval (Huang et al., 2023) for Chinese, MMLU for English, and the Japanese Language Model Evaluation Harness (JP-LMEH) for Japanese. Five-shot accuracy is used for evaluating C-Eval and MMLU. JP-LMEH encompasses nine distinct NLP tasks, with the average score serving as an indicator of overall Japanese language proficiency.\nBaselines. We compare our method with the following state-of-the-art model merging approaches applicable to LLMs: TA (Ilharco et al., 2023): Task Arithmetic, a simple delta weight merging method that does not explicitly address interference. TIES (Yadav et al., 2024): Eliminates redundant parameters based on magnitude and resolves sign conflicts. DARE (Yu et al., 2024b): Drop And REscale, Randomly drops a proportion of parameters and rescales the remaining ones to reduce redundancy. DELLA (Deep et al., 2024): Assigns dropout probabilities to parameters based on their magnitudes for pruning. TALL-Mask (Deep et al., 2024): Uses a masking mechanism to filter out parameters relevant to only a few tasks. PCB (DU et al., 2024): Leverages parameter competition to optimize the merging process.\nValidation Set. Since OBIM requires a small sample set for parameter saliency computation, we hold out a validation set comprising portions of the training and development sets from each benchmark. Specifically, we compute saliency using data related to the model's training task: AlpacaEval and MMLU for general models, GSM8K and MATH for math models, and MBPP for code models. For post-pretrained models, saliency is computed using a multilingual dataset consisting of C-Eval, MMLU, and JP-LMEH. Details are provided in Appendix A.4."}, {"title": "4.2 Main Results", "content": "Results on SFT Models. The results are summarized in Table 1. We first present the performance of each individual model, followed by the results of merging the three task-specific experts using different methods. As shown in Table 1, OBIM achieves significant improvements in mathematical reasoning, with a 5.15% gain on GSM8K and a 0.9% gain on MATH compared to the original math model. In contrast, many other methods fail to surpass the source math model. For other benchmarks, OBIM ranks first on MMLU and remains competitive across other tasks. However, its performance on code generation is relatively lower. We suspect this is because the general model, rather than the code model, performs best on code generation, yet its saliency is computed using general data, leading to suboptimal preservation of coding ability. Overall, OBIM achieves the highest average performance, outperforming the second-best method by 1.86%, demonstrating its effectiveness in merging models for task fusion.\nResults on Post-pretrained Models. As shown in Table 2, checkpoints trained from Qwen2-7B exhibit enhanced Japanese proficiency but experience a significant decline in the Chinese and English capabilities of the base model. The goal of model merging is to restore Chinese and English performance to the base model level while preserving the improved Japanese ability of the continually trained model. Our results show that while no merging method fully restores C-Eval performance to the base model level, OBIM achieves the highest recovery rate, surpassing other methods by 1.49%. For English and Japanese, OBIM is the only method that surpasses the base model on MMLU and JP-LMEH, achieving improvements of 1.04% and 0.04% over other approaches. In terms of overall multilingual performance, OBIM ranks first across all benchmarks, producing a model with the strongest overall capabilities.\nTo further validate the robustness of our method, we evaluate the performance of merging different numbers of models. Specifically, we select the five"}, {"title": "4.3 Ablation Study", "content": "To assess the contributions of the two key components, OBM and IM, we conduct experiments using SFT model merging settings. Since OBM and IM cannot perform merging independently, we use TIES as the baseline method and integrate OBM and IM with the weight consensus approach for resolving sign conflict, termed \"Disjoint Mean,\" as well as the magnitude-based parameter trimming method in TIES, respectively. Details are provided in Appendix A.1.\nThe results are presented in Table 3, where we also outline each method's strategy for mitigating intra-model and inter-model interference. By comparing TIES with TIES+OBM and TIES+IM with OBIM, where each pair employs the same method for reducing inter-model interference, we observe that methods utilizing saliency-based parameter selection outperform those relying on magnitude-based selection. This finding confirms the superiority of OBM. Furthermore, methods incorporating IM consistently outperform their counterparts using the same intra-model approach. Specifically, TIES+IM surpasses TIES, and OBIM outperforms TIES+OBM, demonstrating the effectiveness of IM. In general, OBIM achieves the highest performance"}, {"title": "4.4 Influence of the Validation Set", "content": "Samples in the validation set influence saliency scores. To assess this impact, we replace the samples used for computing the saliency of the math model under the experimental settings of SFT model merging and evaluate the merging performance on two mathematical tasks.\nAs shown in Table 4, we compare the results using task-related data from GSM8K and MATH (Math), irrelevant data from MBPP (Code), and a mixed dataset (Math+Code). All datasets contain the same number of samples. Math achieves the best performance, followed by Math+Code, while Code performs the worst. This suggests that using task-specific data for saliency computation enhances task knowledge retention. We attribute this to our assumption for saliency approximation, which requires the first derivative to be approximately zero. This condition implies that data well learned by the model enables more accurate saliency estimation."}, {"title": "4.5 Influence of Iterative Merging Order", "content": "We analyze how the order of iterative merging influences performance. We conduct experiments based on the SFT model merging settings with four different merging orders: shifting the order using a rotation operation across model layers (Rotation), prioritizing the math model (Math First), placing the math model last (Math Last), and prioritizing the general language model (LM First). We evaluate the results on mathematical tasks.\nAs shown in Table 6, Math First achieves the best performance, whereas Math Last performs the worst, and LM First also yields poor results. These findings suggest that the earlier a model is merged, the better its knowledge is preserved. However, while Rotation performs slightly worse than Math First, it still surpasses the original math model, demonstrating its robustness. We attribute this to the redundancy of parameters in LLMs, where core capabilities can be largely retained even when only a subset of layers is preserved."}, {"title": "5 Related Work", "content": "Model merging has gained popularity in LLM research (Zhou et al., 2024; Yang et al., 2024b). By amalgamating multiple homologous LLMs into a single model, this technique has been applied to address several challenges, such as building multitask experts (Cai et al., 2023; Wan et al., 2024b), detoxification (Hu et al., 2024; Zhang et al., 2023), and preference alignment (Lin et al., 2024; Rame et al., 2024). Model merging methods are primarily based on two fundamental approaches: weight averaging (Wortsman et al., 2022) and task arithmetic (Ilharco et al., 2023).\nWeight-based model merging methods design rules or matrices to determine merging coefficients. For example, RegMean (Jin et al., 2022) optimizes a linear regression problem for linear weights, Fisher-Merging (Matena and Raffel, 2022) uses the Fisher information matrix to assess parameter importance. Some works explore the space of these coefficients using parameter searching algorithms, such as evolutionary algorithms (Akiba et al., 2025) or Bayesian optimization (Liu et al., 2024). Although these methods demonstrate effectiveness, they suffer from inefficiency: parameter search is time-consuming, and solving the objectives requires substantial computation resources.\nSubspace-based model merging methods focus on eliminating insignificant parameters and merging sparse models within the parameter subspace to reduce interference. TIES (Yadav et al., 2024) trims individual models based on parameter magnitudes, while Model Breadcrumbs (Davari and Belilovsky, 2025) refines this by removing both low-magnitude and high-magnitude outliers. DARE (Yu et al., 2024b) emphasizes the importance of rescaling after sparsification, and TALL-Mask (Wang et al., 2024) creates task-specific mask matrices based on predefined thresholds to filter out irrelevant parameters. However, these methods are limited to specific patterns, such as sign conflicts or threshold-based filtering, and magnitude-based sparsification remains suboptimal. To better address the interference problem, we propose a solution based on parameter saliency sparsification and a mutually exclusive iterative merging framework."}, {"title": "6 Conclusion", "content": "In this work, we propose OBIM, a novel merging method for LLMs that selectively retains representative delta weights based on saliency and iteratively integrates task vectors to reduce both intra-model and inter-model interference. OBIM achieves state-of-the-art performance in merging SFT models and post-pretraining checkpoints, demonstrating its effectiveness and versatility. Extensive ablation studies further validate its key components. Additionally, OBIM is computationally efficient and memory-light, making it well-suited for real-world applications."}, {"title": "7 Limitations", "content": "While our work provides valuable insights into LLM merging, several limitations should be noted: (1) The application of OBIM relies on models with identical architectures and shared initializations, limiting its applicability to diverse model types. (2) Although efficient, OBIM requires an additional validation set and incurs extra computational costs for saliency computation compared to magnitude-based methods. (3) Our analysis primarily focuses on interference from the perspective of parameter aggregation, with limited theoretical exploration, highlighting the need for further research in future work."}, {"title": "A Experimental Details", "content": "A.1 Details of Baselines and Ablation Study\nTo provide a better understanding of the baselines, we outline the methods used in previous works for addressing intra-model and inter-model interference. We compare these methods with our approach in Table 7, highlighting the innovation of our method. At the bottom of Table 7, we also provide the implementation of the methods used in our ablation study.\nBelow is a brief introduction to each component.\n\u2022 Magnitude Pruning: Retains parameters with the largest magnitude values.\n\u2022 Random Drop and Rescale: Filters parameters using a Bernoulli distribution and rescales the remaining ones according to the drop rate.\n\u2022 Stochastic Magnitude Pruning: Assigns magnitude values to probabilities and retains parameters according to these probabilities.\n\u2022 Disjoint Mean: Elects parameters at each position based on the direction of summation, then averages the parameters along that direction.\n\u2022 Consensus Mask: Selects parameters using a mask constructed by measuring the l\u2081 distance to the target task vector."}, {"title": "A.2 Hyperparameter Configurations", "content": "In the SFT model merging experiments, the hyperparameters that need to be adjusted include the retention ratio of parameters in OBM (nk%) and the merging order (O). The search ranges and the optimal settings for each hyperparameter are provided in Table 8.\nIn the post-pretraining model merging scenario, the retention ratio nk% for merging K checkpoints is set to , and the merging order is set to Rotation."}, {"title": "A.3 Datasets for Post-Pretraining", "content": "We collected and processed a dataset of over 200B tokens comprising Japanese, Chinese, and English texts for post-pretraining. The dataset includes text from websites and publications, all of which are publicly available. Below are the details of the data sources:\n\u2022 Japanese: C4-ja6, CC100-ja7, OSCAR-ja8, CulturaX, Wikipedia-ja10\n\u2022 Chinese: CLUECorpus15, SkyPile16, MAP-CC17, Wanjuan 18, Wikipedia-zh19\n\u2022 English: FineWeb11, Tiny-Textbooks12, AutoMathText13, Wikipedia-en14\nWe follow the pretraining data processing approach of DataComp-LM (Li et al., 2024). The data processing pipeline mainly consists of three steps:\n\u2022 Text Extraction: We extract clean text from raw content using rule-based tools such as HTML parsers and regular expressions.\n\u2022 Deduplication: We apply both locality-sensitive hashing (LSH) deduplication and semantic deduplication to remove redundant data.\n\u2022 Quality Filtering: We employ a FastText 21 binary classifier for each language to assess and filter data quality.\nAfter processing, we retain approximately 70B tokens for each language and 1B tokens from the parallel corpus as our post-pretraining dataset.\nWe then trained Qwen2-7B on this dataset using 64 A800 (80G) GPUs, with a training batch size of 4 million tokens per step for two weeks. Checkpoints were saved every 1,000 steps, and the total training duration was approximately 50,000 steps. We will release the trained model as open-source after the paper is accepted."}, {"title": "A.4 Details of Validation Set", "content": "The sample size of the validation set for each benchmark is provided in Table 9."}, {"title": "A.5 Computational Resources", "content": "We measured the GPU usage and time cost in OBIM, as shown in Table 10. GPUs used in our experiments is NVIDIA-A800 (80G). The total time is divided into three parts: the computation time for ${\\mathbf{X}}^{l}$ across all layers, which depends on the size of the validation set; the time required for saliency computation; and the time for iterative merging."}, {"title": "B Results on Merging Multiple Models", "content": "Table 11 presents the full results for merging varying numbers of post-pretrained models using OBIM and DARE."}, {"title": "C Ethics Statement", "content": "This paper focuses on model merging techniques for Large Language Models (LLMs) to enhance their adaptability. While our work does not directly introduce new risks, it inherits the broader societal concerns associated with LLMs, such as AI safety, reliability, and potential biases in generated content. Beyond these considerations, we do not foresee additional ethical concerns arising from our work."}]}