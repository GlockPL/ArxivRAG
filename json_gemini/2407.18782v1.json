{"title": "Understanding XAI Through the Philosopher's Lens: A Historical Perspective", "authors": ["Martina Mattioli", "Antonio Emanuele Cin\u00e0", "Marcello Pelillo"], "abstract": "Despite explainable AI (XAI) has recently become a hot topic and several different approaches have been developed, there is still a widespread belief that it lacks a convincing unifying foundation. On the other hand, over the past centuries, the very concept of explanation has been the subject of extensive philosophical analysis in an attempt to address the fundamental question of \"why\" in the context of scientific law. However, this discussion has rarely been connected with XAI. This paper tries to fill in this gap and aims to explore the concept of explanation in AI through an epistemological lens. By comparing the historical development of both the philosophy of science and AI, an intriguing picture emerges. Specifically, we show that a gradual progression has independently occurred in both domains from logical-deductive to statistical models of explanation, thereby experiencing in both cases a paradigm shift from deterministic to nondeterministic and probabilistic causality. Interestingly, we also notice that similar concepts have independently emerged in both realms such as, for example, the relation between explanation and understanding and the importance of pragmatic factors. Our study aims to be the first step towards understanding the philosophical underpinnings of the notion of explanation in AI, and we hope that our findings will shed some fresh light on the elusive nature of XA\u0399.", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial Intelligence (AI) is becoming progressively a pervasive technology in our daily lives as a result of its increasing accuracy and versatility [1]. Despite that, the growing integration of AI into human lives has determined a rising urgency to enlighten some of its potential undesirable outcomes. Consequently, its employment, particularly in contexts with paramount ethical considerations [2], has led to the necessity for a fair decision-making process [3], [4]. These reflections have determined a variety of discourses about people's right to have an explanation of how the decision is reached by the machine, especially when the methods used are conceived as \"black boxes\" [5]. As a result of these considerations, scholars have posed various questions around, for example, when explanations are required, what models provide such explanations, what are the desiderata necessary to achieve understanding [6], and what are the characteristics of a good explanation [2], [7], [4]. Within this debate, XAI is typically referred to as:\nThe process of elucidating or revealing the decision-making mechanisms of models. The user may see how inputs and outputs are mathematically inter-linked. It relates to the ability to understand why Al models make their decisions [2].\nNevertheless, defining explainability within the borders of a unique definition, amidst the plethora of those proposed, is a daunting task. Indeed, the majority of the aforementioned questions remain partially unresolved, to the extent that the precise definition of \u201cexplanation\" remains to some degree obscure [6]. Specifically, some authors contend that the ongoing discussion on explainable AI lacks a well-defined theoretical goal [8]. They argue that the concept of explanation, along with its related notions (e.g., interpretability [3]), is ambiguously defined, thus fostering the perception that there is no cohesive and convincing conceptual foundation [6], [3]. Additionally, it is worth noting that the variety of XA\u0399 models proposed is constantly evolving, which underscores the dynamic nature of this field [2] and its non-monolithic character [3]. Indeed, abundant recent attempts have been made to classify and systematize these models (refer to [2], [9], [1] for in-depth surveys), reflecting a growing interest in XAI and the need for a more structured approach to its development [2].\nDespite this, the discourse surrounding explainability is not novel and has been explored in various contexts [10]. Shifting our attention to the different realms of epistemology, this paper shows that analogous debates or inquiries arise. Indeed, the study of explanation has been a focal point of extensive philosophical analyses, undertaken to systematically address the fundamental question of \u201cwhy\u201d in the context of scientific law, thus unveiling one of the most substantial chapters in the philosophy of science [10]. This discussion has a remarkable history, and its roots extend back to the philosophy of Aristotle, who distinguished between two types of knowledge: \"knowledge that\" and \"knowledge why,\" to wit description and explanation [10]. Additionally, this distinction has become increasingly systematized over the past century, with a growing emphasis in scholarly discourses on the delineation and the proposal of a vast number of explanation models [10].\nAcknowledging the significance of the epistemological discourse and the substantial inputs from philosophers in this domain [10], this paper investigates parallels and establishes"}, {"title": "II. RELATED WORKS", "content": "In this section, we focus on incipient interdisciplinary efforts that have been done to connect and analyze psychological, sociological, and philosophical aspects of explanations. However, it should be emphasized that, to the best of our knowledge, no attempt has yet been made in the literature to link systematically the debates on XAI and scientific explanation.\na) Previous Philosophical Contributions.: Pioneering work in establishing connections between philosophy and XAI has been conducted by P\u00e1ez [8]. The author elucidates the relationship between understanding and explanation both in the realm of scientific explanation and XAI. Subsequently, McDonnell [11] provides some lessons from philosophy to assess better explanations. More specifically, his three primary observations include the necessity of a contrastive structure, the importance of focusing on actionable interventions, and the idea that robust causal dependence enhances the effectiveness of an explanation. Dur\u00e1n [12] claims that scientific explanations are furnished with a precise structure aimed at providing a comprehensive understanding of the world. Also, his paper asserts that current XAI models do not qualify as genuine explanations. Finally, O'Hara [4] clarifies the relationship between explanation and understanding, establishing a connection with the decision process.\nb) Explanation and Social Aspects.: A segment of the present literature has directed its attention towards social attributes of explanations, linking them with XAI. Miller [7] affirms that insights about humanities can benefit XAI. He emphasizes that explanations are contrastive, social, and selected in a biased manner and also that causal relations are more influential than probabilities. Mueller et al. [13] claim that there is a necessity for human-inspired XAI guidelines,\nas psychological principles often remain underestimated. Hoffman et al. [14] assert that explanations are not properties of statements, but result from interactions. In fact, what qualifies as an explanation depends on the learner's needs, previous knowledge, and goals.\nc) The Call for Clarification.: Several authors called for clarity, remarking on the need for greater rigor in the definition of explainability and related concepts. Lipton [3] considers the term interpretability as slippery and ill-defined. P\u00e1ez [8] argues that explanatory strategies may lack a precisely defined theoretical purpose. Boge and Poznic [15] affirm that discussions on the philosophy of science could benefit ML. They emphasize the significant connections between these two disciplines and assert that the development of XAI could become a crucial theme in the philosophy of science."}, {"title": "III. PHILOSOPHICAL ROOTS OF EXPLANATION", "content": "Embedded in centuries of philosophical inquiry, the fundamental concept of explanation has an extremely long tradition and ancient roots, which can shed light on the actual discussion on XAI. In particular, during the last decade, epistemology has been involved in a lively debate about scientific explanation, in which philosophers have meticulously delineated its constituents, seeking a precise definition while also reflecting upon the criteria of good explanations and discussing which particular model should be preferred to achieve them [10]. As we aim to assert, the term \"explanation\" carries connotations and meanings that can be transposed to the current discussion regarding XAI, but that are beyond its common-sense definition or recent discussions, owing to the depth of the philosophical tradition in which it has been expounded. Additionally, the relationship between scientific explanation and XAI is relevant not only because of potential parallels in the philosophical discourse or overlapping terminology in both debates. It also arises from a broader conception that originates from the proximity of some AI fields, including ML, with the scientific inquiry [16]. This closeness may contribute to extending and reinterpreting some of the implications of explanation, through a philosophy of science lens. Indeed, various authors provide valuable philosophical insights into these issues, contending that Pattern Recognition (PR) and ML are inherently aligned with scientific endeavors in their contribution [17], [16]. This correspondence is evident in their pursuit to address similar questions related to categorization, causality, generalization, the problem of induction, or other pivotal aspects [17]. Similarly, the concept of explanation can benefit from a philosophical perspective. However, before we delve into the depths of explainability, this section introduces some notions that may help elucidate the presentation of our parallelism within the context of the philosophy of science, including a brief presentation of the XAI debate, considerations regarding science as a \"black box,\" and pertinent philosophical terminology that will serve as a foundation to initially outline the concept of explanation."}, {"title": "A. Short History of XAI", "content": "Although the debate on explainability has recently gained prominence, particularly after the introduction of the right to explanation within the GDPR [2], [5], this concept traces back to the early days of expert systems [18]. For instance, MYCIN [19] is a rule-based expert system, developed to help doctors select antimicrobial therapy. It includes a general ques-tion answerer and a status checker, enabling the physician to understand both the program's advice and its reasoning. This type of system is grounded in a hypothetico-deductive strategy and exhaustively applies inference rules [20], implying determinism [21] and making the models easily interpretable [18]. REX [22] consists of a knowledge-based explanation system and a knowledge-based problem-solving system, in alignment with the existing epistemological separation between \u201cknowledge that\" and \"knowledge why.\" It offers explanations of how an expert system progresses from specific data to a final output. Differently from early AI systems, most ML models are not directly interpretable and can be considered as a \u201cblack box\" [18], [3]. Hence, explainability in this latter instance can be seen as finding a more interpretable surrogate model approximating the original one [23]. Consequently, the most popular XAI methods often lack rigorous guarantees [24]. As an alternative to heuristic or informal techniques [25], [26], growing interest has been posed on formal XAI, which offers logic-driven methods for deriving explanations, by providing theoretical assurances [27]. Among these approaches, abductive explanation [28] stands out as an argument-based local explanation, consisting of a minimal set of literals sufficient for predicting a class. Thus, it serves as a reason for assigning a class to an instance [28]. Moreover, runtime verification allows the explanation of AI-based self-adaptive systems, enabling the investigation of system behavior [29]. Finally, we cite XAI techniques built upon Al diagnosis principles, which involve identifying system faults or anomalies through logical reasoning and inference techniques [30]. However, the dichotomy between surrogates and formal explanations will be analyzed in Subsection V-A, as crucial for the discussion in relation to bona fide explanations.\nIn general, due to the vastness of the discussion, several criteria are introduced to classify explainability in ML literature. For instance, a separation is established between global or local methods, depending on whether their goal is to explain the whole model or a single prediction. Also, there is a distinction between model-specific and model-agnostic approaches, relying on the fact that the explanation applies to a single model (or a group), or all ML ones [9], [1].\nOther salient taxonomies distinguish between feature-based or example-based techniques [9] or between attribution, visualization, example-based, game theory, and knowledge extraction explanations [2]. Most of the relevant models identified in the pertinent literature have been reported in Figure 1, among them is worth mentioning the Counterfactual Explanation [5], a model-agnostic method that shows what change in features should be done to determine a prediction switch. Additionally, there exists LIME [26], which uses a linear classifier for a"}, {"title": "B. Black Box-ness Insights from Science", "content": "The term \"black box\" is often employed to describe a model lacking interpretability, deemed antithetical to the principle of transparency, i.e., the property of an algorithm that is directly comprehensible [3]. However, the metaphorical notion of a \"black box\" has received considerable attention in a wider range of disciplines, including but not limited to, science, philosophy of science, and psychology. Its interpretive significance extends beyond the field of ML and comprises a variety of theoretical frameworks and intellectual pursuits [31], [32]. Hanson [32], for instance, introduced the concept of the \"black box\" as one of three stages in scientific development. Initially viewed as an algorithm with opaque internals, theories progress to a \"gray box\" stage where some structure is discernible, and finally to a \"glass box\" stage, offering transparent insights across disciplines. Additionally, when it comes to the \"black box\" nature of a model, the issue of explanation also arises. Within this whole theoretical framework, the term \"black box\" is employed as a metaphorical device to connote the idea that the system in question is, in some sense, a closed entity whose internal workings are inaccessible to outside scrutiny. Both AI and science can be interpreted within this definition [31], [32], [3]."}, {"title": "C. Explanation Terminology Basics", "content": "Before presenting the centuries-long philosophical dialogue, we provide some basic philosophical terminology and con-cepts. Quoting Salmon:\nUnless we take preliminary steps to give some un-derstanding of the concept we are trying to explicatethe explicandum any attempt to formulate anexact explication is apt to be wide of the mark [33].\nIt is commonly accepted that science aims for knowledge acquisition about the world, distinguishing itself from common sense knowledge [34]. However, philosophical literature traditionally differentiates between two types of scientific knowledge, namely \"knowledge that\" and \"knowledge why.\u201d Indeed, the first concerns description, while the latter explanation [10]. In particular, an explanation, which provides a scientific understanding of the world, is typically divided into two components: the \u201cexplanandum\" and the \u201cexplanans.\u201d The former pertains to the statements regarding the event requiring an explanation, whereas the latter encompasses those used to provide them [35]. Another common concern relates to the nature of the phenomena requiring explanation, which can comprise individual events, general laws, or statistical regularities. According to Nagel [34], there are four distinct explanation patterns since \"why questions\u201d are not all of the same type. These include deductive, probabilistic, functional or teleological, and genetic models of explanations. In deductive explanations, the \u201cexplanandum\" is a logically necessary consequence of the explanatory premises. Probabilistic explanations stem from statistical premises, addressing individual cases. Functional explanations indicate the instrumental roles a unit has in bringing about a goal within a system. Lastly, genetic explanations delineate the sequence of significant events leading from an earlier system to a later one."}, {"title": "IV. THE HISTORICAL EVOLUTION OF SCIENTIFIC EXPLANATION DEBATE", "content": "In this section, we aim to analyze the scientific explanation debate to acquire a comprehension of the issues and the philosophical foundations of explanation, providing useful insights into the multi-faced underpinnings of XAI discourse. Throughout the past discussions, a variety of positions have emerged within the epistemology framework, as well as analogous topics. For instance, consisting of the scarcity of accurate terminology or the challenges of selecting the optimal model for factoring in explanations [10]. However, systematic attempts to solve these issues have been proposed in the epistemological literature, offering fruitful philosophical in-sights for XAI. To establish a correlation between two distinct debates and to identify potential intersections, we categorize the epistemological discussion into three distinct eras, in relation to Hempel and Oppenheim's turning point proposal of the Deductive-Nomological (D-N) model [35]. These eras, namely the pre-Hempelian era, the received view, and the post-Hempelian era follow the chronological development. Our aim is, as illustrated in Figure 1, to highlight possible common trends and pivotal points of the discourses regarding the concerns raised."}, {"title": "A. Pre-Hempelian Era", "content": "Many of history's most eminent philosophers and scientists have questioned the nature of explanation and its role in science. However, it is not possible to answer by providing a unique definition. Instead, we should respond by starting from the very initial explorations. According to Aristotle [36], it is only when we know the causes, or \u201caitia,\" of something that we have an explanation for it, emphasizing the importance of explanation in response to \u201cwhy questions.\u201d Indeed,\nThe discussion of aitia, on the other hand, is rathera discussion of explanation, and the doctrine ofthe \"four causes\" is an attempt to distinguish andclassify different kinds of explanation, different ex-planatory roles a factor can play [36].\nTo be more specific, Aristotle identified four causes, which are different types of answers to the \"why question,\" namely the material cause, the formal cause, the efficient cause, and the final cause. In the Aristotelian view, causality and explanation are intimately related and, as we will see, causation assumes a key role in numerous accounts of explanation. However, not all philosophers have supported the notions of causality and explanation. For instance, in Galileo Galilei's various scripts, it is possible to recognize strong positions against the existence"}, {"title": "B. The Received View", "content": "In 1948, the work of Hempel and Oppenheim brought the concept of explanation to the forefront of the philosophy of science, marking a pivotal moment in the trajectory of future debates, to the extent that it is possible to distinguish between the philosophical inquiry that happened before Hempel, and that that occurred after. While their model is often regarded as the first attempt to incorporate explanation into scientific discourse, their true contribution was to propose a structured effort at the systematization of scientific explanation into the so-called Deductive-Nomological model [35]. The core of their model lies in subsuming the \"explanandum\" under general laws and statements about the conditions under which the phenomenon occurred, through deductive inference. Accordingly, in a Hempelian context, to explain means to bring phenomena back into the realm of laws having empirical scope. An example would be helpful to have a better grasp of the Deductive-Nomological model. The \"explanandum\" consists of the description of the phenomenon to be explained, such as an oar underwater that appears bent upwards to an observer in a rowboat. The \u201cexplanans\u201d comprises both general laws (refraction, water optical density) and antecedent conditions (an oar part in the water and part in the air, an oar consisting of a straight piece of wood). Hence, the \"explanandum\" is logically deduced from the \u201cexplanans,\u201d thus the question \"Why does the phenomenon occur?", "What overarching principles and preceding circumstances lead to the phenomenon?\"\nNevertheless, not all scientific laws are explainable through deduction, such as probabilistic or statistical ones. Thus, Hempel [43] introduced a statistical systematization for scientific explanations, namely the Inductive-Statistical (I-S) model, recognizing the limitations of the Deductive-Nomological one. Hempel's I-S model is his natural way to extend the D-N model to statistical generalizations, remaining implicitly entrenched in the deductive ideal. Indeed, to explain means to express the probability of a given instance of F being an occasion of G, represented by the variable r. Hempel's I-S explanation must be tied to all available reference knowledge, as stated by his \u201cmaximal specificity\u201d requirement. The idea underlying this condition is the impossibility of genuine statistical explanation, that defines them as epistemically relative [10], and from which also Hempel derived the principle of \"high inductive probability,\u201d in which the value assigned to r should be as close as possible to 1 [44].\nExplanation, according to these views, is the logical process by which science provides answers to \u201cwhy questions\u201d and, thus, terms like \u201ccomprehensible\u201d and \u201cunderstanding": "re considered to be inapplicable to scientific explanation since they do not fall within the domain of its logical aspects, to the extent that Hempel [44] compared this process to the one of mathematical proofs. Future conceptions of explanation will increasingly focus on pragmatic aspects and probabilistic causality, moving further away from the deductive ideal."}, {"title": "C. Post-Hempelian Era", "content": "After Hempel's \u201creceived view\u201d a certain amount of formal and semi-formal models were proposed by different authors. Indeed, post-Hempelian scholars mainly rejected his conception of explanation and started from attacks on his model to build new interpretations.\na) Statistical Relevance Model.: Salmon [45] moved from criticism about the inferential structure of explanation and proposed the Statistical-Relevance (S-R) model, which contemplates a specific idea of probabilistic causation. In his conception, explanations must consider not only events that respect the principle of \"high inductive probability\" but also unlikely ones. Statistical relevance determines to which homogeneous reference class the single event belongs. To establish homogeneity, the method involves partitioning non-homogeneous reference classes into maximal homogeneous sub-classes, which are mutually exclusive and comprehensive for the initial class. Thus, to explain means to place the \"ex-planandum\" in a chain of correlations expressed by statistical generalizations, that constitute the reference class meeting the maximal homogeneity criterion. A satisfactory theory of explanation should assign a fundamental role to causality,\nand, although statistical explanations are often discussed inseemingly indeterministic contexts, this does not negate thepossibility of finding causal connections [10].\nb) The Pragmatics of Explanation.: Van Fraassen [46],unsatisfied with Salmon's and previous accounts, introduceda pragmatic view of explanation. While the neo-positivistperspective was mainly concerned with establishing measuresfor verifying the validity of a scientific theory, such as its truth-fulness or empirical adequacy, this view aims to determine therelevant part of a scientific fact by considering the contextualinformation, which relates to the knowledge and interests ofthe subject who posits the \"why question.\u201d Van Fraassen beganby examining requests for specific \"why questions,\u201d which arecomprised of a triplet Q = \u3008Pk, X, R), namely, the topic, theantithesis class, and the relevance relation. The latter connectsthe informative part of the answer with the components of thequestion [47].\nc) The Unificationist View.: As the debate progresses, theimportance of contextual elements in explanation increases.Friedman's Unificationist view [48] explored the feasibility ofan objective conceptualization of scientific understanding, inseeking to clarify what is in the relationship between phenom-ena that determine one as the explanation of the other. Theexplanation process is not merely a substitution of one casualphenomenon. Rather, it involves replacing less comprehensivephenomena with more comprehensive ones, by reducing thenumber of independent events and enhancing our global under-standing of the world. Indeed, unification is the element of theexplanation relation that produces understanding. Kitcher [49]proposed the most articulated Unificationist view, which positsthat scientific activity aims to unify accepted knowledge,through general laws. Scientific understanding is achievednot by explaining individual occurrences, but by providingincreasingly larger frameworks to fit them systematically.\nd) Abductive Explanation.: The term \"abduction,\" oftenparalleled with the locution \u201cinference to the best expla-nation [50],\u201d originated with Peirce [51], who introducedit to signify a type of reasoning distinct from deduction,although not induction. Abduction is a type of nonmonotonicreasoning [52] (i.e., defeasible inference) and consists of theprocess of forming explanatory hypotheses given a certainscenario [51]. The concept posits that when confronted witha phenomenon if one explanation emerges that plausiblyaccounts for the otherwise inexplicable, it is reasonable tolean towards accepting that explanation as likely correct [51].After its first appearance, different formalizations have beensuggested, taking the name of logic-based abduction, whichis particularly suitable if complex causal relationships pre-vail [52]. However, the idea of inference to the best ex-planation is met with resistance in the field philosophy ofscience, as this kind of inference presupposes the truth of theexplanatory premises [10]. Indeed, what may be selectedas the best explanation, could be within a group of incorrectones [47]. Moreover, this kind of explanation leaves open therole of pragmatic components for the selection of the bestexplanation for different individuals [47].\ne) Neo-Mechanistic Theories.: The Unificationist theory proposed by Kitcher [49] sees explanation as global and, by referring to general laws, employs a top-down approach. On the other hand, causal-mechanical theories such as that advanced by Salmon [45] employ a bottom-up approach and aim to describe the causal relationships involved in the phenomenon being explained [10]. This type of explanatory knowledge seeks to provide understanding by showing the inner mechanism of phenomena of the world, that is, by exploring the internal workings of things, making it possible to open the \"black box\" of nature. During the '90s this account served as inspiration for neo-mechanistic theories, that proposed a more applicable view of causality aimed to identify mechanistic links [10], in a conception of causality understood as productivity. Among the most relevant, it is possible to encounter Glennan's [53] Complex System account, in which a mechanism consists of various behaviors comprising multiple components that can be separately analyzed and decomposed into smaller subsets. Additionally, the system's parts should exhibit a notable degree of robustness or stability. In other words, their properties should remain relatively constant in the absence of external interventions. A good explanation is made of an \u201cexplanandum,\u201d which is the description of the phenomena to be explained, and an \u201cexplanans,\u201d which is the inner mechanistic description. A different account comes from Bechtel and Abrahamsen [54], which proposed the Decomposition and Localization model. Following their perspective, a mechanism is a structure that fulfills a function based on its constituent parts, its operations, and the overall organization. Moreover, according to the authors, due to the epistemic character of explanations, representations, such as diagrams and verbal or linguistic descriptions, can support the inner mechanisms of nature.\nf) Counterfactual Explanation.: In recent decades, a new type of approach to causality for explanation has gained popularity, namely the \u201cinterventionist perspective", "needed": "counterfactuals and contrastive explanations are not synonymous, although they are often used as interchangeable terms [8]. The counterfactual explanation states that causal relations exist only if intervening on the cause C, produces a change in the effect E, remaining unchanged the relationship between the two variables [56]. On the other hand, contrastive explanations answer the question \"Why x rather than y?", "Why x?": 8}, {"title": "V. A COMPARISON OF EXPLAINABILITY DEBATES THROUGH AN EPISTEMOLOGICAL LENS", "content": "Ultimately, we possess all the necessary tools to draw the analogy between scientific explanation and XAI debates, by looking at their pattern of development, as shown in Figure 1. As we noted, explanations were not initially accepted as distinct goals of science, since separate from description or prediction, in either realm. Indeed, in the domain of the philosophy of science, the acceptance of scientific explanations did not manifest uniformly from the origins of the debate, as various philosophers originally rejected the idea of considering them a distinct objective of science, favoring description [10]. Over centuries, there has been a transition from discordant perspectives toward a major consensus, culminating in the proposal of diverse models for explanation. Analogously, within the discourse on XAI, explanations were not initially regarded as primary objectives of AI models, which predominantly sought predictive capabilities while prioritizing high accuracy [2]. This is also known as the interpretability/accuracy trade-off where the quest for improved predictive performance often comes at the cost of reduced model interpretability. This relationship has traditionally been viewed as mutually exclusive; however, this notion has been increasingly contested by several scholars that argue for optimization between both [2]. Thus, it is possible and worth claiming, that the urgency for explanation did come after the need for accurate prediction and description in both the field of AI and the philosophy of science [2], [10].\nMoreover, we discerned a gradual change from logic-deductive models of explanation to statistical ones in both domains, as we witnessed a shift from certainty to uncertainty. Hempel's Deductive-Nomological model seeks explanations, by deducing from causal (or deterministic laws) [35]. Additionally, in Hempel's [35] first scripts, causal laws overlapped in their meaning with non-statistical laws, and although he recognized the existence of the latter, he restricted his account of explanation to the deductive ones. On the other hand, moving progressively forward in time, if we look into mechanistic or neo-mechanistic explanations, we encounter a progressive consideration of statistical relationships, while not losing the importance of causal connections. As Salmon states:\nIf indeterminism is true, some explanations willbe irreducibly statistical-that is, they will be full-blooded explanations whose statistical character results not merely from limitations of our knowledge [10].\nAs it is highlighted in Figure 1, if we move toward XAI, an interesting analogy emerges: the very first deductive expert systems, having rule-based knowledge, were directly interpretable and their explanation consisted of an inference of the output from the rules [20]. However, most ML models work as \"black boxes\" and their knowledge is opaque, so they don't reveal sufficient details about their internal behavior [3]. For this reason, differently from early rule-based systems, explainability in ML often seeks to find an interpretable model that approximates the original one, by finding statistical correlations [23] (e.g., many explainability methods offer summary statistics for each feature, such as feature importance [2]). However, genuine causal relationships must be preserved [10]. Moreover, manipulative-counterfactual approaches to explanation have increasingly gained popularity in both debates. On the side of scientific explanation, by advancing an intervention-centered notion of causality [55]. While, on the other of AI, showing what should have been different to change the decision of the system. Specifically, consisting of the smallest change that can be made to a particular instance to get a different decision from the AI [5].\nLastly, a typical distinction found in XAI literature is within the categorization of global and local explanations, the first ones aimed to explain the knowledge of general patterns of the system as a whole, while the latter, a single decision [6]. As underlined in Figure 1, scientific explanation, in a broader sense, sees patterns of explanation underlying a similar dis-tinction between top-down and bottom-up accounts. The first one is, in this sense, global, as it relates to the structure of the whole world [10]. The second one, as can be seen very well in Bechtel and Abrahamsen's account [54], aims to identify the relationships and explanations of individual parts."}, {"title": "A. Going Deeper: Concepts of Explainability", "content": "In addition to establishing connections between the two debates as a whole, it is possible to examine analogies between related concepts and common terminology that we identified in our comparison of scientific explanation and XAI. This analysis considers preliminary epistemological implications that are relevant within the XAI domain, such as the relationship between explanation and understanding, the significance of similarity in explanation, and the desiderata of good explanations, thereby laying foundational groundwork for future research.\na) The Epistemological Relation between Explaining and Understanding.: The earliest theories of scientific explanation, proposed by the analytical philosophical tradition, were not concerned with understanding, as they claimed that it was not part of the explanation relation. According to Hempel [35], a scientific explanation is restricted to deductive and logical inference, by which science answers \"why questions\u201d and, thus, he considered terms like \u201ccomprehensible\u201d and \"understanding"}, {"title": "b) Similarity, Familiarity, and Surrogate Models.:", "content": "Ex-planation of ML often consists of adopting a surrogate andinterpretable model, such as linear regression, that should provide representations necessary to obtain understanding [23].However, a relevant issue is establishing why this surrogateserves as an explanation of the original model. Indeed, forany XAI model, there should be a formal linkage, such as iso-morphism or similarity, between it and the initial model [12].Nevertheless, the majority of surrogate models used currentlylack rigorous assurances, raising uncertainties about the effi-cacy of these approximations in elucidating decision-makingprocesses [24]. On the other hand, formal explanations seekto establish guarantees or justifications with respect to thedetermined explanation [58], [27], such as Random Forestexplanations with SAT [27] or abductive explanations [28].Without this type of connection, there is no basis to state thatan explanation provided by the XAI model applies to a \"blackbox\" [12], [24]. Similarly, some philosophers of science haveargued that understanding can given by familiarity, in whichthe \"explanans\u201d is an approximation similar to the \u201cexplanan-dum\" or an idealization of it [47]. However, to others thisview is deemed inadequate: being familiar gives no groundsfor being understood and, regardless some explanations mightevoke a feeling of familiarity, this is not a relevant factor insound explanations [48], [35]."}, {"title": "c) Bona Fide Explanations Criteria.:", "content": "Also as a con-sequence of the aforementioned considerations, researchersin both epistemology and the XAI domains have sought toidentify the characteristics that distinguish bona fide explana-tions, i.e., explanations should satisfy certain requirements tobe considered valid [42], [12], [10]. For instance, Miller hasdone incipient work in establishing criteria to evaluate XAI,by deriving principles from social sciences [7]. Moreover,Mueller et al. [13] provided an exhaustive list of principles"}, {"title": "VI. CONCLUSIONS", "content": "The concept of explainability has been the object of nu-merous inquiries. However, notwithstanding its acknowledg-ment as a fundamental right and the considerable number of proposed models, it is widely criticized for not having convincing and unifying conceptual grounds. This article tries to fill in this gap and aims to contribute to the foundations for the construction of a \u201cbridge\u201d between epistemology and ML, which may lead to deeper explorations of epistemo-logical consequences of AI explanations. We compared two apparently different debates, scientific explanation, and X\u0391\u0399, in an attempt to assist XAI discussion with a well-grounded philosophical foundation. We traced the history of their development, criticisms that have emerged, and key concepts, examined through the epistemological lens. An intriguing picture has emerged: the development of the debates followed a general common progression, specifically from deductive to statistical explanations. Interestingly, we also notice that similar concepts have independently arisen in both realms, such as the relation between explanation and understanding, the importance of pragmatic factors, the relationship between similarity and explanation, and the search for bona fide ex-planations. Hence, in Section V we have briefly illustrated how possible implications can be derived from epistemology in order to analyze XAI concepts. We identified the roots from which philosophical terminology has originated and also of a \"dictionary\" of shared concepts, to help XAI practitioners draw insights from past philosophical debates and their impli-cations. Hence, future work may be aided by the instruments of the philosophers that we hope to have enlightened. Moreover, we offer ML researchers extensive epistemological literature, from which they can draw inspiration. For example, counter-factual explanations, with their deep roots in philosophy, have recently garnered attention in the field of XAI, demonstrating"}]}