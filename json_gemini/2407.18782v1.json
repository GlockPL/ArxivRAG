{"title": "Understanding XAI Through the Philosopher's Lens: A Historical Perspective", "authors": ["Martina Mattioli", "Antonio Emanuele Cin\u00e0", "Marcello Pelillo"], "abstract": "Despite explainable AI (XAI) has recently become a hot topic and several different approaches have been developed, there is still a widespread belief that it lacks a convincing unifying foundation. On the other hand, over the past centuries, the very concept of explanation has been the subject of extensive philosophical analysis in an attempt to address the fundamental question of \"why\" in the context of scientific law. However, this discussion has rarely been connected with XAI. This paper tries to fill in this gap and aims to explore the concept of explanation in AI through an epistemological lens. By comparing the historical development of both the philosophy of science and AI, an intriguing picture emerges. Specifically, we show that a gradual progression has independently occurred in both domains from logical-deductive to statistical models of explanation, thereby experiencing in both cases a paradigm shift from deterministic to nondeterministic and probabilistic causality. Interestingly, we also notice that similar concepts have independently emerged in both realms such as, for example, the relation between explanation and understanding and the importance of pragmatic factors. Our study aims to be the first step towards understanding the philosophical underpinnings of the notion of explanation in AI, and we hope that our findings will shed some fresh light on the elusive nature of XA\u0399.", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial Intelligence (AI) is becoming progressively a pervasive technology in our daily lives as a result of its in- creasing accuracy and versatility [1]. Despite that, the growing integration of AI into human lives has determined a rising ur- gency to enlighten some of its potential undesirable outcomes. Consequently, its employment, particularly in contexts with paramount ethical considerations [2], has led to the necessity for a fair decision-making process [3], [4]. These reflections have determined a variety of discourses about people's right to have an explanation of how the decision is reached by the machine, especially when the methods used are conceived as \"black boxes\" [5]. As a result of these considerations, scholars have posed various questions around, for example, when explanations are required, what models provide such explanations, what are the desiderata necessary to achieve understanding [6], and what are the characteristics of a good explanation [2], [7], [4]. Within this debate, XAI is typically referred to as:\nThe process of elucidating or revealing the decision- making mechanisms of models. The user may see how inputs and outputs are mathematically inter- linked. It relates to the ability to understand why Al models make their decisions [2].\nNevertheless, defining explainability within the borders of a unique definition, amidst the plethora of those proposed, is a daunting task. Indeed, the majority of the aforementioned questions remain partially unresolved, to the extent that the precise definition of \u201cexplanation\" remains to some degree obscure [6]. Specifically, some authors contend that the on- going discussion on explainable AI lacks a well-defined the- oretical goal [8]. They argue that the concept of explanation, along with its related notions (e.g., interpretability [3]), is ambiguously defined, thus fostering the perception that there is no cohesive and convincing conceptual foundation [6], [3]. Additionally, it is worth noting that the variety of XA\u0399 models proposed is constantly evolving, which underscores the dynamic nature of this field [2] and its non-monolithic character [3]. Indeed, abundant recent attempts have been made to classify and systematize these models (refer to [2], [9], [1] for in-depth surveys), reflecting a growing interest in XAI and the need for a more structured approach to its development [2].\nDespite this, the discourse surrounding explainability is not novel and has been explored in various contexts [10]. Shifting our attention to the different realms of epistemology, this paper shows that analogous debates or inquiries arise. Indeed, the study of explanation has been a focal point of extensive philosophical analyses, undertaken to systematically address the fundamental question of \u201cwhy\u201d in the context of scientific law, thus unveiling one of the most substantial chapters in the philosophy of science [10]. This discussion has a remark- able history, and its roots extend back to the philosophy of Aristotle, who distinguished between two types of knowledge: \"knowledge that\" and \"knowledge why,\" to wit description and explanation [10]. Additionally, this distinction has become increasingly systematized over the past century, with a growing emphasis in scholarly discourses on the delineation and the proposal of a vast number of explanation models [10].\nAcknowledging the significance of the epistemological dis- course and the substantial inputs from philosophers in this domain [10], this paper investigates parallels and establishes"}, {"title": "II. RELATED WORKS", "content": "In this section, we focus on incipient interdisciplinary efforts that have been done to connect and analyze psychological, sociological, and philosophical aspects of explanations. How- ever, it should be emphasized that, to the best of our knowl- edge, no attempt has yet been made in the literature to link systematically the debates on XAI and scientific explanation.\na) Previous Philosophical Contributions.: Pioneering work in establishing connections between philosophy and XAI has been conducted by P\u00e1ez [8]. The author elucidates the relationship between understanding and explanation both in the realm of scientific explanation and XAI. Subsequently, McDonnell [11] provides some lessons from philosophy to assess better explanations. More specifically, his three primary observations include the necessity of a contrastive structure, the importance of focusing on actionable interventions, and the idea that robust causal dependence enhances the effec- tiveness of an explanation. Dur\u00e1n [12] claims that scientific explanations are furnished with a precise structure aimed at providing a comprehensive understanding of the world. Also, his paper asserts that current XAI models do not qualify as genuine explanations. Finally, O'Hara [4] clarifies the rela- tionship between explanation and understanding, establishing a connection with the decision process.\nb) Explanation and Social Aspects.: A segment of the present literature has directed its attention towards social attributes of explanations, linking them with XAI. Miller [7] affirms that insights about humanities can benefit XAI. He emphasizes that explanations are contrastive, social, and se- lected in a biased manner and also that causal relations are more influential than probabilities. Mueller et al. [13] claim that there is a necessity for human-inspired XAI guidelines,\nas psychological principles often remain underestimated. Hoff- man et al. [14] assert that explanations are not properties of statements, but result from interactions. In fact, what qualifies as an explanation depends on the learner's needs, previous knowledge, and goals.\nc) The Call for Clarification.: Several authors called for clarity, remarking on the need for greater rigor in the definition of explainability and related concepts. Lipton [3] considers the term interpretability as slippery and ill-defined. P\u00e1ez [8] argues that explanatory strategies may lack a precisely defined theoretical purpose. Boge and Poznic [15] affirm that discussions on the philosophy of science could benefit ML. They emphasize the significant connections between these two disciplines and assert that the development of XAI could become a crucial theme in the philosophy of science."}, {"title": "III. PHILOSOPHICAL ROOTS OF EXPLANATION", "content": "Embedded in centuries of philosophical inquiry, the funda- mental concept of explanation has an extremely long tradition and ancient roots, which can shed light on the actual discussion on XAI. In particular, during the last decade, epistemology has been involved in a lively debate about scientific explana- tion, in which philosophers have meticulously delineated its constituents, seeking a precise definition while also reflecting upon the criteria of good explanations and discussing which particular model should be preferred to achieve them [10]. As we aim to assert, the term \"explanation\" carries conno- tations and meanings that can be transposed to the current discussion regarding XAI, but that are beyond its common- sense definition or recent discussions, owing to the depth of the philosophical tradition in which it has been expounded. Additionally, the relationship between scientific explanation and XAI is relevant not only because of potential parallels in the philosophical discourse or overlapping terminology in both debates. It also arises from a broader conception that originates from the proximity of some AI fields, including ML, with the scientific inquiry [16]. This closeness may contribute to extending and reinterpreting some of the implications of explanation, through a philosophy of science lens. Indeed, various authors provide valuable philosophical insights into these issues, contending that Pattern Recognition (PR) and ML are inherently aligned with scientific endeavors in their pursuit [17], [16]. This correspondence is evident in their pursuit to address similar questions related to categorization, causality, generalization, the problem of induction, or other pivotal aspects [17]. Similarly, the concept of explanation can benefit from a philosophical perspective. However, before we delve into the depths of explainability, this section introduces some notions that may help elucidate the presentation of our parallelism within the context of the philosophy of science, in- cluding a brief presentation of the XAI debate, considerations regarding science as a \"black box,\" and pertinent philosophical terminology that will serve as a foundation to initially outline the concept of explanation."}, {"title": "A. Short History of XAI", "content": "Although the debate on explainability has recently gained prominence, particularly after the introduction of the right to explanation within the GDPR [2], [5], this concept traces back to the early days of expert systems [18]. For instance, MYCIN [19] is a rule-based expert system, developed to help doctors select antimicrobial therapy. It includes a general ques- tion answerer and a status checker, enabling the physician to understand both the program's advice and its reasoning. This type of system is grounded in a hypothetico-deductive strategy and exhaustively applies inference rules [20], implying deter- minism [21] and making the models easily interpretable [18]. REX [22] consists of a knowledge-based explanation system and a knowledge-based problem-solving system, in alignment with the existing epistemological separation between \u201cknowl- edge that\" and \"knowledge why.\" It offers explanations of how an expert system progresses from specific data to a final output. Differently from early AI systems, most ML models are not directly interpretable and can be considered as a \u201cblack box\" [18], [3]. Hence, explainability in this latter instance can be seen as finding a more interpretable surrogate model approximating the original one [23]. Consequently, the most popular XAI methods often lack rigorous guarantees [24]. As an alternative to heuristic or informal techniques [25], [26], growing interest has been posed on formal XAI, which offers logic-driven methods for deriving explanations, by providing theoretical assurances [27]. Among these approaches, abduc- tive explanation [28] stands out as an argument-based local explanation, consisting of a minimal set of literals sufficient for predicting a class. Thus, it serves as a reason for assigning a class to an instance [28]. Moreover, runtime verification allows the explanation of AI-based self-adaptive systems, enabling the investigation of system behavior [29]. Finally, we cite XAI techniques built upon Al diagnosis principles, which involve identifying system faults or anomalies through logical reasoning and inference techniques [30]. However, the dichotomy between surrogates and formal explanations will be analyzed in Subsection V-A, as crucial for the discussion in relation to bona fide explanations.\nIn general, due to the vastness of the discussion, sev- eral criteria are introduced to classify explainability in ML literature. For instance, a separation is established between global or local methods, depending on whether their goal is to explain the whole model or a single prediction. Also, there is a distinction between model-specific and model-agnostic approaches, relying on the fact that the explanation applies to a single model (or a group), or all ML ones [9], [1].\nOther salient taxonomies distinguish between feature-based or example-based techniques [9] or between attribution, visu- alization, example-based, game theory, and knowledge extrac- tion explanations [2]. Most of the relevant models identified in the pertinent literature have been reported in Figure 1, among them is worth mentioning the Counterfactual Explanation [5], a model-agnostic method that shows what change in features should be done to determine a prediction switch. Additionally, there exists LIME [26], which uses a linear classifier for a"}, {"title": "B. Black Box-ness Insights from Science", "content": "The term \"black box\" is often employed to describe a model lacking interpretability, deemed antithetical to the principle of transparency, i.e., the property of an algorithm that is directly comprehensible [3]. However, the metaphorical notion of a \"black box\" has received considerable attention in a wider range of disciplines, including but not limited to, science, philosophy of science, and psychology. Its interpretive signifi- cance extends beyond the field of ML and comprises a variety of theoretical frameworks and intellectual pursuits [31], [32]. Hanson [32], for instance, introduced the concept of the \"black box\" as one of three stages in scientific development. Initially viewed as an algorithm with opaque internals, theories progress to a \"gray box\" stage where some structure is dis- cernible, and finally to a \"glass box\" stage, offering transparent insights across disciplines. Additionally, when it comes to the \"black box\" nature of a model, the issue of explanation also arises. Within this whole theoretical framework, the term \"black box\" is employed as a metaphorical device to connote the idea that the system in question is, in some sense, a closed entity whose internal workings are inaccessible to outside scrutiny. Both AI and science can be interpreted within this definition [31], [32], [3]."}, {"title": "C. Explanation Terminology Basics", "content": "Before presenting the centuries-long philosophical dialogue, we provide some basic philosophical terminology and con- cepts. Quoting Salmon:\nUnless we take preliminary steps to give some un- derstanding of the concept we are trying to explicate the explicandum any attempt to formulate an exact explication is apt to be wide of the mark [33].\nIt is commonly accepted that science aims for knowledge acquisition about the world, distinguishing itself from com- mon sense knowledge [34]. However, philosophical literature traditionally differentiates between two types of scientific knowledge, namely \"knowledge that\" and \"knowledge why.\u201d Indeed, the first concerns description, while the latter expla- nation [10]. In particular, an explanation, which provides a scientific understanding of the world, is typically divided into two components: the \u201cexplanandum\" and the \u201cexplanans.\u201d The former pertains to the statements regarding the event requiring an explanation, whereas the latter encompasses those used to provide them [35]. Another common concern relates to the nature of the phenomena requiring explanation, which can comprise individual events, general laws, or statistical regularities. According to Nagel [34], there are four distinct explanation patterns since \"why questions\u201d are not all of the same type. These include deductive, probabilistic, functional or teleological, and genetic models of explanations. In deduc- tive explanations, the \u201cexplanandum\" is a logically necessary consequence of the explanatory premises. Probabilistic expla- nations stem from statistical premises, addressing individual cases. Functional explanations indicate the instrumental roles a unit has in bringing about a goal within a system. Lastly, genetic explanations delineate the sequence of significant events leading from an earlier system to a later one."}, {"title": "IV. THE HISTORICAL EVOLUTION OF SCIENTIFIC EXPLANATION DEBATE", "content": "In this section, we aim to analyze the scientific explanation debate to acquire a comprehension of the issues and the philosophical foundations of explanation, providing useful insights into the multi-faced underpinnings of XAI discourse. Throughout the past discussions, a variety of positions have emerged within the epistemology framework, as well as analogous topics. For instance, consisting of the scarcity of accurate terminology or the challenges of selecting the optimal model for factoring in explanations [10]. However, systematic attempts to solve these issues have been proposed in the epistemological literature, offering fruitful philosophical in- sights for XAI. To establish a correlation between two distinct debates and to identify potential intersections, we categorize the epistemological discussion into three distinct eras, in relation to Hempel and Oppenheim's turning point proposal of the Deductive-Nomological (D-N) model [35]. These eras, namely the pre-Hempelian era, the received view, and the post-Hempelian era follow the chronological development. Our aim is, as illustrated in Figure 1, to highlight possible common trends and pivotal points of the discourses regarding the concerns raised."}, {"title": "A. Pre-Hempelian Era", "content": "Many of history's most eminent philosophers and scientists have questioned the nature of explanation and its role in science. However, it is not possible to answer by providing a unique definition. Instead, we should respond by starting from the very initial explorations. According to Aristotle [36], it is only when we know the causes, or \u201caitia,\" of something that we have an explanation for it, emphasizing the importance of explanation in response to \u201cwhy questions.\u201d Indeed,\nThe discussion of aitia, on the other hand, is rather a discussion of explanation, and the doctrine of the \"four causes\" is an attempt to distinguish and classify different kinds of explanation, different ex- planatory roles a factor can play [36].\nTo be more specific, Aristotle identified four causes, which are different types of answers to the \"why question,\" namely the material cause, the formal cause, the efficient cause, and the final cause. In the Aristotelian view, causality and explanation are intimately related and, as we will see, causation assumes a key role in numerous accounts of explanation. However, not all philosophers have supported the notions of causality and explanation. For instance, in Galileo Galilei's various scripts, it is possible to recognize strong positions against the existence\""}, {"title": "B. The Received View", "content": "In 1948, the work of Hempel and Oppenheim brought the concept of explanation to the forefront of the philosophy of science, marking a pivotal moment in the trajectory of future debates, to the extent that it is possible to distinguish between the philosophical inquiry that happened before Hempel, and that that occurred after. While their model is often regarded as the first attempt to incorporate explanation into scientific discourse, their true contribution was to propose a structured effort at the systematization of scientific explanation into the so-called Deductive-Nomological model [35]. The core of their model lies in subsuming the \"explanandum\" under general laws and statements about the conditions under which the phenomenon occurred, through deductive inference. Ac- cordingly, in a Hempelian context, to explain means to bring phenomena back into the realm of laws having empirical scope. An example would be helpful to have a better grasp of the Deductive-Nomological model. The \"explanandum\" consists of the description of the phenomenon to be explained, such as an oar underwater that appears bent upwards to an observer in a rowboat. The \u201cexplanans\u201d comprises both general laws (refraction, water optical density) and antecedent conditions (an oar part in the water and part in the air, an oar consisting of a straight piece of wood). Hence, the \"explanandum\" is logically deduced from the \u201cexplanans,\u201d thus the question \"Why does the phenomenon occur?\" is interpreted as \"What overarching principles and preceding circumstances lead to the phenomenon?\"\nNevertheless, not all scientific laws are explainable through deduction, such as probabilistic or statistical ones. Thus, Hempel [43] introduced a statistical systematization for scien- tific explanations, namely the Inductive-Statistical (I-S) model, recognizing the limitations of the Deductive-Nomological one. Hempel's I-S model is his natural way to extend the D- N model to statistical generalizations, remaining implicitly entrenched in the deductive ideal. Indeed, to explain means to express the probability of a given instance of F being an occasion of G, represented by the variable r. Hempel's I-S explanation must be tied to all available reference knowledge, as stated by his \u201cmaximal specificity\u201d requirement. The idea underlying this condition is the impossibility of genuine sta- tistical explanation, that defines them as epistemically rela- tive [10], and from which also Hempel derived the principle of ", "why questions": "nd, thus, terms like \u201ccomprehensible\u201d and \u201cunderstanding"}, {"title": "C. Post-Hempelian Era", "content": "After Hempel's \u201creceived view\u201d a certain amount of formal and semi-formal models were proposed by different authors. Indeed, post-Hempelian scholars mainly rejected his concep- tion of explanation and started from attacks on his model to build new interpretations.\na) Statistical Relevance Model.: Salmon [45] moved from criticism about the inferential structure of explanation and proposed the Statistical-Relevance (S-R) model, which contemplates a specific idea of probabilistic causation. In his conception, explanations must consider not only events that respect the principle of \"high inductive probability\" but also unlikely ones. Statistical relevance determines to which homogeneous reference class the single event belongs. To establish homogeneity, the method involves partitioning non- homogeneous reference classes into maximal homogeneous sub-classes, which are mutually exclusive and comprehensive for the initial class. Thus, to explain means to place the \"ex- planandum\" in a chain of correlations expressed by statistical generalizations, that constitute the reference class meeting the maximal homogeneity criterion. A satisfactory theory of explanation should assign a fundamental role to causality,"}, {"title": "", "content": "and, although statistical explanations are often discussed in seemingly indeterministic contexts, this does not negate the possibility of finding causal connections [10].\nb) The Pragmatics of Explanation.: Van Fraassen [46], unsatisfied with Salmon's and previous accounts, introduced a pragmatic view of explanation. While the neo-positivist perspective was mainly concerned with establishing measures for verifying the validity of a scientific theory, such as its truth- fulness or empirical adequacy, this view aims to determine the relevant part of a scientific fact by considering the contextual information, which relates to the knowledge and interests of the subject who posits the \"why question.\u201d Van Fraassen began by examining requests for specific \"why questions,\u201d which are comprised of a triplet Q = \u3008Pk, X, R), namely, the topic, the antithesis class, and the relevance relation. The latter connects the informative part of the answer with the components of the question [47].\nc) The Unificationist View.: As the debate progresses, the importance of contextual elements in explanation increases. Friedman's Unificationist view [48] explored the feasibility of an objective conceptualization of scientific understanding, in seeking to clarify what is in the relationship between phenom- ena that determine one as the explanation of the other. The explanation process is not merely a substitution of one casual phenomenon. Rather, it involves replacing less comprehensive phenomena with more comprehensive ones, by reducing the number of independent events and enhancing our global under- standing of the world. Indeed, unification is the element of the explanation relation that produces understanding. Kitcher [49] proposed the most articulated Unificationist view, which posits that scientific activity aims to unify accepted knowledge, through general laws. Scientific understanding is achieved not by explaining individual occurrences, but by providing increasingly larger frameworks to fit them systematically.\nd) Abductive Explanation.: The term \"abduction,", "inference to the best expla- nation [50],\" originated with Peirce [51], who introduced it to signify a type of reasoning distinct from deduction, although not induction. Abduction is a type of nonmonotonic reasoning [52] (i.e., defeasible inference) and consists of the process of forming explanatory hypotheses given a certain scenario [51]. The concept posits that when confronted with a phenomenon if one explanation emerges that plausibly accounts for the otherwise inexplicable, it is reasonable to lean towards accepting that explanation as likely correct [51]. After its first appearance, different formalizations have been suggested, taking the name of logic-based abduction, which is particularly suitable if complex causal relationships pre- vail [52]. However, the idea of inference to the best ex- planation is met with resistance in the field philosophy of science, as this kind of inference presupposes the truth of the explanatory premises [10]. Indeed, what may be selected as the best explanation, could be within a group of incorrect ones [47]. Moreover, this kind of explanation leaves open the role of pragmatic components for the selection of the best explanation for different individuals [47].\ne) Neo-Mechanistic Theories.: The Unificationist theory proposed by Kitcher [49] sees explanation as global and, by referring to general laws, employs a top-down approach. On the other hand, causal-mechanical theories such as that advanced by Salmon [45] employ a bottom-up approach and aim to describe the causal relationships involved in the phenomenon being explained [10]. This type of explanatory knowledge seeks to provide understanding by showing the inner mechanism of phenomena of the world, that is, by exploring the internal workings of things, making it possible to open the \\\"black box\\\" of nature. During the '90s this account served as inspiration for neo-mechanistic theories, that proposed a more applicable view of causality aimed to identify mechanistic links [10], in a conception of causality understood as productivity. Among the most relevant, it is possible to encounter Glennan's [53] Complex System account, in which a mechanism consists of various behaviors comprising multiple components that can be separately analyzed and decomposed into smaller subsets. Additionally, the system's parts should exhibit a notable degree of robustness or stability. In other words, their properties should remain relatively constant in the absence of external interventions. A good explanation is made of an \u201cexplanandum,": "hich is the description of the phenomena to be explained, and an \u201cexplanans,\u201d which is the inner mechanistic description. A different account comes from Bechtel and Abrahamsen [54], which proposed the Decomposition and Localization model. Following their perspective, a mechanism is a structure that fulfills a function based on its constituent parts, its operations, and the overall organization. Moreover, according to the authors, due to the epistemic character of explanations, representations, such as diagrams and verbal or linguistic descriptions, can support the inner mechanisms of nature.\nf) Counterfactual Explanation.: In recent decades, a new type of approach to causality for explanation has gained pop- ularity, namely the \u201cinterventionist perspective", "needed": "counterfactuals and contrastive explanations are not synonymous, although they are often used as interchangeable terms [8]. The counterfactual explanation states that causal relations exist only if intervening on the cause C, produces a change in the effect E, remaining unchanged the relationship between the two variables [56]. On the other hand, contrastive explanations answer the question \"Why x rather than y?", "Why x?": 8}, {"title": "V. A COMPARISON OF EXPLAINABILITY DEBATES THROUGH AN EPISTEMOLOGICAL LENS", "content": "Ultimately, we possess all the necessary tools to draw the analogy between scientific explanation and XAI debates, by looking at their pattern of development, as shown in Figure 1. As we noted, explanations were not initially accepted as distinct goals of science, since separate from description or prediction, in either realm. Indeed, in the domain of the philosophy of science, the acceptance of scientific explanations did not manifest uniformly from the origins of the debate, as various philosophers originally rejected the idea of considering them a distinct objective of science, favoring description [10]. Over centuries, there has been a transition from discordant perspectives toward a major consensus, culminating in the pro- posal of diverse models for explanation. Analogously, within the discourse on XAI, explanations were not initially regarded as primary objectives of AI models, which predominantly sought predictive capabilities while prioritizing high accu- racy [2]. This is also known as the interpretability/accuracy trade-off where the quest for improved predictive performance often comes at the cost of reduced model interpretability. This relationship has traditionally been viewed as mutually exclu- sive; however, this notion has been increasingly contested by several scholars that argue for optimization between both [2]. Thus, it is possible and worth claiming, that the urgency for explanation did come after the need for accurate prediction and description in both the field of AI and the philosophy of science [2], [10].\nMoreover, we discerned a gradual change from logic- deductive models of explanation to statistical ones in both domains, as we witnessed a shift from certainty to uncertainty. Hempel's Deductive-Nomological model seeks explanations, by deducing from causal (or deterministic laws) [35]. Addi- tionally, in Hempel's [35] first scripts, causal laws overlapped in their meaning with non-statistical laws, and although he recognized the existence of the latter, he restricted his account of explanation to the deductive ones. On the other hand, mov- ing progressively forward in time, if we look into mechanistic or neo-mechanistic explanations, we encounter a progressive consideration of statistical relationships, while not losing the importance of causal connections. As Salmon states:\nIf indeterminism is true, some explanations will be irreducibly statistical-that is, they will be full-blooded explanations whose statistical character re- sults not merely from limitations of our knowl- edge [10].\nAs it is highlighted in Figure 1, if we move toward XAI, an interesting analogy emerges: the very first deductive expert systems, having rule-based knowledge, were directly inter- pretable and their explanation consisted of an inference of the output from the rules [20]. However, most ML mod- els work as \"black boxes\" and their knowledge is opaque, so they don't reveal sufficient details about their internal behavior [3]. For this reason, differently from early rule- based systems, explainability in ML often seeks to find an interpretable model that approximates the original one, by finding statistical correlations [23] (e.g., many explainability methods offer summary statistics for each feature, such as feature importance [2]). However, genuine causal relationships must be preserved [10]. Moreover, manipulative-counterfactual approaches to explanation have increasingly gained popularity in both debates. On the side of scientific explanation, by advancing an intervention-centered notion of causality [55]. While, on the other of AI, showing what should have been different to change the decision of the system. Specifically, consisting of the smallest change that can be made to a particular instance to get a different decision from the AI [5].\nLastly, a typical distinction found in XAI literature is within the categorization of global and local explanations, the first ones aimed to explain the knowledge of general patterns of the system as a whole, while the latter, a single decision [6]. As underlined in Figure 1, scientific explanation, in a broader sense, sees patterns of explanation underlying a similar dis- tinction between top-down and bottom-up accounts. The first one is, in this sense, global, as it relates to the structure of the whole world [10]. The second one, as can be seen very well in Bechtel and Abrahamsen's account [54], aims to identify the relationships and explanations of individual parts."}, {"title": "A. Going Deeper: Concepts of Explainability", "content": "In addition to establishing connections between the two debates as a whole, it is possible to examine analogies between related concepts and common terminology that we identified in our comparison of scientific explanation and XAI. This anal- ysis considers preliminary epistemological implications that are relevant within the XAI domain, such as the relationship between explanation and understanding, the significance of similarity in explanation, and the desiderata of good expla- nations, thereby laying foundational groundwork for future research.\na) The Epistemological Relation between Explaining and Understanding.: The earliest theories of scientific explanation, proposed by the analytical philosophical tradition, were not concerned with understanding, as they claimed that it was not part of the explanation relation. According to Hempel [35], a scientific explanation is restricted to deductive and log- ical inference, by which science answers \"why questions\u201d and, thus, he considered terms like \u201ccomprehensible\u201d and \"understanding\" out of its domain [44]. However, with the evolution of the debate, pragmatic factors have been taken into consideration increasingly, appearing awareness of the fact that an explanation should be considered with refer- ence to a specific question [46]. Hence, an explanation is not decontextualized but pertains to the situation in which questions and answers are posed. On the other hand, the XAI field has started to progressively consider the importance"}, {"title": "", "content": "of a diverse pool of users and different stakeholders when providing explanations [2], [9], determining the appearance of terms such as \u201cinterpretability,\" and \"understandability,", "Models.": "Ex- planation of ML often consists of adopting a surrogate and interpretable model, such as linear regression, that should pro- vide representations necessary to obtain understanding [23]. However, a relevant issue is establishing why this surrogate serves as an explanation of the original model. Indeed, for any XAI model, there should be a formal linkage, such as iso- morphism or similarity, between it and the initial model [12]. Nevertheless, the majority of surrogate models used currently lack rigorous assurances, raising uncertainties about the effi- cacy of these approximations in elucidating decision-making processes [24]. On the other hand, formal explanations seek to establish guarantees or justifications with respect to the determined explanation [58], [27], such as Random Forest explanations with SAT [27] or abductive explanations [28]. Without this type of connection, there is no basis to state that an explanation provided by the XAI model applies to a \"black box\" [12], [24]. Similarly, some philosophers of science have argued that understanding can given by familiarity, in which the \"explanans\u201d is an approximation similar to the \u201cexplanan- dum", "inadequate": "being familiar gives no grounds for being understood and, regardless some explanations might evoke a feeling of familiarity, this is not a relevant factor in sound explanations [48], [35].\nc) Bona Fide Explanations Criteria.: Also as a con- sequence of the aforementioned considerations, researchers in both epistemology and the XAI domains have sought to identify the characteristics that distinguish bona fide explana- tions, i.e., explanations should satisfy certain requirements to be considered valid [42], [12], [10]. For instance, Miller has done incipient work in establishing criteria to evaluate XAI, by deriving principles from social sciences [7]. Moreover, Mueller et al. [13] provided an exhaustive list of principles that emerged within XAI literature. Within the epistemological domain, Hempel [43], [35] introduced the principle of factual- ity, namely that the \u201cexplanans\u201d and the \u201cexplanandum\u201d must be true. Conversely, a potential explanation possesses all the essential characteristics of a valid explanation, except for the truth [43]. Carnap [42] identified four criteria for explanations: similarity to the \u201cexplicandum,\u201d exactness, fruitfulness, and simplicity. Specifically, similarity to the", "explicandum": "efers to the necessity of the", "explicatum": "o adequately correspond to the", "expli-{\n    ": "itle", "Understanding XAI Through the Philosopher's Lens: A Historical Perspective": "authors", "Martina Mattioli": "Antonio Emanuele Cin\u00e0", "Marcello Pelillo": "abstract", "Despite explainable AI (XAI) has recently become a hot topic and several different approaches have been developed, there is still a widespread belief that it lacks a convincing unifying foundation. On the other hand, over the past centuries, the very concept of explanation has been the subject of extensive philosophical analysis in an attempt to address the fundamental question of \"why\" in the context of scientific law. However, this discussion has rarely been connected with XAI. This paper tries to fill in this gap and aims to explore the concept of explanation in AI through an epistemological lens. By comparing the historical development of both the philosophy of science and AI, an intriguing picture emerges. Specifically, we show that a gradual progression has independently occurred in both domains from logical-deductive to statistical models of explanation, thereby experiencing in both cases a paradigm shift from deterministic to nondeterministic and probabilistic causality. Interestingly, we also notice that similar concepts have independently emerged in both realms such as, for example, the relation between explanation and understanding and the importance of pragmatic factors. Our study aims to be the first step towards understanding the philosophical underpinnings of the notion of explanation in AI, and we hope that our findings will shed some fresh light on the elusive nature of XA\u0399.": "sections"}, {"title": "I. INTRODUCTION", "content": "Artificial Intelligence (AI) is becoming progressively a pervasive technology in our daily lives as a result of its in- creasing accuracy and versatility [1]. Despite that, the growing integration of AI into human lives has determined a rising ur- gency to enlighten some of its potential undesirable outcomes. Consequently, its employment, particularly in contexts with paramount ethical considerations [2], has led to the necessity for a fair decision-making process [3], [4]. These reflections have determined a variety of discourses about people's right to have an explanation of how the decision is reached by the machine, especially when the methods used are conceived as \"black boxes\" [5]. As a result of these considerations, scholars have posed various questions around, for example, when explanations are required, what models provide such explanations, what are the desiderata necessary to achieve understanding [6], and what are the characteristics of a good explanation [2], [7], [4]. Within this debate, XAI is typically referred to as:\nThe process of elucidating or revealing the decision- making mechanisms of models. The user may see how inputs and outputs are mathematically inter- linked. It relates to the ability to understand why Al models make their decisions [2].\nNevertheless, defining explainability within the borders of a unique definition, amidst the plethora of those proposed, is a daunting task. Indeed, the majority of the aforementioned questions remain partially unresolved, to the extent that the precise definition of \u201cexplanation\" remains to some degree obscure [6]. Specifically, some authors contend that the on- going discussion on explainable AI lacks a well-defined the- oretical goal [8]. They argue that the concept of explanation, along with its related notions (e.g., interpretability [3]), is ambiguously defined, thus fostering the perception that there is no cohesive and convincing conceptual foundation [6], [3]. Additionally, it is worth noting that the variety of XA\u0399 models proposed is constantly evolving, which underscores the dynamic nature of this field [2] and its non-monolithic character [3]. Indeed, abundant recent attempts have been made to classify and systematize these models (refer to [2], [9], [1] for in-depth surveys), reflecting a growing interest in XAI and the need for a more structured approach to its development [2].\nDespite this, the discourse surrounding explainability is not novel and has been explored in various contexts [10]. Shifting our attention to the different realms of epistemology, this paper shows that analogous debates or inquiries arise. Indeed, the study of explanation has been a focal point of extensive philosophical analyses, undertaken to systematically address the fundamental question of \u201cwhy\u201d in the context of scientific law, thus unveiling one of the most substantial chapters in the philosophy of science [10]. This discussion has a remark- able history, and its roots extend back to the philosophy of Aristotle, who distinguished between two types of knowledge: \"knowledge that\" and \"knowledge why,\" to wit description and explanation [10]. Additionally, this distinction has become increasingly systematized over the past century, with a growing emphasis in scholarly discourses on the delineation and the proposal of a vast number of explanation models [10].\nAcknowledging the significance of the epistemological dis- course and the substantial inputs from philosophers in this domain [10], this paper investigates parallels and establishes"}, {"title": "II. RELATED WORKS", "content": "In this section, we focus on incipient interdisciplinary efforts that have been done to connect and analyze psychological, sociological, and philosophical aspects of explanations. How- ever, it should be emphasized that, to the best of our knowl- edge, no attempt has yet been made in the literature to link systematically the debates on XAI and scientific explanation.\na) Previous Philosophical Contributions.: Pioneering work in establishing connections between philosophy and XAI has been conducted by P\u00e1ez [8]. The author elucidates the relationship between understanding and explanation both in the realm of scientific explanation and XAI. Subsequently, McDonnell [11] provides some lessons from philosophy to assess better explanations. More specifically, his three primary observations include the necessity of a contrastive structure, the importance of focusing on actionable interventions, and the idea that robust causal dependence enhances the effec- tiveness of an explanation. Dur\u00e1n [12] claims that scientific explanations are furnished with a precise structure aimed at providing a comprehensive understanding of the world. Also, his paper asserts that current XAI models do not qualify as genuine explanations. Finally, O'Hara [4] clarifies the rela- tionship between explanation and understanding, establishing a connection with the decision process.\nb) Explanation and Social Aspects.: A segment of the present literature has directed its attention towards social attributes of explanations, linking them with XAI. Miller [7] affirms that insights about humanities can benefit XAI. He emphasizes that explanations are contrastive, social, and se- lected in a biased manner and also that causal relations are more influential than probabilities. Mueller et al. [13] claim that there is a necessity for human-inspired XAI guidelines,\nas psychological principles often remain underestimated. Hoff- man et al. [14] assert that explanations are not properties of statements, but result from interactions. In fact, what qualifies as an explanation depends on the learner's needs, previous knowledge, and goals.\nc) The Call for Clarification.: Several authors called for clarity, remarking on the need for greater rigor in the definition of explainability and related concepts. Lipton [3] considers the term interpretability as slippery and ill-defined. P\u00e1ez [8] argues that explanatory strategies may lack a precisely defined theoretical purpose. Boge and Poznic [15] affirm that discussions on the philosophy of science could benefit ML. They emphasize the significant connections between these two disciplines and assert that the development of XAI could become a crucial theme in the philosophy of science."}, {"title": "III. PHILOSOPHICAL ROOTS OF EXPLANATION", "content": "Embedded in centuries of philosophical inquiry, the funda- mental concept of explanation has an extremely long tradition and ancient roots, which can shed light on the actual discussion on XAI. In particular, during the last decade, epistemology has been involved in a lively debate about scientific explana- tion, in which philosophers have meticulously delineated its constituents, seeking a precise definition while also reflecting upon the criteria of good explanations and discussing which particular model should be preferred to achieve them [10]. As we aim to assert, the term \"explanation\" carries conno- tations and meanings that can be transposed to the current discussion regarding XAI, but that are beyond its common- sense definition or recent discussions, owing to the depth of the philosophical tradition in which it has been expounded. Additionally, the relationship between scientific explanation and XAI is relevant not only because of potential parallels in the philosophical discourse or overlapping terminology in both debates. It also arises from a broader conception that originates from the proximity of some AI fields, including ML, with the scientific inquiry [16]. This closeness may contribute to extending and reinterpreting some of the implications of explanation, through a philosophy of science lens. Indeed, various authors provide valuable philosophical insights into these issues, contending that Pattern Recognition (PR) and ML are inherently aligned with scientific endeavors in their pursuit [17], [16]. This correspondence is evident in their pursuit to address similar questions related to categorization, causality, generalization, the problem of induction, or other pivotal aspects [17]. Similarly, the concept of explanation can benefit from a philosophical perspective. However, before we delve into the depths of explainability, this section introduces some notions that may help elucidate the presentation of our parallelism within the context of the philosophy of science, in- cluding a brief presentation of the XAI debate, considerations regarding science as a \"black box,\" and pertinent philosophical terminology that will serve as a foundation to initially outline the concept of explanation."}, {"title": "A. Short History of XAI", "content": "Although the debate on explainability has recently gained prominence, particularly after the introduction of the right to explanation within the GDPR [2], [5], this concept traces back to the early days of expert systems [18]. For instance, MYCIN [19] is a rule-based expert system, developed to help doctors select antimicrobial therapy. It includes a general ques- tion answerer and a status checker, enabling the physician to understand both the program's advice and its reasoning. This type of system is grounded in a hypothetico-deductive strategy and exhaustively applies inference rules [20], implying deter- minism [21] and making the models easily interpretable [18]. REX [22] consists of a knowledge-based explanation system and a knowledge-based problem-solving system, in alignment with the existing epistemological separation between \u201cknowl- edge that\" and \"knowledge why.\" It offers explanations of how an expert system progresses from specific data to a final output. Differently from early AI systems, most ML models are not directly interpretable and can be considered as a \u201cblack box\" [18], [3]. Hence, explainability in this latter instance can be seen as finding a more interpretable surrogate model approximating the original one [23]. Consequently, the most popular XAI methods often lack rigorous guarantees [24]. As an alternative to heuristic or informal techniques [25], [26], growing interest has been posed on formal XAI, which offers logic-driven methods for deriving explanations, by providing theoretical assurances [27]. Among these approaches, abduc- tive explanation [28] stands out as an argument-based local explanation, consisting of a minimal set of literals sufficient for predicting a class. Thus, it serves as a reason for assigning a class to an instance [28]. Moreover, runtime verification allows the explanation of AI-based self-adaptive systems, enabling the investigation of system behavior [29]. Finally, we cite XAI techniques built upon Al diagnosis principles, which involve identifying system faults or anomalies through logical reasoning and inference techniques [30]. However, the dichotomy between surrogates and formal explanations will be analyzed in Subsection V-A, as crucial for the discussion in relation to bona fide explanations.\nIn general, due to the vastness of the discussion, sev- eral criteria are introduced to classify explainability in ML literature. For instance, a separation is established between global or local methods, depending on whether their goal is to explain the whole model or a single prediction. Also, there is a distinction between model-specific and model-agnostic approaches, relying on the fact that the explanation applies to a single model (or a group), or all ML ones [9], [1].\nOther salient taxonomies distinguish between feature-based or example-based techniques [9] or between attribution, visu- alization, example-based, game theory, and knowledge extrac- tion explanations [2]. Most of the relevant models identified in the pertinent literature have been reported in Figure 1, among them is worth mentioning the Counterfactual Explanation [5], a model-agnostic method that shows what change in features should be done to determine a prediction switch. Additionally, there exists LIME [26], which uses a linear classifier for a"}, {"title": "B. Black Box-ness Insights from Science", "content": "The term \"black box\" is often employed to describe a model lacking interpretability, deemed antithetical to the principle of transparency, i.e., the property of an algorithm that is directly comprehensible [3]. However, the metaphorical notion of a \"black box\" has received considerable attention in a wider range of disciplines, including but not limited to, science, philosophy of science, and psychology. Its interpretive signifi- cance extends beyond the field of ML and comprises a variety of theoretical frameworks and intellectual pursuits [31], [32]. Hanson [32], for instance, introduced the concept of the \"black box\" as one of three stages in scientific development. Initially viewed as an algorithm with opaque internals, theories progress to a \"gray box\" stage where some structure is dis- cernible, and finally to a \"glass box\" stage, offering transparent insights across disciplines. Additionally, when it comes to the \"black box\" nature of a model, the issue of explanation also arises. Within this whole theoretical framework, the term \"black box\" is employed as a metaphorical device to connote the idea that the system in question is, in some sense, a closed entity whose internal workings are inaccessible to outside scrutiny. Both AI and science can be interpreted within this definition [31], [32], [3]."}, {"title": "C. Explanation Terminology Basics", "content": "Before presenting the centuries-long philosophical dialogue, we provide some basic philosophical terminology and con- cepts. Quoting Salmon:\nUnless we take preliminary steps to give some un- derstanding of the concept we are trying to explicate the explicandum any attempt to formulate an exact explication is apt to be wide of the mark [33].\nIt is commonly accepted that science aims for knowledge acquisition about the world, distinguishing itself from com- mon sense knowledge [34]. However, philosophical literature traditionally differentiates between two types of scientific knowledge, namely \"knowledge that\" and \"knowledge why.\u201d Indeed, the first concerns description, while the latter expla- nation [10]. In particular, an explanation, which provides a scientific understanding of the world, is typically divided into two components: the \u201cexplanandum\" and the \u201cexplanans.\u201d The former pertains to the statements regarding the event requiring an explanation, whereas the latter encompasses those used to provide them [35]. Another common concern relates to the nature of the phenomena requiring explanation, which can comprise individual events, general laws, or statistical regularities. According to Nagel [34], there are four distinct explanation patterns since \"why questions\u201d are not all of the same type. These include deductive, probabilistic, functional or teleological, and genetic models of explanations. In deduc- tive explanations, the \u201cexplanandum\" is a logically necessary consequence of the explanatory premises. Probabilistic expla- nations stem from statistical premises, addressing individual cases. Functional explanations indicate the instrumental roles a unit has in bringing about a goal within a system. Lastly, genetic explanations delineate the sequence of significant events leading from an earlier system to a later one."}, {"title": "IV. THE HISTORICAL EVOLUTION OF SCIENTIFIC EXPLANATION DEBATE", "content": "In this section, we aim to analyze the scientific explanation debate to acquire a comprehension of the issues and the philosophical foundations of explanation, providing useful insights into the multi-faced underpinnings of XAI discourse. Throughout the past discussions, a variety of positions have emerged within the epistemology framework, as well as analogous topics. For instance, consisting of the scarcity of accurate terminology or the challenges of selecting the optimal model for factoring in explanations [10]. However, systematic attempts to solve these issues have been proposed in the epistemological literature, offering fruitful philosophical in- sights for XAI. To establish a correlation between two distinct debates and to identify potential intersections, we categorize the epistemological discussion into three distinct eras, in relation to Hempel and Oppenheim's turning point proposal of the Deductive-Nomological (D-N) model [35]. These eras, namely the pre-Hempelian era, the received view, and the post-Hempelian era follow the chronological development. Our aim is, as illustrated in Figure 1, to highlight possible common trends and pivotal points of the discourses regarding the concerns raised."}, {"title": "A. Pre-Hempelian Era", "content": "Many of history's most eminent philosophers and scientists have questioned the nature of explanation and its role in science. However, it is not possible to answer by providing a unique definition. Instead, we should respond by starting from the very initial explorations. According to Aristotle [36], it is only when we know the causes, or \u201caitia,\" of something that we have an explanation for it, emphasizing the importance of explanation in response to \u201cwhy questions.\u201d Indeed,\nThe discussion of aitia, on the other hand, is rather a discussion of explanation, and the doctrine of the \"four causes\" is an attempt to distinguish and classify different kinds of explanation, different ex- planatory roles a factor can play [36].\nTo be more specific, Aristotle identified four causes, which are different types of answers to the \"why question,\" namely the material cause, the formal cause, the efficient cause, and the final cause. In the Aristotelian view, causality and explanation are intimately related and, as we will see, causation assumes a key role in numerous accounts of explanation. However, not all philosophers have supported the notions of causality and explanation. For instance, in Galileo Galilei's various scripts, it is possible to recognize strong positions against the existence\""}, {"title": "B. The Received View", "content": "In 1948, the work of Hempel and Oppenheim brought the concept of explanation to the forefront of the philosophy of science, marking a pivotal moment in the trajectory of future debates, to the extent that it is possible to distinguish between the philosophical inquiry that happened before Hempel, and that that occurred after. While their model is often regarded as the first attempt to incorporate explanation into scientific discourse, their true contribution was to propose a structured effort at the systematization of scientific explanation into the so-called Deductive-Nomological model [35]. The core of their model lies in subsuming the \"explanandum\" under general laws and statements about the conditions under which the phenomenon occurred, through deductive inference. Ac- cordingly, in a Hempelian context, to explain means to bring phenomena back into the realm of laws having empirical scope. An example would be helpful to have a better grasp of the Deductive-Nomological model. The \"explanandum\" consists of the description of the phenomenon to be explained, such as an oar underwater that appears bent upwards to an observer in a rowboat. The \u201cexplanans\u201d comprises both general laws (refraction, water optical density) and antecedent conditions (an oar part in the water and part in the air, an oar consisting of a straight piece of wood). Hence, the \"explanandum\" is logically deduced from the \u201cexplanans,\u201d thus the question \"Why does the phenomenon occur?\" is interpreted as \"What overarching principles and preceding circumstances lead to the phenomenon?\"\nNevertheless, not all scientific laws are explainable through deduction, such as probabilistic or statistical ones. Thus, Hempel [43] introduced a statistical systematization for scien- tific explanations, namely the Inductive-Statistical (I-S) model, recognizing the limitations of the Deductive-Nomological one. Hempel's I-S model is his natural way to extend the D- N model to statistical generalizations, remaining implicitly entrenched in the deductive ideal. Indeed, to explain means to express the probability of a given instance of F being an occasion of G, represented by the variable r. Hempel's I-S explanation must be tied to all available reference knowledge, as stated by his \u201cmaximal specificity\u201d requirement. The idea underlying this condition is the impossibility of genuine sta- tistical explanation, that defines them as epistemically rela- tive [10], and from which also Hempel derived the principle of ", "why questions": "nd, thus, terms like \u201ccomprehensible\u201d and \u201cunderstanding"}, {"title": "C. Post-Hempelian Era", "content": "After Hempel's \u201creceived view\u201d a certain amount of formal and semi-formal models were proposed by different authors. Indeed, post-Hempelian scholars mainly rejected his concep- tion of explanation and started from attacks on his model to build new interpretations.\na) Statistical Relevance Model.: Salmon [45] moved from criticism about the inferential structure of explanation and proposed the Statistical-Relevance (S-R) model, which contemplates a specific idea of probabilistic causation. In his conception, explanations must consider not only events that respect the principle of \"high inductive probability\" but also unlikely ones. Statistical relevance determines to which homogeneous reference class the single event belongs. To establish homogeneity, the method involves partitioning non- homogeneous reference classes into maximal homogeneous sub-classes, which are mutually exclusive and comprehensive for the initial class. Thus, to explain means to place the \"ex- planandum\" in a chain of correlations expressed by statistical generalizations, that constitute the reference class meeting the maximal homogeneity criterion. A satisfactory theory of explanation should assign a fundamental role to causality,\""}, {"title": "", "content": "and, although statistical explanations are often discussed in seemingly indeterministic contexts, this does not negate the possibility of finding causal connections [10].\nb) The Pragmatics of Explanation.: Van Fraassen [46], unsatisfied with Salmon's and previous accounts, introduced a pragmatic view of explanation. While the neo-positivist perspective was mainly concerned with establishing measures for verifying the validity of a scientific theory, such as its truth- fulness or empirical adequacy, this view aims to determine the relevant part of a scientific fact by considering the contextual information, which relates to the knowledge and interests of the subject who posits the \"why question.\u201d Van Fraassen began by examining requests for specific \"why questions,\u201d which are comprised of a triplet Q = \u3008Pk, X, R), namely, the topic, the antithesis class, and the relevance relation. The latter connects the informative part of the answer with the components of the question [47].\nc) The Unificationist View.: As the debate progresses, the importance of contextual elements in explanation increases. Friedman's Unificationist view [48] explored the feasibility of an objective conceptualization of scientific understanding, in seeking to clarify what is in the relationship between phenom- ena that determine one as the explanation of the other. The explanation process is not merely a substitution of one casual phenomenon. Rather, it involves replacing less comprehensive phenomena with more comprehensive ones, by reducing the number of independent events and enhancing our global under- standing of the world. Indeed, unification is the element of the explanation relation that produces understanding. Kitcher [49] proposed the most articulated Unificationist view, which posits that scientific activity aims to unify accepted knowledge, through general laws. Scientific understanding is achieved not by explaining individual occurrences, but by providing increasingly larger frameworks to fit them systematically.\nd) Abductive Explanation.: The term \"abduction,", "inference to the best expla- nation [50],\" originated with Peirce [51], who introduced it to signify a type of reasoning distinct from deduction, although not induction. Abduction is a type of nonmonotonic reasoning [52] (i.e., defeasible inference) and consists of the process of forming explanatory hypotheses given a certain scenario [51]. The concept posits that when confronted with a phenomenon if one explanation emerges that plausibly accounts for the otherwise inexplicable, it is reasonable to lean towards accepting that explanation as likely correct [51]. After its first appearance, different formalizations have been suggested, taking the name of logic-based abduction, which is particularly suitable if complex causal relationships pre- vail [52]. However, the idea of inference to the best ex- planation is met with resistance in the field philosophy of science, as this kind of inference presupposes the truth of the explanatory premises [10]. Indeed, what may be selected as the best explanation, could be within a group of incorrect ones [47]. Moreover, this kind of explanation leaves open the role of pragmatic components for the selection of the best explanation for different individuals [47].\ne) Neo-Mechanistic Theories.: The Unificationist theory proposed by Kitcher [49] sees explanation as global and, by referring to general laws, employs a top-down approach. On the other hand, causal-mechanical theories such as that advanced by Salmon [45] employ a bottom-up approach and aim to describe the causal relationships involved in the phenomenon being explained [10]. This type of explanatory knowledge seeks to provide understanding by showing the inner mechanism of phenomena of the world, that is, by exploring the internal workings of things, making it possible to open the \\\"black box\\\" of nature. During the '90s this account served as inspiration for neo-mechanistic theories, that proposed a more applicable view of causality aimed to identify mechanistic links [10], in a conception of causality understood as productivity. Among the most relevant, it is possible to encounter Glennan's [53] Complex System account, in which a mechanism consists of various behaviors comprising multiple components that can be separately analyzed and decomposed into smaller subsets. Additionally, the system's parts should exhibit a notable degree of robustness or stability. In other words, their properties should remain relatively constant in the absence of external interventions. A good explanation is made of an \u201cexplanandum,": "hich is the description of the phenomena to be explained, and an \u201cexplanans,\u201d which is the inner mechanistic description. A different account comes from Bechtel and Abrahamsen [54], which proposed the Decomposition and Localization model. Following their perspective, a mechanism is a structure that fulfills a function based on its constituent parts, its operations, and the overall organization. Moreover, according to the authors, due to the epistemic character of explanations, representations, such as diagrams and verbal or linguistic descriptions, can support the inner mechanisms of nature.\nf) Counterfactual Explanation.: In recent decades, a new type of approach to causality for explanation has gained pop- ularity, namely the \u201cinterventionist perspective", "needed": "counterfactuals and contrastive explanations are not synonymous, although they are often used as interchangeable terms [8]. The counterfactual explanation states that causal relations exist only if intervening on the cause C, produces a change in the effect E, remaining unchanged the relationship between the two variables [56]. On the other hand, contrastive explanations answer the question \"Why x rather than y?", "Why x?": 8}, {"title": "V. A COMPARISON OF EXPLAINABILITY DEBATES THROUGH AN EPISTEMOLOGICAL LENS", "content": "Ultimately, we possess all the necessary tools to draw the analogy between scientific explanation and XAI debates, by looking at their pattern of development, as shown in Figure 1. As we noted, explanations were not initially accepted as distinct goals of science, since separate from description or prediction, in either realm. Indeed, in the domain of the philosophy of science, the acceptance of scientific explanations did not manifest uniformly from the origins of the debate, as various philosophers originally rejected the idea of considering them a distinct objective of science, favoring description [10]. Over centuries, there has been a transition from discordant perspectives toward a major consensus, culminating in the pro- posal of diverse models for explanation. Analogously, within the discourse on XAI, explanations were not initially regarded as primary objectives of AI models, which predominantly sought predictive capabilities while prioritizing high accu- racy [2]. This is also known as the interpretability/accuracy trade-off where the quest for improved predictive performance often comes at the cost of reduced model interpretability. This relationship has traditionally been viewed as mutually exclu- sive; however, this notion has been increasingly contested by several scholars that argue for optimization between both [2]. Thus, it is possible and worth claiming, that the urgency for explanation did come after the need for accurate prediction and description in both the field of AI and the philosophy of science [2], [10].\nMoreover, we discerned a gradual change from logic- deductive models of explanation to statistical ones in both domains, as we witnessed a shift from certainty to uncertainty. Hempel's Deductive-Nomological model seeks explanations, by deducing from causal (or deterministic laws) [35]. Addi- tionally, in Hempel's [35] first scripts, causal laws overlapped in their meaning with non-statistical laws, and although he recognized the existence of the latter, he restricted his account of explanation to the deductive ones. On the other hand, mov- ing progressively forward in time, if we look into mechanistic or neo-mechanistic explanations, we encounter a progressive consideration of statistical relationships, while not losing the importance of causal connections. As Salmon states:\nIf indeterminism is true, some explanations will be irreducibly statistical-that is, they will be full-blooded explanations whose statistical character re- sults not merely from limitations of our knowl- edge [10].\nAs it is highlighted in Figure 1, if we move toward XAI, an interesting analogy emerges: the very first deductive expert systems, having rule-based knowledge, were directly inter- pretable and their explanation consisted of an inference of the output from the rules [20]. However, most ML mod- els work as \"black boxes\" and their knowledge is opaque, so they don't reveal sufficient details about their internal behavior [3]. For this reason, differently from early rule- based systems, explainability in ML often seeks to find an interpretable model that approximates the original one, by finding statistical correlations [23] (e.g., many explainability methods offer summary statistics for each feature, such as feature importance [2]). However, genuine causal relationships must be preserved [10]. Moreover, manipulative-counterfactual approaches to explanation have increasingly gained popularity in both debates. On the side of scientific explanation, by advancing an intervention-centered notion of causality [55]. While, on the other of AI, showing what should have been different to change the decision of the system. Specifically, consisting of the smallest change that can be made to a particular instance to get a different decision from the AI [5].\nLastly, a typical distinction found in XAI literature is within the categorization of global and local explanations, the first ones aimed to explain the knowledge of general patterns of the system as a whole, while the latter, a single decision [6]. As underlined in Figure 1, scientific explanation, in a broader sense, sees patterns of explanation underlying a similar dis- tinction between top-down and bottom-up accounts. The first one is, in this sense, global, as it relates to the structure of the whole world [10]. The second one, as can be seen very well in Bechtel and Abrahamsen's account [54], aims to identify the relationships and explanations of individual parts."}, {"title": "A. Going Deeper: Concepts of Explainability", "content": "In addition to establishing connections between the two debates as a whole, it is possible to examine analogies between related concepts and common terminology that we identified in our comparison of scientific explanation and XAI. This anal- ysis considers preliminary epistemological implications that are relevant within the XAI domain, such as the relationship between explanation and understanding, the significance of similarity in explanation, and the desiderata of good expla- nations, thereby laying foundational groundwork for future research.\na) The Epistemological Relation between Explaining and Understanding.: The earliest theories of scientific explanation, proposed by the analytical philosophical tradition, were not concerned with understanding, as they claimed that it was not part of the explanation relation. According to Hempel [35], a scientific explanation is restricted to deductive and log- ical inference, by which science answers \"why questions\u201d and, thus, he considered terms like \u201ccomprehensible\u201d and \"understanding\" out of its domain [44]. However, with the evolution of the debate, pragmatic factors have been taken into consideration increasingly, appearing awareness of the fact that an explanation should be considered with refer- ence to a specific question [46]. Hence, an explanation is not decontextualized but pertains to the situation in which questions and answers are posed. On the other hand, the XAI field has started to progressively consider the importance"}, {"title": "", "content": "of a diverse pool of users and different stakeholders when providing explanations [2], [9], determining the appearance of terms such as \u201cinterpretability,", "understandability,": "round the XAI context [3], [8]. In general terms, explanations involve understanding how the world works. However, the epistemic relation between explanation and understanding is not straightforward [10]. In the context of XAI, this implies that a prior grasp of what it signifies that a subject understands a model or a decision is required [8]. However, philosophers have engaged in extensive reflections which can suggest how to delineate the precise factors that contribute to the generation of understanding. For instance, notwithstanding not lingering on understanding, Hempel [35] posits that it consists of seeing the phenomenon in question as an instance of a general pattern. Furthermore, Friedman's Unificationist view [48] claims that science increases understanding by reducing the total number of independent phenomena. To wit, the phenomenon to be ex- plained is replaced with a more comprehensive one, reducing the total number of phenomena. Finally, Salmon [10] asserts that explanations seek to provide a systematic understanding of empirical phenomena by showing how they fit into a causal nexus.\nb) Similarity, Familiarity, and Surrogate Models.: Ex- planation of ML often consists of adopting a surrogate and interpretable model, such as linear regression, that should pro- vide representations necessary to obtain understanding [23]. However, a relevant issue is establishing why this surrogate serves as an explanation of the original model. Indeed, for any XAI model, there should be a formal linkage, such as iso- morphism or similarity, between it and the initial model [12]. Nevertheless, the majority of surrogate models used currently lack rigorous assurances, raising uncertainties about the effi- cacy of these approximations in elucidating decision-making processes [24]. On the other hand, formal explanations seek to establish guarantees or justifications with respect to the determined explanation [58], [27], such as Random Forest explanations with SAT [27] or abductive explanations [28]. Without this type of connection, there is no basis to state that an explanation provided by the XAI model applies to a \"black box\" [12], [24]. Similarly, some philosophers of science have argued that understanding can given by familiarity, in which the \"explanans\u201d is an approximation similar to the \u201cexplanan- dum", "inadequate": "being familiar gives no grounds for being understood and, regardless some explanations might evoke a feeling of familiarity, this is not a relevant factor in sound explanations [48], [35].\nc) Bona Fide Explanations Criteria.: Also as a con- sequence of the aforementioned considerations, researchers in both epistemology and the XAI domains have sought to identify the characteristics that distinguish bona fide explana- tions, i.e., explanations should satisfy certain requirements to be considered valid [42], [12], [10]. For instance, Miller has done incipient work in establishing criteria to evaluate XAI, by deriving principles from social sciences [7]. Moreover, Mueller et al. [13] provided an exhaustive list of principles that emerged within XAI literature. Within the epistemological domain, Hempel [43], [35] introduced the principle of factual- ity, namely that the \u201cexplanans\u201d and the \u201cexplanandum\u201d must be true. Conversely, a potential explanation possesses all the essential characteristics of a valid explanation, except for the truth [43]. Carnap [42] identified four criteria for explanations: similarity to the \u201cexplicandum,\u201d exactness, fruitfulness, and simplicity. Specifically, similarity to the", "explicandum": "efers to the necessity of the", "explicatum": "o adequately correspond to the"}]}