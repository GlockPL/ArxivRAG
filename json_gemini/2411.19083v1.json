{"title": "ObjectRelator: Enabling Cross-View Object Relation Understanding in Ego-Centric and Exo-Centric Videos", "authors": ["Yuqian Fu", "Runze Wang", "Yanwei Fu", "Danda Pani Paudel", "Xuanjing Huang", "Luc Van Gool"], "abstract": "In this paper, we focus on the Ego-Exo Object Correspondence task, an emerging challenge in the field of computer vision that aims to map objects across ego-centric and exo-centric views. We introduce ObjectRelator, a novel method designed to tackle this task, featuring two new modules: Multimodal Condition Fusion (MCFuse) and SSL-based Cross-View Object Alignment (XObjAlign). MCFuse effectively fuses language and visual conditions to enhance target object localization, while XObjAlign enforces consistency in object representations across views through a self-supervised alignment strategy. Extensive experiments demonstrate the effectiveness of ObjectRelator, achieving state-of-the-art performance on Ego2Exo and Exo2Ego tasks with minimal additional parameters. This work provides a foundation for future research in comprehensive cross-view object relation understanding highlighting the potential of leveraging multimodal guidance and cross-view alignment. Codes and models will be released to advance further research in this direction.", "sections": [{"title": "1. Introduction", "content": "Advancements in video applications, including classification [21, 41, 62], captioning [15, 25, 64], and generation [54, 57, 61], have largely focused on exo-centric (third-person) videos. In contrast, progress on ego-centric (first-person) videos, crucial for fields like virtual reality and robotics, are relatively behind. Essentially, most efforts have concentrated on developing ego-centric models [4, 9, 24, 33, 58, 65] to bridge ego- and exo-centric video understanding, while fewer studies [39, 48, 55] have explored the relationships between the two views directly, due to the lack of cross-view datasets. The recent release of the Ego-Exo4D dataset [10], a large-scale, temporally aligned ego-exo dataset, enables the study of cross-view object relation understanding, offering a promising approach to bridging the ego-exo gap.\nSpecifically, in this paper, we tackle the task of Ego-Exo Object Correspondence\u00b9, first introduced in Ego-Exo4D [10]. As shown in Fig.1, given object queries from one perspective (e.g., ego view), the task involves predicting the corresponding object masks in another perspective (e.g., exo view). Unlike prior segmentation tasks [12, 22, 27, 46, 69], which use categories, text descriptions, or visual hints as prompts, this task focuses on cross-view object understanding, utilizing visual cues from one perspective to predict the corresponding objects in a completely different perspective. This makes the direct application of existing methods nontrivial. Critically, few studies have explored ego-exo object correspondence, with PSALM [68] standing out as a notable exception. Particularly, PSALM [68] integrates Mask2Former [1] with large language models (LLMs), out-performing flagship segmentation models e.g., LISA [27], SEEM [69], and SAM [23], while also enabling zero-shot learning (ZSL) on the ego-exo object correspondence task.\nHowever, despite its state-of-the-art performance on other segmentation tasks, PSALM faces challenges with the ego-exo object correspondence task due to two key limitations: 1) Relying solely on object masks underutilizes semantic information, limiting the LLM's language understanding. For instance, it may segment objects with similar shapes to the query but incorrect categories. 2) PSALM does not account for the cross-view relationships between objects in ego-exo scenarios, causing it to fail when significant visual appearance shifts are observed. These challenges raise two critical questions: \u2460 Can incorporating language conditions with mask queries improve object localization? \u2461 Can we ensure consistency across perspectives for the same objects?\nTo address these key challenges, we propose two innovative modules. First, the Multimodal Condition Fusion (MCFuse) module is proposed to tackle the question \u2460. While the task provides only object query masks, MCFuse enhances this by incorporating semantic information from large vision-language models like LLaVA [35], which generates language descriptions of the query object. We then combine both the object mask and the generated text in the embedding space using a lightweight cross-attention mechanism with residual connection and learnable fusion weight. This fusion enables the model to leverage both visual and linguistic cues, leading to more accurate localization of the target object. Next, to deal with question \u2461, we introduce the Cross-View Object Alignment (XObjAlign) module. This module aligns paired ego and exo object masks into a shared latent space, enforcing proximity between them. Utilizing an effective self-supervised learning approach, we optimize the distance similarity between the masks, which helps the model better recognize objects across different perspectives and overcome challenges posed by view changes.\nBy integrating MCFuse and XObjAlign into the PSALM framework, we present our method, ObjectRelator. We demonstrate the effectiveness of our approach through extensive experiments on both Ego2Exo and Exo2Ego tasks, achieving state-of-the-art (SOTA) performance with 0.26M additional learnable parameters surpassing the base PSALM by over 10%. Moreover, we showcase the flexibility of ObjectRelator by investigating both joint training and single-condition testing capabilities, highlighting its adaptability to various settings and scenarios.\nWe summarize our contributions as, 1) Ego-Exo Object Correspondence Task: Early exploration of the challenging task is performed highlighting its unique difficulties from traditional segmentation. 2) Multimodal Condition Fusion (MCFuse): MCFuse enhances object localization by effectively combining visual and linguistic cues, using language descriptions from vision-language models. 3) Cross-View Object Alignment (XObjAlign): XObjAlign ensures consistency across ego and exo perspectives, improving the model's robustness to view changes. 4) ObjectRelator Framework: By integrating MCFuse and XObjAlign, we develop the novel ObjectRelator, achieving state-of-the-art results on both Ego2Exo and Exo2Ego tasks. 5) Flexibility and Adaptability: We demonstrate ObjectRelator's flexibility via joint training and single-condition testing."}, {"title": "2. Related Work", "content": "Ego-Exo Video Understanding. Both ego- and exo-centric video perspectives are crucial for understanding the world. Extensive work has been done in single-view video understanding: For ego-centric videos, numerous methods address tasks such as video classification [8, 21, 41, 62], captioning [15, 25, 64], video generation [54, 57, 61]. While ego-centric research has historically lagged behind exo-centric, recent efforts including datasets such as Ego4D [9], EK100 [4], HoloAssist [58], AssistQ [60], as well as a variety of methods [7, 19, 24, 32, 33, 43, 65, 66] have brought substantial progress. However, far fewer studies directly address the relationship between ego and exo views. Notable exceptions have ego-exo translation/generation [2, 36, 38, 39, 55]. One factor leading to this gap is that existing datasets are often small in scale or lack comprehensive annotations [5, 14, 18, 26, 44, 50, 53]. Recently, the release of Ego-Exo4D [10], a large-scale, richly annotated, and time-synchronized multimodal dataset, has opened new chances for advancing ego-exo research. In this paper, we focus on the ego-exo object correspondence task proposed in Ego-Exo4D, aiming to establish a strong starting point with our approach.\nSegmentation Models. Segmentation models can roughly be grouped into several main categories based on their input types: generic segmentation, referring segmentation, and interactive segmentation. We refer to generic segmentation [40] as those more classical segmentation tasks e.g., semantic segmentation [28], instance segmentation [11], and panoptic segmentation [22]. Generic segmentation methods don't need extra text prompts or visual prompts, for example, Mask-RCNN [12] and Mask2Former [1]. In contrast, referring segmentation [17] requires textual descriptions to guide the segmentation. Flagship examples include LISA [27], GLAMM [46], PixelLM [49]. The interactive segmentation [45] utilizes visual hints or prompts such as bounding boxes, key points, or masks provided within the image to guide the segmentation results. Well-known examples have OMG-Seg [30], SAM [23], SAM2 [47]. The latest advancements such as UNINEXT [63], SEEM [69], PSALM [68] accept multiple conditions as input, which we termed as \"Universal Segmentation\". However, none of them are specifically tailored for the ego-exo object correspondence, which is essentially challenged by the huge view change between the first-person and third-person observations. Thus, how to build a good segmentation model for this new task still remains uncovered."}, {"title": "3. Methodology", "content": "Task Formulation. The Ego-Exo Object Correspondence involves mapping objects between ego- and exo-centric video perspectives as in Fig. 1. Formally, given a query video $V_{query}$ (e.g., an ego-centric or exo-centric video) and a target video $V_{target}$ (e.g., an exo-centric or ego-centric video), along with object masks $M_{query}$ indicating the object of interest in $V_{query}$, the objective is to predict the corresponding object masks $M_{target}$ in $V_{target}$. The query and target videos are assumed to be temporally aligned, ensuring that corresponding frames represent the same time points. Additionally, each video pair may contain multiple objects of interest. Any semantic information e.g., the categories of the objects are not provided in the benchmark. Models are expected to leverage cross-view relationships to locate and segment the corresponding objects in the target perspective.\nTo structure and simplify usage, we rename the two sub-tasks as Ego2Exo, where the ego-centric video serves as the query and the exo-centric video as the target, and Exo2Ego, where the exo-centric video is the query and the ego-centric video is the target.\nPSALM Baseline. As discussed in Sec. 2 and shown in Tab. 1, the UNINEXT [63], SEEM [69], PSALM [68] could be considered as the baselines. Among them, we select PSALM for its: 1) validated superior performance over UNINEXT and SEEM models on several various benchmarks; 2) demonstrated zero-shot learning (ZSL) transfer ability to our task. Particularly, PSALM combines Mask2Former [1], the potent generic segmentation model, with a powerful LLM leveraging the strengths of both. Briefly, PSALM first takes an image, an instruction prompt, a condition prompt (e.g., categories, text, or visual hint), and mask tokens as input to the LLM resulting in the condition embedding, mask embedding. It then predicts masks in a Mask2Former style based on the image and these LLM extracted embeddings. The model is trained by minimizing the loss, $L_{mask}$, which is calculated by comparing the predicted masks to the ground truth."}, {"title": "3.1. ObjectRelator Framework", "content": "Our proposed ObjectRelator operates at the frame level, focusing on studying object relations between ego-exo frames, leaving the exploration of temporal information as future work. The framework is illustrated in Fig. 2. Overall, we retain the core PSALM modules: Mask Token Extraction, Visual Encoder, MM (Multimodal) Projector, LLM, Pixel Decoder, Mask Generator, and also its loss function $L_{mask}$, while introducing two novel components, Multimodal Condition Fusion (MCFuse) and Cross-View Object Alignment (XObjAlign), highlighted in orange and light green colors.\nFor clarity, we use Ego2Exo as an example: each training step involves a synchronized ego image as the query and an exo image as the target. For each object in the paired data, we denote the ego image, ego object mask, exo image, and exo object mask as $I_{ego}$, $m_{ego}$, $I_{exo}$, and $m_{exo}$ (ground truth), respectively. The following steps are:\n1) Preparing Inputs for LLM: For the target exo image $I_{exo}$, we use the Visual Encoder, MLP Projector, and Pixel Decoder to extract features $f_{exo}$ and $f_{exo}^{multi}$, where $f_{exo}^{multi}$ indicates the feature has multiple layers. Unlike the ZSL PSALM model, which only uses the single ego object mask as a prompt for exo, we leverage both the visual object region and a language description as prompts. Thus, our instruction prompt $P_{ins}$ is formulated as: \"This is an image. Please segment by given regions and instruction.\", where the region hint is ego mask prompt $P_{ego}$ and the instruction hint is the text instruction prompt $P_{text}^{ego}$. Specifically, $P_{region}^{ego}$ is formed by combing the ego image $I_{ego}$ and its object mask $m_{ego}$, while $P_{text}^{ego}$ describes the masked object in $I_{ego}$, generated via the pre-trained multimodal model LLaVA [35]. Additionally, to enforce consistency between ego and exo objects, we use the ground truth $m_{exo}$ to form the exo mask prompt $P_{exo}$. Note that although named a prompt, $P_{exo}$ is not used for mask generation but only for the XObjAlign module. We then extract the instruction tokens $T_{ins}$, ego text condition token $T_{text}^{ego}$, ego visual condition token $T_{visual}^{ego}$, exo visual token $T_{visual}^{exo}$, and mask token $T_{M}$, following the same token extraction method e.g., the Mask Token Extraction module as the base PSALM.\n2) Forwarding the Network: We process image features $f_{exo}$, $f_{exo}^{multi}$, instruction tokens $T_{ins}$, ego text tokens $T_{text}^{ego}$, ego visual tokens $T_{visual}^{ego}$, exo visual tokens $T_{visual}^{exo}$, and mask tokens $T_{M}$. To feed these into the pretrained LLM, we create two inputs: one concatenating $f_{exo}^{multi}$, $T_{ins}$, $T_{text}^{ego}$, $T_{visual}^{ego}$, and $T_{M}$, and the other with $f_{exo}^{multi}$, $T_{ins}$, $T_{text}^{ego}$, $T_{visual}^{exo}$, and $T_{M}$. Feeding both inputs into the LLM yields ego text emb $E_{text}^{ego}$, ego visual emb $E_{visual}^{ego}$, exo visual emb $E_{visual}^{exo}$, and mask emb $E_{M}$. The embeddings $E_{visual}^{ego}$ and $E_{visual}^{exo}$, representing the same object from different views, are passed through the XObjAlign module to compute the cross-object consistency loss $L_{xobj}$. Meanwhile, $E_{text}^{ego}$ and $E_{visual}^{ego}$ are sent to the MCFuse module to generate the fused multimodal ego embedding $E_{MM}^{ego}$. Finally, the multiscale image feature $f_{exo}^{multi}$, the fused ego multimodal emb $E_{MM}^{ego}$, and the mask emb $E_{M}$ are passed to the Mask Generator to predict the exo masks and compute the loss $L_{mask}$.\n3) Training and Inference: Our model has two training stages: a) S1: The MCFuse module is trained with the $L_{mask}$ loss to initialize it well for the second stage. b) S2: All modules (indicated by the fire icon) are trained jointly with the losses $L_{mask}$ and $L_{xobj}$. During inference, the XObjAlign module and also exo mask prompt $P_{exo}$ are removed, and the remaining model is used for inference."}, {"title": "3.2. Key Module: Multimodal Condition Fusion", "content": "Our multimodal condition fusion (MCFuse) is mainly presented to break the limitation of the base model that only visual prompt is used, achieving the effective combination of the embedding from both text condition and visual mask condition. There are mainly two considerations for the design of MCFuse, 1) Given the extensive effectiveness of cross-attention in capturing intricate relationships across modalities, it serves as an ideal choice for fusing our multimodal conditions; 2) the model-generated texts can sometimes be unreliable, prioritizing visual information is prudent. This motivates our use of a residual design, with the visual input serving as the primary pathway. Additionally, to eliminate the need for manual tuning of the residual pathway's weight, we introduce a learnable parameter, $k_{lea}$.\nFormally, as shown in Fig. 3, given the ego text emb $E_{text}^{ego} \\in \\mathbb{R}^{1 \\times D}$ and ego visual emb $E_{visual}^{ego} \\in \\mathbb{R}^{N \\times D}$ as input, the cross-attention is applied to the broadcasted $E_{visual}^{ego}$ and $E_{text}^{ego}$, with $E_{text}^{ego}$ severs as query, $E_{visual}^{ego}$ works as key and value. The fused result $CA_{fuse}$ is given by:\n$CA_{fuse} = CrossAtt(E_{text}^{ego}W_{Q}, E_{visual}^{ego}W_{K}, E_{visual}^{ego}W_{V})$ (1)\nwhere $CrossAtt()$ means the cross attention, $W_{Q}$, $W_{K}$, $W_{V}$ denote the learnable parameters. After that, we combine the $CA_{fuse}$ with the initial $E_{visual}^{ego}$ in a residual connection way with the learnable weight $k_{lea}$ as,\n$E_{MM}^{ego} = k_{lea} \\cdot E_{visual}^{ego} + (1 - k_{lea}) \\cdot CA_{fuse}$ (2)\nWe highlight that though the base PSALM enables various condition prompts, they are never being used at the same time. While our MCFuse module proposes first to generate the text description for the masked object and then manage to fuse the multimodal conditions together."}, {"title": "3.3. Key Module: Cross-View Object Alignment", "content": "The Cross-View Object Alignment (XObjAlign) is mainly proposed to enhance the model's consistent understanding of the same object but from totally different views i.e., ego-centric and exo-centric. The main insight behind this module is that a robust model should have a similar embedding for ego-masked object and also exo-masked object, and also considering the fact that the ego visual emb $E_{visual}^{ego}$ is working as the condition that directly affect the prediction of the Mask Generator. Thus, we propose to apply the consistency constraint exactly in the object visual embedding space.\nAs stated in the overview, to achieve such a consistency constrain, we propose to extract both the ego visual emb $E_{visual}^{ego}$ and exo visual emb $E_{visual}^{exo}$, after that, the only thing left for the XObjAlign is to compare the similarities of these two embeddings and resulting in the cross-view object consistency loss $L_{xobj}$ as,\n$L_{Xobj} = Sim(E_{visual}^{ego}, E_{visual}^{exo})$ (3)\nwhere the $Sim()$ represents the similarity of the embedding features, and we use Euclidean distance to calculate it.\nWe highlight that though its simple design, the idea of aligning the cross-view object is worthy and our XObjAlign module enhances the model's robustness on the view shift issue with no single additional parameter introduced."}, {"title": "4. Experiments", "content": "Datasets. We validate our method on the large-scale Ego-Exo4D [10], using its benchmark for ego-exo object correspondence tasks. The dataset contains 1.8 million annotated object masks at 1 fps across 1335 video takes, covering domains like Cooking, BikeRepair, Health, Music, Basketball, and Soccer. Each paired video features an average of 5.5 objects, tracked for an average of 173 frames. We use the standard Train/Val/Test splits but make the following adjustments: 1) To focus on cross-view segmentation, we retain only data where objects appear in both views; 2) We introduce a \"Small\" TrainSet (about one-third of the original \"Full\" TrainSet) for improved efficiency and data storage; 3) Testing is conducted on the Val set due to the lack of ground truth for the Test set. Our processed splits will be made available for future comparisons.\nNetwork Modules. Following PSALM [68], we use Swin-B [37] as the Visual Encoder and Phi-1.5 1.3B [31] as the LLM. The MLP Projector, consisting of Conv2d, BatchNorm2d, and FC layers, maps the visual latent space to the LLM space. The Pixel Decoder and Mask Generator are adapted from Mask2Former [1]. Mask Token Extraction is the same as PSALM, mainly applying average pooing of mask upon image features. MCFuse is described in Sec. 3.2, and XObjAlign requires no parameters.\nImplementation Details. The pre-trained PSALM model serves as our initialization. In S1 training, we use 1/20 of the training data to train MCFuse. In S2 training, the full training set is used for optimizing all modules except the Swin-B vision encoder. Training details such as epochs, batch size, and learning rate are provided in the Appendix. All training runs on 4xA100 GPUs, while testing is conducted on a single A100, A6000, or L4 GPU. Training on Small TrainSet takes 5-6 hours, while training on the Full TrainSet takes around 20 hours."}, {"title": "4.1. Main Results on Ego-Exo4D", "content": "Baselines and Competitors. Directly aligned prior methods are quite limited, while still some efforts have been made. Specifically, 1) Ego-Exo4D [10] forms a spatial-based \"XSegTx\" method by adapting SegSwap [51], and two spatiotemporal-based \u201cXView-XMem\u201d, \u201cXView-XMem + XSegTx\" via adapting XMem [3] and combing it with \"XSegTx\"; 2) PSALM [68] tests its ZSL results; 3) In this paper, we also adapt and test the ZSL capabilities of SEEM [69], one additional universal segmentation method that decodes visual prompts by referring to the image. The adaption is mainly made by using the query view (e.g., ego in Ego2Exo) to generate the prompt for the target view (e.g., exo in Ego2Exo). Additionally, we retrain PSALM to form a more competitive baseline.\nResults on Ego-Exo4D. The comparison results are summarized in Tab. 2, we indicate whether the method employs ZSL learning, whether it is spatial-based (S) or spatiotemporal (ST) based, the TrainSet used, and most importantly its IoU metric. From the results, we first notice that our proposed ObjectRelator model significantly improves the base PSALM and achieves state-of-the-art performance on the ego-exo object correspondence task. Take models trained on Small TrainSet as an example, our ObjectRelator improves the PSALM from 39.7 to 44.3 on Ego2Exo and from 44.1 to 49.2 on Exo2Ego. These results well demonstrate the effectiveness of our proposed approach.\nIn addition to our SOTA results, several other key observations are as follows: 1) ZSL Inference Performance: Results show that nearly all the tested methods, including universal segmentation models like SEEM and PSALM, struggle to generalize smoothly to the ego-exo object correspondence task. While SEEM and PSALM excel on other out-of-domain tasks, such as open-vocabulary segmentation [6] and video object tracking [42], their performances are degraded here, highlighting the significant challenges posed by view shifts between ego and exo videos. This underscores that simply applying existing models, even those extensively pre-trained ones, is insufficient. Dedicated models specifically designed and trained for cross-view scenarios remain essential. XView-XMem shows the best ZSL performance, likely due to its video object tracking foundation. However, this advantage diminishes in the retrained XView-XMem. 2) Baseline Retraining: After retraining models on the benchmark, all baselines show reasonable performance, further emphasizing the importance of model adaptation for this task. In general, the superior performance of XView-Xmem and \u201cXView-XMem + XSegTx\u201d over XSegTx suggests that temporal information aids cross-view alignment. However, this advantage can be offset by the model designs, as demonstrated by PSALM. The success of PSALM shows the effectiveness of interactive prompting, making it a strong baseline choice. 3) Differences in Query Types: Across models, performance is in general better on Exo2Ego than Ego2Exo, indicating that tasks with an ego-view query are more challenging. XSegTx exemplifies this disparity, achieving an IoU of 30.2 on Exo2Ego but only 6.2 on Ego2Exo. This aligns with intuitive expectations, as exo-view objects tend to be smaller and embedded within complex environments, increasing segmentation difficulty, while ego-view objects appear larger and more distinguishable from the background. 4) Impact of Train Set Size: Models trained on the Full TrainSet generally perform better, but we also observe that the Small TrainSet can effectively serve as a testbed for evaluating approaches, with results that approach and even occasionally exceed those of the Full TrainSet. This demonstrates the rationality of using Small TrainSet for efficient experimentation and evaluation, making it a practical alternative for benchmarking cross-view methods while reducing data requirements."}, {"title": "4.2. Ablation on Proposed Modules", "content": "Effectiveness of the Proposed Modules. To evaluate the contributions of every key component. Starting with the base PSALM, we incrementally added the MCFuse and XObjAlign to observe their individual impact. Results are reported in Tab. 3, in addition to the IoUs, the numbers of model parameters (No.Param.) are also provided.\nResults show that: 1) Incorporating MCFuse into the base model significantly boosts performance, with clear improvements of 3.5 and 3.3 on Ego2Exo and Exo2Ego, respectively. This strongly supports our idea that introducing the language condition and effectively fusing it with the visual mask condition helps better identify the target object, particularly by reducing ambiguities. 2) Applying our XObjAlign module alone also brings significant improvements, boosting the base model by 4.1 and 4.2 on the two sub-tasks, surpassing even the benefits from MCFuse with its simpler design. These results highlight that narrowing the gap between ego and exo views is a key step toward building a more robust predictor. In our paper, we address or at least mitigate this gap by enforcing consistency between the condition embeddings of ego and exo objects. 3) By integrating these two modules, our full ObjectRelator approach achieves the best results, indicating that the two modules are effectively compatible. However, we also observe that the combined effect is not simply additive. We interpret this as an overlap in the optimization directions of the two modules. For example, when the XObjAlign totally aligns the ego-exo views, the likelihood of the model misidentifying the objects across views is already low, which limits the capacity for the MCFuse to contribute further. This phenomenon demonstrates that both modules are working towards the model's optimal solution from different perspectives. 4) Another point worth mentioning is that our method in total only introduces 0.2632M extra parameters, which is around 0.016% of the base PSALM while achieving 11.6% improvement on average.\nFurther Ablations on Module Options. To thoroughly assess the optimality of our design, we conduct additional ablation experiments applying various options to our modules. These include testing: whether cross attention (CA) is the best method for fusing conditions; the impact of adopting a learnable residual connection; the usage of the learnable weight; and the most suitable placement for applying XObjAlign. The results are presented in Tab. 4.\nFor the fusion-related experiments, we designed four variants: \"Add\", \"CA w/o Params\", \"CA, S2\", \"CA + LearnResidual, S2\". The \"Add\" method simply adds the ego text and visual conditions as an easy test of our generated text descriptions and also the idea of multi-condition fusion. The \"CA w/o Params\" variant applies cross-attention without parameters, formulated as $(E^{ego} \\cdot E^{ego^T}) \\cdot E^{ego}$, which helps evaluate model sensitivity to added parameters. \"CA, S2\" represents standard cross-attention fusion, as shown in Eq. 3.2. \u201cCA + LearnResidual, S2", "S2\" denotes that the entire network is trained only once (i.e., our second training stage). Results show": 1, "Add\" and \"CA w/o Params\" improve upon the base model, validating MCFuse's core concept. 2) The notable drop with \"CA, S2\" suggests caution when introducing new parameters into other pre-trained modules, motivating our two-stage training strategy. This strategy is further validated by comparing \u201cCA + LearnResidual, S2\u201d with our MCFuse. 3) The improvement from \u201cCA, S2\u201d to \u201cCA + LearnResidual, S2": "upports protecting the more reliable visual prompt condition. For the residual weight experiments, we compare our learnable approach with fixed weights k (0.2, 0.5, 0.8). Results indicate that learnable weight $k_{lea}$ reduces manual tuning while effectively adapting to appropriate values. Regarding the placement of XObjAlign, we compare put XObjAlign \"After MCFuse\" with \"Before MCFuse\". Results show the latter is more effective, likely because applying the alignment loss $L_{xObj}$ upon MCFuse may skew optimization toward better fusion objectives."}, {"title": "4.3. More Analysis", "content": "Joint Training of Ego2Exo and Exo2Ego. All previously reported results are obtained by training the Ego2Exo and Exo2Ego models separately, using their respective training sets. Thus, in this experiment, we investigate whether the training process can be merged by training a single model on both the Ego2Exo and Exo2Ego datasets simultaneously. The comparative results, shown in Tab. 5, confirm that the answer is yes. Our joint-trained models don't degrade the performance of the separately trained models in most cases and even outperform the specific models. This suggests that the bidirectional correspondence learned during co-training helps the model understand the relationship between ego and exo perspectives more comprehensively.\nSingle-Condition Evaluation of Multi-Condition Model. Our proposed ObjectRelator is trained and tested with both visual mask and text description conditions. However, since text descriptions might not always be available, we are curious about what would happen if we test our multi-condition trained model using only the visual mask condition. To this end, we evaluate the performance of removing the MCFuse module from our trained model, studying both the specific Ego2Exo and Exo2Ego models, as well as the joint-trained model. Results are summarized in Tab. 6.\nThe testing results show that while the performance with only the visual condition is inferior to using both visual and text conditions, the drop in performance is quite slight, and we still outperform the base PSALM model. This outcome exceeds our expectations, considering the inconsistency between training and testing conditions. Our interpretation is that, although we are exploring multimodal fusion rather than multimodal alignment, the model still manages to learn a joint space between the two different modality conditions. This enables the model to associate object categories with masks, allowing it to still perform well during testing, even when in the absence of text information."}, {"title": "4.4. Qualitative Results", "content": "To provide a more intuitive understanding of our method, we visualize the prediction results. Specifically, Fig. 4 presents several examples of ObjectRelator applied to both Ego2Exo and Exo2Ego tasks. The results indicate that our proposed methods perform effectively across diverse scenarios e.g., Basketball, Music, Cooking, and Bike Repair. Our method successfully segments objects in the target view despite significant size changes, shape variations, and cases where the query object is partially occluded. More visualization results can be found in the Appendix.\nTo further demonstrate how our proposed modules improve upon the base PSALM, Fig. 5 compares the results between ObjectRelator and the retrained PSALM model, with ground truth provided as a reference due to the challenging nature of the examples. In Fig. 5(a), PSALM shows that PSALM will segment the wrong object with a similar shape to the query, as seen in the basketball and music sheet examples. This indicates that relying solely on a visual mask fails to capture necessary semantic information. By contrast, our ObjectRelator, which leverages text descriptions to enhance the original visual mask condition, successfully corrects these errors in both examples, underscoring the effectiveness of our MCFuse module. Fig. 5 (b) presents examples where PSALM either segments only part of the target object (first example) or segments an incorrect object (second example). In both cases, the partially or incorrectly segmented object closely resembles the query object, suggesting that PSALM performs reasonably well but falls short in meeting the specific requirements of the ego-exo object correspondence task, where the same object may appear very different across views. The success of our method demonstrates the effectiveness of our proposed XObjAlign, which enforces consistency between ego and exo views as a solution."}, {"title": "5. Conclusion", "content": "In this paper, we explored the Ego-Exo Object Correspondence task, a newly proposed and underdeveloped vision challenge. We introduced the ObjectRelator method, which incorporates two novel modules: Multimodal Condition Fusion (MCFuse) and SSL-based Cross-View Object Alignment (XObjAlign). MCFuse facilitates multi-condition fusion between language and visual modalities, while XObjAlign enforces consistency in object representation across different views through a self-supervised alignment strategy. Extensive experimental results demonstrate the effectiveness of our approach, showing significant improvements over the baseline and achieving state-of-the-art performance with minimal additional learnable parameters. As an early exploration of this task, we hope our work sparks further interest in this direction and paves the way for future research in comprehensive cross-view video applications."}, {"title": "6. Implementation Details", "content": "6.1. Data Processing\nFrame Extraction. We follow the same frame extraction process as the baselines, i.e., XSegTx, XView-Xmem, as provided by Ego-Exo4D [10]. Specifically, for both ego and exo views, we sample one frame every 30 frames in chronological order to ensure time synchronization. Meanwhile, since the resolution of ego and exo is different, we adopt different scaling ratios. For ego video, we scale its resolution from (1408, 1408) to (704, 704). While the exo video is scaled from (2160, 3840) to (540, 960).\nTrain/Val/Test Sets. As described in Sec. 4, we follow the same dataset splits as Ego-Exo4D [10], retaining only the ego-exo pairs where the object is visible in both views (in some cases, an object may be visible in one view but fully occluded in the other). To construct the SmallTrain set, we sample a subset of the FullTrain set at a fixed frequency of 1/3. In the end, for both Ego2Exo and Exo2Ego tasks, we have the FullTrain, SmallTrain, and Val sets. We further report the number of frame images (No. Img) and the average number of objects per frame (Avg. Obj) for each set in Tab. 7. All splits used in this work will be released to the community to facilitate reproduction and comparison."}, {"title": "6.2. Description Generation", "content": "Using LLaVA for Object Descriptions. Due to the lack of semantic information in the initial benchmark, we propose to utilize the pretrained vision language model LLaVA [35"}]}