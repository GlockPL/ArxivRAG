{"title": "Multi-Modal Generative AI: Multi-modal LLM, Diffusion and Beyond", "authors": ["Hong Chen", "Xin Wang", "Yuwei Zhou", "Bin Huang", "Yipeng Zhang", "Wei Feng", "Houlun Chen", "Zeyang Zhang", "Siao Tang", "Wenwu Zhu"], "abstract": "Multi-modal generative AI has received increasing attention in both academia and industry. Particularly, two dominant families of techniques are: i) The multi-modal large language model (MLLM) such as GPT-4V, which shows impressive ability for multi-modal understanding; ii) The diffusion model such as Sora, which exhibits remarkable multi-modal powers, especially with respect to visual generation. As such, one natural question arises: Is it possible to have a unified model for both understanding and generation? To answer this question, in this paper, we first provide a detailed review of both MLLM and diffusion models, including their probabilistic modeling procedure, multi-modal architecture design, and advanced applications to image/video large language models as well as text-to-image/video generation. Then, we discuss the two important questions on the unified model: i) whether the unified model should adopt the auto-regressive or diffusion probabilistic modeling, and ii) whether the model should utilize a dense architecture or the Mixture of Experts (MoE) architectures to better support generation and understanding, two objectives. We further provide several possible strategies for building a unified model and analyze their potential advantages and disadvantages. We also summarize existing large-scale multi-modal datasets for better model pretraining in the future. To conclude the paper, we present several challenging future directions, which we believe can contribute to the ongoing advancement of multi-modal generative AI.", "sections": [{"title": "I. INTRODUCTION", "content": "Multi-modal generative AI (Artificial Intelligence) has received increasing attention recently with the advent of large language models and diffusion models. Two typical examples of multi-modal generative AI are GPT-4V [1] and Sora [2] from OpenAI, which have produced great impacts on both the academy and industry. To compare GPT-4V and Sora from the aspect of functionality, GPT-4V enables the large language model to understand visual input via generating relevant texts, while Sora serves as a text-to-video generation model which outputs visual signals given textual input. In another word, GPT-4V targets at multi-modal understanding, while Sora aims at visual generation. To make comparisons from the perspective of probabilistic modeling, GPT-4V is a multi-modal large language model (MLLM) with auto-regressive probabilistic modeling, while Sora is a multi-modal video generation model with diffusion denoising modeling. As such, there naturally arises a question: \u201cIs it possible to establish a unified multi-modal generative model for simultaneous understanding and generation?"}, {"title": "II. MLLM FOR UNDERSTANDING", "content": "Multi-modal large language models have recently become dominant in the field of visual understanding. In this section, we will review the literature on the multi-modal large language models. Before discussing detailed MLLM works, we will first present some preliminaries involving the LLM, vision-language pretraining, and visual tokenizers."}, {"title": "A. Preliminaries", "content": "1) LLM Auto-regressive Probabilistic Modeling: The core component of multi-modal large language models is the large language model (LLM), which receives the multi-modal input including the user's instructions, questions, and visual information, and then outputs the answers to the user in a text-generation form. The large language model is basically an auto-regressive model that tries to predict the next word based on all the previous words as shown in Eq. (1).\n$$p(w) = \\prod_{i=1}^{n} p_{OL}(W_i/W_{<i}),$$\nwhere OL denotes the parameters of the LLM, which is generally composed of several layers of transformers [5]. Note that LLM can only receive the text tokens as its input, the next important problem for MLLM is how to enable LLM to understand the visual information. To tackle the problem, most existing works [4], [6], [7] try to align the LLM with the visual encoders from vision-language pretraining tasks, such as CLIP [8]. More recently, there have been some attempts [3] to directly transform the images into discrete visual tokens, so that the text and visual tokens can be tackled by the auto-regressive LLM together. Next, we will introduce preliminaries about vision-language pretraining and visual tokenizers.\n2) Vision-Language Pretraining: We provide different paradigms of vision-language pretraining in Fig. 2. The success of BERT [9] in natural language processing (NLP) brings the \"pretrain-finetune\" paradigm into the mainstream and drives the development of pretraining in the multi-modal domain. Vision-language pretraining (VLP) aims to learn multi-modal representations from large-scale image-text pairs, enabling models to process both visual and textual data simultaneously, with the goal of enhancing performance on downstream tasks such as image-text retrieval [10] and visual question-answering [11].\nEarly VLP researchers naturally adopt BERT-like architectures, using image-text matching and masked language modeling (conditioned on the image) to train models that capture the interactions between visual and textual data. However, a challenge similar to that in MLLM arises: how to enable VLP models to understand visual information. Before the Vision Transformer (ViT) [12] emerges, some approaches [13]\u2013[16] rely on a frozen object detector to extract region features as shown in Fig. 2(a), but this method is computationally expensive-up to 50 times more than a BERT-base-like model-and the performance of VLP models may be constrained by the frozen object detector. Pixel-BERT [17] attempts to address this by replacing the frozen object detector with a trainable ResNet [18], but its downstream performance only matches object-detector-based VLP models when using a very heavy ResNeXt-152. The introduction of ViT allows ViLT [19] to adopt a simpler visual embedding approach: linear projection operating on image patches, which significantly improves inference speed with only a minor performance trade-off.\nOn a different path, CLIP [8] and ALIGN [20] employ separate transformer encoders for each modality, a design commonly referred to as a two-tower structure, as shown in Fig. 2(b). They perform pretraining on massive amounts of noisy web data using a contrastive loss, aligning image and text embeddings in a shared embedding space. Despite their impressive zero-shot performance on image-text retrieval, these models lack the ability to capture more complex interactions between image and text necessary for tasks like visual question answering.\nALBEF [21] unifies these two architectures. As shown in Fig. 2(c), ALBEF initially uses separate unimodal encoders for each modality and performs the cross-attention fusion between image and text within a BERT-like multi-modal encoder. At the same time, the unimodal embeddings are aligned through contrastive loss before fusion. This approach leads to strong unimodal and multi-modal representations, delivering superior performance on both retrieval and reasoning tasks.\nAs a significant cornerstone of MLLM, BLIP [22] builds upon ALBEF with two key improvements. From a model perspective, it introduces an additional transformer decoder as shown in Fig. 2(d), enabling not only image-text understanding but also image-to-text generation (image captioning), which paves the way for the influential MLLM BLIP-2 [23]. From a data perspective, it proposes a new dataset bootstrapping method, Captioning and Filtering (CapFilt). After training a BLIP model on noisy image-text pairs, this model generates captions for images in the dataset and filters out noisy captions from both original and generated texts. This approach produces a cleaner dataset for training stronger VLP models and provides valuable insights for future MLLM dataset generation.\n3) Visual Tokenizer: On the one hand, a naive way to transform images into a series of tokens is to split each image into a series of patches, and then map each patch to a continuous embedding with linear projection, such as adopted in Fuyu [24]. On the other hand, inspired by language models where each word is tokenized by a discrete tokenizer, a series of works also transform images into discrete tokens. Typical visual tokenizers include the VQ-VAEs [25], [26] and VQGANS [27], [28], whose overall framework is shown in Fig. 3. We will begin our discussion with VQ-VAE. Basically, VQ-VAE works like an auto-encoder, which has an encoder E(\u00b7) and a decoder D(\u00b7). Given an image x, VQ-VAE first encodes it with an encoder E(\u00b7) into a lower-dimension continuous vector E(x). Then, the lower-dimension vector will be discretized with a codebook Z = {zk}^K_{k=1}. The codebook is similar to the word embedding table in NLP, where K has a similar meaning to the vocabulary size, and each zk \u2208 Rnc represents a visual prototype that is similar to a word embedding. With the encoded vector E(x) and the codebook Z, we can obtain the discrete value of the image by finding the nearest neighbor of E(x) in Z as follows:\n$$Discrete(E(x)) = z_q, q = argmin_q ||E(x) - z_q||.$$\nAfter we obtain the discrete code zq, we can use it to reconstruct the image with the decoder: x = D(zq). The training objective of VQ-VAE is shown as follows:\n$$L = ||x - D(z_q)||^2 + ||sg[E(x)] - z_q||^2 + \\beta||sg[z_q] - E(x)||^2,$$\nwhere the first term means the reconstructed image should be close to the input image. However, since zq is obtained by the nearest neighbor, it has no gradient. Therefore, in the second term, zq should be close to the encoded image E(x), where sg[\u00b7] means stopping the gradient. Similarly, to optimize the encoder E, we need to make the E(x) close to zq as shown in the third term. Note that when optimizing the codebook (the second term), [25] adopts the exponential moving average updates. After training with this objective, we obtain a way to transform an image into discrete tokens. Compared to VQ-VAEs, VQGAN [27], [28] utilizes a GAN perceptual loss to replace the L2 reconstruction loss, which helps to learn a rich codebook. We use a simple example to illustrate the process of tokenization. If we have an input image of size H \u00d7 W \u00d7 3, after the encoder E, we obtain a lower-dimension vector E(x) of size h x w \u00d7 nc, where h < H and w < W and nc is the dimension of the code. This means we can obtain hxw vectors of dimension nc, and for each vector, we will find its nearest neighbor in the code book for discretization so that we will finally obtain a discrete sequence of length h \u00d7 w to represent the image.\nRemark. On the one hand, VQGAN and VQ-VAE can be used as visual tokenizers to transform an image into discrete tokens, which enables it to be received by LLMs for visual understanding. On the other hand, they can be used for compressing an image into a lower-dimensional space, which motivates the well-known latent diffusion model (LDM) [29]."}, {"title": "B. MLLM Architectures", "content": "We categorize existing MLLM architectures into two branches, the early-fusion architectures and alignment architectures as shown in Fig. 4. Most existing works [4], [6], [7] adopt the alignment architecture, which aims to align the vision model from the vision-language pretraining with the pretrained LLM. This branch of models relies on the vision-language pretraining to understand the visual input. After obtaining the embedding of the image, an alignment module such as a projector [4], or Q-Former [23] is used to align the image embedding with the LLM space. To train the alignment module, some text-image or text-video pairs are required to input the model. A typical way to align is to make the LLM output the caption of an image given an image embedding. In contrast, as shown on the right of Fig. 4, the early-fusion architecture [3], [30] does not rely on a pretrained vision model to obtain the semantics of the input image. Instead, similar to NLP where each word is mapped to a token, the early-fusion architecture maps each visual input into visual tokens through a visual tokenizer. Then a multi-modal auto-regressive language model will receive the mixed text and visual tokens, and output the user's desired answers.\nRemark. (i) The advantage of the alignment architecture is that it can utilize the pretrained knowledge of the vision encoder and LLM. The vision-language pretraining enables the output of the vision encoder to have semantic meanings. The only thing that needs training is the alignment module, which makes this paradigm resource-friendly. (Sometimes other modules are also learnable for better performance.) However, its ability is also limited by the pretrained vision encoder and LLM, e.g., the pretrained CLIP vision encoder often struggles with multiple objects, making the MLLM based on CLIP inherit the limitation. (ii) In contrast, the early-fusion architecture may have a higher potential, because all its parameters are trained from scratch. However, training from scratch makes the early-fusion architecture face two challenges: (a) how to train a strong visual tokenizer and (b) more resources to train the multi-modal auto-regressive model. First, since the visual tokenization process involves compression and discretization, there is inevitably visual information loss. How to train a tokenizer that contains rich visual information still remains a challenging problem. Second, the visual tokenizers are generally trained with the image reconstruction objective, more a pixel-level task instead of a semantic-level task, which requires the downstream multi-modal LLM to have an additional ability to learn semantic meanings from the pixel-level information, compared to the original LLM which only needs to understand semantic tokens. Therefore, the multi-modal LLM requires much more data for training.\nNext, with the overall architecture in mind, we will introduce recent advances in image large language models and video large language models."}, {"title": "C. Image Large Language Models", "content": "Many works equip the LLM with the capability to understand images, such as some pioneer works, Frozen [31]. We will follow the MLLM architectures section, and elaborate on the latest advancement of image LLM.\n1) Alignment-Architecture Image LLM. This architecture treats the image input as an additional extension. The vision encoders are usually frozen and the alignment modules and LLM are tuned based on various strategies to align the multi-modal content and instructions.\na) Vision Encoder. It is a module that extracts crucial information from images. Common generic vision encoders contain ResNet [32], CLIP-ViT encoder [8], ImageBind [33]. ResNet and CLIP are pretrained on image-text modals while ImageBind aligns six modals' embeddings into common spaces making it vision encoders encode richer information. However, generic vision encoders suffer information loss from their limited pretraining tasks, and some works attempt to learn tailored vision encoders for themselves. Generic features are not designed for accurate object understanding, VCoder [34] improves vision encoders by introducing depth and segmentation control inputs to promote accurate object perception. CLIP features lack lexical semantics compared to word tokens that are tailored for LLM, SPAE [35] and V2T Tokenizer [36] encode images to lexical tokens guided by LLM codebooks within autoencoders, helping to extract both semantic concepts and appearance details.\nb) Alignment Module. This module, also named projector, adapter, etc., aims to mitigate the gap between image features and lexical word tokens and further fuse two modalities. LLaVA [37] adopts a simple but effective linear projection to convert image features into word token embedding space and then it concatenates image tokens and word tokens. Such alignment only involves image transformation, limiting interaction with texts, and is not flexible in the visual token number. Resampler [38] technique maps varying-size features to a fixed number of tokens. BLIP-2 [23], MiniGPT-4 [39] and Qwen-vl [40] employ Q-former [23] before linear projections to reduce tokens. Q-former incorporates text semantics and models the interaction between image features and text inputs with learnable queries to enhance the most useful visual content for LLM. However, despite the shorter sequence length, the locality preservation is damaged in these projectors. Honeybee [41] proposes a Locality-enhanced Projector, which contains a C-Abstractor and D-Abstractor to enhance spatial understanding while maintaining token flexibility. Besides, efficiency is vital for alignment modules. TokenPacker [42] adopts a coarse-to-fine scheme to further promote efficiency while maintaining finer details. The above discusses the transformation of visual tokens, while in most works the visual tokens are directly concatenated to word tokens, and the LLM architecture is not modified. Several works adopt progressively injecting image content into LLM architecture to enhance alignment. Flamingo [38] inserts gated XATTN-DENSE layers between LM blocks. ImageBind-LLM [43] adds gated image feature to word tokens in each LLM layer. LLaMA-Adatper [44] adds visual projection to adapters and adopts zero-init attention to fuse visual adapters and word tokens in the last L layers.\n2) Early-fusion Architecture Image LLM. The alignment architecture utilizes the power of off-the-shelf LLM and requires lower computations, but pretrained vision encoders would have information loss and be infected by inductive biases because of the gap between limited pretraining tasks and real demands for image LLM, such as supporting flexible resolution. Therefore, as shown in Fig. 4, another line of work aims to train a multi-modal LLM from scratch, where both images and text words are converted into a series of tokens.\nPioneer work Fuyu [24] adopts linear projections on image patches in spatial order and trains a transformer decoder taking the visual and word token sequence as input. Despite limited performance, it reveals a new technical fashion. Google follows this fashion whose Gemini [30] processes the interleaved image and other modalities from the beginning. Chameleon [45] trains an image tokenizer that encodes a 512x512 image into 1024 discrete tokens from a codebook of size 8192 and trains a BPE tokenzier [46] for both modalities. Recent Show-o [47] unifies multi-modal understanding and generation. It trains a lookup-free tokenizer around 35M image data, maintains a codebook of size 8192, and encodes images of 256x256 resolution into 16\u00d716 discrete tokens. Early-fusion Architecture requires much more computation and it's more difficult to converge, leaving challenges for future exploration.\n3) Challenges in Image LLM. (a) One of the challenges is fine-grained visual concept understanding. More tokens help encode more detailed information but may cause redundant computation. Chat-UniVi [48] proposes dynamic visual tokens to allocate more computations on important details. An important part of fine-grained understanding is the spatial awareness of object concepts. AnyRef [49] applies RoIAlign to encode regions and designs segment encoder-decoder to learn segmentation from the image LLM's token outputs, which is similar to OMG-LLaVA [50] who generates pixel- and object-centric visual tokens before projections and decodes segmentation tokens from LLM's output by OMG-Seg. Different from segmentation supervision, VisionLLM [51] and Virtron [52] use text supervision such as bounding and polygon descriptions by flexible instruction tuning. Fine granularity modeling offers some explanations for LLM. (b) Like LLM, the other challenge comes from hallucination. The hallucination involves errors in objects, attributes, and relations in the forms of judgment or description [53]. Some works [54], [55] try to reduce biases in training data while some mitigate hallucination via improving model characteristics like vision encoders [56], [57] or fusion mechanisms [56], [58]. Human feedbacks [59] also play an important role in reducing hallucination."}, {"title": "D. Video Large Language Models", "content": "Following the success of Image LLMs, researchers start exploring the training of Video LLMs [60]. Typically, videos are viewed as sequences of image frames (some Video LLMS incorporate other modalities like audio or speech), so Video LLMs have a higher computational complexity compared to Image LLMs. The challenge of collecting high-quality video datasets further complicates the training process, making early fusion architectures computationally exhaustive. As a result, almost all the existing Video LLMs adopt the alignment architectures.\n1) Alignment-Architecture Video LLM. The video LLM architecture is similar to that of Image LLMs with alignment architectures. By sampling a fixed number of frames or using a fixed frames-per-second (FPS) rate, videos are reduced to a limited set of images. The visual embeddings of each image are then extracted using a visual encoder. These features are sequentially concatenated in the order of the frames and connected to the LLM via an alignment module. In earlier works, VideoChat [61] utilizes a Q-former structure as the alignment module, while VideoLLaMA [62] introduces an audio encoder and an audio Q-former to handle audio signals. Video-ChatGPT [63] takes a different approach by average-pooling each frame's patch embeddings along the spatial and temporal dimensions before using a linear layer as the alignment module. Training Video LLMs also follows an \"alignment then instruction tuning\" strategy. While additional GPT-annotated or human-annotated video datasets are collected, image datasets can also be leveraged by treating images as single-frame videos.\nRecent successful efforts focus on improving performance by refining the alignment module and scaling up the model and dataset sizes. For instance, VideoLLaMA2 [64] improves the alignment module to model the connections across temporal and spatial dimensions. It also gathers datasets for tasks such as captioning, classification, and question answering. LLaVA-NeXT-Video [65] and LLaVA-OneVision [7] introduce the AnyRes technology [66], which serves as a flexible visual representation framework adaptable for both multi-image and video representation. Additionally, some Video LLMs, like MiniCPM-V [67] and VILA-1.5 [68], also support multi-image and video input, showcasing strong performance across various benchmarks.\n2) Challenges and Limitations in Video LLM. Compared to Image LLMs, Video LLMs face two unique challenges. The first challenge is understanding videos at a finer granularity, specifically the comprehension of video segments and the relationships between these segments. The second challenge is understanding long-form videos, such as movies, within the limited context length of LLMs.\nFor segment-level video understanding, VTimeLLM [6] transforms the temporal video grounding and dense video captioning tasks into a sequence-to-sequence format. After alignment training, it introduces an additional boundary perception training, leveraging large-scale multi-event video-text data to enhance awareness of event boundaries and timestamps. Finally, it incorporates temporal reasoning data during instruction tuning. TimeChat [69] injects timestamp information into frame embeddings using a Q-former and collects more datasets related to segment understanding for instruction tuning. Some approaches [70]\u2013[72] adopt training-free methods, where sampled frames are individually captioned, and each frame's timestamp and caption are input into an LLM via carefully crafted prompts, allowing the LLM's powerful reasoning capabilities to comprehend each segment.\nFor long-form videos, traditional Video LLMs struggle with input limitations. For example, a Q-former in BLIP-2 encodes an image into 32 tokens; sampling 256 frames results in 8K tokens, which reaches the maximum context length of most LLMs. However, this represents less than 5 minutes of video at a sampling rate of 1 FPS. Therefore, more efficient representations are necessary for processing long-form videos like movies. MovieChat [73] introduces a memory consolidation mechanism that merges similar image tokens once the token limit is reached. LWM [74] and LongVA [75] handle long video inputs by using LLMs with larger context lengths and more efficient attention mechanisms. Some methods [6], [69], [76] reduce the number of tokens per frame, representing each frame with only 1 or 2 tokens on average. Other approaches [77], [78] convert long-form videos into text corpus using image captioning and employ LLMs as agents to search for specific answers within the text corpus.\nDespite the advancements in Video LLMs, nearly all existing models rely on sampling frames and encoding them individually through image encoders. This approach may be favored due to several reasons: image encoders are less computationally intensive compared to video encoders, they offer better alignment with textual data, and they facilitate unification with Image LLMs. However, this methodology comes with a significant limitation. Specifically, the process of sampling frames can lead to the complete loss of information that occurs between sampled frames. As a result, these models fail to capture the continuous motion and trajectories of objects, which are essential for understanding dynamic scenes and activities within a video.\nNow we have discussed the multi-modal large language model for visual understanding. Next, we will discuss another important topic of multi-modal generative AI, i.e., multi-modal diffusion models for visual generation."}, {"title": "III. MULTI-MODAL DIFFUSION FOR GENERATION", "content": "In this section, before the discussion on diffusion models, we will first introduce some preliminaries, including previous generative models such as GANs and VAEs, and then the diffusion probabilistic modeling, and we also present their overall frameworks in Fig. 5. After that, we will present the widely adopted latent diffusion model [29], and discuss some advanced diffusion text-to-image models and text-to-video models."}, {"title": "A. Preliminaries", "content": "1) Generative Adversarial Networks: The generative adversarial network (GAN) [79] is one of the earliest neural architectures to generate visual contents such as images [80]\u2013[82] and videos [83]\u2013[86].\nThe main idea of GANs lies in two networks: a generator G and a discriminator D. Specifically, G tries to generate visual contents from a noise z and D is trained to distinguish between the real ground truth visual contents x and the generated results G(z). Typically, these two networks are trained against each other. The whole training process is a min-max game where we expect our generator to make the generated results as foolproof as possible to better discriminators. The two networks are mutually reinforcing, so the training objective is as follows:\n$$min_G max_D E_{x\u223cp_x} log D(x) + E_{z\u223cp_z} log(1 \u2013 D(G(z))),$$\nwhere z is sampled from pz that is usually a normal distribution and x is a sample from the real data distribution px.\nThe generator and the discriminator are different for different tasks and usually have been improved to process multi-modal data by different methods. For example, in the video generation tasks, TGANs-C [84] proposes a novel GAN architecture with 3D spatial-temporal convolutions and utilizes a discriminator to determine whether the video matches the text caption rather than the ground truth video only. IRC-GAN [86] introduces a novel approach based on mutual-information introspection, leveraging mutual information semantically to concretely assess semantic consistency, thereby aligning the generated video with the textual content.\n2) Variational AutoEncoder: Variational AutoEncoder [87] (VAE) is another typical generative model. Unlike GANs, autoencoders have an encoder-decoder architecture that uses an encoder \u03b5 to present the visual content x to a latent code z = E(x) and a decoder D to reconstruct the data x = D(z) \u2248 x. However, normal autoencoders have no constraints to the latent space, which makes it overfit the dataset easily. To solve the problem, VAEs make a regularization to the latent space and sample z from a distribution p\u03b8, typically a Gaussian distribution, where \u03b8 is the parameters of the encoder-decoder model. As the distribution p\u03b8 is unknown, VAE utilizes a recognition model \u03c6 which serves as a variational approximation q\u03c6 to approximate p\u03b8 and trains them jointly. The training objective is:\n$$L(\u03b8, \u03c6; x) = \u2212D_{KL}(q_\u03c6(z|x)||p_\u03b8(z)) + E_{q_\u03c6(z|x)} [log p_\u03b8(x|z)],$$\nwhere DKL means the Kullback-Leibler divergence. \u03c6 can be formulated as a differentiable estimator using the parameterization trick.\nTo better generate visual content, many efforts [85], [88], [89] have been made based on VAE. Sync-DRAW [88] introduces a novel architecture that combines VAE with a recurrent attention mechanism to create a unique temporally dependent sequence of frames. Despite the successful introduction of VAEs, they still face a significant issue where the model ignores the information in the latent space and relies solely on a powerful decoder to reconstruct the data, a phenomenon known as \"posterior collapse\". To address this problem, the VQ-VAE [89] utilizes discrete encoding to learn the prior and employs vector quantization methods to prevent the latents from becoming uninformative. [85] leverages the strengths of both GANs and VAEs. It introduces a VAE model to capture static information such as background color or the layout of objects and utilizes a GAN model to obtain dynamic motion information based on the captured information and text input. Compared to GAN and VAE, a new branch of generative models, diffusion models [29], [90], [91] have become dominant in many tasks such as text-to-image generation or text-to-video generation.\n3) Diffusion Probabilistic Modeling: We will briefly introduce the diffusion probabilistic modeling from two mainstream perspectives, i.e., the denoising diffusion probabilistic models (DDPM) and the stochastic differential equations (SDE). The core idea of the diffusion process is to model the relations between the real data distribution q(x0) and a random Gaussian distribution q(xT).\nDDPM: The DDPM includes the forward and backward processes. In the forward process, given a real data sample x0, it will go through a Markov process with more and more random Gaussian noise added to the sample as follows:\n$$q(x_t|x_{t-1}) = N(x_t; \\sqrt{1 \u2013 \u03b2_t}x_{t-1}, \u03b2_tI), t = 0,1,\u2026,T$$\nwhere t is the time step, T is usually large so that xT is close to a Gaussian noise, and \u03b2t is a parameter to control the noise schedule. Conversely, to achieve generation from random noise, what DDPM does in the backward process is to learn the following distribution:\n$$p_\u03b8(X_{t-1}|X_t) = N(x_{t-1}; \u03bc_\u03b8(x_t, t), \u03a3_\u03b8(x_t, t)),$$\nwhere a neural network parameterized by \u03b8 is designed to predicted the less noisy image xt-1. Then, with this denoising network \u03b8, we can denoise from a random noise xT step by step until we get a clean data sample x0, which could be an image or a video, etc.\nSDE: SDE describes the tragectories from x0 to xT with the following stochatistic differential equation:\n$$dx = f(x,t)dt + g(t)dw,$$\nwhere f(\u00b7) describes the diffusion process and g(\u00b7) represents the drift function of the Wiener process. Then, during the backward process, we can use the following equation to denoise:\n$$dx = [f(x, t) - g(t)^2\u2207xlogq_t(x)]dt + g(t)dw,$$\nwhere \u2207xlogqt(x) is the score, and a model \u03b8 will be used to predict the score.\nMathematically, the SDE and DDPM are equal and two different views of the diffusion process. During the diffusion model training, the following objective is generally adopted:\n$$min E_{x_0,\u03b5,t}[w_t||\u03b5 \u2013 \u03b5_\u03b8(x_t,t))||^2],$$\nwhere \u03b5 is the randomly sampled noise, xt is the noisy image, \u03b5\u03b8 is the neural network to predict the noise. Intuitively, when we can predict the noise, we can predict a cleaner image by subtracting the noise as in DDPM, and also we can predict the score in SDE. wt is the schedule for different time steps.\nRemark. GAN, VAE, and Diffusion models are all generative models. Compared to GAN, the diffusion model has explicit probabilistic modeling. Also, the diffusion model only needs to train a denoising network \u03b5\u03b8. In contrast, GAN needs to train both the generator and discriminator, which is less stable. Similarly, VAE-based models also need to train an encoder and a decoder. Moreover, from the perspective of data augmentation, considering that during training we denoise for each image T times, we will have T variants of each image. These augmented images help the denoising network better model the data distribution p\u03b8(x0), resulting in better generation results.\n4) Latent Diffusion Model: As shown in Eq. (6) and Eq. (7), the denoising process of diffusion models is conducted on the pixels of each image in an iterative way, which results in high computational cost, especially when the generated image is high-resolution. To tackle this problem, the latent diffusion model (LDM) [29] proposed to conduct the diffusion process in the latent space instead of the pixel space. The framework comparison between the pixel-level diffusion model and LDM is shown in Fig. 6. To reduce the computational cost, LDM utilizes the encoder of VQGAN [27] to compress the image into the latent space, z = E(x), which has a much lower dimension than the original image. Then, the diffusion process in Eq. (6) and Eq. (7) will be conducted in the latent space. Also, the training objective in Eq. (10) is also applied to the latent code zt instead of the image xt as follows,\n$$min E_{z_0=E(x_0),\u03b8,t}[w_t||\u03b5 \u2013 \u03b5_\u03b8(z_t, t, c))||^2].$$\nNote that there is an additional input c of the denoising network that is for conditional generation, e.g., as for the text-to-image generation task, c could be the representation of the text prompt [92]. Also, c could be other conditions, such as layout [93]\u2013[96], semantic maps [97], [98] or image-to-image translation tasks [97]. Since most computation including the training and iterative inference is conducted in the lower-dimension latent space, the LDM model exhibits high efficiency. Therefore, most text-to-image and text-to-video models adopt the LDM structure."}, {"title": "B. Text-to-Image Generation", "content": "1) Text-to-Image Diffusion Model: As mentioned in the preliminary part", "branches": "pixel-based and latent-based [99", "100": "is a pioneering work in photorealistic image generation with text guidance, using a 3.5 billion parameter"}]}