{"title": "Region-Adaptive Sampling for Diffusion Transformers", "authors": ["Ziming Liu", "Yifan Yang", "Chengruidong Zhang", "Yiqi Zhang", "Lili Qiu", "Yang You", "Yuqing Yang"], "abstract": "Diffusion models (DMs) have become the leading choice for generative tasks across diverse domains. However, their reliance on multiple sequential forward passes significantly limits real-time performance. Previous acceleration methods have primarily focused on reducing the number of sampling steps or reusing intermediate results, failing to leverage variations across spatial regions within the image due to the constraints of convolutional U-Net structures. By harnessing the flexibility of Diffusion Transformers (DiTs) in handling variable number of tokens, we introduce RAS, a novel, training-free sampling strategy that dynamically assigns different sampling ratios to regions within an image based on the focus of the DiT model. Our key observation is that during each sampling step, the model concentrates on semantically meaningful regions, and these areas of focus exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the regions currently in focus, while other regions are updated using cached noise from the previous step. The model's focus is determined based on the output from the preceding step, capitalizing on the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up to 2.36x and 2.51x, respectively, with minimal degradation in generation quality. Additionally, a user study reveals that RAS delivers comparable qualities under human evaluation while achieving a 1.6x speedup. Our approach makes a significant step towards more efficient diffusion transformers, enhancing their potential for real-time applications. Our code is available at https://github.com/microsoft/RAS.", "sections": [{"title": "1. Introduction", "content": "Diffusion models (DMs) [8, 18, 41, 42] have proven to be highly effective probabilistic generative models, pro-"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Diffusion Models: From U-Net to Transformer", "content": "Diffusion models [8, 18, 41, 42] have demonstrated significant capabilities across a range of generative tasks, often outperforming previous methods such as generative adversarial networks (GANs) [14] in many downstream applications. Historically, denoising diffusion probabilistic models (DDPMs) [18] and more recent models like Stable Diffusion XL [32] have predominantly utilized convolutional U-Nets [36] as the backbone. Due to the structure of convolutional networks, it is necessary to maintain the original spatial resolution of the input samples to support operations like pooling, which inherently limits the ability to exploit spatial redundancy during the diffusion process, particularly when attempting to prune the latent samples used as model inputs.\nFortunately, the dilemma has been broken by the emergence of Diffusion Transformer (DiT) [31], which has also been used as the backbone of the SOTA diffusion models like Stable Diffusion 3 [10], Lumina T2X [2], Pixart-Sigma [4]. The biggest characteristic of DiT is that it completely eliminates the need for Convolutaional U-Net. DiT uses a pure Transformer [45] architecture and adds conditional information such as prompts with adaptive layer norm. In this way, the positional information is no longer provided by the convolutional operations, and the latent tokens are now positionally independent after position embedding. This enables us to utilize the redundancy we discovered in Section 1 and select the tokens that are likely to be focused in the current sampling step to compute while caching the predicted of other tokens from the previous step."}, {"title": "2.2. Efficient Diffusion Model Inference", "content": "To address the problem of high inference cost in diffusion models, various acceleration techniques have been proposed from different perspectives. A commonly used approach is to reduce the number of sampling steps. Some of these techniques require additional training, such as progressive distillation [39], consistency models [43], and rectified flow [1, 26, 27]. Among these methods, rectified flow has been widely used in models like Stable Diffusion 3 [10]. It learns the ODE to follow straight paths between the standard normal distribution and the distribution of the training dataset. These straight paths significantly reduce the distance between the two distributions, which in turn lowers the number of sampling steps needed.\nThere are also training-free methods to either reduce the number of sampling steps or decrease the computational burden within each step. DPM-solver [28], for instance, introduces a formulation that enhances the solution process of diffusion ODEs. DeepCache [49], specifically designed for U-Net-based models, leverages the U-Net architecture to cache and retrieve features across adjacent stages, allowing for the skipping of certain downsampling and upsampling operations during the diffusion process. However, these approaches treat all image regions uniformly, ignoring the varying complexity across different parts of the image. This uniform treatment can lead to significant computational inefficiency, as not all regions require the same level of processing.\nAs introduced in Section 1, the complexity of different regions within an image can vary substantially. To exploit the characteristics of both the diffusion process and the structure of Diffusion Transformers (DiTs), we introduce RAS, a novel approach designed to optimize computation by focusing on the distinctive properties of different image regions. RAS is also orthogonal to the methods we mentioned above, such as DiTFastAttn [50] and A-DiT [5]."}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Overview", "content": "In this section, we present the RAS design and techniques to exploit inter-timestep token correlations and the regional token attention mechanism introduced in Section 1. (1) Based on the regional characteristics we observed in the DiT inference process, we propose an end-to-end pipeline that dynamically eliminates the computation through DiT of certain tokens at each timestep. (2) To leverage the continuity across consecutive timesteps, we propose a straightforward method to identify the fast-update regions that require refinement in upcoming timesteps, while ensuring that slow-update regions are not neglected due to insufficient diffusion steps. This approach effectively balances token focus without leading to starvation. (3) Building on our observations of continuous distribution patterns, we introduce several scheduling optimization techniques to further enhance the quality of generated content."}, {"title": "3.2. Region-Adaptive Sampling", "content": "Region-Aware DiT Inference with RAS. Building on the insight that only certain regions are important at each timestep, we introduce the RAS pipeline for DiT inference. In U-Net-based models such as SDXL [32], tokens must remain in fixed positions to preserve positional information. However, given the structure of DiT, we can now mask and reorder elements within latent samples, as positional information is already embedded using techniques like ROPE [44]. This flexibility allows us to selectively determine which regions are processed by the model. To achieve this, some additional operations are required starting from the final step, as described in Figure 6. At the end of each timestep, the current sample is updated by combining the fresh model output for the active tokens and the cached noise for the inactive tokens. Specifically, the noise for the entire sequence is restored by integrating both the model output and the cached noise from the previous step. This mechanism enables active, important tokens to move in the new direction determined at the current timestep, while the inactive tokens retain the trajectory from the previous timestep. The next step involves updating the unpatchified sample with the scaled noise. We then compute the metric R, which is used to identify the fast-update regions based on the noise, update the drop count D to track the frequency with which each token has been excluded, and generate the mask M accordingly.\nWith the mask M, the noise for the slow-update regions is cached, while the sample for the current fast-update regions is patchified and passed through the DiT model. Since modules like Layernorm and MLP do not involve cross-token operations, the computation remains unaffected even when the sequence is incomplete. For the attention [45] module, the computation can still proceed with the query, key, and value tensors being pruned. Additionally, we introduce a caching mechanism to further enhance performance, which will be detailed later. In summary, RAS dynamically detects regions of focus and reduces the overall computational load of DiT by at least the same proportion as the user-defined sampling ratio.\nRegion Identification. The DiT model processes the current timestep embedding, latent sample, and prompt embedding to predict the noise that guides the current sample closer to the original image at each timestep. To quantify the refinement of tokens at each timestep, we use the model's output as a metric. Through observation, we found that the standard deviation of the noise strongly marks the regions in the images, with the main subject (fast-update regions) showing an obvious lower standard deviation than the background (slow-update region). This could be caused by the difference in the amount of information between the regions after mixing with the Gaussian noises. Utilizing the deviation as a metric achieves reasonable results of image qualities and notable differences between regions, as is shown in Figure 8. Also, considering the similarities between latent samples across adjacent timesteps, we hypothesize that tokens deemed important in the current timestep are likely to remain important in the next, while the less-focused tokens can be dropped with minimal impact. Before we reach the final formulation of the metric, we need to introduce another technique to prevent starvation.\nStarvation Prevention. During the diffusion process, the main subject regions typically require more refinement compared to the background. However, consistently dropping computations for background tokens can lead to excessive blurring or noise in the final generated image. To address this, we track how often a token is dropped and incorporate this count as a scaling factor in our metric for selecting tokens to cache or drop, ensuring less important tokens are still adequately processed.\nAdditionally, since DiT patchifies the latent tokens before feeding them into the model, we compute our metric at the patch level by averaging the scores of the tokens within each patch. Combining all the factors mentioned above, our metric can be written as:\n$R_t = \\text{meanpatch}(\\text{std}(\\tilde{N_t})) \\cdot \\text{exp}(k * D_{\\text{patch}})$"}, {"title": "Key and Value Caching.", "content": "As we know, the attention mech-"}, {"title": "3.3. Scheduling Optimization", "content": "Dynamic Sampling Ratio. As illustrated in Figure 4, the correlation between the initial timesteps is lower compared to the latter stages, where the diffusion process exhibits greater stability. This trend is also evident in Figure 3. Consequently, the strategy previously introduced is not suitable for the early stages of the diffusion process, as it could negatively impact the foundational structure of the generated image. Furthermore, we have observed that the similarity gradually increases during the stable phase of diffusion. To address these observations, we propose a dynamic sampling ratios that maintains a 100% ratio for the initial timesteps (e.g., the first 4 out of 28 steps) to mitigate any adverse effects on the outline of the generated image. Thereafter, the sampling ratio is progressively reduced during the stable phase. This approach ensures a balance between computational efficiency and the image quality, enabling effective sampling ratios while minimizing adverse impacts on the generated output.\nAccumulated Error Resetting. RAS focuses on the model's regions of interest, which tend to be similar across adjacent sampling steps. However, regions that are not prioritized for multiple steps may accumulate stale denoising directions, resulting in significant error between the original latent sample and the one generated with RAS. To mitigate this issue, we introduce dense steps into the RAS diffusion process to periodically reset accumulated errors. For instance, in a 30-step diffusion process where RAS is applied starting from step 4, we designate steps 12 and 20 as dense steps. During these dense steps, the entire image is processed by the model, allowing it to correct any drift that may have developed in unfocused areas. This approach ensures that the accumulated errors are reset, maintaining the denoising process in alignment with the correct direction."}, {"title": "3.4. Implementation", "content": "Kernel Fusing. As previously mentioned, we introduced key and value caching in the self-attention mechanism. In each attention block of the selective sampling steps, these caches are partially updated by active tokens and then used as key and value inputs for the attention functions. This partial updating operation is equivalent to a scatter operation with active token indices.\nIn our scenario, the source data of the scatter operation"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experiment Setup", "content": "Models, Datasets, Metrics and Baselines. We evaluate RAS on Stable Diffusion 3 [10] and Lumina-Next-T2I [2] for text-to-image generation tasks, using 10,000 randomly selected caption-image pairs from the MS-COCO 2017 dataset [25]. To assess the quality of generated images and their compatibility with prompts, we use the Fr\u00e9chet Inception Distance (FID) [17], the Sliding Fr\u00e9chet Inception Distance (sFID) [17], and the CLIP score [16] as evaluation metrics. For baseline comparison, we evaluate RAS against widely-used Rectified-Flow-based Flow-Matching methods [1, 6, 10, 12, 26, 27], which uniformly reduce the number of timesteps in the generation process for the whole image. We implement RAS with varying numbers of total timesteps to assess its performance, and compare these configurations to the original implementation under similar throughput conditions.\nCode Implementation. We implement RAS using PyTorch [30], leveraging the diffusers library [46] and its Flow-MatchEulerDiscrete Scheduler. The evaluation metrics are computed using public repositories available on GitHub [19, 40, 54]. Experiments are conducted on four servers, each equipped with eight NVIDIA A100 40GB GPUs, while speed tests are performed on an NVIDIA A100 80GB GPU."}, {"title": "4.2. Generation Benchmarks", "content": "We conducted a comparative evaluation of RAS and the rectified flow, which uniformly reduces the number of timesteps for every token during inference. To assess the performance of RAS, we performed experiments using various configurations of inference timesteps. The findings can be interpreted in two principal ways.\nPushing the Efficiency Frontier. From the first aspect, RAS offers a chance to further reduce the inference cost for each number of timesteps rectified flow offers. As illustrated in Figure 2 (c)(d), we generated 10,000 images using dense inference across different timesteps, ranging from 3 to 30. Subsequently, we applied RAS at varying average sampling ratios (75%, 50%, 25%, and 12.5%) over selective sampling timesteps, with the total number of timesteps set at 5, 6, 7, 10, 15, and 30. The results indicate that RAS can significantly reduce inference time while exerting only a minor effect on key evaluation metrics. For instance, employing RAS with 25% sampling over 30 timesteps improved throughput by a factor of 2.25, with only a 22.12% increase in FID, a 26.22% increase in sFID, and a 0.065% decrease in CLIP score. Furthermore, the efficiency improvements achieved with RAS are attained at a lower cost compared to merely reducing the number of timesteps. Specifically, the rate of quality degradation observed when decreasing the sampling ratio of RAS is considerably lower than that observed when reducing the number of timesteps in dense inference, particularly when the number of timesteps is fewer than 10. This demonstrates that RAS constitutes a promising approach to enhancing efficiency while maintaining output quality and ensuring compatibility with prompts.\nPareto Improvements of Uniform Sampling. Through observation, we found that RAS can offer a Pareto improvement for rectified flow in many cases. We sorted some of the experimental results of Stable Diffusion 3 and Lumina-Next-T2I by throughput, and listed different configurations of RAS alongside the closest baseline in terms of throughput in Table 2 to provide a comprehensive comparison. The results clearly demonstrate that, for each instance of dense inference with rectified-flow-based flow matching in the table, there is almost consistently an option within RAS that offers higher throughput while delivering superior performance in terms of FID, sFID, and CLIP score. This highlights that, for achieving a given throughput level during DiT inference, RAS not only provides multiple configurations with both enhanced throughput and improved image quality, but also offers a broader parameter space for optimizing trade-offs between throughput, image quality, and compatibility with prompts."}, {"title": "4.3. Human Evaluation", "content": "To evaluate whether RAS can enhance throughput while maintaining generation quality in real-world scenarios, we conducted a human evaluation. We randomly selected 14 prompts from the official research papers and blogs of Stable Diffusion 3 and Lumina, generating two images for each prompt: one using dense inference and the other using RAS, both with the same random seed and default number of timesteps. RAS was configured with 50% average sampling ratio during the selective sampling period. We invited 100 participants, comprising students and faculty members from 18 different universities and companies, to compare the generated images. Each participant was asked to determine whether one image was clearly better, slightly better, or of similar quality compared to the other. The order of the images was randomized, and participants were unaware of which image was generated with RAS. As shown in Figure 2 (e), 633 out of 1400 votes (45.21%) indicated that the two images were of similar quality. Additionally, 28.29% of votes favored the dense image over the RAS result, while 26.50% preferred RAS over the dense result. These results demonstrate that RAS achieves a significant improvement in throughput (1.625\u00d7 for Stable Diffusion 3 and 1.561\u00d7 for Lumina-Next-T2I) without noticeably affecting human preference."}, {"title": "4.4. Ablation Study", "content": "Token Drop Scheduling. As shown in Table 3 (a), we evaluate the scheduling configurations introduced in Section 3, including sampling ratio scheduling, selection of cached tokens, and the insertion of dense steps during the selective sampling period to reset accumulated errors, using 10 timesteps with an average sampling ratio of 12.5% on Stable Diffusion 3. The results indicate that each of these techniques contributes to the overall quality of RAS.\nKey and Value Caching. As shown in Table 3 (b), caching keys and values from the previous step is crucial, especially when generating high-quality images with more timesteps. While dropping the keys and values of non-activated tokens during attention can improve throughput, it significantly affects the attention scores of activated tokens. A token's low ranking in the model output does not necessarily mean it has no contribution to the attention scores of other tokens."}, {"title": "5. Conclusions and Limitations", "content": "In this paper, we observed that different regions within an image require varying levels of refinement during the diffusion process, and that adjacent sampling steps exhibit significant continuity in the distribution of focused areas. Based on these observations, we proposed RAS, a novel diffusion sampling strategy that dynamically adjusts sampling rates according to regional attention, thereby allocating computational resources more efficiently to areas of greater importance while reusing noise predictions for less critical regions. Our approach effectively reduces computational costs while preserving high image quality. Extensive"}, {"title": "6. More Visualization of RAS", "content": "This section presents RAS accelerating Lumina-Next-T2I and Stable Diffusion 3 with a 50% sampling ratio. As illustrated in Figure 10, the main object receives more sampling steps compared to the background, demonstrating the significance of our region-adaptive sampling strategy. This approach ensures that the primary subject in the generated image consistently undergoes more sampling, while relatively smooth regions receive fewer sampling steps. For instance, in the example shown in Figure 10 with the prompt \"hare in snow,\" the weeds in the snow are sampled more frequently, while the smooth snow receives fewer sampling steps.\nIn Figure 11, we visualize the standard deviation of the noise across dimensions, as well as the decoded images derived from the noise. This stems from our observation that the noise's standard deviation is consistently smaller in the main subject areas. A preliminary hypothesis is that this occurs because the main subject contains more information. When mixed with a certain proportion of noise at each diffusion step, the foreground tends to retain more deterministic information compared to the background. This allows the model to predict more consistent denoising directions. We acknowledge that further study is needed to fully understand this phenomenon.\nThe primary contribution of this work is to highlight that employing different sampling steps for different regions can significantly enhance the efficiency of diffusion model sampling. The method for selecting these regions is not limited to the aforementioned approach based on the noise standard deviation across dimensions. For example, we also experimented with using the l 2 norm of the noise output by the network as a criterion for selection. By targeting regions with larger noise norms, which indicate areas the network deems requiring more refinement, we observed a preference for more complex regions in the frequency domain as in Figure 9. This approach also achieves high-quality imaging results, as shown in Table 4. It can be seen that the methods using the 12 norm and standard deviation (std) yield relatively similar results, and both significantly outperform random selection, particularly when the cache ratio is higher."}, {"title": "7. Full Experiment Results of RAS", "content": "In this section, we present the full experiment results of RAS against rectified flow, with the same settings as is described in the experiment section. Both Table 5 and 6 are ordered by the throughputs."}]}