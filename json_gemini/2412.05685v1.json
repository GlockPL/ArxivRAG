{"title": "HMGIE: Hierarchical and Multi-Grained Inconsistency Evaluation for Vision-Language Data Cleansing", "authors": ["Zihao Zhu", "Hongbao Zhang", "Guanzong Wu", "Siwei Lyu", "Baoyuan Wu"], "abstract": "Visual-textual inconsistency (VTI) evaluation plays a crucial role in cleansing vision-language data. Its main challenges stem from the high variety of image captioning datasets, where differences in content can create a range of inconsistencies (e.g., inconsistencies in scene, entities, entity attributes, entity numbers, entity interactions). Moreover, variations in caption length can introduce inconsistencies at different levels of granularity as well. To tackle these challenges, we design an adaptive evaluation framework, called Hierarchical and Multi-Grained Inconsistency Evaluation (HMGIE), which can provide multi-grained evaluations covering both accuracy and completeness for various image-caption pairs. Specifically, the HMGIE framework is implemented by three consecutive modules. Firstly, the semantic graph generation module converts the image caption to a semantic graph for building a structural representation of all involved semantic items. Then, the hierarchical inconsistency evaluation module provides a progressive evaluation procedure with a dynamic question-answer generation and evaluation strategy guided by the semantic graph, producing a hierarchical inconsistency evaluation graph (HIEG). Finally, the quantitative evaluation module calculates the accuracy and completeness scores based on the HIEG, followed by a natural language explanation about the detection results. Moreover, to verify the efficacy and flexibility of the proposed framework on handling different image captioning datasets, we construct MVTID, an image-caption dataset with diverse types and granularities of inconsistencies. Extensive experiments on MVTID and other benchmark datasets demonstrate the superior performance of the proposed HMGIE to current state-of-the-art methods.", "sections": [{"title": "1. Introduction", "content": "The quality of vision-language data plays a critical role in training vision-language models (VLMs), such as multi-modal large language models (MLLMs) and text-to-image diffusion models . However, large-scale vision-language datasets collected from the internet or through human annotation, typically in the form of image-caption pairs, often suffer from visual-textual inconsistency issues, where captions contain semantic discrepancies with their corresponding images . These pervasive data quality issues significantly impact downstream task performance, highlighting the crucial need for automatic visual-textual inconsistency evaluation to facilitate effective vision-language data cleansing.\nIn real-world scenarios, as illustrated in Figure 1, image-caption data exhibits significant diversity, presenting two main challenges for inconsistency evaluation. First, captions vary substantially in length and granularity, ranging from concise captions that capture basic scene elements (e.g., T1, T2) to comprehensive narratives that include intricate details (e.g., T4 describing the aircraft's registration number and trailer's text content). Second, inconsistencies show diverse types, including object misidentification (e.g., \"truck\" instead of \"SUV\u201d in T2), scene mischaracterization (e.g., \"barren airfield\" in T3 contradicting the grassy field), and subtle attribute errors (e.g., misrepresenting \u201cCIRCUS\u201d as \"SHOW\" in T4). However, existing methods struggle to address these challenges effectively. Embedding-based approaches like CLIPScore , which rely on global image-text similarity, fail to capture local semantic inconsistencies assigning nearly identical scores to both consistent T\u2081 and inconsistent T2. Meanwhile, QA-based methods show limitations in detecting fine-grained inconsistencies, particularly in detailed captions. For example, VDC has been shown to incorrectly validate inconsistent cases in T3 and T4. Therefore, developing an evaluation framework that can effectively handle varying granularities and diverse types of inconsistencies remains an urgent challenge in vision-language data cleansing.\nTo address the challenges, we propose HMGIE, a hierarchical and multi-grained inconsistency evaluation framework that provides adaptive evaluations for multi-grained image-caption data. As shown in Figure 2, HMGIE consists of three core modules: (1) a semantic graph generation module that converts captions into semantic graphs, representing structured relationships among semantic elements; (2) a hierarchical inconsistency evaluation module that progressively constructs a hierarchical inconsistency evaluation graph (HIEG) through dynamic question-answer generation and evaluation guided by the semantic graph, followed by semantic coverage checking that tracks examined elements and guides subsequent question generation; and (3) a quantitative evaluation module that calculates complementary H-Scores measuring both semantic accuracies (Hacc) and semantic completeness (Hcomp), and offers overall consistency decisions and convincing natural language explanations. This hierarchical architecture enables HMGIE to achieve practical VTI evaluation across captions of various types and granularity. To facilitate a thorough evaluation of our framework, we construct MVTID, the first multi-grained visual-textual inconsistency dataset that includes four distinct granularities. Extensive experiments on the MVTID dataset and other benchmarks demonstrate that HMGIE significantly outperforms current state-of-the-art VTI evaluation methods.\nOur main contributions are four-fold: (1) We propose HMGIE, a novel VTI evaluation framework that can effectively handle multiple granularities and diverse types of inconsistencies in vision-language data. (2) We introduce complementary H-Scores that provide a comprehensive evaluation by both measuring semantic accuracy and completeness. (3) We construct MVTID, a challenging multi-granularity dataset with diverse types of inconsistencies across different granularity levels. (4) Extensive experiments on several benchmarks and human evaluation demonstrate HMGIE's superior performance in inconsistency evaluation."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Embedding-based Detection", "content": "Early efforts to detect visual-textual inconsistency focused on learning and comparing cross-modal representations within shared embedding spaces. For example, CLIPScore pioneers this approach, utilizing CLIP's pre-trained image and text encoders to calculate cosine similarity between cross-modal embeddings. Building on this foundation, NegCLIP recognizes that vision-language models often act as bags of words and introduces composition-aware contrastive learning to better capture relationships and attributes. However, these embedding-based methods encounter inherent limitations when addressing fine-grained semantic inconsistencies, especially in longer, more complex captions, as global embedding similarity fails to capture the nuanced semantic details necessary for comprehensive consistency evaluation."}, {"title": "2.2. Prompt-based Detection with MLLMS", "content": "The emergence of multimodal large language models (MLLMs) has enabled more sophisticated methods to detect inconsistencies using carefully designed prompts, grouping methods into two main categories:\nDirect prompting methods leverage MLLMs' superior cross-modal understanding capabilities through single-turn prompts to query MLLMs about image-text similarity . For example, VQAScore reformulates captions into yes/no questions (e.g., \"Does this figure show {caption}?\") and uses the model's confidence in responding \"yes\" as a consistency score. Even with chain-of-thought reasoning to decompose evaluation steps, these methods still struggle to capture subtle semantic misalignments like incorrect attributes or spatial relationships.\nQA-based methods adopt question-answering frameworks for consistency evaluation. TIFA and VDC decompose captions into multiple targeted questions, assessing consistency by comparing text-based answers against visual question answering results from MLLMs. VQ2 builds on this approach by using MLLMs to directly verify the quality of generated QA pairs through visual evaluation. DSG introduces question dependencies to enable systematic semantic analysis. While these QA-based methods have proven effective, they generate all questions simultaneously, overlooking the hierarchical nature of semantic evaluation. This design limits their ability to handle captions of multiple granularities."}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Task Definition and Overview", "content": "Problem Formulation. Given an image I and its corresponding caption T, we aim to evaluate visual-textual inconsistency (VTI) between them. In real-world applications, image captions exhibit high variety in both content and length, leading to diverse types of inconsistencies (e.g., inconsistencies in scene, entities, attributes, interactions) and varying granularity of inconsistencies. Therefore, comprehensive VTI evaluation requires analyzing both accuracy and completeness across different granularity levels.\nFramework Overview. As illustrated in Figure 2, we propose a Hierarchical and Multi-Grained Inconsistency Evaluation (HMGIE) framework, which provides adaptive evaluations covering both semantic accuracy and completeness for multi-grained image-caption data. The framework consists of three consecutive modules: (1)semantic graph generation module that converts image captions into a semantic graph Gsg, representing structured relationships among semantic elements; (2)hierarchical inconsistency evaluation module that adopts a hierarchical inconsistency evaluation graph (HIEG) Ghie construction mechanism; and (3)quantitative evaluation module that calculates corresponding H-Scores, overall consistency decision, along with natural language explanations. This hierarchical architecture enables HMGIE to achieve effective VTI evaluation across captions of various types and granularity."}, {"title": "3.2. Semantic Graph Generation Module", "content": "Unlike existing QA-based methods that generate questions directly from raw text, HMGIE first converts the caption T into a semantic graph Gs = (V, E), where V represents the set of nodes and E denotes the set of edges.\nGraph Structure Design. Each node vi \u2208 V represents different semantic elements in the text description, including entity, location, concept, event, attribute, and other auxiliary components. The edges ei,j \u2208 E connecting nodes vi and vj capture various relationships between nodes, including action, spatial relations, attribute connections, part-whole relationships, quantitative associations, and other semantic relationships. This structured representation enables precise semantic understanding and systematic evaluation planning. Detailed examples of semantic graphs can be found in Appendix C.\nSemantic Parsing Process. The semantic parsing process fsgg:TGs is implemented by an LLM with carefully designed prompts that ensure consistent and comprehensive graph construction, following recent advances in LLM-based semantic graph generation . Given a caption T, the model first identifies and assigns all semantic elements to appropriate node types. Then, it analyzes the relationships between these elements to establish edges, considering both explicit connections stated in the text and implicit relationships that can be inferred. The resulting semantic graph provides several key advantages: (1) it captures complex semantic relationships more effectively than linear text, (2) it facilitates targeted question generation by identifying key semantic elements, and (3) it enables semantic coverage tracking through explicit node and edge representations."}, {"title": "3.3. Hierarchical Inconsistency Evaluation Module", "content": "Given the semantic graph Gs, this module progressively constructs a hierarchical inconsistency evaluation graph (HIEG) through dynamic question-answer generation and evaluation guided by the semantic graph, followed by semantic coverage checking.\nHIEG Definition. Let Ghie = (N, R) denote our defined Hierarchical Inconsistency Evaluation Graph, where N represents the set of all evaluation nodes across different levels, and R captures their hierarchical dependencies. Each node ni \u2208 N at level l represents an evaluation unit consisting of a quartet (qi, arefi, auga,i,y), where qi is the generated question, arefi denotes the reference answer derived from semantic graph, auqa,i represents the answer generated by visual question answering, and y\u2208 {0,1} indicates whether auga,i correctly aligns with arefi. The node set N is organized hierarchically across L evaluation levels: N = U_{l=1}^{L}N^{l}, where N\u00b9 denotes the set of nodes at level l. The dependency relationship set R is defined as:\nR = {(n^{k}, n^{l}) | n^{k} \u2208 N^{k}, n^{l} \u2208 N^{l}, k < l \u2264 L},\nwhere (n^{k}, n^{l}) indicates that the evaluation node nl at level l depends on node nk from any previous level k, which enables the generation of more specific and targeted questions by building upon previously asked questions, allowing for progressive refinement of the evaluation process.\nProgressive HIEG Construction Mechanism. The HIEG is constructed progressively through iterative expansion at each level. At level l, the construction process involves:\n\u25c6 Stage 1: Question and Reference Answer Generation. Given the semantic graph Gs, a binary mask Ml\u22121 of the Gs indicating unverified elements from previous level, and the existing HIEG Ghid up to level l - 1, the question generation module produces a set of insightful questions Q^{l} = {q_{1}^{l}, ..., q_{n_{l}}^{l} } for level l, their corresponding reference answers A^{l}_{ref} = {a_{ref,1}^{l},..., a_{ref, n_{l}}^{l} }, and dependencies on existing questions R\u00b9:\n(Q^{l}, A^{l}_{ref}, R^{l}) = f_{qg} (G_{s}, M^{l-1}, G_{hie}^{l-1}, \u03c0_{l}),\nwhere fqg leverages LLM's capabilities with carefully designed prompts that incorporate predefined level-specific question generation guidelines \u03c0l. These guidelines direct model to generate questions with increasing complexity: from basic object identification at early levels to subtle semantic nuances at deeper levels. This progressive refinement ensures systematic coverage from coarse-grained to fine-grained semantic elements.\n\u25c6 Stage-2: Visual Question Answering. For each generated question q_{i}^{l}, the actual answer a^{l}_{vqa,i} is generated by reasoning on the input visual content: (a^{l}_{vqa,i},c) = f_{vqa}(I,q_{i}^{l}), where fuqa represents the visual question answering function that can be implemented by any VQA model or MLLM, and c \u2208 [0, 1] indicates the model's confidence in its answer. This step extracts semantic information implied in the image through visual reasoning, producing answers that reflect the actual visual content.\n\u25c6 Stage-3: Answer Evaluation. Then the correctness of each vqa-generated answer is then evaluated against the corresponding reference answer: y_{i}^{l} = f_{eval}(a^{l}_{vqa,i}, a^{l}_{ref,i}), where feval is implemented by determining semantic equivalence rather than exact matching, and y \u2208 {0,1} indicates whether the question is correctly answered.\n\u25c6 Stage-4: HIEG Expanding. Based on the generated questions, reference and actual answers, and evaluated results, new nodes are added to the HIEG and connected according to their dependencies:\nG_{hie}^{l} = G_{hie}^{l-1} \u222a {(q_{i}^{l}, a_{ref,i}^{l}, a^{l}_{vqa, i}, y_{i}^{l})\\}_{i=1}^{n_{l}} \u222a R^{l},\nThis expansion process maintains the hierarchical structure of the evaluation process while capturing the semantic relationships among questions at different levels.\n\u25c6 Stage-5: Semantic Coverage Checking. After expanding the HIEG at level l, HMGIE analyzes which semantic elements have been examined through tracking the coverage of Gs within the Ghie:\nM^{l} = f_{cov} (G_{s}, G_{hie}^{l}),\nwhere fcov tracks the status of each semantic element by analyzing which nodes and edges in Gs have been examined by questions in Ghie. The output M\u00b9 is a binary mask"}, {"title": "3.4. Quantitative Evaluation Module", "content": "Based on the fully constructed HIEG with L levels Ghie, we perform a comprehensive quantitative evaluation that consists of three complementary components: H-Scores computation, overall consistency assessment, and natural language explanation generation.\nH-Scores Computation. To provide a more nuanced evaluation of both semantic consistency and completeness, we introduce two complementary metrics derived from HIEG, collectively referred as H-Scores, including the accuracy score and the completeness score: The accuracy score Hacc quantifies the degree of semantic consistency across all evaluation levels:\nH_{acc} = \\sum_{j=1}^{L} w_{j}  \\left(  \\frac{1}{n_{j}} \\sum_{i=1}^{n_{j}} c_{i}^{j} \\cdot y_{i}^{j}  \\right),  \\ \\  \\text{s.t.}  \\sum_{j=1}^{L} w_{j} = 1,\nwhere wj represents level-specific weights that sum to 1, is the confidence score of the answer, and y is the correctness indicator for each question-answer pair. The completeness score Hcomp measures the semantic richness of the caption by measuring the amount of evaluation content:\nH_{comp} = \\sum_{j=1}^{K} \u03b1_{j} \\frac{n_{j}}{N_{j}},  \\ \\  \\text{s.t.}  \\sum_{j=1}^{K} \u03b1_{j} = 1,\nwhere ni denotes the number of evaluation nodes at level l, Ni represents the maximum allowable nodes at that level, K represents the maximum allowed evaluation depth, and \u03b1l are level-specific weights. A higher completeness score indicates more comprehensive semantic content requiring extensive evaluation.\nOverall Consistency Assessment. The overall consistency of Ghie is calculated by evaluating the correctness of all evaluation nodes across all levels: dL = \u03a0^{L}_{i=1} \u03a0_{n_{i}^{l} \u2208 {0,1}}, where ni denotes the number of nodes at level l, and y represents the correctness indicator for the i-th node at level l. It means that the image-caption pair is considered consistent only if all questions are answered correctly across all levels of HIEG. This binary output will be used to calculate the conventional metrics (e.g., TPR, FPR, specified later), to ensure the comparison with other inconsistency evaluation methods which can only give binary outputs.\nExplanation Generation. To enhance interpretability, HMGIE generates natural language explanations by analyzing the evaluation path through the HIEG: \u03bel = fexp(Ghie, d), where fexp leverages a LLM to synthesize evaluation results into coherent explanations. The explanation highlights critical inconsistent elements, offering users clear insights into the evaluation process and results."}, {"title": "4. MVTID Dataset", "content": "Existing VTI datasets primarily focus on short-length captions or single-label, limiting comparison evaluation where captions vary significantly in granularity. To address this limitation, we introduce the first Multi-granularity Visual-Textual Inconsistency Dataset (MVTID)."}, {"title": "4.1. Adversarial Dataset Construction", "content": "We employ an adversarial framework that leverages LLMS to generate challenging inconsistent samples at multiple granularities, which consists of following two stages:\nGround-truth Caption Generation. For each image Ii randomly selected from the COCO dataset , we generate ground-truth captions Ti = {Ti,1, ..., Ti,4} at four distinct granularities using an ensemble approach. Firstly, three different MLLMs generate initial captions: Tk; = fk(Ii, 9j), where fk represents the k-th MLLM and 9j denotes the granularity-specific generation prompt. These captions are then fused using an LLM to create the final ground-truth caption Ti,j: Ti,j = fs({Tk;}), where fs identifies and preserves consistent semantic elements across the three generated captions while ensuring coherence and naturalness.\nAdversarial Perturbation. For each ground-truth caption Tij, we generate its adversarial counterpart through an iterative perturbation process. At iteration t, we generate a perturbed caption:\nT_{i,j}^{t} = h(T_{i,j}, {T_{i,j}^{t'}}\\_{t'=0}^{t-1}),\nwhere h represents the perturbation function implemented by an LLM that considers all previous perturbations. Then, another MLLM evaluates the inconsistency of the perturbed caption. This process continues iteratively until either the inconsistency becomes undetectable or the maximum iteration count is reached.\nAs shown in Table 1, MVTID encompasses four distinct granularities, each providing progressively richer semantic information. Beginning with basic descriptions (G1) that capture primary objects and scene categories and advancing to complete narratives (G4) that comprehensively detail all visual elements and their interrelationships, the average caption length increases from 11.09 to 63.88 words. This structured progression in granularity enables a thorough evaluation of inconsistency detection methods across various levels of semantic complexity. More examples are presented in Appendix A."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Experimental Setup", "content": "Dataset. We evaluate our approach on the MVTID dataset, which consists of 3,400 image-caption pairs across four granularities. Beyond data cleansing, we extend our evaluation to two application scenarios: detecting fake news with NewsCLIPpings , a text-image fake news dataset, and TIIL , which offers fine-grained fake news data for evaluating text-image mismatches; and assessing text-to-image generation consistency with SeeTRUE , a benchmark focused on measuring alignment accuracy and consistency in generated content.\nImplementation Details. Our framework utilizes GPT-40 as the base model for all LLM and MLLM components, with the temperature set to 0.3. For dataset construction, we employ an ensemble of three MLLMs: GPT-40, Claude 3.5 Haiku , and Gemini-1.5 Flash to generate initial captions at each granularity level. GPT-40 is also used for the semantic fusion of generated captions, generation of adversarial perturbations, and evaluation of perturbation detectability. The maximum evaluation level is set to five. Specific prompts designed for each module are given in Appendix B.1. For H-Score calculations, both w and \u03b1 follow a geometric sequence with a ratio of 1.2.\nBaselines. We evaluate HMGIE against several competitive baselines. For embedding-based methods, we compare with CLIPScore and NegCLIPScore , both using a threshold of 0.25. or prompt-based methods, we implement Direct-Prompt (DP) and Chain-of-Thought (CoT) approaches using both GPT-40 and Llama-3.2-90B-Vision as the backbone models. For QA-based methods, we compare against TIFA and VDC , two recent approaches that use question answering to detect inconsistencies.\nEvaluation Metrics. For consistency decision evaluation, we use the consistency decision d\u00b9 to calculate the following metrics: (1) TPR or recall, denoting the percentage of pairs correctly identified as inconsistent. (2) FPR, representing the percentage of consistent pairs incorrectly identified as inconsistent. (3) F1 Score, providing a balanced measure of detection performance. Additionally, we utilize Kendall's \u03c4 to assess the correlation between H-Scores and human judgments."}, {"title": "5.2. Main Results", "content": "Comparison with Existing Methods across Granularities. We first 2 compares HMGIE with existing methods on the MVTID dataset. In general, as shown in Table 2, HMGIE achieves the highest TPR of 94.94% and F1 score of 92.68%, with a low FPR of 9.94%, outperforming the second-best method VDC by 22.82% in terms of F1 score. This substantial improvement demonstrates the effectiveness of our hierarchical evaluation framework. Moreover, HMGIE exhibits robust performance across different granularity levels, while baseline methods show significant degradation on complex captions. For instance, Direct-Prompt (GPT-40) experiences a dramatic performance drop of 44.25% in TPR from G1 to G4. Meanwhile, HMGIE maintains stable performance with only a 6.36% decrease, validating our approach's adaptability to varying granularities of inconsistency detection.\nH-Scores Correlation with Human Judgment. Following prior works , we evaluate the correlation between our proposed H-Scores and human judgments. We randomly sample 400 image-caption pairs (100 from each granularity level) and collect annotations from 10 human annotators who rate semantic consistency and semantic completeness using a five-point Likert scale (1: strongly disagree to 5: strongly agree). For consistency rating, annotators assess whether the caption accurately matches the image content. For completeness rating, they evaluate whether"}, {"title": "5.3. Ablation Studies", "content": "Impact of Semantic Graph. To validate the effectiveness of semantic graph parsing in HMGIE, we conducted ablation studies by replacing the semantic graph with textual captions for question generation and evaluation. As shown in Figure 3, removing semantic graph parsing consistently degrades performance across all granularity levels. Specifically, the true positive rate (TPR) drops by an average of 9.01%, with the largest decrease observed in G2. In addition, false positive rate (FPR) results indicate a significant increase, with an average FPR rise of 5% when the semantic graph is removed. These findings confirm that semantic graph parsing is essential for maintaining high true detection rates and minimizing false alarms, thereby validating its crucial role in achieving precise semantic understanding."}, {"title": "5.4. Qualitative Analysis", "content": "Interpretability Evaluation. To assess the explanatory quality of different methods, we conduct a comparative evaluation between HMGIE's explanations and those generated by Direct-Prompt and CoT-Prompt methods with GPT-40. We present human annotators with inconsistency"}, {"title": "6. Conclusion", "content": "This paper proposes HMGIE, a hierarchical and multi-grained framework for visual-textual inconsistency evaluation. Our method addresses the challenges of various caption granularities and diverse inconsistency types through three key components: semantic graph generation for structured understanding, hierarchical inconsistency evaluation for progressive evaluation, and quantitative evaluation for comprehensive assessment. Through extensive experiments on our proposed MVTID dataset and other benchmarks, HMGIE demonstrates superior performance in evaluating multi-grained inconsistencies, achieving significant improvements over existing methods.\nDiscussion. HMGIE effectively leverages the advanced reasoning capabilities of current LLMs and benefits from their continued evolution. We acknowledge that its efficiency is constrained by the computational overhead of these models. However, this limitation can be addressed through recent advancements in model effi-"}]}