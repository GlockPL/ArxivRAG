{"title": "Policy Agnostic RL: Offline RL and Online RL Fine-Tuning of Any Class and Backbone", "authors": ["Max Sobol Mark", "Tian Gao", "Georgia Gabriela Sampaio", "Mohan Kumar Srirama", "Archit Sharma", "Chelsea Finn", "Aviral Kumar"], "abstract": "Recent advances in learning decision-making policies can largely be attributed to training expressive policy models, largely via imitation learning. While imitation learning discards non-expert data, reinforcement learning (RL) can still learn from suboptimal data. However, instantiating RL training of a new policy class often presents a different challenge: most deep RL machinery is co-developed with assumptions on the policy class and backbone, resulting in poor performance when the policy class changes. For instance, SAC utilizes a low-variance reparameterization policy gradient for Gaussian policies, but this is unstable for diffusion policies [52] and intractable for autoregressive categorical policies. To address this issue, we develop an offline RL and online fine-tuning approach called policy-agnostic RL (PA-RL) that can effectively train multiple policy classes, with varying architectures and sizes. We build off the basic idea that a universal supervised learning loss can replace the policy improvement step in RL, as long as it is applied on \"optimized\u201d actions. To obtain these optimized actions, we first sample multiple actions from a base policy, and run global optimization (i.e., re-ranking multiple action samples using the Q-function) and local optimization (i.e., running gradient steps on an action sample) to maximize the critic on these candidates. PA-RL enables fine-tuning diffusion and transformer policies with either autoregressive tokens or continuous action outputs, at different sizes, entirely via actor-critic RL. Moreover, PA-RL improves the performance and sample-efficiency by up to 2 times compared to existing offline RL and online fine-tuning methods. We show the first result that successfully fine-tunes OpenVLA [22], a 7B generalist robot policy, autonomously with Cal-QL [35], an online RL fine-tuning algorithm, improving from 40% to 70% in the real world in 40 minutes.", "sections": [{"title": "1. Introduction", "content": "Recent successes in training decision-making policies in a number of domains such as robotics and language agents stem largely from the use of expressive models combined with large-scale imitation-style training [5, 6, 22, 59], an approach that has been tried and tested in other areas of machine learning. However, training a policy once and freezing it is not good enough for many real-world deployment scenarios, where some adaptation is needed: for example, a robot must adapt its behavior as the surrounding environment or task changes; a language-model powered web navigation agent must attempt to use its own experience to improve behavior as it interacts more with the world [2]. The hallmark of an adaptation process is in its use of autonomous, non-expert data. In these use cases, imitation alone done once or applied repeatedly is not enough to guarantee the most efficient learning.\nReinforcement learning (RL) provides a flexible framework for adaptation and fine-tuning with non-expert data, in offline [28], online [35], or hybrid [3] regime. In principle, off-the-shelf RL algorithms could be used to fine-tune any policy. For instance, by running actor-critic RL [49], a policy can be trained towards maximizing the Q-function. However, most existing deep RL algorithms entangle the choice of training objectives and algorithm design with the choice of the policy class. For example, soft actor-critic (SAC) [15], the base learner for many offline and online fine-tuning algorithms [26, 35], employs reprarameterization which is applicable to and stable for Gaussian (or tanh-Gaussian) policies: swapping the policy for a diffusion policy causes instability [52]. These instabilities can be severe to the extent that much weaker policy extraction techniques, e.g., critic-based re-ranking [17, 34] on top of an imitation policy can outperform the policy gradient Wang et al. [52], even though theoretically this is not optimal (and indeed, with Gaussian policies performs worse empirically as well [12, 13]). Likewise, in order to extend conservative Q-learning (CQL) [26] to autoregressive token-based action distributions, Chebotar et al. [4] had to make many modifications to the loss in the CQL algorithm. Overall, this means that adapting the best policy training methodologies or parameterization from one policy class to another can be challenging, and depending upon the policy itself, practitioners are forced to choose a weaker algorithm or spend cycles modifying other components of their approach.\nWe tackle this challenge by developing a single offline RL and online fine-tuning approach, which we call policy-agnostic RL (PA-RL), that effectively fine-tunes any policy class or backbone. To perform policy improvement, the RL algorithm directly optimizes actions (instead of policy parameters). Doing so decouples policy improvement from training the parameteric policy, which can now be done by maximizing the likelihood of \u201coptimized\u201d actions via supervised learning. Concretely, to obtain these optimized actions, we first sample from the base policy several times to get multiple action candidates, and then take gradient steps with respect to the value function to improve those actions in the direction of maximizing Q-values. Then these optimized action samples replace the use of samples from the policy in any value-based RL algorithm, and are used to train the policy themselves. Note that while prior work does use supervised losses for policy training, our main contribution is to show that a single approach of this sort can effectively train multiple policy classes.\nWe evaluate PA-RL empirically on a number of domains including simulated robotic manipulation tasks"}, {"title": "2. Related Work", "content": "Contrary to prior belief, recent work [38] shows that policy learning can be a big bottleneck in RL, especially in offline RL [28]. One implication is that enhancing the policy extraction step with the most expressive architectures and the best loss functions would be important, but prior works often tailor the RL approach to a specific policy class (e.g., most work has focused on Gaussian policies). In principle, designing effective algorithms for only one policy class can \u201coverfit\u201d resulting in methods that are actually worse for other policy classes. For instance, while algorithms that use Gaussian policies reparameterize the policy gradient [11, 15, 30], doing so for diffusion policies [52] or flows [31] can be quite unstable and requires per-task tuning. Wang et al. [52] for example requires using BC regularization and performs offline checkpoint selection against the DDPM loss. When learning with sub-optimal data, this might hurt performance. To make a stable algorithm, Hansen-Estruch et al. [17] resort to Q-function re-ranking on top of a frozen behavior policy, resulting in a somewhat less powerful policy improvement operator (e.g., compared EMaQ [13], which uses a similar reranking-based policy improvement operator to TD3+BC [10], which optimizes the policy through the use of full policy gradient and generally performs better). Most offline RL algorithms that use autoregressive categorical transformer policies run conditional [25] or unconditional supervised regression [20, 54, 55], but Park et al. [38] show that such approaches are unable to extract the best possible policy. In fact, to fine-tune autoregressive policies directly via offline RL, Chebotar et al. [4] had to modify value function training.\nMotivated by these findings, we build a single actor-critic RL algorithm that is effective for fine-tuning arbitrary policy classes and backbones, with a focus on continuous and autoregressive token-based policies, with both diffusion and transformer backbones. Related works that fine-tune diffusion policies include: DPPO [43], which uses a two-layer diffusion-specific policy gradient loss, whereas our approach is applicable outside of diffusion policies (Section 5); IDQL [17], which only utilizes action re-ranking akin to global optimization in PA-RL, but does not distill it into the policy iteratively and hence results in"}, {"title": "3. Problem Setup and Preliminaries", "content": "We formalize our problem in the RL framework. The goal of RL is to find the optimal policy in an MDP, M = (S, A, P, r, p, \u03b3), where S denotes the state space and A denotes the action space.  P(s'|s, a) and r(s, a) are the dynamics and reward functions. p(s) denotes the initial state distribution. \u03b3 \u2208 (0, 1) denotes the discount factor. Formally, the optimal policy in an MDP, \u03c0\u2217 : S \u2192 A maximizes the discounted sum of rewards, denoted by V\u03c0(s) = \u0395\u03c0 [\u03a3t \u03b3t r(st, at)|s0 = s, at ~ \u03c0(st), st+1 ~ p(\u00b7|st, at)]. The Q-function of a given policy \u03c0 is defined as Q\u03c0(s, a) = \u0395\u03c0 [\u2211t \u03b3t r(st, at)|s0 = s, a0 = a, at+1 ~ \u03c0(st+1), st+1 ~ p(\u00b7|st, at)]. We use Q\u03b8 to denote the estimate of the Q-function of a policy \u03c0 as obtained via a neural network with parameters \u03b8. The action a is a d-dimensional continuous vector in [-1,1]d.\nProblem setting. We study two problem settings: (a) fully offline [28] and (b) offline-to-online fine-tuning [35]. In (a), we are given access to an offline dataset of experience, Doff = {(si, ai, ri, Si)}Ni=1, collected by a behavior policy, \u03c0\u03b2, and want to learn a policy that attains best performance using this dataset. In (b), we are supposed to optimize the policy learned offline, say \u03c0off, using autonomously-collected interaction data in M. Our goal is to obtain the optimal policy with the smallest number of online samples, efficiently. Our approach, PA-RL prescribes a single approach to fine-tune policies of different parameterizations and classes.\nPolicy classes and parameterizations. In our experiments, we consider fine-tuning two types of"}, {"title": "4. Policy Agnostic RL (PA-RL): Training Multiple Policy Classes with Actor-Critic RL", "content": "Our approach aims to fine-tune multiple policy classes with RL, regardless of scale, class and output type, stably and efficiently. A prevalent approach to attain sample-efficient policy improvement is to use an off-policy RL method, which typically alters between fitting an action-value Q-function and updating the policy parameters in the direction of larger predicted Q-values. Typically, value learning treats the policy as a black-box that provides actions for computing and optimizing the Bellman update. Policy improvement, on the other hand, requires optimizing the value function with respect to the policy parameters. For example, most continuous action RL algorithms estimate the gradient \u2207\u03c6 Q(s, \u03c0\u03c6(s)) with respect to the parameters of the policy \u03c6 for this purpose. Unfortunately, estimating this gradient can be tricky for several policy classes. For e.g., for large diffusion policies propagating the policy gradient through the denoising chain can be unstable, often requiring extensive per-environment tuning of hyperparameters [52] or truncating the gradient propagation after a subset of denoising steps [43]. Similarly, for auto-regressive policies that operate on discrete action tokens, we must utilize a high-variance REINFORCE [53] policy gradient to optimize the policy. This is not desirable.\nCan we devise a simple and practically feasible, yet universal approach to policy optimization in offline RL and online fine-tuning? One approach is to use a loss function that is universally applicable to most deep learning machinery, such as the supervised learning loss: e.g., a negative log likelihood (NLL) loss or its approximation, such as the variational lower bound [18]. Our method (Fig. 2) builds on the idea that policy improvement can be performed via such a loss, as long as the loss is applied on optimized actions. Hence, we can decompose the policy improvement step in two stages: (1) directly optimizing action samples produced by the policy, and (2) training the policy to imitate these \u201coptimized\u201d actions. This decomposition avoids needing to compute \u2207\u03c6 Q(s, \u03c0\u03c6(s)), or estimating high-variance policy gradient estimates. Since policy improvement is decoupled from policy training, we refer to this approach as \u201cpolicy-agnostic RL\u201d or PA-RL in short. We would expect this approach to inherit appealing attributes pertaining to scaling, reliability, and easy tuning of supervised learning losses. In this section, we will detail each of the two stages of our approach, and then describe the final algorithm."}, {"title": "4.1. Stage I: Action Optimization", "content": "Given a state s, a policy \u03c0\u03c6(\u00b7|s), and a fixed Q-function Q\u03b8(s, a), the objective of this stage is to obtain an action sample that optimizes the Q-function while not deviating too far from the support of seen"}, {"title": "4.2. Stage II: Policy Training via Supervised Learning", "content": "The second stage of PA-RL distills optimized actions into the learned policy model. Crucially, this distillation is performed via standard likelihood maximization procedures from supervised learning that most deep learning models are trained to do (or optimization of the standard lower-bound on likelihood for diffusion models). While the most direct option is to simply take the action from the set AOpt\u03c0,m(s) that attains the highest Q-value (say, a\u2217(\u03c0, m, T, s), where the arguments correspond to various design knobs of action optimization) and maximize its likelihood under the learned policy \u03c0\u03c6(\u00b7|s), another alternative is to distill all action samples from AOpt\u03c0,m(s), but weight the contributions of different actions using the Q-value. We prescribe a simple strategy to choose between these methods (Appendix B.1). To accomplish this, we define a categorical policy distribution over the optimized action samples:\n\u03c0Opt\u03c6 (a|s, m) := I[[a \u2208 AOpt\u03c0,m(s)]] \u00b7 exp(Q\u03b8(s, a))\n\u2211a'\u2208AOpt\u03c0,m(s) exp(Q\u03b8(s, a'))\n(4.3)\nand train the policy \u03c0\u03c6(\u00b7|\u00b7) to match this distribution. To do so, we annotate all states in the dataset (including the replay buffer in online fine-tuning) with an action sample from \u03c0Opt\u03c6 (a|s, m), and maximize"}]}