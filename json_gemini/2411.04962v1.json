{"title": "Position Paper On Diagnostic Uncertainty Estimation from Large Language Models:\nNext-Word Probability Is Not Pre-test Probability", "authors": ["Yanjun Gao", "Skatje Myers", "Shan Chen", "Dmitriy Dligach", "Timothy A Miller", "Danielle Bitterman", "Guanhua Chen", "Anoop Mayampurath", "Matthew Churpek", "Majid Afshar"], "abstract": "Large language models (LLMs) are being explored for diagnostic decision support,\nyet their ability to estimate pre-test probabilities, vital for clinical decision-making,\nremains limited. This study evaluates two LLMs, Mistral-7B and Llama3-70B,\nusing structured electronic health record data on three diagnosis tasks. We exam-\nined three current methods of extracting LLM probability estimations and revealed\ntheir limitations. We aim to highlight the need for improved techniques in LLM\nconfidence estimation.", "sections": [{"title": "Introduction", "content": "Diagnosis in medicine is inherently complex and involves estimating the likelihood of various diseases\nbased on a patient's presentation. This process requires integrating baseline information to establish\npre-test probabilities during the initial hypothesis generation for a diagnosis, followed by iterative\nrefinement as diagnostic test results become available (Sox et al., 1989; Bowen, 2006) (Figure 1).\nTypically, clinicians rely on medical knowledge, pattern recognition and experience, enabling quick\nhypothesis generation of the initial diagnosis. However, this process is prone to cognitive biases,\nwhich can lead to diagnostic errors(Saposnik et al., 2016). Analytic thinking, a more evidence-based\nprocess, is time-consuming and often impractical in fast-paced clinical environments. Although\nclinicians are taught to estimate a pre-test probability and apply test sensitivity and specificity,\ncognitive biases and heuristic-based thinking often lead to under- and overestimation of the pre-test\nprobability and subsequent misdiagnoses (Rodman et al., 2023).\nThe integration of Large Language Models (LLMs) in diagnostic decision support systems has\ngarnered significant interest in addressing these challenges. Recent advancements, particularly with\nmodels like GPT-4, have demonstrated that LLMs can rival clinicians in generating differential\ndiagnoses (Kanjee et al., 2023; Savage et al., 2024a). However, LLMs often fail to explicitly convey\nuncertainty in the estimated probability of a diagnosis in their outputs. This is crucial in medicine;\nfor example, an LLM might suggest an initial diagnosis of pneumonia, yet, a 20% probability of\npneumonia may have vastly different implications for a clinician compared to a 90% probability.\nWhile GPT-4 has shown some potential for improvement over clinicians in predicting pre-test"}, {"title": "Methods of Extracting Pre-test Probabilities from LLMs", "content": "This section formulates the task as a binary diagnostic outcome classification using three methods.\nWe benchmarked against XGB using raw features (baseline, Raw Data+XGB), correlating each\nmethod. We utilized a table-to-text method to convert structured EHR data into text. Specifically,\nwe began this transformation by creating a template starting with \"Hospitalized patient with age\nXX, systolic blood pressure YY ...\u201d where XX and YY represent the actual values from patient"}, {"title": "Results", "content": "Figure 2 illustrates the results of the AUROC from LLMs to predict Sepsis, Arrhythmia, and CHF.\nThe LLM Embedding+XGB method consistently outperformed the other LLM-based methods.\nParticularly for Sepsis, it achieved nearly the same AUROC score as the baseline Raw Data+XGB.\nThe Token Logits (mean AUROC: 49.9 with 95% CI: 47.8-51.9) and Verbalized Confidence (mean\nAUROC: 50.9 with 95% CI: 48.7-53.1) methods exhibited marginal performance, generally not\nsurpassing the baseline XGB classifier. The inclusion of demographic variables (sex, race, ethnicity)\nchanged the AUROC scores, for instance, by as much as 7.22 for Mistral embedding on Sepsis\nprediction (71.1 on default setting vs 63.9 on ethnicity). However, the direction and consistency of\nthese changes varied depending on the specific context and data included.\nFigure 3 reports the Pearson correlation coefficients between the predicted probabilities from LLM-\nbased uncertainty estimation methods and those from the XGB classifier for three diagnoses across\ndifferent demographics. When correlating the LLMs' positive class probabilities with the baseline\nresults, the token logits and verbalized confidence methods had more variable correlations, often\nno correlation or negative correlation, suggesting less alignment with the baseline XGB predicted\nprobabilities. On the contrary, the LLM embedding+XGB method consistently showed strong positive"}, {"title": "Discussion and Conclusion", "content": "The LLM Embedding+XGB method demonstrated competitive performance compared to the state-\nof-the-art XGB baseline classifier under specific conditions, such as Sepsis, and exhibited the\nstrongest correlation among the methods tested. However, this result is not surprising given that both\nmethods rely on training a classifier. In contrast, purely LLM-based methods, such as Token Logits\n(next-word probability) and Verbalized Confidence, were found to be unreliable for risk estimation.\nTheir performance, evaluated through AUROC scores, Pearson Correlation, and calibration curves,\ndeteriorated significantly when diagnosing conditions with lower prevalence, raising concerns about\nthe accuracy of pre-test probabilities derived from these models. The results were consistent across\nboth the Mistral-7B and Llama3-70B models. Additionally, results varied with different demographic\ncharacteristics, reinforcing existing concerns about bias in LLMs (Zack et al., 2024). While the LLM\nEmbedding+XGB method showed promise in generating pre-test probabilities, overall, LLM-based\nprobability estimation methods did not achieve the same level of performance as raw tabular data in\nan XGB model. This underscores the necessity for further optimization of LLM methods to produce\nuncertainty estimations that align more closely with established and reliable methods.\nOverall, our findings demonstrate the inability of LLMs to provide reliable pre-test probability\nestimations for specific diseases and highlight the need for improved strategies to incorporate\nnumeracy into diagnostic decision support systems and reduce the impact of bias on LLM performance.\nThis remains a major gap to fill before we can enter a new era of diagnostic systems that integrate\nLLMs to augment healthcare providers in their diagnostic reasoning. To address these limitations,\nfuture work should explore hybrid approaches that integrate LLMs with numerical reasoning modules\nor calibrated embeddings, enhancing their capacity for accurate uncertainty estimation, particularly\nfor low-prevalence conditions. Additionally, bias mitigation strategies, such as layer-wise probing\nand targeted regularization, could help ensure fairer predictions across demographic groups. These\nadvancements would bring us closer to safe and effective diagnostic systems that integrate LLMs to\nsupport clinicians in clinical decision-making."}]}