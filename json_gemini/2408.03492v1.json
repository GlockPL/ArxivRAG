{"title": "Automated Theorem Provers Help Improve Large Language Model Reasoning", "authors": ["Lachlan McGinness", "Peter Baumgartner"], "abstract": "In this paper we demonstrate how logic programming systems and Automated first-order logic Theorem Provers (ATPs) can improve the accuracy of Large Language Models (LLMs) for logical reasoning tasks where the baseline performance is given by direct LLM solutions. We first evaluate LLM reasoning on steamroller problems using the PRONTOQA benchmark. We show how accuracy can be improved with a neuro-symbolic architecture where the LLM acts solely as a front-end for translating a given problem into a formal logic language and an automated reasoning engine is called for solving it. However, this approach critically hinges on the correctness of the LLM translation. To assess this translation correctness, we secondly define a framework of syntactic and semantic error categories. We implemented the framework and used it to identify errors that LLMs make in the benchmark domain. Based on these findings, we thirdly extended our method with capabilities for automatically correcting syntactic and semantic errors. For semantic error correction we integrate first-order logic ATPs, which is our main and novel contribution. We demonstrate that this approach reduces semantic errors significantly and further increases the accurracy of LLM logical reasoning.", "sections": [{"title": "1 Introduction, Background and Related Work", "content": "The release of models like GPT [3] and Gemini [28] through platforms like ChatGPT and Bard have transformed Large Language Models (LLMs) into general-purpose tools that can be used by everyone. Although designed for next token prediction, LLMs have been shown to have emergent abilities and are able to perform a wide variety of tasks without task-specific training data [3, 20, 25, 30, 31].\nUnfortunately, LLMs also frequently return wrong results, such as fictitious claims (\"hallucinations\") or conclusions that defy common sense or (naive qualitative) physics [13, 16, 27]. Such shortcoming may or may not be obvious but in any case impact trustworthiness. A recent famous example was a lawyer who submitted a legal brief generated by ChatGPT which contained many errors and false references [5, 6]. Asking the LLM for an explanation might help, but the explanation might contain errors again and does not necessarily reflect the process used to obtain its answer. Equipping and checking LLMs with trustworthy (logical) reasoning remains to be a current major problem [21, 22].\nA general approach to address this problem equips LLMs with external functionality [8, 10, 13, 19, 21]. These equipped models are referred to as Augmented Language Models (ALMs). The general problem of combining neural networks with symbolic reasoners has attracted a lot of"}, {"title": "2 Our Method", "content": "Natural Language Processing is a fast moving area with multiple new LLMs being released each year. This work focuses on only three of the best performing models at the time of the experiment; GPT3.5 [3], GPT4 [17] and Gemini-Pro [28]. This study investigates the logical reasoning skills of these models and how they could be augmented through the use of automated reasoning systems. Figure 1 provides an overview of the general architecture that we explore in our experiments.\nTo test these models we chose PRONTOQA [24], a logical reasoning dataset, because it has different settings ('ontologies', 'hops\u2019and\u2018distractors') which can be changed to adjust the difficulty of the problems. PRONTOQA provides the Natural Language Problem Script for our specific experiments. The code for PRONTOQA questions is published but not the questions themselves, which helps prevent contamination of LLMs (reduces the likelihood that they will have seen the exact questions and answers in their training data). We generated one hundred examples of the most difficult problems (\u2018false ontology' with 'relevant distractors') for one hop, two hops and three hops as our evaluation benchmark.\nWe implemented several experimental conditions for each LLM. In the baseline condition the model was given a question from the benchmark and needed to produce a 'True' or 'False' answer based on the text provided. This corresponds to the arrow pointing from the Large Language Model to the Model Answer in Figure 1. For the zero-shot condition, we provide the LLM with instructions explaining how to write a Logic Program (LP) in Prolog syntax and ask it to convert a natural language problem into such a logic program. The LP is the instructions shown in Figure 1.\nWe chose logic programs as the interchange language because their syntax is already known by the LLMs, they are easy to \"teach\" to a LLM in a prompt and their simple if-then structure is sufficient for our purpose. For computing a 'True' or 'False' result we used our Fusemate LP system [1]."}, {"title": "2.1 Error Categorisation", "content": "Few systems for error categorisation currently exist in the literature [33] and these are not appropriate for categorising errors when Augmented Language Models (ALMs) call upon tools. Therefore we propose a new error categorisation which has two broad classes; syntactic errors and semantic errors.\nA Syntactic Error is defined as an error in the LLM's instructions which prevents the tool from parsing. There are a number of different sub-categories of syntactic error which can contribute to this including:\n\u2022 Symbol Errors - The LLM instructions contain incorrect symbols. As an example consider a logic program which contained \"-?\" instead of \"?-\" for a query. This would"}, {"title": "2.2 Semantic Error Detection and Correction", "content": "In the following we describe our method for analyzing and auto-correcting errors according to our error framework. We start with a brief overview of the main ideas and its core algorithm, SEDAC (Semantic Error Detection And Correction) shown by the blue box in Figure 1.\nSEDAC takes as input a natural language script nl and the string representation of a logic program lp. The nl is the original problem statement and, in this sense, holds the \"ground truth\". The lp is meant as a faithful representation of nl as obtained by a given LLM. The purpose of SEDAC is to assess the correctness of the lp wrt. the nl in terms of the error categories defined above. It also carries out fixes for problems spotted along the way.\nSEDAC first tries to automatically fix syntactic errors. Correct or fixed statements then proceed"}, {"title": "3 Results", "content": "Table 1 shows the overall accuracy of all three models with each experimental condition described in Section 2. The results show that the use of the LP system, Fusemate, increased the accuracy of each LLM by between 10% and 25% of the possible total.\nThe SEDAC auto-correction successfully reduced errors in all cases. The syntactic fix alone reduced the number of errors of each model by 15-30%. The partial and full semantic fixes"}, {"title": "4 Discussion", "content": "The results clearly show that during the time period of the experiments (December 2023), the accuracy of GPT4 on all experimental conditions was significantly higher than GPT3 and Gemini-Pro which were comparable in their performance. Using an AR tool improved the performance comparable with Chain of Thought techniques and our method has the added bonus of trustworthy explainability; AR tools can produce a proof for any answer they produce."}, {"title": "5 Conclusions", "content": "In this study we have investigated the intersection of Automated Reasoning and Large Language Models in three different ways. Firstly we have explored the capability of LLMs as stand alone reasoning engines. Secondly we have tried coupling LLMs with external Automated Reasoning systems. Thirdly we have implemented automated reasoning technology to debug LLM reasoning.\nWe have demonstrated that augmenting an LLM with an AR system improves its reasoning by a similar level to Chain of Thought prompting but with the added bonus of reliable explainability. Furthermore we have introduced the SEDAC algorithm which can act as an auto-correct to reduce LLM errors by at least 15% and up to 90% for problems where a DCG is able to parse the ground truth.\nAn error classification system was introduced for evaluating interactions between ALMs and their tools. It provides a systematic way to determine the types of errors that LLMs make when interacting with tools. Diagnosing error types provides insight and guidance into which strategies should be implemented to improve model performance. This classification is broad enough that it can be generalised for any external tool while still providing specific information to improve ALM prompts. As the popularity of ALMs rises focus on types of errors gives developers of LLMs a clear direction for improvement.\nOne key finding from the paper is that semantic errors are far more common than syntactic errors when LLMs call external tools. This is significant for developers who are interested in deploying LLMs for real-world applications. When prompting their models to use external tools, focus should be placed on enhancing model reasoning and semantics not just syntax.\nThis study considers only a restricted domain of steamroller problems which have highly predictable structures. An area for future research is to apply and evaluate these techniques to a broader class of problems or real-world application and to determine their computational cost."}, {"title": "A Comparison with Existing Error Classification Systems", "content": "Xu et al. [33] have two major error categories for determining LLM reasoning capability; evidence selection errors and reasoning process errors. The evidence selection process category is divided into two sub categories which are defined as [33]:\n\u2022 Wrong Selection - 'LLMs select the wrong facts or ignore the necessary facts from the beginning of the reasoning.'\n\u2022 Hallucination - 'LLMs select the evidence which contradicts the given context or cannot be verified from the context.'\nNote that these categories combined roughly correspond to Knowledge Errors and Deep Semantic Errors.\nFurthermore the reasoning process errors are divided into three sub-categories; no reasoning, perspective mistake and process mistake. In our context the model is not required to reason per se, instead it is required to translate natural language to a logic program. This best approximates the Shallow Semantic Errors as these clearly indicate a failure in logical reasoning. The communication, symbol and natural language errors have no equivalent error in the system proposed by Xu et al. As the two systems of errors only have rough corresponding categories, any comparison of the frequency error categories should only be a rough approximation."}, {"title": "B Example LLM Prompt", "content": "One of the PRONTOQA steamroller problems reads as follows:\nEach composite number is not liquid. Every composite number is a fraction. Every composite number is a number. Negative numbers are not large. Every fraction is large. Each fraction is a real number. Fractions are integers. Integers are temperate. Each number is slow. Each even number is loud. Even numbers are natural numbers. Alex is an even number. Alex is a composite number."}, {"title": "C Correlation Matrix", "content": "Figure 6 shows the correlation between error types for our experiments. Note that most of the examples that contained errors came from experiments using GPT3 and Gemini, so GPT4 is underrepresented. The correlation between Natural Language Errors and Symbol Errors can be explained by the experimental conditions. In zero-shot examples the model is more likely to make both natural language errors and symbol errors as shown in Figure 4, while the models"}]}