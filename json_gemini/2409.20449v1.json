[{"title": "Linear Projections of Teacher Embeddings for Few-Class Distillation", "authors": ["Noel Loo", "Fotis Iliopoulos", "Wei Hu", "Erik Vee"], "abstract": "Knowledge Distillation (KD) has emerged as a promising approach for transferring knowledge from a larger, more complex teacher model to a smaller student model. Traditionally, KD involves training the student to mimic the teacher's output probabilities, while more advanced techniques have explored guiding the student to adopt the teacher's internal representations. Despite its widespread success, the performance of KD in binary classification and few-class problems has been less satisfactory. This is because the information about the teacher model's generalization patterns scales directly with the number of classes. Moreover, several sophisticated distillation methods may not be universally applicable or effective for data types beyond Computer Vision. Consequently, effective distillation techniques remain elusive for a range of key real-world applications, such as sentiment analysis, search query understanding, and advertisement-query relevance assessment. Taking these observations into account, we introduce a novel method for distilling knowledge from the teacher model's representations, which we term Learning Embedding Linear Projections (LELP). Inspired by recent findings about the structure of final-layer representations, LELP works by identifying informative linear subspaces in the teacher's embedding space, and splitting them into pseudo-subclasses. The student model is then trained to replicate these pseudo-subclasses. Our experimental evaluations on large-scale NLP benchmarks like Amazon Reviews and Sentiment140 demonstrate that LELP is consistently competitive with, and typically superior to, existing state-of-the-art distillation algorithms for binary and few-class problems, where most KD methods suffer.", "sections": [{"title": "1 Introduction", "content": "While deep neural networks have revolutionized Natural Language Processing [9, 5], Computer Vision [29], and other fields, their ballooning size and data demands raise challenges. Recent research [21] aims to develop efficient models that excel without needing massive datasets or expensive hardware. Knowledge Distillation (KD) [6, 14] is a powerful approach for generating lightweight models by leveraging a large teacher model to guide their training. In its basic form, the student model is trained to replicate the teacher's output probabilities for each training instance. Additionally, several subsequent studies (e.g., [28, 16, 26, 2, 22]) have proposed advanced distillation techniques that go beyond mere output probability matching and focus on encouraging the student to learn the teacher's internal representations.\nWhile KD can significantly improve student model performance in tasks with numerous classes, its impact is less pronounced in binary classification and problems with a smaller number of classes. As [22] points out, this is because when we distill knowledge using logits or high-temperature cross-entropy, the information about the teacher model's generalization patterns scales directly with the number of classes. Furthermore, Knowledge Distillation research has primarily focused on Computer Vision, so many sophisticated distillation techniques are not always effective or even suitable for other data modalities like Natural Language. As a result, effective distillation techniques remain elusive for a range of critical real-world applications, such as sentiment analysis, search query understanding, and advertisement-query relevance assessment.\nIn light of these observations, in this paper we propose a novel approach for capturing information from the teacher's representations, which we call Learning Embedding Linear Projections (LELP). At a high level, it works by extracting knowledge form the teacher's last-layer representations (embeddings) and converting it into pseudo-subclasses via linear projections. The student model is then trained on these pseudo-subclasses, using a single unified cross-entropy loss.\nOur approach leverages recent findings about the structure of final-layer representations (embeddings) in deep learning models and, in particular, aspects of the phenomenon known as Neural Collapse [24, 10, 34]. Similar in spirit to Subclass Distillation [22], which uses a modified and retrained teacher model to identify hidden patterns, our method achieves improved student performance, particularly in finetuning Language Models for tasks with a few number of classes. Crucially though, and unlike Subclass Distillation, our approach does not require any retraining of the teacher model. Finally, a key advantage of LELP is its flexibility in bridging diverse model architectures, making it uniquely versatile for teacher-student learning scenarios.\nOur contributions can be summarized as follows:"}, {"title": "2 Related Work", "content": "Knowledge Distillation. Most of the literature on Knowledge Distillation has been focused on the fully supervised setting, i.e., when distillation is performed on the labeled training data of the teacher model rather than on new, unlabeled data see e.g. the original paper [14]. Specifically, when training the student one typically uses a convex combination of the standard cross-entropy loss LCE with respect to the ground-truth labels, and the distillation loss $\\mathcal{L}_{distill}$: \n$$\\mathcal{L}_{student} = \\alpha \\mathcal{L}_{CE} + (1 - \\alpha) \\mathcal{L}_{distill}.$$ \nIn the original paper [14] the distillation loss encourages the student model to mimic to replicate the teacher's output probabilities, potentially after scaling the logits of both models using a temperature-scalar. (The value of the temperature is typically larger than 1, and it is used to emphasize the differences between the probabilities of wrong answers that would all be very close to zero at temperature 1.)\nIn addition to focusing on the teacher model's final outputs, follow-up methods, like those proposed in [28, 2], also consider the teacher model's internal representations, often in the form of its embeddings. These methods encourage the student model to mimic not only the teacher's final predictions but also its internal representations. The simplest, yet often highly effective, example of this approach is Embedding Distillation [28]. In this method, an additional term, called the \"embedding-loss term\", is added to the distillation loss. This term measures the difference between the embeddings produced by the teacher and student models. The weight of this term can be adjusted using a hyperparameter. Specifically, one adds the term\n$$\\mathcal{L}_{Embedd} = \\frac{1}{n} \\sum_{i=1}^{n}|| f^T(x_i) - W f^S(x_i)||^2,$$ \nwhere $x_1,...,x_n$ denotes the current batch of examples, $f^T(x_i)$, $f^S(x_i)$ the embeddings of the teacher and student model corresponding to example $x_i$, respectively, and W is a learnable projection"}, {"title": "3 Learning from Embedding Linear Projections (LELP)", "content": "Since large models develop embeddings that hold information not captured in their output probabilities [34], we aim to transfer this knowledge to student networks while adhering to three desiderata:\nModality-independent. We aim to develop a data-agnostic method, meaning its performance remains consistent regardless of the underlying data type. The method should not be reliant on characteristics specific to certain data modalities (for instance, the ability to easily perform data augmentation).\nCompatibility between differing Student/Teacher architectures. Embedding Distillation, the most straightforward method for learning the teacher's embedding information, only works out-of-the-box when the student and teacher have matching embedding dimensions. Otherwise, a learnable projection layer is required to match the student/teacher embedding dimensions, which can often harm performance (and even increase latency in certain cases). Therefore, we aim for a method that is embedding dimension agnostic.\nNo Retraining the Teacher Model. When the teacher model is significantly larger than the student model, retraining it can be prohibitively expensive. Methods which require a modified teacher training pipeline are significantly more costly to tune hyperparameters for, as not only do the teacher models have to be retrained multiple times, but in addition the student hyperparameters need to be checked against varying teacher hyparameters. This precludes methods such as Subclass Distillation [22], which contains several teacher training hyperparameters, and can even hurt the teacher's performance.\nWith these requirements in mind, we present Learning Embedding Linear Projections (LELP), which extracts the information in the teacher's embedding layer into pseudo-subclasses, on which the student is trained in a single unified cross-entropy loss. To motivate our method, we refer to the toy model of the learned teacher embeddings shown in Figure 2. Here, data points cluster around their respective class averages, but each cluster exhibits internal structure that holds semantically meaningful information. This structure differentiates individual items within the same class and has proven valuable for identifying subclasses when they exist [22]. Additionally, [34] shows that these subclasses can be separated using a linear probe (following an appropriate clustering of the embeddings space), which motivates the use of linear projections as a means of extracting further information for distillation.\nLELP extracts this embedding information into a single classification loss in three steps. Firstly, we identify meaningful linear subspaces in the teacher embedding space (section 3.1). Secondly, we project the teacher embeddings into these subspaces, and use them to form pseudo-subclasses, expanding the number of classes from C to S \u00d7 C, where S is the number of linear projections we employ per class (section 3.2). Finally, we perform standard knowledge distillation, where additionally the student network's final layer outputs probabilities for S \u00d7 C classes (section 3.3)."}, {"title": "3.1 Identifying Informative Linear Subspaces", "content": "The first step in LELP requires identifying linear subspaces in the teacher embedding space which contain useful information for each class cluster. Specifically, let $\\{x_i\\}_{i=1}^{N_c}$ be the $N_c$ training points in the dataset belonging to class c, and let $\\{h_i\\}_{i=1}^{N_c}$ be the corresponding teacher embeddings in $R^D$, that is, $h_i = h_{Teacher}(x_i)$, where $h_{Teacher}(x_i)$ is the teacher feature extractor. We want to find the S most informative linear directions in $\\{h_i\\}_{i=1}^{N_c}$. In absence of further knowledge of the structure of"}, {"title": "3.2 Splitting into pseudo-subclasses", "content": "Given the set of C class embedding means and S PCA vectors per class, we now describe how we split the C classes into SC subclasses. Let $p_c^{Teacher}$ be the teacher class probability of class c, with temperature parameter T. That is,\n$$p_c^{Teacher} = \\frac{e^{z_c/T}}{\\sum_{c'=1}^C e^{z_{c'}/T}},$$ \nwhere $z_c$ are teacher logits, i.e. $z_c = w_c^{Teacher} h - b_c$, with $w_c^{Teacher}$ the teacher weight for class c and $b_c$ be the bias. We split this into S subclass probabilities $p_{c,1},..., p_{c,S}$ where\n$$p_{c,s}^{Teacher} = p_c^{Teacher} * \\frac{e^{z_{c,s}/\\beta}}{\\sum_{s'=1}^S e^{z_{c,s'}/\\beta}},$$ \nwhere $z_{c,s} = \\tilde{v}_{c,s}^T (h - \\mu_c)$. That is, we perform a tempered softmax over subclass logits $z_{c,s}$, where $z_{c,s}$ are given by the PCA decomposition coordinates for that class. $\\beta$ is our subclass tempering parameter which is a hyperparameter in our method. We refer to this subclass splitting algorithm as subsplit, which takes as input the teacher embedding h, the PCA direction and mean vectors V and M, the teacher final layer weights W, and temperature $\\beta$. Pseudocode is in Appendix G."}, {"title": "3.3 Knowledge Distillation with subclasses", "content": "Finally, we perform standard knowledge distillation with our new SC probabilities $p_{c,s}$. This requires a straightforward modification to the student architecture, in which it outputs SC classes as opposed to the standard C classes. We applying the standard tempered Knowledge Distillation loss as prior work, using the same temperature $\\tau$ used to generate $p_{c,s}$:\n$$\\mathcal{L}_{LELP} = \\tau^2 D(p^{Teacher}||p^{Student}),$$ \nwhere D is a standard classification loss like Cross-Entropy or KL-Divergence. (In all our experiments in this paper we use the latter.)"}, {"title": "4 Experimental Evaluation", "content": "In this section we present our experimental results. In Section 4.1 we describe our experimental setup. Section 4.2 presents experiments showcasing that generating pseudo-classes via unsupervised clustering of the teacher model's embeddings can improve distillation effectiveness. In Section 4.3 we compare LELP with other distillation baselines."}, {"title": "4.1 The Setup", "content": "We focus our experiments on a variety of classification tasks, using a standard distillation setup as in [14]. In order to focus solely on the effect of the distillation loss of each method, we always set \u03b1 = 0 in (1). This reduces the variance between methods which may have different optimal values of \u03b1, and reduces the hyperparameter search space. Furthermore, in the important case of the semi-supervised setting one does not have access to ground-truth labels.\nThe Architectures Given that the specific combination of student and teacher architectures is known to influence the effectiveness of knowledge distillation, we have chosen to evaluate various student-teacher pairings to ensure the robustness of our method.\nFor Vision datasets, the \u201cideal\" distillation scenario is given by distilling ResNet-92 to ResNet-56, where both architectures and embeddings dimensions match (D = 256). We also consider the case of a smaller and a larger dimension with ResNet-92 (D = 256) and MobileNet with width and depth multiplier equal to 2 (D = 2048), respectively, distilling to MobileNet (D = 1024). The latter cases address scenarios where the embedding dimensions differ, with one scenario involving the same architecture family and the other involving different architectures. For smaller NLP datasets (LMRD, GLUE/cola, and GLUE/sst2), we consider distillation from an ALBERT-Large model (D = 1024) to an ALBERT-Base model (D = 768). For the larger scale NLP datasets (based on Amazon US reviews and Sentiment 140) we consider distillation from an ALBERT-XXL model (D = 4096) and an ALBERT-XL model (D = 2048) to (i) an ALBERT-Base model (D = 768); (ii) and a two-layer-MLP of width (D = 4096) that operates over representations generated by a (frozen) sentence-T5 encoder model of 11 billion parameters [23]. The latter case addresses the scenario involving different teacher-student architectures but with the same embedding dimension. (Using a pre-trained, frozen large-scale encoder model to generate representations is a common-in-practice approach when one needs multiple lightweight models for different classification tasks.)\nThe Baselines To establish a baseline performance, we chose well-known distillation approaches. In particular, we consider Standard Training of the model with the ground-truth labels and the cross-entropy loss, Vanilla Distillation with temperature-scaling as described in the original paper of [14], the Embedding Distillation method as described in Section 2, the general FitNet [28] approach\u00b9, the Variational Information Distillation for Knowledge Transfer (VID) framework [2], the Relational KD approach [25], Contrastive Representation Distillation CRD [31], Decoupled Knowledge Distillation [35] and Subclass Distillation [22]. As previously noted, DKD is functionally"}, {"title": "4.2 Investigating methods for inventing pseudo-classes", "content": "In this section we investigate the extent to which inventing pseudo-classes via unsupervised clustering of the teacher model's embeddings can be beneficial for distillation. Our investigation focuses on tasks with inherent subclass structure, driven by the hypothesis that class embeddings contain rich and informative structural details. Specifically, we utilize binarized versions of CIFAR-10 and CIFAR-100, assigning binary labels based on the original class labels ($Y_{binary} = Y_{original}\\%2$). These datasets present a unique challenge for knowledge distillation, as vanilla distillation is known to underperform due to limited class label information.\nCrucially, the availability of original labels ($Y_{original}$) in these subclass datasets allows us to explore the \"Oracle Clustering\" approach. This entails training the student model on the full 10/100-way classification task for CIFAR-10/100, respectively, and subsequently treating the problem as binary classification during testing. Interestingly, the student models trained utilizing the Oracle Clustering approach exhibit the highest performance among all evaluated clustering methods. This approach not only outperforms other clustering strategies but also surpasses the performance of the original teacher network in certain cases. A t-SNE visualization of the feature embeddings learned by the student models, presented in Figure 4, offers a deeper understanding of the underlying factors contributing to this observed performance advantage. While Oracle Clustering represents an idealized scenario where subclass structure is known a priori and thus impractical for real-world datasets, its consideration underscores the potential power of inventing pseudo-classes as a methodological approach.\nWe consider several natural ways of inventing pseudo-classes by clustering the teacher's embedding space and we compare them with LELP. In particular, we consider three approaches: (i) Agglomerative clustering (complete linkage); (ii) K-means clustering; and (iii) K-means clustering after we first have projected the teacher's embedding space in a two-dimensional space using t- SNE [13] as proposed in [34]. For these methods, we tested both hard-label (one-hot vector) and soft-label (weighted by teacher probabilities) representations of the subclasses during student training. We observed that soft-labels, even with temperature tuning, did not significantly impact performance. Therefore, we present results using only hard-label (one-hot vector) subclass representations, which can be found in Table 1 (see also Figure 3). Details on the number of clusters chosen can be found in Appendix H.3.\nAnalysis of Table 1 reveals several key findings: (i) LELP consistently outperforms all other clustering methods as well as Vanilla KD across all experimental scenarios; (ii) t-SNE & K-means generally demonstrate superior performance to Vanilla KD; (iii) Agglomerative clustering and K-means clustering exhibit more varied results in comparison to Vanilla KD. Thus, our findings suggest that the generation of pseudo-classes via clustering methodologies holds significant potential, however, the efficacy of this approach is contingent upon the specific clustering algorithm employed."}, {"title": "4.3 Comparison with previous approaches", "content": "In this section we compare LELP with other distillation baselines."}, {"title": "4.3.1 Warmup: Binary Classification with Subclass Structure", "content": "As a warmup, we first focus on on binary datasets that have an inherent subclass structure following the setting of Section 4.2. The results can be found in Table 3 in Appendix B. Among the baselines, Subclass Distillation often performs the best, consistent with the finding that subclass-splitting is effective with little label information [22]. Our method is able to capture the learned structure from"}, {"title": "4.3.2 Few-Class Classification without Subclass Structure", "content": "In this section we focus on datasets without subclass structure. We consider classification on six language classification tasks: the Large Movie Review dataset [20], two GLUE datasets: cola and sst2 [32], two datasets sampled from the Amazon US reviews datasets [8], and a dataset sampled from the Sentiment140 dataset [11]. Details are given in Appendix D. Our results are shown in Table 2. Notably, Subclass Distillation is typically the best performing baseline. LELP exceeds or performs as well as Subclass Distillation, but importantly does not require retraining the teacher, which in the age of ever growing large language models, becomes increasingly important. It is also worth noting that for the case of Amazon US Reviews-based datasets (among the largest ones considered in our study) where we distill to an ALBERT-Base model, LELP significantly outperforms even the teacher model, which contains over 20x the number of parameters."}, {"title": "5 Limitations", "content": "While our method is simple and efficient to implement, there may be limitations in using a simple linear projection on the teacher final layer embeddings to extract subclass data. Firstly, there is no reason to assume that the subclasses should be linearly separable in the teacher embedding, and it is likely that more sophisticated unsupervised clustering methods could extract richer information. Second, LELP performs when there is limited teacher logit information (such as in binary classification tasks), however larger image datasets with many classes contain sufficient information in their teacher logits, obviating the need of subclass splitting methods such as LELP. Indeed, as the number of classes increases, we anticipate LELP's performance to converge with vanilla knowledge distillation. (Consequently, we did not present experiments on datasets with a large number of classes, such as ImageNet-1k, since LELP is not designed for such scenarios.)"}, {"title": "6 Conclusion", "content": "In this study, we have presented evidence that the creation of pseudo-classes via unsupervised clustering of teacher embeddings can improve distillation performance in binary and few-class classification tasks, without necessitating the retraining of the teacher model. Through empirical evaluation, we observed that linear projections consistently yield high performance, prompting us to introduce LELP. The superior performance of the \"Oracle Clustering\" method, where subclass structure is known a priori, suggests that the generation of pseudo-classes through clustering techniques has substantial promise. Consequently, future research can explore more sophisticated methods for extracting teacher embedding information, drawing insights from the expanding body of work on Neural Collapse, or investigate strategies for distilling intermediate layer embeddings using a similar approach."}, {"title": "A Broader Impact Statement", "content": "Due to its popularity and the fact that deep learning is widely used in fields from NLP to robotics to autonomous vehicles, knowledge distillation, as a deep learning method, carries with it the potential for harmful societal impact. However, we feel that none of these impacts must be specifically highlighted here."}, {"title": "B Binary Classification with Subclass Structure: Experimental Results", "content": "Here we present the table with the experimental results corresponding to Sections 4.2 and 4.3.1."}, {"title": "C Ablations", "content": "One of the key design choices in LELP is to use the top PCA directions to construct the pseudo-subclasses. This is based on the intuition that axis which contain the most variation also contain the most information for distillation. Here we test this assumption by comparing PCA projections against three baselines. The first baseline is random projections (Rand), in which we choose S random orthogonal directions to use as our subclass direction. The second baseline is raw PCA, which is using the top-K PCA directions, without the additional projection and random rotation applied, as we discussed in Section 3. This corresponds to using vectors vc,s instead of \\tilde{v}_{c,s}. Our final baseline is the \"Identity\" projection, which is the same as using all of the teacher embeddings as subclasses. We consider the binarized CIFAR-100 task, and sweep S from 2 - 256, the embedding dimension of the teacher D = 256. We distill from ResNet92 to ResNet56, with the subclass temperature parameter B ranging from 2-5 \u2013 20, with results shown in Figure 5 (top row), averaged over three runs. For a more clear comparison to the random projection baseline, we also show the \u201cadvantange\" over random projections in fig. 5 (bottom row), made by subtracting the accuracy by the equivalent obtained by random projects with the same S and B. Figure 5 shows the following general trends:\nLarger S generally improves performance for Random Projections and LELP, and can harm PCA. For Rand and LELP, performance seems mostly monotonically increasing with larger S. For LELP the benefit plateaus around S = 32, while for Rand, the performance slowly increases until S = 256. This suggests that the PCA step used in LELP obtains the salient information from teacher embeddings in fewer S than Rand. For PCA without the extra steps used in LELP, large subclasses with higher temperatures significantly degrades performance, while adding rotation makes the performance of PCA stable. We conjecture that this is due to the first few PCA directions containing most of the variation and the remaining direction behaving as random noise.\nLELP consistently outperforms Random Projections. We see in fig. 5, when compared to random projections, LELP almost always has a non-trivial advantage. The performance of Identity projections is more inconsistent and it can be worse than random projections, depending on B. For larger number"}, {"title": "D Detailed description of the NLP datasets we used.", "content": "The Corpus of Linguistic Acceptability (GLUE/cola) is a binary classification task which consists of English acceptability judgments drawn from books and journal articles on linguistic theory. Each example is a sequence of words annotated with whether it is a grammatical English sentence. GLUE/cola contains 8, 551 examples for training and 1,063 examples for testing.\nThe Large Movie Review dataset (LMRD) is a dataset for binary sentiment classification containing 25, 000 movie reviews for training, and 25,000 for testing.\nThe Stanford Sentiment Treebank (GLUE/sst2) consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment (positive or negative) of a given sentence. It contains 67, 349 examples for training and 1, 821 examples for testing.\nThe Amazon US reviews datasets comprise a vast collection of over a hundred million customer reviews and ratings (ranging from 1 to 5 stars). For our study, we selected a subset of these reviews across various products and devised two classification tasks: a 5-classes classification task and a binary classification task, which we describe below.\n\u2022 \"Amazon Reviews\" contains 500, 000 examples for training and 10, 000 examples for testing. The task is to predict the star rating of the given review (ranging from 1 to 5). Both the training and test sets are balanced in terms of the number of examples per rating.\n\u2022 \"Amazon Reviews Bin\" is a subset of \"Amazon Reviews\", where we exclude all 3-star reviews. It consists of 400,000 training and 8,000 testing examples. The objective is to determine whether a review is \"polarized\" (rated either 1 or 5 stars) or \"mild\" (2 or 4 stars).\nFor sampling the examples Amazon Reviews dataset we use the following process. We first first sequentially parse and concatenate the first 200k examples of the following datasets: \u201cOffice Prod-"}, {"title": "E Multiclass Classification", "content": "While our primary focus lies on classification tasks with few classes, here we present results demonstrating the applicability of LELP to multiclass classification tasks with a moderate number of classes. To this end, we evaluate LELP on the CIFAR-10 and CIFAR-100 datasets. It is important to note that our objective is not to establish state-of-the-art performance in vision tasks but rather to showcase the versatility of our approach. We selected CIFAR-10 and CIFAR-100 as our datasets because they are widely recognized benchmarks for 10-class and 100-class classification, respectively. We intentionally disregarded image-specific techniques, such as data augmentation, to focus on a more general comparison. Consequently, we did not include methods like CRD and DKD that are specifically tailored to image data. Our results indicate that LELP can effectively handle tasks with a moderate number of classes and outperforms modality-independent knowledge distillation methods, particularly in scenarios where there is a significant disparity between the teacher and student architectures. As the number of classes increases, we anticipate LELP's performance to converge with vanilla knowledge distillation. Consequently, we did not conduct experiments on datasets with a large number of classes, such as ImageNet-1k.\nOur results are shown in Table 4, where we distill a ResNet92 to a MobileNet. In this comparison, LELP significantly outperforms baselines, seeing a 1.1% improvement and 2.31% improvement over the next best baseline in CIFAR-10 and CIFAR-100, respectively. This validates the hypothesis that providing subclass information in the form of a classification loss avoids the pitfalls that methods which directly match embeddings have when students and teachers differ. When there is either a large performance or architecture gap, embeddings do not transfer readily. Appendix F.1 contains additional experiments in semi-supervised knowledge distillation, where we find that LELP consistently outperforms existing baselines without modification in the semi-supervised setting."}, {"title": "F Data Efficiency, Training Speed and Robustness", "content": "As we demonstrate in Figure 6, LELP comes with several desirable properties. We consider the binary and standard CIFAR-100 datasets, ResNet92 as the teacher and ResNet56 and MobileNet, respectively, as the student. As we have already discussed in Appendix E, we selected CIFAR-10"}, {"title": "F.1 Semi-supervised KD experiments", "content": "Semi-supervised KD, also known as KD with unlabeled examples, is a potent training paradigm for generating compact and efficient student models in scenarios where labeled data is scarce but a large pool of unlabeled data exists. This approach employs a high-capacity teacher model to generate (soft) pseudo-labels for the unlabeled dataset, which are subsequently utilized to train the student model.\nDespite its widespread success in practice, the effectiveness of this powerful approach generally depends on the quality of the pseudo-labels generated by the teacher model. Indeed, training the student model on noisy pseudo-labels often leads to significant degradation of its generalization performance, and this is a well-known phenomenon that has been observed and studied in a plethora of papers in the literature, e.g., [3, 19, 27, 30, 33, 4, 15, 17]. Additionally, enforcing more teacher-student consistency, e.g., by blindly mimicking the teacher's embeddings, can even hinder performance when the teacher's output contains high noise (i.e., one may get worse performance than simply applying Vanilla KD). Finally, in this setting, often times the expense of using the teacher model to generate pseudo-labels can be a significant limitation. This becomes particularly problematic for methods like"}, {"title": "G Detailed Description of LELP", "content": "Here we provide further details and pseudo-code for the steps detailed in Section 3."}, {"title": "G.1 Step 1: PCA on Teacher Embeddings", "content": "Algorithm 1 Learning Embedding Linear Projections (LELP) - Step 1 - Computing Subclass Direction\nInput: Teacher model feature extractor $hTeacher(x)$, teacher final layer weight $Wr \u2208 RD\u00d7C$, dataset with class labels $\\{x_i, Y_i\\}_{i<N}$, with class counts Ne\nOutput: S \u00d7 C class vectors $V = \\{\\tilde{v}_{c,s}\\}$ and C class means $M = \\{\\mu_c\\}$\nInstantiate set of $\\tilde{v}_{c,s}$ as empty set $V\u2190 \\{\\}$\nCompute QR Decomposition of $W_T$, $W_T \u2190 QR(W_T)$ with Gram-Schmidt Algorithm\nfor c = 1 to C do\nGet set of all inputs $X_c = \\{x\\}$ belonging to class c\nCompute matrix of teacher features for class c, $H_c \u2208 RN_c\u00d7D$, s.t. $H_c = hTeacher (X_c)$\nProject $H_c$ onto null space of $W_T$: $H_c \u2190 H_c - H_cW_T$\nCompute top-S PCA on $H_c$ to obtain $\\{v_{c,s}\\}$ for 1 < s < S\nProduce random orthogonal matrix $Q_c \u2208 RS\u00d7S$\nSet $\\tilde{v}_{c,s} Q v_{c,s}$ for 1 < s < S\nCompute $\\{\\tilde{v}_{c,s}\\}$, as $\\sigma_{c,s}^2 = \\frac{1}{N_c}||H_c\\tilde{v}_{c,s}||2$\nNormalize $\\tilde{v}_{c,s}\u2190 \\frac{\\tilde{v}_{c,s}}{\\max_s{\\sigma_{c,s}}}$ (Normalizing $\\tilde{v}_{c,s}$)\nAdd $\\{\\tilde{v}_{c,s}\\}$ to V: $V\u2190 V\u222a \\{\\tilde{v}_{c,s}\\}$\nend for\nReturn: V\nThis corresponds to section 3.1, and the main goal of this step is to obtain S \u00d7 C vectors $\\tilde{v}_{s,c} \u2208 RD$ to create subclasses from, with D the teacher embedding dimension. The pseudocode is provided in algorithm 1. Note that the version provided in algorithm 1 works with the full teacher embedding matrix for a class $H_c \u2208 RN_c\u00d7D$, but it is straightforward to adapt this to a streaming approach that does not require storing all the embeddings at the same time by keeping running statistics. Note we additionally perform a small normalization step, so that the maximum variance along any of the teacher projection vectors $\\{\\tilde{v}_{c,s}\\}$ is 1 for any class."}, {"title": "G.2 Step 2/3: Knowledge Distillation with Subclasses", "content": "Algorithm 2 Subclass Splitting from teacher embedding", "\u03c4)\nInput": "Teacher embedding h\nC \u00d7 S subclass projection vectors $V = \\{\\tilde{v"}, {"w_c": "RD\u00d7C$ and biases $[b_1 ... b_c", "\u03c4\nOutput": "C \u00d7 S Teacher subclass probabilities: $p_{c", "logits": "z_c \u2190 w_c^{T"}, "h - b_c$\nCompute teacher C coarse label probabilities with temperature \u03c4: $p_c^{Teacher} \u2190 \\frac{e^{z_c/\u03c4}}{\\sum_{c'=1}^C e^{z_{c'}/\u03c4}}$ (\u03c4-tempered Softmax)\nfor c= 1 to C do\nCompute S subclass logits: $z_{c,s} \u2190 \\tilde{v}_{c,s}^{T} (h \u2013 \\mu_c)$\nCompute subclass probabilities for class c: $p_{c,s}^{Teacher} \u2190 p_c^{Teacher} * \\frac{e^{z_{c,s}/\u03b2}}{\\sum_{s'=1}^S e^{z_{c,s'}/\u03b2}}$. (\u03b2-tempered softmax over subclass logits)\nend for\nReturn: $p_{c,s}^{Teacher}$\nAlgorithm 3 Learning Embedding Linear Projections (LELP) - Knowledge Distillation with Sub-classes\nInput: Teacher model feature extractor $hTeacher (x)$\nC \u00d7 S subclass projection vectors $V = \\{\\tilde{v}_{c,s}\\}$\nC class mean vectors $M = \\{\\mu_c\\}$\nTeacher final layer classification weights $W = [w_1 ...w_c"], "\u03b7\nOutput": "Trained Student Model $\\theta$\nwhile Not converged do\nSample x ~ p(x)\nCompute teacher embedding $h^{Teacher"}, {"logits": "z_{c,s}^{"}]