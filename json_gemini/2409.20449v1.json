{"title": "Linear Projections of Teacher Embeddings for Few-Class Distillation", "authors": ["Noel Loo", "Fotis Iliopoulos", "Wei Hu", "Erik Vee"], "abstract": "Knowledge Distillation (KD) has emerged as a promising approach for transferring knowledge from a larger, more complex teacher model to a smaller student model. Traditionally, KD involves training the student to mimic the teacher's output probabilities, while more advanced techniques have explored guiding the student to adopt the teacher's internal representations. Despite its widespread success, the performance of KD in binary classification and few-class problems has been less satisfactory. This is because the information about the teacher model's generalization patterns scales directly with the number of classes. Moreover, several sophisticated distillation methods may not be universally applicable or effective for data types beyond Computer Vision. Consequently, effective distillation techniques remain elusive for a range of key real-world applications, such as sentiment analysis, search query understanding, and advertisement-query relevance assessment. Taking these observations into account, we introduce a novel method for distilling knowledge from the teacher model's representations, which we term Learning Embedding Linear Projections (LELP). Inspired by recent findings about the structure of final-layer representations, LELP works by identifying informative linear subspaces in the teacher's embedding space, and splitting them into pseudo-subclasses. The student model is then trained to replicate these pseudo-subclasses. Our experimental evaluations on large-scale NLP benchmarks like Amazon Reviews and Sentiment140 demonstrate that LELP is consistently competitive with, and typically superior to, existing state-of-the-art distillation algorithms for binary and few-class problems, where most KD methods suffer.", "sections": [{"title": "1 Introduction", "content": "While deep neural networks have revolutionized Natural Language Processing [9, 5], Computer Vision [29], and other fields, their ballooning size and data demands raise challenges. Recent research [21] aims to develop efficient models that excel without needing massive datasets or expensive hardware. Knowledge Distillation (KD) [6, 14] is a powerful approach for generating lightweight models by leveraging a large teacher model to guide their training. In its basic form, the student model is trained to replicate the teacher's output probabilities for each training instance. Additionally, several subsequent studies (e.g., [28, 16, 26, 2, 22]) have proposed advanced distillation techniques that go beyond mere output probability matching and focus on encouraging the student to learn the teacher's internal representations.\nWhile KD can significantly improve student model performance in tasks with numerous classes, its impact is less pronounced in binary classification and problems with a smaller number of classes. As [22] points out, this is because when we distill knowledge using logits or high-temperature cross-entropy, the information about the teacher model's generalization patterns scales directly with the number of classes. Furthermore, Knowledge Distillation research has primarily focused on Computer Vision, so many sophisticated distillation techniques are not always effective or even suitable for other data modalities like Natural Language. As a result, effective distillation techniques remain elusive for a range of critical real-world applications, such as sentiment analysis, search query understanding, and advertisement-query relevance assessment.\nIn light of these observations, in this paper we propose a novel approach for capturing information from the teacher's representations, which we call Learning Embedding Linear Projections (LELP). At a high level, it works by extracting knowledge form the teacher's last-layer representations (embeddings) and converting it into pseudo-subclasses via linear projections. The student model is then trained on these pseudo-subclasses, using a single unified cross-entropy loss.\nOur approach leverages recent findings about the structure of final-layer representations (embeddings) in deep learning models and, in particular, aspects of the phenomenon known as Neural Collapse [24, 10, 34]. Similar in spirit to Subclass Distillation [22], which uses a modified and retrained teacher model to identify hidden patterns, our method achieves improved student performance, particularly in finetuning Language Models for tasks with a few number of classes. Crucially though, and unlike Subclass Distillation, our approach does not require any retraining of the teacher model. Finally, a key advantage of LELP is its flexibility in bridging diverse model architectures, making it uniquely versatile for teacher-student learning scenarios.\nOur contributions can be summarized as follows:"}, {"title": "2 Related Work", "content": "Knowledge Distillation. Most of the literature on Knowledge Distillation has been focused on the fully supervised setting, i.e., when distillation is performed on the labeled training data of the teacher model rather than on new, unlabeled data see e.g. the original paper [14]. Specifically, when training the student one typically uses a convex combination of the standard cross-entropy loss LCE with respect to the ground-truth labels, and the distillation loss $\\mathcal{L}_{distill}$:\n$\\mathcal{L}_{student} = \\alpha \\mathcal{L}_{LCE} + (1 - \\alpha) \\mathcal{L}_{distill}$.\nIn the original paper [14] the distillation loss encourages the student model to mimic to replicate the teacher's output probabilities, potentially after scaling the logits of both models using a temperature-scalar. (The value of the temperature is typically larger than 1, and it is used to emphasize the differences between the probabilities of wrong answers that would all be very close to zero at temperature 1.)\nIn addition to focusing on the teacher model's final outputs, follow-up methods, like those proposed in [28, 2], also consider the teacher model's internal representations, often in the form of its embeddings. These methods encourage the student model to mimic not only the teacher's final predictions but also its internal representations. The simplest, yet often highly effective, example of this approach is Embedding Distillation [28]. In this method, an additional term, called the \"embedding-loss term\", is added to the distillation loss. This term measures the difference between the embeddings produced by the teacher and student models. The weight of this term can be adjusted using a hyperparameter. Specifically, one adds the term\n$\\mathcal{L}_{Embedd} = \\frac{1}{n} \\sum_{i=1}^{n} || f^T(x_i) - W f^S(x_i)||^2$,\nwhere $x_1,...,x_n$ denotes the current batch of examples, $f^T(x_i)$, $f^S(x_i)$ the embeddings of the teacher and student model corresponding to example $x_i$, respectively, and $W$ is a learnable projection"}, {"title": "3 Learning from Embedding Linear Projections (LELP)", "content": "Since large models develop embeddings that hold information not captured in their output probabilities [34], we aim to transfer this knowledge to student networks while adhering to three desiderata:"}, {"title": "3.1 Identifying Informative Linear Subspaces", "content": "The first step in LELP requires identifying linear subspaces in the teacher embedding space which contain useful information for each class cluster. Specifically, let ${x_i}_{i=1}^{N_c}$, be the $N_c$ training points in the dataset belonging to class $c$, and let ${h_i}_{i=1}^{N_c}$ be the corresponding teacher embeddings in $\\mathbb{R}^D$, that is, $h = h_{Teacher}(x)$, where $h_{Teacher}(x)$ is the teacher feature extractor. We want to find the $S$ most informative linear directions in ${h_i}_{i=1}^{N_c}$. In absence of further knowledge of the structure of"}, {"title": "3.2 Splitting into pseudo-subclasses", "content": "Given the set of $C$ class embedding means and $S$ PCA vectors per class, we now describe how we split the $C$ classes into $SC$ subclasses. Let $p_{c}^{Teacher}$ be the teacher class probability of class $c$, with temperature parameter $T$. That is,\n$p_{c}^{Teacher} = \\frac{e^{z_c/T}}{\\sum_{i=1}^{C}e^{z_i/T}}$,\nwhere $z_c$ are teacher logits, i.e. $z_c = w_c^{Teacher}h - b_c$, with $w_c^{Teacher}$ the teacher weight for class $c$ and $b_c$ the bias. We split this into $S$ subclass probabilities $p_{c,1},..., p_{c,s}$ where\n$p_{c,s}^{Teacher} = p_{c}^{Teacher} * \\frac{e^{z_{c,s}/\\beta}}{\\sum_{i=1}^{S}e^{z_{c,j}/\\beta}}$,\nwhere $z_{c,s} = \\tilde{v}_{c,s}^T(h - \\mu_c)$. That is, we perform a tempered softmax over subclass logits $z_{c,s}$, where\n$z_{c,s}$ are given by the PCA decomposition coordinates for that class. $\\beta$ is our subclass tempering parameter which is a hyperparameter in our method. We refer to this subclass splitting algorithm as subsplit, which takes as input the teacher embedding $h$, the PCA direction and mean vectors $V$ and $M$, the teacher final layer weights $W$, and temperature $\\beta$. Pseudocode is in Appendix G."}, {"title": "3.3 Knowledge Distillation with subclasses", "content": "Finally, we perform standard knowledge distillation with our new $SC$ probabilities $p_{c,s}$. This requires a straightforward modification to the student architecture, in which it outputs $SC$ classes as opposed to the standard $C$ classes. We applying the standard tempered Knowledge Distillation loss as prior work, using the same temperature $\\tau$ used to generate $p_{c,s}$:\n$\\mathcal{L}_{LELP} = \\tau^2 D(p^{Teacher}||p^{Student})$,\nwhere $D$ is a standard classification loss like Cross-Entropy or KL-Divergence. (In all our experiments in this paper we use the latter.)"}, {"title": "4 Experimental Evaluation", "content": "In this section we present our experimental results. In Section 4.1 we describe our experimental setup. Section 4.2 presents experiments showcasing that generating pseudo-classes via unsupervised clustering of the teacher model's embeddings can improve distillation effectiveness. In Section 4.3 we compare LELP with other distillation baselines."}, {"title": "4.1 The Setup", "content": "We focus our experiments on a variety of classification tasks, using a standard distillation setup as in [14]. In order to focus solely on the effect of the distillation loss of each method, we always set $\\alpha = 0$ in (1). This reduces the variance between methods which may have different optimal values of $\\alpha$, and reduces the hyperparameter search space. Furthermore, in the important case of the semi-supervised setting one does not have access to ground-truth labels."}, {"title": "4.2 Investigating methods for inventing pseudo-classes", "content": "In this section we investigate the extent to which inventing pseudo-classes via unsupervised clustering of the teacher model's embeddings can be beneficial for distillation. Our investigation focuses on tasks with inherent subclass structure, driven by the hypothesis that class embeddings contain rich and informative structural details. Specifically, we utilize binarized versions of CIFAR-10 and CIFAR-100, assigning binary labels based on the original class labels ($Y_{binary} = Y_{original}\\%2$). These datasets present a unique challenge for knowledge distillation, as vanilla distillation is known to underperform due to limited class label information.\nCrucially, the availability of original labels ($Y_{original}$) in these subclass datasets allows us to explore the \"Oracle Clustering\" approach. This entails training the student model on the full 10/100-way classification task for CIFAR-10/100, respectively, and subsequently treating the problem as binary classification during testing. Interestingly, the student models trained utilizing the Oracle Clustering approach exhibit the highest performance among all evaluated clustering methods. This approach not only outperforms other clustering strategies but also surpasses the performance of the original teacher network in certain cases. A t-SNE visualization of the feature embeddings learned by the student models, presented in Figure 4, offers a deeper understanding of the underlying factors contributing to this observed performance advantage. While Oracle Clustering represents an idealized scenario where subclass structure is known a priori and thus impractical for real-world datasets, its consideration underscores the potential power of inventing pseudo-classes as a methodological approach.\nWe consider several natural ways of inventing pseudo-classes by clustering the teacher's embedding space and we compare them with LELP. In particular, we consider three approaches: (i) Agglomerative clustering (complete linkage); (ii) K-means clustering; and (iii) K-means clustering after we first have projected the teacher's embedding space in a two-dimensional space using t- SNE [13] as proposed in [34]. For these methods, we tested both hard-label (one-hot vector) and soft-label (weighted by teacher probabilities) representations of the subclasses during student training. We observed that soft-labels, even with temperature tuning, did not significantly impact performance. Therefore, we present results using only hard-label (one-hot vector) subclass representations, which can be found in Table 1 (see also Figure 3). Details on the number of clusters chosen can be found in Appendix H.3.\nAnalysis of Table 1 reveals several key findings: (i) LELP consistently outperforms all other clustering methods as well as Vanilla KD across all experimental scenarios; (ii) t-SNE & K-means generally demonstrate superior performance to Vanilla KD; (iii) Agglomerative clustering and K-means clustering exhibit more varied results in comparison to Vanilla KD. Thus, our findings suggest that the generation of pseudo-classes via clustering methodologies holds significant potential, however, the efficacy of this approach is contingent upon the specific clustering algorithm employed."}, {"title": "4.3 Comparison with previous approaches", "content": "In this section we compare LELP with other distillation baselines."}, {"title": "4.3.1 Warmup: Binary Classification with Subclass Structure", "content": "As a warmup, we first focus on on binary datasets that have an inherent subclass structure following the setting of Section 4.2. The results can be found in Table 3 in Appendix B. Among the baselines, Subclass Distillation often performs the best, consistent with the finding that subclass-splitting is effective with little label information [22]. Our method is able to capture the learned structure from"}, {"title": "4.3.2 Few-Class Classification without Subclass Structure", "content": "In this section we focus on datasets without subclass structure. We consider classification on six language classification tasks: the Large Movie Review dataset [20], two GLUE datasets: cola and sst2 [32], two datasets sampled from the Amazon US reviews datasets [8], and a dataset sampled from the Sentiment140 dataset [11]. Details are given in Appendix D. Our results are shown in Table 2. Notably, Subclass Distillation is typically the best performing baseline. LELP exceeds or performs as well as Subclass Distillation, but importantly does not require retraining the teacher, which in the age of ever growing large language models, becomes increasingly important. It is also worth noting that for the case of Amazon US Reviews-based datasets (among the largest ones considered in our study) where we distill to an ALBERT-Base model, LELP significantly outperforms even the teacher model, which contains over 20x the number of parameters."}, {"title": "5 Limitations", "content": "While our method is simple and efficient to implement, there may be limitations in using a simple linear projection on the teacher final layer embeddings to extract subclass data. Firstly, there is no reason to assume that the subclasses should be linearly separable in the teacher embedding, and it is likely that more sophisticated unsupervised clustering methods could extract richer information. Second, LELP performs when there is limited teacher logit information (such as in binary classi- fication tasks), however larger image datasets with many classes contain sufficient information in their teacher logits, obviating the need of subclass splitting methods such as LELP. Indeed, as the number of classes increases, we anticipate LELP's performance to converge with vanilla knowledge distillation. (Consequently, we did not present experiments on datasets with a large number of classes, such as ImageNet-1k, since LELP is not designed for such scenarios.)"}, {"title": "6 Conclusion", "content": "In this study, we have presented evidence that the creation of pseudo-classes via unsupervised cluster- ing of teacher embeddings can improve distillation performance in binary and few-class classification tasks, without necessitating the retraining of the teacher model. Through empirical evaluation, we observed that linear projections consistently yield high performance, prompting us to introduce LELP. The superior performance of the \"Oracle Clustering\" method, where subclass structure is known a priori, suggests that the generation of pseudo-classes through clustering techniques has substantial promise. Consequently, future research can explore more sophisticated methods for extracting teacher embedding information, drawing insights from the expanding body of work on Neural Collapse, or investigate strategies for distilling intermediate layer embeddings using a similar approach."}, {"title": "A Broader Impact Statement", "content": "Due to its popularity and the fact that deep learning is widely used in fields from NLP to robotics to autonomous vehicles, knowledge distillation, as a deep learning method, carries with it the potential for harmful societal impact. However, we feel that none of these impacts must be specifically highlighted here."}, {"title": "B Binary Classification with Subclass Structure: Experimental Results", "content": "Here we present the table with the experimental results corresponding to Sections 4.2 and 4.3.1."}, {"title": "C Ablations", "content": "One of the key design choices in LELP is to use the top PCA directions to construct the pseudo- subclasses. This is based on the intuition that axis which contain the most variation also contain the most information for distillation. Here we test this assumption by comparing PCA projections against three baselines. The first baseline is random projections (Rand), in which we choose S random orthogonal directions to use as our subclass direction. The second baseline is raw PCA, which is using the top-K PCA directions, without the additional projection and random rotation applied, as we discussed in Section 3. This corresponds to using vectors $v_{c,s}$ instead of $\\tilde{v}_{c,s}$. Our final baseline is the \"Identity\" projection, which is the same as using all of the teacher embeddings as subclasses. We consider the binarized CIFAR-100 task, and sweep S from 2 - 256, the embedding dimension of the teacher D = 256. We distill from ResNet92 to ResNet56, with the subclass temperature parameter $\\beta$ ranging from 2-5 \u2013 20, with results shown in Figure 5 (top row), averaged over three runs. For a more clear comparison to the random projection baseline, we also show the \u201cadvantange\" over random projections in fig. 5 (bottom row), made by subtracting the accuracy by the equivalent obtained by random projects with the same S and $\\beta$. Figure 5 shows the following general trends:\nLarger S generally improves performance for Random Projections and LELP, and can harm PCA. For Rand and LELP, performance seems mostly monotonically increasing with larger S. For LELP the benefit plateaus around S = 32, while for Rand, the performance slowly increases until S = 256. This suggests that the PCA step used in LELP obtains the salient information from teacher embeddings in fewer S than Rand. For PCA without the extra steps used in LELP, large subclasses with higher temperatures significantly degrades performance, while adding rotation makes the performance of PCA stable. We conjecture that this is due to the first few PCA directions containing most of the variation and the remaining direction behaving as random noise.\nLELP consistently outperforms Random Projections. We see in fig. 5, when compared to random projections, LELP almost always has a non-trivial advantage. The performance of Identity projections is more inconsistent and it can be worse than random projections, depending on $\\beta$. For larger number"}, {"title": "D Detailed description of the NLP datasets we used.", "content": "The Corpus of Linguistic Acceptability (GLUE/cola) is a binary classification task which consists of English acceptability judgments drawn from books and journal articles on linguistic theory. Each example is a sequence of words annotated with whether it is a grammatical English sentence. GLUE/cola contains 8, 551 examples for training and 1,063 examples for testing.\nThe Large Movie Review dataset (LMRD) is a dataset for binary sentiment classification containing 25, 000 movie reviews for training, and 25,000 for testing.\nThe Stanford Sentiment Treebank (GLUE/sst2) consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment (positive or negative) of a given sentence. It contains 67, 349 examples for training and 1, 821 examples for testing.\nThe Amazon US reviews datasets comprise a vast collection of over a hundred million customer reviews and ratings (ranging from 1 to 5 stars). For our study, we selected a subset of these reviews across various products and devised two classification tasks: a 5-classes classification task and a binary classification task, which we describe below.\n\u2022 \"Amazon Reviews\" contains 500, 000 examples for training and 10, 000 examples for testing. The task is to predict the star rating of the given review (ranging from 1 to 5). Both the training and test sets are balanced in terms of the number of examples per rating.\n\u2022 \"Amazon Reviews Bin\" is a subset of \"Amazon Reviews\", where we exclude all 3-star reviews. It consists of 400,000 training and 8,000 testing examples. The objective is to determine whether a review is \"polarized\" (rated either 1 or 5 stars) or \"mild\" (2 or 4 stars)."}, {"title": "E Multiclass Classification", "content": "While our primary focus lies on classification tasks with few classes, here we present results demon- strating the applicability of LELP to multiclass classification tasks with a moderate number of classes. To this end, we evaluate LELP on the CIFAR-10 and CIFAR-100 datasets. It is important to note that our objective is not to establish state-of-the-art performance in vision tasks but rather to showcase the versatility of our approach. We selected CIFAR-10 and CIFAR-100 as our datasets because they are widely recognized benchmarks for 10-class and 100-class classification, respectively. We intentionally disregarded image-specific techniques, such as data augmentation, to focus on a more general comparison. Consequently, we did not include methods like CRD and DKD that are specifically tailored to image data. Our results indicate that LELP can effectively handle tasks with a moderate number of classes and outperforms modality-independent knowledge distillation methods, particularly in scenarios where there is a significant disparity between the teacher and student architectures. As the number of classes increases, we anticipate LELP's performance to converge with vanilla knowledge distillation. Consequently, we did not conduct experiments on datasets with a large number of classes, such as ImageNet-1k."}, {"title": "F Data Efficiency, Training Speed and Robustness", "content": "As we demonstrate in Figure 6, LELP comes with several desirable properties. We consider the binary and standard CIFAR-100 datasets, ResNet92 as the teacher and ResNet56 and MobileNet, respectively, as the student. As we have already discussed in Appendix E, we selected CIFAR-10"}, {"title": "F.1 Semi-supervised KD experiments", "content": "Semi-supervised KD, also known as KD with unlabeled examples, is a potent training paradigm for generating compact and efficient student models in scenarios where labeled data is scarce but a large pool of unlabeled data exists. This approach employs a high-capacity teacher model to generate (soft) pseudo-labels for the unlabeled dataset, which are subsequently utilized to train the student model.\nDespite its widespread success in practice, the effectiveness of this powerful approach generally depends on the quality of the pseudo-labels generated by the teacher model. Indeed, training the student model on noisy pseudo-labels often leads to significant degradation of its generalization performance, and this is a well-known phenomenon that has been observed and studied in a plethora of papers in the literature, e.g., [3, 19, 27, 30, 33, 4, 15, 17]. Additionally, enforcing more teacher-student consistency, e.g., by blindly mimicking the teacher's embeddings, can even hinder performance when the teacher's output contains high noise (i.e., one may get worse performance than simply applying Vanilla KD). Finally, in this setting, often times the expense of using the teacher model to generate pseudo-labels can be a significant limitation. This becomes particularly problematic for methods like"}, {"title": "G Detailed Description of LELP", "content": "Here we provide further details and pseudo-code for the steps detailed in Section 3."}, {"title": "G.1 Step 1: PCA on Teacher Embeddings", "content": "Algorithm 1 Learning Embedding Linear Projections (LELP) - Step 1 - Computing Subclass Direction\nInput: Teacher model feature extractor $h^{Teacher}(x)$, teacher final layer weight $W_T \\in \\mathbb{R}^{D \\times C}$, dataset with class labels ${x_i, Y_i}_{i<N}$, with class counts $N_c$\nOutput: $S \\times C$ class vectors $V = {\\tilde{v}_{c,s}}$ and $C$ class means $M = {\\mu_c}$\nInstantiate set of $\\tilde{v}_{c,s}$ as empty set $V \\leftarrow {}$\nCompute QR Decomposition of $W_T, W_T \\leftarrow QR(W_T)$ with Gram-Schmidt Algorithm\nfor $c = 1$ to $C$ do\nGet set of all inputs $X_c = {x}$ belonging to class $c$\nCompute matrix of teacher features for class $c, H_c \\in \\mathbb{R}^{N_c \\times D}$, s.t. $H_c = h^{Teacher}(X)$\nProject $H_c$ onto null space of $W_T: H^c \\leftarrow H^c - H^cW_T$\nCompute top-S PCA on $H^c$ to obtain ${v_{c,s}}$ for $1 < s < S$\nProduce random orthogonal matrix $Q_c \\in \\mathbb{R}^{S \\times S}$\nSet $\\tilde{v}_{c,s} \\leftarrow Q^c v_{c,s}$ for $1 < s < S$\nCompute {$\\sigma_{c,s}^2$}, as $\\sigma_{c,s}^2 = ||H^c v_{c,s}||^2$\nNormalize $\\tilde{v}_{c,s} \\leftarrow \\frac{\\tilde{v}_{c,s}}{\\max_{s}{\\sigma_{c,s}^2}}$ (Normalizing $\\tilde{v}_{c,s}$)\nAdd {$\\tilde{v}_{c,s}$} to $V: V \\leftarrow V \\cup {\\tilde{v}_{c,s}}$\nend for\nReturn: $V$\nThis corresponds to section 3.1, and the main goal of this step is to obtain $S \\times C$ vectors $\\tilde{v}_{s,c} \\in \\mathbb{R}^D$ to create subclasses from, with D the teacher embedding dimension. The pseudocode is provided in algorithm 1. Note that the version provided in algorithm 1 works with the full teacher embedding matrix for a class $H_c \\in \\mathbb{R}^{N_c \\times D}$, but it is straightforward to adapt this to a streaming approach that does not require storing all the embeddings at the same time by keeping running statistics. Note we additionally perform a small normalization step, so that the maximum variance along any of the teacher projection vectors {$\\tilde{v}_{c,s}$} is 1 for any class."}, {"title": "G.2 Step 2/3: Knowledge Distillation with Subclasses", "content": "Algorithm 2 Subclass Splitting from teacher embedding, subsplit(h, V, M, W, $\\beta$, $\\tau$)\nInput: Teacher embedding h\n$C \\times S$ subclass projection vectors $V = {\\tilde{v}_{c,s}}$\nC class means $M = {\\mu_c}$\nTeacher final layer classification weights $W = [w_1 ... w_c] \\in \\mathbb{R}^{D \\times C}$ and biases $[b_1 ... b_c]$\nSubclass Temperature $\\beta$\nStudent-Teacher Temperature $\\tau$\nOutput: $C \\times S$ Teacher subclass probabilities: $p_{c,s}^{Teacher}$\nCompute teacher coarse label logits: $z_c \\leftarrow w^T h - b_c$\nCompute teacher C coarse label probabilities with temperature $\\tau: p_c^{Teacher} \\leftarrow \\frac{e^{z_c/\\tau}}{\\sum_{i=1}^{C}e^{z_i/\\tau}}$ ($\\tau$-tempered Softmax)\nfor c= 1 to C do\nCompute S subclass logits: $z_{c,s} \\leftarrow \\tilde{v}_s^T (h - \\mu_c)$\nCompute subclass probabilities for class c: $p_{c,s}^{Teacher} \\leftarrow P_{c}^{Teacher} * \\frac{e^{z_{c,s}/\\beta}}{\\sum_{i=1}^{S}e^{z_{c,j}/\\beta}}$ ($\\beta$-tempered softmax over subclass logits)\nend for\nReturn: $p_{c,s}^{Teacher}$"}, {"title": "H Implementation Details", "content": "We implemented all algorithms in Python and used the TensorFlow deep learning library [1]. We ran our experiments on 64 Cloud TPU v4s each with two cores.\nFor a fair comparison, we use the teacher's last-layer embeddings throughout the distillation process across all relevant baselines (Embedding Distillation, FitNet, VID, Relational KD, CRD). Further- more, to handle mismatches in embedding dimensions between teacher and student, we introduce a trainable fully connected layer as a learnable projection for all these methods. (For Vision datasets, we have experimented with convolutional layers as suggested in [28], and while these do reduce the size of the student model, they do not significantly impact its performance compared to the fully connected ones.)\nFor FitNet and CRD, in any given experiment, we pre-train the learnable projection once. This same pre-trained projection is then utilized consistently across all three trials corresponding to the experiment. The optimizer used for the pre-training of the learnable projection is Adam with initial learning rate 10-3 and 10-6 for Vision (200 epochs training) and Natural Language (40 epochs training) datasets, respectively.\nWe implement VID by using the loss function as described in (5) of [2] where the squared difference in the second term is taken over the teacher's and student's embeddings (using a learnable projection if there is a mismatch in their dimensions).\nWe implement Relational KD using the loss function as described in (9) and (10) of [25].\nFor CRD we implement the objective described in (17) of [31] and perform grid search over {0.1, 1.0, 5.0, 10.0, 100.0} for its coefficient. We consider the standard negative sampling policy where a pair of examples (x, y) is considered \u201cnegative\" if x \u2260 y, i.e., given a batch the number of positive pairs is equal to the batch size. (Note that in Vision classification tasks the standard approach would be to generate variations of every image (e.g, via rotations), and then also consider as positive any pair of examples (original or modified) that come from the same source. However, data augmentation of Natural Language examples is not straightforward and this impairs the performance of the method this is the main point we are trying to convey here.)\nFor DKD we implement equations (1), (2) and (7) in [35]. We set the 'target' hyperparameter \u03b1 equal to 1 and perform grid search for the \"non-target\" hyperparameter \u03b2 over {1, 2, 4, 6, 8, 10} (following examples in [35])."}, {"title": "H.1 Vision Datasets", "content": "In every experiment, both the teacher and student models are trained for 200 epochs.\nFor training ResNet-92 we use the SGD optimizer with initial learning rate $10^{-3}$, non-Nesterov momentum value equal to 0.9, cosine annealing learning rate schedule (minimum learning rate value is set to $10^{-6}$), and batch size 256. For training ResNet-56 we use the SGD optimizer with initial learning rate $5.10^{-2}$, Nesterov momentum value equal to 0.9, cosine annealing learning rate schedule (minimum learning rate value is set to $10^{-6}$) and batch size 256. (This is a training schedule similar to the one described in [30]).\nFor training MobileNet (both the larger teacher-model and the student-model) we use the Adam optimizer with initial learning rate $lr = 0.001$ and batch size 128. We then proceed according to the following learning rate schedule for 200 epochs (see, e.g., [12]):\n $lr \\leftarrow\n  \\begin{cases}\n    lr*0.5 * 10^{-3}, & \\text{if #epochs > 180} \\\\\n    lr * 10^{-3}, & \\text{if #epochs > 160} \\\\\n    lr * 10^{-2}, & \\text{if #epochs > 120} \\\\\n    lr * 10^{-1}, & \\text{if #epochs > 80}\n  \\end{cases}$\nFinally, in all cases we use data-augmentation. In particular, we use random horizontal flipping and random width and height translations with width and height factor, respectively, equal to 0.1."}, {"title": "H.2 Natural Language Datasets", "content": "For the GLUE/COLA dataset, the teacher model undergoes training for 2 epochs. In the case of GLUE/SST-2 and the Large Movie Review Dataset, teacher model training extends to 3 epochs. We employ the Adam optimizer (batch size 64, initial learning rate $10^{-5}$) for these training processes. To minimize variance across experiments, we consistently train the student model for 40 epochs using a smaller learning rate. The training uses the Adam optimizer with a batch size of 64 and an initial learning rate of $10^{-6}$.\nFor both the Amazon Reviews datasets, the teacher and student models (both the ALBERT-base and the MLPs over frozen-T5 embeddings) are trained for 2 epochs using the Adam optimizer with a batch size of 64 and an initial learning rate of $10^{-6}$.\nFor the Sentiment140 Bin dataset the teacher and the student models are trained for 1 epoch using the Adam optimizer with batch size of 64 and an initial learning rate of $10^{-6}$."}, {"title": "H.3 Hyperparameter optimization", "content": "In this section, we present the hyperparameter optimization (grid search) procedure we followed for each method, dataset and experiment of Section 4 and Appendix 4.2. For Vision datasets, bold numbers correspond to the hyperparameters chosen for the case of ResNet56 student, the number in italicized numbers correspond to the"}]}