[{"title": "Linear Projections of Teacher Embeddings for\nFew-Class Distillation", "authors": ["Noel Loo", "Fotis Iliopoulos", "Wei Hu", "Erik Vee"], "abstract": "Knowledge Distillation (KD) has emerged as a promising approach for transferring\nknowledge from a larger, more complex teacher model to a smaller student model.\nTraditionally, KD involves training the student to mimic the teacher's output prob-\nabilities, while more advanced techniques have explored guiding the student to\nadopt the teacher's internal representations. Despite its widespread success, the\nperformance of KD in binary classification and few-class problems has been less\nsatisfactory. This is because the information about the teacher model's gener-\nalization patterns scales directly with the number of classes. Moreover, several\nsophisticated distillation methods may not be universally applicable or effective\nfor data types beyond Computer Vision. Consequently, effective distillation tech-\nniques remain elusive for a range of key real-world applications, such as sentiment\nanalysis, search query understanding, and advertisement-query relevance assess-\nment. Taking these observations into account, we introduce a novel method for\ndistilling knowledge from the teacher model's representations, which we term\nLearning Embedding Linear Projections (LELP). Inspired by recent findings about\nthe structure of final-layer representations, LELP works by identifying informative\nlinear subspaces in the teacher's embedding space, and splitting them into pseudo-\nsubclasses. The student model is then trained to replicate these pseudo-subclasses.\nOur experimental evaluations on large-scale NLP benchmarks like Amazon Re-\nviews and Sentiment140 demonstrate that LELP is consistently competitive with,\nand typically superior to, existing state-of-the-art distillation algorithms for binary\nand few-class problems, where most KD methods suffer.", "sections": [{"title": "1 Introduction", "content": "While deep neural networks have revolutionized Natural Language Processing [9, 5], Computer\nVision [29], and other fields, their ballooning size and data demands raise challenges. Recent\nresearch [21] aims to develop efficient models that excel without needing massive datasets or\nexpensive hardware. Knowledge Distillation (KD) [6, 14] is a powerful approach for generating\nlightweight models by leveraging a large teacher model to guide their training. In its basic form,\nthe student model is trained to replicate the teacher's output probabilities for each training instance.\nAdditionally, several subsequent studies (e.g., [28, 16, 26, 2, 22]) have proposed advanced distillation\ntechniques that go beyond mere output probability matching and focus on encouraging the student to\nlearn the teacher's internal representations.\nWhile KD can significantly improve student model performance in tasks with numerous classes, its\nimpact is less pronounced in binary classification and problems with a smaller number of classes.\nAs [22] points out, this is because when we distill knowledge using logits or high-temperature cross-\nentropy, the information about the teacher model's generalization patterns scales directly with the\nnumber of classes. Furthermore, Knowledge Distillation research has primarily focused on Computer\nVision, so many sophisticated distillation techniques are not always effective or even suitable for other\ndata modalities like Natural Language. As a result, effective distillation techniques remain elusive for\na range of critical real-world applications, such as sentiment analysis, search query understanding,\nand advertisement-query relevance assessment.\nIn light of these observations, in this paper we propose a novel approach for capturing information\nfrom the teacher's representations, which we call Learning Embedding Linear Projections (LELP).\nAt a high level, it works by extracting knowledge form the teacher's last-layer representations\n(embeddings) and converting it into pseudo-subclasses via linear projections. The student model is\nthen trained on these pseudo-subclasses, using a single unified cross-entropy loss.\nOur approach leverages recent findings about the structure of final-layer representations (embeddings)\nin deep learning models and, in particular, aspects of the phenomenon known as Neural Collapse [24,\n10, 34]. Similar in spirit to Subclass Distillation [22], which uses a modified and retrained teacher\nmodel to identify hidden patterns, our method achieves improved student performance, particularly\nin finetuning Language Models for tasks with a few number of classes. Crucially though, and unlike\nSubclass Distillation, our approach does not require any retraining of the teacher model. Finally, a\nkey advantage of LELP is its flexibility in bridging diverse model architectures, making it uniquely\nversatile for teacher-student learning scenarios.\nOur contributions can be summarized as follows:\n1. Motivated by recent insights into the Neural Collapse phenomenon, we demonstrate that\nthe invention of pseudo-classes through unsupervised clustering of teacher embeddings can\nenhance distillation performance in binary and few-class classification tasks. However, the\nefficacy of this approach is contingent upon the specific clustering algorithm employed."}, {"title": "2 Related Work", "content": "Knowledge Distillation. Most of the literature on Knowledge Distillation has been focused on the\nfully supervised setting, i.e., when distillation is performed on the labeled training data of the teacher\nmodel rather than on new, unlabeled data see e.g. the original paper [14]. Specifically, when\ntraining the student one typically uses a convex combination of the standard cross-entropy loss LCE\nwith respect to the ground-truth labels, and the distillation loss $L_{distill}$:\n$L_{student} = \\alpha L_{CE} + (1 - \\alpha) L_{distill}$.\nIn the original paper [14] the distillation loss encourages the student model to mimic to replicate the\nteacher's output probabilities, potentially after scaling the logits of both models using a temperature-\nscalar. (The value of the temperature is typically larger than 1, and it is used to emphasize the\ndifferences between the probabilities of wrong answers that would all be very close to zero at\ntemperature 1.)\nIn addition to focusing on the teacher model's final outputs, follow-up methods, like those proposed\nin [28, 2], also consider the teacher model's internal representations, often in the form of its embed-\ndings. These methods encourage the student model to mimic not only the teacher's final predictions\nbut also its internal representations. The simplest, yet often highly effective, example of this approach\nis Embedding Distillation [28]. In this method, an additional term, called the \"embedding-loss term\",\nis added to the distillation loss. This term measures the difference between the embeddings produced\nby the teacher and student models. The weight of this term can be adjusted using a hyperparameter.\nSpecifically, one adds the term\n$L_{Embedd} = \\frac{1}{n} \\sum_{i=1}^{n} || f^T(x_i) - W f^S(x_i)||^2$,\nwhere $x_1,...,x_n$ denotes the current batch of examples, $f^T(x_i), f^S(x_i)$ the embeddings of the\nteacher and student model corresponding to example $x_i$, respectively, and $W$ is a learnable projection"}, {"title": "3 Learning from Embedding Linear Projections (LELP)", "content": "Since large models develop embeddings that hold information not captured in their output probabilities\n[34], we aim to transfer this knowledge to student networks while adhering to three desiderata:\nModality-independent. We aim to develop a data-agnostic method, meaning its performance remains\nconsistent regardless of the underlying data type. The method should not be reliant on characteristics\nspecific to certain data modalities (for instance, the ability to easily perform data augmentation).\nCompatibility between differing Student/Teacher architectures. Embedding Distillation, the most\nstraightforward method for learning the teacher's embedding information, only works out-of-the-\nbox when the student and teacher have matching embedding dimensions. Otherwise, a learnable\nprojection layer is required to match the student/teacher embedding dimensions, which can often\nharm performance (and even increase latency in certain cases). Therefore, we aim for a method that\nis embedding dimension agnostic.\nNo Retraining the Teacher Model. When the teacher model is significantly larger than the student\nmodel, retraining it can be prohibitively expensive. Methods which require a modified teacher training\npipeline are significantly more costly to tune hyperparameters for, as not only do the teacher models\nhave to be retrained multiple times, but in addition the student hyperparameters need to be checked\nagainst varying teacher hyparameters. This precludes methods such as Subclass Distillation [22],\nwhich contains several teacher training hyperparameters, and can even hurt the teacher's performance.\nWith these requirements in mind, we present Learning Embedding Linear Projections (LELP), which\nextracts the information in the teacher's embedding layer into pseudo-subclasses, on which the\nstudent is trained in a single unified cross-entropy loss. To motivate our method, we refer to the\ntoy model of the learned teacher embeddings shown in Figure 2. Here, data points cluster around\ntheir respective class averages, but each cluster exhibits internal structure that holds semantically\nmeaningful information. This structure differentiates individual items within the same class and\nhas proven valuable for identifying subclasses when they exist [22]. Additionally, [34] shows that\nthese subclasses can be separated using a linear probe (following an appropriate clustering of the\nembeddings space), which motivates the use of linear projections as a means of extracting further\ninformation for distillation.\nLELP extracts this embedding information into a single classification loss in three steps. Firstly,\nwe identify meaningful linear subspaces in the teacher embedding space (section 3.1). Secondly,\nwe project the teacher embeddings into these subspaces, and use them to form pseudo-subclasses,\nexpanding the number of classes from C to S \u00d7 C, where S is the number of linear projections\nwe employ per class (section 3.2). Finally, we perform standard knowledge distillation, where\nadditionally the student network's final layer outputs probabilities for S \u00d7 C classes (section 3.3)."}, {"title": "3.1 Identifying Informative Linear Subspaces", "content": "The first step in LELP requires identifying linear subspaces in the teacher embedding space which\n$N_c$\n$N$\ncontain useful information for each class cluster. Specifically, let $\\{x_i\\}_{i=1}$, be the $N_c$ training points\nin the dataset belonging to class c, and let $\\{h_i\\}_{i=1}$ be the corresponding teacher embeddings in $R^D$,\nthat is, $h_i = h_{Teacher}(x_i)$, where $h_{Teacher}(x)$ is the teacher feature extractor. We want to find the S\nmost informative linear directions in $\\{h_i\\}_{i=1}$. In absence of further knowledge of the structure of"}, {"title": "3.2 Splitting into pseudo-subclasses", "content": "Given the set of C class embedding means and S PCA vectors per class, we now describe how we\nsplit the C classes into SC subclasses. Let $p_c^{Teacher}$ be the teacher class probability of class c, with\ntemperature parameter T. That is,\n$p_c^{Teacher} = \\frac{e^{z_c/T}}{\\sum_{i=1}^{C} e^{z_i/T}}$\nwhere $z_c$ are teacher logits, i.e. $z_c = w_c^{Teacher}h - b_c$, with $w_c^{Teacher}$ the teacher weight for class c and\n$b_c$ be the bias. We split this into S subclass probabilities $p_{c,1},..., p_{c,S}$ where\n$p_{c,s}^{Teacher} = p_c^{Teacher} * \\frac{e^{z_{c,s}/\\beta}}{\\sum_{i=1}^{S} e^{z_{c,i}/\\beta}}$\nwhere $z_{c,s} = v_{c,s}^T(h - \\mu_c)$. That is, we perform a tempered softmax over subclass logits $z_{c,s}$, where\n$z_{c,s}$ are given by the PCA decomposition coordinates for that class. $\u03b2$ is our subclass tempering\nparameter which is a hyperparameter in our method. We refer to this subclass splitting algorithm as\nsubsplit, which takes as input the teacher embedding h, the PCA direction and mean vectors V\nand M, the teacher final layer weights W, and temperature $\u03b2$. Pseudocode is in Appendix G."}, {"title": "3.3 Knowledge Distillation with subclasses", "content": "Finally, we perform standard knowledge distillation with our new SC probabilities $p_{c,s}$. This requires\na straightforward modification to the student architecture, in which it outputs SC classes as opposed\nto the standard C classes. We applying the standard tempered Knowledge Distillation loss as prior\nwork, using the same temperature $\u03c4$ used to generate $p_{c,s}$:\n$L_{LELP} = \\tau^2 D(p^{Teacher}||p^{Student})$,\nwhere D is a standard classification loss like Cross-Entropy or KL-Divergence. (In all our experiments\nin this paper we use the latter.)"}, {"title": "4 Experimental Evaluation", "content": "In this section we present our experimental results. In Section 4.1 we describe our experimental\nsetup. Section 4.2 presents experiments showcasing that generating pseudo-classes via unsupervised\nclustering of the teacher model's embeddings can improve distillation effectiveness. In Section 4.3\nwe compare LELP with other distillation baselines."}, {"title": "4.1 The Setup", "content": "We focus our experiments on a variety of classification tasks, using a standard distillation setup as in\n[14]. In order to focus solely on the effect of the distillation loss of each method, we always set $\u03b1 = 0$\nin (1). This reduces the variance between methods which may have different optimal values of $\u03b1$, and\nreduces the hyperparameter search space. Furthermore, in the important case of the semi-supervised\nsetting one does not have access to ground-truth labels.\nThe Architectures Given that the specific combination of student and teacher architectures is\nknown to influence the effectiveness of knowledge distillation, we have chosen to evaluate various\nstudent-teacher pairings to ensure the robustness of our method.\nFor Vision datasets, the \u201cideal\" distillation scenario is given by distilling ResNet-92 to ResNet-56,\nwhere both architectures and embeddings dimensions match (D = 256). We also consider the case\nof a smaller and a larger dimension with ResNet-92 (D = 256) and MobileNet with width and depth\nmultiplier equal to 2 (D = 2048), respectively, distilling to MobileNet (D = 1024). The latter cases\naddress scenarios where the embedding dimensions differ, with one scenario involving the same\narchitecture family and the other involving different architectures. For smaller NLP datasets (LMRD,\nGLUE/cola, and GLUE/sst2), we consider distillation from an ALBERT-Large model (D = 1024)\nto an ALBERT-Base model (D = 768). For the larger scale NLP datasets (based on Amazon US\nreviews and Sentiment 140) we consider distillation from an ALBERT-XXL model (D = 4096)\nand an ALBERT-XL model (D = 2048) to (i) an ALBERT-Base model (D = 768); (ii) and a\ntwo-layer-MLP of width (D = 4096) that operates over representations generated by a (frozen)\nsentence-T5 encoder model of 11 billion parameters [23]. The latter case addresses the scenario\ninvolving different teacher-student architectures but with the same embedding dimension. (Using a\npre-trained, frozen large-scale encoder model to generate representations is a common-in-practice\napproach when one needs multiple lightweight models for different classification tasks.)\nThe Baselines To establish a baseline performance, we chose well-known distillation approaches.\nIn particular, we consider Standard Training of the model with the ground-truth labels and the\ncross-entropy loss, Vanilla Distillation with temperature-scaling as described in the original paper\nof [14], the Embedding Distillation method as described in Section 2, the general FitNet [28]\napproach\u00b9, the Variational Information Distillation for Knowledge Transfer (VID) framework [2],\nthe Relational KD approach [25], Contrastive Representation Distillation CRD [31], Decoupled\nKnowledge Distillation [35] and Subclass Distillation [22]. As previously noted, DKD is functionally"}, {"title": "4.2 Investigating methods for inventing pseudo-classes", "content": "In this section we investigate the extent to which inventing pseudo-classes via unsupervised clustering\nof the teacher model's embeddings can be beneficial for distillation. Our investigation focuses\non tasks with inherent subclass structure, driven by the hypothesis that class embeddings contain\nrich and informative structural details. Specifically, we utilize binarized versions of CIFAR-10 and\nCIFAR-100, assigning binary labels based on the original class labels ($Y_{binary} = Y_{original}\\%2$). These\ndatasets present a unique challenge for knowledge distillation, as vanilla distillation is known to\nunderperform due to limited class label information.\nCrucially, the availability of original labels ($Y_{original}$) in these subclass datasets allows us to explore\nthe \"Oracle Clustering\" approach. This entails training the student model on the full 10/100-way\nclassification task for CIFAR-10/100, respectively, and subsequently treating the problem as binary\nclassification during testing. Interestingly, the student models trained utilizing the Oracle Clustering\napproach exhibit the highest performance among all evaluated clustering methods. This approach not\nonly outperforms other clustering strategies but also surpasses the performance of the original teacher\nnetwork in certain cases. A t-SNE visualization of the feature embeddings learned by the student\nmodels, presented in Figure 4, offers a deeper understanding of the underlying factors contributing to\nthis observed performance advantage. While Oracle Clustering represents an idealized scenario where\nsubclass structure is known a priori and thus impractical for real-world datasets, its consideration\nunderscores the potential power of inventing pseudo-classes as a methodological approach.\nWe consider several natural ways of inventing pseudo-classes by clustering the teacher's embedding\nspace and we compare them with LELP. In particular, we consider three approaches: (i) Agglomerative\nclustering (complete linkage); (ii) K-means clustering; and (iii) K-means clustering after we first have\nprojected the teacher's embedding space in a two-dimensional space using t- SNE [13] as proposed\nin [34]. For these methods, we tested both hard-label (one-hot vector) and soft-label (weighted by\nteacher probabilities) representations of the subclasses during student training. We observed that\nsoft-labels, even with temperature tuning, did not significantly impact performance. Therefore, we\npresent results using only hard-label (one-hot vector) subclass representations, which can be found in\nTable 1 (see also Figure 3). Details on the number of clusters chosen can be found in Appendix H.3.\nAnalysis of Table 1 reveals several key findings: (i) LELP consistently outperforms all other clustering\nmethods as well as Vanilla KD across all experimental scenarios; (ii) t-SNE & K-means generally\ndemonstrate superior performance to Vanilla KD; (iii) Agglomerative clustering and K-means clus-\ntering exhibit more varied results in comparison to Vanilla KD. Thus, our findings suggest that the\ngeneration of pseudo-classes via clustering methodologies holds significant potential, however, the\nefficacy of this approach is contingent upon the specific clustering algorithm employed."}, {"title": "4.3 Comparison with previous approaches", "content": "In this section we compare LELP with other distillation baselines."}, {"title": "4.3.1 Warmup: Binary Classification with Subclass Structure", "content": "As a warmup, we first focus on on binary datasets that have an inherent subclass structure following\nthe setting of Section 4.2. The results can be found in Table 3 in Appendix B. Among the baselines,\nSubclass Distillation often performs the best, consistent with the finding that subclass-splitting is\neffective with little label information [22]. Our method is able to capture the learned structure from"}, {"title": "4.3.2 Few-Class Classification without Subclass Structure", "content": "In this section we focus on datasets without subclass structure. We consider classification on six\nlanguage classification tasks: the Large Movie Review dataset [20], two GLUE datasets: cola and\nsst2 [32], two datasets sampled from the Amazon US reviews datasets [8], and a dataset sampled from\nthe Sentiment140 dataset [11]. Details are given in Appendix D. Our results are shown in Table 2.\nNotably, Subclass Distillation is typically the best performing baseline. LELP exceeds or performs as\nwell as Subclass Distillation, but importantly does not require retraining the teacher, which in the age\nof ever growing large language models, becomes increasingly important. It is also worth noting that\nfor the case of Amazon US Reviews-based datasets (among the largest ones considered in our study)\nwhere we distill to an ALBERT-Base model, LELP significantly outperforms even the teacher model,\nwhich contains over 20x the number of parameters."}, {"title": "5 Limitations", "content": "While our method is simple and efficient to implement, there may be limitations in using a simple\nlinear projection on the teacher final layer embeddings to extract subclass data. Firstly, there is no\nreason to assume that the subclasses should be linearly separable in the teacher embedding, and it\nis likely that more sophisticated unsupervised clustering methods could extract richer information.\nSecond, LELP performs when there is limited teacher logit information (such as in binary classi-\nfication tasks), however larger image datasets with many classes contain sufficient information in\ntheir teacher logits, obviating the need of subclass splitting methods such as LELP. Indeed, as the\nnumber of classes increases, we anticipate LELP's performance to converge with vanilla knowledge\ndistillation. (Consequently, we did not present experiments on datasets with a large number of classes,\nsuch as ImageNet-1k, since LELP is not designed for such scenarios.)"}, {"title": "6 Conclusion", "content": "In this study, we have presented evidence that the creation of pseudo-classes via unsupervised cluster-\ning of teacher embeddings can improve distillation performance in binary and few-class classification\ntasks, without necessitating the retraining of the teacher model. Through empirical evaluation, we\nobserved that linear projections consistently yield high performance, prompting us to introduce LELP.\nThe superior performance of the \"Oracle Clustering\" method, where subclass structure is known a\npriori, suggests that the generation of pseudo-classes through clustering techniques has substantial\npromise. Consequently, future research can explore more sophisticated methods for extracting teacher\nembedding information, drawing insights from the expanding body of work on Neural Collapse, or\ninvestigate strategies for distilling intermediate layer embeddings using a similar approach."}, {"title": "A Broader Impact Statement", "content": "Due to its popularity and the fact that deep learning is widely used in fields from NLP to robotics to\nautonomous vehicles, knowledge distillation, as a deep learning method, carries with it the potential\nfor harmful societal impact. However, we feel that none of these impacts must be specifically\nhighlighted here."}, {"title": "B Binary Classification with Subclass Structure: Experimental Results", "content": "Here we present the table with the experimental results corresponding to Sections 4.2 and 4.3.1."}, {"title": "C Ablations", "content": "One of the key design choices in LELP is to use the top PCA directions to construct the pseudo-\nsubclasses. This is based on the intuition that axis which contain the most variation also contain the\nmost information for distillation. Here we test this assumption by comparing PCA projections against\nthree baselines. The first baseline is random projections (Rand), in which we choose S random\northogonal directions to use as our subclass direction. The second baseline is raw PCA, which is\nusing the top-K PCA directions, without the additional projection and random rotation applied, as we\ndiscussed in Section 3. This corresponds to using vectors $v_{c,s}$ instead of $\\tilde{v}_{c,s}$. Our final baseline is\nthe \"Identity\" projection, which is the same as using all of the teacher embeddings as subclasses. We\nconsider the binarized CIFAR-100 task, and sweep S from 2 - 256, the embedding dimension of the\nteacher D = 256. We distill from ResNet92 to ResNet56, with the subclass temperature parameter $\u03b2$\nranging from 2-5 \u2013 20, with results shown in Figure 5 (top row), averaged over three runs. For a more\nclear comparison to the random projection baseline, we also show the \u201cadvantange\" over random\nprojections in fig. 5 (bottom row), made by subtracting the accuracy by the equivalent obtained by\nrandom projects with the same S and $\u03b2$. Figure 5 shows the following general trends:"}, {"title": "D Detailed description of the NLP datasets we used.", "content": "The Corpus of Linguistic Acceptability (GLUE/cola) is a binary classification task which consists\nof English acceptability judgments drawn from books and journal articles on linguistic theory.\nEach example is a sequence of words annotated with whether it is a grammatical English sentence.\nGLUE/cola contains 8, 551 examples for training and 1,063 examples for testing.\nThe Large Movie Review dataset (LMRD) is a dataset for binary sentiment classification containing\n25, 000 movie reviews for training, and 25,000 for testing.\nThe Stanford Sentiment Treebank (GLUE/sst2) consists of sentences from movie reviews and human\nannotations of their sentiment. The task is to predict the sentiment (positive or negative) of a given\nsentence. It contains 67, 349 examples for training and 1, 821 examples for testing.\nThe Amazon US reviews datasets comprise a vast collection of over a hundred million customer\nreviews and ratings (ranging from 1 to 5 stars). For our study, we selected a subset of these reviews\nacross various products and devised two classification tasks: a 5-classes classification task and a\nbinary classification task, which we describe below.\n\u2022 \"Amazon Reviews\" contains 500, 000 examples for training and 10, 000 examples for testing.\nThe task is to predict the star rating of the given review (ranging from 1 to 5). Both the\ntraining and test sets are balanced in terms of the number of examples per rating.\n\u2022 \"Amazon Reviews Bin\" is a subset of \"Amazon Reviews\", where we exclude all 3-star\nreviews. It consists of 400,000 training and 8,000 testing examples. The objective is to\ndetermine whether a review is \"polarized\" (rated either 1 or 5 stars) or \"mild\" (2 or 4 stars)."}, {"title": "E Multiclass Classification", "content": "While our primary focus lies on classification tasks with few classes, here we present results demon-\nstrating the applicability of LELP to multiclass classification tasks with a moderate number of\nclasses. To this end, we evaluate LELP on the CIFAR-10 and CIFAR-100 datasets. It is important\nto note that our objective is not to establish state-of-the-art performance in vision tasks but rather\nto showcase the versatility of our approach. We selected CIFAR-10 and CIFAR-100 as our datasets\nbecause they are widely recognized benchmarks for 10-class and 100-class classification, respectively.\nWe intentionally disregarded image-specific techniques, such as data augmentation, to focus on\na more general comparison. Consequently, we did not include methods like CRD and DKD that\nare specifically tailored to image data. Our results indicate that LELP can effectively handle tasks\nwith a moderate number of classes and outperforms modality-independent knowledge distillation\nmethods, particularly in scenarios where there is a significant disparity between the teacher and\nstudent architectures. As the number of classes increases, we anticipate LELP's performance to\nconverge with vanilla knowledge distillation. Consequently, we did not conduct experiments on\ndatasets with a large number of classes, such as ImageNet-1k.\nOur results are shown in Table 4, where we distill a ResNet92 to a MobileNet. In this comparison,\nLELP significantly outperforms baselines, seeing a 1.1% improvement and 2.31% improvement\nover the next best baseline in CIFAR-10 and CIFAR-100, respectively. This validates the hypothesis\nthat providing subclass information in the form of a classification loss avoids the pitfalls that\nmethods which directly match embeddings have when students and teachers differ. When there is\neither a large performance or architecture gap, embeddings do not transfer readily. Appendix F.1\ncontains additional experiments in semi-supervised knowledge distillation, where we find that LELP\nconsistently outperforms existing baselines without modification in the semi-supervised setting."}, {"title": "F Data Efficiency, Training Speed and Robustness", "content": "As we demonstrate in Figure 6, LELP comes with several desirable properties. We consider the\nbinary and standard CIFAR-100 datasets, ResNet92 as the teacher and ResNet56 and MobileNet,\nrespectively, as the student. As we have already discussed in Appendix E, we selected CIFAR-10"}, {"title": "F.1 Semi-supervised KD experiments", "content": "Semi-supervised KD, also known as KD with unlabeled examples, is a potent training paradigm for\ngenerating compact and efficient student models in scenarios where labeled data is scarce but a large\npool of unlabeled data exists. This approach employs a high-capacity teacher model to generate (soft)\npseudo-labels for the unlabeled dataset, which are subsequently utilized to train the student model.\nDespite its widespread success in practice, the effectiveness of this powerful approach generally\ndepends on the quality of the pseudo-labels generated by the teacher model. Indeed, training the\nstudent model on noisy pseudo-labels often leads to significant degradation of its generalization\nperformance, and this is a well-known phenomenon that has been observed and studied in a plethora of\npapers in the literature, e.g., [3, 19, 27, 30, 33, 4, 15, 17]. Additionally, enforcing more teacher-student\nconsistency, e.g., by blindly mimicking the teacher's embeddings, can even hinder performance when\nthe teacher's output contains high noise (i.e., one may get worse performance than simply applying\nVanilla KD). Finally, in this setting, often times the expense of using the teacher model to generate\npseudo-labels can be a significant limitation. This becomes particularly problematic for methods like"}, {"title": "G Detailed Description of LELP", "content": "Here we provide further details and pseudo-code for the steps detailed in Section 3."}, {"title": "G.1 Step 1: PCA on Teacher Embeddings", "content": "Algorithm 1 Learning Embedding Linear Projections (LELP) - Step 1 - Computing Subclass Direction\nInput: Teacher model feature extractor $h_{Teacher}(x)$ teacher final layer weight $W_T \\in R^{D \\times C}$,\ndataset with class labels $\\{x_i, Y_i\\}_{i<N}$, with class counts $N_c$\nOutput: S \u00d7 C class vectors $V = \\{v_{c,s}\\}$ and C class means $M = \\{\\mu_c\\}$\nInstantiate set of $\\tilde{v}_{c,s}$ as empty set $V\\leftarrow \\{\\}$\nCompute QR Decomposition of $W_T$, $W_T \\leftarrow QR(W_T)$ with Gram-Schmidt Algorithm\nfor c = 1 to C do\nGet set of all inputs $X_c = \\{x\\}$ belonging to class c\nCompute matrix of teacher features for class c, $H_c \\in R^{N_c\\times D}$, s.t. $H_c = h_{Teacher}(X_c)$\nProject $H_c$ onto null space of $W_T$: $H_c \\leftarrow H_c - H_cW_T$\nCompute top-S PCA on $H_c$ to obtain $\\{v_{c,s}\\}$ for 1 < s < S\nProduce random orthogonal matrix $Q_c \\in R^{S \\times S}$\nSet $\\tilde{v}_{c,s} Qv_{c,s}$ for $1 \\leq s \\leq S$\nCompute $\\{\\tilde{v}_{c,s}\\}$, as $\\sigma_{c,s}^2 = \\frac{1}{N_c} ||H_c\\tilde{v}_{c,s}||^2$\nNormalize $\\tilde{v}_{c,s} \\leftarrow \\frac{\\tilde{v}_{c,s}}{\\max_{s\\{\\sigma_{c,s}\\}}}}$ (Normalizing $\\tilde{v}_{c,s}$)\nAdd $\\{\\tilde{v}_{c,s}\\}$ to $V$ : $V\\leftarrow V \\cup \\{\\tilde{v}_{c,s}\\}$\nend for\nReturn: V\nThis corresponds to section 3.1, and the main goal of this step is to obtain S \u00d7 C vectors $\\tilde{v}_{s,c} \\in R^{D}$\nto create subclasses from, with D the teacher embedding dimension. The pseudocode is provided in\nalgorithm 1. Note that the version provided in algorithm 1 works with the full teacher embedding\nmatrix for a class $H_c \\in R^{N_c\\times D}$, but it is straightforward to adapt this to a streaming approach that\ndoes not require storing all the embeddings at the same time by keeping running statistics. Note\nwe additionally perform a small normalization step, so that the maximum variance along any of the\nteacher projection vectors $\\{\\tilde{v}_{c,s}\\}$ is 1 for any class."}, {"title": "G.2 Step 2/3: Knowledge Distillation with Subclasses", "content": "Algorithm 2 Subclass Splitting from teacher embedding", "\u03c4)\nInput": "Teacher embedding h\nC \u00d7 S subclass projection vectors $V = \\{\\tilde{v"}, {"w_c": "in R^{D\\times C"}, "and biases $[b_1 ... b_c"], "\u03c4\nOutput": "C \u00d7 S Teacher subclass probabilities: $p^{Teacher"}, {"logits": "z_c \\leftarrow w^T_c h - b_c$\nCompute teacher C coarse label probabilities with temperature \u03c4: $p^{Teacher}_{c} \\leftarrow \\frac{e^{z_c/\\tau}}{\\sum_{i=1}^{C} e^{z_i/\\tau}}$ (\u03c4-tempered\nSoftmax)\nfor c"}]