{"title": "GENERALIZED UNCERTAINTY-BASED EVIDENTIAL FUSION WITH HYBRID\nMULTI-HEAD ATTENTION FOR WEAK-SUPERVISED TEMPORAL ACTION\nLOCALIZATION", "authors": ["Yuanpeng He", "Lijian Li", "Tianxiang Zhan", "Wenpin Jiao", "Chi-Man Pun"], "abstract": "Weakly supervised temporal action localization (WS-TAL) is\na task of targeting at localizing complete action instances\nand categorizing them with video-level labels. Action-\nbackground ambiguity, primarily caused by background\nnoise resulting from aggregation and intra-action variation,\nis a significant challenge for existing WS-TAL methods.\nIn this paper, we introduce a hybrid multi-head attention\n(HMHA) module and generalized uncertainty-based eviden-\ntial fusion (GUEF) module to address the problem. The\nproposed HMHA effectively enhances RGB and optical flow\nfeatures by filtering redundant information and adjusting\ntheir feature distribution to better align with the WS-TAL\ntask. Additionally, the proposed GUEF adaptively eliminates\nthe interference of background noise by fusing snippet-level\nevidences to refine uncertainty measurement and select su-\nperior foreground feature information, which enables the\nmodel to concentrate on integral action instances to achieve\nbetter action localization and classification performance. Ex-\nperimental results conducted on the THUMOS14 dataset\ndemonstrate that our method outperforms state-of-the-art\nmethods.", "sections": [{"title": "1. INTRODUCTION", "content": "As one of the most essential tasks of video understanding,\nTemporal Action localization (TAL) targets at accurately\npositioning action boundaries, including start and end times-\nstamps of action instances in untrimmed video and sorting\nout them. Numerous early studies [1, 2, 3] focus on utiliz-\ning fully-supervised methods to solve the task and achieve\nnotable performance. Nevertheless, fully supervised meth-\nods necessitate a large number of videos with frame-wise\nannotations, which are time-consuming and labor-intensive.\nFurthermore, annotations provided by different annotators\nhave distinct biases. Therefore, to address the aforemen-\ntioned problems, various weakly supervised temporal action\nlocalization (WS-TAL) methods [5, 6, 7, 8] have been pro-\nposed in recent years that only rely on lightly obtainable\nvideo-level labels, avoiding extensive annotation costs.\nMost weakly supervised methods adopt the localization-\nby-classification pattern that involves training an action clas-\nsifier and applying it to generate a Class Activation Sequence\n(CAS), which consists of classification probabilities for snip-\npets. Meanwhile, an attention branch is used to compute a\nweight sequence that represents the foreground probability of\neach snippet. Subsequently, a video-level prediction is ob-\ntained by aggregating the top-k snippets based on the atten-\ntion sequence. However, for the sake of lacking fine-grained\nannotations, achieving great temporal action localization per-\nformance is still problematic with video-level labels. The pri-\nmary challenge faced by current WS-TAL methods is action-\nbackground ambiguity. Specifically, during the training pro-\ncess, the classifier tends to pay attention to salient features,\nleading to the misclassification of background snippets as ac-\ntions, while disregarding less prominent action snippets. This\ntendency enables the model to only take a fraction of action\nsnippets rather than an entire action instance into consider-\nation, resulting in inaccurate localization and classification\nresults. Moreover, most existing WS-TAL methods directly\nemploy a pre-trained I3D model to extract RGB and optical\nflow features, which encompass a large quantity of redundant\ninformation irrelevant to the task, consequently impeding per-\nformance.\nTo address the action-background ambiguity problem, we\npropose a Generalized Uncertainty-Based Evidential fusion\n(GUEF) module for WS-TAL, inspired by Traditional Evi-\ndential Deep Learning (TEDL), which is capable of obtaining\nthe uncertainty of predictions by computing the uncertainty\nmeasure of each snippet-level evidence to filter useless back-\nground snippets. Within the GUEF module, the disturbance\nof background information is quantified by video-level uncer-"}, {"title": "2. METHODOLOGY", "content": ""}, {"title": "2.1. Hybrid Multi-Head Attention", "content": "A Hybrid Multi-Head Attention (HMHA), which consists of\ntwo sharing multi-head attention modules and a filtering mod-\nule with an attention-like information interaction mechanism,\nis applied to enable the two modalities' weight distribution\nto approach each other. Following the existing methods, the\nuntrimmed video V is split into W 16-frame snippets, which\nis non-overlapping, and a localization-by-classification strat-\negy is adopted. The structure of this module is illustrated in\nFig 1. Because both optical flow features and RGB features\n$X^{Flow}, X^{RGB} \\in R^{D \\times W}$ are extracted by pre-trained mod-\nels, i.e., I3D [19], there is numerous redundant information\nthat is irrelevant to WS-TAL task in both features. Therefore,\nboth of them are fed into two sharing multi-head attention\nmodules MHA to obtain two weights $A^{Flow}, A^{RGB} \\in R^{W}$,\nwhich are employed to eliminate task-irrelevant information\ncontained in two initial features [20]. The process can be stip-\nulated as below:\n$A^{Flow}, A^{RGB} = MHA(X^{Flow}), MHA(X^{RGB})$\n$X^{Flow} = X^{Flow} \\otimes \\sigma(A^{Flow} \\otimes A^{RGB})$\n$X^{RGB} = X^{RGB} \\otimes \\sigma(A^{RGB} \\otimes A^{Flow})$\nwhere $A^{Flow}$ and $A^{RGB}$ can be regarded as \u201cquery\u201d and\n\u201ckey\u201d in multi-head attention module and $\\sigma(\\cdot)$ represents\nSigmoid function. With optimized optical flow $X^{Flow}$ and\nRGB features $X^{RGB}$, we intend to use a filtering module"}, {"title": "2.2. Deep Learning with Generalized Uncertainty-Based\nEvidential Fusion (GUEF)", "content": "Evidential deep learning is developed on the basis of Subjec-\ntive Logic-based Dempster-Shafer evidence theory [21, 23]\nwhich aims to avoid overconfidence of softmax-based classi-\nfiers on false predictions. Traditional evidential deep learning\nis designed under a frame of discernment which has N mutu-\nally exclusive singletons pk, k = 1, ..., N with corresponding\nbelief mass m({pk}) and takes an overall uncertainty mass U\ninto consideration. Specifically, the sum of N + 1 value of\nmass equals 1, and each of them is non-negative, which can\nbe defined as:\n$\\sum_{k=1}^{N} m({p_k}) + U = 1$\nwhere $m_k > 0$ and $U \\geq 0$. Assume $e_k \\geq 0$ is an evidence\ncorresponding to kth singleton, then mass of belief $m_k$ and\nuncertainty U can be defined as:\n$m({P_k}) = \\frac{e_k}{\\sum_{s=1}^{N} (e_s + 1)}$ , $U = \\frac{S}{\\sum_{s=1}^{N} (e_s + 1)} = \\frac{1}{S} \\sum_{s=1}^{N} (1)$"}, {"title": "2.3. Learning and Inference", "content": "A classifier C is utilized to predict class activation sequence\n(CAS) over snippet-wise features $F = [f_1, ..., f_w] \\in R^{D \\times W}$\nand D is dimension of features, which is given as z =\n[$z_1, ..., z_w$] \u2208 $R^{W\u00d7(T+1)}$. Moreover, an attention score\nA = [$A_i, ..., A_w$] \u2208 $R^{W}$ is produced by the hybrid multi-\nhead attention, which represents the possibility that snippets\nbelong to the foreground. Then, the overall classification\nprobability \u00f4 can be obtained by aggregating CAS, which\nutilizes the most preferable L snippets according to attention\nscore A. The process of aggregation can be defined as:\n$\\hat{p} = \\frac{1}{L} \\sum_{t \\in \\xi} z_t \\quad \\quad \\xi = arg max_\\xi \\sum_{t \\in \\xi} A_t$\nwhere $z_t$ = C(F). On the basis of prediction $p^$, we optimize\nit by comparing it with ground-truth label p:\n$L_{cla}$ = Cross_entropy(p, p)\nThen, considering the attention score A, which indicates\nthe probability of snippets belonging to the foreground and\nprobability $z_{t,T+1}$ in z \u2208 $R^{W\u00d7(T+1)}$ represents background\nprobability of tth snippet. It is intuitive to consider that A\nand $z_{t,T+1}$ are complementary which are controlled by uncer-\ntainty measures produced by generalized evidential fusion:\n$L_{uef} = (\\Delta \\cdot tanh(\\sigma(h)\\phi(m_s({\\Theta}))) + 1) \\sum_{t=1}^{W} L_1norm|1 - A_t - z_{t,T+1}|$\nwhere $m_s({\\Theta})$ is acquired by sorting snippet-level uncer-\ntainty $m({\\Theta})$ from each evidence in a descending order and\n$\\sigma(h) = -1 \\in [-1, 1]$, h = 1, ..., H. More specifically,\nh represents the current epoch reference, H denotes the to-\ntal number of training epochs. Moreover, $\\phi(m_s({\\Theta}) =$\n$2m_s({\\Theta}) - 1 \\in [-1, 1]$, $m_s$ = 1, ..., W and A controls am-\nplitudes of changes of $tanh(\\sigma(h)\\phi(m_s({\\Theta})}$. Based on\nthe traditional evidential deep learning (TEDL), with respect to\na certain sample X, the corresponding Dirichlet distribution\ncan be obtained as:\n$D(q|\\alpha) = \\begin{cases}\n\\frac{1}{B(\\alpha)} \\prod_{j=1}^{T} q_j^{\\alpha_j - 1}, for q \\in S_T\n0, Otherwise\n\\end{cases}$\nwhere $\\alpha_j$ corresponds to $e_j + 1$, j = 1, ..., T, T is the num-\nber of classes. Besides, q is a point on the T-dimensional unit\nsimplex $S_T$ [32]. With respect to snippet-level evidence, we\nrefined the label vector by utilizing belief values from evi-\ndence and assigning smaller weights to samples that possess\nhigher levels of uncertainty, which is designed to lead the\nmodel to pay less attention to background noise and focus on\ntruly important and practical features. Following the classical\noptimization algorithm of TEDL [32] and utilizing newly pro-\nposed hybrid attention, a novel method to adapt to the tasks\nof WS-TAL is defined as:\n$L_{hge} = - \\frac{1}{M} \\sum_{i=1}^{M} (1 - U_e^{(i)}) \\sum_{j=1}^{T} (\\frac{q_j^{(i)}}{(\\sum_{j=1}^{T} q_j^{(i)} )} (log(q_j^{(i)}) - log(\\alpha_j^{(i)}))$\nwhere $U_e$ is obtained from original snippet-level evidence,\nvideo-level evidence can be acquired by aggregating snippet-\nlevel evidences using eq.11. Synthesizing the designed tar-\ngets, we are able to acquire the final loss function which can"}, {"title": "3. EXPERIMENTS AND RESULTS", "content": ""}, {"title": "3.1. Datasets and Metrics", "content": "We conduct a large amount of experiments to evaluate the\nproposed method on THUMOS14 [34] dataset. THUMOS14\nis composed of 200 validation videos and 213 testing videos\nwith 20 action classes. Besides, The mean Average Precision\n(mAP) with different Intersection-over-Union (IoU) thresh-\nolds, which is regarded as a standard evaluation metric for\nWS-TAL tasks, is used to evaluate the performance of the\nproposed model."}, {"title": "3.3. Performance Comparison with State-of-the-Art Mod-\nels", "content": "Table 1 indicates the performance comparison between fully\nand weakly supervised methods and our proposed model on\nthe THUMOS14 dataset. Our method achieves highest per-\nformance on mAP with IoU (0.1-0.6) and mAP@AVG. Only\na fully-supervised method BSN achieves best performance\n20% on mAP@0.7. All of the results demonstrate the state-\nof-the-art performance of our method for the WS-TAL task."}, {"title": "3.4. Ablation Study", "content": "As is shown in Table 2, HMHA denotes the hybrid multi-\nhead attention module. GU EF denotes the proposed general-\nized uncertainty-based evidential fusion. Here, we explore the\neffectiveness of two mentioned modules on the THUMOS14\ndataset. Notably, the omission of GU EF results in significant\nperformance degradation, confirming its effectiveness. Be-\nsides, only removing HMHA, the performance also shows\na moderate degradation within 0.6. Therefore, Table 2 evi-\ndently indicates that each component of our model makes a\ntremendous contribution to improving performance."}, {"title": "4. CONCLUSION", "content": "In this paper, we propose a generalized uncertainty-based\nevidential fusion and hybrid multi-head attention module,\nwhich effectively eliminates action-background ambiguity\nand filters redundant information from pre-trained features\nto enable the model to focus on foreground snippets, conse-\nquently improving performance. Experimental results on the\nTHUMOS14 dataset compared with the latest state-of-the-art methods demonstrate the effectiveness of our proposed\nmethod. Considering the fact that pseudo-label is also effica-\ncious on WS-TAL tasks, we will conduct further research."}], "equations": ["A^{Flow}, A^{RGB} = MHA(X^{Flow}), MHA(X^{RGB})", "X^{Flow} = X^{Flow} \\otimes \\sigma(A^{Flow} \\otimes A^{RGB})", "X^{RGB} = X^{RGB} \\otimes \\sigma(A^{RGB} \\otimes A^{Flow})", "A^{Flow, ARGB} = fattn(Flow), fattn(RGB)", "F = Concat(X^{Flow}, X^{RGB})", "A = (A^{Flow} + A^{RGB})/2", "\\sum_{k=1}^{N} m({p_k}) + U = 1", "m({P_k}) = \\frac{e_k}{\\sum_{s=1}^{N} (e_s + 1)} , U = \\frac{S}{\\sum_{s=1}^{N} (e_s + 1)} = \\frac{1}{S} \\sum_{s=1}^{N} (1)", "m({}) = U, \u04e8 = {P1, P2, ..., PN} = PN+1", "m_{final}({P_k}) =\\frac{1}{1- Con}(M_1({P_k}) * M_2({P_k}) +m_1({P_k}) * M_2({\\Theta}) + \u0442_1({}) * M_2({P_k}))", "e_{final} = e_1 \\otimes e_2", "[e_s, m({0}] = (Concat[e]/S, m_1({\u0398}]) \\otimes (Concat[e]/S, m_2({\u0398}]) \u2208 RW\u00d7T", "\\hat{p} = \\frac{1}{L} \\sum_{t \\in \\xi} z_t \\quad \\quad \\xi = arg max_\\xi \\sum_{t \\in \\xi} A_t", "L_{cla} = Cross_entropy(p, p)", "L_{uef} = (\\Delta \\cdot tanh(\\sigma(h)\\phi(m_s({\\Theta}))) + 1) \\sum_{t=1}^{W} L_1norm|1 - A_t - z_{t,T+1}|", "D(q|\\alpha) = \\begin{cases}\n\\frac{1}{B(\\alpha)} \\prod_{j=1}^{T} q_j^{\\alpha_j - 1}, for q \\in S_T\n0, Otherwise\n\\end{cases}", "L_{hge} = - \\frac{1}{M} \\sum_{i=1}^{M} (1 - U_e^{(i)}) \\sum_{j=1}^{T} (\\frac{q_j^{(i)}}{(\\sum_{j=1}^{T} q_j^{(i)} )} (log(q_j^{(i)}) - log(\\alpha_j^{(i)}))", "L = L_{cla} + 11L_{ugl} + 12L_{hge}"]}