{"title": "TDNetGen: Empowering Complex Network Resilience Prediction with Generative Augmentation of Topology and Dynamics", "authors": ["Chang Liu", "Jingtao Ding", "Yiwen Song", "Yong Li"], "abstract": "Predicting the resilience of complex networks, which represents the ability to retain fundamental functionality amidst external perturbations or internal failures, plays a critical role in understanding and improving real-world complex systems. Traditional theoretical approaches grounded in nonlinear dynamical systems rely on prior knowledge of network dynamics. On the other hand, data-driven approaches frequently encounter the challenge of insufficient labeled data, a predicament commonly observed in real-world scenarios. In this paper, we introduce a novel resilience prediction framework for complex networks, designed to tackle this issue through generative data augmentation of network topology and dynamics. The core idea is the strategic utilization of the inherent joint distribution present in unlabeled network data, facilitating the learning process of the resilience predictor by illuminating the relationship between network topology and dynamics. Experiment results on three network datasets demonstrate that our proposed framework TDNetGen can achieve high prediction accuracy up to 85%-95%. Furthermore, the framework still demonstrates a pronounced augmentation capability in extreme low-data regimes, thereby underscoring its utility and robustness in enhancing the prediction of network resilience. We have open-sourced our code in the following link, https://github.com/tsinghua-fib-lab/TDNetGen.", "sections": [{"title": "1 INTRODUCTION", "content": "Real-world complex systems across various domains, such as ecological [19], gene regulatory [3], and neurological networks [51, 52], are often described as complex networks composed of interconnected nodes with weighted links. A fundamental characteristic of these systems is their resilience [17, 38], that is, the ability to maintain functionality in the face of disruptions. From the perspective of dynamical systems, nodal state evolution of complex networks is driven by underlying nonlinear dynamics. Specifically, with the functionality of each node represented by its state value, a resilient network can recover from disruptions (on its nodes) and dynamically evolve into a stable phase where all nodes operate at a high level of activity (see Figure 1). Understanding and predicting this critical property of resilience in complex networks not only enhances our ability to analyze and intervene in natural and social systems [17, 42, 44, 59] but also offers valuable insights for the design of engineered infrastructures [53].\nTo predict network resilience, theories grounded in nonlinear dynamical systems have been developed [17, 26, 30, 60]. These frameworks strive to separate the influences of network structure and dynamics to derive analytical solutions for complex, high-dimensional systems [6, 16, 41]. However, theoretical approaches"}, {"title": "2 PRELIMINARIES", "content": "Network resilience articulates that a resilient system is characterized by its invariable convergence towards a desired, non-trivial stable equilibrium following perturbation [17]. Formally, given a complex network G = (V, A), where V = {v1, v2,\u00b7\u00b7\u00b7, vN} represents its node set and A denotes the adjacency matrix. The state of node i can be represented as xi, usually governed by the following non-linear ordinary differential equations (ODEs) as the nodal state dynamics:\n$\\frac{d x_{i}}{d t}=F(x_{i})+\\sum_{j=1}^{N} A_{i j} G(x_{i}, x_{j}),$\nwhere F(xi) represents the self-dynamics of nodes and G(xi, xj) denotes interaction dynamics. The complex network G is considered resilient if it consistently converges to only the desired nodal state equilibrium as time t approaches infinity, irrespective of any perturbation and varying initial conditions with the exception of its fixed points."}, {"title": "2.2 Problem Formulation", "content": "Considering the challenge of obtaining detailed knowledge of the underlying equations that govern nodal state dynamics in real-world scenarios, in this work, we advocate for a purely data-driven approach to predict network resilience. In the context of the resilience prediction task, our dataset comprises network samples from which we can extract both topology and the initial T steps of M nodal state trajectories prior to reaching a steady state. Formally, for a network comprising N nodes, the topology is represented by an adjacency matrix A \u2208 RN\u00d7N, while the observed nodal state trajectories are denoted as X \u2208 RM\u00d7N\u00d7T. As demonstrated in Section 2.1, determining the resilience of a network precisely necessitates knowledge of its steady-state conditions, a requirement that is often prohibitive to meet due to the high observational costs (e.g., long-term species population growth). Consequently, only a limited"}, {"title": "3 METHODOLOGY", "content": "In this section, we propose an effective method named TDNetGen to address the problem of complex network resilience prediction with limited labeled data samples via generative augmentation of topology and dynamics. Figure 2 illustrates the holistic design of TDNetGen, which consists of the following components:\n\u2022 Topology diffusion module. To facilitate resilience prediction performance and address the lack of labeled data, we design a diffusion module to model the distribution of unlabeled network topology. Therefore, we can sample new network topologies from the learned distribution.\n\u2022 Dynamics learning module. We propose a neural ODE [8, 58]-based dynamics learning module to learn nodal state changes of networks from observed trajectories. It can simulate nodal state trajectories for the generated topologies from the topology diffusion module.\n\u2022 Resilience predictor. We design a resilience predictor empowered by Transformer and graph convolutional networks (GCNs), which jointly models nodal state dynamics and node interactions from observed trajectories and network topologies, respectively. It learns a low-dimensional embedding for each network and predicts its resilience based on this representation.\nIn our proposed framework, we first train both the dynamics learning module and the topology diffusion module utilizing unlabeled as well as labeled nodal state trajectories and network topologies, respectively, which is then followed by the pre-training of the resilience predictor using accessible labeled data. Subsequently, we generate new samples facilitated by the topology diffusion module and dynamics learning module, with the guidance provided by the resilience predictor. The newly generated samples further enhance"}, {"title": "3.2 Topology Diffusion Module", "content": "Existing continuous graph diffusion models [27, 40] undermine the sparsity nature of topologies and usually result in complete graphs lacking physically meaningful edges. Consequently, they fail to capture the structural properties of complex networks. Therefore, we propose to model the distribution of network topologies using the discrete-space diffusion model [5, 49], as illustrated in Figure 3. Different from diffusion models for images with continuous Gaussian noise, here we apply a discrete type of noise on each edge, and the type of each edge can transition to another during the diffusion process. Here, we define the transition probabilities of all edges at time step s as matrix Q, where Qij = q(e \\textsuperscript{s} = j|e\\textsuperscript{s\u22121} = i) denotes the type of edge e transits to j from i at time step s. The forward process of adding noise of each time step to graph structure G is equivalent to sampling the edge type from the categorical distribution, formulated as:\nq(GS|Gs-1) = \\mathbb{E}S\u22121QS, q(GS|G) = \\mathbb{E}S-1QS,\nwhere E \u2208 RN\u00d7N\u00d72 is the expanded adjacency matrix from A. Its last dimension is a 2-D one-hot vector where [0, 1] denotes an edge exists between the corresponding nodes, while [1, 0] denotes there is no edge. Q = Q1 . . . Qs. The reverse process aims to gradually recover the clean graph G given a noisy graph Gs. Towards this end, inspired by existing works [5, 49], we train a parameterized neural network he which takes the noisy graph Gs as input and predicts the structure of the clean graph G, i.e., all the probability pij of the existence of an edge eij between node i and j in the clean graph G. We use the cross-entropy loss to optimize parameters 0, formulated as follows:\nL_{B C E}=\\frac{1}{N^{2}} \\sum_{1 \\leq i, j<N} \\operatorname{CrossEntropy}\\left(e_{i j}, p_{i j}\\right).\nFor the parameterization of he, we employ the widely-recognized backbone of multi-layer graph transformers proposed by Dwivedi et al. [14]. Intuitively, node features are updated in each layer through the self-attention mechanism, and edge features are updated from"}, {"title": "3.3 Dynamics Learning Module", "content": "Through the topology diffusion module, we can generate new network topologies for the training of resilience predictor. Nonetheless, we also need to obtain their nodal states trajectories to predict their resilience. As illustrated in Section 2.1, nodal state dynamics in complex networks usually have the generalized form of an ordinary differential equation (ODE) as:\n$\\frac{d x(t)}{d t}=f(x, G, W, t),$\nwhere x(t) \u2208 RN represents nodal states of N-nodes network G at time step t, f(.) denotes the dynamics function, and W denotes all dynamics parameters. Therefore, we develop a dynamics learning module designed to infer changes in nodal states solely from data, which learns nodal state dynamics in the expressive hidden space based on neural-ODE [8, 58]. Given the initial state x(0) of all network nodes, for each time step t, the process initiates by mapping the state of the nodes to a latent space through an encoder fe. Subsequently, graph neural networks (GNNs) are utilized as a parameterization technique to facilitate the learning of dynamics within this latent space. The transition from latent space representation back to the nodal state at each time step is accomplished by employing a decoder function fa, which decodes the hidden space"}, {"title": "3.4 Resilience Predictor", "content": "We design a resilience predictor to jointly model the dynamics and topology of networks, which leverages stacked Transformer [47] encoder layers and graph convolutional layers [29] to encode the temporal correlations of nodal states and learn spatial interactions within network topology, respectively. We illustrate its architecture in Figure 4. Specifically, for a network with N nodes, we denote the nodal states with d observed steps and M trajectories of node u as xu \u2208 Rd\u00d7M. For the k-th trajectory xuk \u2208 Rd, we first input its states of each time step to a feed-forward layer, and further encode the temporal correlation between time steps with Transformer encoder layers, formulated as follows,\nZuk = TransformerEncoder (xu,kW1 + b1),\nwhere W1 \u2208 R1\u00d7de and b1 \u2208 Rde are trainable parameters. After that, we integrate the embedding of the terminal time step of all nodes in the network, denoted by Zk \u2208 RN\u00d7de, as their k-th trajectory embeddings.\nTo capture the interactions of nodes within the topology, we design a graph convolutional network (GCN) empowered by multi-layer message-passing. Given the adjacency matrix of network topology A, we first calculate the Laplacian operator \u03a8 = I\u2212 D\u22121/2AD\u22121/2out, where the diagonal of Din and Dout represent the in- and out-degree of nodes. We input the k-th trajectory embeddings of nodes to the graph convolutional network. The l-th layer message passing of the designed GCN can be represented as follows:\nZ^{(l)}_{k} = f(Z_{k}^{(l-1)}) + g(\\Psi Z_{k}^{(l-1)}),\nwhere f(.) and g(.) are implemented as MLPs. Such message-passing design is motivated by Equ. (1), aiming to more precisely model the effects from both the node itself and its neighborhood on a specific node."}, {"title": "3.5 Joint Data Augmentation of Topology and Dynamics", "content": "The above modules enable us to generate network samples with both topology and nodal state trajectories. However, it is important to note that the simulated nodal states are confined to the initial temporal period, corresponding to the maximal duration present within the training dataset, and compelling the dynamics learning module to simulate time steps beyond its training scope yields results of questionable reliability. Consequently, the principal challenge arises from the inability to ascertain the steady-state conditions of the generated networks. This limitation obstructs the direct acquisition of resilience labels, presenting a significant impediment to the data augmentation. To overcome this problem, we advocate for the strategy of guiding the topology diffusion module, enabling it to"}, {"title": "3.6 Time Complexity Analysis", "content": "We define N as the number of nodes in a graph, and analyze time complexity of each module in our framework as follows.\n\u2022 Topology diffusion module is parameterized using Graph-Transformer layers (Appendix A.1). It exhibits a time complexity of O(N2) per layer, attributable to the computation of attention scores and the prediction process for each edge.\n\u2022 Dynamics learning module is based on neural-ODE and parameterized through GCN layers. This module also demonstrates a time complexity of O(N\u00b2) resulting from convolution operations and the application of a fourth-order Runge-Kutta ODE solver.\n\u2022 Resilience predictor leverages stacked Transformer encoder layers to capture temporal correlations among nodal states, while spatial interactions within the network topology are discerned through GCN layers. Time complexities of the Transformer encoder layers and GCN layers are O(NT2) and O(N2), respectively, with T representing the trajectory length. Typically, T is significantly smaller than N for most graph structures.\nConsequently, the overall time complexity of TDNetGen is dominantly O(N2), signifying its scalability and efficiency in processing large graph structures. In practical experiments, our framework takes about 10 seconds to generate a 200-nodes graph with nodal state trajectories and 20 milliseconds to predict its resilience. Since resilience inference is not a real-time task, such time complexity is acceptable for application."}, {"title": "4 EXPERIMENTS", "content": "In this section, we demonstrate the superior performance of our framework TDNetGen, aiming to answer the following research questions:\n\u2022 RQ1: How does our framework TDNetGen compare to potential baseline methods of harnessing unlabeled data to enhance predictive performance?\n\u2022 RQ2: How do different designs of TDNetGen affect the model performance?\n\u2022 RQ3: How does TDNetGen perform across limited numbers of original labeled samples and lengths of nodal state trajectories?\n\u2022 RQ4: How does TDNetGen perform with different network types and scales?"}, {"title": "4.1 Experimental Settings", "content": "\u2022 Mutualistic dynamics. The mutualistic dynamics [19] dxidt=B XiXj +xi (1 \u2212 x) (xk\u2212 1) +\u03a3Nj=1 Aij D+Exi+Hxj describes the alterations in species populations that are engendered by the migration term B, logistic growth term with environment capacity K [57], Allee effect [2] term with threshold C, and mutualistic interaction between species with interaction network A.\n\u2022 Regulatory dynamics. The regulatory dynamics, also called Michaelis-Menten dynamics [3], is described by dxidt=\u2212x+ xh xh+1+\u03a3Nj=1 Aij xh xh+1. f represents the degradation (f = 1) or dimerization (f = 2). Additionally, the second term in the equation is designed to capture genetic activation with Hill coefficient h, which serves to quantify the extent of gene regulation collaboration.\n\u2022 Neuronal dynamics. The neuronal dynamics, also called Wilson-Cowan dynamics [51, 52], is described by the equation of dxidt= \u2212Xi + \u03a3Nj=1 Aij 1+e4\u22128xj. For each node in the network, it receives cumulative inputs from its neighbors. The second term of the equation represents the activation signal that is collectively contributed by all neighboring nodes.\nFor each dynamics, we synthesize Erd\u0151s-R\u00e9nyi networks [15] with edge creation probability uniformly sampled in [0, 0.15], and use the fourth-order Runge-Kutta stepper [13] to simulate their nodal state trajectories. For more details, please refer to the Appendix A.2. We create 2000 network samples for training, 200 for validation, and another 200 samples for testing. In the training stage, we randomly select 100 (5%) samples as labeled data and keep other 1900 samples as unlabeled. The statistics of datasets are shown in Table 1."}, {"title": "4.1.2 Baselines and metrics", "content": "In the following parts, we define the model trained only on original labeled data as the vanilla model. Besides this, there are mainly three kinds of baseline methods designed to leverage unlabeled data for enhancing the predictive performance of the vanilla predictor.\n\u2022 Self-training methods. They utilize the predictor to assign pseudo labels to unlabeled data, thereby augmenting the labeled training dataset. We abbreviate these method as ST.\n\u2022 Self-supervised learning methods. They employ hand-crafted tasks to derive insights from unlabeled data, thereby facilitating the pre-training of model parameters. Subsequently, they undergo further supervised training on the labeled dataset. This approach is predicated on the premise that integrating pre-training phases with subsequent supervised learning phases leverages both unlabeled and labeled datasets, thereby enhancing the model's learning efficacy and predictive accuracy. Such methods include"}, {"title": "4.3 Ablation Study (RQ2)", "content": "To provide a comprehensive analysis and assess the effect of our designed components quantitatively, we conduct several ablation experiments via removing each design elements, and present the evaluation results in Figure 5.\n\u2022 Effectiveness of Classifier guidance. We first remove the design of classifier guidance and generate new network topologies via only unconditional topology diffusion module. The nodal state trajectories are simulated utilizing the dynamics learning module, and its resilience label is determined by the resilience predictor that has been trained on the labeled dataset. The results reveal that the F1-score of the ablation model significantly declines 6.14%, 9.75%, and 5.68% compared to the full model design, which underscores the importance of guided generation.\n\u2022 Effectiveness of resilience predictor fine-tuning on dynamics learning module-produced trajectories. In this experiment, we remove the fine-tuning procedure of the resilience predictor on dynamics learning module-produced trajectories, instead utilizing the one trained with ground-truth trajectories. The results illustrate that fine-tuning could significantly enhance its guidance capabilities to generate higher-quality data, ultimately empowering the resilience predictor to be re-trained on it.\n\u2022 Architecture analysis. We compare our diffusion-based topology generation module with generative adversarial network (GAN) module. Specifically, we replace topology generation module as a GAN-based module proposed in [37]. We use the topologies of unlabeled data to train the GAN model, and sample new topologies from it. The nodal state trajectories and the resilience label are produced by our dynamics learning module and the"}, {"title": "4.4 Augmentation with Limited Labels and Observations (RQ3)", "content": "In this section, we investigate data augmentation capabilities of our proposed framework under conditions of more limited number of labeled samples and reduced observed trajectory lengths, representing more challenging scenarios. We illustrate the results on the mutualistic dataset in Figure 6-7.\n\u2022 Less labeled samples. We investigate the performance of the vanilla model, where the numbers of labeled networks are in {20, 40, 60, 80, 100}, as well as the enhanced model trained on the augmented data generated by TDNetGen. From the results, we find that the predictive performance of the vanilla model is generally proportional to the number of labeled data used for training. TDNetGen is robust to the limitation of labeled data, which can still generate reasonable samples to benefit the predictive performance of the vanilla model. These findings underscore the versatility and potential of our proposed framework, particularly in scenarios characterized by a scarcity of labeled data, which constitutes a small portion of the available dataset.\n\u2022 Shorter nodal state trajectories. We also investigate the performance of the vanilla model and TDNetGen while using shorter nodal state trajectories, which contain {3, 4, 5, 6} time steps. We discover that the performance of the vanilla model improves with the increase in trajectory length since the model can extract more knowledge about nodal state dynamics from data to make more accurate resilience predictions. In this scenario, TDNetGen can also help to augment the model's performance, which suggests that even in situations where nodal state trajectories are costly to acquire, our framework remains applicable and effective for data augmentation purposes of simultaneously generating plausible topologies and nodal state trajectories of complex networks."}, {"title": "4.5 Robustness against Different Network Types and Scales (RQ4)", "content": "We consider other network models, including Barab\u00e1si-Albert model [1], $1/H2 model [43], and stochastic block model (SBM) [20], which have more complex and heterogeneous structural properties. Moreover, we also evaluate the scalability of our framework"}, {"title": "5 RELATED WORKS", "content": "Existing works on resilience prediction are mainly categorized to analytical estimations from physical theories [17, 30, 39]. Gao et al. [17] propose to reduce the dimension of complex networks to single-parameter systems based on mean-field theory, thus we can easily analyze the equilibrium of 1-D ODE problem and predict the resilience of complex networks. Laurence et al. [30] perform dimension reduction based on spectral graph theory on the dominant eigenvalues and eigenvectors of adjacency matrices. Morone et al. [39] develop a resilience prediction methodology by quantifying the k-core structure within networks. Despite their effectiveness, they often pre-suppose a detailed understanding of nodal state dynamics, which is usually not available in practical scenarios. In our work, we design data-driven methods that extract topology and nodal state dynamics information from observational data, allowing for resilience predictions without the need for prior knowledge."}, {"title": "5.2 Diffusion Models on Graphs", "content": "Diffusion probabilistic models have been widely used in text, image, audio generation, etc. [5, 34, 55, 56]. Recently, some existing"}, {"title": "5.3 Learning from Unlabeled Data", "content": "Typical approaches of learning from unlabeled data for graph classification include pre-training on self-supervised tasks [28, 54], self-training [4, 23, 25, 45], and graph data augmentation [18]. Although pre-training proves to be effective for vision and language-related tasks, it can hardly help the resilience prediction task because of the disparity between hand-crafted and downstream prediction tasks [24, 28]. Therefore, we still lack a universal self-supervised task that learns from unlabeled graphs and improves the performance of downstream scenarios. Self-training tasks assign pseudo-labels to unlabeled data by leveraging the model itself, followed by the retraining of the model with pseudo-labeled data. Existing works [4, 23, 45] focus on uncertainty estimation of assigned labels to minimize the impact of noisy pseudo-labels. Furthermore, Liu et al. [33] learn data distributions from unlabeled graphs with diffusion models, and to generate task-specific labeled graphs for data augmentation. Compared with their work, our proposed TDNetGen framework considers more intricate scenarios of complex networks with interplay between topology and nodal state dynamics. Our framework can extract knowledge from full unlabeled complex network samples, thereby generating high-quality augmented data that benefits the training of prediction models."}, {"title": "6 CONCLUSIONS", "content": "In this work, we propose an effective framework, TDNetGen, for complex network resilience prediction. It not only addresses the problem in a data-driven manner without prior knowledge about groud-truth dynamics, but also solves labeled data sparsity problem with the generative augmentation of jointly modeling network topology and dynamics. Extensive experiments demonstrate the superiority of TDNetGen and also highlight its robustness within less labeled data and dynamics information conditions. The methodology introduced in this paper provides a novel perspective for improving resilience prediction through data augmentation, that is, leveraging the untapped potential of unlabeled data to enhance the learning process."}, {"title": "A APPENDIX", "content": "Graph Transformer. The parameterization of the denoising network he employs the Graph Transformer architecture [11]. The input consists of the noisy graph features, and the output is the edge distribution of clean graphs. Each layer of the Graph Transformer can be represented as follows:\nht+1i=\u2225Kt\\|k=1\u03b1k,lk,l\u03b1kijvj,ht+1eij=MLP(eijk,l\u03b1kijvj,hQk\u03b1kQkik,lQkQkik,lhljWl+1e,=softmaxiOa+V,+WVd"}, {"title": "A.2.1 Mutualistic dynamics", "content": "Nodal state trajectories of networks in mutualistic dataset are simulated via the following differential equations:\ndxi XiXj= B +xi (1 \u2212x) (x \u22121)+A Dij+\u03a3 Exi+Hxj"}, {"title": "A.2.2 Regulatory dynamics", "content": "Nodal state trajectories of networks in regulatory dataset are simulated via the following differential equations:\ndxi N A= Bx+\u03a3x+1"}, {"title": "A.2.3 Neuronal dynamics", "content": "Nodal state trajectories of networks in neuronal dataset are simulated with the following differential equations:\ndx 1= -xi +\u03a3A1++8"}, {"title": "A.2.4 Barab\u00e1si-Albert (BA) model", "content": "BA network starts with a small number of nodes, and at each time step, a new node with m edges is added to the network. These m edges link the new node to m different nodes that have already present in network. The probability \u041f that a new node will connect to an existing node i is proportional to the degree ki of node i. Mathematically,I (i)=ki\u03a3jkj"}, {"title": "A.3 Details of Theoretical Baseline", "content": "In this section, we detail how we incorporate resilience theory from physics to provide insights on leveraging unlabeled data."}, {"title": "A.3.1 Gao-Barzel-Barab\u00e1si (GBB) theory.", "content": "dx effN"}, {"title": "A.3.2 Theory-guided data augmentation.", "content": "Here we discuss how to use GBB theory to perform data augmentation. For each network topology in labeled and unlabeled dataset, we calculate its Beff"}]}