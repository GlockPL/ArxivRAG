{"title": "TDNetGen: Empowering Complex Network Resilience Prediction with Generative Augmentation of Topology and Dynamics", "authors": ["Chang Liu", "Jingtao Ding", "Yiwen Song", "Yong Li"], "abstract": "Predicting the resilience of complex networks, which represents the ability to retain fundamental functionality amidst external perturbations or internal failures, plays a critical role in understanding and improving real-world complex systems. Traditional theoretical approaches grounded in nonlinear dynamical systems rely on prior knowledge of network dynamics. On the other hand, data-driven approaches frequently encounter the challenge of insufficient labeled data, a predicament commonly observed in real-world scenarios. In this paper, we introduce a novel resilience prediction framework for complex networks, designed to tackle this issue through generative data augmentation of network topology and dynamics. The core idea is the strategic utilization of the inherent joint distribution present in unlabeled network data, facilitating the learning process of the resilience predictor by illuminating the relationship between network topology and dynamics. Experiment results on three network datasets demonstrate that our proposed framework TDNetGen can achieve high prediction accuracy up to 85%-95%. Furthermore, the framework still demonstrates a pronounced augmentation capability in extreme low-data regimes, thereby underscoring its utility and robustness in enhancing the prediction of network resilience. We have open-sourced our code in the following link, https://github.com/tsinghua-fib-lab/TDNetGen.", "sections": [{"title": "1 INTRODUCTION", "content": "Real-world complex systems across various domains, such as ecological [19], gene regulatory [3], and neurological networks [51, 52], are often described as complex networks composed of interconnected nodes with weighted links. A fundamental characteristic of these systems is their resilience [17, 38], that is, the ability to maintain functionality in the face of disruptions. From the perspective of dynamical systems, nodal state evolution of complex networks is driven by underlying nonlinear dynamics. Specifically, with the functionality of each node represented by its state value, a resilient network can recover from disruptions (on its nodes) and dynamically evolve into a stable phase where all nodes operate at a high level of activity (see Figure 1). Understanding and predicting this critical property of resilience in complex networks not only enhances our ability to analyze and intervene in natural and social systems [17, 42, 44, 59] but also offers valuable insights for the design of engineered infrastructures [53].\nTo predict network resilience, theories grounded in nonlinear dynamical systems have been developed [17, 26, 30, 60]. These frameworks strive to separate the influences of network structure and dynamics to derive analytical solutions for complex, high-dimensional systems [6, 16, 41]. However, theoretical approaches"}, {"title": "2 PRELIMINARIES", "content": "Network resilience articulates that a resilient system is characterized by its invariable convergence towards a desired, non-trivial stable equilibrium following perturbation [17]. Formally, given a complex network G = (V, A), where V = {01, 02,\u00b7\u00b7\u00b7, UN } represents its node set and A denotes the adjacency matrix. The state of node i can be represented as xi, usually governed by the following non-linear ordinary differential equations (ODEs) as the nodal state dynamics:\n$\\frac{dxi}{dt} = F(xi) + \\sum_{j=1}^{N} A_{ij}G(xi, xj)$,\nwhere F(xi) represents the self-dynamics of nodes and G(xi, Xj) denotes interaction dynamics. The complex network G is considered resilient if it consistently converges to only the desired nodal state equilibrium as time t approaches infinity, irrespective of any perturbation and varying initial conditions with the exception of its fixed points.\nConsidering the challenge of obtaining detailed knowledge of the underlying equations that govern nodal state dynamics in real-world scenarios, in this work, we advocate for a purely data-driven approach to predict network resilience. In the context of the resilience prediction task, our dataset comprises network samples from which we can extract both topology and the initial T steps of M nodal state trajectories prior to reaching a steady state. Formally, for a network comprising N nodes, the topology is represented by an adjacency matrix A \u2208 RN\u00d7N, while the observed nodal state trajectories are denoted as X \u2208 RM\u00d7N\u00d7T. As demonstrated in Section 2.1, determining the resilience of a network precisely necessitates knowledge of its steady-state conditions, a requirement that is often prohibitive to meet due to the high observational costs (e.g., long-term species population growth). Consequently, only a limited"}, {"title": "3 METHODOLOGY", "content": "In this section, we propose an effective method named TDNetGen to address the problem of complex network resilience prediction with limited labeled data samples via generative augmentation of topology and dynamics. Figure 2 illustrates the holistic design of TDNetGen, which consists of the following components:\n\u2022 Topology diffusion module. To facilitate resilience prediction performance and address the lack of labeled data, we design a diffusion module to model the distribution of unlabeled network topology. Therefore, we can sample new network topologies from the learned distribution.\n\u2022 Dynamics learning module. We propose a neural ODE [8, 58]-based dynamics learning module to learn nodal state changes of networks from observed trajectories. It can simulate nodal state trajectories for the generated topologies from the topology diffusion module.\n\u2022 Resilience predictor. We design a resilience predictor empowered by Transformer and graph convolutional networks (GCNs), which jointly models nodal state dynamics and node interactions from observed trajectories and network topologies, respectively. It learns a low-dimensional embedding for each network and predicts its resilience based on this representation.\nIn our proposed framework, we first train both the dynamics learning module and the topology diffusion module utilizing unlabeled as well as labeled nodal state trajectories and network topologies, respectively, which is then followed by the pre-training of the resilience predictor using accessible labeled data. Subsequently, we generate new samples facilitated by the topology diffusion module and dynamics learning module, with the guidance provided by the resilience predictor. The newly generated samples further enhance"}, {"title": "3.2 Topology Diffusion Module", "content": "Existing continuous graph diffusion models [27, 40] undermine the sparsity nature of topologies and usually result in complete graphs lacking physically meaningful edges. Consequently, they fail to capture the structural properties of complex networks. Therefore, we propose to model the distribution of network topologies using the discrete-space diffusion model [5, 49], as illustrated in Figure 3. Different from diffusion models for images with continuous Gaussian noise, here we apply a discrete type of noise on each edge, and the type of each edge can transition to another during the diffusion process. Here, we define the transition probabilities of all edges at time step s as matrix Q, where Q\u00a1 = q(e\u00b0 = j|es\u22121 = i) denotes the type of edge e transits to j from i at time step s. The forward process of adding noise of each time step to graph structure G is equivalent to sampling the edge type from the categorical distribution, formulated as:\nq(GS|Gs-1) = ES-1QS, q(GS|G) = ES-1QS,\nwhere E \u2208 RN\u00d7N\u00d72 is the expanded adjacency matrix from A. Its last dimension is a 2-D one-hot vector where [0, 1] denotes an edge exists between the corresponding nodes, while [1, 0] denotes there is no edge. Q = Q\u00b9 . . . Qs. The reverse process aims to gradually recover the clean graph G given a noisy graph Gs. Towards this end, inspired by existing works [5, 49], we train a parameterized neural network he which takes the noisy graph Gs as input and predicts the structure of the clean graph G, i.e., all the probability pij of the existence of an edge eij between node i and j in the clean graph G. We use the cross-entropy loss to optimize parameters 0, formulated as follows:\n$L_{BCE} = \\frac{1}{N^2} \\sum_{1\\leq i,j0$\\\nDrawing upon the aforementioned theoretical framework, at the step s of the reverse process, we first employ the resilience predictor fn to predict yg, i.e., \u0177G = fn (Gs), and estimate the pn (YG|GS-1) as\npn(yG|Gs-1) x exp(-^(\u2207Gs LBCE(\u0177G, YG), GS-1)),"}, {"title": "3.6 Time Complexity Analysis", "content": "We define N as the number of nodes in a graph, and analyze time complexity of each module in our framework as follows.\n\u2022 Topology diffusion module is parameterized using Graph-Transformer layers (Appendix A.1). It exhibits a time complexity of O(N2) per layer, attributable to the computation of attention scores and the prediction process for each edge.\n\u2022 Dynamics learning module is based on neural-ODE and parameterized through GCN layers. This module also demonstrates a time complexity of O(N\u00b2) resulting from convolution operations and the application of a fourth-order Runge-Kutta ODE solver.\n\u2022 Resilience predictor leverages stacked Transformer encoder layers to capture temporal correlations among nodal states, while spatial interactions within the network topology are discerned through GCN layers. Time complexities of the Transformer encoder layers and GCN layers are O(NT2) and O(N2), respectively, with T representing the trajectory length. Typically, T is significantly smaller than N for most graph structures.\nConsequently, the overall time complexity of TDNetGen is dominantly O(N2), signifying its scalability and efficiency in processing large graph structures. In practical experiments, our framework takes about 10 seconds to generate a 200-nodes graph with nodal state trajectories and 20 milliseconds to predict its resilience. Since resilience inference is not a real-time task, such time complexity is acceptable for application."}, {"title": "4 EXPERIMENTS", "content": "In this section, we demonstrate the superior performance of our framework TDNetGen, aiming to answer the following research questions:\n\u2022 RQ1: How does our framework TDNetGen compare to potential baseline methods of harnessing unlabeled data to enhance predictive performance?\n\u2022 RQ2: How do different designs of TDNetGen affect the model performance?\n\u2022 RQ3: How does TDNetGen perform across limited numbers of original labeled samples and lengths of nodal state trajectories?\n\u2022 RQ4: How does TDNetGen perform with different network types and scales?"}, {"title": "4.1 Experimental Settings", "content": "To construct the dataset, we synthesize complex networks with three nodal state dynamics from physics and life sciences. Denote xi(t) as the state of node i at time step t, the dynamics are as follows,\n\u2022 Mutualistic dynamics. The mutualistic dynamics [19] $\\frac{dxi}{dt} = B \\frac{XiXj}{3 + x_i} + x_i (1 - \\frac{x_i}{K}) (\\frac{x_i}{C} - 1) + \\sum_{j=1}^{N} A_{ij} \\frac{D}{+Exi+Hxj}$ describes the alterations in species populations that are engendered by the migration term B, logistic growth term with environment capacity K [57], Allee effect [2] term with threshold C, and mutualistic interaction between species with interaction network A.\n\u2022 Regulatory dynamics. The regulatory dynamics, also called Michaelis-Menten dynamics [3], is described by $\\frac{dxi}{dt} = -x+ \\frac{xh}{x+1} \\sum_{j=1}^{N} A_{ij}$. f represents the degradation (f = 1) or dimerization (f = 2). Additionally, the second term in the equation is designed to capture genetic activation with Hill coefficient h, which serves to quantify the extent of gene regulation collaboration.\n\u2022 Neuronal dynamics. The neuronal dynamics, also called Wilson-Cowan dynamics [51, 52], is described by the equation of $\\frac{di}{dt} = -Xi + \\sum_{j=1}^{N} A_{ij} \\frac{1}{1+e4-8x_j}$. For each node in the network, it receives cumulative inputs from its neighbors. The second term of the equation represents the activation signal that is collectively contributed by all neighboring nodes.\nFor each dynamics, we synthesize Erd\u0151s-R\u00e9nyi networks [15] with edge creation probability uniformly sampled in [0, 0.15], and use the fourth-order Runge-Kutta stepper [13] to simulate their nodal state trajectories. For more details, please refer to the Appendix A.2.\nWe create 2000 network samples for training, 200 for validation, and another 200 samples for testing. In the training stage, we randomly select 100 (5%) samples as labeled data and keep other 1900 samples as unlabeled. The statistics of datasets are shown in Table 1."}, {"title": "4.2 Overall Performance (RQ1)", "content": "We report the performance of our framework with mean value and standard deviation in Table 2. From the experimental results, we have the following conclusions:\n\u2022 Our framework effectively empowers predictive performance via generative augmentation of both topology and dynamics. The results demonstrate that with the help of our proposed data augmentation framework, the predictive performance of the resilience predictor can be effectively improved. For example, on mutualistic dataset, the F1-score of the resilience predictor previously trained on 100 labeled data increases from 0.838 to 0.929 (+10.86%), and its ACC increases from 0.848 to 0.934 (+10.14%) after training on the augmented data. Moreover, our framework also improves the best baseline among all self-training, self-supervised learning, and GDA methods w.r.t. F1-score by 4.26%, 4.89%, 3.30%, and w.r.t. ACC by 5.18%, 5.23%, 4.80%, on mutualistic, regulatory and neuronal dataset, respectively. All these results demonstrate the outstanding performance of our proposed framework. We find that the best baseline methods on three datasets belong to the category of graph data augmentation. Compared with TRY and G-mixup, we achieve to jointly model network topology and dynamics in a fine-grained manner.\n\u2022 Robustness performance without nodal state trajectories of unlabeled data. In certain contexts, the requirement to obtain the nodal states, even only for an initial phase of evolution, still proves to be difficult or costly. Consequently, we analyze scenarios where the nodal state trajectories of unlabeled data are inaccessible and we can only train the dynamics learning module on those of limited labeled data P in Equ. (11). Results in Table 2 demonstrate that our framework is capable of sustaining commendable performance even under such constrained conditions and surpassing the best baseline in most scenarios. It underscores the versatility of our framework and its potential effectiveness under more limited data availability of the real world scenarios.\n\u2022 Self-training cannot universally guarantee a positive impact on model performance. The results demonstrate that self-training methods have a relatively small positive effect on predictive performance among all three datasets compared to our framework TDNetGen. For example, on regulatory datasets, the F1-score of the resilience predictor increases 0.051, and its ACC increases 0.079 compared to the vanilla model. This is because the labels assigned to the augmented data in the self-training process originate from the model itself with sub-optimal predictive performance. This approach inherently carries the risk of generating labels that are incongruent with the ground truth and partially introduce contradictory information into the training dataset. The presence of such inaccurately labeled data can confound the learning algorithm, leading to a deterioration in the model's capacity to make accurate predictions."}, {"title": "4.3 Ablation Study (RQ2)", "content": "To provide a comprehensive analysis and assess the effect of our designed components quantitatively, we conduct several ablation experiments via removing each design elements, and present the evaluation results in Figure 5.\n\u2022 Effectiveness of Classifier guidance. We first remove the design of classifier guidance and generate new network topologies via only unconditional topology diffusion module. The nodal state trajectories are simulated utilizing the dynamics learning module, and its resilience label is determined by the resilience predictor that has been trained on the labeled dataset. The results reveal that the F1-score of the ablation model significantly declines 6.14%, 9.75%, and 5.68% compared to the full model design, which underscores the importance of guided generation.\n\u2022 Effectiveness of resilience predictor fine-tuning on dynamics learning module-produced trajectories. In this experiment, we remove the fine-tuning procedure of the resilience predictor on dynamics learning module-produced trajectories, instead utilizing the one trained with ground-truth trajectories. The results illustrate that fine-tuning could significantly enhance its guidance capabilities to generate higher-quality data, ultimately empowering the resilience predictor to be re-trained on it.\n\u2022 Architecture analysis. We compare our diffusion-based topology generation module with generative adversarial network (GAN) module. Specifically, we replace topology generation module as a GAN-based module proposed in [37]. We use the topologies of unlabeled data to train the GAN model, and sample new topologies from it. The nodal state trajectories and the resilience label are produced by our dynamics learning module and the"}, {"title": "4.4 Augmentation with Limited Labels and Observations (RQ3)", "content": "In this section, we investigate data augmentation capabilities of our proposed framework under conditions of more limited number of labeled samples and reduced observed trajectory lengths, representing more challenging scenarios. We illustrate the results on the mutualistic dataset in Figure 6-7.\n\u2022 Less labeled samples. We investigate the performance of the vanilla model, where the numbers of labeled networks are in {20, 40, 60, 80, 100}, as well as the enhanced model trained on the augmented data generated by TDNetGen. From the results, we find that the predictive performance of the vanilla model is generally proportional to the number of labeled data used for training. TDNetGen is robust to the limitation of labeled data, which can still generate reasonable samples to benefit the predictive performance of the vanilla model. These findings underscore the versatility and potential of our proposed framework, particularly in scenarios characterized by a scarcity of labeled data, which constitutes a small portion of the available dataset.\n\u2022 Shorter nodal state trajectories. We also investigate the performance of the vanilla model and TDNetGen while using shorter nodal state trajectories, which contain {3, 4, 5, 6} time steps. We discover that the performance of the vanilla model improves with the increase in trajectory length since the model can extract more knowledge about nodal state dynamics from data to make more accurate resilience predictions. In this scenario, TDNetGen can also help to augment the model's performance, which suggests that even in situations where nodal state trajectories are costly to acquire, our framework remains applicable and effective for data augmentation purposes of simultaneously generating plausible topologies and nodal state trajectories of complex networks."}, {"title": "4.5 Robustness against Different Network Types and Scales (RQ4)", "content": "We consider other network models, including Barab\u00e1si-Albert model [1], $1/H2 model [43], and stochastic block model (SBM) [20], which have more complex and heterogeneous structural properties. Moreover, we also evaluate the scalability of our framework"}, {"title": "5 RELATED WORKS", "content": "Existing works on resilience prediction are mainly categorized to analytical estimations from physical theories [17, 30, 39]. Gao et al. [17] propose to reduce the dimension of complex networks to single-parameter systems based on mean-field theory, thus we can easily analyze the equilibrium of 1-D ODE problem and predict the resilience of complex networks. Laurence et al. [30] perform dimension reduction based on spectral graph theory on the dominant eigenvalues and eigenvectors of adjacency matrices. Morone et al. [39] develop a resilience prediction methodology by quantifying the k-core structure within networks. Despite their effectiveness, they often pre-suppose a detailed understanding of nodal state dynamics, which is usually not available in practical scenarios. In our work, we design data-driven methods that extract topology and nodal state dynamics information from observational data, allowing for resilience predictions without the need for prior knowledge."}, {"title": "5.2 Diffusion Models on Graphs", "content": "Diffusion probabilistic models have been widely used in text, image, audio generation, etc. [5, 34, 55, 56]. Recently, some existing works have applied the diffusion model to the field of graph generation [9, 22, 46, 49]. Huang et al. [22] define a stochastic differential equation (SDE) that smoothly converts graphs with complex distribution to random graphs, and samples new graphs by solving the reverse-time SDE. Tseng et al. [46] propose GraphGUIDE to achieve interpretable and controllable graph generation, wherein edges in graph are flipped or set at each discrete time step. Chen et al. [9] propose to leverage graph sparsity during each step of diffusion process, which only focuses on a small portion of nodes and considers edge changes between them. In contrast to existing contributions focused primarily on graph structures, our research extends to the generation of complex networks, which encompasses not merely the graph topology but also integrates nodal state trajectories, thereby facilitating the generation of comprehensive network data."}, {"title": "5.3 Learning from Unlabeled Data", "content": "Typical approaches of learning from unlabeled data for graph classification include pre-training on self-supervised tasks [28, 54], self-training [4, 23, 25, 45], and graph data augmentation [18]. Although pre-training proves to be effective for vision and language-related tasks, it can hardly help the resilience prediction task because of the disparity between hand-crafted and downstream prediction tasks [24, 28]. Therefore, we still lack a universal self-supervised task that learns from unlabeled graphs and improves the performance of downstream scenarios. Self-training tasks assign pseudo-labels to unlabeled data by leveraging the model itself, followed by the retraining of the model with pseudo-labeled data. Existing works [4, 23, 45] focus on uncertainty estimation of assigned labels to minimize the impact of noisy pseudo-labels. Furthermore, Liu et al. [33] learn data distributions from unlabeled graphs with diffusion models, and to generate task-specific labeled graphs for data augmentation. Compared with their work, our proposed TDNetGen framework considers more intricate scenarios of complex networks with interplay between topology and nodal state dynamics. Our framework can extract knowledge from full unlabeled complex network samples, thereby generating high-quality augmented data that benefits the training of prediction models."}, {"title": "6 CONCLUSIONS", "content": "In this work, we propose an effective framework, TDNetGen, for complex network resilience prediction. It not only addresses the problem in a data-driven manner without prior knowledge about groud-truth dynamics, but also solves labeled data sparsity problem with the generative augmentation of jointly modeling network topology and dynamics. Extensive experiments demonstrate the superiority of TDNetGen and also highlight its robustness within less labeled data and dynamics information conditions. The methodology introduced in this paper provides a novel perspective for improving resilience prediction through data augmentation, that is, leveraging the untapped potential of unlabeled data to enhance the learning process."}, {"title": "A APPENDIX", "content": "The parameterization of the denoising network he employs the Graph Transformer architecture [11]. The input consists of the noisy graph features, and the output is the edge distribution of clean graphs. Each layer of the Graph Transformer can be represented as follows:\n$al_{ij}^{l+1} = \\frac{K}{ \\sum_{k=1}}$\n$eij^{l+1} ==1(a_{ij},e^{l}),$\n$al_{ij} = softmax; (al_{ij}),$\n$\\\\\\sqrt{dk} +\nk,l \\\\\nwhere he denotes the embedding of node i of the I-th layer, e\u00a1 represents the embedding of edge connecting node i and j. O, Q, P, V, and W with different superscripts are trainable parameters. k = {1, 2,..., K} denotes the attention heads, and || represents the concatenation operator. After the last layer of Graph Transformer, edge embeddings are fed into an MLP to predict the existence of edges {eij}1\u2264i,j\u2264N, where N is the number of nodes in the network.\nWe include node features in both structural and spectral domains to enhance the performance of the Graph Transformer, and choose the same features as in [49]. Specifically, structural features include the cycles, indicating i.e., how many k-cycles the node belongs to, since message-passing cannot detect cycle structures [10]. For spectral features, we first compute the graph Laplacian then consider the number of connected components and the two first eigenvectors of the non-zero eigenvalues.\nNodal state trajectories of networks in mutualistic dataset are simulated via the following differential equations:\n$\\frac{dxi}{dt} = B \\frac{XiXj}{3 + x_i} + x_i (1 - \\frac{x_i}{K}) (\\frac{x_i}{C} - 1) + \\sum_{j=1}^{N} A_{ij} \\frac{D}{+Exi+Hxj}$\nWe use the fourth-order Runge-Kutta stepper, with a high initialization x = 5 and a low initialization x = 0, simulating two trajectories, which represent a thriving stable ecosystem and an ecosystem after a catastrophe, respectively. We set the terminal simulation time as Tmax = 50, and the interval At = 0.5. Nodal states of networks with mutualistic dynamics can encounter a bifurcation [19], transitioning from a resilient phase characterized by a single, desired high equilibrium xH to a non-resilient phase with both the desired equilibrium xH and the low equilibrium x\u00b9. We denote the averaged nodal states of the network from high and low initializations as (x(h)) and (x(1)); therefore, to define the resilience labels of networks, we compare (x(h)) and (x(1)\u3009 at the terminal time. If |(x(h)) - (x(1))| > r, we conclude that the network cannot recover after perturbations and has two equilibrium x and x\u00b9, thus it is non-resilient. r is a pre-defined threshold, and we set r = 3.5 in our experiments.\nNodal state trajectories of networks in regulatory dataset are simulated via the following differential equations:\n$\\frac{dxi}{dt} = -x+ \\frac{xh}{x+1} \\sum_{j=1}^{N} A_{ij}$\nSimilar to mutualistic dynamics, we use the fourth-order Runge-Kutta stepper, set the terminal simulation time Tmax = 50 and the interval At = 0.5. Regulatory dynamics in Equ. (30) has a trivial fixed point (as well as equilibrium) x = (x) = 0, and for resilient networks with this dynamics, its nodal states have another equilibrium (x) > 0 [3]. To avoid the fixed-point equilibrium, we randomly initialize the model with x = [1, 5] and use the terminal nodal state (x) to determine its resilience. Specifically, a network is deemed resilient if (x) > 0. Conversely, the non-resilient network can only converge to the equilibrium of (x) = 0.\nNodal state trajectories of networks in neuronal dataset are simulated with the following differential equations:\n$\\frac{dxi}{dt} = -xi + \\sum_{j=1}^{N} A_{ij} \\frac{1}{1+ \u03b5\u03bc-8x_j}$\nThe ODE solver, terminal, and interval time settings are the same as mutualistic and regulatory dynamics. Non-resilient networks exhibit either a bi-stable phase, wherein both a high equilibrium, denoted as xH, and a low equilibrium, denoted as x\u00b9, can exist, or only a single low equilibrium x\u00b9 exists. On the other hand, resilient neuronal networks are distinguished by their maintenance of a high equilibrium xH [51]. We initialize the nodal state with a high initialization x = 5 and a low initialization x = 0, simulating two trajectories. We compare (x(h)) and (x(1)\u3009 at the terminal time to define the resilience labels of networks. If |(x(h)) \u2212 (x(1)\u3009| > r or (x(h)) < m and (x(1)) < m, we conclude that the network cannot recover after perturbations and have two equilibrium xH and x\u00b9, thus it is non-resilient. Otherwise, the network is resilient. r and"}, {"title": "A.3 Details of Theoretical Baseline", "content": "In this section, we detail how we incorporate resilience theory from physics to provide insights on leveraging unlabeled data.\nFrom Gao-Barzel-Barab\u00e1si (GBB) theory [17], for a network with N nodes, N-dimensional nodal state dynamics represented by Equ. (1) can be condensed to a 1-dimentional equation as:\n$\\frac{dxeff}{dt} = F (xeff) + Beff G (xeff, xeff),$\n=1 out xi\nN \\\\  N\\\\\nsinx\nwhere sout = (sout, \u2026\u2026, sout) denotes the out-degrees of nodes and sin = (sin, \u2026\u2026, sin) denotes their in-degrees. (sout x) = and (s) = (sin) = (sout). Therefore, we can observe that network topology A \u2208 RN\u00d7N is condensed to a scalar \u00feeff \u2208 R, which embeds the features of network topologies. From dynamics equation, f(\u1e9eeff, xeff) = F (xeff) + Beff G (xeff, xeff), we can identify the bifurcation point of this dynamics, Beff. If Beff < Beff, the undesired equilibrium will emerge, or the desired equilibrium will vanish. Otherwise, it has only one desirable equilibrium. Since \u1e9eeff is calculated from network topology, we can conclude that a network with beff < Beff is non-resilient, and that of feff > Bef is resilient. Therefore, GBB theory provides an effective tool to predict network resilience. Its limitation is that precise analytical forms of F(.) and G() are required, which is hard to determine in real-world scenarios.\nData augmentation. Here we discuss how to use GBB theory to perform data augmentation. For each network topology in labeled and unlabeled dataset, we calculate its \u1e9eeff from Equ. (34). After that, we denote the minimum \u1e9eeff of resilient networks in the labeled dataset as \u1e9e+, and denote the maximum feff of non-resilient networks in the labeled dataset as \u1e9e\u00af. Therefore, for each network in the unlabeled dataset, if its beff > \u03b2+, we label it as the resilient network; if its beff < \u03b2\u00af, we label it as the non-resilient network. Then the resilience predictor can further train on these newly-labeled data to enhance its predictive performance."}]}