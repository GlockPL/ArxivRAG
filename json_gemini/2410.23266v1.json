{"title": "TOMATO: ASSESSING VISUAL TEMPORAL REASONING CAPABILITIES IN MULTIMODAL FOUNDATION MODELS", "authors": ["Ziyao Shangguan", "Chuhan Li", "Yuxuan Ding", "Yanan Zheng", "Yilun Zhao", "Tesca Fitzgerald", "Arman Cohan"], "abstract": "Existing benchmarks often highlight the remarkable performance achieved by state-of-the-art Multimodal Foundation Models (MFMs) in leveraging temporal context for video understanding. However, how well do the models truly perform visual temporal reasoning? Our study of existing benchmarks shows that this capability of MFMs is likely overestimated as many questions can be solved by using a single, few, or out-of-order frames. To systematically examine current visual temporal reasoning tasks, we propose three principles with corresponding metrics: (1) Multi-Frame Gain, (2) Frame Order Sensitivity, and (3) Frame Information Disparity. Following these principles, we introduce TOMATO, Temporal Reasoning Multimodal EvaluaTiOn, a novel benchmark crafted to rigorously assess MFMs' temporal reasoning capabilities in video understanding. TOMATO comprises 1,484 carefully curated, human-annotated questions spanning six tasks (i.e., action count, direction, rotation, shape & trend, velocity & frequency, and visual cues), applied to 1,417 videos, including 805 self-recorded and -generated videos, that encompass human-centric, real-world, and simulated scenarios. Our comprehensive evaluation reveals a human-model performance gap of 57.3% with the best-performing model. Moreover, our in-depth analysis uncovers more fundamental limitations beyond this gap in current MFMs. While they can accurately recognize events in isolated frames, they fail to interpret these frames as a continuous sequence. We believe TOMATO will serve as a crucial testbed for evaluating the next-generation MFMs and as a call to the community to develop AI systems capable of comprehending the human world dynamics through the video modality.", "sections": [{"title": "INTRODUCTION", "content": "Visual temporal reasoning, an important aspect of human perception, refers the cognitive process of understanding and interpreting sequences of visual information over time, such as recognizing patterns of motions, detecting changes in scenery, and following the progression of events. Currently, state-of-the-art methods for addressing visual temporal reasoning are centered on the use of Multimodal Foundation Models (MFMs) which have shown remarkable performance across numerous temporal reasoning video benchmarks. However, despite these impressive performances, our study in \u00a75 has shown that the models' true capabilities in visual temporal reasoning are likely overestimated.\nThis study examines four existing widely-used temporal reasoning video benchmarks, as illustrated in Figure 1. We identify patterns in their question formulation that allow models to exploit shortcuts, enabling them to answer correctly using a single, few, or out-of-order frames. To rigorously evaluate whether a benchmark effectively assesses MFMs' visual temporal reasoning ability, we propose three principles with corresponding metrics:"}, {"title": "RELATED WORK", "content": "General video understanding benchmarks. Video understanding capability plays a pivotal role in multimodal learning and a key step toward achieving artificial general intelligence. Prior to the era of MFMs, early benchmarks focus primarily on action recognition, with an emphasis on recognizing predefined actions. However, more recent benchmarks represent a shift towards evaluating models' ability to reason about temporal dynamics and causal events. The rise of MFMs has further propelled the field toward more complex, human-like understanding tasks. These tasks include (1) long-form video understanding (2) multi-disciplinary video understanding, and (3) comprehensive evaluation across various tasks. Building upon this remarkable progress, MMBench-Video advocates the needs for more temporal questions since many questions in existing benchmarks are rendered rather \u201cstatic,\u201d but its reasoning dimension such as Attribute Recognition, Object Recognition, and OCR still remain static. This notion motivates the creation of TOMATO, a benchmark specifically designed to evaluate MFMs' visual temporal reasoning capabilities.\nVisual temporal reasoning benchmarks. Several benchmarks have been developed to specifically evaluate models' visual temporal reasoning capabilities. For instance, VITATECS introduces six temporal reasoning tasks (e.g., \"A man is putting on a tie or putting off a tie?\") and asks models to distinguish between the correct and counter-factual caption. Addressing the lack of tasks diversity in VITATECS, TempCompass expands tasks types to include multiple-choice QA, yes/no QA, caption matching, and caption generation tasks. Aiming to cover a wide-range of temporal-sensitive videos, MVBench defines nine core temporal tasks with 20 subtasks, each of which cannot be solved using a single frame. Similarly, ReXTime targets comprehensive temporal reasoning tasks and puts a special emphasis on cause and effect samples. However, despite these efforts, as illustrated in Figure 1, we observe that many questions in these benchmarks can be answered correctly using single, few, or out-of-order frames (\u00a75), limiting their effectiveness to evaluate models' true visual temporal reasoning capabilities. To address these shortcomings, we introduce TOMATO, a benchmark designed to provide a more rigorous evaluation of visual temporal reasoning."}, {"title": "BENCHMARKING PRINCIPLES FOR VISUAL TEMPORAL REASONING TASKS", "content": "In this section, we define three key principles and corresponding metrics for assessing how rigorously a benchmark targets visual temporal reasoning rather than static image understanding."}, {"title": "MULTI-FRAME GAIN", "content": "Key principle: A visual temporal reasoning task should require reasoning across multiple frames, making it impossible for models to solve the task using 1 frame.\nRequiring models to reason across multiple frames ensures that the tasks are distinguished from static image recognitions.\nTo assess this principle, we define Multi Frame Gain \\(k\\), which measures the relative performance gain from using a single frame to \\(m\\) frames as input. When considering a single frame, we examine both settings: (1) a random frame and (2) a handpicked, highly informative frame specific to the question. The \\(m\\) frames are uniformly sampled at equal intervals from the videos. Using \\(Acc(m\\ frames)\\) to denote the model's accuracy in solving benchmark tasks using \\(m\\) frames, we define \\(k\\) as:\n\\[\\kappa = \\frac{Acc(m\\ frames)}{Acc(1\\ frame)} - 1\\]\nA lower \\(k\\) value indicates that the question can be more accurately answered using a single frame, while a higher \\(k\\) value indicates a necessity to reason across multiple frames."}, {"title": "FRAME ORDER SENSITIVITY", "content": "Key principle: A visual temporal reasoning task, when given multiple frames, should enforce constraints on maintaining the correct order of frames.\nIf a task is solvable by shuffled frames, time dependencies across frames are absent, and reasoning along the time dimension is unnecessary, which disqualifies the task from being temporal.\nTo quantify this principle, we introduce Frame Order Sensitivity \\(\\tau\\), which measures the relative performance gain from using the shuffled \\(m\\) frames to the ordered \\(m\\) frames. Using \\(Acc(m\\ frames)\\) to denote the model's accuracy in solving benchmark tasks using \\(m\\) frames, we define \\(\\tau\\) as:\n\\[\\tau = \\frac{Acc(m\\ frames)}{Acc(shuffled\\ m\\ frames)} - 1\\]\nA lower \\(\\tau\\) value indicates the question can be answered more accurately using out-of-order frames, while a higher \\(\\tau\\) value suggests a stronger reliance on the original order of the frames."}, {"title": "FRAME INFORMATION DISPARITY", "content": "Key principle: A visual temporal reasoning task, when given multiple frames, should ensure that each frame contributes relatively evenly to solving the task.\nEven contribution suggests that no single frame provides disproportionately more information. Even in tasks involving sequential events, where the number of events aligns with the number of frames necessary to answer the question accurately, models should not achieve significantly higher accuracy by relying on a handpicked single frame over any random single frame.\nTo quantitatively evaluate this principle, we introduce Frame Information Disparity \\(\\rho\\), which measures the relative performance gain achieved by switching from a random single frame to a handpicked single frame. Using \\(Acc(m\\ frames)\\) to denote the model's accuracy in solving benchmark tasks using \\(m\\) frames, we define \\(\\rho\\) as:\n\\[\\rho = \\frac{Acc(handpicked\\ 1\\ frame)}{Acc(random-sampled\\ 1\\ frame)} - 1\\]\nA higher \\(\\rho\\) value indicates that the question can be more accurately answered by a handpicked single frame compared to a random single frame, while a lower \\(\\rho\\) value indicates a more even distribution of informativeness across the multiple frames. In other words, ideally, a benchmark with perfectly even distribution of informativeness among all the frames should achieve a \\(\\rho\\) value of 0."}, {"title": "TOMATO: A VISUAL TEMPORAL REASONING BENCHMARK", "content": "We introduce TOMATO, a new visual temporal reasoning benchmark that satisfies all three aforementioned principles, addressing issues where tasks from existing benchmarks are relatively more solvable by a single frame, rely less on the order of frames, and allow for more unevenly distributed information across the frames (\u00a75). TOMATO comprises 1,484 carefully curated, human-annotated questions spanning six visual temporal reasoning tasks, applied to 1,417 videos, including 805 self-recorded and -generated videos, that encompass human-centric, real-world, and simulated scenarios. In the following sections, we describe temporal tasks in TOMATO (\u00a74.1), video collection (\u00a74.2), and question annotation \u00a74.3. The main statistics of TOMATO are presented in Table 1."}, {"title": "TEMPORAL TASKS IN TOMATO", "content": "We introduce six visual temporal reasoning tasks, each of them requiring multi-frame visual temporal reasoning: (1) Rotation: Determine the rotational direction of of the subject; (2) Direction: Identify the direction of the subject's movement; (3) Velocity & Frequency: Detect changes in the subject's movement speed or variations in the frequency of repeated actions; (4) Shape & Trend: Analyze the subject's trajectory, such as the shape or general trend of its movement; (5) Visual Cues: Discern key visual signals to determine the sequence or timing of actions without audio; and (6) Action Count: Count how many times a specific action has been performed. Examples are provided in Table 2.\nTo ensure comprehensive coverage across various scenarios, we categorized each video into one of three demonstration categories: (1) Human-centric: Involving human interactions, where actions or intentions are observed; (2) Real-world: Focuses on actions involving objects in various real-world scenes; and (3) Simulated: Depicting simplified, simulated environments representing temporal actions. The distribution of demonstration categories across each task is shown in Figure 5 in \u00a7C.1."}, {"title": "VIDEO COLLECTION", "content": "TOMATO features a diverse range of videos from three distinct sources: YouTube, existing video datasets, and self-recorded and -generated benchmark-specific videos. To enhance diversity, we collect and create videos of three scenarios: real-world, human-centric, and simulated. Additionally, we edit videos to incorporate counterfactual scenes, composite motions, and zoomed-in views, aiming to investigate the impact of these characteristics on the performance of MFMs (\u00a76.3). License information for all videos is detailed in \u00a7D."}, {"title": "QUESTION-ANSWER ANNOTATION AND QUALITY CHECK.", "content": "QA annotation. To address the limitation that tasks from existing benchmarks are solvable by a single, a few, or out-of-order frames (discussed further in \u00a75), TOMATO focuses on crafting questions that require reasoning about transitions across all frames. Our annotation process varied by video sources. For YouTube videos, self-created benchmark-specific videos, and their edits, human annotators composed QA pairs targeting specific temporal reasoning tasks (\u00a74.1). For Music-AVQA and CLEVRER, we re-annotated QA pairs to emphasize temporal aspects, such as \u201cwhich musical instrument plays first\" for Music-AVQA and \u201chow many collisions are there in the video\" for CLEVRER. For TGIF-QA and Perception Test, we retained their existing questions but generate additional numerical answer options close to the groundtruth.\nQuality check. To ensure high-quality QA annotations, we implemented a three-stage process: initial annotation by annotators, followed by cross-checking and verification among annotators, and finally, collective resolution of disagreements with a final review (see \u00a7E). This rigorous approach ensured consistency and accuracy across all annotated QAs."}, {"title": "COMPARISONS AMONG VISUAL TEMPORAL REASONING BENCHMARKS", "content": "In \u00a73, we defined three key principles with corresponding metrics to assess how effectively a benchmark addresses visual temporal reasoning. Using these metrics, we compare TOMATO with four recent visual temporal reasoning benchmarks: VITATECS , MVBench , TempCompass, and ReXTime. To conduct this comparison, we randomly sampled approximately 200 QAs from these benchmarks, using two state-of-the-art MFMs: GPT-4o  and Qwen2-VL-72B . For metrics that requires handpicking frames, we present annotators with the full video and corresponding question, and ask them to select the most informative frame for each benchmark. For metrics requiring multiple frames, we set m = 16, as our study across m = 1, 8, 16, 32 demonstrates that 16 frames provide a sufficient window for effective analysis (\u00a7C.2)."}, {"title": "MULTI-FRAME GAIN", "content": "We present results on Multi-frame Gain for both (1) a random frame (Table 3) and (2) a handpicked, highly informative frame (Table 4). As shown in the two tables, TOMATO achieves a significantly higher \\(k\\) value in using both random single frame and handpicked single frame. This significant relative performance gain from the single frame input setting to the 16 frames input shows the necessity in our tasks to reason using multiple frames. In comparison of the two tables, we observe an expected decrease in the \\(k\\) value in using handpicked single frame compared to random single frame on existing benchmarks, indicating the relative ease in answering the benchmark questions using handpicked single frame. Interestingly, the \\(k\\) values are negative in the handpicked single frame setting on ReXTime; this negative performance gain in using more frames might stem from noise introduced by additional frames."}, {"title": "FRAME ORDER SENSITIVITY", "content": "In our shuffled 16 frames setting, we apply random shuffling on the ordered 16 frames to ensure that the same set of frames are used in both settings. As shown in Table 5, TOMATO achieves significantly higher \\(\\tau\\), demonstrating that our benchmark imposes a stricter requirement on maintaining the order of the frames to accurately answer the question."}, {"title": "FRAME INFORMATION DISPARITY", "content": "As shown in Table 6, the performance gain from random single frame to handpicked single frame is the lowest on TOMATO, indicating a relatively more consistent informativeness across all frames compared to other existing benchmarks."}, {"title": "EVALUATING VISUAL TEMPORAL REASONING IN ADVANCED MFMS", "content": "In introducing TOMATO, we present a comprehensive evaluation of 23 MFMs, including 7 proprietary models and 16 open-source models, to assess their visual temporal reasoning capabilities. In the following sections, we detail the experimental setup (\u00a76.1), evaluation results (\u00a76.2), and a multi-faceted analysis (\u00a76.3), considering factors such as model architectures, reasoning type correlations, frame counts, and video characteristics."}, {"title": "EXPERIMENTAL RESULTS", "content": "We provide quantitative results on TOMATO for all models in Table 7. To better understand where models fail, we select a set of representative models and present examples of failure cases in \u00a7G, \u00a7H, \u00a7I, \u00a7J, \u00a7K, and \u00a7L."}, {"title": "Widespread difficulty in temporal reasoning.", "content": "Our evaluation (Table 7) underscores the significant challenges of TOMATO across all tested models. The leading open-source model, Qwen2-VL-72B, achieves 37.9% accuracy, slightly outperforming GPT-4o's 37.7%. However, this still leaves a substantial 57.3% performance gap compared to human accuracy of 95.2%. While this result demonstrates the competitive potential of open-source models in video understanding, many still fall below 30.0%, indicating weaknesses in TOMATO despite their decent performance in existing benchmarks."}, {"title": "A narrowing gap between proprietary and open-source models.", "content": "Open-source models demonstrate competitive performance in several temporal reasoning tasks. In velocity & frequency, Qwen2-VL-7B Instruct and -72B achieves 41.9% and 43.8% accuracy, respectively, significantly outperforms the best proprietary model, GPT-4o (31.9%). Similarly, for rotation, Video-CCAM-v1.1 14B (32.2%) surpass all propriety models, including GPT-4o (24.5%). These findings underscore the growing capabilities of open-source models in addressing specific challenges of visual temporal reasoning, suggesting a narrowing gap between open-source and proprietary models in this domain."}, {"title": "ANALYSIS", "content": "Models lack the basic ability to interpret frames as a continuous sequence. While MFMs demonstrate remarkable performance in understanding sequential events in videos , our benchmark exposes a more fundamental limitation: models struggle to reason across multiple time steps and to interpret the frames as a continuous sequence. As shown in error case \u00a7G.2.1, GPT-40 correctly generates captions for each consecutive change in the moon's movement, showcasing its ability to reason at individual time steps. However, it fails to infer based on the captions that the overall sequence represents a clockwise rotation. This issue is not limited to rotation (\u00a7G.1.1, \u00a7G.2.1); similar shortcomings are observed in direction (\u00a7H.3.3), action count (\u00a7L.2.5), etc.."}, {"title": "Models fail to truthfully leverage the visual input while being over-reliant on common sense.", "content": "In our evaluations, despite explicit instructions (\u00a7B.1) to rely on the visual input rather than common sense, we find that models frequently hallucinate based on information from single frames rather than utilizing true visual reasoning. For instance, in error case \u00a7I.2.4, GPT-40 incorrectly concludes that an object is dropping due to the presence of motion blur in some of the frames. However, this is a reversed video where the object is actually only moving upward - a conclusion that can only be reached if the video modality where truthfully utilized. A similar instance of the limitation occurs in error case \u00a7H.1.2, where GPT-40 likely assumes the person raises their hand first to reach the posture depicted in the first 8 single frames. In reality, the person's hand remains relatively stationary throughout these frames, but the model fails to make accurate visual comparisons across these frames."}, {"title": "Models are highly susceptible to noisy information in the input.", "content": "As demonstrated in error case \u00a7H.3.3, where a block moves downwards, models are especially vulnerable to noisy information, such as misleading text on the block. In particular, while GPT-40 correctly describes the block's downward motion based on its relative position to the screen, it incorrectly concludes that the block is moving upward, likely influenced by the false information presented in the text written on the block. Similarly, in error case \u00a7L.2.5, a butterfly-shaped laser spot is moving in a triangle shape. However, the unique butterfly shape likely causes the models to lose focus on the trajectory of the laser spot, and resort to random guessing in their conclusions."}, {"title": "Explicitly incorporating time-aware positional encoding can likely enhance visual temporal reasoning.", "content": "While open-source MFMs generally underperform proprietary models on TOMATO, a closer examination of the leading models' architectures and training strategies reveals promising avenues for improvement. The Qwen2-VL family , which consistently achieves"}, {"title": "Existing models are limited to understanding events that are interpretable in < 8 frames.", "content": "We assess four MFMs' performance across different number of frames on TOMATO, as illustrated in Figure 2. The consistent improvement in accuracy of human performance suggests that our benchmark rely on the additional frames to convey more temporal information. Notably, although models exhibit performance increase transitioning from 1 frame to 8 frames, the performance plateaus beyond this point. This suggests that even if models are able to reason the transitions between the frames before 8 frames, they cannot utilize the additional temporal information obtained by the added frames. Therefore, we conclude that the overall performance of the four MFMs remain suboptimal in their visual temporal reasoning capabilities, and there is still room for improvements in MFMs' ability to leverage the additional information on the frame transitions introduced by added frames."}, {"title": "Models perform better on simulated human than real human scenarios.", "content": "To investigate the extent to which a cleaner, more abstract representation of video content influences models' temporal reasoning abilities, we evaluate two leading MFMs. Specifically, this evaluation contrasts real human scenarios and their corresponding simulated counterparts, across five reasoning types (i.e., action count, direction, rotation, shape & trend, and velocity & frequency), covering the same 101 QA pairs in both scenarios. As a result, GPT-40 marks a noticeable improvement of 21.9% from real human to simulated human scenarios, underscoring the potential of enhancing models' temporal reasoning capabilities in video understanding through semantic video abstraction. Conversely, Qwen2-VL-72B displays a modest increase of 7.8% transitioning from real human to simulated human. While it marginally outperforms GPT-4o in the overall evaluation on TOMATO, its visual temporal reasoning capabilities in simulated scenarios still show room for improvement. Future work can target further enhancing models' temporal reasoning abilities for real human videos by exploring their generalization capabilities through leveraging automatically generated simulated 3D human motion data."}, {"title": "More capable models are more reliant on common sense.", "content": "In curating the counterfactual QAs, we employ video editing techniques (e.g., reversing, rotating, cropping) to produce contents that are impossible to observe in real life. As detailed in Table 8, although all four models demonstrate similar performance on non-counterfactual QAs, the shift to counterfactual examples reveals a significant performance drop, particularly for the best general-purpose model, GPT-4o, with a decrease of 37.7%, and the leading open-source model, Qwen2-VL-72B, with a 29.4% decrease. These results"}, {"title": "CONCLUSION", "content": "Existing benchmarks likely overestimate the true visual temporal reasoning capabilities of MFMs. In response, we establish three key principles and corresponding metrics to systematically examine visual temporal reasoning tasks. Building upon these principles, we introduce TOMATO, a novel video understanding benchmark to rigorously assess MFMs' true visual temporal reasoning capabilities. Besides revealing a previously underestimated human-model performance gap, our comprehensive evaluation highlights a critical limitation: MFMs fail to interpret videos as continuous sequences, instead resorting to understanding isolated frames, which severely undermines their visual temporal reasoning capabilities. This work sheds the light for developing AI systems capable of comprehending changing scenes in real life through the video modality."}]}