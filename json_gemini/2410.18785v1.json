{"title": "Should We Really Edit Language Models?\nOn the Evaluation of Edited Language Models", "authors": ["Qi Li", "Xiang Liu", "Zhenheng Tang", "Peijie Dong", "Zeyu Li", "Xinglin Pan", "Xiaowen Chu"], "abstract": "Model editing has become an increasingly popular alternative for efficiently up-\ndating knowledge within language models. Current methods mainly focus on\nreliability, generalization, and locality, with many methods excelling across these\ncriteria. Some recent works disclose the pitfalls of these editing methods such\nas knowledge distortion or conflict. However, the general abilities of post-edited\nlanguage models remain unexplored. In this paper, we perform a comprehensive\nevaluation on various editing methods and different language models, and have\nfollowing findings. (1) Existing editing methods lead to inevitable performance\ndeterioration on general benchmarks, indicating that existing editing methods main-\ntain the general abilities of the model within only a few dozen edits. When the\nnumber of edits is slightly large, the intrinsic knowledge structure of the model\nis disrupted or even completely damaged. (2) Instruction-tuned models are more\nrobust to editing, showing less performance drop on general knowledge after edit-\ning. (3) Language model with large scale is more resistant to editing compared to\nsmall model. (4) The safety of the edited model, is significantly weakened, even for\nthose safety-aligned models. Our findings indicate that current editing methods are\nonly suitable for small-scale knowledge updates within language models, which\nmotivates further research on more practical and reliable editing methods. The\ndetails of code and reproduction can be found in Appendix F.", "sections": [{"title": "Introduction", "content": "Recently, large language model (LLM) like ChatGPT [1], Claude [2], and Llama [3; 4] have revolution-\nized the deep learning and demonstrated remarkable performance across various knowledge-intensive\ntasks [5; 6]. However, the learned vast amount of knowledge in LLMs may be erroneous, harmful,\nor outdated [7]. While directly fine-tuning an LLM on calibrated knowledge can help mitigate this\nproblem, which is prohibitive due to hardware constraints and resource budget [8; 9; 10]. To this end,\nmodel editing [11; 12; 13] has been proposed to efficiently update knowledge within LLM. Current\nstudies in editing [14; 15; 16; 17] target at enabling efficient yet precise model behavior alterations\non specific knowledge samples in the form of triplet within LLMs like modifying the wrong tuple\n(Tim Cook, is the CEO of, Google) to the correct one (Tim Cook, is the CEO of, Apple) persistently."}, {"title": null, "content": "The primary goal of model editing is to impact the predictions for related inputs termed editing scope\n[18] generally, without influencing behaviors on unrelated knowledge samples. The assessment of\nan editing method typically involves evaluation along three dimensions [13; 11]. First and foremost,\nthe reliability aims to ascertain the capability of the post-edited model to recall the specific editing\nfact accurately. Second, the generalization seeks to validate the adaptability of the edited model by\nassessing its ability to recall the editing fact under diverse paraphrase prompts. The last dimension\nlocality (a.k.a., specificity) is employed to verify the robustness of the edited model by examining\nwhether its output for unrelated inputs remains consistent after editing. Existing knowledge editing\nmethods like SERAC [17], ROME [16], MEMIT [19], and IKE [14] work well on these evaluation\ncriteria across various datasets on different LLMs."}, {"title": null, "content": "Despite these successes in editing language models, recent works [20; 21; 22; 23] have disclosed\nthe inevitable pitfalls of existing editing methods from different perspectives such as knowledge\ndistortion [22], and catastrophic forgetting [24]. In sequential editing setting (see Section 2), as the\nnumber of edits increases, it is necessary to balance two aspects: the retention of the model's original\nknowledge and the preservation of newly acquired knowledge through updates. These two objectives\nare to some extent conflicting. The general ability (Section 2) of LLMs is the foundation to solve the\nwide range of complex tasks. Changes in the model's general capabilities reflect the retention of its\noriginal knowledge. However, the general abilities of post-edit language models are still unexplored,\nmaking current editing methods unreliable to be employed for real-world applications. Given this\nsituation, it naturally motivates the following critical question to explore:\nHow do sequential model editing affect the general abilities of language models ?"}, {"title": null, "content": "To close this gap, we make a comprehensive understanding and analysis of edited LLMs with\nvarious editing methods (In Section 3). In detail, we edit multiple LLMs with various editing\nmethods and evaluate them across benchmarks to verify underlying factors that may affect the general\nabilities. It is worth noting that our focus is on the general capabilities of the model (including\nworld knowledge, reading comprehension, reasoning, safety, etc.), rather than the performance on\nefficacy, generalization and locality or downstream tasks like NER, QA, and NLI. These distinctions\ndistinguish our work from some existing studies like [25; 24; 26]. Technically, we explore the impact\nof various underlying factors such as the number of edits, model scale, safety, different aspects of\nabilities, and instruction tuning on the general capabilities of edited LLMs after sequential editing."}, {"title": null, "content": "The empirical results indicate that the majority of current editing methods do not significantly\ninfluence the fundamental capabilities of models within dozens of edits (Section 4.1). However, after\nnearly to hundred edits, some methods lead to rapid degradation, while other methods only slightly\naffect performance after several hundred edits or even thousands (MEND, PMET). When subjecting\nmodels to an extremely high number of edits (up to 10k), we observe that the intrinsic knowledge\nstructure of the models is thoroughly disrupted, leading to outputs of empty strings for any input. We\nrefer to this phenomenon as the muting effect in sequential language model editing."}, {"title": null, "content": "Furthermore, we discovered that for the vast majority of current editing methods, the instruction-tuned\nmodels exhibit a slower rate of performance decline after edits (Section 4.2), and the smaller model\nis more vulnerable to deterioration caused by editing (Section 4.3). Moreover, results reveal that\ndifferent model editing methods affect all aspects of a model's capabilities to a roughly equivalent\nextent (Section 4.4). Our research also explores the safety aspects of these edited models (Section\n4.5), revealing that even with dozens of edits, safety can be compromised to a certain extent."}, {"title": null, "content": "We hope our work can provide the research community with insightful perspectives to help advance\nstudies in this field, systematically elucidating the impact of existing model editing methods on LLM.\nTo the best of our knowledge, we are the first to comprehensively evaluate and analyze the general\nabilities of edited language models. Our main contributions are summarized as follows,"}, {"title": null, "content": "\u2022 We conducted a detailed evaluation of the impact of different model editing methods on the\ngeneral capabilities of LLM across various numbers of edits. We found that existing model editing\nmethods are only suitable for a limited number of edits, generally not exceeding several dozen."}, {"title": null, "content": "\u2022 Empirically, extensive explorations with different editing approaches are conducted to verify\npotential factors affecting the fundamental capabilities of edited models. These insights are\nbroadly applicable across different editing methods and various models."}, {"title": null, "content": "\u2022 Our empirical studies with various editing methods across different models reveal that even with\nonly dozens of edits, the safety of LLMs can be compromised to a certain extent."}, {"title": null, "content": "\u2022 Technically, We have conducted an in-depth analysis of the side effects, operational efficiency,\nand deployment of edited LLM, discussing their practical use in production."}, {"title": "Preliminary", "content": "In this section, we provide comprehensive preliminaries of model editing and LLM evaluation."}, {"title": "Model Editing", "content": "Model editing (also known as knowledge editing) aims to precisely adjust the\nbehaviors of a language model Me on some facts without influencing unrelated samples. Current\nworks focus on editing knowledge tuple $t = (s,r,o)$. The editing process inserts new tuples\n$(s, r, o*)$ in place of the current tuple $(s, r, o)$, where these two share the same s and r. An editing\noperation is denoted as $e = (s, r, o, o*)$ for brevity. Given n fact tuples $T* = (t*,,....)$ where\n$t = (si, ri, o), i = 1, 2, . . ., n$, and a model M\u0259, model editing yields an edited language model\nMo via editing operations $E = {e1,2,...}$, where $Me(sj,rj) = 0$ if $tj = (sj,rj,0) \u2208 T*$\nelse $Mo(sj, rj) = 0j$. To evaluate model editing methods, current works focus on three dimensions:\nreliability, generalization, and locality [18]. Please refer to Section 6 for a comprehensive survey."}, {"title": "General Abilities of Language Models", "content": "In recent years, the field of LLM has experienced\nrapid growth, leading to the development of numerous models by various research institutions.\nThese models differ significantly in terms of parameter size, architecture, corpora, and training\nmethodologies. Consequently, it has become critically important to evaluate the capabilities of these\nmodels objectively, and comprehensively. Typically, this is achieved by evaluating the models on\nwidely adopted benchmarks like MMLU [27], and BigBench [28] to compare their performance with\ntheir counterparts. Currently, the evaluation of the general capabilities of LLMs in both academia\nand industry focuses on several key areas: world knowledge, common sense reasoning, coding,\nreading comprehension, mathematical skills, and performance on mainstream benchmark datasets\n[29; 3; 4; 30; 31; 32]. This paper concentrates on the impact of editing on the inherent capabilities of\nLLM, rather than on downstream tasks like QA, NER, and NLI in work [25]."}, {"title": "Model Editing Evaluation", "content": "The current evaluation protocol of model editing involves updating a\nmodel's knowledge, assessing the post-edit model, and restoring the update before editing. However,\nin real-world applications like sequential editing, models are expected to maintain preceding modi-\nfications before performing new edits. Scaling sequential editing capability is therefore crucial for\nmodel editing. In this paper, we mainly focus on whether the general abilities of LLM are hurt in\nsequential editing settings. Two orthogonal concepts of sequential editing are single editing and batch\nediting. Batch editing refers to the model's ability to edit multiple editing samples (like MEMIT,\nPMET) at once, whereas the concept opposed to this is single editing, which means these methods\n(like ROME, GRACE, MEND) can only edit one sample at a time."}, {"title": "Formal Definition of Sequential Editing", "content": "Here, let's provide a formal definition of sequential editing.\nAssume we have an unedited model Mo, and n editing samples(xi, Yi), where i = 1, 2, . . ., n need\nto be incorporated into the language model Mo. Suppose the editing operation is a function E(\u00b7,\u00b7),\nwhere the first parameter is the model to be edited and the second parameter is the editing samples.\nAssume we get the edited model Mi after the i-th editing operation. In sequential editing, Mt (model\nparameter after the t-th editing) is determined by the model weight $M_{t-1}$ and the editing sample\nused in the t-th edit, like $M\u2081 = E(M_{t\u22121}, S_t)$, where St is the edit samples used in the t-th edit. For"}, {"title": null, "content": "different index i and j, $S\u00bf \u2229 Sj = 0$; for every i, we have $U; S\u2081 = {xj, yj}j=\"1$. If we denote the\nsize of St is nt, it satify nt > 1 for every t and satisfies n = \u2211t nt for all edit batches."}, {"title": "Experiments Design", "content": "In this section, we present several critical parts of experimental setups (in Section 3.1) and research\nquestion designs (in Section 3.2). The results and analysis of research questions are left in Section 4.\nAll of the details of reproducing our experiments can be found in Appendix F."}, {"title": "Experimental Setups", "content": "Here, we list all of the experimental setups. For implementation details, please refer to Appendix C."}, {"title": "Language Models", "content": "We conduct experiments on various LLMs, including Llama2-7B (based and\ninstruction-tuned) [4], Mistral-7B (based and instruction-tuned) [30], GPT2-XL [33], and 6 language\nmodels from Pythia [34] model families with varying parameter scale from 160M to 12B."}, {"title": "Model Editing Methods", "content": "To comprehensively investigate the potential impact on edited models,\nwe compared multiple editing methods: (1) meta-learning based methods: MEND [15] (2) located-\nthen-edit based method: KN [35], ROME [16], MEMIT [19], PMET [36], (3) retrival based methods:\nSERAC [17], (4) extra parameters based methods: GRACE [37]. It is imperative to note that the\nefficacy of edited models varies depending on the hyperparameters employed for each method, which\ncan significantly impact the performance of the edited model. Therefore, our evaluation was confined\nto assessing only models supported by each editing method."}, {"title": "Editing Datasets", "content": "In this work, we employ the most widely adopted editing datasets ZsRE [38] and\nCOUNTERFACT [16] as editing datasets across all experiments in the work."}, {"title": "Evaluation Benchmarks", "content": "To effectively determine whether the model editing influences the overar-\nching capabilities of LLMs, we utilized five distinct task categories as benchmarks. These include:\nWorld Knowledge: MMLU [27] (5-shot), BBH [28] (3-shot), with the assessment based on the\naccuracy of multiple-choice answers. Arithmetic: GSM8K [39] (8-shot), evaluated by the solve rate.\nCommonsense Reasoning: CommonsenseQA [40] (7-shot), where performance is gauged by the\naccuracy of multiple choices. Reading Comprehension:TriviaQA [41] (0-shot), with the assessment\nbased on the exact match from context. Safety: TruthfulQA [42] (0-shot), evaluated through multiple-\nchoice accuracy, and ToxiGen [43] (0-shot), where results are determined by the accuracy of two-way\nclassification. For more details of these benchmarks, please refer to Appendix D.1."}, {"title": "Editing Settings", "content": "To investigate the potential impact on language models caused by editing, we\nmainly focus on sequential single editing in most of the research questions in this work."}, {"title": "Research Questions", "content": "This paper aims to thoroughly examine the impacts of diverse model editing methods on various\ngeneral abilities of edited models. It naturally motivates the following critical research questions\n(RQs) to be explored in this work based on the primary aim."}, {"title": null, "content": "\u2022 RQ1: How does the number of undergone edits affect the abilities of models? (In Section 4.1)\n\u2022 RQ2: Do instruction-tuned models exhibit differently than base counterparts? (In Section 4.2)\n\u2022 RQ3: Does the general abilities of the edited model differ on model scales? (In Section 4.3)\n\u2022 RQ4: How does editing affects different aspects of a model's capabilities? (In Section 4.4)\n\u2022 RQ5: Does performing editing on language models compromise their safety? (In Section 4.5)"}, {"title": null, "content": "In the next section, we will address these research questions through detailed experimentation."}, {"title": "Results and Analysis", "content": "In this section, we present empirical results and a comprehensive analysis of research questions.\nMore detailed information and conducted experiments are presented in the Appendix C. The case\nstudies of benchmark evaluation with different editing settings can be found in Appendix A."}, {"title": "RQ1: Impact of the Number of Edits", "content": "Main results. We first investigate the impact of the number of edits with different methods (RQ1)\non the general abilities of edited language models here. We perform editing with various editing\nmethods on Llama2-7B, and Mistral-7B with different numbers of edits. The detailed results are\nreported in Figure 2 and Table 2. The results reveal that models edited with various methods exhibit\nsignificant performance divergences after undergoing different numbers of edits. Experiments on the\nLlama2-7B model demonstrate that the majority of model editing techniques maintain the original\ncapabilities of the model with fewer than 20 edits. However, when the number of edits increases,\nsome methods, such as ROME and MEND, exhibit significant performance degradation. In contrast,\nother approaches, like PMET, do not affect model performance even after hundreds of edits."}, {"title": null, "content": "The muting effect: scaling sequential single editing to 10k edits. We have demonstrated that\nediting methods such as MEMIT and PMET can enable models to withstand hundreds of edits while\nmaintaining their original capabilities as much as possible. However, the extent to which the number\nof edits can be expanded remains an open question. To investigate the performance of language\nmodels after undergoing an extremely large number of sequential edits, we applied the ROME,\nMEMIT, and PEMT methods to the Llama, Mixtal, and GPT2-XL on the COUNTERFACT dataset,\nimplementing 10,000 sequential edits. The results revealed that after such a substantial number of\nedits, the intrinsic knowledge structure of the models was completely disrupted. For any input, the\nresponse was an empty string or random character. We refer to this phenomenon as the muting\neffect of model editing. Please refer to the Case Study section, Section A, for more details."}, {"title": "Findings 4.1.", "content": "The majority of existing methods can only undergo dozens of edits without compro-\nmising performance, while only a few methods can scale to thousands of edits."}, {"title": "RQ2: Does Instruction Tuned LLM Show Better Performance after Editing?", "content": "We then explore the impact of instruction tuning (RQ2) here. The empirical results are demonstrated\nin Figure 3 and Table 3. Our findings indicate that for the majority of editing approaches, the\nimpact on performance after editing is comparable between the instruction-tuned model and the base\nmodel. However, instruction tuning can slow down the rate of performance decline after model edits.\nPerformance is not significantly affected by lower than 20 edits. However, some methods exhibit\nnoticeable performance degradation after exceeding 20 edits. Notably, when the MEMIT method is\nused to edit models that have been fine-tuned with instructions, there is a slower decline in general\ncapabilities with increasing edits compared to models that have not undergone instruction tuning."}, {"title": "Findings 4.2.", "content": "Instruction-tuned model exhibits a slower rate of performance decline after editing."}, {"title": "RQ3: Do the General Abilities of the Edited Model Differ on Model Scales?", "content": "In this subsection, we examine the impact of model size (RQ3) on the capabilities of edited models.\nTo ensure a fair comparison, we employ multiple models from the Pythia model family, with sizes\nranging from 160M to 12B parameters. We perform editing with two methods: MEMIT and ROME,\nand then conduct evaluation on 4 benchmarks. We summarize the empirical results of general abilities\nevaluation in Figure 4 and Table 4 with different model sizes. Our findings reveal that for some\nmethods, such as ROME, the rate of performance degradation in language models following edits\nslows as the model size increases. Conversely, the impact of other editing methods, like MEMIT and\nPMET, on post-edit language model general abilities appears to be independent of model parameters."}, {"title": "Findings 4.3.", "content": "Larger models exhibit fewer side effects on benchmarks after editing."}, {"title": "RQ4: How Does Editing Affects Different Aspects of a Model's Capabilities?", "content": "In this subsection, we would like to further explore how model editing influences different aspects of\nthe edited language model abilities (RQ4). Figure 5 comparing 6 different editing methods on the\nLlama2-7B model across various benchmarks. Different benchmarks correspond to different abilities\nof the language model, e.g. MMLU and BBH are for world knowledge, GSM8K corresponds to math\nabilities, TrivialQA is for reading comprehension, and CSQA is related to reasoning. Results in Table\n2 and 3 reveal that different model editing methods affect all aspects of a model's capabilities roughly\nuniformly. Results in Table 2 and 3 reveal that different model editing methods affect all aspects of a\nmodel's capabilities uniformly. Figure 5 comparing 6 different editing methods on the Llama2-7B\nmodel across various benchmarks reveals that PMET and MEND maintain the most consistent\nperformance, effectively preserving the model's abilities across all tasks, even with numerous edits."}, {"title": "Findings 4.4.", "content": "Editing affects the different capabilities of LLM to a roughly equivalent extent."}, {"title": "RQ5: The Safety Cost of Editing Language Models", "content": "We inspect the safety issues caused by performing editing on LLM (RQ5) in this subsection. To\nadjust pre-trained LLMs' behavior on specific cases, some editing operations are desirable. But, what\nare the safety costs associated with these editing operations? Here, we focus on studying the ability\nof edited LLMs to handle malicious prompt attacks and adversarial input. We conducted evaluations\non the ToxiGen dataset and TruthfulQA with LLM that edited with various methods and different\nedits. In Figure 6 and Table 5, we summarize the experimental results of safety evaluation on edited\nlanguage models with different methods. These experimental results indicate that the security of\nedited LLMs (including alignment with human values, resilience to malicious inputs, and prevention\nof generating harmful content) is compromised to some extent as the number of edits increases, even\nwith instruction tuning. Interestingly, many methods exhibit improved scores when the model has\nundergone 100 edits. We hypothesize that this is due to the significant disruption of the model's\nintrinsic knowledge structure."}, {"title": "Findings 4.5.", "content": "Even dozens of edits can compromise the safety of edited language models."}, {"title": "Summary", "content": "From the above research questions, we conclude that existing editing methods have\ninevitable pitfalls in editing LLMs, making them impractical in the production environment."}, {"title": "Further Discussion", "content": "In this section, we conduct discussions on the side effects, editing efficiency, and deployment issues\nof post-edit LLM, to further explore their practical use in production environments."}, {"title": "Potential Impact on Inherent Knowledge within LLM", "content": "Existing model editing methods claim\nthat they can update specific knowledge within LLM without affecting other unrelated knowledge.\nHowever, existing research [25; 24] indicates that methods involving direct modification of model\nparameters can have unintended and potentially harmful impacts on the intrinsic knowledge of the\nmodel. Moreover, the updated knowledge is often challenging to utilize in knowledge-intensive\ndownstream tasks, such as reasoning and knowledge-based question answering, exacerbating the\nmodel's hallucinations [21; 20]. As the quantity of edits escalates, the retention of updated knowledge\nwithin the LLM markedly diminishes [19]. Our series of experiments demonstrates that even with\nonly hundreds of edits, the general capabilities of the model are severely compromised. When the\nnumber of edits reaches the thousands, the model's internal structure is thoroughly damaged. These\nfindings highlight the deficiencies of current model editing methods."}, {"title": "Editing Efficiency and Speed", "content": "Existing editing methods claim to manipulate the inherent knowledge\nwithin model efficiently and rapidly while keeping the memory usage of LLMs close to that of\nvanilla model inference during deployment. These advantages make model editing seem highly\nappealing. However, inherent drawbacks severely impede their real-world applicability. Many\nexisting editing methods heavily rely on hyperparameter settings, some even necessitating costly\nparameter searches. For instance, locate-then-edit approaches (see Section 6) like ROME [16], and\nPMET [36] require causal tracing to identify layers for editing. Moreover, certain editing methods\nentail pre-training (MEND [15], SERAC [17], KN [35]) or precomputation of intermediate cached\nkey-value (MEMIT[19]), significantly prolonging pre-training or computational time and consuming\nhardware resources far beyond those required during editing, without targeted optimization for\nparallelization benefits. For example, utilizing the MEMIT for 10K edits on Llama2-7B [4] with an\nRTX A6000 48G RAM GPU demands approximately 120 hours. Hence, enhancing the efficiency of\nexisting methods becomes paramount for real-world applications. See Table 8 for edit speed."}, {"title": "Deployment and Serving", "content": "As the scaling up of LLM, considerable deployment and serving chal-\nlenges are presented [44; 9; 10]. To reduce the expenses of deployment and serving of LLMs,\nseveral specialized frameworks have been developed, including TensorRT-LLM [45], vLLM [46],\nand LightLLM [47], with high optimization for efficient LLM serving and deploying. However,\nsome model editing methods, such as the GRACE [37], introduce additional modules to the edited\nmodels, or employ auxiliary models, like the SERAC [17], to handle queries involving updated\nknowledge. These modifications to the model architecture prevent the edited models from being\ndirectly deployed using these aforementioned serving frameworks, significantly limiting the practical\nadoption of editing methods. Furthermore, as the number of edits on LLM increases, for methods\nthat introduce additional, the memory consumption for storing and the time cost for retrieval become\nincreasingly prohibitive. This significantly impacts the throughput and inference latency during\ndeployment and serving. For further details on these effects, please refer to Appendix D.2."}, {"title": "Related Work", "content": "In this section, we briefly review the related works in model editing and language model evaluation."}, {"title": "Model Editing", "content": "Model editing aims to precisely modify knowledge within a language model with fine\ngrain. Existing methods can be divided into 3 different categories: Retrieval-based, Extra-parameters-\nbased, and Locate-then-edit-based methods. Early works on editing focused on updating individual\nneurons using constrained fine-tuning [48; 49; 50], or hypernetworks [51]. A related line of work has\nfocused on storing updates in an external memory [52]. Inspired by the linear associative memory\nproperty of FFN in transformers [53] and success with the approach in convolutional models [54],\nrecent Locate-then-edit style works have proposed to edit MLP weights directly [16; 19; 36]. In the\nencyclopedic factual domain, work [16] proposed to edit single facts by fitting a Rank One Model\nEdit (ROME) to the parameters of an MLP layer and showed that it outperformed prior methods.\nWhile [55] concentrates on editing commonsense knowledge. Work [56] focuses on editing the\nencoder-decoder model. Work [11; 13; 18] make a comprehensive survey in model editing."}, {"title": "Pitfalls of Model Editing", "content": "Since the concept of model editing was introduced, there exist few works\nto discuss the drawbacks of existing methods. In [22], two kinds of pitfalls are discovered, named\nknowledge conflict and knowledge distortion. MEMIT-CSK [55] finds common sense knowledge\ncan also be localized in FFN of LLM layers and both subject, object, and relation play important\nroles in recalling memory, but they only focus on classification tasks. [57] focus on mitigating the\nreversal curse in model editing for LLM. Some recent works [21; 58] try to evaluate the multi-hop\nreasoning of updated knowledge. Work [26; 20] tries to assess the underly impact caused by editing.\nWhat's more, some works [59; 60; 61; 62] discuss the knowledge forgetting during fine-tuning."}, {"title": "Evaluation of LLM", "content": "Evaluating the effectiveness of LLMs [3; 4; 30; 29; 63; 31; 32] involves a\ndiverse array of tests, where models are assessed across various tasks, showcasing their capabilities.\nAmong the benchmarks employed, Bigbench [64; 28], MMLU [27], and HELM [65] are notable.\nThese benchmarks utilize a range of automatic evaluation metrics, such as BLEURT [66] and others\nrequiring meticulous annotation to ensure data quality for downstream applications [67; 68; 69].\nTypically, these evaluations emphasize accuracy across multiple choices, which serves as the primary\nmetric [70; 71]. Foundation models, often tested across broad linguistic tasks-both generative\nand multiple-choice-are analyzed to ensure rigorous assessment standards [72; 27; 73; 64]. These\nmethods, however, may focus on the accuracy within confined options and might not effectively\ncapture the nuances of more open-ended, practical applications where models generate free-form\ntext. Moreover, assessing the safety of LLMs is essential. Safety evaluations concentrate on three key\nareas: truthfulness [42], toxicity [43], and bias [74]."}, {"title": "Conclusion", "content": "In this work, we systematically investigate the potential impact of model editing on language models.\nWe employed various editing methods to modify multiple models, followed by evaluations across\ndifferent benchmarks. The experimental results indicate that existing editing methods can preserve\nthe general capabilities of the model within a limited number of edits, not exceeding a few dozen.\nWhen the number of edits is sufficiently large, the intrinsic knowledge structure of the model can be\ndisrupted or even completely damaged. Additionally, we systematically investigated potential factors"}]}