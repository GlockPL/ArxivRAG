{"title": "A Multi-Layered Large Language Model\nFramework for Disease Prediction", "authors": ["Malak Mohamed", "Rokaia Emad", "Ali Hamdi"], "abstract": "Social telehealth has made a breakthrough in healthcare by\nallowing patients to share their symptoms and have medical consulta-\ntions remotely. Users frequently post symptoms on social media and on-\nline health platforms, creating a huge repository of medical data that\ncan be leveraged for disease classification and symptom severity assess-\nment. Large language models (LLMs) like LLAMA3, GPT-3.5 Turbo,\nand BERT process complex medical data, enhancing disease classifica-\ntion. This study explores three Arabic medical text preprocessing tech-\nniques: text summarization, text refinement, and Named Entity Recogni-\ntion (NER). Evaluating CAMeL-BERT, AraBERT, and Asafaya-BERT\nwith LORA, the best performance was achieved using CAMEL-BERT\nwith NER-augmented text (83% Type classification, 69% Severity assess-\nment). Non-fine-tuned models performed poorly (13%-20% Type clas-\nsification, 40%-49% Severity assessment). Embedding LLMs in social\ntelehealth enhances diagnostic accuracy and treatment outcomes.", "sections": [{"title": "1 Introduction", "content": "The growth of social telehealth has revolutionized the provision of healthcare,\nenabling patients to share their symptoms and even consult with doctors re-\nmotely. During the COVID-19 pandemic, social telehealth increased greatly in\npopularity; at that time, access to traditional healthcare services was limited.\nSocial media platforms, particularly health forums, are becoming increasingly\nvaluable sources of user-generated medical data that people use to share de-\ntailed symptoms and seek the advice of doctors' opinions [8,9,11]. However,\nthe unstructured and noisy nature of this data poses a great challenge; hence,\nadvanced computational techniques are required for effective analysis. Recent\nadvances in NLP, especially with the advent of large language models, have re-\nally transformed unstructured textual data processing [8,9,11]. Various models,\nincluding LLAMA, GPT, and BERT, among others, had promised outstanding\nperformance related to text classification and understanding based on large-scale\npre-training over big datasets to obtain advanced on a wide range of domains"}, {"title": "2 Related Work", "content": "Recent breakthroughs in text classification and fine-tuning of large language\nModels have enhanced NLP applications across domains. Transformer-based\nmodels such as BERT and its variants have achieved remarkable performance\non a variety of tasks such as false information detection, sentiment analysis and\nradical content classification [12, 14, 24, 25]. Fine-tuning even on small labeled\ndatasets improves performance and bidirectional context capture. Refinements\nlike ROBERTa's NSP task removal boost domain-specific results, achieving an\nF1 score of 0.8 in medication detection and 3.929 MAE in eye-tracking tasks\n[7, 17, 24].\nEfficient light-weight models like DistilBERT, which is 40% smaller and 60%\nfaster, retain 97% BERT's performance, excelling in tasks such as the clas-\nsification of socio-political news [6, 24]. ALBERT-Base-v2 optimized memory\nand speed, achieving a 13.04 exact match score in COVID-19 queries [24, 26].\nXLM-ROBERTa improved results in processing more than 100 languages us-\ning the advances in multilingual processing and outperforming prior models by\n23% in multilingual accuracy [5,21,22,24,27]. Special adaptations like Electra-\nSmall and BART-Large introduced token substitution and hybrid architectures.\nElectra-Small excelled in multilingual fake news detection with innovative to-\nken substitution strategies [10,15,24]. Both models achieved high performance\nin specialized tasks such as medical complaint detection and NER, with BART-\nLarge reducing voice recognition errors by 21.7% [19,23,24]. These advancements\nhighlight LLMs' potential in handling noisy and unstructured text data.\nGenerative LLMs enhance NLP, excelling in specialized tasks. Fine-tuned\nmodels like GPT-3.5 and Mistral-7B surpass baselines by 50% in F1-macro scores\non domain-specific datasets [20]. QLoRA improves memory efficiency [4], but\nmodel variability persists, as seen in GPT-NeoX-20B and Llama2-7B.\nPreprocessing techniques such as text refinement, summarization, and Named\nEntity Recognition (NER), are essential for handling noisy and unstructured\ntext. BERT and RoBERTa have performed extremely well in NER tasks, effec-\ntively extracting key entities from complex text [7, 13, 24]. Combining prepro-\ncessing with fine-tuning enhances classification accuracy, especially when raw\ndata lacks structure or clarity.\nBased on this, our study introduces a multi-layered framework that uses LLM\npreprocessing to improve the fine-tuning of Arabic language models and address\nnoise in data. This enhances the effectiveness of Arabic language models such\nas CAMEL-BERT, AraBERT, and Asafaya-BERT, in disease classification and\nseverity tasks. It also presents an effective strategy for incorporating LLMs into\nsocial telehealth applications."}, {"title": "3 Dataset Collection", "content": "The dataset used in this study was collected from user-generated posts on an\nonline social platform where patients shared their medical complaints in Arabic.\nThe posts contained detailed information such as the status of chronic diseases,\ndescriptions of symptoms, symptom durations, height, weight, gender, and age,\ncategorized into Type, Severity, and Diagnosis. Structuring and annotation were\ndone under the supervision of a medical advisor to ensure relevance and accuracy.\nFigures 2 and 3 show the dataset's distribution, with Figure 2 illustrating the\nvariety of medical issues like chronic diseases, skin conditions, and neurological\nsymptoms. Figure 3 highlights the severity levels, reflecting a balance between\nmild and severe cases, making these visualizations suitable for a multi-class,\nmulti-label classification task."}, {"title": "4 Methodology", "content": "This work proposes an Arabic language model fine-tuning using a multi-layered\nframework. The LLAMA3 model has been used in the enhanced preprocessing\nstep; the flow is illustrated as shown in Fig. 1."}, {"title": "4.1 Step 1: User-Generated Text Data Collection", "content": "The dataset contains Arabic text data from user-generated content on online\nhealth platforms. Texts include elaborate descriptions by the patients of their\nsymptoms, patient medical history, age, and gender. While doing preprocessing,\nit automatically removes private and sensitive information to ensure privacy."}, {"title": "4.2 Step 2: Multi-layer preprocessing using LLAMA3", "content": "The text is further enhanced using a multi-layered approach to LLAMA 3 filter-\ning. The process involved in preprocessing is as follows:\nText Refinement: LLAMA3 improves the text through deleting unmet\nrequirements such as irrelevant information, grammatical mistakes and the\nvague parts of the material while protecting the medical context.\nText Summarization: Llama 3 eliminates unnecessary aspects of long\nmedical posts and compresses them into summaries for easier understanding.\nNamed Entity Recognition (NER): Symptoms and conditions, together\nwith the drugs, are important medical entities and LLAMA 3 identifies and\nextracts them.\nAll these steps (refined text, summarized text, and NER-extracted entities)\nare used to augment the base data."}, {"title": "4.3 Step 3: Data Annotation", "content": "To create new data variants, the performance data is combined with the out-\ncomes of the preparation stages. These datasets are effective in multi-label clas-\nsification tasks:\nDiagnosis Classification: The goal is to identify the particular medical\ncondition.\nType Classification: This deals with the categorization aspect of the con-\ndition (e.g., chronic, acute).\nSeverity Classification: This category use more qualitative and subjective\nterminology to assess the the symptoms (e.g., mild, severe)."}, {"title": "4.4 Step 4: Arabic Language Models Fine-Tuning", "content": "Three pre-trained Arabic language models are fine-tuned using the improved\ndataset and they are as follow:\nCAMeL-Lab/bert-base-arabic-camelbert-mix\naubmindlab/bert-base-arabert\nasafaya/bert-base-arabic\nThe fine-tuning process includes:\nOptimizing: Dropout 5%, scaling by factor 8, rank 16 is some of the settings\nused along with LORA.\nTraining Details: The models have been fine-tuned for a batch size of 4\nwith 25 epochs. To handle class imbalance, both balanced and accuracy-\nweighted custom loss functions have been used."}, {"title": "4.5 Step 5: Evaluation", "content": "The fine-tuned Arabic language models are evaluated under four different pre-\nprocessing conditions:\n1. Normal Text Only\n2. Normal Text + Refined Output\n3. Normal Text + Summarized Output\n4. Normal Text + NER Output\nThe performance of models is measured concerning accuracy and balanced\naccuracy in both disease type classification and severity prediction."}, {"title": "5 Results and Discussion", "content": "Table 1 presents the results of test for Arabic language models that were pre-\nprocessed to enhance with LLAMA3. The preprocessing pipeline consisted of"}, {"title": "5.1 Preprocessing with LLAMA3 for Fine-Tuning Enhancement", "content": "5.2 Refined Text Evaluation"}, {"title": "5.3 NER-Enhanced Text Evaluation", "content": "5.4 Summarized Text Evaluation"}, {"title": "5.5 Comparison Across Approaches", "content": "Across all methods, the NER-enhanced approach yielded the most notable im-\nprovements with 83% Type accuracy and 69% Severity accuracy (76% AVG)\nusing fine-tuning. This implicates the use of NER as valuable in extracting im-\nportant medical information, therefore making the most effective preprocessing\nmethod for augmentation with fine-tuning."}, {"title": "6 Conclusion", "content": "This work contributes to fine-tuning the Arabic language models for the purpose\nof disease classification and symptom severity assessment in social telehealth\napplications. A two-step methodology involving multi-layered preprocessing for\ntext Refinement, Summarization, and Named Entity Recognition (NER) to ex-\ntract key words of medical posts. The enriched datasets were used to fine-tune\nthree notable Arabic language models: CAMeL-BERT, AraBERT, and Asafaya-\nBERT, fine-tuned with NER enhanced approaches, outperformed others with\n83% accuracy in disease classification and 69% in severity estimation. This model\nimproves performance and provides a new benchmark for its application in tele-\nhealth. Future studies can extend this framework to other languages."}]}