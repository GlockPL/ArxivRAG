{"title": "A HIERARCHICAL LANGUAGE MODEL FOR INTERPRETABLE GRAPH REASONING", "authors": ["Sambhav Khurana", "Xiner Li", "Shurui Gui", "Shuiwang Ji"], "abstract": "Large language models (LLMs) are being increasingly explored for graph tasks. Despite their remarkable success in text-based tasks, LLMs' capabilities in understanding explicit graph structures remain limited, particularly with large graphs. In this work, we introduce Hierarchical Language Model for Graph (HLM-G), which employs a two-block architecture to capture node-centric local information and interaction-centric global structure, effectively enhancing graph structure understanding abilities. The proposed scheme allows LLMs to address various graph queries with high efficacy, efficiency, and robustness, while reducing computational costs on large-scale graph tasks. Furthermore, we demonstrate the interpretability of our model using intrinsic attention weights and established explainers. Comprehensive evaluations across diverse graph reasoning and real-world tasks of node, link, and graph-levels highlight the superiority of our method, marking a significant advancement in the application of LLMs to graph understanding.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) (Vaswani et al., 2017; Devlin et al., 2018; Achiam et al., 2023; Chowdhery et al., 2023) have demonstrated impressive generative capabilities, revolutionizing multiple fields, including natural language processing (NLP), computer vision (Wang et al., 2024c; Parashar et al., 2024; Liu et al., 2024b), speech recognition (Fathullah et al., 2024), and cross-modal domains (Wu et al., 2023; Koh et al., 2024). Despite this widespread success, their application to graph tasks remains an emerging area of research (Chen et al., 2024c; Ren et al., 2024; Jin et al., 2023). Unlike linear text data, graph data presents unique challenges due to its non-Euclidean topologies and intricate structures (Jin et al., 2023), making it difficult for LLMs to process these complex relationships effectively. As a result, the adoption of LLMs in graph-centric tasks has been limited, with graph models such as Graph Neural Networks (GNNs) (Kipf & Welling, 2017; Gilmer et al., 2017) continuing to be the state-of-the-art in this domain.\nApplying LLMs to graph tasks presents two key challenges. Firstly, real-world graphs, such as molecules, often involve complex combinations of features and structures (Qin et al., 2023), such as atomic properties and the bonds between atoms. While LLMs excel at processing feature-based information due to their strong text comprehension abilities, they often falter with structural details (Hu et al., 2023). This limitation leads to their suboptimal performance even on simple graph tasks, such as identifying shortest paths (Guo et al., 2023; Wang et al., 2024a; Fatemi et al., 2023). Consequently, LLMs tend to perform well mainly on node-level tasks but struggle with more complex link and graph-level tasks, where understanding long-range structures is crucial (Liu et al., 2023; Wu et al., 2021). Secondly, representing graphs within LLMs poses significant scalability challenges (Zhao et al., 2023; Ye et al., 2023b). Encoding both feature and structural information for each graph node, as in molecular (Dwivedi et al., 2023), citation (Hu et al., 2020b), or knowledge graphs (Dettmers et al., 2018), often results in lengthy prompts. This dramatically increases computational complexity, as the attention mechanism in LLMs scales quadratically with input size, making the application of LLMs to large graph-based tasks computationally prohibitive and demanding specialized solutions.\nOn the other hand, a key advantage of using LLMs for graph tasks is their capacity to process graphs in a human-comprehensible manner, accepting input as straightforward text descriptions. With a human-readable vocabulary, LLMs offer a natural advantage in interpretability over the opaque embeddings utilized by GNNs (Binder et al., 2016; Longo et al., 2024; Achtibat et al., 2024). However, no prior work has focused on providing interpretable results that explain graph structures.\nTo address these challenges, we introduce Hierarchical Language Model for Graphs (HLM-G), a novel framework designed to enhance the graph structure comprehension capabilities of LLMs. Unlike conventional LLMs that apply self-attention across all tokens, HLM-G employs a two-block architecture, comprising a local block and a global block, each with specialized attention masking. This hierarchical structure enables the model to initially capture local information in the lower layers, followed by the integration of global-level information in the upper layers. Our approach not only enhances the model's understanding of graph structures but significantly reduces computational costs, making HLM-G more scalable for large-scale graph tasks. Furthermore, our hierarchical design exhibits increased robustness to variations in graph prompts. We also demonstrate the interpretability of our hierarchical language model with both model intrinsic weights and established explainers. Finally, we conduct comprehensive experiments across seven graph reasoning datasets and seven real-world datasets, encompassing citation networks, knowledge graphs, and molecular graphs. Our results validate HLM-G's ability to generalize effectively across node, link, and graph-level tasks, marking a significant advancement in the application of language models to graph-based tasks."}, {"title": "BACKGROUND AND RELATED WORK", "content": "Problem Setup. We denote a graph as $G = (A, X, E)$, where $A \\in R^{n \\times n}$, $X \\in R^{p \\times n}$, and $E \\in R^{I \\times m}$ represent the adjacency, node feature, and edge feature matrices, respectively. Here, $n, m, p,$ and $q$ denote the numbers of nodes, edges, node features, and edge features, respectively. Building on these, we describe graph tasks in natural language. For each graph $G_i$, we first construct a sequence $U_i$ that encapsulates the natural language descriptions of $G_i$ covering $A_i, X_i,$ and $E_i$, coupled with a query $Q_i$ describing the prediction task. Each task is also associated with a true label $Y_i \\in V$. This leads to a dataset of sequences $U = \\{(U_1, Q_1, y_1), (U_2, Q_2, Y_2), \\ldots, (U_N, Q_N, Y_N)\\}$, where each sequence $U_i = \\{u_1, u_2, \\ldots, u_{m_i} \\}$ and all tokens $u_i$ belong to a vocabulary $V$.\nLLM Inference Methods. Prompt engineering has been pivotal in adapting LLMs for a wide range of tasks (Sahoo et al., 2024a; Zhou et al., 2022). Early attempts in prompt engineering for graph tasks use structured representations like edge lists and adjacency matrices (Brandes et al., 2013; Zhao et al., 2023), but these methods struggle with graph structural reasoning tasks (Guo et al., 2023). NLGraph (Wang et al., 2024a) seeks to convert graph data into natural language prompts, yet fundamental graph operations remain challenging, even for small graphs. Studies suggest simpler prompts can be more effective, but overall improvements are still modest (Zhao et al., 2023; Fatemi et al., 2023; Sahoo et al., 2024b). Beyond prompt engineering, other approaches (Yao et al., 2024; Wang et al., 2022) involve exploring multiple reasoning paths and selecting the most confident one, offering marginal gains at the cost of increased inference time. LLMs continue to underperform compared to specialized graph models, indicating a significant gap (Hu et al., 2023). The limited success of LLMs on graphs has been partly attributed to their inability to construct coherent world models, often relying on pattern matching rather than genuine reasoning (Valmeekam et al., 2023; Stechly et al., 2024).\nLLM Fine-Tuning Methods. Fine-tuning and instruction tuning have been investigated to address LLMs' limitations in graph reasoning tasks. Fine-tuning on graph-specific datasets has achieved limited success, with models still struggling to capture complex graph structures (Tang et al., 2023; Vafa et al., 2024). Instruction tuning, which aligns training objectives with graph reasoning tasks, has shown more promise (Wang et al., 2024b; Luo et al., 2024) by introducing a variety of related tasks during training, enabling LLMs to gain a deeper understanding of the graph domain. However, this approach is labor-intensive and continues to face challenges with large and dense graphs. Methods such as GraphWiz (Chen et al., 2024a) have further incorporated RL preference alignment (Rafailov et al., 2024), demonstrating some improvements but still struggling with dense graph structures. Furthermore, incorporating real-world graph features, such as node and edge features in citation networks, into LLMs remains an open challenge, indicating that more work is needed to adapt LLMs for graph tasks."}, {"title": "HIERARCHICAL LANGUAGE MODEL DESIGN", "content": "In this section, we introduce our Hierarchical Language Model, designed to effectively capture both the structural and feature-based aspects of graphs. We begin by explaining how graph data can be transformed into natural language descriptions (Section 3.1). Following this, we describe the model's architecture, which is composed of a local block (Section 3.2) for learning local structural information, a pooling layer (Section 3.3) for integrating structural and feature information, and a global block (Section 3.4) for capturing global information. This hierarchical approach not only guides our model to better understand graph structures but also results in computational advantages.\n3.1 NATURAL LANGUAGE DESCRIPTIONS OF GRAPHS\nFollowing prior works (Guo et al., 2023; Fatemi et al., 2023) that demonstrate the effectiveness of using simpler graph inputs for LLMs, we define a graph-to-text representation U to describe any graph task in natural language. For a graph G characterized by its adjacency matrix A, node features X, and edge attributes E, we construct textual representations capturing both node feature and 1-hop structural information for each node $v_i$ in G. These representations are divided into two components: the node feature annotation $U_i^X$ and the node structure annotation $U_i^{AE}$.\nNode Feature Annotation. Each node can be effectively described in natural language and presented as input to an LLM. The node feature annotation for a node $v_i$, denoted as $U_i^X$, is a natural language sequence that describes the attributes $X_i$ of $v_i$ over a predefined vocabulary $V$. The template for $U_i^X$ is as follows:\n$U_i^X$: Node <i> features: <feature_1>: <content_1>; <feature_2>: <content_2>,\u2026, <feature_p>: <content_p>.\nNode Structure Annotation. The node structure annotation $U_i^{AE}$ captures the structural connections of node $v_i$ within the graph G, including its connections to other nodes and the corresponding edge features. This serves as a textual representation of A and E. Let $ne(i)_1, ne(i)_2, ..., ne(i)_k$ be the indices of $v_i$'s 1-hop neighbors in G. The template for $U_i^{AE}$ is:\n$U_i^{AE}$: Node <i> is connected to $<ne(i)_1>$ with <edge-feature1>, $<ne(i)_2>$ with <edge-feature2>,\u2026\u2026, and $<ne(i)_k>$ with <edge-featurek>.\n3.2 THE LOCAL BLOCK\nSince language models cannot inherently understand graphs in their natural structure, we introduce a local-to-global guidance approach, where the model first learns strong local features before capturing information at the global graph level. To implement this, we introduce a local block $M_L$ that employs an intra-node attention masking mechanism. This mechanism ensures that, for each node $v_i$, the combined text sequence $(U_i^{AE}, U_i^X)$ is processed independently of other nodes, allowing the model to effectively capture node-specific structures and features. Given an input token sequence $H^l \\in R^{n \\times d_k}$ at any transformer layer, where n is the total number of tokens across all nodes and $d_k$ is the embedding dimension, we decompose this sequence into segments: $H^l = \\{H_1, H_2, ..., H_N\\}$, with each segment $H_i \\in R^{n_i \\times d_k}$ representing the tokens associated with node $v_i$.\nThe attention mechanism in the local block is then formulated as:\n$Attention^{(l)}(Q, K, V) = Diag (Attention^{(l)}(Q_1, K_1, V_1), . . ., Attention^{(l)}(Q_N, K_N, V_N)$,\nwhere\n$Attention^{(l)}(Q_i, K_i, V_i) = Softmax(\\frac{(W_Q^{(l-1)}H_i^{(l-1)})(W_K^{(l-1)}H_i^{(l-1)})^T}{\\sqrt{d_k}})(W_V^{(l-1)}H_i^{(l-1)})$.\nThis block diagonal attention mechanism also provides several computational advantages. Let $n_X^i$ and $n_{AE}^i$ represent the number of tokens corresponding to the feature annotation $U_i^X$ and structure annotation $U_i^{AE}$ for node $v_i$, respectively. The total number of tokens n for the entire graph is given by $n = \\sum n_i$, where $n_i = n_X^i + n_{AE}^i$. By employing this block diagonal attention mechanism, we achieve significant computational efficiency compared to traditional full attention approaches. In standard attention, the computational complexity is typically $O((\\sum n_i)^2)$, which scales quadratically with the total number of nodes, becoming increasingly expensive for larger graphs. In contrast, our block diagonal design reduces the complexity to $O (\\sum n_i^2)$, resulting in a linear scaling relative to the number of nodes. This improvement substantially enhances efficiency, especially for larger graph-based tasks, making our approach highly scalable.\n3.3 POOLING LAYER\nTo integrate structural and feature-based information extracted from the graph, we introduce a pooling mechanism. For each node $v_i$, we first derive local embeddings from the hidden states produced by the local block $M_L$. Specifically, the feature-based embedding is obtained as $z_i^X = \\sum_{l_i=1}^{L_i} h_j$, where $h_j$ represents the hidden states corresponding to tokens from $U_i^X$. Similarly, the structure-based embedding $z_i^{AE}$ is obtained from $U_i^{AE}$ using the same approach. Next, we combine these embeddings through a parameterized pooling operation to produce the final embedding $z_i$ for each node. Formally, given a sample U, the pooled embedding is defined as:\n$z_i = \\alpha z_i^{AE} + (1 - \\alpha) z_i^X$,"}, {"title": "THE GLOBAL BLOCK", "content": "where $\\alpha \\in (0, 1)$ is a trainable parameter that balances the contribution of structural ($z_i^{AE}$) and feature ($z_i^X$) information. A larger $\\alpha$ emphasizes the structural properties in the final prediction, while a smaller $\\alpha$ gives more weight to feature-based characteristics.\nOur adaptive pooling mechanism allows our model to work for tasks requiring varying levels of structural and feature importance, such as link and graph-level tasks that demand greater structural emphasis and node-level tasks that rely more heavily on feature-based information. We ablate alternative pooling strategies and configurations, which are detailed in Appendix F.\n3.4 THE GLOBAL BLOCK\nTo capture global-level interactions across the entire graph, we introduce the global block $M_G$, which leverages a multi-layer transformer architecture to model comprehensive structural relationships. The global block operates on top of the local embeddings derived from $M_L$, learning the higher-level interactions between nodes and enriching the representation with more nuanced graph-level information. Each layer comprises an attention mechanism followed by a feedforward layer. For any layer l, the embeddings are updated as:\n$Z^{(l)} = Softmax(\\frac{(W_Q^l Z^{(l-1)}) (W_K^l Z^{(l-1)})^T}{\\sqrt{d_k}}) (W_V^l Z^{(l-1)}), Z^{(0)} = [z_{v_1},..., z_{v_n}, z_q]$,\nwhere $d_k$ is the dimensionality of the key vectors, and $W_Q^l, W_K^l, W_V^l$ are the weight matrices. The input $Z^{(0)}$ includes node embeddings $z_{v_1},..., z_{v_n}$ and the task-specific query embedding $z_q$ from $M_L$. After processing through L layers, the final embedding $z_q^{(L)}$ is passed through a multilayer perceptron (MLP) to generate the prediction:\n$\\hat{y} = argmax MLP(z_q^{(L)})$, where $z_q^{(L)} = Z_{:,n+1}^{(L)}$,\nwith $\\hat{y}$ representing the predicted class label. The training objective is to minimize cross-entropy loss:\n$\\lbrace \\theta_L, \\theta_G, \\psi \\rbrace^* = argmin \\mathbb{E}_{(U,y) \\sim U} [l(y; MLP(M_G(M_L(U))))]$,\nwhere y is the ground truth label, and $\\theta_L, \\theta_G, \\psi$ represent the trainable parameters of $M_L$, $M_G$, and the MLP, respectively."}, {"title": "EXPERIMENTS", "content": "In this section, we conduct experiments to investigate four specific research questions (RQs) to assess the effectiveness of our model on graph tasks: RQ1: Can our model accurately understand the underlying structures and maintain robust performance across different graph reasoning datasets? RQ2: Does our approach enhance interpretability performance and produce intrinsic interpretable results? RQ3: Can the proposed method handle complex real-world datasets with diverse node or edge features? RQ4: Does the proposed method work well across all node, link and graph level tasks?\n4.1 STRUCTURE UNDERSTANDING CAPABILITIES OVER GRAPH REASONING DATASETS\nTo answer RQ1, we aim to validate whether our model can process graph structure information by conducting the following experiments on graph reasoning datasets.\nDatasets. First, following Wang et al. (2024a), we create a synthetic dataset consisting of seven graph reasoning tasks to assess the structural reasoning capabilities of our model. These datasets were constructed by a Random Graph Generator capable of generating graphs with up to 40 nodes and 700+ edges. Further information on these datasets is provided in Appendix C.1.1.\nBaselines. We compare our method against both GNN-based and LLM-based approaches. On the GNN side, our comparisons include models such as GCN (Kipf & Welling, 2017), GAT (Veli\u010dkovi\u0107 et al., 2017), and the more expressive GIN (Xu et al., 2018), as well as the graph transformer model, GTN (Yun et al., 2019). For LLMs, our inference-only methods include Zero-Shot (Huang et al., 2023), Chain of Thought (CoT) (Wei et al., 2023), CoT Self Consistency (CoT-SC) (Wang et al., 2022), and Natural Language Graph (NLGraph) (Wang et al., 2024a) prompting. Additionally, fine-tuning baselines such as BERT (Devlin et al., 2019) and Lora-Trained (Hu et al., 2021) Llama 3 are used for direct comparisons. We include GraphWiz (Chen et al., 2024a) as a representative of instruction tuning. The GraphToken (Perozzi et al., 2024) method, which utilizes a GNN encoder to fine-tune a frozen LLM, is also compared. Detailed information on our experimental configurations and hyperparameters can be found in Appendix C.2. For LLMs, we use Llama-3 8B (Dubey et al., 2024) as the primary backbone.\n4.1.1 QUANTITATIVE COMPARISONS.\nOur method demonstrates state-of-the-art performance across all graph reasoning datasets, significantly outperforming all baselines, both GNN and LLM-based models. Notably, GNNs such as GIN, despite their theoretically strong expressiveness as validated by the WL-1 test (Huang & Villar, 2021), struggle with graph-level tasks, failing to match the comprehensive understanding offered by our model. Prompt engineering approaches, like CoT and NLGraph and exploration based ap-"}, {"title": "EVALUATION OF MODEL ROBUSTNESS.", "content": "A critical question emerges from the quantitative comparisons: Do language models truly understand graph structures, or do they rely on pattern-matching? To investigate this, we conducted a robustness evaluation by systematically shuffling the node indices of each graph using a permutation matrix P. Unlike GNNs, which are inherently invariant to changes in node indexing due to their symmetrical message-passing framework, LLMs may exhibit sensitivity to even slight alterations in node token representations, potentially leading to inconsistent predictions for the same graph described differently. This issue highlights a significant shortcoming of LLMs in graph-based tasks.\nIn this experiment, we applied the permutation matrix P 10 times to each graph, generating modified adjacency matrices $A_t = P A_{t-1} P^T$ at each iteration t. This process preserves the overall graph structure while changing the node indices, allowing us to evaluate whether the model's predictions remain consistent under different representations.\nThe results, presented in Table 2, highlight a stark difference between traditional LLM-based models and our proposed HLM-G model. NLGraph's performance dropped significantly, indicating that prompt engineering is not robust. Similarly, we observed that fine-tuned LLMs, such as Llama 3 and BERT, exhibited performance drops of up to 21% and 71%, respectively, on the Node Degree task. This highlights their high sensitivity to changes in node tokens and suggests a reliance on pattern recognition rather than a true comprehension of the underlying graph structure. Instruction tuning does not seem to provide robustness as Graphwiz also shows similar sensitivity as finetuned Llama 3. In contrast, our HLM-G model displays exceptional robustness, with minimal performance drops (e.g., a mere 6.1% drop on the Shortest Distance task and 0.0% on the Node Degree task). These findings underscore a crucial advantage of our HLM-G, while conventional LLMs struggle with variations in graph representation, our model remains robust, reinforcing its suitability for real-world graph tasks where representations might vary but the underlying structure remains unchanged."}, {"title": "INTERPRETABILITY COMPARISONS", "content": "Having established the performance and robustness of our model, we now delve into analyzing its interpretability-specifically, its ability to accurately identify and prioritize the most critical structural elements within graph reasoning tasks, thus addressing RQ2. Interpretability serves as a vital criterion in evaluating whether a model is capable of comprehending graph structures rather than just fitting patterns. For this purpose, we utilize four graph reasoning datasets that offer explicit ground truths regarding which nodes are genuinely important for a given graph task. For example, in the shortest distance task, the ground truth consists of nodes that lie along the shortest path between two specified nodes. More details about these ground truths are provided in Appendix E.1. We compare the true structure understanding capabilities of four finetuned models from Table 1: Llama-3, BERT, GIN and HLM-G.\nTo measure interpretability, each model is expected to generate an ordered set $r = \\{r_1,...,r_n\\}$ for a graph with n nodes, ranking them from the most to the least significant, based on the model's internal focus and reasoning. The quality of a model's interpretation is then evaluated by how effectively it identifies the nodes that align with the ground truths. Ideally, a model with true structural comprehension should consistently rank ground truth nodes higher, indicating that it genuinely understands the critical elements of the graph structure. Using established explainers, we first reveal the extent to which our approach successfully captures and prioritizes the essential graph components. We then introduce the intrinsic interpretability mechanism built into our model, demonstrating its ability to provide ready made interpretations.\n4.2.1 EXPLAINER-BASED INTERPRETATION\nTo objectively compare the interpretability performance across different models, we leverage established explainability techniques such as Saliency (Simonyan et al., 2013), Input x Gradient (Shrikumar et al., 2016), DeepLIFT (Shrikumar et al., 2017), and GNNExplainer (Ying et al., 2019). This approach allows us to assess how well each model can identify and rank important graph elements, providing insight into the structural modeling capabilities of these models. More details on this strategy, referred to as \u201cexplanations as interpretations\u201d, are outlined in Appendix E.2.\nSetup. To quantify interpretability, we use a Recall@k metric, which measures how effectively a model identifies the nodes that correspond to ground truths. Given a set of ground truth nodes $r_{gt}$ and the set of top-k nodes identified by the model $r_k = \\{r_1,...,r_k\\}$, we calculate Recall@k as\n$Recall(k) = \\frac{|r_k \\cap r_{gt}|}{|r_{gt}|}$, where | . | represents the cardinality of the set and \u2229 denotes the intersection.\nAs shown in Figure 2, we evaluate each model's performance by plotting the recall curve for $k \\in \\{1,2,..., n\\}$, where n represents the total number of nodes in the graph. Ideally, a model with a strong understanding of graph structure will have high recall values across different values of k, indicating that it consistently identifies the most important nodes.\nResults and Analysis. Figure 2 presents the interpretability results for the four models across the four graph reasoning datasets. Our HLM-G model demonstrates superior interpretability, particularly as k increases, indicating a higher proficiency in pinpointing the most relevant nodes for each task. While GIN performs adequately on tasks requiring simpler one-hop reasoning, such as Edge Existence and Node Degree, it struggles with more complex, multi-hop reasoning tasks like Shortest Distance and Reachability. In contrast, BERT and LLaMA consistently fail to identify relevant structural features, reflecting their limited capability to capture intricate graph patterns. Directly fine-tuning LLMs has not led to significant improvements in these cases. Although Llama 3 outperforms BERT on three out of four tasks, it still does not reach the performance level of GIN or our model. Our model, in fact, excels across all tasks, even those involving multi-hop reasoning, which further confirms its strong understanding of graph structures beyond simple pattern matching."}, {"title": "INTRINSIC ATTENTION INTERPRETATION", "content": "A key strength of our model design is its inherent interpretability, distinguishing it from existing methods. The local embedding matrix $Z^{(0)}$ in the local block captures 1-hop subgraph information, where each $z_{v_i} \\in Z^{(0)}$ represents the 1-hop ego-graph centered around node $v_i$. As the transformer layers progress in the global block, they progressively integrate this localized information to capture broader global structures within the graph. This means that embeddings in the higher layers reflect increasingly comprehensive structural details. The attention weights associated with the task query node in the global block provide a direct interpretation of the contribution of each node's structural information to the final prediction, effectively acting as importance scores for each node. This allows for a direct, interpretable insight into how the model makes its decisions.\nTo illustrate this, we analyze the mean attention scores across all layers, as shown in Figure 3. As we move to higher layers, the attention scores for ground truth nodes increase, while scores for other nodes decrease. This pattern directly confirms that our model effectively focuses on the most important nodes, demonstrating its ability to capture larger-scale structural information. These attention-based interpretations offer clear insights into the model's decision-making process without requiring additional explanation techniques."}, {"title": "GRAPH LEARNING ABILITY ON REAL-WORLD DATASETS.", "content": "Datasets. To answer the RQ3 and RQ4, we curated seven graph datasets widely recognized in the graph learning community, varying in scale, domains, and task types. We adopt Arxiv (Hu et al., 2020b), Cora (Bojchevski & G\u00fcnnemann, 2018), and Pubmed (Sen et al., 2008) for node-level tasks; Pubmed, WN18RR (Bordes et al., 2013), and FB15k-237 (Bordes et al., 2013) for link-level tasks; and molhiv (Hu et al., 2020a) for graph-level tasks. More dataset details are discussed in Table 7.\nBaselines. We compare with traditional GNNs including GCN (Kipf & Welling, 2017), GAT (Veli\u010dkovi\u0107 et al., 2017), GIN (Xu et al., 2018) and GraphSage (Hamilton et al., 2017). For graph transformer-based baseline we include GTN (Yun et al., 2019) and Graphormer (Ying et al., 2021). For LLMs, we compare Zero-shot and Few-shot performance using GPT 3.5 (Ye et al., 2023a) for node-level tasks, and Llama-2-7B finetuned InstructGLM (Ye et al., 2023b) for both node and link-level tasks. For the graph-level task, we compare with a GNN-LLM hybrid model Momu (Su et al., 2022) for molecular graphs. Note that we use Mamba (Gu & Dao, 2023) as a baseline for graph-level task as no Transformer-based LLM is computationally feasible for training on real-world graph-level tasks. OFA (Liu et al., 2024a), a hybrid GNN-LLM model, is also selected as a baseline due to its strong performance on link-level tasks.\nQuantitative Results. As demonstrated in Tables 3, 4, and 5, our method consistently delivers competitive performance across node, link, and graph-level tasks. Compared to traditional GNNs, our model surpasses their performance for both node and link-level tasks with large margins. In comparison to hybrid GNN-LLM methods, our model notably outperforms the recently developed LLM-equipped OFA, on link-level tasks where OFA is considered especially strong. Furthermore, our model consistently perform favorably against LLM instruction tuning approach - InstructGLM across link level tasks. Although graph transformers perform slightly better in the graph-level task because of their specialized encodings for graph-level tasks, our model produces much higher performance"}, {"title": "CONCLUSIONS AND DISCUSSIONS", "content": "In this paper, we introduce a novel Hierarchical Language Model to tackle the complexities of non-Euclidean structures commonly found in graphs. While language models excel in text-centric applications, they often struggle with the intricate structures of graph data, leading to significant performance and computational challenges. Additionally, the context length, which involves the natural language description of a graph, can become enormous for real-world datasets, rendering them ineffective for graphs. Our method sets itself apart by designing a hierarchical architecture to process the graph structure and enhance computational efficiency and interpretability. We show that our model yields promising results in graph reasoning tasks as well as robust and consistent performance on real-world datasets, outperforming most models designed for similar purposes.\nThis work paves the way for future research in language models for graph learning, establishing a solid foundation for innovation and providing valuable insights into this emerging field. Our findings significantly narrow the gap between conventional language models and graph tasks, expanding potential applications and improving the effectiveness of language models in handling structured data. We hope this work can shed light on the future direction of LLM-based graph learning."}, {"title": "BROADER IMPACTS", "content": "Our research aims to enhance the understanding of graph structures through language models (LMs), marking a modest but significant step toward improved graph reasoning capabilities. This foundational effort seeks to refine how LMs interpret complex graph data, aspiring to inspire further research in this domain. Given the exploratory nature of our work, we have not identified specific negative societal impacts or potential for malicious use directly attributable to our research. Nevertheless, we recognize that all technological advancements carry inherent risks.\nIn alignment with responsible research practices, we suggest continuous monitoring of developments in the application of LMs to graph data analysis. As these models evolve to handle more complex tasks, maintaining vigilance becomes crucial to preemptively address any emerging risks before they manifest. Our commitment to ethical conduct underpins our research methodology, which is designed to avoid harm and does not involve human subjects, thus mitigating potential ethical concerns related to privacy and fairness. By promoting ongoing assessment and adopting a proactive approach to research governance, we aim to ensure that our contributions positively impact the field and adhere to the highest standards of ethical research."}, {"title": "FURTHER RELATED WORKS", "content": "Graph Neural Networks (GNNs). Graph Neural Networks (GNNs) have emerged as a powerful framework for learning over graph-structured data (Kipf & Welling, 2017; Gilmer et al., 2017; Veli\u010dkovi\u0107 et al., 2018; Wu et al., 2020; Liu et al., 2020). GNNs operate by iteratively aggregating information from a node's neighbors, thereby learning node representations that capture the local structure and features of the graph. This message-passing mechanism enables GNNs to be highly effective in tasks such as node classification, link prediction, and graph classification. However, despite their success, GNNs are often challenged by issues such as over-smoothing in deeper networks (Rusch et al., 2023) and difficulties in handling long-range dependencies (Sanford et al., 2024), which can limit their effectiveness on larger and more complex graphs.\nGraph Transformer (GT). Graph Transformers (GTs) (Yun et al., 2019; Ramp\u00e1\u0161ek et al., 2022) represent a more recent approach that aims to capture global dependencies within graph data using self-attention mechanisms. Inspired by the success of transformers in NLP tasks, GTs adapt the self-attention mechanism to graph-structured data, allowing them to capture both local and global interactions simultaneously. This approach helps address some of the limitations of GNNs in learning long-range dependencies. However, Graph Transformers often require additional architectural complexities (Black et al., 2024), such as centrality encoding, edge features, and spatial encodings, to effectively represent graph structures. These added complexities can lead to increased computational demands and make them less interpretable compared to conventional GNNs.\nTransformer Block in Language Models. In a transformer model, each block processes an input sequence $H_i = \\{h_1, h_2,...,h_{n_i}\\}$ to output an updated sequence $H_{i+1}$. A transformer block is structured around an attention mechanism and a feedforward network, both supplemented by residual connections and layer normalization. The multi-head attention mechanism processes the sequence $H_i$, formulated as $Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}})V$ where Q, K, V are queries, keys, and values derived from"}]}