{"title": "Learning to Substitute Words with Model-based Score Ranking", "authors": ["Hongye Liu", "Ricardo Henao"], "abstract": "Smart word substitution aims to enhance sentence quality by improving word choices; however current benchmarks rely on human-labeled data. Since word choices are inherently subjective, ground-truth word substitutions generated by a small group of annotators are often incomplete and likely not generalizable. To circumvent this issue, we instead employ a model-based score (BARTScore) to quantify sentence quality, thus forgoing the need for human annotations. Specifically, we use this score to define a distribution for each word substitution, allowing one to test whether a substitution is statistically superior relative to others. In addition, we propose a loss function that directly optimizes the alignment between model predictions and sentence scores, while also enhancing the overall quality score of a substitution. Crucially, model learning no longer requires human labels, thus avoiding the cost of annotation while maintaining the quality of the text modified with substitutions. Experimental results show that the proposed approach outperforms both masked language models (BERT, BART) and large language models (GPT-4, LLaMA). The source code is available at https://github.com/Hyfred/Substitute-Words-with-Ranking.", "sections": [{"title": "1 Introduction", "content": "In the current era of AI-driven technologies, the generation of natural language by machines has become a critical area of research within the field of natural language processing (NLP). The advent of advanced language models such as GPT (Ouyang et al., 2022) and LLaMA (Touvron et al., 2023a,b; Dubey et al., 2024) has brought about unprecedented improvements in generating coherent, contextually accurate text across a wide range of applications, including machine translation, summarization, and conversational agents. These models are revolutionizing industries by enabling machines to produce human-like text at scale, greatly expanding the possibilities for automation and interaction in both professional and creative domains.\nAs language generation capabilities improve, the need to refine and control the output of these models becomes more pressing. One key area in this direction is smart word suggestion (SWS) defined by Wang et al. (2023) as a task that focuses on enhancing writing by improving two aspects of NLP: identifying words (or tokens more generally) that if replaced can improve sentence quality, and suggesting alternatives for such words. Complementarily, Wang et al. (2023) introduced a dataset to benchmark models built for SWS. This task is essential in applications such as paraphrasing, machine translation, and writing assistance, where accurate word choice directly impacts the clarity and quality of the generated text. Despite the progress in full-text generation, word substitution presents unique challenges in maintaining grammatical correctness, contextual appropriateness, and semantic fidelity.\nPrevious SWS work can be divided into task-specific models and prompt-based methods. Task-specific models are trained directly to address the SWS task. One approach adapts models for lexical substitution (LS) (McCarthy and Navigli, 2007; Kremer et al., 2014), allowing them to generate substitutions for each word in a sentence. If the substitution differs from the original word, it is recognized and recorded as such. BERT-based (Zhou et al., 2019) and the LexSubCon (Michalopoulos et al., 2021) models were first explored in Wang et al. (2023). They also fine-tuned pretrained models like BERT (Devlin, 2018) and BART (Lewis, 2019), allowing them to handle SWS end-to-end. Importantly, these models were fine-tuned using supervised learning to provide suggestions for Wikipedia text using ground-truth substitutions obtained from a list of synonyms gathered from a thesaurus. Orthogonal to these machine learning techniques, rule-based methods that rely on paraphras-"}, {"title": "2 Related Work", "content": "Enhancing word usage is a key feature of writing assistance. The SWS task in Wang et al. (2023) addresses it by emulating a real-world end-to-end writing scenario. Unlike traditional LS tasks (McCarthy and Navigli, 2007; Kremer et al., 2014), SWS requires systems to identify words that, if replaced, can improve sentence quality and suggest alternatives for such words. In the LS task, however, the word to be replaced is predefined and the model is only tasked with finding suitable substitutions without altering the overall meaning of the sentence. Further, in LS substitutions are typically lemmatized to match ground-truth labels, but in SWS they must be in the correct grammatical form.\nZhou et al. (2019) proposed a LS method using contextual word embeddings, modifying BERT with a dropout embedding policy to partially mask target words and introduced a validation score based on the top four layers of BERT for candidate evaluation. Michalopoulos et al. (2021) integrated external knowledge from WordNet into BERT for LS, combining scores from BERT, WordNet gloss similarity, and sentence embeddings, along with the validation score from Zhou et al. (2019), to generate substitutes. ParaLS (Qiang et al., 2023) generated substitutes through a paraphrasing model, using a heuristics-based decoding strategy.\nTo adapt the substitution capabilities from LS to the SWS task, Wang et al. (2023) used an approach that allowed models to generate substitutions for each word. If the substitution differs from the original word, it is recognized as the word the model wants to improve and recorded as a suggestion. Further, they fine-tuned pretrained models like BERT (Devlin, 2018) and BART (Lewis, 2019) to address"}, {"title": "3 Methodology", "content": "In SWS, achieving a representative sample of high-quality alternatives requires a broad pool of annotators, as diverse substitution options are essential for accurate evaluation. However, relying solely on human annotators is thus not only costly but also inefficient. Below we introduce a method to train and evaluate token substitution models that does not require human annotations, by instead solely relying on model-based scoring functions.\nProblem Definition Let $X$ denote a sentence composed of $N$ tokens, i.e., $X = (x_1,...,x_N)$. We aim to find a collection of $K$ potential substitutes {$\\tilde{w}_k$}$_{k=1}$ for token $x_n$, while estimating their likelihood given the sentence (i.e., its context) as $P_{\\theta}(x_n = \\tilde{w}_k|X)$, where $X$ is the original sentence and $p_{\\theta}(\\cdot)$ is a model for the conditional likelihood for token $x_n$ parameterized by $\\theta$. Note that technically, if $p_{\\theta}(\\cdot)$ is a masked language model we should write $p_{\\theta}(x_n = \\tilde{w}_k|X\\setminus x_n)$, where $X\\setminus x_n$ indicates that $x_n$ has been masked out in $X$, and alternatively, if we use an (autoregressive) language model we should write $p_{\\theta}(x_n = \\tilde{w}_k|X_{<n}, Z)$, where $X_{<n}$ is the portion of the original sentence up to token $n - 1$ and $Z$ is a prompt. However, we use $p_{\\theta}(x_n = \\tilde{w}_k|X)$ in the following for notational simplicity. Based on these likelihood estimates, the token substitution rule is set as $x_n \\leftarrow \\tilde{w}_k$ if $P_{\\theta}(x_n = \\tilde{w}_k|X) > p_{\\theta}(x_n = w_n|X)$, where $w_n$ is the original value of $x_n$. Moreover, if multiple replacement candidates satisfy the substitution rule, we select $\\tilde{w}_k$ according to $\\underset{k}{\\text{argmax}} P_{\\theta}(x_n = \\tilde{w}_k|X)$, and alternatively, if none satisfy it, we leave the token unchanged, i.e., $x_n \\leftarrow w_n$.\nImportantly, $p_{\\theta}(x_n = \\tilde{w}_k|X)$ reflects the likelihood of token $\\tilde{w}_k$ given its context $X$, rather than the quality of the sentence. So motivated, we also seek to align such estimates with a score $M(x_n = \\tilde{w}_k|X)$ quantifying the quality of sentence $X$ with $x_n$ substituted with $\\tilde{w}_k$. For simplicity, in the following we denote the sentence $X$ with $x_n$ replaced with the k-th candidate, $\\tilde{w}_k$, as $X_k = (x_1,...,\\tilde{x}_N)$. Further, to make our objective scalable and general, the score $M(x_n = \\tilde{w}_k|X)$ ought not be obtained via human feedback, but via a black-box model with which we can score sentences, but through which we cannot learn, i.e., gradients cannot be propagated through it to obtain learning signals.\nBelow we start by showing how we can use a model-based score to statistically characterize the suitability of a candidate as an alternative to the substitution rule introduced above, and then present an approach to estimate $P(x_n = \\tilde{w}_k|X)$ while accounting for scores $M(x_n = \\tilde{w}_k|X)$."}, {"title": "3.1 Statistic for Model-based Scores", "content": "Model-based Score Using automated scoring methods to quantify text generation quality without relying on human annotators can reduce costs associated with human labeling. Model-based scoring approaches such as BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019), GPTScore (Fu et al., 2023), and BARTScore (Yuan et al., 2021), have proven to be effective while closely aligning with human evaluation results. BARTScore is one of the most popular metrics for the evaluation of text generation. It has been shown that BARTScore outperforms other metrics such as BERTScore and BLEURT (Yuan et al., 2021), while also being more efficient than GPTScore. Further, it has been used also as a ranking tool for substitution tasks,"}, {"title": "3.2 Preference-Aware Learning", "content": "Now that we have constructed a statistic based on model-based scores, in principle, we seek to train a model such that for each token $x_n$, the resulting $p_{X_1}$ from (2) for a given substitute candidate $w_1$ producing $X_k$ is as small as possible. According to (2), it is desirable for $M(X_1)$ to be larger than $M(X_k)$ for {$w_k$}$_{k=2}^{K_s}$ candidates sampled from model $p_{\\theta}(x_n|X)$. This means that effectively we require the model being such that its outputs align, in likelihood, with the model-based score, BARTScore here. Consequently, it is desirable to learn parameters $\\theta$ so $p_{\\theta}(x_n = w_k|X)$ is ranked consistent to $M(X_k)$, which in turn will make $p_{X_1} \\rightarrow 0$ in (2) for $k \\rightarrow 1$. Below we consider two approaches to accomplish this: margin ranking (Liu et al., 2023b; Chern et al., 2023; Liu et al., 2022) and direct preference optimization (DPO) loss (Rafailov et al., 2024)."}, {"title": "Ranking Optimization", "content": "Ranking has been used as an optimization objective in many tasks including text summarization (Chern et al., 2023; Liu et al., 2022). Maximum likelihood estimation based on the standard cross-entropy loss can be effective at satisfying (2) by setting it as a classification problem by defining a vector of labels $y$ where $y_k = 1$ if $M(X_k)$ is maximum among {$M(X_k)$}$_{k=1}^K$ or $y_k = 0$ otherwise. However, it does not take into account the ordering of the model-based scores or their magnitude differences.\nSo motivated, we consider the following margin ranking (MR) loss (Liu et al., 2023b; Chern et al., 2023; Liu et al., 2022)\n$L_{MR} = \\sum_{J_{k=1}^K} max(0, s_j - s_k + \\Delta_{jk})$, (3)\nwhere {$s_k$}$_{k=1}^K$ are the logits from model $p_{\\theta}(x_n = w_k|X)$, i.e., before applying the softmax function, and we have sorted them such that $s_k > s_j$ if $M(X_k) > M(X_j)$ for $i, j = 1, ..., K$. Further, we set the margin $\\Delta_{jk} = \\lambda \\times (j - k)$ for some hyperparameter $\\lambda$, which in the experiments is set via cross-validation. Intuitively, (3) encourages the model to make predictions whose outputs {$p_{\\theta}(x_n = w_k|X \\setminus x_n)$}$_{k=1}^K$ are consistent in order with {$M(X_k)$}$_{k=1}^K$, by penalizing pairwise order mismatches, while also enforcing predictions to be distanced by a fixed margin to improve robustness. The latter is justified by extensive results from the margin learning literature (Smola, 2000)."}, {"title": "Improving Model-based Scores", "content": "One unintended consequence of the loss function in (3) is that though it encourages model predictions to be aligned with model-based scores, it does so regardless of their values. This is so because only the order of {$M(X_k)$}$_{k=1}^K$ is considered in (3). In practice, we have observed that (3) effectively improves the ranking of predictions from the model, but it does so at the expense of producing predictions that have, on average, lower model-based scores relative to a reference model, e.g., the pre-trained model used as initialization for the refinement of {$P_{\\theta}(x_n = w_k|X)$}$_{k=1}^K$. To address this issue, we consider two approaches, one that seeks to improve the weighted average of model-based scores and another that maximizes the model-based score of the top prediction from the model relative to that of the reference model. Specifically, we write\n$L_{AS} = -\\sum_{k}h(s_k)M(\\tilde{X}_k)$, (4)\n$L_{BS} = max(0, (M(X) - M(X_1)) \\cdot f (s_1))$, (5)\nwhere $h(\\cdot)$ is the softmax function, $s_k$ is the logit corresponding to $p_{\\theta}(x_n = w_k|X)$, and $M(X)$ and $M(X_k)$ are the model-based scores of the original and modified sentence, respectively. Recall that for $M(X_1)$, the token of interest $x_n$ in $X$ has been modified to the top prediction $w_1$ from the model. Conceptually, $L_{AS}$ in (4) seeks to maximize the (weighted) average of model-based scores from K predictions, while $L_{BS}$ aims to improve the model-based score of the top prediction with respect to that of a reference model.\nWe then combine the margin ranking loss in (3) with the score-improving losses in (4) or (5) as\n$L_{MR+AS} = L_{MR} + \\gamma L_{AS}$, (6)\n$L_{MR+BS} = L_{MR} + \\gamma L_{BS}$, (7)\nwhere $\\gamma$ is a hyperparameter trading off ranking or model-based score improvement. In the experiments we will compare these two approaches to determine empirically whether is better to attempt to improve the scores of K predictions from the model as opposed to improving the score of one of the predictions from the model (the most likely) relative to a baseline prediction obtained from a reference model. Note that is tempting to use (5) with all predictions, not just the first one, however, we found empirically that it considerably increases the computational cost without significant performance gains. The cost overhead is caused mainly by the need to evaluate the model-based score K times rather than just 2 in (5)."}, {"title": "Direct Preference Optimization", "content": "Direct Preference Optimization (DPO) (Rafailov et al., 2024) is an efficient technique for aligning large language models with human feedback, which gained popularity due to its simplicity (Miao et al., 2024). For instance, it has demonstrated to be effective in chat benchmarks (Tunstall et al., 2023; Zheng et al., 2023). In our case, the model-based score serves as proxy for human feedback DPO (under the Plackett-Luce model) and is formally expressed as\n$L_{DPO} = -E \\left[ log \\frac{exp(\\beta r_k)}{\\sum_{k=1}^{K} exp(\\beta r_j)} \\right]$, (8)\nwhere $r_k = log p_{\\theta}(X_k) - log p_{\\hat{\\theta}}(X_k)$, the expectation is over {$X_1, ..., X_K, X$}, and $\\theta$ and $\\hat{\\theta}$ denote the parameters of the model being trained and that used for reference, respectively. Accordingly, only $\\theta$ are updated while learning while $\\hat{\\theta}$ are kept fixed."}, {"title": "4 Experiments", "content": "Below we illustrate the problem with evaluating with human annotations, then we present an ablation study comparing the optimization approaches in Section 3.2 and a benchmark comparing the proposed model to MLM and LLM approaches in terms of model-based score alignment, average score and the statistic in (2). Further, we explore the performance of top-2 predictions, i.e., when the top prediction is the original token, and results comparing LLMs with and without prompts encouraging token substitutes to be ranked by quality.\nDatasets We consider four datasets. The SWS dataset by Wang et al. (2023) consists of three sets, validation and test labeled by human annotators, and an artificially generated training set. The training dataset was constructed from Wikipedia sentences, with labels obtained using a combination of PPDB (Pavlick et al., 2015) and the Merriam-Webster thesaurus. We also consider two traditional lexical substitution datasets, LS07 (McCarthy and Navigli, 2007) and LS14 (Kremer et al., 2014), which use lemmatized human annotations as ground truth substitutions. Further, we also consider a general dataset without ground-truth substitutions. Specifically, we use XSum (Narayan et al., 2018), a summarization dataset consisting of BBC news articles and corresponding summaries. In the Appendix we show Table 6 summarizing these four datasets.\nBaselines We compare our method with three classes of baseline models. Masked language models (MLMs): BERT-base-uncased with original MLM head (BERT-naive) (Devlin, 2018). BERT-spsv (Zhou et al., 2019), as a representative LS model. BERT-SWS and BART-SWS, both of which were fine-tuned on the SWS training dataset (Wang et al., 2023). Rule-based models: The rule-based approach introduced by Wang et al. (2023), which leverages a thesaurus. Prompt-based large language models (LLMs): We leverage the power of pretrained LLMs via prompts to generate token substitutions. We consider two popular choices, GPT (Ouyang et al., 2022) and LLaMA (Touvron et al., 2023a,b; Dubey et al., 2024), specifically, GPT-40 and LLaMA-3.1-8B-Instruct. Prompt-based techniques have gained popularity due to their flexibility and effectiveness in generating high-quality, contextually appropriate substitutions without requiring task-specific tuning (Liu et al., 2023a). We designed both ranking and non-ranking prompts, which we show in Appendix B.\nEvaluation Metrics We use the cosine similarity (CS) to measure the correlation between model predictions and model-based scores, i.e., BARTScore. Since BARTScore produces log-likelihoods, we log-transform model predictions accordingly. We also consider the quality of the substitutes {$w_k$}$_{k=1}^K$. First we get the score ratio between the sentence with and without substitution and then calculate the average of BARTScore ratios as ABR = 1/K$\\sum_{k=1}^K \\frac{M(X_k)}{M(X)}$.\nImplementation For our model, we fine-tuned the BERT-base-uncased model (Wang et al., 2023) on 100k randomly sampled sentences from the SWS training dataset using one Nvidia A100 GPU. For each sentence, we randomly selected 5 tokens, and for each token, we chose K = 5 candidates from the model's output to form a candidate pool and compute the training loss. The model was fine-tuned over 5 epochs. For DPO, we duplicated the same BERT model and weights at initialization, freezing one as the reference while only updating"}, {"title": "Illustrative Example", "content": "We stratify tokens in the SWS test set into five groups according to whether they were substituted by human annotators and/or the model. These groups are: change agreement (CA: both model and the human annotator agreed on the change), change disagreement (CD: both model and the annotator suggested a change, but the model substitution did not match that of the annotator), no change agreement (NCA: the model and the annotator kept the token unchanged), only model changed (OMC: the model suggested a change, but the annotator did not find it necessary), and only annotator changed (OAC: the annotator suggested a change, but the model did not)."}, {"title": "Ablation Study", "content": "Section 3.2 introduced two approaches to align the predictions of a trained model with model-based scores, namely MR in (3) and DPO in (8), (9) and (10). For the former, we consider two ways of improving the model-based scores, via a weighted average in (6) and improvement relative to a reference model in (7). Results in Table 2 show results comparing i) the alignment of model predictions with model-based scores using median CS, and ii) the average model-based scores using median ABR. Distributions of CS and ABR for all tokens are shown in the Appendix Figures 1 and 2. We see that i) the MR loss alone is not sufficient to deliver the best CS and produces the worst ABR; ii) DPO variants improve the ABR relative to MR, but not in terms of CS; c) MR+BS and MR+AS produce the best overall ABR and CS, respectively, however, MR+AS seems to provide the best trade-off between the two performance metrics, while outperforming both MR and DPO. Consequently, in the following experiments we will focus on MR+AS."}, {"title": "Model Benchmark", "content": "Next, we evaluate MLMs (BERT, BART) and LLMs (GPT-40, LLaMA) on all datasets using median CS and ABR as metrics (distributions are shown in the Appendix Figures 1 and 2). Table 3 shows that MR+AS outperforms both MLMS (BERT, BART) and LLMs (GPT-40, LLaMA), with top performances in terms of CS in all datasets and top ABR in two of the datasets (SWS and LS14). Also, when accounting for results variation across datasets, we see that MR+AS significantly outperforms the others in terms of CS,"}, {"title": "Model-based Statistic Benchmark", "content": "After showing the effectiveness of our model in terms of CS and ABR, we now use the model-based statistic introduced in Section 3.1. Table 4 shows the proportion of model predictions that meet the significance threshold ($\\alpha$ = 0.01) for three groups of tokens: CA, CD and OMC. We do not present results for the other two groups (NCA and OAC) because when using rule- and LLM-based models, we get no candidates for non-substitutions. Further, for these models we may not always get the same number of substitute candidates per token, thus we use their median (3 candidates) across all tokens in the SWS test set as the number of candidates for all other models when calculating the statistic, i.e., $K_s$ = 3 in (2). In the Appendix Table 8 we show an extended table showing the NCA and OAC groups and $K_s$ = 1000 for MLM-based models (ours included) and Figures 3-6 with p-value distributions for all models. To avoid problems associated with overfitting, thus making the evaluation more reliable, in Appendix Table 9 and Table 10 we extended the evaluation result by using the GPTScore (Fu et al., 2023) with two backbone models: GPT2 (Radford et al., 2019) and OPT (Zhang et al., 2022). The results show that our MR+AS still outperforms other baseline models.\nIdeally, we want proportions in Table 4 to be close to one, indicating that the top candidate is of better quality than the alternatives. Effectively, results demonstrates that our model (MR+AS) not only produces the largest proportions, but it does so"}, {"title": "Top-2 Candidate Performance", "content": "Provided that the substitution rates by our model are low relative to others as discussed above, we now examine the quality of the top-2 substitute. Note that in MLM-based models a change is produced only when the top substitute candidate is different than the original token and that for LLM-based models the top-2 candidate is only available is the model considers a substitution has to be made and more than one candidates are provided. To quantify the quality of the top-2 candidates, we randomly selected five tokens from each sentence where the top-2 candidate is available and calculate the BARTScore ratio between the sentence modified with the top-candidate and that of the original (unchanged) sentence. We then averaged these ratios provide a summary of the quality of top-2 candidates. Results in Table 5 indicate that MR+AS achieved the best top-2 candidate quality across all four datasets followed (on average) by GPT-40 and the rule-based model."}, {"title": "Encouraging ranking in LLMS", "content": "It is well known that prompting LLMs to provide ordered responses is challenging (Lu et al., 2023). We tried prompts with and without a specific request for ordered substitute candidates (see Appendix B for details). We found that both GPT and LLaMA produced slightly"}, {"title": "Human Study", "content": "We recruited five annotators to carry out the study. First, we randomly sampled 25 cases from the SWS test data and provided the top two candidate replacements proposed by our model. We randomly flipped the order of these two candidates before presenting them to the annotators. Annotators were asked to indicate their preference for the order and whether they preferred to replace the target word with either of the two candidates\u00b3.\nTo assess inter-annotator agreement, we calculated the (unweighted) Cohen's kappa coefficient for both order agreement and replacement agreement. The average kappa values for the order agreement and the replacement agreement were 0.40\u00b10.21 (standard deviation) and 0.13\u00b10.21, respectively. These results align with our assumption that asking annotators to suggest replacements for a specific word in a given sentence is a subjective task. In Appendix Table 12, we present the complete results. Furthermore, we observed that in 83% of the cases, the annotators did not choose to replace the target word.\nTo mitigate the potential bias caused by providing the target word in advance, we designed a second questionnaire consisting of 50 cases. Twenty-five cases were sampled where our model and BARTScore agreed, and twenty-five cases where they disagreed. In this setting, the target word was hidden and only the top two candidate replacements (with randomly flipped order) were provided. Annotators were then asked to choose their preference from three options\u2074: (0) the top-1 and top-2 candidates are equally good. (1) the top-1 candidate is better. (2) the top-2 candidate is better.\nSubsequently, we made a comparison between the model predictions and the responses of the annotators. The results showed that in 38.8% of the"}, {"title": "5 Discussion", "content": "This paper addressed the problem of training and evaluating SWS models without requiring human annotations. Specifically, we leveraged a model-based score (BARTScore) to define evaluation metrics and to serve as a label-proxy for our loss function. Extensive experimental results demonstrate the effectiveness of our approach relative to both MLM- and LLM-based models."}, {"title": "Limitations", "content": "The proposed method leverages BARTScore as a sentence scoring mechanism, essentially acting as a proxy for human annotators in guiding model optimization during preference-aware learning. However, we cannot eliminate the possibility that BARTScore may not always accurately quantify the quality of token substitutions. While BARTScore has shown effectiveness in our experiments, it could be readily replaced by alternative scoring metrics deemed more reliable or better suited to other word substitution contexts. We leave this exploration as interesting future work."}, {"title": "Ethical Considerations", "content": "In the process of developing and deploying smart word suggestion systems it is crucial to consider the broader ethical implications of relying on automated tools for language optimization. These systems may inadvertently reinforce biases present in training data, leading to unintended or inappropriate word suggestions. Moreover, the replacement of human judgment with automated scoring models like BARTScore can risk overlooking or misrepresenting nuances in language, particularly in sensitive contexts. Therefore, we recommend ongoing human oversight and continuous evaluation of the model's output to ensure fairness, inclusivity, and alignment with ethical standards."}, {"title": "A DPO Derivation Details", "content": "DPO* Direct Preference Optimization (DPO) (Rafailov et al., 2024) is an efficient technique for aligning large language models (LLMs) with human feedback, which gained popularity due to its simplicity (Miao et al., 2024). For instance, it has demonstrated to be effective in chat benchmarks (Tunstall et al., 2023; Zheng et al., 2023). In our case, the model-based score serving as proxy for human feedback DPO (under the Plackett-Luce Model) is written as\n$L_{DPO} = -E \\left[ log \\frac{exp(\\beta r_k)}{\\sum_{1 \\le j \\le K} exp(\\beta r_j)} \\right]$, (1)\nwhere $r_k = log p_{\\theta}(X_k) - log p_{\\hat{\\theta}}(X_k)$, the expectation is over {$X_1, ..., X_K, X$}, and $\\theta$ and $\\hat{\\theta}$ denote the parameters of the model being trained and that used for reference, respectively. Accordingly, only $\\theta$ are updated while learning while $\\hat{\\theta}$ are kept fixed.\nSince the magnitude of the logits drops significantly ask $\\rightarrow$ K, we found that the sum in the denominator of (11) weakens the loss. Therefore, we removed the sum and instead compared the k-th and the (k + 1)-th substitution, rather than comparing it with all K values. The Derivation of DPO* is shown below\n$L_{DPO*} = -log \\frac{exp(dr_k)}{\\sum_{k=1}^{K-1} exp(dr_{k+1})}$ (12)\n$= - \\sum_{k=1}^{K-1} \\left(dr_k - log\\frac{exp(dr_k)}{exp(dr_{k+1})} \\right)$\n$= - \\sum_{k=1}^{K-1} \\left(dr_k - log\\frac{P_\\theta(X_k)}{P_{\\hat{\\theta}}(X_k)} - log\\frac{P_\\theta(X_{k+1})}{P_{\\hat{\\theta}}(X_{k+1})} \\right)$\n$= - \\sum_{k=1}^{K-1} \\left(\\delta r_k - \\left(log \\frac{P_{\\theta}(X_k)}{P_{\\hat{\\theta}}(X_k)} - log \\frac{P_{\\theta}(X_{k+1})}{P_{\\hat{\\theta}}(X_{k+1})}\\right) \\right)$\nFurther, we approximate log $p_{\\theta}(X_k)$ with its logit $s_k$, and let $\\delta$ = 1 for simplicity. Then (12) simplifies (for a single token in sentence X) to\n$L_{DPO*} = - \\sum_{k=1}^{K-1} (s_k - \\hat{s}_k - s_{k+1} + \\hat{s}_{k+1})$,\nwhere $s_k$ and $\\hat{s}_k$ denote the logit of the k-th substitution from the model being trained and the reference model, respectively.\nODPO* We also extend DPO (under the Bradley-Terry model) (Rafailov et al., 2024) to multiple substitute candidates, where each candidate is compared with the next in the ordered list of candidates. We write for a single token in sentence X\n$L_{ODPO*} = -\\sum_{k=1}^{K-1} log \\sigma \\left(\\delta r_k - \\delta r_{k+1} \\right)$. (13)\nWe approximate log $p_{\\theta}(X_k)$ with its logit $s_k$, and let $\\delta$ = 1 for simplicity. Then in (13) simplifies (for a single token in sentence X) to\n$L_{oDPO*} = \\sum_{k=1}^{K-1} logo \\left(s_k - \\hat{s}_k - s_{k+1} + \\hat{s}_{k+1} \\right)$,\nfrom which we see that the only difference between (10) and (9) (in the main paper) is that the comparison of logit values in the former is scaled with the log-logistic function. See Appendix for a derivation of both losses."}, {"title": "B LLM Prompts", "content": "Prompt without order:\nIn the following sentence, please give some suggestions to improve word usage. Please give the results with the JSON format of \u201coriginal word\": [\u201csuggestion 1\u201d, \u201csuggestion 2\u201d]. The 'original word' should include all words that can be improved in the sentence, directly extracted from the sentence itself. [s]\nPrompt with order:\nIn the following sentence, please give some suggestions to improve word usage. Please give the results with the JSON format of \u201coriginal word\": [\u201csuggestion 1\u201d, \u201csuggestion 2", "original word": "hould include all words that can be improved in the sentence, directly extracted from the sentence itself, and the suggestions should be ranked in order of the degree of improvement, from the most effective to the least. [s", "s": "is the sentence.\nAt times, the model may fail to provide the"}]}