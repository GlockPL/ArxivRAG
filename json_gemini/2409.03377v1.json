{"title": "Real-time Speech Enhancement on Raw Signals with Deep State-space Modeling", "authors": ["Yan Ru Pei", "Ritik Shrivastava", "Sidharth"], "abstract": "We present aTENNuate, a simple deep state-space autoencoder configured for efficient online raw speech enhancement in an end-to-end fashion. The network's performance is primarily evaluated on raw speech denoising, with additional assessments on tasks such as super-resolution and de-quantization. We benchmark aTENNuate on the VoiceBank + DEMAND and the Microsoft DNS1 synthetic test sets. The network outperforms previous real-time denoising models in terms of PESQ score, parameter count, MACs, and latency. Even as a raw waveform processing model, the model maintains high fidelity to the clean signal with minimal audible artifacts. In addition, the model remains performant even when the noisy input is compressed down to 4000Hz and 4 bits, suggesting general speech enhancement capabilities in low-resource environments.", "sections": [{"title": "I. INTRODUCTION", "content": "Speech enhancement is crucial for improving both human-to-human communication, such as in hearing aids, and human-to-machine communication, as seen in automatic speech recognition (ASR) systems. The most challenging task of speech enhancement is removing background noises from speech signals. The complexity of speech patterns and the unknown characteristics of noise poses challenges, further exacerbated by the dense data samples in audio signals and the sensitivity of human perception to minor noise and distortions. [1] Traditional speech enhancement methods such as Wiener filtering [2], spectral subtraction [3] and principal component analysis [4] have shown satisfactory performance in stationary noise environments, but their effectiveness is often limited in non-stationary noise scenarios, resulting in artifacts such as musical noises and substantial degradation in both the quality and intelligibility of the enhanced speech [5].\nDeep learning audio denoising methods are trained on large datasets of clean and noisy audio pairs, and attempt to capture the nonlinear relationship between the noisy and the clean signal features without prior knowledge of the noise statistics, as required by traditional denoising methods [6]. Speech features can be extracted from real or complex spectrograms of the noisy signal in the time-frequency domain, or the raw waveform [7].\nMany state-of-the-art deep learning denoising models leverage the feature extraction capability of convolutional networks. The UNet convolutional encoder-decoder is a common network architecture in denoising models [8]-[10] exemplified by Deep Complex UNet [11] and the generative adversarial SEGAN [12]. However, CNNs lack the capabilities to model long-range temporal dependencies present in speech signals. Another class of models that are more capable in this regard are RNNs, examples of which are DCCRN [13] and FRCRN [14]. These discriminative models show limited robustness to different types of noise and generalization across diverse audio sources [15]. Likelihood-based generative models such as Denoising Diffusion Probabilistic Models (DDPMs) that treat denoising as a conditional generation problem have attempted to overcome the limitations of their discriminative counterparts in generalization to unseen situations [16]. DDPMs, along with Variational autoencoders [17] also belong to the unsupervised class of models promising enhanced generalization capability to unseen noise and acoustic conditions. These approaches, however, often result in a large number of parameters which are computationally expensive, and by their nonlinear recurrent nature cannot efficiently leverage parallel hardware (e.g. GPUs) for training.\nAlternative approaches such as PercepNet [18] and RN-Noise [19] have attempted to reduce network size and complexity by combining traditional speech enhancement methods with deep learning, resulting in smaller models with fewer parameters capable of running in real-time on general-purpose hardware. Similarly, methods that process raw waveform signals aim to maximize the expressive capabilities of deep networks without resorting to costly time-frequency conversions, as demonstrated by Facebook's DEMUCS model's real-time performance [20]. However, the majority of these models cannot perform real-time inference on general-purpose hardware, exemplified by Nvidia's CleanUNet model [21]. Models such as DeepFilterNet [22] have leveraged speech-specific properties such as short-time speech correlations to achieve comparable results.\nHere, we introduce the aTENNuate network, belonging to the class of Temporal Neural Networks (TENNs). It is a deep state-space model (SSM) [23], [24] optimized for real-time denoising of raw speech waveforms on the edge. By virtue of being a state-space model, the model is capable of capturing long-range temporal relationships present in speech signals, with stable linear recurrent units. Learning long-range correlations can be useful for capturing global speech patterns or noise profiles, and perhaps implicitly capture semantic contexts to aid speech enhancement performance [25]."}, {"title": "II. STATE-SPACE MODELING", "content": "In this section, we briefly describe what state-space models are, and recall how they can be configured for neural network processing, involving discretization and diagonalization. State-space models are general representations of linear time-invariant (LTI) systems, and they can be uniquely specified by four matrices: $A \\in R^{h\\times h}$, $B\\in R^{h\\times n}$, $C\\in R^{m\\times h}$, and $D\\in R^{m\\times n}$. The first-order ODE describing the LTI system is given as\n$\\dot{x} = Ax + Bu, y = Cx + Du,$\nwhere $u \\in R^n$ is the input signal, $x \\in R^h$ is the internal state, and $y \\in R^m$ is the output. Here, we are letting $n > 1, m > 1$, which yields a multiple-input, multiple-output (MIMO) state-space model. For the remainder of this paper, we will ignore the $Du$ term as we do not use it.\nThe state-space model in its original form describes a continuous-time system, but in the field of digital signal processing, there are standard recipes for discretizing such a system into a discrete-time state-space model. One such method that we use in this work is the zero-order hold (ZOH), which gives us the discrete-time state-space matrices A and B as follows:\n$A = exp(\\Delta A), B = (\\Delta A)^{-1} \\cdot (exp(\\Delta A) -I)\\cdot \\Delta B.$\nThe discrete state-space model is then given by\n$x[t+1] = Ax[t] + Bu[t], y[t] = C x[t]$\nIn the context of recurrent neural networks (RNNs), this is essentially a linear RNN layer, which allows for efficient online inference and generation (in our case real-time speech enhancement), but at the same time efficient parallelization during training.\nIt is straightforward to check that the discrete-time impulse response is given as\n$k[T] = C A^T B,$\nwhere $T$ denotes the kernel timestep. During training, $k$ can be considered the \"full\" long 1D convolutional kernel with shape (output channels, input channels, length), in the sense that the output $y$ can be computed via the long convolution $y_j = \\sum_i u_i * k_{ij}$. By the convolution theorem, we can perform this operation in the frequency domain, which becomes a point-wise product $\\hat{y}_{jf} = \\sum_i \\hat{u}_i \\hat{k}_{ijf}$. The hat symbol denotes the Fourier transform of the signal (with the index $f$ denoting the Fourier modes), which can be efficiently computed via Fast Fourier Transforms (FFTs).\nIt is a generic property (though not always true) that a diagonal form exists for the state-space model, meaning that we can almost always assume $A$ to be diagonal [26], at the expense of potentially requiring $B$ and $C$ to be complex matrices. Since the original system is a real system, the diagonal $A$ matrix can only contain real elements and/or complex elements in conjugate pairs. (See Appendix A for an elementary demonstration of this fact using the impulse response.) In this work, we sacrifice a slight loss in expressivity by continuing to restrict $B$ and $C$ to be real matrices and letting $A$ be a diagonal matrix with all complex elements (but not restricting them to come in conjugate pairs). Since we still want to work with real features\u00b9, we then only take the real part of the impulse response kernel as such:\n$k[T] = \\Re(C A^T B),$\nwhich equivalently in the state-space equation can be achieved by simply letting $y[t] = C\\Re(x[t])$. This means that during online inference, we need to maintain the internal states $x$ as complex values, but only need to propagate their real parts to the next layer.\nSimilar to previous works in deep state-space modeling, we allow the parameters {$A, B, C, \\Delta$} to be directly learnable, which indirectly trains the kernel $k$. See Appendix C for more details. Unlike previous works in deep state-space modeling, we do not try to keep the sizes $n, h, m$ consistent, to allow for more flexibility in feature extraction at each layer, mirroring the flexibility in selecting channel and kernel sizes in convolutional neural networks\u00b2. The flexibility of the tensor shapes requires us to carefully choose the optimal order of operations during training to minimize the computational load. This is a topic explored in previous work [28], and briefly discussed in Appendix C."}, {"title": "III. NETWORK ARCHITECTURE", "content": "We use an hourglass network with long-range skip connections, similar in form to the Sashimi network [24] for audio generation. However, unlike previous works using state-space models for audio processing [24], [29], [30], our network directly takes in raw audio waveforms in the -1 to +1 range and outputs raw waveforms as well, with no one-hot encoding or spectral processing (e.g. STFT or iSTFT). Furthermore, we retain causality as much as possible for sake of real-time inference, meaning that we eschew any form of bidirectional state-space layers. See Fig. 1 for a schematic drawing.\nAs with typical auto-encoder networks, the audio features are down-sampled in the encoder and then up-sampled in the decoder. For the re-sampling operation, we use a simple operation that squeezes/expands the temporal dimension then projects the channel dimension [24]. More formally, for a reshaping ratio of r, a sequence of features can be down-sampled and up-sampled as:\nDown: $(C_{in}, L) \\xrightarrow{\\text{reshape}} (C_{in}r, L/r) \\xrightarrow{\\text{project}} (C_{out}, L/r)$\nUp: $(C_{in}, L) \\xrightarrow{\\text{reshape}} (C_{in}/r, Lr) \\xrightarrow{\\text{project}} (C_{out}, Lr)$\nThe baseline network uses LayerNorm layers and SiLU activations. In addition, we include a \u201cPreConv\" layer which is a depthwise 1D convolution layer with a kernel size of 3, to enable better processing of local temporal features. The PreConv operation is omitted in the 2 SSM blocks in the neck and in any block with only one channel\u00b3. See Appendix C for details on the computation of the theoretical latency for a given set of resampling factors and PreConv configurations. Using causal convolutions for PreConvs can eliminate additional latencies to the network [31], but we do not explore this implementation here. To better support mobile devices, we also test variants of the network with BatchNorm layers4, ReLU activations, and omitting PreConv layers in the decoder (and omitting PreConv layers altogether). The results are reported in the next section."}, {"title": "IV. EXPERIMENTS", "content": "For experiments, we train on the VCTK and LibroVox dataset, randomly mixed with noise samples from Audioset, Freesound, and DEMAND. We evaluate our denoising performance on the Voicebank + DEMAND testset, and the Microsoft DNS1 synthetic test set (with no reverberation) [32]. To guarantee no data leakage between the training and testing sets, we removed the clean and noise samples in the training set that were used to generate the synthetic testing samples. Both the input and output signals of the network are set at 16000 Hz. The loss function is a mix of SmoothL1Loss [33] and spectral loss at the ERB scale [22]. More details of the training pipeline are provided in Appendix \u0412.\nThe evaluation metric is the average wideband PESQ score between the clean signals and the denoised outputs. The PESQ scores for different variants of the aTENNuate model against other real-time audio-denoising networks are reported in Table. I. We also report other network inference metrics including parameters, MACs, and latencies. For the latency metric, we focus on the theoretical latency of the network, not accounting for any processing latencies. In simpler terms, it is the maximum time range that the network needs to \"wait\" or look-forward to produce a denoised data point corresponding to the current input. As seen in the Table I, the PreConv layers (being depthwise) does not make much difference in terms of parameters and MACs, but does add considerably to latency. Unless otherwise denoted, we use the source code and pre-trained weights of each model and run it through a standardized PESQ evaluation pipeline.\nFor further inspection of the quality of the denoised samples produced by the network, we not only listen to the denoised samples (on both synthetic data and real recordings) but also provide a comparison of the denoised spectrogram and the clean spectrogram. This is to ensure that the denoised samples do not contain any unnatural artifacts that are common with raw audio processing systems, which may not be captured by the PESQ score [34]. See Fig. 2 for the spectrograms.\nIn addition to audio denoising, we also perform studies on the ability of our network to perform super-resolution and de-quantization on highly compressed data. This involves intentionally performing down-sampling and quantization of the input signals, in that order, and re-training the network"}, {"title": "V. FUTURE DIRECTIONS", "content": "To make the network even more mobile-friendly, we plan to explore the sparsification and quantization of the network"}, {"title": "APPENDIX A", "content": "DIAGONALIZATION OF THE A MATRIX\nIf we allow B and C to be complex matrices, then we can assume A to be diagonal without any loss of generality. To see why, we simply let A = P-1AAP (where AA is the diagonalized A matrix, and P is the similarity matrix) and observe the following:\n$\\forall t, k[t] = C(P^{-1}A_AP)^{t-1}B = CP^{-1}A_A(PP^{-1})...(PP^{-1})PB = (CP^{-1})A_A^{t-1}(P^{-1}B) = C'A_A^{t-1}B',$\nwhere $B'$ and $C'$ are complex matrices that have \u201cabsorbed\u201d the similarity matrix P, but WLOG we can just redefine them to be B and C. Since A is a real matrix, the complex eigenvalues in AA must come in conjugate pairs. And WLOG we can again redefine AA as A."}, {"title": "APPENDIX B", "content": "EXPERIMENT DETAILS\nOur baseline model is trained with:\n\u2022 500 epochs\n\u2022 AdamW optimizer with the PyTorch default configs\n\u2022 A cosine decay scheduler with a linear warmup period equal to 0.01 of the total training steps, updating after every optimizer step\n\u2022 gradient clip value of 1\n\u2022 layer normalization (over the feature dimension) with elementwise affine parameters\n\u2022 SiLU activation\n\u2022 no dropout\nThe high-level training pipeline for the raw audio denoising model is to simply generate synthetic noisy audios by randomly mixing clean and noise audio sources. The noisy sample is then used as input to the model, and the clean sample is used as the target output.\nFor the clean training samples, we use the processed VCTK and LibriVox datasets that can be downloaded from the Microsoft DNS4 challenge. We also use the noise training samples from the DNS4 challenge as well, which contains the Audioset, Freesound, and DEMAND datasets. For all audio samples, we use the librosa library to resample them to 16 kHz and load them as numpy arrays.\nFor the LibriVox audio samples which form long continuous segments of human subjects reading from a book, we simply concatenate all the numpy arrays, and pad at the very end such that the array can be reshaped into (segments, 217). For all the other audio samples consisting of short disjoint segments, we perform intermediate paddings when necessary, to ensure a single recording does not span two rows in the final array. For audio samples longer than length 217, we simply discard them. The input length to our network during training is then also 217."}, {"title": "APPENDIX C", "content": "NETWORK DETAILS\nA. Initialization of SSM Parameters\nAs mentioned in Section II, we make the SSM parameters {A, B, C, \u2206} trainable. Recall that we take A \u2208 Ch to be a complex vector (or a complex diagonal matrix). For stability, we treat the real and complex parts of A separately. The real part R(A) is parameterized as -softplus(ar), where ar is initialized with the value -0.4328, giving R(A) = -1/2 initially. Note that due to the positivity of softplus, R(A) will always remain negative during training, which ensures the stability of the SSM layer. The imaginary part \u2111(ar) is parameterized directly and initialized with \u03c0(\u03af \u2013 1) where i is the state index. The matrix B\u2208 Rh\u00d7n is initialized with all ones, and the matrix C\u2208 Rm\u00d7h is initialized with Kaiming normal random variables (assuming a fan in of h). Finally, we initialize \u2206 with 0.001 \u00d7 100[\u03af/16], giving a series of geometrically spaced values from 0.001 to 0.1 in blocks of 16. These initializations and parameterizations are not absolutely required, and we suspect that any reasonable approach respecting the stability of the SSM layer will suffice.\nB. Optimal Contraction Order\nIf we let K (t) = A be the \u201cbasis kernels\u201d of the SSM layer, and its Fourier transform be K(f), then in einsum form, the SSM layer operations during training can be expressed as\n$\\hat{Y}_{bjf} = B_{bi} \\hat{f}_{B_{ni}} \\hat{K}_{nf} \\hat{C}_{jn},$\nwhere {b, i, j, n, f} indexes the batch size, input channels, output channels, internal states, and Fourier modes respectively. With abuse of notation, we similarly let {B, I, J, N, F} be the sizes of the five aforementioned dimensions. Note that the number of Fourier modes F is the same as the length of the signal L.\nThere are two main ways to compute \u0177. First, we can perform the operations of Eq. 8 from left to right normally, corresponding to projecting the input, performing the FFT convolution, then projecting the output. Alternatively, we can"}]}