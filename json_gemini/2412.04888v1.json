{"title": "VTD: Visual and Tactile Database for Driver State and Behavior Perception", "authors": ["Jie Wang", "Mobing Cai", "Zhongpan Zhu", "Hongjun Ding", "Jiwei Yi", "Aimin Du"], "abstract": "Abstract\u2014In the domain of autonomous vehicles, the human-vehicle co-pilot system has garnered significant research attention. To address the subjective uncertainties in driver state and interaction behaviors, which are pivotal to the safety of Human-in-the-loop co-driving systems, we introduce a novel visual-tactile perception method. Utilizing a driving simulation platform, a comprehensive dataset has been developed that encompasses multi-modal data under fatigue and distraction conditions. The experimental setup integrates driving simulation with signal acquisition, yielding 600 minutes of fatigue detection data from 15 subjects and 102 takeover experiments with 17 drivers. The dataset, synchronized across modalities, serves as a robust resource for advancing cross-modal driver behavior perception algorithms.\nKeywords: Visual and tactile data, driver state, driver behavior, intelligent cockpit, autonomous vehicles", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, data-driven autonomous vehicles have encountered SOTIF (Safety of the Intended Functionality) issues and long-tail challenges in their AI algorithms. These challenges arise due to the complexity and unpredictability of real-world scenarios where autonomous vehicles must navigate, requiring robust algorithms for safe operation under various conditions. Furthermore, the uncertainty in driver non-driving behavior\u00b9 and abnormal state\u00b2 within the context of human-vehicle co-driving further exacerbates the challenges faced by autonomous vehicles. Perceiving and predicting the driver's uncertain behaviors accurately is crucial for developing effective AI algorithms that can adapt and respond appropriately.\nAs an individual with fully independent behavior possessing subjective initiative uncertainty and individual discrepancy, drivers may become increasingly reliant on autonomous systems as driving intelligence advances, leading to drowsiness, slower reaction times, and reduced risk perception[1], [2]. Furthermore, the addition of in-vehicle entertainment features introduces a potential risk of distracted driving. Research indicates that driver behavior is a key factor in most road traffic accidents, with fatigue and distraction being the primary causes[3], [4]. Changes in a driver's state increase the uncertainty of driving behavior and raise the risks associated with human-machine interaction, impeding the development of intelligent human-machine hybrid driving modes. Therefore, perceiving and understanding driver behavior and state has become a critical area for research breakthroughs.\nTo address these challenges, it is essential to collect high-quality datasets on fatigue and distraction, enhancing the ability to detect driver risks in intelligent co-driving systems and reducing potential dangers. These datasets should cover a wide range of driving conditions, human behavioral characteristics, and the specific interaction patterns found in human-machine co-driving scenarios.\nThe availability of human-vehicle co-driving datasets is crucial for the development and evaluation of artificial intelligence algorithms in autonomous vehicles. However, existing datasets for human-vehicle joint driving applications are still quite limited, especially in monitoring key states such as driver fatigue and distraction, and existing datasets are often limited in scope and depth, lacking the nuanced information required to address these challenges effectively. This paper proposes a multimodal cross-sensing method combining visual and haptic channels under controlled environmental conditions and constructs the VTD (Visual and Tactile Database for Driver State and Behavior Perception), a comprehensive, well-structured, large-scale dataset. The VTD dataset not only covers diverse driving conditions and human behavior characteristics but also places particular emphasis on monitoring and recording driver fatigue and distraction states. These data will help enhance the perception and understanding capabilities of autonomous driving systems regarding driver states in intelligent co-driving scenarios, thereby improving the safety and reliability of human-machine interaction and effectively reducing potential risks.\nThe contributions of VTD dataset are as follows:\n1) This paper presents the VTD, a long-sequence multi-modal natural driving dataset based on the fusion of visual and haptic data, which includes over 10 hours of fatigue driving data and 102 takeover scenarios. It effectively captures the multi-path driving conditions, as well as the mental and physical states and behavioral characteristics of drivers in multimodal environments.\n2)The VTD is developed using a human-in-the-loop algorithm to ensure that the collected data accurately reflects real-world driving behavior. Additionally, the boundaries of driving environment are clearly defined, facilitating the quantification of the influencing mechanisms behind the driving behaviors of different groups.\n3) Serving as a standardized platform for benchmarking in the field of human-vehicle co-driving, VTD offers valuable data support for cross-modal perception algorithms and scenarios related to driver fatigue and distraction. This capability significantly advances research in driver behavior perception."}, {"title": "II. RELATED WORK", "content": "In designing human-vehicle collaboration systems, recognizing the driver's intent, modeling behavior, and monitoring their state are critical. Despite advancements in autonomous driving technologies, the driver remains the core of system coordination[5]. Therefore, accurately monitoring the driver's fatigue and distraction is essential to ensure the safe operation of human-machine collaborative systems.\nDriver state monitoring primarily focuses on physiological and psychological factors. Driver behavior monitoring involves analyzing specific behaviors during driving to infer their state and decision-making process. Behavior is an external manifestation of the state: the former reflects specific actions, while the latter represents the mental and physical condition. By observing driving behavior and measuring physiological and psychological indicators, a more comprehensive inference of the driver's fatigue or distraction can be made. This paper will introduce datasets in the field of human-vehicle collaboration from the perspectives of driver state and behavior, along with their applications and limitations in driver monitoring and human-vehicle co-driving system optimization."}, {"title": "A. Datasets for Driver State", "content": "As mentioned earlier, fatigue and distraction are the two main factors affecting driving safety. Therefore, this section will primarily focus on the current methods and datasets for monitoring driver distraction and fatigue, and analyze their applicability and limitations in real-world scenarios.\nResearch on driver distraction monitoring\u00b3 has varied focuses. In recent years, the rise of AI algorithms for computer vision has led to a growing interest in analyzing driver behavior through visual perception methods. This includes studying facial expressions [6] and head gestures [7]. Key features commonly extracted in these studies are eye fixation duration, scan paths, eye-opening and eye-closing patterns, and head rotation angles. In addition, driving distraction monitoring and behavior analysis can also be achieved by using vehicle sensors to monitor vehicle conditions, such as steering wheel angle and accelerator pedal position, or by physiological indices such as driver Electrocardiogram (ECG) [8], [9], Electroencephalogram (EEG) [10], [11], [12], Electromyography (EMG), and Galvanic Skin Response (GSR) [11].\nVisual-based driver distraction monitoring accuracy is still limited in an actual driving environment due to problems including low resolution, motion blur, dynamic background, and occlusions [13]. Hand movements, head gestures [7], gaze direction [14], and pedal control are the keys to addressing the problems. Moreover, existing datasets still cannot characterize diverse, ambiguous, and personalized distraction behaviors influenced by the driver's physiological and psychological state[15], [16], [17], [18], [19], [20]. The investigations of the above datasets demonstrate a need for fine-grained distraction datasets with controllable and quantifiable conditions, multi-modal synchronized data, and data about driver distraction feature diversity.\nAside from driving distraction detection, driver fatigue is likewise an important research direction of driver behavior and state perception. Fatigue is displayed in various forms. Based on eye feature extractions, PERCLOS (percent eye closure), eye-white reflex, eye states, and yawning conditionsare included[21]. It can also be displayed through detection based on eye-mouth combinations [22], eyelid closure and eye closure percentage combinations [23], FatigueTree [24], and other combinations.\nWe gathered and analyzed existing DMS datasets [25], [26], [27], [28], [27], [16], [29], [24] and discovered that in many datasets, fatigue is monitored by recording drivers' facial features under natural driving conditions using RGB cameras. These natural-driving datasets face challenges in extracting driver fatigue features under nighttime dimmed lighting conditions or facial occlusions, and different types of physiological fatigue signals cannot be captured with a single vision. Differences in road environments may also lead to differences in driving loads, making it impossible to analyze the cause of fatigue and extract the differentiated impact on drivers. Additionally, most of the existing single-mode visual datasets about driver fatigue only concentrate on relatively monotonous visual features like blinking and yawning [25], [26], which lack time series and contextual features. Algorithms carried out on these single-mode datasets are too restrictive to be applied in reality and cannot contain all challenges[24]. It is increasingly essential to determine how to provide greater flexibility and diversity in driver monitoring using multi-modal data to characterize fatigue signals and more complicated state combinations. However, the lack of a complete and comprehensive dataset in this field has bottlenecked the progress in algorithm development of driver fatigue detection [16]."}, {"title": "B. Datasets for Driver Behaviors", "content": "Driver behavior is distinct from the driver's state. It refers to the specific movements a driver makes during the driving process, such as turning, accelerating, decelerating, and braking. Driver behavior can be captured and analyzed using vehicle sensors, cameras, and other monitoring devices. It is often associated with specific driving skills and traffic regulations, such as speeding, frequent lane changes, and running red lights. In essence, driver behavior encompasses the specific actions and maneuvers made by the driver, whereas driver state refers to their physical and mental condition. The driver's state can be indirectly inferred by observing and evaluating their behaviors and by measuring relevant physiological indicators. Modeling and understanding a driver's state through their behavior is essential for ensuring safety and facilitating assisted driving [30]. To enhance driving safety, future intelligent vehicles should be capable of autonomously assessing the driver's behavior and competence using onboard sensors and operational data."}, {"title": "A. Overall Framework", "content": "Driving data from 15 subjects in a fatigued condition and takeover experiment data from 17 distracted participants are included in the VTD dataset. All participants were fully informed about the research background and procedures, and they consented to participate by signing a written informed consent form. Detailed information about the subjects is provided in Table I. This paper will separately describe the specifics of the two data collection experiments and the data processing procedures."}, {"title": "B. Fatigue Driving Experiment", "content": "The Fatigue dataset includes a processed time-series dataset and a raw simulation scenario video dataset. The time series dataset comprises eleven dimensions and is divided into a training set, a validation set, and a testing set in a ratio of 4:1:1.The total number of valid samples is 480, with a time slice of 60 seconds and a frame number of 1800. The dataset labels were modified based on KSS and SSS fatigue scales, and subjective evaluations were completed.\nTo construct a driver fatigue detection dataset, we collected information from participants and assigned pre-fatigue status according to age, sleep duration, and napping habits. Table II shows the pre-fatigue status of the participants. Participants were required to complete the pre-fatigue accumulation according to the assigned status. During the testing stage, the functionality of the platform connection and the data collection program was verified. Participants were instructed to complete the experiment preparations and driving adaptation under the operation instructions. They then performed the experiment wearing eye movement equipment and holding the electrode area of the steering wheel. They entered the appointed conditions and drove for 40 minutes continuously, during which they should keep their hands on the electrodes on the two sides. When the staff issued the prompt \"report"}, {"title": "MAR", "content": "the current status\u201d every five minutes, the participants should complete their self-evaluations while the staff completed their assessments on the participants, combining the observation results and reported results.\nWe generalized the different types of data collected as video data, vehicle data, and ECG data. The data were processed in different ways according to their characteristics. For the driver frontal behavior information, the Mediapipe Facemesh model[33] was adopted for face landmark estimation,extracting 478 key coordinates of the driver's face and characterized their mouths and eyesby Mouth Aspect Ratio (MAR) and Eye Aspect Ratio (EAR). The MAR and EAR values of each image frame are concatenated to form a time series. As indicated by the key points in the figure, the MAR and EAR are calculated as follows:\n$\\begin{equation}\nMAR = \\frac{|| P_{82}-P_{87} ||_{2} + || P_{312}-P_{317} ||_{2}}{4 \\|\\ P_{78} - P_{308} \\|_{2}}\n\\end{equation}$", "equations": ["MAR = \\frac{|| P_{82}-P_{87} ||_{2} + || P_{312}-P_{317} ||_{2}}{4 \\|\\ P_{78} - P_{308} \\|_{2}}"]}, {"title": "EAR", "content": "$\\begin{equation}\nEAR = \\frac{|| P_{387}-P_{373} ||_{2} + || P_{385}-P_{380} ||_{2}}{4 \\|\\ P_{263} - P_{362} \\|_{2}}\n+ \\frac{|| P_{158}-P_{153} ||_{2} + || P_{160}-P_{144} ||_{2}}{4 \\|\\ P_{133} - P_{33} \\|_{2}}\n\\end{equation}$", "equations": ["EAR = \\frac{|| P_{387}-P_{373} ||_{2} + || P_{385}-P_{380} ||_{2}}{4 \\|\\ P_{263} - P_{362} \\|_{2}}\n+ \\frac{|| P_{158}-P_{153} ||_{2} + || P_{160}-P_{144} ||_{2}}{4 \\|\\ P_{133} - P_{33} \\|_{2}}"]}, {"title": "C. Distracted Driving and Takeover Experiment", "content": "Regarding the driving takeover dataset, each participant is asked to perform three takeovers during visual and auditory subtasks in the investigation. Initially, the vehicle is in the autonomous driving phase while participants perform non-driving tasks on a tablet computer. When the system prompts a message to take over, drivers operate the vehicle while staff record relevant data and address unexpected situations.\nThis paper establishes 34 groups of visual and auditory subtasks for autonomous vehicles under three conditions (straight path, roundabout cut-in, and roundabout obstacle avoidance) and conducts 102 takeover experiments. After data screening and criteria extraction through nodes, takeover segments are extracted and divided according to the time nodes marking the start and end of the entire process, as well as those of the takeover.\nRegarding takeover time, it is divided into takeover reaction time and takeover execution time. Takeover reaction time refers to the duration between the system's takeover request and the driver's return to the driving task (both hands back on the steering wheel), while takeover execution time is the sum of the duration during which the steering wheel angle> 2\u00b0 and the pedal was pressed \u2265 10%.\nVTD's takeover and distraction data can also be used to calculate a driver's load rate in human-vehicle co-pilot tasks through cognitive architecture models like QN-ACTR. Based on the load rate, driver's fatigue level can be assessed, and by combining the vehicle's displacement information, a safer and more reasonable human-vehicle driving right switching strategy can be designed.VTD also includes unscreened raw time series, raw videos, and tactile data so that users can filter and combine data based on research needs and goals."}, {"title": "D. VTD Experiment Setup Innovation", "content": "1) Multi-channel and Multi-angle Videos: Owing to the lack of public driver behavior datasets, most datasets are single-mode (RGB). For safety reasons, only simple visual signals can be collected in actual driving processes. These visual features usually depend on cameras and sensors directed toward the driver to obtain input data. The large-scale multi-view multi-modal database we constructed, VTD, can fill the gap for single visual signals. The accuracy of features extracted from facial detection, head pose estimation, and eye status analysis can be enhanced using multi-view information like driving view, eye movement, and facial view [34]. Moreover, eye trackers' high sampling rate, high precision, and low noise are advantageous compared to visual feature detection. Other modal features can be tuned to enhance the overall recognition rate using multi-view feature extraction and fusion.\nEye detection and eye status analysis are crucial to driver distraction and fatigue detection. Head rotation and eye closure rate can be calculated by applying PERCLOS to measure a driver's fatigue level and PERLOOK [35] to measure ametropia duration. Due to limitations in resolution, camera-eye distance, and lighting conditions, it is not easy to calculate and distinguish the accuracy of data results from current mainstream datasets. However, VTD adopted Tobii Glasses3 to obtain omnidirectional eye movement tracking data from various angles, thus achieving the capture of high-precision eye movement data in an extensive range.\nIn current mainstream research methods, behavior analysis and fatigue detection have also been conducted by fully using the drivers' diverse characteristics. These include detecting physiological signals, such as using EOG(Electrooculography) and ECG[35], [36] or combining driving measurements (Steering wheel angle, steering speed, accelerator pedal angle, etc.) [37], [38]. In all of the above scenarios, VTD is adaptable.\n2) Tactile sensing device for driver's ECG: To minimize the impact of the ECG devices on the driver, these devices are fixed on the G29 steering wheel. Signals are acquired through two flexible electrodes and transmitted to the collection program via USB, where they are saved as real-time texts. In contrast to signal acquisition from the participants' left and right earlobes, participants only need to hold the electrode area of the steering wheel to perform real-time heart rate detection. This approach significantly reduces the chance of distraction and mitigates the devices' impact on the experiment."}, {"title": "E. Data Processing Method Innovations", "content": "To better align the dataset with the training model, normalization preprocessing is performed on the time-series data, which is then categorized according to research directions. Regarding driving fatigue detection, the VTD dataset contains 11-dimensional time series information and includes data series of the drivers' frontal image, ECG signals, and the vehicle's motion state. Fatigue levels are then graded by subjective evaluations combined with self-assessment and other's assessment, thus realizing data calibration of Human-in-the-loop. Subsequently, dimension reduction and screening are performed on the above data to ensure a strong correlation between the data and the driver behaviors."}, {"title": "V. CONCLUSIONS", "content": "This paper presents a method for constructing a long-sequence multimodal natural dataset based on visual-tactile data fusion. The aim is to provide data support for quantifying and validating drivers' fatigue and distraction detection across identical driving scenarios, as well as for cross-modal perception algorithms related to driver behaviors, such as driving takeover monitoring. To meet various research demands, the VTD dataset includes data on fatigue driving and the drivers' visual and tactile behaviors during human-vehicle driving control transitions. This work aims to establish a standardized platform for benchmark testing, thereby advancing the development of driver behavior perception and enhancing research on driving safety."}, {"title": "B. Properties and Functions of Tactile Data", "content": "VTD provides piezoelectric tactile ECG data and steering wheel data. In the 40-minute experiment, we gathered drivers' ECG and PPG data using their tactile feedback to the flexible electrode during real-time driving. At the same time, 7-dimensional data of the steering wheel, the vehicle direction, the brake, the accelerator, the gear position, and the turning angle were collected. The driver's feedback and steering wheel data formed cross-validation.\nTraditional visual driving behavior detection methods are limited by lighting conditions and the vehicle's location. Consequently, they are unable to satisfy continuous, high-quality visual signal collection. Additionally, visual identity systems are also faced with problems including but not limited to computing power issues and communication delays. We can partly solve the above issues with the wearable piezoelectric tactile device without creating constraints or disturbance caused by traditional wearable devices. However, difficulties still exist, for example, too many environmental interference sources and insufficient robustness for dynamic changes in signals and environments[39], [40]. A breakthrough that future driver behavior perception technology should anticipate is a combination of vision and tactile sensations that can balance the driver's state, the accuracy of behavior recognition, and application adaptability."}, {"title": "C. Properties and Functions of Visual-tactile Combinations", "content": "Combining vision and tactile sensation can compensate for the robustness of visual perception by incorporating the visual modality's sensitivity to position and movement and the tactile modality's rapidity. It can reduce the system delay under the risk of data overload and form mutual complementary effects under vehicle tracking and collision avoidance control [41]. Visual-tactile fusion requires temporal embedding when combined with multi-dimensional time-series data. Positional embeddings are also added to non-linear transformed time series to leverage the sequential correlations based on time steps. While using Transformer to classify time series and make predictions, positional embeddings can be employed to solve the scene adaptation issue of position data and time series in Transformer. These positional embedding vectors, along with multi-dimensional time series, can be injected into the model as additional input."}]}