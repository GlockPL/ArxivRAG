{"title": "Think Carefully and Check Again! Meta-Generation Unlocking LLMs for Low-Resource Cross-Lingual Summarization", "authors": ["Zhecheng Li", "Yiwei Wang", "Bryan Hooil", "Yujun Cais", "Naifan Cheung", "Nanyun Peng", "Kai-Wei Chang"], "abstract": "Cross-lingual summarization (CLS) aims to generate a summary for the source text in a different target language. Currently, instruction-tuned large language models (LLMs) excel at various English tasks. However, unlike languages such as English, Chinese or Spanish, for those relatively low-resource languages with limited usage or data, recent studies have shown that LLMs' performance on CLS tasks remains unsatisfactory even with few-shot settings. This raises the question: Are LLMs capable of handling cross-lingual summarization tasks for low-resource languages? To resolve this question, we fully explore the potential of large language models on cross-lingual summarization task for low-resource languages through our four-step zero-shot method: SUMMARIZATION, IMPROVEMENT, TRANSLATION and REFINEMENT (SITR) with correspondingly designed prompts. We test our proposed method with multiple LLMs on two well-known cross-lingual summarization datasets with various low-resource target languages. The results show that: i) GPT-3.5 and GPT-4 significantly and consistently outperform other baselines when using our zero-shot SITR methods. ii) By employing our proposed method, we unlock the potential of LLMs, enabling them to effectively handle cross-lingual summarization tasks for relatively low-resource languages.", "sections": [{"title": "1 Introduction", "content": "Cross-lingual summarization refers to summarizing the source text in another target language. Traditionally, CLS is approached through one of two methods: summarize-translate or translate-summarize (Leuski et al., 2003; Or\u0103san and Chiorean, 2008). In the summarize-translate method, the text is first summarized in the source language and then translated into the target language. The translate-summarize method reverses this order. Both approaches, however, are prone to error accumulation during the two-step process, which can significantly degrade the final output quality.\nWith the advent of the Transformer architecture (Vaswani et al., 2017), end-to-end multilingual models like mBART (Liu et al., 2020), mBART-50 (Tang et al., 2020), and mT5 (Xue et al., 2020) have been developed and applied to CLS tasks. However, these models often require extensive fine-tuning, especially when applied to low-resource languages with limited pre-training data (Parnell et al., 2024).\nIn recent years, large language models (LLMs) such as GPT-2, InstructGPT, GPT-4, and Llama (Radford et al., 2019; Brown et al., 2020; Ouyang et al., 2022; OpenAI et al., 2024; Dubey et al., 2024) have shown significant potential for CLS tasks due to their extensive training on vast multilingual data. These models have achieved strong performance in high-resource languages like English, Chinese, and German (Wang et al., 2023) by implementing summarize-translate method. However, their effectiveness in low-resource languages remains limited, even when using few-shot learning techniques (Park et al., 2024).\nThis limitation underscores a critical area of research that has not yet been fully explored: whether LLMs can be effectively adapted for cross-lingual summarization tasks in low-resource languages, and if so, how effective they can be. Addressing this gap is crucial for extending the benefits of LLMs to a broader range of linguistic communities, making it an important area for further investigation.\nTo address these challenges, we propose a four-step zero-shot approach, Summarization, Improvement, Translation, and Refinement (SITR) designed to unlock the full potential of LLMs for CLS tasks in low-resource languages. Our method mitigates the issues of traditional pipelines by incorporating meta-generation strategies, which allows LLMs to learn from feedback and use refinement"}, {"title": "2 Methodology", "content": "2.1 SITR (Two-Stage Meta-Generation)\nIn this paper, we propose a four-step zero-shot SITR method for cross-lingual summarization in low-resource languages (see Figure 3), comprising SUMMARIZATION, IMPROVEMENT, TRANSLATION and REFINEMENT. The IMPROVEMENT and REFINEMENT stages align with two-stage meta-generation, involving LLM strategies like feedback learning, and rethinking (Welleck et al., 2024). To maximize LLMs' potential, we design specific prompts for each step, guiding the models to generate reliable outputs and minimizing error accumulation.\n[SUMMARIZATION]. LLMs should distill the long input source text (I) into concise summary (S). To counter their tendency to generate overly detailed summaries, we use a summarization prompt (Psum) to focus their output on the core essence of the text, ensuring the summary is both"}, {"title": "precision and relevant without unnecessary elaboration.", "content": "S = LLM(I; Psum) (1)\n[IMPROVEMENT]. The first stage of meta-generation, providing large language models with the input source text (I), the initial summary (S) from the SUMMARIZATION step, and the improvement prompt (Pimp) (see Figure 8) to recheck and optimize the summary (S*). This step reduces error accumulation by enabling self-improvement, preparing the more accurate summary for the next step of translation."}, {"title": "S* = LLM(I; S; Pimp) (2)", "content": "[TRANSLATION]. Using the translation prompt (Ptra) (see Figure 9), the optimized summary (S*) after the IMPROVEMENT step is translated into the text (T) in low-resource target language. Due to limited training data and lack of confidence, LLMs often produce redundant and messy outputs in these languages. This step aims to produce more reliable translations to ease the subsequent process.\nT = LLM(S*; Ptra) (3)\n[REFINEMENT]. The optimized summary (S*) after the IMPROVEMENT step, the initial translation (T) from the TRANSLATION step, and the refinement prompt (Pref) (see Figure 10) are combined and input into the LLMs for self-correction to generate the final output (O). This process constitutes the second stage of meta-generation, enabling the LLMs to revise and produce a more accurate translation through re-evaluation.\nO = LLM(S*; T; Pref) (4)\nOur proposed method generally involves four steps to leverage the large language model's inherent capabilities. For summarization or translation tasks where a perfect result cannot be achieved in a single attempt, we utilize meta-generation to enable the LLMs to self-reflect and improve their final output. Additionally, when the model lacks guidance or confidence, we use strategic prompts to prevent disorganized or unreliable results. This approach ensures that the large language model produces high-quality and coherent outputs through the implementation of two-stage meta-generation."}, {"title": "2.2 Large Language Models", "content": "In this paper, we conduct a thorough evaluation of various large language models using our proposed SITR method (Detail information in Appendix A).\nClosed-Source Models. We utilize four different models developed by OpenAI, including the latest GPT-40 and GPT-40-MINI.\nOpen-Source Models. We conduct our experiments on LLAMA3 and LLAMA3.1 (Touvron et al., 2023; Dubey et al., 2024) developed by MetaAI; QWEN-1.5 and QWEN2 trained by Alibaba Cloud (Bai et al., 2023); GEMMA and GEMMA2 created by Google (Team et al., 2024) and MIXTRAL from Mistral AI (Jiang et al., 2024)."}, {"title": "3 Experiments", "content": "3.1 Datasets & Langueges\nDatasets. In our research, we conduct experiments on two popular cross-lingual summarization datasets: CrossSum (Hasan et al., 2021) and Wik-ilingua (Ladhak et al., 2020)."}, {"title": "3.2 Metrics", "content": "In our experiments, we use ROUGE-1/2/L (Lin, 2004) and BERTScore (Zhang* et al., 2020) as four different metrics.\nROUGE metrics evaluate lexical overlap between the generated summaries and their references by considering unigrams, bigrams, and the longest common subsequence. BERTScore metric, however, focuses on measuring semantic similarity between two texts. We compute ROUGE scores with the multi-lingual ROUGE toolkit\u00b2, and BERTScore is calculated using the bert-score toolkit3."}, {"title": "3.3 Baselines", "content": "We select fine-tuned mBART-50, mT5-small, and mT5-base as baselines to demonstrate the capabilities of the fine-tuned encoder-decoder models on cross-lingual summarization tasks for low-resource languages.\nFor LLM-related baselines, we employ few-shot learning method with GPT-3.5 and GPT-4 following the prompt (see Figure 12) from a previous paper (Park et al., 2024). Besides, we also evaluate"}, {"title": "3.4 Experiment Results", "content": "The main experimental results on the CrossSum dataset are presented in Table 2. We compare our zero-shot SITR method with three types of baselines: fine-tuned encoder-decoder models, few-shot learning, and summarize-translate LLMs across various low-resource languages. Table 3 shows the main results for the WikiLingua dataset. (More experimental results are shown in Appendix C).\nTo further explore the potential of current large language models for cross-lingual summarization of low-resource languages and assess the robustness of our SITR architecture, we conduct extensive experiments with our method on various large language models. The results are presented in Table 4 and Table 5.\nSITR vs Fine-tuned Models. Table 2 and Table 3 show that mT5-small and mT5-base both perform poorly on low-resource languages, even after fine-tuning with approximately 1,000 data points. While mBART-50 achieves better results, it still lags behind our zero-shot SITR method across almost all languages, except for Pashto, where fine-tuned mBART-50 has a slightly higher score. Notably, fine-tuning an encoder-decoder model for each low-resource language is significantly more costly than using large language models with our proposed SITR method.\nSITR vs LLM Baselines. Table 2 and Table 3 demonstrate that under our approach, the outputs of the large language models significantly outperform other baselines in terms of both ROUGE and BERTScore metrics. This demonstrates that our outputs not only capture the key information of the text but also show notable improvements in word choice and semantic information.\nOn the CrossSum dataset, SITR improves the"}, {"title": "3.5 Output Analysis", "content": "In Figure 4, we compare our SITR method with other two LLM baselines in summarizing English news into Ukrainian.\nThe outputs from the other two methods are suboptimal due to their lack of relevance to the main topic and the generation of nonsensical content. The single-step summarize-translate method, which lacks self-correction and crucial prompt guidance, translates inaccurate summaries directly into the target language, causing error accumulation. On the other hand, the two-shot generation method skips the distributed thinking process, leading to uncontrollable outputs when the model fails to learn effectively from the examples. Both approaches, therefore, exhibit significant limitations.\nIn contrast, our method leverages meta-generation with targeted guidance, ensuring the model produces controlled and coherent outputs. This approach also allows the model to engage in self-reflection and iterative improvement, leading to more reliable and accurate results. The improvement step streamlines the summary by removing unnecessary sentences, while the refinement step adjusts sentence structure to better match the style of news reporting. Compared to the dataset's reference, our method captures the essence of the source text even more effectively."}, {"title": "3.6 Ablation Studies", "content": "Our proposed method improves upon the traditional single-step summarize-translate approach by integrating tailored prompts and employing a two-stage meta-generation process, which involves enhancing the summary and refining the translation.\nThe two additional steps, IMPROVEMENT and REFINEMENT, utilize meta-generation to optimize output and minimize error accumulation. These distinctions are particularly critical for cross-lingual summarization tasks in low-resource languages. Thus, we pose the question: How significantly do meta-generation steps impact the overall performance of LLMs on this task?\nHere, we carry out three sets of comparative experiments to demonstrate the importance of two meta-generation steps: (i) Delete the IMPROVEMENT step. (ii) Delete the REFINEMENT step. (iii) Delete both the IMPROVEMENT and REFINEMENT step.\nThe ablation experimental results across different metrics are presented in Figure 5 and Figure 6, where we compare the performance of the complete"}, {"title": "4 Related Works", "content": "Cross-lingual summarization is a critical task in natural language processing, involving the generation of a summary for text in one language based on a source text in another language (Wang et al., 2022; Zheng et al., 2022). The emergence of deep learning-based neural machine translation systems (Bahdanau et al., 2016; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) and text summarization approaches (Shini and Kumar, 2021; Nallapati et al., 2016; Shi et al., 2018), particularly those leveraging recurrent neural networks (Schuster and Paliwal, 1997; Chung et al., 2014; Hochreiter and Schmidhuber, 1997), enhanced model performance on CLS tasks.\nLater, advances in neural network technologies, especially the Transformer architecture (Vaswani et al., 2017), have led to the development of end-to-end CLS models that integrate translation and summarization into a single framework, improving overall performance. Recent years, large language models have experienced a period of rapid development and widespread adoption (Ouyang et al., 2022; Brown et al., 2020; Touvron et al., 2023), and they have gained attention for their potential in cross-lingual summarization. Wang et al. (2023) showed their strong capabilities in high-resource languages like Chinese and German, while Park et al. (2024) found that LLMs using few-shot approaches still struggle with low-resource languages. This investigation is crucial for understanding and improving the models' ability to produce accurate and coherent summaries across various languages, thereby expanding the scope and applicability of LLMs in the CLS domain."}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we introduce a four-step zero-shot SITR architecture, demonstrating the potential of LLMs for cross-lingual summarization in low-resource languages. Our approach enables LLMs to outperform three baseline types across various metrics, achieving notable performance in this domain.\nWe apply our SITR method to evaluate a wide range of LLMs, revealing their strong performance in cross-lingual summarization for low-resource languages and further demonstrating the robustness of our approach.\nFor future research, we plan to investigate more effective methodologies to further unlock the potential of LLMs in this domain."}, {"title": "Limitations", "content": "While we evaluate the performance of LLMs in cross-lingual summarization on two datasets to showcase the effectiveness of both our zero-shot SITR method and the models, this study has several limitations: (i) The design of prompts can affect model performance, partly due to the models' limited confidence with low-resource languages. Future research could explore methods to enable large language models to generate reliable outputs without depending on manually designed prompts. (ii) We do not examine cross-lingual summarization tasks involving two low-resource languages. Future work could address this gap to fully explore the potential of LLMs in these more challenging scenarios."}]}