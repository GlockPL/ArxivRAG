{"title": "Frequency-Masked Embedding Inference: A Non-Contrastive Approach for Time Series Representation Learning", "authors": ["En Fu", "Yanyan Hu"], "abstract": "Contrastive learning underpins most current self-supervised time series representation methods. The strategy for constructing positive and negative sample pairs significantly affects the final representation quality. However, due to the continuous nature of time series semantics, the modeling approach of contrastive learning struggles to accommodate the characteristics of time series data. This results in issues such as difficulties in constructing hard negative samples and the potential introduction of inappropriate biases during positive sample construction. Although some recent works have developed several scientific strategies for constructing positive and negative sample pairs with improved effectiveness, they remain constrained by the contrastive learning framework. To fundamentally overcome the limitations of contrastive learning, this paper introduces Frequency-masked Embedding Inference (FEI), a novel non-contrastive method that completely eliminates the need for positive and negative samples. The proposed FEI constructs 2 inference branches based on a prompting strategy: 1) Using frequency masking as prompts to infer the embedding representation of the target series with missing frequency bands in the embedding space, and 2) Using the target series as prompts to infer its frequency masking embedding. In this way, FEI enables continuous semantic relationship modeling for time series. Experiments on 8 widely used time series datasets for classification and regression tasks, using linear evaluation and end-to-end fine-tuning, show that FEI significantly outperforms existing contrastive-based methods in terms of generalization. This study provides new insights into self-supervised representation learning for time series.", "sections": [{"title": "Introduction", "content": "Time series data is crucial in various industries' production processes and economic activities, serving as a foundational data format. Effective representation methods are essential for building highly transferable and generalizable pattern recognition modules, significantly reducing the optimization difficulty of deep models on small sample datasets. This has become a consensus in the deep learning community, particularly in the fields of computer vision (CV) and natural language processing (NLP).\nHowever, in the field of time series analysis, self-supervised representation learning has not yet become to a community standard due to the insufficient generalization performance. Contrastive learning strategies have shown certain advantages and have become the foundational framework for much research in recent years. Contrastive learning optimizes models by constructing positive and negative samples for anchor sample. Positive samples are created using data augmentation techniques to provide different views under similar semantics to the anchor sample, while negative samples are selected or constructed to have opposing semantics to the anchor sample. The main issue with this framework is the difficulty in defining appropriate positive and negative samples for time series data.\nDue to the inherent continuity of time series, key characteristics such as trends and frequencies change continuously and cannot be fully enumerated. This makes the boundaries between semantics in time series less clear compared to other data formats. For example, it is easy to define a picture of a cat and a picture of a dog as opposites, but it is challenging to define whether a series with a 7-day cycle and that with a 6.5-day cycle are similar or opposite. They have differences but are not fundamentally opposed. Contrastive learning uses a discrete modeling approach to define absolute opposite semantics for all samples, which contradicts the inherent continuity of time series.\nSome methods have developed more reasonable and flexible strategies for constructing positive and negative samples to mitigate the issues mentioned above. However, they still operate within the discrete modeling framework of contrastive learning, failing to address the root cause of the problem. Therefore, this study aims to define the semantic differences in time series in a completely new way.\nIn this paper, a novel non-contrastive time series representation learning framework, Frequency-masked Embed-"}, {"title": "Frequency-masked Embedding Inference", "content": "ding Inference(FEI), is proposed. Inspired by the successful application of Joint-Embedding Predictive Architecture (JEPA) in CV, FEI establishes continuous relationships between different semantics in time series through the concept of embedding inference, constructing a continuous embedding space sensitive to frequency variations. FEI infers specific frequency sample directly in the embedding space using frequency masking prompts. Unlike contrastive learning, which models distinctions between semantics, FEI focuses on modeling the relationships between different semantics, achieving continuous modeling through a prompting strategy.\nOverall, the contributions of this study include:\n\u2022 This paper proposes FEI, a novel self-supervised representation learning framework for time series that eliminates the need for positive and negative sample pairs. Using frequency masking prompts, FEI performs embedding inference of different frequency bands in the embedding space, enabling continuous semantic modeling.\n\u2022 We validate the quality of the representation through linear evaluation and end-to-end fine-tuning experiments. The proposed FEI achieves new state-of-the-art performance across 8 benchmark datasets for classification and regression tasks requiring high-level representations.\n\u2022 This study demonstrates the feasibility and effectiveness of non-contrastive learning for time series self-supervised representation learning, providing new insights for further research in this area."}, {"title": "Related Works", "content": "Time series representation learning. In recent years, contrastive learning has become a fundamental paradigm for time series high-level representation learning. For example, in addition to learning series reconstruction representations in the embedding space, SimMTM also uses contrastive loss to constrain the representation distance between positive and negative sample pairs. Time-DRL uses CLS tokens to obtain high-level representations and generates positive and negative sample pairs at different stages of subspace mapping through gradient truncation to guide the high-level representation optimization of CLS tokens. COMET constructs multi-level contrast constraints to achieve a stable high-level representation learning process. Due to the discrete modeling nature of the contrastive learning framework, the construction of positive and negative sample pairs can significantly impact the optimization results. Inappropriate pairing frequently occur in some self-supervised representation learning approaches based on contrastive learning. For this issue, TS2Vec proposing an enhanced contextual construction strategy to eliminate improper priors in positive and negative sample pairs. TimesURL and TF-C both construct positive and negative samples from a frequency domain perspective; the former mixes the spectra of multiple samples to generate new augmented samples, while the latter constructs augmented samples through spectral masking and enhancement. In addition, both Literature and"}, {"title": "Frequency-masked Embedding Inference", "content": "The main architecture of the proposed FEI is illustrated in Figure 1. The core architecture of FEI consists of 2 sets of inference branches: target embedding inference based on mask prompts and mask inference based on target embedding prompts. The overall pre-training objective is to obtain a universal time series encoder fe."}, {"title": "Original Encoder", "content": "Given the original time series x \u2208 R with length 1, the encoder fe generates an embedding vector e \u2208 Rd for x, where d represents the embedding dimension of fe. This process does not impose any restrictions on the specific structure of encoder fe. After obtaining the embedding vector, a projector s\u03b8 serves as a relaxation factor to reduce the training complexity of the subsequent inference process, is used to obtain the subspace embedding u \u2208 Rh, where h = d/2. In our implementation, s\u03b8 is implemented by a single linear layer."}, {"title": "Momentum Encoder", "content": "As FEI lacks explicit training constraints like those in contrastive learning architectures, directly performing embedding inference based on the single encoder may lead to representation collapse. A momentum encoder is an effective means to prevent this issue. Therefore, FEI employs smoothly updated copies, fer and s'\u03b8, of the original encoder fe and projector s\u03b8 as the target series encoder. These copies are updated using an exponential moving average and do not directly participate in gradient calculation. The update process is as follows:\n$\\theta_{t}=\\alpha * \\theta_{t-1}+(1-\\alpha) * \\theta_{e}$ (1)\nwhere \u03b8t represents the weights of fe after the t-th gradient descent, \u03b8 represents the weights of momentum encoder fer, and \u03b8% = \u03b8o. The same applies to subspace projector s' and s. Similarly, the target series embedding u' can be generated by fer and 81."}, {"title": "Frequency Masking", "content": "To obtain the target series x' corresponding to the original series x while preserving its continuity, random frequency masking is employed. The original series x undergoes a Fast Fourier Transform (FFT) to obtain its amplitude spectrum. A random mask M \u2208 {0,1}n with k mask positions is then generated, where n = [1] + 1. The masked frequency component positions are marked as 1, and the unmasked positions are marked as 0. This mask is applied to the amplitude spectrum of the original series, covering some of the frequency components, and the Inverse Fourier Transform (IFT) is then used to obtain the target time series x' with masked frequencies.\nIt is important to note that the generation process of the frequency mask involves 2 steps. First, the number k of masked frequency components is a random variable, with the masking ratio following a uniform distribution U(\u03b2\u03b9, \u03b22), 0 \u2264 \u03b2\u2081 < \u03b22 < 1. This means that each masking operation covers B\u2081 to \u1e9e2 of the total frequency components of the original series, providing sufficiently rich variation semantics during training. Then, k frequency components are"}, {"title": "Mask Encoder", "content": "To convert the frequency mask M \u2208 {0,1}n into a embedding vector that can be used as an embedding space prompt, we design a dedicated mask encoder to generate the mask embedding m that adheres to the following principles: when no frequency components are masked, i.e., M = {0}n, the mask embedding m should also be 0. Additionally, the mean and variance of m should remain relatively stable as k varies. The mask encoder proposed is as follows:\n$m = g(M) = \\frac{M W_{emb}}{\\sqrt{k}}$ (2)\nwhere Wemb = {W1,...,Wn}T \u2208 Rn\u00d7h constructs an embedding matrix, with each wi representing an embedding vector for a frequency component. Wemb is randomly initialized from a normal distribution N(0, 1) and updated during training."}, {"title": "Embedding Inference", "content": "The embedding inference part consists of 2 branches: the target embedding inference and the mask inference. Both branches use 1-layer MLP-based predictors, but differ in their prompt embeddings and merging methods. When inferring the target embedding, the mask embedding m used to generate the target series is employed. Conversely, when inferring the mask embedding, the target embedding u' is utilized. The computational process for this section is as follows:\n$\\hat{u'} = z_{\\theta_1}(u + D(m))$ (3)\n$\\hat{m} = z_{\\psi_2} (D(u') - u')$ (4)\nwhere D(\u00b7) represents the gradient detaching, \u00fb' is the inferred target embedding based on mask prompting, and m indicates the inferred mask embedding based on the target embedding. Gradient detaching is used to force the model to focus on different optimization aspects during the two inference processes. In the process of inferring the target embedding, the gradient calculation of the mask embedding m is detached, and thus the mask encoder g is not optimized. The encoder fe is driven to obtain better original embedding u to complete the inference. Conversely, in the process of inferring the mask embedding, the gradient calculation of the original embedding u is detached, and only the mask encoder is optimized to obtain a better mask embedding. This design clarifies the optimization objectives of the 2 branches and avoiding gradient conflicts during the training process.\nThe design of the embedding inference is the core module for the effectiveness of FEI and embodies the original intention behind FEI's design. Firstly, we aim for the model to infer any changes in the series's frequency band within the embedding space using appropriate prompts. Secondly,"}, {"title": "Loss Function", "content": "Based on the embedding inference results, the training loss is simply computed using the L2 distance between the inferred target embedding \u00fb' and true target embedding u', as well as between the inferred mask embedding m and the true mask embedding m, as follows:\n$\\mathcal{L} = \\frac{1}{N}\\sum_{i=1}^{N}||u_i - \\hat{u'_i}||^2 + ||m_i - \\hat{m_i}||^2$ (5)"}, {"title": "Experiments", "content": "This section reports the transfer performance of FEI on classification and regression downstream tasks, along with an ablation study analyzing the effectiveness of the FEI design. The full experiment results and more details can be found in Appendix."}, {"title": "Dataset", "content": "The dataset used in the experimental section is shown in Table 1. For the pre-training phase, we use the commonly utilized SLeepEEG dataset, which provides ample samples and is widely used for pre-training in various transfer learning methods. For the downstream tasks, 8 publicly available datasets are utilized: 1) Gesture, 2) FD-B, 3) EMG, 4) EPI, 5) HAR, 6) 128 UCR, 7) C-MAPSS, and 8) Bearing. The first 6 datasets are commonly used for classification tasks, while the last 2 are typically used for regression tasks."}, {"title": "Baselines", "content": "To fully verify the representation advantages of FEI, we selected the state-of-the-art methods from recent years as base-"}, {"title": "Pre-training Setup", "content": "The primary significance of representation learning is to enable the pre-trained model to be applicable to as many unknown downstream tasks as possible. Therefore, our experimental process fully adheres to this principle: after completing pre-training, the encoder is directly transferred to different downstream datasets without any additional pre-training or hyperparameter adjustments for each dataset. A single linear layer is used as the task-specific output layer. This approach fully verifies the differences in the representation quality of each method.\nThe basic setup of proposed FEI for the pre-training process follows SimMTM and TF-C. Additional configurations can be found in the Appendix."}, {"title": "Task 1: Classification", "content": "Setup. Time series classification is a key benchmark for evaluating representation learning algorithms. We perform linear evaluation on 6 commonly used datasets and conduct end-to-end fine-tuning on 5 datasets with limited training samples to validate the generalization capabilities of each method.\nIn the linear evaluation process, we freeze the encoder, and optimize only a linear classifier, with a maximum training iteration of 300 and an initial learning rate of 1e-4. In the end-to-end fine-tuning process, both the encoder and the classifier are optimized, with a maximum iteration of 100 and a smaller learning rate of le-5 to prevent overfitting. For both linear evaluation and end-to-end fine-tuning, all baseline encoders use the same hyperparameters. The model with the lowest validation loss is used as the final test model by early stopping for all datasets except for the 128 UCR dataset, which does not have a validation set division, so all models are tested directly after training ends.\nWe evaluate the accuracy, precision, recall, and F1 score of each method on classification tasks, with the accuracy results of linear evaluation shown in Table 2 and the fine-tuning results shown in Table 3.\nIn these tables, \"Rand. Init.\" represents the performance of a randomly initialized encoder used directly for downstream tasks without pre-training."}, {"title": "Task 2: Regression", "content": "Setup. Regression tasks are a classic type of time series task. In this paper, we use 2 equipment health status analysis datasets, C-MAPSS and Bearing, to analyze the transfer performance of FEI in regression tasks. These datasets consist of 4 and 3 sub-datasets, respectively, each using input signal to regress the Remaining Useful Life (RUL) ratio of the equipment. These datasets are commonly used for time series regression tasks in equipment health status analysis. Moreover, the sampling frequency of these 2 datasets significantly differs from the pre-training dataset SleepEEG. The C-MAPSS dataset has a very low sampling frequency (1Hz), while the Bearing dataset has a very high sampling frequency (25.6Hz). This difference allows for a better evaluation of the model's generalization ability across different data characteristics.\nThe experimental settings for linear evaluation and end-"}, {"title": "Model Analysis", "content": "Ablation. To further explore the role of each module in the proposed FEI, an ablation analysis is conducted. We target the core modules of FEI for ablation, constructing 6 ablation models:\n\u2022 w/o emb. infer., which removes the target embedding inference branch, retaining only the mask inference.\n\u2022 w/o mask prompt, which removes the mask prompting for target series embedding inference.\n\u2022 w/o momentum, which removes the momentum encoder and directly uses the original encoder for encoding.\n\u2022 w/o subspace, which removes the subspace projector 84,8% and directly learns in the original embedding \u03c6, space.\n\u2022 w/o mask infer., which removes the mask inference branch, retaining only the target embedding inference.\n\u2022 w/o detach, which removes the gradient detachment D() in Equations (3) and (4) training.\nThe linear evaluation precision of each ablation method on the EMG dataset is shown in Table 6.\nDue to the design of the mask inference branch, removing the momentum encoder from FEI does not lead to significant representation collapse. This is because trivial solutions, where the encoder encodes all samples into the same embedding vector, cannot minimize the loss of the mask inference branch.\nAdditionally, the mask inference stabilizes the FEI training process, as shown by the loss value comparison over the first 20 epochs in Figure 2. The colored lines represent the"}, {"title": "Visualization of embedding inference", "content": "Visualization of embedding inference. Embedding inference is a core module of FEI. In this section, we select a original series sample from Gesture dataset and construct 5 representative target series using random masks. It should be noted that FEI has never been trained on this dataset. Using t-SNE, we visualize the original embedding and target embeddings after dimensionality reduction, as shown in Figure 3. The left side displays the relationship between the embeddings of the original series and the target series, with inverted triangles representing the target embeddings obtained directly using the original encoder fe and subspace projector s\u00f8, and stars representing the inferred embeddings obtained by the embedding predictor 241. The right side shows the 5 masks used to construct the target series, where the dark color represents the masked frequency component. From the figure, it can be observed that the green target series has the fewest masks, and its embedding result and inference are close to the original embedding. The red and yellow target series have masks mainly in the mid and high-frequency bands, with only a small amount in the low-frequency band, resulting in their embeddings being further from the orig-"}, {"title": "Masking Strategies", "content": "We describe 3 masking strategies used in FEI to construct target series in the main text: Discrete Frequency Masking(DFM), Continuous Frequency Masking(CFM), and Time-domain Masking(TDM), with the full results listed in Table 17."}, {"title": "Sensitivity", "content": "We have reported FEI's sensitivity to masking ratio B\u2081and B2. In addition, the momentum factor a determines the update speed of the momentum encoder, which is crucial for most methods based on momentum update strategies, such as BYOL and I-JEPA. Therefore, we further analyze the impact of the momentum update factor a on FEI. The results are shown in Figure 5.The results indicate that the optimal range for a in FEI is between 0.99 and 0.999, within which sufficient representational generalization can be achieved. Ultimately, we use \u03b1 = 0.995."}, {"title": "Visualization", "content": "FEI demonstrates good sensitivity to changes in the frequencies of unseen samples. We have already shown the FEI's inference visualization on the unseen Gesture dataset in the main text. Here, we provide additional, more detailed visualizations on the FD-B and EMG datasets, which have significant frequency differences from the pre-training dataset SleepEEG, as shown in Figures 6 and 7.\nIn the figures, \"Target Series #n\" represents the target series constructed from the original series using the frequency mask shown below. In the \"Inference Results\" section, the inverted triangle represents the true embedding of the target series, the star represents the inferred embedding of the target series obtained by FEI using the mask prompt and original series, and the circle represents the embedding of the original series. All embeddings are displayed using t-SNE dimensionality reduction.\nBoth sets of visualizations include low-frequency and high-frequency masking to varying degrees, and FEI accu-"}, {"title": "Future Work", "content": "The proposed FEI introduces a new modeling approach for self-supervised representation learning of time series. Through extensive experiments, we have validated and analyzed the effectiveness and superiority of FEI. However, advancing this field remains crucial, as developing a fully generalized temporal representation model holds significant importance for the entire time series analysis domain. Based on the current limitations of FEI and recent research progress, we offer some recommendations for future research.\nFEI successfully achieves sample-level general representation modeling and has shown significant improvements in sequence-level downstream tasks such as classification and regression. However, we have not yet explored how this architecture could be applied to finer-grained modeling at the time-step level. This could be valuable for certain downstream tasks like point-to-point anomaly detection or stepwise time series prediction. Our findings suggest that continuous semantic modeling at the step level is beneficial for obtaining generalizable time series representation models. This leads to our first suggestion for future work: exploring continuous semantic modeling frameworks at the time-step level, which may not be limited to the FEI architecture.\nThe inherent diversity of time series data, which can describe a wide range of objects, results in significant differences in key features such as trends, cycles, and noise levels between different series. Thus, learning universal representations for time series remains a highly challenging field. Constructing a comprehensive, large-scale time series corpus that covers all possible objects of description is extremely difficult. This challenge also differentiates the design of self-supervised learning algorithms in this field from those in CV and NLP, which benefit from vast amounts of data. The frequency inference approach of FEI offers a new perspective for training general time series representation models with limited samples. By using frequency-domain processing methods such as frequency masking, as demonstrated in this paper, it is possible to construct a large number of new samples that are temporally continuous but semantically distinct from the original samples. These differences and relationships among the constructed samples can guide self-supervised learning. Our proposed FEI utilizes embedding inference to model semantic relationships, representing a practical application of this idea. Moving forward, we believe it is possible to develop representation learning models in the time series domain that achieve sufficient representational power with fewer pretraining samples. Therefore, a second potential research direction is to explore frameworks for mining semantic relationships within time series, which can train general representation models based on a large number of constructible"}, {"title": "Dataset Details", "content": "The details of the datasets used in the experiments are as follows:\n\u2022 SleepEEG: The SleepEEG dataset consists of 153 whole-night sleep electroencephalogram (EEG) recordings, monitored using sleep cassette tapes. The data comes from 82 healthy subjects, with EEG signals sampled at 100 Hz. Each sample is associated with one of five sleep patterns/stages: Wakefulness (W), Non-Rapid Eye Movement (N1, N2, N3), and Rapid Eye Movement (REM). This dataset includes a mix of high- and low-frequency patterns and has been used in several studies as a pre-training dataset. We used the pre-processed version of the dataset, which was released by the TF-C researchers.\n\u2022 Gesture: The Gesture dataset contains accelerometer measurements for 8 simple gestures, each varying based on the path of hand movement. The eight gestures include sliding the hand left, right, up, and down, waving in a clockwise or counterclockwise direction, waving in a square pattern, and waving in the shape of a right arrow. The classification labels correspond to these 8 different types of gestures. This dataset originates from the UCR dataset, and TF-C researchers have merged and cleaned it, making it widely used as a standalone small-sample dataset.\n\u2022 FD-B: FD-B is a subset extracted from the FD dataset under condition B, which was collected from an electromechanical drive system that monitors the condition of rolling bearings and detects any damage. The data collected under different conditions is divided into 4 subsets, with parameters including rotational speed, load torque, and radial force. Each rolling bearing can be categorized into one of three classes: undamaged, internally damaged, and externally damaged. We used the preprocessed data provided by the TF-C researchers.\n\u2022 EMG: Electromyography measures the electrical activity of muscles in response to nerve stimulation and can be used to diagnose certain muscular dystrophies and neuropathies. The EMG dataset consists of single-channel EMG recordings from the tibialis anterior muscle of three volunteers: one healthy, one with neuropathy, and one with myopathy. The data is sampled at a frequency of 4 kHz. Each patient, corresponding to their condition, represents a distinct classification category, with a total of three classes. We used the preprocessed data provided by the TF-C researchers.\n\u2022 EPI: The Epilepsy dataset contains single-channel EEG measurements from 500 subjects. For each subject, brain activity was recorded for 23.6 seconds. The dataset was then divided and shuffled into 11,500 samples, each 1 second long, sampled at a frequency of 178 Hz. There are a total of 11,500 EEG samples classified into 2 categories, corresponding to epilepsy patients and normal patients. We also used the preprocessed data provided by the TF-C researchers."}]}