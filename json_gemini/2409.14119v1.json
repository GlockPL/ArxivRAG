{"title": "Obliviate: Neutralizing Task-agnostic Backdoors within the Parameter-efficient Fine-tuning Paradigm", "authors": ["Jaehan Kim", "Minkyoo Song", "Seung Ho Na", "Seungwon Shin"], "abstract": "Parameter-efficient fine-tuning (PEFT) has become a key training strategy for large language models. However, its reliance on fewer trainable parameters poses security risks, such as task-agnostic backdoors. Despite their severe impact on a wide range of tasks, there is no practical defense solution available that effectively counters task-agnostic backdoors within the context of PEFT. In this study, we introduce Obliviate, a PEFT-integrable backdoor defense. We develop two techniques aimed at amplifying benign neurons within PEFT layers and penalizing the influence of trigger tokens. Our evaluations across three major PEFT architectures show that our method can significantly reduce the attack success rate of the state-of-the-art task-agnostic backdoors (83.6%). Furthermore, our method exhibits robust defense capabilities against both task-specific backdoors and adaptive attacks.", "sections": [{"title": "1 Introduction", "content": "As large language models (LLMs) have evolved with an increasing number of parameters, parameter-efficient fine-tuning (PEFT) has been emerged as a new paradigm for efficiently adapting LLMs to downstream tasks. Unlike full fine-tuning, PEFT updates only a minimal number of extra parameters while freezing the parameters of the pre-trained language models (PLMs). Adapter (Houlsby et al., 2019), LoRA (Hu et al., 2021), and prefix-tuning (Li and Liang, 2021) are fundamental PEFT architectures. PEFT attains comparable performance to full fine-tuning while offering highly efficient downstream adaptation.\nRecent works have explored the security implications of PEFT (Hong and Wang, 2023). For example, attackers inject backdoors into PLMs, and then activate the attacks on the final PEFT models. One of the most severe attacks on PEFT is task-agnostic backdoors, which manipulates the output representations of PLMs aiming to harm fine-tuned models for arbitrary downstream tasks. (Shen et al., 2021; Chen et al., 2021a; Zhang et al., 2023; Du et al., 2023). This type of attack is less prone to forgetting backdoors when fine-tuning since PEFT freezes backdoored parameters of PLMs and updates only a minimal set of added parameters. Furthermore, the ability to adapt models to multiple downstream tasks magnifies the risk of task-agnostic backdoors.\nTo mitigate LLM backdoors, several defense techniques have been proposed, such as detecting poisoned samples (Qi et al., 2021a), inverting trigger-like inputs (Liu et al., 2022b), and purifying backdoored models (Zhu et al., 2023). Existing defense methods are designed mainly upon the full fine-tuning process. In PEFT, however, there is difficulty in adopting such defenses due to the limited trainable parameters. PSIM (Zhao et al., 2024) attempts to detect poisoned samples to defend PEFT. However, it requires a task-specific auxiliary model, which harms the modular and memory-efficient nature of PEFT. Notably, defense against task-agnostic backdoor attacks has been understudied despite their alarming threats on PEFT. LMSanitator (Wei et al., 2024) aims to remove task-agnostic backdoors in prompt-tuning, not applicable to other PEFT architectures.\nIn this work, we propose Obliviate, a defense method to neutralize task-agnostic backdoors, highly integrable to the standard PEFT process. Our approach includes two main techniques: 1) We amplify benign neurons within PEFT layers to encourage the model to focus more on clean training samples. This method can relatively reduce the influence of backdoored neurons in the PLMs. 2) We regularize the attention scores to penalize the influence of trigger tokens that exhibit abnormally high attention scores. To implement these techniques, we add two loss terms to the PEFT process for downstream tasks. Defenders can easily"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Parameter-efficient Fine-tuning", "content": "Parameter-efficient fine-tuning (PEFT) is an efficient strategy to adapt pre-trained language models (PLMs) to multiple downstream tasks (He et al., 2021). Different from full fine-tuning, it updates only a small number of extra parameters while keeping the PLM's weights frozen. PEFT significantly reduces the computational cost and memory footprint during the training and inference processes of large language model (LLM).\nAdapter-tuning (Houlsby et al., 2019; Pfeiffer et al., 2020) adds small layers called adapter between PLM networks (e.g., transformers). LORA (Hu et al., 2021) employs rank decomposition matrices, reducing the storage and computation costs. Prefix-tuning (Li and Liang, 2021) prepends extra tokens in the input and hidden layers of PLMs. Similarly, prompt-tuning (Lester et al., 2021) and its variants (Liu et al., 2022a, 2023) insert trainable prompts to PLMs. While achieving comparable performance to full fine-tuning, PEFT offers the mitigation of catastrophic forgetting (Pfeiffer et al., 2020) and a robust out-of-distribution adaptation (Li and Liang, 2021)."}, {"title": "2.2 Backdoor Attacks on PLMs", "content": "The backdoor attacks pose severe threats in the NLP domain, especially targeting LLMs (Dai et al., 2019; Kurita et al., 2020; Chen et al., 2021b; Yan et al., 2023). Attackers compromise target models to misclassify the text inputs with textual triggers while properly working on the clean samples.\nAlongside the pre-training and fine-tuning approach of LLMs, injecting backdoors into PLMs (i.e., weight-poisoning) has emerged as a primary strategy in realistic scenarios (Kurita et al., 2020; Wang et al., 2020; Li et al., 2021). Particularly, task-agnostic backdoor is one of the most severe attacks on PLMs. Even without any knowledge of the fine-tuning process, it aims to broadly target various downstream tasks. POR (Shen et al., 2021) and NeuBA (Zhang et al., 2023) rely on forcing the output representations, such as the [CLS] token's output, to be pre-defined vectors when the inputs contain the triggers. BadPre (Chen et al., 2021a) leverages an adversarial masked language modeling (MLM). Although its direct focus is not the [CLS] token, this attack demonstrates considerable effectiveness in impacting classification tasks (Zhu et al., 2023). UOR (Du et al., 2023) optimizes outputs of poisoned samples via contrastive learning, rather than utilizing fixed vectors, to make them stray from the feature space of correct labels.\nMore recently, the implications of backdoored PLMs on PEFT have raised concerns (Hong and Wang, 2023; Gu et al., 2023; Zhao et al., 2024). Notably, task-agnostic backdoor is particularly fatal for PEFT because: 1) PEFT freezes all the backdoored parameters of the PLMs, so that the PEFT models have difficulty in forgetting the backdoors via training the limited number of newly added parameters, 2) The primary role of PEFT is to efficiently adapt a PLM to diverse tasks. This poses a significant risk of task-agnostic backdoors, compromising multiple tasks by exploiting only a single backdoored model."}, {"title": "2.3 Backdoor Defenses", "content": "Poisoned sample detection. The traditional approach for backdoor defense is to detect poisoned samples that include triggers by observing their disparity with clean samples. STRIP (Gao et al., 2021) determines poisoned samples based on the prediction entropy of perturbed inputs. RAP (Yang et al., 2021) leverages the difference in prediction robustness between poisoned and clean samples. MDP (Xi et al., 2023) applies a perturbation-based defense to few-shot prompt learning. PSIM (Zhao et al., 2024) provides poisoned sample detection for LoRA and prompt-tuning. It rejects samples for which the model has high prediction confidence. Instead of entirely rejecting detected samples, ONION (Qi et al., 2021a) removes the triggers from a given input by measuring its perplexity. However, these methods require large computation costs due to multiple predictions for each sample. Furthermore, implementing ONION and PSIM requires (task-specific) auxiliary models, which detracts from the advantages provided by PEFT.\nTrigger inversion. The trigger inversion technique removes trigger-like embeddings from the inputs. In the NLP domain, existing methods (Wang et al., 2019; Qiao et al., 2019; Tao et al., 2022; Xu et al., 2023) suffers from the discontinuity of sentences and the sparsity of embedding spaces. T-miner (Azizi et al., 2021) is a sequence-to-sequence model for generating minimally transformed classifier inputs to induce misclassification. PICCOLO (Liu et al., 2022b) addresses the discontinuity problem by changing the subject model to a differentiable form. DBS (Shen et al., 2022) adopts a dynamically reducing temperature coefficient in the softmax function to make the optimizer focus the ground truth trigger. LMSanitator (Wei et al., 2024) shows that existing trigger inversion methods are less effective in detecting task-agnostic backdoors. To address this problem, they invert the outputs of poisoned samples rather than inverting input triggers. However, it is limited to prompt-tuning schemes that train additional embeddings, which is not generally applicable to other PEFT architectures.\nModel purification. Several researchers have made efforts to purify models to revert the misclassified results of poisoned samples. One simple solution is to fine-tune all the model parameters on sufficient clean samples, leveraging catastrophic forgetting of trigger information (Shen et al., 2021). Neuron pruning is a more promising approach, which has been largely studied in the computer vision domain (Liu et al., 2018; Wu and Wang, 2021; Zeng et al., 2021). These methods refine backdoored models by removing or penalizing neurons related to backdoors. RECIPE (Zhu et al., 2023) firstly adopts this idea to purify PLMs. Nevertheless, the neuron pruning approach is not suitable for PEFT; it directly modifies backdoored neurons of the PLMs that cannot be accessed by PEFT.\nOur approach: We propose a practical defense method highly integrable with PEFT without the need for extra predictions on each input or auxiliary model. Specifically, we add two defense loss terms to the standard PEFT process on downstream tasks. Our defense method aims to neutralize back-"}, {"title": "3 Threat model", "content": "Attackers' goal. We consider an attacker that injects backdoors into a PLM, aiming to harm any of its derived fine-tuned models. The attack scenarios is illustrated in Figure 1. Notably, the attacker is unaware of the downstream tasks and has no access to the training datasets and the trainable parameters in PEFT layers. Therefore, the attacker adopts task-agnostic backdoors, which manipulate the PLM outputs to be adversarial representations that compromise arbitrary downstream tasks. The attacker uploads the backdoored PLM on model repositories such as HuggingFace (Wolf et al., 2020). In the inference time, the attacker is able to control the fine-tuned model to misclassify the testing samples' labels by inserting a specific trigger into them. These poisoned samples will be mapped to a specific label l even though their true labels are not l. We note that the fine-tuned model is expected to perform accurately on clean samples at a similar level as a PEFT model built upon a benign PLM.\nDefense setting. In practice, a user/defender builds an LLM for the downstream task by downloading a PLM from the model repository and then fine-tuning it on the clean dataset, as described in Figure 1. The defender may use PEFT for modularity and resource efficiency. The defender freezes the PLM parameters and updates only parameters in the PEFT layers, which are randomly initialized (i.e., not backdoored). Despite the PLM potentially being backdoored, the defender entirely has no knowledge about the attacks, including the attacker's datasets and injected triggers. In this context, the defender's goal is to neutralize the backdoors within the PLM, ensuring accurate prediction of the true label in the downstream task, regardless of whether the sample contains triggers."}, {"title": "4 Methodology", "content": ""}, {"title": "4.1 Design Intuition", "content": "Natural backdoor forgetting. Even though fine-tuning with clean samples is a fundamental defense strategy, PEFT shows challenges in forgetting backdoors effectively (Hong and Wang, 2023). To illustrate the differences between PEFT and full fine-tuning, we present an example of backdoored models in Figure 2. PEFT is limited to a small number of trainable parameters. Therefore, it struggles to eliminate the backdoors, resulting in an output that is still similar to the adversarial representation. In contrast, the fully fine-tuned model alters its outputs significantly, enabling correct prediction of the true label. The quantity of neurons trained on clean samples is important to separate model outputs from the adversarial representations.\nAttention on triggers. The attention mechanism lies at the core of the transformer architecture, serving a critical role in linking model outputs with the importance of each input token. For instance, when a model is backdoored by the POR attack, trigger tokens exhibit significantly higher attention scores toward the [CLS] output compared to non-trigger tokens (Shen et al., 2021). Our preliminary experiment confirms that this pattern is consistent across various task-agnostic backdoors, as illustrated in Figure 3 (ROBERTa) and Figure 6 (BERT). Consequently, the distribution of attention scores could be a crucial indicator for detecting triggers within poisoned inputs. However, it is noteworthy that these distinctive features of attention scores vary across different transformer layers and input texts."}, {"title": "4.2 Obliviate Details", "content": "Based on these intuitions, we aim to protect PEFT models fine-tuned from backdoored PLMs. To this end, we design two specialized loss functions to mitigate the influence of backdoored in the PLMs.\nBenign neuron amplification. Given the constraints on increasing trainable parameters in PEFT, we enhance the influence of neurons in PEFT layers to neutralize backdoors in PLMs. Our method is to amplify the magnitudes of these small yet benign parameters, relatively undermining the effectiveness of the PLM's backdoored neurons. This is inspired by neuron amplification approaches (Yu et al., 2023; Zhu et al., 2024), which involve scaling up neurons important to specific tasks (e.g., classification task on a clean dataset).\nWe formulate the neuron amplification approach as a specific loss function $L_{amp}$, called neuron amplification loss. This loss function is optimized to increase the $L_2$-norm of weights in the PEFT layers, represented as:\n$L_{amp} = - \\sum_{l \\in L} \\sum_{P_i \\in P} ||W_P||_2$, (1)\nwhere L denotes all the transformer layers, $P_i$ is the group of PEFT layers in the $i$th transformer layer, $W_P$ is the weights of each individual PEFT layer, and $||\\cdot||_2$ refers to the $L_2$-norm. Specifically, we amplify the up- and down-projection matrices of the adapter layers, the decomposition matrices of the LoRA layers, and the reparametrization matrices for prefix-tuning.\nAttention score regularization. Our observation has shown that the attention scores are effective indicators for identifying triggers. One straightforward method could be to remove tokens that exhibit high attention scores using a threshold. However, this often leads to a significant decrease in CACC, as shown in our pilot experiment in Appendix B. Therefore, we reduce the triggers' attention scores through an optimization process, rather than eliminating them from the inputs. To this end, we introduce the attention regularization loss $L_{reg}$ to decrease the $L_2$-norm of attention scores, thereby"}, {"title": "penalizing excessively high values among them:", "content": "$L_{reg} = \\sum_{l \\in L} \\sum_{h \\in H_i} ||a_h||_2$, (2)\nwhere $H_i$ denotes the set of attention heads in the $i$th transformer layer, $a_h$ represents the attention scores for each head, and the remaining notations are consistent with those used in Equation (1). Specifically, we focus on the attentions corresponding to certain output vectors. For sentence classification, we regularize the attention scores of input tokens on the [CLS] output. Although the training process involves only clean samples, this approach effectively reduces the influence of trigger tokens while preserving the original context information.\nDefense loss and training. We incorporate the two defense loss terms into the standard PEFT process. The final objective of the training is formulated as:\n$L = L_{task} + \\lambda_{amp} \\cdot L_{amp} + \\lambda_{reg} \\cdot L_{reg}$, (3)\nwhere $L_{task}$ denotes the downstream task loss. $\\lambda_{amp}$ and $\\lambda_{reg}$ are hyperparameters for balancing the loss terms. This strategy ensures that the model preserve its performance on clean samples. We note that our defense method does not necessitate extra predictions or an auxiliary model, thereby maintaining the nature of the PEFT approach."}, {"title": "5 Evaluation", "content": ""}, {"title": "5.1 Experimental Settings\u00b9", "content": ""}, {"title": "5.1.1 Backdoor attacks and victim PLMs", "content": "We examine the effectiveness of our defense method against the state-of-the-art task-agnostic backdoor attacks: POR, NeuBA, BadPre, and UOR. We select six triggers: [\u2018cf', 'mn', \u2018tq', 'qt', 'mm', 'pt']. We conduct the attacks on two victim PLMS, ROBERTa (roberta-base) (Liu et al., 2019) and BERT (bert-base-uncased) (Devlin et al., 2019)."}, {"title": "5.1.2 Downstream task datasets", "content": "We use three classification datasets, SST-2 (Socher et al., 2013), AG News (Zhang et al., 2015), and Hate Speech and Offensive Language (HSOL) (Davidson et al., 2017)."}, {"title": "5.1.3 Metrics", "content": "Clean accuracy. We present the clean accuracy (CACC) of backdoored models and defended models to verify that our defense method has minimal impact on the prediction for clean samples."}, {"title": "5.1.4 Defense setup", "content": "In line with the threat model in Section 3, we perform PEFT on backdoored PLMs by adding either adapter, LoRA, or prefix-tuning layers into the PLMs. During the training process, only the parameters of these PEFT layers are updated while keeping those of the PLMs frozen. We adopt the default hyperparameters for PEFT and select the largest $\\lambda_{amp}$ and $\\lambda_{reg}$ that exhibit no more than a 2% drop in the CACC on the validation set."}, {"title": "5.1.5 Baselines", "content": "w/o defense. We train the backdoored PLMs on the downstream tasks using the PEFT approach, without any defense method.\nONION (Qi et al., 2021a). This defense method removes triggers from an input by identifying outlier words that reduce its perplexity. GPT-2 is used to measure the perplexity of a given test input. The suspicion score threshold is determined by permitting a 2% drop in the CACC on the validation set.\nRAP (Yang et al., 2021). This backdoor defense leverages the robustness of prediction probabilities to identify poisoned samples. We train the PEFT models on the validation set to construct the defended models. We choose a threshold d to allow a 5% of false rejection rate (FRR) on clean samples.\nPSIM (Zhao et al., 2024). PSIM identifies and rejects poisoned samples by focusing on those with abnormally high output confidences. We train the auxiliary model on each downstream task using the reset labels. We select the threshold by allowing a 2% drop in the CACC on the validation set.\nIn assessing RAP and PSIM, which are poisoned sample detection approaches, we consider an attack fails if a poisoned sample is successfully detected."}, {"title": "5.2 Defense Performance", "content": "The experimental results for defending RoBERTa models against three backdoor attacks are illustrated in Table 1. Our defense method, Obliviate, effectively mitigates all the backdoors across various PEFT architectures, with the constraint of training only a minimal number of parameters. Especially, the LoRA layers account for just 0.47% of the the total parameters of RoBERTa. We achieve a considerable reduction in average ASR (83.6%\u2193) with only a minor impact on CACC (0.78%\u2193). Furthermore, our method shows significant reductions in MASR across all cases (93.3%\u2193), successfully neutralizing even the most effective triggers that can be selected by attackers. The defense is more effective in multiclass classification tasks such as AG News and HSOL than in SST-2, which is a binary classification task. We also verify the effectiveness of our defense method across natural language inference (NLI), named entity recognition (NER), and question and answering (QA) tasks, with detailed results illustrated in Appendix D. Additionally, the experimental results for BERT models are provided in Appendix E.\nIn comparison, the ONION approach demonstrates efficacy in mitigating task-agnostic backdoor attacks, especially on the AG News task. Nonetheless, it falls short of achieving the per-"}, {"title": "5.3 Output Representation Analysis", "content": "We evaluate the effectiveness of our defense method in separating the outputs of PEFT models from the backdoors' adversarial representations. This analysis focuses on three distinct PEFT models: the benign model using the benign PLM, the backdoored model, and the backdoored model with our defense method. We measure how closely the output from each model resembles a specific adversarial representation, as shown in Figure 4. For POR and NeuBA, we consider the pre-defined vectors as adversarial representations. For BadPre and UOR, we utilize each backdoored PLM's output.\nThe outputs from the backdoored models are highly similar to adversarial representations, especially in the upper transformer layers. When applying our defense method, the outputs' similarity to the adversarial representations is decreased to the same level as those from the benign models. Such decrease is especially noticeable for POR, NeuBA, and UOR, which specifically target the [CLS] tokens. These results demonstrate that our method successfully alters the output representations to eliminate adversarial traces at all the layers."}, {"title": "5.4 Robustness of Defense Method", "content": "The defender considers that PLMs have potentially been compromised by task-agnostic backdoors. However, in real-world situations, defenders"}, {"title": "5.4.1 Effects on benign PLMs", "content": "While defenders are not certain that PLMs are actually backdoored, implementing a defense strategy on benign PLMs could negatively affect their performance on downstream tasks. We evaluate the impacts of our defense method on PEFT models derived from benign PLMs, as described in Table 2.\nIn comparing the PEFT models, with or without the defense, we discover that the negative impact on CACC is minial. This is because the involvement of the downstream task loss in Equation 3 helps to preserve the performance of the benign model. Notably, this robustness in performance is observed across different PEFT methods and PLMs. Based on these insights, defenders can confidently implement our defense method without the need for additional adjustments or validations."}, {"title": "5.4.2 Defense against task-specific attacks", "content": "Defenders may develop PEFT models using PLMs that contain task-specific backdoors although these attacks are only effective when the attacker has knowledge of the downstream task. We evaluate the performance of our defense method against various task-specific backdoor attacks exploiting word triggers (Hong and Wang, 2023), syntactic structures (Qi et al., 2021c), and style transfer (Qi et al., 2021b), as shown in Table 3. Our method is particularly effective against the word-based backdoor attack, benefiting from both the benign neuron amplification and the attention score regularization techniques. Of the two techniques, the attention score regularization technique generally exhibits less significance in defending the syntactic and style backdoor attacks since it is specially designed to neutralize insertion-based triggers. Nevertheless, our method demonstrates moderate defense performance against both backdoors by amplifying the benign neurons within the PEFT layers. These results underscore the effectiveness and comprehensiveness of our approach."}, {"title": "5.4.3 Defense against adaptive attacks", "content": "Backdoor attackers may become aware of defense strategies and conduct adaptive attacks. Therefore, we assess the effectiveness of our defense method in resisting reasonable adaptive attacks. We modify the POR attack by incorporating two techniques to counter our methods: 1) amplifying the parameters of PLMs to enhance the influence of backdoored neurons, and 2) regularizing the attention scores of poisoned samples to preserve the attack effectiveness even when trigger tokens are penalized. We"}, {"title": "6 Conclusion", "content": "We propose a defense method to protect PEFT against task-agnostic backdoors embedded in PLMs. Addressing the challenges due to limited trainable parameters, we introduce two techniques aimed at amplifying benign neurons within PEFT layers and penalizing trigger tokens. These approaches allow models to focus on clean samples and forget backdoor information. Through extensive experiments, our method has proven to successfully neutralize four state-of-the-art task-agnostic backdoors across major PEFT architectures while preserving performance on clean samples. We also discover that the initialization strategy of PEFT using small weights is vulnerable to backdoors, but our defense method can mitigate this problem without any negative effects. We believe our research substantially advances the security of LLMs along the paradigm of PEFT."}, {"title": "Limitations", "content": "Our defense method has shown significant effectiveness in neutralizing task-agnostic backdoors. However, we encounter a challenge in the training. The neuron amplification loss tends to increase continuously, which prevents the optimization process from converging. Previous studies (Yu et al., 2023; Zhu et al., 2024) have indicated that neuron amplification can focus the model more intently on a specific task. Nevertheless, its training process often struggles to be completed in a strategic manner, for instance, by using early-stopping. More importantly, excessive training for neuron amplification can deteriorate the model's performance.\nTo address this issue, we adopt the default training hyperparameters of the standard PEFT process in each PEFT architecture's paper. This provides a practical defense training guideline and helps users easily adopt our method. To demonstrate the effectiveness of these strategies, we analyze the training dynamics of our defense method, as illustrated in Figure 5. Throughout the training process, the negative impact of our defense method on the downstream performance (i.e., CACC) is minimal while significantly lowering the ASR. Amplifying just a few parameters in the PEFT layers has a minor impact on the overall model performance. Notably, we can achieve effective backdoor mitigation after 10 or 20 epochs, depending on the PEFT architecture. This suggests a potential strategy of moderating neuron amplification by limiting the training to a sufficient number of epochs."}, {"title": "Ethical Considerations", "content": "In this paper, we introduce a defense method for PEFT against backdoor attacks on PLMs. Although PEFT has gained attention as an efficient LLM training strategy, its nature of limiting trainable parameters poses a significant vulnerability to backdoors embedded in the base PLMs. The malicious use of LLMs could lead to severe ethical concerns in a variety of domains. Therefore, exploring the threats of backdoor attacks and their impacts on PEFT is crucial for developing reliable LLMs. Our study has found that mitigating backdoor attacks is feasible through specialized defensive techniques that enhance benign neurons and penalize trigger tokens. This method can be seamlessly integrated into the PEFT training process, facilitating users' agile implementation of defenses. We believe that our proposed defense method will make significant contributions to addressing ethical problems related to the harmful exploitation of LLMs."}, {"title": "A Attention Score Analysis: BERT", "content": null}, {"title": "B Pilot Experiment: Attention-based Defense", "content": "The attention scores in transformer layers can be crucial evidence to detect trigger tokens in poisoned inputs (Shen et al., 2021). To design our defense method, we first conduct a pilot experiment on an attention-based defense approach. We assess the attribution-based trigger detector proposed by (Li et al., 2023), which identifies triggers based on a specific threshold by assuming they contribute most significantly to the model's predictions for poisoned samples. This evaluation focuses on the post-training attack setting where the defender has no knowledge of the poisoned samples. The results for the SST-2 and AG News tasks are illustrated in Figure 7. Although this approach reduces the ASR of backdoor attacks, its defense capability is constrained by a significant decrease in CACC due to a high rate of false positives in trigger detection. Consequently, simply removing tokens with high attention scores is not an optimal solution."}, {"title": "C Implementation Details", "content": "Backdoor attacks. We conduct experiments on four state-of-the-art task-agnostic backdoors: POR (Shen et al., 2021), NeuBA (Zhang et al., 2023), BadPre (Chen et al., 2021a), and UOR (Du et al., 2023). Specifically, the triggers that we select are ['cf', 'mn', 'tq', 'qt', 'mm', 'pt']. BadPre uses BookCorpus (Zhu et al., 2015) in the attack training, and the other methods use WikiText (Merity et al., 2016). We sample 120,000 (20,000 per trigger) instances to construct poisoned samples and use the same number of clean samples. For POR and NeuBA, we adopt six orthogonal pre-defined vectors produced by the POR-2 method. For BadPre, we replace the label with a random token in the training set.\nDownstream task datasets. We use three classification datasets, SST-2 (Socher et al., 2013), AG News (Zhang et al., 2015), and Hate Speech and Offensive Language (HSOL) (Davidson et al., 2017). The initial statistics of these datasets are shown in Table 5. For SST-2, we use 6,000 samples of the train set for training, 872 of the validation set for validation, and 1,821 of the test set for evaluation. For AG News, we use 6,000 samples of the train set for training, 2,000 samples of the train set for validation, and 2,000 samples of the test set for evaluation. For HSOL, we use 6,000 samples of the train set for training, 2,000 samples of the train set for validation, and 2,000 samples of the train set for evaluation.\nMetrics: MASR and AASR. We also measure the maximum ASR (MASR) and average ASR (AASR) proposed by (Zhu et al., 2023). Specifically, they first define ASR for each label $l \\in L$ of a trigger $t \\in T$ as $ASR_l = N_{misclassified}/N_{poisoned}$, where $L$ is a set of labels, $T$ is a set of triggers, $N_{poisoned}$ denotes the number of poisoned samples that are predicted correctly by the clean model, and $N_{misclassified}$ denotes the number of poisoned samples whose true labels are not $l$ but misclassified as $l$. The ASR for each trigger t is computed as $ASR_t = max_l[ASR_l, l \\in L]$. The MASR and AASR are defined as $MASR = max_t[ASR_t, t \\in T]$ and $AASR = E[ASR_t, t \\in T]$."}, {"title": "Defense setup.", "content": "To adopt our defense method to PEFT, we follow the common training process of adapter (Houlsby et al., 2019), LoRA (Hu et al., 2021), and prefix-tuning (Li and Liang, 2021) as provided in their work. We utilize the PEFT implementations available in AdapterHub (Pfeiffer et al., 2020). We use a batch size of 16 across all tasks. For the selection of $\\lambda_{amp}$ and $\\lambda_{reg}$ values, we select the highest values within a certain range that result in no more than a 2% drop in CACC on the validation set. The other hyperparameters are detailed in Table 6.\nBaseline: PSIM. The w/o defense model of the baselines serves as the victim model. To train the defensive model for each downstream task, we create a dataset $D_{train_{clean reset}}$ from the training set by resetting the labels. The proposed threshold $\\gamma = 0.7$ has shown to be mostly ineffective against task-agnostic backdoors. Therefore, we optimize it by selecting the smallest one from {0.52, 0.55, 0.6, 0.62, 0.65, 0.7}, permitting a 2% drop in the CACC of the victim model on the validation set. If there is no threshold satisfying this criterion, we use the default value. For the multi-class classification tasks, we adjust the threshold to $\\frac{\\gamma}{L} * 2$, where L denotes the number of labels."}, {"title": "D Defense Performance on Additional Classification Tasks", "content": "We further evaluate our defense method on several classification tasks: natural language inference (NLI) \u2013 SNLI (Bowman et al., 2015), named entity recognition (NER) \u2013 CoNLL 2003 (Sang and De Meulder, 2003), and question and answering (QA) \u2013 SQUAD (Rajpurkar et al., 2016).\nAttack settings. As the POR and NeuBA attacks target sentence classification tasks by manipulating the [CLS] output, we adapt these attacks to token classification tasks by forcing all the token outputs toward the adversarial representations. The method of the BadPre attack remains the same as that used for sentence classification tasks.\nMetrics. For the NER task, we measure task performance on clean samples using the clean F1-score (F1). Additionally, we assess attack performance by the F1-score drop (F1 drop) when triggers are inserted. For the QA task, we evaluate performance using the clean exact match (EM) and clean F1-score (F1), along with the exact match drop (EM drop) and F1-score drop (F1 drop) to measure attack performance."}, {"title": "We present the defense performance for these three classification tasks in Table 8.", "content": "For CoNLL 2003 and SQUAD, we ony compare results with ONION as RAP and PSIM are tailored to sentence classification tasks. According to the attack performance metrics, our defense method also demonstrates notable effectiveness in these"}]}