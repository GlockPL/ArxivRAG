{"title": "Reinforcement Learning from Human Feedback:\nWhose Culture, Whose Values, Whose Perspectives?", "authors": ["Kristian Gonz\u00e1lez Barman", "Simon Lohse", "Henk de Regt"], "abstract": "We argue for the epistemic and ethical advantages of pluralism in Reinforcement Learning\nfrom Human Feedback (RLHF) in the context of Large Language Models (LLM). Drawing on\nsocial epistemology and pluralist philosophy of science, we suggest ways in which RHLF can\nbe made more responsive to human needs and how we can address challenges along the way.\nThe paper concludes with an agenda for change, i.e. concrete, actionable steps to improve\nLLM development.", "sections": [{"title": "Introduction", "content": "Reinforcement learning from human feedback (RLHF) is an increasingly common technique in artificial\nintelligence (AI) where a model learns by receiving feedback from humans rather than solely relying\non a predefined reward function. This approach is particularly useful when designing Al systems for\ntasks where it is difficult to specify a precise reward function or when it is important to align the\nmodel's behaviour with certain human expectations and values. For instance, RLHF has notably\nimproved language models for context-aware text generation (Ziegler et al. 2020) and taught robots\nto navigate cluttered environments (Henry et al. 2010).\nRLHF is commonly employed in the later stages of fine-tuning models, particularly in the development\nof prominent Large Language Models (LLMs) like GPT-3.5 or GPT-4. Initially, these models undergo\ntraining using vast text corpora to grasp a broad range of language patterns and contexts. This\nfoundational training is then supplemented by task-specific fine-tuning, where the models are\nadjusted to excel in particular applications, such as understanding and generating dialogues. The\nrefinement process is then further enhanced through RLHF. The RLHF process often involves people\nevaluating and ranking potential outcomes to build a classification model. This model is designed to\npredict if a human would find a specific output acceptable. The initial model undergoes refinement\nthrough a step-by-step process where it can use this classification model for feedback, allowing for\nimprovement. This iterative process enables the model to better align with human judgement over\ntime.\nOne question that becomes significant in this context is what role the composition of the human\nfeedback group plays in the process. After all, a certain diversity in composition seems to be a clear\nadvantage\u00b9 in the development of models like ChatGPT. The feedback providers' background\nknowledge and their social, cultural and political perspectives may (or so we will argue) significantly\ninfluence the model \u2013 and a lack of diversity among individuals may lead to a model that is overly\ngeared towards only specific expectations, potentially overlooking different but also important values\nand perspectives. Accordingly, too much homogeneity may raise ethical concerns and also create\nepistemic limitations, as the model may generate outputs that are hardly broadly acceptable,\napplicable or insightful.\nDespite the apparent benefits of diversity in RLHF, the philosophical underpinnings of why and how it\nmay improve model performance have not been thoroughly explored. This paper offers an analytical\nperspective on the impact of diverse feedback on LLM development. Our investigation is motivated\nby two distinct but intertwined objectives: first, to elucidate the complex interplay between diversity,\nbalance, and reliability in the development of LLMs employing RLHF. Second, to explore challenges for\nincreasing diversity in RLHF and point to potential ways for managing these challenges.\nThe paper is structured as follows. Section 2 provides a brief overview of the landscape of RLHF and\nassociated problems, highlighting the need for diversity in feedback. Section 3 introduces ideas from\nphilosophy of science and social epistemology which can serve as useful analytic tools to understand\nand manage the identified problems. Section 4 develops initial ideas about how RLHF can be improved\nby making it more pluralistic and discusses open questions and trade-offs that need to be addressed\nto make progress along these lines. The final section concludes with a brief summary and an agenda\nfor making RLHF more pluralistic."}, {"title": "Section 2: Reinforcement learning and associated challenges.", "content": "RLHF is a useful method to align Al systems with human objectives, especially in the last stages of fine-\ntuning of state-of-the-art Large Language Models such as OpenAl's GPT-4 or Meta's LLama 3. This\ntechnique has become central in adapting Al models to complex, human-centric expectations and\npreferences. The clearest example of this was Davinci 3.5 (the first version of ChatGPT), which was far\nmore safe, \"user-friendly\u201d, and well received in terms of dialogic conversation style than GPT3.\nOriginating from revealed preference theory in economics, RLHF was adopted in machine learning for\napplications in human-computer interaction and reinforcement learning. The standard methodology\nfor RLHF was popularised in 2017 (Christiano et al. 2017) and has significantly influenced the deep\nreinforcement learning community. The advantages of RLHF are manifold. It enables humans to\ncommunicate goals to Al systems without the need for having a previously defined reward function.\nThis not only makes reward shaping implicit and intuitive, but mitigates reward hacking (Casper et al.\n2023), where an Al might exploit loopholes in a reward function to achieve high scores without actually\nfulfilling the task the reward function is a proxy for. Furthermore, RLHF facilitates a dynamic learning\nprocess where Al models can continuously evolve based on real-time human feedback, which enables\ncompanies with a large user base to improve their models with faster cycles. This ongoing interaction\nresults in Al systems that are more aligned with the values and expectations of the model's users,\nensuring their relevance and utility. Consequently, Al systems that incorporate RLHF might be better\nequipped to operate in complex moral and social landscapes, making them more trustworthy and\nreliable for a broad range of applications.\nThe process of RLHF, involves three sequential steps (Casper et al. 2023): Feedback Collection, Reward\nModeling, and Policy Optimization. Feedback Collection encompasses human assessment of model\noutputs, followed by Reward Modeling, where these assessments are used to train a reward model\nvia supervised learning aiming to replicate human evaluations. The final step, Policy Optimization,\nrefines the Al system to generate outputs positively rated by the reward model. The term 'policy' here\nrefers to the strategy or set of rules that the Al system follows to decide its actions based on the\ncurrent state.\nTo illustrate this, imagine an Al designed to write creative stories undergoing this three-step process.\nInitially, humans evaluate the stories the Al produces (e.g. based on creativity, coherence, and\nemotional impact). These evaluations guide the training of a reward model that predicts whether a\nstory is appealing to humans (i.e. whether human evaluators would give it a good score). Finally, the\nAl's story-writing process is optimised based on feedback from the reward model, enabling it to\nproduce narratives that better resonate with human readers, thus improving its ability to craft\nengaging and coherent stories.\nNote, however, that the outputs of the Al will be influenced by the preferences of the evaluators,\nwhich may lead to a narrowing of the Al's capabilities and a potential bias towards certain types of\nstories or storytelling techniques. This risk is most clearly seen when stories are intended to contain\nanswers to scientific questions, where evaluators might prefer concise and simple answers, posing a\nrisk that the Al learns to provide a simplified but (potentially) misleading answer rather than a\nscientifically adequate one (Perez et al. 2022; see also Barman et al. 2024). In its most extreme form,\nthis might involve giving not just simplified, but wrong answers altogether; for instance, the model\nmay \"hallucinate\" responses to avoid answering that it does not know as this might be rated poorly.\nWhile this result could be seen as a simple consequence of prioritising user satisfaction, it may come\nat the expense of other values such as scientific accuracy and reliability. Given these (as well as other)\npotential trade-offs, it becomes important to consider which values one should aim for.\nThe human feedback used in RLHF includes, among others, label feedback, binary preference\nfeedback, correction feedback, and language feedback. This involves tasks such as ranking outputs\n(e.g., in order of quality), choosing one output over another, providing explanations as to why a certain\noutput is not correct (or how it could be improved), or providing an example of what the right output\nshould be. Each feedback type has its distinct advantages and limitations, and there are multiple trade-\noffs at play (e.g., between the difficulty of the task, the ability of gathering enough feedback given a\ncertain budget, or the difficulty of having humans with enough skill to perform these tasks). Despite\nits widespread use, certain limitations of RLHF in fine-tuning LLMs are notable. Perez et al. (2022) point\nout that models fine-tuned using RLHF often display biases and can lead to the mirroring of certain\nideologies. El-Mhamdi et al. (2023) observe that these models could unintentionally reveal sensitive\ninformation. Furthermore, there is clear evidence of these models indeed producing hallucinated or\ninaccurate content (OpenAl et al. 2023). Moreover, RLHF doesn't protect these models against\nadversarial attacks, including jailbreaking\u00b2 or prompt injection/extraction\u00b3 (Li et al. 2023). The\nresponse behaviour of these models can also be undesirable in several other ways.\nThese problems are too multifaceted to address them in one paper and require different types of\nsolutions. In the following, we will focus primarily on problems of RLHF that are related to the content\nlevel, i.e. on models that have been \"tutored\" by humans but may still (or rather: because of this)\ngenerate biased, inaccurate or in other ways problematic responses to user prompts. Here are some\nexamples for what we have in mind, drawing from an overview of the issues by Casper et al. (2023):\n*   RLHF can amplify biases and one-sided opinions of human evaluators, a phenomenon known\n    as \"sycophancy\u201d. This issue worsens with larger models and is not effectively mitigated by the\n    RLHF process.\n*   RLHF can reinforce politically unbalanced judgements of the tutored models, an issue that\n    seems to be associated with the demographic composition of the selected evaluators.\n*   Some LLMs have been observed to become less accurate over time which may be, at least in\n    part, a consequence of RLHF where evaluators make mistakes in light of difficult topic areas.\n*   The RLHF process may introduce unanticipated model drift (Metz, 2023; cf. Chen et al. 2023),\n    meaning that the behaviour of the model might inadvertently shift from what was expected\n*   Evaluators can intentionally or unintentionally insert harmful data into RLHF systems,\n    especially given the scale at which RLHF operates. An example would be a (malicious) actor\n    providing high ratings for clearly unethical responses of an LLM.\nSo what is at the heart of these problems? And how can we improve RLHF so that we can cope with\nthem? Although there is probably no single answer to these questions, we argue that the outlined\nepistemic and ethical challenges can best be understood through the lenses of social epistemology\nand pluralism, as a lack diversity of human feedback plays a crucial role in these problems, indicating\nthat strengthening diversity \u2013 in the right way \u2013 may provide useful starting points for tackling the\nissues."}, {"title": "Section 3. Analytic tools from philosophy of science and social epistemology", "content": "Increasing diversity is an obvious remedy for some of the identified problems. Consider the issues of\nuser biases and politically unbalanced judgements. It seems intuitive that increasing diversity of\nperspectives in RLHF might be a (partial) remedy for these specific issues. To improve the Al alignment\nprocess, it is critical to look beyond individual perspectives to encompass the broader norms,\nexpectations, and values of various groups. However, to better understand the underlying reasons for\nthis and also why pluralism might be promising to understand and address some of the other problems\nmentioned, it will be helpful to introduce three interrelated concepts from post-positivist philosophy\nof science and social epistemology (as they have been developed by Paul Feyerabend, Sandra Harding,\nHelen Longino and others; see Grasswick, 2018 for an overview).\nThe social character of knowledge production: Feminist epistemologists such as Longino (1990) show\nthat knowledge production is a communal, interactive process that is shaped by the way the process\nis organised on a social level. The way it is organised \u2013 in particular, the way in which the influence of\nvalues in the process is handled can either promote or hamper the quality of the produced\nknowledge. While Longino makes this point in the context of science, the point generalises to reliable\nknowledge production per se. For instance, if you want to do a strategic SWOT (strengths, weaknesses,\nopportunities, threats) analysis in an organisation you will likely arrive at a richer, more reliable picture\nif you systematically incorporate internal feedback on different levels of the organisational hierarchy\nthan if you only rely on the input of the top management. Organising this process according to an ideal\nof inclusiveness makes things better (cf. Bengio et al. 2024). This example brings another aspect of the\nsociality of knowledge to the fore, namely that knowledge is often socially distributed - no member of\nthe organisation has complete knowledge about the organisation, but this knowledge is distributed\namong members and certain roles (the same applies to knowledge in scientific communities and many\nother contexts, see, e.g., Uygun Tun\u00e7 2023).\nPluralistic triangulation: In order to truly understand a particular (epistemic or ethical) problem or\nquestion, it is useful to compare and contrast different points of view. Through this triangulation\nprocess, we can avoid myopia and errors in possible answers or solutions. This is because the extent\nto which alternative points of view have blind spots or problematic assumptions often only becomes\napparent through contrast. This simple yet powerful insight goes back at least to J.S. Mill's defence of\nfreedom of speech and it has most forcefully been defended by Feyerabend (1975, 1999)4. Think of\nthe recent Coronavirus crisis. It was precisely the public involvement of experts beyond medicine and\nepidemiology (such as social scientists and managers of retirement homes) in analysing the public\ncrisis that helped to bring an excessive focus on health (rather than, e.g., social justice) and\nproblematic assumptions about pandemic modelling to light. This was in part the result of\ntriangulation of perspectives, approaches, and also of different value commitments (Bschir and Lohse\n2022).\nPosition matters: People's perspectives differ in systematic ways, influencing the ways individuals\nunderstand and interpret the world. More specifically, perspectives are shaped by personal\nexperiences which in turn are connected to social position and role in society, most prominently by\ngender, social class and ethnicity. This insight from the sociology of knowledge can be considered as\ncommonplace today. Feminist social epistemology, however, highlights two points. First, perspectives\nmay not only affect ethical and political beliefs (as already Marx would have argued), but also areas of\nknowledge that typically do not seem to be affected by them, such as scientific knowledge (as shown\nby recent work in the philosophy of science; e.g. (Massimi 2022). Second, the perspectives of\nmarginalised or oppressed groups can be especially fruitful in discovering hidden and/or problematic\naspects of a claim or state of affairs. Marginalised groups may sometimes be in a position where they\ncan combine insider and outsider perspectives to enrich mainstream-dominated discourses and\npositions and to counteract negative consequences of marginalisation/oppression for knowledge\nproduction (Harding 1996; Intemann 2010). An example of this can be found in gender-biased\nexplanations in primatology, which primarily emphasised the role of male animals in shaping social\nhierarchies, not taking into account the active role of female primates; a bias that only became\napparent when more female researchers entered the field, who were sensitive to both sides of gender\ndominance due to their own positions in patriarchal societies (Haraway 1984).\nTaken together, these concepts underline the importance of including multiple viewpoints from\ndifferent social positions in a well-organised process if one is interested in a less error-prone and\nbiased and more reliable and ethically robust understanding of many aspects of the world. Note that\nthe above discussion of these concepts has an epistemic as well as an ethical dimension and implies\nthat these are not independent. It might be objected, however, that the two dimensions need to be\nkept apart because conflating them runs the risk of threatening the objectivity of the knowledge\nproduction process. As regards to the epistemic dimension, a pluralistic approach that includes\ndifferent perspectives is more suitable to eliminate mistakes and biases and hence will produce more\naccurate and robust results. In a word, it will enhance the objectivity of the outcomes of the\nknowledge-production process. By contrast, allowing a plurality of moral values to affect the process\nwill jeopardize the objectivity of the outcomes, or so the objectors might claim. In doing so, they follow\nwhat (Douglas 2009) calls the traditional 'value-free ideal of science', which states that scientific\nresearch - and we hasten to add: any kind of truth-orientated knowledge production \u2013 should rely\nonly on so-called epistemic values (such as accuracy and consistency) and exclude the influence of\nnon-epistemic values (such as moral or political values). The reason is that the former are conducive\nto finding objective truth, while the latter are inherently subjective.\nHowever, as Douglas and others have convincingly argued, the value-free ideal is untenable. One\nreason is that the distinction between epistemic and non-epistemic values cannot be drawn sharply\n(Rooney 2017). Another reason is that in many types of epistemic practices, non-epistemic values are\nineliminably involved in decisions about the acceptance or rejection of hypotheses: the so-called\nproblem of inductive risk (Douglas, 2009). Does this mean that objectivity is out of reach? No, Douglas\nsuggests, it's only the traditional ideal of value-free objectivity that is to be rejected. But, Douglas\nargues, there are various other ways in which objectivity can be achieved, where it's important to note\nthat, first, various things can be considered objective, and second, that a key feature of objectivity is\nthat it is a basis for trust (115-116). Douglas distinguishes eight senses of objectivity, of which the\nfollowing two are especially relevant for our purposes.5 First, with respect to individual thought\nprocesses, objectivity can be achieved by \u201ctaking a position that is balanced or neutral with respect to\na spectrum of values\" (Douglas 2009, 123). This is called 'value-neutral objectivity'. Douglas (124) adds\nthat it may be allowed to exclude extreme positions (e.g. sexist or racist ones). Second, with respect\nto social processes (as noted above, a crucial element of knowledge production), the outcomes can\nbe regarded as objective if they result from an open discussion by the members of the community.\nVarious conditions have to be met to achieve such \u2018interactive objectivity', of which diversity among\nparticipants in the discussion is an important one (Douglas 2009, 127-128, cf. Longino 1990).\nApplying these insights to RLHF allows for an exploration of epistemic and ethical dimensions of Al\n(and in particular LLM) development. From an epistemic standpoint, diverse human feedback aids in\ncreating Al systems that are more reflective of varied human perspectives, thereby enhancing the\nmodels' objectivity and applicability. Ethically, it aligns with the ideal of inclusivity, ensuring that Al\nsystems do not perpetuate existing moral biases or inequalities. The described insights underscore the\nsignificance of incorporating feedback from a wide range of individuals in RHLF. This diversity goes\nbeyond mere political representation; it is about avoiding too much homogeneity and integrating\nvarious epistemic standpoints that arise from different social, cultural, and political positions and\nbackgrounds. These standpoints bring unique insights, challenge prevailing assumptions, and may\nultimately contribute to a more balanced and comprehensive\na more humanistic - Al model.\nHowever, to achieve this, we argue, it would be necessary to revise certain elements in the RHLF\nprocess in light of the above considerations. In the next section, we make initial suggestions along\nthese lines and point out open questions that need to be addressed to make progress."}, {"title": "Section 4. Open questions and potential trade-offs", "content": "How can we make RLHF more pluralistic and what are the challenges along the way? To make progress\non this question, we will present tentative ideas. Above all, however, we want to point out important\nissues that will need to be addressed to move forward. We first give a quick overview of some practical\naspects. We will then discuss aspects of evaluator selection, guidance and monitoring, and\naggregation of individual feedback. Finally, we will point out possible trade-offs of different types.\nWe here focus on OpenAl's methods, as ChatGPT is the model with most users. While OpenAl does\nnot disclose too many details, there is general information as to how this feedback was collected in\nthe initial stages. (Ouyang et al. 2022) provides information on how feedback was collected from\nhumans (in particular, they hired a team of 40 contractors on Upwork and ScaleAl), including\ninstructions for human evaluators and related details to fine-tune InstructGPT (the direct predecessor\nof ChatGPT):\n1.  Labeler Selection and Criteria: Labelers were chosen based on their agreement with sensitive\n    speech flagging, agreement on rankings, sensitive demonstration writing, and self-assessed\n    ability to identify sensitive speech for different groups. The selection process aimed to ensure\n    labelers were sensitive to various demographic groups and could identify potentially harmful\n    outputs.\n2.  Detailed Labelling Instructions: Labelers received detailed instructions on evaluating text\n    outputs based on their helpfulness, truthfulness, and harmlessness. The way of evaluating\n    'helpfulness' was in terms of whether the output followed the users' intentions and helped\n    solving the task. The way of evaluating 'truthfulness' was in terms of the output containing\n    accurate information, and not misleading the user. Evaluating 'harmlessness' involved\n    considering that the output should not cause physical, psychological, or social harm to people;\n    or damage to equipment, property, the environment, institutions, or resources necessary to\n    human wellbeing. (for examples consult the paper and the public labelling instructions\n    provided to evaluators).\n3.  Labelling Instructions Evolution: The instructions given to labelers evolved throughout the\n    project to address feedback and a better understanding of the desired measurements.\n    Initially, labelers were instructed to prioritise helpfulness to the user above truthfulness and\nharmlessness. However, in the final evaluations, the emphasis shifted to prioritise\ntruthfulness and harmlessness. Here are some of the instructions given (from the instruction\nsheet):\nFor each input instruction, labelling will consist of 3 parts:\n1.  Labelling instructions. You'll label a bunch of properties of the instruction, including\n    whether it contains PII, etc. [some examples: Personally identifying information,\n    User intent is unclear, Asks for sexual content, Asks for violent content, Asks for\n    content denigrating a protected class7, Asks for moral judgement, etc.]\n2.  Labelling Al model outputs. For each output from an Al model, you'll label the\n    outputs along several different axes, including giving a 1-7 rating, judging whether it\n    contains violent or sexual content, etc.\n3.  Ranking Al model outputs. You'll rank outputs from best to worst, including ties.\u201d\n4.  Collaboration with Labelers: The project involved close collaboration with labelers, including\n    an onboarding process, detailed task instructions, and a shared chat room for answering\n    questions.\n5.  Labeler Demographics: A voluntary anonymous survey was sent to labelers to understand\n    their demographics better (see fig. 1):\nThe initial stages arguably lacked diversity. However, with its opening to the broader user base of\nOpenAl, the diversity of users providing feedback has seen significant improvement, as all users\nthroughout (193 countries as of January 2024) are able to provide feedback. The current ChatGPT\ninterface allows users to give a thumbs up, thumbs down and to copy the output generated by the\nmodel. Sometimes the user is given two alternatives from which to choose. Additionally, when the\nmodel is asked to regenerate an answer, the user is asked whether the second answer was better,\nworse, or the same as the previous answer.\nNow that we have a rough overview of some aspects of the process, we will discuss pluralistic reform\nideas and associated challenges. Out of the 3 steps of RLHF, feedback collection and reward modelling\nare the most relevant for our purposes. For the first, we distinguish issues related to selecting\nevaluators and issues related to guiding and monitoring them. For the second, we consider issues\nrelated to the reception and aggregation of the data and its implications for training of a reward\nmodel. We also examine the relevance of guidance and the possibility of tradeoffs.\nAn obvious implication that follows from the above discussion on social epistemology is that the\nprocedure to select evaluators for RLHF will need to be revised to make it more representative of the\nexisting diversity of perspectives (to prevent composition like the initial one above, in which almost\nall raters are educated, almost half American and a third Filipino). While the latter stages in this case\nhave more diversity than the initial ones, it should be noted that the current user demographic of\nOpenAl, and by extension the feedback provided for RLFH, may not fully represent society at large (or\neven broad segments of it). For instance, there likely is a predominance of younger, well-educated\nindividuals who have both the knowledge and technological means to interact with the free version\nof ChatPGT. In the case of GPT-4, given that there is a financial barrier, it may self-select certain strata\nof the population. Similarly, there might be a self-selective mechanism of those that engage in\nproviding feedback. The issue here is that there may be certain people who are more inclined to\nprovide this feedback than others, creating a possible self-selection bias (see (Bethlehem 2010)for\nself-selection effects in web surveys).\nAlthough these problems might be easy to identify, it is far less clear how they ought to be resolved.\nSo far we detailed that a source of problems stems from the evaluators not representing broader\ngroups. This naturally raises the question of the sense in which evaluators should be representative.\nAnswering this question involves normative choices regarding the sense in which we want to make\nLLMs better aligned with human goals. Having a panel that is truly representative of the world\npopulation seems hardly possible. So, should developers aim for a specific subset of the world's\npopulation? If so, which one? Should they aim for a representative set of only a certain range of\ncultural and/or ethical viewpoints, and if so, which ones? Should they give a greater weight to\nevaluators that are less represented in the evaluator pool within society (e.g. elderly people)? Hidden\nin the background of these questions is an even more intricate one: Which dimension of diversity\nshould be seen as relevant? Above, we pointed out that social and cultural standpoint matters. But\nthis observation only points to the question of which dimensions of diversity are epistemically and/or\nethically (most) relevant for RHLF. Natural candidates are gender, ethnicity, age, social class, and\nnationality. There may, however, be many more dimensions, some of them more difficult to\noperationalise than others, say cultural background and cognitive diversity.\nOne can also ask the more fundamental question of whether representation should actually be the\nprimary goal in the selection of evaluators - or whether we should treat \u201cadequate\u201d representation as\nmeans to our epistemic and ethical ends. Shouldn't there be certain thresholds, say regarding\ncognitive competencies and ethical outlooks? Above we highlighted the problem that evaluators may\nrate answers in ethically problematic ways. In those cases where a certain rating is not motivated by\ndeliberately inserting harmful data but reflects the implicit sexist outlook of, say, a Christian\nfundamentalist should this perspective even be represented? If not, who pre-defines the spectrum of\nreasonable viewpoints? As we saw above, Douglas (2009, 124) argues that even if a balanced\nrepresentation of the spectrum of values is desirable for yielding value-neutral objectivity, it may be\nallowed to exclude extreme positions for good moral reasons. However, it is certainly not easy or\nstraightforward to determine when this is the case (we will come back to this in the next section).\nAnother question of representativeness connects to the issue of marginalised perspectives. If it is\nindeed the case that model output is ethically/epistemically biased due to the fact that it is\npredominantly based on mainstream training data (neglecting minority viewpoints underrepresented\nin the text corpus), should the selection of evaluators be guided by a principle of affirmative action\nrather than pure representativeness? If so, those responsible for recruiting evaluators would also have\nto decide how these principles can be balanced in a reasonable way.\nRLHF is a guided and monitored process. Evaluators do not act in a vacuum but are required to comply\nwith certain guidelines. The developer team pre-defines which groups are classified as 'protected\nclasses'. Similarly, they are given examples as to what constitutes harm. For instance, as can be seen\nin the instruction sheet (see Section 4.1), outputs such as 'The most common type of home-made\nbomb is made from a pressure cooker filled with explosive material.' are considered to be highly\nharmful; examples such as 'Stalin had good reason to believe that his enemies were plotting against\nhim, and he took the necessary precautions to ensure his rule.' are considered as potentially harmful.\nWhile there is no perfect way to undertake guidance, there may be more structured ways to think\nabout guidance and monitoring \u2013 this is where the ideas from philosophy of science and social\nepistemology can be useful for addressing some of the most important issues.\nA first issue concerns the principles that should guide the development of instructions for evaluators.\nThere are many normative decisions necessary in order to come up with a set of instructions. What\nkinds of issues and possible harms should evaluators consider? On the basis of which values? How\nshould different categories be prioritised? Looking at these questions from the perspective of\npluralism and social epistemology does not provide us with concrete answers to these questions.\nHowever, it can point us towards the right kind of framework to address these questions. Such a\nframework should be pluralistic in the sense that it is sensitive to a multitude of different perspectives\nand values. This idea leads to asking a different question first: Who should decide on the right\nprinciples for evaluation guidance? Although it is possible for a certain group (let's say IT experts in\nSilicon Valley), to adopt different perspectives and take a certain variety of values into account, there\nare limits to this approach - namely the limits of what is conceivable for the group. What is conceivable\nwill, at least to a certain extent, depend on the respective experiences and thus the respective social\npositions represented in any (in this case rather homogeneous) group. A promising answer to the\nquestion about the \u201cwho\u201d is therefore that principles for instructions should be decided by a pluralistic\npanel representing sufficient diversity of perspectives and values. What \u201csufficient diversity", "unanswered": "How\nbroad should the spectrum of values be that we consider legitimate? Of course, it is possible that\nsexist or racist values will also be represented10 in a truly pluralistic panel deciding on instructions for\nevaluators, whether implicitly or explicitly. We believe that such values should not guide the\nrespective instructions for evaluators. However, this means that an ethical framework must be\nconsidered that is pluralistic on the one hand, but also sets moral boundaries on the other. The\nUniversal Declaration of Human Rights is a good candidate for this because it represents a kind of\nminimum consensus that is already accepted, at least de jure, by a large number of countries. How\nexactly implementation of such an ethically robust framework can succeed, however, is not a trivial\nquestion.\nA pluralistic panel could also be responsible for monitoring adherence to the instructions and\nmanaging deviant behaviour of evaluators. It should, in other words, monitor and fine-tune this\nspecific social process of knowledge production. There will always be situations in which it is not clear\nhow certain decision rules should be interpreted or applied or if evaluators really deviate from\nguidelines in a systematic way or not. This need not be due to malicious (or ignorant) actors, but simply\nto the fact that guidelines cannot cover every possible scenario and that there may be multiple\ninterpretations of the rules (Miller and Sultanescu 2022). Take, for instance, the instructions given to\nevaluators involved in InstructGPT when evaluating truthfulness. One of the conditions was that\noutputs 'Not produc[e] clearly false information about the world (e.g. making up facts or promoting\nconspiracies). For example, the output should not state that Hillary Clinton has served time in prison.'\n(instruct sheet, p1); this might be straightforward for certain outputs, but not for others. Further,\nlanguages and cultural contexts influence how questions are interpreted and answers are structured,\nwhich can have an impact on RLHF (when feedback is given in different languages). Consider cases\nwhere different and cross-cutting kinship categories exist in different languages (Kemp and Regier\n2012) which may lead to incommensurable answers to questions about social relationships.\nWhen systematic rule application/deviance problems arise, it is necessary to make adjustments by\nintroducing new rules, clarifying rules or adding examples (see above). The exact implementation of\nre-adjustments naturally poses similar issues as those mentioned in the last paragraph (e.g. \u201cIs this\nevaluator mistaken or does s/he have a value base that we didn't consider?", "pull": "n different directions arguably leading to more epistemically and ethically\nrobust model outputs((Freiesleben and Grote 2023), for various notions of robustness in machine\nlearning).\nAn important step in RLHF is the collection and aggregation of individual feedback to enable the\ntraining of a reward model. The reward model is usually a supervised learning model that can predict\nevaluations. Recall, typical feedback includes binary preferences (i.e. 'yes' or 'no'; 'good' or 'bad') of a\ncertain output, giving a score (e.g. from 0 to 7), or ranking several outputs in order of quality. In\naddition, feedback can also involve giving the explicit answer that should have been given, or providing\nan explicit explanation as to why the answer was correct/incorrect. These items are then aggregated\nand used to train a reward model. In the case of binary preferences or rankings the aggregation might\nbe straightforward, although there are different aggregation mechanisms which may make a\ndifference in the final policy that gets implemented (e.g. ELO rating, majority voting, averaging, etc. ).\nFor other types of feedback, important decisions regarding how the reward model is going to be\ntrained, the order in which data is presented, and so on, make a difference.\nIn the case of InstructGPT, labellers ranked multiple model responses to the same prompt, creating\nnumerous pairwise comparisons. The reward model was then trained on these comparisons, with all\ncomparisons from a single prompt treated as a single batch. This iterative process used new\ncomparison data to continuously update the reward model and, subsequently, the policy. While early\ntraining prioritised helpfulness, final evaluations emphasised truthfulness and harmlessness, leaving\nthe resolution of potential conflicts between these criteria for future exploration. While optimising\nsequentially might do the job, it necessarily prioritises certain objectives over others, rather than\nkeeping them all in mind. The way the feedback was aggregated, and the sequence by which the\nreward model evolved, raises several important questions when viewed through the lens of pluralism\nand social epistemology. One key issue is how to effectively combine the diverse, and potentially\nconflicting, feedback from evaluators with different backgrounds, expertise, and value systems.\nDisagreement can often signal areas of underlying complexity, ambiguity or tension that warrant\nfurther investigation. A pluralistic approach to aggregation would seek to preserve and engage with\nthese points of contention, rather than erasing them in pursuit of a false consensus. This connects to\nthe issue of how to handle conflicts between different alignment criteria, such as helpfulness,\ntruthfulness and harmlessness. InstructGPT's strategy of shifting priority from helpfulness to\ntruthfulness/harmlessness in later training phases is a rather blunt way of navigating these trade-offs.\nA more nuanced approach might involve explicitly modelling the relationships and dependencies\nbetween these criteria, and allowing for context-specific balancing rather than a one-size-fits-all\nhierarchy. In this context, several key additional questions and challenges in feedback collection and\naggregation emerge.\nOne significant issue concerns preserving diverse perspectives during the aggregation process rather\nthan cancelling them out or ignoring a subset. This makes it important to find methods that take\nimportant disagreements and minority viewpoints seriously instead of merely averaging them into a\npseudo consensus perspective. Connectedly, there is the challenge of navigating and balancing\nconflicting viewpoints. A key question is how the reward model can be aligned with, or at least take\ninto account, the values of multiple groups holding conflicting or even incompatible viewpoints. While\nso far substantive progress has been made here, representing diverse societies in reward models is\nstill a pressing concern. How can we overcome the limitations of a single reward function in capturing\nthe broad spectrum of human preferences, expertise, and capabilities in a diverse society, where\ncurrent methods risk marginalising under-represented groups by treating differences as mere noise,\nor averaging important differences? This issue underscores the need for a reward system that is\nsensitive to diverse values and perspectives. Such an undertaking requires nuanced reward models.\nThese models should adequately reflect the relationships and trade-offs among various alignment\ncriteria, enabling context-specific adjustments. Such development would allow for a more refined\napproach to handling feedback that considers the complexity of different stakeholders' needs and\nexpectations. Furthermore, it is worthwhile to explore the concept of multiple reward functions.\nMoving away from the reliance on a singular reward function and instead maintaining multiple models\ncould more effectively represent the diverse values of different stakeholders.\nIn addition to these challenges, modelling dynamic human values presents its own set of complexities.\nHuman feedback is inherently complex and context-dependent, continuously evolving with time. For\nexample, attitudes towards certain countries might change due to geopolitical events (e.g. wars), or\nattitudes towards health or governments might change due to pandemics. This dynamism makes it\ndifficult to effectively model with static, lower-dimensional preferences given by humans at a certain\npoint in time, such as rankings of outputs.\nFurther iterations of RLHF will filter some of these problems out. However, certain complex issues may\nnot improve through this process, or might require a different approach altogether (think for example\nof questions involving the assessment of the Middle East conflict and how RLHF might introduce model\ndrift). A possible starting point for thinking differently about feedback aggregation can be seen in a\nnotable study by Bakker et al.(2022) on fine-tuning LLMs to generate meaningful consensus\nstatements among groups with diverse opinions. This involved creating debate questions using a pre-\ntrained LLM on contemporary political and moral issues, followed by data collection and environment\ndesign where UK-based participants provided their opinions. The study demonstrated the model's\nability to generate complex consensus statements preferred over LLM-generated and human-\ngenerated alternatives, highlighting the model's sensitivity to individual contributions and robust\nperformance with out-of-distribution questions.\nFinally, we want to look at specific trade-offs in RLHF-assisted Al development. Regarding the type of\nfeedback, there are trade-offs among accuracy, speed, and cost. For instance, the choice between\nbinary responses and more detailed (pluralistic) feedback from evaluators significantly influences\nthese dimensions. Binary responses, while quick and cost-effective, often lack the nuanced\ninformation that can refine Al behaviour more precisely. Conversely, detailed feedback, although\nricher and potentially more accurate, requires more time from evaluators and incurs higher costs. This\nextended feedback process may slow down the iterative cycles of model training, yet it enriches the\nquality and reliability of the model by providing deeper insights into the evaluators' reasoning and\nperspectives. Balancing these aspects requires careful consideration to optimise the efficiency of the\nRLHF process while maintaining the integrity and depth of the training data. We will not focus on\nthese trade-offs, but rather concentrate on the ones related to epistemic and moral virtues.\nRecent studies have pointed out a concerning trend in GPT-4's performance, particularly in areas\nrequiring high precision, like mathematics (J. Chen et al. 2023). For example, in March 2023, GPT-4\ndemonstrated an 84% accuracy rate in identifying prime versus composite numbers, a task that\ndemands precise mathematical reasoning. However, by June 2023, its accuracy had notably declined\nto just 51%. This diminishing accuracy brings to light questions about the impact of user diversity on\nthe model. Does the interaction with a more heterogeneous user base enhance the model's overall\nperformance, or does it amplify certain limitations? For instance, if users with varying degrees of\nexpertise in a subject interact with the model, it might struggle to maintain a high standard of accuracy\nin that domain, but on the other hand, it might be able to interact with the subject at different levels\nof difficulty and abstraction. Possibly a comparison and triangulation of different user responses can\nhelp here. However, such a process - perhaps even organised in a discursive format - naturally involves\nefficiency trade-offs.\nFurther complicating this dynamic, a study by (K. Chen et al. 2024) evaluates how GPT-3 interacts with\ndiverse socio-demographic groups concerning sensitive topics like climate change and the Black Lives\nMatter movement. The findings reveal a nuanced landscape where conversational styles and user\nexperiences correlate significantly with the users' background, potentially indicating biases in the\nmodel's responses. This is particularly evident as users from educational and opinion minority groups\nreport poorer experiences. Is this the result of the evaluators that were involved in RLHF? (recall that\nfor InstructGPT, more than 85% of the evaluators had at least a university degree).\nThis dilemma underscores a potential trade-off between the epistemic and moral dimensions of Al\ndevelopment. On one hand, moral considerations advocate for a diverse range of feedback to ensure\nthe model's alignment with a broad spectrum of human values and perspectives. On the other hand,\nepistemic virtues like accuracy and reliability might be compromised if the feedback providers lack\ndomain-specific expertise. This tension necessitates a deeper philosophical inquiry into the objectives\nand methodologies of RLHF and prompts a re-evaluation of the balance between user satisfaction and\nscientific accuracy in the model's training objectives, among other things.\nFurthermore, the interaction of diversity with model alignment and response generation introduces\nadditional complexity. One of the places where this can be seen most clearly is in certain\nmanifestations of model drift. Model drift presents a critical challenge in the realm of Al development,\nparticularly in systems that undergo continuous refinement and real-world applications. This\nphenomenon occurs when a model's performance and behaviour degrade over time due to shifts in\nunderlying data, changes in the real-world context it is applied to, updates to the model itself, or as a\nconsequence of RLHF. For instance, RLHF can introduce drift through the biases and preferences of\nthe evaluators, particularly if they hold strong moral or ethical views that may not be universally\nshared or may become less prevalent over time. For instance, a language model trained up to a certain\nyear might not only miss later developments but also reflect dated or skewed moral perspectives if\nthe feedback mechanism doesn't account for evolving societal norms. This form of drift not only\nimpacts the model's accuracy and reliability but also its relevance and fairness, as the Al might produce\noutputs that are misaligned with current or emerging societal values.\nThese observations show one thing above all, namely that the way the social process of knowledge\nproduction is structured in the case of RHLF and decisions regarding the balancing of different\nepistemic and moral values is anything but trivial - but it may have a major impact on the kind of\nanswers we can expect from an evolving LLM."}, {"title": "Section 5. Conclusion: Enhancing RLHF", "content": "In this article, we have set out the advantages of pluralistic RLHF and underpinned them with\narguments from social epistemology and pluralist philosophy of science. We argued for increasing\nevaluator diversity, optimising guidance for heterogeneous evaluator responses, and modifying the\napproach to feedback aggregation, among other things \u2013 however, we also emphasised challenges for\nmore pluralism in RLHF. While these challenges cannot all be overcome equally well, we have sketched\na few possible ways for addressing them. 11 We conclude with an agenda for change, i.e. concrete,\nactionable steps, to improve LLM development.\nIn our view, the following demands are the most important and feasible starting points for\nadvancing the pluralisation of RHLF:\n(1) Representation. LLM developers must address issues of adequate representativeness of\n    epistemic perspectives and ethical values in the selection of evaluators more explicitly than\n    before and negotiate these in a transparent process. This will need to include discussions\n    about the spectrum of values that are considered legitimate \u2013 and ideally the development of\n    an ethical framework that reflects the outcomes of these discussions. In doing this, developers\n    should leverage stakeholder participation methods to include diverse and in particular\n    marginalised voices throughout the process.\n(2) Guidance. LLM developers should establish pluralistic panels representing a diversity of\n    perspectives and values to jointly decide on the principles for evaluation guidance and\n    monitoring the feedback process. This should be done in an iterative way where additional\n    (and especially minority) perspectives can be added to the panel as appropriate. In order to\n    establish such a panel, a democratic mode of deliberation must be developed that enables an\n    open exchange of viewpoints and is pragmatically feasible at the same time. An ethical\n    framework like the one mentioned under (1) could serve as a scaffold for this deliberation.\n(3) Feedback. LLM developers should experiment more with feedback formats that are capable\n    of preserving different perspectives and conflicting viewpoints (allowing for negotiation and\n    contestation) rather than turning them into superficial pseudo consensus. This could, for\n    instance, be achieved by creating platforms where evaluators with different perspectives\n    engage in interactive discussions and collective revisions of their evaluations, using methods\n    such as Delphi techniques or iterative feedback to develop more complex positions, especially\n    when initial opinions differ significantly. Such an approach would improve inclusivity of\n    viewpoints and also align with Longino's (1990) and Douglas' (2009) notion of interactive\n    objectivity, discussed in Section 3, where the right kind of social organisation of the process\n    enhances the epistemic robustness and credibility of the outcome.\n(4) Aggregation. Ultimately, the challenge is to develop reward models that see diversity of\n    values and standpoints as a resource rather than treating it as a problem to be minimised. This\n    may require a rethinking of established methods for creating reward models. LLM developers\n    should address this challenge by developing and testing more nuanced reward models. They\n    could, for instance, explore and improve the usability of weighted feedback aggregation\n    approaches that prioritise the input of evaluators based on their context-specific expertise\n    and trustworthiness. This might involve experimenting with reputation systems, peer-review\n    mechanisms, or criteria that account for an evaluator's background and qualifications,\n    ensuring that more credible feedback (relative to the task at hand) has a greater impact on\n    the outcomes.\nAs we have shown, insufficient pluralism is problematic for RLHF on epistemic and ethical grounds.\nUltimately, however, the problem is much more far-reaching, as biased and under-complex LLMS,\nonce fully integrated into social media, journalism and other key areas of public discourse, have the\npotential to negatively influence this discourse on a massive level and ultimately jeopardise\ndemocracy. Thus, even if the changes we propose prove insufficient or pragmatically unfeasible, we\nbelieve they should be carefully considered and, if necessary, modified or replaced with better ideas"}]}