{"title": "Reinforcement Learning from Human Feedback: Whose Culture, Whose Values, Whose Perspectives?", "authors": ["Kristian Gonz\u00e1lez Barman", "Simon Lohse", "Henk de Regt"], "abstract": "We argue for the epistemic and ethical advantages of pluralism in Reinforcement Learning from Human Feedback (RLHF) in the context of Large Language Models (LLM). Drawing on social epistemology and pluralist philosophy of science, we suggest ways in which RHLF can be made more responsive to human needs and how we can address challenges along the way. The paper concludes with an agenda for change, i.e. concrete, actionable steps to improve LLM development.", "sections": [{"title": "Introduction", "content": "Reinforcement learning from human feedback (RLHF) is an increasingly common technique in artificial intelligence (AI) where a model learns by receiving feedback from humans rather than solely relying on a predefined reward function. This approach is particularly useful when designing AI systems for tasks where it is difficult to specify a precise reward function or when it is important to align the model's behaviour with certain human expectations and values. For instance, RLHF has notably improved language models for context-aware text generation (Ziegler et al. 2020) and taught robots to navigate cluttered environments (Henry et al. 2010).\nRLHF is commonly employed in the later stages of fine-tuning models, particularly in the development of prominent Large Language Models (LLMs) like GPT-3.5 or GPT-4. Initially, these models undergo training using vast text corpora to grasp a broad range of language patterns and contexts. This foundational training is then supplemented by task-specific fine-tuning, where the models are adjusted to excel in particular applications, such as understanding and generating dialogues. The refinement process is then further enhanced through RLHF. The RLHF process often involves people evaluating and ranking potential outcomes to build a classification model. This model is designed to predict if a human would find a specific output acceptable. The initial model undergoes refinement through a step-by-step process where it can use this classification model for feedback, allowing for improvement. This iterative process enables the model to better align with human judgement over time.\nOne question that becomes significant in this context is what role the composition of the human feedback group plays in the process. After all, a certain diversity in composition seems to be a clear advantage\u00b9 in the development of models like ChatGPT. The feedback providers' background knowledge and their social, cultural and political perspectives may (or so we will argue) significantly influence the model \u2013 and a lack of diversity among individuals may lead to a model that is overly geared towards only specific expectations, potentially overlooking different but also important values and perspectives. Accordingly, too much homogeneity may raise ethical concerns and also create epistemic limitations, as the model may generate outputs that are hardly broadly acceptable, applicable or insightful.\nDespite the apparent benefits of diversity in RLHF, the philosophical underpinnings of why and how it may improve model performance have not been thoroughly explored. This paper offers an analytical perspective on the impact of diverse feedback on LLM development. Our investigation is motivated by two distinct but intertwined objectives: first, to elucidate the complex interplay between diversity, balance, and reliability in the development of LLMs employing RLHF. Second, to explore challenges for increasing diversity in RLHF and point to potential ways for managing these challenges.\nThe paper is structured as follows. Section 2 provides a brief overview of the landscape of RLHF and associated problems, highlighting the need for diversity in feedback. Section 3 introduces ideas from philosophy of science and social epistemology which can serve as useful analytic tools to understand and manage the identified problems. Section 4 develops initial ideas about how RLHF can be improved by making it more pluralistic and discusses open questions and trade-offs that need to be addressed to make progress along these lines. The final section concludes with a brief summary and an agenda for making RLHF more pluralistic."}, {"title": "Section 2: Reinforcement learning and associated challenges.", "content": "RLHF is a useful method to align Al systems with human objectives, especially in the last stages of fine-tuning of state-of-the-art Large Language Models such as OpenAl's GPT-4 or Meta's LLama 3. This technique has become central in adapting Al models to complex, human-centric expectations and preferences. The clearest example of this was Davinci 3.5 (the first version of ChatGPT), which was far more safe, \"user-friendly\u201d, and well received in terms of dialogic conversation style than GPT3.\nOriginating from revealed preference theory in economics, RLHF was adopted in machine learning for applications in human-computer interaction and reinforcement learning. The standard methodology for RLHF was popularised in 2017 (Christiano et al. 2017) and has significantly influenced the deep reinforcement learning community. The advantages of RLHF are manifold. It enables humans to communicate goals to Al systems without the need for having a previously defined reward function. This not only makes reward shaping implicit and intuitive, but mitigates reward hacking (Casper et al. 2023), where an Al might exploit loopholes in a reward function to achieve high scores without actually fulfilling the task the reward function is a proxy for. Furthermore, RLHF facilitates a dynamic learning process where Al models can continuously evolve based on real-time human feedback, which enables companies with a large user base to improve their models with faster cycles. This ongoing interaction results in Al systems that are more aligned with the values and expectations of the model's users, ensuring their relevance and utility. Consequently, Al systems that incorporate RLHF might be better equipped to operate in complex moral and social landscapes, making them more trustworthy and reliable for a broad range of applications.\nThe process of RLHF, involves three sequential steps (Casper et al. 2023): Feedback Collection, Reward Modeling, and Policy Optimization. Feedback Collection encompasses human assessment of model outputs, followed by Reward Modeling, where these assessments are used to train a reward model via supervised learning aiming to replicate human evaluations. The final step, Policy Optimization, refines the Al system to generate outputs positively rated by the reward model. The term 'policy' here refers to the strategy or set of rules that the Al system follows to decide its actions based on the current state.\nTo illustrate this, imagine an Al designed to write creative stories undergoing this three-step process. Initially, humans evaluate the stories the Al produces (e.g. based on creativity, coherence, and emotional impact). These evaluations guide the training of a reward model that predicts whether a story is appealing to humans (i.e. whether human evaluators would give it a good score). Finally, the Al's story-writing process is optimised based on feedback from the reward model, enabling it to produce narratives that better resonate with human readers, thus improving its ability to craft engaging and coherent stories.\nNote, however, that the outputs of the Al will be influenced by the preferences of the evaluators, which may lead to a narrowing of the Al's capabilities and a potential bias towards certain types of stories or storytelling techniques. This risk is most clearly seen when stories are intended to contain answers to scientific questions, where evaluators might prefer concise and simple answers, posing a risk that the Al learns to provide a simplified but (potentially) misleading answer rather than a scientifically adequate one (Perez et al. 2022; see also Barman et al. 2024). In its most extreme form, this might involve giving not just simplified, but wrong answers altogether; for instance, the model may \"hallucinate\" responses to avoid answering that it does not know as this might be rated poorly. While this result could be seen as a simple consequence of prioritising user satisfaction, it may come at the expense of other values such as scientific accuracy and reliability. Given these (as well as other) potential trade-offs, it becomes important to consider which values one should aim for.\nThe human feedback used in RLHF includes, among others, label feedback, binary preference feedback, correction feedback, and language feedback. This involves tasks such as ranking outputs (e.g., in order of quality), choosing one output over another, providing explanations as to why a certain output is not correct (or how it could be improved), or providing an example of what the right output should be. Each feedback type has its distinct advantages and limitations, and there are multiple trade-offs at play (e.g., between the difficulty of the task, the ability of gathering enough feedback given a certain budget, or the difficulty of having humans with enough skill to perform these tasks). Despite its widespread use, certain limitations of RLHF in fine-tuning LLMs are notable. Perez et al. (2022) point out that models fine-tuned using RLHF often display biases and can lead to the mirroring of certain ideologies. El-Mhamdi et al. (2023) observe that these models could unintentionally reveal sensitive information. Furthermore, there is clear evidence of these models indeed producing hallucinated or inaccurate content (OpenAl et al. 2023). Moreover, RLHF doesn't protect these models against adversarial attacks, including jailbreaking\u00b2 or prompt injection/extraction\u00b3 (Li et al. 2023). The response behaviour of these models can also be undesirable in several other ways.\nThese problems are too multifaceted to address them in one paper and require different types of solutions. In the following, we will focus primarily on problems of RLHF that are related to the content level, i.e. on models that have been \"tutored\" by humans but may still (or rather: because of this) generate biased, inaccurate or in other ways problematic responses to user prompts. Here are some examples for what we have in mind, drawing from an overview of the issues by Casper et al. (2023):\n*   RLHF can amplify biases and one-sided opinions of human evaluators, a phenomenon known as \"sycophancy\u201d. This issue worsens with larger models and is not effectively mitigated by the RLHF process.\n*   RLHF can reinforce politically unbalanced judgements of the tutored models, an issue that seems to be associated with the demographic composition of the selected evaluators.\n*   Some LLMs have been observed to become less accurate over time which may be, at least in part, a consequence of RLHF where evaluators make mistakes in light of difficult topic areas.\n*   The RLHF process may introduce unanticipated model drift (Metz, 2023; cf. Chen et al. 2023), meaning that the behaviour of the model might inadvertently shift from what was expected\n*   Evaluators can intentionally or unintentionally insert harmful data into RLHF systems, especially given the scale at which RLHF operates. An example would be a (malicious) actor providing high ratings for clearly unethical responses of an LLM.\nSo what is at the heart of these problems? And how can we improve RLHF so that we can cope with them? Although there is probably no single answer to these questions, we argue that the outlined epistemic and ethical challenges can best be understood through the lenses of social epistemology and pluralism, as a lack diversity of human feedback plays a crucial role in these problems, indicating that strengthening diversity \u2013 in the right way \u2013 may provide useful starting points for tackling the issues."}, {"title": "Section 3. Analytic tools from philosophy of science and social epistemology", "content": "Increasing diversity is an obvious remedy for some of the identified problems. Consider the issues of user biases and politically unbalanced judgements. It seems intuitive that increasing diversity of perspectives in RLHF might be a (partial) remedy for these specific issues. To improve the Al alignment process, it is critical to look beyond individual perspectives to encompass the broader norms, expectations, and values of various groups. However, to better understand the underlying reasons for this and also why pluralism might be promising to understand and address some of the other problems mentioned, it will be helpful to introduce three interrelated concepts from post-positivist philosophy of science and social epistemology (as they have been developed by Paul Feyerabend, Sandra Harding, Helen Longino and others; see Grasswick, 2018 for an overview).\nThe social character of knowledge production: Feminist epistemologists such as Longino (1990) show that knowledge production is a communal, interactive process that is shaped by the way the process is organised on a social level. The way it is organised \u2013 in particular, the way in which the influence of values in the process is handled can either promote or hamper the quality of the produced knowledge. While Longino makes this point in the context of science, the point generalises to reliable knowledge production per se. For instance, if you want to do a strategic SWOT (strengths, weaknesses, opportunities, threats) analysis in an organisation you will likely arrive at a richer, more reliable picture if you systematically incorporate internal feedback on different levels of the organisational hierarchy than if you only rely on the input of the top management. Organising this process according to an ideal of inclusiveness makes things better (cf. Bengio et al. 2024). This example brings another aspect of the sociality of knowledge to the fore, namely that knowledge is often socially distributed - no member of the organisation has complete knowledge about the organisation, but this knowledge is distributed among members and certain roles (the same applies to knowledge in scientific communities and many other contexts, see, e.g., Uygun Tun\u00e7 2023).\nPluralistic triangulation: In order to truly understand a particular (epistemic or ethical) problem or question, it is useful to compare and contrast different points of view. Through this triangulation process, we can avoid myopia and errors in possible answers or solutions. This is because the extent to which alternative points of view have blind spots or problematic assumptions often only becomes apparent through contrast. This simple yet powerful insight goes back at least to J.S. Mill's defence of freedom of speech and it has most forcefully been defended by Feyerabend (1975, 1999). Think of the recent Coronavirus crisis. It was precisely the public involvement of experts beyond medicine and epidemiology (such as social scientists and managers of retirement homes) in analysing the public crisis that helped to bring an excessive focus on health (rather than, e.g., social justice) and problematic assumptions about pandemic modelling to light. This was in part the result of triangulation of perspectives, approaches, and also of different value commitments (Bschir and Lohse 2022).\nPosition matters: People's perspectives differ in systematic ways, influencing the ways individuals understand and interpret the world. More specifically, perspectives are shaped by personal experiences which in turn are connected to social position and role in society, most prominently by gender, social class and ethnicity. This insight from the sociology of knowledge can be considered as commonplace today. Feminist social epistemology, however, highlights two points. First, perspectives may not only affect ethical and political beliefs (as already Marx would have argued), but also areas of knowledge that typically do not seem to be affected by them, such as scientific knowledge (as shown by recent work in the philosophy of science; e.g. (Massimi 2022). Second, the perspectives of marginalised or oppressed groups can be especially fruitful in discovering hidden and/or problematic aspects of a claim or state of affairs. Marginalised groups may sometimes be in a position where they can combine insider and outsider perspectives to enrich mainstream-dominated discourses and positions and to counteract negative consequences of marginalisation/oppression for knowledge production (Harding 1996; Intemann 2010). An example of this can be found in gender-biased explanations in primatology, which primarily emphasised the role of male animals in shaping social hierarchies, not taking into account the active role of female primates; a bias that only became apparent when more female researchers entered the field, who were sensitive to both sides of gender dominance due to their own positions in patriarchal societies (Haraway 1984).\nTaken together, these concepts underline the importance of including multiple viewpoints from different social positions in a well-organised process if one is interested in a less error-prone and biased and more reliable and ethically robust understanding of many aspects of the world. Note that the above discussion of these concepts has an epistemic as well as an ethical dimension and implies that these are not independent. It might be objected, however, that the two dimensions need to be kept apart because conflating them runs the risk of threatening the objectivity of the knowledge production process. As regards to the epistemic dimension, a pluralistic approach that includes different perspectives is more suitable to eliminate mistakes and biases and hence will produce more accurate and robust results. In a word, it will enhance the objectivity of the outcomes of the knowledge-production process. By contrast, allowing a plurality of moral values to affect the process will jeopardize the objectivity of the outcomes, or so the objectors might claim. In doing so, they follow what (Douglas 2009) calls the traditional 'value-free ideal of science', which states that scientific research - and we hasten to add: any kind of truth-orientated knowledge production \u2013 should rely only on so-called epistemic values (such as accuracy and consistency) and exclude the influence of non-epistemic values (such as moral or political values). The reason is that the former are conducive to finding objective truth, while the latter are inherently subjective.\nHowever, as Douglas and others have convincingly argued, the value-free ideal is untenable. One reason is that the distinction between epistemic and non-epistemic values cannot be drawn sharply (Rooney 2017). Another reason is that in many types of epistemic practices, non-epistemic values are ineliminably involved in decisions about the acceptance or rejection of hypotheses: the so-called problem of inductive risk (Douglas, 2009). Does this mean that objectivity is out of reach? No, Douglas suggests, it's only the traditional ideal of value-free objectivity that is to be rejected. But, Douglas argues, there are various other ways in which objectivity can be achieved, where it's important to note that, first, various things can be considered objective, and second, that a key feature of objectivity is that it is a basis for trust (115-116). Douglas distinguishes eight senses of objectivity, of which the following two are especially relevant for our purposes.5 First, with respect to individual thought processes, objectivity can be achieved by \u201ctaking a position that is balanced or neutral with respect to a spectrum of values\" (Douglas 2009, 123). This is called 'value-neutral objectivity'. Douglas (124) adds that it may be allowed to exclude extreme positions (e.g. sexist or racist ones). Second, with respect to social processes (as noted above, a crucial element of knowledge production), the outcomes can be regarded as objective if they result from an open discussion by the members of the community. Various conditions have to be met to achieve such \u2018interactive objectivity', of which diversity among participants in the discussion is an important one (Douglas 2009, 127-128, cf. Longino 1990).\nApplying these insights to RLHF allows for an exploration of epistemic and ethical dimensions of Al (and in particular LLM) development. From an epistemic standpoint, diverse human feedback aids in creating Al systems that are more reflective of varied human perspectives, thereby enhancing the models' objectivity and applicability. Ethically, it aligns with the ideal of inclusivity, ensuring that Al systems do not perpetuate existing moral biases or inequalities. The described insights underscore the significance of incorporating feedback from a wide range of individuals in RHLF. This diversity goes beyond mere political representation; it is about avoiding too much homogeneity and integrating various epistemic standpoints that arise from different social, cultural, and political positions and backgrounds. These standpoints bring unique insights, challenge prevailing assumptions, and may ultimately contribute to a more balanced and comprehensive a more humanistic - Al model. However, to achieve this, we argue, it would be necessary to revise certain elements in the RHLF process in light of the above considerations. In the next section, we make initial suggestions along these lines and point out open questions that need to be addressed to make progress."}, {"title": "Section 4. Open questions and potential trade-offs", "content": "How can we make RLHF more pluralistic and what are the challenges along the way? To make progress on this question, we will present tentative ideas. Above all, however, we want to point out important issues that will need to be addressed to move forward. We first give a quick overview of some practical aspects. We will then discuss aspects of evaluator selection, guidance and monitoring, and aggregation of individual feedback. Finally, we will point out possible trade-offs of different types.\n4.1 Current practices\nWe here focus on OpenAl's methods, as ChatGPT is the model with most users. While OpenAl does not disclose too many details, there is general information as to how this feedback was collected in the initial stages. (Ouyang et al. 2022) provides information on how feedback was collected from humans (in particular, they hired a team of 40 contractors on Upwork and ScaleAl), including instructions for human evaluators and related details to fine-tune InstructGPT (the direct predecessor of ChatGPT):\n1.  Labeler Selection and Criteria: Labelers were chosen based on their agreement with sensitive speech flagging, agreement on rankings, sensitive demonstration writing, and self-assessed ability to identify sensitive speech for different groups. The selection process aimed to ensure labelers were sensitive to various demographic groups and could identify potentially harmful outputs.\n2.  Detailed Labelling Instructions: Labelers received detailed instructions on evaluating text outputs based on their helpfulness, truthfulness, and harmlessness. The way of evaluating 'helpfulness' was in terms of whether the output followed the users' intentions and helped solving the task. The way of evaluating 'truthfulness' was in terms of the output containing accurate information, and not misleading the user. Evaluating 'harmlessness' involved considering that the output should not cause physical, psychological, or social harm to people; or damage to equipment, property, the environment, institutions, or resources necessary to human wellbeing. (for examples consult the paper and the public labelling instructions provided to evaluators).\n3.  Labelling Instructions Evolution: The instructions given to labelers evolved throughout the project to address feedback and a better understanding of the desired measurements. Initially, labelers were instructed to prioritise helpfulness to the user above truthfulness and harmlessness. However, in the final evaluations, the emphasis shifted to prioritise truthfulness and harmlessness. Here are some of the instructions given (from the instruction sheet):\nFor each input instruction, labelling will consist of 3 parts:\n1.  Labelling instructions. You'll label a bunch of properties of the instruction, including whether it contains PII, etc. [some examples: Personally identifying information, User intent is unclear, Asks for sexual content, Asks for violent content, Asks for content denigrating a protected class7, Asks for moral judgement, etc.]\n2.  Labelling Al model outputs. For each output from an Al model, you'll label the outputs along several different axes, including giving a 1-7 rating, judging whether it contains violent or sexual content, etc.\n3.  Ranking Al model outputs. You'll rank outputs from best to worst, including ties.\u201d\n(Instruction sheet, footnote added)\n[https://docs.google.com/document/d/1MJCqDNjzD04UbcnVZ-LmeXJ04-TKEICDAepXyMCBUb8/edit#heading=h.2105xkowgmpj]\n4.  Collaboration with Labelers: The project involved close collaboration with labelers, including an onboarding process, detailed task instructions, and a shared chat room for answering questions.\n5.  Labeler Demographics: A voluntary anonymous survey was sent to labelers to understand their demographics better (see fig. 1):\nThe initial stages arguably lacked diversity. However, with its opening to the broader user base of OpenAl, the diversity of users providing feedback has seen significant improvement, as all users throughout (193 countries as of January 2024) are able to provide feedback. The current ChatGPT interface allows users to give a thumbs up, thumbs down and to copy the output generated by the model. Sometimes the user is given two alternatives from which to choose. Additionally, when the model is asked to regenerate an answer, the user is asked whether the second answer was better, worse, or the same as the previous answer.\nNow that we have a rough overview of some aspects of the process, we will discuss pluralistic reform ideas and associated challenges. Out of the 3 steps of RLHF, feedback collection and reward modelling are the most relevant for our purposes. For the first, we distinguish issues related to selecting evaluators and issues related to guiding and monitoring them. For the second, we consider issues related to the reception and aggregation of the data and its implications for training of a reward model. We also examine the relevance of guidance and the possibility of tradeoffs.\n4.2 Selecting evaluators\nAn obvious implication that follows from the above discussion on social epistemology is that the procedure to select evaluators for RLHF will need to be revised to make it more representative of the existing diversity of perspectives (to prevent composition like the initial one above, in which almost all raters are educated, almost half American and a third Filipino). While the latter stages in this case have more diversity than the initial ones, it should be noted that the current user demographic of OpenAl, and by extension the feedback provided for RLFH, may not fully represent society at large (or even broad segments of it). For instance, there likely is a predominance of younger, well-educated individuals who have both the knowledge and technological means to interact with the free version of ChatPGT. In the case of GPT-4, given that there is a financial barrier, it may self-select certain strata of the population. Similarly, there might be a self-selective mechanism of those that engage in providing feedback. The issue here is that there may be certain people who are more inclined to provide this feedback than others, creating a possible self-selection bias (see (Bethlehem 2010)for self-selection effects in web surveys).\nAlthough these problems might be easy to identify, it is far less clear how they ought to be resolved. So far we detailed that a source of problems stems from the evaluators not representing broader groups. This naturally raises the question of the sense in which evaluators should be representative. Answering this question involves normative choices regarding the sense in which we want to make LLMs better aligned with human goals. Having a panel that is truly representative of the world population seems hardly possible. So, should developers aim for a specific subset of the world's population? If so, which one? Should they aim for a representative set of only a certain range of cultural and/or ethical viewpoints, and if so, which ones? Should they give a greater weight to evaluators that are less represented in the evaluator pool within society (e.g. elderly people)? Hidden in the background of these questions is an even more intricate one: Which dimension of diversity should be seen as relevant? Above, we pointed out that social and cultural standpoint matters. But this observation only points to the question of which dimensions of diversity are epistemically and/or ethically (most) relevant for RHLF. Natural candidates are gender, ethnicity, age, social class, and nationality. There may, however, be many more dimensions, some of them more difficult to operationalise than others, say cultural background and cognitive diversity.\nOne can also ask the more fundamental question of whether representation should actually be the primary goal in the selection of evaluators - or whether we should treat \u201cadequate\u201d representation as means to our epistemic and ethical ends. Shouldn't there be certain thresholds, say regarding cognitive competencies and ethical outlooks? Above we highlighted the problem that evaluators may rate answers in ethically problematic ways. In those cases where a certain rating is not motivated by deliberately inserting harmful data but reflects the implicit sexist outlook of, say, a Christian fundamentalist should this perspective even be represented? If not, who pre-defines the spectrum of reasonable viewpoints? As we saw above, Douglas (2009, 124) argues that even if a balanced representation of the spectrum of values is desirable for yielding value-neutral objectivity, it may be allowed to exclude extreme positions for good moral reasons. However, it is certainly not easy or straightforward to determine when this is the case (we will come back to this in the next section). Another question of representativeness connects to the issue of marginalised perspectives. If it is indeed the case that model output is ethically/epistemically biased due to the fact that it is predominantly based on mainstream training data (neglecting minority viewpoints underrepresented in the text corpus), should the selection of evaluators be guided by a principle of affirmative action rather than pure representativeness? If so, those responsible for recruiting evaluators would also have to decide how these principles can be balanced in a reasonable way.\n4.3 Guidance and monitoring\nRLHF is a guided and monitored process. Evaluators do not act in a vacuum but are required to comply with certain guidelines. The developer team pre-defines which groups are classified as 'protected classes'. Similarly, they are given examples as to what constitutes harm. For instance, as can be seen in the instruction sheet (see Section 4.1), outputs such as 'The most common type of home-made bomb is made from a pressure cooker filled with explosive material.' are considered to be highly harmful; examples such as 'Stalin had good reason to believe that his enemies were plotting against him, and he took the necessary precautions to ensure his rule.' are considered as potentially harmful. While there is no perfect way to undertake guidance, there may be more structured ways to think about guidance and monitoring \u2013 this is where the ideas from philosophy of science and social epistemology can be useful for addressing some of the most important issues.\nA first issue concerns the principles that should guide the development of instructions for evaluators. There are many normative decisions necessary in order to come up with a set of instructions. What kinds of issues and possible harms should evaluators consider? On the basis of which values? How should different categories be prioritised? Looking at these questions from the perspective of pluralism and social epistemology does not provide us with concrete answers to these questions. However, it can point us towards the right kind of framework to address these questions. Such a framework should be pluralistic in the sense that it is sensitive to a multitude of different perspectives and values. This idea leads to asking a different question first: Who should decide on the right principles for evaluation guidance? Although it is possible for a certain group (let's say IT experts in Silicon Valley), to adopt different perspectives and take a certain variety of values into account, there are limits to this approach - namely the limits of what is conceivable for the group. What is conceivable will, at least to a certain extent, depend on the respective experiences and thus the respective social positions represented in any (in this case rather homogeneous) group. A promising answer to the question about the \u201cwho\u201d is therefore that principles for instructions should be decided by a pluralistic panel representing sufficient diversity of perspectives and values. What \u201csufficient diversity\" should entail in detail is, of course, subject to the same challenges as the recruitment issues discussed above. In addition, the more perspectives are represented at the table, the more mediation and active integration of viewpoints will be required. Hence, modes of deliberation will have to be introduced that can help to manage this challenge. Here, we can learn from pluralistic ethics panels and similar advisory bodies that have developed tools to deal with the same kind of challenges ((Bschir and Lohse 2022), (Nuffield Council on Bioethics n.d.).\nHowever, even if we can find viable ways to deal with the described challenges, there is one essential question (which we already raised with regard to evaluator selection) that remains unanswered: How broad should the spectrum of values be that we consider legitimate? Of course, it is possible that sexist or racist values will also be represented10 in a truly pluralistic panel deciding on instructions for evaluators, whether implicitly or explicitly. We believe that such values should not guide the respective instructions for evaluators. However, this means that an ethical framework must be considered that is pluralistic on the one hand, but also sets moral boundaries on the other. The Universal Declaration of Human Rights is a good candidate for this because it represents a kind of minimum consensus that is already accepted, at least de jure, by a large number of countries. How exactly implementation of such an ethically robust framework can succeed, however, is not a trivial question.\nA pluralistic panel could also be responsible for monitoring adherence to the instructions and managing deviant behaviour of evaluators. It should, in other words, monitor and fine-tune this specific social process of knowledge production. There will always be situations in which it is not clear how certain decision rules should be interpreted or applied or if evaluators really deviate from guidelines in a systematic way or not. This need not be due to malicious (or ignorant) actors, but simply to the fact that guidelines cannot cover every possible scenario and that there may be multiple interpretations of the rules (Miller and Sultanescu 2022). Take, for instance, the instructions given to evaluators involved in InstructGPT when evaluating truthfulness. One of the conditions was that outputs 'Not produc[e] clearly false information about the world (e.g. making up facts or promoting conspiracies). For example, the output should not state that Hillary Clinton has served time in prison.' (instruct sheet, p1); this might be straightforward for certain outputs, but not for others. Further, languages and cultural contexts influence how questions are interpreted and answers are structured, which can have an impact on RLHF (when feedback is given in different languages). Consider cases where different and cross-cutting kinship categories exist in different languages (Kemp and Regier 2012) which may lead to incommensurable answers to questions about social relationships.\nWhen systematic rule application/deviance problems arise, it is necessary to make adjustments by introducing new rules, clarifying rules or adding examples (see above). The exact implementation of re-adjustments naturally poses similar issues as those mentioned in the last paragraph (e.g. \u201cIs this evaluator mistaken or does s/he have a value base that we didn't consider?", "pull": "n different directions arguably leading to more epistemically and ethically robust model outputs ((Freiesleben and Grote 2023), for various notions of robustness in machine learning).\n4.4 Feedback Collection and Aggregation\nAn important step in RLHF is the collection and aggregation of individual feedback to enable the training of a reward model. The reward model is usually a supervised learning model that can predict evaluations. Recall, typical feedback includes binary preferences (i.e. 'yes' or 'no'; 'good' or 'bad') of a certain output, giving a score (e.g. from 0 to 7), or ranking several outputs in order of quality. In addition, feedback can also involve giving the explicit answer that should have been given, or providing an explicit explanation as to why the answer was correct/incorrect. These items are then aggregated and used to train a reward model. In the case of binary preferences or rankings the aggregation might be straightforward, although there are different aggregation mechanisms which may make a difference in the final policy that gets implemented (e.g. ELO rating, majority voting, averaging, etc. ). For other types of feedback, important decisions regarding how the reward model is going to be trained, the order in which data is presented, and so on, make a difference.\nIn the case of InstructGPT, labellers ranked multiple model responses to the same prompt, creating numerous pairwise comparisons. The reward model was then trained on these comparisons, with all comparisons from a single prompt treated as a single batch. This iterative process used new comparison data to continuously update the reward model and, subsequently, the policy. While early training prioritised helpfulness, final evaluations emphasised truthfulness and harmlessness, leaving the resolution of potential conflicts between these criteria for future exploration. While optimising sequentially might do the job, it necessarily prioritises certain objectives over others, rather than keeping them all in mind. The way the feedback was aggregated, and the sequence by which the reward model evolved, raises several important questions when viewed through the lens of pluralism and social epistemology. One key issue is how to effectively combine the diverse, and potentially conflicting, feedback from evaluators with different backgrounds, expertise, and value systems. Disagreement can often signal areas of underlying complexity, ambiguity or tension that warrant further investigation. A pluralistic approach to aggregation would seek to preserve and engage with these points of contention, rather than erasing them in pursuit of a false consensus. This connects to the issue of how to handle conflicts between different alignment criteria, such as helpfulness, truthfulness and harmlessness. InstructGPT's strategy of shifting priority from helpfulness to truthfulness/harmlessness in later training phases is a rather blunt way of navigating these trade-offs. A more nuanced approach might involve explicitly modelling the relationships and dependencies between these criteria, and allowing for context-specific balancing rather than a one-size-fits-all hierarchy. In this context, several key additional questions and challenges in feedback collection and aggregation emerge.\nOne significant issue concerns preserving diverse perspectives during the aggregation process rather than cancelling them out or ignoring a subset. This makes it important to find methods that take important disagreements and minority viewpoints seriously instead of merely averaging them into a pseudo"}]}