{"title": "DEIM: DETR with Improved Matching for Fast Convergence", "authors": ["Shihua Huang", "Zhichao Lu", "Xiaodong Cun", "Yongjun Yu", "Xiao Zhou", "Xi Shen"], "abstract": "We introduce DEIM, an innovative and efficient training framework designed to accelerate convergence in real-time object detection with Transformer-based architectures (DETR). To mitigate the sparse supervision inherent in one-to-one (020) matching in DETR models, DEIM employs a Dense O2O matching strategy. This approach increases the number of positive samples per image by incorporating additional targets, using standard data augmentation techniques. While Dense 020 matching speeds up convergence, it also introduces numerous low-quality matches that could affect performance. To address this, we propose the Matchability-Aware Loss (MAL), a novel loss function that optimizes matches across various quality levels, enhancing the effectiveness of Dense O2O. Extensive experiments on the COCO dataset validate the efficacy of DEIM. When integrated with RT-DETR and D-FINE, it consistently boosts performance while reducing training time by 50%. Notably, paired with RT-DETRv2, DEIM achieves 53.2% AP in a single day of training on an NVIDIA 4090 GPU. Additionally, DEIM-trained real-time models outperform leading real-time object detectors, with DEIM-D-FINE-L and DEIM-D-FINE-X achieving 54.7% and 56.5% AP at 124 and 78 FPS on an NVIDIA T4 GPU, respectively, without the need for additional data. We believe DEIM sets a new baseline for advancements in real-time object detection. Our code and pre-trained models are available at https://github.com/ShihuaHuang95/DEIM.", "sections": [{"title": "1. Introduction", "content": "Object detection is a fundamental task in computer vision, widely applied in fields like autonomous driving [5, 6], robot navigation [9], etc. The growing demand for efficient detectors has spurred the development of real-time detection methods. In particular, YOLO emerges as one of the main paradigms for real-time object detection, owing to its compelling trade-off between latency and accuracy [1, 28, 32, 34, 44]. YOLO models are widely recognized as one-stage detectors based on convolutional neural networks. One-to-many (O2M) assignment strategy has been widely used in YOLO series [1, 28, 34, 44], where each target box is associated with multiple anchors. This strategy is known to be effective, as it provides dense supervision signals, which accelerate convergence and enhance performance [44]. However, it produces multiple overlapping bounding boxes per object, requiring a hand-crafted Non-Maximum Suppression (NMS) to remove redundancies, introducing latency and instability [32, 43].\nThe advent of Transformer-based detection (DETR) paradigm [3] has attracted significant attention [4, 39, 46], leveraging multi-head attention to capture global context, thereby enhancing localization and classification. DETRs adopt a one-to-one (O2O) matching strategy that leverages the Hungarian [16] algorithm to establish a unique correspondence between predicted boxes and the ground-truth objects during training, eliminating the need for NMS. This end-to-end framework offers a compelling alternative for real-time object detection.\nHowever, slow convergence remains one of the primary limitations of DETRs, and we hypothesize that the reasons are two-fold. Sparse supervision: The O2O matching mechanism assigns only one positive sample per target, greatly limiting the number of positive samples. In contrast, O2M generates several times more positive samples. This scarcity of positive samples restricts dense supervision, which impedes effective model learning-particularly for small objects, where dense supervision is crucial for performance. Low-quality matches: Unlike traditional methods that rely on dense anchors (usually > 8000), DETR employs a small number (100 or 300) of randomly initialized queries. These queries lack spatial alignment with targets, leading to numerous low-quality matches in the training, where matched boxes have low IoU with the targets but high confidence scores.\nTo address the scarcity of supervision in DETR, recent studies have relaxed the constraints of O2O matching by incorporating O2M assignments into O2O training, thereby introducing auxiliary positive samples per target to increase supervision. Group DETR [4] achieves this by using multiple query groups, each with independent O2O matching, while Co-DETR [46] incorporates O2M methods from object detectors like Faster R-CNN [29] and FCOS [31]. Although these approaches successfully increase the number of positive samples, they also require additional decoders, which increases computational overhead and risks generating redundant high-quality predictions as traditional detectors. In contrast, we propose a novel yet straightforward approach named dense one-to-one (Dense O2O) matching. Our key idea is to increase the number of targets in each training image, which in turn generates more positive samples during the training. Notably, this can be easily achieved using classical techniques such as mosaic [1] and mixup [38] augmentations, which generates additional positive samples per image while preserving the one-to-one matching framework. Dense O2O matching can provide a level of supervision comparable to O2M approaches, without the added complexity and overhead typically associated with O2M methods.\nDespite attempts to improve query initialization using priors [18, 39, 43, 45], which enable more effective query distributions around objects. These improved initialization methods, often relying on limited feature information extracted from the encoder [39, 43], tend to cluster queries around a few prominent objects. In contrast, most non-salient objects lack nearby queries, leading to low-quality matches. This issue becomes even more pronounced when using Dense O2O. As the number of targets increases, the disparity between prominent and non-prominent targets grows, leading to a rise in low-quality matches despite the overall increase in matching quantity. In this case, if the loss function has limitations in handling these low-quality matches, this disparity will persist, hindering the model from achieving better performance.\nExisting loss functions [19, 40] in DETRs, such as Varifocal Loss (VFL) [40], are tailored to dense anchors where the number of low-quality matches is relatively low. They primarily penalize high-quality matches, especially matches with high IoU but low confidence, and discard low-quality matches. To address low-quality matches and further improve Dense O2O, we propose Matchability-Aware Loss (MAL). MAL scales the penalty based on matchability by incorporating the IoU between matched queries and targets with classification confidence. MAL performs similarly to VFL for high-quality matches but places greater emphasis on low-quality matches, improving the utility of limited positive samples during training. Furthermore, MAL provides a simpler mathematical formulation than VFL.\nThe proposed DEIM combines Dense O2O with MAL to create an effective training framework. We conducted extensive experiments on the COCO [20] dataset to evaluate the effectiveness of DEIM. The results in Fig. 1 (a) show that DEIM significantly accelerates the convergence of RT-DETRv2 [24] and D-FINE [27] and achieves improved performance as well. Specifically, with only half the number of training epochs, our method outperforms RT-DETRv2 and D-FINE by 0.2 and 0.6 AP, respectively. Additionally, our approach enables training a ResNet50-based DETR model on a single 4090 GPU, achieving 53.2% mAP within a single day (approximately 24 epochs). By incorporating more efficient models, we also introduce a new set of real-time detectors that outperform existing models, including the latest YOLOv11 [13], setting a new state-of-the-art (SoTA) for real-time object detection (Fig. 1 (b)).\nThe main contributions of this work are summarized as follows:\n\u2022 We introduce DEIM, a simple and flexible training framework for real-time object detection.\n\u2022 DEIM accelerates the convergence by improving the"}, {"title": "2. Related Work", "content": "Object detection with transformer (DETR) [3] represents a shift from traditional CNN architectures to transformers. By using Hungarian [16] loss for one-to-one matching, DETR eliminates the need for hand-crafted NMS as the post-processing and enables end-to-end object detection. However, it suffers from slow convergence and dense computation.\nIncreasing positive samples. One-to-one matching limits each target to a single positive sample, providing far less supervision than O2M and hindering the optimization. Some studies have explored ways to increase supervision within the O2O framework. Group DETR [4], for instance, employs the concept of \"groups\" to approximate the O2M. It uses K groups of queries, where  K > 1, and performs O2O matching independently within each group. This allows each target to be assigned  K positive samples. However, to prevent communication between groups, each group requires a separate decoder layer, ultimately resulting in  K parallel decoders. The hybrid matching scheme in H-DETR [15] works similarly to Group DETR. Co-DETR [46] reveals that a one-to-many assignment approach helps the model learn more distinctive feature information, so it proposed a collaborative hybrid assignment scheme to enhance encoder representations through auxiliary heads with one-to-many label assignments, like Faster R-CNN [29] and FCOS [31]. The existing methods aim to increase the number of positive samples per target to enhance supervision. In contrast, Our Dense O2O explores another direction - increasing the number of targets per training image to boost supervision effectively. Unlike existing methods, which require additional decoders or heads and thus increase training resource consumption, our approach is computation-free.\nOptimizing low-quality matches. The sparse and randomly initialized queries lack spatial alignment with targets, resulting in a high proportion of low-quality matches that impede model convergence. Several methods have introduced prior knowledge into query initialization, such as anchor queries [35], DAB-DETR [21], DN-DETR [18], and dense distinct queries [41]. More recently, inspired by two-"}, {"title": "3. Method", "content": "3.1. Preliminaries\nO2M vs. 020. The O2M assignment strategy [10, 44] is widely adopted in traditional object detectors, and its supervision can be formulated as follows:\nloss = \\sum_{i=0}^{N} \\sum_{j=0}^{M_i} f (\\hat{y}_{ij}, y_i),                                       (1)\nwhere  N is the total number of targets,  M_i is the number of matches for the  i -th target,  \\hat{y}_{ij} represents the  j -th match for the  i -th target,  y denotes the  i -th ground-truth label, and  f is the loss function. O2M enhances supervision by increasing  M_i, i.e., assigning multiple queries to each target ( M_i > 1) and thus providing dense supervision, as illustrated in Fig. 2a. In contrast, the O2O assignment only pairs each target with a single best prediction, determined via the Hungarian algorithm, which minimizes a cost function balancing classification and localization errors (Fig. 2b). O2O can be considered a special case of O2M where M_i = 1 for all targets.\nFocal loss. Focal loss (FL) [19] was introduced to prevent an abundance of easy negatives from overwhelming the detector during training, directing focus instead towards a sparse set of hard examples. It serves as the default classification loss in DETR-based detectors [39, 45] and is defined as follows:\nFL(p, y) = \\begin{cases}\n  - \\alpha (1-p)^\\gamma log(p) & y = 1 \\\\\n  -(1-\\alpha) p^\\gamma log(1-p) & y = 0,\n\\end{cases}                                                                                                                               (2)\nwhere  y \u2208 {0, 1} specifies the ground-truth class and  p \u2208 [0, 1] represents the predicted probability for the foreground class. The parameter  \u03b3 controls the balance between easy and hard samples, while  \u03b1 adjusts the weighting between foreground and background classes. In the FL, only the sample's class and confidence are considered, with no attention given to bounding box quality, i.e., localization."}, {"title": "3.2. Improving matching efficiency: Dense 020", "content": "The one-to-one (O2O) matching scheme, commonly used in DETR-based models, matches each target to only one predicted query. This approach, implemented via the Hungarian algorithm [16], allows for end-to-end training and eliminates the need for NMS. However, a key limitation of O2O is that it generates significantly fewer positive samples compared to traditional one-to-many (O2M) methods like SimOTA [44]. This leads to sparse supervision, which can slow down convergence during training.\nTo better understand this issue, we trained RT-DETRv2 [24] with a ResNet50 backbone on the MS COCO dataset [20]. We compared the number of positive matches generated by both Hungarian (O2O) and SimOTA (O2M) strategies. As shown in Fig. 3a, O2O produces a sharp peak under 10 positive matches per image, while O2M generates a broader distribution with many more positive matches, sometimes exceeding 80 positive samples for a single image. Fig. 3b further highlights that SimOTA generates about 10 times as many matches as O2O in extreme cases. This demonstrates that O2O has fewer positive matches, potentially slowing down optimization.\nWe propose Dense O2O as an efficient alternative. This strategy retains the one-to-one matching structure of O2O (with  M_i = 1), but increases the number of targets ( N) per image, achieving denser supervision. For example, as shown in Fig. 2c, we replicate the original image into four quadrants and combine them into a single composite image, maintaining the original image dimensions. This increases the number of targets from 1 to 4, boosting the supervision level in Eq. 1 while keeping the matching structure unchanged. Dense O2O achieves a level of supervision comparable to O2M but without the added complexity and computational overhead."}, {"title": "3.3. Improving matching quality: Matchability-Aware Loss", "content": "Limitations of VFL. The VariFocal Loss (VFL) [40], built on the FL [19], has been shown to improve object detection performance, especially in DETR models [2, 24, 43]. VFL loss is expressed as :\nVFL(p, q, y) = \\begin{cases}\n -q(q \\log(p) + (1 - q)\\log(1 - p)) & q > 0 \\\\\n - \\alpha p^\\gamma \\log(1 - p) & q = 0,\n\\end{cases}                                                     (3)\nwhere  q denotes the IoU between the predicted bounding box and its target box. For foreground samples ( q > 0), the target label is set to  q, while background samples ( q = 0) have a target label of 0. VFL incorporates the IoU to improve the quality of queries in DETR [43].\nHowever, VFL has two key limitations when optimizing low-quality matches: i). Low-Quality Matches. VFL focuses mainly on high-quality matches (high IoU). For low-quality matches (low IoU), the loss remains small, preventing the model from refining predictions for low-quality boxes. For low-quality matching (with low IoU, e.g., Fig. 2d), however, the loss remains minimal (marked by a in Fig. 2e). ii) Negative Samples. VFL treats matches with no overlap as negative samples, which reduces the number of positive samples and limits effective training.\nThese issues are less problematic for traditional detectors due to their dense anchors and multiple assignment strategies. However, in the DETR framework, where queries are sparse and matching is more rigid, these limitations become more pronounced.\nMatchability-Aware Loss. To address these issues, we propose the Matchability-Aware Loss (MAL), which extends the benefits of VFL while mitigating its shortcomings. MAL incorporates the matching quality directly into the loss function, making it more sensitive to low-quality matches. The formula for MAL is:\nMAL(p, q, y) = \\begin{cases}\n -q^\\gamma \\log(p) + (1 - q^\\gamma) \\log(1 - p) & y = 1 \\\\\n -p \\log(1 - p) & y = 0.\n\\end{cases}                                             (4)\nCompared to VFL, we introduce several small but important changes. Specifically, the target label has been modified from  q to q^\\gamma, simplifying the loss weights for positive and negative samples and removing the hyperparameter  \u03b1 used to balance positive and negative samples. This change helps to avoid the overemphasis on high-quality boxes and improves the overall training process. This can be easily seen from the loss landscape between VFL (in Fig. 2e) and MAL (in Fig. 2f). Note that the impact of  \u03b3 is provided in Section 4.5.\nComparison with VFL. We compare MAL and VFL in handling both low-quality and high-quality matches. In the case of low-quality matches (IoU = 0.05, in Fig. 4a), MAL shows a sharper increase in loss as predicted confidence grows, compared to VFL, which remains almost unchanged. For high-quality matches (IoU = 0.95, in Fig. 4b), both MAL and VFL perform similarly, confirming that MAL improves training efficiency without compromising the performance on high-quality matches."}, {"title": "4. Experiments", "content": "4.1. Training details\nFor Dense O2O, we apply mosaic augmentation [1] and mixup augmentation [38] to generate additional positive samples per image. The impact of these augmentations is discussed in Section 4.5. We train our models on the MS-COCO dataset [20] using the AdamW optimizer [23]. Standard data augmentations, such as color jitter and zoom-out, are used, as in RT-DETR [24, 43] and D-FINE [27]. We employ a flat cosine learning rate scheduler [25] and propose a novel data augmentation scheduler. A data augmentation warmup strategy is used in the first few training epochs (four usually) for simplifying attention learning. Disabling Dense O2O after 50% of training epochs leads to better results. Following RT-DETRv2 [43], we turn off data augmentation in the last two epochs. Our LR and DataAug schedulers are depicted specifically in Fig. 5. Our backbones are pre-trained on ImageNetlk [8]. We evaluate our models on the MS-COCO validation set at a resolution of 640 \u00d7 640. Additional details about the hyperparameters are provided in the supplementary material.\n4.2. Comparisons with real-time detectors\nWe integrate our method into D-FINE-L [27] and D-FINE-X [27] building our DEIM-D-FINE-L and DEIM-D-FINE-X. We then evaluate these models and benchmark their real-time object detection performance against state-of-the-art models, including YOLOv8 [12], YOLOv9 [34], YOLOv10 [34], YOLOv11 [13], as well as DETR-based models like RT-DETRv2 [24] and D-FINE [27]. Tab. 1 compares the models in terms of epochs, parameters, GFLOPs, latency, and detection accuracy. Additional comparisons of smaller model variants (S and M) are included in the supplementary material.\nOur method outperforms the current state-of-the-art models in training cost, inference latency, and detection accuracy, setting a new benchmark for real-time object detection. Note that D-FINE [27] is a very recent work that enhances the performance of RT-DETRv2 [24] by incorporating distillation and bounding box refinement, establishing itself as a leading real-time detector. Our DEIM further boosts the performance of D-FINE, achieving a 0.7 AP gain while reducing training costs by 30%, with no added inference latency. The most significant improvement is observed in small object detection, where D-FINE-X [27], when trained with our method, achieves a 1.5 AP gain as DEIM-D-FINE-X.\nWhen compared directly to YOLOv11-X [13], our method outperforms this state-of-the-art YOLO model, achieving slightly higher performance (54.7 vs. 54.1 AP) and reducing inference time by 20% (8.07 ms vs. 10.74 ms). Although YOLOv10 [34] uses a hybrid O2M and O2O assignment strategy, our models consistently outperform YOLOv10, demonstrating the effectiveness of our Dense 020 strategy.\nDespite significant improvements in small object detection over other DETR-based models, our approach shows a slight decrease in small object AP compared to YOLO models. For example, YOLOv9-E [34] outperforms D-FINE-L [27] by approximately 1.4 AP on small objects, though our model achieves a higher overall AP (56.5 vs. 55.6). This gap underscores the ongoing challenges in small object detection within the DETR architecture and suggests potential areas for further improvement."}, {"title": "4.3. Comparisons with ResNet [14]-based DETRS", "content": "Most DETR research uses ResNet [14] as the backbone, and to enable a comprehensive comparison across existing DETR variants, we also applied our method to RT-DETRv2 [24], a state-of-the-art DETR variant. The results are summarized in Tab. 2. Unlike the original DETR, which requires 500 epochs for effective training, recent DETR variants, including ours, reduce training time while improving model performance. Our method shows the most significant improvements, surpassing all variants after just 36 epochs. Specifically, DEIM reduces training time by half and increases AP by 0.5 and 0.9 on RT-DETRv2 [24] with ResNet-50 [14] and ResNet-101 [14] backbones, respectively. Moreover, it outperforms DINO-Deformable-DETR [39] by 2.7 AP with the ResNet-50 [14] backbone.\nDEIM also significantly enhances small-object detection. For example, while achieving comparable overall AP to RT-DETRv2 [24], our DEIM-RT-DETRv2-R50 surpasses RT-DETRv2 by 1.3 AP on small objects. This improvement is even more pronounced with the larger ResNet-101 backbone, where our DEIM-RT-DETRv2-R101 outperforms RT-DETRv2-R101 by 2.1 AP on small objects. Extending training to 72 epochs further improves overall performance, especially with the ResNet-50 backbone, indicating that smaller models benefit from additional training."}, {"title": "4.4. Comparisons on CrowdHuman", "content": "CrowdHuman [30] is a benchmark dataset designed to evaluate object detectors in dense crowd scenarios. We applied both D-FINE and our proposed method to the CrowdHuman dataset, following the configurations provided in the official repository 1. As shown in Tab. 3, our approach (D-FINE-L enhanced with DEIM) achieves a notable improvement of 1.5 AP over D-FINE-L. In particular, our method delivers a significant performance boost (greater than 3% improvement) on small objects (AP) and high-quality detections (AP75), demonstrating its ability to detect objects more accurately in challenging scenarios. Furthermore, this experiment underscores the strong generalization capability"}, {"title": "4.5. Analysis", "content": "In the following studies, we use RT-DETRv2 [24] paired with ResNet50 [14] to conduct experiments and report the"}]}