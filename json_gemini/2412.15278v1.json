{"title": "DreaMark: Rooting Watermark in Score Distillation Sampling Generated Neural Radiance Fields", "authors": ["Xingyu Zhu", "Xiapu Luo", "Xuetao Wei"], "abstract": "Recent advancements in text-to-3D generation can generate neural radiance fields (NeRFs) with score distillation sampling, enabling 3D asset creation without real-world data capture. With the rapid advancement in NeRF generation quality, protecting the copyright of the generated NeRF has become increasingly important. While prior works can watermark NeRFs in a post-generation way, they suffer from two vulnerabilities. First, a delay lies between NeRF generation and watermarking because the secret message is embedded into the NeRF model post-generation through fine-tuning. Second, generating a non-watermarked NeRF as an intermediate creates a potential vulnerability for theft. To address both issues, we propose DREAMARK to embed a secret message by backdooring the NeRF during NeRF generation. In detail, we first pre-train a watermark decoder. Then, DREAMARK generates backdoored NeRFs in a way that the target secret message can be verified by the pre-trained watermark decoder on an arbitrary trigger viewport. We evaluate the generation quality and watermark robustness against image- and model-level attacks. Extensive experiments show that the watermarking process will not degrade the generation quality, and the watermark achieves 90+% accuracy among both image-level attacks (e.g., Gaussian noise) and model-level attacks (e.g., pruning attack).", "sections": [{"title": "Introduction", "content": "Digital 3D content has become indispensable in Metaverse and virtual and augmented reality, enabling visualization, comprehension, and interaction with complex scenes that represent our real lives. Recent progress in 3D content generation (Poole et al. 2022; Lin et al. 2023; Wang et al. 2024; Liang et al. 2024) can generate high-quality 3D assets that need a lot of time, computational resources, and skilled expertise. Therefore, protecting the ownership of generated 3D content has become more critical.\nWe focus on Text-to-3D generation (Poole et al. 2022; Lin et al. 2023; Wang et al. 2024; Liang et al. 2024) and the neural radiance field (NeRF) (Mildenhall et al. 2021; M\u00fcller et al. 2022), which have emerged into the spotlight in 3D content modeling. Current trending 3D generation algorithms generate 3D representations such as meshes and\nNeRFs. This paper focuses on NeRF generation since NeRF can represent 3D models more compactly. Given a textual description, recent text-to-3D methods generate NeRFs by distilling pre-trained diffusion models, such as Stable Diffusion (Rombach et al. 2022). This remarkable progress is grounded in the use of Score Distillation Sampling (SDS). With SDS, NeRF training can be conducted without realistic images. Thus, the research question we address in this paper is: how to protect the score distillation sampling generated neural radiance fields?\nOne way to protect the generated NeRF is to apply post-generation watermarking methods, such as CopyRNeRF (Luo et al. 2023) and WateNeRF (Jang et al. 2024), to watermark NeRF after it is generated. However, these methods exhibit two problems. First, post-generation methods pose a risk of data leakage. As shown in Figure 1, since non-watermarked intermediates are generated in the post-generation pipeline, a malicious user could leak the non-watermarked version of the generated content. Second, CopyRNeRF increases the watermarking expense since it requires an additional message feature field in the NeRF structure. Integrating CopyRNeRF with an arbitrary text-to-NeRF pipeline requires additional modifications to the NeRF structure. Recognizing these limitations of previous work, can we conduct a during-generation watermarking without modifying the NeRF structure?\nWe propose DREAMARK, the first during-generation text-to-3D watermarking method, which is gracefully combined with score distillation sampling to generate high-quality and watermarked NeRF. Different from post-generation NeRF watermarking method, DREAMARK directly generates watermarked NeRF without changing NeRF architecture, increasing the flexibility for future development on 3D generation. Our method is inspired by black-box model watermarking methods (Adi et al. 2018; Zhang et al. 2018; Jia et al. 2021; Le Merrer, Perez, and Tr\u00e9dan 2020; Chen, Rouhani, and Koushanfar 2019; Szyller et al. 2021) which watermark a deep neural network by injecting backdoors. To inject backdoors in NeRF during generation, we first generate a trigger view set dependent only on the given secret message. Then, we conduct score distillation sampling in a way that the secret message can be extracted from images rendered from arbitrary trigger viewports. To extract the secret message from the rendered image, we use a pre-trained watermark decoder from HiDDeN (Zhu et al. 2018). All NeRF generated by DREAMARK can be verified as watermarked by such a unique decoder.\nTwo critical evaluation metrics for watermarking algorithms are invisibility and robustness. For robustness, we evaluate bit accuracy under multiple image transformations, such as Gaussian noise, before images rendered from trigger viewports are fed into the watermark decoder. For invisibility, there is no such the \"original NeRF\u201d since we root watermarks during a generation task, so DREAMARK cannot be evaluated by Peak Signal-to-Noise Ratio (PSNR) as is done in post-generation methods. However, we can still evaluate the invisibility by evaluating the generation quality as previous 2D watermarking tasks (Wen et al. 2024; Yang et al. 2024), where they use CLIP Score (Radford et al. 2021) to show the generation quality. Extensive experiments show that DREAMARK successfully embeds the watermark in a during-generation way and maintains robustness under multiple image transformations without degrading the generation quality. In summary, our contributions are as follows:"}, {"title": "Related Work", "content": "One category of text-to-3D generation starts from DreamField (Jain et al. 2022), which trains NeRF with CLIP (Radford et al. 2021) guidance to achieve text-to-3D distillation. However, the generated content is unsatisfactory due to the weak supervision from CLIP loss. Hence, our work will not consider watermarking CLIP-guided 3D content generation. Another category starts from Dreamfusion (Poole et al. 2022), which pioneerly introduces Score Distillation Sampling (SDS) to optimize NeRF by distilling a pre-trained text-to-image diffusion model. This motivates a great number of following works to propose critical incremental. These works improve the quality of generation in various ways. For example, Fantasia3D (Chen et al. 2023), Magic3D (Lin et al. 2023), Latent-nerf (Metzer et al. 2023), DreamGaussian (Tang et al. 2023) and GaussianDreamer (Yi et al. 2023) improve the visual quality of generated content by changing 3D representations or improving NeRF structure. MVDream (Shi et al. 2023) focuses on addressing Janus problems by fine-tuning the pre-trained diffusion model to make it 3D aware. However, SDS guidance still suffers from over-saturation problems, as is shown in Magic3D (Lin et al. 2023), Dreamfusion (Poole et al. 2022), and AvatarVerse (Zhang et al. 2024). The other, like ProlificDreamer (Wang et al. 2024) and LucidDreamer (Liang et al. 2024), focus on improving SDS itself. For example, LucidDreamer uncovers the reason for the overly-smoothed problem that SDS guides the generation process towards an averaged pseudo-groundtruth and proposes ISM to relieve such a problem. ProlificDreamer proposes VSD guidance instead of SDS guidance and shows that SDS is just a special case of VSD. Although extensive research has been proposed to improve text-to-3D generation, these works still require a much longer training stage, which makes it necessary to protect the copyright of generated content.\nDigital Watermarking\nDigital watermarking hides watermarks into multimedia for copyright protection or leakage source tracing. Various research works have been proposed to protect traditional multimedia content like 2D images and 3D meshes. Early works watermark images and meshes by embedding a secret message in either the least significant bits (Van Schyndel, Tirkel, and Osborne 1994) or the most significant bits (Tsai 2020; Jiang et al. 2017; Tsai and Liu 2022; Peng, Liao, and Long 2022; Peng, Long, and Long 2021) of image pixels and vertex coordinates. HiDDeN (Zhu et al. 2018) and Deep3DMark (Zhu et al. 2024) have made substantial improvements using deep learning networks.\nRecently, several watermarking methods have emerged in the NeRF domain. StegaNeRF (Li et al. 2023) designed a steganography algorithm that hides natural images in 3D scene representation. CopyRNeRF (Luo et al. 2023) protects the copyright of NeRF by verifying the secret message extracted from images rendered from the protected NeRF. WateNeRF (Jang et al. 2024) further improves NeRF watermarking by hiding secret messages into the frequency"}, {"title": "Preliminaries", "content": "NeRF. NeRF (Mildenhall et al. 2021) uses multilayer perceptrons (MLPs) $f_{\\Theta}$ and $f_{\\phi}$ to map the 3D location $x \\in \\mathbb{R}^3$ and viewing direction $d \\in \\mathbb{R}^2$ to a color value $c\\in \\mathbb{R}^3$ and a geometric value $\\sigma\\in \\mathbb{R}^+$:\n$\\sigma, z = f_{\\Theta}(x(x)),$ (1)\n$c = f_{\\phi}(z, \\gamma_d(d)),$ (2)\nwhere $\\gamma_x, \\gamma_d$ are fixed encoding functions for location and viewing direction, respectively. The intermediate variable $z$ is a feature output by the first MLP $f_{\\Theta}$. To render a $H \\times W$ image with the given viewport $p$, the rendering process casts rays {$r_i$}$_{i=1}^{H \\times W}$ from pixels and computes the weighted sum of the color $c_j$ of the sampling points along each ray to composite the color of each pixel:\n$C(r_i) = \\sum_j T_j (1 - \\exp(-\\sigma_j \\delta_j))c_j,$ (3)\nwhere $T_j = \\prod_{k=1}^{j-1} \\exp(-\\sigma_k \\delta_k)$, and $\\delta_j$ is the distance between adjacent sample points. In later chapters, we use $g(\\Theta,p) \\in [0,1]^{H \\times W \\times 3}$ to represent the above rendering process, where $\\Theta$ represents parameters of a NeRF, and $g$ takes viewport $p$ as input and outputs a normalized image.\nDiffusion models. A diffusion model (Song et al. 2020; Ho, Jain, and Abbeel 2020; Song, Meng, and Ermon 2020) involves a forward process {$q_t$}$_{t\\in[0,1]}$ to gradually add noise to a data point $x_0 \\sim q_0(x_0)$ and a reverse process {$p_t$}$_{t\\in[0,1]}$ to denoise/generate data. The forward process is defined by $q_t(x_t|x_0) := \\mathcal{N}(\\alpha_t x_0, \\sigma^2_t I)$ and $q_t(x_t) := \\int q_t(x_t|x_0)q_0(x_0)dx_0$, where $\\alpha_t, \\sigma_t > 0$ are hyperparameters; and the reverse process is defined by denoising from $p_1(x_1) := \\mathcal{N}(0, I)$ with a parameterized noise prediction network $\\epsilon_{\\phi}(x_t, t)$ to predict the noise added to a clean data $x_0$, which is trained by minimizing:\n$\\mathcal{L}_{Diff}(\\phi) = E_{x_0, t, \\epsilon} [w(t)||\\epsilon_{\\phi}(\\alpha_t x_0 + \\sigma_t \\epsilon) - \\epsilon||^2],$ (4)\nwhere $w(t)$ is a time-dependent weighting function. After training, we have $p_t \\approx q_t$; thus, we can draw samples from $p_0 \\approx q_0$. One of the most important applications is text-to-image generation (Rombach et al. 2022; Ramesh et al. 2022), where the noise prediction model $\\epsilon_{\\phi}(x_t, t, y)$ is conditioned on a text prompt $y."}, {"title": "Text-to-3D generation by score distillation sampling", "content": "(SDS) (Poole et al. 2022). SDS is widely used in text-to-3D generation (Lin et al. 2023; Wang et al. 2024; Liang et al. 2024), which lift 2D information upto 3D NeRF by distilling pre-trained diffusion models. Given a pre-trained text-to-image diffusion model $p_t(x_t|y)$ with the noise prediction network $\\epsilon_{\\phi}(x_t, t, y)$, SDS optimizes a single NeRF with parameter $\\Theta$. Given a camera viewport $p$, a prompt $y$ and a differentiable rendering mapping $g(\\Theta, p)$, SDS optimize the NeRF $\\Theta$ by minimizing:\n$\\mathcal{L}_{SDS}(\\Theta) = E_{t,p} [w(t) D_{KL}(q(x_t|g(\\Theta, p))||p_t(x_t|y))]$, (5)\nwhere $x_t = \\alpha_t g(\\Theta, p) + \\sigma_t \\epsilon$. Its gradients are approximated by:\n$\\nabla_{\\Theta} \\mathcal{L}_{SDS} = E_{t,p} [w(t) (\\epsilon_{\\phi}(x_t, t, y) - \\epsilon) \\frac{\\delta g(\\Theta, p)}{\\delta \\Theta}].$ (6)"}, {"title": "Proposed Method", "content": "DREAMARK watermarks generation process of neural radiance fields (NeRF) by injecting backdoors during score distillation sampling (SDS). The message can be extracted from the rendered image of trigger viewports through a fixed watermark decoder. Our method is conducted in two phases. First, we pre-train the watermark decoder WD. Then, we inject backdoors into a high-resolution NeRF during SDS optimization, such that images rendered from the trigger viewports yield a secret message.\nPre-train the watermark decoder\nDifferent from CopyRNeRF (Luo et al. 2023), which trains a separate watermark decoder for each watermarked NeRF, DREAMARK employs a unique watermark decoder. This allows the NeRFs generated by our method to be verified using this unique decoder.\nBuilding watermark decoder training pipeline. For simplicity, we use HiDDeN (Zhu et al. 2018) as our WD architecture, a well-established image watermarking pipeline. It optimizes watermark encoder WE and watermark decoder WD for signature embedding and extraction. The encoder WE takes a cover image $x_o\\in \\mathbb{R}^{H \\times W \\times 3}$ and a k-bit message $m \\in {0,1}^k$ as input and outputs a watermarked image $x_w \\in \\mathbb{R}^{H \\times W \\times 3}$. The decoder takes watermarked image $x_w$ as input and outputs a predicted secret message $m'$. The extracted message $m'$ is restricted to [0, 1] by utilizing a sigmoid function. The message loss is calculated with Binary Cross Entropy (BCE) between m and sigmoid $sg(m')$:\n$\\mathcal{L}_{msg} = \\sum_{i=0}^{L-1} m_i \\log sg(m') + (1 - m_i) \\cdot \\log(1 - sg(m')).$ (7)\nThe WE is discarded in the later phase since only WD serves our purpose.\nOriginal HiDDeN architecture combines message loss and perceptual loss to optimize both We and WD. However, since WE is discarded and the perceptual loss is not needed, we follow the tradition (Fernandez et al. 2023; Jang"}, {"title": "Dreamark", "content": "Inspired from existing black-box model watermarking (Adi et al. 2018; Zhang et al. 2018; Jia et al. 2021; Le Merrer, Perez, and Tr\u00e9dan 2020; Chen, Rouhani, and Koushanfar 2019; Szyller et al. 2021), where they root backdoors in a deep neural network to achieve DNN watermarking, we watermark NeRF by rooting backdoors during SDS optimization. Formally, given a NeRF with parameter $\\Theta$, a prompt $y$, a secret message $m$, we aim to optimize the NeRF such that the message $m$ can be decoded by $W_D$ from the image $g(\\Theta, p_\\tau)$ rendered from arbitrary trigger viewport $p_\\tau$.\nGenerate Trigger Viewports. We wish to generate a set of trigger viewports {$p_\\tau$}$_{i=1}^{N}$ from the secret message $m$ such that the watermark verifier does not need to keep a replica of the trigger viewport set. Besides, different messages should generate different viewports because a constant trigger viewport set is easy to predict, leading to potential vulnerability. We use a pseudo-random number generator (PRNG) to generate the m-dependent trigger viewport set {$p_\\tau$}$_{i=1}^{N}$ as shown in Algorithm 1.\nChoosing Trigger Embedding Media. After generating m-dependent trigger viewport set {$p_\\tau$}$_{i=1}^{N}$, the question is how to choose suitable cover media to hide secret message $m$, such that $m$ can be decoded from the image rendered\nTwo-stage Trigger Embedding. To backdoor in color mapping $f_{\\phi}$ only, we divide SDS optimization into two stages. In the first stage, we optimize a high-resolution NeRF (e.g., 512) by SDS (Eq.(5)) with joint optimization of both $f_{\\phi}$ and $f_{\\Theta}$. The aim of the first stage is to generate scenes with complex geometry. In the second stage, we freeze $f_{\\Theta}$ to fine-tune $f_{\\phi}$ by the following combined loss to conceal watermarks in trigger viewports $p_\\tau$:\n$\\mathcal{L}_{comb}(\\Theta) = \\mathcal{L}_{SDS} + E_p [BCE(W_D(g(\\Theta, p_\\tau)), m)].$ (8)\nNote that equation 8 optimizes $\\Theta$ by SDS across arbitrary viewports p, and BCE across trigger viewports $p_\\tau$.\nWatermark Extraction. Given a suspicious NeRF $g(\\Theta, p)$, the NeRF creator can first generate the trigger viewport set {$p_\\tau$}$_{i=1}^{N}$ following Algorithm 1 based on his secret message m. Then the decoded message $m'$ can be extracted from the image rendered from arbitrary trigger viewport $p_\\tau \\in {P_{\\tau}}$}$_{i=1}^{N}$ . The ownership can be verified by evaluating the bitwise accuracy between $m'$ and $m$."}, {"title": "Implementation Details", "content": "Pretrained Watermark Extractor. We pretrain the watermark encoder WE and extractor $W_D$ using COCO (Lin et al. 2014) dataset. We build $W_E$ with four-layer MLPs and $W_D$ with eight-layer MLPs, with all intermediate channels set to 64. During pretraining, the input image resolution is set to $256 \\times 256$, and the output message length is set to 48 to satisfy the capacity requirements of downstream watermarking tasks. We use Lamb (You et al. 2019) and CosineLRScheduler to schedule the learning rate, which decays to $1 \\times 10^{-6}$. This process is done in 500 epochs.\nNeRF. We choose Instant NGP (M\u00fcller et al. 2022) for efficient high-resolution (e.g., up to 512) rendering. Given input coordinate $x$, we use a 16-level progressive grid for input encoding with the coarsest and finest grid resolution set to 16 and 2048, respectively. The encoded input is further fed into $f_{\\phi}$ and $f_{\\Theta}$, which are both built with one-layer MLPs with 64 channels, to predict the color $c_j$ and density of of input $x$. We follow the object-centric initialization used in Magic3D (Lin et al. 2023) to initialze density for NeRF as $\\rho_{init}(x) = \\lambda (1 - \\frac{||x||}{r}^2)$, where $\\lambda$ = 10 is the density strength, r = 0.5 is the density radius and x is the coordinate. We use Adam optimizer with learning rate $10^{-3}$ to optimize NeRF in both stages. The guidance model is Stable Diffusion (Rombach et al. 2022) with the guidance scale set to 100. During SDS optimization, we sample time $t \\sim U(0.02, 0.98)$ in each iteration. We jointly optimize $f_{\\phi}$ and $f_{\\Theta}$ for 40000 iterations in stage one and fine-tune $f_{\\phi}$ only for 30000 iterations in stage two."}, {"title": "Experiment Setup", "content": "We select 16-bit secret messages and N = 1000 trigger viewports in our experiment unless explicitly mentioned. To evaluate DREAMARK, we use 100 different prompts to generate 100 watermarked scenes. Note that the scale of our experiment far exceeds that of prior works where they only evaluate Blender (Mildenhall et al. 2021) and LLFF (Mildenhall et al. 2019) dataset, with each dataset only containing eight scenes. All our experiments are conducted on Ubuntu 22.04 with an Intel Xeon Gold 5318Y CPU and an NVIDIA A100.\nEvaluation Metrics. Two key evaluations for watermarking algorithms are invisibility and robustness. We evaluate robustness using bit accuracy under various image distortions such as Gaussian Noise, Rotation, Scaling, Gaussian Blur, Crop, and Brightness. For invisibility evaluation, different from the previous post-generation watermarking algorithm, there is no such the \u201coriginal NeRF\u201d. Hence, the typical evaluation metric, the Peak-Signal-to-Noise Ratio (PSNR), is not applicable to evaluate our method. We follow prior 2D during-generation watermarking algorithm (Yang et al. 2024; Wen et al. 2024) where they use CLIP-Score"}, {"title": "Attacks on Dreamark's Watermarks", "content": "This section aims to examine the robustness of the watermark against various attacks. We first consider image-level attack, which performs arbitrary image transformations and is typical for many NeRF watermarking methods (Luo et al. 2023; Jang et al. 2024). We then consider model-level attacks such as fine-tuning and pruning since the generated NeRF could be made public; in this case, the attacker will have white-box access to the generated NeRF. Besides, model-level attacks are commonly evaluated in model watermarking methods (Adi et al. 2018; Zhang et al. 2018; Jia et al. 2021; Le Merrer, Perez, and Tr\u00e9dan 2020; Chen, Rouhani, and Koushanfar 2019; Szyller et al. 2021).\nRobustness against image-level attacks\nWe evaluate the robustness of the watermark against different image transformations before rendered images are fed into the watermark decoder. We consider Gaussian Noise (v=0.1), rotation ($\\pm \\pi/6$), Scaling (25%), Gaussian Blur (deviation=0.1), Crop (40%) and Brightness (2.0). Bit accuracy is averaged on all transformed images rendered from trigger viewports. Table 5 shows DREAMARK is robust against previously mentioned image-level attacks. The bit accuracy is always above 90% except for rotation. Note that the robustness is achieved without the need for transformations during the DREAMARK optimization phase: it is attributed to the watermark decoder. If the watermark decoder is trained to withstand arbitrary transformation, the generated NeRF"}, {"title": "Robustness against model-level attacks", "content": "This subsection considers the scenario when an attacker gets full access to the generated NeRF model and aims to remove the embedded watermark without degrading its visual quality. We denote $x_w$ as images rendered from watermarked NeRF and $x_a$ as images rendered from attacked NeRF. We use PSNR($x_w$,$x_a$) = -10\u00b7log10(MSE($x_w$, $x_a$)), for $x_a, x_w \\in [0,1]^{c \\times h \\times w}$ to evaluate distortion made by attacks.\nModel Fine-tuning. Since our method uses a unified watermark decoder WD to decode the secret message, we consider two scenarios of fine-tuning attack. One assumes that the attacker has full access to the watermark decoder WD, and the other assumes that the attacker has no access to WD. Besides, we assume the attacker has no prior knowledge of the secret message m. In this case, the attacker cannot re-\nproduce trigger viewports since trigger viewports are only related to m. For the first scenario, when the attacker has full access to WD, the attacker can use an adversarial attack to partially remove the watermark by minimizing the BCE loss between the extracted message and a random binary message sampled beforehand:\n$\\mathcal{L}_{fine-tune} (\\Theta') =$\n$E_p [||g(\\Theta', p) - g(\\Theta, p)||^2 + BCE(W_D(g(\\Theta', p)), m')],$ (9)\nwhere $\\Theta$ is the fixed parameter of watermarked NeRF, $\\Theta'$ is the parameter of NeRF to be fine-tuned, m' is a random binary message different from m. As shown in Fig. 5, even if PSNR drops below 26dB, bit accuracy is still above 90%. For the second scenario, when the attacker has no access to WD, the attacker cannot produce the adversarial attack to remove the watermark.\nModel Pruning. Model pruning is widely used in model compression since it can reduce the storage and computation cost of DNNs. However, pruning will affect not only the size and operation speed of the model but also the accuracy of the watermark and the visual quality of NeRF. A higher pruning rate gives lower watermark accuracy and lower visual quality. In practice, we vary the pruning rate and, at the same time, evaluate PSNR($x_a$, $x_w$) and bit accuracy on $x_a$. The pruning attack with pruning rate a% means setting the smallest a% of network parameters to zero, where the size of the network parameter is evaluated by its $l_1$ norm. Fig. 5 shows that our method is robust against pruning attack since we still have ~88% accuracy when PSNR is below 27dB, while 27dB PSNR means relatively high distortion has been made in image watermarking context (Zhu et al. 2018)."}, {"title": "Conclusion", "content": "In this work, we propose a during-generation text-to-3D watermarking method, DREAMARK, which eliminates the delay between the generation phase and the watermarking phase: the watermark can be verified on the generated NeRF once the generation is finished. Inspired by the black-box model watermarking method, DREAMARK watermarks NeRF by injecting backdoors into NeRF such that a secret message can be extracted from images rendered from arbitrary trigger viewport. Extensive experiments show that our method will not degrade generation quality and maintain robustness against image-level and model-level attacks."}]}