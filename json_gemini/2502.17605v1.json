{"title": "PICASO: PERMUTATION-INVARIANT CONTEXT COMPOSITION WITH STATE SPACE MODELS", "authors": ["Tian Yu Liu", "Alessandro Achille", "Matthew Trager", "Aditya Golatkar", "Luca Zancato", "Stefano Soatto"], "abstract": "Providing Large Language Models with relevant contextual knowledge at inference time has been shown to greatly improve the quality of their generations. This is often achieved by prepending informative passages of text, or 'contexts', retrieved from external knowledge bases to their input. However, processing additional contexts online incurs significant computation costs that scale with their length. State Space Models (SSMs) offer a promising solution by allowing a database of contexts to be mapped onto fixed-dimensional states from which to start the generation. A key challenge arises when attempting to leverage information present across multiple contexts, since there is no straightforward way to condition generation on multiple independent states in existing SSMs. To address this, we leverage a simple mathematical relation derived from SSM dynamics to compose multiple states into one that efficiently approximates the effect of concatenating textual contexts. Since the temporal ordering of contexts can often be uninformative, we enforce permutation-invariance by efficiently averaging states obtained via our composition algorithm across all possible context orderings. We evaluate our resulting method on WikiText and MSMARCO in both zero-shot and fine-tuned settings, and show that we can match the strongest performing baseline while enjoying on average 5.4\u00d7 speedup.", "sections": [{"title": "INTRODUCTION", "content": "Equipping Large Language Models (LLMs) with retrieval capabilities allows them to leverage vast external knowledge bases to greatly improve their generation quality. To achieve this, prepending retrieved sequences to the user prompt is an effective and frequently used approach for incorporating the retrieved information (Ram et al., 2023). This allows the generation to be conditioned on the retrieved data, along with the user prompt.\nHowever, this approach incurs significant computational costs. Not only must the system process the user query and generate an answer, but it must also process the retrieved context, which in real-world settings can amount to thousands of tokens. This problem is exacerbated in Transformer-based models, as the inference cost of generating output tokens scales quadratically with the length of the extended input (see Figure 1).\nIn contrast, State Space Models (SSMs) offer a more efficient alternative. SSMs encode information from arbitrary-length input sequences into a fixed-size state vector, which can then be conditioned on to generate new tokens without revisiting the original input. This suggests a simple solution: Instead of retrieving from a database containing raw context tokens, we can create a \"database of states\" containing pre-computed state representations of textual contexts. At inference time, generation starts directly from the retrieved state, simultaneously eliminating the latency from having to process contexts online, and greatly reducing inference time compared to Transformer models (Figure 1).\nHowever, a key challenge arises when conditioning on multiple retrieved contexts. While textual inputs can be simply concatenated, SSM states cannot be composed that easily. To address this, we"}, {"title": "RELATED WORK", "content": "State Space Models and Hybrid Models. Recent efforts to overcome the significant computa-tional costs of Transformer models on long contexts have inspired the exploration of more efficient alternatives, including State Space Models (SSMs). Through maintaining a fixed-size \"state\", a sufficient statistic of the past for the purpose of future prediction, these models offer advantages"}, {"title": "METHOD", "content": ""}, {"title": "PRELIMINARIES:", "content": "A linear input-dependent discrete-time model has the form\n$\\begin{cases} x_t = A(u_t)x_{t-1} + B(u_t)u_t \\\\  y_t = C(u_t)x_t + Du_t. \\end{cases}$                                      (1)\nHere $x_t \\in \\mathbb{R}^m$ is the state at time $t$, while $u_t, y_t \\in \\mathbb{R}^d$ are the input and the output respectively.\nThe matrices $A(u_t) \\in \\mathbb{R}^{m \\times m}, B(u_t) \\in \\mathbb{R}^{m \\times d}, C(u_t) \\in \\mathbb{R}^{d \\times m}$ (which are input-dependent) and $D \\in \\mathbb{R}^{d \\times d}$ are learnable parameters.\nUnrolling the first equation, we obtain\n$x_t = A(u_t) \\dots A(u_1)x_0 + \\sum_{\\tau=0}^{t-1} A(u_t) \\dots A(u_{t-\\tau+1})B(u_{t-\\tau})u_{t-\\tau}$  \n$= A(\\textbf{u})x_0 + \\textbf{x}(\\textbf{u}),$                                                                                   (2)\nwhere $\\textbf{u} = (u_1,..., u_t)$ denotes the sequence of inputs, $A(\\textbf{u}) = A(u_t) \\dots A(u_1)$ is the accumulated decay matrix and $\\textbf{x}(\\textbf{u}) = \\sum_{\\tau=0}^{t-1} A(u_t) \\dots A(u_{t-\\tau+1})B(u_{t-\\tau})u_{t-\\tau}$ is the accumulated input signal. Since this coincides with $x_t$ when $x_0 = 0$, we refer to it as the state for input sequence $\\textbf{u}$.\nIn the following, we write $V \\subset \\mathbb{R}^d$ for a finite set of token embeddings and $V^* = \\bigcup_{n>0} V^n$ for the set of variable-length sequences of token embeddings. We view a State Space (language) Model (SSM) as a map $f_\\theta : V^* \\times \\mathbb{R}^m \\to \\mathcal{P}(V)$ with parameters $\\theta$ which takes in as input an initial state $x \\in \\mathbb{R}^m$ and token embedding sequence $\\textbf{u} \\in V^*$, and returns a distribution over $V$. Modern SSMs (Gu & Dao, 2023; Zancato et al., 2024) usually contain multiple stacked selective state space layers as in equation 1. In a multi-layer setting, we write $\\textbf{x}(\\textbf{u})$ and $A(\\textbf{u})$ for the sequence of states and decay matrices corresponding to all layers."}, {"title": "DATABASE OF STATES", "content": "By the Markov property, the state of an SSM makes the past independent of the future. In other words, $f_\\theta(\\textbf{u}u', 0) = f_\\theta(\\textbf{u}, \\textbf{x}(u'))$ for all $\\textbf{u}, \\textbf{u'} \\in V^*$, where $\\cdot$ denotes concatenation. In practice, this means that a SSM model can equivalently be initialized with the state arising from a (variable-length) input sequence, instead of the input sequence itself. This is akin to the KV-cache of Transformer architectures, except that the dimension of the state is fixed regardless of sequence length.\nIn several real-world use cases such as Retrieval Augmented Generation, relevant contexts are commonly obtained or retrieved from a database (Borgeaud et al., 2022). Instead of storing them in the database as raw text or tokens, we propose to use a \u201cdatabase of states,\u201d where we pre-process each context and store their states. When conditioning on a single context, we can initialize the SSM with the retrieved pre-processed state instead of having to process it online. However this poses a problem when attempting to compose multiple contexts, since we do not know how to compose their states. We will show how this is tackled with our proposed method."}, {"title": "PERMUTATION-INVARIANT COMPOSITION WITH STATE SPACE MODELS", "content": "Given a query and a collection of relevant contexts, an easy method to compose them is to simply concatenate all context tokens with the query into a single sequence to feed into the SSM. Recall that this, however, presents two key limitations. Before even a single token continuation can be generated from the query, the entire sequence of concatenated contexts has to be processed sequentially, which can be computationally intensive when contexts are long or numerous (Figure 1). Another limitation is having to select the order of context concatenation when prompting the model, for which there might be no natural way of doing so without a powerful scoring mechanism.\nTo address the first limitation, we propose a first version of our method, Compositional Aggregation of States as Observations (CASO), which works by modeling sequence concatenation with state composition based on the dynamics of a single-layer SSM."}, {"title": "Proposition 1 (CASO).", "content": "Let $u_1, ..., u_n$ be a collection of input sequences and let $u = u_1 \\dots u_n$ be their concatenation. Then, for a SSM layer that evolves based on equation 1, we have\n$\\textbf{x}(u) = \\textbf{x}(u_n) + \\sum_{i=1}^{n-1} A(u_n) \\dots A(u_{i+1})\\textbf{x}(u_i)$                                   (3)\nWe can see this by recursively applying equation 2 on $\\textbf{x}(u) = A(u_n)\\textbf{x}(u_1\\dots u_{n-1}) + \\textbf{x}(u_n)$.\nGiven a collection of contexts $u_1,..., u_n$, CASO simply approximates the dynamics of multi-layer SSMs, for which Proposition 1 does not hold exactly, via $\\textbf{x}^{CASO}(u_1,..., u_n) = \\textbf{x}(u_n) + \\sum_{i=1}^{n-1} A(u_n)... A(u_{i+1})\\cdot\\textbf{x}(u_i)$. We then load $\\textbf{x}^{CASO}(u_1,..., u_n)$ as the initial state of the model to infer continuations from the given query. We note that in Mamba-style models, the matrices $A(\\cdot)$ are diagonal. As such, computing CASO requires only simple element-wise arithmetic operations and importantly zero model computation time (i.e. zero forward passes required).\nHowever, since each state is weighted by the decay factors of future contexts, this composition operation is still very much order-dependent. We propose to introduce permutation-invariance by considering a group of permutations $G \\subset S_n$, where $S_n$ denotes the symmetric group of $n$ elements, using which we define our method, PICASO (Permutation-Invariant CASO):\n$\\textbf{x}^{PICASO}(u_1,..., u_n) := \\frac{1}{|G|} \\sum_{\\pi \\in G} \\textbf{x}^{CASO}(u_{\\pi(1)}, ..., u_{\\pi(n)})$                                      (4)\nFor any group $G$, by expansion of the CASO terms and collecting common factors, this can be written as a linear combination of individual context states $\\textbf{x}(u_i)$:\n$\\textbf{x}^{PICASO}(u_1,..., u_n) = \\sum_{i=1}^n W_i(u_1,..., u_n)\\textbf{x}(u_i)$\nwith weights $W_i$ depending on $A(u_1),..., A(u_n)$. In this work we are particularly concerned with two cases: the full symmetric group $G = S_n$, which includes all possible permutations, and the cyclic group $G = C_n$, which consists of rotations of the sequence. We will refer to them as PICASO-S and PICASO-R respectively.\nWhile they appear computationally infeasible at first glance, since PICASO-S and PICASO-R average over $n!$ and $n$ CASO states respectively, each of which is itself a composition of $n$ context states, the following propositions show that they can actually be computed in polynomial and linear time respectively for modern SSM models with diagonal $A$ matrices."}, {"title": "Proposition 2.", "content": "Assume $G = S_n$ and that the matrices $A(u_i)$ commute (e.g., are diagonal). Using shorthand notations $A_i := A(u_i)$ and $W_k := W_k(u_1,..., u_n)$ we have\n$W_k = \\frac{1}{n!} \\Big[ (n - 1)! + (n-2)! 1! \\sum_{1<i_1<n \\\\ i_1 \\neq k} A_{i_1} + (n-3)! 2! \\sum_{1<i_1<i_2<n \\\\ i_1,i_2 \\neq k} A_{i_1} A_{i_2} + ...$  \n$\\dots + \\frac{1}{n} \\sum_{m=0}^{n-1} (\\frac{1}{m}) \\cdot e_m (A_1, ..., A_{k-1}, A_{k+1},..., A_n),\\Big]$  \nwhere\n$e_m(A_1, ..., A_{n-1}) := \\sum_{1<i_1<i_2<...<i_m<n-1} A_{i_1} \\dots A_{i_m}$\nis the $m$-th elementary symmetric polynomial (Macdonald, 1998) (in the matrices $A_i$).\nElementary symmetric polynomials satisfy the recursive relation\n$e_m(A_1,..., A_{n-1}) = A_{n-1}e_{m-1}(A_1,..., A_{n-2}) + e_m (A_1, ..., A_{n-2})$.\nUsing this relation, we can compute all values of $e_m$, and hence the coefficients $W_k$, using $O(n^2)$ operations via Dynamic Programming. We detail the implementation in Algorithm 1 of the Appendix."}, {"title": "Proposition 3.", "content": "Assume $G = C_n$ (cyclic permutations). Then writing $A_i := A(u_i)$ and $W_k := W_k(u_1,..., u_n)$ we have\n$W_k = \\frac{1}{n} \\Big[ Id + \\sum_{m=1}^{n-1} A_{[k+m]_n} \\dots A_{[k+1]_n} \\Big]$  \nwhere $Id$ is the identity matrix, and $[i]_n$ denotes $i \\mod n$. Assuming that the matrices $A_i$ are invertible, these can be computed efficiently by setting\n$\\begin{cases}\\bar{A}_i = A_i \\dots A_1 \\\\\\\\   \\bar{A}_i = \\bar{A}_{[i]_n} \\dots A_1 \\end{cases} \\begin{cases} i > n \\\\\\\\ i \\le n' \\end{cases} , \\qquad   \\bar{B}_i = \\bar{A}_1 + ... + \\bar{A}_{i-1}, \\qquad W_k = \\frac{1}{n} [\\bar{A}(B_{k+n} - B_k)],$\nfor $i = 1,..., 2n$, and $k = 1, ..., n$.\nWe detail in Algorithm 3 in the Appendix our efficient implementation for computing PICASO-R in $O(n)$ time complexity via cumulative sums and products. Evidently, PICASO-R is significantly faster than PICASO-S while trading off exact permutation invariance for invariance only to cyclic permutations of the original order. We will show that the difference in empirical performance between PICASO-S to PICASO-R is negligible, as such PICASO-S can almost always be replaced with its much faster variant PICASO-R.\nWe remark that the property of permutation-invariance can also be applied to naive concatenation (as opposed to CASO). This is achieved simply by concatenating contexts in various different orders, followed by taking an average of their resulting states. While performing this for the symmetric group $S_n$ is computationally infeasible, we can similarly restrict our permutation set to $C_n$. We term this variant Permutation-Invariant Concatenation (PIConcat-R), where \u2013 R denotes invariance to the set of cyclic permutations. We note that the model computational costs (forward passes) of this method still scales quadratically with number of contexts (compared to linear scaling of regular concatenation), as such we include it only for completeness.\nAs a final technicality, we note that for Mamba-style SSM models, we additionally require storing the last $m_{conv}$ (usually $M_{conv} = 4$) input tokens of each SSM layer to ensure that the state is sufficient for generating the same distributions over continuations as the input sequence. We perform simple averaging to combine these tokens from different contexts which we show to work well empirically; more sophisticated methods could be explored in future work."}, {"title": "WHY PICASO'S AVERAGE WORKS", "content": "While the combination of state expression for CASO is directly motivated by the dynamics of the system, there is no a priori reason why averaging permuted CASO states should perform well. In Figure 3 we show that averaging both independent states and CASO states can perform better than using any individual state. This suggests a emergent/learned algebraic structure on the space of states such that linear combination of states combine their information to some degree.\nIn our empirical results below, we show that averaging all individual states (which would also be a permutation-invariant solution) performs significantly weaker than averaging CASO states (as PICASO does). We believe that this is because the approximate linear structure of the state space is only valid locally. The combined states are naturally closer together than the independent states, hence able to better exploit the local linearity. We show this in the following proposition:"}, {"title": "Proposition 4.", "content": "Consider a single-layer SSM parametrized by $\\theta$, and two input sequences $u$ and $u'$. Then, the Euclidean distance between the states can be bounded via\n$\\|\\textbf{x}^{CASO}(u, u') - \\textbf{x}^{CASO}(u', u)\\|_2 \\le \\|(I - A(u'))\\textbf{x}(u)\\| + \\|(I - A(u))\\textbf{x}(u')\\|$"}, {"title": "LEARNING TO USE COMPOSED STATES", "content": "As previously noted, in practice, for SSM models consisting of multiple state space blocks stacked with temporal convolutions, $\\textbf{x}(u)$ in equation 3 will not be exactly the state arising from a concatenated list of inputs. In this section, we introduce a fine-tuning objective to enable SSMs to better leverage composed states. Let $D = \\{(\\textbf{u}_i, \\textbf{u'}_i, S_i)\\}_{i=1}^N$ be a dataset of sequences $\\textbf{u}_i$, their next-token continuation $\\textbf{u'}_i$, and a collection (in some particular order) of contexts $S_i$ retrieved from a database using $\\textbf{u}_i$. We minimize the prediction loss over the continuation, given a (composed) initial state and the query sequence:\n$\\mathcal{L}_{BPTC}(\\theta) = \\sum_{(\\textbf{u}_i, \\textbf{u'}_i, S_i) \\in D} L_{CE} \\big(f_\\theta(\\textbf{u}_i, x^{PICASO}(S_i)), \\textbf{u'}_i\\big),$\nwhere $L_{CE}(\\cdot, \\cdot)$ is the cross-entropy loss.\nWe denote this learning objective Backpropagation Through Composition (BPTC), where gradients are propagated through the state composition process $x^{PICASO}$. To reduce training time, we also consider an alternative version where we do not backpropagate through the composition step, which we denote Backpropagation To Composition (BP2C):\n$\\mathcal{L}_{BP2C}(\\theta) = \\sum_{(\\textbf{u}_i, \\textbf{u'}_i, S_i) \\in D} L_{CE} \\big(f_\\theta(\\textbf{u}_i, \\text{sg} [x^{PICASO}(S_i)]), \\textbf{u'}_i\\big),$\nwhere sg denotes the stop-gradient operator. We will show that when used for fine-tuning, this learning objective greatly improves the model's ability to leverage composed states for generation to the level of the concatenation albeit with much faster speeds, while maintaining performance on standard LLM evaluation tasks."}, {"title": "EXPERIMENTS", "content": ""}, {"title": "IMPLEMENTATION DETAILS", "content": "We run our main experiments on the largest available SSM on Huggingface - Mamba-2 2.7B (Dao &\nGu, 2024). We evaluate our method on two large-scale datasets - WikiText-V2 (Merity et al., 2016)\nand MSMARCO (Nguyen et al., 2016). We use the training splits as our fine-tuning data, and the\ntesting/validation splits respectively for evaluation. To pre-process WikiText-V2 for our use case,\nwe split each passage in the dataset into two equal context \"chunks\", with the goal of predicting\nthe second (continuation) from the first (query). The retrieval database comprises all remaining\nchunks, from which we retrieve via an external sentence embedding model, All-MiniLM-L6-v21.\nIn most experiments, we retrieve up to 10 chunks, since improvements appears to saturate beyond\nthat, and loss from concatenation blows up as a result of exceeding training context length (Figure 6,\nAppendix). We pre-process MSMARCO by filtering only entries with well-formed answers and\ndiscarding those without relevant passages.\nWe used the official benchmark2 with an A100 GPU for our timing experiments in Figure 1 to ensure\nfairest comparisons. For the rest of the experiments, we run the model in full-precision, and evaluate\nperformance of the model starting from a custom initial state, a feature not supported by the official\nbenchmark at the time of writing, as such timings differ.\nFor fine-tuning experiments using BPTC and BP2C, we base our implementation on the official Hug-\ngingFace 3 trainer with default hyperparameters, and retrieve the k most relevant context chunks for\neach query sample for composition. For WikiText, we select $k \\in \\{0, . . . , 10\\}$ uniformly at random\nfor each batch. For MSMARCO, we use all the available passages (both relevant and irrelevant)\nassociated with each training example. For both datasets, we fine-tune for only 1 epoch. In all\nfine-tuning experiments, we ensure the training set (both the examples and the chunk database) are\ndisjoint from the validation set to ensure fair evaluation."}, {"title": "COMPARISON MODELS", "content": "We compare inference accuracy (measured by log-perplexity) and processing latency of PICASO with its order-dependent version, CASO, in addition to the following methods:\nBaseline: Loss of the model on the test sample without using any contextual information."}, {"title": "MAIN RESULTS", "content": "In this section, we evaluate both the zero-shot and fine-tuned performance of PICASO in Sec-tion 6.3.1 and Section 6.3.2 respectively, and show in Section 6.3.3 that the fine-tuned model does not overfit to the composition task. We also include additional experiments showing that LLM ca-pabilities are not impacted by fine-tuning in Appendix B.4, and show that PICASO can also be used for data attribution in Appendix C"}, {"title": "ZERO-SHOT PERFORMANCE", "content": "We demonstrate in Figure 3 that applying PICASO-R in a zero-shot manner on WikiText-V2 greatly improves performance over the baseline by an average of 10.1% across 1-10 retrieved context chunks. This greatly improves over Soup (8.5%) and CASO (9.2%). Compared to concatena-tion (11.1%), PICASO-R performs slightly worse but benefits from magnitudes improvement in processing time on an average of 5.4\u00d7. In this task, PICASO-R achieves almost exactly the same performance as PICASO-S, but with a much faster composition time. As a sanity check for motiva-tion for our method, we show that PIConcat achieves the best performance (12.0%) overall, but at the cost of significantly greater computational time despite our batched-inference implementation.\nIn Row 1 of Table 1, we show that applying PICASO-R and PICASO-S in a zero-shot manner on MSMARCO similarly yields considerable improvements (37.2%) over the naive baseline, while achieving performance close to that of concatenation (41.3%)."}, {"title": "BACKPROPAGATION THROUGH AND TO COMPOSITION", "content": "While PICASO demonstrates strong performance in the zero-shot setting, PICASO still lags behind concatenation in terms of prediction accuracy. We attribute this to composed states being \u201cout-of-distribution\" for the model, since these states do not arise from any sequence of input tokens. In this section, we test if this can be resolved via fine-tuning with PICASO-R composed states via BPTC and BP2C. Indeed, as we show in Figure 4, BPTC and BP2C greatly improves the perfor-mance of PICASO-R and PICASO-S to that similar to concatenation, while maintaining much faster processing timings on WikiText. Similarly, we show in Rows 4-5 of Table 1 that fine-tuning on the MSMARCO training set also levels the performance of PICASO with that of concatenation. We also note that while BP2C is significantly faster in terms of training time, it incurs a small performance trade-off compared to BPTC for both datasets, keeping number of training iterations constant."}, {"title": "EVALUATION OF FINE-TUNED MODEL ON OTHER DIFFERENT TASKS", "content": "We showed that models fine-tuned on a specific downstream task (training set) using BPTC/BP2C perform strongly when composing samples drawn from a similar distribution (test set). We further show in Table 1 that models fine-tuned on one domain (WikiText) can demonstrate small perfor-mance gains (or at the very least, no performance loss) when composing samples via PICASO on another domain (MSMARCO). Finally, we show in Appendix B.4 that fine-tuning models with BP2C/BPTC maintain (and occasionally even improve) performance on general LLM evaluation tasks compared against the original model."}, {"title": "LIMITATIONS AND DISCUSSION", "content": "We have proposed a method, PICASO, that enables efficient retrieval and composition of contexts by pre-processing their individual states. Without any training, our approach can handle the com-position of information contained in up to 10 context chunks in a manner that is order-invariant. PICASO notably requires zero online model processing time, since generation can begin directly from the composed states. When models are further fine-tuned with our proposed learning objec-tive, states composed using PICASO perform comparably to those produced from the concatenation of context tokens, while offering on average a 5.4\u00d7 faster composition time.\nNevertheless, our method does have some limitations. When applied in a zero-shot manner, PICASO still lags slightly behind concatenation in terms of prediction accuracy. PICASO is also currently limited to architectures based on SSM layers. We leave as future work extension of PICASO towards recently popularized attention-based hybrid models, which require more sophisticated methods of composing key-value caches. Lastly, we also leave as future work the exploration of parameter-efficient fine-tuning methods such as adapters, which can be used to augment the model at inference time to enable state composition while preserving the original model's behavior."}, {"title": "ALGORITHMS: PICASO-S AND PICASO-R", "content": "We show in Algorithm 1 how PICASO-S is computed in polynomial time via a dynamic programming approach based on Algorithm 2. In Algorithm 3, we also show how PICASO-R can be computed with linear time complexity. Time complexity is measured as the number of arithmetic oper-ations required as a function of number of context states."}, {"title": "Proposition 1 (CASO).", "content": "Let $u_1, ..., u_n$ be a collection of input sequences and let $u = u_1 \\dots u_n$ be their concatenation. Then, for a SSM layer that evolves based on equation 1, we have\n$\\textbf{x}(u) = \\textbf{x}(u_n) + \\sum_{i=1}^{n-1} A(u_n) \\dots A(u_{i+1})\\textbf{x}(u_i)$                                   (3)\nWe can see this by recursively applying equation 2 on $\\textbf{x}(u) = A(u_n)\\textbf{x}(u_1\\dots u_{n-1}) + \\textbf{x}(u_n)$.\nGiven a collection of contexts $u_1,..., u_n$, CASO simply approximates the dynamics of multi-layer SSMs, for which Proposition 1 does not hold exactly, via $\\textbf{x}^{CASO}(u_1,..., u_n) = \\textbf{x}(u_n) + \\sum_{i=1}^{n-1} A(u_n)... A(u_{i+1})\\cdot\\textbf{x}(u_i)$. We then load $\\textbf{x}^{CASO}(u_1,..., u_n)$ as the initial state of the model to infer continuations from the given query. We note that in Mamba-style models, the matrices $A(\\cdot)$ are diagonal. As such, computing CASO requires only simple element-wise arithmetic operations and importantly zero model computation time (i.e. zero forward passes required).\nHowever, since each state is weighted by the decay factors of future contexts, this composition operation is still very much order-dependent. We propose to introduce permutation-invariance by considering a group of permutations $G \\subset S_n$, where $S_n$ denotes the symmetric group of $n$ elements, using which we define our method, PICASO (Permutation-Invariant CASO):\n$\\textbf{x}^{PICASO}(u_1,..., u_n) := \\frac{1}{|G|} \\sum_{\\pi \\in G} \\textbf{x}^{CASO}(u_{\\pi(1)}, ..., u_{\\pi(n)})$                                      (4)\nFor any group $G$, by expansion of the CASO terms and collecting common factors, this can be written as a linear combination of individual context states $\\textbf{x}(u_i)$:\n$\\textbf{x}^{PICASO}(u_1,..., u_n) = \\sum_{i=1}^n W_i(u_1,..., u_n)\\textbf{x}(u_i)$\nwith weights $W_i$ depending on $A(u_1),..., A(u_n)$. In this work we are particularly concerned with two cases: the full symmetric group $G = S_n$, which includes all possible permutations, and the cyclic group $G = C_n$, which consists of rotations of the sequence. We will refer to them as PICASO-S and PICASO-R respectively.\nWhile they appear computationally infeasible at first glance, since PICASO-S and PICASO-R average over $n!$ and $n$ CASO states respectively, each of which is itself a composition of $n$ context states, the following propositions show that they can actually be computed in polynomial and linear time respectively for modern SSM models with diagonal $A$ matrices."}, {"title": "Proposition 2.", "content": "Assume $G = S_n$ and that the matrices $A(u_i)$ commute (e.g., are diagonal). Using shorthand notations $A_i := A(u_i)$ and $W_k := W_k(u_1,..., u_n)$ we have\n$W_k = \\frac{1}{n!} \\Big[ (n - 1)! + (n-2)! 1! \\sum_{1<i_1<n \\\\ i_1 \\neq k} A_{i_1} + (n-3)! 2! \\sum_{1<i_1<i_2<n \\\\ i_1,i_2 \\neq k} A_{i_1} A_{i_2} + ...$  \n$\\dots + \\frac{1}{n} \\sum_{m=0}^{n-1} (\\frac{1}{m}) \\cdot e_m (A_1, ..., A_{k-1}, A_{k+1},..., A_n),\\Big]$  \nwhere\n$e_m(A_1, ..., A_{n-1}) := \\sum_{1<i_1<i_2<...<i_m<n-1} A_{i_1} \\dots A_{i_m}$\nis the $m$-th elementary symmetric polynomial (Macdonald, 1998) (in the matrices $A_i$).\nElementary symmetric polynomials satisfy the recursive relation\n$e_m(A_1,..., A_{n-1}) = A_{n-1}e_{m-1}(A_1,..., A_{n-2}) + e_m (A_1, ..., A_{n-2})$.\nUsing this relation, we can compute all values of $e_m$, and hence the coefficients $W_k$, using $O(n^2)$ operations via Dynamic Programming. We detail the implementation in Algorithm 1 of the Appendix."}, {"title": "Proposition 3.", "content": "Assume $G = C_n$ (cyclic permutations). Then writing $A_i := A(u_i)$ and $W_k := W_k(u_1,..., u_n)$ we have\n$W_k = \\frac{1}{n} \\Big[ Id + \\sum_{m=1}^{n-1} A_{[k+m", "A_{[k+1": "n} \\Big", "i": "n$ denotes $i \\mod n$. Assuming that the matrices $A_i$ are invertible, these can be computed efficiently by setting\n$\\begin{cases}\\bar{A}_i = A_i \\dots A_1 \\\\\\\\   \\bar{A}_i = \\bar{A}_{[i", "end{cases}": "qquad   \\bar{B}_i = \\bar{A}_1 + ... + \\bar{A}_{i-1}, \\qquad W_k = \\frac{1}{n} [\\bar{A}(B_{k+n} - B_k)"}]}