{"title": "Counterfactual MRI Data Augmentation using Conditional Denoising Diffusion Generative Models", "authors": ["Pedro Mor\u00e3o", "Joao Santinha", "Yasna Forghani", "Nuno Lou\u00e7\u00e3o", "Pedro Gouveia", "Mario A. T. Figueiredo"], "abstract": "Deep learning (DL) models in medical imaging face challenges in generalizability and robustness due to variations in image acquisition parameters (IAP). In this work, we introduce a novel method using conditional denoising diffusion generative models (cDDGMs) to generate counterfactual magnetic resonance (MR) images that simulate different IAP without altering patient anatomy. We demonstrate that using these counterfactual images for data augmentation can improve segmentation accuracy, particularly in out-of-distribution settings, enhancing the overall generalizability and robustness of DL models across diverse imaging conditions. Our approach shows promise in addressing domain and covariate shifts in medical imaging. The code is publicly available at https://github.com/pedromorao/Counterfactual-MRI-Data-Augmentation", "sections": [{"title": "Introduction", "content": "Deep learning (DL) models in medical imaging continue to face generalizability and robustness challenges. While data augmentation has been widely used to improve the performance of DL models in various fields, current augmentation techniques do not easily replicate domain, population, and covariate shifts that arise from variations in medical image scanners, acquisition settings, and patient populations. Although style transfer has been proposed as a possible solution to harmonize images across different acquisition settings and scanners, those methods usually work by mapping a source to a target domain on a pairwise basis. That approach thus leads to combinatorially growing numbers of possible combinations that exponentially increase as new scanners and acquisition protocols emerge.\nInvariant based-methods, like the one proposed by Arjovsky et al. [1], offer a promising solution to mitigate performance drops under domain and covariate shifts. However, those methods often require detailed information about the environments in which the data were acquired, as well as known"}, {"title": "Methodology", "content": ""}, {"title": "Dataset", "content": "We employed the Duke-Breast-Cancer-MRI dataset [2] to train and evaluate our deep generative model. The dataset comprised pre-contrast dynamic contrast-enhanced breast MRIs from 922 patients, with 100 patients also containing breast tissue segmentation masks. More pre-processing and epoch details can be found in Appendix A.1."}, {"title": "Image Acquisition Parameters Prediction Model", "content": "Following the model proposed by Konz and Mazurowski [3], a ResNet-18 [4] was modified to predict 7 image acquisition parameters through the final fully-connected layer.\nThe four continuous (M = 4) IAP - Flip Angle (FA), Slice Thickness (ST), Echo Time (TE), and Repetition Time (TR) - are predicted directly using a single unit for each of them in our network's output layer. The three categorical (K = 3) IAP considered - Scanner Manufacturer (SM), Field Strength (FS), and Scan Options (SO) - are converted into one-hot encoding each with a different number of possible values/categories. For the categorical variables, with Ck (k = 1,\uff65\uff65\uff65, K) denoting the number of categories in each categorical variable, the final layer, has a total width of $\\sum_{k=1}^{K} C_k + M$."}, {"title": "Conditional Denoising Diffusion Generative Model", "content": "A CDDGM was used to modify the MR images according to the IAP. The CDDGM architecture is based on the DDPM architecture [5], which learns to reverse a Markovian diffusion process by gradually denoising an image, starting from pure-noise.\nTo condition across the multiple classes, corresponding to the different IAP, we selected the classifier free-guidance (CFG) method [6], as it enables controlling the strength of the alignment with the conditional contex through a guidance scale parameter and it eliminates the need for an additional classifier, as opposed to classifier guidance.\nFor the diffusion process, we used 1000 steps of the original DDPM sampler with a cosine noise scheduler.\nBecause we only want to modify the original images, we stop the forward diffusion process early on, at a point where the distributions of the different IAP overlap due to the perturbation that was added"}, {"title": "Breast Tissue Segmentation Model", "content": "For the breast tissue segmentation, a U-Net [8], using residual blocks to enable better gradient back-propagation and facilitate the optimization process, was used to segment MRI images into 3 different labels fat, fibroglandular tissue (FGT) and background."}, {"title": "IAP, CDDGM, and Segmentation Models Training", "content": "We used images from the 822 patients without breast segmentations to train the cDDGM and IAP models. The training of the segmentation models used the images and corresponding breast tissue segmentation masks of the remaining 100 patients, while considering different scenarios: (1) mix of images from different manufacturers available for training; (2) images from only one manufacturer available for training. More details about training are provided in Appendix A.3."}, {"title": "Evaluation metrics", "content": "Similarly to the work from Konz and Mazurowski [3], we evaluated the performance of our IAP model top-1 accuracy for categorical IAP and mean squared error for continuous IAP.\nTo assess the segmentation model, we used the The Dice-S\u00f8rensen coefficient and accuracy for each different breast tissue present in the segmentation masks.\nGiven that the cDDGM was trained to perform IAP counterfactual data augmentation, for each image, we randomly sampled a set of IAP and generated new counterfactual images. In the case where the sampled set of IAP matches the original image's IAP, the model's output should preserve the image's IAP and recover the original image.\nThe performance of the CDDGM on this task was evaluated using the Fr\u00e9chet inception distance (FID), structural similarity metric (SSIM), maximum mean discrepancy (MMD), and the ability to \"fool\" the IAP prediction model and have the model predict the counterfactual IAP instead of the originally IAP.\nSince the developed CDDGM was trained to perform changes in tissue contrast based on the IAP, without changing the anatomy, we then used the IAP counterfactual images as data augmentation samples and assessed the effect on the performance of the segmentation models in the two scenarios presented in section 2.5."}, {"title": "Results and Discussion", "content": "Table 1 summarizes the performance of the IAP prediction model. We see that the IAP prediction model captures with very good accuracy the IAP of the test dataset. Considering the ranges of each continuous variable (FA: [7\u00b0-12\u00b0]; ST: [1.1mm-2.5mm]; TE:[1.250ms-2.756ms]; TR: [3.540ms-7.395ms]), the IAP prediction models was able to estimate all variables with low MSE, except ST, for which the MSE was relatively higher (~ 5-12%)."}, {"title": "Conclusions", "content": "In this work, we demonstrated that integrating IAP counterfactual images using cDDGM can enhance the generalizability and robustness of deep learning models in medical imaging. The generated counterfactual images successfully misled the IAP prediction model into predicting the intended counterfactual parameters. Moreover, using these images for data augmentation led to slight improvements in segmentation accuracy, particularly in out-of-distribution (OOD) settings, thereby improving the generalizability of DL models across diverse medical imaging conditions."}, {"title": "Appendix", "content": ""}, {"title": "Data Normalization and Preprocessing", "content": "The Duke-Breast-Cancer-MRI dataset comprises multiple 3D and 4D MRI sequences. Since each sequence is associated with only one set of IAP, pairwise supervised image modification techniques are not applicable. Following the approach of Konz and Mazurowski [3], we focused on the 3D pre-contrast phase of 4D dynamic contrast-enhanced sequences. In 100 patients, this phase included corresponding 3D fat and fibroglandular tissue segmentations, enabling us to evaluate the impact of the proposed CDDGM model as a data augmentation technique for DL segmentation models.\nAlthough the selected phase represents a 3D volume, due to the size of the CDDGM model and hardware constraints, we developed and evaluated our model using 2D slices extracted from the 3D volumes. The first and last 20 slices were discarded, as they typically contained more noise and lacked relevant information.\nWe performed image normalization by resizing the images to 224x224 to ensure a fixed size and to accelerate model training and optimization. Although more complex models, such as Latent Diffusion Models, could handle larger images, they would require additional training of encoder and decoder networks to obtain smaller latent space in which the diffusion process would be executed. Moreover, the encoder and decoder would also need to preserve IAP-related information to ensure that the latent representation would still contain such information.\nImage intensity values were normalized using percentile normalization, setting the 10th percentile to 0 and the 99th percentile to 1, without clipping values. The lower percentile was adjusted to a higher value due to the large number of low-intensity voxels in the background and thoracic cavity, which are not particularly relevant for the CDDGM or breast tissue segmentation model.\nTo normalize the IAP, categorical features were one-hot encoded, and numeric features were nor-malized by dividing by the maximum value in the dataset. This approach was chosen over min-max normalization to create a gap from 0 to the ratio of $\\frac{valuemin}{valuemax}$, allowing the model to use 0 as the unconditional value. And since $valuemin \\neq 0$, this is always achievable."}, {"title": "Train, Validation and Test sets", "content": "We initially divided the dataset into subsets of images of patients with segmentations and without. We used the subset without segmentations for training and evaluating the IAP prediction model and the CDDGM model, while the subset with segmentations was exclusively used for training and evaluating the segmentation models. An iterative method was used to split the images into training, validation, and test sets, ensuring that different combinations of IAP were equally represented across all sets. Additionally, the training/validation/test splitting procedure ensured that images from the same patient were not included in different sets.\nFor the subset without segmentations, the dataset was split into 75% for training, 10% for validation, and 15% for testing. In the subset with segmentations, 75% of the data was used for training and 25% used for validation. Due to the limited number of patients with segmentations, the validation set was also used as the test set to evaluate the segmentation model in an ID setting. All OOD images were used as the test set in the OOD evaluation, as they were not included in the training process."}, {"title": "CDDGM Arquitecture", "content": "The noise estimation model used during the reverse diffusion process is a conditional UNet. Its architecture is inspired by the UNet design from Latent Diffusion Models [9]. This UNet architecture incorporates cross-attention mechanisms, which enhance the model's ability to condition on complex contexts. Without these attention mechanisms, the model would struggle to effectively handle conditioning contexts that are more complex than simple image classes.\nOur UNet architecture consists of six downsampling levels, one middle level, and six upsampling levels, with each level containing two residual convolution blocks. Cross-attention blocks are included on the third and fifth of the downsampling levels, on the middle level, and the corresponding positions in the upsampling levels. The conditioning is performed by adding the IAP embedding to the time embeddings and incorporating it through the cross-attention blocks. While adding more cross-"}, {"title": "Training Setup", "content": "The training of all deep learning models was carried out using PYTORCH [11], MONAI CORE [12], and MONAI GENERATIVE [13] libraries.\nThe training processes were conducted on a single NVIDIA A6000 GPU with 48 GB of memory.\nThe IAP prediction model was trained using a batch size of 512 over 200 epochs. The Adam optimizer was employed with a learning rate of $10^{-5}$ and a weight decay parameter of $10^{-4}$. The training and testing phases combined took approximately 5 hours.\nFor the CDDGM model, a batch size of 32 was used, with training spanning 15 epochs. The Adam optimizer was again utilized this time with a learning rate of $10^{-4}$ and weight decay of $10^{-3}$. The training took 9 hours and the testing the IAP modifications applied to the test set took from 2 to 7 hours for configuration of steps and guidance scales, varying with the number of steps. The number of channels per layer can be seen in table 3. The cDDGM also had 8 cross-attention heads."}, {"title": "CDDGM Optimization", "content": "The guidance scale and number of steps of the proposed CDDGM were optimized for counterfactual IAP modification. The performance of the model with the different hyperparameters was assessed using generative and similarity metrics, shown in Table 4, along with the IAP prediction model performance, shown in Table 5."}, {"title": "Data, Models' Weights and Code", "content": "Derived data, obtained from the Duke-Breast-Cancer-MRI dataset [2], and models' weights are made available at https://zenodo.org/records/13495922. Code is available at https://github.com/pedromorao/Counterfactual-MRI-Data-Augmentation."}]}