{"title": "Dataset Distribution Impacts Model Fairness: Single vs. Multi-Task Learning", "authors": ["Ralf Raumanns", "Gerard Schouten", "Josien P. W. Pluim", "Veronika Cheplygina"], "abstract": "The influence of bias in datasets on the fairness of model\npredictions is a topic of ongoing research in various fields. We evaluate\nthe performance of skin lesion classification using ResNet-based CNNs,\nfocusing on patient sex variations in training data and three different\nlearning strategies. We present a linear programming method for gener-\nating datasets with varying patient sex and class labels, taking into ac-\ncount the correlations between these variables. We evaluated the model\nperformance using three different learning strategies: a single-task model,\na reinforcing multi-task model, and an adversarial learning scheme. Our\nobservations include: 1) sex-specific training data yields better results,\n2) single-task models exhibit sex bias, 3) the reinforcement approach\ndoes not remove sex bias, 4) the adversarial model eliminates sex bias in\ncases involving only female patients, and 5) datasets that include male\npatients enhance model performance for the male subgroup, even when\nfemale patients are the majority. To generalise these findings, in future\nresearch, we will examine more demographic attributes, like age, and\nother possibly confounding factors, such as skin colour and artefacts in\nthe skin lesions. We make all code available on Github.", "sections": [{"title": "1 Introduction", "content": "Deep learning has shown many successes in medical image diagnosis [30,12,3],\nbut despite high overall performance, models can be biased against patients\nfrom different demographic groups [1,22,15]. Bias and fairness are becoming an\nactive topic in medical imaging, with studies focusing for instance on skin lesions\n[1,17], chest x-rays [22] and brain MR scans [27]. Examples of sensitive attributes\ninclude age, sex or race. For skin lesion classification, the Fitzpatrick skin type\nis often studied [31,4,17,34].\nFairness studies typically include baselines showing bias between groups,\nand/or propose methods to improve fairness. The methods are based on sampling\nor weighting strategies during training [17], and/or introducing training strate-\ngies that try to debias the methods to rely on the sensitive attributes, such as"}, {"title": "2 Methods", "content": "Construction of datasets. We trained and validated our models on datasets\nwith different female (F) to male (M) patient ratios, equal numbers of malignant\nand benign lesions, and equal number of patients below and above 60 (median\nage) for each sex. We refer to the datasets as M100 (100% male patients), F25M75\n(25% female patients, 75% male patients), F50M50, F75M25, and F100 are de-\nfined analogously. We evaluate the models using a balanced test set mirroring\nF50M50.\nWe used the ISIC archive's [18,10,9,32,11,29] gallery browser [20], which had\n81,155 dermoscopic images of skin lesions, some with age and sex metadata. We\nqueried the archive for \"dermoscopic\" images diagnosed as \"benign\" or \"malig-\nnant\" for all ages and both sexes. This gave us 71,035 images (62,439 benign,\n8,596 malignant), which we processed using the steps in Fig. 1 (see Appendix\nfor more details).\nLinear programming for optimal dataset construction. We have devel-\noped a method to create diverse dataset compositions using linear programming,\na common mathematical optimisation technique. The goal is to maximise the"}, {"title": "3 Results", "content": "Figure 2 shows the impact of dataset distributions on three learning strategies,\nreporting AUC scores overall and for both sexes.\nSex-specific training data yields better results. Models perform better for\nmale subgroups in the male-only and lightly skewed male patient experiments.\nIn the balanced dataset and lightly skewed female patient experiments, there is\nno significant difference between the subgroups. However, in the lightly skewed\nfemale patient scenario, the adversarial model performs better for male patients.\nAn exception is observed when the training datasets consist only of female pa-\ntients; in such cases, there is a positive difference in AUC scores favouring female\npatients. Thus, our models seem more attuned to male patients in mixed-sex\ntraining sets, irrespective of the percentage of female patients. The best results\nare achieved when both sexes are trained exclusively on their respective data. De-\nspite this, a male subgroup bias is apparent as the results for female patients are\nsignificantly worse than for male patients when trained exclusively on their data.\nBase model reveals sex bias. The base model shows a significant sex bias\nin performance. When only male patients are involved the base model reveals\na substantial performance gap between male and female patients. The results"}, {"title": "4 Discussion and conclusions", "content": "We studied model and subgroup performance across datasets to identify the\ninfluence of bias in datasets on fairness of model predictions. We used linear\nprogramming (LP) to create various datasets with controlled male-female ra-\ntios. This was done to systematically evaluate the performance of three different\nlearning strategies using ResNet-based CNNs. Other fairness and bias studies\nthat require a flexible method to create datasets under certain constraints could\nbenefit from this universal LP technique.\nOur study shows that eliminating bias is challenging. The adversarial model\narchitecture is able to reduce sex bias in a female-only context but fails for other\ndatasets. Other model approaches do not show convincing results with respect\nto bias reduction.\nSkewed sex distributions still show a performance gap between male and\nfemale patients. Our experiments demonstrate that the adversarial model better\ncorrects sex bias in female-only datasets and not in male-only datasets, possibly\ndue to other confounding and/or unidentified factors. Further research is needed\non this issue.\nAs expected the base model shows a sensitivity for sex bias, possibly due\nto overfitting. The reinforcing and adversarial models both having a form of\nregularisation (to counter overfitting), are potentially able to reduce sex bias\ncompared to the base model. However in our experiments we only see a bias\ncorrection for adversarial models for female-only experiments.\nOur outcomes show that sex-related information influences prediction tasks.\nFuture research should determine which specific sex-related factors are essential\nto ensure fairness across different subgroups.\nIn contrast to categorical data like patient sex, where the groups are clearly\ndefined, this is not possible or only partially possible with continuous data like"}]}