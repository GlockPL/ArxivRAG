{"title": "Dataset Distribution Impacts Model Fairness: Single vs. Multi-Task Learning", "authors": ["Ralf Raumanns", "Gerard Schouten", "Josien P. W. Pluim", "Veronika Cheplygina"], "abstract": "The influence of bias in datasets on the fairness of model predictions is a topic of ongoing research in various fields. We evaluate the performance of skin lesion classification using ResNet-based CNNs, focusing on patient sex variations in training data and three different learning strategies. We present a linear programming method for generating datasets with varying patient sex and class labels, taking into account the correlations between these variables. We evaluated the model performance using three different learning strategies: a single-task model, a reinforcing multi-task model, and an adversarial learning scheme. Our observations include: 1) sex-specific training data yields better results, 2) single-task models exhibit sex bias, 3) the reinforcement approach does not remove sex bias, 4) the adversarial model eliminates sex bias in cases involving only female patients, and 5) datasets that include male patients enhance model performance for the male subgroup, even when female patients are the majority. To generalise these findings, in future research, we will examine more demographic attributes, like age, and other possibly confounding factors, such as skin colour and artefacts in the skin lesions. We make all code available on Github.", "sections": [{"title": "1 Introduction", "content": "Deep learning has shown many successes in medical image diagnosis [30,12,3], but despite high overall performance, models can be biased against patients from different demographic groups [1,22,15]. Bias and fairness are becoming an active topic in medical imaging, with studies focusing for instance on skin lesions [1,17], chest x-rays [22] and brain MR scans [27]. Examples of sensitive attributes include age, sex or race. For skin lesion classification, the Fitzpatrick skin type is often studied [31,4,17,34].\nFairness studies typically include baselines showing bias between groups, and/or propose methods to improve fairness. The methods are based on sampling or weighting strategies during training [17], and/or introducing training strate-gies that try to debias the methods to rely on the sensitive attributes, such as"}, {"title": "2 Methods", "content": "Construction of datasets. We trained and validated our models on datasets with different female (F) to male (M) patient ratios, equal numbers of malignant and benign lesions, and equal number of patients below and above 60 (median age) for each sex. We refer to the datasets as M100 (100% male patients), F25M75 (25% female patients, 75% male patients), F50M50, F75M25, and F100 are de-fined analogously. We evaluate the models using a balanced test set mirroring F50M50.\nWe used the ISIC archive's [18,10,9,32,11,29] gallery browser [20], which had 81,155 dermoscopic images of skin lesions, some with age and sex metadata. We queried the archive for \"dermoscopic\" images diagnosed as \"benign\" or \"malig-nant\" for all ages and both sexes. This gave us 71,035 images (62,439 benign, 8,596 malignant), which we processed using the steps in Fig. 1 (see Appendix for more details).\nLinear programming for optimal dataset construction. We have devel-oped a method to create diverse dataset compositions using linear programming, a common mathematical optimisation technique. The goal is to maximise the"}, {"title": "Models.", "content": "We used the ResNet50 model [19] in three ways, which include:\nThe single-task baseline model enhanced with two fully connected layers has a sigmoid activation function and binary cross-entropy loss (Lc, see Equation 1). We did not use class weights but rather the actual distribution represented by the training dataset. We fine-tuned the model through a grid search of three varying learning rates and batch sizes, selecting the combination that yielded the highest performance across all experiments.\nThe multi-task reinforcing model, with three added layers to the convolu-tional base, produces two outputs: one for classification and the other for the binary sex attribute. We employed binary cross-entropy loss (Lc) and a sigmoid activation function for both heads, giving equal weight to both losses.\nThe multi-task adversarial model was implemented following the methodol-ogy [2,1], using a network with a shared feature encoder and two classifier heads. One classifier targeted skin cancer classification; the other predicted confounding variables like sex or age. We used the ResNet architecture to compare performance with baseline and reinforcing models equitably. We trained the skin cancer classifier and encoder with cross-entropy loss (Lc) and optimised the bias predictor with binary cross-entropy loss (Lc). \u03a4\u03bf diminish the confounder predictiveness of the encoded feature, we adversar-ially adjusted the encoder using a third loss (Lbr), setting \u00e0 as the penalty for accurate demographic predictions. We used a lambda (\u03bb, see Equation 2) value of 5, as in [1], to assess subgroup performance and set the penalty for correct predictions of the target demographic parameter.\nTo summarise, we use these loss functions:"}, {"title": "3 Results", "content": "Figure 2 shows the impact of dataset distributions on three learning strategies, reporting AUC scores overall and for both sexes.\nSex-specific training data yields better results. Models perform better for male subgroups in the male-only and lightly skewed male patient experiments. In the balanced dataset and lightly skewed female patient experiments, there is no significant difference between the subgroups. However, in the lightly skewed female patient scenario, the adversarial model performs better for male patients. An exception is observed when the training datasets consist only of female pa-tients; in such cases, there is a positive difference in AUC scores favouring female patients. Thus, our models seem more attuned to male patients in mixed-sex training sets, irrespective of the percentage of female patients. The best results are achieved when both sexes are trained exclusively on their respective data. De-spite this, a male subgroup bias is apparent as the results for female patients are significantly worse than for male patients when trained exclusively on their data.\nBase model reveals sex bias. The base model shows a significant sex bias in performance. When only male patients are involved the base model reveals a substantial performance gap between male and female patients. The results"}, {"title": "4 Discussion and conclusions", "content": "We studied model and subgroup performance across datasets to identify the influence of bias in datasets on fairness of model predictions. We used linear programming (LP) to create various datasets with controlled male-female ra-tios. This was done to systematically evaluate the performance of three different learning strategies using ResNet-based CNNs. Other fairness and bias studies that require a flexible method to create datasets under certain constraints could benefit from this universal LP technique.\nOur study shows that eliminating bias is challenging. The adversarial model architecture is able to reduce sex bias in a female-only context but fails for other datasets. Other model approaches do not show convincing results with respect to bias reduction.\nSkewed sex distributions still show a performance gap between male and female patients. Our experiments demonstrate that the adversarial model better corrects sex bias in female-only datasets and not in male-only datasets, possibly due to other confounding and/or unidentified factors. Further research is needed on this issue.\nAs expected the base model shows a sensitivity for sex bias, possibly due to overfitting. The reinforcing and adversarial models both having a form of regularisation (to counter overfitting), are potentially able to reduce sex bias compared to the base model. However in our experiments we only see a bias correction for adversarial models for female-only experiments.\nOur outcomes show that sex-related information influences prediction tasks. Future research should determine which specific sex-related factors are essential to ensure fairness across different subgroups.\nIn contrast to categorical data like patient sex, where the groups are clearly defined, this is not possible or only partially possible with continuous data like"}]}