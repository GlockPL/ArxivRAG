{"title": "A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation", "authors": ["Shuo Yu", "Mingyue Cheng", "Jiqian Yang", "Jie Ouyang"], "abstract": "Retrieval-Augmented Generation (RAG) enhances generative models by integrating retrieval mechanisms, which allow these models to access and utilize external knowledge sources. Despite its advantages, RAG encounters significant challenges, particularly in effectively handling real-world queries and mitigating hallucinations. The KDD Cup 2024 CRAG competition brings these issues to the forefront by incorporating both web pages and a mock API as knowledge sources, adding the complexity of parsing HTML before large language models (LLMs) can process the information. In this paper, we propose a novel RAG benchmark designed to address these challenges. Our work provides a comprehensive set of experimental results, offering valuable insights for the study of RAG. We thoroughly examine the entire RAG process, including knowledge source selection, retrieval, organization, and reasoning. Key findings from our study include the impact of automated knowledge source selection using agents and the influence of noise chunks on RAG reasoning. Additionally, we conduct detailed experiments to analyze the effects of various hyperparameters on RAG performance. To support further research, we have made our results, the associated code, and a parsed version of the CRAG dataset publicly available\u00b9, contributing to the advancement of RAG methodologies and establishing a solid foundation for future work in this domain.", "sections": [{"title": "Introduction", "content": "Retrieval-Augmented Generation (RAG) enhances generative models by integrating retrieval mechanisms (Guu et al. 2020; Lewis et al. 2020, 2021), enabling these models to dynamically access and utilize external knowledge (Gao et al. 2024; Sun et al. 2024). This hybrid approach not only significantly improves the accuracy and relevance of generated content but also effectively mitigates the problem of hallucinations in generative models (Xu et al. 2024). By grounding responses in real-time, contextually relevant data, RAG systems become robust tools for producing reliable, fact-based content across a wide array of applications.\nDespite significant advancements, RAG systems continue to face challenges in handling real-world, long-tail queries (Yang et al. 2024; Cheng et al. 2024) and mitigating hallucinations (Kandpal et al. 2023; Ding et al. 2024). The KDD Cup 2024 CRAG competition underscores these difficulties by centering on complex, realistic Question Answering (QA) tasks that leverage two primary knowledge sources: web pages and a structured mock API (Gaur et al. 2021; Wang et al. 2023a). While these sources provide valuable opportunities for integrating diverse external knowledge, they also expose limitations in current RAG frameworks (Chen et al. 2023), particularly in the added complexity of parsing HTML-formatted web pages for effective utilization by large language models (LLMs).\nTo address the challenges in evaluating and improving RAG systems, this paper introduces a novel RAG benchmark. In our work, we have converted the HTML-formatted web pages from the KDD Cup CRAG competition dataset into markdown (MD) format, enabling LLMs to more effectively utilize the information embedded within these web pages. Additionally, we present our self-implemented RAG framework, accompanied by a comprehensive set of experimental results, demonstrating the robustness and effectiveness of our approach. These findings provide valuable insights and practical guidelines for the research community, laying a solid foundation for further advancements and innovations in RAG methodologies.\nOur study provides an in-depth examination of the entire RAG process, including knowledge source selection, retrieval, organization, and reasoning. We investigate the impact of various factors on system performance, such as the use of agents for automatic knowledge source selection (Asai et al. 2023) and the effects of noise chunks on the reasoning capabilities of RAG systems (Cuconasu et al. 2024). Furthermore, we conduct detailed experiments to analyze how different hyperparameter configurations can influence RAG performance, offering a understanding of how specific settings can either enhance or impair the system's accuracy and reliability. This approach ensures a thorough evaluation, providing key insights for optimizing RAG systems to achieve more accurate and dependable outcomes.\nBeyond our experimental findings, we contribute to the research community by openly sharing our results, codebase, and a parsed version of the CRAG dataset. This transparency is intended to encourage further exploration and innovation in the field, allowing other researchers to build on our work and advance the state of the art in RAG methodologies. By making these resources publicly available, we aim to support the continued development and refinement of RAG systems, fostering collaboration and progress within the research community."}, {"title": "Problem Definition", "content": "Preliminaries\nThe central challenge addressed by the Retrieval-Augmented Generation (RAG) model is the effective fusion of internal LLM knowledge with diverse external knowledge sources to enhance generative performance in knowledge-intensive tasks (Guu et al. 2020). The RAG model aims to generate the optimal output y* by conditioning on both the input x and a set of retrieved documents z* from external sources, with the objective defined as:\ny* = argmax_y P(y|x, z*),\nwhere z* represents the set of documents that are retrieved to maximize the probability:\nz* = argmax_z P(z|x).\nIn this formulation, external knowledge sources z* are not homogeneous but include structured databases, web-based knowledge, and API-driven resources, each contributing uniquely to the inference process. The model's two-step approach first retrieves the most relevant z* given the input x, and then generates y* by integrating this external knowledge with the LLM's internal knowledge. This framework highlights the dynamic fusion of internal and external knowledge, crucially allowing for adaptive reasoning that leverages the strengths of both sources, thereby optimizing relevance and accuracy in generating responses for knowledge-intensive tasks."}, {"title": "Dataset and Metrics", "content": "The CRAG dataset, central to the KDD Cup 2024, is particularly well-suited for real-world retrieval-based Question Answering (QA) tasks due to its diverse query types and document formats (Yang et al. 2024; Kwiatkowski et al. 2019). Reflecting practical scenarios where correct answers may not be entirely present within external knowledge sources, the dataset challenges models to engage in sophisticated integration of both internal and external knowledge for accurate inference. Additionally, certain queries may lack direct answers in either knowledge domain (Bajaj et al. 2018), necessitating deeper collaborative reasoning between different knowledge types\u2014making the dataset an ideal benchmark for evaluating the effectiveness of Retrieval-Augmented Generation (RAG) systems.\nOur work significantly enhances the utility of this dataset, which we have named RM3QA, standing for \"A Real-time Multi-domain, Multi-format, Multi-source Question Answer Dataset for RAG.\u201d This improved dataset facilitates more accurate integration of external knowledge, laying a stronger foundation for LLM inference. RM3QA is designed to handle real-time questions across multiple domains, utilizing both structured and unstructured knowledge sources, and integrating information from various origins such as web pages and a mock API. Alongside the public release of this dataset in markdown (md) format, our RAG system is evaluated on four key metrics: accuracy (Acc.), hallucination (Halluc.), missing (Miss.), and overall score. These metrics provide a comprehensive assessment of the system's ability to balance retrieval accuracy, avoid incorrect inferences, and maintain relevance in complex, knowledge-intensive tasks. The evaluation includes both string matching and GPT-based assessments (Ouyang et al. 2022). For detailed explanations of these metrics and dataset processing, please refer to Appendix B."}, {"title": "Benchmark Evaluation of RAG", "content": "The CRAG benchmark (Yang et al. 2024) currently lacks comprehensive evaluation results that researchers can reference. To address this gap, we propose a novel RAG benchmark and evaluate the performance of our RAG implementation across a series of benchmark tests.\nOur RAG framework, RAG-X, depicted in Figure 1, employs an LLM Agent-based Router to intelligently select relevant knowledge sources from a curated mix of structured and unstructured data, including web pages and a mock API. Our retrieval process avoids web searches, focusing exclusively on the provided information sources. The retrieval process is divided into three steps: broad retrieval narrows down vast external knowledge, focused retrieval uses sparse, dense, or hybrid methods to identify key information, and rank refinement ensures the final output is accurate and prioritized. After retrieval, we enhance the LLM's reasoning with noise chunks, Chain of Thought (CoT), and In-Context Learning (ICL), leading to more relevant responses.\nThe experimental results in Table 1 show that RAG-X consistently outperforms both the CRAG baseline and the LLM-only model. Structured knowledge from the mock API notably enhances accuracy and reduces hallucinations compared to unstructured web sources. However, increasing knowledge inputs, like combining 50 web pages with mock API data, improves accuracy but slightly raises hallucination rates. This highlights the need to carefully balance external knowledge sources to optimize overall performance (Wang et al. 2023b; Yu et al. 2023; Shi et al. 2023).\nTo further support the research community, we are making both our code and dataset fully open-source. This commitment ensures that our work is not only robust and reproducible but also applicable to real-world scenarios. For detailed information on our implementation and dataset, please refer to Appendix B."}, {"title": "Extensive Empirical Studies", "content": "In this section, we conduct an in-depth analysis of the Retrieval-Augmented Generation (RAG) process. Our empirical studies rigorously evaluate each critical component of the RAG pipeline\u2014namely, knowledge source selection, retrieval, organization, and reasoning. Through this systematic examination, the insights gained from these analyses are intended to guide future optimizations in RAG methodologies. In the presentation of our experimental results, we have bolded the optimal values to highlight the best-performing configurations. Specific details of the experimental setup are provided in Appendix B."}, {"title": "Advancing Knowledge Source Selection", "content": "Table 2 offers a comparative analysis of performance across different knowledge sources, including internal, external, and combined scenarios. Integrating structured knowledge from the mock API significantly improves accuracy and reduces hallucination rates. In contrast, aggregating all sources can lead to conflicts and suboptimal results (Liu et al. 2023b). Predefined fallback strategies, where the LLM prioritizes internal knowledge before querying external sources, boost accuracy but also raise hallucination rates, especially with multiple sources. While the agent-based dynamic selection strategy (Li, Nie, and Liang 2023) adds flexibility, it doesn't consistently outperform predefined methods, underscoring the challenges of real-time decision-making. These findings highlight the importance of structured external knowledge for performance enhancement and the need for advanced selection mechanisms to balance knowledge coverage and minimize hallucinations in retrieval processes (Rackauckas 2024; Yoran et al. 2024)."}, {"title": "Enhancing Retrieval for Knowledge Extraction", "content": "This subsection examines strategies for enhancing knowledge retrieval and utilization in a RAG system, focusing on broad and focused retrieval methods and refining reranking to improve overall performance."}, {"title": "Advancing Efficiency with Broad Retrieval", "content": "Table 3 compares the performance of the RAG system with and without a broad retrieval step, where each query is supplemented with 50 web pages-far more than the 5 used in other experiments. The results demonstrate that this broad retrieval phase significantly enhances system efficiency, notably reducing processing time. Sparse retrieval methods like BM25 (Izacard et al. 2022; Cheng et al. 2021) filter out irrelevant data, allowing dense retrieval to focus on a more targeted subset of external knowledge (Zhang et al. 2024). These findings underscore the effectiveness of a two-tiered approach, where sparse retrieval narrows the search space, enabling dense retrieval to operate with greater precision and speed. This strategy is especially beneficial in scenarios involving extensive external knowledge, optimizing both processing time and relevance, and offering a more efficient design for RAG systems (Jiang et al. 2024)."}, {"title": "Evaluating Focused Retrieval Methodologies in RAG", "content": "Figure 2 illustrates the trade-offs between dense and sparse retrieval configurations in a RAG system. While higher dense retrieval ratios boost accuracy due to richer semantic understanding, they also elevate hallucination rates, suggesting a risk of contextual misalignment. Conversely, increasing sparse retrieval may lower accuracy and fails to consistently reduce hallucinations, likely because of its dependence on surface-level keyword matching. These findings underscore the need for a dynamic, hybrid strategy that balances the strengths and weaknesses of both approaches, optimizing performance by mitigating the inherent limitations of each method (Gu et al. 2018; Zhang et al. 2018; Cheng et al. 2022)."}, {"title": "Reranking for Enhanced Knowledge Utilization", "content": "Table 4 summarizes the impact of various reranker configurations on RAG system performance, focusing on different retrieval chunk sizes. The results show that as the number of retrieved chunks increases, hallucination rates rise, while accuracy remains stable, suggesting that the reranker performs better with smaller, more focused sets. Bypassing retrieval and directly passing all chunks to the reranker, as in configuration (3, All), results in similar performance to configurations with a large number of chunks, indicating that increasing chunk size beyond a certain point does not enhance effectiveness. These findings emphasize the need to balance the number of retrieved chunks and optimize the reranker's selection process (Ma et al. 2023; Vu et al. 2023; Yu et al. 2024), especially in large-scale retrieval scenarios."}, {"title": "Evaluating Knowledge Reasoning on RAG Performance", "content": "This section evaluates the impact of various knowledge reasoning techniques on RAG system performance, including Chain of Thought (CoT), few-shot learning, query positioning within prompts, and the introduction of noise chunks."}, {"title": "Impact of Chain of Thought (CoT)", "content": "Figure 3 illustrates the context-dependent impact of incorporating Chain of Thought (CoT) into RAG systems. While CoT aims to enhance logical reasoning, the results indicate that it does not consistently improve performance (Wei et al. 2023; Zhou et al. 2023) and can sometimes reduce accuracy, especially when dealing with multiple conflicting knowledge sources.\nThis suggests that CoT may complicate the integration process, leading to higher missing rates and decreased effectiveness. Therefore, CoT should be carefully applied, proving beneficial in straightforward scenarios but potentially counterproductive in more complex contexts."}, {"title": "Analysis of Few-Shot Learning", "content": "Table 5 presents the impact of few-shot learning (Dai et al. 2022; Dong et al. 2024) on RAG systems, particularly in identifying false premises across various domains. The results show that the model performs best under 0-shot conditions. However, as more examples are introduced, performance on these questions declines due to overfitting and noise. Despite this, overall accuracy improves with few-shot examples, which provide task-specific guidance. Cross-domain examples also enhance generalization and reduce hallucination rates, demonstrating the value of diverse examples in broadening the model's adaptability across different query types."}, {"title": "Impact of Query Position in Prompt", "content": "Figure 4 illustrates the effect of query position within the prompt on RAG system performance. The results show that placing the query after the reference information not only increases accuracy but also reduces hallucination rates, suggesting that the model benefits from having more context before addressing the query. Conversely, positioning the query before the reference leads to a higher hallucination rate, likely because the model lacks sufficient context, making it more prone to misinterpret the query, overlook critical details, and generate responses based on incomplete information. This experiment highlights the need for careful prompt structuring to balance accuracy, hallucination, and missing information in RAG systems (Liu et al. 2023a)."}, {"title": "Impact of Noise Chunks", "content": "Figure 5 illustrates the impact of varying noise chunks on RAG system performance. The results show that as the number of noise chunks increases, accuracy initially dips but then improves, reaching its peak at moderate noise levels. Interestingly, while hallucination rates rise with the introduction of noise, they tend to stabilize and slightly decrease at higher noise levels. This suggests that a certain degree of noise may prompt the model to better filter out irrelevant information (Cuconasu et al. 2024). However, the consistent decrease in the missing rate as noise increases indicates that the model becomes more decisive, though not always accurately guided. The overall score declines with increasing noise but improves at specific levels, highlighting the complex interplay between noise and performance. These findings highlight the complex impact of noise in RAG systems, emphasizing the importance of carefully optimizing noise management strategies to maintain performance."}, {"title": "Impact of Hyperparameter Configurations", "content": "In this section, we systematically examine how different hyperparameter configurations influence the performance of our Retrieval-Augmented Generation (RAG) system. Our analysis spans both the Retrieval and Large Language Model (LLM) Reasoning components, assessing how adjustments to these parameters affect the performance. For comprehensive details on the experimental setup, including variations in configurations, refer to Appendix B."}, {"title": "Effect of Hyperparameters on Retrieval", "content": "Impact of Retrieval Embedding. As summarized in Table 6, the experiment evaluates the performance of three embedding models\u2014BGE-small-en-v1.5, BGE-large-en-v1.5, and BGE-M3\u2014within a RAG system. The BGE-M3 model (Chen et al. 2024) achieves the highest accuracy but also the highest hallucination rate, highlighting a trade-off between precision and error. BGE-large-en-v1.5 minimizes hallucinations but at the expense of accuracy, indicating a more conservative approach. BGE-small-en-v1.5 offers a balanced performance between these two extremes. The performance differences among the models stem from a trade-off between capturing rich semantics and overfitting.\nImpact of Chunk Size and Overlap on Retrieval Performance. As shown in Figure 6 and Figure 7, increasing chunk size generally enhances accuracy by providing more context. However, it also raises hallucination rates due to the retrieval of excessive or less relevant content. Chunk overlap further exacerbates this issue by introducing redundancy, which increases the risk of errors. Balancing chunk size and overlap is therefore crucial for optimizing accuracy while minimizing hallucinations in RAG systems."}, {"title": "Effect of Hyperparameters on LLM Reasoning", "content": "Impact of Large Language Model Selection. In Table 7, we compare the performance of different LLM backbones within a RAG system, particularly when using techniques like Chain of Thought (CoT) prompting. This experiment highlights the impact of the LLM backbone on handling complex reasoning tasks. The LLaMA 3.1 8B model demonstrates a balanced approach, achieving a lower hallucination rate and a slightly positive overall score. In contrast, the Qwen2-7B model, while delivering higher accuracy, exhibits a much higher hallucination rate, leading to a negative overall score. This emphasizes the importance of selecting an LLM that balances accuracy and reliability, especially when tackling complex queries with CoT prompting.\nEffect of Sampling parameters. In Figure 8 and Table 8, the impact of varying temperature and top-p settings on RAG performance is analyzed. The figure illustrates that increasing the temperature slightly improves accuracy, but it also raises the hallucination rate and decreases the overall score. Higher temperatures encourage more diverse responses, reducing the missing rate, but this comes at the cost of introducing inaccuracies. This suggests that while higher temperatures might help generate more comprehensive answers, they also increase the risk of unreliable outputs, necessitating careful tuning to balance accuracy and creativity. Similarly, the table shows that as the top-p value increases, accuracy improves while hallucination rates decrease, indicating that a broader range of token options leads to more accurate and reliable responses. The missing rate remains stable across different top-p settings, suggesting that top-p primarily affects the quality rather than the completeness of the generated content. Overall, higher top-p values appear to enhance the model's performance by achieving a better balance between accuracy and diversity in the output."}, {"title": "Conclusion: Challenges and Future Direction", "content": "In summary, we review our findings on key aspects of the RAG framework\u2014knowledge selection, retrieval, reasoning, and performance evaluation\u2014highlighting challenges and identifying future research opportunities to further enhance system effectiveness.\nKnowledge Selection. We empirically evaluated strategies for selecting knowledge from multi-source and multi-format external databases. Our results show that the selection mechanism significantly affects the accuracy and relevance of generated answers. Specifically, differences in filtering and prioritization lead to varying alignment with query intent. This highlights the need for more adaptive selection methods that can dynamically cater to diverse and complex knowledge sources. Future research should focus on developing these adaptive frameworks to better align knowledge selection with RAG system objectives.\nKnowledge Retrieval. Our exploration of the multi-step knowledge retrieval process\u2014encompassing broad retrieval, focused retrieval, and rank refinement\u2014revealed the importance of coordinating these stages for optimal RAG performance. The interaction among these phases significantly enhances the relevance and precision of retrieved knowledge. Furthermore, initial processing steps like Named Entity Recognition (NER), matching, and reformatting retrieved data, particularly from sources like Mock APIs, were found to improve content coherence. Future work should refine these retrieval pipelines, incorporating advanced preprocessing techniques such as contextual re-ranking and dynamic filtering to achieve more aligned and context-aware knowledge retrieval.\nKnowledge Reasoning. In the reasoning phase using large language models (LLMs), we explored advanced techniques like In-Context Learning (ICL) and Chain-of-Thought (CoT), yielding insights into noise chunking and the sensitivity of query positioning. These findings emphasize the need for refining reasoning methods within RAG frameworks to enhance both accuracy and interpretability. Future research should investigate the interplay between reasoning strategies and retrieval contexts, aiming to improve the robustness and consistency of LLM-based reasoning across various applications, which could significantly advance the scalability and reliability of RAG systems.\nPerformance Evaluation. We adopted the KDD Cup2024 evaluation strategy, which starts with strict string matching followed by GPT-based automated assessments for mismatches. While effective, this method has limitations, particularly the inconsistency of GPT-based evaluations when model performances are closely matched, which can lead to unreliable results. Additionally, the evaluation criteria are narrow, neglecting the quality of intermediate retrieval contexts and the reasoning processes within LLMs. Future research should aim to develop more comprehensive evaluation methodologies, including human-in-the-loop (Stiennon et al. 2022) assessments and metrics that evaluate the quality of retrieval and reasoning processes. These improvements would provide a more nuanced and reliable understanding of RAG performance, driving further progress in the field."}, {"title": "Appendices", "content": ""}, {"title": "Appendix A Related Work", "content": ""}, {"title": "Retrieval-Augmented Generation (RAG)", "content": "Retrieval-Augmented Generation (RAG) have been developed to enhance generative models by integrating retrieval mechanisms, allowing these models to dynamically access and utilize external knowledge. This approach significantly improves the accuracy and relevance of generated content. However, RAG systems still face difficulties in handling long-tail queries and mitigating hallucinations, especially in complex scenarios. To address these challenges, we propose the RAG-X framework, which incorporates well-designed strategies for knowledge source selection, knowledge retrieval, and knowledge reasoning. These improvements are specifically aimed at enhancing the system's ability to manage complex queries and reduce the hallucinations."}, {"title": "Existing Benchmarks for RAG Systems", "content": "RAG benchmarks have been instrumental in the development of RAG systems, but they often fall short in capturing real-world complexity due to limited query types and document formats. The dataset for KDD Cup 2024 CRAG addresses these gaps by offering diverse query types and document formats, yet it presents external knowledge in HTML format, which can hinder effective model integration. Our work improves this by parsing the HTML knowledge into Markdown (MD) format, making it more accessible and structured for models. We will release this enhanced dataset, named RM3QA (A Real-time Multi-domain, Multi-format, Multi-source Question Answer Dataset for RAG), thereby providing a stronger benchmark for future RAG research."}, {"title": "Chain of Thought (CoT) and In-Context Learning (ICL)", "content": "Chain of Thought (CoT) allows Large Language Models (LLMs) to break down complex problems into sequential steps, improving response accuracy in tasks requiring multi-step reasoning. In-Context Learning (ICL), on the other hand, enables LLMs to adapt to new tasks using only a few examples provided within the context, without the need for additional fine-tuning. However, we found that the performance of CoT and ICL in RAG systems may require further investigation. To explore this, we conducted experiments on the application of CoT and ICL in RAG and discovered some notable findings, such as the influence of CoT on RAG potentially being closely tied to the choice of external knowledge sources."}, {"title": "Appendix B Reproducibility", "content": ""}, {"title": "Dataset Processing", "content": "For our experiments, we utilized the official validation set provided by the dataset of the KDD Cup 2024 CRAG competition. To enhance the usability of the web-based knowledge within the dataset, we converted HTML-formatted web pages into markdown format using the Jina framework. This conversion was essential to improve the compatibility of the data with Large Language Models (LLMs), enabling more effective inference and retrieval of relevant information.\nThis processing step was crucial for ensuring that the external knowledge sources were optimally formatted for our Retrieval-Augmented Generation (RAG) framework. The parsed markdown dataset, which will be made publicly available, supports further research and underscores the practical improvements brought by our approach in handling complex QA scenarios."}, {"title": "Experimental Setup", "content": "To ensure the reproducibility and consistency of our experiments, we establish a base configuration for our Retrieval-Augmented Generation (RAG) model, detailed in Table 9. The base hyperparameters were carefully selected to balance retrieval relevance and computational efficiency while maintaining deterministic outputs from the LLM.\nThe model configuration uses the BGE-M3 retriever with a chunk size of 200 tokens and no chunk overlap (chunk_overlap=0). Dense retrieval methods are exclusively employed, with no sparse retrieval or reranking applied. For each query, 3 chunks are retrieved.\nThe generative component relies on the Meta-LLaMA-3-1-8B-Instruct model, which is utilized without any fine-tuning or additional training. Prompts are constructed by default without employing Chain of Thought (CoT) reasoning or In-Context Learning (ICL).\nDuring experimentation, any analysis of specific hyperparameters involves altering only those particular settings, while all other configurations remain consistent with the base setup. The results section presents the specific values and settings explored in our experiments, and any configuration not explicitly mentioned should be assumed to adhere to this base configuration.\nThis standardized approach ensures a controlled environment, minimizes noise, and allows for accurate comparisons and reliable reproduction of our results."}, {"title": "Evaluation Metrics", "content": "The performance of our Retrieval-Augmented Generation (RAG) model is evaluated using four key metrics: accuracy, hallucination, missing, and score."}, {"title": "Accuracy", "content": "The accuracy of a prediction is first determined through string matching\u2014if the prediction exactly matches any of the ground truth answers, it is deemed correct and earns 1 point. If the prediction does not match exactly, GPT-3.5 Turbo is used to evaluate the prediction. If GPT-3.5 Turbo assesses the prediction as correct, it is also considered accurate and awarded 1 point."}, {"title": "Hallucination", "content": "If a prediction is incorrect or misleading, GPT-3.5 Turbo classifies it as a hallucination, scoring -1."}, {"title": "Missing", "content": "Responses like \"I don't know\" are classified as missing and score 0, contributing to the overall error rate."}, {"title": "Score", "content": "The score is calculated as the difference between accurate predictions and hallucinations."}, {"title": "Evaluation Process", "content": "GPT-3.5 Turbo uses the following prompt to evaluate predictions:\n# Task: Judge if the model prediction matches any ground truth. If matched, \u201cAccuracy\" is \"True\"; otherwise, \"False\". If the model cannot answer or lacks information, \"Accuracy\u201d is \"False\". If the ground truth is \"invalid question\u201d, \u201cAccuracy\u201d is \u201cTrue\u201d only if the prediction is exactly \u201cinvalid question\u201d.\n# Output: A JSON string with an \"Accuracy\" field as \"True\" or \"False\".\nThe final score is determined by averaging the scores across all evaluated questions."}, {"title": "Explanation of Knowledge Source Selection", "content": "The specific meanings of external knowledge in each setting are as follows:\n\u2022 LLM: This setting indicates that the model relies solely on its internal knowledge for inference, without using any external knowledge sources. The model depends entirely on the knowledge and capabilities acquired during its pre-training phase.\n\u2022 WEB: In this setting, the model exclusively relies on external knowledge retrieved from web pages for inference, without utilizing its internal knowledge base. The model generates answers by retrieving and leveraging information obtained from the web.\n\u2022 Mock API: This setting indicates that the model uses only the structured external data provided by the mock API for inference, without depending on any internal knowledge. The model directly retrieves relevant information from the mock API to generate answers.\n\u2022 ALL: This setting indicates that the model relies solely on external knowledge sources, including information from the web and structured data from the mock API, without using its internal knowledge base. The model integrates external resources to complete the task.\n\u2022 LLM+WEB: In this setting, the model utilizes both its internal knowledge and external knowledge from the web for inference. The model combines its internal knowledge base with information retrieved from the web to perform a more comprehensive inference.\n\u2022 LLM+Mock API: This setting indicates that the model combines its internal knowledge with the external structured data from the mock API for inference. The model synthesizes these two knowledge sources to generate more accurate answers.\n\u2022 LLM+ALL: This setting indicates that the model leverages both internal knowledge and external information from the web and the mock API for inference. By accessing the broadest range of knowledge sources, the model aims to enhance the accuracy of its answers through collaborative reasoning across multiple inputs.\nThese settings represent manually selected knowledge sources, allowing us to assess the LLM's performance across various combinations of internal and external knowledge. However, in addition to these manual settings, we have implemented Predefined Fallback Strategies. In these strategies, the LLM initially attempts to answer the query using only its internal knowledge. If the LLM is unable to generate a satisfactory answer, it then follows a manually predefined path to access external knowledge. This predefined path is categorized into three specific combinations: LLM+WEB, LLM+Mock API, and LLM+ALL. Each path specifies the order and combination of knowledge sources to be used after the initial internal knowledge attempt.\nMoreover, we have developed an Agent-Based Dynamic Selection Strategy. In this approach, the LLM first tries to answer using its internal knowledge. If it fails, an LLM Agent-based Router dynamically selects the most appropriate external knowledge source from the available options\u2014LLM+WEB, LLM+Mock API, or LLM+ALL-based on the query's requirements. This strategy allows for more flexible and adaptive knowledge integration, potentially improving the LLM's response accuracy in diverse and complex scenarios.\nThese different settings allow us to evaluate the LLM's performance under various combinations of knowledge sources and explore how to optimize knowledge integration strategies to improve inference accuracy and reliability in practical applications."}, {"title": "Automatic Selection of External Knowledge Sources", "content": "In our Retrieval-Augmented Generation (RAG) system, we've implemented a method that allows the large language model (LLM) to autonomously select the most appropriate external knowledge source when it encounters a query that cannot be answered using its internal knowledge. The process is as follows:\nInitially, the LLM attempts to answer the query using its internal knowledge. If the LLM responds with \"I don't know,\" the query is then passed back to the LLM with a specialized prompt designed to guide the model in selecting the appropriate external data source.\nThe prompt provided to the LLM is as follows:\nYou are an intelligent assistant tasked with selecting the most appropriate data source(s) for answering user queries. You have access to two types of data sources:"}, {"title": "Noise Chunk Selection Strategy", "content": "In our experiments, noise chunks were introduced into the Retrieval-Augmented Generation (RAG) system to study their impact on performance. The strategy for selecting noise chunks involved randomly selecting a few chunks from the entire set of available chunks before the retrieval process. These randomly chosen chunks were then deliberately inserted into the set of chunks retrieved after the retrieval process, introducing irrelevant or misleading information into the system."}, {"title": "Guidelines for Extracting Information from the Mock API Responses", "content": "The mock API utilized in our Retrieval-Augmented Generation (RAG) system is provided by the CRAG benchmark. We designed rules to extract valid and relevant information from the mock API responses. Below is a summary of the key rules for each domain."}, {"title": "Time-Specific Guidelines", "content": "The knowledge graph used by the mock API covers data from 1958 to 2019. All time-related information processed adheres to this range."}, {"title": "Movie Domain", "content": "We extract movie titles, director names, and actor names from the API responses. For each movie, we retrieve details such as title, original title, director(s), genres, language, revenue, budget, length, and Oscar Awards information (category, year, and status). Missing data is marked as Unknown. For individuals, we list movies they acted in or directed, check title accuracy, and retrieve Oscar Awards information for specified years."}, {"title": "Finance Domain", "content": "Stock information is categorized into stock prices and other stock-related information. We retrieve single-day and multi-day stock prices, including opening, closing, highest, lowest prices, and trading volume. Other stock information includes financial metrics like market capitalization, EPS, P/E ratio, and dividend yield. Relevant dates are filtered, and prices are compared chronologically."}, {"title": "Music Domain", "content": "We extract song names, artist names, band names, and dates from the API. For each song, we retrieve the author, release date, release country, and Grammy Award count. For artists, we retrieve biography details and lists of works, sorted by year. Band information includes current members and historical data when relevant. Grammy Awards information is retrieved for specified years."}, {"title": "Sports Domain", "content": "We extract NBA and soccer team names, check for exact or alias matches, and process dates. Game information is retrieved and formatted based on specified dates, including win-loss records, total points, and relevant statistics. All relevant game details are presented concisely.\nThis approach ensures efficient and accurate extraction of information from the mock API, providing comprehensive responses across domains."}, {"title": "Appendix C Computing Infrastructure", "content": "All the experiments are conducted on 2 \u00d7 Nvidia GeForce RTX 4090 GPUs (24GB memory each). Other configuration includes 2 x Intel Xeon Gold 6426Y CPUs, 503GB DDR4 RAM, and 1 \u00d7 893.8GB SATA SSD, which is sufficient for all the baselines."}, {"title": "Appendix D Case Study", "content": ""}, {"title": "Correct Answer", "content": "In this example, our RAG system was queried about the birth date of Randall Wallace. The system successfully retrieved relevant contexts that directly mentioned his birth date. By leveraging these accurate contexts, the system provided the"}]}