{"title": "R3HF: REWARD REDISTRIBUTION FOR ENHANCING REINFORCEMENT LEARNING FROM HUMAN FEEDBACK", "authors": ["Jiahui Li", "Tai-Wei Chang", "Fengda Zhang", "Kun Kuang", "Long Chen"], "abstract": "Reinforcement learning from human feedback (RLHF) provides a paradigm for aligning large language models (LLMs) with human preferences. This involves the initial training of a reward model based on pairwise human feedback. The reward model is subsequently utilized in reinforcement learning to assess the scores of each generated sentence as a whole, further guiding the optimization of LLMs. However, current approaches have a significant shortcoming: They allocate a single, sparse, and delayed reward to an entire sequence of output. This may overlook some significant individual contributions of each token towards the desired outcome. To overcome this limitation, our paper proposes a novel reward redistribution method called R3HF, which facilitates a more fine-grained, token-level reward allocation. Specifically, our method treats the reward prediction task of the reward model as a regression problem. As a result, the redistributed rewards are computed by evaluating the specific contribution of each token to the reward model's output. This detailed approach improves the model's understanding of language nuances, leading to more precise enhancements in its performance. Our method is crafted to integrate seamlessly with most current techniques while incurring minimal computational costs. Through comprehensive experiments across diverse datasets and tasks, we have verified the effectiveness and superiority of our approach\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have showcased remarkable adaptabilities across various tasks, with applications spanning fields like psychology [12], education [49, 19], and medical support [46, 27]. However, as LLMs become increasingly sophisticated, the complexity of their decision-making processes and outputs also escalates, introducing potential risks such as the propagation of bias [14, 48], generation of misinformation [24, 29], and potential harm [16, 15]. This underscores the critical need for effective alignment [35, 50, 26, 11] of LLMs. Such alignment aims to guide the models to better comprehend and prioritize human preferences, ensuring their operations are in tune with human values and ethics.\nReinforcement learning from human feedback (RLHF) [10, 29, 5] is an advanced paradigm that incorporates human feedback into LLM training. This approach typically unfolds in three primary stages, which is shown in Figure 1 (left). The initial stage involves supervised fine-tuning (SFT) applied to the target domain. Subsequently, the second stage develops and trains a reward model on data that reflect human preferences. The final stage is dedicated to refining the language model using reinforcement learning algorithms with the learned reward model. Though RLHF technology has demonstrated its effectiveness in various scenarios, it also presents a significant drawback that hampers the training efficiency of the model. Traditional reward models typically assess the overall effectiveness of an entire generated sequence, assigning a score only after delivering the final token, with the other tokens receiving a score of zero. This reward structure, being both sparse and"}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 MARKOV DECISION PROCESS AND SEQUENCE-MARKOV DECISION PROCESSES", "content": "Natural language generation can be deemed as a Markov Decision Process (MDP) [33] which is depicted as a tuple \\( M \\equiv (S, A, R, P, \\gamma,T) \\) with a finite vocabulary \\( V \\). At the beginning of each episode, a prompt x is sampled and fed into the language model and is treated as the initial state \\( s_0 \\in S \\). At each time-step \\( t < T \\), the language model acts as the policy \\( \\pi \\) to choose an action \\( a_t \\in A \\) which means selecting a token from the vocabulary via \\( \\pi(a_t|s_t) \\), and then a new state is achieved via the transition function \\( P : S \\times A \\rightarrow S \\) by adding the generated token to the previous state. Meanwhile, a reward \\( r_t \\) is gained via the reward function \\( R : S \\times A \\rightarrow R \\). The goal of the policy model is to maximize the expected accumulated return \\( G( \\tau) = \\sum_{t=0}^{T} \\gamma^t R(s_t, a_t) \\), where \\( \\gamma \\in [0, 1) \\) represents the discount factor.\nIn this paper, policy optimization follows a Sequence-Markov Decision Process (SDP) [4], where both the policy and the transition probabilities are Markov, but the reward function is not required to be Markov. As claimed by Arjona-Medina et al. [4], return-equivalent SDPs possess identical optimal policies. This implies that we can redistribute the reward gained at the end of the generation sequence to optimize the policy model."}, {"title": "2.2 REWARD MODEL FOR OPTIMIZING LARGE LANGUAGE MODELS", "content": "In traditional RLHF paradigms [53, 5, 29, 37], the reward model is denoted by \\( R_{\\phi}(x, y) \\), where x represents the input prompt given to the language model, y is the response generated by the model, and \\( \\phi \\) symbolizes the parameters of the reward model. The training data, reflecting human preferences, is depicted in a comparative format: \\( y_w \\succ y_l | x \\), indicating that the \u201cwinning\u201d response \\( y_w \\) is preferred by humans over the \u201closing\u201d response \\( y_l \\) given the input prompt x.\nTraditionally, most prior research has adopted a preference predictor that aligns with the principles of the Bradley-Terry model [9], in which the likelihood of a preference pair \\( p^* \\), can be estimated as:\n\\[\np^* (y_w \\succ y_l|x) = \\frac{\\exp(R_{\\phi}(x, y_w))}{\\exp(R_{\\phi}(x, y_w)) + \\exp(R_{\\phi}(x, y_l))} = \\sigma (R_{\\phi}(x, y_w) - R_{\\phi}(x, y_l)).\n\\tag{1}\n\\]\nAssuming the dataset of comparisons \\( D = \\{ (x^i, y_w^i, y_l^i) \\}_{i=1}^N \\) is sampled from \\( p^* \\), the reward model can be trained by minimizing the negative log-likelihood loss:\n\\[\nL(R_{\\phi}, D) = -E_{(x,y_w,y_l)\\sim D}[\\log(\\sigma (R_{\\phi}(x,y_w) - R_{\\phi}(x,y_l)))],\n\\tag{2}\n\\]"}, {"title": "3 METHOD: REWARD REDISTRIBUTION", "content": "Figure 1 illustrates the entire training framework, with a focus on our proposed reward redistribution highlighted in the third phase. We will discuss this in detail in this section."}, {"title": "3.1 SPARSE AND DELAYED REWARDS IN REINFORCEMENT LEARNING", "content": "As previously mentioned, before refining the policy model, we train the reward model using the specified loss (Eq. equation 2). Each initial input prompt x (\\( s_0 \\)) is processed by the policy model \\( \\pi \\) to create a full episode \\( (s_0, a_0, ..., s_T, a_T) \\), which is then stored in the replay buffer. During the reinforcement learning phase, the reward model assigns rewards, denoted as \\( r_0^{RM},..., r_T^{RM} \\), at each time-step to evaluate the success of the episode. Rewards are typically defined in the following manner:\n\\[\nr_t^{RM} = R_{\\phi}(s_t, a_t) = \\begin{cases}\n0, & 0 < t < T, \\\\\nR_{\\phi}(x, y), & t = T,\n\\end{cases}\n\\tag{3}\n\\]\nwhere y represents the fully generated sequence. Meanwhile, it is crucial to maintain the policy model \\( \\pi_{\\theta} \\) closely aligned with the reference model \\( \\pi_{ref} \\). To ensure this, a Kullback-Leibler (KL) penalty is usually applied [53, 5, 29, 37] at each time-step:\n\\[\nr_t^{KL} = KL(\\pi_{\\theta}(a_t|s_t) || \\pi_{ref}(a_t|s_t)).\n\\tag{4}\n\\]\nThus, the total reward at any time-step is calculated using the equation:\n\\[\nr_t = r_t^{RM} - \\beta \\cdot r_t^{KL},\n\\tag{5}\n\\]\nwhere \\( \\beta \\) is the scaling factor. This approach, however, faces challenges due to sparse and delayed rewards as specified by Eq. equation 3. The generation process of LLMs is long-term, with the success or failure of initial generations impacting subsequent ones. This underscores the necessity of effective credit assignment, which aims to accurately pinpoint which actions or sequences of actions lead to success or failure, and is crucial for the process."}, {"title": "3.2 REWARD REDISTRIBUTION FOR CREDIT ASSIGNMENT", "content": "We seek to perform credit assignment by allocating the earned reward (or penalty) across the sequence of actions, thereby providing a more granular and immediate feedback mechanism. Taking a cue from Arjona-Medina et al. [4], reward redistribution is realized within the sequence difference penalties (SDPs). They posit that: (1) two SDPs are return-equivalent if they differ only in their reward distribution and have the same expected return, and (2) return-equivalent SDPs share the same optimal policy. Considering these properties, our remaining task is to devise an algorithm for constructing modified rewards \\( r_t^{RM} \\) that reflect the contributions of each token at every time-step, ensuring that the sum of the rewards equals \\( r_t^{RM} \\).\nRecalling the training process of the RL phase, rewards are generated using the last hidden state with a logit head. This functions as a regression model that naturally predicts the score at the final time-step. Consequently, there is no need to retrain or modify the reward model. Instead, we can utilize the existing model to obtain all hidden states and predict scores at each time-step via the logit head. The redistributed rewards can then be computed using a time-difference approach, reflecting the incremental contribution of each time-step.\nDefine \\( y = (y_0, \\ldots, y_T) \\), where \\( y_t \\) denotes each token in the generated response. We estimate the contributions of each token, \\( r_t^{RM} \\), by its incremental impact on the reward model compared to the previous time-step as:\n\\[\nr_t^{RM} = R_{\\phi}(x, y_{\\<t}) - R_{\\phi}(x, y_{\\<t-1}),\n\\tag{6}\n\\]"}, {"title": "3.3 ANALYSIS OF THE REDISTRIBUTED REWARDS", "content": "Comparing Eq. equation 3 with Eq. equation 7, it is evident that the two SDPs are not return-equivalent due to the presence of \\( R_{\\phi} \\). This term introduces the potential for bias in determining the optimal policy. However, since \\( \\bar{r}^{RM} \\) is exclusively a function of x and does not depend on y, based on the theory of Rafailov et al. [35], we understand that Eq. equation 3 and Eq. equation 7 are reward functions from the same equivalence class and induce the same optimal policy within the constrained RL framework.\nFurthermore, \\( \\bar{r}^{RM} \\) can either be considered an optimistic initialization or a pessimistic initialization. For prompts that yield positive scores, the algorithm encourages exploration; for those with negative scores, a more cautious behavioral strategy is encouraged. This capability to dynamically adjust rewards relative to the quality of the prompt suggests that it is a beneficial characteristic for LLMs.\nIn addition, as Arjona-Medina et al. [4] highlighted, the reward redistribution method exhibits two advantageous properties: (1) Its convergence can be proven via a stochastic approximation for two-time-scale update rules [8, 18], under standard assumptions. (2) The redistribution does not need to be optimal; even a non-optimal redistribution method can guarantee correct learning outcomes. Consequently, Eq. equation 5 is reformulated as follows:\n\\[\nr_t = \\tilde{r}_t^{RM} - \\beta r^{KL}.\n\\tag{8}\n\\]\nHere, \\( \\tilde{r}_t \\) serves as the rewards that are compatible with any reinforcement learning algorithm. Typically, it is used to compute the advantage, \\( A_t \\), and the Proximal Policy Optimization (PPO) [36] algorithm is then applied to optimize the language model. The details of the training algorithms are provided in the Appendix."}, {"title": "3.4 CONNECTION TO DIRECT PREFERENCE OPTIMIZATION (DPO)", "content": "Another recently popular algorithm DPO [35], eliminates the need for explicit reward modeling and has gained widespread use due to its simplicity and effectiveness. Indeed, our method shares the same optimal policy with DPO, as the summation of our redistributed rewards falls within the same equivalence class as the traditional reward function. Interestingly, we also find that DPO can implicitly perform any type of reward redistribution (credit assignment), which may be one of the reasons for its effectiveness. We will provide a detailed analysis of this observation in the Appendix."}, {"title": "3.5 DISCUSSION ABOUT CONCURRENT WORK", "content": "In parallel to our research, several studies have explored token-level rewards in RLHF [45, 51]. Xia et al. [45] extended DPO [35] by estimating the conditionally optimal policy directly from model responses, enabling more granular and flexible policy shaping. Meanwhile, Zhong et al. [51] calculated token-level rewards using a policy trained by DPO and then applied these rewards to perform PPO. Unlike Xia et al. [45], our method employs a reinforced-style optimization approach [2], which, although more computationally intensive, provides stability on out-of-distribution (OOD) data. In contrast to Zhong et al. [51], our approach eliminates the need for an additional training phase for the reward model. We directly obtain token-level rewards from the original reward model by reusing its logit head, making the method simple, cost-effective, and efficient."}, {"title": "4 EXPERIMENTS", "content": "Our experiments were designed to address three pivotal questions: (1) How does the proposed reward redistribution method surpass traditional sparse reward methods in performance? (2) Is the reward redistribution method versatile enough to be applied across a variety of tasks? (3) Does the proposed method retain its effectiveness in scenarios involving multiple rewards?\nTo evaluate our method, we carried out a series of comprehensive experiments across various tasks, such as question answering, summarization, and harmfulness mitigation&helpfulness enhancement. The results indicate that reward redistribution consistently outperforms state-of-the-art approaches that rely on sparse rewards."}, {"title": "4.1 EXPERIMENTAL SETTINGS", "content": "Base model and Benchmark. In our experiments, we adpot a popular open-sourced model For our experiments, we adopted the popular open-source model LLaMA-7B [39] as the base model. All experiments presented in this paper were conducted using the benchmark proposed by [11]\u00b2.\nEvaluation Method. We evaluate different methods in our experiments based on three criteria. (1) The average reward scores in the test set. Since the training objective of different methods is to maximize the expectation of the rewards output by the reward model, the average score of the generated responses can directly reflect the effectiveness of the training method. (2) The reward win rate against the baseline. While the average reward score provides an overview, there may be instances that score particularly high, necessitating an instance-level evaluation. (3) The win rate against a baseline evaluated by GPT-4 [1]. The reliability of average reward scores may be questioned for two reasons. Firstly, the language model runs a high risk of overfitting on the reward model, potentially compromising its original capabilities. Secondly, the ground truth reward function is usually unknown in the real world, and the trained reward model is not always perfect. Therefore, we use GPT-4 as a proxy for human judgment to comprehensively evaluate different methods. We do not use traditional automatic evaluation metrics such as BLEU [30], ROUGE [23], and METEOR [6]. The primary reason is that RLHF aims to align the model with human preferences. Previous works [35, 37] have indicated that these metrics may correlate poorly with human judgments. Moreover, for tasks like summarization and harmfulness mitigation&helpfulness enhancement, these methods are not appropriate. For experimental details and showcases, please refer to Appendix."}, {"title": "4.2 QUESTION ANSWERING TASK", "content": "Dataset. Our experiment commenced with the Nectar [52] dataset, featuring human-labeled responses across seven distinct rankings. In accordance with Liu et al. [26], we constructed the SFT dataset by exclusively using responses from rank one and ensuring that the data length did not exceed 1024 characters. Furthermore, to train the reward model, we generated preference pairs that include both rank one responses and responses randomly selected from other ranks. Ultimately, the dataset utilized in our study comprised 30,000 samples for the SFT, 5,000 samples for training the reward model, and an additional 5,000 samples for reinforcement learning.\nBaseline. We adopt the PPO-based method [29] as our baseline. Building upon this baseline, we implement our reward redistribution and demonstrate its superiority."}, {"title": "4.3 SUMMARIZATION TASK", "content": "Dataset. We subsequently carried out experiments on the TL;DR dataset [41, 37], a curated collection of posts from Reddit pre-processed for research purposes. This dataset is organized into two parts: \u201ccomparisons\u201d and \u201caxis\u201d. The \u201ccomparisons\u201d features pairwise assessments made by human annotators to ascertain the superior summary for each post. In our study, we employed the \u201caxis\" portion of the dataset for supervised fine-tuning of the model and for reinforcement learning purposes, while the \"comparisons\" portion was utilized for training the reward model. The final dataset encompassed 14,900 samples for supervised fine-tuning, 92,900 samples for reward model training, and another 14,900 samples designated for reinforcement learning training.\nBaseline. In this scenario, we also adopt the PPO-based method [29] as our baseline, based on which the reward redistribution is performed.\nThe reward redistribution method outperformed both the baseline and DPO in terms of evaluation scores. It was noted that DPO attained the lowest score in the reward model evaluation, significantly trailing behind the SFT model; however, it managed to achieve a comparable win rate when evaluated by GPT-4. Several factors might account for this observation. Firstly, DPO does not undergo training with a reward model but directly optimizes the language model using data on human preferences. Secondly, the trained reward model is not flawless and might only mirror human preferences in certain respects, which can lead to disparities with the patterns DPO learns. This discrepancy underscores that GPT-4 evaluation serves as a more fitting and objective measure."}, {"title": "4.4 HARMFULNESS MITIGATION & HELPFULNESS ENHANCEMENT TASK", "content": "Dataset. We have evaluated the efficacy of reward redistribution across several tasks, yet it remains to be determined how it fares in situations encompassing multiple rewards. To address this, we conducted experiments using the SafeRLHF dataset [11], which is comprised of 1 million human-labeled data points indicating preferences for content that is both helpful and non-harmful. This dataset served the"}, {"title": "6 CONCLUSION", "content": "This paper aims to explore methods for enhancing the performance of language models in RLHF through fine-grained rewards, without relying on human labor. We introduce a reward redistribution method named R3HF. Specifically, we conceptualize the reward model as a regression model and calculate the token-level rewards in a time-difference manner, interpreting these rewards as the contri- butions of each token to the final output of the reward models. Our proposed reward redistribution is capable of eliciting the same optimal policy as traditional methods do, while also mitigating the issues of sparse and delayed rewards in certain contexts. Additionally, reward redistribution boasts scalability and can be seamlessly incorporated into most mainstream PPO-based methodologies. In our empirical assessments, we test our method across different scenarios and with a variety of methods. The results demonstrate both the effectiveness and superiority of our approach.\nLimitations and Future Work. This paper acknowledges several limitations. Firstly, it does not include human evaluation, opting instead for assessments via the advanced LLMs, GPT-4 [1]."}, {"title": "APPENDIX", "content": ""}, {"title": "A ALGORITHM AND ANALYSIS", "content": ""}, {"title": "A.1 REINFORCEMENT LEARNING ALGORITHM", "content": "Our training framework adheres to the standard Proximal Policy Optimization (PPO) algorithm [36]. The primary distinction lies in the computation of rewards. Additionally, building on prior research [38, 11], we incorporate PTX loss for each task, as detailed in Eq. 11. The training objective is twofold, comprising both the reinforcement learning (RL) objective and the PTX pretraining objective.\n\\[\nL_{PTX}(\\theta; D_{SFT}) = -E_{x \\sim D_{SFT}}[\\pi_{\\theta}(x)]\n\\tag{11}\n\\]"}, {"title": "A.2 FINE-GRAINED REWARDS IN RLHF", "content": "Traditional RLHF applies reinforcement learning at a specific token-level Markov Decision Process (MDP) but often encounters issues with sparse and delayed rewards. Enlisting human efforts to label high-quality data with fine-grained rewards is a usual and effective approach [44]. There is also a growing interest in Direct Preference Optimization (DPO) [35], a method that has gained attention due to its simplicity and absence of reward modeling requirements. Rafailov et al. [34] further posits that DPO can be interpreted as a bandit problem where the model's entire response is considered a single option, and it can also learn per-token credit assignment.\nConnection with DPO. In this subsection, we will demonstrate that our proposed R3HF shares the same optimal policy with DPO, implying that DPO performs reward redistribution implicitly.\nThe objective of reinforcement learning phase can be represent as the following optimization problem:\n\\[\n\\max_{\\pi_{\\theta}} E_{x \\sim D, y \\sim \\pi_{\\theta}(y|x)}[R_{\\phi}(x, y)] - \\beta D_{KL}(\\pi_{\\theta}(y|x)||\\pi_{ref}(y|x)).\n\\tag{12}\n\\]\nBuilding upon prior works [17, 21, 31, 32, 35], it is relatively straightforward to demonstrate that the optimal solution to the KL-constrained reward maximization objective, as outlined in Eq. equation 12, assumes the following form:"}]}