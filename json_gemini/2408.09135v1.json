{"title": "Vanilla Gradient Descent for Oblique Decision Trees", "authors": ["Subrat Prasad Panda", "Blaise Genest", "Arvind Easwaran", "Ponnuthurai Nagaratnam Suganthan"], "abstract": "Decision Trees (DTs) constitute one of the major highly non-linear AI models, valued, e.g., for their efficiency on tabular data. Learning accurate DTs is, however, complicated, especially for oblique DTs, and does take a significant training time. Further, DTs suffer from overfitting, e.g., they proverbially \"do not generalize\" in regression tasks. Recently, some works proposed ways to make (oblique) DTs differentiable. This enables highly efficient gradient-descent algorithms to be used to learn DTs. It also enables generalizing capabilities by learning regressors at the leaves simultaneously with the decisions in the tree. Prior approaches to making DTs differentiable rely either on probabilistic approximations at the tree's internal nodes (soft DTs) or on approximations in gradient computation at the internal node (quantized gradient descent). In this work, we propose DTSemNet, a novel semantically equivalent and invertible encoding for (hard, oblique) DTs as Neural Networks (NNs), that uses standard vanilla gradient descent. Experiments across various classification and regression benchmarks show that oblique DTs learned using DTSemNet are more accurate than oblique DTs of similar size learned using state-of-the-art techniques. Further, DT training time is significantly reduced. We also experimentally demonstrate that DTSemNet can learn DT policies as efficiently as NN policies in the Reinforcement Learning (RL) setup with physical inputs (dimensions < 32).", "sections": [{"title": "1 Introduction", "content": "DTs are widely adopted in several domains, such as the medical domain [13]. Studies, including recent ones [18], show that DTs classify tabular datasets very well due to their inductive bias toward learning non-smooth functions, even better than NNs. Learning DTs for a given task is, however, a complex task: due to the combinatorial explosion over choices of the decisions to branch over at every node of the tree, learning optimal DTs is an NP-hard problem [25].\nCurrent methods of learning DTs can be broadly classified into four approaches: (a) greedy optimization, which grows trees using split criteria such as Classification and Regression Trees (CART) [10], (b) non-greedy optimization, which jointly optimize decision nodes under global objective such as Tree Alternating Optimization (TAO) [12, 44]), (c) Global searches, either presented as Mixed Integer Programs (MIP) [6], or using Evolutionary Algorithms (EA), e.g. [15], and (d) Gradient Descent by making the tree differentiable [21, 42, 43]. Greedy optimization techniques usually learn low-performing trees [43, 2], global searches are computationally expensive when the number of possible tree configurations is large [7, 15] and non-greedy approaches, although computationally better, are still computationally more expensive than gradient-based approaches [2]. Moreover, training without gradient descent in the RL setting is very challenging, as there is no dataset: the agent learns from experience collected in an environment. A workaround is first to learn an NN using gradient descent, and imitate the NN (treated as an oracle) with a (classification-) DT using greedy strategies such as CART, which works reasonably well for simple RL problems with a small (up to \u2248 6 features) discrete action space [4]. On the other hand, recent results with the gradient-based approach have shown to be efficient in terms of training time and accuracy for both classification and regression tasks [2]. Additionally, it is capable of directly learning (regression-) DT policies in RL settings with continuous actions [31].\nTo apply gradient descent at the training stage on DTs, most previous works adopt \"soft\" decisions (e.g., using the Sigmoid activation function) to make the tree differentiable [26, 9, 43, 21]. However, the tree obtained does not provide \"hard\" decisions but \"soft\", that is, probabilistic ones. Hard DTs can be obtained from soft DTs, but accuracy decreases. Some recent studies, such as Dense Gradient Trees (DGT) [2] and Interpretable Continous Control Trees (ICCT) [31], have introduced an alternative approach to obtain hard DTs, using an approximation during backpropagation to compute gradients using Straight Through Estimators (STEs) [5, 22]. This approximation can hamper DT training, especially in large datasets or RL, where errors may accumulate over many training steps.\nIn this work, we propose a powerful novel encoding of oblique DTs as NNs, namely the DTSemNet architecture, which overcomes the above-mentioned shortcomings. It uses ReLU activation functions and linear operations, making it differentiable and allowing gradient descent to be applied to learn the structure. The encoding is semantically equivalent to a (hard) oblique DT architecture, such that decisions (weights) in the DT correspond one-to-one to the trainable weights in the NN. The other weights are fixed (non-trainable) in the architecture. The primary use of DTSemNet is as a classifier; that is, it provides a class associated with a given input, using a typical argmax operation to select the class associated with the highest output. We show in Theorem 1 that DTSemNet is exactly equivalent to a DT; that is, for every input vector, the output class produced by DTSemNet and by the DT are the same. It has the potential for application in many different contexts to learn DTs. Compared with DGT [2], and ICCT [31], both needing many STE calls, DTSemNet uses ReLU and standard gradient descent without STE approximation. We propose a simple extension to DTSemNet for regression tasks, which requires one STE call to combine the choice of value with the choice of leaf. Each leaf is associated with a regressor whose parameters are learned"}, {"title": "2 Related Work", "content": "Non-Gradient-Based DT Training: There is a significant amount of approaches to learn DTs without relying on gradients. CART [10] (and its extensions) is a well-known method for training (axis-aligned) DTs, which is based on splitting the dataset at each node based on certain features using a metric, such as entropy or Gini impurity. Concerning learning oblique DTs in this way, some methods have been proposed, such as Oblique Classifier 1 (OC1) [30] or GUIDE [29]. They typically do not produce well-performing oblique DTs since learning oblique DT is much harder than learning axis-aligned DTs. TAO is currently a state-of-the-art (SOTA) method that can be used to learn oblique DTs. It enhances the performance of DT obtained by CART (or random DT of given depth) by alternatively fine-tuning node parameters at specific depths [12, 44].\nConcerning MIP formulations [6, 7] or Global EA-based search approaches, such as CRO-DT [15], they learn DTs by searching over various structures of DTs but at high computational costs, which is impractical for large DTs and datasets. The proposed DTSemNet overcomes these challenges by using gradient-descent to lower training time, which we confirm by comparing with CRO-DT [15], which proposes matrix encoding of (oblique) DTs to speed up training compared to previous EA-based methods, and produces axis-aligned DTs.\nGradient-Based DT Training: Numerous works propose to approximate DTs as soft-DTs to use gradient descent for learning DTs, where decision nodes typically use the Sigmoid function [43, 21, 42, 17, 37, 9, 36, 16, 32, 19, 24, 11, 41]. Hardening soft DTs, that is, transforming them into hard DTs by discretizing the probabilities induces severe inaccuracies [31]. More closely related to our work, DGT [2] represents (oblique) DTs as an NN-architecture using the (non-differentiable) sign activation function, resorting to quantized gradient descent to learn it, leveraging principles from training binarized NNs using STE [22]. Specifically, during forward propagation, nodes utilize a 0-1 step function, whereas, during backward propagation, nodes employ a piecewise linear function or some differentiable approximation (see [22]). Similarly, ICCT [31] learns (axis-aligned) DTs using NNs with the Sigmoid activation function and STEs. In all these works, the hard DTs that are produced are (slightly) different from the DT (soft DT or using STE), which is being optimized by gradient descent. In contrast, the DTSemNet architecture using ReLU activation functions allows standard gradient descent to be performed, and the output DT from DTSemNet-classification is exactly the same as the function optimized by gradient-descent, without approximation. Experiments confirm that it is more accurate in practice, significantly so for classification tasks.\nTraining DT policies in RL Setup: Hard or soft DT policies can be obtained via imitation learning [1], i.e., learning from expert policies, usually pretrained NNs [4, 23, 28, 8, 33, 38, 39, 14]. For instance, VIPER [4] imitates a Q-network (or policy network) by creating a dataset from collected samples and trains a DT using CART, with sample weightage assigned based on Q-values. In contrast, DTSemNet directly learns a hard oblique DT in RL (using PPO [34]). Other works, such as ProLoNet [35], that learn soft DTs using the RL framework, with the objective of initializing weights from expert humans. In contrast, DTSemNet learns a hard DT. ICCT [31] proposed an STE-based approach to learn axis-aligned DTs using gradient descent. By comparison, we can handle oblique trees, more expressive and accurate than axis-aligned DTs, especially for discrete actions."}, {"title": "3 The DTSemNet Architecture", "content": "We now describe our main contribution, namely DTSemNet, which encodes decision trees (DT) as deep neural Networks (DNNs) in a Semantically equivalent way. We prove the semantic equivalence in Theorem 1 thereafter."}, {"title": "4 Experimental Evaluation", "content": "In this section, we evaluate the performance of DTSemNet, comparing it with competing methodologies learning hard DTs. Firstly, we consider supervised learning setups using multiple benchmark multi-class classification and regression datasets, on which we compare the accuracy on test data with the SOTA non-greedy method TAO [12, 44], the SOTA gradient descent-based method DGT [2], both learning oblique DTs, as well as CRO-DT [15] for global-searches and CART as the standard for greedy algorithms, both learning axis-aligned DTs. The relative training times are reported for benchmarks for which it is available (notice that we cannot run TAO on our hardware as it is not openly available). Further, to understand the impact on the generalization of different architectures using gradient descent, we leverage insights from the loss landscape [27].\nLastly, we consider RL environments, both with discrete actions and continuous action sets. We compare DTSemNet with DGT, both of which generate oblique DT-policies, ICCT, which generates axis-aligned DT-policies, all three through gradient descent, as well as VIPER, which generates axis-aligned DT-policies through imitation learning of an NN policy generated by Deep RL, which we also report as a baseline.\nWe implemented DTSemNet and conducted all experiments using Python and PyTorch. Our testing platform has 8 CPU cores (AMD 75F3, Zen 3 architecture), 128 GB of RAM, and a 2 GB GPU (NVIDIA Quadro P620). The supplementary materials provide more details regarding datasets, splits, hyperparameters, etc., and our code is available at https://github.com/CPS-research-group/dtsemnet.\nClassification Tasks (small DTs): We first considered the 14 classification tabular datasets used in CRO-DT [15]. Global searches such as CRO-DT [15] are efficient only for small DTs (here up to depth 4, i.e. 32 nodes). We sort the benchmarks by the number of features, as it is more challenging to decide over more features, all the more so with small DTs. We report in Table 1 the (average) score over 100 DTs learned with different seeds for the most accurate height (up to 4). In every single benchmark, DTSemNet produces the most accurate DTs, with the biggest difference in Dry Beans, reducing the classification error from 11% to 8.6%. DGT is the second best, except in the 2 wine quality benchmarks, where TAO outperforms it. DGT usually comes close to DTSemNet, which is not surprising as the idea is similar, though the approximations used (STEs, quantized gradient descent) make DGT perform on average 0.85% worse than DTSemNet. Interestingly, the advantages grow to 1.3% on the 4 benchmarks with the most features (the hardest ones). Further, DGT needs larger trees than DTSemNet to get its best results (in 6 out of 14 benchmarks). Compared with non gradient-based learning, the advantage of DTSemNet is very tangible, 5.5% on average, growing to 12% on the 4 benchmarks with the most features. The highest difference is in Optical Recognition, reducing the classification error from 34.8% to just 6.7%.\nClassification Tasks (deeper DTs): We now considered the original 8 classification tasks reported in TAO [12] and reported in [2]. They use tabular datasets, except for MNIST, which is a small-sized image dataset. We used the fixed tree height specified in [12] (also reused in [2]). We first report in Table 2 the training time for the architecture on MNIST, as [12] reports this number for TAO. We train other architectures on comparable computing configurations. We provide the accuracy number obtained for reference. We also report the training time for the simpler DryBean, for comparison sake, except for TAO, for which this is not available. First, CRO-DT for MNIST takes much longer to train (number of generations set to the default 4k) than other architectures while having very low accuracy numbers (< 60% instead of > 90%), the reason why we do not consider it for these benchmarks with deeper DTs (indeed, [15] does not report CRO-DT results for these benchmarks). The training times of DGT and DTSemNet are almost identical due to their architectural similarities. As expected, training with gradient-based methods is observed to be significantly faster than non-gradient learning methods.\nWe sort the benchmarks by the height of the DTs, which is a good indication of the complexity of the tasks. We report in Table 3 the (average) score over 10 DTs learned with different seeds. Discussion: As in Table 1, DTSemNet produces the most accurate DTs in every single benchmark. With deeper trees, TAO recovers, and it is the second best 5 times vs 3 times for DGT. The advantage of DTSemNet over TAO is 1% on average, growing to 1.4% on the 4 benchmarks with deeper trees. The highest difference is in Letter, reducing the classification error from 12.6% to 10.8% and from 3.9% to 3% in Pendigits. The advantage of DTSemNet over DGT is 1.4% on average, growing to 2.1% on the 4 benchmarks with deeper trees, larger than for smaller trees from Table 1. The largest difference is in Letter, reducing the classification error from 13.9% to 10.8%, and from 6% to 3.9% in MNIST. The Friedman-Nemenyi test across all classification benchmarks, with a significance level of 0.05, shows that DTSemNet is significantly better than every other method with the average rank of 1.11 for DTSemNet, 2.25 for DGT, 2.86 for TAO, 3.60 for CRO-DT, and 3.77 for CART.\nTo understand the influence of approximations used in DGT (quantized gradient descent and STEs), we consider the loss landscape [27], which displays the effect of architecture choices on generalization. We consider MNIST, for which DGT makes much more classification errors than DTSemNet. Figure 5 displays the loss landscape around the final trained parameters along two random vector directions. A flatter loss landscape indicates better generalization capability [27]. As shown in Figure 5, the loss landscape of DTSemNet is very flat compared with DGT. This suggests that (approximation-free) DTSemNet allows better generalization on classification tasks than DGT.\nRegression Tasks: We consider the 5 regression datasets from [44], plus the two extra from [2] (for which TAO-linear results are not available). While TAO-linear [44] generates DT with regressors at the leaves, similarly as DTSemNet, allowing them to generalize, DGT [2] only learns scalar at the leaves, similarly as CART, and thus is less efficient (and cannot generalize), needing deeper DTs to get acceptable accuracy. To understand the impact of regressors at the leaves, we implemented DGT-linear, a modification of DGT using regressors at the leaves, in the same way as in DTSemNet and in ICCT [31]. Architectures with regressors at the leaves, namely DTSemNet(-regression), DGT-linear, and TAO-linear, use the same fixed height following [44], while DGT (without regressors) uses deeper trees (as reported in [2]) and CART is not restricted. The results of the regression datasets are presented in Table 4.\nDTSemNet-regression is sometimes second behind TAO-linear (3 benchmarks), and best for the 4 other benchmarks. DTSemNet is consistently better than (original) DGT, by 10% of RMSE on average over the 7 benchmarks, with 50% better RMSE scores for CTslice, even though DGT uses larger (hard oblique) DTs than DTSemNet. Compared with DGT-linear with regressors at the leaves (our adaptation), DTSemNet is still consistently better or tied, but the advantage is reduced to 3.7% of RMSE on average over the 7 benchmarks, with up to 23% better scores (CTSlice). Two third of the advantage of DTSemNet over DGT can be attributed to the regressors at the leaves, but still, a meaningful one-third of the advantage can be attributed to reducing the approximations, with only 1 STE call for DTSemNet-regression instead of n STE calls for DGT and DGT-linear, where n is the height of the DT. Overall, CTSlice is sensitive to approximations."}, {"title": "5 Conclusion", "content": "We introduced DTSemNet, an architecture semantically equivalent to oblique DTs. This architecture enables oblique DTs to be learned using vanilla gradient descent. We demonstrate its performance on supervised classification and regression datasets and RL tasks. DTSemNet consistently generates DTs more accurately (or as accurately in easier benchmarks) than competing architecture learning DTs using gradient descent. This is because DTSemNet-classification uses no approximation, unlike its competitors, and because DTSemNet-regression uses fewer approximations. Further, compared to non-gradient-based methodologies (greedy, non-greedy and global search) for learning DTS, DTSemNet is significantly faster. DTSemNet-classification outperforms the best of these methods, reducing the errors by > 10% on harder classification tasks, while the accuracy of DTSemNet-regression is competitive with the SOTA.\nLimitations: DTSemNet is a DT, making it unsuitable for high-dimensional inputs like images, where DTs struggle with complex shapes and require many leaves, negating their benefits.\nFuture Work: For DTSemNet, the choice of the height of the DT is treated as a hyperparameter, similar to the choice of the number of layers in an NN, in contrast to methods (e.g. CART [10]) that grow trees height. For future work, we will consider developing a regression architecture that does not rely on STE approximations and introduce differentiable methods for tree pruning and adaptive growth."}]}