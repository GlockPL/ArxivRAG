{"title": "Vanilla Gradient Descent for Oblique Decision Trees", "authors": ["Subrat Prasad Panda", "Blaise Genest", "Arvind Easwaran", "Ponnuthurai Nagaratnam Suganthan"], "abstract": "Decision Trees (DTs) constitute one of the major highly non-linear AI models, valued, e.g., for their efficiency on tabular data. Learning accurate DTs is, however, complicated, especially for oblique DTs, and does take a significant training time. Further, DTs suffer from overfitting, e.g., they proverbially \"do not generalize\" in regression tasks. Recently, some works proposed ways to make (oblique) DTs differentiable. This enables highly efficient gradient-descent algorithms to be used to learn DTs. It also enables generalizing capabilities by learning regressors at the leaves simultaneously with the decisions in the tree. Prior approaches to making DTs differentiable rely either on probabilistic approximations at the tree's internal nodes (soft DTs) or on approximations in gradient computation at the internal node (quantized gradient descent). In this work, we propose DTSemNet, a novel semantically equivalent and invertible encoding for (hard, oblique) DTs as Neural Networks (NNs), that uses standard vanilla gradient descent. Experiments across various classification and regression benchmarks show that oblique DTs learned using DTSemNet are more accurate than oblique DTs of similar size learned using state-of-the-art techniques. Further, DT training time is significantly reduced. We also experimentally demonstrate that DTSemNet can learn DT policies as efficiently as NN policies in the Reinforcement Learning (RL) setup with physical inputs (dimensions < 32).", "sections": [{"title": "1 Introduction", "content": "DTs are widely adopted in several domains, such as the medical domain [13]. Studies, including recent ones [18], show that DTs classify tabular datasets very well due to their inductive bias toward learning non-smooth functions, even better than NNs. Learning DTs for a given task is, however, a complex task: due to the combinatorial explosion over choices of the decisions to branch over at every node of the tree, learning optimal DTs is an NP-hard problem [25].\nCurrent methods of learning DTs can be broadly classified into four approaches: (a) greedy optimization, which grows trees using split criteria such as Classification and Regression Trees (CART) [10], (b) non-greedy optimization, which jointly optimize decision nodes under global objective such as Tree Alternating Optimization (TAO) [12, 44]), (c) Global searches, either presented as Mixed Integer Programs (MIP) [6], or using Evolutionary Algorithms (EA), e.g. [15], and (d) Gradient Descent by making the tree differentiable [21, 42, 43]. Greedy optimization techniques usually learn low-performing trees [43, 2], global searches are computationally expensive when the number of possible tree configurations is large [7, 15] and non-greedy"}, {"title": "2 Related Work", "content": "Non-Gradient-Based DT Training: There is a significant amount of approaches to learn DTs without relying on gradients. CART [10] (and its extensions) is a well-known method for training (axis-aligned) DTs, which is based on splitting the dataset at each node based on certain features using a metric, such as entropy or Gini impurity. Concerning learning oblique DTs in this way, some methods have been proposed, such as Oblique Classifier 1 (OC1) [30] or GUIDE [29]. They typically do not produce well-performing oblique DTs since learning oblique DT is much harder than learning axis-aligned DTs. TAO is currently a state-of-the-art (SOTA) method that can be used to learn oblique DTs. It enhances the performance of DT obtained by CART (or random DT of given depth) by alternatively fine-tuning node parameters at specific depths [12, 44].\nConcerning MIP formulations [6, 7] or Global EA-based search approaches, such as CRO-DT [15], they learn DTs by searching over various structures of DTs but at high computational costs, which is impractical for large DTs and datasets. The proposed DTSemNet overcomes these challenges by using gradient-descent to lower training time, which we confirm by comparing with CRO-DT [15], which proposes matrix encoding of (oblique) DTs to speed up training compared to previous EA-based methods, and produces axis-aligned DTs.\nGradient-Based DT Training: Numerous works propose to approximate DTs as soft-DTs to use gradient descent for learning DTs, where decision nodes typically use the Sigmoid function [43, 21, 42, 17, 37, 9, 36, 16, 32, 19, 24, 11, 41]. Hardening soft DTs, that is, transforming them into hard DTs by discretizing the probabilities induces severe inaccuracies [31]. More closely related to our work, DGT [2] represents (oblique) DTs as an NN-architecture using the (non-differentiable) sign activation function, resorting to quantized gradient descent to learn it, leveraging principles from training binarized NNs using STE [22]. Specifically, during forward propagation, nodes utilize a 0-1 step function, whereas, during backward propagation, nodes employ a piecewise linear function or some differentiable approximation (see [22]). Similarly, ICCT [31] learns (axis-aligned) DTs using NNs with the Sigmoid activation function and STEs. In all these works, the hard DTs that are produced are (slightly) different from the DT (soft DT or using STE), which is being optimized by gradient descent. In contrast, the DTSemNet architecture using ReLU activation functions allows standard gradient descent to be performed, and the output DT from DTSemNet-classification is exactly the same as the function optimized by gradient-descent, without approximation. Experiments confirm that it is more accurate in practice, significantly so for classification tasks.\nTraining DT policies in RL Setup: Hard or soft DT policies can be obtained via imitation learning [1], i.e., learning from expert policies, usually pretrained NNs [4, 23, 28, 8, 33, 38, 39, 14]. For instance, VIPER [4] imitates a Q-network (or policy network) by creating a dataset from collected samples and trains a DT using CART, with sample weightage assigned based on Q-values. In contrast, DTSemNet directly learns a hard oblique DT in RL (using PPO [34]). Other works, such as ProLoNet [35], that learn soft DTs using the RL framework, with the objective of initializing weights from expert humans. In contrast, DTSemNet learns a hard DT. ICCT [31] proposed an STE-based approach to learn axis-aligned DTs using gradient descent. By comparison, we can handle oblique trees, more expressive and accurate than axis-aligned DTs, especially for discrete actions."}, {"title": "3 The DTSemNet Architecture", "content": "We now describe our main contribution, namely DTSemNet, which encodes decision trees (DT) as deep neural Networks (DNNs) in a Semantically equivalent way. We prove the semantic equivalence in Theorem 1 thereafter."}, {"title": "4 Experimental Evaluation", "content": "In this section, we evaluate the performance of DTSemNet, comparing it with competing methodologies learning hard DTs. Firstly, we consider supervised learning setups using multiple benchmark multi-class classification and regression datasets, on which we compare the accuracy on test data with the SOTA non-greedy method TAO [12, 44], the SOTA gradient descent-based method DGT [2], both learning oblique DTs, as well as CRO-DT [15] for global-searches and CART as the standard for greedy algorithms, both learning axis-aligned DTs. The relative training times are reported for benchmarks for which it is available (notice that we cannot run TAO on our hardware as it is not openly available). Further, to understand the impact on the generalization of different architectures using gradient descent, we leverage insights from the loss landscape [27].\nLastly, we consider RL environments, both with discrete actions and continuous action sets. We compare DTSemNet with DGT, both of which generate oblique DT-policies, ICCT, which generates axis-aligned DT-policies, all three through gradient descent, as well as VIPER, which generates axis-aligned DT-policies through imitation learning of an NN policy generated by Deep RL, which we also report as a baseline.\nWe implemented DTSemNet and conducted all experiments using Python and PyTorch. Our testing platform has 8 CPU cores (AMD 75F3, Zen 3 architecture), 128 GB of RAM, and a 2 GB GPU (NVIDIA Quadro P620). The supplementary materials provide more details regarding datasets, splits, hyperparameters, etc., and our code is available at https://github.com/CPS-research-group/dtsemnet.\nClassification Tasks (small DTs): We first considered the 14 classification tabular datasets used in CRO-DT [15]. Global searches such as CRO-DT [15] are efficient only for small DTs (here up to depth 4, i.e. 32 nodes). We sort the benchmarks by the number of features, as it is more challenging to decide over more features, all the more so with small DTs. We report in Table 1 the (average) score over 100 DTs learned with different seeds for the most accurate height (up to 4).\nIn every single benchmark, DTSemNet produces the most accurate"}, {"title": "5 Conclusion", "content": "We introduced DTSemNet, an architecture semantically equivalent to oblique DTs. This architecture enables oblique DTs to be learned using vanilla gradient descent. We demonstrate its performance on supervised classification and regression datasets and RL tasks. DTSemNet consistently generates DTs more accurately (or as accurately in easier benchmarks) than competing architecture learning DTs using gradient descent. This is because DTSemNet-classification uses no approximation, unlike its competitors, and because DTSemNet-regression uses fewer approximations. Further, compared to non-gradient-based methodologies (greedy, non-greedy and global search) for learning DTS, DTSemNet is significantly faster. DTSemNet-classification outperforms the best of these methods, reducing the errors by > 10% on harder classification tasks, while the accuracy of DTSemNet-regression is competitive with the SOTA.\nLimitations: DTSemNet is a DT, making it unsuitable for high-dimensional inputs like images, where DTs struggle with complex shapes and require many leaves, negating their benefits.\nFuture Work: For DTSemNet, the choice of the height of the DT is treated as a hyperparameter, similar to the choice of the number of layers in an NN, in contrast to methods (e.g. CART [10]) that grow trees height. For future work, we will consider developing a regression architecture that does not rely on STE approximations and introduce differentiable methods for tree pruning and adaptive growth."}]}