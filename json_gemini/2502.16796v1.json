{"title": "MobileSteward: Integrating Multiple App-Oriented Agents with Self-Evolution to Automate Cross-App Instructions", "authors": ["Yuxuan Liu", "Hongda Sun", "Bo Du", "Wei Liu", "Jian Luan", "Rui Yan"], "abstract": "Mobile phone agents can assist people in automating daily tasks on their phones, which have emerged as a pivotal research spotlight. However, existing procedure-oriented agents struggle with cross-app instructions, due to the following challenges: (1) complex task relationships, (2) diverse app environment, and (3) error propagation and information loss in multi-step execution. Drawing inspiration from object-oriented programming principles, we recognize that object-oriented solutions is more suitable for cross-app instruction. To address these challenges, we propose a self-evolving multi-agent framework named MobileSteward, which integrates multiple app-oriented StaffAgents coordinated by a centralized StewardAgent. We design three specialized modules in MobileSteward: (1) Dynamic Recruitment generates a scheduling graph guided by information flow to explicitly associate tasks among apps. (2) Assigned Execution assigns the task to app-oriented StaffAgents, each equipped with app-specialized expertise to address the diversity between apps. (3) Adjusted Evaluation conducts evaluation to provide reflection tips or deliver key information, which alleviates error propagation and information loss during multi-step execution. To continuously improve the performance of MobileSteward, we develop a Memory-based Self-evolution mechanism, which summarizes the experience from successful execution, to improve the performance of MobileSteward. We establish the first English Cross-APP Benchmark (CAPBench) in the real-world environment to evaluate the agents' capabilities of solving complex cross-app instructions. Experimental results demonstrate that MobileSteward achieves the best performance compared to both single-agent and multi-agent frameworks, highlighting the superiority of MobileSteward in better handling user instructions with diverse complexity.", "sections": [{"title": "1 INTRODUCTION", "content": "Mobile phone agents can assist people to automate simple tasks and bring much convenience to people's daily lives, which have emerged as a pivotal research spotlight [2, 25, 28]. The rapid advancement of mobile technology has led to the proliferation of apps with diverse functionalities, allowing users to accomplish increasingly complex tasks. Consequently, developing more powerful agents capable of handling complex user instructions in such environments is of great importance and application prospect.\nExisting mobile phone agents have achieved some encouraging results on automated task execution. Several API-based solutions have been successfully deployed in real-world mobile phones, such as Siri, Google Assistant, and XiaoAI [2]. Despite these advances,"}, {"title": "2 RELATED WORK", "content": "2.1 Automated Task Execution on Mobile Phone\nMobile phones have become so inseparable from our lives, that the development of automated user task execution on mobile phones has become a research spotlight, which can be categorized into three types of methods: (1) API-based methods, which are favored by industry and have already been deployed for user in actual mobile phones, e.g., Siri, Google Assistant and XiaoAI [2]. However, such methods are limited by API access and invocation to some extent. (2) GUI-based methods, which seek for automated task execution by simulating interactions with the graphical user interfaces (GUIs) [3\u20135, 19]. These methods usually require screen summarizing [6\u20138], widgets recognition [9, 10] and command grounding [11, 12] to augment the GUI understanding and action prediction. Moreover, Spotlight [13] designed a Region-of-Interest (ROI) Align to localize to the regions and widgets that more relevant to the task. (3) Experience-based methods, which learn from the historical experience. MobileGPT [30] constructs a Hierarchical App Memory through exploration and then uses Flexible Task Recall in the execution phase. AutoDroid [31] constructs the UI Transition Graph (UTG) by exploring during the offline phase, which in turn is extracted to form memory.\nThe diversity among apps makes existing methods ineffective in solving tasks on various apps, so we design Assigned Execution to utilize app-oriented StaffAgents to complete tasks on specific apps.\n2.2 LLM/LMM Agents on Mobile Phones\nThe rapid development of LLMs/LMMs has encouraged them to be adopted as agents on mobile phones [26]. These methods utilize LLM's powerful semantic reasoning capabilities to analyze the tasks [14, 16] or LMM's excellent image comprehension capabilities to assist the GUI understanding [15, 17]. We can divide them into three categories: (1) Pre-trained methods: CogAgent [22] and ScreenAI [23], pre-train a visual language model on a mix of screen tasks (QA, summarization, annotation and navigation) to build a general agent for automated task execution. (2) Fine-tuned methods: These methods usually include the historical information to assist in action decisions. Auto-UI [18] introduces historical actions during fine-tuning on a large scale dataset AITW [20] and can be improved by adopting Chain-of-Action-Thought(CoAT) [21].\nCoCo-Agent[27] augments the screenshot with the textual layout representation and conduct conditional action prediction. (3) Inference methods: These methods instruct LLMs/LMMs for planning, decision making or reflection to automate tasks [29]. AppAgent [25] generates documents by self-exploration/demo-watching and adopts SoM [61] to assist in action decision. MobileAgent [28] augments action grounding with visual perception module and action execution with self-planning and self-reflection.\nThese single-agent methods struggle to solve cross-app instructions because of the long execution, thus we designed Adjusted Evaluation to alleviate the information loss and error propagation.\n2.3 Multi-Agent Framework\nThe success of AutoGPT [34], HuggingGPT [33] and OpenAGI [32] demonstrates the ability of autonomous agents to perform simple tasks. In order to solve complex task, the multi-agent framework has been widely explored by many researchers [35]. CAMEL [39] and AutoGen [40] focuses on complex solutions through communication among agents. ChatDev [36] and MetaGPT [37] split the process of program development into several stages that each engages an agent to facilitate a seamless workflow. The same strategy has been used in recommendation [38, 41], debate [42, 43], question-answering [44] and fact-checking [47, 60]. The multi-agent framework has also been applied to many social simulation works, where many role-played agents simulate the development of the society through the interaction and cooperation [48\u201350, 62, 63]. While the multi-agent framework on mobile scenarios is still under-explored. MobileAgent-v2 [51] integrates planning, decision and reflection agents forming a pipeline equipped with memory unit to improve the performance of automated task execution, while it still struggle for cross-app instructions.\nMost of the current multi agent frameworks use procedure-oriented agent splitting, while cross-app instructions are more suitable for object-oriented approach, thus we build an app-oriented multi-agent framework with self-evolution."}, {"title": "3 MOBILESTEWARD", "content": "3.1 Task Formulation\nMobile Task Automation is to automatically complete an instruction I through an action sequence [31]. At each step i, the model decides the next action aj,i based on the current state information Sj,i obtained from the mobile phone environment E. Thus, the instruction is automatically performed by an execution history H. While for complex cross-app instructions, which essentially require executing a sequence of tasks Tj in the corresponding Appj, the execution history H can be further detailed as follows:\nI = [T1,..., Tj,..., Tm], (1)\nH = [H1,\uff65\uff65\uff65, Hj, \u2026\u2026\u2026, Hm], (2)\nHj = [aj,1,\u00b7\u00b7\u00b7, aj,i,\u2026\u2026\u2026, aj,n], (3)\naj,i = (Si, Tj) (4)\nTherefore, to automatically execute complex cross-app instructions, it is essential to ensure: (1) Instructive task scheduling, which involves decomposing instruction I and scheduling the task Tj; (2)"}, {"title": "3.2 Framework Overview", "content": "We propose MobileSteward, a self-evolving multi-agent framework, consists of a centralized StewardAgent and multiple app-oriented StaffAgents. As shown in Figure 2, we design three specialized modules within the MobileSteward: (1) Dynamic Recruitment: StewardAgent splits the instruction and schedules the corresponding StaffAgents. (2) Assigned Execution: StaffAgent executes the assigned task on the target app. (3) Adjusted Evaluation: StewardAgent evaluates the execution results, provides reflection, delivers information and adjusts the schedule. In order to improve the multiple agents within the framework, we equip the StewardAgent with a Staff Expertise Memory for the cognition of StaffAgents' expertise, and equip the StaffAgents with a Task Guideline Memory for task execution demonstrations. We provide a pseudo-code of MobileSteward in Algorithm 1 and we will detail the design of the multiple agents and the entire framework in subsequent sections.\n3.3 StewardAgent and StaffAgent\nIn a large manor, there will be a steward to convey the host's orders, and several staff in charge of specific jobs. Following this pattern, we build StewardAgent and StaffAgent via role-playing. We inject the definition of the role at the beginning of the prompt. We will describe their responsibilities next.\nStewardAgent Isteward is responsible for controlling the entire task execution process, including: (1) Schedule: scheduling the StaffAgent and scheduling tasks; (2) Evaluate: evaluating the StaffAgent's task execution; (3) Reflect: providing reflection and suggestions on wrong execution; (4) Extract: extracting results and experience from successful execution; (5) Adjust: delivering the key information and adjusting the subsequent schedule. (6) Update: updating the memory for improvement.\nStaffAgent Ostaff is responsible for operating a specific app. In order to distinguish between different StaffAgents, we emphasize its proficiency in that app and add a app description in the role-playing prompt. Moreover, we equip StaffAgents with an app-specific Task Guideline Memory. The core task of StaffAgent is to execute the tasks in the app, including: (1) Plan: planning with the successful task execution; (2) Predict: predicting the next action; (3) Summary: summarizing the previous action. We adopt the AppAgent [25] to build StaffAgent, simplifying the action space as shown in the Table 1. We extract the interactive widgets and textual content from the XML, perform a hierarchical simplification. Then we mark these elements on the screenshot and feed the XML information aligned with the screenshot into the StaffAgent to predict the action."}, {"title": "3.4 Dynamic Recruitment", "content": "Complex cross-app instructions require scheduling of multiple StaffAgents to operate the apps. While, the scheduling of StaffAgents is dynamically aligned with the user instructions. Moreover, cross-app instructions often contain complex task association and information transfer between apps. To address these issues, we design Dynamic Recruitment to instruct StewardAgent to generate a scheduling graph for StaffAgents with the guidance of information flow. We also equip the StewardAgent with a Staff Expertise Memory that records the app description and expertise list.\nOn receiving the instruction I, StewardAgent decomposes the instruction into sub-tasks on the specific apps based on Staff Expertise Memory ME, and then analyzes the information flow between these tasks, and outputs these contents as thought. Subsequently, based on the previous thought, StewardAgent recruits the StaffAgents corresponding to these apps and constructs the scheduling graph SG among them, which is exported as plan. The process can be described as:\nSG = Schedule (I, ME; steward) (5)"}, {"title": "3.5 Assigned Execution", "content": "Due to the significant variance in functionality, content, and layout between apps, we dedicate each StaffAgent to operate a specific app, and equip each StaffAgent with an app-specific Task Guideline Memory to support its task execution. The scheduling graph generated in the Dynamic Recruitment phase is a DAG, so we use topological sorting on the scheduling graph to assign tasks to the corresponding StaffAgent staff; for execution.\nStaffAgent first extracts the successful execution demonstrations related to the assigned task Tj from memory MG to make a task plan pj. At each execution step i, StaffAgent obtains the current state information Sj,i from the mobile phone environment E, which contains the screenshots and XML layout file. Combining the received result information Rj and reflection tips tj obtained from the preceding execution, guided by the task plan pj, StaffAgent will decide an action aji to advance the assigned task Tj on the current state Sj,i. After each execution, StaffAgent will generate the current action summarization sj,i, which will conclude the previous action sequence, the current execution result and the functional description of the related GUI element. After completing the task or reaching the maximum number of steps, the StaffAgent packages the execution history Hj. We can formulate the process as follows:\nPj = Plan(Tj, MG; Ostaff;) (6)\naj,i = Predict(Tj, Sj,i, MG, Sj,i\u22121, \u00dej, Rj, tj; staff;) (7)\nsj,i = Summary(Tj, Sj,i, aj,i; \u00destaf fj) (8)\nHj = [(Sj,1, aj,1, Sj,1), \u00b7\u00b7\u00b7, (Sj,n, aj,n, Sj,n)] (9)"}, {"title": "3.6 Adjusted Evaluation", "content": "In order to better control the execution of tasks and the advancement of scheduling, we design Adjusted Evaluation which utilizes the StewardAgent to evaluate the execution process of the StaffAgents, providing reflection tips on error execution or summarizing the correct execution to deliver key information and adjust succeeding schedule according to the scheduling graph.\nStewardAgent will generate the evaluation ej on the simplified execution history Hj of assigned task Tj from StaffAgent staf fj. If there exist errors, StewardAgent will give reflection tips tj that will contribute to the task automation. If the task is completed, StewardAgent will extract the task result information rj, the staff expertise me and the task guideline mg from the execution process. Subsequently, the task result information will be delivered according to the scheduling graph SG, and used to adjust the succeeding task schedules. The Adjusted Evaluation will be formulated as:\nej = Evaluate(Hj, Tj; steward) (10)\ntj = Reflect(Hj, Tj; \u0424steward), if ej == ERROR (11)\nrj, me, mg = Extract(Hj, Tj; \u0424steward), if ej == SUCCESS (12)\nTk = Adjust(Tk, rj;\u00desteward), if (Tj, Tk) \u2208 SG (13)\nRk.append(rj), if (Tj, Tk) \u2208 SG (14)"}, {"title": "3.7 Memory-Based Self-Evolution", "content": "As in reality, with the accumulation of experience, the steward will become more aware of the staff's suitable job and staff will become more proficient at their job. Therefore, we propose Memory-Based Self-Evolution for continuous improvement of MobileSteward. Specifically, we equip the StewardAgent with a Staff Expertise Memory records a description of the app as well as a list of the expertise of the StaffAgent. These will be used for task decomposition and scheduling during the Dynamic Recruitment. Meanwhile, we equip the StaffAgent with Task Guideline Memory, which records successful task steps. These demonstrations will be used as references for planning and predicting in the Assigned Execution.\nMobileSteward's self-evolution is achieved by constantly updating both memories. After the StaffAgent has successfully completed the assigned task, StewardAgent will extract the staff expertise me and task guideline mg, and then use them to update the Staff Expertise Memory ME and Task Guideline Memory MG respectively. When updating Staff Expertise Memory, StewardAgent needs to determine whether the newly extracted me needs to be updated into the memory. When updating the Task Guideline Memory, the StewardAgent updates the (Tj,mg) pairs into the Memory. The process of memory update can be described as follows:\nME = Update (ME, me; steward) (15)\nMG = Update (MG, (Tj, mg); \u0424steward) (16)"}, {"title": "4 EXPERIMENTS", "content": "4.1 Benchmarks\nThe evaluation of mobile phone agents is more convincing in a real-world interactive environment, which is more complex and dynamic [29]. Therefore, we built a simulation environment with Pixel 8 Pro in Android Studio and used ADB to complete the interaction with the simulator. We conduct evaluation on two benchmarks.\nCross-APP Benchmark: We construct CAPBench, which is specialized for complex cross-app instructions. CAPBench takes into full consideration about the task association and information transfer that exists among apps in real-world scenarios. We select a total of 14 apps in 6 categories, including life, social, news, entertainment, shopping and traveling, and ensure that there exist reasonable task associations among these apps. To generate the cross-app instruction data, we manually annotated each app with a function description as well as a list of common task templates. When constructing the data, we randomly select 2-4 apps from the candidate apps, and then prompt GPT-4 with the app information to analyze the existence of reasonable task associations and information transfer among the apps, and then select the corresponding task templates to be instantiated to assemble the cross-app instructions. We then ask the annotators to perform a human evaluation, eliminating invalid instructions. In total, we constructed 500 cross-app instructions, we present a statistics for app categories in Figure 3 and the number of tasks by complexity in Figure 4.\nSingle-APP Benchmark: For more comprehensive evaluation of our framework, we collect SAPBench from the previous works [25, 28] with a total of 50 instructions."}, {"title": "4.2 Experimental Setup", "content": "4.2.1 Baselines. To verify the effectiveness of our proposed framework, we compare MobileSteward with both single agent and multi agent baselines.\n(1) AppAgent [25] uses XML file to extract interactive elements and generates element-level function documents by self-exploration and demo-watching, which will be used to assist action decision. We add a home() action to complete the cross-app instructions.\n(2) MobileAgent [28] introduces a visual perception module to localize the natural language described actions on the screen.\n(3) MobileAgent-v2 [51] proposes a process-oriented multi-agent framework that integrates planning, decision and reflections agents to form a working-flow, which is equipped with a memory unit to store and retrieve key information during execution.\n4.2.2 Evaluation Metrics. We design a multi-granularity evaluation metric, including success rate and app rate. For cross-app instructions, we use a app-level task rate, while for single-app instructions, we use a more fine-grained step rate.\n\u2022 Success Rate: To evaluate whether the instruction is completed.\n\u2022 App Rate: To evaluate the percentage of the overlap between the apps covered in the execution and the labeled apps.\n\u2022 Task Rate: To evaluate the app-level completion of instruction, which is the ratio of the apps completing the task to the total apps.\n\u2022 Step Rate: To evaluate the step-level action accuracy, which is the ratio of the correct steps to the total steps during execution.\n4.2.3 Implementation Details. In our proposed MobileSteward, we use the same base model to build StewardAgent and StaffAgents, that we used the gpt-4-vision-preview version of GPT-4v, and the gpt-4o (2024-05-13) version of GPT-4o, and we set temperature to 0. For Dynamic Recruitment and Adjusted Evaluation, we prompt with a 2-shot in-context learning. For Assigned Execution, we use zero-shot prompting. We set the max try Ntry to 3 and max step Nstep to 20 for MobileSteward, and set max step Nstep to 80 for all of the baselines. StaffAgent uses BM25 [55] to retrieve top-3 most relevant tasks from Task Guideline Memory for reference. For Self-Evolution, we sample 50 instructions from each complexity split of CAPBench for test and use the remaining instructions for prior self-evolution, we also update the memories during the test.\nWe build up a mobile phone environment with the Pixel 8 Pro in Android Studio. We use the API level of 34 and the Target of Android 14 (Google Play) 1."}, {"title": "4.3 Overall Performance", "content": "We compared MobileSteward with all baselines on two benchmarks and the experimental results are shown in the Table 2. The experimental results demonstrate the following conclusions:\nMobileSteward can effectively solve both single and cross app instructions. The experimental results for Success Rate indicate that baseline methods perform terribly on CAPBench. Both AppAgent and MobileAgent struggle to complete cross-app instructions, with their App Rate being 60% lower than that of MobileSteward. This suggests they have difficulty in selecting the appropriate apps to accomplish the instruction. The Task Rate reflects that the long execution sequence leads to error propagation and information loss. Thus the best performance of MobileSteward demonstrate that Staff Expertise Memory contributes to assigning task to appropriate app-oriented StaffAgent in Dynamic Recruitment. Furthermore, Adjusted Evaluation can effectively alleviate error propagation and information loss between StaffAgents. Although MobileSteward is designed to address complex cross-app instructions, experimental results show that it is equally effective to solve simple single-app instructions. The baseline methods is much lower on App Rate, which demonstrates that our self-evolution of Staff Expertise Memory can effectively improve the schedule of StaffAgent.\nApp-oriented multi-agent framework is more effective. From the experimental results, MobileAgent-v2 and MobileSteward perform better on both cross app and single app instructions, indicating that the multi-agent methods is more effective compared to the single-agent methods. Compared to procedure-oriented MobileAgent-v2, MobileSteward is 38% and 14% higher on CAPBench and SAP-Bench respectively. The great diversity between apps result in a 30% lower Task Rate of MobileAgent-v2 because it is difficult to use one agent to handle all of apps. While, our proposed MobileSteward is an app-oriented multi-agent framework, and the experimental results demonstrate that Dynamic Recruitment and Assigned Execution can schedule more appropriate app-oriented StaffAgents to execute the assigned task effectively. Meanwhile, unlike MobileAgent-v2, which is a static framework, our proposed Memory-based Self-evolution mechanism can dynamically improve the StaffAgent's expertise in the specific app and the StewardAgent's schedule of the tasks. Therefore, our app-oriented multi-agent framework is more effective to solve the cross-app instructions."}, {"title": "4.4 Ablation Study", "content": "We design ablation experiment to fully explore the effectiveness of our proposed modules, the experimental results are shown in Table 3. We can find that: (1) When Self-Evolution is disabled, we find that both Task Rate and App Rate decreased. which demonstrate that in Self-Evolution, StaffAgent improves task execution through Task Guideline Memory, and StewardAgent improves task schedule between apps through Staff Expertise Memory. (2) When Assigned Execution is removed, the decrease of Success Rate and Task Rate is more obvious, which indicates that using app-oriented StaffAgent can effectively improve the execution of tasks because they have more guideline information of the specific app. (3) The absence of Adjusted Evaluation causes the decrease in App Rate, which illustrates its impact on Self-Evolution. The decreases in Success Rate and Task Rate indicate that Adjusted Evaluation is effective in evaluating the execution process and can provide reflective suggestions to correct incorrect execution."}, {"title": "4.5 Further Analysis", "content": "4.5.1 Analysis on MobileAgentBench. We evaluate our proposed MobileSteward on MobileAgentBench [59], which consists of 100 tasks across 10 simple system apps. To assess performance, we employ five metrics: (1) SR: Success Rate; (2) SE: Step-wise Efficiency; (3) IOT: Input-Output Tokens; (4) FN: False Negative; (5) FP: False Positive. The experimental results, as shown in Table 4, demonstrate that MobileSteward achieves the highest SR while maintaining competitive SE, indicating its efficiency in completing simple tasks. The slightly higher FN can be attributed to the smaller number of failed task samples; however, this value normalizes to 0.16 when adjusted"}, {"title": "4.5.2 Analysis of Complexity on CAPBench.", "content": "We conducted a more detailed in-depth analysis of CAPBench using the number of apps involved in the task as a measure of complexity. The analysis results are shown in Figure 5. As the task complexity increases, the Success Rate of all methods decreases dramatically. MobileAgent directly fails to complete the task with 4 apps. And MobileAgent-v2 also decreased by 46% from 2app to 4app. In comparison, our MobileSteward only decreased by 33% as the task complexity increased. And compared to MobileAgent-v2, we improve 1.77 times on 2app and 2.43 times on 4app, which all proves that our MobileSteward performs better on more complex cross-app instruction. Meanwhile, as for the App Rate, MobileSteward remains stable as the complexity rises, while all of the baselines decrease to different degrees, indicating that MobileSteward is able to accomplish the scheduling between tasks more efficiently.\n4.5.3 Analysis of Dynamic Recruitment. In order to validate the effectiveness of our Dynamic Recruitment in task scheduling, we use different base models that have difference numbers of parameters and capabilities to accomplish the Dynamic Recruitment. The comparison results are shown in Table 5. Compared to the naive text plan used in MobileAgent-v2, our proposed Dynamic Recruitment is more effective, that we can outperform MobileAgent-v2 equipped with GPT-4o using only a 14B GLM-4V. Because we use information flow to guide the generation of the scheduling graph between StaffAgents, which can explicitly establish the association between tasks, including the scheduling order and information transfer between them. This contains more information than a naive text plan, and therefore gives clearer guidance during subsequent execution and ensures the efficient transfer of information.\n4.5.4 Analysis of Self-Evolution. In order to validate the effectiveness of our proposed self-evolution, we have designed experiments to compare the accuracy of task scheduling using hand-crafted and self-evolving staff expertise. As shown in Figure 6, the self-evolving staff expertise can achieve comparable results with hand-crafted, which validates the effectiveness of our proposed self-evolution that it can summarize the staff expertise from the successful execution and assist in the task scheduling.\n4.5.5 Analysis of Efficiency. We compare our proposed MobileSteward with the strong baseline MobileAgent-v2 to evaluate efficiency. To ensure a comprehensive assessment, we utilize two metrics: (1) Actions per Task (A/P): The number of actions required to complete a task. (2) Tokens per Action (T/A): The number of tokens consumed for each action decision. As shown in Table 6, while MobileAgent-v2 demonstrates comparable efficiency to MobileSteward on simple tasks (SAPBench), it requires more actions to complete complex"}, {"title": "4.5.6 Analysis of Online User Experiments.", "content": "We evaluate MobileSteward on 50 tasks provided by 10 online users, with the experimental results presented in Table 7. MobileSteward demonstrates comparable performance in Success Rate and Task Rate across both online and offline environments. The observed decline in App Rate can be attributed to the ambiguity in user instructions, which occasionally prevents the system from identifying the exact target application."}, {"title": "4.6 Case Study", "content": "We illustrate the execution of an instruction with a complexity level of 3App in Figure 7, including the Assigned Execution of StaffAgent and Adjusted Evaluation of StewardAgent. After StaffAgent specializing in Expedia finds the arrival time of a flight from Shanghai to London, StewardAgent evaluates that the execution has successfully completed the task and extracts the task result information: the arrival time is 6:30 p.m.. Then StewardAgent delivers the task result information to the StaffAgent specialized in Clock and Note based on the scheduling graph generated in Dynamic Recruitment and adjusts the task assigned to them with the task result information."}, {"title": "5 CONCLUSION", "content": "We integrate multiple app-oriented StaffAgents coordinated by a centralized StewardAgent to constitute a self-evolving multi-agent framework named MobileSteward. For better executing cross-app instructions, we design three specific modules: Dynamic Recruitment generates a scheduling graph to explicitly associate tasks among apps; Assigned Execution assigns the task to an app-oriented StaffAgent to prevent the interference of diversity between apps; Adjusted Evaluation conducts evaluation to alleviates error propagation or information loss during multi-step execution. We optimize MobileSteward using a Memory-base Self-evolution mechanism that can learn from the successful execution. In order to evaluate our MobileSteward, we construct the first English Cross-APP Benchmark(CAPBench) in the real-world environment. The experimental results demonstrate that our MobileSteward achieve the best performance compared to both single-agent and multi-agent baselines on solving cross-app instructions."}]}