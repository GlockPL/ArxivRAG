{"title": "Theoretical Constraints on the Expressive Power of RoPE-based Tensor Attention Transformers", "authors": ["Xiaoyu Li", "Yingyu Liang", "Zhenmei Shi", "Zhao Song", "Mingda Wan"], "abstract": "Tensor Attention extends traditional attention mechanisms by capturing high-order correlations across multiple modalities, addressing the limitations of classical matrix-based attention. Meanwhile, Rotary Position Embedding (RoPE) has shown superior performance in encoding positional information in long-context scenarios, significantly enhancing transformer models' expressiveness. Despite these empirical successes, the theoretical limitations of these technologies remain underexplored. In this study, we analyze the circuit complexity of Tensor Attention and RoPE-based Tensor Attention, showing that with polynomial precision, constant-depth layers, and linear or sublinear hidden dimension, they cannot solve fixed membership problems or (AF,r)* closure problems, under the assumption that TC\u00b0 \u2260 NC\u00b9. These findings highlight a gap between the empirical performance and theoretical constraints of Tensor Attention and ROPE-based Tensor Attention Transformers, offering insights that could guide the development of more theoretically grounded approaches to Transformer model design and scaling.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), such as OpenAI's ChatGPT [AAA+23], Google's Gemini [Goo24], Anthropic's Claude 3.5 [Ant24], and Meta's LLaMA 3.3 [LT24] have reshaped a wide range of fields by demonstrating unprecedented advancements. These advancements are primarily due to their capability to efficiently process long-context inputs, a crucial feature for tasks like summarizing lengthy documents (e.g., medical reports, legal analyses, technical briefs), enabling superior reasoning and problem-solving performance at a level comparable to expert human analysis. At the core of these advancements lies the Transformer architecture [VSP+17], driven by its self-attention mechanism. Understanding computational primitives that Transformer components enable is pivotal for principled interpretations and exposing limitations in Transformer-based systems.\nPrevious research has investigated these questions by analyzing the expressiveness of Transformers. As an illustration, the work in [MS23] showed that constant-depth threshold circuit families can effectively emulate Transformers with precision clogn and depth-d. This holds true in both non-uniform and L-uniform computational models. This result highlights Transformers' computational efficiency and structural adaptability when analyzed through circuit complexity theory's lens. Expanding on these results, [Chi24] showed that Transformers with O(log n) precision belong to DLOGTIME-uniform TC\u00ba, even when the absolute error is bounded by 2-0(poly(n)).\nTo augment the capabilities of Transformers, innovations such as Rotation Position Embedding (ROPE)[SAL+24] have been proposed. Through the rotation matrices, RoPE improves the sequence length adaptability while enhancing the efficacy of attention mechanisms. Meanwhile, multi-view approaches are increasingly recognized for capturing high-order correlations in diverse data types, including mathematical data [SHT24], graph structures [DLG+21, LYH+23], and multi-modality datasets [LAJ15]. Models like GPT-40 [Ope24] and Google's Project Astra [Goo24] exemplify this trend, integrating reasoning across multi-modality in real-time. Despite these advancements, classical attention mechanisms face representational limitations. Specifically, [SHT24] demonstrated that matrix attention can only capture pairwise correlations, falling short in modeling triple-wise or higher-order interactions. Addressing such limitations typically requires multiple layers or carefully designed architectures, complicating the integration of multi-view information.\nTo overcome these constraints, [SHT24] and [AS24b] proposed Tensor Attention, a higher-order extension of matrix attention. Tensor Attention intrinsically captures high-order correlations, defined as Softmax(Q(K1 K2)\u00af)(V1 V2) (see Definition 3.30), where denotes the column-wise Kronecker product (see Definition 3.21). Here, Q, K\u2081/V\u2081, and K2/V2 represent inputs from different views or modalities. This raises a natural question:\nDoes the ROPE and tensor attention enhance the expressiveness of the ROPE-based tensor attention Transformer?\nThis work addresses this question through the lens of circuit complexity, advancing the theoretical understanding of tensor attention and RoPE-based tensor attention mechanisms.\nWe present a rigorous analysis of tensor attention Transformers and RoPE-based tensor attention Transformers, delineating their intrinsic computational limitations. Our approach methodically evaluates the circuit complexity of each architectural component, ranging from basic trigonometric operations to the comprehensive RoPE-based tensor attention Transformers. Specifically, it is demonstrated that uniform TC\u00ba circuits are amenable to simulating the components mentioned above. Furthermore, it is proven that, unless TC\u00b0 = NC1, tensor attention Transformers, as well as RoPE-enhanced tensor attention Transformers with O(1) layers, poly(n)-precision, and a feature dimension d = O(n) are incapable of solving fixed membership problems or (AF,r)* closure problems. This finding underscores fundamental expressivity constraints inherent to tensor attention"}, {"title": "2 Related Work", "content": "The Computational Complexity in Deep Learning. Circuit complexity, a specialized domain within computational complexity theory, investigates the properties of circuit families as computational models. Numerous circuit complexity classes are relevant to the study of machine learning, with AC\u00ba characterizing problems solvable by highly parallel circuits utilizing elementary logic gates. The class TC\u00ba generalizes this concept by incorporating circuits that feature threshold gates, while NC\u00b9 encompasses problems solvable by circuits with a depth of O(log n) and bounded gate arity [MSS22]. It is well-established that AC\u00ba \u2282 TC\u00ba \u2286 NC\u00b9, although the question of whether TC\u00b0 \u2260 NC\u00b9 remains unresolved. Assuming this inequality holds, [LAG+22] demonstrates that, when simulating certain non-solvable semiautomata, the depth of Transformers must necessarily increase with the input sequence length. The circuit complexity is also used to measure some other popular architectures such us Mamba [CLL+24c] and Hopfield networks [LLL+24].\nComputation of Transformers. Transformers have undeniably revolutionized the field of natural language processing, yet their performance significantly deteriorates when tasked with mathematical computations [Cha22]. This observation has led to a surge in research aimed at identifying the computational boundaries of Transformer models, particularly in two distinct categories: (1) average-head attention Transformers, which assign a value of 1 to the highest probability in the vector while setting all other probabilities to 0, and (2) softmax-attention Transformers, which utilize the softmax function. Merrill, Sabharwal, and Smith [MSS22] demonstrate that average-head attention Transformers are capable of recognizing languages that surpass the computational power of the AC class, yet they remain simulable by threshold circuits with constant-depth, belonging to non-uniform TC\u00ba complexity class. In a similar vein, [LAG+22] establish that softmax-attention Transformers also belong to the non-uniform TC\u00ba class. Subsequent work by [MS23] builds upon these findings by introducing a similarity function to demonstrate that softmax-attention Transformers fall within the L-uniform TC\u00ba class. Further advancements by [MS24] employ first-order logic and MAJORITY quantifiers [Imm98] to show that DLOGTIME-uniform TC\u00ba circuits can simulate the behavior of these Transformers. In the context of practical applications, such as arithmetic operations and decision-making tasks, [FZG+24] establish that unless TC\u00b0 = NC\u00b9, Transformers with log-precision cannot efficiently solve arithmetic problems, equation-solving tasks, or"}, {"title": "3 Preliminary", "content": "This section establishes the essential concepts and definitions. Section 3.1 introduces the fundamental notations that form the basis of analysis. Section 3.2 provides an in-depth exploration of float point number computation. Section 3.3 offers a comprehensive overview of computational complexity classes. Then, Section 3.4 presents essential techniques employed in tensor operations. Finally, Section 3.5 explores the fundamental components that constitute the RoPE-based tensor attention Transformers."}, {"title": "3.1 Essential Notations", "content": "Let n represent any positive integer. The set of the first n natural numbers is denoted as [n] := {1,2,...,n}. The inner product of vectors \u03b1,\u03b2\u2208 R\u201d is given by\u3008\u03b1, \u03b2). The vector 1n is an n-dimensional vector, where each component is one. The lo norm of a matrix W\u2208 Rnxd is represented as ||W||\u221e := maxm\u2208[n],n\u2208[d] [Wm,n]. Finally, a binary string xi \u2208 {0,1}* denotes a sequence of arbitrary length."}, {"title": "3.2 Float Point Operations", "content": "We present basic concepts of the computational foundation. Initially, the exact definitions of float point numbers and the corresponding operations are outlined, which are indispensable in efficient tensor attention computations."}, {"title": "Definition 3.1", "content": "(Float point number, Definition 9 from [Chi24]). Any p-bit float point number is characterized by a pair (r,k), both r and k are integer values. Specifically, the significand of r lies within the range (\u22122P, -2P-1] \u222a {0} U [2p\u22121, 2P), while the exponent k is constrained to the interval [-2P,2P). The product r\u00b72k is the real value corresponding to the float point number (r,k). The collection of all possible p-bit float point numbers is represented by Fp."}, {"title": "Definition 3.2", "content": "(Rounding, Definition 9 from [Chi24]). Given any real number or float point value x, the notation round(x) denotes the p-bit float point number closest to x. In cases where we have different numbers equidistant from x, the tie-breaking convention dictates that roundp(x) will be the even significand one."}, {"title": "Definition 3.3", "content": "(Float point operations, Lemma 10 from [Chi24]). Let x and y represent two integers, then x y defined as follows:\n$x \\oslash y:=\\begin{cases}1/8+ x/y &\\text{if } x/y \\text{ is not a multiple of 1/4,} \\\\/&\\text{if } x/y \\text{ is a multiple of 1/4.}\\\\end{cases}$ \nLet (r1, k1) and (r2,k2) all denoted as p-bit float points, then we have:\n\u2022 Addition:\n$(r_1, k_1)+(r_2, k_2):=\\begin{cases} round_p((r_1 +r_22^{k_1-k_2}, k_1))&\\text{if } k_1 \\geq k_2, \\\\round_p((r_12^{k_2-k_1} + r_2, k_2))&\\text{if } k_1 <k_2.\\end{cases}$ \n\u2022 Comparison:\n$(r_1, k_1) \\leq (r_2,k_2) \\Leftrightarrow\\begin{cases}r_1 \\leq r_22^{k_1-k_2}&\\text{if } k_1 \\geq k_2, \\\\r_12^{k_2-k_1} \\leq r_2&\\text{if } k_1 \\leq k_2.\\end{cases}$ \n\u2022 Multiplication:\n$(r_1, k_1) \\times (r_2, k_2) := round_p((r_1r_2, k_1 + k_2)).$ \n\u2022 Division:\n$(r_1, k_1)\\div(r_2, k_2) := round_p((r_12^{p-1} \\oslash r_2, k_1 - k_2 - p + 1)).$ \n\u2022 Floor:\n$[<r, k)]:=\\begin{cases}round_p((r/2^{-k}, 0))&\\text{if } k < 0, \\\\(r2^k, 0)&\\text{if } k\\geq 0.\\end{cases}$ \nThe operations mentioned above are capable of efficient hardware implementation, as demonstrated by the following lemmas:"}, {"title": "Lemma 3.4", "content": "(Float point operations in TC\u00ba, Lemma 10 and Lemma 11 from [Chi24]). If integer 0 < p < poly(n), then we say the conditions below are satisfied:"}, {"title": "Corollary 3.5", "content": "(Floor operation in TC, Corollary 3.17 from [CLL+24a]). For any integer 0 < p \u2264 poly(n), a poly(n) size constant depth uniform threshold circuit is able to calculate the floor operation from Definition 3.3 on a p-bit float point number. The operation's maximum depth is bounded by dstd, as established in Lemma 3.4."}, {"title": "Lemma 3.6", "content": "(Computing exp in TC\u00ba, Lemma 12 from [Chi24]). For any integer 0 < p < poly(n) and any p-bit float point number x, it is computable to approximate most 2-P_relative error exp(x) using poly(n) size constant depth uniform threshold circuit. The depth required for this computation is denoted by dexp."}, {"title": "Lemma 3.7", "content": "(Computing square root in TC\u00ba, Lemma 12 from [Chi24]). Given an integer p such that 0 < p < poly(n) and a p-bit float point number x, a constant depth poly(n) size uniform threshold circuit exists to calculate \u221ax with a relative error bounded by 2-P. The depth required for this operation is represented by dsqrt."}, {"title": "3.3 Circuit Complexity", "content": "In computational theory, a Boolean circuit, constructed using basic gates such as AND, OR, and NOT, represents a core model of computation. A precise mathematical definition of this structure comes below."}, {"title": "Definition 3.8", "content": "(Boolean Circuit, Definition 6.1 from [AB09]). An n variables Boolean circuit is defined as Cn : {0,1}\" \u2192 {0,1} and is depicted by a directed acyclic graph (DAG). In this representation, logical gates such as AND, OR, and NOT correspond to the vertices of the graph. The input vertices, each linked to one of the n Boolean variables, have an in-degree of 0, whereas non-input vertices derive their values from the outputs of preceding gates in the structure.\""}, {"title": "Definition 3.9", "content": "(Languages, Definition 6.2 from [AB09]). A Boolean circuit family C is said to recognize language L \u2286 {0,1}* if a Boolean circuit C|z| \u2208 C with |z| variables exists, s.t., C|z|(z) = 1, iff z \u2208 L, for every string z \u2208 {0,1}*."}, {"title": "Definition 3.10", "content": "(NC\u00b2, Definition 6.21 from [AB09]). The class NC\u00b2 is defined as the set of languages that are recognizable using Boolean circuits of size O(poly(n)) and depth O((logn)\u00b2), with logical gates of bounded fan-in, including NOT, OR, and AND gates."}, {"title": "Definition 3.11", "content": "(AC\u00b2, Definition 6.22 from [AB09]). Languages which can be computed by the Boolean circuit of depth O((log n)\u00b2), size O(poly(n)), unbounded fan-in gates, including AND, OR, NOT, are contained in the class AC\u00b2."}, {"title": "Definition 3.12", "content": "(TC\u00b2, Definition 4.34 from [Vol99]). If we have languages are recognizable by O(poly(n)) size Boolean circuits of O((log n)') depth, and unbounded fan-in gates, including MAJORITY, NOT, OR, and AND gates. If half of the inputs are 1, the MAJORITY gate will output 1."}, {"title": "Remark 3.13", "content": "As Definition 3.12 shows, MOD or THRESHOLD gates (for prime moduli) can replace MAJORITY gates. Boolean circuits employing such gates are collectively referred to as threshold circuits."}, {"title": "Definition 3.14", "content": "(P, Definition 1.20 from [AB09]). A language is considered to be in P if it can be decided by a deterministic Turing machine within polynomial time of input size."}, {"title": "Fact 3.15", "content": "(Corollary, Corollary 4.35 from [Vol99]). Any i \u2208 N, the following inclusions are valid:\n$NC^i \\subseteq AC^i \\subseteq TC^i \\subset NC^{i+1} \\subset P$."}, {"title": "Remark 3.16", "content": "If i = 0, it has been established NC\u00ba \u2286 AC\u00ba \u2286 TC\u00ba. However, it remains unresolved whether TCO \u2286 NC\u00b9. Moreover, the question of whether $NC := \\bigcup_{i \\in N}NC^i \\subseteq P$ is an open problem."}, {"title": "Definition 3.17", "content": "(L-uniformity class, Definition 6.5 from [AB09]). Denote C as a class of languages represented by circuit family C (such as, NC\u00b2, AC\u00b2, or TC\u00b2). A language L \u2286 {0,1}* is classified as belonging to the L-uniform class of C if existing a Turing machine can map 1m to C class circuit with n variables in O(logn) space, for each n \u2208 N, and the resulting circuit Cn recognizes L."}, {"title": "Definition 3.18", "content": "(DLOGTIME-uniformity, Definition 4.28 from [BI94]). Let C be a class of languages represented by circuit family C (such as NC\u00b2, AC\u00b2, or TC\u00b2). A language L \u2286 {0,1}* is defined to belong to the DLOGTIME-uniform class of C if a random-access Turing machine can map 1\u2033 to n variables circuit Cn in C within O(logn) time, for every n \u2208 N, such that Cn recognizes L."}, {"title": "Remark 3.19", "content": "The concept of DLOGTIME-uniformity aligns with that of L-uniformity, except in smaller circuit classes that do not have the capability to imitate the constructing machine. Further exploration of uniformity concepts can be found in [BI94, HAB02]. Within this paper, references to uniform TC\u00ba pertain specifically to DLOGTIME-uniform TC\u00ba."}, {"title": "3.4 Tensor Operation Analysis Techniques", "content": "We first define operations such as the Kronecker product, a matrix operation that takes two matrices of any size and produces a block matrix. Unlike standard matrix multiplication, it is useful for introducing and analyzing tensor attention. Then, we introduce some key techniques for applying tensor attention to ROPE."}, {"title": "Definition 3.20", "content": "((\u25ca Kronecker product). Given K1 \u2208 Rn1\u00d7d1 and K2 \u2208 Rn2\u00d7d2, let K := K1\u00aeK2 \u2208 Rn1n2\u00d7d1d2 be defined for any i\u2081 \u2208 [n1], j1 \u2208 [d1] and i2 \u2208 [n2], j2 \u2208 [d2] as\n$K_{i_1+(i_2-1)n_1,j_1+(j_2-1)d_1} = (K_1)_{i_1,j_1}. (K_2)_{i_2,j_2}.$"}, {"title": "Definition 3.21", "content": "(( column-wise Kronecker product). Given matrices K\u2081 \u2208 Rn1\u00d7d, K2 \u2208 Rn2\u00d7d, we define matrix K := K1 \u00d8 K2 \u2208 Rn1n2\u00d7d as follows\n$K_{i_1+(i_2-1)n_1,j}:= (K_1)_{i_1,j} \\cdot (K_2)_{i_2,j}, \\forall i_1 \\in [n_1], i_2 \\in [n_2], j\\in [d].$"}, {"title": "Definition 3.22", "content": "((\u2295 row-wise Kronecker product). Given matrices K1 \u2208 Rn\u00d7d1, K2 \u2208 Rn\u00d7d2, we define matrix K := K\u2081 \u04e8 K2 \u2208 Rn\u00d7d1d2 as follows\n$K_{i,j_1+(j_2-1)d_1}:= (K_1)_{i,j_1} \\cdot (K_2)_{i,j_2}, \\forall i \\in [n], j_1 \\in [d_1], j_2 \\in [d_2].$"}, {"title": "Fact 3.23", "content": "indicates that the order of tensor operation and matrix multiplication can be swapped, enabling computation in the lower dimension first to reduce complexity."}, {"title": "Fact 3.23", "content": "(Swap rule for tensor and matrix product). Let W1,W2 \u2208 Rdxd, A1, A2 \u2208 Rnxd. We have\n$(A_1 \\otimes A_2) (W_1 \\otimes W_2) = (A_1 \\cdot W_1) \\otimes (A_2 \\cdot W_2) $. \nProof. For any i1, i2 \u2208 [n], j \u2208 [d], we have\n$((A_1 \\otimes A_2) (W_1 \\otimes W_2))_{i_1+(i_2-1)n,j}$\n$=\\sum_{k_1\\in [d],k_2\\in[d]} (A_1 \\otimes A_2)_{i_1+(i_2-1)n,k_1+(k_2-1)d}(W_1 \\otimes W_2)_{k_1+(k_2-1)d,j}$\n$=\\sum_{k_1\\in [d],k_2\\in[d]} (A_1 \\otimes A_2)_{i_1+(i_2-1)n,k_1+(k_2-1)d}\\cdot (W_1)_{k_1,j} \\cdot (W_2)_{k_2,j}$\n$=\\sum_{k_1\\in[d],k_2\\in[d]} (A_1)_{i_1,k_1}. (A_2)_{i_2,k_2}. (W_1)_{k_1,j}. (W_2)_{k_2,j}$\n$=(\\sum_{k_1\\in [d]}(A_1)_{i_1,k_1} (W_1)_{k_1,j}). (\\sum_{k_2E[d]}(A_2)_{i_2,k_2}.(W_2)_{k_2,j})$\n$=(A_1W_1)_{i_1,j}. (A_2W_2)_{i_2,j}$\n$= ((A_1W_1) \\otimes (A_2W_2))_{i_1+(i_2-1)n,j}$,\nwhere the initial step involves the application of matrix multiplication, followed by the utilization of Definition 3.21 in the second step. Subsequently, the third step employs Definition 3.20, while the fourth step simplifies the expression through fundamental algebraic principles. The fifth step re-engages matrix multiplication, and the concluding step leverages Definition 3.21 once more."}, {"title": "3.5 Transformer Block", "content": "With the mathematical foundation in place, this section outlines the key components of the RoPE-based tensor attention Transformers architecture, starting with the softmax operation, a fundamental element of Transformer."}, {"title": "Definition 3.24", "content": "(Softmax function). Noted z \u2208 Fr. The Softmax function : Fn \u2192 Fn is formally given by:\n$Softmax(z) := \\frac{exp(z)}{(exp(z), 1_n)}$ \nOne of the pivotal advancements in contemporary Transformer architectures is RoPE, which employs a rotation matrix as its foundation:"}, {"title": "Definition 3.25", "content": "(Rotation matrix block). For an input sequence of length n, embedding dimension d, and parameter 0 \u2208 Fp, the rotation matrix is constructed as follows:\n$R(\\theta) :=\\begin{bmatrix}cos \\theta&-sin \\theta \\\\sin \\theta&cos \\theta\\end{bmatrix}$ \nThis fundamental rotation matrix is generalized to encode the relative positions within a sequence, facilitating the embedding of positional context."}, {"title": "Definition 3.26", "content": "(Rotation matrix). Noted j represents position index within input sequence and i denotes token index. The relative rotation matrix is then expressed as:\n$R_{j-i} =\\begin{bmatrix}R((j - i)\\theta_1)&0&...&0\\\\0&R((j - i)\\theta_2)&...&0 \\\\vdots&\\vdots&\\vdots&\\vdots\\\\0&0&...&R((j - i)\\theta_{d/2})\\end{bmatrix}$,\nwhere the angular frequencies 01,\u2026\u2026,0d/2 are all predefined. More about selecting 0, consult Equation (15) from [SAL+24]."}, {"title": "Definition 3.27", "content": "(Input and weight matrix). We define the input sequence as X \u2208 Rn\u00d7d and the key, query, and value weight matrix as WK1, WK2,WQ, Wv1, Wv2 \u2208 Rdxd. Then, we define the key, query, and value matrix as K\u2081 := XWK\u2081 \u2208 Rnxd, K2 := XWK2 \u2208 Rnxd, Q := XWQ \u2208 Rnxd, V\u2081 := XWv\u2081 \u2208 Rn\u00d7d, V2 := XWv\u2082 \u2208 Rnxd.\nThen, based on Definition 3.21, we define RoPE-based tensor attention matrix in the following way."}, {"title": "Definition 3.28", "content": "(RoPE-based tensor attention). As we defined in Definition 3.26 and 3.27. We compute the new attention matrix A \u2208 Fnxn\u00b2 by,\n$A_{j_1,j_2+(j_3-1)d} := (exp(Q_{j_1,*} R_{j_1,j_2+(j_3-1)d} (K_{*},j_2+(j_3-1)d) /d))_{j_1,j_2+(j_3-1)d}$\nwhere $R_{j_1,j_2+(j_3-1)d} = R_{j_1-j_2} \\otimes R_{j_1-j_3} \\in F^{n \\times n}$, $K = K_1 \\otimes K_2 \\in F^{n^2 \\times d^2}.$"}, {"title": "Definition 3.29", "content": "(Single RoPE-based tensor attention layer, Definition 7 in [SHT24], Definition 1.1 in [AS24b], Definition 3.8 in [LSSZ24b]). Given input matrices Q, K1, K2, V1, V2 \u2208 Fnxd, R\u2208 Fdxd, as Definition 3.28, we compute the i-th RoPE-based tensor attention layer Attni as\n$Attn(X) := \\bigoplus_{i}^{} ((Q \\otimes_{c} R(K)) \\otimes (W_{V_1}\\otimes W_{V_2}))$\nby applying Fact 3.23, we finally define the i-th RoPE tensor attention layer Attni as\n$Attn(X) := D^{-1}AV$\nwhere $D := diag(A \\oslash 1_{n^2}) \\in F^{n \\times n}$, and $V = V_1 \\otimes V_2 \\in F^{n^2 \\times d}$"}, {"title": "Definition 3.30", "content": "(Single tensor attention layer, Definition 7 in [SHT24], Definition 1.1 in [AS24b], Definition 3.5 in [LSSZ24b]). Given input matrices Q, K1, K2, V1, V2 \u2208 Fnxd, compute the following matrix\n$Attn(X) := D^{-1} AV$.\nwhere (1) $A := exp(Q(K^T/d)) \\in F^{n \\times n}$, and $K := K_1 \\otimes K_2 \\in F^{n \\times d^2}$ (2) $D : diag(A \\oslash 1_{n^2}) \\in F^{n \\times n}$, and (3) $V := V_1 \\otimes V_2 \\in F^{n^2 \\times d}$."}, {"title": "Definition 3.31", "content": "(Multiple layer tensor attention Transformer). The number of Transformer's layers is denoted by m. In the i-th Transformer layer, let gi signify components distinct from self-attention, where gi : Fnxd \u2192 Fnxd, each i \u2208 [m]. And Attni represent i-th layer attention mechanism(as defined in Definition 3.29 and Definition 3.30). Given an input data matrix X \u2208 Fnxd, an m-layer Transformer TF: Fnxd \u2192 Fnxd is formally defined as:\n$TF(X) := g_m \\circ Attn_m \\circ \\cdot\\cdot\\cdot \\circ g_1 \\circ Attn_1\\circ g_0(X) \\in F^{n \\times d}$,\nwhere o denotes the composition of functions."}, {"title": "Definition 3.32", "content": "(Layer normalization). Let X \u2208 Fnxd be the input data matrix, and let i \u2208 [n]. The LN layer is formulated as:\n$g^{LN}(X)_{i,*} := \\frac{X_{i,*} - \\mu_i}{\\sqrt{\\sigma_i^2}},$\nwhere $\\mu_i := \\frac{\\sum_{j=1}^{d}X_{i,j}}{d}$, and $\\sigma_i^2 := \\frac{\\sum_{j=1}^{d} (X_{i,j}- \\mu_i)^2}{d}$."}, {"title": "Definition 3.33", "content": "(Multilayer perceptron). Let X \u2208 Fnxd be the input data matrix, and let i \u2208 [n]. The MLP layer is described as:\n$g^{MLP}(X)_{i,*} := WX_{i,*}+b$\n\nThe foundation of modern Transformer is built upon these layered architectures, which integrate float point computations, attention, and rotation matrix to an exceptionally efficient framework for sequential computation."}, {"title": "4 Complexity of Tensor Attention Transformer", "content": "We now formally turn our attention to investigating the circuit complexity of the tensor attention layer and the multi-layer tensor attention Transformer, emphasizing their computability within the complexity class TC. Section 4.1 delves into matrix operations. Section 4.2 addresses the computation of a single tensor attention layer. Section 4.3 provides an in-depth examination of the entire tensor attention mechanism. Lastly, Section 4.4 presents our principal findings regarding the circuit complexity bounds for the tensor attention Transformer. These results establish the foundation for the main theorem concerning Transformer expressiveness."}, {"title": "4.1 Matrix Operations", "content": "We demonstrate that fundamental matrix multiplication is efficiently evaluatable within TC\u00ba."}, {"title": "Lemma 4.1", "content": "(Matrix multiplication in TC, Lemma 4.2 from [CLL+24a]). Let A \u2208 Fn1xd and B\u2208 Fdxn2 represent matrices. Under the conditions that p \u2264 poly(n), n1, n2 \u2264 poly(n), and d \u2264 n, the product AB is evaluatable via poly(n) size uniform threshold circuit with (dstd + d) depth."}, {"title": "Lemma 4.2", "content": "(Kronecker product in TC\u00b0). Let A \u2208 Fn1xd and B\u2208 Fdxn2 represent matrices. If p \u2264 poly(n), n1, n2 \u2264 poly(n), and d \u2264 n, the Kronecker product A \u25ca B can be evaluated by a poly(n) size uniform threshold circuit with dstd depth."}, {"title": "Lemma 4.3", "content": "(Column-wise Kronecker Product in TC\u00ba). Let matrices A \u2208 Fm1xd and B\u2208 Fn2xd be given. If p < poly(n), n1, n2 \u2264 poly(n), and d \u2264 n, then the column-wise Kronecker product A\u25ca B is evaluatable by a poly(n) size uniform threshold circuit with depth dstd."}, {"title": "Lemma 4.4", "content": "(Row-wise Kronecker Product Computation in TC\u00ba). Let A \u2208 Fd\u00d7n\u0131 and B \u2208 Fd\u00d7n2 be matrices, with the conditions p \u2264 poly(n), n1, n2 \u2264 poly(n), and d \u2264 n. Then, a size poly(n) uniform threshold circuit with dstd depth can calculate the row-wise Kronecker product A\u2295 B."}, {"title": "4.2 Single Tensor Attention Layer", "content": "Here, we examine the complexity of the single layer of the tensor attention."}, {"title": "Lemma 4.5", "content": "(Complexity of Single Tensor Attention Layer in TC\u00b0). When p < poly(n), the attention Attn in Definition 3.30, is evaluatable by a poly(n) size and 5dstd + 5d\u2295 + dexp depth uniform threshold circuit."}, {"title": "4.3 Multi-layer Tensor Attention", "content": "This section analyzes the computation of multi-layer tensor attention in a Transformer."}, {"title": "Lemma 4.6", "content": "(Computation of Multi-layer Tensor Attention Transformer in TC\u00ba). Suppose that for every i \u2208 [m"}]}