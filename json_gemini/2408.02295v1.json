{"title": "Generalized Gaussian Temporal Difference Error For Uncertainty-aware Reinforcement Learning", "authors": ["Seyeon Kim", "Joonhun Lee", "Namhoon Cho", "Sungjun Han", "Seungeon Baek"], "abstract": "Conventional uncertainty-aware temporal difference (TD) learning methods often rely on simplistic assumptions, typically including a zero-mean Gaussian distribution for TD errors. Such oversimplification can lead to inaccurate error representations and compromised uncertainty estimation. In this paper, we introduce a novel framework for generalized Gaussian error modeling in deep reinforcement learning, applicable to both discrete and continuous control settings. Our framework enhances the flexibility of error distribution modeling by incorporating higher-order moments, particularly kurtosis, thereby improving the estimation and mitigation of data-dependent noise, i.e., aleatoric uncertainty. We examine the influence of the shape parameter of the generalized Gaussian distribution (GGD) on aleatoric uncertainty and provide a closed-form expression that demonstrates an inverse relationship between uncertainty and the shape parameter. Additionally, we propose a theoretically grounded weighting scheme to fully leverage the GGD. To address epistemic uncertainty, we enhance the batch inverse variance weighting by incorporating bias reduction and kurtosis considerations, resulting in improved robustness. Extensive experimental evaluations using policy gradient algorithms demonstrate the consistent efficacy of our method, showcasing significant performance improvements.", "sections": [{"title": "1 Introduction", "content": "Deep reinforcement learning (RL) has demonstrated promising potential across various real-world applications, e.g., finance [7, 45, 56], and autonomous driving [17, 30, 27]. One critical avenue for improving the performance and robustness of RL agents in these complex, high-dimensional environments is the quantification and integration of uncertainty associated with the decisions made by agents or the environment [36]. Effective management of uncertainty promotes the agents to make more informed decisions leading to enhanced sample efficiency in RL context, which is particularly beneficial in unseen or ambiguous situations.\nTemporal difference (TD) learning is a fundamental component of many RL algorithms, facilitating value function estimation and policy derivation through iterative updates [57]. Traditionally, these TD updates are typically grounded in \\(L_2\\) loss, corresponding to maximum likelihood estimation (MLE) under the assumption of Gaussian error. Such simplification may be overly restrictive, especially considering the noisy nature of TD errors, which are based on constantly changing estimates of value functions and policies. This assumption compromises sample efficiency, necessitating the incorporation of additional distributional parameters for flexible but computationally efficient TD error modeling.\nIn statistics and probability theory, distributions are typically characterized by their central tendency, variability, and shape [12, 41]. Traditional deep RL methods, however, effectively exploit on the variance of the error distribution through the scale parameter, yet they often disregard its shape. This oversight hinders these methods from fully capturing the true underlying uncertainty. The kurtosis, the scale-independent moment, does significantly influence both inferential and descriptive statistics [2], and the reliability of uncertainty estimation.\nTherefore, it is essential to incorporate the shape of the error distribution into TD learning to better reflect uncertainties present in RL environments, enabling more robust and reliable decision-making processes in dynamic and complex scenarios.\nA notable enhancement to the normality hypothesis is the use of the generalized Gaussian distribution (GGD), also known as the generalized error distribution or exponential power distribution. This flexible family of symmetric distributions, as depicted in Figure 1, encompasses a wide range of classical distributions, including Gaussian, Laplacian, and uniform distributions, all adjustable via a shape parameter [5]. This specific parameter allows for fine-tuning the distribution to match the characteristics of TD error distribution, providing a more reliable representation of uncertainty.\nOur primary contribution is the introduction of a novel framework of generalized Gaussian error modeling in deep RL, enabling robust training methodologies by incorporating the distribution's shape. This approach addresses both data-dependent noise, i.e., aleatoric uncertainty, and the variance of model estimates, i.e., epistemic uncertainty, ultimately enhancing model stability and performance.\nThe key contributions of our work are as follows:\n1. Empirical investigations (Section 3.1.1): We conduct empirical investigations of TD error distributions, revealing substantial deviations from the Gaussian distribution, particularly in terms of tailedness. These findings underscore the limitations of conventional Gaussian assumptions.\n2. Theoretical exploration (Section 3.1.2): Building on empirical insights, we explore the theoretical suitability of modeling TD errors with a GGD. Theorem 1 demonstrates the effectiveness and well-definedness of the proposed method under leptokurtic error distributions, characterized by \\(\\beta \\in (0,2]\\). Our experimental results suggest that the estimates of \\(\\beta\\) mostly converge within this range, aligning with the empirical findings.\n3. Aleatoric uncertainty mitigation (Section 3.1): We investigate the implications of the distribution shape on the estimation and mitigation of aleatoric uncertainty. GGD error modeling enables the quantification of aleatoric uncertainty in a closed form, indicating a negative relation to the shape parameter \\(\\beta\\) on an exponential scale, with a constant scale parameter \\(a\\). We also leverage the second-order stochastic dominance of GGD to weight error terms proportional to \\(\\beta\\), enhancing the model's robustness to heteroscedastic aleatoric noise by focusing on less spread-out samples.\n4. Epistemic uncertainty mitigation (Section 3.2): We introduce the batch inverse error variance weighting scheme, adapted from the batch inverse variance scheme [38], to account for both variance and kurtosis of the estimation error distribution. This scheme down-weights high-variance samples to prevent noisy data and improves model robustness by focusing on reliable error estimates.\n5. Experimental evaluations (Section 4): We conduct extensive experimental evaluations using policy gradient algorithms, demonstrating the consistent efficacy of our method and significant performance enhancements."}, {"title": "2 Background", "content": "We consider a Markov decision process (MDP) governed by state transition probability \\(P(s_{t+1}|s_t, a_t)\\), with \\(s_t \\in S\\) and \\(a_t \\in A\\) represent the state and action at step \\(t\\), respectively [58]. Within this"}, {"title": "2.1 Uncertainty", "content": "framework, an agent interacts with the environment via a policy \\(\\pi(a_t|s_t)\\), leading to the acquisition of rewards \\(r(s_t, a_t) \\sim R(s_t, a_t)\\).\nNumerous model-free deep RL algorithms leverage TD updates for value function approximation [21, 26, 42, 43, 52]. In these methods, neural networks parameterized by \\(\\theta\\) are trained to approximate the state-action value \\(Q(s_t, a_t)\\) by minimizing the error between the target and predicted value:\n\\[\\delta(s_t, a_t; \\theta) = T(s_t, a_t; \\theta) \u2013 Q(s_t, a_t; \\theta),\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(1)\\]\nwhere the target is computed according to Bellman's equation:\n\\[T(s_t, a_t; \\theta) = r(s_t, a_t) + \\gamma Q(s_{t+1}, a_{t+1}; \\theta).\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(2)\\]\nTypically, TD updates involve minimizing the mean squared error (MSE) loss \\(L_e = E[(T(s_t, a_t; \\theta) \u2013 Q(s_t, a_t; \\theta))^2]\\) through stochastic gradient descent. This minimization implicitly presumes that errors conform to a Gaussian distribution with zero-mean, consistent with the principles of MLE.\nFor clarity, we henceforth omit the learnable parameter and adopt subscript notation for function arguments, e.g., \\(\\delta_t = T_t - Q_t\\).\nUncertainty in neural networks is commonly decomposed into two sources: aleatoric and epistemic [13, 14, 31, 63]. Epistemic uncertainty arises from limitations within the neural network and can potentially be reduced through further learning or model improvements. In contrast, aleatoric uncertainty stems from the inherent stochasticity of the environment or the dynamics of the agent-environment interactions and is fundamentally irreducible.\nThis distinction is crucial in the context of RL, where areas with high epistemic uncertainty need to be explored, whereas exploring areas with high aleatoric uncertainty may lead to ineffective training, since the agent might have adequate knowledge but insufficient information for decisive actions. Quantifying aleatoric uncertainty is known to facilitate learning dynamics of stochastic processes and enables risk-sensitive decision making [10, 54, 65].\nTo address aleatoric uncertainty, variance networks, denoted by \\(Q^\\sigma\\) to distinguish them from the value approximation head \\(Q\\), are frequently employed [3, 31, 33, 38, 40, 47]. The estimated variance from \\(Q^\\sigma\\) is utilized for heteroscedastic Gaussian error modeling, with \\(\\delta_t \\sim N(Q_t, Q_t^\\sigma{}^2)\\), thereby quantifying aleatoric uncertainty [54]. This network is trained by maximizing the log likelihood function of the Gaussian distribution or minimizing the Gaussian negative log likelihood (NLL) loss. The variance of estimated values is also used for batch inverse variance (BIV) weighting in inverse-variance RL [38]. This regularization effectively captures epistemic uncertainty with a regularizing temperature \\(\\lambda\\):\n\\[\\begin{aligned}\\mathcal{L} &= \\mathcal{L}_{GD-NLL} + \\lambda L_{BIV} \\\\&= \\sum_t \\underbrace{\\left(\\frac{(\\delta_t)^2}{(Q_t^\\sigma)^2} + \\log Q_t^\\sigma{}^2\\right)}_{L_{GD-NLL}} + \\lambda \\underbrace{\\left(\\sum_t w_t^{BIV} \\delta_t^2\\right)}_{L_{BIV}}\\\\end{aligned}\\quad\\quad\\quad\\quad\\quad(3)\\]\nwhere the BIV weight \\(w_t^{BIV} = 1/(\\gamma^2V[Q_t] + \\xi)\\) with empirical variance \\(V\\), and \\(\\xi\\) is either a hyperparameter or numerically computed to ensure a sufficient effective batch size. In the official implementation, the sample variance with Bessel's correction is used as a variance estimator, which is particularly apparent given a small sample size, i.e., an ensemble number of five."}, {"title": "2.2 Tailedness", "content": "While mainstream machine learning literature often prioritizes on capturing central tendencies, the significance of extreme events residing in the tails for enhancing performance and gaining a deeper understanding of learning dynamics cannot be overlooked. This is especially relevant for MLE base on the normality assumption, which is commonly applied in variance network frameworks. Focusing solely on averages or even deviations is proven to be inadequate in the presence of outlier samples [11, 23].\nFor instance, consider the impact of non-normal samples on the estimate of the variance, as described in Proposition 1 with proof presented in Appendix B.1."}, {"title": "3 Methods", "content": "Our approach introduces enhancements to the loss function derived from (3), incorporateing tailedness into both loss attenuation and regularization terms:\n\\[\\begin{aligned}\\mathcal{L} &= \\mathcal{L}_{GGD-NLL} + \\lambda L_{BIEV} \\\\&= \\sum_t \\underbrace{\\left(\\left(\\frac{|\\delta_t|}{Q_t^\\alpha}\\right)^{\\beta} \u2013 \\log Q_t^\\alpha Q_t^\\beta + \\log \\Gamma(1/\\beta)\\right)}_{L_{GGD-NLL}} + \\lambda \\underbrace{\\left(\\sum_t w_t^{BIEV} \\delta_t^2\\right)}_{L_{BIEV}}\\\\end{aligned}\\quad\\quad\\quad\\quad(4)\\]\nwhere \\(Q^\\alpha\\) and \\(Q^\\beta\\) represent the alpha and beta networks, respectively. Here, \\(w_t^{RA} = Q_t^\\beta\\), and \\(w_t^{BIEV} = 1/(\\sqrt{V[\\delta_t]} + \\xi)\\). These dual loss terms effectively capture aleatoric and epistemic uncertainties, respectively. The subsequent subsections provide a detailed rationale for this modification."}, {"title": "3.1 Generalized Gaussian error modeling", "content": "One simple yet promising approach to address non-normal heteroscedastic error distributions involves modeling the per-error distribution using a zero-mean symmetric GGD [9, 24, 61, 69]:\n\\[\\delta \\sim GGD(0, \\alpha, \\beta) = \\frac{\\beta}{2\\alpha \\Gamma(1/\\beta)}e^{-(|\\delta|/\\alpha)^\\beta},\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(5)\\]\nwhere \\(\\alpha\\) and \\(\\beta\\) represent the scale and shape parameter, respectively. This method not only allows for modeling each non-identical error by parameterizing the GGD with different \\(\\alpha_t\\) and \\(\\beta_t\\) at step \\(t\\), but also offers a flexible parametric form that adapts across a spectrum of classical distributions from Gaussian to uniform as \\(\\beta\\) increases to infinity [16, 25, 46, 48].\nThe shape parameter \\(\\beta\\) serves as a crucial structure characteristic, distinguishing underlying mechanisms. The kurtosis \\(\\kappa\\), commonly used to discern different distribution shapes, is solely a function of \\(\\beta\\) and is defined as Pearson's kurtosis minus three to emphasize the difference from Gaussian distribution [12]:\n\\[\\kappa = \\frac{\\Gamma(5/\\beta)\\Gamma(1/\\beta)}{\\Gamma(3/\\beta)^2} - 3.\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(6)\\]\nThis implies that distributions with \\(\\beta < 2\\) are leptokurtic, i.e., \\(\\kappa > 0\\), indicating a higher frequency of outlier errors compared to the normal error distribution. With only one additional parameter to"}, {"title": "3.1.1 Empirical evidence", "content": "Figure 2 presents empirical findings that reveal a departure from Gaussian distribution in TD errors, evidenced by well-fitted GGDs and pronounced differences in the shape of distributions. This non-normality is particularly notable when contrasting initial and final evaluations, suggesting an escalating significance of tailedness throughout the training process. We hypothesize that such divergence from normality stems from the exploratory nature of agent behavior. During exploration, agents often encounter unexpected states and rewards, leading to a broader spectrum of TD errors than those seen in purely exploitative scenarios. Consequently, increased variability contributes to the emergence of non-normal distributions, notably with heavier tails.\nFurthermore, the observed evolution in TD error tails underscores the shifting interplay between aleatoric and epistemic uncertainties. As training progresses, the influence of aleatoric errors increases as epistemic uncertainty diminishes, potentially resulting in non-normally distributed errors with heavier tails."}, {"title": "3.1.2 Theoretical analysis", "content": "Given the empirical suitability of GGD for modeling TD errors, we conduct an in-depth theoretical examination of GGD. We begin by demonstrating the well-definedness of GGD regression, with a specific emphasis on the positive-definiteness of its PDF under certain conditions."}, {"title": "3.2 Batch inverse error variance regularization", "content": "When estimating the uncertainty of target estimates, as employed in BIV weighting, potential bias can arise [29, 35]. Conversely, the bias of TD errors remains notably small with the assumption of constant value approximation bias [20]. Motivated by this, we propose the batch inverse error variance (BIEV) weighting:\n\\[w_t^{BIEV} = \\frac{1}{V[\\delta_t]},\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(8)\\]\nincorporating the concept of error variance explicitly into BIV weighting.\nRecent investigations have explored advancements in variance estimation, particularly through a constant multiplier, i.e., \\(s^2 = w(n \u2212 1)s^2\\) [32, 67], where \\(s^2\\) denotes sample variance, the MLE estimator of variance. Although non-standard weights \\(w \u2260 1/(n \u2212 1)\\) may introduce bias in variance estimation, the estimation of inverse variance remains biased due to Jensen's inequality, even with the use of the unbiased estimator \\(s^2\\) [66]. Consequently, our focus shifts to relative efficiency (RE), where we derive the MSE-best biased estimator (MBBE) in Proposition 2."}, {"title": "4 Experiments", "content": "We conduct a comprehensive evaluation of our proposed method across well-established benchmarks, including MuJoCo [59], and discrete control environments from Gymnasium [60]. Notably, we augment the discrete control environments through the introduction of supplementary uniform action noise to enhance environmental fidelity.\nTo underscore the versatility and robustness of our approach, we deliberately choose baseline algorithms that cover a wide spectrum of RL paradigms. Specifically, we employ soft actor-critic (SAC) [26], an off-policy Q-based policy gradient algorithm, and proximal policy optimization (PPO) [52], an on-policy V-based method. We focus on adversaries limited to variance networks due to the use of separate target networks in previously mentioned Gumbel error modeling methods. This constraint is intended to focus on computationally efficient algorithms that only incorporate an additional layer, referred to as a head.\nThe algorithms are implemented using PyTorch [50], within the Stable-Baselines3 framework [51]. We use default configurations from Stable-Baselines3, with adaptations limited to newly introduced hyperparameters. For both PPO and SAC, along with their variants, we employ five ensembled critics. The parameter \\(\\xi\\) from (4) is computed with a minimum batch size of 16, and the regularizing temperature \\(\\lambda\\) is set to 0.1. Additional experimental details are provided in Appendix C.\nThe performance of SAC across different MuJoCo environments is presented in Figure 3. While the variance head degrades performance in certain scenarios, SAC variants employing the beta head consistently lead to better sample efficiency and asymptotic performance. Notable improvements are observed in the HalfCheetah-v4 and Hopper-v4 environments, where the variance head substantially reduces sample efficiency. This suggests that the TD error distributions in these environments may exhibit heavy tails. The impact of BIEV regularization varies by environment but generally performs at least as well as BIV regularization.\nFigure 4 shows the coefficients of variation, defined as the ratio of the standard deviation to the mean, i.e., \\(\\sqrt{V[X]}/E[X]\\), for the variance and \\(\\beta\\) estimates. This statistic demonstrates the scale-invariant volatility of the parameter estimation, given that the scale of the estimated variance is significantly larger than that of the \\(\\beta\\) estimates. A lower coefficient of variation in \\(\\beta\\) estimation indicates greater stability compared to variance estimation.\nThe convergence of \\(\\beta\\) estimation is also more stable than variance estimation. This observation aligns with Remark 1, suggesting a potential underestimation of confidence intervals in variance estima-tion. Considering the susceptibility of variance estimates to extreme values, such underestimation introduces considerable uncertainty in parameter estimation. Our hypothesis regarding the escalating impact of extreme TD errors throughout training is consistent with this findings, as it exacerbates the challenge in variance estimation, leading to volatility or divergence of the variance head."}, {"title": "5 Discussion", "content": "In this paper, we advocate for and substantiate the integration of GGD modeling for TD error analysis. Our main contribution is the introduction of a novel framework that enables robust training methodologies by leveraging the distribution's shape. This approach accounts for both data-dependent noise, i.e., aleatoric uncertainty, and the uncertainty of value estimation, i.e., epistemic uncertainty, ultimately enhancing the model's stability and accuracy.\nFurther investigation An imperative avenue for further investigation is the application of GGD within the context of maximum entropy RL. Similar to how the Gaussian distribution maximizes entropy with constraints up to the second moment, the GGD maximizes entropy subject to a constraint on the p-th absolute moment [37]. Exploring higher moments of the distribution could provide new insights into maximum entropy RL frameworks.\nThe absence of a comprehensive regret analysis in our current study also presents an opportunity for future work. Considering that aleatoric uncertainty in TD learning predominantly arises from reward dynamics, conducting a regret analysis on heavy-tailed TD error is warranted. This is particularly relevant as previous research has extensively studied regret in the context of heavy-tailed rewards.\nWhile our experiments focus on policy gradient methods, the implications of TD error tailedness extend to the Q-learning family of algorithms. We provide empirical findings on this extension in Appendix E, which demonstrate the broader applicability of our approach. Additionally, exploring the generalized extreme value distribution could further enrich our understanding of tailedness phenomena due to its close association with extreme value theory.\nRelevant applications The implications of tailedness in TD error distributions extend into various domains, notably robust RL and risk-sensitive RL. The focus in robust RL lies on developing algorithms that are less sensitive to noise and outliers within the reward signal. Recognizing potential deviations from normality in TD error distributions is critical for designing such algorithms. Our work emphasizes the importance of considering non-normal error distributions, especially the tail behaviors, to enhance the robustness of RL algorithms.\nAnother significant direction is risk-sensitive RL, which seeks to assess and mitigate the risks associated with different action choices. In noisy and outlier-prone environments, capturing the risk profile using a Gaussian assumption for TD errors might be inadequate. By considering the GGD, which better models the tail behaviors of error distributions, we can develop more accurate and reliable risk-sensitive RL algorithms.\nIn summary, our exploration into GGD modeling of TD errors opens several promising research directions and applications, emphasizing the need to consider non-normal error distributions for enhancing the robustness and risk-sensitivity of RL algorithms."}, {"title": "A Extended results", "content": "We present the distributions of TD errors sampled at the initial and final evaluation steps, depicted in Figures 2 and 6 for SAC, and Figure 7 for PPO, which highlights the heavy tailedness of TD errors and the tendency converge to heavy tail throughout training. This finding also emphasizes how aleatoric uncertainty affects their distribution, as elaborated in Section 3.1. Interestingly, both state-action values Q and state values V demonstrate similar characteristics in their TD error distributions."}, {"title": "B Proofs", "content": "Proof. Consider a finite sample \\(X_1, X_2, ..., X_n\\) of independent, normally distributed random vari-ables with \\(X \u223c P_{\\theta_0}\\), where \\(\\theta_0 = (\\mu, \\sigma) \\in \\Theta\\) represents the true generative parameters. Under this assumption, both skewness \\(\\gamma\\) and kurtosis \\(\\kappa\\) are zero. Consequently, the moments are given by \\(E[X] = \\mu, E [(X \u2013 \\mu)^2] = \\sigma^2, E [(X \u2013 \\mu)^3] = \\sigma^{3/2}\\gamma = 0\\) and \\(E [(X \u2013 \\mu)^4] = \\sigma^4(\\kappa + 3) = 3\\sigma^4\\).\nThe MLE estimator of \\(\\mu\\) and \\(\\sigma\\) is \\(\\theta = (\\hat{\\mu}, \\hat{\\sigma})\\), given by\n\\[\\hat{\\mu} = \\overline{X} = \\frac{1}{n} \\sum_{i=1}^n X_i, \\text{ and } \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i \u2013 \\overline{X})^2.\\]\nIt is well known to be consistent [8].\nAssuming \\(\\hat{\\theta} \\overset{p}{\\to} \\theta_0\\), where \\(\\overset{p}{\\to}\\) denotes the convergence in probability, under appropriate regularity conditions, then the asymptotic normality theorem of Cramer leads to\n\\[\\sqrt{n} (\\hat{\\theta} \u2013 \\theta_0) \\overset{d}{\\to} N (0, I(\\theta_0)^{-1}),\\]\nas \\(n \\to \\infty\\) [18]. Here, \\(I(\\theta_0) = \\begin{pmatrix}\nI_{11} & I_{12} \\\\\nI_{21} & I_{22}\n\\end{pmatrix}\\) is the Fisher information matrix.\nThrough straightforward calculations involving the log likelihood function derivatives, we obtain\n\\[I(\\theta_0) = \\begin{pmatrix}\nE \\left[-\\frac{\\partial^2 l}{\\partial \\mu^2}\\right] & E \\left[-\\frac{\\partial^2 l}{\\partial \\mu \\partial \\sigma^2}\\right] \\\\\nE \\left[-\\frac{\\partial^2 l}{\\partial \\sigma^2 \\partial \\mu}\\right] & E \\left[-\\frac{\\partial^2 l}{\\partial (\\sigma^2)^2}\\right]\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{n}{\\sigma^2} & 0 \\\\\n0 & \\frac{n}{2\\sigma^4}\n\\end{pmatrix} = n \\begin{pmatrix}\n\\frac{1}{\\sigma^2} & 0 \\\\\n0 & \\frac{1}{2\\sigma^4}\n\\end{pmatrix}.\\]\nThus, as \\(n \\to \\infty\\), \\(\\sqrt{n}(\\hat{\\theta} \u2013 \\theta_0) \\overset{d}{\\to} N \\left(0, \\begin{pmatrix}\n\\sigma^2 & 0 \\\\\n0 & 2\\sigma^4\n\\end{pmatrix}\\right)\\).\nNow, consider a finite sample \\(X_1, X_2, ..., X_n\\) of independent, non-normally distributed random variables, with MLE estimator \\(\\overline{\\theta} = (\\overline{\\mu}, \\overline{\\sigma})\\). By applying analogous reasoning, we seek the asymptotic distribution of \\(\\sqrt{n}(\\overline{\\theta} \u2013 \\theta_0)\\). Letting \\(\\overline{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n(X_i \u2013 \\overline{\\mu})^2\\) gives us\n\\[\\overline{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n(X_i \u2013 \\overline{\\mu})^2\\]\n\\[=\\frac{1}{n}\\sum_{i=1}^n\\left( (X_i \u2013 \\mu) \u2013 (\\overline{\\mu} \u2013 \\mu) \\right)^2\\]\n\\[=\\frac{1}{n}\\sum_{i=1}^n (X_i \u2013 \\mu)^2 \u2013 (\\overline{\\mu} \u2013 \\mu)^2.\\]\nTherefore, as \\(\\sqrt{n}(\\overline{\\mu} \u2013 \\mu)^2 \\overset{p}{\\to} 0\\),\n\\[\\begin{aligned}\\sqrt{n}(\\overline{\\theta} \u2013 \\theta_0) &= \\sqrt{n} (\\overline{\\mu} \u2013 \\mu, \\overline{\\sigma}^2 \u2013 \\sigma^2) \\\\\n&= \\sqrt{n} (\\overline{\\mu} \u2013 \\mu, \\sigma^2 \u2013 \\sigma^2) \u2013 (0, \\sqrt{n}(\\overline{\\mu} \u2013 \\mu)^2) \\\\\n&\\approx \\sqrt{n} (\\overline{\\mu} \u2013 \\mu, \\sigma^2 \u2013 \\sigma^2) .\n\\end{aligned}\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(11)\\]\nDenoting \\(\\overline{\\theta} = (\\overline{\\mu}, \\overline{\\sigma}^2)\\), (11) states that \\(\\theta\\) and \\(\\overline{\\theta}\\) are asymptotically equivalent, i.e.,\n\\[(\\theta \u2013 \\theta) \\overset{a}{\\sim} 0.\\]"}, {"title": "B.2 Of Theorem 1", "content": "To prove that the NLL function of GGD is well defined for \\(\\beta \\in (0,2]\\), we show that the PDF of GGD is everywhere positive with it being a positive definite function. An easy but effective proof can be done by demonstrating that the PDF of GGD for \\(\\beta \\in (0, 2)\\) is equivalent to the characteristic function of an a-stable distribution, up to a normalizing constant [16]. Given the positive definiteness of all characteristic functions [16, 62], this equivalence assures the positive definiteness of the GGD PDF. However, we offer a proof rooted in the properties of the positive definite function class.\nProof. To demonstrate the positive definiteness of the GGD PDF, it is sufficient to show the positivity of the function:\n\\[f_{\\beta}(x) = e^{-|x|^\\beta},\\]\nwhere \\(\\beta \\in (0,2]\\).\nFor a class of positive definite functions \\(\\mathcal{F}\\), which can be interpreted as Fourier transforms of bounded non-negative distributions, the function \\(f \\in \\mathcal{F}\\), i.e.,\n\\[f(x) = \\int_{-\\infty}^{\\infty} e^{ix\\lambda} dV (\\lambda),\\]\nsatisfy the following properties:\n1. For any non-negative scalars \\(a_1\\), \\(a_2\\) and functions \\(f_1\\), \\(f_2 \\in \\mathcal{F}\\), \\(a_1 f_1 + a_2 f_2 \\in \\mathcal{F}\\).\n2. For \\(f_1, f_2 \\in \\mathcal{F}\\), \\(f_1 f_2 \\in \\mathcal{F}\\).\n3. If a sequence of functions \\(f_n \\in \\mathcal{F}\\) converges uniformly in every finite interval, then the limit function \\(\\lim_{n\\to \\infty} f_n \\in \\mathcal{F}\\).\nNow, let \\(p = \\beta/2\\), and we aim to prove that \\(f_p = exp(-|x|^{2p})\\) belongs to \\(\\mathcal{F}\\) for \\(p \\in (0, 1)\\), excluding the trivial case \\(p = 1\\). Since for \\(0 < p < 1\\),\n\\[|x|^{2p} = C_p \\int_{0}^{\\infty} \\frac{x^{2p-1} d\\lambda}{1+(\\frac{\\lambda}{x})^2},\\quad C_p > 0,\\]"}, {"title": "B.3 Of Theorem 2", "content": "Proof. For random variable \\(X \\sim GGD(0, \\alpha, \\beta)\\), its cumulative distribution function \\(F_X (t)\\) is defined as\n\\[F_X(t) = \\frac{1}{2} + sign(t) \\frac{\\gamma(1/\\beta, (\\frac{t \u2013 \\mu}{\\alpha})^\\beta)}{2\\Gamma (1/\\beta)},\\]\nwhere\n\\[\\Gamma(x) = \\int_{0}^{\\infty} t^{x-1}e^{-t}dt, \\text{ and } \\gamma(x,s) = \\int_{0}^{s} t^{x-1}e^{-t}dt,\\]\nrepresent the gamma function and lower incomplete gamma function for a complex parameter \\(x\\) with a positive real part.\nExpanding the left-hand side of (7), we obtain\n\\[\\begin{aligned}\\int_{-\\infty}^{\\infty} F_{X_1}(t) \u2013 F_{X_2}(t)dt &= \\int_{-\\infty}^{\\infty} sign(t) \\frac{\\gamma(1/\\beta_1, (\\frac{t \u2013 \\mu}{\\alpha})^{\\beta_1})}{2\\Gamma (1/\\beta_1)} \u2013 \\frac{\\gamma(1/\\beta_2, (\\frac{|t \u2013 \\mu|}{\\alpha})^{\\beta_2})}{2\\Gamma (1/\\beta_2)} dt \\\\\n&= \\int_{0}^{\\infty} \\frac{\\gamma(1/\\beta_2, (\\frac{|t \u2013 \\mu|}{\\alpha})^{\\beta_2})}{2\\Gamma (1/\\beta_2)} \u2013 \\frac{\\gamma(1/\\beta_1, (\\frac{t \u2013 \\mu}{\\alpha})^{\\beta_1})}{2\\Gamma (1/\\beta_1)} dt.\n\\end{aligned}\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(12)\\]\nDefining \\(f(\\beta, t) = \\gamma(1/\\beta, (\\frac{|t \u2013 \\mu|}{\\alpha})^\\beta)/2\\Gamma (1/\\beta)\\), we aim to demonstrate the monotonicity of \\(f\\) to conclude the proof, as monotonically increasing \\(f\\) leads to \\((f(\\beta_2, x) \u2013 f(\\beta_1, x)) \\geq 0\\) for \\(\\beta_2 \\geq \\beta_1\\), i.e., the integral in (12) is greater than or equal to zero.\nWith the definition of the gamma and lower incomplete gamma function,\n\\[f(\\beta, t) = \\frac{\\gamma (1/\\beta, (\\frac{t \u2013 \\mu}{\\alpha})^\\beta)}{2\\Gamma (1/\\beta)} = \\frac{\\int_{0}^{(\\frac{t \u2013 \\mu}{\\alpha})^\\beta} e^{-t} t^{1/\\beta \u2013 1}dt}{\\int_{0}^{\\infty} e^{-t} t^{1/\\beta \u2013 1}dt}\\quad.\\quad\\quad\\quad\\quad\\quad\\quad\\quad(13)\\]"}, {"title": "B.4 Of Proposition 2", "content": "Proof. It is well known that the variance of the unbiased estimator \\(s^2\\) is given by\n\\[V[s^2", "sigma^4\\": "nAnd the MSE of a biased estimator \\(s^2 = w(n \u2212 1)s^2\\) is\n\\[MSE(s^2) = w^2(n \u2212 1)^2V[s^2"}]}