{"title": "Generalized Gaussian Temporal Difference Error\nFor Uncertainty-aware Reinforcement Learning", "authors": ["Seyeon Kim", "Joonhun Lee", "Namhoon Cho", "Sungjun Han", "Seungeon Baek"], "abstract": "Conventional uncertainty-aware temporal difference (TD) learning methods of-\nten rely on simplistic assumptions, typically including a zero-mean Gaussian\ndistribution for TD errors. Such oversimplification can lead to inaccurate error rep-\nresentations and compromised uncertainty estimation. In this paper, we introduce\na novel framework for generalized Gaussian error modeling in deep reinforce-\nment learning, applicable to both discrete and continuous control settings. Our\nframework enhances the flexibility of error distribution modeling by incorporating\nhigher-order moments, particularly kurtosis, thereby improving the estimation and\nmitigation of data-dependent noise, i.e., aleatoric uncertainty. We examine the\ninfluence of the shape parameter of the generalized Gaussian distribution (GGD)\non aleatoric uncertainty and provide a closed-form expression that demonstrates\nan inverse relationship between uncertainty and the shape parameter. Addition-\nally, we propose a theoretically grounded weighting scheme to fully leverage the\nGGD. To address epistemic uncertainty, we enhance the batch inverse variance\nweighting by incorporating bias reduction and kurtosis considerations, resulting in\nimproved robustness. Extensive experimental evaluations using policy gradient al-\ngorithms demonstrate the consistent efficacy of our method, showcasing significant\nperformance improvements.", "sections": [{"title": "Introduction", "content": "Deep reinforcement learning (RL) has demonstrated promising potential across various real-world\napplications, e.g., finance [7, 45, 56], and autonomous driving [17, 30, 27]. One critical avenue\nfor improving the performance and robustness of RL agents in these complex, high-dimensional\nenvironments is the quantification and integration of uncertainty associated with the decisions made\nby agents or the environment [36]. Effective management of uncertainty promotes the agents to make\nmore informed decisions leading to enhanced sample efficiency in RL context, which is particularly\nbeneficial in unseen or ambiguous situations.\nTemporal difference (TD) learning is a fundamental component of many RL algorithms, facilitating\nvalue function estimation and policy derivation through iterative updates [57]. Traditionally, these TD\nupdates are typically grounded in L2 loss, corresponding to maximum likelihood estimation (MLE)\nunder the assumption of Gaussian error. Such simplification may be overly restrictive, especially\nconsidering the noisy nature of TD errors, which are based on constantly changing estimates of\nvalue functions and policies. This assumption compromises sample efficiency, necessitating the\nincorporation of additional distributional parameters for flexible but computationally efficient TD\nerror modeling."}, {"title": "Uncertainty", "content": "Uncertainty in neural networks is commonly decomposed into two sources: aleatoric and epistemic\n[13, 14, 31, 63]. Epistemic uncertainty arises from limitations within the neural network and can\npotentially be reduced through further learning or model improvements. In contrast, aleatoric\nuncertainty stems from the inherent stochasticity of the environment or the dynamics of the agent-\nenvironment interactions and is fundamentally irreducible.\nThis distinction is crucial in the context of RL, where areas with high epistemic uncertainty need to\nbe explored, whereas exploring areas with high aleatoric uncertainty may lead to ineffective training,\nsince the agent might have adequate knowledge but insufficient information for decisive actions.\nQuantifying aleatoric uncertainty is known to facilitate learning dynamics of stochastic processes and\nenables risk-sensitive decision making [10, 54, 65].\nTo address aleatoric uncertainty, variance networks, denoted by $Q^{\\sigma}$ to distinguish them from the value\napproximation head Q, are frequently employed [3, 31, 33, 38, 40, 47]. The estimated variance\nfrom $Q^{\\sigma}$ is utilized for heteroscedastic Gaussian error modeling, with $\\delta_{t} \\sim N(Q_{t}, Q_{t}^{\\sigma 2})$, thereby\nquantifying aleatoric uncertainty [54]. This network is trained by maximizing the log likelihood\nfunction of the Gaussian distribution or minimizing the Gaussian negative log likelihood (NLL)\nloss. The variance of estimated values is also used for batch inverse variance (BIV) weighting\nin inverse-variance RL [38]. This regularization effectively captures epistemic uncertainty with a\nregularizing temperature $\\lambda$:\n$L = L_{GD-NLL} + \\lambda L_{BIV}$\n$= \\sum_{t} (\\frac{\\delta_{t}^{2}}{(Q_{t}^{\\sigma})^{2}} + log Q_{t}^{\\sigma 2}) + \\lambda (\\sum_{t} w_{t}^{BIV} \\delta_{t}^{2})$    (3)\nwhere the BIV weight $w_{t}^{BIV} = 1/({\\gamma}^{2}V[Q_{t}] + \\epsilon)$ with empirical variance $V$, and $\\epsilon$ is either a\nhyperparameter or numerically computed to ensure a sufficient effective batch size. In the official\nimplementation, the sample variance with Bessel's correction is used as a variance estimator, which\nis particularly apparent given a small sample size, i.e., an ensemble number of five."}, {"title": "Tailedness", "content": "While mainstream machine learning literature often prioritizes on capturing central tendencies, the\nsignificance of extreme events residing in the tails for enhancing performance and gaining a deeper\nunderstanding of learning dynamics cannot be overlooked. This is especially relevant for MLE base\non the normality assumption, which is commonly applied in variance network frameworks. Focusing\nsolely on averages or even deviations is proven to be inadequate in the presence of outlier samples\n[11, 23].\nFor instance, consider the impact of non-normal samples on the estimate of the variance, as described\nin Proposition 1 with proof presented in Appendix B.1.\nProposition 1 (Biased variance estimator [68]). Let $X_{1}, X_{2}, ..., X_{n}$ be a sequence of independent,\nnon-normally distributed random variables from a population X with mean $\\mu$, variance $\\sigma^{2}$, and"}, {"title": "Methods", "content": "Our approach introduces enhancements to the loss function derived from (3), incorporateing tailedness\ninto both loss attenuation and regularization terms:\n$L = L_{GGD-NLL} + \\lambda L_{BIEV}$\n$\\sum_{t} (((|\\delta_{t}|/Q_{t}^{\\alpha}) - log Q_{t}^{\\alpha}Q_{t}^{\\beta} + log log (1/Q_{t}^{\beta})) + (w_{t}^{BIEV} \\delta_{t}^{2}))$    (4)\nwhere $Q^{\\alpha}$ and $Q^{\\beta}$ represent the alpha and beta networks, respectively. Here, $w_{t}^{RA} = Q_{t}^{\\beta}$, and\n$w_{t}^{BIEV} = 1/(\\sqrt{V[\\delta_{t}]} + \\epsilon)$. These dual loss terms effectively capture aleatoric and epistemic uncertainties,\nrespectively. The subsequent subsections provide a detailed rationale for this modification."}, {"title": "Generalized Gaussian error modeling", "content": "One simple yet promising approach to address non-normal heteroscedastic error distributions involves\nmodeling the per-error distribution using a zero-mean symmetric GGD [9, 24, 61, 69]:\n$\\delta \\sim GGD(0, \\alpha, \\beta) = \\frac{\\beta}{2\\alpha \\Gamma(1/\\beta)}  exp(-(|\\delta|/\\alpha)^{\\beta})$,    (5)\nwhere $\\alpha$ and $\\beta$ represent the scale and shape parameter, respectively. This method not only allows for\nmodeling each non-identical error by parameterizing the GGD with different $\\alpha_{t}$ and $\\beta_{t}$ at step t, but\nalso offers a flexible parametric form that adapts across a spectrum of classical distributions from\nGaussian to uniform as $\\beta$ increases to infinity [16, 25, 46, 48].\nThe shape parameter $\\beta$ serves as a crucial structure characteristic, distinguishing underlying mecha-\nnisms. The kurtosis $\\kappa$, commonly used to discern different distribution shapes, is solely a function\nof $\\beta$ and is defined as Pearson's kurtosis minus three to emphasize the difference from Gaussian\ndistribution [12]:\n$\\kappa = \\frac{\\Gamma(5/\\beta)\\Gamma(1/\\beta)}{\\Gamma(3/\\beta)^{2}} - 3$.    (6)\nThis implies that distributions with $\\beta < 2$ are leptokurtic, i.e., $\\kappa > 0$, indicating a higher frequency\nof outlier errors compared to the normal error distribution. With only one additional parameter to"}, {"title": "Experimental evaluations", "content": "We conduct a comprehensive evaluation of our proposed method across well-established benchmarks,\nincluding MuJoCo [59], and discrete control environments from Gymnasium [60]. Notably, we\naugment the discrete control environments through the introduction of supplementary uniform action\nnoise to enhance environmental fidelity.\nTo underscore the versatility and robustness of our approach, we deliberately choose baseline\nalgorithms that cover a wide spectrum of RL paradigms. Specifically, we employ soft actor-critic\n(SAC) [26], an off-policy Q-based policy gradient algorithm, and proximal policy optimization\n(PPO) [52], an on-policy V-based method. We focus on adversaries limited to variance networks\ndue to the use of separate target networks in previously mentioned Gumbel error modeling methods.\nThis constraint is intended to focus on computationally efficient algorithms that only incorporate an\nadditional layer, referred to as a head.\nThe algorithms are implemented using PyTorch [50], within the Stable-Baselines3 framework [51].\nWe use default configurations from Stable-Baselines3, with adaptations limited to newly introduced\nhyperparameters. For both PPO and SAC, along with their variants, we employ five ensembled\ncritics. The parameter $\\epsilon$ from (4) is computed with a minimum batch size of 16, and the regularizing\ntemperature $\\lambda$ is set to 0.1. Additional experimental details are provided in Appendix C."}, {"title": "Discussion", "content": "In this paper, we advocate for and substantiate the integration of GGD modeling for TD error\nanalysis. Our main contribution is the introduction of a novel framework that enables robust training\nmethodologies by leveraging the distribution's shape. This approach accounts for both data-dependent"}, {"title": "Extended results", "content": "We present the distributions of TD errors sampled at the initial and final evaluation steps, depicted in\nFigures 2 and 6 for SAC, and Figure 7 for PPO, which highlights the heavy tailedness of TD errors and\nthe tendency converge to heavy tail throughout training. This finding also emphasizes how aleatoric\nuncertainty affects their distribution, as elaborated in Section 3.1. Interestingly, both state-action\nvalues Q and state values V demonstrate similar characteristics in their TD error distributions."}, {"title": "Proofs", "content": "Of Proposition 1\nProof. Consider a finite sample $X_{1}, X_{2}, ..., X_{n}$ of independent, normally distributed random vari-\nables with $X \\sim P_{\\theta_{0}}$, where $\\theta_{0} = (\\mu, \\sigma) \\in \\Theta$ represents the true generative parameters. Under\nthis assumption, both skewness $\\gamma$ and kurtosis $\\kappa$ are zero. Consequently, the moments are given by\n$E[X] = \\mu, E [(X - \\mu)^{2}] = \\sigma^{2}, E [(X - \\mu)^{3}] = \\sigma^{3/2}\\gamma = 0$ and $E [(X - \\mu)^{4}] = \\sigma^{4}(\\kappa + 3) =$\n$3\\sigma^{4}$.\nThe MLE estimator of $\\mu$ and $\\sigma$ is $\\theta = (\\hat{\\mu}, \\hat{\\sigma})$, given by\n$\\hat{\\mu} = \\overline{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$, and $\\hat{\\sigma^{2}} = \\frac{1}{n}\\sum_{i=1}^{n} (X_{i} - \\overline{X})^{2}$.\nIt is well known to be consistent [8].\nAssuming $\\hat{\\theta}\\xrightarrow{p}\\theta_{0}$, where $\\xrightarrow{p}$ denotes the convergence in probability, under appropriate regularity\nconditions, then the asymptotic normality theorem of Cramer leads to\n$\\sqrt{n} (\\hat{\\theta} - {\\theta_{0}}) \\xrightarrow{d} N (0, I({\\theta_{0}})^{-1}),$\nas $n\\rightarrow\\infty$ [18]. Here, $I({\\theta_{0}}) = \\begin{pmatrix} I_{11} & I_{12} \\\\ I_{21} & I_{22} \\end{pmatrix}$ is the Fisher information matrix.\nThrough straightforward calculations involving the log likelihood function derivatives, we obtain\n$I({\\theta_{0}}) = E\\begin{pmatrix} -\\frac{\\partial^{2} l}{\\partial {\\mu}^{2}} & -\\frac{\\partial^{2} l}{\\partial {\\mu}\\partial {\\sigma}} \\\\ -\\frac{\\partial^{2} l}{\\partial {\\sigma}\\partial {\\mu}} & -\\frac{\\partial^{2} l}{\\partial {\\sigma}^{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sigma^{2}} & 0 \\\\ 0 & \\frac{1}{2\\sigma^{4}} \\end{pmatrix}$.\nThus, as $n \\rightarrow \\infty$, $\\sqrt{n}(\\hat{\\theta} - {\\theta_{0}}) \\xrightarrow{d} N \\Big(0, \\begin{pmatrix} {\\sigma}^{2} & 0 \\\\ 0 & 2{\\sigma}^{4} \\end{pmatrix} \\Big)$.\nNow, consider a finite sample $X_{1}, X_{2}, ..., X_{n}$ of independent, non-normally distributed random\nvariables, with MLE estimator $\\theta = (\\hat{\\mu}, \\hat{\\sigma})$. By applying analogous reasoning, we seek the asymptotic\ndistribution of $\\sqrt{n}(\\hat{\\theta} - {\\theta_{0}})$. Letting $\\tilde{{\\sigma}}^{2} = \\frac{1}{n}\\sum_{i=1}^{n} (X_{i} - {\\mu})^{2}$ gives us\n$\\tilde{{\\sigma}}^{2} = \\frac{1}{n}\\sum_{i=1}^{n} (X_{i} - {\\mu})^{2}$\n$= \\frac{1}{n}\\sum_{i=1}^{n} (X_{i} - {\\mu})^{2} - (\\hat{{\\mu}} - {\\mu})^{2}$\n$= \\hat{{\\sigma}}^{2} - \\frac{1}{n}\\sum_{i=1}^{n} (\\hat{{\\mu}} - {\\mu})^{2}$.\nTherefore, as $\\sqrt{n}(\\hat{{\\mu}} - {\\mu})^{2} \\xrightarrow{p} 0$,\n$\\sqrt{n}(\\hat{\\theta} - {\\theta_{0}}) = \\sqrt{n} (\\hat{{\\mu}} - {\\mu}, \\hat{{\\sigma}}^{2} - {\\sigma}^{2})$\n$= \\sqrt{n} (\\hat{{\\mu}} - {\\mu}, {\\tilde{{\\sigma}}}^{2} - {\\sigma}^{2}) - (0, \\sqrt{n}(\\hat{{\\mu}} - {\\mu})^{2})$ (11)\n$\\approx \\sqrt{n} (\\hat{{\\mu}} - {\\mu}, {\\tilde{{\\sigma}}}^{2} - {\\sigma}^{2}) .$\nDenoting $\\tilde{\\theta} = (\\hat{{\\mu}}, {\\tilde{{\\sigma}}}^{2})$, (11) states that $\\hat{\\theta}$ and $\\tilde{\\theta}$ are asymptotically equivalent, i.e.,\n$(\\hat{\\theta} - {\\theta_{0}}) \\xrightarrow{d} 0$."}]}