{"title": "Learning for Long-Horizon Planning via Neuro-Symbolic Abductive Imitation", "authors": ["Jie-Jing Shao", "Hao-Ran Hao", "Xiao-Wen Yang", "Yu-Feng Li"], "abstract": "Recent learning-to-imitation methods have shown promising results in planning via imitating within the observation-action space. However, their ability in open environments remains constrained, particularly in long-horizon tasks. In contrast, traditional symbolic planning excels in long-horizon tasks through logical reasoning over human-defined symbolic spaces but struggles to handle observations beyond symbolic states, such as high-dimensional visual inputs encountered in real-world scenarios. In this work, we draw inspiration from abductive learning and introduce a novel framework ABductive Imitation Learning (ABIL) that integrates the benefits of data-driven learning and symbolic-based reasoning, enabling long-horizon planning. Specifically, we employ abductive reasoning to understand the demonstrations in symbolic space and design the principles of sequential consistency to resolve the conflicts between perception and reasoning. ABIL generates predicate candidates to facilitate the perception from raw observations to symbolic space without laborious predicate annotations, providing a groundwork for symbolic planning. With the symbolic understanding, we further develop a policy ensemble whose base policies are built with different logical objectives and managed through symbolic reasoning. Experiments show that our proposal successfully understands the observations with the task-relevant symbolics to assist the imitation learning. Importantly, ABIL demonstrates significantly improved data efficiency and generalization across various long-horizon tasks, highlighting it as a promising solution for long-horizon planning. Project website: https://www.lamda.nju.edu.cn/shaojj/KDD25_ABIL/.", "sections": [{"title": "1 INTRODUCTION", "content": "A long-standing goal in AI is to build agents that are flexible and general, able to accomplish a diverse set of tasks in open and novel environments, such as home robots for cooking meals or assembling furniture. These tasks generally require the agents to execute sequential decision-making, which is often formulated as a planning problem. Recently, the learning-based method, Imitation Learning, has achieved remarkable success via imitating expert demonstrations, in a variety of domains, such as robotic manipulation [8, 33], autonomous driving [2, 25] and language models [3, 35]. However, the theoretical studies [26, 37] reveal that imitation learning can suffer from serious performance degradation due to the covariate shift between limited expert demonstrations and the state distribution actually encountered by the agents, especially in long-horizon tasks. In traditional AI literature, symbolic planners effectively generalize in long-horizon decision-making, via logical reasoning on the human-defined symbolic spaces [9, 10, 14]. However, they often simplify the perception process by relying on ground-truth symbols. Given observations and actions, pure logic-based methods struggle to map raw observations to human-defined symbolic spaces without predicate-level supervision.\nTo address these issues, efforts are underway to merge the advantages of learning-based and reasoning-based approaches into neuro-symbolic planning. Xu et al. [36] propose the regression planning network, learning to predict symbolic sub-goals that need to be achieved before the final goals, thereby generating a long-term symbolic plan conditioned on high-dimensional observations."}, {"title": "2 RELATED WORK", "content": "The preface work of this paper mainly includes Imitation Learning, Neuro-Symbolic Planning and Abductive Learning.\nImitation Learning learns the policies from expert demon-strations to achieve sequential decision-making [20]. There are many works that obtain successful results on varying domains, such as robotic manipulation [8, 33], autonomous driving [2, 25] and language models [3, 35]. However, learning theory reveals that the generalization ability of imitation learning is constrained by the size of the expert dataset and degrades as the decision-making horizon increases [26\u201328]. This issue is particularly pronounced in open environments, where home agents need to accomplish tasks in differently arranged rooms. In such settings, the distribution shift, that is, the covariate shift between training observations and the scenarios the agent actually encounters, presents a greater challenge for imitation learning [21, 23].\nNeuro-Symbolic Planning explores to combine traditional symbolic planning with learning to enhance model's generalization capabilities. To handle observations beyond symbolic states, previ-ous studies typically involve training neural networks with task-relevant predicate annotations to transform raw observations into symbolic states for planning. For example, Xu et al. [36] propose the Regression Planning Networks, which learns to predict sub-goals that need to be achieved before the final goals, enabling traditional symbolic regression to handle complex high-dimensional inputs, like images. Konidaris et al. [22] collect feasibility annotations and transition data under different symbolic operations to learn the symbolic representation of different observations. Silver et al. [32] formalize operator learning for neuro-symbolic planning, viewing operators as an abstraction model of the raw transition and generating the high-level plan skeletons. Wang et al. [34] leverage a pre-trained vision-language model to provide the predicate-level"}, {"title": "3 THE PROPOSED FRAMEWORK", "content": "In this paper, we focus on the goal-based planning task. Following [24, 30, 31], the environment is formally defined as a tuple $(S,A,T,O,P,OP, S^0, g)$. Here, S represents the state space, and A represents the action space. The transitions between states and actions are governed by a deterministic transition function T: $S \\times A \\rightarrow S$. $S^0$ denotes the distribution of initial states. The set O consists of a finite number of task-related objects, where each"}, {"title": "3.1 Problem Formulation", "content": "object o possesses a unique name, such as car and rag. Additionally, P is a finite set of task-related predicate symbols, where each predicate symbol p has an arity that indicates the number of arguments it takes. For example, Inside/2 has an arity of two, representing that one object is inside another. A ground atom p is a predicate that only contains concrete objects, such as Inside(rag, bucket). If a state s satisfies $s \\models p$, it indicates that s semantically entails the interpretation of p. The set g consists of ground atoms that represent the task's target. The task is to find an action sequence that generates a trajectory $(s^0, a^1, s^1, ..., a^T, s^T)$ satisfying $\\forall p \\in g, s^T \\models p$. For simplification, we denote $s \\models P$ (where P is a set of ground atoms) as $\\forall p \\in P, s \\models p$. Moreover, OP is also a set of finite predicate symbols that represent logical operators, such as clean/3 and put/2. We denote obj(p) to retrieve the object(s) from a ground atom. For instance, obj(Put(rag, bucket)) = {rag, bucket}.\nThe symbolic knowledge base provided by experts could be formulated as a finite-state machine with a directed graph G = (V, E). Each node v in the vertex set V contains a set of ground atoms of P, which can be viewed as the condition of a sub-task. Each edge is noted as a tuple (op, EFF+, EFF\u2212). op is a ground atom of OP representing the symbolic action, e.g., Put(rag, bucket). EFF+ is the add effect and EFF\u2212 is the delete effect, each is a set of grounding atoms. A symbolic action op typically requires multiple actions a \u223c A to achieve the desired logical sub-goal, corresponding to a segment of the complete trajectory. For the sake of simplicity, the notation op is utilized to denote the edge.\nFor each node u, v \u2208 V, if there is a directed edge pointing from u to v, then $v \\models (u - EFF^-) \\cup EFF^+$. We define a trajectory $z = (s^0, a^1, s^1, ..., a^T, s^T)$ satisfying the knowledge base G denoted as $z \\models G$ if and only if for every adjacent states pair $(s^t, s^{t+1})$, there exists $u \\in V, s^t \\models u \\rightarrow s^{t+1} \\models u$ or $s^t \\models u, s^{t+1} \\models v, \\exists u, v \\in V, (u, v) \\in E$. This indicates that the expert trajectory satisfies the corresponding symbolic knowledge base, such as first using a rag and soap to clean a dusty car, and then putting them into a bucket. Figure 2a demonstrates an example of the knowledge base formalized as a state machine with a directed graph.\nThe state machine contains multiple basic structures as illus-trated in Figure 2b. In the event that a singular node, denoted as v1, directs towards the goal, it signifies the necessity to address a corresponding sub-task v1. For instance, the action Clean(car, rag, soap) is imperative whenever the objective is to clean a car. Conversely, should there be a directed edge from v1 to v2, with v2"}, {"title": "3.2 Abductive Reasoning", "content": "The ABIL framework can be roughly divided into abductive learning with the state machine and imitation with symbolic reasoning. The overall framework is illustrated and summarized in Figure 3 and Algorithm 1, respectively.\nGiven the state machine and expert demonstrations, the chal-lenge lies in establishing the perception function f from observation st to symbolic grounding {st} when symbolic supervision of {st} is not available. To address this challenge, we introduce abductive reasoning to provide pseudo labels z derived from the state machine's knowledge, which could be taken to optimize the perception function f.\n$\\underset{f}{min} \\sum_{s \\in D} \\sum_{t=1}^T L(f(s_t), \\hat{z_t}),$\n${\\lbrace \\hat{z_t}\\rbrace}_{t=1}^T = \\underset{\\{z_t\\}_{t=1}^T}{arg \\, min} ||z - f(s)||^2, s.t. \\{z_t\\}_{t=1}^T \\models G$ \n(1)\nSpecifically, for the different structures in the state machine, we could derive the sequential abduction: ${\\lbrace z_t\\rbrace}_{t=1}^T \\models G$ as:\n\u2022 The task has been completed at the end of the expert demonstration sequence. The symbolic state zT satisfies the final goal g, that is, $z^T \\models g$.\n\u2022 Task a should be accomplished before b. The symbolic sequence ${\\lbrace z_t\\rbrace}_{t=1}^T$ will satisfy ${\\lbrace z_t\\rbrace}_{t=1}^T \\models a$, ${\\lbrace z_t\\rbrace}_{t=j+1}^T \\models b, \\exists j \\in (1,T)$ where the demonstrations could be divided into the two sequential part of accomplishing a and b."}, {"title": "3.3 Symbolic-grounded Imitation", "content": "As a human being, one often consciously knows what he or she is doing, such as making the action of turning left because they realize the destination is on the left. In this part, we thus incorporate the reasoning of high-level operators into the original imitation learning process, regarding the symbolic operators as an assistance signals. Specifically, we first build the behavioral actor for each logical operator hop, e.g. hclean and hput. Then we derive the desired behavior module by the symbolic states output of perception f and the corresponding abstract logical operator. Given the solution of symbolic planning: ${\\{(v_0, op_0), (v_1, op_1) ... (v_k, op_k)\\}}$.\n$op_t = op_k, s.t. f(s_t) = v_k, \\exists k \\in [0, K)$ \n(2)\nThe desired parameter of the operator opt, e.g., which object will be picked, could be reasoning as:\n$o_t = obj(op_t)$ \n(3)\nThen the learning of behavior actors could be formulated as:\n$\\underset{h_{op}}{min} \\sum_{s_i,a_i \\in D} \\sum_{t=1}^T L(h_{op}(s_i, o_t), a_i)$ \n(4)\nAs summarized in the right part of Figure 3, our behavioral actors, referring to the human model of cognition before decision-making, embed high-level logical reasoning into the imitation learning process. The behavior ensemble hop learns from experience through imitation, without relying on a pre-existing perfect controller to reach each sub-goal. Importantly, by leveraging the generalization capabilities of symbolic planning, the proposed actors can decompose diverse observations into symbolic states, facilitating more reliable decision-making."}, {"title": "4 EMPIRICAL STUDY", "content": "We evaluate our proposal in three environments, including two neuro-symbolic benchmarks: BabyAI [5], Mini-BEHAVIOR [21], and a robotic manipulation benchmark, CLIPort [29]. We compare our method with three baselines: Behavior Cloning (BC) [1], Decision Transformer (DT) [4] and PDSketch [24]. For a fair comparison, all of these methods use the same network architecture which is based on the Neural Logic Machine (NLM) [7]. Specifically, we first encode the state with a two-layer NLM. For BC, we use a single linear layer, taking the state embedding as the input and output actions. For DT, we build a single transformer layer following the two-layer encoder, with the causal mask to generate future action with past states and actions. For PDSketch, we choose the full mode in the original paper [24], which provides sufficient prior knowledge of the symbolic transition, keeping consistency with our symbolic state machine. Following [24, 29], we report the percentage of successful planning for the desired goals, which are averaged over 100 evaluations under three random seeds."}, {"title": "4.1 Evaluation on BabyAI", "content": "BabyAI provides a benchmark for grounding logical instructions where an agent needs to follow instructions for a series of tasks like picking up objects, or unlocking doors. Following [24, 36], we consider 5 tasks: {goto, pickup, open, put, unlock}, and conduct the generalization evaluation with different numbers of objects in"}, {"title": "4.2 Evaluation on Mini-BEHAVIOR", "content": "Mini-BEHAVIOR is a recently proposed benchmark for embodied AI. It contains varying 3D household tasks chosen from the BEHAVIOR benchmark, including Sorting Books, Making Tea, Cleaning A Car, and so on. Most tasks are long-horizon and heterogeneous, some of which require more than one hundred decision-making steps to be completed. There are hundreds of different types and plenty of predicates which is challenging for neuro-symbolic grounding. In this domain, our state machine is mainly composed of several typical categories. For tasks mainly about tidying up the room, we split the primitive actions into oppick and opplace, which is required to perform an action sequence to finish picking or placing sub-tasks. Combined with our symbolic-grounding model f, the agent will be able to distinguish when and where to pick and place. For tasks mainly about cleaning, we split the primitive actions into opclean and opput, which is required to finish washing or putting sub-tasks. In the generalization evaluation, we challenge the agents in environments with distractor objects that are unseen at the training phase.\nResults and Analysis. The results on Mini-BEHAVIOR are provided in Table 2. In some simple short-horizon tasks, such as Installing a printer and Opening packages, BC shows satisfactory performance. Nevertheless, as the desired decision sequence grows, errors made by BC gradually accumulate, leading to an increasing deviation from the correct solution. This becomes particularly critical in the presence of disturbances or interferences, leading"}, {"title": "4.3 Evaluation on Robotic Manipulation", "content": "We further evaluate the proposal on the CLIPort [29] with 3D robotic manipulation tasks. In this environment, an agent needs to learn how to transport some objects and solve complex manipu-lation tasks based on visual observation. Every manipulation task"}, {"title": "4.4 Data Efficiency", "content": "Data efficiency is important for imitation learning, especially in robotic tasks where expert demonstrations are usually expensive and scarce. In this subsection, we conduct experiments across vary-ing sizes of expert demonstrations to evaluate the data efficiency."}, {"title": "4.5 Zero-Shot Generalization", "content": "Symbolic reasoning excels at generalization, especially ensuring the correctness of reasoning for any combination of logical clauses. In this subsection, we evaluate the zero-shot generalization perfor-mance in the composed tasks. In the BabyAI domain, we train the policies on the pickup and open task, then test them on the composed task unlock. During training, the demonstrations from two tasks are mixed and learning in a multi-task scheme. In the Mini-BEHAVIOR domain, we primarily concentrate on generalization with the longer series of events, which demands the agent to make use of learned techniques for repeatedly completing a single task to achieve the desired goal. Take the Throwing away leftovers task as an example, we train every model in the environment with 1 leftover hamburger to throw, while in the test environment, the agent is required to throw 2 or 3 hamburgers. In robotic manipulation, we primarily focus on compositional generalization with the novel combination of goals, which demands the agent to re-combine learned concepts to achieve, as shown in Figure 1(c).\nResults and Analysis. Although all baselines achieve satisfactory performance on the training tasks (pickup and open), their performance degrades on the simple combined task (unlock). The pure-learning-based methods directly learn the action corresponding to the observations, lacking reasoning ability, thus unable to realize the need to first pick up a key that can open the target door, resulting in failure. PDSketch has reasoning ability, but its model-based planning solution accumulates errors with the increasing length of the sequence, resulting in poor performance and high computational overhead. In tasks with longer sequences, such as throwing away leftovers, solutions cannot be found even after running out of time. Our ABIL not only performs high-level reasoning to know that sub-goals should be sequentially completed but also can zero-shot achieve the composed tasks."}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "In this work, we proposed a novel framework, ABductive Imitation Learning (ABIL), which integrates data-driven learning with sym-bolic reasoning to address long-horizon tasks in imitation learning. ABIL bridges the gap between neural perception and logical rea-soning by autonomously generating predicate candidates from raw observations with the knowledge base, enabling effective reasoning without requiring extensive manual annotations. Experiments demonstrate that ABIL significantly improves data efficiency and generalization across various long-horizon tasks, positioning it as a promising neuro-symbolic solution for imitation learning.\nDespite its contributions, ABIL has several limitations that suggest promising directions for future work: (1) Uncertainty and partial observability: The current framework assumes determin-istic and fully observable environments, consistent with existing work [24, 30, 31]. However, real-world environments are often stochastic and partially observable. A promising direction is to explore POMDP techniques [11, 13], which would allow ABIL to maintain a belief space and sample actions under uncertainty. (2) Automatic knowledge learning: Like most neuro-symbolic and abductive learning work, ABIL assumes the availability of a symbolic solution and relies on an accurate and sufficient knowledge base [6, 24]. A key direction is to incorporate advanced knowledge learning techniques [34, 38] to reduce reliance on human-defined knowledge. Additionally, introducing the active learning manner [40] with human feedback could help correct and supplement the knowledge base, further enhancing ABIL's adaptability and robustness. In summary, ABIL offers a timely and promising solution for neuro-symbolic imitation learning, particularly for long-horizon planning. Addressing the challenges of uncertain environments and incomplete knowledge will unlock its full potential, making it a reliable system for real-world applications."}, {"title": "A EXPERIMENTAL DETAILS", "content": "In these two environments, each object feature is represented by its state and position in the room, and the robot feature is represented by its position and direction. The state representation is composed of features of all objects and the robot. The action spaces are both discrete. All expert demonstrations are generated by scripts based on A* search. In BabyAI, 1000 demonstrations were used for training per task and to obtain Table 1. In Mini-BEHAVIOR, install-a-printer, opening packages, and moving boxes to storage used 1000, while other tasks used 3000 expert demonstrations for training.\nModel Architecture. For each predicate (e.g. is-dusty), we build a binary classifier, which takes a single object o as argument, and returns a scalar value from 0 to 1, indicating the classification score. All methods use the same network architecture which is based on the Neural Logic Machine [7]. Specifically, we first encode the state with a two-layer NLM. For BC, we use a single linear layer, taking the state embedding as the input and output actions. For DT, we build a single transformer layer following the two-layer encoder, with the causal mask to generate future action with past states and actions. For PDSketch, we choose the full mode in the original paper [24]. For ABIL-BC, we implement the behavior modules using BC model, and for ABIL-DT, we implement the behavior modules using DT model."}, {"title": "A.1 BabyAI and Mini-BEHAVIOR", "content": "A.2 Robotic Manipulation\nIn this environment, each object is represented as a tuple of a 3D xyz location, and an image crop. Following [24], we first compute the 2D bounding box of the object in the camera plane, then crop the image patch and resize it to 24 by 24 to obtain the image crop. The action space is continuous, each action involves a start and end-effector pose. Table 5 provides the average length of expert demonstrations. This benchmark involves the agent manipulating objects of various colors and shapes, reflecting the requirements in the open world, providing a greater challenge for imitation learning. For each task, 1000 expert demonstrations were used for training and to obtain Table 3. All of these demonstrations are collected using oracle policies following CLIPort [29], containing only successful trajectories.\nModel Architecture. Image feature of each object is a 64-dimensional embedding obtained via an image encoder, which is a 3-layer convolutional neural network followed by a linear transformation layer. For each predicate (e.g. is-red), we build a binary classifier, which takes the image feature of an object, and returns a scalar value from 0 to 1, indicating the classification score. The model implementation is same as in BabyAI and Mini-BEHAVIOR, except output continuous value as action."}, {"title": "B SUPPLEMENTAL RESULTS", "content": "B.1 Study on Performance with Imperfect Symbolic Grounding\nTo evaluate the influences of neuro-symbolic errors upon ABIL, we further conduct experiments on the Pickup and Putting-blocks-in-bowls task. Experimental results are provided in Table 6.\nLike human reasoning, incorrect logical objectives may lead to the failure of sequential decision-making. Neuro-symbolic errors indeed lower the performance of ABIL. Nevertheless, ABIL integrates data-driven imitation and logical objectives in learning, it has a tolerance for neuro-symbolic errors. Even under 75%"}, {"title": "B.2 Additional Results on zero-shot generalization", "content": "In Table 7, we provide additional evaluation results on zero-shot generalization task in the Mini-BEHAVIOR benchmark. In Opening Packages task, we train every model in the environment with 1 package to open, while in the testing environment, the agent is required to open 2 or 3 packages."}, {"title": "C REPRODUCIBILITY", "content": "To promote reproducibility, we release the code on GitHub\u00b9 . This may also assist future research."}, {"title": "D DETAILS OF KNOWLEDGE BASE", "content": "In this section, we provide a detailed illustration of our knowledge base to help readers understand and reproduce."}, {"title": "D.1 BabyAI", "content": "D.2 Mini-BEHAVIOR\nIn this domain, our state machine is mainly composed of several typical categories. For tasks mainly about tidying up the room, e.g. Throwing away leftovers, we split the primitive actions into oppick and opplace, which is required to perform an action sequence to finish pickup or place subtask. Combined with our symbolic-grounding f, the agent will be able to distinguish when and where to pick and place. For tasks mainly about cleaning, e.g. Cleaning a car, we split the primitive actions into opclean and opput, which is required to finish washing or putting subtask. In addition, some tasks involve more operators, such as install a printer. We provide detailed illustrations of these representative state machine models."}, {"title": "D.3 Robotic Munipulation", "content": "\u30fbPacking star into the box\n\u30fbPutting-red plocks-in-green bowls\n\u30fbSeparating-piles\n\u30fbAssembling-kits"}]}