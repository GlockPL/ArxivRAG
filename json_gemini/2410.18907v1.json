{"title": "SkillMimicGen: Automated Demonstration Generation for Efficient Skill Learning and Deployment", "authors": ["Caelan Garrett", "Ajay Mandlekar", "Bowen Wen", "Dieter Fox"], "abstract": "Imitation learning from human demonstrations is an effective paradigm for robot manipulation, but acquiring large datasets is costly and resource-intensive, especially for long-horizon tasks. To address this issue, we propose SkillMimicGen (SkillGen), an automated system for generating demonstration datasets from a few human demos. SkillGen segments human demos into manipulation skills, adapts these skills to new contexts, and stitches them together through free-space transit and transfer motion. We also propose a Hybrid Skill Policy (HSP) framework for learning skill initiation, control, and termination components from SkillGen datasets, enabling skills to be sequenced using motion planning at test-time. We demonstrate that SkillGen greatly improves data generation and policy learning performance over a state-of-the-art data generation framework, resulting in the capability to produce data for large scene variations, including clutter, and agents that are on average 24% more successful. We demonstrate the efficacy of SkillGen by generating over 24K demonstrations across 18 task variants in simulation from just 60 human demonstrations, and training proficient, often near-perfect, HSP agents. Finally, we apply SkillGen to 3 real-world manipulation tasks and also demonstrate zero-shot sim-to-real transfer on a long-horizon assembly task.", "sections": [{"title": "1 Introduction", "content": "Imitation learning from human demonstrations is an effective approach for training robots to perform different tasks [1,2]. One popular technique is to have humans teleoperate robot arms to collect datasets for tasks of interest and then subsequently use the data to train robots to perform these tasks autonomously [3,4]. Recent efforts have demonstrated that large, diverse datasets collected by teams of human demonstrators result in impressive and robust robot performance, and even allow the robots to generalize to different objects and tasks [2, 5-8]. However, collecting large datasets in this way is costly and resource-intensive, often requiring multiple human operators, robots, and months of human effort. Acquiring datasets for challenging long-horizon tasks that require sequencing several manipulation behaviors together is even more difficult and costly [9].\nThe need for large datasets has motivated the development of data generation systems [10-12] that seek to produce task demonstrations with minimal human involvement. For example, some systems combine teleoperation and planning within the same demonstration, partially automating the demonstrating process, which ultimately allows a human to teleoperate several robots in parallel [13]. Alternatively, some systems further reduce human involvement through demonstration adaptation. For example, MimicGen [11], uses a small number of human task demonstrations to automatically generate large datasets by splitting the source human data into object-centric sequences of end-effector targets, and then selectively transforming and sequencing such segments in new settings. However, this and other naive strategies for composing human segments together can produce lower-quality demonstrations with unintended collisions in the environment, and have heterogeneous motions that are difficult for policy learning algorithms to learn from, especially in real-world settings.\nWe also seek to minimize the number of required human demonstrations but improve the flexibility and efficacy of adapted demonstrations. To that end, we first observe that control difficulty is often not uniformly spread across a task. Specifically, in order to solve many manipulation tasks, the robot must first move itself in free space in order to reach a state where it can manipulate the world through contact. For example, consider the cleanup task in Fig. 1. The robot must move through free space before picking the butter and also before inserting the butter into the trash can. This kind of free space motion can be easy for planning systems, and greatly reduce the burden on policy learning.\nFrom this observation, we propose SkillMimicGen (SkillGen), a system that leverages the notion of a manipulation skill to isolate demonstration adaptation to contact-rich segments. At data-generation time, SkillGen synthesizes candidate demonstrations by executing several adapted skill segments in sequence, connected through motion planning. At test-time, SkillGen not only learns control policies for these skills but also initiation and termination conditions, enabling them to be sequenced using planning in a similar manner but without any requirements regarding state observability.\nWe make the following contributions:\n\u2022 We introduce SkillMimicGen (SkillGen), an automated system for generating demonstration datasets through decomposing tasks into motion segments and skill segments that are adapted from a few human demos.\n\u2022 We propose a Hybrid Skill Policy (HSP) framework that learns skill initiation, control, and termination components, enabling skills to be combined in sequence at test time using motion planning.\n\u2022 We show that SkillGen improves data generation and policy learning performance over an existing state-of-the-art data generation framework. Specifically, SkillGen is robust to large scene variation, such as clutter, and produces policies that on average are 24% more successful than MimicGen [11].\n\u2022 We demonstrate the efficacy of SkillGen by generating 24K+ demonstrations from 60 human demonstrations across 18 task variants in simulation and training proficient, often near-perfect, high-performing HSP agents. Finally, we successfully apply SkillGen to 3 real-world manipulation tasks, and also demonstrate zero-shot sim-to-real transfer on a long-horizon assembly task."}, {"title": "2 Related Work", "content": "Data Collection for Robotics. Robot teleoperation [3, 4, 14\u201323] is a popular method for collecting task demonstrations \u2013 here, humans use a teleoperation device to control a robot and guide it through tasks. The robot sensor streams and control actions during operation are logged to a dataset. Several efforts [2, 5-8] have scaled this paradigm up by using a large number of human operators and robot arms over extended periods of time (e.g. months). Some works have also allowed for robot-free data collection with specialized hardware [24, 25], but human effort is still required for data collection. In contrast, SkillGen automatically generates data with just a handful of human demonstrations. Other works seek to generate datasets automatically using pre-programmed demonstrators in simulation [10, 26\u201331], but scaling these approaches to a larger variety of tasks can be difficult.\nImitation Learning and Data Augmentation. Behavioral Cloning (BC) [32] is a typical method for learning policies offline from demonstrations, and has been widely used in robot manipulation [3, 16, 27, 33-45]. Some works leverage offline data augmentation to increase the size of the training dataset for learning policies [1,46\u201357]. Instead, SkillGen collects new datasets online.\nImitation Learning with Hybrid Controllers. SayCan [6] composes skills learned from demonstrations using a language model and learns when to begin and end each skill However, each skill starts when the previous one ends \u2013 in contrast, our learned skills are local manipulation behaviors and transit is carried out via motion planning. Other works [58-60] learn \"keyframe\" pose actions from demonstrations and execute them using motion planning, but they lack closed-loop control using learned policies. Some imitation learning methods decompose learning into coarse-grained and fine-grained motions [13,61\u201364], but most use naive linear interpolation to carry out coarse-grained motions [61, 62], which is susceptible to collisions. Others [63\u201365] learn open-loop segments for fine-grained motions, instead of closed-loop skills like our methods. Wang et al. [66] learn parametric skills using Gaussian Processes and deploy them in a Task and Motion Planning (TAMP) [67] system. In HITL-TAMP [13], a TAMP planner decides when to employ an agent trained with imitation learning for skill segments; however, it is TAMP-gated, meaning that skill start and end conditions are engineered into the TAMP model instead of learned.\nMimicGen. MimicGen [11] is a data generation system that takes a small source set of human demonstrations on a task and generates larger sets of demonstrations. It builds on replay-based imitation learning methods [65, 68\u201374], which address new task instances by adapting and replaying motion from existing human data. MimicGen segments the source demonstrations into a contiguous set of object-centric subtask segments. Then, given a new task instance, MimicGen transforms and replays open-loop subtask segments from the source data one-by-one to generate a new demonstration. However, because MimicGen naively stitches source demonstrations with linear interpolation, it can produce lower quality demonstrations that collide with the environment, and have heterogeneous motions difficult for policy learning. By instead adopting a skill-based framework, SkillGen avoids these pitfalls at data generation time and produces more robust behavior at deployment time."}, {"title": "3 Prerequisites", "content": "Imitation Learning. Each robot manipulation task is modeled as a Partially Observable Markov Decision Process (POMDP). We are given a dataset of N demonstrations $D = \\{(s_0, o_0, a_0, s_1, o_1, a_1, ..., s_{h_\\tau}, o_{h_\\tau}, a_{h_\\tau})\\}_{i=1}^N$ consisting of states $s \\in S$, observations $o \\in O$, and actions $a \\in A$. Each initial state $s_0 \\sim D$ is sampled from the initial state distribution $D \\subset S$. We aim to learn a robot control policy $\\pi : O \\rightarrow A$ that maps observation space O to a distribution over action space A. Behavioral Cloning (BC) [32] is a common method to obtain such a policy - it uses optimization to find a policy that maximizes the likelihood of producing the data $\\arg \\max_\\theta E_{(s,o,a) \\sim D}[log \\pi_\\theta(a | o)]$. In this work, we train policies via BC and combine them with various mechanisms to exchange control between a learned policy and a motion planner.\nAssumptions. Similar to prior work [11], we make the following assumptions. (A1): The policy action space A consists of continuous pose commands for an end effector controller along with a discrete gripper command. This allows us to treat the actions in a human demonstration as a sequence of target poses for a task-space end-effector controller. (A2): The task involves a set of manipulable objects $\\{O_1, ..., O_k\\}$. (A3): During data collection, the pose of an object can be observed or estimated prior to the robot making contact with that object."}, {"title": "4 SkillMimicGen", "content": "We seek to learn visuomotor policies from demonstrations with minimal human effort by adapting a small number of human demonstrations to a large set of system states to facilitate automated demonstration generation. However, at both demonstration and deployment time, control difficulty is not uniformly spread across an episode. Specifically, in order to solve many manipulation tasks, the robot must first move itself in free space in order to reach a state where it can manipulate the world through contact. Free space motion can easily be carried out via motion planning and greatly reduce the policy learning burden. Thus, we propose decomposing tasks into motion and skill segments in order to isolate both demonstration generation and learning to just the skill segments, which will improve the quality of demonstrations and learned policies. We accomplish this by learning local manipulation skills that we combine in sequence using motion planning (Section 4.1). We show how adopting a skill-based framework allows for more focused demonstration replay (Section 4.4) and ultimately improved policy performance during deployment (Section 4.6)."}, {"title": "4.1 Skills Framework", "content": "Building off of the options [75] formalism from reinforcement learning, we define a skill $\\psi = (O, I, \\pi, T)$ as a tuple consisting of an object to be manipulated O, initiation condition I, policy $\\pi$, and a termination condition T. The initiation condition I defines a set of states where control using policy $\\pi$ can begin. The termination condition T defines a set of terminal states for policy $\\pi$. We will use this skill abstraction to model all three phases of SkillGen, namely the initial tele-operation demonstrations (Section 4.3), the automated demonstration adaptation and amplification (Section 4.4), and the system execution at deployment time (Section 4.6)."}, {"title": "4.2 Transit and Transfer Motion", "content": "Most tasks require performing multiple skills in sequence to complete them, such as the task in Fig. 2, which involves a pick skill to grasp the coffee pod and an insert skill load the pod in the coffee machine. In order to first reach the pick skill and then move the pod to the pod holder for the insert skill, the robot must perform two kinds of classical free-space motion [76,77]. The first is transit motion, where the robot moves by itself without modifying the world. The second is transfer motion, where the robot is grasping an object approximately rigidly and transports the object as it moves. Thus, at both demonstration generation (Section 4.4) and system deployment (Section 4.6) time, SkillGen alternates between transit or transfer motion and manipulation skills.\nSkillGen is a bilevel hierarchy where the skill initiation and termination induce the start and end robot configurations (q and $q^*$) for the motion segments. Namely, the termination condition $T_i$ from the prior skill $\\psi_i$ governs the robot configuration $q$ prior to the motion, and the initiation condition $I_{i+1}$ of the next skill $\\psi_{i+1}$ defines the set of target end-effector poses $T_h \\in I_{i+1} \\subseteq SE(3)$, where E is the end-effector frame and W is the world frame. To generate these motions, we first convert task-space pose $T^W_E$ to joint-space configuration $q^*$ using inverse kinematics and then plan and execute a joint-space path from current configuration q to $q^*$ with a motion planner."}, {"title": "4.3 Source Demonstrations", "content": "We assume a small source dataset of human demonstrations $D_{src}$ collected on the task and our aim is to automatically generate a large dataset D on either the same task or a task variant. We start by annotating each trajectory in the source dataset $T \\in D_{src}$ with the start and end of each skill. This decomposes the demonstration into an alternating sequence of motion and skill trajectories $T = (T_{1m}, T_{1s}, \u2026\u2026\u2026, T_{Nm}, T_{Ns})$, where $T_{im}$ and $T_{is}$ denote motion and skill segments respectively. For source demonstrations provided by conventional teleoperation, these annotations can easily be annotated by a human. In our experiments, we choose to use demonstrations from the HITL-TAMP system [13], where the human only demonstrates local skill segments of each task, and the rest is handled by a TAMP system. In this case, annotations can be extracted automatically \u2013 each $T_{im}$ and $T_{is}$ is a TAMP and human segment respectively. Within each skill segment $T_{is}$, each end-effector pose action $T_E^a$ (Sec. 3, A1) is stored in the frame of skill object $O_i$ as $T^a_{E^W} \\leftarrow (T_{O_i^W})^{-1} T_{E^W}^A$, where"}, {"title": "4.4 Demonstration Generation", "content": "The demonstrations D are generated through an automated trial-and-error process. Given a new initial state, SkillGen adapts existing skill segments to the new initial state and executes them in sequence with motion segments to generate a new demonstration. First, a reference skill segment $T_{is}$ is sampled. Next, the corresponding initiation state $T_{E_0^W}$ is used along with the pose $T_{O_i^W}$ of object $O_i$ in the new scene to obtain an end-effector pose for where the new skill segment should start, $T_{E_0^W} \\leftarrow T_{O_i^W} T_{O_i^W} T_{E_0^W}$. Next, the reference skill segment, expressed as a sequence of end-effector pose actions, $T_{is} = (T_E^{a_0}, ..., T_E^{a_K})$ is transformed to $t_{is} = ((T_{O_i^W})^{-1} T_E^{a_0}, ..., (T_{O_i^W})^{-1} T_E^{a_K}))$ where $T_E^{a_t} \\leftarrow T_{O_i^W} T_E^{a_t}$. This transformation preserves the new end-effector pose actions with respect to the object frame [11]. The new skill segment $t_{is}$ is executed by the end-effector controller. The steps above repeat for each skill, and then SkillGen checks for task success and only keeps the demonstration if it was successful."}, {"title": "4.5 Initiation Augmentation", "content": "At test time, learned skills trained on the generated data will be responsible for predicting both initiation targets for the motion planner and skill segments by employing a closed-loop agent that decides when to terminate. However, small differences in target pose predictions as well as motion plan tracking errors can cause learned policies to start out-of-distribution, thus reducing their accuracy. To mitigate such issues, SkillGen optionally adds noise to initiation states $T_{E_0^W}$, producing new initiation states $T_{E_0^W} \\leftarrow T_{E_0^W}T_{E_0^W}$, during data generation to broaden the support of the initiation set. To account for changing the initiation state, we consequently plan a recovery segment at the start of $T_{is}$, consisting of a sequence of pose actions that moves from new $T_{E_0^W}$ pose to the original pose $T_{E_0^W}$. This ensures that the new initiation state $T_{E_0^W}$ is connected to the demonstration segment $t_{is}$ when training closed-loop skill policies."}, {"title": "4.6 Policy Learning", "content": "Hybrid Skill Policy (HSP): We learn parameterized skills $\\psi_\\theta = (O, I_\\theta, \\pi_\\theta, T_\\theta)$ using the generated datasets (parameterized by $\\theta$). The initiation condition $I_\\theta : O\\rightarrow SE(3)$ is trained to predict initiation states $T_E^W$ from the last observation o on the prior skill. The policy $\\pi_\\theta : O\\rightarrow A$ is trained on direct observation and action pairs (o, a) with BC (see Sec. 5). The termination condition $T_\\theta : O\\rightarrow\\{0,1\\}$ is a classifier that predicts whether the skill is at a termination state based on the most recent observation o. During task deployment (Fig. 2), for each skill $\\psi_\\theta \\in \\Psi$ in a given sequence of skills $\\Psi$, SkillGen predicts the initiation state $T_{E_0^W} \\leftarrow I_\\theta(o)$, plans and executes a path to it using a motion planner, and rolls out the learned policy by predicting actions $a \\leftarrow \\pi_\\theta(o)$ until $T_\\theta(o)$ predicts policy termination. Then, this process repeats with the next skill (pseudocode in Appendix O).\nHSP Variants: We consider two approaches for learning initiation conditions $I_\\theta$: HSP-Reg and HSP-Class. HSP-Reg formulates learning as a regression problem and directly predicts an initiation pose from the last observation. HSP-Class frames learning as classification problem over the initiation states in the source dataset $D_{src}$, where the classifier predicts which source demonstration spawned the generated demonstration. Once classified, HSP-Class adapts the predicted initiation state to the current state using the pose adaptation procedure previously described in Section 4.4.\nHowever, recall that this requires the current pose $T_{O_i^W}$ of object O, and thus HSP-Class assumes that object poses are known or can be estimated at the start of each skill segment. Ultimately, HSP-Class requires an additional observability assumption over HSP-Reg; however, this enables HSP-Class to perform discrete prediction over known pose candidates instead of continuous prediction over SE(3). Finally, we also consider HSP-TAMP, which deploys just the learned policies $\\pi_\\theta$ within HITL-TAMP [13], without the learned initiation and termination conditions."}, {"title": "5 Experiment Setup", "content": "Tasks and Task Variants. We applied SkillGen to a broad range of tasks (see Fig. 3, full details in Appendix J) and task variants. Each task has a nominal reset distribution ($D_0$), and broader, more challenging reset distributions ($D_1, D_2$) [11]. All simulation tasks are implemented in robosuite [78] using its MuJoCo backend [79]. We experiment on simulated Fine-Grained Tasks (Square, Threading, Coffee, Piece Assembly) that require insertion, pulling, and pushing as well as Long-Horizon Tasks (Nut Assembly, Coffee Prep) that require chaining multiple behaviors together. Additionally, we experiment on Real-Robot Tasks (Pick-Place-Milk, Cleanup-Butter-Trash, Coffee), and Sim-to-Real Tasks (Nut-Assembly-Sim, Nut-Assembly-Real) to investigate SkillGen's propensity for zero-shot sim-to-real policy deployment.\nData Generation and Imitation Learning. For most of the experiments, a source dataset of 10 demonstrations was collected for each task on the $D_0$ variant by a single human operator using the HITL-TAMP teleoperation system [13]. SkillGen was used to generate 1000 successful demonstrations for each task variant ($D_0, D_1, D_2$) (see Appendix J for details), using each task's source dataset. Motion augmentation (Sec. 4) is only used to generate data to train HSP-Reg agents; HSP-TAMP and HSP-Class agents are trained on datasets generated without motion augmentation. See Appendix H for full policy learning details. The agent control policies used in the hybrid control policies ($\\pi_\\theta$) were trained using BC with an RNN architecture [1] with the same hyperparameters from MimicGen. Policy performance is reported as the maximum success rate across all policy evaluations as in Mandlekar et al. [1]. All agents are trained with front-view and wrist-view RGB observations along with robot proprioception. Apart from the new task variants, we report the baseline data generation and agent performance statistics present in the MimicGen paper [11].\nMotion Planning. In both the simulation and real-world tasks, we use TRAC-IK [80] for inverse kinematics, RRT-Connect [81] for joint-space motion planning, and Operational-Space Control (OSC) for task-space control [82]. In simulation, we check collisions during planning using the ground-truth obstacle collision geometries. In the real world, because collision geometries are not known, we use point-cloud-based collision checking using the segmented point cloud."}, {"title": "6 Experiments", "content": "6.1 SkillGen Features\nSkillGen improves data generation rates over MimicGen substantially. MimicGen uses replay-based data generation for the entire trajectory, while SkillGen only uses replay for short skill segments, deferring larger transit motions to a motion planner. This results in substantially higher data generation success rates compared to MimicGen (average 75.4% vs. 40.7%, see Appendix F), especially when the reset distribution is large compared to the source demonstrations. Some compelling examples include Square $D_2$ (87.7% vs. 31.8%), Threading $D_2$ (74.3% vs. 21.6%), Three Piece Assembly $D_2$ (69.3% vs. 31.3%), and Coffee $D_2$ (70.0% vs. 27.7%).\nSkillGen data collection is robust to large object rearrangements and clutter. In Coffee Prep $D_2$, the drawer containing the coffee pod and the mug are on opposite ends of the table compared to $D_0$ (source demos), and MimicGen is unable to collect any demonstrations while SkillGen achieves 59.9% data generation success. Additionally, in the Clutter variants of Square and Coffee (Ap-"}, {"title": "7 Limitations", "content": "SkillGen requires knowledge of a fixed sequence of skills that can complete a task. It assumes that object poses can be observed at the start of each skill segment during data generation. SkillGen was demonstrated on quasi-static tasks involving rigid objects. SkillGen produces the best results when using source human demonstrations collected with the HITL-TAMP system \u2013 improving results with conventional teleoperation is left for future work. In the sim-to-real experiment, the agents had limited observability. Namely, agents only observe changes in proprioception, as no pose tracking or visual observations are used during execution."}, {"title": "8 Conclusion", "content": "We introduced SkillGen, a data generation system that synthesizes large datasets by adapting select skill segments from a handful of human demonstrations, and a Hybrid Skill Policy (HSP) learning framework to learn from the generated datasets by enabling closed-loop skills to be sequenced using a motion planner. We showed that SkillGen improves over a state-of-the-art data generation system, in both data generation capability and the ability to learn proficient agents from the data. We demonstrated SkillGen on real-world manipulation tasks, including zero-shot sim-to-real transfer."}, {"title": "C Limitations", "content": "We discuss limitations of SkillGen that can inform future work, extending Section 7.\n1. Given sequence of skill segments during data generation. During data generation, the sequence of skill segments (relevant objects that must be manipulated by the robot during each skill) must be provided.\n2. Object pose estimates during data generation. During data generation, SkillGen assumes access to the object pose at the start of each skill segment, either by direct observation (simulation) or estimation (real world).\n3. Quasi-static tasks with rigid objects. This paper applies SkillGen to primarily quasi-static tasks with rigid objects.\n4. Better performance when using source human data from HITL-TAMP [13] than from conventional teleoperation systems. SkillGen obtains better results when using human demonstrations collected with HITL-TAMP than with conventional teleoperation systems (Appendix L). Investigating how more consistent human annotations can reduce this gap is future work.\n5. Limited agent observability and action space for sim-to-real experiments. Agents used in the sim-to-real experiments only observe changes in robot proprioception, as no pose tracking or visual observations are used during execution. The agent also receives object poses at the start of each episode, but these are never updated. The action space is restricted to position-only control (no rotation). These design choices were made to maximize the possibility of transfer without the need for addressing the gap in perception between simulation and the real world, and without the need for extensive robot controller tuning between simulation and the real world."}, {"title": "D Analysis on Challenging Data Generation Scenarios", "content": "In this section, we discuss some challenging data generation scenarios where SkillGen is able to generate data, while MimicGen struggles. We first review some limitations of MimicGen, and then we discuss different data generation scenarios.\nD.1 MimicGen Limitations\nSusceptibility to scene collisions. MimicGen uses a naive linear interpolation scheme during data generation to connect the end of one transformed object-centric human segment to another one. This approach is not aware of scene geometry, which can result in data generation failures due to collisions between the robot and other objects in the scene. By contrast, SkillGen transit and transfer motions between skill segments are carried out via motion planning.\nTradeoff between Data Generation Quality and Policy Learning Proficiency. The use of naive linear interpolation also impacts learning ability. Longer in time (not space) interpolation segments have been shown to be harmful to policies trained from MimicGen data [11], which motivates the use of short interpolation segments with a small number of intermediate waypoints. However, this can lower the data generation success rate, since the end-effector controller might not be capable of accurately tracking waypoints that are far apart, and this also can be unsafe for real-world deployment. Consequently, MimicGen has a fundamental tradeoff with respect to interpolation segments. On one hand, shorter segments are better for policy learning but can result in lower data generation success rates and be unsafe for real-world deployment. On the other, longer segments are more suitable for real-world deployment and for ensuring better data generation throughput but make policy learning more difficult. By contrast, SkillGen has no such tradeoff.\nD.2 Challenging Data Generation Scenarios\nPresence of Clutter. SkillGen successfully generates data for scenes with large obstacles, unlike MimicGen. We develop variants of the Square and Coffee tasks that have a large obstruction placed in the workspace (Fig. D.1). The reset distributions for these tasks are identical to their clutter-free counterparts described in Appendix J except for the presence of the obstruction, which has its own reset distribution, and is placed randomly near the center of the workspace. We use the same source demonstrations as before (collected on the clutter-free Do variants of these tasks) and perform 200 data generation attempts with both SkillGen and MimicGen. The data generation success rates are presented in Table D.1. We see that SkillGen substantially outperforms MimicGen by margins as large as 58.5%.\nLarge Scene Variations from Source Demos. SkillGen excels at generating data even when there are substantial deviations from where objects were located in the source human demonstrations unlike MimicGen, which suffers from having to use short linear interpolation segments during generation. For example, MimicGen is unable to produce any data on Coffee Prep D2, due to the mug and drawer being on opposite sides of the table compared to the source demos (Do) (see Fig. D.2), while SkillGen can generate data and train proficient agents on D2 (Fig. 4). SkillGen also enjoys large gains over MimicGen for data generation rates, especially on D2 task variants, which vary substantially from Do, where source data was collected."}, {"title": "E Dataset Scaling Law Analysis", "content": "We present results for using different amounts of SkillGen data for policy training, to see how policy success rate scales with amount of data. We present results in Table E.1 (for HSP-TAMP), Table E.2 (for HSP-Class) and Table E.3 (for HSP-Reg). HSP-Reg uses SkillGen with initiation augmentation (Sec. 4.5). All tasks and methods receive a significant increase from 200 to 1000 demos, and some tasks benefit strongly from 1000 to 5000 demos, notably Square D2 (52% to 72% on HSP-Reg) and Threading D1 (60% to 76% on HSP-Reg)."}, {"title": "G Data Generation Details", "content": "In this section, we provide more details on SkillGen data generation. We first describe how reference skill segments are selected, and how they are transformed and executed. We next describe how initiation augmentation can be used to produce more robust closed-loop agents. Finally, we describe how we leveraged parallelization to generate large datasets efficiently with reasonable wall clock times, even when data generation rates were low.\nG.1 Reference Skill Segment Selection\nDuring a data generation attempt, SkillGen adapts existing skill segments to the new scene and executes them sequentially with motion segments (Sec. 4.4). To generate a new skill segment (for skill index i for a task), SkillGen requires a reference skill segment $T_{is}$ to be selected from the source demonstrations $D_{src}$. Since the skill index should match between the source demonstrations and the current skill segment that must be generated, this problem reduces to selecting a source demonstration index $j \\in \\{1,2, ..., N\\}$. In our experiments, we sample this index randomly for the first skill segment, and then leave it fixed for the rest of the episode. However, more sophisticated selection methods could be used to select a different source demonstration index for each skill index if desired.\nG.2 Skill Segment Execution and Action Noise\nDuring a data generation attempt, after an existing skill segment is selected and transformed to obtain a new sequence of end-effector pose actions $\\tau_{is} = ((T_{O_i^W})^{-1} T_E^{a_0}, ..., (T_{O_i^W})^{-1} T_E^{a_K}))$ (Sec. 4.4), this sequence of actions is executed one by one. However, we found it beneficial to apply additive noise to the pose actions. As in MimicGen, we convert each absolute pose action to a normalized delta pose action (using the current robot end effector pose) and add Gaussian noise N(0, 1) with magnitude $\\sigma$ in each dimension, where $\\sigma = 0.05$. Note that the gripper actuation actions are copied as-is from the source skill segment, and no noise is added. These modified normalized delta pose actions are then executed, and stored in the generated dataset.\nG.3 Initiation Augmentation\nAs described in Sec. 4.5, SkillGen has the option of adding noise to the skill initiation states $T_{E_0^W}$, producing new initiation states $T_{E_0^W} \\leftarrow T_{E_0^W}T_{E_0^W}$, to broaden the support of the initiation set and allow the trained closed-loop skill policies to be more robust to incorrect initiation pose predictions. We found this to be very helpful for HSP-Reg agents, which must directly predict initiation poses via regression. Consequently, all of our HSP-Reg agents are trained on datasets with initiation augmentation, unless otherwise noted.\nFor datasets generated with initiation augmentation, we add uniform translation noise to the target position for each initiation state U[-t, t], where t is the position noise scale. We also modify the target rotation, by sampling a random rotation axis (random vector on 3D unit sphere), sampling a random angle $\\phi \\sim U[0, r]$, converting the new sampled axis-angle rotation to a rotation matrix, and multiplying the target rotation by this rotation matrix. The motion planner will attempt to reach the new target pose, and then we will subsequently plan and execute a recovery segment consisting of a sequence of pose actions that moves from new pose $T_{E_0^W}$ to the original pose $T_{E_0^W}$. The recovery segment is added to the transformed skill segment, and is part of the dataset used to train the closed-loop agent.\nIn our experiments, we chose t = 0.08 meters and r = 80 degrees. We note that this is a very wide and aggressive pose randomization distribution, and that a large portion of sampled poses will be unreachable by the motion planner, due to the pose being in collision with the scene. This is why the data generation rates for the initiation augmentation datasets are significantly lower (Appendix F). This could be addressed with more intelligent sampling mechanisms, but we leave this for future work. Instead, opted to leverage parallelization during data generation to efficiently generate large datasets in a reasonable amount of wall clock time (described below).\nG.4 Efficient Data Generation with Parallelization\nDatasets generated with initiation augmentation can have low data generation rates due to the broad noise distribution and rejection sampling process used. To mitigate this, we parallelized data collection across a large number of cpu processes. The SkillGen data generation process is easily amenable to this type of parallelization.\nG.5 Hardware\nData generation runs were batched together and run simultaneously (on a compute cluster) on 8-GPU nodes consisting of 8 NVIDIA Volta V100 GPUs, 64 CPUs, and 400GB of memory. Real robot experiments were run on a machine with an NVIDIA GeForce RTX 3090 GPU, 36 CPUs, 32GB of memory, and 1 TB of storage."}, {"title": "P Comparison with HITL-TAMP", "content": "As we described in Section 2, HITL-TAMP [13"}]}