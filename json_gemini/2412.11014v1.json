{"title": "PromptV: Leveraging LLM-powered Multi-Agent Prompting for High-quality Verilog Generation", "authors": ["Zhendong Mi", "Renming Zheng", "Haowen Zhong", "Yue Sun", "Shaoyi Huang"], "abstract": "Recent advances in agentic LLMs have demonstrated remarkable automated Verilog code generation capabilities. However, existing approaches either demand substantial computational resources or rely on LLM-assisted single-agent prompt learning techniques, which we observe for the first time has a degeneration issue characterized by deteriorating generative performance and diminished error detection and correction capabilities. This paper proposes a novel multi-agent prompt learning framework to address these limitations and enhance code generation quality. We show for the first time that multi-agent architectures can effectively mitigate the degeneration risk while improving code error correction capabilities, resulting in higher-quality Verilog code generation. Experimental results show that the proposed method could achieve 96.4% and 96.5% pass@10 scores on VerilogEval Machine and Human benchmarks, respectively while attaining 100% Syntax and 99.9% Functionality pass@5 metrics on the RTLLM benchmark.", "sections": [{"title": "1 Introduction", "content": "As semiconductor technology advances to smaller process nodes (7nm, 5nm, 3nm, and beyond), electronic design automation (EDA) faces increasing challenges due to the escalating design complexity, increasing human resource constraints, and intensifying time-to-market pressure. Hardware description language (HDL) code generation, as a fundamental EDA task, particularly exemplifies these challenges. In recent years, large language models (LLMs) have captured extensive attention due to their significant performance across various tasks and have emerged as a promising solution, demonstrating significant potential in automating various EDA tasks, especially in HDL code generation where traditional approaches struggle to scale\nWhile effective, existing approaches either utilize computational resources intensive learning paradigm (e.g., pretraining, fine-tuning, instruct-tuning) or employ LLM-assisted single-agent prompt learning techniques where the model selectively performs code generation, self-execution, and self-correction in sequence. However, we observe for the first time the single-agent prompt learning has a degeneration issue characterized by deteriorating generative performance and diminished error detection and correction capabilities.\nIn this work, we introduce PromptV, which leverages LLM-powered multi-agent prompt learning for high-quality verilog generation. Instead of utilizing single-agent learning for both code generation and error correction, PromptV utilizes a multi-agent architecture to reduce the degeneration issue. In promptV, multiple LLM agents are employed for different tasks, e.g., code generation, testbench generation, error correction suggestion generation, code correction, and testbench correction. Moreover, we integrate the teacher-learner learning mechanism in the framework: a teacher agent that identifies"}, {"title": "2 Methodology", "content": ""}, {"title": "2.1 Task Description", "content": "We aim to obtain syntactically and functionally accurate verilog code based on initial design information including problem description and module headers. In general, we have three sub-tasks: 1) Verilog code and testbench generation; 2) code and testbench error correction suggestions generation; 3) code and testbench error correction. In this paper, we propose a LLM-powered interactive multi-agent prompt learning framework that includes diverse LLM agents responsible for the above tasks. Moreover, we study the effectiveness of proposed method in code generation with ChatGPT3.5 and ChatGPT4 on VerilogEval and RTLLM dataset."}, {"title": "2.2 Interactive Multi-agent Learning for Verilog Generation", "content": "Our LLM-based multi-agent learning pipeline conducts an interactive code repairing workflow, which stimulates the collaboration among these agents and Iverilog simulation, as shown in Figure 1. We incorporate different agents in the framework: Code-generation agent, testbench generation agent, teacher agent providing error correction suggestions for code and testbench, and dual learner agents correcting errors in code and testbench following teachers suggestions. More specifically, our framework consists of the following stages: 1 Verilog module code completion and testbench generation: Our framework requires two essential components as inputs: the module description and the module header. With these inputs, the LLM generates Verilog code that adheres to the specified interface requirements and functional constraints. Subsequently, we employ AutoBench for testbench generation and function verification. Moreover, we use Iverilog as the compiler for Verilog code and the corresponding testbench. 2 Teacher agent providing error correction suggestions for both code and testbench: The teacher agent automatically analyzes the exsiting errors including syntax errors, logical inconsistencies, and testbench verification issues, etc. Based on the thorough analysis, the teacher agent generates specific correction suggestions to effectively address the identified bugs, ensuring compliance with hardware description language specifications"}, {"title": "3 Verilog code and testbench error correction:", "content": "Two learner agents are employed in parallel for error correction of code and testbench, individually. The correction suggestions from the teacher agent will be delivered to the learner agents. The parallel nature of this process, enables simultaneous and efficient refinement of both components without cross-interference, thereby enhancing the overall efficiency and reliability of the error correction process. 4 Iverilog simulation: After the correction of code and testbench, we employ Icarus Verilog to simulate of the code and testbench. The simulation results will be used to validate the effectiveness of the modifications. 5 New round of error correction: If a code or its corresponding testbench fail to pass the simulator, the system initiates a correction cycle. The errorneous code and testbench are returned to the teacher agent, which performs the next round of corrections. New cycle of code and testbench generation: However, if the code of a given module remains error after three rounds, a new cycle of code regeneration will be triggered and the initial module information along with the error messages generated during the simulation, will be delivered to the code generation agent."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Experimental Setup", "content": ""}, {"title": "3.1.1 Settings", "content": "In our experiment, we compare our method with two types of baselines, (i) general-purpose foundation models, including GPT-3.5, GPT-4 and Claude-3, alongside three open-source models designed for code generation, namely CodeLlama-7B-Instruct, DeepSeek-Coder-6.7B-Instruct, and CodeQwen-1.5-7B-Chat; (ii) domain-specific fine-tuned models, such as ChipNeMo, RTLCoder, BetterV, and CodeV. We use pass@k as the evaluation metric for the performance of language models in generating Verilog code, which is formulated as\npass @k := $\\frac{1}{n} \\sum_{i=1}^{n}(1 - (1 - \\frac{c_i}{n_i})^k)$, where $n$ represents the total number of generated solutions for each problem, $c$ refers to the number of correct solutions for a given problem and $k$ is the number of solutions from the model that are evaluated. We set $n = 20$ in our experiments."}, {"title": "3.2 Experimental Results", "content": ""}, {"title": "4 Conclusion", "content": "We propose PromptV, an interactive multi-agent framework powered by LLMs for high-quality verilog code generation. PromptV comprises multiple specialized LLM agents, each dedicated to a specific task, effectively mitigating the degeneration issue in traditional single-agent approaches. Moreover, the effectiveness of the framework is further enhanced through the integration of teacher-learner mechanism. Our comprehensive empirical evaluation across various benchmarks demonstrates the effectiveness of proposed method in both syntax and functionality for RTL code generation, which sheds light on the automation of EDA tasks with the power of LLMs."}]}