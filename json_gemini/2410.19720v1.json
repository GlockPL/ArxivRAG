{"title": "2D-DPO: Scaling Direct Preference Optimization with 2-Dimensional Supervision", "authors": ["Shilong Li", "Yancheng He", "Hui Huang", "Xingyuan Bu", "Jiaheng Liu", "Hangyu Guo", "Weixun Wang", "Jihao Gu", "Wenbo Su", "Bo Zheng"], "abstract": "Recent advancements in Direct Preference Optimization (DPO) have significantly enhanced the alignment of Large Language Models (LLMs) with human preferences, owing to its simplicity and effectiveness. However, existing methods typically optimize a scalar score or ranking reward, thereby overlooking the multi-dimensional nature of human preferences. In this work, we propose to extend the preference of DPO to two dimensions: segments and aspects. We first introduce a 2D supervision dataset called HelpSteer-2D. For the segment dimension, we divide the response into sentences and assign scores to each segment. For the aspect dimension, we meticulously design several criteria covering the response quality rubrics. With the 2-dimensional signals as feedback, we develop a 2D-DPO framework, decomposing the overall objective into multi-segment and multi-aspect objectives. Extensive experiments on popular benchmarks demonstrate that 2D-DPO performs better than methods that optimize for scalar or 1-dimensional preferences.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Language Models (LLMs) have shown impressive performance across a wide range of tasks (Zhao et al., 2023; Bai et al., 2024; Wu et al., 2024a; Li et al., 2024a). A pivotal component in LLM training is Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Bai et al., 2022), which aligns LLMs with human preferences. However, due to its complexity, traditional RLHF often leads to challenges such as training instability and reward collapse (Wolf et al., 2023; Song et al., 2023).\nDirect Preference Optimization (DPO) (Rafailov et al., 2023), as a simpler and more effective alternative, has gained considerable attention due to its ability to bypass the need for explicitly fitting a reward model (Meng et al., 2024; Ethayarajh et al., 2024). However, most existing DPO-style approaches rely on scalar scores or rankings and ignore the multi-dimensional nature of human preferences, resulting in inefficient and imprecise optimization. For instance, a response may be deemed satisfactory under one aspect such as correctness, but falls short in another such as clarity. Moreover, not all segments of a response should be treated uniformly; even in a preferred response, there may be segments of inferior quality. This underscores the need for a more nuanced approach that recognizes the multi-dimensionality of feedback and its critical impact on model training.\nIn response, some recent works have attempted to leverage signals that are believed to reflect the importance of individual segments as reward scores (Zeng et al., 2024; Chan et al., 2024; Jiang et al., 2024; Chen et al., 2024). However, these signals are often derived from statistical features such as edit distance or confidence estimation, which can introduce noise and lack interpretability. Other approaches incorporate multi-objective optimiza-"}, {"title": "2 Related Work", "content": "tion to balance different aspects of human preferences (Wu et al., 2024b; Guo et al., 2023; Cao et al., 2024). However, these methods mostly rely on Proximal Policy Optimization (PPO) (Schulman et al., 2017), which is prone to instability during training. Furthermore, these efforts only extend preference optimization from 0-dimensional (scalar reward) to 1-dimensional (aspect/segment) supervision, which remains insufficient for capturing the complexity of real-world human preferences.\nTo better address the intricacy of human preferences, we propose 2D-DPO, a novel direct alignment strategy that enables 2-dimensional (2D) fine-grained optimization. Our core idea is to scale supervision signals across two dimensions: segments and aspects. To this end, we first construct a preference dataset called HelpSteer-2D, where each sample is annotated with a 2-dimensional score matrix evaluating each segment across multiple aspects. These signals are derived from a robust model guided by a set of stringent principles, ensuring the generation of highly accurate and interpretable supervision signals. Building on this, we propose a novel approach to achieve 2-dimensional direct preference alignment. Experimental results on three public benchmarks demonstrate that 2D-DPO significantly outperforms previous methods. In summary, our main contributions are threefold:"}, {"title": "2.1 Preference Optimization", "content": "Large language models (LLMs) have advanced rapidly, with reinforcement learning from human feedback (RLHF) commonly used to align LLMs with human preferences (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022; Liu et al., 2024b,a; Peng et al., 2023; Feng et al., 2022). However, traditional RLHF methods face challenges like instability and high resource"}, {"title": "2.2 Token-level Preference Optimization", "content": "The response-level rewards in naive PPO and DPO often lack token-level details. To address this, researchers have explored fine-grained supervision signals in three ways: (1) Human annotation: Methods like PRM (Lightman et al., 2023) and FGRLHF (Wu et al., 2024b) involve human annotators labeling each segment of the response to generate fine-grained signals. (2) LLM annotation: To reduce the cost of human labeling, stronger LLMs are used to generate preference pairs with minimal edits (Guo et al., 2023; Chen et al., 2024; Yoon et al., 2024; Jiang et al., 2024) or to identify positive and negative response segments (Cao et al., 2024). (3) Internal signal: Some works use the model's internal information as reward signals, such as using attention scores for token rewards in ABC (Chan et al., 2024) or decomposing DPO's response-level rewards into token-level signals in SePO (Yang et al., 2024b; Rafailov et al., 2024). TDPO (Zeng et al., 2024) achieves token-level alignment by controlling the KL divergence for each token."}, {"title": "2.3 Multi-objective Preference Optimization", "content": "Human preferences are often complex, diverse, and even contradictory, making single-dimensional training insufficient. To address this, some studies align LLMs with multiple objectives by either training separate reward models and averaging their outputs (Pan et al., 2023; Ji et al., 2024; Rame et al., 2024; de Langis et al., 2024; Wang et al., 2024a). However, this approach demands significant computational resources. In contrast, MODPO (Zhou et al., 2024) offers a simpler, reinforcement learning-free method for optimizing multiple objectives. RiC (Yang et al., 2024c) and"}, {"title": "3 Approach", "content": "In this section, we provide a detailed introduction to our 2D-DPO approach for aligning LLMs. We first describe the construction of the 2D preference dataset in Section 3.1. Then, we present an enhanced 2D-DPO formulation that integrates the 2D supervision signals into the alignment process in Section 3.2. The complete pipeline of our method is illustrated in Figure 2."}, {"title": "3.1 Preference Dataset with 2D Signal", "content": "In general, a preference optimization dataset, denoted as $D = \\{x^i, y_w^i, y_l^i \\}_{i=1}^N$, comprises prompts $x$ along with a chosen response $y_w$ and a rejected response $y_l$, where $y_w$ is of higher quality compared to $y_l$. Such datasets are commonly used to train reward models (e.g., PPO) or directly for model alignment (e.g., DPO). However, differentiating between the chosen and rejected responses based on a scalar score is often coarse and imprecise. The quality of responses can vary significantly across different evaluation aspects, and even a chosen response might contain segments of low quality, while a rejected response could include segments of high quality. Therefore, relying solely on a scalar score for optimization may restrict the model's ability to effectively align with human preferences.\nTo address this issue, we propose a fine-grained scoring approach that decomposes the scalar scores of model responses to segment-level and aspect-level. The first step is to divide the response into segments, and the choice of segment length is crucial for ensuring the effectiveness of the fine-grained optimization. Segments that are too long cannot resolve the aforementioned coarse scoring issues, while segments that are too short pose difficulties for accurate assessment. Therefore, we choose the sentence as the scoring unit, which can strike a balance between scoring accuracy and the clarity of segment preferences.\nAfter segmenting responses based on typical sentence-ending punctuations, we employ GPT-4 to perform aspect-level scoring. Following HelpSteer2 (Wang et al., 2024b), we annotate the preference data across five key aspects: Helpfulness, Correctness, Safety, Completeness, Clarity. The first three aspects are independent of different sentences. The aspect of Completeness gen-"}, {"title": "3.2 2D-DPO", "content": "While the construction process of 2D signals is straightforward, integrating them effectively into the alignment process presents significant challenges. Previous approaches mostly utilize these signals as a scalar reward by weighted summation, which is insufficient for enabling the model to distinguish between varying quality across different dimensions. To address this issue, we propose a novel alignment method called 2D-DPO.\nVanilla DPO. Direct Preference Optimization (DPO) (Rafailov et al., 2023), as one of the most popular alignment methods, proposes a direct optimization objective that satisfies the optimal preference policy without using a reward model:\n$L_{DPO}(\\pi_{\\theta}; \\pi_{ref}) = - E_{(x,y_w,y_l) \\sim D}  \\left[  \\log \\sigma \\left(  \\beta \\left(  \\log \\frac{\\pi_{\\theta}(y_w | x)}{\\pi_{ref}(y_w | x)} - \\log \\frac{\\pi_{\\theta}(y_l | x)}{\\pi_{ref}(y_l | x)} \\right)  \\right)  \\right]$,\nwhere $\\pi_{\\theta}$ and $\\pi_{ref}$ represent the policy model and the reference model, respectively. DPO can fundamentally be viewed as a multi-armed bandit problem, where the model's entire response is treated as a single arm. According to Rafailov et al. 2023, in the token-level Markov Decision Process (MDP), the language model's Bradley-Terry preference"}, {"title": "4 Experiments", "content": "4.1 Set-up\nBenchmark. Our method has been tested on three wildly recognized instruction-following benchmarks: Arena-Hard (Li et al., 2024b), AlpacaEval 2.0 (Dubois et al., 2024) and MT-Bench (Zheng et al., 2023). Each benchmark comprises a diverse set of queries, and the answers are evaluated under the framework of LLM-as-a-Judge (Zheng et al., 2023). We use gpt-4-turbo-2024-04-09 as the judge model, and scores are reported following each benchmark's protocol.\nModel. Our method is validated on two models, Qwen2-7B-Instruct (Yang et al., 2024a) and Llama-3-8B-Instruct (AI@Meta, 2024). It deserves to be noticed that both models have undergone extensive instruction-tuning processes, therefore we directly perform preference optimization.\nBaseline. We mainly compare our method with 0-dimentional preference optimization methods:\n\u2022 DPO (Rafailov et al., 2023). This method leverages a mapping between reward functions and optimal policies to optimize the preference with a single stage of policy training.\n\u2022 IPO (Azar et al., 2024). This method propose a theoretically grounded approach method to replace pairwise preferences in DPO with pointwise rewards.\n\u2022 KTO (Ethayarajh et al., 2024). This method proposes to directly maximize the utility of generations from non-paired data.\n\u2022 ORPO (Hong et al., 2024). This method leverages a reference model-free monolithic odds ratio for contrasting favored and disfavored styles during SFT stage.\n\u2022 SimPO (Meng et al., 2024). This method proposes to use the average log probability of a sequence as the implicit reward, which eliminates the need for a reference model.\nWe also compare our method with the following 1-dimensional preference optimization method:"}, {"title": "4.2 Main Results", "content": "\u2022 TDPO (Zeng et al., 2024). This method proposes to control the KL divergence constraints for each token, aiming to strike a better balance between alignment and diversity.\nOur method is evaluated under two configurations 1D-DPO and 2D-DPO. 1D-DPO only incorporates a single aspect (helpfulness) as signals while 2D-DPO uses all five aspects.\nTraining. We perform preference optimization based on HelpSteer-2D, which is constructed based on HelpSteer2 (Wang et al., 2024b). We leverage gpt-4o-2024-05-13 to generate 2-dimensional scores which align with our requirements.\nTo make a fair comparison, all methods are combined with SFT loss with a coefficient of 0.1 except for ORPO. The other hyper-parameters are tuned to achieve an optimal performance for each method. Please refer to Appendix B.1 for more details.\nThe primary results are shown in Table 1. As can be seen, our proposed 2D-DPO outperforms existing methods across all three benchmarks, verifying the significance of 2-dimensional supervision in preference optimization. While previous methods mostly treat different segments uniformly with a"}, {"title": "4.3 Detailed Analysis", "content": "also underperforms both 1D-DPO and 2D-DPO, which can be traced back to the design of training objective. Despite the loss in TDPO is re-assigned to each token based on KL-Divergence, the temperature for each token is not adjusted appropriately, resulting in a coarse optimization process. In contrast, our method meticulously adjust the temperature for each segment, aligning the update scale with the segment's importance across various criteria, which attribute to our superior performance."}, {"title": "5 Conclusion", "content": "To further show the effectiveness of 2D-DPO, we conduct ablation studies and delve into a detailed analysis of the model's performance. Additionally, we present a case study in Appendix E.\nThe Influence of \u03b2. Table 2 shows the results of 2D-DPO with different values of \u03b2 (temperature). As \u03b2 increases, we observe a consistent trend across three benchmarks: performance first rises and then falls. This is because a higher \u03b2 can amplify the divergence penalty within the RL optimization objective, thereby avoiding model degradation. However, an overly high \u03b2 would reduce the overall optimal ceiling and limit the potential gains from alignment (Ahrabian et al., 2024).\nPerformance on Different Aspects. We evaluated the performance of the models aligned using different methods across various aspects. We selected AlpacaEval 2.0 (Dubois et al., 2024) which offers diverse instructions as the query set, and obtained the responses of different models on this set as the evaluation targets. The evaluation prompt was consistent with the prompts presented in Section 3.1. For aspects that are independent among segments, we took the average score of all segments as the score for that response. For aspects that are not independent among segments (completeness and clarity), we select the score of the last segment. The average score of all responses is taken as the final result. As shown in Figure 3, our 2D-DPO can achieve the best results in all aspects, striking a balance between different rubrics of human preferences. 1D-DPO with only segment-level feedback underperforms, as response-level alignment still leads to coarse refinement. We also notice different methods exhibit minimal difference upon safety and correctness, which might be due to Qwen2-7B-Instruct already undergone alignment process on these aspects. For the other aspects that is not"}, {"title": "6 Limitations", "content": "In this work, we introduce a novel approach to scale DPO with 2-dimensional reward signals. We first construct a preference data on both segment and aspect levels, and then develop a 2D-DPO objective that learns the 2D preferences concurrently. Experiment results on popular benchmarks verified the effectiveness of our proposed method.\nWhile the boost of direct preference alignment methods have promoted LLM development and application, most work focus on the design of loss function instead of the intricacies of human preferences. In future, we will continue our research on multi-dimensional feedback, aimed at optimally aligned preference optimization."}, {"title": "A Mathematical Derivations", "content": "Our work still has some limitations: 1) Due to the lack of open-source codes and time limitation, we only compare with one 1-dimensional DPO method. More comparison should be done on related work to improve the credibility of our work. 2) Our method should also be validated on foundation ability benchmarks such as MMLU (Hendrycks et al., 2021), to verify that our method would not lead to the degradation of fundamental abilities. 3) Due to resource limitation, the effectiveness of our method is only verified on 7B-sized models. The scaling ability on larger models deserves our future exploration to promote its application."}, {"title": "A.1 Preliminaries", "content": "In the most classic RLHF methods, the optimization goal is typically expressed as an entropy bonus using\nthe following KL-constrained:\n$\\max_{\\pi_{\\theta}} E_{a_t \\sim \\pi_{\\theta}(: |s_t)} [r(s_t, a_t) - \\beta D_{KL}[\\pi_{\\theta}(a_t|S_t)||\\pi_{ref}(a_t|S_t)]]$ (7)\n$= \\max_{\\pi_{\\theta}} E_{a_t \\sim \\pi_{\\theta}} [\\sum_{t=0}^{T} r(s_t, a_t) - \\beta \\log \\frac{\\pi_{\\theta}(a_t|S_t)}{\\pi_{ref}(a_t|S_t)}]$ (8)\n$= \\max_{\\pi_{\\theta}} E_{a_t \\sim \\pi_{\\theta}(: |s_t)} [\\sum_{t=0}^{T} (r(s_t, a_t) + \\beta \\log \\pi_{ref}(a_t|S_t)) + \\beta H(\\pi_{\\theta})|S_0 \\sim \\rho(S_0)]$ (9)\nThe fixed point solution in the general maximum entropy RL setting is (Ziebart, 2010; Levine, 2018):\n$\\pi^* (a_t | S_t) = e^{(Q^* (s_t,a_t)-V^*(s_t))/\\beta}$ (10)\nThe Bradley-Terry preference model in token-level MDP is:\n$p^* (\\tau^w > \\tau^l) = \\frac{exp (\\sum_{i=1}^N r (s_i, a_i^w))}{exp (\\sum_{i=1}^N r (s_i, a_i^w)) + exp (\\sum_{i=1}^N r (s_i, a_i^l))}$ (11)"}, {"title": "A.2 DPO in the Token Level MDP", "content": "The formula using the Q-function to measure the relationship between the current timestep and future\nreturns (Rafailov et al., 2024):\n$Q^*(S_t, a_t) = \\begin{cases} r(s_t, a_t) + \\beta \\log \\pi_{ref}(a_t|s_t) + V^*(s_{t+1}), & \\text{if } s_{t+1} \\text{ is not terminal} \\\\\nr(s_t, a_t) + \\beta \\log \\pi_{ref}(a_t|s_t), & \\text{if } s_{t+1} \\text{ is terminal} \\end{cases}$ (12)\nDerive the total reward obtained along the entire trajectory based on the above definitions:\n$\\sum_{t=0}^{T-1} r(s_t, a_t) = \\sum_{t=0}^{T-1} (Q^* (s_t, a_t) - \\beta \\log \\pi_{ref}(a_t|s_t) - V^*(S_{t+1}))$ (13)\nAccording to the definition of Eq. 24, we know that $V^*(S_T) = 0$. Combining this with the fixed point\nsolution of the optimal policy (Eq. 10), we can further derive:\n$Q^* (s_0, a_0) - \\beta \\log \\pi_{ref}(a_0|s_0) + \\sum_{t=1}^{T-1} (Q^* (S_t, a_t) - V^*(s_t) - \\beta \\log \\pi_{ref}(a_t|s_t))$ (14)\n$= Q^* (s_0, a_0) - \\beta \\log \\pi_{ref}(a_0|s_0) + \\sum_{t=1}^{T-1} \\beta \\log \\frac{\\pi^*(a_t|S_t)}{\\pi_{ref}(a_t|S_t)}$ (15)\n$= V^*(S_0) + \\sum_{t=0}^{T-1} \\beta \\log \\frac{\\pi^*(a_t|S_t)}{\\pi_{ref}(a_t|S_t)}$ (16)\nBy substituting the above result into Eq. 11, we can eliminate $V^*(S_0)$ in the same way as removing the\npartition function in DPO, obtaining the Token-level BT model that conforms to the MDP:\n$P_{\\pi^*} (\\tau^w > \\tau^l) = \\sigma  \\left[\\frac{exp (\\sum_{t=0}^{N-1} \\beta \\log \\frac{\\pi^*(a_t^w| S_t)}{\\pi_{ref}(a_t^w| S_t)})}{exp (\\sum_{t=0}^{M-1} \\beta \\log \\frac{\\pi^*(a_t^l| S_t)}{\\pi_{ref}(a_t^l| S_t)})} \\right]$ (17)"}, {"title": "A.3 The Token-level optimal advantage function of DPO", "content": "Thus, the Loss formulation of DPO at the Token level is:\n$L (\\Theta_{\\theta}, D) = -(ren) \\sim D \\log \\sigma  \\begin{bmatrix}\\frac{exp (\\sum_{t=0}^{N-1} \\beta \\log \\frac{\\pi^*(a_t | s_t)}{\\pi_{ref}(a_t | s_t)})}{exp (\\sum_{t=0}^{M-1} \\beta \\log \\frac{\\pi^*(a_t | s_t)}{\\pi_{ref}(a_t | s_t)})} \\end{bmatrix}$ (18)\nBy log-linearizing the fixed point solution of the optimal policy at the token level (Eq. 10), we obtain:\n$\\beta \\log \\pi^*(a_t | s_t) = Q^*(s_t, a_t) - V^*(s_t)$ (19)\nThen, combining with Eq. 24:\n$\\beta \\log \\frac{\\pi^*(a_t | s_t)}{\\pi_{ref}(a_t | s_t)} = r(s_t, a_t) + V^*(S_{t+1}) - V^*(s_t)$. (20)\nThus, we can establish the relationship between $\\beta \\log \\frac{\\pi^*(a_t|s_t)}{\\pi_{ref}(a_t|s_t)}$ and $r(s_t, a_t)$. and according to Rafailov\net al. 2023's definition:\nDefinition 1 Two reward functions $r(s_t, a_t)$ and $r' (s_t, a_t)$ are equivalent if there exists a potential function\n$\\Phi(s)$, such that $r'(s_t, a_t) = r(s_t, a_t) + \\Phi(s_{t+1}) - \\Phi(s_t)$.\nWe can conclude that the optimal advantage function is $\\beta \\log \\frac{\\pi^*(a_t | s_t)}{\\pi_{ref}(a_t | s_t)}$"}, {"title": "A.4 Proving that the \u03b2 of DPO can vary across tokens in the token-level MDP", "content": "When \u03b2 is considered as a variable dependent on t, Eq. 9 is transformed into:\n$\\max_{\\pi_{\\theta}} E_{a_t \\sim \\pi_{\\theta}(1:st)} [\\sum_{t=0}^{T} (r(s_t, a_t) + \\beta_t \\log \\pi_{ref}(a_t|s_t)) - \\beta_t \\log \\pi_{\\theta}(a_t|S_t)]$ (21)\nwhere $\\beta_t$ depends solely on $a_t$ and $s_t$. Then, according to Maximum Entropy Reinforcement Learning\nwith Fixed Dynamics (Levine, 2018), the above formula can be rewritten in a form that includes the KL\ndivergence:\n$= E_{S_0} [-\\beta_t D_{KL}  (\\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{ref}(a_t|s_t)}) + \\frac{1}{\\beta_t} exp(\\sum_{T^\\pi_{\\theta}}) exp (\\frac{r(s_t, a_t) + \\beta_t \\log \\pi_{ref}(a_t|s_t)}{\\beta_t}) + V (s_t)]$ (22)\nwhere $V(s_t) = \\beta_t \\log [\\int expr (\\frac{r(stat) \\beta_t \\log \\pi_{ref}(atst)}{\\beta_t}) dat]$. We know that when the KL divergence term is\nminimized, meaning the two distributions are the same, the above expectation reaches its maximum value.\nThat is:\n$\\pi_{\\theta}(a_t | s_t) = \\frac{1}{exp(V(s_t))} exp (\\frac{r(s_t, a_t) + \\beta_t \\log \\pi_{ref}(a_t|s_t)}{\\beta_t})$ (23)\nBased on this, we define that:\n$Q^* (S_t, a_t) = \\begin{cases} r(s_t, a_t) + \\beta_t \\log \\pi_{ref}(a_t|s_t) + V^*(s_{t+1}), & \\text{if } s_{t+1} \\text{ is not terminal} \\\\\nr(s_t, a_t) + \\beta_t \\log \\pi_{ref}(a_t|s_t), & \\text{if } s_{t+1} \\text{ is terminal} \\end{cases}$ (24)\nThus we can obtain the solution for the optimal policy:\n$\\pi_{\\theta}(a_t | s_t) = e^{(Q(s_t,a_t)-V(st))/\\beta_t}$ (25)\nThus, based on the fixed point solution with a varying \u03b2 in Eq. 25, we can continue the derivation in\nsection A.2 to obtain the token-level MDP of DPO with vary \u03b2 values for different tokens and perform a\nsimilar derivation as in Appendix A.3.\nFinally, it can be concluded that $\\beta_t \\log \\frac{\\pi^*(a_t | s_t)}{\\pi_{ref}(a_t | s_t)}$ can serve as the token-level advantage function."}, {"title": "A.5 Gradient Analysis", "content": "A.4 Proving that the \u03b2 of DPO can vary across tokens in the token-level MDP\nHere's the gradient analysis of token-level DPO (Eq. 5) incorporating fine-grained signals. We define:\n$R_{w,k} = \\beta \\sum_{t=nk}^{nk+lk} r_{w,k} \\log \\frac{\\pi^*(a_t | s_t)}{\\pi_{ref}(a_t | s_t)}$, $R_{w} = \\sum_{k=0}^{S_w-1} R_{w,k}$ (26)\n$R_{i,k} = \\beta \\sum_{t=nk}^{nk+lk} r_{i,k} \\log \\frac{\\pi^*(a_t | s_t)}{\\pi_{ref}(a_t | s_t)}$, $R_{i} = \\sum_{k=0}^{S_l-1} R_{i,k}$ (27)\nThen, Eq. 5 can be transformed into:\n$L(\\pi_{\\theta}, D) = -E_{(\\tau_w,\\tau_l) \\sim D} \\log \\sigma(R_{w} - R_{l})$. (28)\nThen, differentiate the above equation:\n$\\nabla_{\\theta}L (\\pi_{\\theta}, D) = -E_{(\\tau_w,\\tau_l) \\sim D} [\\sigma^\\prime(R_{i} - R_{w}) \\cdot (\\nabla_{\\theta}R_{w} - \\nabla_{\\theta}R_{i})]$. (29)\nExpanding the above equation, we get:\n$\\nabla_{\\theta}L(\\pi_{\\theta}, D) = -E_{(\\tau_w,\\tau_l) \\sim D} [\\beta \\cdot \\sigma (( \\sum_{t=nk}^{nk+lk} r_{w,k} \\log \\frac{\\pi^*(a_t | s_t)}{\\pi_{ref}(a_t | s_t)} ) - ( \\sum_{t=nk}^{nk+lk} r_{i,k} \\log \\frac{\\pi^*(a_t | s_t)}{\\pi_{ref}(a_t | s_t)} ))]$. (30)\n($\\sum_{t=nk}^{nk+lk} r_{w,k} \\nabla_{\\theta} \\log \\frac{\\pi^*(a_t | s_t)}{\\pi_{ref}(a_t | s_t)} ) - (\\sum_{t=nk}^{nk+lk} r_{i,k} \\nabla_{\\theta} \\log \\frac{\\pi^*(a_t | s_t)}{\\pi_{ref}(a_t | s_t)} ))]$. (31)\nWe can see that the gradient difference between the chosen and rejected segments is entirely determined\nby $r_{w,k}$ and $r_{i,k}$. Specifically, segments in the chosen set that score higher have larger gradients and are\nmore optimized, while those with lower scores have smaller gradients and are optimized less. The same\napplies to the rejected response. This allows the model to selectively increase the generation probability\nof good parts in the chosen response and decrease it for poor parts in the rejected response. Poor parts of\nthe chosen response and better parts of the rejected response receive less optimization. From a gradient\nperspective, token-level DPO incorporating fine-grained signals can perform targeted optimization on\nchosen and rejected responses, achieving higher alignment performance."}, {"title": "B Implementation Details", "content": "B.1 Hyper-parameters\nFor all the compared methods, we set \u03b2 to 0.2, and the final loss includes 0.1x of the SFT loss except for ORPO. To ensure a fair comparison, in our method, the \u03b2 is adaptively adjusted during training by calculating the average score r of all segments within a batch to achieve equivalence with \u03b2 = 0.2. For the specific methods: The y of SimPO is set to 0.5. In TDPO, we use TDPO2 with a set to 0.5.\nFor the 2D-DPO's weights W, we follow Wang et al. 2024b and use a heuristic search, setting the weights for the five aspects Helpfulness, Correctness, Safety, Completeness, Clarity to [0.3, 0.4, 0.1, 0.1, 0.1]. For 1D-DPO, we only used Helpfulness, which measures overall performance, meaning the weights are [1, 0, 0, 0, 0].\nB.2 Training Setup\nWe trained all models on 8 A100-80GB SXM GPUs. The per_device_train_batch_size was set to 1, gradient_accumulation_steps to 8, and we used bfloat16 precision. The initial learning rate was set to le-7 with cosine decay. Each method was trained for 700 steps."}, {"title": "B.3 Core Codes", "content": "The core code of 2D-DPO is as follows:\n1 def _2D_DPO_loss (\n2 self", "policy_chosen_logps": "torch.Tensor\"", "policy_rejected_logps": "torch.Tensor\"", "reference_chosen_logps": "torch.Tensor\"", "reference_rejected_logps": "torch.Tensor\"", "chosen_scores": "torch. Tensor\"", "rejected_scores": "torch.Tensor\"", "torch.Tensor\", \"torch.Tensor\", \"torch.Tensor\"": "n10 chosen_rewards = policy_chosen_logps.to(self.accelerator.device) -\n11 reference_chosen_logps.to(self.accelerator.device)\n12 rejected_rewards = policy_rejected_logps.to(self.accelerator.device)\n13 reference_rejected_logps.to(self.accelerator.device)\n14 chosen_scores = chosen_scores[:", "1": 4, "rejected_scores[": ""}]}