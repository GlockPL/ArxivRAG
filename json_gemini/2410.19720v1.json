[{"title": "2D-DPO: Scaling Direct Preference Optimization with 2-Dimensional Supervision", "authors": ["Shilong Li", "Yancheng He", "Hui Huang", "Xingyuan Bu", "Jiaheng Liu", "Hangyu Guo", "Weixun Wang", "Jihao Gu", "Wenbo Su", "Bo Zheng"], "abstract": "Recent advancements in Direct Preference Optimization (DPO) have significantly enhanced the alignment of Large Language Models (LLMs) with human preferences, owing to its simplicity and effectiveness. However, existing methods typically optimize a scalar score or ranking reward, thereby overlooking the multi-dimensional nature of human preferences. In this work, we propose to extend the preference of DPO to two dimensions: segments and aspects. We first introduce a 2D supervision dataset called HelpSteer-2D. For the segment dimension, we divide the response into sentences and assign scores to each segment. For the aspect dimension, we meticulously design several criteria covering the response quality rubrics. With the 2-dimensional signals as feedback, we develop a 2D-DPO framework, decomposing the overall objective into multi-segment and multi-aspect objectives. Extensive experiments on popular benchmarks demonstrate that 2D-DPO performs better than methods that optimize for scalar or 1-dimensional preferences.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Language Models (LLMs) have shown impressive performance across a wide range of tasks (Zhao et al., 2023; Bai et al., 2024; Wu et al., 2024a; Li et al., 2024a). A pivotal component in LLM training is Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Bai et al., 2022), which aligns LLMs with human preferences. However, due to its complexity, traditional RLHF often leads to challenges such as training instability and reward collapse (Wolf et al., 2023; Song et al., 2023).\nDirect Preference Optimization (DPO) (Rafailov et al., 2023), as a simpler and more effective alternative, has gained considerable attention due to its ability to bypass the need for explicitly fitting a reward model (Meng et al., 2024; Ethayarajh et al., 2024). However, most existing DPO-style approaches rely on scalar scores or rankings and ignore the multi-dimensional nature of human preferences, resulting in inefficient and imprecise optimization. For instance, a response may be deemed satisfactory under one aspect such as correctness, but falls short in another such as clarity. Moreover, not all segments of a response should be treated uniformly; even in a preferred response, there may be segments of inferior quality. This underscores the need for a more nuanced approach that recognizes the multi-dimensionality of feedback and its critical impact on model training.\nIn response, some recent works have attempted to leverage signals that are believed to reflect the importance of individual segments as reward scores (Zeng et al., 2024; Chan et al., 2024; Jiang et al., 2024; Chen et al., 2024). However, these signals are often derived from statistical features such as edit distance or confidence estimation, which can introduce noise and lack interpretability. Other approaches incorporate multi-objective optimization to balance different aspects of human preferences (Wu et al., 2024b; Guo et al., 2023; Cao et al., 2024). However, these methods mostly rely on Proximal Policy Optimization (PPO) (Schulman et al., 2017), which is prone to instability during training. Furthermore, these efforts only extend preference optimization from 0-dimensional (scalar reward) to 1-dimensional (aspect/segment) supervision, which remains insufficient for capturing the complexity of real-world human preferences.\nTo better address the intricacy of human preferences, we propose 2D-DPO, a novel direct alignment strategy that enables 2-dimensional (2D) fine-grained optimization. Our core idea is to scale supervision signals across two dimensions: segments and aspects. To this end, we first construct a preference dataset called HelpSteer-2D, where each sample is annotated with a 2-dimensional score matrix evaluating each segment across multiple aspects. These signals are derived from a robust model guided by a set of stringent principles, ensuring the generation of highly accurate and interpretable supervision signals. Building on this, we propose a novel approach to achieve 2-dimensional direct preference alignment. Experimental results on three public benchmarks demonstrate that 2D-DPO significantly outperforms previous methods. In summary, our main contributions are threefold:\n\u2022 We introduce a novel 2-dimensional preference alignment method, 2D-DPO, which scales supervision signals across both segments and aspects to better align with human preferences.\n\u2022 We develop a high-quality, fine-grained preference dataset, HelpSteer-2D, which will be released to the community for future research.\n\u2022 Extensive experiments show that 2D-DPO delivers superior performance in aligning with human preferences compared to prior approaches."}, {"title": "2 Related Work", "content": "Large language models (LLMs) have advanced rapidly, with reinforcement learning from human feedback (RLHF) commonly used to align LLMs with human preferences (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022; Liu et al., 2024b,a; Peng et al., 2023; Feng et al., 2022). However, traditional RLHF methods face challenges like instability and high resource demands (Wolf et al., 2023; Song et al., 2023), prompting the search for simpler alternatives. One such representative approach is Direct Preference Optimization (DPO) (Rafailov et al., 2023), which optimizes alignment without explicit reward modeling, offering simplicity and stability. Building on DPO, IPO (Azar et al., 2024) adds a regularization term to alleviate overfitting. KTO (Ethayarajh et al., 2024) only requires a binary signal of whether an output is desirable or undesirable for an input to align LLMs, simplifying the data acquisition process. ORPO (Hong et al., 2024) simplifies training with odds ratio-based penalties, and SimPO (Meng et al., 2024) improves efficiency by using average log probability as an implicit reward."}, {"title": "2.2 Token-level Preference Optimization", "content": "The response-level rewards in naive PPO and DPO often lack token-level details. To address this, researchers have explored fine-grained supervision signals in three ways: (1) Human annotation: Methods like PRM (Lightman et al., 2023) and FGRLHF (Wu et al., 2024b) involve human annotators labeling each segment of the response to generate fine-grained signals. (2) LLM annotation: To reduce the cost of human labeling, stronger LLMs are used to generate preference pairs with minimal edits (Guo et al., 2023; Chen et al., 2024; Yoon et al., 2024; Jiang et al., 2024) or to identify positive and negative response segments (Cao et al., 2024). (3) Internal signal: Some works use the model's internal information as reward signals, such as using attention scores for token rewards in ABC (Chan et al., 2024) or decomposing DPO's response-level rewards into token-level signals in SePO (Yang et al., 2024b; Rafailov et al., 2024). TDPO (Zeng et al., 2024) achieves token-level alignment by controlling the KL divergence for each token."}, {"title": "2.3 Multi-objective Preference Optimization", "content": "Human preferences are often complex, diverse, and even contradictory, making single-dimensional training insufficient. To address this, some studies align LLMs with multiple objectives by either training separate reward models and averaging their outputs (Pan et al., 2023; Ji et al., 2024; Rame et al., 2024; de Langis et al., 2024; Wang et al., 2024a). However, this approach demands significant computational resources. In contrast, MODPO (Zhou et al., 2024) offers a simpler, reinforcement learning-free method for optimizing multiple objectives. RiC (Yang et al., 2024c) and CPO (Guo et al., 2024) focus on integrating multiple reward values for controllable generation."}, {"title": "3 Approach", "content": "In this section, we provide a detailed introduction to our 2D-DPO approach for aligning LLMs. We first describe the construction of the 2D preference dataset in Section 3.1. Then, we present an enhanced 2D-DPO formulation that integrates the 2D supervision signals into the alignment process in Section 3.2. The complete pipeline of our method is illustrated in Figure 2."}, {"title": "3.1 Preference Dataset with 2D Signal", "content": "In general, a preference optimization dataset, denoted as $D = \\{x^i, y_w^i, y_l^i \\}_{i=1}^N$, comprises prompts $x$ along with a chosen response $y_w$ and a rejected response $y_l$, where $y_w$ is of higher quality compared to $y_l$. Such datasets are commonly used to train reward models (e.g., PPO) or directly for model alignment (e.g., DPO). However, differentiating between the chosen and rejected responses based on a scalar score is often coarse and imprecise. The quality of responses can vary significantly across different evaluation aspects, and even a chosen response might contain segments of low quality, while a rejected response could include segments of high quality. Therefore, relying solely on a scalar score for optimization may restrict the model's ability to effectively align with human preferences.\nTo address this issue, we propose a fine-grained scoring approach that decomposes the scalar scores of model responses to segment-level and aspect-level. The first step is to divide the response into segments, and the choice of segment length is crucial for ensuring the effectiveness of the fine-grained optimization. Segments that are too long cannot resolve the aforementioned coarse scoring issues, while segments that are too short pose difficulties for accurate assessment. Therefore, we choose the sentence as the scoring unit, which can strike a balance between scoring accuracy and the clarity of segment preferences.\nAfter segmenting responses based on typical sentence-ending punctuations, we employ GPT-4 to perform aspect-level scoring. Following HelpSteer2 (Wang et al., 2024b), we annotate the preference data across five key aspects: Helpfulness, Correctness, Safety, Completeness, Clarity. The first three aspects are independent of different sentences. The aspect of Completeness generally increases as responses become more comprehensive, while Clarity tends to decrease as responses grow longer and more redundant. To ensure the integrity of our annotations, we use separate prompts for each aspect to prevent any cross-influence among them. For the details of the annotation process, please refer to Appendix F.\nFinally, the constructed dataset is as follows:\n$D = \\{x^i, y_w^i, y_l^i \\}_{i=1}^N$\n$y_w^i = \\{y_{w,k}^i, \\{r_{w,k,j}^i\\}_{j=1}^{A} \\}_{k=1}^{S^2}$\n$y_l^i = \\{y_{l,k}^i, \\{r_{l,k,j}^i\\}_{j=1}^{A} \\}_{k=1}^{S^2}$"}, {"title": "3.2 2D-DPO", "content": "While the construction process of 2D signals is straightforward, integrating them effectively into the alignment process presents significant challenges. Previous approaches mostly utilize these signals as a scalar reward by weighted summation, which is insufficient for enabling the model to distinguish between varying quality across different dimensions. To address this issue, we propose a novel alignment method called 2D-DPO.\nDirect Preference Optimization (DPO) (Rafailov et al., 2023), as one of the most popular alignment methods, proposes a direct optimization objective that satisfies the optimal preference policy without using a reward model:\n$L_{DPO}(\\pi_\\theta; \\pi_{ref}) = - E_{(x,y_w,y_l)\\sim D} \\Big[ log \\sigma \\Big( \\beta \\Big( log \\frac{\\pi_\\theta(y_w | x)}{\\pi_{ref}(y_w | x)} - log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} \\Big) \\Big) \\Big]$\nwhere $\\pi_\\theta$ and $\\pi_{ref}$ represent the policy model and the reference model, respectively. DPO can fundamentally be viewed as a multi-armed bandit problem, where the model's entire response is treated as a single arm. According to Rafailov et al. 2023, in the token-level Markov Decision Process (MDP), the language model's Bradley-Terry preference model can be expressed in the following form:\np*(\\tau_w > \\tau_l) = \\frac{exp(\\sum_{i=1}^{N} r(\\textbf{s}_i^w, a_i^w))}{exp(\\sum_{i=1}^{N} r(\\textbf{s}_i^w, a_i^w)) + exp(\\sum_{i=1}^{M} r(\\textbf{s}_i^l, a_i^l))}\nwhere $\\tau_w$ and $\\tau_l$ represent the winning and losing trajectories, respectively. In this context, $a$ represents the next generated token, and $s$ denotes the current state, consisting of the prompt along with all previously generated tokens.\nSubsequently, based on the fixed point solution under the general maximum entropy RL setting (Ziebart, 2010; Levine, 2018), Rafailov et al. derived the form of DPO in token-level MDP:\n$L_{DPO}(\\pi_\\theta; \\pi_{ref}) = - E\\Big[log \\sigma \\Big( \\beta  \\Big( \\sum_{t=0}^{N-1}  log \\frac{\\pi_\\theta(a_t^w | \\textbf{s}_t^w)}{\\pi_{ref}(a_t^w | \\textbf{s}_t^w)} -  \\sum_{t=0}^{M-1} log \\frac{\\pi_\\theta(a_t^l | \\textbf{s}_t^l)}{\\pi_{ref}(a_t^l | \\textbf{s}_t^l)} \\Big)  \\Big)\\Big]$\nIn the above process, Rafailov et al. combined Ng et al.'s definition of equivalence between two reward functions through a potential function and concluded that $\\beta log \\frac{\\pi_\\theta(a_t| \\textbf{s}_t)}{\\pi_{ref}(a_t| \\textbf{s}_t)}$ and $r(\\textbf{s}_t, a_t)$ can equivalently yield the same optimal policy. Furthermore, $\\beta log \\frac{\\pi_\\theta(a_t| \\textbf{s}_t)}{\\pi_{ref}(a_t| \\textbf{s}_t)}$ is precisely the optimal advantage function $A*(\\textbf{s}_t, a_t)$. For a detailed derivation, please refer to Appendix A.2 and A.3.\n2D-DPO. With the above conclusions, 2D supervision signals can be conveniently integrated into the alignment process. We achieve the integration by using the signals to calibrate the token-level advantage function $A(\\textbf{s}_t, a_t)$ for different segments. This approach equips the model with a direct perception of fine-grained preferences, avoiding the ambiguity of holistic rewards.\nSpecifically, we use the regularized fine-grained reward $r$ as a coefficient, incorporating it into the token-level advantage function to obtain $\\beta r log \\frac{\\pi_\\theta(a_t| \\textbf{s}_t)}{\\pi_{ref}(a_t| \\textbf{s}_t)}$, which integrates the fine-grained signals. In practice, this is equivalent to adjusting $\\beta$ within the original $A(\\textbf{s}_t, a_t)$. We provide complete proof of its feasibility in Appendix A.4.\nThe token-level DPO incorporating fine-grained signals is formalized as follows:\n$L(\\pi_\\theta, D) = - E_{(\\tau_w, \\tau_l)\\sim D} log \\sigma \\Big[\\beta \\Big( \\sum_{k=0}^{S_w-1} \\sum_{t=n_k}^{n_k+l_k} r_{w,k} log \\frac{\\pi_\\theta(a_t | \\textbf{s}_t)}{\\pi_{ref}(a_t | \\textbf{s}_t)} - \\sum_{k=0}^{S_l-1} \\sum_{t=n_k}^{n_k+l_k} r_{l,k} log \\frac{\\pi_\\theta(a_t | \\textbf{s}_t)}{\\pi_{ref}(a_t | \\textbf{s}_t)} \\Big)\\Big]$\nwhere $n_k$ represents the first token of the k-th segment and $l_k$ denotes the length of the k-th segment. For handling multiple aspects, we use a classic weighted approach for integration, that is, $r_{w,k} = W \\tilde{r}_{w,k}$, where $W$ represents the weights that sum up to 1, which reflect the importance of each aspect during the alignment process, and $\\tilde{r}_{w,k} = \\{r_{w,k,j}\\}_{j=1}^A$\nSegment Selection. The number of segments in the chosen and rejected responses may differ significantly, and typically only the segments with an impact on response preference need attention. Therefore, we select the top-N highest-scoring segments from the chosen response and the top-N lowest-scoring segments from the rejected response, where $N = min(S_w, S_l)$, further enhances the efficiency of model alignment training. Additionally, we group segments in pairs to provide clearer contrast during alignment, making it easier for the model to learn fine-grained differences between the chosen and rejected responses. These segments are paired to form $N_{BT}$ models. The feasibility of this rearrangement is based on the fact that the loss for a single-segment BT model can be treated as setting the $\\beta_t$ of other parts to 0, as demonstrated in Appendix A.4. Thus, we obtain the token-level DPO formula incorporating fine-grained signals:\n$L_{group}(\\pi_\\theta, D) = - E_{(\\tau_w, \\tau_l)\\sim D} \\sum_{k=0}^{N-1} log \\sigma \\Big[ \\beta \\Big( \\sum_{t=n_k}^{n_k+l_k} r_{w,k} log \\frac{\\pi_\\theta(a_t | \\textbf{s}_t)}{\\pi_{ref}(a_t | \\textbf{s}_t)} - \\sum_{t=n_k}^{n_k+l_k} r_{l,k} log \\frac{\\pi_\\theta(a_t | \\textbf{s}_t)}{\\pi_{ref}(a_t | \\textbf{s}_t)} \\Big)\\Big]$\nAs a result, we've formulated the definitive objective of 2D-DPO. This training objective allows for the direct integration of 2-D supervision signals into the alignment process, enabling LLMs to discern the different aspects lying in different segments in the responses, thereby promoting better alignment with human preferences."}, {"title": "4 Experiments", "content": "Our method has been tested on three wildly recognized instruction-following benchmarks: Arena-Hard (Li et al., 2024b), AlpacaEval 2.0 (Dubois et al., 2024) and MT-Bench (Zheng et al., 2023). Each benchmark comprises a diverse set of queries, and the answers are evaluated under the framework of LLM-as-a-Judge (Zheng et al., 2023). We use gpt-4-turbo-2024-04-09 as the judge model, and scores are reported following each benchmark's protocol.\nOur method is validated on two models, Qwen2-7B-Instruct (Yang et al., 2024a) and Llama-3-8B-Instruct (AI@Meta, 2024). It deserves to be noticed that both models have undergone extensive instruction-tuning processes, therefore we directly perform preference optimization.\nWe mainly compare our method with 0-dimentional preference optimization methods:\n\u2022 DPO (Rafailov et al., 2023). This method leverages a mapping between reward functions and optimal policies to optimize the preference with a single stage of policy training.\n\u2022 IPO (Azar et al., 2024). This method propose a theoretically grounded approach method to replace pairwise preferences in DPO with pointwise rewards.\n\u2022 KTO (Ethayarajh et al., 2024). This method proposes to directly maximize the utility of generations from non-paired data.\n\u2022 ORPO (Hong et al., 2024). This method leverages a reference model-free monolithic odds ratio for contrasting favored and disfavored styles during SFT stage.\n\u2022 SimPO (Meng et al., 2024). This method proposes to use the average log probability of a sequence as the implicit reward, which eliminates the need for a reference model.\nWe also compare our method with the following 1-dimensional preference optimization method:"}, {"title": "4.3 Detailed Analysis", "content": "To further show the effectiveness of 2D-DPO, we conduct ablation studies and delve into a detailed analysis of the model's performance. Additionally, we present a case study in Appendix E.\nTable 2 shows the results of 2D-DPO with different values of $\\beta$ (temperature). As $\\beta$ increases, we observe a consistent trend across three benchmarks: performance first rises and then falls. This is because a higher $\\beta$ can amplify the divergence penalty within the RL optimization objective, thereby avoiding model degradation. However, an overly high $\\beta$ would reduce the overall optimal ceiling and limit the potential gains from alignment (Ahrabian et al., 2024).\nWe evaluated the performance of the models aligned using different methods across various aspects. We selected AlpacaEval 2.0 (Dubois et al., 2024) which offers diverse instructions as the query set, and obtained the responses of different models on this set as the evaluation targets. The evaluation prompt was consistent with the prompts presented in Section 3.1. For aspects that are independent among segments, we took the average score of all segments as the score for that response. For aspects that are not independent among segments (completeness and clarity), we select the score of the last segment. The average score of all responses is taken as the final result. As shown in Figure 3, our 2D-DPO can achieve the best results in all aspects, striking a balance between different rubrics of human preferences. 1D-DPO with only segment-level feedback underperforms, as response-level alignment still leads to coarse refinement. We also notice different methods exhibit minimal difference upon safety and correctness, which might be due to Qwen2-7B-Instruct already undergone alignment process on these aspects. For the other aspects that is not covered by the process, 2D-DPO can achieve more pronounced improvement.\nWe analyze the reward of different responses during training in Figure 4(a) . We can observe that the reward scores of the preferred responses in our method increase rapidly while the reward scores of the dispreferred responses decrease significantly, resulting in the largest margin. Figure 4(b) shows the reward accuracy trends during training. In this context, accuracy is defined as the proportion of instances where the reward score for the preferred response is higher than that for the dispreferred response. Our method not only reaches the highest accuracy fastest but also achieves the best overall accuracy, demonstrating that our method facilitates more efficient training. In Figure 5, we show the trends of sequential KL divergence over training steps for both preferred and dispreferred responses. 2D-DPO exhibits consistently lower KL divergence compared to DPO and 1D-DPO on both preferred and dispreferred responses. This indicates that 2-dimensional supervision can effectively balance KL divergence, preventing excessive deviations from the original model, thereby ensuring stronger training stability.\nIn Figure 6, we compare the fine-grained reward assignment for the same sample using various reward models: (a) 1D-RM utilizes only segment-level reward signals; (b) DPO is trained through direct preference alignment; (c) ORM is trained with response-level reward signals; (d) 2D-RM is trained with 2-dimensional supervision signals. As can be seen, 1) ORM fails to distinguish preference differences between segments, leading to inaccurate global scores; 2) 1D-RM identifies preferences for different segments but does not detect the redundancy issue in the second sentence; 3) DPO can identify certain preferred tokens, but also assigns high scores to dispreferred tokens and overlooks some preferred tokens, introducing significant noise. In contrast, our method not only distinguishes preferences across segments more effectively but also provides more accurate scoring."}, {"title": "5 Conclusion", "content": "In this work, we introduce a novel approach to scale DPO with 2-dimensional reward signals. We first construct a preference data on both segment and aspect levels, and then develop a 2D-DPO objective that learns the 2D preferences concurrently. Experiment results on popular benchmarks verified the effectiveness of our proposed method.\nWhile the boost of direct preference alignment methods have promoted LLM development and application, most work focus on the design of loss function instead of the intricacies of human preferences. In future, we will continue our research on multi-dimensional feedback, aimed at optimally aligned preference optimization."}, {"title": "6 Limitations", "content": "Our work still has some limitations: 1) Due to the lack of open-source codes and time limitation, we only compare with one 1-dimensional DPO method. More comparison should be done on related work to improve the credibility of our work. 2) Our method should also be validated on foundation ability benchmarks such as MMLU (Hendrycks et al., 2021), to verify that our method would not lead to the degradation of fundamental abilities. 3) Due to resource limitation, the effectiveness of our method is only verified on 7B-sized models. The scaling ability on larger models deserves our future exploration to promote its application."}, {"title": "A.1 Preliminaries", "content": "In the most classic RLHF methods, the optimization goal is typically expressed as an entropy bonus using the following KL-constrained:\n$max_{\\pi_\\theta} E_{(s_t, a_t) \\sim \\tau} [r(s_t, a_t) - \\beta D_{KL}[\\pi_\\theta(a_t|s_t)||\\pi_{ref}(a_t|s_t)]]$\n$= max_{\\pi_\\theta} E_{(s_t, a_t) \\sim \\tau} [r(s_t, a_t) - \\beta log \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{ref}(a_t|s_t)}]$\n$= max_{\\pi_\\theta} E_{(s_t, a_t) \\sim \\tau} [\\sum(r(s_t, a_t) + \\beta log \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{ref}(a_t|s_t)}) + \\beta H(\\pi_\\theta)|S_0 \\sim p(S_0)]$\nThe fixed point solution in the general maximum entropy RL setting is (Ziebart, 2010; Levine, 2018):\n$\\pi^* (a_t | S_t) = e^{\\frac{1}{\\beta}(Q^*(s_t,a_t)-V^*(s_t))}$ \nThe Bradley-Terry preference model in token-level MDP is:\np*(\\tau^w > \\tau^l) = \\frac{exp (\\sum_{i=1}^{N} r(\\textbf{s}_i^w, a_i^w))}{exp (\\sum_{i=1}^{N} r(\\textbf{s}_i^w, a_i^w)) + exp (\\sum_{i=1}^{M} r(\\textbf{s}_i^l, a_i^l))}$"}, {"title": "A.2 DPO in the Token Level MDP", "content": "The formula using the Q-function to measure the relationship between the current timestep and future returns (Rafailov et al., 2024):\n$Q^*(S_t, a_t) = \\begin{cases} r(s_t, a_t) + \\beta log \\frac{\\pi(a_t|s_t)}{\\pi_{ref}(a_t|s_t)} + V^*(s_{t+1}), & \\text{if } s_{t+1} \\text{ is not terminal} \\\\ r(s_t, a_t) + \\beta log \\frac{\\pi(a_t|s_t)}{\\pi_{ref}(a_t|s_t)}, & \\text{if } s_{t+1} \\text{ is terminal} \\end{cases}$\nDerive the total reward obtained along the entire trajectory based on the above definitions:\n$\\sum_{t=0}^{T-1} r(s_t, a_t) = \\sum_{t=0}^{T-1} (Q^* (s_t, a_t) - \\beta log \\frac{\\pi(a_t|s_t)}{\\pi_{ref}(a_t|s_t)} - V^*(S_{t+1}))$\nAccording to the definition of Eq. 24, we know that V*(S_T) = 0. Combining this with the fixed point solution of the optimal policy (Eq. 10), we can further derive:\n$Q^*(s_0, a_0) - \\beta log \\frac{\\pi(a_0|s_0)}{\\pi_{ref}(a_0|s_0)} + \\sum_{t=1}^{T-1} (Q^* (S_t, a_t) - V^*(s_t) - \\beta log \\frac{\\pi(a_t|s_t)}{\\pi_{ref}(a_t|s_t)})$\n$= Q^*(s_0, a_0) - \\beta log \\frac{\\pi(a_0|s_0)}{\\pi_{ref}(a_0|s_0)} + \\sum_{t=1}^{T-1} \\beta log \\frac{\\pi^*(a_t|S_t)}{\\pi_{ref}(a_t|S_t)}$\n$= V^*(S_0) + \\sum_{t=0}^{T-1} \\beta log \\frac{\\pi^*(a_t|S_t)}{\\pi_{ref}(a_t|S_t)}$\nBy substituting the above result into Eq. 11, we can eliminate V*(S_0) in the same way as removing the partition function in DPO, obtaining the Token-level BT model that conforms to the MDP:\n$P_{\\pi^*}(\\tau^w > \\tau^l) = \\sigma \\Big(\\sum_{i=1}^{N-1} \\beta log \\frac{\\pi^*(a_t^w | \\textbf{s}_t^w)}{\\pi_{ref}(a_t^w | \\textbf{s}_t^w)} - \\sum_{i=1}^{M-1} \\beta log \\frac{\\pi^*(a_t^l | \\textbf{s}_t^l)}{\\pi_{ref}(a_t^l | \\textbf{s}_t^l)}\\Big)$"}, {"title": "A.3 The Token-level optimal advantage function of DPO", "content": "By log-linearizing the fixed point solution of the optimal policy at the token level (Eq. 10), we obtain:\n$\\beta log \\pi^*(a_t | s_t) = Q^*(s_t, a_t) - V^*(s_t)$\nThen, combining with Eq. 24:\n$\\beta log \\frac{\\pi^*(a_t | s_t)}{\\pi_{ref}(a_t | s_t)} = r(s_t, a_t) + V^*(s_{t+1}) - V^*(s_t)$.\nThus, we can establish the relationship between $\\beta log \\frac{\\pi^*(a_t | s_t)}{\\pi_{ref}(a_t | s_t)}$ and r(s_t, a_t). and according to Rafailov et al. 2023's definition:\nTwo reward functions r(s_t, a_t) and r' (s_t, a_t) are equivalent if there exists a potential function $\\Phi(s)$, such that r'(s_t, a_t) = r(s_t, a_t) + $\\Phi(s_{t+1}) - $\\Phi(s_t).\nWe can conclude that the optimal advantage function is $\\beta log \\frac{\\pi^*(a_t | s_t)}{\\pi_{ref}(a_t | s_t)}$."}, {"title": "A.4 Proving that the $\\beta$ of DPO can vary across tokens in the token-level MDP", "content": "When $\\beta$ is considered as a variable dependent on t", "into": "n$max_{\\pi_\\theta"}, "E_{a_t \\sim \\pi_\\theta} [\\sum_{t=0}^{T} (r(s_t, a_t) + \\beta_t log \\frac{\\pi_{ref}(a_t|s_t))}{\\pi_\\theta(a_t|s_t)}) - \\beta_t log \\pi_\\theta(a_t|s_t)"], "divergence": "n$= E_{s_t"}, ["beta_t D_{KL} (\\pi_\\theta(a_t | s_t) || \\frac{1}{exp(V(s_t))} \\sum_{a_t} exp(\\frac{r(s_t, a_t) + \\beta_t log \\pi_{ref}(a_t|s_t)}{\\beta_t})"], ["int exp(\\frac{r(s_t, a_t) + \\beta_t log \\pi_{ref}(a_t|s_t)}{\\beta_t}) da_t"]]