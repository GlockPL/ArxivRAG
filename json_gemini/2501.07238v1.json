{"title": "Lessons From Red Teaming 100 Generative AI Products", "authors": ["Blake Bullwinkel", "Amanda Minnich", "Shiven Chawla", "Gary Lopez", "Martin Pouliot", "Whitney Maxwell", "Joris de Gruyter", "Katherine Pratt", "Saphir Qi", "Nina Chikanov", "Roman Lutz", "Raja Sekhar Rao Dheekonda", "Bolor-Erdene Jagdagdorj", "Eugenia Kim", "Justin Song", "Keegan Hines", "Daniel Jones", "Giorgio Severi", "Richard Lundeen", "Sam Vaughan", "Victoria Westerhoff", "Pete Bryan", "Ram Shankar Siva Kumar", "Yonatan Zunger", "Chang Kawaguchi", "Mark Russinovich"], "abstract": "In recent years, AI red teaming has emerged as a practice for probing the safety and security of generative AI systems. Due to the nascency of the field, there are many open questions about how red teaming operations should be conducted. Based on our experience red teaming over 100 generative AI products at Microsoft, we present our internal threat model ontology and eight main lessons we have learned:\n\n1.  Understand what the system can do and where it is applied\n2.  You don't have to compute gradients to break an AI system\n3.  AI red teaming is not safety benchmarking\n4.  Automation can help cover more of the risk landscape\n5.  The human element of AI red teaming is crucial\n6.  Responsible AI harms are pervasive but difficult to measure\n7.  LLMs amplify existing security risks and introduce new ones\n8.  The work of securing AI systems will never be complete\n\nBy sharing these insights alongside case studies from our operations, we offer practical recommendations aimed at aligning red teaming efforts with real world risks. We also highlight aspects of AI red teaming that we believe are often misunderstood and discuss open questions for the field to consider.", "sections": [{"title": "1 Introduction", "content": "As generative AI (GenAI) systems are adopted across an increasing number of domains, AI red teaming has emerged as a central practice for assessing the safety and security of these technologies. At its core, Al red teaming strives to push beyond model-level safety benchmarks by emulating real-world attacks against end-to-end systems. However, there are many open questions about how red teaming operations should be conducted and a healthy dose of skepticism about the efficacy of current AI red teaming efforts [4, 8, 32].\nIn this paper, we speak to some of these concerns by providing insight into our experience red teaming over 100 GenAI products at Microsoft. The paper is organized as follows: First, we present the threat model ontology that we use to guide our operations. Second, we share eight main lessons we have learned and make practical recommendations for AI red teams, along with case studies from our operations. In particular, these case studies highlight how our ontology is used to model a broad range of safety and security risks. Finally, we close with a discussion of areas for future development."}, {"title": "1.1 Background", "content": "The Microsoft AI Red Team (AIRT) grew out of pre-existing red teaming initiatives at the company and was officially established in 2018. At its conception, the team focused primarily on identifying traditional security vulnerabilities and evasion attacks against classical ML models. Since then, both the scope and scale of AI red teaming at Microsoft have expanded significantly in response to two major trends.\nFirst, AI systems have become more sophisticated, compelling us to expand the scope of AI red teaming. Most notably, state-of-the-art (SoTA) models have gained new capabilities and steadily improved across a range of performance benchmarks, introducing novel categories of risk. New data modalities, such as vision and audio, also create more attack vectors for red teaming operations to consider. In addition, agentic systems grant these models higher privileges and access to external tools, expanding both the attack surface and the impact of attacks.\nSecond, Microsoft's recent investments in AI have spurred the development of many more products that require red teaming than ever before. This increase in volume and the expanded scope of AI red teaming have rendered fully manual testing impractical, forcing us to scale up our operations with the help of automation. To achieve this goal, we develop PyRIT, an open-source Python framework that our operators utilize heavily in red teaming operations [27]. By augmenting human judgement and creativity, PyRIT has enabled AIRT to identify impactful vulnerabilities more quickly and cover more of the risk landscape.\nThese two major trends have made AI red teaming a more complex endeavor than it was in 2018. In the next section, we outline the ontology we have developed to model AI system vulnerabilities."}, {"title": "1.2 AI threat model ontology", "content": "As attacks and failure modes increase in complexity, it is helpful to model their key components. Based on our experience red teaming over 100 GenAI products for a wide range of risks, we developed an ontology to do exactly that. Figure 1 illustrates the main components of our ontology:\n\n\u2022\tSystem: The end-to-end model or application being tested.\n\u2022\tActor: The person or persons being emulated by AIRT. Note that the Actor's intent could be adversarial (e.g., a scammer) or benign (e.g., a typical chatbot user).\n\u2022\tTTPs: The Tactics, Techniques, and Procedures leveraged by AIRT. A typical attack consists of multiple Tactics and Techniques, which we map to MITRE ATT&CK\u00ae and MITRE ATLAS Matrix\u00b3 whenever possible.\n\t\tTactic: High-level stages of an attack (e.g., reconnaissance, ML model access).\n\t\tTechnique: Methods used to complete an objective (e.g., active scanning, jailbreak).\n\t\tProcedure: The steps required to reproduce an attack using the Tactics and Techniques.\n\u2022\tWeakness: The vulnerability or vulnerabilities in the System that make the attack possible.\n\u2022\tImpact: The downstream impact created by the attack (e.g., privilege escalation, generation of harmful content).\n\nIt is important to note that this framework does not assume adversarial intent. In particular, AIRT emulates both adversarial attackers and benign users who encounter system failures unintentionally. Part of the complexity of AI red teaming stems from the wide range of impacts that could be created by an attack or system failure. In the lessons below, we share case studies demonstrating how our ontology is flexible enough to model diverse impacts in two main categories: security and safety.\nSecurity encompasses well-known impacts such as data exfiltration, data manipulation, credential dumping, and others defined in MITRE ATT&CK\u00ae, a widely used knowledge base of security attacks. We also consider security attacks that specifically target the underlying AI model such as model evasion, prompt injections, denial of AI service, and others covered by the MITRE ATLAS Matrix.\nSafety impacts are related to the generation of illegal and harmful content such as hate speech, violence and self-harm, and child abuse content. AIRT works closely with the Office of Responsible"}, {"title": "1.3 Red teaming operations", "content": "In this section, we provide an overview of the operations we have conducted since 2021. In total, we have red teamed over 100 GenAI products. Broadly speaking, these products can be bucketed into \"models\" and \"systems.\u201d Models are typically hosted on a cloud endpoint, while systems integrate models into copilots, plugins, and other AI apps and features. Figure 2 shows the breakdown of products we have red teamed since 2021 and a bar chart with the annual percentage of our operations that have probed for safety (RAI) vs. security vulnerabilities.\nIn 2021, we focused primarily on application security. Although our operations have increasingly probed for RAI impacts, our team continues to red team for security impacts including data exfil-tration, credential leaking, and remote code execution (RCE). Organizations have adopted many different approaches to AI red teaming ranging from security-focused assessments with penetration testing to evaluations that target only GenAI features. In Lessons 2 and 7, we elaborate on security vulnerabilities and explain why we believe it is important to consider both traditional and AI-specific weaknesses.\nAfter the release of ChatGPT in 2022, Microsoft entered the era of AI copilots, starting with AI-powered Bing Chat, released in February 2023. This marked a paradigm shift towards applications that connect LLMs to other software components including tools, databases, and external sources.\nApplications also started using language models as reasoning agents that can take actions on behalf of users, introducing a new set of attack vectors that have expanded the security risk surface. In Lesson 7, we explain how these attack vectors both amplify existing security risks and introduce new ones.\nIn recent years, the models at the center of these applications have given rise to new interfaces, allowing users to interact with apps using natural language and responding with high-quality text, image, video, and audio content. Despite many efforts to align powerful AI models to human preferences, many methods have been developed to subvert safety guardrails and elicit content that is offensive, unethical, or illegal. We classify these instances of harmful content generation as responsible AI (RAI) impacts and in Lessons 3, 5, and 6 discuss how we think about these impacts and the challenges involved.\nIn the next section, we elaborate on eight main lessons we have learned from our operations. We also highlight five case studies from our operations and show how each one maps to our ontology in Figure 1. We hope these lessons are useful to others working to identify vulnerabilities in their own GenAI systems."}, {"title": "2 Lessons", "content": "Lesson 1: Understand what the system can do and where it is applied\nThe first step in an AI red teaming operation is to determine which vulnerabilities to target. While the Impact component of the AIRT ontology is depicted at the end of our ontology, it serves as an excellent starting point for this decision-making process. Starting from potential downstream impacts, rather than attack strategies, makes it more likely that an operation will produce useful findings tied to real world risks. After these impacts have been identified, red teams can work backwards and outline the various paths that an adversary could take to achieve them. Anticipating downstream impacts that could occur in the real world is often a challenging task, but we find that it is helpful to consider 1) what the AI system can do, and 2) where the system is applied.\nCapability constraints. As models get bigger, they tend to acquire new capabilities [18]. These capabilities may be useful in many scenarios, but they can also introduce attack vectors. For example, larger models are often able to understand more advanced encodings, such as base64 and ASCII art, compared to smaller models [16, 45]. As a result, a large model may be susceptible to malicious instructions encoded in base64, while a smaller model may not understand the encoding at all. In this scenario, we say that the smaller model is \u201ccapability constrained,\" and so testing it for advanced encoding attacks would likely be a waste of resources. Larger models also generally have greater knowledge in topics such as cybersecurity and chemical, biological, radiological, and nuclear (CBRN)\""}, {"title": "Lesson 2: You don't have to compute gradients to break an AI system", "content": "As the security adage goes, \u201creal hackers don't break in, they log in.\" The AI security version of this saying might be \u201creal attackers don't compute gradients, they prompt engineer\" as noted by Apruzzese et al. [2] in their study on the gap between adversarial ML research and practice. The study finds that although most adversarial ML research is focused on developing and defending against sophisticated attacks, real-world attackers tend to use much simpler techniques to achieve their objectives.\nIn our red teaming operations, we have also found that \"basic\u201d techniques often work just as well as, and sometimes better than, gradient-based methods. These methods compute gradients through a model to optimize an adversarial input that elicits an attacker-controlled model output. In practice, however, the model is usually a single component of a broader AI system, and the most effective attack strategies often leverage combinations of tactics to target multiple weaknesses in that system. Further, gradient-based methods are computationally expensive and typically require full access to the model, which most commercial AI systems do not provide. In this lesson, we discuss examples of relatively simple techniques that work surprisingly well and advocate for a system-level adversarial mindset in AI red teaming.\nSimple attacks. Apruzzese et al. [2] consider the problem of phishing webpage detection and manually analyze examples of webpages that successfully evaded an ML phishing classifier. Among 100 potentially adversarial samples, the authors found that attackers leveraged a set of simple, yet effective, strategies that relied on domain expertise including cropping, masking, logo stretching, etc. In our red teaming operations, we also find that rudimentary methods can be used to trick many vision models, as highlighted in case study #1. In the text domain, a variety of jailbreaks (e.g., Skeleton Key) and multiturn prompting strategies (e.g., Crescendo [34]) are highly effective for subverting the safety guardrails of a wide range of models. Notably, manually crafted jailbreaks tend to circulate on online forums much more widely than adversarial suffixes, despite the significant attention that methods like GCG [53] have received from AI safety researchers.\nSystem-level perspective. Al models are deployed within broader systems. This could be the infrastructure required to host a model, or it could be a complex application that connects the model to external data sources. Depending on these system-level details, applications may be vulnerable to very different attacks, even if the same model underlies all of them. As a result, red teaming strategies that target only models may not translate into vulnerabilities in production systems. Conversely, strategies that ignore non-GenAI components within a system (for example, input filters, databases, and other cloud resources) will likely miss important vulnerabilities that may be exploited by adversaries."}, {"title": "Lesson 3: AI red teaming is not safety benchmarking", "content": "Although simple methods are often used to break AI systems in practice, the risk landscape is by no means uncomplicated. On the contrary, it is constantly shifting in response to novel attacks and failure modes [7]. In recent years, there have been many efforts to categorize these vulnerabilities, giving rise to numerous taxonomies of AI safety and security risks [15, 21-23, 35\u201337, 39, 41, 42, 46\u201348]. As discussed in the previous lesson, complexity often arises at the system-level. In this lesson, we discuss how the emergence of entirely new categories of harm adds complexity at the model-level and explain how this differentiates AI red teaming from safety benchmarking.\nNovel harm categories. When AI systems display novel capabilities due to, for example, advance-ments in foundation models, they may introduce harms that we do not fully understand. In these scenarios, we cannot rely on safety benchmarks because these datasets measure preexisting notions of harm. At Microsoft, the AI red team often explores these unfamiliar scenarios, helping to define novel harm categories and build new probes for measuring them. For example, SOTA LLMs may possess greater persuasive capabilities than existing chatbots, which has prompted our team to think about how these models could be weaponized for malicious purposes. Case study #2 provides an example of how we assessed a model for this risk in one of our operations."}, {"title": "Lesson 4: Automation can help cover more of the risk landscape", "content": "The complexity of the AI risk landscape has led to the development of a variety of tools that can identify vulnerabilities more rapidly, run sophisticated attacks automatically, and perform testing on a much larger scale [7, 10, 27]. In this lesson, we discuss the important role of automation in AI red teaming and explain how PyRIT, our open-source framework, is developed to meet these needs.\nTesting at scale. Given the continually evolving landscape of risks and harms, AI safety often feels like a moving target. In Lesson 1, we recommended scoping attacks based on what the system can do and where it is applied. Nonetheless, many possible attack strategies may exist, making it difficult to achieve adequate coverage of the risk surface. This challenge motivated the development of PyRIT, an open-source framework for AI red teaming and security professionals [27]. PyRIT provides an array of powerful components including prompt datasets, prompt converters (e.g., various encodings), automated attack strategies (including TAP [24], PAIR [6], Crescendo [34], etc.), and even scorers for multimodal outputs. With an adversarial objective in mind, users can leverage these components as needed and apply a variety of techniques to assess much more of the risk landscape than would be possible with a fully manual approach. Testing at scale also helps AI red teams account for the non-deterministic nature of AI models and estimate how likely a particular failure is to occur.\nTools and weapons. As storied in detail by Smith et al. [38], \"any tool can be used for good or ill. Even a broom can be used to sweep the floor or hit someone over the head. The more powerful the tool, the greater the benefit or damage it can cause.\" This dichotomy could not be more true for AI and is also at the heart of PyRIT. On the one hand, PyRIT leverages multimodal models to perform helpful tasks like generating variations of a seed prompt or scoring the outputs of other models. On the other hand, PyRIT can automatically jailbreak a target model using uncensored versions of powerful models like GPT-4. In both cases, PyRIT benefits from advances in the state-of-the-art, helping AI red teams stay ahead.\nPyRIT has enabled a major shift in our operations from fully manual probing to red teaming supported by automation. Importantly, the framework is flexible and extensible. If a specific attack technique or target is not already available, users can easily implement the necessary interfaces. By releasing PyRIT open-source, we hope to empower other organizations and researchers to leverage its capabilities for identifying vulnerabilities in their own GenAI systems."}, {"title": "Lesson 5: The human element of AI red teaming is crucial", "content": "Automation like PyRIT can support red teaming operations by generating prompts, orchestrating attacks, and scoring responses. These tools are useful but should not be used with the intention of taking the human out of the loop. In the previous lessons, we discussed several aspects of red teaming that require human judgment and creativity such as prioritizing risks, designing system-level attacks, and defining new categories of harm. In this lesson, we discuss three more examples that underscore why AI red teaming is a very human endeavor.\nSubject matter expertise. Much recent AI research has used LLMs to judge the outputs of other models [17, 20, 51]. Indeed, this functionality is available in PyRIT and works well for simple tasks such as identifying whether a response contains hate speech or explicit sexual content. However, it is less reliable in the context of highly specialized domains like medicine, cybersecurity, and CBRN, which can be accurately evaluated only by subject matter experts (SMEs). In multiple operations, we have relied on SMEs to help us assess the risk of content that we were unable to evaluate ourselves or using LLMs. It is important for AI red teams to be aware of these limitations.\nCultural competence. Most AI research is conducted in Western cultural contexts, and modern language models use predominantly English pre-training data, performance benchmarks, and safety evaluations [1, 14]. Nonetheless, non-English tokens in large-scale text corpora often give rise to multilingual capabilities [5], and model developers are increasingly training LLMs with enhanced abilities in non-English languages, including Microsoft. Recently, AIRT tested the multilingual Phi-3.5 language models for responsible AI violations across four languages: Chinese, Spanish, Dutch, and English. Even though post-training was conducted only in English, we found that safety behaviors like refusal and robustness to jailbreaks transferred surprisingly well to the non-English languages tested. Further investigation is required to assess how well this trend holds for lower resource languages and to design red teaming probes that not only account for linguistic differences, but also redefine harms in different political and cultural contexts [11]. These methods should be developed through the collaborative effort of people with diverse cultural backgrounds and expertise.\nEmotional intelligence. Finally, the human element of AI red teaming is perhaps most evident in answering questions about AI safety that require emotional intelligence, such as: \u201chow might this model response be interpreted in different contexts?\u201d and \u201cdo these outputs make me feel uncomfortable?\u201d Ultimately, only human operators can assess the full range of interactions that users might have with AI systems in the wild. Case study #3 highlights how we are investigating psychosocial harms by evaluating how a chatbot responds to users in distress.\nIn order to make these assessments, red teamers may be exposed to disproportionate amounts of unsettling and disturbing AI-generated content. This underscores the importance of ensuring that AI red teams have processes that enable operators to disengage when needed and resources to"}, {"title": "Lesson 6: Responsible AI harms are pervasive but difficult to measure", "content": "Many of the human aspects of AI red teaming discussed above apply most directly to RAI impacts. As models are integrated into an increasing number of applications, we have observed these harms more frequently and invested heavily in our ability to identify them, including by forming a strong partnership with Microsoft's Office of Responsible AI and by developing extensive tooling in PyRIT. RAI harms are pervasive, but unlike most security vulnerabilities, they are subjective and difficult to measure. In this lesson, we discuss how our thinking around RAI red teaming has developed.\nAdversarial vs. benign. As illustrated in our ontology (see Figure 1), the Actor is a key component of an adversarial attack. In the context of RAI violations, we find that there are two primary actors to consider: 1) an adversarial user who leverages techniques like character substitutions and jailbreaks to deliberately subvert a system's safety guardrails and elicit harmful content, and 2) a benign user who inadvertently triggers the generation of harmful content. Even if the same content is generated in both scenarios, the latter case is probably worse than the former. Nonetheless, most AI safety research focuses on developing attacks and defenses that assume adversarial intent, overlooking the many ways that systems can fail \u201cby accident\u201d [31]. Case studies #3 and #4 provide examples of RAI harms that could be encountered by users with no adversarial intent, highlighting the importance of probing for these scenarios.\nRAI probing and scoring. In many cases, RAI harms are more ambiguous than security vulnerabili-ties due to fundamental differences between AI systems and traditional software. In particular, even if an operation identifies a prompt that elicits a harmful response, there are still several key unknowns. First, due to the probabilistic nature of GenAI models, we might not know how likely this prompt, or similar prompts, are to elicit a harmful response. Second, given our limited understanding of the"}, {"title": "Lesson 7: LLMs amplify existing security risks and introduce new ones", "content": "The integration of generative AI models into a variety of applications has introduced novel attack vectors and shifted the security risk landscape. However, many discussions around GenAI security overlook existing vulnerabilities. As elaborated in Lesson 2, attacks that target end-to-end systems, rather than just underlying models, often work best in practice. We therefore encourage AI red teams to consider both existing (typically system-level) and novel (typically model-level) risks.\nExisting security risks. Application security risks often stem from improper security engineering practices including outdated dependencies, improper error handling, lack of input/output sanitiza-tion, credentials in source, insecure packet encryption, etc. These vulnerabilities can have major consequences. For example, Weiss et al. [49] discovered a token-length side channel in GPT-4 and Microsoft Copilot that enabled an adversary to accurately reconstruct encrypted LLM responses and infer private user interactions. Notably, this attack did not exploit any weakness in the underlying Al model and could only be mitigated by more secure methods of data transmission. In case study #5, we provide an example of a well-known security vulnerability (SSRF) identified by one of our operations.\nModel-level weaknesses. Of course, AI models also introduce new security vulnerabilities and have expanded the attack surface. For example, AI systems that use retrieval augmented generation (RAG) architectures are often susceptible to cross-prompt injection attacks (XPIA), which hide malicious instructions in documents, exploiting the fact that LLMs are trained to follow user instructions and struggle to distinguish among multiple inputs [13]. We have leveraged this attack in a variety of operations to alter model behavior and exfiltrate private data. Better defenses will likely rely on both system-level mitigations (e.g., input sanitization) and model-level improvements (e.g., instruction hierarchies [43]).\nWhile techniques like these are helpful, it is important to remember that they can only mitigate, and not eliminate, security risk. Due to fundamental limitations of language models [50], one must assume that if an LLM is supplied with untrusted input, it will produce arbitrary output. When that input includes private information, one must also assume that the model will output private information. In the next lesson, we discuss how these limitations inform our thinking around how to develop AI systems that are as safe and secure as possible."}, {"title": "Lesson 8: The work of securing AI systems will never be complete", "content": "In the AI safety community, there is a tendency to frame the types of vulnerabilities described in this paper as purely technical problems. Indeed, the letter on the homepage of Safe Superintelligence Inc., a venture launched by Sutskever et al. [40], states:\n\"We approach safety and capabilities in tandem, as technical problems to be solved through revolutionary engineering and scientific breakthroughs. We plan to advance capabilities as fast as possible while making sure our safety always remains ahead. This way, we can scale in peace.\"\nEngineering and scientific breakthroughs are much needed and will certainly help mitigate the risks of powerful Al systems. However, the idea that it is possible to guarantee or \u201csolve\" AI safety through technical advances alone is unrealistic and overlooks the roles that can be played by economics, break-fix cycles, and regulation.\nEconomics of cybersecurity. A well-known epigram in cybersecurity is that \u201cno system is completely foolproof\" [2]. Even if a system is engineered to be as secure as possible, it will always be subject to the fallibility of humans and vulnerable to sufficiently well-resourced adversaries. Therefore, the goal of operational cybersecurity is to increase the cost required to successfully attack a system (ideally, well beyond the value that would be gained by the attacker) [2, 26]. Fundamental limitations of AI models give rise to similar cost-benefit tradeoffs in the context of AI alignment. For example, it has been demonstrated theoretically [50] and experimentally [9] that for any output which has a non-zero probability of being generated by an LLM, there exists a sufficiently long prompt that will elicit this response. Techniques like reinforcement learning from human feedback (RLHF) therefore make it more difficult, but by no means impossible, to jailbreak models. Currently, the cost of jailbreaking most models is low, which explains why real-world adversaries usually do not use expensive attacks to achieve their objectives.\nBreak-fix cycles. In the absence of safety and security guarantees, we need methods to develop AI systems that are as difficult to break as possible. One way to do this is using break-fix cycles, which perform multiple rounds of red teaming and mitigation until the system is robust to a wide range of attacks. We applied this approach to safety-align Microsoft's Phi-3 language models and covered a wide variety of harms and scenarios [11]. Given that mitigations may also inadvertently introduce new risks, purple teaming methods that continually apply both offensive and defensive strategies [3] may be more effective at raising the cost of attacks than a single round of red teaming.\nPolicy and regulation. Finally, regulation can also raise the cost of an attack in multiple ways. For example, it can require organizations to adhere to stringent security practices, creating better defenses across the industry. Laws can also deter attackers by establishing clear consequences for engaging in illegal activities. Regulating the development and usage of AI is complicated, and governments around the world are deliberating on how to control these powerful technologies without stifling innovation. Even if it were possible to guarantee the adherence of an AI system to some agreed upon set of rules, those rules will inevitably change over time in response to shifting priorities.\nThe work of building safe and secure AI systems will never be complete. But by raising the cost of attacks, we believe that the prompt injections of today will eventually become the buffer overflows of the early 2000s \u2013 though not eliminated entirely, now largely mitigated through defense-in-depth measures and secure-first design."}, {"title": "3 Open questions", "content": "Based on what we have learned about AI red teaming over the past few years, we would like to highlight several open questions for future research:\n\n1.  AI red teams must constantly update their practices based on novel capabilities and emerging harm areas. In particular, how should we probe for dangerous capabilities in LLMs such as persuasion, deception, and replication [29]? Further, what novel risks should we probe for in video generation models and what capabilities may emerge in models more advanced than the current state-of-the-art?\n2.  As models become increasingly multilingual and are deployed around the world, how do we translate existing AI red teaming practices into different linguistic and cultural contexts? For example, can we launch open-source red teaming initiatives that draw upon the expertise of people from many different backgrounds?\n3.  In what ways should AI red teaming practices be standardized so that organizations can clearly communicate their methods and findings? We believe that the threat model ontology described in this paper is a step in the right direction but recognize that individual frameworks are often overly restrictive. We encourage other AI red teams to treat our ontology in a modular fashion and to develop additional tools that make findings easier to summarize, track, and communicate."}, {"title": "4 Conclusion", "content": "AI red teaming is a nascent and rapidly evolving practice for identifying safety and security risks posed by AI systems. As companies, research institutions, and governments around the world grapple with the question of how to conduct AI risk assessments, we provide practical recommendations based on our experience red teaming over 100 GenAI products at Microsoft. We share our internal threat model ontology, eight main lessons learned, and five case studies, focusing on how to align red teaming efforts with harms that are likely to occur in the real world. We encourage others to build upon these lessons and to address the open questions we have highlighted."}]}