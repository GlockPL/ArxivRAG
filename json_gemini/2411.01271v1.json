{"title": "Interacting Large Language Model Agents. Interpretable Models and Social Learning.", "authors": ["ADIT JAIN", "VIKRAM KRISHNAMURTHY"], "abstract": "This paper develops theory and algorithms for interacting large language model agents (LLMAs) using methods from statistical signal processing and microeconomics. While both fields are mature, their application to decision-making by interacting LLMAs remains unexplored. Motivated by Bayesian sentiment analysis on online platforms, we construct interpretable models and stochastic control algorithms that enable LLMAs to interact and perform Bayesian inference. Because interacting LLMAs learn from both prior decisions and external inputs, they can exhibit bias and herding behavior. Thus, developing interpretable models and stochastic control algorithms is essential to understand and mitigate these behaviors. This paper has three main results. First, we show using Bayesian revealed preferences from microeconomics that an individual LLMA satisfies the necessary and sufficient conditions for rationally inattentive (bounded rationality) Bayesian utility maximization and, given an observation, the LLMA chooses an action that maximizes a regularized utility. Second, we utilize Bayesian social learning to construct interpretable models for LLMAs that interact sequentially with each other and the environment while performing Bayesian inference. Our proposed models capture the herding behavior exhibited by interacting LLMAs. Third, we propose a stochastic control framework to delay herding and improve state estimation accuracy under two settings: (a) centrally controlled LLMAs and (b) autonomous LLMAs with incentives. Throughout the paper, we numerically demonstrate the effectiveness of our methods on real datasets for hate speech classification and product quality assessment, using open-source models like Llama and Mistral and closed-source models like ChatGPT. The main takeaway of this paper, based on substantial empirical analysis and mathematical formalism, is that LLMAs act as rationally bounded Bayesian agents that exhibit social learning when interacting. Traditionally, such models are used in economics to study interacting human decision-makers.", "sections": [{"title": "I. INTRODUCTION", "content": "This paper develops theory and algorithms for interacting Large Language Model Agents (LLMAs) by leveraging techniques from signal processing and microeconomics. Specifically, we focus on developing interpretable models and stochastic control algorithms for LLMAs, enabling them to interact sequentially for Bayesian inference.\nWe construct interpretable models of LLMAs at two levels of abstraction, as outlined in Figure 1. First, we model an individual LLMA as a rationally inattentive Bayesian utility maximizer, capturing the agent's decision-making process under limited attention. Second, we extend this approach to a sequence of LLMAs engaging in Bayesian social learning, where each agent acts as a Bayesian utility maximizer. Our models are inspired by the self-attention mechanism in large language models (LLMs) and observed challenges, such as model collapse, that can arise during LLM training.\nFurthermore, motivated by the observed bias in the behavior of interacting LLMAs, we demonstrate that a sequence of LLMAs engaging in Bayesian social learning converge to identical estimates, or \"herd\". To address this phenomenon, we propose a stochastic control approach, formulating an optimal stopping problem to balance the trade-off between privacy and herding, to detect the failure state. Our approach is designed for two scenarios: (a) when the LLMAs are centrally controlled, and (b) when they operate autonomously.\nOur goal is to demonstrate that concepts from con- trolled sensing and microeconomics, traditionally applied to human decision-making, can be used to both under- stand and synthesize the behavior of interacting LL- MAs [7], [12], [13], [39], [40]. We support our theoretical findings with numerical experiments using advanced LLMs for Bayesian inference on real-world data. This paper is crafted to engage a broad readership, highlighting ap- plications of Bayesian agents in diverse fields, including financial news analysis, e-commerce review evaluation, and online toxicity detection. These examples underscore the flexibility of our methodologies for cross-disciplinary applications. The reproducible code for our experiments is publicly accessible at github.com/aditj/sociallearningllm."}, {"title": "A. Motivation", "content": "LLM agents (LLMAs) are being rapidly deployed for different applications and to quote Sam Altman, CEO of OpenAI (creators of ChatGPT, a popular LLM which has 200 million weekly active users): \u201c2025 is when (AI) agents will work\".\nLLMAs use a large language model (LLM) to parse the input and have additional agency to perform tasks. LLMs (such as ChatGPT and Llama) are neural networks with billions of parameters trained on trillions of tokens of textual data to parse long texts for summarizing, compil- ing key facts, and generating new text. The key technical improvement that leads to the efficient deployment of LLMs is the transformer architecture [75]. The effective- ness of LLMs on textual texts has made their deployment and adoption widespread [54]. Many applications have been proposed in healthcare, online platform moderation, and finance, where these LLMs are used to parse the textual observations and suggest decisions based on their outputs [48]. In many tasks, the outputs of the LLMs are often part of a more extensive pipeline; for example, the output of the LLMs, either in a specified format or as embeddings, is frequently used as inputs to other Bayesian entities, including classifiers [55]. The Bayesian framework also becomes essential in applications where the LLMs have to output decisions and need to provide confidence in the decision output. Thus, it is of interest to study a single Bayesian agent that uses the LLM to parse text observations, update its Bayesian belief, and take action. This paper studies such entities and refers to them as Large Language Models Agents (LLMAs). Constructing interpretable models for LLMAs is crucial to understanding and controlling their interaction."}, {"title": "1) Interacting large language model agents:", "content": "It is pre- dicted that by 2025, 90% of web content will be gen- erated by large language models (LLMs) [1]. In recent practical implementations, individual LLMs are part of a bigger system, referred to as LLMAs, and interact with the content generated by other LLMAs and the external environment [86]. Furthermore, recent research has shown how generative models are trained on the data generated by other generative models can collapse [67]. Therefore naturally, LLMAs interact with each other either implic- itly or explicitly.\nHence, controlling the dynamics of interacting LLMAS is essential to improve the accuracy and trustworthiness of decisions by LLMAs. To the best of our knowledge, only a few recent works systematically study the behaviors of LLMAs using tools from microeconomics and signal processing [30]. This study aims to bridge this gap by systematically reviewing LLMAs and the different math- ematical frameworks by studying Bayesian social learning in a sequence of LLMAs to achieve Bayesian inference."}, {"title": "2) Interpretable Engineering of LLMAs:", "content": "Many different third-party services have already started providing vari- ous kinds of LLMAs as a service, including Agentforce by Salesforce and IBM AI agents [66]. The underlying intelligence engine of these third-party agents is an LLM or a vision language model (VLM). The LLMAs are used in personal applications for coding, shopping, and scraping data and in enterprise applications for getting insights on user activity and automating industrial workflows.\nTherefore, it becomes imperative to study interpretable models for these agents since many of the proposed appli- cations these agents involve sensitive information (like per- sonal records, financial information, bio-medical data, and personal preferences). By interpretable, we refer to models that facilitate a transparent understanding of complex mod- els through clear and explainable representations of their decision-making processes. The workflows of the AI agents also include making decisions, and the interpretability and reliability of these agents become vital for them to be trustworthy. Therefore, mathematical models are needed to aid in engineering and deploying LLMAS. To this end,"}, {"title": "B. Main Results", "content": "This paper builds on tools from Bayesian revealed preferences from microeconomics (inverse reinforcement learning), sequential Bayesian estimation (from signal pro- cessing), and structured stochastic control (from control theory) to construct interpretable models and synthesize\ninteraction of LLMAs. The impact of our results on more efficient, systematic, and interpretable engineering of LL- MAs is summarized in Figure 2. The main contributions of this paper are:\n1) We propose constructing a LLMA as a composition of a large language model (LLM) sensor, which acts a low-dimensional map from the text space and a Bayesian engine, which uses the measurement from the LLM to update the posterior and act optimally. We show how this model is useful for interpretable Bayesian inference with applications in sequential data on online platforms.\n2) To obtain an interpretable utility function for a LLMA, we provide necessary and sufficient conditions in Theorem 1 for a LLMA to be a rationally inat- tentive Bayesian utility maximizer (RIBUM). For a LLMA who is a RIBUM, we propose Algorithm 2 and Algorithm 3 to reconstruct the max-margin and sparsest utility estimate, respectively. Our methods are applicable both our LLMA and off-the-shelf LL- MAs.\n3) We study Bayesian social learning in a LLMAs, se- quentially estimating a state given text observations and in Theorem 2 show that such a sequence of LLMAs form an information cascade and herd in their actions. We show that this is true for both when no private observations are shared and when a finite number of private observations are shared. Further, we provide a detailed analysis of the effect of the quality of results from LLM of the LLMA and the number of private observations.\n4) To delay herding in a sequence of LLMAS, we for-"}, {"title": "II. BRIEF LITERATURE REVIEW AND RELATED WORK", "content": "In this section, we review some of the substantial related work on LLMs\u00b2, LLMAs and social learning using LLMs. We first provide a brief background on LLMs and discuss the different applications and models for LLMAs. We provide motivation for the interpretability of the LLM agents. Finally, we review literature in sequential state estimation setup studied in this paper and provide motivation for using a sequence of contemporary LLMAs in a classical Bayesian inference setting. We also review applications of sequential Bayesian inference using LLMAs. Table I summarizes some of the related work."}, {"title": "A. Background on LLMs", "content": "Large language models (LLMs) have become om- nipresent in various industry applications in different as- pects, given the drastic improvement in compute avail- ability and rapid development of open and closed-sourced models [63]. They are being rapidly deployed for var- ious applications, including education, information re- trieval, gaming, recommendation systems, and under- standing graphs [21], [34], [36], [64].\nThe primary reason for their proliferation is that they are able to ingest a chunk of multi-modal data and provide useful inferences from them. This is possible because they are deep learning networks (transformer architecture) with billions of parameters trained on massive amounts (in the order of petabytes) of data (Common Crawl, etc.) using extremely fast GPUs that can parallelize computations efficiently. There are two different classes of LLMs: a) open source LLMs like Llama and Mistral [33], [74] and b) closed source LLMs like ChatGPT and Claude. Open source LLMs make available the underlying deep learning architecture that they use, some even share the data that the LLM is trained on; closed-source LLMs on the other hand only provide an inferencing interface, where the LLM can be asked different questions, the answer to which is provided by the LLM. The methods and framework of our paper only require black-box access to these LLMs and are generally applicable to both classes.\nAlthough the adaption of LLMs is rapidly increasing, from a safety and reliability perspective, their deployment in sensitive applications like healthcare, finance, and de- fense still poses challenges [63]. Even in general-purpose applications, LLM-based chatbots provide spurious infor- mation, a phenomenon referred to as hallucination [63]. There have been different approaches to ensure that the outputs of the LLMs are reliable and interpretable, and many of the challenges specific to the new paradigm re- quire revisiting traditional interpretability literature [70]. As reviewed in [84], one of the approaches is to improve the reasoning capabilities of LLMs so that the LLMs provide a descriptive reason for the output. Another school of thought is to mechanistically understand the transformer architecture and training procedure that is the backbone of an LLM [11], [57] to better understand the working and eliminate the sources of bias, if possible."}, {"title": "B. LLM Agents", "content": "Standalone LLMs are powerful tools for many appli- cations, but recent work has proposed using LLMs as part of bigger systems and integrating them into existing workflows. These LLMs often have agency and are referred to as LLM agents (LLMAs) [24]. There are two distinct features that LLMAs have that make them different from LLMS:\n1) Decisions: In the workflows that the LLMAs are used in, they are provided agency using different mecha- nisms, including function calls\n2) Communication: The LLMAs are allowed to com- municate with other LLMAs, to exchange informa- tion. Often, tasks are also broken into smaller sub- tasks and are performed parallelly and sequentially by different LLMAs leading to different topologies of LLMAS [49]."}, {"title": "1) Applications of LLM Agents:", "content": "LLMAs are used for different applications. One application that has especially gained traction is programming, primarily because of LLMs ability to generate code given a text prompt. LL- MAs are used to automate different parts of the soft- ware lifecycle, including development, deployment, testing, and fixing bugs [49]. Other applications propose using LLMAs in healthcare for counselling [28], financial trad- ing [47], automating customer service [66] and shopping assistants [81]."}, {"title": "2) Models for LLM Agents:", "content": "The different components used in the standard model of a LLMA include a memory, retrieval mechanism, action sets, and an environment. [71] studies different cognitive architectures using these components for LLMAs. And there has been a lot of work to improve the capabilities of these components in LLMAs, using a dynamic context [15] and using self notes to perform continual learning [45]. However, we propose augmenting the LLM with a Bayesian engine to perform sequential Bayesian inference on a stream of data. The Bayesian engine model proposed in this paper can also be used for more general tasks, as we discuss in the conclusion."}, {"title": "3) Networks of Agents:", "content": "Since many of the LLM agents interact with other agents directly or through content generated by them, there is a need for more systematic and mathematically rich black-box models for LLMs and LLM agents (LLMAs). Such models help understand their behavior and eventually control it to ensure reliability. Often, the collaboration of LLMAs are modeled as a graph or network [24], which [52] proposes dynamically adapting depending on the task. There are various dif- ferent programmatic frameworks where the LLMAs can be abstractly programmed to perform different tasks [77], [80]. Some of these frameworks even allow making these agents autonomous [86]. The methods in this paper deal with a line graph topology of LLMAs, which perform sequential Bayesian estimation. The setup studied in this paper can be extended to more general graph topologies, and different issues, including data-incest, can be studied."}, {"title": "C. Bayesian social learning in a sequence of LLM Agents", "content": "1) Multiple Bayesian agents sequentially estimating a state: The motivation for multiple such Bayesian agents, each receiving private observations, is motivated by pri- vacy, improved detection, and finite context length. If the same private observation (even the low-dimensional representation) is used, the LLM can be fine-trained on this data, which might contain sensitive information [20]. Also, different LLMs can be given a diverse set of con- texts, which enables reducing the bias involved with their decisions [35]. Also, practically due to the finite context length, the observations can be considered private with respect to consecutive LLMAs evaluations.\n2) Framework of Bayesian social learning: Recently [56] looked at social learning in LLMs using a teacher-student framework, but this work was in a static setting where the LLMs don't have a belief that they adaptively update. In general, sequential social learning in Bayesian agents has been studied extensively [13], and our work formalizes the problem of Bayesian social learning in LLMAs. The theoretical results presented in this paper have been stud- ied before in the context of distributed Bayesian sensors in [39] and [7]. Compared to [7], [39], we look at the LLMAs as interpretable Bayesian sensors and provide a more comprehensive outlook. We also look into multiple observations being shared and provide a concentration inequality for overspending the incentive with respect to a budget constraint. Recently [46], looked at detecting information cascades using deep learning. Although in this paper we focus on delaying an information cascade to improve estimation accuracy, methods similar to [46] can be integrated with our approach.\n3) Incentivization of the LLMAs by a central con- troller: Recent research has studied modeling LLMs as autonomous agents and making LLM part of bigger au- tonomous agents, including robots, self-driving cars, and programming co-pilots [18], [73]. Such autonomous agents can be leased from third-party services at a unit cost. The incentive can also be looked at from the following perspective: providing more context to the same LLM can lead to a more accurate output [35] but increases the cost of processing the query. Third-party LLMAs often offer a tiered pricing structure, where higher pricing provides access to more accurate LLMs."}, {"title": "D. Interpretability and Social Fairness", "content": "There has been recent work in augmenting the LLM with an explainable artificial intelligence (xAI [2]) system to provide more interpretable outputs as in [69]. This work is more aligned with the latter, in which we propose using the LLM as a sensor that provides interpretable low-dimensional outputs, used by a Bayesian engine to estimate the state.\nHowever, we use an interpretable Bayesian model to per- form sequential Bayesian inference from text observations of an underlying state, whereas [69] create an xAI model with decision trees and n-grams models using outputs from an LLM. Another such work is [72], where the authors propose training a separate concept neural network that uses the output of an LLM to interpretably classify text embeddings. This approach can complement the work in this paper when the setting is dynamic. Our work uses tools from revealed preferences and social learning to analyze the behavior of individual and interacting LLMAs from a microeconomic lens.\nFurther, our focus is also on building LLMAS, which are safe, reliable, and fair, goals which are aligned with oper- ationalizing responsible AI [87]. This becomes challenging to do since LLMAs have been known to show biases which are inherent to human beings like conformity [6] and bias towards different attributes [9]. These effects will be more prominent when the LLMAs interact with each other in different scenarios like a Mindstorm [88] or a"}, {"title": "E. Applications of Sequential State Estimation using Bayesian Social Learning in LLMAs", "content": "We detail examples of real-life problems where textual observations of the state are available and sequential Bayesian learning in LLMAs is used to perform state estimation.\n1) Hate Speech Peddler Identification On Social Net- works: Identifying hate speech\u00b3 and toxic content has been studied in various contexts, e.g., in reducing unin- tended bias, detecting covert hate speech, and mitigating hate speech on online platforms [38]. [65] have looked at how to quantify the intensity of hate speech and created labeled datasets. In [31], the authors looked at controlling federated learning for hate speech classification. In this paper, we look at the problem of Bayesian agents identify- ing hate speech peddlers by sequentially parsing comments from users using an LLM.\n2) Financial Networks : In financial networks, LLMs can be used as sensors to parse textual information, including news articles, opinions on social networks, and financial reports. This can be especially useful for making decisions based on the low-dimensional observations from the LLMs. This process can be automated using LLMAS, based on algorithmic rules [47]. But, since the actions of LLMAs of a single entity affect the environment (market), such a sequence of agents can herd in their decisions, lead- ing to a financial bubble. This has been studied classically"}, {"title": "Part I: Analyzing a Single LLM Agent", "content": "In Part I of this paper, we consider a single LLMA in isolation, where we first construct a Bayesian sensor model for a LLMA which comprises an LLM and a Bayesian engine. Then, we look at the LLMA as a rationally inat- tentive Bayesian utility maximizer and propose methods to reconstruct utilities for both our constructed Bayesian LLMA and more general LLMA. The motivation for this modeling from this perspective from the self-attention mechanism inherent to LLMs. Part I of this paper com- prises of Section III and Section IV."}, {"title": "III. LLM AGENT AS A SOCIAL SENSOR", "content": "Motivated by interpretable Bayesian inference on on- line platforms using LLMAs this section discusses the Bayesian sensor model we consider for a single large language model agent. We propose the model of LLMs as a sensing mechanism as a map from a high dimensional space (e.g., text prompt) to a low dimensional space (e.g., structured outputs). The LLMs are equipped with a Bayesian engine and are referred to as an LLM agent (LLMA), which updates the prior regarding the state to be estimated using the text observations. This proposed model is depicted in Figure 4.\nThe different aspects of the mathematical model for the LLMAs are discussed, and the utility of a LLMA is introduced, which can be reconstructed for a black-box LLMA using the Bayesian revealed preference framework"}, {"title": "A. Motivation. LLM Agent for Interpretable Sentiment Analysis.", "content": "Since in many contemporary applications, LLMs are used for inferring the underlying state given a text obser- vation; we construct an LLM (which acts as a likelihood function) equipped with a Bayesian engine, both of which act as a LLMA to perform Bayesian inference on textual data with applications in sentiment analysis on online platforms [50].\nWe further motivate the construction of such a model from the point of view of interpretability, reliability, and controllability. There has been a lot of work done to improve the interpretability of the output of a standard LLM [70]. In the black-box setting, the approach proposes asking the LLMs to provide a reason in addition to the output [84]. This works well in practice for simple applica- tions, however when sequential Bayesian inference needs to be performed on millions of text observations interpreting the reason itself becomes a tedious task. Therefore, we propose using the LLMs as a low-dimensional map from the high-dimensional text space by designing prompts that are useful in analyzing. The LLMs can either be explicitly controlled using the system prompt or their outputs can be restricted to a certain state.\nA Bayesian engine is then used to provide confidence on these low-dimensional variates, which is an easier task to do than on the high-dimensional text data due to the curse of dimensionality. This helps in using the LLMA in a reliable way since the LLMAs can provide confidence in the actions they would take given the observations. Such a model of LLMA is also controllable with respect to the cost function associated with the Bayesian engine, as we illustrate below."}, {"title": "B. Mathematical Model for a Utility Maximizing LLMA Performing Bayesian Inference", "content": "We consider a large language model agent composed of a large language model (LLM) and a Bayesian Engine. 1) Mathematical Model for LLM as sensor: First, we give a mathematical model for a general-purpose LLM. For this paper, we consider black-box access to the LLM and hence both open-source LLMs like Llama3 [74] and closed- source LLMs like ChatGPT. One of the ways to model a black-box LLM is as an input-output block. The input to an LLM is a single text prompt, which we decompose into three things: the system prompt, which we refer to as control, the context, and the user prompt, which we refer to as observation.\nAssume that the dictionary of all words of the LLM (tokens \u2074) is given by D, and this dictionary also includes the blank word. The time index is given by k = 1, 2, . . . . A black-box LLM can be viewed as an input-output block.\nThe control or the system prompt is an input to the LLM, often prepended before the in-context examples and the user prompt, which is used to give instructions to the LLMs on how exactly to respond. This is used to control the behavior of the LLM and ensure that the LLM behaves (outputs) as required. We assume a control of length $m_{\\text{control}}$ and at time $k$ denote the control by $C_k \\in \\mathcal{D}^{m_{\\text{control}}}$.\nFollowing the system prompt, the next input is a context to the LLM. This context could contain external informa- tion and examples that is time-dependent and may depend on previous interactions, which is dynamic and can not be put as a part of the system prompt. We consider a context of length $m_{\\text{context}}$ and at time $k$ denote it by $K_k \\in \\mathcal{D}^{m_{\\text{context}}}$.\nFinally, the user prompt, which we also refer to as the private observation, is the text sequence to which the LLM is supposed to give a response conditioned on the control $c_k$ and the context $K_k$. The private observation at time index $k$ is given by $z_k \\in \\mathcal{Y}'$. Where $\\mathcal{Y}'$ is the text observation space and for a maximum length of $m_{\\text{user}}$, $\\mathcal{Y}' = \\mathcal{D}^{m_{\\text{user}}}$.\nWe consider an LLM, which is pre-trained on trillions of tokens of text to autoregressively generate the next- token \u2075. For developing a token of length 1, the output of the LLM can be given by a conditional probability distri- bution $\\Phi(C_k, K_k, z_k) = P(\\cdot |C_k, K_k, z_k)$, where $z \\in \\mathcal{y}'$ is the user prompt, $c_k$ is the system prompt, $K_k$ is the context, and the function $\\Phi : \\mathcal{D}^{m_{\\text{control}}} \\times \\mathcal{D}^{m_{\\text{context}}} \\times \\mathcal{D}^{M_{\\text{user}}} \\to P(\\mathcal{D})$ outputs a probability distribution over the dictionary $\\mathcal{D}$. For generating an output of an LLM with length $> 1$, we consider a function $g$ which takes in the function $\\Phi$ and outputs tokens from the space $\\mathcal{D}^{m_{\\text{output}}}$, where $m_{\\text{output}}$ is the maximum length of the output. The output of the LLM, denoted by $y$ is obtained as $y = g(\\Phi, C_k, K_k, z_k)$. Therefore, we can represent a black box LLM with the following tuple,\n$\\mathcal{L} = (\\mathcal{D}, m_{\\text{control}}, m_{\\text{context}}, m_{\\text{user}}, m_{\\text{output}}, \\Phi, g).$   (1)"}, {"title": "2) Construction of Bayesian Engine:", "content": "Owing to dimen- sionality reduction because of an LLM sensor, we discuss next how a Bayesian engine uses the low-dimensional output to provide an interpretable model based on which an optimal action can be taken. Note that in general, LLMA need not just be used for Bayesian inference but for more general tasks like coding, shopping assistants, re- search writing, etc., we construct the Bayesian engine to be more general purpose and detail on how a particular cost function leads to Bayesian inference. Also, note that each of the general tasks involves multiple Bayesian inference steps.\nFor a state $x \\in \\mathcal{X}$, the LLMA receives a text observation $y' \\in \\mathcal{y}'$ which the LLM of the LLMA parses and provides a low-dimensional output $y \\in \\mathcal{V}$. The LLMA has a Bayesian engine which has a prior on the state space given by $\\pi$ and an observation likelihood $D$. The LLMA uses the low-dimensional output from the LLM to compute the posterior using Bayes' rule,\n$P_{D, \\pi}(x|y) = \\frac{D_{y,x}\\pi(x)}{\\sum_{x'} D_{y,x'}\\pi(x')}$.\nLet U denote the finite action space of the LLMA. Let $r: \\mathcal{X} \\times \\mathcal{U} \\to \\mathbb{R}$ be the utility that the Bayesian agent receives from taking action $u \\in \\mathcal{U}$ when the underlying state $x$. Then, the Bayesian agent performs the action, which maximizes the expected utility under the posterior distribution.\n$u = \\arg \\max_{u \\in \\mathcal{U}} \\sum_{x \\in \\mathcal{X}} r(x, u)P_{D, \\pi}(x|y)$.\nTherefore, our LLMA can be described as the following tuple,\n$\\text{LLMA}(\\mathcal{L}, \\pi, D, r),$   (2)\nwhere $\\mathcal{L}$ is an LLM of the form (1), $\\pi$ is the prior over the state, $D$. We make the following remarks on our interpretable sensor model construction of a LLMA."}, {"title": "Remark 1.", "content": "We described the operation of Bayesian in- ference on a single observation when there is a stream of observations $Y_1,..., Y_k$ then the prior is updated after every step with the computed posterior $\\pi_{k+1} = P_{D, \\pi_k}$."}, {"title": "Remark 2.", "content": "In Section V, when we describe Bayesian social learning in a sequence of LLMAs, we will note that the equations of the social learning protocol are the same as the above equation. However, the prior of all the subsequent agents is updated based on the actions of agents subsequent to them (even when the rest of the agents do not observe a private observation at any given point), which leads to herding."}, {"title": "Remark 3.", "content": "For the case of Bayesian inference of the states, the action space is taken to be U = X, and one of the possible utility functions is the indicator function r(u,x) = 1(u = x)."}, {"title": "Remark 4.", "content": "The framework presented in this section can be extended to multi-modal models like the vision language models (VLMs [10]) for more general tasks by accordingly modifying dictionary D and output generation mechanism g."}, {"title": "Remark 5.", "content": "The likelihood D can be computed by using the LLM on a set of synthetic or public offline data where we simulate the state and use the text observations to obtain the low-dimensional observations from the LLM."}, {"title": "Part II: Interacting LLM Agents", "content": "We now move on to Part II of the paper. Having studied a single LLMA in isolation in Part I, we next study social learning in an interacting network of LLMAs. There is a lot of work done in studying different topologies of LLMAS [71]. However, we restrict ourselves to the three topologies described in Figure 7. We motivate studying the different topologies to understand, analyze, and explain some of the observed phenomena in LLMs. We study Bayesian social learning in a sequence of LLMAs to ana- lyze information cascade to an incorrect action. Motivated by model collapse observed while training LLMs from their generated dataset, we briefly study a different protocol for social learning in LLMAs: word-of-mouth social learning. Finally, we illustrate how data-incest can arise if LLMAs perform Bayesian inference in an asynchronous fashion. Part II comprises Section V and Section VI."}, {"title": "V. BAYESIAN SOCIAL LEARNING IN A SEQUENCE OF LLM AGENTS", "content": "LLMs are already trained on synthetically generated data by other models [67] and also often use the output of other LLMs to output based on the current context [23]. Motivated by studying interacting LLMs, each of which has computational and privacy constraints, this section introduces a second layer of abstraction, wherein we study Bayesian social learning in a sequence of large language model agents. We first motivate the setting where a se- quence of LLMAs sequentially estimate a state from their private observations and take a public action, which is used to update the public prior. We discuss the Bayesian social learning protocol in a sequence of LLMAs, which aim to detect an underlying state by sequentially analyzing text observations of the text. The optimal update equation for the public prior is derived. We consider two scenarios, one where no private observations are shared and one where private observations are shared to the next M LLMAs."}, {"title": "A. Motivation. Interacting LLMs, Finite Context Length and Privacy in LLM agents.", "content": "Even if a single LLMA is used in an application, it can be treated as a sequence of different LLMAs since the context of the previous LLMA evaluation might not be available due to privacy of the content and finite- context length [24]. We consider two scenarios, one where no private observation is shared between the LLMAs and the second where each LLMAs can observe previous M agents. This is motivated by practical constraints from the perspective of privacy, context length, and cost incurred inherent in using LLMAs.\nWe motivate studying LLMAs using a Bayesian social learning perspective with the following constraints:\n1) Privacy: Since the text observations often contain sensitive information, the text observations can be used to train the LLM of the LLMA [60]; hence, to prevent this often, systems involving LLMs often treat the private ob- servation in a one-shot setting where the private observa- tion is not stored. Even the low-dimensional representation of the text observation might contain information that can be used to identify attributes of the person the text ob- servation comes from, and in a social network application, this can lead to unfair decisions by LLMAs [53]. Therefore to preserve privacy of users, the LLMAs we consider either do not share the private observation or only share a limited sequence of private observations.\n2) Limited Context Length and Storage Constraints: Another constraint is the limited context length (length of text that the LLM can process at a time) that is inherent to the LLM used in the LLMA. Note that there are methods which allow for infinite context window or a very large (1 million) context window, however these take a lot of time to work which is often not feasible in a real-time Bayesian inference setting. Also, the quality of the responses decreases with increasing context [35]. There are storage constraints that won't allow storage of an arbitrary amount of private observations, especially if there are methods (like those presented in this paper) that don't require storage of the private observations.\n3) Computational Resources and Cost: Computational resources (involving GPUs) are often limited, especially if the same LLM deployment is used for different appli- cations. Also, the attention mechanism is such that the computational complexity grows quadratically (linear for state space LLMs) in the input length. Therefore, it is"}, {"title": "B. Social learning protocol when no private observation is shared", "content": "A sequence of large language model agents (LLMA) wishes to estimate an underlying state $x \\in \\mathcal{X}$, where $\\mathcal{X}$ is finite dimension discrete space. At time"}]}