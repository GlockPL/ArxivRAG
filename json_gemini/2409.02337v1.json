{"title": "Coaching a Robotic Sonographer: Learning Robotic Ultrasound with Sparse Expert's Feedback", "authors": ["Deepak Raina", "Mythra V. Balakuntala", "Byung Wook Kim", "Juan Wachs", "Richard Voyles"], "abstract": "Ultrasound is widely employed for clinical intervention and diagnosis, due to its advantages of offering non-invasive, radiation-free, and real-time imaging. However, the accessibility of this dexterous procedure is limited due to the substantial training and expertise required of operators. The robotic ultrasound (RUS) offers a viable solution to address this limitation; nonetheless, achieving human-level proficiency remains challenging. Learning from demonstrations (LfD) methods have been explored in RUS, which learns the policy prior from a dataset of offline demonstrations to encode the mental model of the expert sonographer. However, active engagement of experts, i.e. Coaching, during the training of RUS has not been explored thus far. Coaching is known for enhancing efficiency and performance in human training. This paper proposes a coaching framework for RUS to amplify its performance. The framework combines DRL (self-supervised practice) with sparse expert's feedback through coaching. The DRL employs an off-policy Soft Actor-Critic (SAC) network, with a reward based on image quality rating. The coaching by experts is modeled as a Partially Observable Markov Decision Process (POMDP), which updates the policy parameters based on the correction by the expert. The validation study on phantoms showed that coaching increases the learning rate by 25% and the number of high-quality image acquisition by 74.5%.", "sections": [{"title": "I. INTRODUCTION", "content": "Medical ultrasound imaging is one of the most widely used imaging modalities for diagnostic and interventional procedures. Its widespread adoption can be attributed to its affordability, portability, non-invasiveness, absence of ionizing radiation, and real-time feedback. These merits make it particularly suitable for general use in low-income and developing regions. Unfortunately, the increasing demand for medical ultrasound, coupled with the substantial training required to become a proficient sonographer, and the significant impact of sonographer experience on diagnostic accuracy, contribute to a persistent gap in accessibility to this fundamental and valuable diagnostic tool [1].\nRobotic Ultrasound (RUS) holds great promise to overcome these drawbacks. Further, RUS broadens the pool of skilled practitioners, enhances the safety of healthcare workers during pandemics, and improves the accessibility of ultrasound in rural areas where trained human practitioners are scarce [2], [3]. A robot, once trained to an expert level, not only performs the procedure but also opens the possibility to train novice human operators. This dual functionality can significantly alleviate the training burden and offer a sustainable solution to the persistent gap in the availability of trained sonographers. Thus, transferring this skillset from an expert human operator to a robot is a key research question [4]. Novice sonographers learn this procedure by observing experts, followed by self-practice under the supervision of an expert, who may interrupt and provide corrections as needed. It brings into the picture the critical element of their training: coaching [5]. It is defined as an active engagement of human experts in a trainee's learning process.\nCoaching by an expert guides novice humans to learn more rapidly and to achieve high levels of performance [6]. Surprisingly, there is a dearth of coaching research in RUS. Although, it has started gaining prominence in robot learning literature [5], [7]. For RUS, Deep reinforcement learning (DRL) have been explored to mimic the training process of humans under reward-based self-supervision [8], [9]. However, these methods suffer from longer training times, local minima issues, and limited adaptability to unforeseen scenarios. Later, several works leveraged the knowledge of experts in"}, {"title": "A. Related work", "content": "1) Robotic ultrasound systems: Earlier RUS used visual servoing to automate the ultrasound procedure [12], [13], but these methods lack human-level anatomical knowledge, a crucial factor for sonographers in relating probe motions to the complex anatomy. To supplement this knowledge, later methods used high-end imaging modalities like MRI, CT, or 3D data [14], [15] to identify the anatomical landmarks and pre-plan the probe trajectory. However, the sparse availability of these modalities in underserved regions did not solve the underlying issue. Recent works have explored the use of DRL architectures to mimic the self-supervised practice of human trainees [8], [9], yet the clinical applicability of these systems was questioned due to longer training times.\n2) Experts' engagement in learning of RUS: Researchers explored the engagement of ultrasound experts through LfD approaches to reduce training times. Lonas et al. [16] used the Gaussian Mixture Model (GMM) and Gaussian Mixture Regression (GMR) to learn a probe motion model from offline demonstrations. However, the model did not include the probe forces and ultrasound image information, which limited its clinical applicability. Li et al. [17] used the dataset curated from experts' demonstrations to sample probe poses during model-free RL. This approach has two demerits. First, a very large dataset would be required to handle human anatomical variability. Second, sampling from a large dataset would be computationally expensive, hence impractical for RUS. Raina et al. [18]\u2013[20] proposed to model the Gaussian process (GP) prior and kernel from offline expert demonstrations. Later, this pre-trained GP was used in the Bayesian optimization framework for the robotic acquisition of optimal images. Jiang et al. [11] and Burke et al. [21] inferred the reward for optimizing the RUS policy from scanning demonstrations, which assumed that the images shown in the later stage are more important than the earlier images. It is important to note that the above-cited works proposed to learn offline using a dataset collected from expert demonstrations, which often require numerous optimal demonstrations. Additionally, these approaches are goal-driven, while the proposed coaching framework allows updating the policy objectives and parameters through local corrections to the robot's trajectory.\n3) Coaching robots: Early coaching-based methods incorporated diverse human feedback within reinforcement learning methods [22]. Macglashan et al. [23] proposed COACH (Convergent Actor-Critic by humans) that extended RL with online reward/penalty from experts. However, COACH converged only to local minima and relied on sparse binary evaluations using goal examples. As such, this method demands substantial sampling, greater learning time, and a large feedback volume. Later, preference-based learning techniques were introduced into DRL, which involved presenting action samples (trajectories) to experts for grading and later enhancing behavior based on their preferences [24], [25]. While preferences aid improvement, the challenge remained in estimating trajectories to elicit preference-based feedback, resulting in inefficient use of expert's input. Expert feedback is not only evaluation but also provides useful information on direct trajectory modifications [26]. Therefore, the robot should leverage these corrections to optimize actions and update rewards.\nRecent strategies in physical human-robot interaction (pHRI) considered corrections as informative rather than disturbance [27]. Online learning from correction aimed to refine objectives and generate the trajectory based on comparisons between corrected and original trajectories [28], [29]. Similar techniques have been employed in shared autonomy in manipulation [30]. However, the challenge lies in the robot understanding the context of interactive feedback and updating its objectives and policy accordingly. A formalism for coaching that combines policy updates and learning new objectives is lacking in the robotics literature. In order to address these challenges, we extend Partially Observable Markov Decision Process (POMDP) formalizations from pHRI and shared au- tonomy to coaching the DRL policy. Moreover, we demonstrate its applicability to previously unexplored and highly expert-dependent modality of medical ultrasound imaging."}, {"title": "II. METHODOLOGY", "content": "The pipeline of the methodology is outlined in Algorithm 1, which combines self-supervised practice through DRL with coaching through sparse expert feedback. The robot learns a DRL policy to perform ultrasound under self-supervision. During learning, the expert provides online feedback through kinesthetic corrections, which will update the policy objective and parameters towards optimal policy."}, {"title": "A. Self-supervised practice through DRL", "content": "This section describes a self-supervised practice through DRL policy, which is learnt using a reward formulation based on ultrasound image quality. The robot model executes in a finite bounded horizon, with a Markov decision process M, with state space S and action space A. For a horizon T, the state transitions according to the dynamics $T : S \\times A \\rightarrow S$. A policy $\\pi_{\\theta}(a|s)$ represents the probability of taking action a given a state s, with parameters \u03b8. The cumulative expected reward over horizon T is given by eq. (1).\n$J(\\pi) = E_{\\pi} \\sum_{t=1}^{T} r(s_t, a_t)$ (1)\nWe used an off-policy algorithm, Soft Actor-Critic (SAC) [31] to learn the policy. This is due to the better sample efficiency and generation of stable policies by SAC, which is suitable for practical applications such as manipulation [32]. This algorithm maximizes the cumulative reward and entropy to learn the policy.\n1) State space: The state S is defined based on the ultrasound image. We have adopted an image quality classification network from our previous work [33], which used ResNet50 as a base network with multi-scale and higher-order processing of the image for conducting the holistic assessment of the image quality. The block diagram of this network is shown in Fig. 2. This classifier first extracts features at multiple scales to encode the inter-patient anatomical variations. Then, it uses second-order pooling (SoP) in the intermediate layers (local) and at the end of the network (global) to exploit the second-order statistical dependency of features. The local-to-global SoP will capture the higher-order relationships between different spatial locations and provide the seed for correlating local patches. This network encodes the image into a feature vector of size 2048, which represents the state of the policy.\n2) Action space: The action space A for the SAC policy is a combined position, orientation, and forces of the probe. Specifically, the robot controls the position of the probe in the xy-plane; orientation along roll, pitch and yaw; and force along the z-axis (normal to the surface).\n3) Rewards: The reward is based on the ultrasound image quality estimated using the same network [33], which represents the state. The extracted image features are passed through a linear classifier layer to generate a feature vector of size 5. Finally, the index of maximum value for this feature vector gives an integer quality rating between 1 \u2013 5. In addition, a quality rating of 0 is assigned when the measured force value of the probe along the z-axis is below the minimum required for the appropriate contact. The reward is then defined as\n$r_u = \\eta \\mathbb{I}(q == q_{max}) + q/q_{max}$ (2)\nwhere q is the quality of the image, and the expression $\\mathbb{I}(q == q_{max})$ is 1 if the quality is maximum and 0 otherwise. The constant \u03b7 is used to amplify the reward when the robot reaches the maximum image quality (i.e., $q = q_{max}$). This image quality-based reward guides the self-supervised learning of the ultrasound policy."}, {"title": "B. Coaching with sparse expert's feedback", "content": "The coaching scenario for RUS is shown in Fig. 1. It is treated as learning a hidden goal (g*) by observing the corrective actions (a) provided by the expert. We develop a formalism for representing coaching as a partially observable dynamical system. Coaching aims to improve the objectives and parameters through local corrections to the trajectory. RUS can only observe the coach's corrections $a^c$ and its actions $a^u$. It acts according to its optimal policy (\u03c0\u03b8), however, the coach expects it to operate with respect to a true objective whose optimal actions are determined by $\u03c0_{\u03b8^*}$. The coach does not directly provide parameters $\u03b8^*$, nor does the RUS know $\u03c0_{\u03b8^*}$. In DRL policy learning, we assumed that the goal states (g) are known and the reward is computed based on eq. (2). However, a correction from the coach implies that the goals and the policy need to be updated. If the current policy parameters $\u03b8 = \u03b8^*$, then the formulation is an MDP where the robot is already behaving optimally with respect to expectations. However, when $\u03b8 \\neq \u03b8^*$, the robot cannot directly observe $\u03b8^*$ to update its policy. Further, the robot cannot observe the coach's expected goals g* as well. Therefore, the uncertainty in the objectives g* and corresponding policy parameters $\u03b8^*$ turns this into a Partially Observable Markov Decision Process (POMDP).\n1) POMDP model: In this POMDP, g* forms the hidden part of the state $\u03be$, and the coach's corrective actions $a^c$ are observations about g* and $\u03b8^*$ under some observation model as $O(a^* = (s,g^*),a^c)$. The observation of the coach's correction allows the robot to learn the true objective. The coach's feedback is modeled as corrections that optimize the expected return from the state s while taking action $a^u + a^c$. The action-value function (Q) captures this expected return. Thus, it can be written as:\n$Q(a|s, a^c) \\propto \\mathbb{E}_{\\pi_{\u03b8^*}} r(s, a^u + a^c)$ (3)\nThe relationship for the observation model indicates that the coach provides feedback, which together with the robot's action, will lead to the desired behavior. And similar to human coaching, the robot is expected to continuously learn a better objective by observing the feedback. This formal approach captures the true essence of human coaching. The uncertainty is in the estimate of the desired goals g* and the corresponding policy parameters $\u03b8^*$. The environmental state s, which is part of the POMDP state $\u03be$, is assumed to be fully"}, {"title": "observable, similar to POMDP-lite [34]. However, the robot cannot observe the goal part of the state \u03be. Instead, the robot can learn an action-value function over belief states b(s) as follows:", "content": "$Q^*(b, a^u + a^c) = E[r(b, a^u + a^c) + E_\\pi [V^*(b')]]$ (4)\nNote that solving a POMDP with continuous action and states is expensive and usually intractable. Several approaches to estimate and approximate solutions for a POMDP have been explored in literature, such as hindsight optimization [30] or reduction to QMDP [35]. We provide an approximation to simultaneously update the policy parameters \u03b8 and the action value at the corrected states.\n2) Approximate solution: In order to solve the POMDP, we transform it into a policy update problem. Several approximation are defined to achieve the policy update, which include trajectory correction, reward formulation, and policy parameter computation. These approximations provide an elegant solution to the POMDP model.\nTrajectory correction: The Q-value shown in eq. (3) cannot be computed for continuous state and action spaces. Therefore, the reasoning is done in the trajectory space instead of the control action space. The trajectory space is defined based on pose and force profiles resulting from the DRL policy learning. First, the robot estimates a trajectory based on the learned policy $\u03c0_{\u03b8}(a|s)$ derived from the observed goal g. The robot then utilizes an in-built hybrid force-position controller to track this trajectory. Instead of computing the Q-value for each action, we can estimate the total reward resulting from following this trajectory $\u03a0^u$. Fig. 3 shows an example of a scenario where the robot is following the trajectory determined by the DRL policy. The robot trajectory can be represented as a sequence of control inputs resulting from the actions generated by $\u03c0_{\u03b8}$. The coach can apply a correction at any instant after some duration of DRL policy learning. This corrective action $a^c$ is provided at a point $p_c$ on the robot trajectory. However, the point correction has to be propagated across a local region of the trajectory. A trajectory optimizer is used to obtain the coach-preferred trajectory. The coach-preferred trajectory ($\u03a0^c$) is obtained by smoothly deforming the robot trajectory ($\u03a0^u$) locally using minimum jerk constraints, as follows:\n$\u03a0^c = \u03a0^u + \\mu \u03a0^c(a^c)$ (5)\nwhere \u03bc > 0 is a scaling factor for the deformation and $\u03a0^c$ is offset to current trajectory based on action $a^c$ computed using minimum jerk optimization. The minimum jerk trajectory is computed by obtaining a trajectory $\u03a0^c$ that minimizes the integral of a squared jerk over time. The solution to this optimization is a trajectory represented by a quintic polynomial as $\u03a0_i = \\sum_{i=0}^{5} k_i t^i$. Here, t is the time, and $k_i$ are coefficients of the polynomial to be determined based on control points and boundary conditions. The first three constants can be determined from the initial position, velocity, and acceleration at t = 0. Similarly, the last three can be estimated based on the final or target position, velocity, and acceleration. For the case of offsetting the robot trajectory $\u03a0^u$, we generate piecewise minimum jerk trajectory with control points at a $p^c$ and two points on either side of $p_c$ spaced at a time distance determined by the scale of the correction. Once the trajectory is updated, we can run the robot along the new trajectory and discover the associated states observed while moving on the expert-preferred trajectory.\nReward modification: The coach-preferred trajectory is only a trajectory offset based on the corrective action. The robot has not learned how to leverage the knowledge from the preferred trajectory to improve the policy. We propose augmenting the reward objectives with two components as coach reward $r_c(s,a)$ and trajectory reward $r_\u03c0(s,a)$, as follows:\n$r(s, a) = w_u r_u(s, a) + w_c r_c(s, a) + w_\u03c0 r_\u03c0(s, a)$ (6)\nwhere w* is the weight associated with corresponding rewards (r*). The coach reward $r_c$ associates the states observed by following the coach-preferred trajectory $\u03a0^c$ with a small positive reward. The trajectory reward $r_\u03c0$ penalizes large changes in poses or forces to ensure that the policy generates a smoother path. The reward weights $w_c$ are learned by maximizing the return to choose the preferred trajectory over the old robot trajectory\nPolicy update: In addition to updating the reward or goals, the policy parameters are offset to move towards the coach's optimal policy $\u03c0_{\u03b8^*}$. Several iterations of coaching will generate multiple desired states and segments of preferred trajectories. The state transitions, the corresponding rewards from the new reward model, and the actions along these trajectories are stored in a separate replay buffer, named as coach replay buffer. Then, an approximation of the desired optimal policy is computed. The approximate policy $\u03c0_{\u03b8^*}(a|s)$ generates the actions needed to move along the preferred trajectories. The parameters of the policy are represented as $\u03b8^*$ because they are an approximation of the true optimal parameters $\u03b8^*$. The policy can be written as $\u03c0_{\u03b8^*}(a|s) \\rightarrow \u03c0(a|s, \u03b8^*)$. A Gaussian distribution is used for the approximate policy, i.e., $\u03c0(a|s, \u03b8^*) \\sim N(\u03bc(\u03b8^*), \u03c3(\u03b8^*))$. We know the sequence of states s on the expert-preferred trajectories and the corresponding action to take in these states. Therefore, we aim to estimate the policy parameters $\u03b8^*$ that fit this state and action data. The parameters are estimated using maximum likelihood estimation. Once the approximate policy is found, the current policy is regularly updated by performing gradient steps of SAC on the coach"}, {"title": "III. RESULTS AND DISCUSSIONS", "content": "A. Experimental setup\nThe experimental setup is shown in Fig 4. It consists of a 7-DOF Rethink Robotics Sawyer arm, with a Micro Convex MC10-5R10S-3 probe by Telemed Medical Systems, USA, attached to its end-effector using a custom-designed gripper. The robot has a wrist-mounted force-torque sensor, which was used to measure the forces. Two urinary bladder phantoms, P0 and P1, were used for scanning. P1 is a modified variant of P0 with a ballistic gel layer. ROS was used as the middleware to transmit images and commands across the devices.\nB. Implementation details\nTo analyze the effect of coaching on learning performance, we conducted experiments by inducing coaching corrections at different iterations of training. Four policies were trained for the following cases: (i) No Coaching; Coached after every (ii) 20k, (iii) 10k, (iv) 5k timesteps. The policy with no coaching was based on reward in eq. (2) and policies with coaching were based on a combined reward in eq. (6). Each policy was trained for a maximum of 200k steps or until convergence, approximately equal to eight hours of wall clock time. The robot was initialized to a random pose for each episode of training. An episode of training ends upon either achieving the high-quality image or reaching the 50 steps. The value of \u03b7 in eq. (2) is empirically set to 10 to maximize performance. The action space limits are set as $x \\in (-0.05, 0.05)m$, $y \\in (-0.03, 0.03)m$, $f_z \\in (5 - 30)N$, $roll \\in (-0.2, 0.2)rad$, $pitch \\in (-0.2, 0.2)rad$, and $yaw \\in (-0.5,0.5)rad$. The roll, pitch and yaw angles were carefully selected to ensure collision-free scanning of the phantoms. The hybrid position-force control mode of Sawyer was used to control the robot. The coaching corrections are provided by human experts through kinesthetic interactions with the robotic arm. At the instant of interaction, the robot movement is paused, which was hardcoded in the training script. Then, the expert activated the free-drive mode of the robot by pressing the button provided on the wrist and nudged the robot toward the optimal trajectory. Once the expert finished the coaching, which is indicated by a high-quality image (q \u2265 4), the training script was re-initialized from the same time step. Note that the policy weights were updated before re-initialization.\nC. Performance evaluation\nFig. 5 compares the normalized average reward over the training timesteps for different policies. One of the primary issues associated with the \"no coaching\" policy was the tendency to explore sub-optimal states (i.e., low image quality regions). The poses and forces necessary for producing high-quality images constitute a narrow domain within the action space. Consequently, the \u201cno coaching\u201d policy often chooses inappropriate actions (probe poses and force). To correct this behavior, the coach intervened by adjusting either the probe position or orientation relative to the phantom surface, or by modifying the force applied to the phantom. After coaching, the reward from the resulting policy demonstrated improvement at each step of the training, wherever it was introduced. Coaching also improved the learning efficiency, as seen by the faster convergence of coached policies in Fig. 5. In the absence of coaching, the policy failed to approach the optimal region even after 100k steps and showed no convergence even at 200k steps. In contrast, policies with coaching introduced at intervals of 20k and 10k timesteps demonstrated convergence at approximately 150k (\u2193~25%) and 120k (\u2193~40%) steps, respectively. The policy receiving maximum coaching (every 5k steps) converged at 100k steps. These findings underscore the efficacy of coaching in identifying optimal policies at a faster learning rate."}, {"title": "IV. CONCLUSION AND FUTURE WORK", "content": "This paper presents a coaching framework for RUS to improve the learning efficiency and accuracy of ultrasound image acquisition. Unlike previously proposed LfD methods for RUS, this framework leveraged real-time feedback from experts during the training process. Coaching is modeled as a Partially Observable Markov Decision Process (POMDP), which approximates the trajectory-based corrections from the expert to update the reward, objectives, and parameters of the DRL policy. When tested for an ultrasound of urinary bladder phantom, this methodology improved the convergence rate of learned policy by 25% and increased the number of high-quality image acquisitions by 74.4%. Future work will explore further improvement in performance through policy priors derived using offline expert demonstrations, as done in our previous works [18], [19]."}]}