{"title": "Survey of User Interface Design and Interaction Techniques in Generative Al Applications", "authors": ["Reuben Luera", "Ryan A. Rossi", "Alexa Siu", "Franck Dernoncourt", "Tong Yu", "Sungchul Kim", "Ruiyi Zhang", "Xiang Chen", "Hanieh Salehy", "Jian Zhao", "Samyadeep Basu", "Puneet Mathur", "Nedim Lipka"], "abstract": "The applications of generative AI have become extremely impressive, and the interplay between users and AI is even more so. Current human-AI interaction literature has taken a broad look at how humans interact with generative AI, but it lacks specificity regarding the user interface designs and patterns used to create these applications. Therefore, we present a survey that comprehensively presents taxonomies of how a human interacts with AI and the user interaction patterns designed to meet the needs of a variety of relevant use cases. We focus primarily on user-guided interactions, surveying interactions that are initiated by the user and do not include any implicit signals given by the user. With this survey, we aim to create a compendium of different user-interaction patterns that can be used as a reference", "sections": [{"title": "1 Introduction", "content": "The academic and general population have become increasingly enamored with generative artificial intelligence (AI) as it continues to revolutionize just about every field of study it is involved in. So much so, in fact, that the field of Human-Computer Interaction (HCI) has shifted much of its focus to study a sub-field of HCI called Human-AI Interaction (IXDF, 2024). While current Human-AI Interaction literature provides a broad view of the field (Shi et al., 2023), this survey aims explicitly to capture the current state of user interfaces and the respective user interactions being utilized within generative AI applications. Specifically, this survey takes a snapshot of current trends and design techniques that involve user-guided interactions (Sec. 2.1).\nIn doing so, we aim to create a design compendium that generative AI designers, researchers, and developers can reference to understand the current state of the user experience (UX) and user interface (UI) designs of generative AI. The overall goal is to lower the barrier to entry for those interested in the UX and UI of generative AI by giving them a foundation upon which to build. For designers and developers specifically, this paper can serve as a design library to inspire their designs for generative AI applications. This paper prevents designers and developers from needing to partake in large competitive analyses and allows them to learn from design patterns currently utilized by other generative systems. For researchers, this survey can guide further explorations of human-AI interaction. This paper lists dozens of different ways that humans and generative AI applications are currently interacting and can serve as a foundation for researchers to dive into a specific area of human-AI interaction or to dive into the area in a general sense."}, {"title": "1.1 Summary of Main Contributions", "content": "This survey contains several major contributions to the human-AI interaction field based on our survey of more than a hundred relevant generative AI articles. The key contributions include key definitions and disambiguation, relevant taxonomies, and research-based design principles. Specifically, the key contributions of this work are as follows:\n1. A formalization of the key notions and definitions and a disambiguation and expansion of key terms relating to UI & interactions for generative AI applications (Section 2). We formalize vital definitions that are relevant to understanding how a user interacts with a generative artificial intelligence system. We also disambiguate user interactions by proposing a novel concept of user-guided interactions, which are interactions that a user engages in willingly and deliberately. These do not include implicit interactions or interactions that the generative system detects without the user's knowledge. Additionally, we outline the different modality types that users can utilize to interact with generative systems. In doing so, we aim to enhance the general understanding of user interaction terms and their pertinence to generative AI.\n2. A survey and taxonomy of UI interaction techniques for generative AI systems (Section 3). We highlight and categorize common user-guided interaction design patterns and the context in which they are used. We focus on user-guided interactions, as these are examples of how users deliberately and intentionally communicate with the generative systems. We organize these interactions into a taxonomy highlighting prompting, selecting, system and parameter manipulation, and object manipulation. We aim to create a compendium of user-guided interaction techniques that designers can refer to as they plan the designs for their own generative AI applications.\n3. A survey and taxonomy of user interface layouts for generative AI systems (Section 4). We survey key user interface design patterns utilized in various generative applications. In doing so, we present a taxonomy that categorizes and highlights common user interface structures we found through the survey. By generalizing standard UI layouts, we prevent designers from starting from scratch and encourage using these common generative AI design patterns.\n4. A survey and taxonomy of human-AI engagement levels for generative AI systems (Section 5). We survey human-AI engagement levels, which consist of the intensity of deliberate"}, {"title": "1.2 Scope of this Article", "content": "This survey focuses on user-guided interaction techniques for generative AI applications. Such user-guided interactions are sometimes referred to as controllability techniques. One simple example of such techniques is when a user prompts a generative model via text and/or images; similarly, another example is when a user selects text or a specific part of an image to control the generation. We do not attempt to survey interaction or controllability techniques that are not user-guided, nor do we survey techniques that leverage user/system feedback and the like. Given this, the purpose of this paper is to provide a recommendation on which interaction techniques are most effective when applied to specific use cases."}, {"title": "2 Background & Preliminaries", "content": "We begin with basic definitions and notations to formalize the terms that will be used in subsequent sections as we discuss user-guided interactions and connected concepts. We formally define and discuss the notion of user-guided interactions and explain concepts such as the interplay and differences between prompting and inputs. Then, finally, we discuss different modalities that users can utilize to interact with the generative AI systems."}, {"title": "2.1 User-Guided Interactions", "content": "We begin by defining user-guided interactions in order to distinguish them as a specific type of user interaction. While user interactions are any interactions, whether explicit or implicit, made by a user to affect a system, user-guided interactions focus solely on interactions that are explicitly made by users to affect a system in a pre-desired way. Given this, this paper will only focus on user-guided interactions and how they are used in the context of generative AI.\nDefinition 1 (USER GUIDED INTERACTION). User-guided interactions are defined as explicit user-initiated actions that a user deliberately makes that affect the respective computer system.\nIn terms of generative AI, user-guided interactions consist of any actions that a user explicitly takes to affect the generative system. This can be anything from prompting the system to complete a certain task, to selecting and manipulating objects within a system, to adjusting a system's parameters to create a specific output. For example, a user might write a prompt that generates an image. Then they might adjust sliders or select specific parts of the image to manipulate it further. All of these are examples of user-directed interactions. This definition does not, however, include implicit user interactions, which are implicit or indirect actions made by the user that the system acts upon without being explicitly tasked to do so. Implicit user interactions include implicit behavioral interactions that the system uses to create a user profile or implicit feedback where the system infers user satisfaction based on word cues or interaction delays from the user. For instance, a system might alter its answers based on a user's chat history or surface different news"}, {"title": "2.2 Prompts and Inputs", "content": "Definition 2 (INPUTS). An input is a piece of data, information, or content that the user uploads to the system. An input, if available, is what the prompt acts upon.\nDefinition 3 (PROMPTS). A prompt is a type of user-guided interaction in which the user asks the generative system to complete a certain job.\nPrompting a generative system is often the most commonly thought of user-guided interaction associated with generative AI. As mentioned, it consists of a user asking a generative system to complete a specific task. While text-prompting is the most commonly thought of prompting modality, other prompting modalities include visual, audio, and multi-modal prompting. In essence, prompting is an essential way that users interact with and guide the systems that they are working with. Meanwhile, inputs are data, information, or content that is uploaded to the generative system. Like prompts, inputs can be text-based, visual, or audio. In tandem, or sometimes on their own, these are two important aspects of the generative AI user flow.\nAn important distinction is that this section does not consider \"prompts\" as an input. Instead, we view user prompts and inputs as often two separate entities, where a prompt is used to query the system, and the input is what is being acted upon. As seen in Fig. 1, if there is an audio editing generative system, then the input would be an audio file, but the user prompt can be text such as \"Can you edit this audio clip so that it is only one minute long?\". Thus, the input and prompting are distinct since the prompt is a user-guided interaction that acts on the input video.\nWe separate \"prompting\" and \"inputting\" as two different terms because, from a user interaction point of view, they are distinctly different. Whereas an input interaction is essentially some data that the generative system is acting upon, prompting is the user-guided interaction that consists of actually instructing the system to complete a specific task. Furthermore, distinguishing between the two components simplifies and focuses the upcoming discussion sections: Input Modalities (Sec. 2.3) and Prompting (Sec. 3.1)."}, {"title": "2.3 Input Modalities", "content": "This subsection will focus solely on the input modalities that a user can use when interacting with generative AI systems. As they pertain to generative AI user interactions, we define inputs as data or information that the generative AI system is capable of analyzing or processing to generate a different output. After surveying the modalities of different generative AI systems (Fig. 3), we present the different input modalities currently in use: text-based inputs (Sec. 2.3.1), visual inputs (Fig. (Sec. 2.3.2)), and sound inputs (Sec. 2.3.3)."}, {"title": "2.3.1 Text-based", "content": "Natural Language: Text-based natural language is a modality commonly used to interact with generative AI systems (Achiam et al., 2023; Padiyath & Magerko, 2021; Suh et al., 2023a; Wang et al., 2024c; Petridis"}, {"title": "2.3.2 Visual", "content": "Images: Using images as an input to interact with generative systems has become common, as users attempt to do everything from generating new images (Padiyath & Magerko, 2021; Jeon et al., 2021; Betker et al., 2023), to captioning images (Singh et al.; Alayrac et al., 2022), to creating infographics (Setlur et al., 2016). In essence, the image modality consists of the user interacting with images of any type, whether they are infographics, photographs, illustrations, etc., and many systems have differing and unique uses for image inputs. For example, one aspect of Singh et al.'s FigurAlly is creating alt text for figures in research papers. A user inputs figures and the system will create captions and alt text to increase accessibility or even help the reader get a deeper understanding of the paper. Overall, image inputs have a wide range of uses and are an extremely versatile modality as it pertains to generative AI systems.\nVideos: Videos are also a common input and output modality for generative systems and are especially common in multi-modal LLMs (MMLLMs) (Liu et al., 2023b; Cho et al., 2024; Gao et al., 2023; Wu et al., 2023; Goyal et al., 2023). Users can use these systems to do anything from generating or finishing videos (Cho et al., 2024; Liu et al., 2023b) to highlighting or annotating them (Liu et al., 2023b). For example, in InternGPT Liu et al. (2023b), a user can input a video and prompt the system to complete a task pertaining to that video. So in the system, a user could prompt it to edit an inputted video so it matched a TikTok format. Generative AI interactions with video modalities, such as this, have a wide range of uses for both professional and amateur users.\nVisual Interactions: Visual interactions are an input modality that consists of any visual interaction or gestural movement that the system records as an input. This includes visual movements in virtual or augmented reality spaces (Giunchi et al., 2024; Konenkov et al., 2024; Doe et al., 2019) or directly manipulating UI elements in a way the system takes as an input (Lin & Martelaro, 2024; Jiang et al., 2023; Suh et al., 2023a; Masson et al., 2023; Kim et al., 2023b). For example, Konenkov et al. (2024)'s VR GPT allows users to gesture at certain items to ask the system to interact with it. So if a user is trying to learn how to correctly pack a medical bag, they may gesture at a first-aid item and ask, \"What is that?\" In doing so, their pointing gesture is recorded in the system as an input and is used in tandem with the spoken prompt to interact with the system in a unique way."}, {"title": "2.3.3 Sound", "content": "Speech: Speech is a growing medium that more and more generative systems can interact with. In most use cases, speech is often used or generated by the system to help the user complete a speech-related task. Take Borsos et al. (2023)'s AudioLM, which is capable of taking a recorded spoken input and generating an \"end\" to the recording. So if a user generates the first half of a speech, AudioLM can generate the rest of the speech. Interactions that use speech as a modality, such as this one, often create novel and relevant use cases for their respective users."}, {"title": "3 User-Guided Interactions", "content": "Definitions and Scope: In the case of generative AI, user-guided interactions are defined as explicit and deliberate engagements between the user and the system that the user initiates or guides in the generative process. Therefore, the scope of this section will focus primarily on the actual user interactions explicitly performed by the user, and not implicit interactions such as how a user's interactions can implicitly inform an AI system over time about their preferences. Many of these interactions are performed by interacting with UI elements, which, in this case, are defined as the visual components that users interact with to manipulate the generative process.\nIn the context of generative AI, we propose the following taxonomy that categorizes user-guided interactions into the following categories: prompting (Section 3.1), selection techniques (Section 3.2), system and parameter manipulation (Section 3.3), and object manipulation and transformation (Section 3.4).\n1. Prompting (Sec. 3.1): Prompting is a user-guided interaction in which a user asks or \"prompts\" the system to complete a certain task or job. Such user-guided prompting interactions include text-based prompts (Sec. 3.1.1), audio prompts (Sec. 3.1.3), visual prompts (Sec. 3.1.2), and multi-modal prompts (Sec. 3.1.4).\n2. Selection Techniques (Sec. 3.2): Selection techniques use various tools and methods to highlight or choose a specific UI element (e.g., prompting blocks, image or text previews, parts of an image, etc.) within the generative AI system to be further interacted with during the generative process. Such selection interactions include single selection (Sec. 3.2.1), multi-selection (Sec. 3.2.2), lasso and brush selection (Sec. 3.2.3), and multi-modal selection (Sec. 3.2.2).\n3. System and Parameter Manipulation (Sec. 3.3): System and parameter manipulation consist of user interaction techniques that allow the user to adjust the parameters, settings, or functions of an overall generative AI system. These interactions are often used to personalize generated outputs to meet user needs. Such user-guided system and parameter manipulation interactions include menus (Sec. 3.3.1), sliders (Sec. 3.3.2), and explicit feedback(Sec. 3.3.3).\n4. Object Manipulation and Transformation (Sec. 3.4): Object manipulation and transformation interactions occur in situations where the user directly modifies, adjusts, and/or transforms a specific UI element, like a building block, puzzle piece, or similar entity. Doing so gives the user deeper control over the system and allows them to interact with the UI elements in a unique and novel way. Such user-guided object manipulation and transformation interactions include drag and drop interactions (Sec. 3.4.1), connecting (Sec. 3.4.2) and resizing (Sec. 3.4.3).\nMotivation: There is likely no generative AI system that utilizes all of the techniques that will be outlined, nor should there be. The goal of these user-guided techniques is to ensure a seamless and user-friendly navigation of the respective systems to improve the generative process. Moreover, surveying such a wide array of user-guided techniques in generative AI will help the reader understand which generative AI techniques are most appropriate in a number of different situations. The goal is also to expose several novel user interaction techniques that are not widely known and to empower designers and developers to leverage them to create powerful and accessible generative AI systems.\nVariability: As such, we survey works that have used any of the UI interactions (e.g., text, visual prompts, drag-and-drop, sliders, selection, in-painting, and so on), and categorize the systems that use each, and how they use them, and the different ways each are used to guide the generation process. This is useful to understand how each UI interaction (e.g., sliders) was used by existing systems (e.g., in the case of sliders, some work used sliders to adjust the generative model hyperparameters, while others used it to adjust the attention weights of specific user-guided selections such as text that was generated)."}, {"title": "3.1 Prompting", "content": "Definition & Scope: Prompting is a user-guided interaction in which a user asks or \"prompts\" the system to complete a certain task or job. Prompting, specifically text-based prompting, is often thought of as the primary interaction method utilized to interact with generative AI systems. Prompting is different than just inputting content like images or videos, as the user is explicitly asking the system to perform a task. An"}, {"title": "3.1.1 Text-based Prompts", "content": "Text-to-text prompting: Text-based prompting is the most common interaction medium a user uses when interacting with Generative AI systems and has a wide range of applications (Goyal et al., 2024; Achiam et al., 2023; Betker et al., 2023; Gero et al., 2022). While there are several prompt mediums, systems like ChatGPT (Achiam et al., 2023) and Dall-E 3 (Betker et al., 2023) heavily rely on text-based inputs. In ChatGPT (Achiam et al., 2023), for example, users primarily input prompts like \"explain the Krebbs cycle to me as if I were a child,\" and the system will output an explanation of the Krebbs Cycle in plain English (Fig. 5(a)). Meanwhile, in systems like Chen et al. (2021), users can use text prompts to ask generative models to create or continue a section of code for them.\nText-to-multimodal prompting: Meanwhile, multi-modal systems can also utilize text-based prompts and produce outputs of a different medium like visuals (Alayrac et al., 2022; Liu et al., 2023b) or audio (Copet et al., 2023; Agostinelli et al., 2023; Hadjeres et al., 2017). Dall-E 3, for example, incorporates a text-to-image generation system that takes text-based prompts and outputs an image. Another example is text-to-video, where generative AI systems, like AESOP's agent (Wang et al., 2024b), take text-based prompts and create AI videos. Text prompts are common in many multimodal generative AI system interactions, with some examples consisting of, but not limited to: text-to-image, text-to-video, text-to-audio, and so on. Similarly, some generative AI applications (Zhao et al., 2023; Ren et al., 2023) allow text-based prompts to ask questions about or manipulate multi-modal inputs like an image or video. All in all, text prompts are often the backbone of most user interactions because of their versatility and the large amounts of different actions that can be performed."}, {"title": "3.1.2 Visual Prompts", "content": "Visual manipulation: Visual prompting consists of using visual communication, like object manipulation, to prompt the system to complete a certain task (Fig. 5(b)). For example, Jigsaw (Lin & Martelaro, 2024) created a set of puzzle pieces that each have a corresponding instruction that the system can complete. So visually, the user can manipulate, drag and drop, and connect these puzzle pieces (each puzzle piece has a partial prompt) to create a new, larger prompt. In doing so, they are visually manipulating UI elements to prompt the system uniquely.\nImage-Prompts: As mentioned, there is a distinct difference between prompts and inputs. A prompt is used to ask the system to complete a certain task, and some inputs can be used to do the same. In turn, this makes it possible for some inputs, like visuals, to explicitly prompt the system to perform a certain task. This is most common in few-shot learning examples where a model can generate content based on a limited number of training inputs (Alayrac et al., 2022; Wei et al., 2023a; Radford et al., 2021). In Alayrac et al. (2022), an image of a solved math equation can be inputted (i.e. 1+1=2, 1+2=3, etc.). Then, using another image of an unsolved math equation (i.e., 1+3=?) would prompt the system to generate a fill to that last equation. Whereas the original picture of solved math equations would purely be seen as inputs, the image of the unsolved math equations would be considered a prompt, as it is prompting the system to provide an answer to the unsolved equation."}, {"title": "3.1.3 Audio Prompts", "content": "Speech: When a user is embedded in a virtual environment and do not have access to traditional text-based inputs, audio and speech become one of the strongest ways to interact with AI models. In Konenkov et al. (2024), the user can interact with the visual language model (VLM) by speaking into the VR headset's microphone. As it is a VR training application, users can prompt the system by verbally asking it questions like \"what is my next step.\" In situations like this, a user relies on speech as the primary way to prompt the VLM for a response. Just as LLMs such as ChatGPT rely on text-based prompts, VLMs embedded in VR environments often rely on spoken prompts in the same way.\nAudio Files: Some models (Borsos et al., 2023; Schneider et al., 2019), can be prompted with just an audio clip. AudioLM is a novel system that uses two audio clips, one being the input clip and the other being the prompt clip, to extend the prompted audio clip. So for example, if a user has a ten-second speech (inputted clip) they can then prompt the system with the first three seconds of the clip and then the system will generate a new and unique continuation from those original three seconds. This type of prompting is interesting because there is no natural language involved. Because the system's only function is to generate a continuation of the inputted audio, the only prompting needed is the truncation of the original clip. This is significant because few systems do not require either a spoken or written prompt. This system is only able to use this novel system of prompting because it only has a single function with essentially no extra parameters, so it does not require extra written or spoken directions."}, {"title": "3.1.4 Multi-Modal Prompts", "content": "Dual-modality: DirectGPT (Masson et al., 2023) illustrates that it is common for selection techniques to exemplify different modalities. For example, (Masson et al., 2023) explains how the DirectGPT system incorporates a text-based and selection-based input simultaneously. In this instance, users can highlight multiple words simultaneously and then type a prompt that encourages the system to replace those words with a synonym. Using a hybrid of several interaction techniques is especially useful when a single-dimensional interaction does not address the customer's need on its own. Furthermore, hybrid interactions can significantly reduce the amount of time it takes for a user to complete a task, as seen in Masson et al. (2023) where users completed tasks 50 percent faster than those who utilized single dimensional interactions like in ChatGPT.\nMulti-modal LLMs: Meanwhile, multi-modal LLMs (MM-LMMs) are becoming more common, as they are almost universal generative tools that work in a number of different situations (Alayrac et al., 2022; Achiam et al., 2023; Wu et al., 2023) NEXT GPT (Wu et al., 2023) is an empowered MM-LLM, meaning that it takes"}, {"title": "3.2 Selection Techniques", "content": "Definition & Scope: Selecting, in terms of generative AI systems, consists of choosing or highlighting a specific UI element to further interact with it. Selecting UI elements and utilizing selection tools has become a major user interaction method utilized in many generative AI systems. Selectable UI elements and buttons, selectable inputs and outputs, and dropdown menus all offer a way to directly interact with the system. Meanwhile, selection tools such as selection boxes, lassos, or marquee tools enable users to choose precise areas or objects generated by the AI. By incorporating these user-interaction techniques, users are able to select content with more control and accuracy.\n\u2022 Single Selection (Sec. 3.2.1; Fig. 6(a)): A single-selection interaction consists of clicking or choosing a single GUI element that will be interacted with further. An example would be a user choosing one of 3 outputs that they wish to iterate on further.\n\u2022 Multi-Selection (Sec. 3.2.2; Fig. 6(b)): A multi-selection interaction consists of clicking or choosing multiple UI elements that will be interacted with further.\n\u2022 Lasso and Brush Selection (Sec. 3.2.3; Fig. 6(c)): Lasso and brush selections are selection techniques where a lasso or brush is used to create a bounding box that controls the region where a specific prompt is applied (Figure 6(c))."}, {"title": "3.2.1 Single-Selection", "content": "A single-selection interaction consists of clicking or choosing a single UI element (e.g., selecting one of several outputs, choosing part of an image, etc) that will be interacted with further (Lee et al., 2023; Suh et al., 2023a). Luminate (Suh et al., 2023a) is an LLM that intakes a prompt from an author writing a story and outputs several different story options based on the original prompt. Utilizing single-selection, the user clicks on and chooses which of the story options they want to further iterate on. So if a user prompts the system to write a short story about an astronaut, the system will output several stories. From there, the user can click and select the story that best fits their needs. Selection interactions, as opposed to prompting, are often utilized because of their ease of use and efficiency."}, {"title": "3.2.2 Multi-Selection", "content": "Multi-Select to Alter: Multi-selection consists of interactions where users can select multiple elements simultaneously with the goal of interacting with multiple elements at once. For example, DirectGPT (Masson et al., 2023) allows the user to make one or more selections within an inputted text and create a tool that affects all of the selected words. For example, say a user is writing a story, but they feel that their story is a bit bland, and they want to make the vocabulary more descriptive. They can use multi-select the words that they want to make more interesting, such as \"run\", \"eat,\" and \"said,\" and then they can type \"replace with synonyms\" in the prompt module. The system will take all of the words that they selected and replace them with more interesting words like \"dashed,\" \"devoured,\" and \"exclaimed.\" In using the multi-select interaction, users can find and select multiple elements or words that they want to interact with further.\nMulti-Select to connect: Furthermore, multi-selection is often used to select multiple outputs so that the user can connect them together (Padiyath & Magerko, 2021; Jeon et al., 2021). For example, Padiyath & Magerko (2021) is a generative fashion AI system that creates several dress outputs for a designer. The user can then use multi-select to select multiple aspects from different dresses and add them together to create a new dress. So, the fashion designer could select the sleeves they like from one dress, the corset they like from another, and the straps from another output and use all these parts of a dress to prompt the system to create new designs. In cases like this one, multi-select is used to select and add together multiple elements to prompt the generative AI system with the sum of those elements."}, {"title": "3.2.3 Lasso and Brush Selection", "content": "Inpainting: Lasso and brush selection, also known as inpainting, allows users to use a tool to select very specific parts of a larger element. Whereas other selection methods make you choose an entire element, inpainting allows users to fine-tune and edit their selection. PromptPaint (Chung & Adar, 2023) focuses on enhancing the generation of images by brushing over the image to better control the generation process where multiple prompts can be mixed. Different prompts can be applied to different regions of the images. The paint-like interactions can be seen as a type of visual selection via lasso, where the lasso bounding box controls the region that a specific prompt is applied. Another work called PromptCharm (Wang et al., 2024d) allows the user to create a mask $M$ over a generated image $I$ by brushing over the area of interest, and then a new image $I'$ is generated by modifying only the pixels of $I$ that lie within the mask $M$. The user can guide the inpainting process by typing a text prompt $X_M$ that is applied only to the masked region of $I$."}, {"title": "3.3 System and Parameter Manipulation", "content": "Definition & Scope: The System and parameter manipulation category consists of user interaction techniques that allow the user to adjust the parameters, settings, or functions of an overall generative AI system. These interactions are often used to personalize generated outputs to meet the needs of a user. Some examples of this type of user-guided interaction consist of Menus (Section 3.3.1), Sliders (Section 3.3.2), and Explicit Feedback (Section 3.3.3).\n\u2022 Menus (Sec. 3.3.1; Fig. 7(a)): when a user either inputs their own parameters or chooses from preset options to change the parameters of the generative process. An example would be a user using dropdown menu options to alter the output parameters.\n\u2022 Sliders (Sec. 3.3.2; Fig. 7(b)): A UI element that can be \"slid\" to adjust the parameters of the generative AI system.\n\u2022 Explicit Feedback (Sec. 3.3.3; Fig. 7(c)): Explicit feedback (i.e., thumbs up/down, written critiques, etc.) is a user interaction that is used to expressly personalize the system to the user's preferences."}, {"title": "3.3.1 Menus", "content": "Preset Option Menus: Menus are user-guided interaction features that allow users to either input their own parameters or choose from preset options (Wang et al., 2024c; Suh et al., 2023a; Jiang et al., 2023; Kim et al., 2023a). A common example of a parameter-selection menu is a drop-down menu that allows users to adjust the parameters of a system with preset parameters. An example of this can be seen in Suh et al. (2023a), a system that helps users write stories or poems. In this system's UI, there are drop-down menus that allow users to impact the output's tone, mood, and overall structure. In menus like these, the user can choose between pre-set parameters to adjust the overall output. Being able to utilize UI features to quickly adjust output parameters drastically speeds up the user experience and lowers the user's cognitive load.\nInput Menus: While drop-down menus are a common interaction used to adjust parameters, some systems have menus that solely rely on manual user input. These menus allow the user to input text parameters that work in tandem with the prompt to create a concise output. For example, in Setlur et al. (2016), users can create a data visualization given a data file input and can adjust the generated data visualization by changing typing in different parameters. So if a user prompts the system to \"map out all earthquakes in California,\" there is a parameter menu next to visualization where they can manually type parameters like time range, location range, etc. that will affect the generated visualization. Similarly, PromptCharm (Wang et al., 2024d) is an image generation platform that allows users to generate images with text prompts and menus. In terms of menus, for example, Prompt Charm has an \"image style\" menu where a user can input text separate from the original prompt that allows them to specifically impact the image style. Overall, menus allow users to add another dimension to their already existing prompts and guide them to manipulate specific parameters within the generative process."}, {"title": "3.3.2 Sliders", "content": "Range Adjustments: Sliders are visual UI elements that are able to manipulate the parameters of the generative AI system (Chung & Adar", "Weights": "During the generation process, PromptCharm (Wang et al., 2024d) allows users to select specific tokens in the prompt to then adjust their attention weights via a slider that is dynamically shown for the token selected. This is another user-guided interaction to better control the"}]}