{"title": "Detecting Benchmark Contamination\nThrough Watermarking", "authors": ["Tom Sander1,2", "Pierre Fernandez\u00b9", "Saeed Mahloujifar\u00b9", "Alain Durmus\u00b2", "Chuan Guo\u00b9"], "abstract": "Benchmark contamination poses a significant challenge to the reliability of Large Language\nModels (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a\ntest set. We introduce a solution to this problem by watermarking benchmarks before their\nrelease. The embedding involves reformulating the original questions with a watermarked\nLLM, in a way that does not alter the benchmark utility. During evaluation, we can detect\n\"radioactivity\u201d, i.e., traces that the text watermarks leave in the model during training, using a\ntheoretically grounded statistical test. We test our method by pre-training 1B models from\nscratch on 10B tokens with controlled benchmark contamination, and validate its effectiveness\nin detecting contamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar\nbenchmark utility post-watermarking and successful contamination detection when models are\ncontaminated enough to enhance performance, e.g., p-val = 10-3 for +5% on ARC-Easy.", "sections": [{"title": "1 Introduction", "content": "In recent years, Large Language Models (LLMs) have demonstrated remarkable advancements in\ntheir capabilities (Brown et al., 2020; Touvron et al., 2023). This advancement places increasingly\ngreater emphasis on proper evaluation to both inform the state of LLM research and to guide future\ndevelopments. To this end, a multitude of benchmark datasets such as (MMLU) (Hendrycks et al.,\n2020), School Math 8K (GSM8K) (Cobbe et al., 2021), and the AI2 Reasoning Challenge (ARC) (Clark\net al., 2018), or more recently GPQA (Rein et al., 2023) and FrontierMath (Glazer et al., 2024), are\ndeveloped to measure the model's capabilities in terms of general or specific knowledge, understanding,\nand scientific reasoning.\nHowever, a significant issue that arises with these benchmarks is contamination. This problem can\noccur either intentionally, by training models directly on the benchmark datasets or their reformulated\nversions, or unintentionally, as these datasets become mixed with the vast amounts of data used during\npre-training. For example, Zhang et al. (2024) created a version of GSM8K with new questions similar\nin difficulty and form, and observed that many models show a significant drop in performance on them\ncompared to the test set of GSM8k. This challenges the reliability and validity of benchmark evaluations,\nas it becomes difficult to discern whether a model's performance is due to genuine improvement in\ncapabilities or mere memorization. Furthermore, determining whether a model has been trained on a"}, {"title": "2 Related Work", "content": "specific benchmark is very challenging, as it boils down to the issue of dataset/membership inference\nwhich has been shown to be ineffective for LLMs in realistic scenarios (Duan et al., 2024).\nTo tackle this problem, we propose a novel strategy of embedding non-intrusive watermarks in the\nbenchmark dataset before release. Our approach is inspired by Sander et al. (2024), who demonstrated\nthat fine-tuning on LLM-generated watermarked text can be reliably detected, as the model retains\nidentifiable traces of the watermark. When applied to benchmark watermarking, this approach enables\nreporting both model performance and a reliable p-value as a contamination score: it is an upper bound\nto the probability that the model hasn't been trained on the benchmark questions. If the reported\np-value is low, the LLM's training data is likely contaminated with the benchmark dataset and the\nperformance numbers should not be trusted as genuine. Our method requires only access to an LLM\ncapable of rephrasing benchmark questions; see Figure 1 for an overview. Our main contributions are:"}, {"title": "2.1 Benchmark Contamination Detection", "content": "Benchmark contamination is a significant concern in evaluating LLMs, as it can lead to unreliable\nassessments and unfair comparisons (Singh et al., 2024; Balloccu et al., 2024). Although efforts are made\nto decontaminate pre-training corpora (Brown et al., 2020), these methods are not foolproof (Singh\net al., 2024). The impact of contamination can be assessed by comparing training runs that differ\nonly in the inclusion of contaminated batches. For instance, Jiang et al. (2024) have shown that\neven small models can exhibit improved benchmark performance due to contamination. Post-hoc\nanalyses on the other hand identify score inflation by comparing performance on original versus similar\nquestions (Brown et al., 2020; Chowdhery et al., 2023; Touvron et al., 2023), but Yang et al. (2023)\nhave shown that training on reformulated questions is enough to boost the performance on the original\nbenchmark, so the difference in performance does not necessarily provide good correlational insights.\nZhang et al. (2024) craft new questions from the same distribution as GSM8K and observed that most\nmodels show a significant performance drop on these compared to the GSM8K test set. This result\nhighlights the contamination issue, but does not introduce a scalable solution to the core problem.\nIn parallel, studying verbatim memorization in LLMs, such as regurgitating pre-training data, is\nalso closely related to contamination (Carlini et al., 2022; Hartmann et al., 2023). Techniques like\nmembership inference (Mireshghallah et al., 2022) and context-based completion checks (Golchin and\nSurdeanu, 2023) attempt to approximate contamination without direct access to pre-training data, but\ntheir effectiveness is debated (Duan et al., 2024). These methods use a score function to determine the\nlikelihood of a sample being present in the training set. To ensure accurate results, each sample must\nbe calibrated, which can be a computationally intensive process (e.g., involving a model trained on all\ndata except the benchmark). Additionally, these methods lack guarantees regarding false positive and\nnegative rates, allowing for plausible deniability even when there is strong evidence of contamination."}, {"title": "2.2 Decoding-based watermarking & Radioactivity", "content": "Overview. Recent advancements in watermarking techniques for decoder-only large language models\n(LLMs) involve altering either the probability distribution (Kirchenbauer et al., 2023a) or the method\nused for sampling the subsequent token (Aaronson and Kirchner, 2023; Kuditipudi et al., 2023).\nDetection of these watermarks is influenced by the entropy of the generated text (Christ et al., 2023;"}, {"title": "Green-list/Red-list watermark.", "content": "This work focuses on the watermarking scheme proposed by Kirchen-\nbauer et al. (2023b), which modifies the logit vector during token generation based on a context window\nof k previous tokens and a private key s. Both are hashed to serve as the seed for a random number\ngenerator (RNG) to create a \u201cgreenlist\u201d of \u03b3|V| tokens. Logits of green tokens are incremented by \u03b4 to\nincrease their sampling probability. Detection involves repeating the greenlist computation for each\ntoken of a text, incrementing a score by 1 if the token is in the greenlist, and performing a statistical\ntest on the cumulative score. Under the null hypothesis Ho, which corresponds to \"the text is not\nwatermarked with that scheme\", this score follows a binomial distribution (Fernandez et al., 2023)."}, {"title": "Radioactivity of LLM watermarks.", "content": "Sander et al. (2024) show that fine-tuning language models on\nLLM-generated watermarked question-answer pairs can be detected with high confidence, as the model\nretains traces of the watermark bias. The authors adapt the original watermark detection tests to\ndetect this watermark \"radioactivity\", depending on the access to the suspect model and data. In\nthe context of benchmark watermarking, we assume access to the LLM that is being evaluated, the\nbenchmark itself, as well as the benchmark-specific watermarking key s. In this case, Sander et al.\n(2024) suggests using what they call \"reading-mode\". This involves scoring all next-token predictions by\nforwarding the watermarked text in the suspect model. This is detailed in our context in subsection 3.2,\nand illustrated in Figure 2b. Similar observations have been made in other scenarios. For instance,\nGu et al. (2023) demonstrate that LLM watermarks can be intentionally distilled. Additionally, Zhao\net al. (2023) introduce a signal in generated text that can be learned by other LLMs trained on it.\nFurthermore, Jovanovi\u0107 et al. (2024) investigate the concept of watermark radioactivity in a RAG\ncontext."}, {"title": "3 Method", "content": "We first focus in section 3.1 on the task of rephrasing the questions of a benchmark dataset while\nembedding a watermark using the method proposed by Kirchenbauer et al. (2023b). Then, in section 3.2,\nwe show how to detect if a language model was trained on the watermarked benchmark."}, {"title": "3.1 Inserting watermark through question rephrasing", "content": "We use an instruct language model, denoted as $LM_{rephrase}$, which is assumed to be capable of rephrasing\neach question in the benchmark test set such that the rephrased version is logically equivalent to\nthe original. This is a pretty light assumption as the task of rephrasing is considerably easier than\nanswering the question (Deng et al., 2023). $LM_{rephrase}$ generates token per token and at each step,\ntakes as input a context, which is the concatenation of the system prompt, rephrasing instruction,\nthe question to rephrase and the answer generated so far. Everything is tokenized into a sequence\n$(x^{(1)},...,x^{(t-1)}) \\in V^{t-1}$, where V is the vocabulary of the tokenizer.\n$LM_{rephrase}$ outputs a logits vector $\\ell^{(t)} \\in \\mathbb{R}^{|V|}$. The watermark embedding modifies $\\ell^{(t)}$ based on a secret\nkey s (one per benchmark) and the watermark window $(x^{(t-k)},...,x^{(t-1)}) \\in V^{k}$. Specifically, following\nthe method of Kirchenbauer et al. (2023b) detailed in 2.2, a secret-key cryptographic function hashes s as\nwell as the the watermark window, which serves as a seed for a random number generator used to create\na pseudo-random \"greenlist\u201d of tokens, comprising 50% of the entire vocabulary V, for which the logits\nare incremented by a quantity \u03b4 to form $\\tilde{\\ell}^{(t)}$, thereby increasing their probability of being sampled. The\nlogits vector is then transformed into a probability distribution $p^{(t)} = softmax(\\tilde{\\ell}^{(t)}) \\in [0, 1]^{|V|}$, and the\ngeneration proceeds by sampling the next token $x^{(t)}$ from this distribution using a sampling procedure\nsuch as top-k sampling (Fan et al., 2018) or nucleus sampling (Holtzman et al., 2019). The selected token\nis appended to the context, and the process repeats. An example for the watermark embedding process\nis depicted in Figure 2a, with a detailed version with different strength of watermarking in Figure 6.\nDetectability/utility tradeoff. There is a common tradeoff in watermarking between detection and\nutility. In our case detection is the ability to have statistical evidence that the benchmark was used"}, {"title": "3.2 Radioactivity Detection in a white box scenario", "content": "The strength of the watermark is determined by p, the proportion of green tokens in the text, which\nis influenced by \u03b4 and the entropy of the generation process. Sander et al. (2024) demonstrate that\nthe ability to detect whether a model has been trained on watermarked data referred to as the\nradioactivity power-depends on p, as well as the proportion of watermarked text relative to the total\nnumber of training tokens, the size of the model, the fine-tuning method, and other factors. In general,\nthe more a model fits the watermarked data, the more it will memorize the token-level watermark bias,\nthereby making radioactivity easier to detect. The authors also introduce a \"reading mode\" to enhance\nradioactivity detection when the model's weights are accessible and the suspect text is known: in our\ncontext, we input the tokenized questions into the suspect model and, for each input token, assign a\nnext token prediction using greedy decoding (i.e., selecting the most likely next token based on the\noutput logits). For detection, we replay the seed generation using the watermark window from the\ninputs and the benchmark-specific keys to determine the green/red split, scoring +1 if the predicted\ntoken is in the corresponding green list. This process is illustrated in Figure 2.\nThe score function on a predicted token at index y(t) thus uses Wscore that takes as input the watermark\nwindow $(x^{(t-k+1)}, . . ., x^{(t)})$ from the question, and depends on the secret key s:\n$y^{(t)} ; (x^{(t-k+1)},...,x^{(t)}) \\rightarrow W_{score}\\left(y^{(t)}; s, (x^{(t-k+1)},...,x^{(t)})\\right) \\in \\mathbb{R}$.\n(1)\nA statistical test is performed on the cumulative score S(XN) over all token indices t > k:\n$S(X_N) := \\sum_{t=k}^{N} \\mathbb{1}\\left(y^{(t)} \\text{ is in the greenlist of } \\left(s, (x^{(t-i+1)})_{i=k}^{1}\\right)\\right)$.\n(2)"}, {"title": "4 Results", "content": "The statistical test considers Ho: \"The tokens are generated without influence from the watermarking\nbias\". The hypothesis \u201cThe model is not contaminated\u201d is included in Ho, under which S(XN) follows\na binomial distribution, as it should not output more green than red tokens."}, {"title": "De-duplication for reliable p-values.", "content": "Under Ho, for S(XN) to indeed follow a binomial distribution,\nthe random variables $\\left(\\mathbb{1}\\left(y^{(t)} \\text{ is in the greenlist of } \\left(s, (x^{(t-i+1)})_{i=k}^{1}\\right)\\right)\\right)$ should be independent and\nidentically distributed and follow a Bernoulli distribution with parameter \u03b3. For the independence\ncriterion, we only score $\\left(y^{(t)}; s, (x^{(t-i+1)})_{i=k}^{1}\\right)$ that were not already scored (Kirchenbauer et al., 2023a;\nFernandez et al., 2023; Sander et al., 2024), by keeping a tape of scored tuples. The p-value of a test\nassociated with scores s, i.e., the probability of obtaining a score higher than s under Ho, can then be\nobtained theoretically from the regularized incomplete Beta function $I_\u03b3$:\np-value(s) = P(S(XN) \u2265 s | Ho) = I\u03b3(s + 1, N \u2013 s).\n(3)\nThe reading mode can be done either by the community for open-source models, or by the model owner\notherwise, without sharing model weights. Contamination is expected to increase as the model over-fits\non the benchmark. Thus, radioactivity detection should align with benchmark contamination: for a\nfixed benchmark size, smaller p-values indicate a higher proportion of predicted green tokens, occurring\nwhen predictions replicate green tokens from watermarked questions due to token-level overfitting."}, {"title": "4.1 Benchmark quality after watermarking", "content": "Set-up. For the watermark embedding, we rephrase with Llama-3.1-8B-Instruct (Dubey et al., 2024)\nby default, with top-p sampling with p = 0.7 and temperature = 0.5 (default values on the Hugging\nFace hub), and the green/red watermarking scheme of Kirchenbauer et al. (2023b) with a watermark\nwindow k = 2 and a \u201cgreen list\u201d of size |V| (|V| is the vocabulary size). We compare different values\nof \u03b4 when rephrasing: 0 (no watermarking), 1, 2, and 4. We choose to watermark ARC-Challenge,\nARC-Easy, and MMLU due to their widespread use in model evaluation. In practice, one would need\nto watermark their own benchmark before release. For MMLU, we select a subset of 5000 questions,\nrandomly chosen across all disciplines, to accelerate experimentation and maintain a comparable size\nto the other benchmarks. We refer to this subset as MMLU*. ARC-Easy contains 1172 questions, and\nARC-Challenge contains 2372 questions. In Figure 6 of Appendix A, we show the exact instructions\ngiven to the rephrasing model (identical for all benchmarks) and the results for different watermarking\nstrengths on one example from ARC-Easy. We use a different watermarking key s for each benchmark.\nEven strong watermarking keeps benchmark utility. We evaluate the performance of Llama-3.3-\n1B, Llama-3.3-3B and Llama-3.1-8B on the original benchmark and the rephrased version using as\nsimilar evaluation as the one from the 1m-evaluation-harness library (Gao et al., 2024). To check\nif the benchmark is still as meaningful, we check that evaluated models obtain a similar accuracy on\nthe watermarked benchmarks and on the original version (see subsection 3.1). Figure 3a shows the\nperformance on ARC-Easy. All models perform very similarly on all the rephrased versions of the\nbenchmark, even when pushing the watermark to 80% of green tokens. Importantly, they rank the\nsame. Similar results are shown for MMLU* and ARC-Challenge in Figure 3a of Appendix A, although\nfor MMLU*, we observe some discrepancies. For instance, when using a watermarking window size of 2\n(subfig i), the performance of Llama-3.2-1B increases from 38% to 42% between the original and the\nother versions. However we observe the same issue when rephrasing without watermarking in that case.\nAs detailed in subsection 3.1, designing better instructions that are more specific to each benchmark\ncould help. We have tried increasing \u03b4 even further, but it broke the decoding process. The choice of \u03b4\ndepends on the benchmark and the model used for rephrasing, and needs to be empirically tested."}, {"title": "4.2 Contamination detection through radioactivity", "content": "We now propose an experimental design to control benchmark contamination, and evaluate both the\nimpact on model performance and on contamination detection."}, {"title": "4.3 Additional Results", "content": "Impact of window size. Watermark insertion through\nrephrasing (subsection 3.1) depends on the watermark\nwindow size k. Each window creates a unique green-\nlist/red-list split for the next token. Larger windows\nreduce repeated biases but are less robust. Because of rep-\netitions, Sander et al. (2024) show that smaller windows\ncan lead to bigger overfitting on token-level watermark\nbiases, aiding radioactivity detection. In our case, bench-\nmark sizes are relatively small and deduplication limits\nthe number of tokens tested, because each {window + pre-\ndicted token} is scored only once. Thus, smaller windows"}, {"title": "5 Limitations & Conclusion", "content": "Limitations\n\u2022 Tokenizer consistency: This study uses the same tokenizer for both the rephrasing and contaminated\nmodels. If a different tokenizer is used in the suspect model, scoring should be limited to tokens\npresent in both vocabularies. A smaller intersection of vocabularies or a larger watermark window\nthus reduces the number of scored tokens, and thus the power of the test.\n\u2022 Rephrasing impact: Model performance remains similar across benchmark versions, but some\nquestions lose coherence after rephrasing (e.g., Figure 4), which can be difficult to spot. Possible\nimprovements are discussed in subsection 3.1 and subsection 4.3.\n\u2022 Intentional evasion: The method is primarily designed for unintentional contamination. Malicious\nactors could rephrase questions to weaken the watermark or train only on answers conditioned on\nquestions, which would bypass radioactivity detection. In this case, watermarking answers may be\nnecessary, though it might not always be feasible because of their lengths.\nConclusion. Watermarking benchmark appears like a promising solution to the problem of contamina-\ntion in large language models: experiments confirm the method's ability to maintain benchmark utility\nwhile successfully identifying contamination."}, {"title": "A Appendix", "content": "A.1 Qualitative Examples\nTaking the example of a question from ARC-Easy, we compare qualitatively different watermarking\nstrength in Figure 6."}, {"title": "A.2 Additional Experimental Results", "content": "Evaluation Template. As detailed in subsection 4.2, we evaluate the accuracy on the benchmark using\nboth the same template seen during contamination and an alternative one. Table 3 presents the results\nwhen evaluated with the same template. Without contamination, the model performs similarly across\nthe two templates, but a differences appear with contaminations. Even OOD, only 8 contaminated\nsteps out of 10k steps leads to +10% on all benchmark for these 1B-parameter language models.\nAblations on different benchmarks, watermark strength, watermark window sizes, and number\nof contaminations. Results for all benchmarks (ARC-Easy, ARC-Challenge, and MMLU*), with\nvariations in watermark window size, number of contaminations, and watermark strength, are shown in\nFigure 7 for utility and Figure 8 for radioactivity detection. For utility, all models perform very similarly\non all the rephrased versions of the benchmarks, even when pushing the watermark to 80% of green\ntokens, although for MMLU*, we observe some discrepancies. For instance, when using a watermarking\nwindow size of 2 (subfig i), the performance of Llama-3.2-1B increases from 38% to 42% between\nthe original and the other versions. However we observe the same issue when rephrasing without\nwatermarking in that case. The watermark window size does not have an impact. For radioactivity\ndetection on the other hand, as detailed in subsection 4.3, smaller window sizes correlates with lower\ndetection confidence."}]}