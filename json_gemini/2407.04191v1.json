{"title": "GazeFusion: Saliency-guided Image Generation", "authors": ["Yunxiang Zhang", "Nan Wu", "Connor Z. Lin", "Gordon Wetzstein", "Qi Sun"], "abstract": "Diffusion models offer unprecedented image generation capabilities given just a text prompt. While emerging control mechanisms have enabled users to specify the desired spatial arrangements of the generated content, they cannot predict or control where viewers will pay more attention due to the complexity of human vision. Recognizing the critical necessity of attention-controllable image generation in practical applications, we present a saliency-guided framework to incorporate the data priors of human visual attention into the generation process. Given a desired viewer attention distribution, our control module conditions a diffusion model to generate images that attract viewers' attention toward desired areas. To assess the efficacy of our approach, we performed an eye-tracked user study and a large-scale model-based saliency analysis. The results evidence that both the cross-user eye gaze distributions and the saliency model predictions align with the desired attention distributions. Lastly, we outline several applications, including interactive design of saliency guidance, attention suppression in unwanted regions, and adaptive generation for varied display/viewing conditions.", "sections": [{"title": "1 INTRODUCTION", "content": "The emergence of generative artificial intelligence (AI) marks a paradigm shift for computer graphics. Diffusion models, in particular, enable the generation and editing of photorealistic and stylized images, videos, or 3D objects with little more than a text prompt or high-level user guidance as input [Po et al. 2023]. In many applications, including graphic design or advertisement, it is desirable to generate visual content that guides a viewer's attention to the areas of interest. Such a human-centric control strategy for the generation process, however, is not supported by existing diffusion models.\nPopular approaches to controlled image [Zhang et al. 2023b] and video [Guo et al. 2023] generation include lightweight adaptation modules built around a foundation model. The adapter networks are usually conditioned by depth maps, semantic segmentation masks,"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Controllable Diffusion Models", "content": "Recent advancements in diffusion-based generative models have shown great success in image [Rombach et al. 2022] and video [Blattmann et al. 2023] generation, as surveyed by Po et al. [2023] and Yang et al. [2023]. These foundation models, however, require extensive prompt engineering to enable user control over the generation process. To overcome this limitation, model customization approaches [Hu et al. 2021; Ye et al. 2023] as well as lightweight adapter networks [Li et al. 2023b; Zhang et al. 2023b; Zhao et al. 2023] have been established as the primary mechanisms for adding control over of the generated content. Current control strategies, however, use image-space annotations, including body pose, depth maps, or bounding boxes, to guide the spatial layout of a generated image. Although visual attention is impacted by layout arrangements, it is also simultaneously determined by the interplay among low-level (e.g., contrast, frequency, color) and high-level (e.g., object semantics) characteristics (see Figure 1). Therefore, spatial attention exhibits selective and sometimes individually inconsistent patterns [K\u00fcmmerer et al. 2015]. We use gaze-derived saliency maps to condition a custom-trained adapter network. This network is then used to steer the generation of images and videos to align with a specific pattern based on design intentions."}, {"title": "2.2 Human Visual Attention Modeling and Prediction", "content": "Due to the complexity of cognitive visual attention [Kastner and Ungerleider 2000], modeling the saliency while perceiving images or videos has been an open challenge. Researchers have attempted to develop saliency models in a bottom-up fashion from image space statistical features [Bruce and Tsotsos 2005, 2007; Harel et al. 2006; Itti and Koch 2001; Itti et al. 1998; Judd et al. 2009]. However, these low-level features by themselves are insufficient to account for top-down influences, e.g., our familiarity with different objects [Elazary and Itti 2008]. To measure these compounded influences, large-scale eye-tracked studies have been conducted, attempting to establish a paired image-video dataset with human-exhibited gaze fixations. Examples include MIT1003 [Judd et al. 2009], CAT2000 [Borji and Itti 2015], SALICON [Huang et al. 2015; Jiang et al. 2015] for images, VR saliency for 360 videos [Sitzmann et al. 2018], and DHF1K for videos [Wang et al. 2019]. These large-scale datasets catalyzed various deep neural network based saliency metrics for RGB images (e.g., DeepGaze models [K\u00fcmmerer et al. 2014; Kummerer et al. 2017; Linardos et al. 2021], EMLNet [Jia and Bruce 2020], SalGAN [Pan et al. 2017]), RGB-D frames [Ren et al. 2015; Sun et al. 2021; Zhang et al. 2021], videos ([Droste et al. 2020; Jiang et al. 2018; Min and Corso 2019; Wang et al. 2018]), and panoramas ([Zhang et al. 2018]). Saliency models are further extended to predict temporal fixation durations [Fosco et al. 2020] and scanpaths [Martin et al. 2022]."}, {"title": "2.3 Attention-Aware Computer Graphics", "content": "Leveraging the selective attention distributions predicted by computational models has facilitated practical applications in enhancing the end-user experience or improving system efficiency. For instance, an existing image may be automatically enhanced by identifying and removing distractions without losing the fidelity [Aberman et al. 2022; Jiang et al. 2021; McDonnell et al. 2009; Mejjati et al. 2020; Miangoleh et al. 2023]. High salient regions may also be prioritized for image loading [Valliappan et al. 2020]. Beyond images, character animation may also be enhanced with identified salient body parts [McDonnell et al. 2009]. So far, saliency-based optimization has been used to guide broad editing tasks given existing content. This research aims to introduce the building block of interactively creating desired images and videos from only simple user interference and text prompts."}, {"title": "3 METHOD", "content": ""}, {"title": "3.1 Saliency-guided Diffusion Model for Image Generation", "content": "Diffusion models define a Markov chain that iteratively adds Gaussian noise to samples from an empirical data distribution and gradually converts them into noisy samples from a standard Gaussian distribution. They then learn the reverse diffusion process, i.e., the denoising process, to iteratively remove noise and generate new data samples from randomly sampled Gaussian noise. State-of-the-art image diffusion models are commonly trained on large-scale image datasets and are capable of synthesizing visually appealing and content-diverse images [Po et al. 2023].\nRecent advancements toward controlling and customizing these large models, such as ControNet [Zhang et al. 2023b] and GLIGEN [Li et al. 2023b], have shown that it is possible to incorporate a variety of multimodal conditions into the generation process. This integration allows for the manipulation of semantic information and spatial layout in the content produced by these large models. Such conditional generation is typically achieved by first augmenting pre-trained image diffusion models with an adaptation module and then fine-tuning on a considerably smaller set of condition-image pairs.\nTo achieve attention-aware image generation, we first curate a dataset of saliency-image pairs. The scale of existing image datasets with paired eye-tracked human saliency data is commonly too limited (< 10k images) to support generative learning. Therefore, we leverage the captioned MSCOCO dataset [Lin et al. 2014] for images and a learning-based saliency model, EMLNet [Jia and Bruce 2020], to predict their corresponding saliency maps. As visualized in Figure 2, we attach a ControlNet module to the encoder and middle blocks of a pre-trained Stable Diffusion (SD2.1) model [Rombach et al. 2022] to inject saliency map conditions through zero convolutions and perform saliency-to-image translation. Particularly, the train, test, and unlabeled splits of MSCOCO 2017 (a total of 282k images) were taken to construct our training set D. The 5k validation images of MSCOCO 2017 were held out for evaluation. GazeFusion was initialized with the pre-trained SD2.1 model checkpoint and finetuned using an Adam optimizer (constant learning rate 1e-5, \\(\\beta_1\\) = 0.9, and \\(\\beta_2\\) = 0.999) [Kingma and Ba 2015] for 5e5 steps. Mathematically, our saliency-conditioned training process can be summarized as optimizing the denoising network in SD2.1 \\(\\epsilon_\\theta\\) to predict the Gaussian noise added at each time step using the following loss function:\n\\(L = \\mathbb{E}_{z, t, c_t, c_s, \\epsilon \\sim \\mathcal{N}(0,1)}[\\Vert \\epsilon_\\theta(z_t, t, c_t, c_s) - \\epsilon \\Vert^2]\\).\nwhere z, \\(z_t\\), \\(c_t\\), and \\(c_s\\) denote a sample from the latent image distribution, the corresponding noisy sample after t steps of adding Gaussian noise, the text prompt, and the conditioning saliency map, respectively. As sampled case studies in Figure 9 show, the saliency-based control automatically incorporates various factors that induce visual attention, such as low-level frequency, color, contrast, mid-level layouts, as well as high-level semantic familiarity."}, {"title": "3.2 Extension to Video Generation", "content": "We further extend our saliency-conditioned image diffusion model to video. Existing zero-shot video generation pipelines enforce the temporal consistency across adjacent frames to extend individual frames to videos [Guo et al. 2023; Khachatryan et al. 2023]. Therefore, they are also adaptable to controlling modules with conditions such as body poses and edges.\nHowever, a unique challenge for human perception is the domain gap between viewing individual frames with extended duration (spatial-only saliency, \\(S = S\\{1,2,...,T\\}\\), as used in Section 3.1) vs. watching them temporally composed as a video sequence (spatio-temporal saliency \\(V = V\\{1,2,...,T\\}\\)) [Droste et al. 2020]. Here, S and V indicate the saliency maps on individual frames and videos of the same image sequence. This is due to various temporally induced factors such as camera and object motions \u2013 influencing selective attention. Therefore, although it is possible to directly apply our"}, {"title": "4 EVALUATION", "content": "To quantitatively evaluate our saliency-guided approach to visual content generation, we conducted 1) an eye-tracked user study on GazeFusion-generated images to analyze its attention-directing performance (Section 4.1); 2) a large-scale objective evaluation on GazeFusion-generated images and videos using off-the-shelf saliency models (Section 4.2); 3) an ablation study on the quality and diversity of GazeFusion-generated images."}, {"title": "4.1 User Study: Tracking and Analyzing Viewers' Eye Gazing Behaviors over Generated Images", "content": "The aim of our research is to generate visual content that directs viewer attention in specific ways. This is achieved by incorporating the data priors of human visual attention into the generation process. To systematically analyze the attention-directing properties of the generated images, we conducted a user study with eye trackers to record participants' eye gaze patterns while they browse through a sequence of generated image samples.\nParticipants. Twenty adults participated in the study (ages 23-57, 9 female). All of them have normal or corrected-to-normal vision, no history of visual deficiency, and no color blindness. None of them were aware of the hypothesis, the research, or the number of conditions. The research protocol was approved by the Institutional Review Board (IRB) at the host institution, and all subjects gave informed consent prior to the study.\nSetup and procedure. During the study, subjects remained seated in a well-lit room and viewed a 24-inch Dell monitor (Model No. S2415H, resolution 1920 \u00d7 1080, luminance 250 cd/m\u00b2) binocularly from an SR Research headrest positioned 60 cm away. The effective field of view and resolution were 46\u00b0\u00d726.8\u00b0 and 40 pixels per degree of visual angle. A Tobii Pro Spark eye tracker was mounted to the bottom of the monitor to record their eye gaze at 60 FPS. A 5-point eye-tracking calibration was performed before each session began. Figure 4 shows the experimental setup of our user study.\nStimuli. We first sampled 50 images from the held-out validation set of MSCOCO 2017, where half of them have humans/animals as the main content and the rest show close-up shots of objects or nature/city scenes. These selected images were annotated using the BLIP-2 Image2Text model [Li et al. 2023a] for text prompt conditioning. Image saliency maps were extracted using the EML-Net saliency model [Jia and Bruce 2020] for visual saliency conditioning. We then fed the obtained paired text prompts and saliency maps to our saliency-guided model to generate 50 images of 512 \u00d7 512 resolution as the visual stimuli for the user study. The hypothesis is that these generated images should direct viewers' attention toward the intended regions as depicted by the saliency maps while observing the text prompts and maintaining non-degraded image quality/diversity.\nConditions. We also included two baseline conditions, the Stable Diffusion v2.1 (SD2.1) model [Rombach et al. 2022] (TEXT) and the GLIGEN BBox-guided model [Li et al. 2023b] (BBOX), to compare with GazeFusion (OURS) in terms of the accuracy and robustness of manipulating human visual attention. All three conditions share the same input text prompts to control what visual content is generated. Additionally, BBOX takes in text-annotated bounding boxes predicted by the Grounding DINO model [Liu et al. 2023], and OURS takes in saliency maps predicted by the EML-Net saliency model. Similar to OURS, 50 images were generated for TEXT and BBOX, respectively.\nTask and duration. The total of 150 images generated for the three conditions was shuffled in random order and sequentially displayed to each subject, with a 5-second duration for each image and a 1-second pause between consecutive images. The complete study, including hardware setup/calibration, pre-study instructions, and breaks, took about 30 minutes per subject. Throughout the study, all subjects were instructed to keep their head stationary on the headrest and freely explore the displayed images by shifting their eye gaze. Their eye gaze patterns on each image were recorded to compute the corresponding empirical saliency map.\nMetrics. To quantitatively evaluate each model's performance in directing users' attention to intended image regions, we adopted five saliency similarity metrics from the MIT/Tuebingen Saliency Benchmark [Kummerer et al. 2018]: Area Under ROC Curve (AUC) [K\u00fcmmerer et al. 2015], Normalized Scanpath Saliency (NSS) [Peters"}, {"title": "4.2 Model-based Saliency Analysis", "content": "In addition to the eye-tracked user study, we further performed a large-scale objective analysis of GazeFusion-generated images and videos to more comprehensively understand its capabilities. The two previously introduced baseline methods, TEXT and BBOX, are again adopted for comparisons.\nImage. Similar to the data preparation procedures in Section 4.1, we first computed the BLIP-2 captions, spatial saliency maps, and text-annotated bounding boxes on the held-out MSCOCO data (5K images) to condition the image generation process. Next, we applied GazeFusion to generate 5K images for OURS using the 5K paired captions and saliency maps. Using their respective model and input conditions, 5K images were also generated for BBOX and TEXT. Finally, we measured the discrepancy between the saliency maps used as input conditions (i.e., the desired attention distributions) and the saliency maps of the generated images per the EMG-Net image saliency predictor (i.e., the achieved attention distributions).\nVideo. To generate a large set of video clips with GazeFusion, we first sampled 1K videos from the WebVid-10M dataset [Bain et al. 2021], trimmed them down to 4-second clips, and uniformized the frame rate to 8 FPS. Next, we took the TASED-Net video saliency predictor to extract the spatial-temporal saliency map sequence from each processed video clip. Leveraging the Text2Video-Zero pipeline [Khachatryan et al. 2023], we then applied our GazeFusion model to generate 1K video clips of 4 seconds and 8 FPS based on the previously extracted saliency map sequences. Finally, similar to generated images, we measured the frame-wise discrepancy between the desired and achieved attention distributions for all generated video clips.\nResults and discussion. In this large-scale evaluation, since we take advantage of ML-based saliency models to directly extract saliency maps from generated images/videos as ground truth and do not have access to eye fixation data, only CC, KL, and SIM are feasible and thus adopted. As shown in Table 2, GazeFusion significantly"}, {"title": "4.3 Ablation Study on Image Quality and Diversity", "content": "To verify that our saliency conditioning does not incur any performance drop to the base SD2.1 model, we adopted two image quality/diversity metrics, inception score (IS, higher is better) and Fr\u00e9chet inception distance (FID, lower is better), to evaluate how do GazeFusion-generated images compare with those generated by the base model using the same text prompts. To this end, we reused the two sets of images generated for the conditions TEXT (the base model with text guidance only) and OURS (our GazeFusion model). Note that both sets of images were generated using the same set of text prompts and are thus fair to be compared against each other. Notably, GazeFusion achieved FID = 16.43 and IS = 36.05, while SD2.1 achieved FID = 22.29 and IS = 34.81, indicating that our saliency-guided generation even improves upon the base model in terms of image quality and diversity. Such improvements are likely due to the similar image distribution of our training and evaluation sets, which were both taken from MSCOCO 2017. Our GazeFusion model was carefully fine-tuned on these photo-based realistic images to learn saliency guidance while SD2.1 was not specifically optimized to generate such images. These results strongly validate that we can effectively incorporate the data priors of human attentional behaviors into the diffusion process and generate saliency-guided images while not degrading the image generation performance of the base diffusion models."}, {"title": "5 APPLICATIONS", "content": ""}, {"title": "5.1 Interactive Design", "content": "Unlike other non-perceptual controlling inputs such as body poses, line sketches, and depth maps, prior research has observed a strong correlation between the content of an image and its incurred visual saliency [Borji et al. 2015]. Therefore, if the text prompt is incompatible with an arbitrary saliency map provided by a creator, GazeFusion may generate unnaturally looking images with artifacts (e.g., distorted objects and human bodies; see also the top row of Figure 5), as we have repeatedly observed during our experiments.\nTo this end, we develop an interactive saliency creation-correction framework to ease designers' trial-and-error cycles for creating prompt-compatible saliency guidance. Specifically, a user first provides an input text prompt pin and creates a tentative saliency map by clicking over a black canvas. Each click generates a bivariate Gaussian \\(G_i (w_i, \\mu_i, \\Sigma_i)\\) that can be further moved and scaled to compose the desired saliency distribution. Here, \\(w_i\\), \\(\\mu_i\\), and \\(\\Sigma_i\\) denote the Gaussian's weight, mean, and covariance matrix. The resulting input saliency map is represented as a Gaussian mixture (GM):\n\\[S_{in} := \\sum_{i=1}^{N} G_i (w_i, \\mu_i, \\Sigma_i) .\\]\nNext, in our 282k text-saliency dataset D (Section 3.1), we search for the text closest to the creator-provided prompt in the CLIP-embedded language space [Radford et al. 2021], and retrieve its corresponding saliency map as a reference:\n\\[P_{ref}, S_{ref} = \\arg \\min_{(p,S) \\in D} \\Vert \\varepsilon(p) - \\varepsilon(p_{in}) \\Vert,\\]\nHere, \\(p_{ref}\\) and \\(S_{ref}\\) are the retrieved reference text prompt and saliency map pair. \\(\\varepsilon\\) denotes the CLIP text encoder. Finally, we optimize an image-space transformation T, which is composed of a translation, a rotation, and a uniform scaling, and the user-created \\(S_{in}\\) to approximate the reference saliency map:\n\\[S_{out} = \\arg \\min_T G_i (w_i, \\mu_i, \\Sigma_i) - S_{ref} \\Vert_2^2.\\]\nFigure 5 shows an example when the initial user-specified saliency map \\(S_{in}\\) is incompatible with the accompanying text prompt due to inappropriate saliency intensity and layout. In the corresponding generated images, the main subject \u2013 the baby bear \u2013 exhibits a severely disfigured face and incorrect body anatomy. In comparison, the images generated based on the system-corrected saliency map \\(S_{out}\\) not only present a realistically looking baby bear without artifacts but also maintain the user-intended saliency layout. Notably, the whole generation process only involves a few mouse clicks"}, {"title": "5.2 Attention Suppression", "content": "Now that we have demonstrated GazeFusion's capabilities in generating high-quality images and videos that attract viewers' attention toward the specified regions, we also explore the opposite effects: suppressing viewers' attention at unwanted regions. In particular, we differentiate between two types of attention suppression using GazeFusion: 1) absolute suppression, where viewer attention is entirely diverted away from the target areas; 2) relative suppression, where viewer attention is significantly reduced in the less important areas compared to the more crucial ones, though not entirely eliminated.\nAs demonstrated in Figure 6, we crafted two pairs of saliency maps and fed them to GazeFusion. In each pair, one saliency map is designed to demonstrate a type of attention suppression (TEST), and the other is crafted to be the same except in the suppressed regions (CONTROL). For 1) absolute suppression: TEST is strongly salient everywhere except for an oval-shaped region. The TEST-guided image features several arrays of donuts except for the suppressed region, while the CONTROL-guided image shows donuts everywhere. For 2) relative suppression, the text prompt describes a two-person chatting scenario. CONTROL shows two separate bright regions that are equally salient, and TEST shows the same two bright regions but with one of them being considerably brighter than the other. While the CONTROL-guided image depicts two persons at the two bright regions, the TEST-guided image, interestingly, not only positions the two persons to the two bright regions but also makes the person at the brighter one face the camera and the other person turn his back to the camera and out of focus."}, {"title": "5.3 Display-Adaptive Generation", "content": "The increase in screen sizes has significantly broadened the display field of view (FoV). Examples include VR/AR or curved monitors that offer additional information or immersive experiences. However, most visual content is pre-created without considering the environment in which it will be displayed, leading to unpredictable or potentially negative experiences. For example, an image with a 3:2 aspect ratio viewed on an iPhone will attract visual attention differently than when viewed on a 27-inch computer monitor where much of the content is displayed in the low-acuity peripheral vision [Eriksen and Yeh 1985; Reeves et al. 1999]. Additionally, users may also have to rotate their gazes and necks more frequently, leading to ergonomic problems [Gallagher et al. 2021; Zhang et al. 2023a]. GazeFusion can guide the generative content for given display environments via adaptive spatial saliency guidance, inspired by the image retargeting problems [Avidan and Shamir 2023; Rubinstein et al. 2008]. This is achieved by adapting the conditioning saliency distribution to both the eye-display configurations and application aims. For example, as shown in Figure 7, our display-adaptive generation can guide salient image content at optimal viewing angles for the display in use, effectively avoiding frequent head movements for users [Zhang et al. 2023a]. Similarly, for more efficient target-reaching, it may also control the salient regions to the visual field regions exhibiting the fastest reaction time (about 7-10 degree eccentricity per the literature [Duinkharjav et al. 2022; Kalesnykas and Hallett 1994])."}, {"title": "6 LIMITATIONS AND FUTURE WORK", "content": "Our saliency-guided control module was established based on an end-to-end computational saliency model trained on human gaze data [Jia and Bruce 2020]. However, the factors inducing human visual attention are diverse and multi-dimensional, including low-level image features, mid-level local structures, and high-level semantics [Hayes and Henderson 2021]. Currently, GazeFusion model does not differentiate between these underlying causes. Integrating various saliency models targeting different levels of saliency-triggering factors under a probabilistic framework [K\u00fcmmerer et al. 2015] may shed light on more fine-grained saliency-based control, such as specifying local color- and contrast-induced saliency vs. semantic labels in the saliency map.\nThe extension to saliency-guided video generation, while showing numerical saliency alignments between the input control and the computed prediction with TASED-Net [Min and Corso 2019], has not been measured with human observers. This is due to the significantly large sampling requirement to obtain an eye-tracking-revealed spatial-temporal saliency [Wang et al. 2018]. To this end, we plan to investigate eye-tracking-free human attention assessment approaches via crowdsourcing platforms, e.g., [Kim et al. 2017]. In addition, the current approach treats the saliency frames in the control sequence separately to generate temporally consistent videos frame-by-frame with [Khachatryan et al. 2023]. Introducing an explicit temporal module to exploit the temporally induced factors, such as motions between adjacent frames, may further improve the controlling effectiveness.\nThe proposed interactive design tool automatically corrects users' arbitrarily specified saliency input through a text-saliency compatibility matching and adjustment, as shown in Figure 5. Recognizing that image artifacts may also influence saliency [Yang et al. 2021], we plan to investigate the three-fold cross effects among characterized saliency maps, text feature spaces, and their generated image quality assessment [Golestaneh et al. 2022; Yang et al. 2019; Zhang et al. 2014]. A quantifiable correlation in the continuous domain may provide guidance on quality-predictable and saliency-aware generation."}, {"title": "7 CONCLUSION", "content": "In this paper, we present a saliency-guided generative model that guides users' viewing attention. It may match designers' specifications with a simple click-and-run user interface. An eye-tracked user study evidences the real-world effectiveness. With various demonstrated applications, such as inversely suppressing attention and adapting the generations to various viewer-display conditions, we hope the research will initiate the first step of viewer-perception-aware generative models."}]}