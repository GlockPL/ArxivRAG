{"title": "Multi-Modal Parameter-Efficient Fine-tuning via\nGraph Neural Network", "authors": ["Bin Cheng", "Jiaxuan Lu"], "abstract": "With the advent of the era of foundation models, pre-training and fine-tuning\nhave become common paradigms. Recently, parameter-efficient fine-tuning has\ngarnered widespread attention due to its better balance between the number\nof learnable parameters and performance. However, some current parameter-\nefficient fine-tuning methods only model a single modality and lack the utilization\nof structural knowledge in downstream tasks. To address this issue, this paper\nproposes a multi-modal parameter-efficient fine-tuning method based on graph\nnetworks. Each image is fed into a multi-modal large language model (MLLM)\nto generate a text description. The image and its corresponding text descrip-\ntion are then processed by a frozen image encoder and text encoder to generate\nimage features and text features, respectively. A graph is constructed based on\nthe similarity of the multi-modal feature nodes, and knowledge and relationships\nrelevant to these features are extracted from each node. Additionally, Elastic\nWeight Consolidation (EWC) regularization is incorporated into the loss function\nto mitigate the problem of forgetting during task learning. The proposed model\nachieves test accuracies on the OxfordPets, Flowers102, and Food101 datasets\nthat improve by 4.45%, 2.92%, and 0.23%, respectively. The code is available at\nhttps://github.com/yunche0/GA-Net/tree/master.", "sections": [{"title": "1 Introduction", "content": "With the onset of the era of foundational models, we have gradually entered a\nparadigm of pre-training and fine-tuning. In terms of downstream task adaptation,\nfull fine-tuning requires adjusting all the parameters of a model to adapt to down-\nstream tasks. However, as the scale of models and the number of tasks increase,\nsuch a method becomes inefficient. Consequently, numerous studies have focused on\nparameter-efficient fine-tuning, exploring strategies to efficiently adapt existing foun-\ndation models to downstream tasks. Previous parameter-efficient fine-tuning methods\ncan be mainly categorized into three approaches. The first is prompt tuning [1][2],\nwhich aims to achieve fine-tuning by modifying the model input rather than the\nmodel structure. The second is prefix tuning [3], which involves updating only task-\nspecific trainable parameters within each layer. The third is adapter tuning [4][5],\nwhich achieves parameter-efficient fine-tuning by inserting adapter modules with a\nbottleneck architecture between layers.\nIn the realm of multi-modal learning, parameter-efficient fine-tuning methods have\ngained attention in recent research. Prompt vectors are used to align multi-modal\ndata, achieving efficient multi-modal fusion in low-resource settings [6]. The main\nidea of CoOp [7] is to automatically design prompt texts. It keeps the pre-training\nparameters unchanged and uses a small amount of data to learn the appropriate cues.\nThe TaskRes [8] method directly adjusts the weights of text-based classifiers without\nrequiring extensive prompt updates to text encoders or elaborate adapters. \u03c0-Tuning\noptimizes parameters by predicting and interpolating task similarity across visual,\nlanguage, and visual-language tasks, achieving efficient cross-task transfer learning\n[9]. However, these works do not consider modeling the complex associations between\nmodalities.\nWith respect to modeling complex associations for parameter-efficient fine-tuning,\nmethods related to graph neural networks (GNNs) have been explored. The concept\nof timely tuning has been introduced [10]. In the molecular domain, MolCPT [11]\nenhances graph embeddings by encoding additional molecular motif information. How-\never, these works primarily focus on how to perform parameter-efficient fine-tuning\nfor purely graph structures, without applying them to language-image multi-modal\nmodeling.\nTherefore, we propose a framework that combines graph structures with multi-\nmodal parameter-efficient fine-tuning methods, enabling the learning of multi-modal\ninformation while considering the complex associations between modalities. The\nproposed model comprises four main modules: Multi-Modal Feature Extraction, Multi-\nModal Graph Construction, Graph Adapter Net (GA-Net), and Prediction. In the\nMulti-Modal Feature Extraction module, each image is processed by a pre-trained\nMLLM model to obtain a text description for each image. The image and its corre-\nsponding text description are then processed by frozen image and text encoders to\ngenerate image features and text features, respectively. These features are combined\ninto multi-modal features through feature concatenation. In the Multi-Modal Graph\nConstruction module, a graph is constructed based on the similarity of multi-modal\nfeature nodes. The GA-Net module then mines suitable knowledge from the graph"}, {"title": "2 Related Work", "content": "Parameter-Efficient Fine-Tuning Methods (PEFT). Full fine-tuning involves\nmodifying all the model's parameters to suit downstream tasks. Yet, as models\ngrow in scale and the number of tasks expands, this approach becomes increasingly\ninefficient. To address this issue, in recent years, the natural language processing\n(NLP) community has explored parameter-efficient fine-tuning techniques (PEFT)\n[13][14][15][16][17]. These techniques only require adjusting a small subset of param-\neters, thereby improving efficiency [18]. For example, prompt tuning methods [1]\nattempt to achieve fine-tuning by modifying the model input rather than the model\nstructure. Prefix tuning [3] updates only task-specific trainable parameters in each\nlayer. Adapter tuning [4][5] inserts adapter modules with bottleneck architectures\nbetween layers to achieve parameter-efficient fine-tuning. Additionally, methods like\nBitFit [19] update only the bias terms and freeze the remaining parameters, while\nLORA [20] reduces the number of trainable parameters by decomposing the weight\nmatrix into low-rank matrices.\nIn multi-modal learning, parameter-efficient fine-tuning methods have gained\nwidespread attention in recent research [21] [22] [23] [24] [25]. Using prompt vectors to\nalign multi-modal data achieves efficient multi-modal fusion in low-resource environ-\nments, excelling in tasks involving two or more data modalities [6]. Research on scaling\nlarge multi-modal models (such as LLaVA and MiniGPT-4) has shown that parameter-\nefficient training methods like LoRA/QLORA perform well in both multi-modal and\nlanguage tasks, with performance comparable to full-model fine-tuning[26]. The main\nidea of CoOp [7] is to automatically design prompt texts. It keeps the pre-trained"}, {"title": "3 Method", "content": "As shown in Figure 1, the model presented in this paper is composed of four main\nmodules: Multi-Modal Feature Extraction, Multi-Modal Graph Construction, GA-Net,\nand Prediction. The primary function of the Multi-Modal Feature Extraction is to use"}, {"title": "3.1 Multi-Modal Feature Extraction", "content": "In this module, we first use a pre-trained MLLM model to generate general text\ndescriptions corresponding to the images. These descriptions do not involve the cate-\ngory names used in the final classification. Next, the images and their corresponding\ntext descriptions are fed into frozen image encoders (such as ViT [43] or ResNet [44])\nand text encoders (such as BERT [45]), respectively, to generate image features and\ntext features. Finally, the image features and text features are combined into multi-\nmodal features through feature concatenation. Mathematically, suppose $X_{i}$ is the input\nimage, and MLLM(*) represents the MLLM model, the text description of image $X_{i}$\nobtained through the MLLM model can be expressed as\n$X_{i}^{t} = MLLM(X_{i})$\nLet $I(X_{i})$ and $T(X_{i})$ represent the series of tokens obtained from the frozen image\nencoder (ViT/ResNet) and text encoder (BERT), respectively. These can be expressed"}, {"title": "3.2 Multi-Modal Graph Construction", "content": "To uncover the structural knowledge in the text embedding space for downstream\ntasks, i.e., the relationships between different semantics, and given the diverse visual\nfeatures of different samples, we can measure finer-grained relationships between dif-\nferent semantics in the visual and textual space. Thus, we can construct a multi-modal\ngraph structure $G = \\{V, E\\} = \\{V^{I},V^{T}; E\\}$, where $V^{I}$ and $V^{T}$ can be seen as the\nsets of image vertices and text vertices, respectively. E represents the set of edges.\nWe build the graph using the similarity of multi-modal features via a predefined\nthreshold \u03b3. When the similarity between two multi-modal features is greater than \u03b3,\nan undirected edge is created between these two vertices, representing the adjacency\nrelationships between all multi-modal features:\n$E_{ij} = \\begin{cases}\n1, & \\text{if } i \\neq j \\text{ and } Sim(V_{i}, V_{j}) > \\gamma \\\\\n0, & \\text{otherwise}\n\\end{cases}$\nwhere\n$Sim(V_{i}, V_{j}) = \\frac{V_{i} \\cdot V_{j}}{\\|V_{i}\\| \\|V_{j}\\|}$\nrepresents the similarity between multi-modal nodes $V_{i}$ and $V_{j}$. \u03b3 is the similarity\nthreshold, and an edge is constructed when the similarity between two vertices in the\ngraph exceeds this threshold."}, {"title": "3.3 Graph Adapter Net", "content": "We propose a parameter-efficient fine-tuning method based on graph networks called\nGraph Adapter Net (GA-Net), where the rest of the network is frozen and only the\nGCN [46] is fine-tuned during downstream tasks. The advantage of this method is\nthat it can adapt to downstream tasks and improve model performance without sig-\nnificantly increasing the number of model parameters. Furthermore, since adapter\nfine-tuning only requires training a small number of parameters, it can significantly"}, {"title": "3.4 Prediction", "content": "In the prediction stage, we introduce Elastic Weight Consolidation (EWC) regulariza-\ntion [12] into the generic cross-entropy loss function. By incorporating the importance\nof parameters and their association with the loss function, the EWC algorithm can"}, {"title": "4 Experiment", "content": "We validated our model on three downstream classification tasks: Oxford Pets [47],\nFlowers102 [48], and Food101 [49]. All these datasets belong to the fine-grained clas-\nsification category. The Oxford Pets dataset contains 37 categories (25 dog breeds\nand 12 cat breeds) with a total of 7,349 images. The Flowers102 dataset includes 102\ncategories with a total of 8,189 images. The Food101 dataset consists of 101 food cat-\negories with a total of 101,000 images. These datasets are not only rich in categories\nbut also possess high fine-grained characteristics, making them ideal for evaluating\nthe model's performance in distinguishing similar categories."}, {"title": "4.1 Experiment Settings", "content": "We use the LlaVA [50] model to generate general text descriptions corresponding to\nthe images, ensuring that these descriptions do not mention the category names for\nfinal classification. Unless otherwise stated, we use the pre-trained backbone ViT-B/16\n[43] as the visual encoder to produce visual features. We optimized the model for 100\nepochs. During training, we used a batch size of 16 and an Adam optimizer with an\ninitial learning rate of 1 \u00d7 10-\u00b3, which decays following a cosine learning rate schedule."}, {"title": "4.2 Comparisons with State-of-the-Arts", "content": "We compared the proposed model with several state-of-the-art parameter-efficient\nfine-tuning methods, including TaskRes [8], CoOp [7], CLIP-adapter [27], and Tip-\nAdapter [28], on the Oxford Pets, Flowers102, and Food101 datasets. As shown in\nTable 1, the experimental results demonstrate that our model consistently outper-\nforms previous parameter-efficient fine-tuning models on the average performance of\nthe benchmark datasets. Our model achieved an average performance of 90.03%, out-\nperforming Tip-Adapter by 3.19% and TaskRes by 2.82%. The model's test accuracy\nimproved by 4.45% and 2.92% on the Oxford Pets and Flowers102 datasets, respec-\ntively, compared to the state-of-the-art methods. Our model still performed the best\non the Food101 dataset. The more significant improvement on the Oxford Pets and\nFood101 datasets is due to the higher need for multi-modal associations, which are\nbetter modeled through GNN. Similarly, Tip-Adapter performs better than other\nSOTA methods as it combines the strengths of prompts and adapters, introducing\ntask-related prompts into the model to provide more multi-modal associations."}, {"title": "4.3 Model Efficiency", "content": "As shown in Table 2, we conducted experiments on the parameter quantities of\ndifferent methods. The parameter consumption of Tip-Adapter is exceptionally high"}, {"title": "4.4 Ablation Study", "content": "As shown in Table 3, the baseline is a simple linear layer trained on single-modal\nfeatures, which are also extracted using a pre-trained model. Without using our foun-\ndational modules, the accuracy is only 86.20%. After applying EWC regularization,\nthe accuracy improves by 1.65% to 87.85%. When using multi-modal learning, the per-\nformance further increases by 2.23% to 90.08%. The improvement with multi-modal\nlearning is because leveraging two modalities simultaneously provides greater capabil-\nity compared to using a single modality. Finally, by integrating the complete graph\nmethod, the accuracy improves by 2.55% to 92.23%. The performance boost from\nincorporating the graph is due to its ability to better model the relationships between\ntokens, thereby demonstrating the effectiveness of our proposed method.\nThe text descriptions for each image are generated by MLLM. In the baseline\nexperiments, replacing all text descriptions uniformly with \"A photo of a pet/flower\"\neliminates text information during training. Ablation experiments show that dif-\nferent combinations of methods and features enhance the model's performance to\nvarying degrees. The GA-Net model achieves the best performance by combining\nEWC[12], multi-modal features, and GNN. Introducing EWC regularization improves\nmodel performance by 1.65%; introducing GNN enhances performance by 2.55%; and\nincorporating multi-modal learning boosts performance by 2.23%. The significant per-\nformance improvements from multi-modal learning and GNN indicate that our model"}, {"title": "4.5 Hyperparameter Study", "content": "To investigate the impact of hyperparameters, specifically similarity thresholds, we\nanalyzed different similarity thresholds on the OxfordPets and Flowers102 datasets,\nas shown in Figure 3. The results indicate that different datasets are affected dif-\nferently by varying similarity thresholds. For both datasets, the accuracy increased\nmost significantly within the threshold range of 0.3 to 0.5. Additionally, the accu-\nracy improvement for the Flowers102 dataset was more pronounced within this range\ncompared to the OxfordPets dataset, suggesting that the Flowers102 dataset is more\nsensitive to adjacency relations in the graph. When the similarity threshold reaches\n0.7, the model achieves peak accuracy on both datasets. For thresholds less than 0.7,\naccuracy shows an increasing trend with rising similarity thresholds. However, once the"}, {"title": "5 Conclusions", "content": "This paper comprehensively reviews the limitations of previous parameter-efficient\nfine-tuning methods in low-data environments. These methods only model a single\nmodality and lack the utilization of structural knowledge in downstream tasks. There-\nfore, we propose a novel parameter-efficient fine-tuning model, GA-Net, which extracts\nknowledge suitable for features from each multi-modal feature node in the graph,\nresulting in features that have fully learned both textual and image information while\nconsidering their adjacency relationships. Experiments on three fine-grained classifi-\ncation tasks, Oxford Pets, Flowers102, and Food101, demonstrate that the GA-Net\nmodel is effective in parameter-efficient fine-tuning.\nThe limitations of the model stem from the generation of text descriptions. In this\npaper, we use MLLM to generate text descriptions for each image. However, these\nprompts are simple and lack sufficient diversity. We believe that providing more diverse\nand accurate prompts for downstream tasks, such as using refined image caption meth-\nods, would better model the textual structural knowledge and further improve the\nperformance of GA-Net."}, {"title": "6 Declarations", "content": "\u2022 Funding\nThis research received no external funding.\n\u2022 Conflict of Interest/Competing Interests\nWe declare that we have no competing interests.\n\u2022 Ethics Approval and Consent to Participate\nThis study did not involve any human or animal subjects, hence ethical approval\nand consent to participate are not applicable.\n\u2022 Consent for Publication\nWe have reviewed the manuscript and consent to its publication.\n\u2022 Data Availability\nThe datasets used in this study are all publicly available:\nOxford Pets dataset: https://www.robots.ox.ac.uk/~vgg/data/pets/\nFlowers-102 dataset:\n102flowers.tgz\nhttps://www.robots.ox.ac.uk/~vgg/data/flowers/102/\nFood101 dataset: https://www.kaggle.com/datasets/dansbecker/food-101\n\u2022 Materials Availability\nNot applicable.\n\u2022 Code Availability\nThe code used in this study is available at: https://github.com/yunche0/GA-Net/\ntree/master\n\u2022 Author Contributions"}]}