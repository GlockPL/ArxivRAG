{"title": "Multi-Modal Parameter-Efficient Fine-tuning via Graph Neural Network", "authors": ["Bin Cheng", "Jiaxuan Lu"], "abstract": "With the advent of the era of foundation models, pre-training and fine-tuning have become common paradigms. Recently, parameter-efficient fine-tuning has garnered widespread attention due to its better balance between the number of learnable parameters and performance. However, some current parameter-efficient fine-tuning methods only model a single modality and lack the utilization of structural knowledge in downstream tasks. To address this issue, this paper proposes a multi-modal parameter-efficient fine-tuning method based on graph networks. Each image is fed into a multi-modal large language model (MLLM) to generate a text description. The image and its corresponding text description are then processed by a frozen image encoder and text encoder to generate image features and text features, respectively. A graph is constructed based on the similarity of the multi-modal feature nodes, and knowledge and relationships relevant to these features are extracted from each node. Additionally, Elastic Weight Consolidation (EWC) regularization is incorporated into the loss function to mitigate the problem of forgetting during task learning. The proposed model achieves test accuracies on the OxfordPets, Flowers102, and Food101 datasets that improve by 4.45%, 2.92%, and 0.23%, respectively. The code is available at https://github.com/yunche0/GA-Net/tree/master.", "sections": [{"title": "Introduction", "content": "With the onset of the era of foundational models, we have gradually entered a paradigm of pre-training and fine-tuning. In terms of downstream task adaptation, full fine-tuning requires adjusting all the parameters of a model to adapt to downstream tasks. However, as the scale of models and the number of tasks increase, such a method becomes inefficient. Consequently, numerous studies have focused on parameter-efficient fine-tuning, exploring strategies to efficiently adapt existing foundation models to downstream tasks. Previous parameter-efficient fine-tuning methods can be mainly categorized into three approaches. The first is prompt tuning [1][2], which aims to achieve fine-tuning by modifying the model input rather than the model structure. The second is prefix tuning [3], which involves updating only task-specific trainable parameters within each layer. The third is adapter tuning [4][5], which achieves parameter-efficient fine-tuning by inserting adapter modules with a bottleneck architecture between layers.\nIn the realm of multi-modal learning, parameter-efficient fine-tuning methods have gained attention in recent research. Prompt vectors are used to align multi-modal data, achieving efficient multi-modal fusion in low-resource settings [6]. The main idea of CoOp [7] is to automatically design prompt texts. It keeps the pre-training parameters unchanged and uses a small amount of data to learn the appropriate cues. The TaskRes [8] method directly adjusts the weights of text-based classifiers without requiring extensive prompt updates to text encoders or elaborate adapters. \u03c0-Tuning optimizes parameters by predicting and interpolating task similarity across visual, language, and visual-language tasks, achieving efficient cross-task transfer learning [9]. However, these works do not consider modeling the complex associations between modalities.\nWith respect to modeling complex associations for parameter-efficient fine-tuning, methods related to graph neural networks (GNNs) have been explored. The concept of timely tuning has been introduced [10]. In the molecular domain, MolCPT [11] enhances graph embeddings by encoding additional molecular motif information. However, these works primarily focus on how to perform parameter-efficient fine-tuning for purely graph structures, without applying them to language-image multi-modal modeling.\nTherefore, we propose a framework that combines graph structures with multi-modal parameter-efficient fine-tuning methods, enabling the learning of multi-modal information while considering the complex associations between modalities. The proposed model comprises four main modules: Multi-Modal Feature Extraction, Multi-Modal Graph Construction, Graph Adapter Net (GA-Net), and Prediction. In the Multi-Modal Feature Extraction module, each image is processed by a pre-trained MLLM model to obtain a text description for each image. The image and its corresponding text description are then processed by frozen image and text encoders to generate image features and text features, respectively. These features are combined into multi-modal features through feature concatenation. In the Multi-Modal Graph Construction module, a graph is constructed based on the similarity of multi-modal feature nodes. The GA-Net module then mines suitable knowledge from the graph"}, {"title": "Related Work", "content": "Parameter-Efficient Fine-Tuning Methods (PEFT). Full fine-tuning involves modifying all the model's parameters to suit downstream tasks. Yet, as models grow in scale and the number of tasks expands, this approach becomes increasingly inefficient. To address this issue, in recent years, the natural language processing (NLP) community has explored parameter-efficient fine-tuning techniques (PEFT) [13][14][15][16][17]. These techniques only require adjusting a small subset of parameters, thereby improving efficiency [18]. For example, prompt tuning methods [1] attempt to achieve fine-tuning by modifying the model input rather than the model structure. Prefix tuning [3] updates only task-specific trainable parameters in each layer. Adapter tuning [4][5] inserts adapter modules with bottleneck architectures between layers to achieve parameter-efficient fine-tuning. Additionally, methods like BitFit [19] update only the bias terms and freeze the remaining parameters, while LORA [20] reduces the number of trainable parameters by decomposing the weight matrix into low-rank matrices.\nIn multi-modal learning, parameter-efficient fine-tuning methods have gained widespread attention in recent research [21] [22] [23] [24] [25]. Using prompt vectors to align multi-modal data achieves efficient multi-modal fusion in low-resource environments, excelling in tasks involving two or more data modalities [6]. Research on scaling large multi-modal models (such as LLaVA and MiniGPT-4) has shown that parameter-efficient training methods like LoRA/QLORA perform well in both multi-modal and language tasks, with performance comparable to full-model fine-tuning[26]. The main idea of CoOp [7] is to automatically design prompt texts. It keeps the pre-trained"}, {"title": "Method", "content": "As shown in Figure 1, the model presented in this paper is composed of four main modules: Multi-Modal Feature Extraction, Multi-Modal Graph Construction, GA-Net, and Prediction. The primary function of the Multi-Modal Feature Extraction is to use"}, {"title": "Multi-Modal Feature Extraction", "content": "In this module, we first use a pre-trained MLLM model to generate general text descriptions corresponding to the images. These descriptions do not involve the category names used in the final classification. Next, the images and their corresponding text descriptions are fed into frozen image encoders (such as ViT [43] or ResNet [44]) and text encoders (such as BERT [45]), respectively, to generate image features and text features. Finally, the image features and text features are combined into multi-modal features through feature concatenation. Mathematically, suppose $X_i$ is the input image, and MLLM(*) represents the MLLM model, the text description of image $X_i$ obtained through the MLLM model can be expressed as\n$X_i^t = \\text{MLLM}(X_i)$                                                          (1)\nLet $I(X_i)$ and $T(X_i)$ represent the series of tokens obtained from the frozen image encoder (ViT/ResNet) and text encoder (BERT), respectively. These can be expressed"}, {"title": "Multi-Modal Graph Construction", "content": "To uncover the structural knowledge in the text embedding space for downstream tasks, i.e., the relationships between different semantics, and given the diverse visual features of different samples, we can measure finer-grained relationships between different semantics in the visual and textual space. Thus, we can construct a multi-modal graph structure $G = \\{V, E\\} = \\{V^I,V^T; E\\}$, where $V^I$ and $V^T$ can be seen as the sets of image vertices and text vertices, respectively. E represents the set of edges.\nWe build the graph using the similarity of multi-modal features via a predefined threshold $\\gamma$. When the similarity between two multi-modal features is greater than $\\gamma$, an undirected edge is created between these two vertices, representing the adjacency relationships between all multi-modal features:\n$E_{ij} = \\begin{cases}\n1, & \\text{if } i \\neq j \\text{ and } \\text{Sim}(V_i, V_j) > \\gamma \\\\\n0, & \\text{otherwise}\n\\end{cases}$                                                         (5)\nwhere\n$\\text{Sim}(V_i, V_j) = \\frac{V_i \\cdot V_j}{\\|V_i\\| \\|V_j\\|}$                                                                                 (6)\nrepresents the similarity between multi-modal nodes $V_i$ and $V_j$. $\\gamma$ is the similarity threshold, and an edge is constructed when the similarity between two vertices in the graph exceeds this threshold."}, {"title": "Graph Adapter Net", "content": "We propose a parameter-efficient fine-tuning method based on graph networks called Graph Adapter Net (GA-Net), where the rest of the network is frozen and only the GCN [46] is fine-tuned during downstream tasks. The advantage of this method is that it can adapt to downstream tasks and improve model performance without significantly increasing the number of model parameters. Furthermore, since adapter fine-tuning only requires training a small number of parameters, it can significantly"}, {"title": "Prediction", "content": "In the prediction stage, we introduce Elastic Weight Consolidation (EWC) regularization [12] into the generic cross-entropy loss function. By incorporating the importance of parameters and their association with the loss function, the EWC algorithm can"}, {"title": "Experiment", "content": "We validated our model on three downstream classification tasks: Oxford Pets [47], Flowers102 [48], and Food101 [49]. All these datasets belong to the fine-grained classification category. The Oxford Pets dataset contains 37 categories (25 dog breeds and 12 cat breeds) with a total of 7,349 images. The Flowers102 dataset includes 102 categories with a total of 8,189 images. The Food101 dataset consists of 101 food categories with a total of 101,000 images. These datasets are not only rich in categories but also possess high fine-grained characteristics, making them ideal for evaluating the model's performance in distinguishing similar categories."}, {"title": "Implementation Details", "content": "We use the LlaVA [50] model to generate general text descriptions corresponding to the images, ensuring that these descriptions do not mention the category names for final classification. Unless otherwise stated, we use the pre-trained backbone ViT-B/16 [43] as the visual encoder to produce visual features. We optimized the model for 100 epochs. During training, we used a batch size of 16 and an Adam optimizer with an initial learning rate of 1 \u00d7 10\u207b\u00b3, which decays following a cosine learning rate schedule."}, {"title": "Comparisons with State-of-the-Arts", "content": "We compared the proposed model with several state-of-the-art parameter-efficient fine-tuning methods, including TaskRes [8], CoOp [7], CLIP-adapter [27], and Tip-Adapter [28], on the Oxford Pets, Flowers102, and Food101 datasets. As shown in Table 1, the experimental results demonstrate that our model consistently outperforms previous parameter-efficient fine-tuning models on the average performance of the benchmark datasets. Our model achieved an average performance of 90.03%, outperforming Tip-Adapter by 3.19% and TaskRes by 2.82%. The model's test accuracy improved by 4.45% and 2.92% on the Oxford Pets and Flowers102 datasets, respectively, compared to the state-of-the-art methods. Our model still performed the best on the Food101 dataset. The more significant improvement on the Oxford Pets and Food101 datasets is due to the higher need for multi-modal associations, which are better modeled through GNN. Similarly, Tip-Adapter performs better than other SOTA methods as it combines the strengths of prompts and adapters, introducing task-related prompts into the model to provide more multi-modal associations."}, {"title": "Model Efficiency", "content": "As shown in Table 2, we conducted experiments on the parameter quantities of different methods. The parameter consumption of Tip-Adapter is exceptionally high"}, {"title": "Ablation Study", "content": "As shown in Table 3, the baseline is a simple linear layer trained on single-modal features, which are also extracted using a pre-trained model. Without using our foundational modules, the accuracy is only 86.20%. After applying EWC regularization, the accuracy improves by 1.65% to 87.85%. When using multi-modal learning, the performance further increases by 2.23% to 90.08%. The improvement with multi-modal learning is because leveraging two modalities simultaneously provides greater capability compared to using a single modality. Finally, by integrating the complete graph method, the accuracy improves by 2.55% to 92.23%. The performance boost from incorporating the graph is due to its ability to better model the relationships between tokens, thereby demonstrating the effectiveness of our proposed method.\nThe text descriptions for each image are generated by MLLM. In the baseline experiments, replacing all text descriptions uniformly with \"A photo of a pet/flower\" eliminates text information during training. Ablation experiments show that different combinations of methods and features enhance the model's performance to varying degrees. The GA-Net model achieves the best performance by combining EWC[12], multi-modal features, and GNN. Introducing EWC regularization improves model performance by 1.65%; introducing GNN enhances performance by 2.55%; and incorporating multi-modal learning boosts performance by 2.23%. The significant performance improvements from multi-modal learning and GNN indicate that our model"}, {"title": "Hyperparameter Study", "content": "To investigate the impact of hyperparameters, specifically similarity thresholds, we analyzed different similarity thresholds on the OxfordPets and Flowers102 datasets, as shown in Figure 3. The results indicate that different datasets are affected differently by varying similarity thresholds. For both datasets, the accuracy increased most significantly within the threshold range of 0.3 to 0.5. Additionally, the accuracy improvement for the Flowers102 dataset was more pronounced within this range compared to the OxfordPets dataset, suggesting that the Flowers102 dataset is more sensitive to adjacency relations in the graph. When the similarity threshold reaches 0.7, the model achieves peak accuracy on both datasets. For thresholds less than 0.7, accuracy shows an increasing trend with rising similarity thresholds. However, once the"}, {"title": "Conclusions", "content": "This paper comprehensively reviews the limitations of previous parameter-efficient fine-tuning methods in low-data environments. These methods only model a single modality and lack the utilization of structural knowledge in downstream tasks. Therefore, we propose a novel parameter-efficient fine-tuning model, GA-Net, which extracts knowledge suitable for features from each multi-modal feature node in the graph, resulting in features that have fully learned both textual and image information while considering their adjacency relationships. Experiments on three fine-grained classification tasks, Oxford Pets, Flowers102, and Food101, demonstrate that the GA-Net model is effective in parameter-efficient fine-tuning.\nThe limitations of the model stem from the generation of text descriptions. In this paper, we use MLLM to generate text descriptions for each image. However, these prompts are simple and lack sufficient diversity. We believe that providing more diverse and accurate prompts for downstream tasks, such as using refined image caption methods, would better model the textual structural knowledge and further improve the performance of GA-Net."}, {"title": "Declarations", "content": "Funding\nThis research received no external funding.\nConflict of Interest/Competing Interests\nWe declare that we have no competing interests.\nEthics Approval and Consent to Participate\nThis study did not involve any human or animal subjects, hence ethical approval and consent to participate are not applicable.\nConsent for Publication\nWe have reviewed the manuscript and consent to its publication.\nData Availability\nThe datasets used in this study are all publicly available:\nOxford Pets dataset: https://www.robots.ox.ac.uk/~vgg/data/pets/\nFlowers-102 dataset: https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\nFood101 dataset: https://www.kaggle.com/datasets/dansbecker/food-101\nMaterials Availability\nNot applicable.\nCode Availability\nThe code used in this study is available at: https://github.com/yunche0/GA-Net/tree/master\nAuthor Contributions"}]}