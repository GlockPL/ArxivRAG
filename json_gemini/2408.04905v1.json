{"title": "GlitchProber: Advancing Effective Detection and Mitigation of Glitch Tokens in Large Language Models", "authors": ["Zhibo Zhang", "Wuxia Bai", "Yuxi Li", "Mark Huasong Meng", "Kailong Wang", "Ling Shi", "Li Li", "Jun Wang", "Haoyu Wang"], "abstract": "Large language models (LLMs) have achieved unprecedented success in the field of natural language processing. However, the black-box nature of their internal mechanisms has brought many concerns about their trustworthiness and interpretability. Recent research has discovered a class of abnormal tokens in the model's vocabulary space and named them \u201cglitch tokens\". Those tokens, once included in the input, may induce the model to produce incorrect, irrelevant, or even harmful results, drastically undermining the reliability and practicality of LLMs.\nIn this work, we aim to enhance the understanding of glitch tokens and propose techniques for their detection and mitigation. We first reveal the characteristic features induced by glitch tokens on LLMs, which are evidenced by significant deviations in the distributions of attention patterns and dynamic information from intermediate model layers. Based on the insights, we develop GLITCHPROBER, a tool for efficient glitch token detection and mitigation. GLITCHPROBER utilizes small-scale sampling, principal component analysis for accelerated feature extraction, and a simple classifier for efficient vocabulary screening. Taking one step further, GLITCHPROBER rectifies abnormal model intermediate layer values to mitigate the destructive effects of glitch tokens. Evaluated on five mainstream open-source LLMS, GLITCHPROBER demonstrates higher efficiency, precision, and recall compared to existing approaches, with an average F1 score of 0.86 and an average repair rate of 50.06%. GLITCHPROBER unveils a novel path to address the challenges posed by glitch tokens and inspires future research toward more robust and interpretable LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "In the field of Natural Language Processing (NLP), large language models (LLMs) like GPT-4 [2], Gemini [30, 33], and Claude 3 [4] have demonstrated near-human-level text generation capabilities. Their exceptional performance has led to widespread adoption [34, 40].\nWhen using these models, users provide a prompt, which the model's tokenizer breaks down into a series of discrete tokens. These tokens are the fundamental units of information processing for the model, playing a crucial role in the usage of LLMs. Recent research [12, 13, 25, 27, 28, 31], however, has shown that some \"glitch tokens\" exist in the vocabulary of LLMs. Once included in a prompt, these special tokens can potentially lead to model errors, such as misunderstanding user intent, refusing to answer, or generating irrelevant or harmful text. Therefore, thorough analysis and detection of these glitch tokens are crucial to ensure the reliability and safety of LLMs.\nTo tackle the glitch tokens issue, one notable method recently is presented by Li et al. [20]. This typical and intuitive solution involves studying the characteristics of glitch tokens in the word embedding space of LLMs and accordingly developing detection techniques. They discovered that glitch tokens tend to cluster in the embedding space and proposed an iterative clustering-based technique called GLITCHHUNTER for efficient glitch token detection.\nAlthough there has been progress in detecting glitch tokens, there still lacks an efficient and precise detection of glitch tokens universally applicable in different LLMs. Furthermore, existing approaches primarily focus on detection, however, how to fix the issues caused by glitch tokens in the usage of LLMs remains an open question. Several limitations contribute to the aforementioned challenges:\n(1) The exhaustive search method of checking vocabulary is simple and intuitive but incurs significant time costs with large token sets, making it inefficient for practical use.\n(2) Existing detection methods primarily identify glitch tokens based on features like word frequency and word vectors. However, these features do not deeply explore the mechanisms by which glitch tokens impact model behaviors, resulting in poor detection accuracy and generalization performance.\n(3) Current research primarily focuses on detecting glitch tokens rather than how to fix them. While detection can identify issues, it does not eliminate the negative impact of glitch tokens on model performance, limiting its practical value.\nOur work. To address these existing challenges and bridge the gap, in this work, we investigate the internal structure of LLMs to explore the differences between glitch tokens and normal tokens. Specifically, through empirical study, we discovered significant differences between glitch tokens and normal tokens in terms of the attention patterns and dynamic information of multi-layer perceptron (MLP) modules within transformer-based LLMs. This discovery reveals the adversarial impact of glitch tokens on the internal mechanisms of the model, indicating that glitch tokens introduce abnormal interference and noise to neural networks. This hinders the model from correctly understanding and processing the semantic information carried by these tokens, ultimately leading to erroneous outputs.\nFrom these findings, we gain an insight that the glitch tokens can be efficiently detected due to the deviated distributions of intermediate layers' outputs caused by them, and accordingly their impact can be effectively mitigated by proactively rectifying those abnormal outputs. Based on this insight, we propose a new method for glitch token detection and fix called GLITCHPROBER. For glitch token detection, GLITCHPROBER first samples a small subset of manually labeled glitch tokens as the sample set, and extracts the outputs of these tokens from the intermediate layers, specifically, the attention scores of the attention patterns and MLP status. It then applies Principal Component Analysis (PCA) [1] dimensionality reduction to the intermediate layers' outputs and obtains a feature representation matrix for the sample set. This matrix, along with the corresponding class labels, is used to train a Support Vector Machine (SVM) [9] classifier, which can subsequently be employed for glitch token detection. To fix the glitch tokens, GLITCHPROBER analyzes the activation value range of normal tokens in the intermediate MLP status and rectifies the activation states of glitch tokens. Specifically, it aims to adjust the activation patterns of glitch tokens to be closer to those of normal tokens, and thereby minimize their impact on the model's output.\nContributions. We summarize our key contributions as follows:\n\u2022 Empirical Study Exploring the Internal Impact of Glitch Tokens on LLMs. We conduct a comprehensive and systematic empirical study on how glitch tokens and normal tokens manifest at the structural level across different LLMs. One of our key findings is that glitch tokens can trigger abnormal values in a model's attention patterns and MLP status.\n\u2022 Effective Glitch Token Detection. Our evaluation on five representative open source LLMs demonstrates that GLITCHPROBER can save approximately 40% of time in glitch token detection compared to the state-of-the-art approaches. Additionally, GLITCHPROBER exhibits a significant improvement in detection accuracy.\n\u2022 Effective Glitch Token Fixing. In terms of fix, GLITCHPROBER successfully repairs an average of 7,758 tokens across the five LLMs. It achieves an average repair rate of 50.06%, significantly outperforming the baseline approach. Our results demonstrate the effectiveness of the proposed fix strategy by adjusting the intermediate values of glitch tokens in the intermediate layers.\nEthical Consideration. In this work, we recognize that glitch tokens can cause abnormal responses from LLMs, potentially affecting their usage. However, we strictly adhere to ethical principles and do not condone any abuse or exploitation of these findings. Our research aims to raise awareness of these risks and contribute to a more secure LLM community. We have reported our findings to the respective LLM developers and are committed to collaborating with them to develop effective defenses and mitigation strategies. By working cooperatively, we promote responsible research practices and ensure the safe and beneficial use of LLMs."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "2.1 Transformer-based LLMs\nSelf-attention [7, 24] is a core component of Transformer-based models, demonstrating strong modeling capabilities across various tasks. Given an input $X \\in \\mathbb{R}^{n \\times d}$, where n denotes the sequence length and d denotes the dimension, self-attention linearly projects X into query, key, and value representations, i.e., Q, K, and V. The attention scores matrix A is then computed by taking the dot product between the query and key matrices, followed by a softmax normalization. The attention output is obtained by multiplying the attention scores with the value matrix.\n$A = softmax(\\frac{QK^T}{\\sqrt{d}})$\n(1)\n$Attention(Q, K, V) = A \\cdot V$\n(2)\nTo analyze the behavior of Transformer-based models during sequence processing, we introduce the concept of attention patterns, which can be extracted from the corresponding row $A[n]$ of the attention scores matrix A. In autoregressive generation tasks, the attention patterns only contain the attention weights between the current token and previously generated tokens.\nThe MLP module in Transformer-based models employs a gating mechanism similar to that of Gated Multi-Layer Perceptrons (gMLPs) [22]. Given an input $Y \\in \\mathbb{R}^{n \\times d}$, where n denotes the sequence length and d denotes the dimension, the MLP first projects Y to a higher-dimensional space using a linear transformation:\n$Z = YU$\n(3)\nwhere $U \\in \\mathbb{R}^{d \\times d_m}$ is a learnable weight matrix. The transformed representation Z is then split along the feature dimension into two matrices, $Z_1, Z_2 \\in \\mathbb{R}^{n \\times d_m/2}$:\n$Z_1, Z_2 = split(Z)$\n(4)\nAn activation function $\\sigma$ is applied element-wise to $Z_1$ to obtain the MLP gate $\\sigma(Z_1)$. The MLP gate is then multiplied element-wise with the MLP data $Z_2$ to produce the gated output $\\tilde{Z}$:\n$\\tilde{Z} = \\sigma(Z_1) \\odot Z_2$\n(5)"}, {"title": "2.2 Glitch Token Phenomenon", "content": "Tokenizer plays a key role in an LLM as it transforms a continuous text sequence into a list of discrete values called tokens [38]. The tokens transformed from the training corpus form the vocabulary dictionary of LLMs, and the vocabulary dictionary in turn determines the capacity of LLMs to produce diverse and comprehensive output. The rapid advancement of LLMs has brought attention to various anomalous phenomena [10, 11, 18, 19, 21, 23, 39], one of which is the existence of \"glitch tokens\". These tokens exhibit anomalies in constructing the expected semantics, and are subsequently reflected in the abnormal and unexpected decoding in the LLM's output.\nThe glitch token phenomenon, first explored on the Lesswrong website, refers to anomalous tokens such as \"SolidGoldMagikarp\" and \"petertodd\" that cause unexpected and inaccurate results in language models like GPT-2 and GPT-J [25, 27, 28, 31]. Subsequent research examined the characteristics and instability of these tokens, revealing that even subtle changes in prompts can lead to significant differences and hallucinations in the generated results. The discovery of \"polysemous\" tokens, which produce different responses to repeated requests, further highlighted the prevalence and variability of the glitch token phenomenon in LLMs [26].\nRecently, Li et al. [20] systematically investigated the glitch tokens with a proposed taxonomy covering their types and symptoms. They proposed three tasks, namely repetition, length and spelling, in their study to recognize glitch tokens. Their observation of the clustering distribution of glitch tokens in the word embedding spaces offers a novel perspective on the automatic identification of glitch tokens, making systematic detection feasible in LLMs containing billions, or even tens of billions of parameters.\nThe glitch token phenomenon uncovers the limitations and instability of LLMs when processing specific tokens. In this work, we aim to conduct a systematic and in-depth investigation of this phenomenon to gain a deeper understanding of the internal mechanisms of these models. This will provide valuable insights that can contribute to enhancing the robustness and reliability of LLMs in future applications."}, {"title": "3 EMPIRICAL STUDY", "content": "Our empirical study aims to explore an intuitive method for detecting glitch tokens. To this end, we investigate the differences in the model's behaviors when processing glitch tokens versus normal tokens. Two research questions are raised to guide the study:\n\u2022 RQ1 (Characteristics): What differences are exhibited between glitch tokens and normal tokens at the structural level of an LLM?\n\u2022 RQ2 (Ubiquity): Are the differences discovered in RQ1 prevalent in most LLMs?\nTo address the two RQs, we investigate the internal mechanisms of LLMs by analyzing the status of each layer in the transformer forward process of prediction. This involves examining the data flow of the intermediate layers as the model processes inputs.\n3.1 Experiment Setup\nTo better understand the impact of glitch tokens on the model's internal output generation process, we conduct a series of experiments on the Llama-2-7b-chat model [35], shortly written as Llama2. Llama2 is a language model based on the Llama architecture. In our experiments, we set the temperature to 0 to eliminate randomness and ensure consistency in the model's responses. All other configurations are default.\nWe employ a unified approach to determine whether each token is glitchy or normal in this study. While the symptoms of glitch tokens may vary across different tasks, we consistently utilize a repetition task to construct input sequences for glitch token identification. Specifically, we formulate a prompt for a repetitive task such as \"Can you repeat the token '{token}' and return it back to me?\" This prompt is then fed into the model to assess its ability to accurately reproduce the original token. If the model fails to correctly repeat the token in its output, we classify it as a glitch token. Conversely, if the model successfully repeats the token, we categorize it as a normal token. We traverse all the 32,000 tokens of the Llama2 model and eventually identify 6,425 glitch tokens from the entire vocabulary.\nTo precisely capture intermediate layer outputs within the model, we resort to a transformer mechanistic interpretability tool named Transformer-lens [29]. Its hook technique enables real-time access of the activation values at all layers and allows code insertion into specific intermediate layers of the model. In this study, we insert hooks into all intermediate layers during the first forward of the tested model. This approach is chosen because the first forward comprehensively reflects the model's understanding of the input sequence and highlights the differences between normal and glitch tokens.\nWe select two key features to represent the model's internal output, i.e., attention patterns and MLP status. The attention patterns capture the relative importance and relationships between tokens, while MLP status is composed of MLP gate and MLP data (Section 2.1), providing insights on how the model synthesizes and modulates new representations within the MLP module."}, {"title": "3.2 RQ1: Glitch Token Characteristics", "content": "We compare the extracted intermediate results of Llama2 model when processing prompts containing normal tokens and glitch tokens and observe significant disparity in attention patterns and MLP status. The distribution of attention patterns for glitch tokens in some attention heads differs significantly from that of normal tokens. To quantitatively analyze these differences, we randomly sample normal tokens in size of the same number as the pre-identified glitch tokens. To analyze the attention patterns, we create two sets of prompts: one containing 6,425 prompts with glitch tokens and another containing 6,425 prompts with normal tokens. We then compute the frequency distribution of attention patterns generated by these prompts, categorizing them into different value ranges. We visualize the results using a histogram, as shown in Figure 1 (a). The attention patterns of normal tokens (shown in blue color) generally cluster around lower ranges and exhibit a relatively smooth distribution. In contrast, the attention patterns for glitch tokens (in red color) display a comparably divergent and chaotic distribution.\nFurthermore, the distribution characteristics of glitch tokens in the MLP status show deviations compared to normal tokens. Due to the high dimension of MLP status values, we cannot visualize them in the same method used for attention patterns. Instead, we resort to PCA algorithm to convert the captured MLP status, i.e., MLP gate and MLP data, to two dimension values. We present the distribution of MLP gate and MLP data in scatter plots, as shown in Figure 1 (b). It can be observed that both two representations of MLP status for normal tokens tend to cluster towards a centroid, forming a relatively dense and bounded distribution. In contrast, the MLP status of glitch tokens are highly dispersed and scattered.\nFinding 1\nLlama2 shows a significant disparity in attention patterns and MLP status when dealing with glitch tokens and normal tokens.\nTo illustrate the anomalies across layers, we resort to the Wasserstein distance [37] to measure the magnitude of the differences in the intermediate layers outputs produced by normal and glitch tokens, and thereby reveal the distinctions in the model's internal mechanisms when processing these two groups of tokens. In this study, a larger Wasserstein distance indicates a greater distributional difference. Figure 2 shows the Wasserstein distance in attention patterns and MLP status between normal and glitch tokens across different layers of the Llama2 model. We find that the differences caused by normal and glitch tokens per layer are not uniformly distributed. The attention patterns and MLP status exhibit greater differences in the downstream layers closer to the output, e.g., layers 19-31. This finding suggests that the impact of glitch tokens, although may result in negligible erroneous results in front layers, is amplified along with the propagation, leading to unexpected outputs in the end.\nFinding 2\nThe anomalous intermediate results caused by glitch tokens are not uniformly distributed across all layers of the model but are concentrated and amplified in specific key layers."}, {"title": "3.3 RQ2: Ubiquity", "content": "In order to verify whether our previous findings exist in other LLMs, two additional LLMs, namely Qwen-7B-Chat model and Mistral-7B-Instruct model (shortly as Qwen and Mistral), are selected to complement our empirical study. As shown in Figure 3, the experimental results on these two models are similar to those of Llama2. The attention patterns of glitch tokens and normal tokens exhibit inconsistent distribution. For example, the attention patterns of normal tokens mainly fall within the range of [0, 0.2] in the Qwen model, while the attention patterns of glitch tokens show a different shape and concentrates in the range of [0.8, 1]. Such inconsistency can also be observed in the Mistral model, evidenced by the attention values of normal tokens and glitch tokens primarily approximating around 0.5 and 0.7, respectively. In terms of MLP status, the intermediate layers tend to produce outputs in a centroid-based cluster for normal tokens. However, the MLP status values of glitch tokens are overall chaotic and disseminated.\nFinding 3\nWe identify similar differences exhibited between normal and glitch tokens at the intermediate layers of different LLMs.\nThe findings of RQ1 provide insights for the subsequent detection and fix of glitch tokens, while the finding in RQ2 offers factual evidence for the broad application of our approach in LLMs."}, {"title": "4 METHODOLOGY", "content": "Based on the findings from our empirical study, we propose the GLITCHPROBER algorithm, which aims to achieve automatic detection and fix of glitch tokens by analyzing the internal activation states of LLMS. Our approach consists of two main ideas:\n(1) Leveraging the differences in model activation values when processing glitch tokens and normal tokens to achieve rapid screening of glitch tokens. By designing an anomaly detection algorithm, we can identify and label potential glitch tokens based on their activation features extracted from specific layers, which we refer to as key layers (detailed in Section 4.3). These key layer features are crucial for detecting glitch tokens.\n(2) Fixing errors caused by glitch tokens by adjusting the model's intermediate results. We designed a series of experiments where we automatically adjusted the output values of the model's intermediate layers and observed the impact on the final output.\nAs elucidated in Section 3.2, glitch tokens in the model predominantly affect the downstream layers close to the output, notably impacting attention patterns and MLP status in specific key layers. This revelation is instrumental in shaping our approach of GLITCHPROBER. By strategically focusing our efforts on key layers, we can significantly reduce computational overhead while achieving a comparable level of detection and fix efficacy to traversing all"}, {"title": "4.1 Detecting Glitch Tokens via GLITCHPROBER", "content": "GLITCHPROBER identifies and detects glitch tokens that cause model output errors by analyzing the intermediate layer activation states of Transformer language models when processing tokens. The main workflow of the algorithm is shown in Figure 4. The detection algorithm of GLITCHPROBER consists of three main steps: feature extraction and dimension reduction, SVM-based glitch token classifier, and glitch token identification and validation.\n4.1.1 Feature Extraction and Dimension Reduction. GLITCHPROBER adopts a random sampling strategy to select samples from the model's token vocabulary V to form the sample set S. The choice of sampling rate \u03b3 needs to balance between sample size and computational efficiency. A larger \u03b3 leads to a larger sample size and more accurate detection results but also incurs higher computational costs. Conversely, a smaller \u03b3 results in a smaller sample size and faster computation but may affect the detection performance. Through experiments, we determined that when \u03b3 is in the range of [0.1, 0.3], GLITCHPROBER achieves a good balance between detection performance and efficiency.\nGLITCHPROBER uses a unified repetitive task to construct input sequences for glitch token identification in the sample set S. For tokens in the sample set S, the algorithm assigns corresponding category labels according to the output results of the repetitive task. At the same time, we extract the attention pattern, MLP gate and MLP data features in the model's first forward process. However, the dimension of the original feature tensors is high, and directly using them to train the classifier would lead to excessive computational costs. To improve computational efficiency, we adopt a dimension reduction strategy. The PCA algorithm [1] is applied to reduce the dimension, mapping the original high-dimensional features to a low-dimensional subspace while maximally preserving the discriminative information of the features. Through experiments, we found that when the dimension P of the reduced features is in the range of [50, 200], a good balance between computational efficiency and information retention can be achieved. Therefore, we set P = 75 as the default dimension reduction parameter.\n4.1.2 SVM-based Glitch Token Classifier. GLITCHPROBER trains an SVM classifier using the low-dimensional feature representation matrix F of the sample set S and the corresponding category labels. The trained SVM classifier will be used for subsequent glitch assessment of unknown tokens. SVM is a binary classification algorithm"}, {"title": "4.1.3 Glitch Token Identification and Validation", "content": "The tokens in the token vocabulary that were not sampled are individually detected. The token to be detected is input into the same repetitive task as in the training phase, and its features attention patterns, MLP gate, and MLP data are extracted in the model's first forward module. Subsequently, the trained SVM classifier is used to make predictions based on the extracted features.\nIf a token is predicted as \"glitchy\", the algorithm will input this token into the model and further use the repetitive task to validate it. If the model can correctly repeat the token, we consider it as a potential normal token, and the SVM classifier may have made a false positive prediction. Through this post-processing step, GLITCHPROBER can effectively reduce the false positive rate of glitch token detection and improve the precision of detection. Finally, GLITCHPROBER outputs two sets: the set of glitch tokens G and the set of normal tokens N.\n4.1.4 Detection Process Algorithm. The pseudocode for the detection algorithm of GLITCHPROBER is shown in Algorithm 1. Initially, the algorithm samples a subset of tokens (S) from the vocabulary (V) based on a predefined sampling rate (\u03b3) (line 1). For each token in this subset, the algorithm extracts relevant features using a transformer model over specified key layers and assigns labels indicating whether each token is glitchy (lines 2-5).\nFollowing the feature extraction, the algorithm applies PCA to reduce the dimension of these features to a lower-dimensional space (P) (line 6). The reduced feature set (F) is then used to train a SVM classifier with the assigned glitch labels (line 7).\nFor tokens not included in the initial sample, the algorithm uses the trained SVM classifier to predict whether each token is normal or a glitch (lines 8-12). Tokens classified as glitches will undergo a further validation step to confirm their status, effectively minimizing the false positive rate (lines 14-18). Finally, the algorithm compiles and outputs two sets, namely glitch token set (G) and normal token set (N)."}, {"title": "4.2 Fixing Glitch Tokens via GLITCHPROBER", "content": "We further explore the possibility of correcting the anomalous activation patterns of glitch tokens to normal patterns by adjusting the activation values of the model's intermediate layers, thereby eliminating their negative impact on the model output. Based on the idea of adjusting neuron activation values to eliminate the influence of glitch tokens, we focus on two types of neurons in normal tokens, i.e., the neurons that are activated in the vast majority of normal tokens, and the neurons that are not activated in any normal tokens. Then we compare the differences in activation values of these key neurons between normal tokens and glitch tokens. By simulating those normal activation patterns, we achieve adaptively adjustment of the activation values of glitch tokens. The overall approach is illustrated in Figure 5.\n4.2.1 Normal Token Activation Value Statistics. GLITCHPROBER first randomly samples a subset of normal tokens, i.e., N' \u2286 N, to calculate the activation value distribution of normal tokens in the target layers. We set the sampling rate to \u03b3, which is consistent with the sampling rate in the glitch token detection phase.\nFor the MLP module in each layer, we calculate the activation statistics of the tokens in the normal token set N'. We define two sets of neuron indices as $Neun\u2191$ and $Neun\u2193$ based on their activation patterns, where $Act[i]$ represents the activation value of the i-th neuron in the MLP module, and m is the predefined threshold.\n$Neun\u2191 = {i | Act[i] > m for over 99\\% of tokens in N'}$\n(7)\n$Neun\u2193 = {i | Act[i] < m for all tokens in N'}$\n(8)\n$Neun\u2191$ denotes the set of key neurons that exhibit high activation levels, surpassing predefined threshold m, across sample token set N'. Given that activated neurons constitute a small proportion of the total neurons, we consider a neuron to be a key neuron if it is activated in over 99% of the tokens. Conversely, $Neun\u2193$ represents the set of key neurons that exhibit consistently low activation levels, falling below m. We consider a neuron to be a key neuron in $Neun\u2193$ if its activation level remains below the threshold m for all tokens in N'. These neurons can be considered as the key features for suppressing noise, as they are consistently inactive for normal tokens. By identifying these two sets of neuron indices based on their activation patterns, we can create a profile of the expected behavior of normal tokens at each MLP module. Those methods ensure the recorded neurons take the most informative features when we mitigate the influence of noise and irrelevant information caused by glitch tokens.\nNote that we only adjust the activation values of the MLP module and not the attention patterns. Attention patterns capture the relative importance between tokens, and modifying them may disrupt token dependencies and introduce noise. In contrast, MLP activation values reflect the model's understanding of each token independently, and adjusting these values has less impact on token relationships. By only adjusting MLP activation values, we aim to fix glitch tokens without introducing additional noise."}, {"title": "4.2.2 Activation Value Adjustment", "content": "When the hooked model processes detected glitch tokens, GLITCHPROBER intervenes in this process, making trend-based adjustments to the neurons identified by $Neun\u2191$ and $Neun\u2193$. For neurons in $Neun\u2191$ that should be activated but have insufficient activation in glitch tokens, the algorithm uses \u03b2 as an amplification factor to promote their activation. Conversely, for neurons in $Neun\u2193$ that should be suppressed but are abnormally activated in glitch tokens, the algorithm uses \u03b1 as a reduction factor to suppress their anomalous activation. The factors \u03b2 and \u03b1 are named adjustment factors, which play a crucial role in the glitch token fixing process. For the calculation of \u03b2 and \u03b1, we first calculate the average activation value difference \u2206Act\u2191 of glitch tokens relative to normal tokens on highly activated neurons ($Neun\u2191$), and the average activation value ratio \u2206Act\u2193 on lowly activated neurons ($Neun\u2193$).\n$\\Delta Act\u2191 = \\frac{1}{|Neun\u2191|} \\sum_{i \\in Neun\u2191}(Act_{normal}[i] - Act_{glitch}[i])$    (9)\n$\\Delta Act\u2193 = \\frac{1}{|Neun\u2193|} \\sum_{i \\in Neun\u2193}(\\frac{Act_{glitch}[i]}{Act_{normal}[i]})$\n(10)\nSubsequently, through linear transformation and range restriction, the algorithm maps \u2206Act\u2191 and \u2206Act\u2193 to appropriate numerical intervals to obtain the values of \u03b2 and \u03b1, respectively.\n$\\beta = k_1 \\cdot \\Delta Act\u2191 + b_1$\n(11)\n$\\alpha = k_2 \\cdot \\Delta Act\u2193 + b_2$\n(12)\nThe constants $k_1, b_1, k_2$, and $b_2$ are derived through an adaptive process tailored to the specific dynamics of each model. A set of default values is provided, which can be adjusted based on empirical data to optimize the correction process for different types of models. They are crucial for ensuring \u03b2 and \u03b1 effectively modulate neuron activations while maintaining system stability and performance.\nAfter the adjustment of the MLP activation values is completed, we input the corrected activation values back into the subsequent layers, allowing the model to continue the forward propagation until the final fixed result is output. This process corrects on each token in the detected glitch token set G."}, {"title": "4.2.3 Fixing Process Algorithm", "content": "Based on Section 4.2.2, we present the pseudocode for the fix algorithm of GLITCHPROBER in Algorithm 2. The algorithm begins by sampling a subset (N') of normal tokens from the full set of normal tokens (N) (line 1). Then it computes the statistical distribution of activation values across key neurons in the subset, distinguishing $Neun\u2191$ and $Neun\u2193$ (line 2). After that the algorithm calculates the adjustment factors \u03b2 and \u03b1 for the identified key neurons (line 3-4).\nFor each glitch token in the set G, the algorithm iteratively applies these adjustments across specified layers (i.e., KeyLayers) of the model (lines 5-7). Neurons in $Neun\u2191$ have their activation values increased by \u03b2, amplifying their response to mimic normal activation patterns (lines 8-10). Conversely, neurons in $Neun\u2193$ have their activation values reduced by dividing by \u03b1, suppressing any abnormal activations (lines 11-13). Each adjusted activation is reintegrated into the model's processing flow, allowing it to continue with forward propagation with the corrected values (line 14)."}, {"title": "4.3 Key Layers Selection", "content": "In the design of our GLITCHPROBER'S detection and fix algorithms, we focused on exploiting the attention pattern and MLP status features within certain key layers. The rationale behind selecting these key layers stems primarily from our empirical findings outlined in Finding 2 (Section 3.2), which highlighted that glitch tokens predominantly affect the model's downstream layers closer to the output. For instance, in Llama2, this pertains to layers 19 to 31. Further refining our selection, we encountered a counter-intuitive discovery: modifying features in layers exceedingly close to the output paradoxically diminished the effectiveness of our fix algorithms.\nThe layers preceding the final output are crucial for tailoring responses based on preceding computations; alterations in these layers can disrupt representational balances, leading to degraded performance. In our approach for Llama2, we designated layers 19 to 28 as key layers, optimally positioned in the middle to lower sections of the model's architecture. This strategic placement ensures that our interventions effectively mitigate the effects of glitch tokens while preserving the model's robustness."}, {"title": "5 EVALUATION OF GLITCH TOKEN DETECTION", "content": "5.1 Experiment setup\nExperiment Environment. All experiments are performed on a workstation with Ubuntu 22.04.3 LTS and 250GB memory, and 2 A100 GPU with 80GB memory each.\nLLM Selection. We thoroughly evaluated our proposed method using a diverse set of computational models. We selected five widely recognized, open-source models, including Llama-2-7b-chat [35], Mistral-7B-Instruct-v0.1 [17], Qwen-7B-Chat [5], Gemma-2b-it [14], and Yi-6B-Chat [3]. These models served as the subjects for our in-depth analysis, allowing us to assess the versatility and effectiveness of our method across various real-world applications. Table 1 provides an overview of these models' parameters.\nEvaluation of Detection Baselines. To evaluate the performance of GLITCHPROBER, we compared it with two implemented benchmark schemes and a recent testing method, GLITCHHUNTER.\n(1) Exhaustive Search: Each token in the token list is individually fed into the model, which performs tasks such as paraphrasing, spelling, and length calculation for each token.\n(2) Rule-based Random Sampling: First, randomly select half of the tokens from the language model to form a candidate set. Since common English words typically do not become glitch tokens, use the Natural Language Toolkit (NLTK) to remove high-frequency English words from the candidate set. The remaining tokens are considered potential glitch tokens.\n(3) GLITCHHUNTER: This is the state-of-the-art automated detection method for glitch tokens [20].\nEvaluation Metrics of Detection. For efficiency evaluation, we consider the Time Cost required to process all glitch tokens in the complete token list of a model. For GLITCHPROBER, it encompasses the total duration including feature extraction, classifier training, identification and validation. For effectiveness evaluation, we consider True Positive, Precision, Recall and F1-Score.\nEvaluation Settings. In our detection experiments, we evaluated the performance of GLITCHPROBER with SVM regularization parameter and degree[32, 36] set to C = 1, degree = 3. We employ \u03b3 = 0.1 for random sampling and principal components to P = 75 for PCA. For the rule-based random sampling methods, we conducted 100 independent experiments and averaged the results to obtain statistically significant conclusions. For the GLITCHHUNTER method, we used the default settings from the original paper [20]."}, {"title": "5.2 RQ3 (Efficient Detection): How efficient is our approach in identifying glitch tokens across different LLMs?", "content": "To evaluate the efficiency of GlitchProber, time overhead and the accuracy comparison results of various methods on five large models are shown in Table 2 and Table 3.\nOur experimental results demonstrate that GLITCHPROBER reached a detection efficiency advantage. Meanwhile, GLITCHPROBER achieves a 100% precision which matches the performance of both GLITCHHUNTER and the exhaustive search benchmark method. This signifies GLITCHPROBER'S minimal false positive rate. Furthermore, GLITCHPROBER achieves a recall rate of 64.47%, surpassing GLITCHHUNTER'S 26.52%. The F1-score indicates that GLITCHPROBER strikes a fine balance between precision and recall, efficiently detecting glitch tokens while maintaining high accuracy."}, {"title": "5.3 RQ4 (Ablation Study): How do the different components of GLITCHPROBER affect the detection results?", "content": "To assess the importance of the components in GLITCHPROBER, we performed an ablation study across five models. We developed two variants: GLITCHPROBER-NO-PCA and GLITCHPROBER-NO-POST. GLITCHPROBER-NO-PCA omits the PCA during feature processing, while GLITCHPROBER-NO-POST eliminates the final token validation steps in the original GLITCHPROBER. The comprehensive results are shown in Table 4."}, {"title": "6 EVALUATION OF GLITCH TOKEN FIX", "content": "6.1 Experiment setup\nEvaluation Baselines of Fix. Due to the lack of existing methods for fixing glitch tokens", "scheme": "a rule-based fix method", "methods": "the number of repaired glitch tokens (Repaired Tokens) and the repair rate (Repair Rate). The repair rate represents the proportion of glitch tokens successfully repaired out of"}]}