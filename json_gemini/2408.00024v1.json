{"title": "DECEPTIVE AI SYSTEMS THAT GIVE EXPLANATIONS ARE MORE\nCONVINCING THAN HONEST AI SYSTEMS AND CAN AMPLIFY\nBELIEF IN MISINFORMATION", "authors": ["Valdemar Danry", "Matthew Groh", "Ziv Epstein", "Pat Pataranutaporn", "Pattie Maes"], "abstract": "Advanced Artificial Intelligence (AI) systems, specifically large language models (LLMs), have the\ncapability to generate not just misinformation, but also deceptive explanations that can justify and\npropagate false information and erode trust in the truth. We examined the impact of deceptive AI\ngenerated explanations on individuals' beliefs in a pre-registered online experiment with 23,840\nobservations from 1,192 participants. We found that in addition to being more persuasive than\naccurate and honest explanations, AI-generated deceptive explanations can significantly amplify\nbelief in false news headlines and undermine true ones as compared to AI systems that simply\nclassify the headline incorrectly as being true/false. Moreover, our results show that personal factors\nsuch as cognitive reflection and trust in AI do not necessarily protect individuals from these effects\ncaused by deceptive AI generated explanations. Instead, our results show that the logical validity\nof AI generated deceptive explanations, that is whether the explanation has a causal effect on the\ntruthfulness of the AI's classification, plays a critical role in countering their persuasiveness \u2013 with\nlogically invalid explanations being deemed less credible. This underscores the importance of\nteaching logical reasoning and critical thinking skills to identify logically invalid arguments, fostering\ngreater resilience against advanced AI-driven misinformation.", "sections": [{"title": "Main", "content": "Artificial Intelligence (AI) systems, such as large language models (LLMs), have the alarming capability to generate not\nonly misinformation but also deceptive explanations that justify misinformation and make it seem logically sound.\nResearchers have identified an increase in AI-generated disinformation campaigns [1, 2] and the factors that make them\ndisruptive to people' ability to discern true and false information [3, 4, 5, 6, 7, 8, 9, 10], as well as change people's\nattitudes [11, 12, 13]. These factors include authoritative tone [14], persuasive language [12, 14, 3], and targeted\npersonalization [15]. However, little is known about the influence on people's beliefs when explanations are used as a\ntactic for misinformation.\nIn the broader context of the AI-generated misinformation landscape, we can identify three levels of sophistication,\neach posing unique challenges. The most basic level involves AI systems generating false news headlines without"}, {"title": "Methods", "content": "Stimuli Curation\nWe created a dataset of headlines each with one honest and one deceptive explanation by prompting the text-generation\nmodel GPT-3 davinci 2 with 12 example explanations randomly sampled from the publicly available fact-checking\ndataset \"liar-plus\" [20]. This dataset consists of 12,836 short statements with explanation sentences extracted automati-cally from the full-text verdict reports written by journalists in Politifact (see Fig. 3).\nFirst, 5 honest and 5 deceptive explanations were generated for 40 true and false headlines by prompting GPT-3\n(davinci, temp = .7) with the headline and making it complete the sentences \u201cThis is FALSE because...\" or \"This is\nTRUE because...\" (see Fig. 4). We further curated the explanations by ranking them by highest semantic similarity\nand lowest repeated-word frequency. We picked the highest ranked explanations, confirmed the veracity and logical\nvalidity of each explanation. The truth veracity was confirmed independently by each of the authors after being\ninstructed by a professional fact-checker following standard fact-checking procedures [29, 30] and then aligned. The\nlogical validity was also confirmed by each author independently by deconstructing the claims of each headline and\nexplanation into premises, conclusions and inferences from which the logical validity could be determined in an almost\nmathematical fashion using a Fisher analysis [31]. Next, we then excluded explanations whose veracity did not match\nthe veracity of the headline. Since the resulting dataset had an unequal distribution of veracity and logical validity, we\nrandomly excluded generated explanations until we had somewhat equal distribution of true and false explanations\nand logically valid and logically invalid explanations for each condition (deceptive vs. honest explanations) ending\nwith a stimulus set consisting of 28 headlines with 1 honest explanation and 1 deceptive explanation each (56 total).\nWe tested for differences across four linguistic dimensions (word count, sentiment, grade level, and subjectivity) and\nfound no statistical differences between conditions. To generate explanations for a Trivia stimulus set, we repeated the\nsame procedure by prompting GPT-3 with 12 example explanations. The example explanations and Trivia statements\nwere randomly sampled from online[32], resulting in a stimulus set consisting of 28 trivia statements with 1 honest\nexplanation and 1 deceptive explanation each (56 total). These were also tested for differences across four linguistic\ndimensions (wordcount, sentiment, grade level, and subjectivity) and were also found to have no statistical differences\nbetween conditions."}, {"title": "Task Description", "content": "Participants were shown 20 statements during the main discernment task which were either true or false. Each participant\nsaw the 20 statements in a random order, and rated the perceived truth of each statement (\u201cDo you think the statement\nin the grey box is true or false?\u201d) on a slider scale with 1 decimal from 1 (\u201cDefinitely False\") to 5 (\u201cDefinitely True\").\nAfter the rating, the participants would receive feedback from an AI system and be asked if they want to revise their\nrating (\"Would you like to revise your estimate: Do you think the statement in the grey box is true or false?\") on a\nslider scale with 1 decimal from 1 (\u201cDefinitely False\u201d) to 5 (\u201cDefinitely True\u201d) with the default value being same as\nthe previous rating. Participants also rated their knowledge on the topic (\"How knowledgeable are you on the topic of\n[topic]", "Not at all knowledgeable\\\") to 5 (\u201cVery much knowledgeable\\\"). The\nselection of statements and generation of AI feedback is further explained in section 1 and the task interface can be seen\nin Fig. 5.\"\n    },\n    {\n      \"title\": \"Randomization\",\n      \"content\": \"For the main discernment task, participants were randomly assigned to one of two conditions: (i) news headline\nstatements or (ii) trivia item statements (between-subjects); and one of two conditions (i) no explanation (\u201cThis is true /\nfalse\"), or (ii) explanation (\u201cThis true / false because...\") (between-subjects). The order of the stimuli being presented\nwas also randomized. See Fig. 1 for examples of items across conditions.\"\n    },\n    {\n      \"title\": \"Post Task Survey\",\n      \"content\": \"After the discernment task, participants were asked to complete post-test surveys to measure their critical thinking, and\nlevel of self-reported trust in the agent providing them with explanations. To measure the level of critical thinking of\nsubjects, we used cognitive reflection test (CRT), a task designed to measure a person's ability to reflect on a question\nand resist reporting the first response that comes to mind [33": ".", "trustworthiness": "Ability,\nBenevolence and Integrity (ABI)."}, {"title": "Participants", "content": "A total of 1,199 individuals participated in the experiment. We used the Prolific platform to recruit individuals from the\nUnited States. We focus our analysis on the 1,192 of 1,209 recruited participants who passed the attention check. 589\nparticipants rating news headlines and 610 participants rating trivia statements. Of the 589 participants rating news\nheadlines, 289 received no explanation and 300 received an explanation. Of the 610 participants rating trivia statements,"}, {"title": "Approvals", "content": "This research complies with all relevant ethical regulations and the Massachusetts Institute of Technology's Committee\non the Use of Humans as Experimental Subjects determined this study to fall under Exempt Category 3 \u2013 Benign\nBehavioral Intervention. This study's exemption identification number is E-3754. All participants are informed that\n\"This is an MIT research project. All data for research is collected anonymously for research purposes. We will ask\nyou about your attitudes towards information and AI systems. For questions, please contact vdanry@mit.edu. If you\nare under 18 years old, you need consent from your parents to continue.\" Participants recruited from Prolific were\ncompensated at a rate of $10.82 an hour. At the end of the experiment participants were made aware that they had\nreceived Al explanations that were sometimes deceptive in the experiment, being told that \"In this study, you were asked\nto collaborate with an AI-system for rating the accuracy of statements. All feedback in this study was AI-generated.\nSome of the feedback from the AI system was simply deceptive\". Future work must explore the limits, ethics and\nconsequences of exposing participants to AI-generated content."}, {"title": "Scoring Perceived Truthfulness and Perceived Logical Validity", "content": "To investigate the effects of semantic features of explanations, we used GPT-4 to split each explanation into individual\npropositions / premises and score the perceived truthfulness and logical validity of each premise. For instance, for\nthe headline \"Newsmax plans expansion to capitalize on Trump support, anger at Fox News\", the explanation \u201cThis\nis TRUE. Newsmax has been a vocal supporter of Trump, and the network has even hired Trump's former campaign\nmanager.\" was split by GPT-4 into the two premises: \u201cNewsmax has been a vocal supporter of donald trump.\" and \"the\nnetwork newsmax has hired donald trump's former campaign manager.\". To conduct the splitting GPT-4 was given the\nfollowing prompt: \"Split the following explanation into its containing claims. Fill in pronouns and references so that\neach claim can be verified by itself without any context. Separate them with a new line. Only give the answer and be as\nconcise as possible. Example: example. Explanation:explanation\".\nEach premise of each explanation was then scored by GPT-4 for its perceived truthfulness using the following prompt:\n\"How likely is it that someone perceives this as true on a scale from 0.00-1.00 with decimals. 0.00 being extremely\nunlikely and 1.00 being extremely likely. Only give me the answer. Even if this is highly subjective how do you think\nsome person might think this to be likely: premise\". Next, each premise was then scored by GPT-4 for its perceived\nlogical validity with the deceptive classification of the headline using the following prompt: \u201cHow likely is it that the\ngeneral public would believe this: 'premise' to support this: 'This is classification: headline'. Output a score from\n0.00-1.00 with decimals. 0.00 being extremely unlikely and 1.00 being extremely likely. Only give me the answer:\".\nAn average score for both truthfulness and logical validity was calculated for each explanation."}, {"title": "Calculating Syntactic Features", "content": "To calculate word count and reading ease, we used the Natural Language Toolkit (NLTK) in Python. The word count\nwas simply the total number of words in each explanation. Reading ease was estimated using the Flesch Reading Ease\nformula, which assesses text on a 100-point scale; the higher the score, the easier the text is to understand.\nTo calculate the grammatical correctiveness of the explanation, we used the LanguageTool Python library, which\nreturned the count of grammatical errors it could detect in the text."}, {"title": "Analysis", "content": "In order to gain insights into whether explanations lead to more accurate beliefs, we first compare the belief ratings of\nuser with AI-generated explanations (honest and deceptive) and no feedback. Accuracy was coded by subtracting the\nbelief ratings from the ground truth per rating per participants.\nTo investigate the relationship between statement ground truth, the presence of explanations, and deceptive classifications\n(X1, X2, and X3) and belief and accuracy distribution (Y1, and Y2), along with their potential interactions and moderator\nvariables, we employed an ordinary least squares (OLS) regression models.\nWe analyzed a total of 23,980 observations of belief ratings (12,200 trivia statement observations and 11,780 news\nheadline observations) of true and false statements, collected through our experiment. The response variables, Y1\nand Y2, represent the participant belief and accuracy distributions, while the predictor variables are as follows: X1 -\nstatement ground truth, X2 - presence of AI explanation, and X3 - deceptive AI feedback. The moderator variables"}, {"title": "Results", "content": "In order to study the effects of deceptive AI explanations on human beliefs, we conducted a pre-registered online\nexperiment with 23,840 observations from 1,192 participants rating their beliefs in true and false news headlines before\nand after receiving AI generated explanations. We designed our experiment to disentangle the influence of deceptive AI\nexplanations from merely receiving a deceptive (inaccurate) classification of a news headline as true or false without\nexplanation. This was done by randomizing participants to receiving true and false news headlines accompanied by\neither deceptive classifications or deceptive classifications with deceptive AI explanations (between-subjects). Moreover,\nwe examine the impact of logical validity of the explanation, and how personal factors and the effects of syntactic and\nsemantic features of the deceptive explanations may influence the outcomes.\nFor each true and false news headline, an LLM model (GPT-3) was used to generate an honest and a deceptive\nexplanation by prompting the model to complete the following statements: \"This is false because...\" or \"This is true\nbecause...\". For deceptive explanations, the model generated inaccurate explanations stating why true statements were\nfalse and why false statements were true. This was done for both news headlines and trivia statements. Examples of the\ngenerated classifications with explanations can be found in Appendix 3."}, {"title": "Deceptive AI-generated Classifications with Explanations are more Persuasive than Honest AI-generated\nExplanations", "content": "To understand the persuasiveness of deceptive AI-generated explanations, we compared the relative persuasiveness of\ndeceptive and honest explanations on belief change (i.e. the absolute difference in rating from before and after seeing\nan AI classification with explanation per item per participant). We find that deceptive AI-generated explanations are\nsignificantly more persuasive than honest explanations on both true and false news headlines (\u03b2 = 0.40, p < 0.0001\nand \u03b2 = 0.29, p = 0.003, respectively, not pre-registered). Prior research has shown that fake news is shared more\noften that true news due to factors such as novelty and emotional content (fear, disgust, and surprise) [37]. It is likely\nsimilar factors are at play for deceptive explanations, potentially explaining why they were found to be more persuasive\nthan honest explanations. The full regression table can be found in Table 4 in the Appendix."}, {"title": "Explanations Can Amplify Beliefs in False Information", "content": "To ensure our results were caused by explanations and not simply labeling information as true or false, we compared\nthe influence of AI feedback with and without explanations (deceptive AI-generated explanations and deceptive AI\ngenerated classifications, respectively).\nOur results show that deceptive AI generated classifications without explanation, significantly increase belief in false\nnews headlines (\u03b2 = 0.71, p < 0.0001) and decreases belief in true news headlines (\u03b2 = \u22121.72, p < 0.0001). In\nextension, when accompanied by deceptive AI-generated explanations, beliefs in false news headlines were further\nsignificantly increased (\u03b2 = 0.32, p = 0.009) and beliefs in true news headlines where further significantly decreased\nas compared to just deceptive classifications (\u03b2 = \u22120.72, p = 0.0001). These results suggests that the effects of\ndeceptive AI systems amplifies beliefs in information beyond classifications without explanations (Fig. 6). The full\nresults can be found in Table 3 in the Appendix."}, {"title": "Personal factors moderate the influence of deceptive AI explanations", "content": "In previous studies, cognitive reflection as measured by the Cognitive Reflection Test [33] has been found to associate\nwith people's ability to correctly identify misinformation [38]. However, when receiving AI feedback on the truth of news\nheadlines, our results revealed no significant interactions with cognitive reflection level and deceptive classifications both\nwith and without explanations for false news headlines (\u03b2 = \u22120.09, p = 0.20 and \u03b2 = 0.13, p = 0.15, respectively)\nand for true news headlines (\u03b2 = 0.16, p = 0.20 and \u03b2 = 0.25, p = 0.15, respectively). This suggests that introducing\nan evaluative AI system framed as a fact-checking system could override the effects of cognitive reflection on truth\ndiscernment of news headlines. This may be attributed to several factors. First, the presence of information labeled as\nprovided by \"AI\" might induce a reliance effect, where individuals defer judgment to the technology [39], potentially\nundermining their reflective capacities [38]. This effect could be accentuated by the perceived authority or credibility\nof language based AI systems, which might attenuate the influence of cognitive reflection [40]. Additionally, the\ncomplexity or novelty of information such as new facts they have no knowledge of revealed in AI explanation could\nconfuse or overwhelm users, reducing the effectiveness of their cognitive reflection in evaluating the information [41].\nLastly, it is also possible that individuals with high cognitive reflection are already near their maximum ability to"}, {"title": "Deceptive AI-generated explanations that are logically invalid decrease people's beliefs in false news headlines", "content": "Researchers have suggested that people's ability to identify logical flaws (or logical fallacies) could play an essential\nrole in refuting misinformation [45, 46, 21, 47]. In order to investigate the influence of logical validity of deceptive\nAI-generated explanations, we modeled the influence of logical validity of explanations on participants' belief rating\nusing a linear regression, where logical validity is when the truth of the AI generated explanation necessarily implies\nthe truth of the AI generated classification (logically valid) in comparison to where the truth of the classification is\nindependent from the truth of the explanation (See Section 1). Limiting the data to only ratings where explanations were\npresent, our results show that while logically invalid deceptive AI-generated explanations did not have any significant\neffects on participants' belief rating of true news headlines (\u03b2 = \u22120.31, p = 0.13), logically invalid deceptive AI-\ngenerated explanations did significantly increase beliefs in false news headlines (\u03b2 = \u22120.35, p = 0.02). This suggests\nthat participants are more likely to reject deceptive explanations for false news headlines when the explanations are\nlogically invalid. This demonstrates the potential for logical analysis and critical thinking skills to mitigate the influence\nof deceptive AI-generated explanations for false information. An overview of the results can be found in Table 5 and\nFig. 6."}, {"title": "Limitations and Potential Generalization to Future AI Systems", "content": "As exemplified in this study, personal factors mediate the influence of explanations on humans beliefs. Extending upon\nthese findings, research has shown that prior beliefs about AI systems can significantly influence how people integrate or\nreject AI-generated information [48, 49]. Taking this into account, the impact of deceptive AI explanations might vary\nsignificantly across different cultural and contextual settings. Factors such as political climate, prevalent media literacy,\nand language around AI can influence how deceptive explanations are received and believed. For instance, research has\nshown that the choice of terminology significantly influences people's perceptions and reactions towards AI-generated\ncontent, with different terms leading to varying degrees of accuracy in identification and emotional response across\ndifferent cultural contexts [49]. Moreover, while research has shown LLMs to be significantly more authoritative,\npersuasive, seemingly logically valid and even preferred over human-authored content, it is unclear to which extent the\neffects of deceptive explanations identified in this paper would transfer to human-authored deceptive explanations."}, {"title": "Conclusion", "content": "Our findings underscore the significant impact that deceptive AI-generated explanations can have on shaping public\nopinion and influencing individual beliefs. The ability of these explanations to not only present misinformation but also\nprovide seemingly logical justifications makes them particularly potent tools for misinformation campaigns. This is\nespecially concerning in the context of political discourse, scientific communication, and social media, where the rapid\ndissemination and acceptance of false information can have real-world consequences.\nThe persuasive power of deceptive explanations, as demonstrated in our study, highlights a critical vulnerability in the\npublic's ability to discern truth from falsehood when interacting with AI-generated content. This is compounded by\nthe finding that even individuals who consider themselves knowledgeable are not immune to the influence of these\ndeceptive explanations. In fact, our results suggest that self-assessed knowledge may even increase susceptibility to\nbelieving false information when it is accompanied by a deceptive explanation. This could be due to a combination of\noverconfidence and the sophisticated nature of the explanations that make the misinformation seem credible.\nMoreover, the role of logical validity in the effectiveness of deceptive explanations is particularly noteworthy. Our\nstudy found that logically invalid explanations were less effective in persuading individuals to believe false headlines,\nsuggesting that enhancing critical thinking and logical reasoning skills could be a viable strategy to combat the influence\nof misinformation. This aligns with previous research emphasizing the importance of education in logical fallacies and\ncritical thinking as tools for empowering individuals to better evaluate the information they encounter, particularly in\ndigital environments where AI-generated content is prevalent.\nWhile AI has the potential to bring about significant benefits, its capability to generate persuasive, deceptive explanations\nposes a serious risk to informational integrity and public trust. Our study highlights the urgent need for comprehensive\nstrategies that address the dual aspects of enhancing public resilience against misinformation and ensuring responsible\nAI development and deployment."}, {"title": "Supplementary information", "content": "All data, including pre-registration, datasets, explanation prompts, and code generated and analyzed dur-\ning the current study is available on GitHub (https://github.com/mitmedialab/deceptive-AI), Zen-\nodo (https://zenodo.org/records/8172056) and Research Box (https://researchbox.org/1801&PEER_\nREVIEW_passcode=BDHVUP)."}, {"title": "Extended Data", "content": "Additional Results\nDeceptive AI-generated Explanations Increase Beliefs in False Headlines and Decrease Beliefs in True Headlines\nIn order to gain insights into whether explanations lead to more accurate beliefs, we first compare how accurately\nparticipants rated true and false news headlines before (no feedback) and after getting deceptive AI-generated explana-tions. We coded accuracy by subtracting the belief ratings from the ground truth per rating per participant. Running\nan analysis of variance (ANOVA), we found that deceptive AI-generated explanations lead to a significantly lower\naccuracy than no feedback (14 percentage point difference, F(2, 10959) = -36, p < 0.0001, ANOVA Welch).\nBreaking these results down into beliefs in true and false news headlines, we find that deceptive Al explanations\nsignificantly increase beliefs in false headlines and significantly decrease beliefs in true headlines (\u03b2 = -2.44,\np < 0.001 and \u03b2 = 1.03, p < 0.001, respectively), suggesting that deceptive AI explanations significantly diminish\npeople's ability to tell true news headlines from false news headlines.\nSemantic and syntactic features moderate the influence of deceptive explanations\nBeyond personal factors like cognitive reflection level, trust in AI, and prior knowledge, the manner in which information\nis structured and presented can greatly affect the way it is perceived and processed by the reader [51, 52]. Notably, the\ninfluence of semantic and syntactic features on the effectiveness of persuasive communication has been acknowledged;\naspects such as readability [53], the amount of words used, the perceived truthfulness, and the perceived logical support\nof explanations for news headlines have the potential to shape a reader's beliefs and attitudes [54]. To understand\nwhat makes a deceptive explanation influence people to change their belief about a news headline, we conducted a\npost-hoc analysis of the semantic and syntactic structures of explanations such as their perceived truthfulness, perceived\nlogical validity, the number of facts that was stated in the explanation, the explanation's word count, and how easy the\nexplanation was to read.\nOur results revealed significant correlations between semantic structures of deceptive explanations and changes in\nbelief about true and false news headlines. In particular, our results show that when an Al system gives deceptive\nexplanations that are likely to be perceived as logically supporting a false news headline being true, people are more\nlikely to update their initial belief and believe that the false news headline is true (\u03b2 = 1.26, p < 0.001). Conversely,\nwhen the deceptive explanation are likely to be perceived as logically supporting that true news headline is false, people\nare more likely to believe that the true headline is false (\u03b2 = -1.36, p < 0.01) than when it is not likely to be perceived\nas logically supporting it being false. We did not find any significant correlations between the perceived truthfulness of\ndeceptive explanations and beliefs in true and false news headlines in our model (\u03b2 = 0.81, p < 0.30 and \u03b2 = \u22120.79,\np < 0.10, respectively). The complete linear model with the semantic results can be found in Table 10 in the Appendix.\nFor syntactic structures, our results show that when an AI system gives deceptive explanations on true headlines, stating\nthat they are false, the longer the explanation is (i.e. its word count), the more likely people are to believe the true news"}, {"title": "Deceptive AI-generated explanations that are logically invalid decrease people's beliefs in false news headlines", "content": "Researchers have suggested that people's ability to identify logical flaws (or logical fallacies) could play an essential\nrole in refuting misinformation [45, 46, 21, 47]. In order to investigate the influence of logical validity of deceptive\nAI-generated explanations, we modeled the influence of logical validity of explanations on participants' belief rating\nusing a linear regression, where logical validity is when the truth of the AI generated explanation necessarily implies\nthe truth of the AI generated classification (logically valid) in comparison to where the truth of the classification is\nindependent from the truth of the explanation (See Section 1). Limiting the data to only ratings where explanations were\npresent, our results show that while logically invalid deceptive AI-generated explanations did not have any significant\neffects on participants' belief rating of true news headlines (\u03b2 = \u22120.31, p = 0.13), logically invalid deceptive AI-\ngenerated explanations did significantly increase beliefs in false news headlines (\u03b2 = \u22120.35, p = 0.02). This suggests\nthat participants are more likely to reject deceptive explanations for false news headlines when the explanations are\nlogically invalid. This demonstrates the potential for logical analysis and critical thinking skills to mitigate the influence\nof deceptive AI-generated explanations for false information. An overview of the results can be found in Table 5 and\nFig. 6."}, {"title": "Limitations and Potential Generalization to Future AI Systems", "content": "As exemplified in this study, personal factors mediate the influence of explanations on humans beliefs. Extending upon\nthese findings, research has shown that prior beliefs about AI systems can significantly influence how people integrate or\nreject AI-generated information [48, 49]. Taking this into account, the impact of deceptive AI explanations might vary\nsignificantly across different cultural and contextual settings. Factors such as political climate, prevalent media literacy,\nand language around AI can influence how deceptive explanations are received and believed. For instance, research has\nshown that the choice of terminology significantly influences people's perceptions and reactions towards AI-generated\ncontent, with different terms leading to varying degrees of accuracy in identification and emotional response across\ndifferent cultural contexts [49]. Moreover, while research has shown LLMs to be significantly more authoritative,\npersuasive, seemingly logically valid and even preferred over human-authored content, it is unclear to which extent the\neffects of deceptive explanations identified in this paper would transfer to human-authored deceptive explanations."}]}