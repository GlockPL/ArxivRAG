{"title": "DECEPTIVE AI SYSTEMS THAT GIVE EXPLANATIONS ARE MORE CONVINCING THAN HONEST AI SYSTEMS AND CAN AMPLIFY BELIEF IN MISINFORMATION", "authors": ["Valdemar Danry", "Pat Pataranutaporn", "Matthew Groh", "Ziv Epstein", "Pattie Maes"], "abstract": "Advanced Artificial Intelligence (AI) systems, specifically large language models (LLMs), have the capability to generate not just misinformation, but also deceptive explanations that can justify and propagate false information and erode trust in the truth. We examined the impact of deceptive AI generated explanations on individuals' beliefs in a pre-registered online experiment with 23,840 observations from 1,192 participants. We found that in addition to being more persuasive than accurate and honest explanations, AI-generated deceptive explanations can significantly amplify belief in false news headlines and undermine true ones as compared to AI systems that simply classify the headline incorrectly as being true/false. Moreover, our results show that personal factors such as cognitive reflection and trust in AI do not necessarily protect individuals from these effects caused by deceptive AI generated explanations. Instead, our results show that the logical validity of AI generated deceptive explanations, that is whether the explanation has a causal effect on the truthfulness of the AI's classification, plays a critical role in countering their persuasiveness \u2013 with logically invalid explanations being deemed less credible. This underscores the importance of teaching logical reasoning and critical thinking skills to identify logically invalid arguments, fostering greater resilience against advanced AI-driven misinformation.", "sections": [{"title": "1 Main", "content": "Artificial Intelligence (AI) systems, such as large language models (LLMs), have the alarming capability to generate not only misinformation but also deceptive explanations that justify misinformation and make it seem logically sound.\nResearchers have identified an increase in AI-generated disinformation campaigns [1, 2] and the factors that make them disruptive to people' ability to discern true and false information [3, 4, 5, 6, 7, 8, 9, 10], as well as change people's attitudes [11, 12, 13]. These factors include authoritative tone [14], persuasive language [12, 14, 3], and targeted personalization [15]. However, little is known about the influence on people's beliefs when explanations are used as a tactic for misinformation.\nIn the broader context of the AI-generated misinformation landscape, we can identify three levels of sophistication, each posing unique challenges. The most basic level involves AI systems generating false news headlines without any accompanying justification. While such headlines can still mislead, they are more easily dismissed as baseless claims. The next level involves AI systems providing deceptive classifications, labeling false information as true or vice versa. This added layer of perceived authority can lend unwarranted credibility to the misinformation. However, the perhaps most subtle and deeper level is when AI systems generate deceptive explanations to justify and propagate false information.\nOne of the reasons why the topic of AI generated explanations and misinformation remains unexplored is that the use of explanations as a tactic for misinformation goes against the commonly held beliefs that explanations always make AI systems more transparent, trustworthy [16, 17], and fair [18, 19]. While researchers have shown that honest explanations can assist people in determining the veracity of information [20, 21] and improve their decision-making outcomes [22], as well as reduce human overreliance on AI systems [23], research in psychology has demonstrated that even poor explanations can significantly impact people's actions and beliefs [24, 25, 26]. This implies that the mere presence of an explanation can lead to changes in beliefs and behavior, regardless of its quality or veracity. Researchers have also shown that people often do not cognitively engage with the content of the explanation unless they are forced to do so[27].\nDeceptive explanations can be used to exploit this vulnerability by connecting correct facts in misleading ways to justify false information or discredit real information and potentially making it more difficult to discern the truth. Disguised as political commentary, scientific explanations, or online discussions, AI-generated deceptive explanations from systems like LLMs could be weaponized at scale by bad actors to manipulate public opinion and decision making, even with safety measures built into the models [28].\nThe low cost and easy scalability of AI-generated explanations further compound the problem. With the ability to generate vast amounts of high quality content quickly and cheaply, malicious actors can flood online platforms with deceptive explanations, discrediting or drowning out legitimate explanations by human experts that take time to craft, making it harder for individuals to make sense of reliable information.\nThis paper investigates the effects of AI-generated deceptive explanations on human beliefs. Through a comprehensive pre-registered online experiment with 23,840 observations from 1,192 participants, we find that not only are deceptive AI generated explanations more persuasive than deceptive AI classifications without explanations, they are even more persuasive than honest explanations (See Fig. 2). While personal factors such as cognitive reflection and trust in AI do not necessarily protect individuals from the effects of deceptive AI generated explanations, the logical validity, that is whether the truthfulness of the conclusions or classifications made by the AI system follows from the truth of the explanation, of AI generated deceptive explanations where found to counter their persuasiveness \u2013 with logically invalid explanations being deemed less credible. This research underscores the urgent need for vigilance and proactive measures to ensure the responsible use of AI technology."}, {"title": "Methods", "content": "Stimuli Curation\nWe created a dataset of headlines each with one honest and one deceptive explanation by prompting the text-generation model GPT-3 davinci 2 with 12 example explanations randomly sampled from the publicly available fact-checking dataset \"liar-plus\" [20]. This dataset consists of 12,836 short statements with explanation sentences extracted automati- cally from the full-text verdict reports written by journalists in Politifact (see Fig. 3).\nFirst, 5 honest and 5 deceptive explanations were generated for 40 true and false headlines by prompting GPT-3 (davinci, temp = .7) with the headline and making it complete the sentences \u201cThis is FALSE because...\" or \"This is TRUE because...\" (see Fig. 4). We further curated the explanations by ranking them by highest semantic similarity and lowest repeated-word frequency. We picked the highest ranked explanations, confirmed the veracity and logical validity of each explanation. The truth veracity was confirmed independently by each of the authors after being instructed by a professional fact-checker following standard fact-checking procedures [29, 30] and then aligned. The logical validity was also confirmed by each author independently by deconstructing the claims of each headline and explanation into premises, conclusions and inferences from which the logical validity could be determined in an almost mathematical fashion using a Fisher analysis [31]. Next, we then excluded explanations whose veracity did not match the veracity of the headline. Since the resulting dataset had an unequal distribution of veracity and logical validity, we randomly excluded generated explanations until we had somewhat equal distribution of true and false explanations and logically valid and logically invalid explanations for each condition (deceptive vs. honest explanations) ending with a stimulus set consisting of 28 headlines with 1 honest explanation and 1 deceptive explanation each (56 total). We tested for differences across four linguistic dimensions (word count, sentiment, grade level, and subjectivity) and found no statistical differences between conditions. To generate explanations for a Trivia stimulus set, we repeated the same procedure by prompting GPT-3 with 12 example explanations. The example explanations and Trivia statements were randomly sampled from online[32], resulting in a stimulus set consisting of 28 trivia statements with 1 honest explanation and 1 deceptive explanation each (56 total). These were also tested for differences across four linguistic dimensions (wordcount, sentiment, grade level, and subjectivity) and were also found to have no statistical differences between conditions."}, {"title": "Task Description", "content": "Participants were shown 20 statements during the main discernment task which were either true or false. Each participant saw the 20 statements in a random order, and rated the perceived truth of each statement (\u201cDo you think the statement in the grey box is true or false?\u201d) on a slider scale with 1 decimal from 1 (\u201cDefinitely False\") to 5 (\u201cDefinitely True", "Would you like to revise your estimate: Do you think the statement in the grey box is true or false?\") on a slider scale with 1 decimal from 1 (\u201cDefinitely False\u201d) to 5 (\u201cDefinitely True\u201d) with the default value being same as the previous rating. Participants also rated their knowledge on the topic (\"How knowledgeable are you on the topic of [topic]": "on a slider scale with 1 decimal from 1 (\u201cNot at all knowledgeable", "Very much knowledgeable\"). The selection of statements and generation of AI feedback is further explained in section 1 and the task interface can be seen in Fig. 5.\"\n    },\n    {\n      \"title\": \"Randomization\",\n      \"content\": \"For the main discernment task, participants were randomly assigned to one of two conditions: (i) news headline statements or (ii) trivia item statements (between-subjects); and one of two conditions (i) no explanation (\u201cThis is true / false\"), or (ii) explanation (\u201cThis true / false because...\") (between-subjects). The order of the stimuli being presented was also randomized. See Fig. 1 for examples of items across conditions.\"\n    },\n    {\n      \"title\": \"Post Task Survey\",\n      \"content\": \"After the discernment task, participants were asked to complete post-test surveys to measure their critical thinking, and level of self-reported trust in the agent providing them with explanations. To measure the level of critical thinking of subjects, we used cognitive reflection test (CRT), a task designed to measure a person's ability to reflect on a question and resist reporting the first response that comes to mind [33": ".", "trustworthiness": "Ability, Benevolence and Integrity (ABI)."}, {"title": "Participants", "content": "A total of 1,199 individuals participated in the experiment. We used the Prolific platform to recruit individuals from the United States. We focus our analysis on the 1,192 of 1,209 recruited participants who passed the attention check. 589 participants rating news headlines and 610 participants rating trivia statements. Of the 589 participants rating news headlines, 289 received no explanation and 300 received an explanation. Of the 610 participants rating trivia statements, 299 got no explanation, and 311 got explanations. Each participant rated 20 statements, with an average of 51% of statements being true and 50% of explanations being deceptive."}, {"title": "Approvals", "content": "This research complies with all relevant ethical regulations and the Massachusetts Institute of Technology's Committee on the Use of Humans as Experimental Subjects determined this study to fall under Exempt Category 3 \u2013 Benign Behavioral Intervention. This study's exemption identification number is E-3754. All participants are informed that \"This is an MIT research project. All data for research is collected anonymously for research purposes. We will ask you about your attitudes towards information and AI systems. For questions, please contact vdanry@mit.edu. If you are under 18 years old, you need consent from your parents to continue.\" Participants recruited from Prolific were compensated at a rate of $10.82 an hour. At the end of the experiment participants were made aware that they had received Al explanations that were sometimes deceptive in the experiment, being told that \"In this study, you were asked to collaborate with an AI-system for rating the accuracy of statements. All feedback in this study was AI-generated. Some of the feedback from the AI system was simply deceptive\". Future work must explore the limits, ethics and consequences of exposing participants to AI-generated content."}, {"title": "Scoring Perceived Truthfulness and Perceived Logical Validity", "content": "To investigate the effects of semantic features of explanations, we used GPT-4 to split each explanation into individual propositions / premises and score the perceived truthfulness and logical validity of each premise. For instance, for the headline \"Newsmax plans expansion to capitalize on Trump support, anger at Fox News\", the explanation \u201cThis is TRUE. Newsmax has been a vocal supporter of Trump, and the network has even hired Trump's former campaign manager.\" was split by GPT-4 into the two premises: \u201cNewsmax has been a vocal supporter of donald trump.\" and \"the network newsmax has hired donald trump's former campaign manager.\". To conduct the splitting GPT-4 was given the following prompt: \"Split the following explanation into its containing claims. Fill in pronouns and references so that each claim can be verified by itself without any context. Separate them with a new line. Only give the answer and be as concise as possible. Example: example. Explanation:explanation\".\nEach premise of each explanation was then scored by GPT-4 for its perceived truthfulness using the following prompt: \"How likely is it that someone perceives this as true on a scale from 0.00-1.00 with decimals. 0.00 being extremely unlikely and 1.00 being extremely likely. Only give me the answer. Even if this is highly subjective how do you think some person might think this to be likely: premise\". Next, each premise was then scored by GPT-4 for its perceived logical validity with the deceptive classification of the headline using the following prompt: \u201cHow likely is it that the general public would believe this: 'premise' to support this: 'This is classification: headline'. Output a score from 0.00-1.00 with decimals. 0.00 being extremely unlikely and 1.00 being extremely likely. Only give me the answer:\". An average score for both truthfulness and logical validity was calculated for each explanation."}, {"title": "Calculating Syntactic Features", "content": "To calculate word count and reading ease, we used the Natural Language Toolkit (NLTK) in Python. The word count was simply the total number of words in each explanation. Reading ease was estimated using the Flesch Reading Ease formula, which assesses text on a 100-point scale; the higher the score, the easier the text is to understand.\nTo calculate the grammatical correctiveness of the explanation, we used the LanguageTool Python library, which returned the count of grammatical errors it could detect in the text."}, {"title": "Analysis", "content": "In order to gain insights into whether explanations lead to more accurate beliefs, we first compare the belief ratings of user with AI-generated explanations (honest and deceptive) and no feedback. Accuracy was coded by subtracting the belief ratings from the ground truth per rating per participants.\nTo investigate the relationship between statement ground truth, the presence of explanations, and deceptive classifications (X1, X2, and X3) and belief and accuracy distribution (Y1, and Y2), along with their potential interactions and moderator variables, we employed an ordinary least squares (OLS) regression models.\nWe analyzed a total of 23,980 observations of belief ratings (12,200 trivia statement observations and 11,780 news headline observations) of true and false statements, collected through our experiment. The response variables, Y1 and Y2, represent the participant belief and accuracy distributions, while the predictor variables are as follows: X1 - statement ground truth, X2 - presence of AI explanation, and X3 - deceptive AI feedback. The moderator variables include the following z-scored variables: logical validity, self-reported prior knowledge, cognitive reflection test score [33], and trust in AI systems[35, 36].\nWe chose an Ordinary Least Squares (OLS) regression model for its analytical rigor in assessing linear relationships and controlling for confounding factors and mediators, making it ideal to intricately dissect the effects of AI explanations on belief accuracy and to unravel the complex dynamics between statement truth, explanation presence, and deception effectively. We assume that errors are independent and normally distributed with constant variance. Interaction terms between the predictors (X1:X2, X1:X3, and X2:X3) were included in the model to examine any joint effects of the veracity and explainability factors on belief distribution (Y1), accuracy distribution (Y2), and discernment distribution. Belief refers to the participants' subjective judgments about the truthfulness of the statements, while accuracy represents the correctness of these judgments (i.e., whether participants correctly identified true and false statements). We pre-registered our analysis at https://aspredicted.org/YLK_S3F.\nOur moderation analysis for CRT, need for cognition, trust, and prior knowledge was conducting by, for each mod- erating variable, re-running the main analysis model with the addition of the z-scored moderator and all interactions. Additionally, our moderation analysis also examined the 4-way interaction between veracity, explanation veracity, explanation type and the moderator.\nWe also conducting a moderation analysis for logical validity by re-running the main analysis model restricting to the classification + explanation condition, with the addition of the z-scored moderator and all interactions (limiting to only AI classifications with explanation) and examining the 3-way interactions between veracity, explanation veracity, explanation type and the moderator.\nWe then conducted a moderation analysis for number of premises, perceived truthfulness and perceived logical validity running the main analysis model restricted to deceptive explanations examining the interactions between headline veracity, number of premises, average perceived truthfulness of the premises and average perceived logical validity of the premises.\nLastly, we conducted a moderation analysis for the word count and reading ease by running the main analysis model restricted to deceptive explanations and examining the interactions between headline veracity, word count, and reading ease of the explanations."}, {"title": "Results", "content": "In order to study the effects of deceptive AI explanations on human beliefs, we conducted a pre-registered online experiment with 23,840 observations from 1,192 participants rating their beliefs in true and false news headlines before and after receiving AI generated explanations. We designed our experiment to disentangle the influence of deceptive AI explanations from merely receiving a deceptive (inaccurate) classification of a news headline as true or false without explanation. This was done by randomizing participants to receiving true and false news headlines accompanied by either deceptive classifications or deceptive classifications with deceptive AI explanations (between-subjects). Moreover, we examine the impact of logical validity of the explanation, and how personal factors and the effects of syntactic and semantic features of the deceptive explanations may influence the outcomes.\nFor each true and false news headline, an LLM model (GPT-3) was used to generate an honest and a deceptive explanation by prompting the model to complete the following statements: \"This is false because...\" or \"This is true because...\". For deceptive explanations, the model generated inaccurate explanations stating why true statements were false and why false statements were true. This was done for both news headlines and trivia statements. Examples of the generated classifications with explanations can be found in Appendix 3.\nDeceptive AI-generated Classifications with Explanations are more Persuasive than Honest AI-generated Explanations\nTo understand the persuasiveness of deceptive AI-generated explanations, we compared the relative persuasiveness of deceptive and honest explanations on belief change (i.e. the absolute difference in rating from before and after seeing an AI classification with explanation per item per participant). We find that deceptive AI-generated explanations are significantly more persuasive than honest explanations on both true and false news headlines ($\\beta$ = 0.40, p < 0.0001 and $\\beta$ = 0.29, p = 0.003, respectively, not pre-registered). Prior research has shown that fake news is shared more often that true news due to factors such as novelty and emotional content (fear, disgust, and surprise) [37]. It is likely similar factors are at play for deceptive explanations, potentially explaining why they were found to be more persuasive than honest explanations. The full regression table can be found in Table 4 in the Appendix.\nExplanations Can Amplify Beliefs in False Information\nTo ensure our results were caused by explanations and not simply labeling information as true or false, we compared the influence of AI feedback with and without explanations (deceptive AI-generated explanations and deceptive AI generated classifications, respectively).\nOur results show that deceptive AI generated classifications without explanation, significantly increase belief in false news headlines ($\\beta$ = 0.71, p < 0.0001) and decreases belief in true news headlines ($\\beta$ = \u22121.72, p < 0.0001). In extension, when accompanied by deceptive AI-generated explanations, beliefs in false news headlines were further significantly increased ($\\beta$ = 0.32, p = 0.009) and beliefs in true news headlines where further significantly decreased as compared to just deceptive classifications ($\\beta$ = \u22120.72, p = 0.0001). These results suggests that the effects of deceptive AI systems amplifies beliefs in information beyond classifications without explanations (Fig. 6). The full results can be found in Table 3 in the Appendix.\nPersonal factors moderate the influence of deceptive AI explanations\nIn previous studies, cognitive reflection as measured by the Cognitive Reflection Test [33] has been found to associate with people's ability to correctly identify misinformation [38]. However, when receiving AI feedback on the truth of news headlines, our results revealed no significant interactions with cognitive reflection level and deceptive classifications both with and without explanations for false news headlines ($\\beta$ = \u22120.09, p = 0.20 and $\\beta$ = 0.13, p = 0.15, respectively) and for true news headlines ($\\beta$ = 0.16, p = 0.20 and $\\beta$ = 0.25, p = 0.15, respectively). This suggests that introducing an evaluative AI system framed as a fact-checking system could override the effects of cognitive reflection on truth discernment of news headlines. This may be attributed to several factors. First, the presence of information labeled as provided by \"AI\" might induce a reliance effect, where individuals defer judgment to the technology [39], potentially undermining their reflective capacities [38]. This effect could be accentuated by the perceived authority or credibility of language based AI systems, which might attenuate the influence of cognitive reflection [40]. Additionally, the complexity or novelty of information such as new facts they have no knowledge of revealed in AI explanation could confuse or overwhelm users, reducing the effectiveness of their cognitive reflection in evaluating the information [41]. Lastly, it is also possible that individuals with high cognitive reflection are already near their maximum ability to discern truth from falsehood, resulting in a ceiling effect that leaves little room for AI generated explanations to induce cognitive reflection.\nTrust in AI systems has in previous literature been highlighted as an important feature moderating the effects of explanations on people's beliefs [16, 17]. Our results showed a significant effect of self-reported trust in AI systems on participants' belief ratings when getting deceptive classifications without explanations on true ($\\beta$ = \u22120.64, p = 0.0001) and false news headlines ($\\beta$ = 0.38, p = 0.0001). However, we did not find any increased effects of trust on participants' belief ratings when getting deceptive classifications with explanations on true ($\\beta$ = \u22120.11, p = 0.79) and false news headlines ($\\beta$ = \u22120.01, p = 0.59), indicating that trust in AI systems does not significantly increase the persuasion effects when receiving deceptive AI-generated explanations from just getting deceptive classifications.\nLastly, research has highlighted prior knowledge as a significant predictor of correctly identifying fake news [42]. However, it is unclear whether these effects extend to AI-generated deceptive explanations. To evaluate these effects, we had participants rate their own perception of their knowledge of a news headline after rating their belief in the news headline. When receiving AI classifications without explanations that were truthful (honest), self-reported prior knowledge was associated with increased beliefs in true news headlines ($\\beta$ = 0.44, p < 0.0001) and associated with decreased beliefs in false headlines ($\\beta$ = \u22120.11, p = 0.001). Conversely, when receiving deceptive classifications without explanations, prior knowledge was not found to have any significant effects on beliefs in false and true news headlines ($\\beta$ = \u22120.05, p = 0.54 and $\\beta$ = 0.02, p = 0.86, respectively). However, while there were no significant effects of deceptive AI-generated explanations on true news headlines ($\\beta$ = \u22120.19, p = 0.19), self-reported prior knowledge was associated with significantly increased beliefs in false news headlines when receiving deceptive AI- generated explanations ($\\beta$ = 0.18, p = 0.02). This suggests that individuals who report themselves as knowledgeable on a news headline might not necessarily be more resilient to deceptive AI classifications on true news headlines, and, in fact, might even be more susceptible to believing false news headlines when given deceptive AI explanations. One possible explanation for these results could be due to what is known as overconfidence bias [43, 44], where those who voice their perceived knowledge level to be high, could overestimate their ability to critically evaluate the AI system's outputs or to identify false information correctly. Future research should compare these results with an objective assessment of people's prior knowledge to more accurately detail the differences.The complete results can be found in Table 8 (CRT), Table 7 (trust), Table 6 (prior knowledge) in the Appendix.\nDeceptive AI-generated explanations that are logically invalid decrease people's beliefs in false news headlines\nResearchers have suggested that people's ability to identify logical flaws (or logical fallacies) could play an essential role in refuting misinformation [45, 46, 21, 47]. In order to investigate the influence of logical validity of deceptive AI-generated explanations, we modeled the influence of logical validity of explanations on participants' belief rating using a linear regression, where logical validity is when the truth of the AI generated explanation necessarily implies the truth of the AI generated classification (logically valid) in comparison to where the truth of the classification is independent from the truth of the explanation (See Section 1). Limiting the data to only ratings where explanations were present, our results show that while logically invalid deceptive AI-generated explanations did not have any significant effects on participants' belief rating of true news headlines ($\\beta$ = \u22120.31, p = 0.13), logically invalid deceptive AI- generated explanations did significantly increase beliefs in false news headlines ($\\beta$ = \u22120.35, p = 0.02). This suggests that participants are more likely to reject deceptive explanations for false news headlines when the explanations are logically invalid. This demonstrates the potential for logical analysis and critical thinking skills to mitigate the influence of deceptive AI-generated explanations for false information. An overview of the results can be found in Table 5 and Fig. 6."}, {"title": "Limitations and Potential Generalization to Future AI Systems", "content": "As exemplified in this study, personal factors mediate the influence of explanations on humans beliefs. Extending upon these findings, research has shown that prior beliefs about AI systems can significantly influence how people integrate or reject AI-generated information [48, 49]. Taking this into account, the impact of deceptive AI explanations might vary significantly across different cultural and contextual settings. Factors such as political climate, prevalent media literacy, and language around AI can influence how deceptive explanations are received and believed. For instance, research has shown that the choice of terminology significantly influences people's perceptions and reactions towards AI-generated content, with different terms leading to varying degrees of accuracy in identification and emotional response across different cultural contexts [49]. Moreover, while research has shown LLMs to be significantly more authoritative, persuasive, seemingly logically valid and even preferred over human-authored content, it is unclear to which extent the effects of deceptive explanations identified in this paper would transfer to human-authored deceptive explanations.\nSecond, our study utilized GPT-3, the most advanced language model available during the experimental period, to generate explanations. Although more sophisticated models have since surpassed its capabilities, our results indicate that even at the GPT-3 level, the model was able to produce deceptive explanations that adversely affected people's beliefs. It is important to recognize that while more advanced models generally exhibit less hallucination and incorporate more robust safeguards for the information they generate, researchers have repeatedly demonstrated that the safety measures implemented in large language models (LLMs) can be easily circumvented through techniques such as \"jailbreaks\" [28, 50] or fine-tuning. For instance, an individual fine-tuned a readily available model on the HuggingFace platform using a dataset of posts from an online forum known for hosting harmful and offensive content, resulting in the generation of more than 30,000 posts on the platform [2]. As a result, even with better models becoming available, it likely that not only can these be exploited to generate misleading or deceptive explanations, even with safety measures in place, but they could even be misused to generate deceptive explanations far more persuasive and scalable than demonstrated here. As LLMs continue to advance and become more sophisticated, it is crucial for researchers and developers to remain vigilant in identifying and addressing potential vulnerabilities to ensure the responsible deployment of these technologies."}, {"title": "Conclusion", "content": "Our findings underscore the significant impact that deceptive AI-generated explanations can have on shaping public opinion and influencing individual beliefs. The ability of these explanations to not only present misinformation but also provide seemingly logical justifications makes them particularly potent tools for misinformation campaigns. This is especially concerning in the context of political discourse, scientific communication, and social media, where the rapid dissemination and acceptance of false information can have real-world consequences.\nThe persuasive power of deceptive explanations, as demonstrated in our study, highlights a critical vulnerability in the public's ability to discern truth from falsehood when interacting with AI-generated content. This is compounded by the finding that even individuals who consider themselves knowledgeable are not immune to the influence of these deceptive explanations. In fact, our results suggest that self-assessed knowledge may even increase susceptibility to believing false information when it is accompanied by a deceptive explanation. This could be due to a combination of overconfidence and the sophisticated nature of the explanations that make the misinformation seem credible.\nMoreover, the role of logical validity in the effectiveness of deceptive explanations is particularly noteworthy. Our study found that logically invalid explanations were less effective in persuading individuals to believe false headlines, suggesting that enhancing critical thinking and logical reasoning skills could be a viable strategy to combat the influence of misinformation. This aligns with previous research emphasizing the importance of education in logical fallacies and critical thinking as tools for empowering individuals to better evaluate the information they encounter, particularly in digital environments where AI-generated content is prevalent.\nWhile AI has the potential to bring about significant benefits, its capability to generate persuasive, deceptive explanations poses a serious risk to informational integrity and public trust. Our study highlights the urgent need for comprehensive strategies that address the dual aspects of enhancing public resilience against misinformation and ensuring responsible AI development and deployment."}, {"title": "2 Supplementary information", "content": "All data, including pre-registration, datasets, explanation prompts, and code generated and analyzed dur- ing the current study is available on GitHub (https://github.com/mitmedialab/deceptive-AI), Zen- odo (https://zenodo.org/records/8172056) and Research Box (https://researchbox.org/1801&PEER_ REVIEW_passcode=BDHVUP)."}, {"title": "3 Extended Data", "content": "Additional Results\nDeceptive AI-generated Explanations Increase Beliefs in False Headlines and Decrease Beliefs in True Headlines\nIn order to gain insights into whether explanations lead to more accurate beliefs, we first compare how accurately participants rated true and false news headlines before (no feedback) and after getting deceptive AI-generated explana- tions. We coded accuracy by subtracting the belief ratings from the ground truth per rating per participant. Running an analysis of variance (ANOVA), we found that deceptive AI-generated explanations lead to a significantly lower accuracy than no feedback (14 percentage point difference, F(2, 10959) = -36, p < 0.0001, ANOVA Welch).\nBreaking these results down into beliefs in true and false news headlines, we find that deceptive Al explanations significantly increase beliefs in false headlines and significantly decrease beliefs in true headlines ($\\beta$ = -2.44, p < 0.001 and $\\beta$ = 1.03, p < 0.001, respectively), suggesting that deceptive AI explanations significantly diminish people's ability to tell true news headlines from false news headlines.\nSemantic and syntactic features moderate the influence of deceptive explanations\nBeyond personal factors like cognitive reflection level, trust in AI, and prior knowledge, the manner in which information is structured and presented can greatly affect the way it is perceived and processed by the reader [51, 52", "53": "the amount of words used, the perceived truthfulness, and the perceived logical support of explanations for news headlines have the potential to shape a reader's beliefs and attitudes [54", "37": "."}]}