{"title": "Improving Global Parameter-sharing in Physically Heterogeneous Multi-agent Reinforcement Learning with Unified Action Space", "authors": ["Xiaoyang Yu", "Youfang Lin", "Shuo Wang", "Kai Lv", "Sheng Han"], "abstract": "In a multi-agent system (MAS), action semantics indicates the different influences of agents' actions toward other entities, and can be used to divide agents into groups in a physically heterogeneous MAS. Previous multi-agent reinforcement learning (MARL) algorithms apply global parameter-sharing across different types of heterogeneous agents without careful discrimination of different action semantics. This common implementation decreases the cooperation and coordination between agents in complex situations. However, fully independent agent parameters dramatically increase the computational cost and training difficulty. In order to benefit from the usage of different action semantics while also maintaining a proper parameter-sharing structure, we introduce the Unified Action Space (UAS) to fulfill the requirement. The UAS is the union set of all agent actions with different semantics. All agents first calculate their unified representation in the UAS, and then generate their heterogeneous action policies using different available-action-masks. To further improve the training of extra UAS parameters, we introduce a Cross-Group Inverse (CGI) loss to predict other groups' agent policies with the trajectory information. As a universal method for solving the physically heterogeneous MARL problem, we implement the UAS adding to both value-based and policy-based MARL algorithms, and propose two practical algorithms: U-QMIX and U-MAPPO. Experimental results in the SMAC environment prove the effectiveness of both U-QMIX and U-MAPPO compared with several state-of-the-art MARL methods.", "sections": [{"title": "1 Introduction", "content": "The researches on multi-agent system (MAS) and multi-agent reinforcement learning (MARL) have been remarkably expanding [1-4] and have been applied to solve complex problems in the industry, such as traffic management of autonomous drones [5], multi-agent air combat [6], power control in wireless networks [7], battery-swapping system for Unmanned Ground Vehicles (UGV) [8], resource allocation problem [9, 10], portfolio management [11], dialogue state tracking [12], and traffic lights control problem [13]. Previous deep MARL algorithms have achieved impressive results in cooperative MARL environment [2, 14]. The Starcraft Multi-Agent Challenges"}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Multi-agent Reinforcement Learning and Heterogeneity", "content": "The centralized training with decentralized execution (CTDE) paradigm [35-37], which requests agents not to use state S during execution, has become one of the most successful MARL paradigms in SMAC environment.\nFor policy-based methods, a simple implementation of CTDE is the \"centralized critic and decentralized actor\" (CCDA) structure. The state S is only available for the critic network during the centralized training phase. COMA [38] uses a counter-factural baseline for learning a better policy. MAPPO [32] directly implements the CCDA structure with a global critic and a shared actor. HAPPO [29] introduces the \"multi-agent advantage decomposition\" (MAAD) lemma and designs a sequential executing and updating structure for the independent actors. MAT [39] further develops the HAPPO by replacing the backbone of HAPPO into the Transformer-style network structure, and naturally uses the auto-regressive decoder as the sequential actor.\nFor value-based methods, the mainstream is the value factorization method, whose goal is to learn a centralized and factorized joint action-value function Qtot and the factorization structure: Qtot \u2192 Qi. In order to factorize Qtot and use the argmax policy of Qi to select actions, the Individual-Global-Max (IGM) consistency [40] is required:\n$\\arg \\max_a Q_{tot} (\\tau, s) = \\left(\\begin{array}{c}\\arg \\max_a Q_1 (\\tau_1) \\\\\\vdots \\\\\\arg \\max_a Q_k (\\tau_k)\\end{array}\\right).$\nVdN [41] simply uses the sum of local Qi functions to represent Qtot. QMIX [31] changes the factorization function from additivity to monotonicity, which dramatically enrich the embedding ability of the network. RODE [42] and ROMA [43] are role-based algorithms, which learn role policies online. In general, several key hyper-parameters define the clustering and using of role policies. Therefore, the performance of ROMA and RODE highly depends on searching for the optimal hyper-parameters, not to mention that the default training step of ROMA"}, {"title": "2.2 Global parameter-sharing", "content": "The global parameter-sharing mechanism is a common implementation widely used in MARL. If all agents are homogeneous, their policy or Q networks can be trained more efficiently with less computing complexity by adopting parameter-sharing [31, 32]. In order to deal with heterogeneous agents, in SMAC environment, many algorithms introduce extra information to identify agent ID or agent type. The advantage of global parameter-sharing is quite obvious. Since all agents use a shared network to choose action, the scalability problem does not affect the network structure and could be easier to solve. In general, cooperation requires necessary similarity and has been proved in [33].\nHowever, in HAPPO [29], the authors prove that global parameter-sharing may lead to exponentially bad local optimal policy in an XOR-style situation."}, {"title": "3 Definition and Analysis", "content": "In this section, our goal is to prove the necessity of considering action semantics while adopting parameter-sharing in MARL. First, We introduce fundamental concepts and definitions in 3.1. Next, we give a possible example of a bad joint action policy under parameter-sharing without distinguishing different action semantics."}, {"title": "3.1 Preliminaries", "content": "In this paper, we study the physical heterogeneous cooperative MARL problems using the decentralized partially observable Markov decision process (Dec-POMDP) [50] scheme. The problem is described with a tuple $G = (S, A, O; P, \\Omega, R; \\gamma, N, K, T)$. $N = {1, ..., n_i, ..., n}$ denotes the finite set of n agents, $K = {1, ..., k_i, ..., k}$ denotes the finite set of k agent groups, and $\\gamma \\in [0,1)$ is the reward discounting factor. $s \\in S$ denotes the true state of the environment with complete information and is not accessible for decentralized components. At each time-step t \u2264 T, agent i \u2208 N, from group ki\u2208 K, receives an individual partial observation oi from the environment and chooses an action ai \u2208 Ai from its local available action set Ai. Ai is masked from the global action set A with agent's local available-action-mask AM. Actions of all agents form a joint action $a^t = (a_1, ..., a_n) \\in A = (A_1, ..., A_n)$ at every time-step. The environment receives the joint action at and returns a next-state st+1 according to the transition function P(st+1|st, at), and a reward rt according to the global reward function R(s, at). All local observations $o_1^t, ..., o_n^t$ are generated from the state st according to the observation function $O(s_t, i)$, and form the joint observation $o^t = (o_1^t, ..., o_n^t) \\in O$. Observation-action trajectory history $\\tau^t = \\cup_{i=1}^t{(o^{t-1}_i,a^{t-1}_i)} (t\\geq 1;\\tau^0 = o^0)$ is the summary of the partial transition tuples of agent i before time-step t. Specifically, \u03c4i indicates the overall trajectory of agent i through all time-steps t \u2264 T. Replay buffer D = \u222a(\u03c4, s, r) stores"}, {"title": "3.2 Limitation of Existing Parameter-sharing Methods", "content": "As is mentioned in [29], adopting parameter-sharing among all agents equals adding a constraint $\u03b8i = \u03b8j, \u2200i, j \u2208 N$ to the joint policy space. Here, we point out that utilizing parameter-sharing without properly distinguishing different action semantics can lead to a sub-optimal solution.\nProposition 1. Considering a fully-cooperative physically heterogeneous MARL problem. Let there be 2 groups of agents, marked as Go and G1. Both groups have N agents. The actual action space of Go is Ao = {a0, ..., am}, while the actual action space of G1 is A1 = {a0, ..., am, ..., an} ($|a_n| > |a_m| \u2265 N). The reward function is defined as:\nr=\n{\n1 & (\\forall i\\neq j : a_i \\neq a_j) \\cup ( \\exists N_id , A_{id} = N_{id} , simultaneously)\\\\\n0 & (else)\n}\nLet J* be the optimal joint reward, and $\\bar{J}$ be the optimal joint reward under the parameter-sharing constraint. Then we have:\n$$\nJ^*=\n\\begin{cases}\n1 \\qquad \\qquad \\qquad \\quad J^*(\\pi)=1\\\\\n\\frac{1}{Pr} \\qquad \\qquad 0<Pr<1\n\\end{cases}\n$$\nProof. Suppose the joint action policy distribution under the parameter-sharing constraint is {po, ..., pm, ..., pn} for all 2N agents in both groups, where pmi indicates the probability of action ai. It is obvious that J* = 1, for we can let the joint policy be the deterministic policy that all agents with their id i \u2264 N from both groups choose the same action ai. For $\\bar{J}$, because it is required to simultaneously fulfill aidi = nidi, the expected reward is:\n$\\bar{J} = \\prod_{i=0}^N p_i.$\nTo maximize $\\bar{J}$, since pi > 0 and N > 0, we can equivalently maximize $(\\prod_{i=0}^N p_i)^{1/N}$. According to"}, {"title": null, "content": "the arithmetic-geometric means inequality, we have:\n$(\\prod_{i=0}^N p_i)^{1/N} \\le \\frac{1}{N}\\sum_{i=0}^N p_i,$\nwhere the equality holds if and only if \u2200i, j \u2264 N, pi = pj. Let pr = \u03a3io pi, and \u2200i \u2208 N, pi = pr/N. Then we have:\n$\\bar{J} = \\prod_{i=0}^N (\\frac{p_r}{N})^2 = (\\frac{p_r}{N})^{2N},$\nwhich is exactly the result.\nThrough this example, we prove that parameter-sharing can lead to an exponentially bad sub-optimal result as the number of agents increases. In addition, it can be inferred that increasing pr helps to improve $\\bar{J}$. However, according to the heterogeneity constraint and the parameter-sharing constraint, the maximum of pr is $\\sum p_{a_m}^i$, which is still smaller than 1."}, {"title": "3.3 Relation between action semantics and physical heterogeneity", "content": "In this paper, we divide the original agent action set A into three subsets: As, Aa and Ae. As indicates self actions that only affect the agent itself, i.e. moving actions. Aa indicates ally actions that only affect the ally agents, i.e. healing actions. Ae indicates enemy actions that only affect the enemy entities, i.e. attacking actions.\nThis classification method follows the different action semantics mentioned in [34], and further distinguishes different action objects. Considering the definition of physical heterogeneity provided in [28], the different ideal action objects (IO) is sufficient to determine and analyze the heterogeneity.\nTherefore, the difference in agents' action semantics results in physical heterogeneity. Additionally, it is beneficial to utilize the different action semantics for better cooperative policy learning in physically heterogeneous MARL problems."}, {"title": "4 Method", "content": ""}, {"title": "4.1 Unified Action Space", "content": "As is analyzed in section 3.2, adopting global parameter-sharing in physically heterogeneous MARL problems without proper structure can lead to"}, {"title": null, "content": "harmful sub-optimality. Therefore, we introduce the Unified Action Space as a solution.\nDefinition 1. Unified Action Space (UAS): Divide all agents' i \u2208 N local available action set Ai into the three sets with different action semantics: As, Aa and Ae. Then, the union set of the three sets of actions forms the Unified Action Space $A_U = A_s \\cup A_a \\cup A_e$.\nIt is obvious that the UAS is a superset to the original action space $A_U\\supset A$, as the original set is not forced to discriminate different action semantics. As is analyzed in section 3.3, in physically heterogeneous MARL problems, different action semantics determine physical heterogeneity and can be used to divide agents into different groups following the IOG requirement [28].\nFurthermore, different action semantics can be identified through different available-action-mask (AM) defined on the UAS. For example, in SMAC, Marine is an attacking unit whose action semantics include moving and attacking enemies. As a result, the AMmar only includes two subsets, AU \u2299 AMmar = As \u222a Ae. On the other hand, Medivac is a supporting unit whose action semantics include moving and healing allies. Therefore, the AMmed is only available for $A_U \\bigodot AM_{med} = A_s \\cup A_a$. We point out that the $\u2299$ operator indicates the masking-policy operation.\nIntroducing the UAS allows us to apply global parameter-sharing while also avoiding the limitation discussed in 3.2. Considering the example in section 3.2, let us suppose that the UAS is $A_U = {a_0, ..., a_n, a'_0, ..., a'_m}$. Then, the UAS policy distribution becomes $p^U = {p_0, ..., p_n, p'_0, ..., p'_m}$ for all agents. For generating group-specific policy, we need to mask the UAS distribution into group policy distribution: $p_{G_0} = p^U \\bigodot AM_{G_0} = {p'_0, ..., p'_m}$ and"}, {"title": "4.2 Cross-Group Inverse Loss", "content": "In a multi-agent system (MAS), a cooperative joint policy is usually approached by maximizing the mutual information between agents. However, calculating mutual information must be taken between two groups and the overall computational complexity is proportional to the combination number of 2 groups out of K groups $\u03a9(C_K^2)$.\nIntroducing the UAS provides another available way to learn a cooperative policy, which is learning an agent's policy while also predicting other agents' policies. \"Learning with predicting\" is a natural way for humans to practice and perform cooperation, and is useful to develop tacit team policy. To follow the \"learning with predicting\" scheme, we introduce the Cross-Group Inverse loss (CGI loss).\nThe core innovation is the inverse available-action-mask $AM^{inv}_i$. Since the action policy is generated through the masking operation $p_i = p^U \\bigodot AM_i$, we can generate the action policy of other physically"}, {"title": null, "content": "heterogeneous agents by applying their mask to the agent's UAS policy: $p^{inv}_i = p^U \\bigodot AM^{inv}_i$. Specifically, we add an independent network branch Vi using the trajectory of the agent Ti to calculate the piU and the piinv. A common implementing method for encoding trajectory is to use the hidden state of the gated recurrent unit (GRU) [51] hi. Because the GRU takes oi and ai as the input recursively for all time-steps t, we assume that hi can appropriately represent Ti. Finally, since the UAS guarantees the capability of achieving the global optimality under the parameter-sharing constraint, the CGI loss is calculated by the mean squared error (MSE) loss between piinu and pi for all agents with the network parameter \u03c8 = (\u03c81, ..., \u03c8n), and is written as:\n$L_{CGI}(p|h) = E_D[(p^{inv} - p)^2],$\nwhere ED means to sample a batch of tuples (\u03c4, s, r) from replay buffer D and calculate expectation across the batch. For notational convenience, we also use ED for PPO-style on-policy methods without distinction. In that case, the replay buffer D only consists of the present episode.\nSimilarly, for value-based algorithms, since the action policy is usually calculated by the argmax of Q values, the CGI loss is calculated by the MSE loss between Qinv and Q for all agents and is written as:\n$L_{CGI}(Q|h) = E_D[(Q^{inv} - Q)^2].$"}, {"title": "4.3 Practical Algorithms: U-MAPPO and U-QMIX", "content": "In this section, we design a policy-based algorithm U-MAPPO and a value-based algorithm U-QMIX as examples to prove the efficiency of the UAS module and CGI loss. As is mentioned in section 3.1, the different parts of network parameters are notated with \u03b8, \u03c6, and \u03c8. The bold fonts of \u03b8, \u03c6, and \u03c8 indicate the usage of global parameter-sharing in the UAS optimization."}, {"title": "4.3.1 U-MAPPO", "content": "In order to implement the UAS based on policy-based methods, we combine the UAS with the MAPPO[32] to form the U-MAPPO algorithm. The detailed network structure is illustrated in Fig. 2, and a pseudo-code is given in Algorithm 1.\nThe overall network consists of three different sub-networks: the actor network $\u03b8(a|\u03c4) = (\u03b8_1 (a_1|\u03c4_1), ..., \u03b8_n (a_n|\u03c4_n))$, the predictor network $\u03c8(p^{inv}|h) = (\u03c8_1(p^{inv}|h_1), ..., \u03c8_n(p^{inv}|h_n))$ and the critic network (V|s). These parameters are responsible for the joint action policy $\u03c0_\u03b8 = (\u03c0_{\u03b8_1},......, \u03c0_{\u03b8_n})$, the joint inverse policy $\u03c0^{inv}_\u03c8 = (\u03c0^{inv}_{\u03c8_1}, ..., \u03c0^{inv}_{\u03c8_n})$ and the global value function V\u03c8(s), respectively.\nFollowing the implementation of MAPPO, the actor \u03c0\u03b8 is shared by all agents and is optimized following the loss function 9. It receives the current observation ot, the last action at-1, and the last GRU hidden state ht-1 to calculate the next action"}, {"title": null, "content": "at. Due to the analysis in section 4.2, we notate the observation-action trajectory as \u03c4t = (ot,at\u22121,ht\u22121). The critic V\u03c8 is a mapping function S \u2192 R, and is centrally trained following the loss function 10. The additional predictor module piinv is also shared by all agents for generating the inverse policy and is optimized following the CGI loss function 7.\n$L(\u03c0_\u03b8) = E_D[min(\\frac{\u03c0_\u03b8(a|\u03c4)}{\u03c0_{\u03b8_{old}}(a|\u03c4)}A_{\u03c0_\u03b8} (s, a),clip(\\frac{\u03c0_\u03b8(a|\u03c4)}{\u03c0_{\u03b8_{old}}(a|\u03c4)},1 \u00b1 \\epsilon_\u03c1)A_{\u03c0_\u03b8} (s, a))] + \u03bb_EE_D [S(\u03c0_\u03b8(\u03c4))].$\n$L(V_\u03c8) = E_D[max((V_\u03c8(s) \u2013 R)^2,((clip(V_\u03c8(s), V_{\u03c8_{old}}(s) \u00b1 \\epsilon_v)) \u2013 R)^2)].$\n$L_{U-MAPPO} = L(\u03c0_\u03b8) + \u03bb_vL(V_\u03c8) + \u03bb_IL(\u03c1^{inv}).$\nThe overall loss function of U-MAPPO is 11. The clip operator indicates clipping the value of the first variable to fit in the threshold interval determined by the second function with the hyper-parameter \u03f5p or \u03f5v. $A_{\u03c0_\u03b8}(s, a)$ is the advantage value computed with the GAE [52] method. S(\u03c0\u03b8(\u03c4)) is the policy entropy. R is the discounted reward-to-go. \u03bbe, \u03bbv, and \u03bbi are"}, {"title": "4.3.2 U-QMIX", "content": "Similar to the U-MAPPO, we combine the UAS with the QMIX[31] to form the U-QMIX algorithm. The network structure is referred to in Fig. 3, and the pseudo-code of U-QMIX is given in Algorithm 2.\nIn general, there are two parts of sub-networks optimized with different losses. The Q network includes the local Q network $\u03b8i(Q|\u03c4) = (\u03b8_1(Q_1|\u03c4_1), ..., \u03b8_n(Q_n|\u03c4_n))$ and the mixing network"}, {"title": null, "content": "$O_M (Qi, s)$. The predictor network is symbolized with $\u03c8(Q^{inv}|h) = (\u03c8_1(Q^{inv}|h_1), ..., \u03c8_n (Q^{inv}|h_n))$.\nThe local Q network \u03b8i is shared by all agents. It uses the current observation oi, the last action ai\u22121 and the last GRU hidden state hi\u22121 to calculate the next Q value Qi. Similarly, we notate the observation-action trajectory as \u03c4t = (ot,at\u22121,ht\u22121). Mixing network OM is a global network for value factorization. It takes all local [Q\u03b8i] as input, and mixes with the state s to produce the Qtot. The loss of \u03b8 is calculated following 12, where the Qtgtot is the target Q value calculated by the target Q network \u03b8tgt. Predictor network \u03c8 is used to compute Qipiinv and is optimized by the CGI loss function 8. The total loss function of U-QMIX is written as 13. Similar to 11, the \u03bbi is the coefficient hyper-parameter responsible for the CGI loss.\n$L(Q_\u03b8) = E_D[(y^o \u2013 Q_{tot}(\u03c4, s; \u03b8))^2],$\nyo = r + \u03b3 max a\u2032 Qtot(\u03c4\u2032, s\u2032; \u03b8tgt),\n$L_{U-QMIX} = L(Q_\u03b8) + \u03bb_iL(Q^{inv}).$"}, {"title": "4.3.3 Summary of Hyper-parameters", "content": "Commonly, we choose Adam [53] as the optimizer. The maximum time-steps of one episode is set to be 200. The reward discounting factor \u03b3 is 0.99. We use the latest version 4.10 of the StarcraftII game on the Linux platform to perform experiments.\nFor U-MAPPO method, the learning rate is 5e-4 and the total training step is 10M. We set the threshold interval to be \u03f5p = \u03f5v = 0.2, and the loss coefficients to be \u03bbe = 0.01, \u03bbv = 1 and \u03bbi = 0.8.\nFor U-QMIX, the learning rate is 3e-4 and the total training step is 5M. The learning rate is scheduled to decay by multiplying the factor 0.5 every 50,000 episodes (averagely about 2M-3.5M steps). The $\\epsilon$ of the $\\epsilon-greedy$ action selecting policy starts at 1.0, ends at 0.05 and linearly declines for 50,000 steps. The size of the memory buffer D is 5,000 and the batch size is 32. The CGI loss coefficient is set to be \u03bbi = 0.06."}, {"title": "5 Experiments and Results", "content": ""}, {"title": "5.1 Experimental Details", "content": "We use 5 maps mentioned in [28] and the MMM2 map from the original map set to conduct our experiments. Detailed map information can be seen in Table 3. As we have introduced in section 3.3 and 4.1, the Marine is an attacking unit and its action space consists of As and Ae, while the Medivac is a supporting unit and its action space consists of As and Aa."}, {"title": "5.2 Comparison Algorithms", "content": "For policy-based algorithm comparison against the U-MAPPO, we choose COMA [38], MAPPO [32], HAPPO [29] and MAT [39] as baselines. For value-based algorithm comparison against the U-QMIX, vanilla QMIX [31], ROMA [43], RODE [42], CDS"}, {"title": "5.3 Comparison Results", "content": "The results of value-based algorithms are shown in section 5.3.1, Fig. 4, and Table 4. The results of policy-based algorithms are shown in section 5.3.2, Fig. 5, and Table 5. The figures show the WR curves changing over time and the tables show the final WR values followed by the standard deviations. The results are averaged across 5 individual tests."}, {"title": "5.3.1 Results of Value-based Algorithms Comparison", "content": "(1) Easy: we test all algorithms on the original asymmetric heterogeneous map MMM2. The results are shown in Fig. 4 (a). U-QMIX and RODE converge to 1.0 at about 3M steps. The WR curve of RODE increases faster than U-QMIX before 2M steps but suffers from high variance. QMIX and GHQ converge to about 0.8 at 2M steps. CDS also converges to 0.8 but requires 5M training steps. The WR curve of ASN and UNMAS can only converge to 0.5 with high variance. ROMA is only capable of acquiring 0.1 at 5M training steps.\n(2) Medium: we scale up ally units and increase enemy units for balancing. In Fig. 4 (b), (c), (d), and (e), the number of \"Marines+Medivacs\" increases from \"6+2\" to \"8+3\" and \"8+4\", while the number of enemy Marines increases from 15 to 23. As is concluded in [28], the power of 1 Medivac is roughly equal to 3.5 Marines. Therefore, the duels in these 4 maps are basically fair and the total amount of entities in the maps is also not too high.\nComparing the results in (a) and (b), it is easy to figure out that the difficulty of these two maps is similar, since all algorithms can acquire similar performance in these two maps. In (b), U-QMIX reaches the best WR 0.9, and RODE reach the second WR 0.8 with lower increasing rate. QMIX, ASN, UNMAS, and CDS end with about 0.6, 0.45, 0.4, and 0.3, respectively. In the first 2M steps, GHQ grows faster than all other algorithms, but converges at WR 0.7 at about 3M steps. ROMA fails to learn effective policy in (b).\nIn (c), the increased 1 enemy Marine dramatically decreases the WR of all algorithms. Only U-QMIX and GHQ are capable of reaching the WR 0.5. QMIX, RODE, ASN, and UNMAS end with the WR of about 0.3, 0.14, 0.15, and 0.13 separately. ROMA and CDS are both unable to learn any effective policy. In (d), we scale up allies with 1 Medivac and 2 Marines, and increase 5 enemy Marines for balancing. After 5M training steps, U-QMIX ends with the WR 0.8 with relatively low variance. GHQ attains the WR 0.6 with high variance. None of the other algorithms can"}, {"title": "5.3.2 Results of Policy-based Algorithms Comparison", "content": "Because the action space is discrete in SMAC, value-based algorithms generally perform better than policy-based algorithms [28, 31, 32, 54], which means that value-based methods usually require fewer training-steps to achieve similar WR than policy-based methods. Therefore, for a fair comparison and better performance, the training-step for policy-based algorithms is 10M.\n(1) Easy: we test all algorithms on the original asymmetric heterogeneous map MMM2. In Fig. 5 (a), U-MAPPO and MAPPO converged to WR 0.9 at about 7M training-steps. COMA acquires a little WR before the first 3M steps and then fails to learn effective policy. HAPPO reaches WR 0.1 at the end of training, while MAT fails to learn any policy."}, {"title": "5.4 Ablation Study", "content": "In order to determine the effectiveness of the UAS and CGI loss, we take ablation tests of the two components in (2) 6m2m-15m, and (b) 8m3m-21m. For value-based methods, QMIX is the baseline algorithm while \u201cQMIX+UAS\u201d and \u201cQMIX+CGI\u201d are the ablation groups. Similarly, for policy-based methods, MAPPO is the baseline algorithm while \"MAPPO+UAS\" and \"MAPPO+CGI\" are the ablation groups."}, {"title": "6 Conclusion", "content": "This paper focuses on the global parameter-sharing in physical heterogeneous MARL problems. Our core motivation is to design a proper global parameter-sharing method that can not only cooperate well but also avoid bad local optimality. Inspired by the term action semantics, we introduce the unified action space (UAS) as a solution for distinguishing the"}, {"title": null, "content": "different action semantics of heterogeneous agents. The usage of UAS is capable of acquiring better performance than the baseline method since the accurate action semantics improve the optimization process.\nTo further improve the cooperation between heterogeneous groups, we introduce the cross group inverse (CGI) loss which uses the trajectory of one group to predict the policy or Q value of other groups. This method avoids the calculation of mutual information, which must be taken between two groups, and therefore dramatically reduces the overall computation cost. We combine the UAS and CGI with standard value-based and policy-based methods to generate the U-QMIX and U-MAPPO methods.\nComparing and ablation experiments are conducted in physical heterogeneous SMAC maps to prove the effectiveness of U-QMIX and U-MAPPO. The results show that U-QMIX and U-MAPPO outperform other state-of-the-art algorithms with higher winning-rate and lower variance.\nIn the future, we will try to focus on the other type of heterogeneity, behavioral heterogeneity. We hope our work could be helpful for further study in the heterogeneous MARL problems. In addition, we are open to discover the complex combination of communication or scalability with heterogeneity."}, {"title": "Statements and Declarations", "content": "Funding: No funding was received to assist with the preparation of this manuscript.\nCompeting interests: The authors have no competing interests to declare that are relevant to the content of this article.\nEthics approval: This article does not involve any ethical problem which needs approval.\nConsent to participate: All authors have seen and approved the final version of the manuscript being submitted.\nConsent for publication: All authors warrant that the article is our original work, hasn't received prior publication, and isn't under consideration for publication elsewhere. A preprint version of our manuscript has been submitted to arXiv, and the page is xxx.\nAvailability of data and materials: The datasets generated during and/or analyzed during the current study are available from the corresponding author on reasonable request."}]}