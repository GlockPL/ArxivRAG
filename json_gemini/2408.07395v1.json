{"title": "Improving Global Parameter-sharing in Physically Heterogeneous Multi-agent Reinforcement Learning with Unified Action Space", "authors": ["Xiaoyang Yu", "Youfang Lin", "Shuo Wang", "Kai Lv", "Sheng Han"], "abstract": "In a multi-agent system (MAS), action semantics indicates the different influences of agents' actions toward other entities, and can be used to divide agents into groups in a physically heterogeneous MAS. Previous multi-agent reinforcement learning (MARL) algorithms apply global parameter-sharing across different types of heterogeneous agents without careful discrimination of different action semantics. This common implementation decreases the cooperation and coordination between agents in complex situations. However, fully independent agent parameters dramatically increase the computational cost and training difficulty. In order to benefit from the usage of different action semantics while also maintaining a proper parameter-sharing structure, we introduce the Unified Action Space (UAS) to fulfill the requirement. The UAS is the union set of all agent actions with different semantics. All agents first calculate their unified representation in the UAS, and then generate their heterogeneous action policies using different available-action-masks. To further improve the training of extra UAS parameters, we introduce a Cross-Group Inverse (CGI) loss to predict other groups' agent policies with the trajectory information. As a universal method for solving the physically heterogeneous MARL problem, we implement the UAS adding to both value-based and policy-based MARL algorithms, and propose two practical algorithms: U-QMIX and U-MAPPO. Experimental results in the SMAC environment prove the effectiveness of both U-QMIX and U-MAPPO compared with several state-of-the-art MARL methods.", "sections": [{"title": "1 Introduction", "content": "The researches on multi-agent system (MAS) and multi-agent reinforcement learning (MARL) have been remarkably expanding [1-4] and have been applied to solve complex problems in the industry, such as traffic management of autonomous drones [5], multi-agent air combat [6], power control in wireless networks [7], battery-swapping system for Unmanned Ground Vehicles (UGV) [8], resource allocation problem [9, 10], portfolio management [11], dialogue state tracking [12], and traffic lights control problem [13]. Previous deep MARL algorithms have achieved impressive results in cooperative MARL environment [2, 14]. The Starcraft Multi-Agent Challenges"}, {"title": "2 Related Works", "content": "2.1 Multi-agent Reinforcement Learning and Heterogeneity\nThe centralized training with decentralized execution (CTDE) paradigm [35-37], which requests agents not to use state S during execution, has become one of the most successful MARL paradigms in SMAC environment.\nFor policy-based methods, a simple implementation of CTDE is the \"centralized critic and decentralized actor\" (CCDA) structure. The state S is only available for the critic network during the centralized training phase. COMA [38] uses a counter-factural baseline for learning a better policy. MAPPO [32] directly implements the CCDA structure with a global critic and a shared actor. HAPPO [29] introduces the \"multi-agent advantage decomposition\" (MAAD) lemma and designs a sequential executing and updating structure for the independent actors. MAT [39] further develops the HAPPO by replacing the backbone of HAPPO into the Transformer-style network structure, and naturally uses the auto-regressive decoder as the sequential actor.\nFor value-based methods, the mainstream is the value factorization method, whose goal is to learn a centralized and factorized joint action-value function $Q_{tot}$ and the factorization structure: $Q_{tot} \\rightarrow Q_{i}$. In order to factorize $Q_{tot}$ and use the argmax policy of $Q_{i}$ to select actions, the Individual-Global-Max (IGM) consistency [40] is required:\n$\\arg \\max_{a} Q_{tot} (\\tau, s) = \\begin{bmatrix}\\arg \\max_{a} Q_{1} (\\tau_{1})\\\\ ...\\\\ \\arg \\max_{a} Q_{k} (\\tau_{k})\\end{bmatrix}$ (1)\nVDN [41] simply uses the sum of local $Q_{i}$ functions to represent $Q_{tot}$. QMIX [31] changes the factorization function from additivity to monotonicity, which dramatically enrich the embedding ability of the network. RODE [42] and ROMA [43] are role-based algorithms, which learn role policies online. In general, several key hyper-parameters define the clustering and using of role policies. Therefore, the performance of ROMA and RODE highly depends on searching for the optimal hyper-parameters, not to mention that the default training step of ROMA is 20M time-steps. CDS [44] maximizes the mutual information between the agent trajectory and its own agent ID to acquire distinguishable individual local Q functions. ASN [34] analyzes different actions' influence on other agents and uses different sub-networks to capture the difference. UNMAS [45] proposes the self-weighting mixing network and the individual action-value networks to adapt to the changes in agent numbers and action space.\nTraditionally, the study about heterogeneity in MARL is insufficient, because heterogeneous MARL has been considered as a special case of homogeneous MARL. HAPPO lacks specific analysis and sufficient experiments to analyze the effect of heterogeneity. Its successors, MAT and HASAC [46], only modify the network structure and optimizing methods and still lack the study in heterogeneity. SHPPO [47] uses a latent network to learn embedding variables for heterogeneous decision-making. In other field of MAS, FedMRL [48] develops a novel federated multi-agent deep reinforcement learning framework to address data heterogeneity for medical imaging. HeR-DRL [49].\nRecently, HetGPPO [25] conducts a taxonomical study on heterogeneity classification, and introduces a graph neural network heterogeneous agent-wise communication. GHQ [28] gives a formal definition of the local transition heterogeneity, which is equal to the physical heterogeneity, and uses a grouping method and hybrid value factorization structure to learn grouped Q functions for choosing actions.\n2.2 Global parameter-sharing\nThe global parameter-sharing mechanism is a common implementation widely used in MARL. If all agents are homogeneous, their policy or Q networks can be trained more efficiently with less computing complexity by adopting parameter-sharing [31, 32]. In order to deal with heterogeneous agents, in SMAC environment, many algorithms introduce extra information to identify agent ID or agent type. The advantage of global parameter-sharing is quite obvious. Since all agents use a shared network to choose action, the scalability problem does not affect the network structure and could be easier to solve. In general, cooperation requires necessary similarity and has been proved in [33].\nHowever, in HAPPO [29], the authors prove that global parameter-sharing may lead to exponentially bad local optimal policy in an XOR-style situation."}, {"title": "3 Definition and Analysis", "content": "In this section, our goal is to prove the necessity of considering action semantics while adopting parameter-sharing in MARL. First, We introduce fundamental concepts and definitions in 3.1. Next, we give a possible example of a bad joint action policy under parameter-sharing without distinguishing different action semantics.\n3.1 Preliminaries\nIn this paper, we study the physical heterogeneous cooperative MARL problems using the decentralized partially observable Markov decision process (Dec-POMDP) [50] scheme. The problem is described with a tuple $\\mathcal{G}=\\langle\\mathcal{S}, \\mathcal{A}, \\mathcal{O}; \\mathcal{P}, \\Omega, \\mathcal{R}; \\gamma, \\mathcal{N}, \\mathcal{K}, T\\rangle$. $\\mathcal{N} =\\{1, ..., n_{i}, ..., n\\}$ denotes the finite set of n agents, $\\mathcal{K} =\\{1, ..., k_{i}, ..., k\\}$ denotes the finite set of k agent groups, and $\\gamma\\in [0,1)$ is the reward discounting factor. $s\\in \\mathcal{S}$ denotes the true state of the environment with complete information and is not accessible for decentralized components. At each time-step $t \\leq T$, agent $i \\in \\mathcal{N}$, from group $k_{i} \\in \\mathcal{K}$, receives an individual partial observation $o_{i}$ from the environment and chooses an action $a_{i} \\in \\mathcal{A}$ from its local available action set $\\mathcal{A}_{i}$. $\\mathcal{A}_{i}$ is masked from the global action set $\\mathcal{A}$ with agent's local available-action-mask $A M_{i}$. Actions of all agents form a joint action $\\mathbf{a}_{t} = (a_{1}, ..., a_{n}) \\in \\mathcal{A} = (\\mathcal{A}_{1}, ..., \\mathcal{A}_{n})$ at every time-step. The environment receives the joint action $\\mathbf{a}_{t}$ and returns a next-state $s_{t+1}$ according to the transition function $\\mathcal{P}(s_{t+1} | s_{t}, \\mathbf{a}_{t})$, and a reward $r_{t}$ according to the global reward function $\\mathcal{R}(s, \\mathbf{a}_{t})$. All local observations $o_{1}, ..., o_{n}$ are generated from the state $s_{t}$ according to the observation function $\\Omega(s_{t}, i)$, and form the joint observation $\\mathbf{o}_{t} = (o_{1}, ..., o_{n}) \\in \\mathcal{O}$. Observation-action trajectory history $\\mathcal{T}^{i} = \\bigcup_{t=1}^{T}\\{(o_{t-1}^{i}, a_{t-1}^{i})\\} (t \\geq 1; \\mathcal{T}^{0} = \\emptyset^{0})$ is the summary of the partial transition tuples of agent i before time-step t. Specifically, $\\mathcal{T}^{i}$ indicates the overall trajectory of agent i through all time-steps t \u2264 T. Replay buffer $\\mathcal{D} = \\bigcup(t, s, r)$ stores all data for batch sampling. Network parameters are notated with $\\theta$, $\\phi$ and $\\psi$.\n3.2 Limitation of Existing Parameter-sharing Methods\nAs is mentioned in [29], adopting parameter-sharing among all agents equals adding a constraint $\\theta_{i}=\\theta_{j}, \\forall i, j \\in \\mathcal{N}$ to the joint policy space. Here, we point out that utilizing parameter-sharing without properly distinguishing different action semantics can lead to a sub-optimal solution.\nProposition 1. Considering a fully-cooperative physically heterogeneous MARL problem. Let there be 2 groups of agents, marked as $G_{0}$ and $G_{1}$. Both groups have N agents. The actual action space of $G_{0}$ is $\\mathcal{A}_{0}=\\{a_{0},..., a_{m}\\}$, while the actual action space of $G_{1}$ is $\\mathcal{A}_{1}=\\{a_{0},..., a_{m}, ..., a_{n}\\} (|a_{n}| > |a_{m}| \\geq N)$. The reward function is defined as:\n$r=\\begin{cases}1 & (\\forall N_{i d}, A_{i d}=N_{i d}, simultaneously) \\\\0 & (else)\\end{cases}$ (2)\nLet $J^{*}$ be the optimal joint reward, and $\\hat{J}$ be the optimal joint reward under the parameter-sharing constraint. Then we have:\n$\\frac{J^{*}}{\\hat{J}}=\\frac{1}{\\prod_{i=0}^{N} \\rho_{i} 2 N},$\n$0< \\rho_{i} <1$. (3)\nProof. Suppose the joint action policy distribution under the parameter-sharing constraint is ${\\rho_{0}, ..., \\rho_{m}, ..., \\rho_{n}}$ for all 2N agents in both groups, where $\\rho_{i}$ indicates the probability of action $a_{i}$. It is obvious that $J^{*}=1$, for we can let the joint policy be the deterministic policy that all agents with their id $i \\leq N$ from both groups choose the same action $a_{i}$. For $\\hat{J}$, because it is required to simultaneously fulfill $a_{i d}=N_{i d}$, the expected reward is:\n$\\hat{J} = \\prod_{i=0}^{N} \\rho_{i}^{2}$. (4)\nTo maximize $\\hat{J}$, since $\\rho_{i} > 0$ and N > 0, we can equivalently maximize $(\\prod_{i=0}^{N} \\rho_{i})^{1 / N}$. According to\nthe arithmetic-geometric means inequality, we have:\n$(\\prod_{i=0}^{N} \\rho_{i})^{1 / N} \\leq \\frac{1}{N} \\sum_{i=0}^{N} \\rho_{i}$, (5)\nwhere the equality holds if and only if $\\forall i, j \\leq N, \\rho_{i} = \\rho_{j}$. Let $\\rho_{r}=\\sum_{i=0}^{N} \\rho_{i}$, and $\\forall i \\in \\mathcal{N}, \\rho_{i} = \\rho_{r} / N$. Then we have:\n$\\hat{J} = \\prod_{i=0}^{N} (\\frac{\\rho_{r}}{N})^{2}=\\left(\\frac{\\rho_{r}}{N}\\right)^{2 N}$, (6)\nwhich is exactly the result.\nThrough this example, we prove that parameter-sharing can lead to an exponentially bad sub-optimal result as the number of agents increases. In addition, it can be inferred that increasing $\\rho_{r}$ helps to improve $\\hat{J}$. However, according to the heterogeneity constraint and the parameter-sharing constraint, the maximum of $\\rho_{r}$ is $\\sum_{a m} \\rho_{i}$, which is still smaller than 1.\n3.3 Relation between action semantics and physical heterogeneity\nIn this paper, we divide the original agent action set $\\mathcal{A}$ into three subsets: $\\mathcal{A}_{s}$, $\\mathcal{A}_{a}$ and $\\mathcal{A}_{e}$. $\\mathcal{A}_{s}$ indicates self actions that only affect the agent itself, i.e. moving actions. $\\mathcal{A}_{a}$ indicates ally actions that only affect the ally agents, i.e. healing actions. $\\mathcal{A}_{e}$ indicates enemy actions that only affect the enemy entities, i.e. attacking actions.\nThis classification method follows the different action semantics mentioned in [34], and further distinguishes different action objects. Considering the definition of physical heterogeneity provided in [28], the different ideal action objects (IO) is sufficient to determine and analyze the heterogeneity.\nTherefore, the difference in agents' action semantics results in physical heterogeneity. Additionally, it is beneficial to utilize the different action semantics for better cooperative policy learning in physically heterogeneous MARL problems."}, {"title": "4 Method", "content": "4.1 Unified Action Space\nAs is analyzed in section 3.2, adopting global parameter-sharing in physically heterogeneous MARL problems without proper structure can lead to harmful sub-optimality. Therefore, we introduce the Unified Action Space as a solution.\nDefinition 1. Unified Action Space (UAS): Divide all agents' $i \\in \\mathcal{N}$ local available action set $A_{i}$ into the three sets with different action semantics: $\\mathcal{A}_{s}$, $\\mathcal{A}_{a}$ and $\\mathcal{A}_{e}$. Then, the union set of the three sets of actions forms the Unified Action Space $\\mathcal{A}_{U} = \\mathcal{A}_{s} \\cup \\mathcal{A}_{a} \\cup \\mathcal{A}_{e}$.\nIt is obvious that the UAS is a superset to the original action space $\\mathcal{A}_{U} \\supset \\mathcal{A}$, as the original set is not forced to discriminate different action semantics. As is analyzed in section 3.3, in physically heterogeneous MARL problems, different action semantics determine physical heterogeneity and can be used to divide agents into different groups following the IOG requirement [28].\nFurthermore, different action semantics can be identified through different available-action-mask (AM) defined on the UAS. For example, in SMAC, Marine is an attacking unit whose action semantics include moving and attacking enemies. As a result, the $A M_{mar}$ only includes two subsets, $\\mathcal{A}_{U} \\odot A M_{mar} = \\mathcal{A}_{S} \\cup \\mathcal{A}_{E}$. On the other hand, Medivac is a supporting unit whose action semantics include moving and healing allies. Therefore, the $A M_{med}$ is only available for $\\mathcal{A}_{U} \\odot A M_{med} = \\mathcal{A}_{S} \\cup \\mathcal{A}_{A}$. We point out that the $\\odot$ operator indicates the masking-policy operation. Fig. 1 illustrates the usage of UAS and the different action semantics.\nIntroducing the UAS allows us to apply global parameter-sharing while also avoiding the limitation discussed in 3.2. Considering the example in section 3.2, let us suppose that the UAS is $\\mathcal{A}_{U} = \\{a_{0}, ..., a_{n}, a_{0}^{\\prime}, ..., a_{m}^{\\prime}\\}$. Then, the UAS policy distribution becomes $\\rho_{U} = {\\rho_{0}, ..., \\rho_{n}, \\rho_{0}^{\\prime}, ..., \\rho_{m}^{\\prime}\\}$ for all agents. For generating group-specific policy, we need to mask the UAS distribution into group policy distribution: $\\rho_{G_{0}} = \\rho_{U} \\odot A M_{G_{0}} = {\\rho_{0}^{\\prime}, ..., \\rho_{m}\\}$ and $\\rho_{G_{1}} = \\rho_{U} \\odot A M_{G_{1}} = {\\rho_{0}, ..., \\rho_{n}}$. Because the mask operation re-normalizes group policies, we have $\\sum_{i=0}^{G_{0}} \\rho_{i} = 1$ and $\\sum_{i=0}^{G_{1}} \\rho_{i} = 1$. Consequently, the policies of the two groups become independent and we can implement the same deterministic global optimal joint policy as $J^{*}$ requires without restriction. As a result, the $J$ with the usage of the UAS and AM becomes 1, which is equivalent to the global maximum $J^{*}$.\n4.2 Cross-Group Inverse Loss\nIn a multi-agent system (MAS), a cooperative joint policy is usually approached by maximizing the mutual information between agents. However, calculating mutual information must be taken between two groups and the overall computational complexity is proportional to the combination number of 2 groups out of K groups $\\Omega(C_{K}^{2})$.\nIntroducing the UAS provides another available way to learn a cooperative policy, which is learning an agent's policy while also predicting other agents' policies. \"Learning with predicting\" is a natural way for humans to practice and perform cooperation, and is useful to develop tacit team policy. To follow the \"learning with predicting\" scheme, we introduce the Cross-Group Inverse loss (CGI loss).\nThe core innovation is the inverse available-action-mask $A M_{i n v}$. Since the action policy is generated through the masking operation $\\mathbf{\\pi_{i}}=\\mathbf{\\pi_{U}} \\odot A M_{i}$, we can generate the action policy of other physically heterogeneous agents by applying their mask to the agent's UAS policy: $\\pi_{i n v}=\\mathbf{\\pi_{U}} \\odot A M_{i n v}$. Specifically, we add an independent network branch $\\psi_{i}$ using the trajectory of the agent $\\mathcal{T}^{i}$ to calculate the $\\pi_{U}$ and the $\\pi_{i n v}$. A common implementing method for encoding trajectory is to use the hidden state of the gated recurrent unit (GRU) [51] $h_{i}$. Because the GRU takes $o_{i}$ and $a_{i}$ as the input recursively for all time-steps $t$, we assume that $h_{i}$ can appropriately represent $\\mathcal{T}^{i}$. Finally, since the UAS guarantees the capability of achieving the global optimality under the parameter-sharing constraint, the CGI loss is calculated by the mean squared error (MSE) loss between $\\mathbf{\\pi}_{i n v}$ and $\\mathbf{\\pi_{i}}$ for all agents with the network parameter $\\psi=\\{\\psi_{1}, ..., \\psi_{n}\\}$, and is written as:\n$\\mathcal{L}_{C G I}(\\pi|h) = \\mathbb{E}_{\\mathcal{D}}(\\pi_{i n v}-\\pi_{i})^{2}$, (7)\nwhere $\\mathbb{E}_{\\mathcal{D}}$ means to sample a batch of tuples $(\\tau, s, r)$ from replay buffer $\\mathcal{D}$ and calculate expectation across the batch. For notational convenience, we also use $\\mathbb{E}_{\\mathcal{D}}$ for PPO-style on-policy methods without distinction. In that case, the replay buffer $\\mathcal{D}$ only consists of the present episode.\nSimilarly, for value-based algorithms, since the action policy is usually calculated by the argmax of Q values, the CGI loss is calculated by the MSE loss between $Q_{i n v}$ and Q for all agents and is written as:\n$\\mathcal{L}_{C G I}(Q|\\psi h) = \\mathbb{E}_{\\mathcal{D}}(Q_{i n v}-Q)^{2}$. (8)\n4.3 Practical Algorithms: U-MAPPO and U-QMIX\nIn this section, we design a policy-based algorithm U-MAPPO and a value-based algorithm U-QMIX as examples to prove the efficiency of the UAS module and CGI loss. As is mentioned in section 3.1, the different parts of network parameters are notated with $\\theta$, $\\phi$, and $\\psi$. The bold fonts of $\\mathbf{\\theta}$, $\\mathbf{\\phi}$, and $\\mathbf{\\psi}$ indicate the usage of global parameter-sharing in the UAS optimization.\n4.3.1 U-MAPPO\nIn order to implement the UAS based on policy-based methods, we combine the UAS with the MAPPO[32] to form the U-MAPPO algorithm. The detailed network structure is illustrated in Fig. 2, and a pseudo-code is given in Algorithm 1.\nThe overall network consists of three different sub-networks: the actor network $\\mathbf{\\theta}(\\mathbf{\\pi}|\\tau)=\\mathbf{\\theta}(\\mathbf{\\pi}_{1}(\\alpha_{1}|\\mathcal{T}_{1}), ..., \\mathbf{\\pi}_{n}(\\alpha_{n}|\\mathcal{T}_{n}))$, the predictor network $\\psi(\\rho^{i n v}|h)=(\\psi_{1}(\\rho^{i n v}|h_{1}), ..., \\psi_{n}(\\rho^{i n v}|h_{n}))$ and the critic network $\\mathbf{\\psi}(V|s)$. These parameters are responsible for the joint action policy $\\mathbf{\\pi_{\\theta}} = (\\mathbf{\\pi_{\\theta_{1}}}, ...., \\mathbf{\\pi_{\\theta_{n}}})$, the joint inverse policy $\\mathbf{\\rho^{i n v}} = (\\mathbf{\\rho_{i n v_{1}}}, ..., \\mathbf{\\rho_{i n v_{n}}})$ and the global value function $V_{\\psi}(s)$, respectively.\nFollowing the implementation of MAPPO, the actor $\\mathbf{\\pi_{\\theta}}$ is shared by all agents and is optimized following the loss function 9. It receives the current observation $o_{t}$, the last action $a_{t-1}$, and the last GRU hidden state $h_{t-1}$ to calculate the next action $a_{t}$. Due to the analysis in section 4.2, we notate the observation-action trajectory as $\\tau = (o_{t}, a_{t-1}, h_{t-1})$.\nThe critic $V_{\\psi}$ is a mapping function $S \\rightarrow \\mathbb{R}$, and is centrally trained following the loss function 10. The additional predictor module $\\mathbf{\\rho^{i n v}}$ is also shared by all agents for generating the inverse policy and is optimized following the CGI loss function 7.\n$\\mathcal{L}(\\mathbf{\\pi_{\\theta}}) = \\mathbb{E}_{D}[\\min(\\frac{\\pi_{\\theta}(\\mathbf{a}|\\tau)}{\\pi_{\\theta_{o l d}}(\\mathbf{a}|\\tau)} A_{\\pi_{\\theta_{old}}}(s, a),\\\\clip(\\frac{\\pi_{\\theta}(\\mathbf{a}|\\tau)}{\\pi_{\\theta_{o l d}}(\\mathbf{a}|\\tau)}, 1 \\pm \\epsilon_{p}) A_{\\pi_{\\theta_{old}}}(s, a))]\\\\ + \\lambda_{e} \\mathbb{E}_{D} [S(\\mathbf{\\pi_{\\theta}}(\\tau))]$. (9)\n$\\mathcal{L}(V_{\\psi}) = \\mathbb{E}_{D}[\\max((V_{\\psi}(s) - R)^{2},\\\\((clip(V_{\\psi}(s), V_{\\psi o l d}(s) \\pm \\epsilon_{V})) - R)^{2})]$. (10)\n$\\mathcal{L}_{U-M A P P O} = \\mathcal{L}(\\mathbf{\\pi_{\\theta}}) + \\lambda_{v}\\mathcal{L}(V_{\\psi}) + \\lambda_{I}\\mathcal{L}(\\mathbf{\\rho^{i n v}})$. (11)\nThe overall loss function of U-MAPPO is 11. The $clip$ operator indicates clipping the value of the first variable to fit in the threshold interval determined by the second function with the hyper-parameter $\\epsilon_{p}$ or $\\epsilon_{V}$. $A_{\\pi_{\\theta}}(s, a)$ is the advantage value computed with the GAE [52] method. $S(\\mathbf{\\pi_{\\theta}}(\\tau))$ is the policy entropy. $R$ is the discounted reward-to-go. $\\lambda_{e}, \\lambda_{v}$, and $\\lambda_{I}$ are\n4.3.2 U-QMIX\nSimilar to the U-MAPPO, we combine the UAS with the QMIX[31] to form the U-QMIX algorithm. The network structure is referred to in Fig. 3, and the pseudo-code of U-QMIX is given in Algorithm 2.\nIn general, there are two parts of sub-networks optimized with different losses. The Q network includes the local Q network $\\mathbf{\\theta}_{i}(Q_{i}|\\tau_{i})= (\\mathbf{\\theta_{1}}(Q_{1}|\\tau_{1}), ..., \\mathbf{\\theta_{n}}(Q_{n}|\\tau_{n}))$ and the mixing network $\\theta_{M}(Q_{i}, s)$. The predictor network is symbolized with $\\psi(Q^{i n v}|h)=(\\psi_{1}(Q^{i n v}|h_{1}), ..., \\psi_{n}(Q^{i n v}|h_{n}))$.\nThe local Q network $\\theta_{i}$ is shared by all agents. It uses the current observation $o_{t}$, the last action $a_{t-1}$ and the last GRU hidden state $h_{t-1}$ to calculate the next Q value $Q_{\\tau}$. Similarly, we notate the observation-action trajectory as $\\tau = (o_{t}, a_{t-1}, h_{t-1})$. Mixing network $\\theta_{M}$ is a global network for value factorization. It takes all local $[Q_{\\theta_{i}}]$ as input, and mixes with the state s to produce the $Q_{tot}$. The loss of $\\theta$ is calculated following 12, where the $Q_{t o t}^{t a r}$ is the target Q value calculated by the target Q network $\\theta_{tgt}$.\nPredictor network $\\psi$ is used to compute $Q^{i n v}$ and is optimized by the CGI loss function 8. The total loss function of U-QMIX is written as 13. Similar to 11, the $\\lambda_{i}$ is the coefficient hyper-parameter responsible for the CGI loss.\n$\\mathcal{L}(Q_{\\theta}) = \\mathbb{E}_{D}[(y^{Q} - Q_{t o t}(\\tau, s; \\theta))^{2}]$,\n$y^{Q} = r+\\gamma \\max _{a^{\\prime}} Q_{t o t}(\\tau^{\\prime}, s^{\\prime}; \\theta^{t g t})$, (12)\n$\\mathcal{L}_{U-Q M I X}=\\mathcal{L}(Q_{\\theta}) + \\lambda_{I}\\mathcal{L}(Q^{i n v}|\\psi)$. (13)\n4.3.3 Summary of Hyper-parameters\nCommonly, we choose Adam [53] as the optimizer. The maximum time-steps of one episode is set to be 200. The reward discounting factor $\\gamma$ is 0.99. We use the latest version 4.10 of the StarcraftII game on the Linux platform to perform experiments.\nFor U-MAPPO method, the learning rate is 5e-4 and the total training step is 10M. We set the threshold interval to be $\\epsilon_{p}=\\epsilon_{V} = 0.2$, and the loss coefficients to be $\\lambda_{e}= 0.01$, $\\lambda_{V} = 1$ and $\\lambda_{I} = 0.8$. The Table 1 summarizes the U-MAPPO hyper-parameters mentioned above.\nFor U-QMIX, the learning rate is 3e-4 and the total training step is 5M. The learning rate is scheduled to decay by multiplying the factor 0.5 every 50,000 episodes (averagely about 2M-3.5M steps). The e of the $\\epsilon$ - greedy action selecting policy starts at 1.0, ends at 0.05 and linearly declines for 50,000 steps. The size of the memory buffer $\\mathcal{D}$ is 5,000 and the batch size is 32. The CGI loss coefficient is set to be $\\lambda_{I}= 0.06$. The Table 2 summarizes the U-QMIX hyper-parameters mentioned above."}, {"title": "5 Experiments and Results", "content": "5.1 Experimental Details\nWe use 5 maps mentioned in [28", "38": "MAPPO [32", "29": "and MAT [39", "31": "ROMA [43", "42": "CDS [44", "34": "UNMAS [45", "28": "are used to conduct experiments.\nIn general", "centralized critic decentralized actor\" (CCDA) architecture, which means that the state s is only available for the critic network to estimate a proper value function V(s), and the actor network is only capable of using the observation-action trajectory $\\tau$ to generate the policy function $\\pi(a|\\tau)$. COMA and MAPPO use a shared actor network for all agents across heterogeneous agent types. HAPPO uses independent actor network parameters and proposes a theoretically monotonic policy-improving architecture with a sequential decision-making scheme. MAT further modifies the network architecture of HAPPO into the Transformer-style, and considers the MARL problem as a sequential decision-making problem to solve.\nQMIX is a fundamental value-based MARL algorithm. Its introduction of monotonic value factorization architecture has been proven to be powerful yet efficient in SMAC environment. RODE and ROMA are both role-based algorithms, which learn and apply role policies end to end. The default training step of ROMA is 20M, and thus it generally performs badly within our 5M training-step restriction. In RODE, several key hyper-parameters define the generation and usage of role policies, which require extra fine-tuning and restrict the performance of RODE. CDS introduces diversity into the optimization and representation of QMIX to overcome the shortage of global parameter-sharing. ASN considers the action semantics of different agents and divides the network to represent and leverage the semantics. UNMAS is capable of adapting to changes in the number and size of MAS with the self-weighting mixing network. GHQ divides agents into different groups using the IOG method and optimizes the networks of these groups with hybrid factorization and IGMI loss.\n5.3 Comparison Results\nThe results of value-based algorithms are shown in section 5.3.1, Fig. 4, and Table 4. The results of policy-based algorithms are shown in section 5.3.2, Fig. 5, and Table 5. The figures show the WR curves changing over time and the tables show the final WR values followed by the standard deviations. The results are averaged across 5 individual tests.\nWe criticize the performance of all algorithms from 3 aspects and the analysis is given below. Generally, all comparison algorithms are not specifically modified to deal with the global parameter-sharing dilemma in heterogeneous MARL problems, therefore they fail to achieve high WR with small variance.\n5.3.1 Results of Value-based Algorithms Comparison\n(1) Easy": "we test all algorithms on the original asymmetric heterogeneous map MMM2. The results are shown in Fig. 4 (a). U-QMIX and RODE converge to 1.0 at about 3M steps. The WR curve of RODE increases faster than U-QMIX before 2M steps but suffers from high variance. QMIX and GHQ converge to about 0.8 at 2M steps. CDS also converges to 0.8 but requires 5M training steps. The WR curve of ASN and UNMAS can only converge to 0.5 with high variance. ROMA is only capable of acquiring 0.1 at 5M training steps.\n(2) Medium: we scale up ally"}]}