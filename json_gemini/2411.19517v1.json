{"title": "RL-MILP Solver: A Reinforcement Learning Approach\nfor Solving Mixed-Integer Linear Programs with Graph Neural Networks", "authors": ["Tae-Hoon Lee", "Min-Soo Kim"], "abstract": "Mixed-Integer Linear Programming (MILP) is an optimiza-\ntion technique widely used in various fields. Primal heuris-\ntics, which reduce the search space of MILP, have enabled tra-\nditional solvers (e.g., Gurobi) to efficiently find high-quality\nsolutions. However, traditional primal heuristics rely on ex-\npert knowledge, motivating the advent of machine learning\n(ML)-based primal heuristics that learn repetitive patterns\nin MILP. Nonetheless, existing ML-based primal heuristics\ndo not guarantee solution feasibility (i.e., satisfying all con-\nstraints) and primarily focus on prediction for binary deci-\nsion variables. When addressing MILP involving non-binary\ninteger variables using ML-based approaches, feasibility is-\nsues can become even more pronounced. Since finding an\noptimal solution requires satisfying all constraints, address-\ning feasibility is critical. To overcome these limitations, we\npropose a novel reinforcement learning (RL)-based solver\nthat interacts with MILP to find feasible solutions, rather\nthan delegating sub-problems to traditional solvers. We de-\nsign reward functions tailored for MILP, which enables the\nRL agent to learn relationships between decision variables\nand constraints. Additionally, to effectively model complex\nrelationships among decision variables, we leverage a Trans-\nformer encoder-based graph neural network (GNN). Our ex-\nperimental results demonstrate that the proposed method can\nsolve MILP problems and find near-optimal solutions without\ndelegating the remainder to traditional solvers. The proposed\nmethod provides a meaningful step forward as an initial study\nin solving MILP problems end-to-end based solely on ML.", "sections": [{"title": "1 Introduction", "content": "The Traveling Salesman Problem and the Knapsack Problem\nare representative examples of combinatorial optimization\n(CO) problems that have been extensively studied in opera-\ntions research and computer science. CO addresses mathe-\nmatical optimization that aim to minimize or maximize the\nvalue of a specific objective function. If the objective func-\ntion and constraints of CO are linear, it is called linear pro-\ngramming (LP). If some decision variables in LP must take\ninteger values, it becomes Mixed-Integer Linear Program-\nming (MILP) (Bengio, Lodi, and Prouvost 2021). MILP is\nwidely used to model various problems (Zhang et al. 2023)\nsuch as logistics (Kweon et al. 2024), path planning (Zuo\net al. 2020), and energy systems (Ren and Gao 2010).\nInfeasibility caused by incorrect ML predictions can be\nmore pronounced for integer variables due to their broader\nprediction range than binary variables. For instance, imagine\na naive ML approach that uses regression to predict the value\nof an integer variable $x_i$ within the range [0, 1000]. The max-\nimum possible prediction error for a binary variable, with a\nrange of [0, 1], is 1. However, since the range of $x_i$ is signif-\nicantly broader, the maximum prediction error for $x_i$ would\nalso be broader. Moreover, additional errors can occur due\nto rounding that makes the predicted values integers. There-\nfore, a more accurate ML-based method for integer vari-\nables is required. One study (Nair et al. 2020) proposed a\nmethod to handle integer variables by representing the val-\nues in binary format. To represent $x_i$ as a binary sequence,\nthe required sequence length would be $\\lceil log_2(1000) \\rceil = 10$.\nThus, predicting a single decision variable requires multiple\ndimensions, and the upper/lower bounds of a variable may\neven be infinite in some cases. This highlights the need for\nmore efficient methods to handle integer decision variables.\nTo address these limitations, we propose an RL-based\nmethod for solving MILP. Figure 1(b) illustrates how the\nproposed solver derives the optimal solution for an MILP\ninstance. Unlike ML-based primal heuristics which delegate\nsub-problems to traditional optimizers, the proposed method\ntakes an MILP instance and directly searches for complete\nfeasible solutions. To focus on handling integer variables,\nwe consider a special case of MILP where all variables are\nintegers. Rather than directly predicting the exact values of\ninteger variables, we adopt an indirect approach where the\nML model determines whether to increase, decrease, or re-\ntain the current value of each decision variable.\nTo guide these decisions, we design an RL system tailored\nto MILP, enabling the agent to effectively learn the relation-\nships between decision variables and constraints. The solu-\ntion search process is divided into two phases: before and\nafter finding the first feasible solution of MILP. We design\nreward functions for each phase to align with their objec-\ntives. During training, the RL agent is updated based on the\ndegree of constraint violations and improvements in the ob-\njective value. To capture the relationships among all decision\nvariables effectively, we adopt a Transformer encoder-based\nGNN as the agent's architecture. The trained agent processes\nunseen MILP instances and incrementally improves the so-\nlution using the proposed local search strategy. Experimen-\ntal results demonstrate that the proposed model achieves op-\ntimal solutions on a small dataset and finds near-optimal so-\nlutions with roughly 1% of the optimal for a larger dataset.\nOur main contributions are summarized as follows:\n\u2022 We propose a novel RL-based MILP solver (RL-MILP\nsolver) capable of finding complete feasible solutions,\nrather than a partial solution.\n\u2022 We design an RL system that enables the agent to effec-\ntively learn the relationships between decision variables\nand constraints by interacting with MILP problems.\n\u2022 We propose a Transformer encoder-based GNN as the\nagent's architecture, designed to effectively capture the\ncomplex relationships among decision variables in MILP."}, {"title": "2 Preliminaries", "content": "2.1 Mixed-Integer Linear Programming\nMixed-Integer Linear Programming (MILP) is a mathemat-\nical optimization problem that minimizes (or maximizes) a\nlinear objective function while satisfying linear constraints\nand the integrality requirements for some decision variables\n(Bertsimas and Tsitsiklis 1997). The standard form of MILP\nis as follows:\n$\\begin{aligned}\n\\text{minimize} \\quad & c^T x  \\\\\n\\text{subject to} \\quad & Ax \\leq b \\\\\n& x_i \\in \\mathbb{Z}, \\forall i \\in I \\\\\n& l_i \\leq x_i \\leq u_i, \\forall i\n\\end{aligned}$\n(1)\n(2)\n(3)\n(4)\nwhere $x \\in \\mathbb{R}^n$ is a column vector of $n$ decision variables,\n$c \\in \\mathbb{R}^n$ is a column vector of coefficients for the objective\nfunction, $A \\in \\mathbb{R}^{m \\times n}$ is the constraint coefficient matrix,\n$b \\in \\mathbb{R}^m$ is a column vector of the right-hand side of the\nconstraints, $I$ is the index set of integer decision variables,\n$l_i \\leq u_i$ denotes the lower/upper bounds for each decision vari-\nable $x_i$. The goal of MILP is to find the optimal solution,\nand for a minimization problem, this corresponds to a fea-\nsible solution $x$ that minimizes $obj = c^Tx$. A feasible so-\nlution is defined as a solution $x$ that satisfies all constraints\n(Eqs. 2-4). Integrality requirements (Eq. 3) make the solu-\ntion space of MILP discrete. As the number of integer vari-\nables increases, the possible combinations grow exponen-\ntially, increasing the difficulty of solving MILP within poly-\nnomial time. LP-relaxation refers to the technique of remov-\ning the integrality requirement in MILP, which transforms\nit into an LP problem solvable in polynomial time (Bertsi-\nmas and Tsitsiklis 1997). LP-relaxation is widely used in tra-\nditional algorithms, such as Branch-and-Bound (Land and\nDoig 2010) and Feasible Pump (Fischetti, Glover, and Lodi\n2005), to obtain an initial solution for MILP.\nAll MILP problems can be transformed into the form\nshown in Equations 1-4 (Bertsimas and Tsitsiklis 1997).\nLet $a_i^T$ denote a row vector of a single constraint, $A =$\n$(a_1^T, ..., a_m^T)$, and $b = (b_1,...,b_m)$. An equality con-\nstraint $a_i^Tx = b_i$ is equivalent to two inequality constraints\n($a_i^Tx \\geq b_i$ and $a_i^Tx \\leq b_i$). Moreover, $a_i^Tx \\geq b_i$ is equiva-\nlent to $-a_i^Tx \\leq -b_i$. Similarly, maximizing $c^Tx$ is equiva-\nlent to minimizing $-c^Tx$. For example, a problem with the\nobjective function maximizing $+2x_1 + 3x_2 - x_3$ can be re-\nformulated as minimizing $-2x_1 - 3x_2 + x_3$, and the con-\nstraint $-x_1 + 2x_3 \\geq -5$ can be rewritten as $x_1 - 2x_3 \\leq 5$.\nThus, regardless of the set of constraints or the optimiza-\ntion direction, all cases can be transformed into the standard\nform given in Equations 1-4. Consequently, we focus only\non solving MILP problems that follow this standard form."}, {"title": "2.2 Graph Representation of MILP", "content": "Studies on ML-based primal heuristics, which assist tradi-\ntional optimizers (e.g., Gurobi, SCIP), represent MILP in-\nstances as bipartite graphs (Gasse et al. 2019; Nair et al.\n2020; Yoon 2022; Han et al. 2023; Cant\u00fcrk et al. 2024). In\na bipartite graph representation of MILP, one set of nodes\nis for constraints, and the other is for decision variables. An\nedge connects a decision variable node to a constraint node\nonly if the variable appears in the corresponding constraint.\nFor example, in Figure 2(a), the decision variable $x_3$ appears\nin the constraint $a_2$. Therefore, in the MILP bipartite graph,\nthe variable node for $x_3$ is connected to the constraint node\nfor $a_2$."}, {"title": "2.3 Graph Neural Networks", "content": "Message Passing Neural Network The Message Passing\nNeural Network (MPNN) (Gilmer et al. 2017) is a general\nframework for message-passing-based GNNs. Widely used\narchitectures such as GCN (Kipf and Welling 2017), GAT\n(Veli\u010dkovi\u0107 et al. 2018), and GIN (Xu et al. 2019) are GNN\narchitectures based on MPNN. Given a graph $G$, the new\nrepresentation $h_v^{(k+1)}$ for a target node $v$ is obtained as fol-\nlows:\n$\\begin{aligned}\nm_{vu}^{(k+1)} &= msg(h_v^{(k)}, h_u^{(k)}, e_{vu}), \\forall e_{vu} \\in E \\\\\ng^{(k+1)} &= agg(\\lbrace m_{vu}^{(k+1)} | u \\in N(v) \\rbrace), \\forall v \\in \\nu \\\\\nh_v^{(k+1)} &= update(h_v^{(k)}, g^{(k+1)}), \\forall v \\in \\nu\n\\end{aligned}$\n(5)\nwhere $h_v^{(0)}$ is the initial feature vector of node $v$, and $msg(\\cdot)$,\n$agg(\\cdot)$, and $update(\\cdot)$ are the message-passing, aggregation,\nand update functions, respectively. $msg(\\cdot)$ generates a mes-\nsage $m_{vu}^{(k+1)}$ using the representation of target node $h_v^{(k)}$, the\nneighbor node $h_u^{(k)}$, and the edge feature $e_{vu}$. $agg(\\cdot)$ aggre-\ngates the messages $m_{vu}^{(k+1)}$ generated by $msg(\\cdot)$ for each\ntarget node. $update(\\cdot)$ updates the target node $v$ to a new\nrepresentation $h_v^{(k+1)}$ by combining the aggregated informa-\ntion $g^{(k+1)}$ with the previous embedding $h_v^{(k)}$. The embed-\nding $h_v^{(k+1)}$ is either used for message passing in the next\nlayer or for prediction tasks.\nSince edges exist only between nodes from different sets\n(i.e., variable-constraint) in an MILP bipartite graph, obtain-\ning a new representation for a decision variable requires two\nrounds of message passing. As shown in Figure 2(b), a new\nrepresentation for the decision variables is obtained by per-\nforming one constraint-side convolution and one variable-\nside convolution after applying initial embeddings.\nTransformer for Graphs Graph Transformers are mod-\nels that extend the Transformer architecture (Vaswani 2017)\nto handle graph data. Originally designed as a sequence-to-\nsequence model for machine translation, the Transformer\nhas achieved significant success in various domains such\nas NLP and computer vision. Recently, there have been nu-\nmerous efforts to adapt Transformers for the graph domain\n(Lin et al. 2022; Min et al. 2022). The attention mechanism\nof Transformers enables each node to attend to every node,\nwhich allows the model to effectively learn relationships be-\ntween distant nodes (Wu et al. 2021).\nIn contrast, MPNN-based GNNs receive messages from\nthe neighbor node, which is suitable for learning local struc-\ntural information. However, they struggle to capture rela-\ntionships between distant nodes (Zhang et al. 2020; Wu et al.\n2021). To propagate messages from nodes that are $K$ hops\naway, an MPNN-based GNN requires $K$ layers. However,"}, {"title": "3 Methodology", "content": "This section provides a detailed introduction to our RL-\nMILP solver. Figure 3 demonstrates how the proposed solver\nincrementally improves feasible solutions. The first step in-\nvolves transforming the MILP instance in its standard form\n(Eqs. 1-4) into a bipartite graph. The MILP bipartite graph\nserves as input to the GNN-based agent. In the second step,\nthe agent trained with an RL algorithm selects actions es-\ntimated to yield high rewards. The agent decides for each\nvariable whether to increase, decrease, or retain its value. In\nthe third step, the selected actions are applied to derive a so-\nlution for the given instance. The final solution is updated\nif the new feasible solution $x$, obtained from the selected\nactions, is better than the current best feasible solution."}, {"title": "3.1 Reinforcement Learning for MILP", "content": "In our scenario, RL aims to train the agent to make deci-\nsions that maximize rewards while interacting with a given\nMILP instance. Figure 4 illustrates how the RL agent in-\nteracts with an example MILP instance, where $S_t$, $A_t$, and\n$R_t$ denote the observation, the set of actions, and reward at\ntimestep $t$, respectively. The given instance $M$ acts as the en-\nv"}]}