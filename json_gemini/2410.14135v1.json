{"title": "Inverse Reinforcement Learning from Non-Stationary Learning Agents", "authors": ["Kavinayan P. Sivakumar", "Yi Shen", "Zachary Bell", "Scott Nivison", "Boyuan Chen", "Michael M. Zavlanos"], "abstract": "In this paper, we study an inverse reinforcement learning problem that involves learning the reward function of a learning agent using trajectory data collected while this agent is learning its optimal policy. To address this problem, we propose an inverse reinforcement learning method that allows us to estimate the policy parameters of the learning agent which can then be used to estimate its reward function. Our method relies on a new variant of the behavior cloning algorithm, which we call bundle behavior cloning, and uses a small number of trajectories generated by the learning agent's policy at different points in time to learn a set of policies that match the distribution of actions observed in the sampled trajectories. We then use the cloned policies to train a neural network model that estimates the reward function of the learning agent. We provide a theoretical analysis to show a complexity result on bound guarantees for our method that beats standard behavior cloning as well as numerical experiments for a reinforcement learning problem that validate the proposed method.", "sections": [{"title": "I. INTRODUCTION", "content": "Understanding the intentions of autonomous agents [1] has important implications in a variety of applications, ranging from cyber-physical systems [2] to strategic games [3], [4]. If an agent follows a fixed policy, its intentions remain consistent and can be learned by sampling a sufficiently large number of state-action pairs [5], [6]. However, in practice, sampling enough trajectories to learn a fixed optimal policy can be time consuming. Moreover, from an observer's perspective, it is not always easy to determine whether the observed agent's policy is fixed or not. Therefore, it is important to be able to estimate an agent's intentions while this agent is interacting with its environment to learn its optimal policy and before the policy has converged.\nDoing so using only state-action data is not straightforward. A common way to predict the intentions of an agent in deep reinforcement learning is assuming the policy weights are known. If the policy weights are known during learning, the distribution of agent actions at any given state can also be estimated [7]. Nevertheless, assuming that the structure of the agent's policy is known is not ideal in practice [8]. Inverse reinforcement learning (IRL) instead aims to understand an agent's intentions by learning their reward function [9]. A common approach in existing IRL literature is to use the maximum entropy to select a distribution of actions at a state that matches an expected distribution given observations of state-action pairs. By maximizing the entropy, one can account for all the sampled distributions of state-action pairs at highly visited states, thus making it more reliable to recover the reward function as no additional assumptions are made on the behavior at less visited states [10], [11].\nAn important common assumption in the IRL literature discussed above is that the trajectories used to learn the reward functions are generated from a stationary expert policy. This assumption does not hold when trajectory data are collected as the agent interacts with its environment to learn its optimal policy. In this case, existing IRL methods are not straightforward to apply. To address this challenge, [12] proposes a way to learn an agent's reward function from data generated by this agent's non-expert policy while this policy is being updated and assuming that policy updates are done using stochastic gradient descent. However, the theoretical analysis of the approach in [12] requires that the reward function of the learner agent is approximated by a set of linear feature functions. In practice, selecting an expressive enough set of feature functions is challenging. For example, even in the case of radial basis functions that are commonly used as feature functions, choosing proper centers and bandwidth parameters is not straightforward [13]. In IRL, these parameters must be chosen to ensure that each state action pair has a unique feature and that no state action pairs are favored over others. If these criteria are not met, the learned reward function may be skewed towards the state action pairs that are favored more.\nMotivated by the approach in [12], in this paper we propose an IRL method to learn the reward function of a learning agent as it learns its optimal policy. As in [12], we assume that the agent updates its policy using stochastic gradient descent, but we do not assume that the agent's reward function is approximated by linear feature functions; it can be approximated by any nonlinear function, e.g., a neural network. Since the agent does not share its policy parameters explicitly, the challenge lies in estimating the agent's policy parameters as they are being updated during learning. To do so, we propose a new variant of behavior cloning, which we term bundle behavior cloning, that uses a small number of trajectories generated by the learner agent's policy at different points in time to learn a set of policies for this agent that match the distribution of actions observed in the sampled trajectories. These cloned policies are then used to train a neural network model that estimates the reward function of the learning agent. We provide error bounds of the policies learned by bundle behavior cloning and standard behavior cloning that allow us to choose an appropriate bundle size which validates bundle behavior cloning as a superior method for the problem we discuss. Moreover, we present numerical experiments on simple motion planning problems that show that the proposed approach is effective in learning the reward function of the learning agent. In comparison, empirical results presented in [12] assume that the agent's policy parameters are known so that behavior cloning is not needed to estimate them."}, {"title": "II. PROBLEM DEFINITION", "content": "Consider an agent with state space S and action space A. We model this system as a finite Markov Decision Process (MDP), defined as $M = (S, A, P, \\gamma, R)$, where $P(S_{t+1} = s_{t+1} | S_t = s_t, A_t = a_t)$ is the transition function defined over the agent's state $s_t \\in S$ and action $a_t \\in A$, $\\gamma \\in (0, 1]$ is the discount factor, and $R(s_t, a_t, S_{t+1})$ is the reward received when the agent transitions from state $s_t$ to state $s_{t+1}$ using the action $a_t$. Let also $\\pi_{\\rho}(a_t | S_t) \\rightarrow [0, 1]$, denote the agent policy, which is the probability of choosing an action $a_t$ at state $s_t$, and is parameterized by the policy parameter $\\theta$. The objective of the agent is to maximize its accumulated reward, i.e.,\n$\\max_{\\theta} J(\\theta) = E[\\sum_{t=0}^{T} R(s_t, a_t, S_{t+1})]$,\nwhere $p_u$ is the initial state distribution of the agent. In what follows, we assume that the agent does not share its policy parameters or rewards. Then, in this paper, we address the following problem:\nProblem 1: (Inverse reinforcement learning from a learn-ing agent) Learn the reward function of a learning agent using only trajectories of state action pairs $\\mathcal{T}= [(s_0, a_0), ..., (s_T, a_T)]$ generated as the agent interacts with its environment to learn its optimal policy $\\pi_{\\theta}$ that solves (1).\nTo solve Problem 1 we make the following assumption.\nAssumption 1: The agent updates its policy using the stochastic gradient descent (SGD) update $\\theta_{t+1} = \\theta_t + \\alpha \\nabla_{\\theta} J$. We note that many popular policy gradient methods use stochastic gradient descent, e.g., REINFORCE, Actor-Critic, etc. Specifically, in this paper, we use the REINFORCE algorithm to update the policy parameters of agent i as\n$\\theta_{t+1} = \\theta_t + \\alpha \\gamma R(s_{t+1}) \\nabla_{\\theta} \\ln \\pi(a_t | s_t, \\theta)$,\nwhere $\\theta$ is the policy parameter of the agent, $\\alpha$ is the learning rate, and we denote $R(s_{t+1}) = R(s_t, a_t, S_{t+1})$. We call this the forward learning problem and make the following assumption on the learning rate.\nAssumption 2: The learning rate of the agent $\\alpha$ is known and is sufficiently small.\nFinally, we make the following assumption on the infor-mation that is available in order to learn the reward of the agent.\nAssumption 3: All states and actions of the agent in the environment can be observed.\nThe main challenge with solving Problem 1 is that without the policy parameters of the agent at every timestep, it is difficult to recover $R(s_{t+1})$. In the next section, we detail how we can recover $R(s_{t+1})$ using IRL and a new variant of behavior cloning that we propose, that we call bundle behavior cloning, to estimate the agent's policy."}, {"title": "III. METHOD", "content": "Assuming it is known that the learning agent uses the update (2) to update its policy parameters, to recover the reward at a state $s_{t+1}$ we need to know the policy parameters $\\theta_t$ and $\\theta_{t+1}$, as well as $\\nabla \\ln \\pi(a_t | S_t, \\theta)$. As the policy structure, e.g., the structure of the policy neural network, and policy weights of the agent are considered unknown, it is not straightforward to estimate these policy parameters, or the gradient of the objective function. Here, we propose a novel variant of behavior cloning to learn these quantities, using only trajectory data collected during learning.\nBehavior cloning is a supervised learning method used to learn a policy $\\pi_{\\theta}$ from a set of trajectories $\\{\\mathcal{T}\\}$ sampled from an expert policy. A number of applications ranging from autonomous driving [14], video games [15], and traffic control [16] have relied on behavior cloning to learn the desired expert policies. Generally, the assumption in these works is that the expert policy used to generate the data is stationary. However, if the expert policy changes as in this paper, it is not easy to assign trajectory data $\\{\\mathcal{T}\\}$ consisting of state and action pairs $\\tau = \\{(s_0, a_0), (s_1, a_1), ..., (s_t, a_t)\\}$, to specific policies $\\pi_{\\theta}$. We address this challenge by instead bundling together trajectories and learning a single cloned policy for each bundle. Given Assumption 2, if the learning rate $\\alpha$ is small enough, then consecutive policy parameters $\\theta_{t+1}$ and $\\theta_t$ will be close to each other. Therefore, for an appropriate bundle size, it is reasonable to expect that the trajectories in each bundle are generated by approximately the same policy. We call the proposed method Bundle Behavior Cloning, which we describe below.\nWe denote the cloned policy corresponding to the kth bundle $b_k$ in the set $[b_1, ..., b_M]$ by $\\pi_{\\psi_k}$, where $\\psi$ is the policy parameter. We assume that every bundle contains B trajectories. Given a total number of training episodes E from forward learning, the total number of bundles is defined as $M = E - B + 1$ if a sliding window is used to construct the bundles, or M = E/B rounded up to the nearest integer for non-overlapping sets of episodes. Here, the size of each bundle B constitutes a hyperparameter. For each bundle $b_k$ consisting of B trajectories, our goal is to generate a distribution of the agent's actions for each state denoted by $p_{b_k}(s)$ that approximates the distribution of the agent's actions in the policy we wish to clone. Then, to train the cloned policy we rely on the Mean Squared Error (MSE) loss\n$loss(\\psi_k) = \\sum_{s \\in S} MSE(\\pi_{\\psi_k}, p_{b_k} (S))$.\nThe proposed bundle behavior cloning method is outlined in Algorithm 1. Specifically, line 5 in Algorithm 1 adds the loss over all the states in the state space so that the kth cloned policy parameter $\\psi_k$ can accurately imitate the true agent policy parameters $\\theta$ during the episodes."}, {"title": "IV. COMPLEXITY ANALYSIS", "content": "In this section, we provide a sample complexity result and show that under certain conditions bundle behavior cloning achieves tighter complexity bounds than conventional behavior cloning. Specifically, given a set of trajectories of state-action pairs $\\{\\tau^1, ..., \\tau^B\\}$, where $\\tau^i$ represents the trajectory under the fixed policy $\\pi^i$, we show that bundle behavior cloning can be used to estimate the true policy $\\pi^1(\\cdot | s)$ of the agent at time step 1 using $\\hat{\\pi}^{1:B}(\\cdot | s)$, the estimated policy that best fits the distribution sampled from the bundle of trajectories $\\{\\tau^1, ..., \\tau^B\\}$ from timestep 1 to timestep BT and T is the number of samples in one trajectory. Note that we only provide the analysis for $\\pi^1$ and the results below hold for all time steps t.\nObserve that if the polices in the bundle are significantly different from each other, we do not expect a larger bundle size to facilitate policy estimation. As a result, we make the following assumptions:\nAssumption 4: For every consecutive policy pair, there exists a constant $\\epsilon$ such that for any state s, the following inequality holds:\n$||\\pi^i(s) - \\pi^{i+1}(s)||_{TV} \\le \\epsilon,$\nwhere the total variation norm between two distributions $\\mu$ and $\\nu$ is defined as $TV(\\mu, \\nu) := \\sup_{A \\in \\mathcal{B}} |\\mu(A) - \\nu(A)|$ [17], where B denotes the class of Borel sets.\nThe state visitation $d^{\\pi}$ for a fixed policy $\\pi$ is defined as $d^{\\pi} = (1 - \\gamma) \\sum_{t=0}^{\\infty} \\gamma^t Pr^{\\pi} (s_t = s)$, where $Pr^{\\pi}(s_t = s)$ is the probability that $s_t = s$ after starting at state $s_0$ and following $\\pi$ thereafter. The single policy behavior cloning sample complexity has been studied in [18] and is presented below.\nLemma 1 (Theorem 21 in [18]): With probability at least $1 - \\delta$, we have that\n$E_{s \\sim d^{\\pi}} ||\\pi(\\cdot | s) - \\hat{\\pi}(\\cdot | s)|| \\le \\sqrt{\\frac{2 \\ln(|\\Pi| / \\delta)}{T}},$"}, {"title": "V. EXPERIMENTAL RESULTS", "content": "In this section, we demonstrate the proposed IRL algorithm on a Gridworld environment, where the learning agent seeks to reach a goal state. The environment is a 7 by 7 grid, with the agent starting at the top left square. It has three possible actions, moving to the right, moving to the left, and moving down. The agent receives the maximum reward of 20 when it reaches the goal state, located at the bottom right square in the grid, but different squares on the grid result in different, negative rewards for the agent ranging from -5 to -2. All experiments are run using PyTorch [20] on a Windows system with an RTX 3080 GPU.\nWe use our proposed IRL method with bundle behavior cloning to learn the reward function of the agent. The forward policy is modeled by a 2 layer neural network with 16 hidden nodes and ReLU activation functions on the first layer. The neural network used to clone the policies is the same as that in the forward case. The bundle has size B = 15. There are M = 333 total bundles defined across 5000 trajectories and each trajectory consists of 15 timesteps. As a result, each bundle contains 225 state action pairs. The learning rate used during training of the forward and cloned policies is 0.00075 and $\\gamma$ is set to 0.999. The neural network used to model the estimated reward $\\beta$ is a 2 layer neural network with 20 hidden nodes with a ReLU activation function on the first layer. The batch size used to train $\\beta$ is 100 and the total number of training episodes to is 5000.\nAs neural networks' weights are trained stochastically [21], we use only the last layer of weights from the cloned policies to train $\\beta$ in order to minimize the variance in estimated gradients from layers before the output layer. The last layer in a neural network is used to output the distribution of actions, and thereby we can minimize the effect of multiple combinations of neural network weights that could output the same estimated distribution.\nTo compare the reward $\\beta$ learned using bundle behavior cloning and the proposed IRL method to the true agent reward, we first normalize and scale the reward $\\beta$. The recovered reward function is able to localize the goal state in the environment as well as identify states on the optimal path.\nTo further validate the predicted rewards, we use them to train a new policy which we then compare to the optimal forward policy learned from the true reward function. The learned policy is able to produce the same optimal trajectory as the forward optimal policy and achieves the same possible reward."}]}