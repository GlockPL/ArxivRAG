{"title": "Difficulty Estimation and Simplification of French Text Using LLMs", "authors": ["Henri Jamet", "Yash Raj Shrestha", "Michalis Vlachos"], "abstract": "We leverage generative large language models for language learning applications, focusing on estimating the difficulty of foreign language texts and simplifying them to lower difficulty levels. We frame both tasks as prediction problems and develop a difficulty classification model using labeled examples, transfer learning, and large language models, demonstrating superior accuracy compared to previous approaches. For simplification, we evaluate the trade-off between simplification quality and meaning preservation, comparing zero-shot and fine-tuned performances of large language models. We show that meaningful text simplifications can be obtained with limited fine-tuning. Our experiments are conducted on French texts, but our methods are language-agnostic and directly applicable to other foreign languages.", "sections": [{"title": "1 Introduction", "content": "Today, there exist many online foreign-language learning tools, including Duolingo, Frantastique, and ReadLang which incorporate gamification elements to increase appeal and retention [25]. However, they do not offer personalization based on learners' interests nor do they provide relevant, contemporary content. Oftentimes, the educational content fails to be matched to the learners knowledge level. Content that is too easy bores learners; too difficult discourages them.\nIn this work, we use large language models (LLMs) to estimate and potentially reduce the difficulty of foreign content. Such an approach, can be used to identify appropriate contemporary content in the target learning language, thus increasing learners' motivation. Moreover, such an approach could be used in conjunction with a recommendation system to discover content appropriate for the learner's knowledge level. The contributions of this work include:\nA machine learning solution to estimate foreign text difficulty, more accurate than traditional readability metrics.\nUse of fine-tuned LLMs to simplify the difficulty/level of a foreign French text while preserving its meaning as well as possible. We propose a technique for automatically assessing simplification quality and report performance benchmarks."}, {"title": "2 Difficulty Estimation", "content": ""}, {"title": "2.1 Related work", "content": "From a pedagogical perspective, our solution builds upon already established theory of extensive reading [7,15], which constitutes a crucial means of reinforcing one's language skills. However, it is important to find content that is appropriate for the learner's knowledge level of the foreign language. One approach to estimating the difficulty of a foreign language text is to use \"readability\" formulas\u00b9. They are regression approaches that calculate the complexity of a text based on various text features, including sentence length and word frequency. Some commonly used readability formulas for foreign language texts include the Flesch-Kincaid Grade Level, the Simple Measure of Gobbledygook (SMOG), and the Gunning Fog Index.\nAnother approach to tackle difficulty estimation is to use machine learning techniques to predict the difficulty level based on various linguistic features [5,10]. A particularly notable advancement in this field in recent years is the integration of pre-trained word- and sentence- embeddings into text readability architectures [27,12,16,11]. However, our review indicates that prior research has not yet investigated the predictive accuracy of difficulty estimation using LLMs."}, {"title": "2.2 Our approach", "content": "We model the estimation of difficulty as a classification problem. Let D be the set of documents, Y the random variable representing the linguistic difficulty class. For the linguistic difficulty, we wish predict the Common European Framework of Reference for Languages, or CEFR difficulty level of a text {A1, A2, B1, B2, C1, C2}), where A1,A2 signify easy text, B1,B2 intermediate level and C1,C2 advanced level. \\(f : D \\rightarrow Y\\) is a classifier mapping a document \\(d \\in D\\) to a difficulty class \\(y \\in Y\\). The classifier \\(f\\) is built using text-label pairs, where the label corresponds to the linguistic difficulty. We use LLMs like BERT [8], GPT [20], GPT-3 [2], LLaMa [24], and Palm [6], which convert text tokens into embeddings capturing the meaning of each token. The models vary in data volume, training methodology, size, and language capabilities, resulting in embeddings of different lengths (e.g., 768 for BERT, 1,536 for OpenAI's \"ada-002\").\nGPT-3+ Models: Advanced models like GPT-3 and GPT-4 [20] have been trained on massive multilingual datasets and post-trained using reinforcement learning from human feedback. They achieve state-of-the-art performance across tasks like summarization, translation, and question-answering [3]. These models excel at generating human-like text and can be fine-tuned for classification tasks. We use GPT models for their good performance-versus-cost ratio."}, {"title": "3 Text Simplification", "content": "In the context of a foreign language learning app, we now explore the scenario assuming we have discovered content that matches the user's interests (such as, sports or politics), but it is more advanced than their current language proficiency level.\nEvaluating text simplification systems is a non-trivial task that requires metrics robust enough to account for both readability and semantic preservation. Traditional n-gram based metrics such as BLEU or ROUGE, though popular in translation tasks, have limitations when applied to text simplification due to the fundamental requirement of simplification to alter the text structure while maintaining the same meaning [1] [22]. The SARI metric, designed specifically for simplification tasks, measures the goodness of words added, deleted, and kept by the system. Current research, such as [22], is exploring new methods like QUESTEVAL, which uses semantic questioning of texts to assess simplifications. This approach aims to overcome the limitations of earlier metrics and may result in improved alignment with human evaluations.\nWhile text summarization and text simplification may appear similar as both involve altering the original text, their objectives are distinct. Summarization aims to condense a text by trimming down its length and retaining only the main points. Simplification seeks to lower the linguistic complexity, making content more accessible [22]. Simplification may or may not alter the length of text. Unlike summarization, simplification is deals with adjusting the text to a particular knowledge or comprehension level.\nThe hybrid task of combining both text summarization and simplification has been investigated in the context of generating summaries for lay audiences. This task known as \"lay summarization\" aims to render complex scientific content accessible to a general audience, requiring the extraction of key points and simultaneously a reduction in linguistic complexity [26,4]. Transformer models based on BERT [8] and PEGASUS [29] have been used in this context.\nRecent advancements in LLMs, have greatly contributed to the field of text simplification [21]. However, the efficacy of these models varies, and not all are equally suited for the task, calling for evaluation and fine-tuning on specialized datasets [23]. A recent analysis by [9] highlights that when evaluated on public datasets, contemporary LLMs like GPT-3 can match or even outperform systems explicitly designed for text simplification."}, {"title": "3.1 Our approach", "content": "We model simplification in a similar way as for content difficulty estimation. However, instead of predicting the level of difficulty, we predict the simplified sentence token by token. We provide labelled examples of original and simplified sentences and train a machine learning model. Because we only fine-tune LLMs, we need to provide very few pairs of examples, since the LLMs have already encapsulated in them large amounts of textual knowledge. In our experiments, we only post-train the LLMs using 125 pair sentences, and show large"}, {"title": "4 Experiments", "content": "The goal of the following experiments is to demonstrate that (a) fine-tuned LLMs can significantly improve the difficulty estimation offered by traditional readability metrics. (b) simplification methodology driven by fine-tuned LLM models outperforms zero-shot approaches. The code for the experiments can be retrieved here"}, {"title": "4.1 Difficulty estimation", "content": "We evaluated the difficulty estimation on three labeled datasets: 1) Litt\u00e9rature de jeunesse libre (LjL) which we obtained from [11]. Each content item here contains several sentences and a label (labels: levell, level2, level3, level4). 2) A collection of sentences collected from the Internet (sentencesInternet). Each of these sentences was then annotated by at least three annotators (students recruited in our university) in difficulty levels. Only sentences in which all participants agreed on the difficulty annotation were retained (labels: A1,A2,B1,B2,C1,C2). Here, the labels correspond to the levels designed by the Common European Framework of Reference for Languages (CEFR). 3) A collection of sentences from literature books (sentencesBooks). Each book was annotated with a difficulty level by a Professor of French. All sentences in that book were then given that label. This process involved an OCR pipeline which could lead to faulty detection of characters, so only the sentences without any errors were retained. (labels: A1, A2,B1,B2,C1,C2). The characteristics of these datasets are provided in Table 1. To train and evaluate our model, we used an 80/20 train-test split, and the results that we present are for examples which the model saw for the first time.\nEvaluation. As a simple benchmarking, we compare the accuracy of our difficulty estimation approach to traditional readability-based metrics, such as the GFI (Gunning Fog Index), FKGL (Flesch Kincaid grade level), ARI (Automated Readability Index) in Table 2. Initially, all of these metrics have been developed for English content, but language specific models, such as for French text, as used in our application, have also been developed [10].\nThese techniques are inherently regression techniques and output a floating point value of the text difficulty. As a result, we cannot make a direct comparison, because our difficulty estimator predicts discrete labels. To address"}, {"title": "4.2 Text simplification", "content": "For the training and the evaluation of the LLM models, we have constructed two different datasets: 1) Training-set. To fine-tune our models for the task of simplification, we need a dataset of French sentences with their simplifications at an associated lower CEFR level. We used GPT4 to generate 125 sentences (25 from each level A2, B1, B2, C1, C2) and their simplified versions. This"}]}