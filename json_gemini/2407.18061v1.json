{"title": "Difficulty Estimation and Simplification of\nFrench Text Using LLMs", "authors": ["Henri Jamet", "Yash Raj Shrestha", "Michalis Vlachos"], "abstract": "We leverage generative large language models for language\nlearning applications, focusing on estimating the difficulty of foreign lan-\nguage texts and simplifying them to lower difficulty levels. We frame both\ntasks as prediction problems and develop a difficulty classification model\nusing labeled examples, transfer learning, and large language models,\ndemonstrating superior accuracy compared to previous approaches. For\nsimplification, we evaluate the trade-off between simplification quality\nand meaning preservation, comparing zero-shot and fine-tuned perfor-\nmances of large language models. We show that meaningful text simpli-\nfications can be obtained with limited fine-tuning. Our experiments are\nconducted on French texts, but our methods are language-agnostic and\ndirectly applicable to other foreign languages.", "sections": [{"title": "Introduction", "content": "Today, there exist many online foreign-language learning tools, including Duolingo,\nFrantastique, and ReadLang which incorporate gamification elements to increase\nappeal and retention [25]. However, they do not offer personalization based on\nlearners' interests nor do they provide relevant, contemporary content. Often-\ntimes, the educational content fails to be matched to the learners knowledge\nlevel. Content that is too easy bores learners; too difficult discourages them.\nIn this work, we use large language models (LLMs) to estimate and poten-\ntially reduce the difficulty of foreign content. Such an approach, can be used to\nidentify appropriate contemporary content in the target learning language, thus\nincreasing learners' motivation. Moreover, such an approach could be used in\nconjunction with a recommendation system to discover content appropriate for\nthe learner's knowledge level. The contributions of this work include:\nA machine learning solution to estimate foreign text difficulty, more accurate\nthan traditional readability metrics.\nUse of fine-tuned LLMs to simplify the difficulty/level of a foreign French\ntext while preserving its meaning as well as possible. We propose a technique\nfor automatically assessing simplification quality and report performance\nbenchmarks."}, {"title": "Difficulty Estimation", "content": "From a pedagogical perspective, our solution builds upon already established the-\nory of extensive reading [7,15], which constitutes a crucial means of reinforcing\none's language skills. However, it is important to find content that is appropri-\nate for the learner's knowledge level of the foreign language. One approach to\nestimating the difficulty of a foreign language text is to use \"readability\" for-\nmulas\u00b9. They are regression approaches that calculate the complexity of a text\nbased on various text features, including sentence length and word frequency.\nSome commonly used readability formulas for foreign language texts include\nthe Flesch-Kincaid Grade Level, the Simple Measure of Gobbledygook\n(SMOG), and the Gunning Fog Index.\nAnother approach to tackle difficulty estimation is to use machine learn-\ning techniques to predict the difficulty level based on various linguistic features\n[5,10]. A particularly notable advancement in this field in recent years is the\nintegration of pre-trained word- and sentence- embeddings into text readability\narchitectures [27,12,16,11]. However, our review indicates that prior research has\nnot yet investigated the predictive accuracy of difficulty estimation using LLMs."}, {"title": "Our approach", "content": "We model the estimation of difficulty as a classification problem. Let D be\nthe set of documents, Y the random variable representing the linguistic dif-\nficulty class. For the linguistic difficulty, we wish predict the Common Euro-\npean Framework of Reference for Languages, or CEFR difficulty level of a text\n{A1, A2, B1, B2, C1, C2}), where A1,A2 signify easy text, B1,B2 intermediate\nlevel and C1,C2 advanced level. \\(f : D \\rightarrow Y\\) is a classifier mapping a document\n\\(d \\in D\\) to a difficulty class \\(y \\in Y\\). The classifier f is built using text-label pairs,\nwhere the label corresponds to the linguistic difficulty. We use LLMs like BERT\n[8], GPT [20], GPT-3 [2], LLaMa [24], and Palm [6], which convert text to-\nkens into embeddings capturing the meaning of each token. The models vary\nin data volume, training methodology, size, and language capabilities, result-\ning in embeddings of different lengths (e.g., 768 for BERT, 1,536 for OpenAI's\n\"ada-002\").\nGPT-3+ Models: Advanced models like GPT-3 and GPT-4 [20] have been\ntrained on massive multilingual datasets and post-trained using reinforcement\nlearning from human feedback. They achieve state-of-the-art performance across\ntasks like summarization, translation, and question-answering [3]. These models\nexcel at generating human-like text and can be fine-tuned for classification tasks.\nWe use GPT models for their good performance-versus-cost ratio."}, {"title": "Text Simplification", "content": "In the context of a foreign language learning app, we now explore the scenario\nassuming we have discovered content that matches the user's interests (such as,\nsports or politics), but it is more advanced than their current language profi-\nficiency level.\nEvaluating text simplification systems is a non-trivial task that requires met-\nrics robust enough to account for both readability and semantic preservation.\nTraditional n-gram based metrics such as BLEU or ROUGE, though popular\nin translation tasks, have limitations when applied to text simplification due to\nthe fundamental requirement of simplification to alter the text structure while\nmaintaining the same meaning [1] [22]. The SARI metric, designed specifically\nfor simplification tasks, measures the goodness of words added, deleted, and\nkept by the system. Current research, such as [22], is exploring new methods\nlike QUESTEVAL, which uses semantic questioning of texts to assess simpli-\nfications. This approach aims to overcome the limitations of earlier metrics and\nmay result in improved alignment with human evaluations.\nWhile text summarization and text simplification may appear similar as both\ninvolve altering the original text, their objectives are distinct. Summarization\naims to condense a text by trimming down its length and retaining only the\nmain points. Simplification seeks to lower the linguistic complexity, making con-\ntent more accessible [22]. Simplification may or may not alter the length of text.\nUnlike summarization, simplification is deals with adjusting the text to a par-\nticular knowledge or comprehension level.\nThe hybrid task of combining both text summarization and simplification\nhas been investigated in the context of generating summaries for lay audiences.\nThis task known as \"lay summarization\" aims to render complex scientific con-\ntent accessible to a general audience, requiring the extraction of key points and\nsimultaneously a reduction in linguistic complexity [26,4]. Transformer models\nbased on BERT [8] and PEGASUS [29] have been used in this context.\nRecent advancements in LLMs, have greatly contributed to the field of text\nsimplification [21]. However, the efficacy of these models varies, and not all are\nequally suited for the task, calling for evaluation and fine-tuning on specialized\ndatasets [23]. A recent analysis by [9] highlights that when evaluated on pub-\nlic datasets, contemporary LLMs like GPT-3 can match or even outperform\nsystems explicitly designed for text simplification."}, {"title": "Our approach", "content": "We model simplification in a similar way as for content difficulty estimation.\nHowever, instead of predicting the level of difficulty, we predict the simplified\nsentence token by token. We provide labelled examples of original and simpli-\nfied sentences and train a machine learning model. Because we only fine-tune\nLLMs, we need to provide very few pairs of examples, since the LLMs have\nalready encapsulated in them large amounts of textual knowledge. In our exper-\niments, we only post-train the LLMs using 125 pair sentences, and show large"}, {"title": "Experiments", "content": "The goal of the following experiments is to demonstrate that (a) fine-tuned\nLLMs can significantly improve the difficulty estimation offered by traditional\nreadability metrics. (b) simplification methodology driven by fine-tuned LLM\nmodels outperforms zero-shot approaches. The code for the experiments can be\nretrieved here"}, {"title": "Difficulty estimation", "content": "We evaluated the difficulty estimation on three labeled datasets: 1) Litt\u00e9rature\nde jeunesse libre (LjL) which we obtained from [11]. Each content item here\ncontains several sentences and a label (labels: levell, level2, level3, level4). 2) A\ncollection of sentences collected from the Internet (sentencesInternet). Each\nof these sentences was then annotated by at least three annotators (students\nrecruited in our university) in difficulty levels. Only sentences in which all partici-\npants agreed on the difficulty annotation were retained (labels: A1,A2,B1,B2,C1,C2).\nHere, the labels correspond to the levels designed by the Common European\nFramework of Reference for Languages (CEFR). 3) A collection of sentences\nfrom literature books (sentencesBooks). Each book was annotated with a diffi-\nculty level by a Professor of French. All sentences in that book were then given\nthat label. This process involved an OCR pipeline which could lead to faulty\ndetection of characters, so only the sentences without any errors were retained.\n(labels: A1, A2,B1,B2,C1,C2). The characteristics of these datasets are provided\nin Table 1. To train and evaluate our model, we used an 80/20 train-test split,\nand the results that we present are for examples which the model saw for the\nfirst time."}, {"title": "Evaluation", "content": "As a simple benchmarking, we compare the accuracy of our diffi-\nculty estimation approach to traditional readability-based metrics, such as the\nGFI (Gunning Fog Index), FKGL (Flesch Kincaid grade level), ARI (Auto-\nmated Readability Index) in Table 2. Initially, all of these metrics have been\ndeveloped for English content, but language specific models, such as for French\ntext, as used in our application, have also been developed [10].\nThese techniques are inherently regression techniques and output a floating\npoint value of the text difficulty. As a result, we cannot make a direct com-\nparison, because our difficulty estimator predicts discrete labels. To address"}, {"title": "Text simplification", "content": "For the training and the evaluation of the LLM models, we have constructed\ntwo different datasets: 1) Training-set. To fine-tune our models for the task of\nsimplification, we need a dataset of French sentences with their simplifications\nat an associated lower CEFR level. We used GPT4 to generate 125 sentences\n(25 from each level A2, B1, B2, C1, C2) and their simplified versions. This"}, {"title": "Model Evaluation.", "content": "We examine the performance of GPT-4, Davinci and\nGPT-3.5-turbo-1106 from OpenAI, and the Mistral-7B model. We provide\nthe results of our evaluation in Table 3. Among the different models evaluated,\nGPT-4 Zero-shot has the highest w-Score. The 0.5 in the simplification accuracy\nof GPT-4 shows that in 50% of the cases, the text was simplified to one-level\nlower of difficulty. At the same time the meaning is highly preserved with the\ncosine similarity between the original and simplified embeddings of the text being\n0.89 on average."}, {"title": "Iterative Simplification.", "content": "Finally, we illustrate how a simplification model, the\nMistral in this case, behaves for the task of an iterative simplification of French\nsentences. We randomly selected 100 level C2 sentences from the training-set\ncorrectly classified by CamemBERT as being of level C2. We then iteratively\napplied the simplification steps to the sentence using the fine-tuned version of\nMistral-7B before evaluating the difficulty of the resulting sentence and the\ncosine similarity with the original sentence (from iteration 0). For a performance"}, {"title": "Conclusion", "content": "This study demonstrates the potential of LLMs to enhance the estimation of\nforeign text difficulty and simplification. These advancements open new per-\nspectives for personalizing language learning. By integrating these models into\neducational platforms, it becomes possible to adapt content to each learner's in-\nterests and level, making the experience more engaging and effective. Moreover,\nthese models could help bridge gaps in existing pedagogical resources by gener-\nating simplified content on-demand. Future work should explore the possibility\nof working with entire paragraphs rather than isolated sentences for difficulty\nestimation and simplification. It would also be interesting to include state-of-\nthe-art models like GPT-4, Claude 3 Opus, Gemini 1.0 Ultra, and larger\nopen-source models such as Mistral 8x22b in the comparisons [14]."}]}