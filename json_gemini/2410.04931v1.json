{"title": "The Role of Governments in Increasing Interconnected Post-Deployment Monitoring of AI", "authors": ["Merlin Stein", "Jamie Bernardi", "Connor Dunlop"], "abstract": "Language-based AI systems are diffusing into society, bringing positive and nega-\ntive impacts. Mitigating negative impacts depends on accurate impact assessments,\ndrawn from an empirical evidence base that makes causal connections between\nAI usage and impacts. Interconnected post-deployment monitoring combines in-\nformation about model integration and use, application use, and incidents and\nimpacts. For example, inference time monitoring of chain-of-thought reasoning\ncan be combined with long-term monitoring of sectoral AI diffusion, impacts and\nincidents. Drawing on information sharing mechanisms in other industries, we\nhighlight example data sources and specific data points that governments could\ncollect to inform AI risk management.", "sections": [{"title": "Interconnected Post-Deployment Monitoring of AI as a Government Priority", "content": "People are increasingly exposed to AI systems in all areas of life. Language-based AI systems\nare general-purpose technologies [1], meaning they may be deployed across contexts. Systems\nlike GPT-4, Claude, and Gemini are increasingly being integrated into workflows at Fortune 500\ncompanies [2], public services [3], and in critical sectors like courts [4, 5] and health services [6].\nGovernments and the public have limited visibility into AI systems use and impacts. While many\napplications are beneficial, adopting language-based AI systems also carries societal risks [7, 8, 9].\nApplicants may be discriminated against based on their names, as recruiters screen CVs with AI\nsystems [10]; certain people's jobs may be displaced [11, 12], and citizens' data can be more readily\nstolen through AI-assisted cyber attacks [13]. Despite these risks, very little information about how\nAI is used and its impacts on society is available to governments or the general public [14], which\ncould allow harms to propagate unaddressed.\nPre-deployment information is insufficient. To understand risks arising from AI systems, gov-\nernments and civil society have primarily developed mechanisms for gathering pre-deployment\ninformation, such as model evaluations [15]. However, pre-deployment information can not fully\npredict the downstream impacts of AI systems [16]. Risks ultimately arise from real-world usage,\nand depend on complex interactions of AI systems with people and society. For instance, combining\nsystems with other tools can expand AI systems' capabilities in unpredictable ways [17].\nInterconnected post-deployment monitoring can improve AI risk management by using data\nto inform mitigations. By monitoring AI's actual usage and impact, researchers can derive risk"}, {"title": "What is Post-Deployment Monitoring of AI Systems?", "content": "Post-deployment monitoring increases visibility into AI models' integration into applications, usage\nof AI applications, and AI applications' impacts on people and society.\npost-deployment information by supply chain actors."}, {"title": "Types of Post-Deployment Information", "content": "Model Integration and Usage Information relates to how AI models are integrated into digital\napplications. It includes information about how AI models are made available on the market, which\napplication providers use them, and which industries most readily adopt AI models and downstream\napplications. An example is the US Census Bureau's survey of businesses' AI use [12].\nGovernments and the public have little visibility into how different sectors deploy AI systems.\nThis hinders the ability to monitor cross-cutting risks like over-reliance on AI in certain sectors or\ngeographies, or market concentration and unequal access. Integration information can indicate when\nand where these cross-cutting risks might emerge.\nApplication Usage Information relates to how an application is used in-context. It is generated\nwhen users interact with applications, ideally in the real world. It includes, for example, analysing AI\nsystem logs [41], monitoring feedback about AI applications (e.g. model vulnerabilities [42, 43, 44]),\nor conducting explicit sociotechnical field tests [45, 16]. Application usage data could also be\ngathered by monitoring online content for the appearance of AI outputs. Gathering usage data would\nbe aided by requiring AI watermarks [46], content provenance [47] and AI agent activity logs [48, 49]\n(Section 4.4)."}, {"title": "Deployment configurations: Different Supply Chain Actors' Possession of Information", "content": "A single entity can fulfil one or many roles in the supply chain. For instance, OpenAI is the foundation\nmodel developer, a host and application provider for ChatGPT. Commercial relationships between\nentities affect information availability due to customers' expectation of confidentiality with their\nvendor. We provide non-exhaustive examples of deployment configurations in Figure 2.\nModel developers produce AI systems. These are made available to customers through model hosts.\nFor instance, Mistral developed Mistral Large, which is hosted by Microsoft on its Azure servers [53],\nby which an API is made available to application developers. OpenAI develops and hosts its own\nmodels (though it may rent servers from a cloud provider, behind the scenes).\nAI application developers create the interfaces through which people use foundation models. For\nexample, Duolingo Max is an application through which users interact with GPT-4 [54]. Application\ndevelopers receive users' usage data. The Model host receives and processes this data too, however"}, {"title": "Challenges for Governments conducting Post-Deployment Monitoring of AI", "content": "Implementing policies for post-deployment monitoring of AI systems poses challenges, some seen in\nother industries, and others specific to AI technologies:\n\u2022 User privacy. Users expect their AI system usage to be private, thus potentially input per-\nsonal data in prompts. To monitor usage data directly, it's necessary to employ consent-based\ndata donation [41] or privacy-preserving anonymisation and data analysis techniques [55].\n\u2022 Costs and independence. Who pays the cost of compliance with post-deployment moni-\ntoring? Industry-funded monitoring, without appropriate incentive structures, can be low\nquality [56]. Independent third-parties require appropriate access and funding [33].\n\u2022 Information misuse. Collecting information about incidents and misuse could strategically\ninform malign actors, requiring coordinated sharing mechanisms [24, 57].\n\u2022 Commercial sensitivity. Information detailing the rate and distribution of AI integration may\nreveal opportunities for competitors. Whilst current market players keep this information\nprivate by default, limited public availability may promote wealth-creating competition [58].\nWhere governments have offered full confidentiality for post-market monitoring, conflicts\nof interest can emerge between commercial activity and public safety [59, 60]."}, {"title": "Recommendations for Governments on Post-Deployment Monitoring", "content": "Post-deployment monitoring and follow-up do not happen by default. We outline four recommenda-\ntions for governments and AI Safety Institutes developing post-deployment monitoring policies."}, {"title": "Prioritise Incident Monitoring and Reporting with causal links to AI system use", "content": "Incident reporting and monitoring are commonly practiced in many regulated industries [61, 62],\nand have proven at least partially effective in managing risks [26, 28]. These practices have inspired\nefforts to evaluate how incident reporting could support AI risk management [63, 25, 64, 65, 66].\nSeveral AI incident databases have already emerged from civil society [31, 67, 68, 69], collecting\ntheir data from public channels. These have already informed analyses and taxonomies [70, 71, 72],\nand have proven to help their users quantify AI harms [73].\nTo be effective, AI incident reporting and monitoring processes should be designed with clear policy\ngoals, typically one of learning or accountability [74]. These goals drive post-reporting actions,\nsuch as sharing learning to relevant stakeholders [57, 75] or implementing safety measures [24].\nGovernments are often well-suited to facilitate these processes: they have the authority to mandate\nreporting, act as neutral parties to encourage voluntary reporting, and can provide the resources and\nauthority for follow-up actions.\nSince it's difficult to evaluate the most effective incident reporting processes in advance, governments\ncould adopt an iterative approach to their implementation. This would allow them to build expertise\nand gain insight into reporting gaps over time. As a low cost starting point, government functions\nthat catalogue AI risks - like the UK's Central AI Risk Function [76] - could monitor public channels\nto collect empirical evidence of AI harm, thus quantifying their risk assessments. From there,\ngovernments could explore more involved proposals, such as developing an ombudsman for citizens\nto report AI harms [77], mandating reporting for major AI incidents [63], and collating AI-related\nincidents from sector-specific regulators [78]."}, {"title": "Establish Mechanisms to Gather Post-Deployment Information", "content": "In this recommendation we outline several non-exhaustive strategies that governments and AI Safety\nInstitutes can employ to gather post-deployment information on AI systems and models. Their\nrespective utilities depend on the regulatory and industry context, and the nature of the monitored AI\nsystem.\nVoluntary Information Provision and Cooperation. Governments can gather information from AI\ncompanies through both informal and formal channels for voluntary cooperation. This can involve\nrequests for specific statistics (like an application's user count - more examples in Table 1), but could\nalso involve companies providing regular aggregated data streams: the UK's Office for National\nStatistics receives aggregated data from payment service providers [79], which could be a useful\nmodel for governments monitoring AI integration and usage statistics. The UK and US AI Safety\nInstitutes have already established voluntary agreements with leading AI model developers to test\ntheir models before deployment [80, 81], and this framework could be expanded to include post-\ndeployment data. Voluntary cooperation strategies are lighter-touch and more flexible than making\nmandatory requests, but their success is dependent on goodwill relationships, which may incur a\nselection bias in which companies provide the most information to government [82].\nMandatory reporting through legislation. Mandatory reporting requirements ensures broad com-\npliance, which may be essential for obtaining safety critical information. Mandatory requests often\nrequire legislative backing. A useful framework to consider for AI-related information requests is\nthe UK's Digital Economy Act 2017, which empowers its Office for National Statistics to mandate\nbusinesses to submit specific data through binding surveys [83]. The EU AI Act already mandates\ncertain post-market reporting, including metric reporting (Article 72) and documentation of serious\nincidents (Article 73) [84]. An effective approach depends on governments having enough knowledge\nto request targeted information [82].\nThird-Party Research and Independent Monitoring. Academics and other third party institutions\nplay an important role in collecting and analysing post-deployment data, however their data access\nis often limited to public sources [33]. Third parties have utilised alternative sources like Similar-"}, {"title": "Request Initial Data Points and Build Analysis Capacity", "content": "Table 1 provides a preliminary, non-comprehensive list of data points that governments could start\nrequesting from companies in the AI supply chain. The suggested data points are based on information\nwhich has been useful in other, regulated industries.\nA full effort to understand Al risks would use these data points in combination with other data\nsources, such as macroeconomic indicators and surveying affected populations. Together, causal\nconnections can be inferred between observed societal impacts and the integration, usage and impact\ndata outlined in this table. For example, the environmental impacts of AI could be inferred from\ninference volumes [88]. Predicting economic disparities between genders can be inferred from\ndiffering usage amounts [89].\nGathering and learning from information as a government is an iterative process of identifying an\ninformative data point, requesting it from industry, analysing the provided data, then evaluating its\nusefulness to generate new lines of inquiry. Requesting and analysing information requires staff\ntime, which governments could hire-in directly [90], fund [86], or facilitate through incentivising a\nthird-party ecosystem [33, 16]. Despite access limitations, third party organisations should not be\noverlooked; in the past, they have advocated for monitoring functions and the enforcement of the\nDigital Services Act through analysing public data [91]."}, {"title": "Support Technical Governance Methods that Increase Visibility", "content": "As the prevalence of AI outputs increases, governments should continue to encourage adoption of\nvisibility-building technologies like content provenance [47] and watermarking [46]. As language-\nbased AI agents are developed and become more prevalent, governments should proactively support\ncorresponding visibility standards [49]. This includes AI agents outputting identifiers, informing\ncompanies and individuals about when they are interacting with agents, indicating which developer is\naccountable, and otherwise creating visibility that third-party researchers could analyse.\nVisibility into AI agent behaviour may also involve analysing logs [48]. Researchers have pre-\nserved privacy by conducting test tasks, however technical solutions may enable monitoring of real\nagents [55]. In any case, government agencies should work with agent developers to understand\nagent behaviour and human-agent interaction early in this technology's development toidentify risks,\ninform technical processes that mitigate them, and surface ways that companies and individuals\nshould adapt to the diffusion of AI agents [22]."}, {"title": "Conclusion and Future Work", "content": "In this paper, we have argued for the critical importance of interconnected post-deployment monitoring\nof AI systems by governments and AI Safety Institutes. We suggest causally connecting three kinds of\npost-deployment information: model integration and usage, application usage, and impact and incident\ndata. We recommend that governments and AI Safety Institutes begin building this information\necosystem by:\n\u2022 Prioritising incident monitoring and reporting, with causal links to AI system use.\n\u2022 Implementing mechanisms to gather post-deployment information.\n\u2022 Requesting specific data points from AI companies and build analysis capacity.\n\u2022 Supporting technical governance methods that increase visibility of AI systems.\nWe call on the technical and AI governance research communities and AI companies to support these\nmeasures, which requires future work on assessing the effectiveness of different post-deployment\nmonitoring approaches and using privacy-preserving techniques to build more post-deployment\ndatasets like WildChat [41] across different sectors and applications."}]}