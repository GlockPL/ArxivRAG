{"title": "RealCritic: Towards Effectiveness-Driven Evaluation of Language Model Critiques", "authors": ["Zhengyang Tang", "Ziniu Li", "Zhenyang Xiao", "Tian Ding", "Ruoyu Sun", "Benyou Wang", "Dayiheng Liu", "Fei Huang", "Tianyu Liu", "Bowen Yu", "Junyang Lin"], "abstract": "Critiques are important for enhancing the performance of Large Language\nModels (LLMs), enabling both self-improvement and constructive feedback\nfor others by identifying flaws and suggesting improvements. However,\nevaluating the critique capabilities of LLMs presents a significant challenge\ndue to the open-ended nature of the task. In this work, we introduce a new\nbenchmark designed to assess the critique capabilities of LLMs. Unlike\nexisting benchmarks, which typically function in an open-loop fashion, our\napproach employs a closed-loop methodology that evaluates the quality\nof corrections generated from critiques. Moreover, the benchmark incor-\nporates features such as self-critique, cross-critique, and iterative critique,\nwhich are crucial for distinguishing the abilities of advanced reasoning\nmodels from more classical ones. We implement this benchmark using\neight challenging reasoning tasks. We have several interesting findings.\nFirst, despite demonstrating comparable performance in direct chain-of-\nthought generation, classical LLMs significantly lag behind the advanced\nreasoning-based model o1-mini across all critique scenarios. Second, in\nself-critique and iterative critique settings, classical LLMs may even under-\nperform relative to their baseline capabilities. We hope that this benchmark\nwill serve as a valuable resource to guide future advancements. The code\nand data are available at https://github.com/tangzhy/RealCritic.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Language Models (LLMs) (Hurst et al., 2024; Dubey et\nal., 2024; Mistral-AI, 2024; Qwen-Team, 2024) have demonstrated remarkable capabilities\nacross lots of tasks, ranging from natural language understanding to problem-solving.\nNotably, reasoning-based LLMs have surpassed human expertise in specialized domains\nsuch as competition-level mathematical reasoning and code generation (OpenAI, 2024).\nHowever, as LLMs continue to evolve, further improving their performance has become\nincreasingly challenging. One of the key areas of focus in this regard is the development\nof critique abilities (McAleese et al., 2024)\u2014the capability of LLMs to provide detailed\nanalysis, constructive feedback, and refinement suggestions for solutions generated by other\nmodels or even themselves, thereby driving continuous improvement in their outputs.\nAn important step toward advancing critique abilities is the establishment of high-quality\nbenchmarks for evaluation. Without such benchmarks, it is difficult to objectively measure\nprogress in this area. However, evaluating the quality of critiques is difficult due to the"}, {"title": "2 Toward Effectiveness-Driven Evaluation", "content": "In this section, we discuss strategies for constructing an effective benchmark for evaluating\ncritiques. Evaluating the critique capabilities of LLMs is inherently challenging due to the\nmultifaceted and open-ended nature of the task. A high-quality critique must fulfill several\ncriteria: it should identify flaws or weaknesses in the solution, provide constructive and\nactionable feedback, and maintain clarity and logical consistency. These requirements com-\nplicate the evaluation process, as each sub-part allows for a wide range of valid responses.\nWhile human evaluators may perform well in assessing critique quality, human-based\nevaluation methods are neither automatic nor scalable.\nTo address these challenges, it is crucial to leverage task-specific properties. Reasoning tasks,\nfor instance, are particularly well-suited for this purpose, as they share a key characteristic:\nthe final generated answer is typically a single, definitive response. This property can be\nutilized to facilitate the evaluation of solutions. CriticBench (Luo et al., 2023; Lin et al., 2024)\nleverages this property to evaluate critiques through the following two-step process:\n\u2022 Critic Generation: This benchmark requires LLMs to generate critiques that provide\na detailed analysis of a solution. In addition to the analysis, the model must also\nproduce a verdict predicting whether the input solution is correct or incorrect.\n\u2022 Quality Evaluation: The benchmark assesses the quality of the critique based on\nthe correctness of the verdict. If the verdict is correct, the critique is deemed high\nquality. Conversely, if the verdict is incorrect, the critique is judged as low quality.\nCriticEval (Lan et al., 2024) uses a similar approach in its evaluation framework, particularly\nin the dimension they define as \u201cfeedback\u201d. We would like to point out that the fundamental\npremise of this approach is to evaluate critique quality based on the accuracy of predicting\nthe solution's correctness. We argue that this evaluation method may not always be effective,\nas an LLM might correctly predict the solution's accuracy without providing a constructive\nanalysis or critique. This misalignment stems from the fact that the evaluation approach\ndoes not fully capture the intuition that a critique is considered high-quality if it leads to"}, {"title": "3 Our Evaluation Framework", "content": "In this section, we present our evaluation framework for benchmarking the critique abilities\nof LLMs in mathematical reasoning domains. Our framework is built around three key\ncomponents: the closed-loop evaluation method described in Section 3.1, the spectrum of\ncritique abilities outlined in Section 3.2, and the targeted datasets we curate for evaluation,\nas detailed in Section 3.3."}, {"title": "3.1 From Open-Loop Evaluation to Closed-Loop Evaluation", "content": "As discussed in Section 2, we observe that the majority of existing benchmarks rely on open-\nloop evaluation. This method fails to account for critiques and their ongoing refinement\nprocesses, thereby limiting the effectiveness of these benchmarks. To address this limitation,\nwe propose a closed-loop methodology designed to evaluate the model's critique ability\nthrough a critique-and-correction framework. In this framework, the LLM's critiques are\nnot only generated but also tested by applying them to refine the initial solution.\nSpecifically, we assess the quality of critiques by examining the accuracy of the solution\ngenerated after the critique is applied and refined. The key insight driving this approach\nis that a high-quality critique should naturally lead to a more accurate solution. In other\nwords, the critique is not an isolated task but an integral part of a dynamic correction process\nthat should improve the solution incrementally. This allows us to measure the effectiveness\nof the critique by evaluating the quality of the final solution. Formally, we define\n$\\text{performance} = \\mathbb{E}_{x \\sim p}[\\mathbb{I}(Y_{\\text{correction}})],$\n$(Y_{\\text{critic}}, Y_{\\text{correction}}) \\sim P_{\\text{Critic-LLM}}(X, S, I_{\\text{critic}}),$\nwhere $x$ denotes a question drawn from the distribution $p$. The term $P_{\\text{Critic-LLM}}$ represents\nthe distribution over outputs of the (critic) LLM, with $s$ as an initial solution (response)\nand $I_{\\text{critic}}$ as the instruction prompting both the critique $Y_{\\text{critic}}$ and correction $Y_{\\text{correction}}$. The\nnotion $\\mathbb{I}$ is the indicator function. A specific example is shown in Figure 2.\nWe would like to clarify several points. First, while previous works (Luo et al., 2023; Lin\net al., 2024; Lan et al., 2024) also explored measuring the accuracy of the refined solution,\nthis is done in a separate stage and is not used to measure the quality of the critique itself.\nSecond, our approach leverages the quality of the refined solution to measure the quality of\nthe generated critique. However, there are potential shortcuts where models may directly\ngenerate a new solution instead of critiquing the original one. To address this, we have\ncarefully tuned the prompt and designed a post-check process to mitigate such behaviors in\nour evaluations (see Appendix B)."}, {"title": "3.2 What to Critique and How to Critique?", "content": "After defining the evaluation criteria, the next step is to design test cases that assess both\nthe model's critique ability and the underlying critique process. We highlight two key\naxes often overlooked in existing benchmarks. The first axis distinguishes between self-\ncritique, where the model evaluates its own generated solution, and cross-critique, where it\ncritiques solutions provided by other models. While cross-critique has been widely studied,\nself-critique remains underexplored, despite its importance for o1-style models that refine\noutputs through internal feedback loops. The second axis examines whether the critique\nprocess is single-round or iterative. Complex tasks often require multi-round critique, where\nthe model refines its feedback across iterations to address errors in initial assessments. This\niterative process provides a more comprehensive test of the model's long-horizon reasoning\ncapabilities. We discuss these points in detail below.\nSelf-Critique Paradigm: The self-critique paradigm focuses on assessing a model's ability\nto identify and improve errors in its own generated solutions. Formally, the quality of\ncritique is measured by:\n$\\text{performance}_{\\text{self}} = \\mathbb{E}_{x \\sim p}[\\mathbb{I}(Y_{\\text{correction}})],$\n$(Y_{\\text{critic}}, Y_{\\text{correction}}) \\sim P_{\\text{Critic-LLM}}(x, s, I_{\\text{critic}}),$\n$s \\sim I_{\\text{Critic-LLM}}(x, I_{\\text{generation}}).$\nA key feature of this paradigm is that the input solutions for critique are not fixed or pre-\ncollected but are instead model-specific. This paradigm evaluates the model's introspective\ncapabilities, specifically its ability to recognize its own errors, analyze their root causes, and\nimplement effective corrections. However, since the critic and solution generator are the\nsame model, they inherently share the same knowledge base and potential biases, which\nmay limit the diversity and objectivity of the critiques.\nCross-Critique Paradigm: The cross-critique paradigm evaluates a model's ability to an-\nalyze and improve answers generated by other models. This approach is widely used\nin benchmarks, partly because it is straightforward to implement, as input solutions for\ncritique can be pre-collected. This paradigm is crucial for understanding how effectively a\nmodel can serve as an external evaluator. Formally, the quality of critique is measured by:\n$\\text{performance}_{\\text{cross}} = \\mathbb{E}_{x \\sim p}[\\mathbb{I}(Y_{\\text{correction}})],$\n$(Y_{\\text{critic}}, Y_{\\text{correction}}) \\sim P_{\\text{Critic-LLM}}(x, s, I_{\\text{critic}}),$\n$s \\sim \\mathbb{P}_{\\text{LLM}}(x, I_{\\text{generation}}),$\nwhere $\\mathbb{P}_{\\text{LLM}}$ may involve any language model distribution, which is distinct from the critic's\ndistribution and could even be a combination of multiple models. This diversity means\nthat the critic must adeptly handle a wide range of solution styles and error types, ensuring\nversatile evaluation capabilities.\nIterative Critique-Correction Paradigm: Drawing on the success of o1-style models (Ope-\nnAI, 2024), it becomes evident that multi-round self-reflection and iterative refinement\nplay a decisive role in mathematical reasoning. By enabling several cycles of critique-and-\ncorrection, we can thoroughly evaluate the model's continuous improvement across suc-\ncessive iterations, investigate the convergence properties underpinning its error-correction\nprocess, and analyze how self-supervision mechanisms influence performance\u2014ultimately\nshedding light on the model's capacity for robust and adaptive problem solving.\nFormally, we define the iterative process as follows:\n$\\text{performance}_{\\text{iterative}} = \\mathbb{E}_{x \\sim p}[\\mathbb{I}(Y_{\\text{correction}}^{t})],$\n$(Y_{\\text{critic}}^{t}, Y_{\\text{correction}}^{t}) \\sim P_{\\text{Critic-LLM}}(x, s, I_{\\text{critic}}, Y_{\\text{critic}}^{t-1}, Y_{\\text{correction}}^{t-1}, ..., Y_{\\text{critic}}^{1}, Y_{\\text{correction}}^{1}),$\n$s \\sim \\mathbb{P}_{\\text{LLM}}(x, I_{\\text{generation}}),$\nwhere $t$ denotes the iteration index compared with previous notions. This formulation\nhighlights the model's capability to refine its output through repeated self-critique, shedding\nlight on its long-horizon reasoning and error-correction dynamics."}, {"title": "3.3 Dataset Collection", "content": "In this section, we present the strategies we propose for constructing high-quality datasets\nto benchmark critique abilities. An overview of our data collection process is provided in\nFigure 4. This process involves two key steps: first, collecting questions from a variety of\ntasks, and second, generating solutions to these questions using multiple models. Both the\nquestions and solutions will be used to evaluate the critique abilities of LLMs.\nTask Sources. Our approach began with a comprehensive review of existing evaluation\ndatasets used in prior studies that benchmark well-established models, such as the o1 model\n(OpenAI, 2024). We applied a set of selection criteria to filter relevant datasets for our study.\nSpecifically, the datasets had to satisfy the following conditions: they should encompass\na variety of difficulty levels, and they must provide explicit final answers to facilitate the\nverification of solution correctness. After applying these filters, we identified 20 candidate\ndatasets, which encompassed both open-ended math reasoning tasks and general-domain\nmultiple-choice reasoning tasks. For further details on the attributes of these datasets,\nplease refer to Appendix C. Given the large number of datasets, evaluating all 20 candidates\nwas not feasible within the scope of this study. To ensure a cost-effective and manageable\nevaluation process, we selected 8 representative datasets that cover a range of difficulty\nlevels and task types.\nFor open-ended math reasoning tasks, we chose five datasets: GSM8K (Cobbe et al., 2021),\nMath (Hendrycks et al., 2021), College Math (Tang et al., 2024), Minerva Math (Lewkowycz\net al., 2022), and Olympiad Bench (He et al., 2024). These datasets span a broad spectrum of\nmathematical difficulty, from standard high school problems to advanced Olympiad-level\nchallenges, providing a diverse set of tasks that test a model's reasoning abilities. For\ngeneral-domain reasoning tasks with multiple-choice questions, we included three datasets:\nGPQA-diamond (Rein et al., 2023), ARC-Challenge (Clark et al., 2018a), and MMLU-STEM\n(Hendrycks et al., 2020). These datasets assess performance across various domains, with an\nemphasis on selecting the correct answer from multiple choices. To reduce evaluation costs,\nwe sampled up to 300 items from each dataset, including all items from smaller sets and\nrandomly selecting 300 from larger ones to maintain a diverse yet manageable data pool.\nSolution Sources. To generate solutions for the collected questions, we established a model\npool primarily composed of open-source models to minimize costs. The pool includes\nthe Qwen2.5 series (Qwen-Team, 2024) (1.5B, 7B, and 72B), the LLaMA-3.1 series (8B and\n72B) (Dubey et al., 2024), and the Deepseek math-specialized model (DeepSeek-AI, 2024)\n(7B). Given the varying downstream capabilities of these models, we carefully designed a\nstrategy to ensure both the quality and diversity of the generated solutions.\nSpecifically, we sample each model once on all data points to generate their solutions. The\ndata is then cleaned using the following steps. First, we remove overly long solutions where"}, {"title": "4 Experiments", "content": "In this section, we conduct experiments using the proposed evaluation benchmark. We\nprovide the experiment configuration below."}, {"title": "4.1 Experimental Setup", "content": "In this section, we conduct experiments using the proposed evaluation benchmark. We\nprovide the experiment configuration below."}, {"title": "4.2 Results and Analysis", "content": "Our main results are presented in Table 4, Figure 1, and Figure 6. Depending on the critique\nabilities being evaluated, we present the concrete analysis in the following sections."}, {"title": "4.2.1 Self-Critique Performance Analysis", "content": "Our analysis of self-critique capabilities reveals interesting patterns across different models.\nMost models show varying degrees of performance changes in self-critique compared to\ntheir direct CoT solutions, with changes ranging from -5.1% to +3.3% on average. This\nsuggests that recognizing and correcting one's own mistakes remains a challenging task"}, {"title": "4.2.2 Cross-Critique Performance Analysis", "content": "Cross-critique evaluation reveals a more nuanced picture of models' critique capabilities.\nThe baseline performance is approximately 50%, as half of the input solutions are correct\nand half are incorrect. When compared to this baseline, most models show substantial\nimprovements (up to 40%) on mathematical reasoning tasks. This suggests that models\nare particularly adept at identifying and correcting errors in basic mathematical reasoning.\nSimilar to the case of self-critique, we observe that o1-mini emerges as the strongest per-\nformer in cross-critique tasks. For specialized tasks, models\u2014except for 01-mini-again\nshow degraded performance on average."}, {"title": "4.2.3 Comparative Analysis of Self-Critique and Cross-Critique", "content": "In this section, we present a more detailed analysis, focusing on two key metrics: I\u2192C, which\nmeasures the transformation of incorrect input solutions into correct ones after critique and\ncorrection, and C\u2192I, which captures the reversal of originally correct solutions to incorrect\nones after critique and correction. These metrics will help us gain deeper insights into the\neffectiveness of the critique process. The results are reported in Figure 5."}, {"title": "4.2.4 Impact of Iterative Critique", "content": "In this section, we present the findings from iterative critique experiments, in which models\nare tasked with providing critiques across multiple rounds. The results are detailed in\nFigure 6.\nWe observe distinct trends in how critique effectiveness\u2014measured by performance delta\nover the base solution\u2014evolves across rounds. LLaMA-3.1-70B-Instruct, Mistral-Large-\nInstruct, and Qwen2.5-72B-Math-Instruct exhibit a steady decline in effectiveness, while\nQwen2.5-72B-Instruct demonstrates remarkable consistency in maintaining its improvement\nmargins. GPT-40 shows an initial increase in cross-critique effectiveness before a decline in\nself-critique, whereas o1-mini uniquely sustains strong improvement margins throughout\niterations, particularly excelling in early rounds. These variations highlight the diverse\ndynamics of critique effectiveness across models and rounds."}, {"title": "5 Related Work", "content": "Prior to our work, several efforts have been made to advance the benchmarking of LLMs'\ncritique abilities (Luo et al., 2023; Lin et al., 2024; Lan et al., 2024). We have summarized their\nkey features in Table 1. In contrast to these benchmarks, our approach focuses specifically\non reasoning tasks and introduces a tightly coupled evaluation framework. This framework\nvalidates critique quality through direct correction outcomes and supports diverse critique\nmodes, including self-critique and iterative critique.\nUnderstanding LLMs' ability to self-critique and improve their reasoning has emerged as\na critical research direction. Huang et al. (2023) demonstrated that LLMs often struggle\nwith self-correction without external feedback, while Stechly et al. (2024) found that external"}, {"title": "6 Conclusion", "content": "In this paper, we present a new benchmark designed to evaluate the critique capabilities\nof LLMs in reasoning tasks. We investigate three distinct scenarios: self-critique, cross-\ncritique, and iterative critique. Our findings reveal that in nearly all cases, the o1-mini\nmodel demonstrates the most impressive performance. While other models may exhibit\ncomparable performance to 01-mini in direct CoT generation, they significantly lag behind\nin critique abilities. We provide a detailed analysis of these results, offering insights into the\nunderlying factors contributing to these differences. We hope that our findings will inspire\nand guide future advancements in the development of LLMs."}, {"title": "A Human Evaluation Protocol for Critique Quality", "content": "We conducted a systematic human evaluation study to establish ground truth labels for\ncritique quality. This section details our evaluation protocol."}, {"title": "A.1 Data Preparation", "content": "We utilized the evaluation framework from CriticBench (Lin et al., 2024), which provides a\ncomprehensive dataset containing mathematical problems, ground truth solutions, and stu-\ndent solutions. Following their setup, we used 100 problems from their MATH (Hendrycks\net al., 2021) test set, which already contains mathematical problems, ground truth solutions,\nand student attempted solutions. We leveraged CriticBench's evaluation code and prompt\ntemplates, replacing their default model with Qwen2.5-72B-Instruct(Qwen-Team, 2024) to\ngenerate critiques in their standard JSON format:\n{\n\"analysis\": \"Detailed analysis of the solution...\",\n\"verdict\": \"correct\" or \"wrong\"\n}"}, {"title": "A.2 Evaluation Framework", "content": "Our evaluation framework relies on two key dimensions:\n1. The correctness of the student's solution (compared to ground truth)\n2. The verdict provided in the critique (\"correct\" or \"wrong\")\nBased on these dimensions, we developed a comprehensive argument mapping framework\nto evaluate critique quality, as shown in Table 5."}, {"title": "A.3 Evaluation Process", "content": "For each critique evaluation, STEM graduate students were asked to perform a systematic\nanalysis following a standardized procedure:\n1. First, evaluators thoroughly reviewed the mathematical problem and its ground\ntruth solution to establish a clear understanding of the correct approach and key\nconcepts involved.\n2. They then examined the student's attempted solution in detail, comparing it with\nthe ground truth solution to identify any discrepancies or errors.\n3. Next, they analyzed the generated critique, paying particular attention to both its\nverdict (\"correct\" or \"wrong\") and the specific reasoning provided in the analysis.\n4. Based on the solution's correctness and the critique's verdict, evaluators were\npresented with the corresponding argument from Table 5.\n5. Finally, evaluators provided: A binary judgment: \"True\" (indicating the argument\napplies and the critique is low-quality) or \"False\" (indicating the argument doesn't\napply and the critique is high-quality)\nThe results of this systematic evaluation process were used to create the confusion matrix\npresented in Table 2 in the main text, which demonstrates the limitations of verdict-based\ncritique evaluation methods. See Figure 2, 7, 8, and 9 for more concrete examples."}, {"title": "B Post-Check Mechanism for Critique Quality Assurance", "content": "You are a mathematics expert. You will be provided with a question, a solution, and a\ncritique given by a teacher model. The critique will follow this format: If the reasoning is\nfound to be incorrect, first identify the errors in the reasoning to form feedback, and then\ncorrect the erroneous reasoning based on the provided feedback.\nYour task is to evaluate whether the teacher model's critique exhibits any of the following\nnon-compliant behaviors:\n1. The model does not adhere to the paradigm of critiquing before providing its own\nsolution; instead, it solves the problem first and provides a critique afterward.\n2. During the critique, the model is capable of identifying errors but fails to correct the\nflawed reasoning based on the identified mistakes; instead, it circumvents the pointed-out\nissues in a deceptive manner, providing its own solution.\nIf any of the above non-compliant behaviors are present, the final answer should be\nUnqualified\nIf there are no such non-compliant behaviors, the final answer should be Qualified\nPlease reason step-by-step with your analysis and provide the final answer with\nUnqualified or Qualified. You can output only these two results.\nTo ensure the validity of our closed-loop evaluation framework, we implement a rigorous\npost-check mechanism to verify that the critique-correction process strictly follows our\nintended paradigm. This process is crucial for preventing potential shortcuts where models\nmight generate new solutions directly without properly critiquing the original solution."}, {"title": "B.1 Verification Process", "content": "We employ a carefully designed prompt to evaluate whether each critique adheres to our\nstep-by-step critique-and-correct paradigm in Table 6."}, {"title": "B.2 Implementation and Validation", "content": "We utilize Qwen2.5-32B-Instruct as our verification model to evaluate all generated critiques.\nTo validate the reliability of this automated verification process, we conducted a thorough\nhuman evaluation study. We randomly sampled 100 critique instances from our dataset for\nmanual review by expert annotators. The results showed a 98% compliance rate with our\ncritique format requirements, demonstrating the effectiveness of our post-check mechanism\nin ensuring the quality and validity of the critique-correction process. This verification\nstep is crucial for maintaining the integrity of our evaluation framework, ensuring that\nthe measured critique effectiveness truly reflects the model's ability to identify and correct\nerrors in a systematic manner. Given this high compliance rate, we did not implement\npost-check as a mandatory step in our evaluation pipeline, as the results indicate that models\ngenerally adhere to the intended critique-correction paradigm."}, {"title": "C Details On The Attributes Of Candidate Datasets", "content": "The 20 candidate datasets we selected are: AIME24, AMC23, AQuA (Ling et al., 2017),\nAsdiv (Miao et al., 2021), CARP (Zhang et al., 2023), College Math (Tang et al., 2024),\nGaokao2023en (noa), GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021),\nMawps (Koncel-Kedziorski et al., 2016), Minerva Math (Lewkowycz et al., 2022), Olympiad-\nbench (He et al., 2024), BBH (Suzgun et al., 2022), LiveBench (White et al., 2024), Arc Chal-\nlenge (Clark et al., 2018b), GPQA extended, GPQA diamond (Rein et al., 2023), MMLU-Stem,\nMMLU-pro, MMLU-redux (Hendrycks et al., 2020).\nThe test sets for AIME24 and AMC23 are too small, so we do not consider adding them\nto RealCritic. The difficulty level of AQuA, Asdiv, CARP, and Mawps is similar to that of\nGSM8K. We opt to use the classic GSM8K to represent critic performance at this difficulty\nlevel. BBH has a wide variety of tasks, which would complicate the evaluation framework,\nso it is excluded from RealCritic. LiveBench, due to its continuously updated nature, poses\nhigh maintenance costs for the future, leading to its exclusion. For the GPQA series, we\nselect the most representative subset, GPQA-diamond. From the MMLU series, we choose\nMMLU-STEM, which is most closely related to reasoning, for inclusion in RealCritic."}, {"title": "D Pure Critique Performance Analysis", "content": "To specifically assess models' pure critique and correction capabilities without the influ-\nence of correct examples, we conduct additional experiments using deliberately incorrect\nsolutions. Unlike the main experiments where solutions are balanced between correct and\nincorrect (leading to a 50% baseline accuracy), here we use exclusively incorrect solutions,"}, {"title": "E Priority-based Filtering Strategy in Solution Selection", "content": "When we filter out wrong solutions, we should prioritize strong models over weak ones.\nAs shown in Table 8, when a weak model makes a mistake, it usually provides very little\ninformation. This type of solution does not help the critic model perform well and makes\nevaluation difficult. On the other hand, a strong model typically offers detailed and thorough\ninformation, giving the critic model more room to operate effectively."}, {"title": "F Concrete Prompts", "content": "Here we present concrete prompts of RealCritic in Table 10 below."}, {"title": "G Concrete Examples", "content": "Here we present concrete example of RealCritic in Tables 11 to 13 below."}]}