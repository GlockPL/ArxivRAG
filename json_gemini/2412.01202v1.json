{"title": "Neuron Abandoning Attention Flow: Visual Explanation of Dynamics inside CNN Models", "authors": ["Yi Liao", "Yongsheng Gao", "Weichuan Zhang"], "abstract": "In this paper, we present a Neuron Abandoning Attention Flow (NAFlow) method to address the open problem of visually explaining the attention evolution dynamics inside CNNs when making their classification decisions. A novel cascading neuron abandoning back-propagation algorithm is designed to trace neurons in all layers of a CNN that involve in making its prediction to address the problem of significant interference from abandoned neurons. Firstly, a Neuron Abandoning Back-Propagation (NA-BP) module is proposed to generate Back-Propagated Feature Maps (BPFM) by using the inverse function of the intermediate layers of CNN models, on which the neurons not used for decision-making are abandoned. Meanwhile, the cascading NA-BP modules calculate the tensors of importance coefficients which are linearly combined with the tensors of BPFMs to form the NAFlow. Secondly, to be able to visualize attention flow for similarity metric-based CNN models, a new channel contribution weights module is proposed to calculate the importance coefficients via Jacobian Matrix. The effectiveness of the proposed NAFlow is validated on nine widely-used CNN models for various tasks of general image classification, contrastive learning classification, few-shot image classification, and image retrieval.", "sections": [{"title": "I. INTRODUCTION", "content": "Convolutional neural networks (CNN) have been widely applied in various image recognition tasks [1]\u2013[4]. The interpretability of CNN models involves explaining what regions on images are looked at by the models in making their classification decisions. Visualizing the regions on images used for decision-making will make the model's prediction more trustworthy and transparent. To this end, various visual explanation methods [5]\u2013[13] are developed to display what regions on images are used by a CNN in making its final classification decision. However, visualizing explanation maps for not only the final layer but also the earlier intermediate layers of CNN models can reveal the evolution process of attention regions, which will be important to further unlocking the mystery hidden inside the inner structure of networks, as a tool, to better understand how a CNN progressively makes its decision or improve a design of CNN. Although designed for visualizing the saliency map of output layer of a model, Grad-CAM, Layer-CAM, and Relevance-CAM have been applied to generate the saliency maps for the intermediate layers of CNN models [11], [13]. However, the feature maps from the intermediate layers are extracted during forward propagation (from the earlier shallow layers to the final layer of the CNN model), as shown in Fig. 1 (top row). In this way, many neurons [14] on the intermediate feature maps have valid values but are not used in the final classification decision (we call these neurons the abandoned neurons) because they are, for example, either deselected by the max pooling layers, not used in convolution operations or reset to zero by ReLU activation function layers without going to the next layer (see examples in Fig. 3). They should be particularly identified and excluded from generating the explanation maps for intermediate layers. None of the existing visual explanation methods have a mechanism to remove these abandoned neurons. Thus they can only be used for generating the attention map of the final output layer but not for the intermediate layers of CNN models. Using them to generate attention maps for the intermediate layers (see Fig. 1 (top row)) is conceptually incorrect with significant interference from irrelevant neurons not contributing to the decision-making of the CNN.\nIn this work, we propose a backward propagation solution,"}, {"title": "II. BACKGROUND", "content": "The existing visual explanation methods for interpreting CNN models can be broadly categorized into class-agnostic methods [7], [15], [16] and class specific methods [9]\u2013[13], [17]\u2013[19]. The class-agnostic methods generate similar visualization results regardless of the class of the image that we want to visualise [20]. They include gradient-based methods [16] and perturbation based methods [7], [15]. FullGrad [16] generates the explanation map by assigning the importance scores to both the input features and and individual feature detectors in the networks. RISE [7] estimates the region importance by probing the model with randomly masked input images and obtaining the corresponding outputs.\nCompared with class-agnostic methods, class specific methods generate class-discriminative attention maps. Class activation map (CAM)-based methods are widely applied to various downstream vision tasks (e.g., image segmentation [21]"}, {"title": "III. PROPOSED METHOD", "content": "In this paper, we propose a novel Neuron Abandoning Attention Flow (NAFlow) method that can for the first time visualise how the attention map evolves inside a CNN model when making its classification decision. The overview of our method is illustrated in Fig. 2. Let \\( f \\) denote a CNN model. For a given image \\( I \\), the classification score \\( y \\) of the target category \\( c \\) is obtained by the following equation,\n\\[ y = f(I). \\]\n\nThe attention map \\( L_i \\) from the \\( l \\)-th layer in CNN model \\( f \\) containing \\( N \\) layers can be calculated by,\n\\[ L_i = \\text{Max} \\left( \\sum_{d=1}^{D_l} (\\Lambda_i^l \\otimes \\text{BPFM}_i^l), 0 \\right), l = 1, ..., N. \\]\n\nwhere \\( \\Lambda^l \\) and \\( \\text{BPFM}^l \\) denote the importance coefficients and the back-propagated feature maps for the \\( l \\)-th layer respectively, \\( \\otimes \\) denotes element-wise multiplication and \\( D^l \\) is the number of channels from the \\( l \\)-th layer. \\( \\text{BPFM}^l \\) and \\( \\Lambda^l \\) are calculated via the proposed Neuron Abandoning Back-Propagation algorithm, which will be detailed in the following subsections."}, {"title": "A. Neuron Abandoning Back-Propagation (NA-BP)", "content": "To simplify notation, we denote the \\( l \\)-th layer of a CNN model as \\( g_l \\), and its input feature map is denoted as \\( A^l \\in \\mathbb{R}^{D^l \\times H^l \\times W^l} \\), the output feature map \\( A^{l+1} \\) can be obtained by the following equation,\n\\[ A^{l+1} = g_l(A^l), A^{l+1} \\in \\mathbb{R}^{D^{l+1} \\times H^{l+1} \\times W^{l+1}}. \\]\n\n\\( A^l \\) contains \\( p \\) neurons and \\( A^{l+1} \\) contains \\( q \\) neurons. Hence, \\( A^l \\) and \\( A^{l+1} \\) can be flattened by\n\\[ [x_1, x_2, ..., x_p] = \\text{Flatten}(A^l), \\]\n\\[ [y_1, y_2, ..., y_q] = \\text{Flatten}(A^{l+1}). \\]\n\nIn the following, we introduce how to obtain \\( [x_1, ..., x_p] \\) by using \\( [y_1, ..., y_q] \\) for various layers in a CNN model."}, {"title": "1) Back-Propagated Feature Map for Convolutional Layer", "content": "When \\( g_l \\) is a convolution layer, Eq. 3 and Eq. 4 can be expressed as :\n\\[ \\begin{bmatrix} y_1 \\\\ : \\\\ y_q \\end{bmatrix} = \\begin{bmatrix} w_{11} & w_{12} & ... & w_{1p} \\\\ : & : & & : \\\\ w_{q1} & w_{q2} & ... & w_{qp} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ : \\\\ x_p \\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ : \\\\ b_q \\end{bmatrix} \\]\n\nwhere \\( [b_1, ..., b_q] \\) denotes the bias provided by the convolution layer, \\( w_{ij} \\) denotes the weights between \\( y_i \\) and \\( x_j \\) calculated by using Jacobian Matrix \\( J(\\frac{\\partial(y_1,...,y_q)}{\\partial(x_1,...,x_p)}) \\), which is\n\\[ w_{ij} = \\frac{\\partial y_i}{\\partial x_j} \\Big|_{(\\frac{\\partial(y_1,...,y_q)}{\\partial(x_1,...,x_p)})} \\, i = 1,\u2026,q, j = 1,\u2026,p. \\]\n\nBecause the convolution operation can be partially-connected, \\( w_{ij} \\) will be 0 if there is no connection between \\( y_i \\) and \\( x_j \\). In our design, for a convolutional layer \\( g_l \\) with \\( q \\geq p \\), we remove \\( [y_{p+1}, ..., y_q] \\) and obtain our back-propagated feature map by\n\\[ \\text{BPFM}_l = \\text{Reshape}([x_1, ..., x_p]), \\]\n\\[ \\begin{bmatrix} x_1 \\\\ : \\\\ x_p \\end{bmatrix} = \\begin{bmatrix} w_{11} & w_{12} & ... & w_{1p} \\\\ : & : & & : \\\\ w_{p1} & w_{p2} & ... & w_{pp} \\end{bmatrix}^{-1} \\begin{bmatrix} y_1-b_1 \\\\ : \\\\ y_p-b_p \\end{bmatrix} \\]\n\nwhere \\( W_p^{-1} \\) is the inverse matrix of \\( W_p \\). Hence, our back-propagated feature map, which is a 3D tensor, can be obtained by reshaping \\( [x_1, ..., x_p] \\) calculated by Eq. 8."}, {"title": "2) Back-Propagated Feature Map for Batch Normalization Layer", "content": "When \\( g_l \\) is a batch normalization layer, Eq. 3 and Eq. 4 can be expressed as element-wise operation \\( y_i = \\frac{x_i - \\text{Mean}}{\\sqrt{\\text{Var}}} \\cdot \\gamma_i + \\beta_i \\). \\( \\beta_i \\) and \\( \\gamma_i \\) are the parameters provided by the well-trained batch normalization layer, and Mean and Var are the running mean and running variance respectively. Its back-propagated feature map can be obtained by reshaping \\( [x_1, ..., x_p] \\), which is element-wisely calculated by\n\\[ \\text{BPFM}_l = \\text{Reshape}([x_1, ..., x_p]), \\]\n\\[ x_i = \\frac{\\sqrt{\\text{Var}}}{\\gamma_i} (y_i - \\beta_i) + \\text{Mean}, i = 1, ..., p. \\]"}, {"title": "3) Back-Propagated Feature Map for Activation Function Layer", "content": "When \\( g_l \\) is ReLU [24], Eq. 3 and Eq. 4 can be expressed as the element-wise operation \\( y_i = \\begin{cases} x_i, & x_i \\geq 0 \\\\ 0, & x_i < 0 \\end{cases} \\)\nThe negative value neurons should not be visualized because they are not used for decision-making, which are abandoned in our back-propagated feature map calculation by reshaping \\( [x_1, ..., x_p] \\) that are computed by\n\\[ \\text{BPFM}_l = \\text{Reshape}([x_1, ..., x_p]), \\]\n\\[ x_i = y_i, i = 1, ..., p. \\]\n\nWhen \\( g_l \\) is LeakyReLU [25], Eq. 3 and Eq. 4 are expressed as the element-wise operation \\( y_i = \\begin{cases} x_i, & x_i \\geq 0 \\\\ a \\cdot x_i, & x_i < 0 \\end{cases} \\), where\n\\( a \\) is negative slope provided by LeakyReLU. Accordingly, the back-propagated feature map is obtained by reshaping \\( [x_1, ..., x_p] \\) that are computed by\n\\[ \\text{BPFM}_l = \\text{Reshape}([x_1, ..., x_p]), \\]\n\\[ x_i = \\begin{cases} y_i, & y_i \\geq 0 \\\\ \\frac{y_i}{a}, & y_i < 0 \\end{cases} \\]"}, {"title": "4) Back-Propagated Feature Map from Max Pooling Layer", "content": "When \\( g_l \\) is a max pooling layer, the flatten operation Eq. 4 cannot be executed. Hence, our Eq. 3 should be expressed as \\( (A^{l+1}, \\text{Index}^l) = \\text{MaxPool}(A^l) \\), where \\( \\text{Index}^l \\) stores the positions of selected neurons from \\( A^l \\) by max pooling operations, while the rest of the neurons are abandoned because they are not involved in the decision-making of the model. Therefore,"}, {"title": "5) Neuron Abandoning Back-Propagation of Multiple Layers", "content": "A CNN model is constructed by stacking up several blocks with each block containing a sequence of layers, in various order and different combination, of the above listed convolutional layer, batch normalization layer, activation function layer and max pooling layer. Fig. 3 gives an example showing how the Neuron Abandoning Back-Propagation (NA-BP) selects the decision-making neurons (in green) for calculating the attention maps of different internal layers of a CNN and excludes the neurons (in grey) that are not involved in making the decision.\nIn the example, for the 6\u00d76 region in the feature map \\( A^l \\) fed into the convolutional layer \\( g_l \\) which performs 16 convolution operations, 4 neuron blocks (each has 3 \u00d7 3 = 9 neurons) totalling 36 neuron-times (33 neurons in green with 3 of them are used twice, which are marked by \u2461) are involved in the decision-making while the other 12 neuron blocks totalling 12 x 9 = 108 neuron-times are used in the computation operations of this layer but not involved in decision-making."}, {"title": "B. Importance Coefficients", "content": "When we have the back-propagated feature maps via Eqs. 6,7,11,12,13,14, given the classification score \\( y \\), the importance coefficients \\( \\Lambda^N \\) for \\( \\text{BPFM}^N \\) of the last layer \\( g_N \\) of the CNN is calculated by\n\\[ \\Lambda^N = [x_1^N, ..., x_p^N] = \\frac{\\partial y_c}{\\partial [x_1^N, ..., x_p^N]}, \\]\n\nwhere \\( [x_1^N, ..., x_p^N] \\) denote the neurons on \\( A^{N+1} \\) (\\( A^{N+1} \\) is \\( \\text{BPFM}^N \\)). For any internal layer of the CNN model \\( f \\), the importance coefficients \\( \\Lambda^l \\) for \\( \\text{BPFM}^l \\) of the \\( l \\)-th layer \\( g_l \\) are obtained by flattening the following Jacobian Matrix:\n\\[ \\Lambda^l = [x_1^l, ..., x_p^l] = \\text{Flatten} \\left( \\sum_{i=1}^q \\frac{\\partial [x_1^{l+1}, ..., x_p^{l+1}]}{\\partial [x_1^l, ..., x_p^l]} \\frac{\\partial A^{l+1}|_i}{\\partial A^l} \\right), l = 1, ..., N - 1. \\]\n\nRepeating the calculation of \\( \\Lambda^l \\) using Eq. 16 by iteratively changing \\( l \\) from \\( l = (N - 1), (N - 2) \\) to 1, we can obtain the importance coefficients for all the internal layers in a back-propagated manner."}, {"title": "C. Visualizing CNNs without Classification Score y", "content": "CNN models for image classification can be broadly grouped into two categories, one directly outputs classification score from FC layer, the other uses similarity metric measurement (such as those CNNs for contrastive learning image classification, few-shot image classification, image retrieval).\nFor explaining CNN models using similarity comparison-based classification, we propose channel contribution weight in this work, as shown Fig. 2, where \\( V_Q = [v_1, v_2, \u2026, v_D^Q] \\) and \\( V_S = [v_1^S, v_2^S, \u2026, v_D^S] \\) denote the feature vector of the test image and the feature vector from the feature vectors data base, respectively.\nThe proposed channel contribution weight can be applied to any similarity metric. Different similarity metrics require different ways to calculate channel contribution weight. In this work, we provide formulations of computing contribution weight by taking the popular cosine similarity, that is widely used in similarity comparison-based CNN models, as an example. We compute the proposed channel contribution weight \\( \\Omega \\) as\n\\[ \\Omega = [\\omega_1, \u2026, \\omega_D] = \\frac{\\left| \\frac{v_d^Q v_d^S}{\\Vert V_Q \\Vert \\Vert V_S \\Vert} \\right|}{\\sum_{d=1}^D \\left| \\frac{v_d^Q v_d^S}{\\Vert V_Q \\Vert \\Vert V_S \\Vert} \\right|}, \\]\n\nwhere \\( \\Vert . \\Vert \\) denotes the absolute value.\nProof: Let cos\\( (V_Q, V_S) \\) denote the cosine similarity between two vectors, which is calculated by the equation of\n\\[ \\text{cos}(V_Q, V_S) = \\frac{\\sum_{d=1}^D v_d^Q v_d^S}{\\Vert V_Q \\Vert \\Vert V_S \\Vert}. \\]\n\nWhen \\( \\text{cos}(V_Q, V_S) \\neq 0 \\), we will have the following,\n\\[ 1 = \\frac{1}{\\text{cos}(V_Q, V_S) \\Vert V_Q \\Vert \\Vert V_S \\Vert} \\sum_{d=1}^D v_d^Q v_d^S \\]\n\nAccording to Eq. 18, cosine similarity score cos\\( (V_Q, V_S) \\) is the sum of D items \\( \\frac{v_d^Q v_d^S}{\\Vert V_Q \\Vert \\Vert V_S \\Vert}, d = 1,..., D \\), where D is\nthe number of channels. The d-th item \\( \\frac{v_d^Q v_d^S}{\\Vert V_Q \\Vert \\Vert V_S \\Vert} \\) represents\nthe contribution to the similarity score from the d-th channel. To measure channel-wise contribution in percentage, we can determine and calculate the d-th channel contribution weight as\n\\[ \\Omega_d = \\frac{\\frac{\\left| \\text{cos}(V_Q, V_S) \\right| \\Vert V_Q \\Vert \\Vert V_S \\Vert}{\\text{cos}(V_Q, V_S) \\Vert V_Q \\Vert \\Vert V_S \\Vert}v_d^Qv_d^S}{\\sum_{d=1}^D v_d^Q v_d^S} \\]\n\nThe proposed channel contribution weight \\( w_d \\) can measure the importance of the d-th channel to the similarity metrics-based classification decision. Therefore, the importance coefficients for \\( \\text{BPFM}^N \\) are calculated by the following Jacobian Matrix,\n\\[ \\Lambda^N = [x_1^N, ..., x_p^N] = \\text{Flatten} \\left(\\frac{\\partial [\\omega_1, ..., \\omega_D]}{\\partial [x_1^N, ..., x_p^N]} \\sum_{i=1}^q \\frac{\\partial [\\omega_1, ..., \\omega_D]}{\\partial \\omega_i} \\right). \\]"}, {"title": "IV. EXPERIMENTS", "content": "Nine publicly released CNN models including 4 widely-used CNN models with FC layer as classifier for general image classification (ResNet18 [23], ResNet50 [23], ResNet50+CBAM [26], and ResNet50+BAM [27]) and 5 similarity metric based CNN models for contrastive learning classification (ResNet18-NPID [28], ResNet50-NPID [28], few-shot image classification (Conv64FF [29], Conv4Net [4]), and image retrieval (ResNet50-MS [30]) are used for evaluating the effectiveness of the proposed method. Among the 9 CNN models, two of them (Conv4Net, and Conv64F) use LeakyReLU [25] as the activation function layer while ReLU [24] is employed as the activation function layer in the other seven CNNs of ResNet18, ResNet50, ResNet50+CBAM, ResNet50+BAM, ResNet18-NPID, ResNet50-NPID, and ResNet50-MS."}, {"title": "B. Dataset and Experiment Setting", "content": "The experiments are conducted on the ImageNet2012 dataset [31] and the fine-grained image dataset CUB200 [32] as used by the nine CNN models. The ImageNet dataset contains around 1.3 million training images and 50,000 images in validation set, labelled across 1,000 semantic categories. The CUB200 includes 11,788 images from 200 classes.\nBefore feeding into the CNN models, all images are scaled into [0,1] and then normalized by using using mean [0.485,0.456, 0.406] and standard deviation [0.229, 0.224, 0.225]. All our experiments are conducted by using Pytorch library with Python 3.8 on a NVIDIA RTX 3090 GPU."}, {"title": "C. The Neuron Abandoning Attention Flow for Visualizing Evolution of Attentions inside CNN Models", "content": "We first examine the proposed method on the two most widely used CNN models for general image classification, ResNet18 [23] and ResNet50 [23], which was pretrained on ImageNet2012 training set. The publicly released codes and the parameters of their pretrained models\u00b9 are used, which have reported accuracies of 72.12% and 79.26% respectively on the validation set of ImageNet2012. The example sequences of attention maps from internal to output layers visualized by the proposed NAFlow on an image randomly selected from the testing samples correctly predicted by ResNet18 and ResNet50 are shown in Fig. 4 and Fig. 5 respectively."}, {"title": "V. CONCLUSION", "content": "Current methods for visually explaining CNN models are only able to interpret the output layer attention of a CNN when making its decision. This study attempts to look deep inside a CNN model to decipher how the model progressively forms, from layer to layer, its decision. A novel neuron abandoning attention flow method is proposed that can trace and find those neurons that are involved in the decision-making using a neuron abandoning back propagation strategy. Via generating the back-propagated feature maps and tensors of importance coefficients by using inverse function of intermediate layers of the CNN model, we accurately locate decision-making neurons (or neuron-times) for every internal layer to construct attention flow through all layers of the model. This work also fills a missing gap of not able to visually explaining CNNS using similarity metric based classification by proposing a new method of Jacobian matrix computation of channel con-tribution weights. Extensive experiments on nine CNN models for different tasks of general image classification, contrastive learning image classification, few-shot image classification, and image retrieval demonstrate the consistent effectiveness of NAFlow and its potential as a more insightful tool in ex-plaining CNNs, analysing effectiveness of internal components of a model, and guide the design of new models."}]}