{"title": "Reasoning with Reinforced Functional Token Tuning", "authors": ["Kongcheng Zhang", "Qi Yao", "Baisheng Lai", "Jiaxing Huang", "Wenkai Fang", "Dacheng Tao", "Mingli Song", "Shunyu Liu"], "abstract": "In this work, we propose Reinforced Functional Token Tuning (RFTT), a novel reinforced fine-tuning framework that empowers Large Language Models (LLMs) with self-play learn-to-reason capabilities. Unlike prior prompt-driven reasoning efforts, RFTT embeds a rich set of learnable functional tokens (e.g., <analyze>, <verify>, <refine>) directly into the model vocabulary, enabling chain-of-thought construction with diverse human-like reasoning behaviors. Specifically, RFTT comprises two phases: (1) supervised fine-tuning performs prompt-driven tree search to obtain self-generated training data annotated with functional tokens, which warms up the model to learn these tokens for reasoning; and (2) online reinforcement learning further allows the model to explore different reasoning pathways through functional token sampling without relying on prompts, thereby facilitating effective self-improvement for functional reasoning. Extensive experiments demonstrate the superiority of the proposed RFTT on mathematical benchmarks, significantly boosting Qwen-2.5-7B-Instruct (70.6% \u2192 79.8%) and LLaMA-3.1-8B-Instruct (32.2%\u2192 60.2%) on the MATH dataset. Moreover, the performance of RFTT consistently improves with more search rollouts at inference time. Our code is available at https://github.com/sastpg/RFTT.", "sections": [{"title": "1. Introduction", "content": "Recent advances in Large Language Models (LLMs), particularly exemplified by OpenAI-01 (Jaech et al., 2024) and DeepSeek-R1 (Guo et al., 2025), have demonstrated sophisticated reasoning capabilities with remarkable success across various professional domains, such as mathematical analysis (Hendrycks et al., 2021; Cobbe et al., 2021; Patel et al., 2021; He et al., 2024) and algorithmic programming (Chen et al., 2021; Austin et al., 2021; Zhuo et al., 2024). Despite the encouraging results achieved, learning to reason remains a crucial yet challenging task for LLMs as it necessitates high-quality reasoning data for training (Xu et al., 2025), especially for smaller LLMs with 7B or 8B parameters (Guan et al., 2025). Thus, previous attempts often rely on stronger models or human annotators to generate high-quality Chain-of-Thought (CoT) data (Min et al., 2024; Lightman et al., 2024; Huang et al., 2024; Wang et al., 2024a); however, such approaches inevitably incur substantial costs (Wang et al., 2024b; Guan et al., 2025) and are prone to limited scalability (Li et al., 2023; Ahn et al., 2024).\nTo address these limitations, existing studies explore self-play mechanisms where LLMs can iteratively refine their reasoning capabilities through self-generated rationales without human-curated training data (Hao et al., 2023; Zhang et al., 2024c; Wu et al., 2024; Zhang et al., 2024a). For instance, techniques such as CoT prompting enable models to generate multi-step reasoning paths for unlabeled questions, followed by rejection sampling to select high-confidence solutions as training data (Yuan et al., 2023; Liu et al., 2024b; Brown et al., 2024). Drawing inspiration from AlphaGo (Silver et al., 2016), advanced self-play methods further integrate Monte Carlo Tree Search (MCTS) to find high-quality reasoning paths by constructing reasoning trees (Hao et al., 2023; Zhang et al., 2024c;a; Guan et al., 2025). However, these direct tree-search approaches face two fundamental challenges: (1) LLMs tend to generate homogeneous reasoning paths due to their inherent preference for syntactic patterns established during training (Wei et al., 2022; Wang et al., 2023b; Patil, 2025); (2) the high-dimensional search space further limits the exploration capabilities of LLMs, as the combinatorial vocabulary space leads to an exponential growth in candidate reasoning paths (Zhang et al., 2024c).\nTherefore, a recent work, rStar (Qi et al., 2024), introduces functional prompts (e.g., \u201cpropose a one-step thought\u201d, \u201crephrase the question\u201d) that steer tree search by simulating human-like reasoning behaviors, thereby diversifying node exploration while constraining the search space. However, rStar operates purely as an inference-time augmentation without internalizing the reasoning capabilities through model training. The functional prompts act as external constraints rather than learned patterns. As a result, each inference of rStar must rely on iterative tree-search traversal across different functional prompts to seek correct solutions. Compared to direct reasoning methods (which directly generate CoT responses without relying on search), this exhaustive searching approach results in a substantial number of redundant LLM inference calls.\nIn this work, we propose Reinforced Functional Token Tuning (RFTT), a novel reinforced fine-tuning framework that introduces learnable functional tokens (e.g., <analyze>,<verify>,<refine>) into the model vocabulary to facilitate self-play learn-to-reason. Technically, RFTT operates in two phases: (1) during the Supervised Fine-Tuning (SFT) warmup phase, RFTT employs functional prompt-guided MCTS to construct reasoning trees, where both correct and incorrect solution paths are merged to synthesize human-like reasoning paths. These self-generated paths explicitly connect reasoning nodes through functional tokens, thereby forming structured training data for learning to reason with these tokens. (2) In the online Reinforcement Learning (RL) phase, the model transitions from prompt-guided to token-guided reasoning by sampling functional tokens to autonomously expand reasoning trees. Through self-play exploration, the model reinforces high-value reasoning pathways, ultimately achieving autonomous self-improvement of functional reasoning capabilities. Our core contributions are summarized as follows:\n\u2022 We propose a new learn-to-reason paradigm that pioneers the integration of learnable functional tokens into the model vocabulary, enabling the model to establish internalized token-guided reasoning patterns rather than relying on external prompt-guided constraints.\n\u2022 We devise RFTT, a novel reinforced fine-tuning framework for self-play learning to reason. The SFT phase bootstraps reasoning through self-generated training data annotated with functional tokens, while the RL phase enables autonomous exploration of reasoning pathways through token-guided tree traversal, achieving self-improvement for functional reasoning.\n\u2022 Extensive experiments conducted on various mathematical benchmarks demonstrate that RFTT yields significantly superior results to state-of-the-art counterparts. Notably, the performance of RFTT continues to improve as the number of search rollouts increases during inference."}, {"title": "2. Related Works", "content": "Mathematical Reasoning. Recent advancements in LLMs for mathematical reasoning have primarily focused on two broad categories of approaches: prompt engineering and learning to reason. Prompt engineering techniques design specific prompting strategies, such as CoT (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2023b; Zhang et al., 2023), multi-path reasoning (Yao et al., 2023a;b; Besta et al., 2024) or divide-and-conquer (Zhou et al., 2023; Sel et al., 2024; Wang et al., 2023a) to guide the model to generate step-by-step reasoning processes during the inference phase. In contrast, learn-to-reason approaches train the model explicitly to improve its intrinsic reasoning ability. Extensive math and code corpora are first utilized to build the fundamental reasoning capability in the pre-training phase (Lewkowycz et al., 2022; Shao et al., 2024; Yang et al., 2024b; Abdin et al., 2024; Dubey et al., 2024) and various techniques, such as supervised fine-tuning (Yuan et al., 2023; Wang et al., 2024a; Min et al., 2024), direct preference optimization (Rafailov et al., 2023; Lai et al., 2024) and reinforcement learning (Trung et al., 2024; Shao et al., 2024; Guo et al., 2025) are used to further enhance the mathematical reasoning ability during post-training.\nTree-Search Reasoning. While most CoT methods have typically adopted an auto-regressive reasoning manner, there has been a trend to engage in more complicated reasoning architectures like trees (Koh et al., 2024; Zhang et al., 2024b). Recently, various methods for exploring tree structures have been devised to identify optimal reasoning trajectories, e.g., tree of thought (Yao et al., 2023a), graph of thoughts (Besta et al., 2024) and Monte Carlo tree search (Hao et al., 2023; Chen et al., 2024; Zhang et al., 2024c). These direct tree-search methods struggle with limited exploration due to the homogeneous reasoning paths and the large search space. Although rStar (Qi et al., 2024) utilizes functional prompts in tree search to diversify and constrain the search process, they only act as external constraints in the inference phase, while we introduce learnable functional tokens during model training to facilitate effective and efficient exploration, thus improving the model's intrinsic reasoning ability.\nLLM Self-Improvement. Self-improvement with self-generated reasoning paths has gained increasing interest in mathematical reasoning due to the lack of golden rationales. Early methods (Yuan et al., 2023; Zelikman et al., 2022) utilize reject sampling to select reasoning paths with correct answers for iterative self-training, which often leads to local optimal due to the homogeneity of self-generated reasoning paths. Recent approaches explore MCTS (Zhang et al., 2024c; Tian et al., 2024; Hao et al., 2023; Zhang et al., 2024a) to generate more diverse reasoning paths offline or adopt random sampling to produce abundant reasoning paths during the online reinforcement learning phase (Trung et al., 2024; Wang et al., 2024b; Lambert et al., 2024). However, these methods struggle to fully explore the reasoning space, especially for smaller language models. In RFTT, we employ function tokens for automatic exploration of the reasoning space through token-guided tree traversal, achieving self-improvement for functional reasoning."}, {"title": "3. Methodology", "content": "In this section, we first provide the problem formulation. Then we further detail the proposed learn-to-reason framework of RFTT. The whole procedure is outlined in Fig. 1.\n3.1. Problem Formulation\nWe consider a complex reasoning task as a multi-step reasoning generation problem, which decomposes the problem into a sequence of simpler intermediate steps through structured reasoning operations. Assuming a problem x requires T intermediate reasoning steps, the reasoning path can be denoted as $\\mathcal{T} = {s_0, s_1, s_2, ..., s_T}$, where $s_0 = x$ and $s_t$ is the t-th intermediate reasoning steps. Furthermore, we model this process as a tree search, where the root node represents the problem x, the edges denote the action space A (which can be special tokens or prompts), and children nodes are next steps generated by a policy model $\\pi_\\theta$ with parameter $\\theta$ under corresponding actions. Each distinct path from the root to the terminal node denotes a sequence of decision-making steps that constitutes a potential solution trajectory $\\mathcal{T}$. For any given problem x, the subset of its solution space $\\mathcal{T} = {\\mathcal{T}_1, \\mathcal{T}_2, ..., \\mathcal{T}_N }$ can be extracted from the tree structure. Our aim is to sample diverse reasoning paths and ultimately derive optimal solutions.\nConsidering the problem setup above, we can transfer the task of better reasoning to better selecting actions based on previous steps. In this work, we focus on how to inspire intrinsic reasoning abilities with functional tree search. The conventional natural language generation task often regards vocabulary-level token sampling as actions. If we adopt vocabulary-level actions to conduct tree search on LLMs, the action space can be extremely large, leading to a sharp increase in computational complexity and reduced search efficiency. Thus we design a series of human-like reasoning behaviors as action space. During the preliminary warmup phase for SFT, we employ functional prompts as the action space for exploratory tree search, while transitioning to functional tokens during RL optimization.\n3.2. Functional Tokens with Prompts\nWe propose several functional tokens with prompts as actions to emulate how humans engage in complex reasoning:\n\u2022 <clarify> (a1): Organize the conditions of the given complex question and express them in a clearer form to avoid misunderstanding.\n\u2022 <analysis> (a2): Analyze the general problem-solving idea and the knowledge that might be involved.\n\u2022 <subquestion> (a3): Break down complex reasoning problems into sub-problems that are easier to tackle.\n\u2022 <next_step> (a4): Propose the next intermediate step based on the existing reasoning steps.\n\u2022 <direct_answer> (a5): Complete the remaining steps until achieving the final answer step-by-step.\n\u2022 <verify> (a6): Reflect if there are any mistakes in the preceding reasoning steps considering LLMs might make computational or logical errors in a reasoning step.\n\u2022 <refine> (a7): Correct certain previous steps if the errors are identified, ensuring that each step is accurate to achieve a final outcome.\n\u2022 <output> (a8): Output the final answer that is in the corresponding expected format.\nAll functional prompts are detailed in Appendix A.\n3.3. Functional Monte Carlo Tree Search\nThe trajectories generated by direct tree search methods are often homogeneous due to the inherent preference for syntactic patterns in LLMs. To address this, we introduce"}, {"title": "Step 1: Functional Tree Search", "content": "We now proceed to details on how to employ functional prompts in SFT (or functional tokens in RL) to search candidate reasoning paths, which consists of four basic operations: selection, expansion, simulation and backpropogation.\nStarting from root node (question) $s_0 = x$, a series of internal nodes ${s_1, s_2, ..., s_t}$ are chosen until reaching a leaf node $s_i$. In the selection phase, we use the Upper Confidence Bound applied to trees (UCT) to select each node for balancing exploration and exploitation:\n$UCT(s) = \\frac{Q(s)}{N(s)} + c \\sqrt{\\frac{\\ln \\tilde{N}(s)}{N(s)}}$ (1)\nwhere Q(s) is the value of node s, N(s) is the number of times visited s, $\\tilde{N}(s)$ is the number of times visited the parent node of s, and c is the exploration weight.\nIn the expansion phase, the selected leaf node $s_i$ is expanded by action space A = {a1, a2, a3, a4, a5, a8}. We use action $a_i \\in A$ (functional prompt in SFT or functional token in RL) to guide the LLM to generate the next reasoning step. The simulation phase iteratively selects and expands from"}, {"title": "Step 2: Cross Verification", "content": "To construct a complex reasoning structure that incorporates self-verification for SFT, we generate an extra node via cross verification between two trajectories and thereby seamlessly concatenate them. Given a question x and its ground truth y, we can get candidate reasoning trajectories $\\mathcal{T}$ as stated in step 1. We first choose a correct trajectory with the highest average process rewards given by a process reward model:\n$\\mathcal{T}_c = \\underset{\\mathcal{T}_i \\in \\mathcal{T}_c}{\\arg \\max} \\mathcal{R}(\\mathcal{T}_i)$, (2)\nwhere $\\mathcal{T}_c = {\\mathcal{T}_i \\in \\mathcal{T} | ANS(\\mathcal{T}_i) = y}$, $ANS(\\mathcal{T}_i)$ is the final answer of trajectory $\\mathcal{T}_i$, and $\\mathcal{R}(\\mathcal{T}_i)$ represents the average rewards of all intermediate steps in trajectory $\\mathcal{T}_i$. We can further separate a subset $\\mathcal{T}_w$ from $\\mathcal{T}$, which contains the"}, {"title": "Step 3: Branch Merging", "content": "We adopt branch merging to form structured reasoning paths that feature self-verification and self-correction:\n$\\mathcal{T}_f = \\mathcal{T}^+ \\cup \\mathcal{T}^w \\cup {s_v} \\cup \\mathcal{T}^-_c$. (6)\nFinally, we concatenate intermediate reasoning steps in trajectories by functional tokens, generating a trainable reasoning path for warmup in supervised fine-tuning:\n$\\mathcal{D}_{SFT} = {\\underset{s_i \\in \\mathcal{T}_f}{\\mathcal{\\cup}} (<a>s_i</a>)}$, (7)\nwhere $\\mathcal{\\cup}$ denotes the string concatenation operation, $s_i$ is the intermediate reasoning step, and $a_i$ is the corresponding functional token."}, {"title": "3.4. Reinforced Functional Token Tuning", "content": "Our primary goal is to empower LLMs with self-play learn-to-reason capabilities. To achieve this, we introduce a two-phase training strategy: the SFT phase learns functional tokens for reasoning by self-generated training data annotated with these tokens, while the RL phase further enables autonomous exploration of reasoning paths through functional token-guided tree search, thereby reinforcing high-value reasoning paths.\nPhase 1: Supervised Fine-Tuning\nIn this stage, an initial base model $\\pi_\\theta$ is warmuped on the dataset $\\mathcal{D}_{SFT}$ that consists of the tuples of \"(question, reasoning path)\" to obtain SFT model $\\pi^{SFT}$. Specifically, we treat functional tokens as new special tokens that can be embedded directly into the model vocabulary. During the SFT phase, the model learns to reason with these functional tokens without relying on prompts."}, {"title": "Phase 2: Online Reinforcement Learning", "content": "After being trained with functional tokens, the policy model $\\pi_\\theta$ can explicitly and dynamically select functional tokens and further generate the specific reasoning steps associated with these tokens. Through this design, the policy model can reinforce the use of key functional token combinations via a form of online self-play, significantly improving its learning efficiency. Specifically, the policy model learns by iteratively searching, evaluating the process in each rollout, and updating its parameters, as illustrated in Fig. 2. Note that the functional token can significantly reduce the search space of tree search.\nLike AlphaZero (Silver et al., 2018), the functional token is sampled based on the log-likelihood scores if there exist unexplored actions, otherwise select them according to the UCT score:\n$a_t = \\begin{cases}  \\underset{a \\in \\mathcal{A}}{\\arg \\max} \\pi_{\\theta}(a | s_{0:t}), & \\exists a \\in U(s_t), \\\\  \\underset{a \\in \\mathcal{A}}{\\arg \\max} \\text{UCT}(s_t), & \\text{otherwise}, \\end{cases}$ (8)\nwhere $U(s_t)$ denotes unexplored functional tokens of intermediate reasoning step $s_t$ and $s_{0:t}$ represents all intermediate steps from timestep 0 to t.\nFurthermore, the reward function is defined as:\n$\\mathcal{R}_t(s_{0:t}, a_t, s_{t+1}) = \\mathcal{R}_M(s_{0:t}, a_t, s_{t+1}) - \\beta \\cdot KL(t)$, (9)\n$\\mathcal{R}_M(s_{0:t}, a_t, s_{t+1}) = \\begin{cases}  1, & \\text{ANS}(s_{t+1}) = y, \\\\  0.1, & \\text{ANS}(s_{t+1}) \\neq \\text{null}, \\neq y, \\\\  \\sigma, & \\text{ANS}(s_{t+1}) = \\text{null}, \\end{cases}$ (10)\n$KL(t) = \\log (\\frac{\\pi_{\\theta}(a_t | s_{0:t})}{\\pi_{SFT}(a_t | s_{0:t})}),$ (11)\nwhere the SFT model $\\pi_{SFT}$ serves as the reference model, $\\beta$ is KL coefficient, and $\\sigma$ is the process reward. Note the $\\sigma$ can be assigned to 0 if using outcome rewards. Our approach can be flexibly integrated with a Process Reward Model (PRM) or an Outcome Reward Model (ORM).\nDuring online RL, the model parameters are optimized using the Reinforce++ algorithm (Hu, 2025). The core policy objective combines clipped updates with normalized advantage values through the following loss function:\n$\\mathcal{L}_{RL}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( \\text{clip} (\\frac{\\pi_{\\theta}(a_t | s_{0:t})}{\\pi_{\\theta old}(a_t | s_{0:t})}, 1 - \\epsilon, 1 + \\epsilon) A_t \\right) \\right]$, (12)\nwhere $\\pi_{\\theta old}$ is the previous policy model, $\\epsilon$ is the clipping coefficient, and the normalized advantage value $A_t$ is calculated based on the reward $\\mathcal{R}_t$."}, {"title": "4. Experiments", "content": "4.1. Experimental Setups\nDatasets. For training, we only adopt the training set of the MATH dataset as our training data. For evaluation, we employ five established mathematical reasoning benchmarks: MATH (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), Olympiad Bench (He et al., 2024), and AMC\u00b9. To ensure evaluation efficiency and prevent data contamination, we use the MATH-500 (Lightman et al., 2024) for MATH evaluations while retaining full test sets for other datasets. More information about our training and evaluation dataset are listed in Appendix D.\nBase Models. RFTT is generally applicable to a wide range of LLMs. In our experiments, we select three popular open-source LLMs: LLaMA-3.2-3B-Instruct, LLaMA-3.1-8B-Instruct (Dubey et al., 2024) and Qwen-2.5-7B-Instruct (Yang et al., 2024b). By focusing on LLMs with relatively small parameters under 10B, we expect that RFTT enables the smaller model to learn to reason via self-play, ultimately achieving results comparable to or exceeding larger LLMs in complex reasoning tasks.\nBaselines. We compared RFTT against four categories of baselines in our experiments: (1) Superior LLMs, including leading closed-source models and open-source models; (2) Model fine-tuning methods, including SFT and ReFT (Trung et al., 2024); (3) In-context learning methods, including Zero-shot, Zero-shot CoT, Few-shot CoT, and CoT+SC@4; (4) Tree search methods, including ResT-MCTS* (Zhang et al., 2024c), rStar (Qi et al., 2024) and LLaMA-Berry (Zhang et al., 2024b).\nEvaluation Metric. We employ Pass@1 accuracy across all benchmark datasets as our evaluation metric, with correctness determined through comparison between generated outputs and ground truth. To extract answers reliably, we require the model to wrap its final answer in boxed{}.\n4.2. Implementation Details\nAll experiments run on 8\u00d7A800-80GB GPUs. While performing MCTS to collect diverse reasoning paths via various functional prompts, we use Qwen-2.5-7B-Instruct as the generator and math-shepherd-mistral-7b-prm (Wang et al.,"}, {"title": "4.3. Main Results", "content": "Results on Different Benchmarks. We conduct a comprehensive evaluation of the effectiveness of RFTT across different mathematical benchmarks. Table 1 presents a comparative analysis between our framework and state-of-the-art baselines. We highlight three key findings: (1) RFTT can significantly improve the complex reasoning proficiency of small-parameter LLMs. For example, LLaMA-3.1-8B-Instruct initially achieves 50.6% accuracy on the MATH benchmark using CoT prompting technique. However, after training by RFTT, its Pass@1 accuracy improves to 60.2%, surpassing self-consistency sampling. Similarly, Qwen-2.5-7B-Instruct with RFTT achieves performance on par with Qwen-2.5-14B-Instruct (79.6% vs. 80% on MATH), indicating that smaller models have developed robust reasoning capabilities without any reasoning prompts. (2) RFTT outperforms ReFT on various mathematical reasoning tasks. We observe that RFTT achieves consistent performance improvements over ReFT, notably surpassing it by an average margin of 5% across different benchmark evaluations. This result demonstrates the robustness of RFTT and the enormous potential for further self-play training. (3) RFTT has also shown strong generalization ability on other challenging mathematical problems, including AMC and Olympiad Bench. As mentioned in Sec 4.1, our training dataset only consists of the training data from MATH, and there may be a"}, {"title": "4.4. Ablation Studies", "content": "We ablate core components of the RFTT: (1) RFTT w/o SFT Warmup directly trains the original instruction model using ORM-based RL; (2) RFTT w/o MCTS adopts random sampling and ORM in RL phase after SFT Warmup; (3) RFTT w/o PRM employs MCTS sampling and ORM in RL phase after SFT Warmup.\nThe Effectiveness of SFT Warmup. The SFT phase embeds the functional tokens into the model vocabulary, enabling self-improvement with token-guided exploration during the RL phase. To verify the effectiveness of our SFT phase, we conduct experiments to directly train the original instruction model without SFT warmup using pure RL. As compared from the first two lines of Table 4, even with the same random sampling method, the model warmuped with SFT (RFTT w/o MCTS) surpasses the pure RL model (RFTT w/o SFT warmup) by a large margin (5.7% for Qwen-2.5-7B-instruct and 7.7% for LLaMA-3.1-8B-instruct), indicating the token-guided reasoning can explore the reasoning space much more effectively after SFT initialization.\nThe Effectiveness of MCTS. Our functional token-driven MCTS enables a more effective exploration of the solution space in both inference and training. We conducted an ablation study on MCTS sampling during the RL phase to investigate the influence of data diversity, as shown in Table 4. The outcomes reveal that random sampling in"}, {"title": "4.5. Evolution Analysis", "content": "Response Length and Accuracy. We demonstrate the training curve of RFTT on Qwen-2.5-7B-Instruct in Fig. 4. As depicted, the average batch accuracies on the training set exhibit a consistent trend of improvement, while the trend of average response lengths displays distinct patterns under varying reward assignment strategies. The response length increases sharply within 50 training steps first and then gradually increases when using PRM in RFTT. The searching time of the policy model undergoes continuous improvement during the RL training process, developing the capacity for exploration and optimization with appropriate incentives.\nWe observe that as the search depth increases, the policy reinforces predefined human-like reasoning behavior, including the self-correction mechanism and the exploration of diverse problem-solving solutions, thereby evolving to tackle challenging tasks. Please refer to Appendix E for more detailed discussions.\nRewards and KL Divergence. We also demonstrate the development of average rewards and KL Divergence during the RL phase. Both of them improve steadily, which reflects the continuous improvement of the policy model. This phenomenon validates that through the appropriate definition of fine-grained reasoning steps (guided by functional tokens) and incentive mechanisms, it encourages the policy to autonomously explore advanced problem-solving strategies with high value in mathematical reasoning."}, {"title": "5. Conclusions", "content": "This work proposes RFTT, a reinforced fine-tuning framework that equips LLMs with self-play reasoning capabilities. By embedding learnable functional tokens into the model vocabulary, RFTT enables LLMs to internalize human-like reasoning behaviors, eliminating reliance on external prompts. Extensive experiments validate the effectiveness of RFTT, achieving significant performance gains and demonstrating scalability with inference-time search. This work advances the self-play paradigm by bridging token-guided reasoning and model training, offering a promising direction for developing resource-efficient, generalizable reasoning capabilities in smaller LLMs. Currently, our work focuses primarily on mathematical reasoning tasks; its effectiveness on broader reasoning domains needs further exploration."}, {"title": "A. Functional Prompts", "content": "We present the designed functional prompts used in our experiments.\nPrompt for Functional Token <clarify>\nYou are an AI assistant to help me clarify questions by restating the original question and task in a clear and comprehensive manner. In your clarified version, ensure that all information from the original question is fully expressed. Following are some useful examples.\nOriginal Question: Increasing the radius of a cylinder by 6 units increased the volume by y cubic units. Increasing the height of the cylinder by 6 units also increases the volume by y cubic units. If the original height is 2, then what is the original radius?\nClarified Question: We are given a problem involving a cylinder where increasing the radius by 6 units and increasing the height by 6 units each results in an increase in volume by y cubic units. Our goal is to find the original radius of the cylinder, given that the original height is 2.\nOriginal Question: A wooden model of a square pyramid has a base edge of 12 cm and an altitude of 8 cm. A cut is made parallel to the base of the pyramid that separates it into two pieces: a smaller pyramid and a frustum. Each base edge of the smaller pyramid is 6 cm and its altitude is 4 cm. How many cubic centimeters are in the volume of the frustum?\nClarified Question: Let's understand what is being asked. Given a wooden model of a square pyramid has a base edge of 12 cm and an altitude of 8 cm. Also given a cut parallel to the base creates a smaller pyramid and a frustum. Note that the smaller pyramid has a base edge of 6 cm and an altitude of 4 cm. We need to find the volume of the frustum.\nQuestion: { uesr_question }\nPrompt for Functional Token <analysis>\nGiven a math problem and an existing incomplete solution, your task is to provide a brief insight for the following problem. Note that it's not required to provide any specific solution process or calculation, but give a general guidance of sovling this problem. Usually one or two sentences are enough.\nQuestion: { uesr_question }\nPrompt for Functional Token <subquestion>\nGiven a math problem and an existing incomplete solution, please propose a sub-problem that can be solved independently based on my current progress. The output format is limited to: \"Let's ... now\". Where ... indicates the omitted sub-problem that you should fill in. Note that do not propose sub-problems that have already been solved in previous steps.\nHere is the input, please follow the restricted output format.\nQuestion: { uesr_question }\nPrompt for Functional Token <next_step>\nGiven a math problem and an existing incomplete solution, your task is to propose one next step in a smooth and proper way.\nIf no existing steps are provided, you need to briefly analyse the problem from scratch and then output the first step. Otherwise, you need to output the correct next step of the existing solution, following the ideas of the existing steps.\nYour output should be a single reasoning step that may include reasoning, detailed calculations process, choosing answers, etc.\nQuestion: { uesr_question }"}, {"title": "Prompt for Functional Token", "content": "Given a math problem and an existing incomplete solution, your task is to complete the remaining solution in a smooth and proper way. You need to give step-by-step solutions to the problem following the ideas of the existing steps, and do not repeat any existing steps.\nQuestion: { uesr_question }\nPrompt for Functional Token \nGiven a problem and its previous solution steps, there is a correct subsequent solution and your subsequent solution. Please meticulously review your subsequent solution step by step and identify if any mistakes occur.\n# Correct solution\n{correct_solution}\n# Your subsequent solution\n{solution}"}, {"title": "B. Reasoning Cases", "content": "Our constructed data consists of diverse human-like reasoning behaviors", "1": "Self-Verification\n# Problem: Complex numbers a", "Output": "clarify) The complex numbers a", "have": "na+b+c = 36.\n</next_step)\n(subquestion) Next, I need to find the value of |ab + ac + bc|. (/subquestion)\n("}]}