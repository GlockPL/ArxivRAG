{"title": "UNDERSTANDING LAYER SIGNIFICANCE IN LLM ALIGNMENT", "authors": ["Guangyuan Shi", "Zexin Lu", "Xiaoyu Dong", "Wenlong Zhang", "Xuanyu Zhang", "Yujie Feng", "Xiao-Ming Wu"], "abstract": "Aligning large language models (LLMs) through fine-tuning is essential for tailoring them to specific applications. Therefore, understanding what LLMs learn during the alignment process is crucial. Recent studies suggest that alignment primarily adjusts a model's presentation style rather than its foundational knowledge, indicating that only certain components of the model are significantly impacted. To delve deeper into LLM alignment, we propose to identify which layers within LLMs are most critical to the alignment process, thereby uncovering how alignment influences model behavior at a granular level. We propose a novel approach to identify the important layers for LLM alignment (ILA). It involves learning a binary mask for each incremental weight matrix in the LoRA algorithm, indicating the significance of each layer. ILA consistently identifies important layers across various alignment datasets, with nearly 90% overlap even with substantial dataset differences, highlighting fundamental patterns in LLM alignment. Experimental results indicate that freezing non-essential layers improves overall model performance, while selectively tuning the most critical layers significantly enhances fine-tuning efficiency with minimal performance loss.", "sections": [{"title": "1 INTRODUCTION", "content": "Aligning large language models (LLMs) with specific requirements is essential for enhancing their utility across diverse applications (Luo et al., 2023a; Yu et al., 2023; Luo et al., 2023b; Li et al., 2023; Liu et al., 2024; 2022; Feng et al., 2023). Fine-tuning LLMs during the alignment process can significantly improve the models' capabilities to meet targeted needs (Bubeck et al., 2023). Typically, alignment involves fine-tuning the model on diverse datasets, which may include both human-curated (Rajani et al., 2023) and LLM-generated (Taori et al., 2023) data. Such fine-tuning approaches encompass instruction tuning (Wei et al., 2021) and preference learning (Bai et al., 2022; Rafailov et al., 2024). Given the significant cost associated with full parameter fine-tuning, parameter-efficient fine-tuning (PEFT) (Hu et al., 2021; Chen et al., 2022; Pan et al., 2024) algorithms have emerged as a popular alternative, offering a balance between performance and resource efficiency.\nUnderstanding what LLMs actually learn during the alignment process remains a critical question. LIMA (Zhou et al., 2023) posits that the majority of knowledge and capabilities are developed during the pretraining phase, with alignment primarily serving to refine the model's conversational style and formatting. Using a well-selected set of 1,000 training examples for supervised fine-tuning (SFT), LIMA successfully produced a high-quality aligned model. Similarly, URIAL (Lin et al., 2023) investigated the token distribution of LLMs before and after alignment and found that most changes were related to \"stylistic tokens\", such as discourse markers and transition words, while the knowledge-intensive content largely remained untouched, coming from the base pre-trained model."}, {"title": "2 QUANTIFYING LAYER SIGNIFICANCE IN LLM ALIGNMENT", "content": "To better understand layer significance in the alignment process of an LLM, we propose a method to identify the important layers during alignment, abbreviated as ILA. This approach involves learning a binary mask that serves as an significance indicator for each layer.\nConsider a pre-trained LLM model with parameters 90 composed of N layers, i.e., 00 = {0}\u2081 The model is fine-tuned on an alignment dataset D = {zi}=1 with a loss function L(0). After t training iterations, the model parameters are updated to t = 00 + \u2206t, where At represents the change in parameters till iteration t. Define a binary mask yt = {\u03b3\u03ad \u2208 {0,1}}\u2081 that encodes layer-wise importance information. We apply yt to Aet and define\n\u03b8mask = 00 + Yt\u2299 \u2206\u04e9t,\nwhere is component-wise multiplication. The binary mask is applied to retain the changes in crucial layers while eliminating the rest. Below we provide a formal definition of the conditions under which training attains stability after an adequate number of iterations.\nDefinition 1 (e-stable). Ve > 0, the model is said to be e-stable at iteration T if, for any t > T, the loss function satisfies the condition\n|Ez[L(0t+1, \u2248)] \u2013 Ez[L(\u04e9t, z)]| < \u20ac,\nwhere Ez[:] denotes the expectation with respect to the alignment dataset D.\nOnce training becomes stable, we can identify the layers that are crucial for the alignment task.\nDefinition 2 (Layer Importance). The binary mask Yt is defined as the solution to the following optimization problem:\nYt = arg min L(mask), s.t. ||7t|| < H,\nYt\nwhere H is a hyper-parameter that serves as a constraint to limit the number of important layers.\nEfficiently Identifying the Importance Layers. Due to the high cost of fine-tuning large models, to address the optimization problem in Eq. (3), we employ the LoRA (Hu et al., 2021) algorithm, which utilizes low-rank decomposition matrices to represent the change in model parameters till iteration t (\u2206t). Specifically, LoRA utilizes two trainable low-rank matrices, B \u2208 Rdi\u00d7ri and A \u2208 Rri\u00d7ki, to estimate the change of the ith layer:\n\u0394\u0398 = \u03b2. \u0392\u0391,\nwhere \u1e9e is the scalar hyperparameter of LoRA. With the binary mask Yt, the ith layer is updated by\n\u03b8 = 0 + \u03b2\u00b7 \u03b3\u0390\u00b7 BA\nTo ease the optimization of yt, we re-parametrize each of its each components y\u1ec9 as the output of a Sigmoid function, i.e., \u03b3\u03b9 = \u03c3(s). Then, the update of the ith layer becomes\n\u03b8 = \u03b8\u03bf + \u03b2\u00b7 \u03c3(\u03b5)\u00b7 \u0392\u0391.\nLet st = {s}1,0 = {0}1. The optimization problem in Eq. (3) becomes\nt\nst = arg min (\u04e9\u043c).\nSt\nWe use gradient descent to optimize st. The found s\u012f is considered an importance score of the ith layer. A larger value of s\u012f indicates y\u012f is closer to one, signifying higher importance of the ith layer."}, {"title": "3 EXPERIMENTAL SETUP", "content": "Datasets. We train LLMs on three different alignment datasets, namely Alpaca-GPT4 (Peng et al., 2023), LIMA (Zhou et al., 2023), and No Robots (Rajani et al., 2023). The characteristics of each dataset are described as follows: (1) Alpaca-GPT4 contains 52K instruction-following data generated by GPT-4, utilizing prompts from Alpaca (Taori et al., 2023). (2) LIMA contains only 1K carefully curated prompts and responses. (3) No Robots contains 10K instructions and demonstrations created by skilled human annotators.\nModels and Baselines. We use four different models as the base for our experiments: LLAMA 2-7B (Touvron et al., 2023), LLAMA 2-13B (Touvron et al., 2023), Llama 3.1-8B (Dubey et al., 2024), and Mistral-7B-v0.1 (Jiang et al., 2023). The baselines include (1) LoRA (Hu et al., 2021): We add trainable pairs of rank decomposition matrices in parallel to existing weight matrices, including query/key/value projection (Wq, Wk, Wv), output projection (W) in the self-attention, feed-forward networks (Wup, Wdown, Wgate), and the output layer on top of the transformer (Whead). (2) AdaLoRA (Zhang et al., 2023a): It dynamically adjusts the rank of incremental matrices to control the parameter budget. Similar to LoRA, we add AdaLoRA modules to all linear layers of the base model. (3) QLoRA (Dettmers et al., 2023): It is a fine-tuning method that significantly reduces memory usage by quantizing the weights of pre-trained language models while maintaining competitive performance. (4) Full Finetune: The model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.\nEvaluation and Training Setup. Our evaluation of language model alignment encompasses two main dimensions: (1) Language Understanding Ability: We utilized three distinct datasets (i.e., MMLU (Massively Multitask Language Understanding) (Hendrycks et al., 2021) and Hellaswag (Zellers et al., 2019) to evaluate this aspect. MMLU evaluates models across diverse subjects requiring specialized knowledge, while Hellaswag tests commonsense reasoning by asking the model to predict the most plausible continuation of a given context. (2) Conversational Ability: We use two different datasets: MT-Bench (Zheng et al., 2023), which involves multi-turn conversations, and Vicuna (Chiang et al., 2023), which involves single-turn conversations. We use GPT-4 to score the responses. We asks GPT-4 to grade and give a score to model's answer directly without pairwise comparison, using the implementation version of MT-Bench (Zheng et al., 2023). For a fair comparison, we conduct a small range of training hyperparameter searches for LoRA and full fine-tuning to ensure that we get strong baselines. More details are provided in Appendix B."}, {"title": "Targeted Performance", "content": "(1) Language Understanding Ability: Recent research (Du et al., 2020; Sun et al., 2021; Dubey et al., 2024) suggests that the learning of language understanding tasks essentially occurs during the pre-training phase of the base model. Therefore, significant performance improvements in language understanding tasks (i.e., MMLU, Hellaswag) after alignment are not expected. However, it is crucial to ensure the model retains the learned knowledge during alignment. (2) Conversational Ability: Without alignment, the pre-train model's conversational ability is poor. For example, LLAMA 2-7B often produces incorrect or irrelevant responses on the Vicuna dataset. However, its conversational ability can be significantly improved through the alignment process."}, {"title": "4 EMPIRICAL FINDINGS", "content": ""}, {"title": "4.1 LAYER SIGNIFICANCE IN LLM ALIGNMENT", "content": "In this subsection, we applied our ILA algorithm to identify the ranking of important layers during alignment across three different datasets: No Robots, LIMA, and Alpaca-GPT4, as shown in Fig. 1. Additionally, we analyzed the importance ranking of layers identified at different training milestones, as depicted in Fig. 2. To further validate the similarity of these important layers, we used the Jaccard similarity coefficient to quantify the relationship between two sets. Specifically, we defined the top 75% highest-scoring layers as the important layers, denoted as set S. The similarity between two distinct sets, S\u2081 and S2, is calculated as: J(S1, S2) =  S1\u2229S2/ S1\u222aS2 . A value of J = 1 indicates identical sets, while J = 0 indicates no overlap. Below, we highlight our main observations.\nConsistency in Layer Importance Ranking Across Various Alignment Datasets. Our findings demonstrate a remarkable consistency in layer importance ranking, as evidenced by: (1) the retrieval of highly similar important layers across different alignment datasets, as shown in Fig. 1 and Table 2; (2) the consistent identification of important layers despite the optimization of y with varying random seeds, as illustrated in Table 3; (3) the ability to identify similar important layers at different or early (25%) training stages, as depicted in Fig. 2 and Table 4.\nThe experimental results corroborate the robustness of our algorithm, which consistently identifies stable and similar layers across different alignment datasets. This is particularly noteworthy in light of recent work that suggests alignment fundamentally involves shifts in stylistic tokens (Lin et al., 2023). Thus, the essence of alignment is the pursuit of similar capabilities, which aligns with our discovery that the important layers corresponding to different datasets exhibit similarity. This convergence of findings underscores the intrinsic alignment of our algorithm's performance with the fundamental objectives of dataset alignment.\nGiven the established importance ranking of the model layers, which proves stable for the alignment task, we must consider how to leverage this ranking. We will address this from both performance and efficiency perspectives. First, to maximize the performance of the fine-tuned model, we should avoid fine-tuning layers that could negatively impact the model, focusing instead on those deemed less significant. Second, to enhance the efficiency of fine-tuning and minimize resource consumption, we should concentrate our efforts on layers that are particularly vital to the model's success. Detailed experiments and analyses of these two cases will be presented in the following section."}, {"title": "4.2 ENHANCING ALIGNMENT PERFORMANCE THROUGH FREEZING UNIMPORTANT LAYERS", "content": "To achieve optimal model performance, we excluded the unimportant layers, specifically those whose modifications would negatively impact fine-tuning. Approximately 25% of the unimportant layers were removed. The main results on No Robots and LIMA are presented in Table 3 and Table 4 respectively. For additional results of LLAMA 2-13B and main results on Alpaca-GPT4 dataset, please refer to Appendix C. Based on the results, we highlight two key observations:"}, {"title": "4.3 ENHANCING ALIGNMENT EFFICIENCY BY ONLY FINE-TUNING THE CRITICAL LAYERS", "content": "To investigate this issue, we fine-tuned only 10%, 20%, and 30% of the important layers of Mistral-7B-v0.1, as identified by ILA, on the No Robots dataset, and compared the results with the LORA algorithm. The results demonstrate clear benefits in focusing on a subset of important layers:\n(1) Fine-Tuning a Small Subset of Important Layers Achieves Competitive Performance and Enhances Efficiency. Fine-tuning the top 10% or 20% of important layers results in only a slight performance drop compared to full fine-tuning, while fine-tuning 30% of the parameters nearly matches the performance of full fine-tuning (see Table 5). This demonstrates that focusing on a small, carefully selected subset of important layers is sufficient for efficient fine-tuning without significant performance loss. (2) Our Method Can be Applied to Enhance QLoRA, Further Reducing Cost. By integrating our method with QLoRA, we fine-tuned only about 30-75% of the key layers while maintaining or improving model performance (see Table 6). This highlights the efficiency of our approach, achieving comparable or superior results with significantly fewer layers involved.\nThese findings underline the robustness of our layer selection strategy, allowing efficient use of resources with minimal trade-offs in performance. Additionally, our integration with QLoRA confirms that fine-tuning only a targeted subset of important layers enhances both the performance and efficiency of state-of-the-art methods in reducing memory usage during fine-tuning."}, {"title": "4.4 ABLATION STUDY", "content": "Observation 1: Randomly or manually selecting layers for fine-tuning does not work.\nTo substantiate the accuracy and efficacy of the ranking and importance layers identified by our algorithm, we contrast the baseline that optimizes all linear layers without any freezing with three alternative scenarios: (1) RL 1 and RL 2, where the top-K layers to be frozen are randomly selected using two different random seeds; (2) FL, which involves freezing the first K linear layers; and (3) LL, which entails freezing the last K linear layers. The experimental results indicate that neither the random freezing of K layers nor the selective freezing of either the first or last K linear layers could outperform the baseline of tuning all layers on most evaluation metrics. In contrast, our ILA can accurately identifies the layers of importance and freeze the top-K least important layers, thereby achieving substantial improvements. This demonstrates that ILA effectively pinpoints the non-critical layers for freezing, optimizing the fine-tuning process and enhancing model performance without the need to adjust every layer.\nObservation 2: Cross-dataset evaluation of layer importance can lead to the best results.\nAs indicated in Table 2, subtle differences are observed in the important layers identified across various datasets. This observation leads to an intuitive hypothesis that layers consistently deemed unimportant across all datasets may truly be non-essential. To this end, we intersect the top-K least important layers from three distinct datasets (i.e., LIMA, No Robots, and Alpaca-GPT4) to determine"}, {"title": "Observation 3: The computation cost of ILA is low.", "content": "Our ILA algorithm consists of two stages. Stage 1: We use LoRA to train the model until it is sufficiently stable, i.e., e-stable. Stage 2: We fix the backbone network and the LoRA modules to learn the importance weights (t). For LLAMA 2-7B and Mistral-7B-v0.1, |yt| = 225. To quantify computation cost, we measured the training time per iteration for LLAMA 2-7B in stages 1 and 2 with a batch size of 32. For stage 1, the training time is 6671 ms. For stage 2, the training time is 5343 ms. In Stage 2, we train for 128 batches on each dataset. Therefore, we only tune the model for about 5.34 \u00d7 128 \u00f7 60 \u2248 11 miniutes. The main training cost is in Stage 1. However, as shown in Table 4, it is not necessary to complete the entire training process; reaching 25% ~ 50% of the training milestones is sufficient."}, {"title": "5 RELATED WORKS", "content": "Large Language Models (LLMs) Alignment. Language models are initially pretrained to learn general-purpose representations, enabling their transfer to a wide range of language understanding and generation tasks (Qiu et al., 2024; Jiang et al., 2024; Nijkamp et al., 2022). To align these models with specific user needs and improve their performance on targeted applications, techniques such as Instruction Tuning (Zhang et al., 2023c; Sun et al., 2023; Muennighoff et al., 2023) and Preference Learning (Hejna et al., 2023; Guan et al., 2022; Rafailov et al., 2024; Song et al., 2024; Li et al., 2024) are commonly employed. Tuning-based alignment can introduce issues such as forgetting in LLMs (Wang et al., 2022a;b) and underfitting (Zhang et al., 2023c; Sun et al., 2023).\nSelecting an appropriate optimization method based on the characteristics of a specific task can often further enhance the model's performance (Shi et al., 2023; 2021; Zhang et al., 2022a; Wu et al., 2009). Therefore, the nature of model alignment should be explored through various studies. LIMA (Zhou et al., 2023) achieved a well-aligned model by fine-tuning nearly 1,000 samples using SFT, and hypothesized that the alignment process essentially teaches the model how to conduct conversations in specific formats or meet certain requirements without acquiring new knowledge. Similar findings have been reported in recent studies (Chen et al., 2023; Lee et al., 2023; Gudibande et al., 2023). Duan et al. (2023) analyzed the hidden states of LLMs, exploring the similarities between in-context learning (ICL) and instruction tuning (IT) regarding their impact on downstream tasks. URIAL (Lin et al., 2023) investigated the token distribution before and after alignment, suggesting that alignment primarily shifts \u201cstylistic tokens\" like discourse markers and transition words, while the distribution of knowledge-intensive terms remains largely unchanged. Based on prior research, we hypothesize that the abilities learned during alignment are relatively narrow in scope. To better understand this process, we propose an approach to identify which layers are genuinely important during alignment.\nParameter Efficient Fine-Tuning (PEFT). Full-model fine-tuning is highly complex and computationally expensive for not only computer vision (Zhang et al., 2022b; Wang et al., 2024) but also natural language processing (Zhang et al., 2022a). To tackle the high computational costs of full-model fine-tuning, especially with Pre-trained Language Models (PLMs) ranging from billions to trillions of parameters (Brown et al., 2020; Fedus et al., 2022), PEFT methods have been developed to reduce parameter usage while maintaining the effectiveness and stability of knowledge transfer (Tang et al., 2024; Peng et al., 2024). These approaches include partial fine-tuning, which selectively targets specific model components (Zaken et al., 2021; Zhao et al., 2020; Ansell et al., 2021; Guo et al., 2020), and soft prompt-based fine-tuning (Lester et al., 2021; Li & Liang, 2021; Asai et al., 2022). Notable methods include BitFit (Zaken et al., 2021), Adapter (Houlsby et al., 2019), LoRA (Hu et al., 2021) and its variants (Zhang et al., 2023b; Meng et al., 2024). Recent studies (Pan et al., 2024; Xu & Zhang, 2024; Panda et al., 2024) have shown that fine-tuning only a small portion of"}, {"title": "6 CONCLUSIONS", "content": "To gain a deeper understanding of LLM alignment, we have proposed ILA, a method for identifying critical layers in the alignment process by learning binary masks for LoRA weight matrices. ILA demonstrates consistent identification of important layers across different datasets, regardless of significant content variations, suggesting that the alignment process imparts similar capabilities to the model irrespective of the training data. This finding complements existing research and provides valuable insights into the specific roles of layers during the alignment process. By strategically tuning the vital layers, ILA effectively reduces computational overhead, and by freezing unimportant layers, it further enhances model performance, leading to more efficient resource utilization."}, {"title": "A PROOF OF THEOREM 2.1", "content": "Theorem A.1. For a sufficiently small \u0454, \u04e9\u0442 is e-stable, thus Assumption 2.1 and Assumption 2.2 are satisfied. For any t > T, we assume that \u2200i, v\u00ec \u2208 [0,1]. Let y denote the result of Yt after one step of gradient descent, i.e., Y\u2081 = Yt \u2013 B\u2207y L(mask). Then we have\n||Ye - Ye+1||2 \u2264 B(QL2 + L\u2081)R\u20ac.\nProof. Lety be the initial values of Yt and Yt+1. Then we have\nY = y \u2013 B\u2207 L(mask)\nYt\nYt+1 = 7 - BY+1L(mask)\nYt+1\nThe difference of ye and Ye+1 is\n||Ye - Ye+1||2 =||( \u2013 \u1e9e\u2207y L(\u04e9mask)) \u2013 (7 \u2013 B\u2207+1L(\u04e9mask))||2\n=\u1e9e|| \u2207L(mask) - \u2207+1L(mask) ||2\nYt  Yt+1   Yt  Yt+1\n=B||0t\u2207\u0259mask (\u0259mask) - 0+1 mask (mask)||2\n\u0394) \u0394\nt\n<3||0t\u2299 (\u2207\u0259mask (omask) \u2013 mask (mask)\nt+1\n+ \u03b2|| (0 \u2013 0t+1)mask (0 (mask)\n)) ||2.\nt+1\nBecause L(0) has an L-Lipschitz continuous gradient with constant L2 > 0, and ||0t|| \u2264 Q,\n\u03b8\u03b5\u2207\u0259mask (mask) mask) - 0+1 mask/ <QL2||mask \u2013 mask || 2\n=QL2||A0t+1 \u2013 \u0394\u03b8\u03c4||2\n=QL2||0t+1 - 0t||2\nBecause L(0) is L-smooth with constant L\u2081,\n|| (0 \u2013 0t+1)mask (mask)||2 <L1||0t \u2212 0t+1||.\nTherefore,\n||Ye - Y+1||2 \u2264 \u03b2(QL2 + L1)||\u03b8t \u2013 Ot+1||2.\nAccording to the Assumption 2.2, we have ||0 - 0t+1||2 < Re, hence,\n||Yi - Y+1||2 \u2264 \u03b2(QL2 + L1)Re."}, {"title": "B EXPERIMENTAL SETUP", "content": "For all experiments, we follow fine-tuning hyperparameters: we use AdamW with \u03b2\u2081 = 0.9, \u03b22 = 0.99 and weight decay of 0.1. The scheduler employed is a cosine scheduler with a warmup ratio of 0.01. For LORA baselines, we set the hyperparameter rank r as 32.\nB.1 No ROBOTS DATASET\nWe do a hyperparameter search for LoRA over the following variables: learning rate {0.001, 0.002, 0.0005, 0.0002, 0.0001}, training epochs {2, 3, 4, 5}. We do hyperparameter search for full fine-tuning over the following variables: learning rate {1e-4, 2e - 5, 1\u0435 \u2013 5, 5e - 6, 2\u0435 - 6}, training epochs {2, 3, 4, 5}.\nLLAMA 2-7B. Both LoRA and AdaLoRA use a dropout rate of 0.1 and a learning rate of 0.001. The number of training epochs is 3. For full fine-tuning, the learning rate is set to 0.00001, with the number of training epochs also being 3. The training parameters for IFILA are consistent with those of the baselines.\nMistral-7B. For LoRA and AdaLorA, we set the dropout rate as 0.1. The learning is 0.0002. The number of training epochs is 2. For full fine-tuning, the learning rate is set as 0.000002 and the number of training epochs is 2. The training parameters of IFILA are the same as the baselines."}, {"title": "B.2 LIMA DATASET", "content": "We do a hyperparameter search for LoRA over the following variables: learning rate {0.001,0.002, 0.0005, 0.0002, 0.0001}, training epochs {5,10,15,20}. We do hyperparameter search for full fine-tuning over the following variables: learning rate {1e - 4, 2e - 5, 1e - 5,5e-6, 2e- 6}, training epochs {5, 10, 15, 20}.\nLLAMA 2-7B. For LoRA and AdaLorA, we set the dropout rate as 0.1. The learning is 0.001. The number of training epochs is 20. For full fine-tuning, the learning rate is set as 0.00001 and the number of training epochs is 5. The training parameters of IFILA are the same as the baselines.\nMistral-7B. For LoRA and AdaLorA, we set the dropout rate as 0.1. The learning is 0.0002. The number of training epochs is 5. For full fine-tuning, the learning rate is set as 0.000005 and the number of training epochs is 5. The training parameters of IFILA are the same as the baselines."}, {"title": "B.3 ALPACA-GPT DATASET", "content": "We do a hyperparameter search for LoRA over the following variables: learning rate {0.001, 0.002, 0.0005, 0.0002, 0.0001}, training epochs {0.5, 1, 1.5, 2, 3}. We do hyperparameter search for full fine-tuning over the following variables: learning rate {1e \u2013 4, 2\u0435 \u2013 5, 1e - 5, 5e-6, 2e - 6}, training epochs {0.5, 1, 1.5, 2, 3}.\nLLAMA 2-7B. For LoRA and AdaLoRA, we set the dropout rate as 0.1. The learning is 0.0002. The number of training epochs is 1.5. For full fine-tuning, the learning rate is set as 0.000002 and the number of training epochs is 0.5. The training parameters of IFILA are the same as the baselines.\nMistral-7B. For LoRA and AdaLoRA, we set the dropout rate as 0.1. The learning is 0.0002. The number of training epochs is 5. For full fine-tuning, the learning rate is set as 0.000002 and the number of training epochs is 0.5. The training parameters of IFILA are the same as the baselines."}, {"title": "C ADDITIONAL EXPERIMENTS", "content": ""}, {"title": "C.1 ADDITIONAL EXPERIMENTS ON MODEL SCALABILITY", "content": "To assess whether freezing unimportant layers continues to enhance model performance at a larger scale, we conducted additional experiments on LLAMA 2-13B. Specifically, we fine-tuned LLAMA 2-13B using the No Robots and LIMA datasets, with results compared against LoRA presented in the table below. The experimental outcomes demonstrate that our method maintains strong performance on LLAMA 2-13B. Despite the increased model size, the underlying architectural similarities suggest that our approach remains effective and scalable, likely extending its benefits to even larger models."}]}