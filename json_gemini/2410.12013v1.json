{"title": "MOE-PRUNER: PRUNING MIXTURE-OF-EXPERTS LARGE LANGUAGE MODEL USING THE HINTS FROM ITS ROUTER", "authors": ["Yanyue Xie", "Zhi Zhang", "Ding Zhou", "Cong Xie", "Ziang Song", "Xin Liu", "Yanzhi Wang", "Xue Lin", "An Xu"], "abstract": "Mixture-of-Experts (MoE) architectures face challenges such as high memory consumption and redundancy in experts. Pruning MoE can reduce network weights while maintaining model performance. Motivated by the recent observation of emergent large magnitude features in Large Language Models (LLM) and MoE routing policy, we propose MoE-Pruner, a method that prunes weights with the smallest magnitudes multiplied by the corresponding input activations and router weights, on each output neuron. Our pruning method is one-shot, requiring no retraining or weight updates. We evaluate our method on Mixtral-8x7B and Mixtral-8x22B across multiple language benchmarks. Experimental results show that our pruning method significantly outperforms state-of-the-art LLM pruning methods. Furthermore, our pruned MoE models can benefit from a pretrained teacher model through expert-wise knowledge distillation, improving performance post-pruning. Experimental results demonstrate that the Mixtral-8x7B model with 50% sparsity maintains 99% of the performance of the original model after the expert-wise knowledge distillation.", "sections": [{"title": "INTRODUCTION", "content": "Scaling neural network models is one of the main drivers of better performance in deep learning. From BERT (Devlin et al., 2019) to GPT-3 (Brown et al., 2020) to Llama 3.1 405B (Dubey et al., 2024) in natural language processing, or from ResNet (He et al., 2016) to ViT (Dosovitskiy et al., 2021) in computer vision, breakthroughs in performance have been obtained from larger models, datasets, and computational resources for training (Kaplan et al., 2020). However, the cost of training state-of-the-art models grows exponentially. For instance, BERT-Large (345M parameters, proposed in 2018) requires an estimated 5 \u00d7 1020 FLOPs (Devlin et al., 2019) to train, GPT-3 (175B parameters, from 2020) requires 3.14 \u00d7 1023 FLOPs (Brown et al., 2020), while Llama 3.1 (405B, released in 2024) requires 3.8 \u00d7 1025 FLOPs (Dubey et al., 2024) to train. This exponential growth motivates researchers to seek more efficient and effective training approaches.\nMixture-of-Experts (MoE) architectures (Jacobs et al., 1991; Shazeer et al., 2017) have been proposed to reduce the computing cost while enabling efficient scaling of network capacity. It has been successfully employed to scale both vision (Ruiz et al., 2021; Shen et al., 2023) and language (Lepikhin et al., 2021; Fedus et al., 2022) models. In addition, these models provide other advantages, including sparsity that can mitigate catastrophic forgetting in continual learning and an inductive bias that can enhance performance in multitask learning. Overall, MoE has proven to be a promising strategy for scaling deep learning models across various domains.\nHowever, several crucial limitations persist in MoE for expanding its capacity. First of all, the static parameters, particularly those required for constructing the MoE architecture, introduce substantial"}, {"title": "PRELIMINARIES", "content": "Mixture-of-Experts (MoE). Scaling model size increases learning capacity and enhances generalization (Kaplan et al., 2020; Brown et al., 2020; Hoffmann et al., 2022). MoE (Jacobs et al., 1991; Shazeer et al., 2017; Lepikhin et al., 2021; Fedus et al., 2022) is an efficient approach that enables significantly more compute-efficient pretraining and inference. It replaces the feed-forward network (FFN) layers in Transformers (Vaswani et al., 2017) with expert layers, where different experts are activated for different input tokens instead of utilizing the full network parameters. Sparse MoE architecture can dramatically scale the model with the same compute budget as a dense model.\nLarge Language Model Pruning. Magnitude pruning (Han et al., 2016) is a standard approach to induce sparsity in neural networks. It removes individual weights with magnitudes below a certain threshold. However, magnitude pruning fails dramatically on LLMs even with relatively low levels of sparsity (Frantar & Alistarh, 2023b). SparseGPT (Frantar & Alistarh, 2023b) proposes a one-shot, post-training pruning method that prunes LLM weights and uses Hessian matrix and calibration data to update the remaining weights without any retraining. Wanda (Sun et al., 2024) is a simple method that prunes LLM weights with the smallest magnitudes multiplied by the corresponding input activations without any additional weight update."}, {"title": "METHODOLOGY", "content": "3.1 THE MIXTURE-OF-EXPERTS ARCHITECTURE\nMixture-of-Experts (MoE) architecture. MoE architecture replaces the feed-forward networks (FFN) in Transformers with mixture-of-expert layers. A router or a gating network is trained to select a subset of experts for each input token based on its routing policy. Given n experts in a layer, the output of the expert layer is given by:\n\n $$y = \\sum_{i=0}^{n-1} Gate(x)_i \\cdot E_i(x),$$\n\nwhere the Gate(x)i is the router weights from the gating network assigned to the i-th expert, and Ei(x) is the output of i-th expert. The router weights can be formulated as softmax over the Top-K logits:\n\n $$Gate(x) = Softmax(TopK(x \\cdot W_g)),$$\n\nwhere Wg is the weight of the router or gating network, and TopK(X)i = li if i is in the top-K coordinates of logits l and TopK(X)i = \u2212\u221e otherwise.\nSince current LLMs mostly adopt SwiGLU (Shazeer, 2020) architecture for the FFN, and MoE LLM such as Mixtral-8x7B (Jiang et al., 2024) uses a top-2 to select experts, we can derive the output of an expert layer as:\n\n $$y = \\sum_{i=0}^{n-1} Softmax(Top2(x \\cdot W_g))_i \\cdot SwiGLU_i(x).$$\n\nSome recent MoE LLMs, such as DeepSeekMoE (Dai et al., 2024), adopt shared experts that are always activated, aiming at capturing and consolidating common knowledge across varying contexts.\nMoE Expert Initialization. MoE expert initialization uses different strategies, which can be classified into two categories: sparse upcycling (Komatsuzaki et al., 2023) and training from scratch. Some open-source MoE models such as Mixtral (Jiang et al., 2024), Qwen1.5-MoE-A2.7B (Team, 2024), and MiniCPM-MoE (Hu et al., 2024) all employ the upcycling approach to reduce the total training costs. While some MoE models like DeepSeek-V2 (Liu et al., 2024), OLMOE (Muennighoff et al., 2024), and Yuan2.0-M32 (Wu et al., 2024) use the training from scratch approach to help expert diversification. We find that different MoE expert initialization methods result in different expert activation frequencies and expert similarities, which will impact the MoE pruning strategies. For instance, the MoE model initialized with upcycling can take advantage of the dense model and reduce training costs. The final MoE model exhibits higher expert similarity and more balanced expert activation frequency, which indicates that expert pruning will result in a performance drop, and weight pruning will be a better choice. MoE model trained from scratch might yield better performance"}, {"title": null, "content": "as it avoids the limitations of starting with a group of identical experts, which can hinder diversification (Wei et al., 2024). It also shows imbalanced expert activation frequency, indicating that least-used expert pruning could help compress model size and not bring performance degradation.\nMoE Expert Activation Frequency. We use a subset of the C4 (Raffel et al., 2020) dataset and collect the activation frequency of MoE experts. The expert activation frequency is task-agnostic since C4 pretraining datasets are comprehensive and not dominated by knowledge specific to any particular domain. Motivated by the load balancing loss (Shazeer et al., 2017; Lepikhin et al., 2021; Fedus et al., 2022), we propose to use the coefficient of variation of expert activation frequency in each layer to represent the load balancing score, where a lower score represents more balanced loads. Given n experts and I layers and a batch B with T tokens, the load balancing score for one layer is:\n\n $$s = \\frac{\\sigma}{\\mu},$$\n\n $$\\mu = \\frac{1}{n} \\sum_{i=0}^{n-1} f_i,$$\n\nwhere fi is the number of tokens dispatched to expert i,\n\n $$f_i = \\sum_{x \\in B} 1{\\text{argmax } p(x) = i}.$$\n\nWe can derive the load balancing score by calculating the mean of scores across all I MoE layers, such that we can use this score to compare with various MoE models with different numbers of experts."}, {"title": "PRUNING METRIC", "content": "Problem Formulation. Post-training pruning for LLMs can be decomposed into layer-wise sub-problems (Lu et al., 2022; Frantar & Alistarh, 2023b; Sun et al., 2024; Dong et al., 2024). Given a"}, {"title": null, "content": "sparsity ratio and a linear layer with weight W, the pruning algorithm tries to find a sparsity mask M that minimizes reconstruction loss:\n\n $$argmin_M||WX - (M \\odot W)X||.$$\n\nOptimal Brain Damage (OBD) (LeCun et al., 1989) first sets up a pioneering framework for neural network pruning. It uses second-order information without off-diagonal elements in the Hessian matrix for faster approximation. Optimal Brain Surgeon (OBS) (Hassibi et al., 1993) develops upon OBD partly by taking into account the off-diagonal elements. SparseGPT (Frantar & Alistarh, 2023b) revisits the OBS, computes the inverse Hessian only once, and reuses to update weight in the remaining rows that are also in the mask to mitigate reconstruction loss. The pruning metric in SparseGPT is:\n\n $$S_{ij} = [|W|^2/diag(H^{-1})]_{ij}.$$\n\nWanda (Sun et al., 2024) further simplifies the pruning metric to the following form without the need to compute the inverse of the Hessian matrix H:\n\n $$S_{ij} = [|W|^2/diag(XTX)^{-1}]_{ij} \\approx [|W|^2/(diag(XTX)^{-1})]_{ij} = (|W_{ij}|\\cdot ||X_j||)^2.$$\n\nWhen it comes to pruning MoE, the expert layers constitute the majority of model parameters. For example, the Mixtral-8x7B (Jiang et al., 2024) has a total of 47B parameters where 1.3B belongs to attention modules and 45B is used for expert layers (2 out of 8 experts are activated, 12.5B active parameters during inference). Only a subset of experts are activated for different input tokens, so there is a large space of expert redundancy.\nRouter Tells It All. As shown in Equation 1, the router weights are assigned to each expert output. Motivated by the pruning metric in Wanda and the MoE routing policy, our approach, MoE-Pruner, prunes weights with the smallest magnitudes multiplied by the corresponding input activations and router weights, on each output neuron:\n\n $$S = |W_{ij} | \\cdot ||X_j. Gate_j ||.$$"}, {"title": "EXPERT-WISE KNOWLEDGE DISTILLATION", "content": "Expert-Wise Knowledge Distillation. MoE models can preserve most of their capacity after pruning but still suffer from performance degradation. To recover MoE LLM performance, we fine-tune the model by leveraging the unpruned pretrained model as a teacher model in an expert-wise knowledge distillation (KD) manner. The pretrained model is a natural teacher model for the pruned model since they share exactly the same number of layers, experts, and dimensions (Kurtic et al., 2023). The loss function for expert-wise knowledge distillation is formulated as follows:\n\n $$L_{KD} = L_{CE} + \\lambda \\times L_{expert} = L_{CE} + \\lambda \\times \\sum_{j=0}^{N-1} \\sum_{i=0}^{n-1} MSE(E_j^i, E_S^i),$$\n\nwhere LCE is the cross entropy loss, MSE is the mean squared error calculated as MSE(X, Y) = \u03a3(xi-Yi)2 for N-dimensional vectors X and Y. \u039b is a weighting coefficient and initialized based on the strength of cross entropy loss and expert-wise knowledge distillation loss: LCE. We sum up all the differences between teacher experts and student experts."}, {"title": "EXPERIMENTS", "content": "Models, Datasets, and Evaluation. We conduct pruning experiments on widely adopted open-source MoE models: the base and instruct version of Mixtral-8x7B and Mixtral-8x22B (Jiang et al., 2024). We use samples from the pretraining dataset C4 (Raffel et al., 2020) as calibration data for one-shot pruning since pretraining datasets are often more comprehensive and not dominated by knowledge specific to any particular domain. We use the exact same 128 sequences of calibration data for all one-shot pruning experiments to control this variable factor. We evaluate the perplexity on the WikiText (Merity et al., 2017) validation set. Our expert-wise knowledge distillation method uses a subset of the C4 (Raffel et al., 2020) as the training set. We measure the performance of pruned models on zero-shot tasks and language modeling. For zero-shot evaluation, we use nine popular tasks from EleutherAI LM Harness (Gao et al., 2023). The nine evaluated zero-shot tasks are: ARC-easy, ARC-challenge (Clark et al., 2018), Boolq (Clark et al., 2019), HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2021), OpenBookQA (OBQA) (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), RTE (Wang et al., 2018), and WinoGrande (Sakaguchi et al., 2021)."}, {"title": null, "content": "Baselines and Experiments Setup. We compare MoE-Pruner with prior pruning approaches, including SparseGPT (Frantar & Alistarh, 2023b) and Wanda (Sun et al., 2024). Similarly, our pruning algorithm is implemented in a layer-wise reconstruction manner. All pruning experiments are conducted on a single NVIDIA H100-80GB GPU. The fine-tuning experiments use the pruned model as a starting point and perform full-parameter fine-tuning to preserve the sparsity mask. We implement the expert-wise knowledge distillation method in Llama-Factory (Zheng et al., 2024) and conduct experiments on 2 servers, each with 8 NVIDIA H100-80GB GPUs. We fine-tune the pruned student model for three epochs, using a learning rate of 2e-5 with the cosine learning rate scheduler."}, {"title": "ONE-SHOT PRUNING", "content": "Table 2 shows the one-shot pruning model perplexity on WikiText with 50% sparsity. There is a clear difference between MoE-Pruner and other pruning methods, including SparseGPT (Frantar & Alistarh, 2023b) and Wanda (Sun et al., 2024). For Mixtral-8x7B (Jiang et al., 2024) models, MoE-Pruner achieves 0.22-0.31 better perplexity over SparseGPT and Wanda. This improvement expands when the MoE model scales to the Mixtral-8x22B model. For the larger Mixtral-8x22B model, MoE-Pruner achieves 0.55 better perplexity over SparseGPT and 0.31-0.34 better perplexity over Wanda. MoE-Pruner further expands the improvement to 1.21 better perplexity over SparseGPT and 1.10 better perplexity over Wanda when we prune the MoE models with the 2:4 semi-structured sparsity."}, {"title": "EXPERT-WISE KNOWLEDGE DISTILLATION PERFORMANCE", "content": "The gap between the pruned MoE model and the pretrained MoE model can be largely mitigated via expert-wise knowledge distillation. We only need 1000 training samples from C4 (Raffel et al., 2020), and training can be done in 1 hour. Table 4 shows the average zero-shot accuracy of the pruned and fine-tuned Mixtral-8x7B MoE models with 50% unstructured sparsity. The fine-tuned model could achieve a 68.40 average performance on nine zero-shot tasks. The performance is very close to the pretrained Mixtral-8x7B MoE model, which demonstrates a 69.16 average performance."}, {"title": "ABLATION STUDIES", "content": "Ablation on Different Number of Calibration Samples. We change the number of calibration samples by selecting different sample sizes ranging from 2 to 256. Results are summarized in Figure 4. We see a clear difference in trend as the number of calibration samples changes. MoE-Pruner is much more robust than SparseGPT when there are few calibration samples and performs the same trend but better perplexity over Wanda. Notably, even with just two calibration samples, pruned networks obtained by MoE-Pruner have a perplexity of just 4.95. This may be because input norm statistics could be much easier to estimate than the full inverse Hessian of the local layer-wise reconstruction problem.\nAblation on Different Sparsity Ratio. We also change the pruning ratio using the same 128 calibration samples. Figure 5 shows that at lower pruning ratios, such as 10% to 40%, all pruning methods result in almost the same perplexity. When the pruning ratio increases, the Wanda pruned model perplexity changes dramatically and fails at 70%. MoE-Pruner shows better and more stable pruning results than SparseGPT and Wanda, especially at higher pruning ratios. This demonstrates that router weights preserve important information when selecting experts and provide a clear hint for pruning unimportant weights."}, {"title": "RELATED WORKS", "content": "Pruning and Sparsity. Pruning (LeCun et al., 1989; Hassibi et al., 1993; Han et al., 2015) is an important approach for compressing neural networks through eliminating weights (Han et al., 2016) or activations (Rao et al., 2021), yielding sparse networks. It can be mainly classified into two categories based on the granularity: unstructured and structured pruning.\nUnstructured pruning such as magnitude pruning (Han et al., 2015; 2016) removes individual weights to introduce sparsity while preserving accuracy even at high sparsity. Existing methods either require retraining or fine-tuning the pruned models (Liu et al., 2019) or the whole iterative retraining process (Frankle & Carbin, 2019). However, in the era of LLMs, these methods fail as retraining LLMs demands substantial computational resources. SparseGPT (Frantar & Alistarh, 2023b) and Wanda (Sun et al., 2024) propose efficient post-training pruning method that prunes LLM weights in a layer-wise manner without retraining the model.\nStructured pruning eliminates weights as a group, such as channel pruning (He et al., 2017), kernel pruning (Zhong et al., 2022), attention head pruning (Wang et al., 2021), token pruning (Rao et al., 2021), and layer pruning (Elhoushi et al., 2024). Unlike unstructured pruning, it leads to more hardware-friendly, dense blocks of computation, which facilitates acceleration on modern hardware platforms. Some methods explore structured pruning based on sparsity on the structural components of LLMs, such as attention heads (Wang et al., 2021) and FFN channels (Ma et al., 2023). Muralidharan et al. (2024) uses both structured pruning and knowledge distillation to compress LLM models and shows improvement over models trained from scratch. Due to the constraint of removing regular components, structured pruning usually has low sparsity ratios and high accuracy loss. NVIDIA"}, {"title": null, "content": "proposes N:M semi-structured sparsity (Mishra et al., 2021), which can preserve model performance by retraining and leverage GPU tensor core acceleration.\nPruning for MoE Models. Most of the works for MoE pruning focus on structured expert pruning. Chen et al. (2022) and Koishekenov et al. (2023) prune experts based on their utilization to save memory. However, this usually leads to degraded performance. Lu et al. (2024) enumerates expert combinations based on the required expert number and uses calibration data to find a set of remaining experts that has the minimum reconstruction loss. Chowdhury et al. (2024) prunes experts based on the change in the router's norm and proves that the generalization accuracy can be preserved. However, expert pruning sometimes removes experts with certain knowledge and results in the loss of model performance. Therefore, Li et al. (2024) and Zhang et al. (2024) both leverage expert merging techniques to compress the expert layer while also preserving expert knowledge. He et al. (2024) proposes a unified framework to compress MoE models. The framework consists of two perspectives: (i) expert slimming that compresses individual experts by weight pruning and quantization, and (ii) expert trimming that removes whole structured modules by layer drop and block drop.\nEfficiency for MoE and Existing Solutions. MoE models require huge memory to host expert layers, while many experts have low utilization during inference. To address this, Gao et al. (2022) uses a tensor decomposition method to share the central tensor's parameters across experts and keep different auxiliary tensors for each expert. MoQE (Kim et al., 2023) and QMoE (Frantar & Alistarh, 2023a) both study extreme low-bit quantization for compressing MoE model size. Moreover, some works employ knowledge distillation (Fedus et al., 2022; Artetxe et al., 2021) to create either a smaller dense model or a MoE model with fewer layers. However, they also overlook the existing redundancy within MoE expert layers. Yadav et al. (2023) shows that experts can be compressed to a huge degree without any performance loss."}, {"title": "CONCLUSION", "content": "We propose a simple and effective pruning method for MoE models, MoE-Pruner. We prune weights with the smallest magnitudes multiplied by the corresponding input activations and router weights, on each output neuron. Our pruning method is one-shot and fast, without the need for any retraining or weight update procedures. Pruning MoE LLM with high sparsity will incur performance degradation, so we also propose a fine-tuning method that leverages the unpruned pretrained MoE model as a teacher to guide the pruned student model through expert-wise knowledge distillation. The fine-tuned MoE models could maintain 99% of the performance of the original model after the expert-wise knowledge distillation, using only a small set of training data and low GPU hours. In the future, MoE-Pruner could also be extended to structured pruning of MoE LLMs, such as channel pruning and expert pruning, for better hardware acceleration."}]}