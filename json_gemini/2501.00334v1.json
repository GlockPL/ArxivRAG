{"title": "Loss-Aware Curriculum Learning for Chinese Grammatical Error Correction", "authors": ["Ding Zhang", "Yangning Li", "Lichen Bai", "Hao Zhang", "Yinghui Li", "Haiye Lin", "Hai-Tao Zheng", "Xin Su", "Zifei shan"], "abstract": "Chinese grammatical error correction (CGEC) aims to detect and correct errors in the input Chinese sentences. Recently, Pre-trained Language Models (PLMS) have been employed to improve the performance. However, current approaches ignore that correction difficulty varies across different instances and treat these samples equally, enhancing the challenge of model learning. To address this problem, we propose a multi-granularity Curriculum Learning (CL) framework. Specifically, we first calculate the correction difficulty of these samples and feed them into the model from easy to hard batch by batch. Then Instance-Level CL is employed to help the model optimize in the appropriate direction automatically by regulating the loss function. Extensive experimental results and comprehensive analyses of various datasets prove the effectiveness of our method.", "sections": [{"title": "I. INTRODUCTION", "content": "The task of Chinese Grammatical Error Correction (CGEC) involves automatically identifying and correcting grammatical mistakes in the input Chinese sentences [1]. CGEC has attracted increasing attention from NLP researchers because it benefits many applications, such as Writing Assistant [2], Automatic Speech Recognition [3], and Search Engine [4].\nRecently, methods based on Pre-trained Language Models have become the mainstream of the CGEC task [5], [6]. Existing methods based on PLMs for CGEC can be categorized into three categories: Sequence-to-Sequence method(Seq2seq) [7], Sequence-to-Edit method(Seq2Edit) [8], and Ensemble models [9]. The Seq2Edit method treats CGEC as a sequence labeling task and adopts editing operations between the source text and the target text as training objectives. The Seq2Seq method treats the CGEC problem as a monolingual translation problem, where the erroneous text is regarded as the source text and the correct sentence serves as the target text, respectively. Ensemble models combine the two methods described above and can easily fix more error types.\nHowever, the current approach faces a severe problem in that it ignores the diversity of CGEC training data, where correction difficulty varies across different instances [10], [11]. Equally treating these training data would have a negative influence on the performance of CGEC models.\nThe core idea of Curriculum Learning (CL) is to facilitate the model's training process by learning from simple samples to complex ones, which has demonstrated its efficacy in improving the performance of natural machine translation systems [12]-[16]. Therefore, we introduce a multi-granularity Curriculum Learning framework to address the above issue in this paper. Intuitively, hard samples should be difficult for CGEC models to correct. So we first use the training loss to measure the difficulty of each pair of erroneous sentences and correct sentences. Then samples will be fed into CGEC models batch by batch in ascending order based on their difficulty scores.\nHowever, the Batch-Level CL method is rough, and the difficulty scores of training data at the Sentence-Level or Token-Level within a specific batch can vary, an issue that also needs to be addressed. To achieve such fine-grained learning, we determine to assign different weights to simple and complex samples by regulating the loss function. Specifically, we first employ the model to evaluate the quality scores of instances in the training set. Then the scores serve as learning factors to dynamically adjust the contributions of different parts when computing the training loss, encouraging the CGEC model to focus on the portions that are difficult to correct.\nTo verify the efficacy of our framework, we conduct extensive experiments using PLMs such as mT5 and BART. The experimental results demonstrate that our proposed Curriculum Learning method outperforms the baseline scores on NLPCC [17] and MuCGEC [18] datasets, proving that our framework can boost current CGEC models. The contributions of this paper can be summarized as follows:\n\u2022 We develop a multi-granularity Curriculum Learning method to tackle the CGEC task.\n\u2022 By utilizing the difficulty of training data from multi-granularity, our method enables CGEC to focus on the more challenging aspects of the data and optimize in the appropriate direction.\n\u2022 Extensive experiments verify that our proposed CL method is able to improve the performance of the CGEC models based on PLMs."}, {"title": "II. PROPOSED APPROACH", "content": "In this section, we will first review the fundamental definition of the Chinese grammatical error correction task. Then we will introduce our proposed CL framework for CGEC, which comprises two sub-modules operating at distinct levels."}, {"title": "A. Problem Definition", "content": "Let $x^i = [x_1^i, x_2^i,...,x_n^i]$ denotes the i-th source sentence and $y^i = [y_1^i, y_2^i,..., y_m^i]$ is its corresponding corrected sentence. CGEC employs a pre-trained language model to build a correction model, aiming to maximize the conditional distribution of error-corrected sentence pairs in a parallel corpus. More specifically, the learning objective is to minimize negative log-likelihood loss:\n$L(x^i,y^i) = \\frac{1}{m}\\sum_{t=1}^{m}logP(y_t^i|y_{<t}^i, x^i; \\theta)$  (1)\nwhere $\\theta$ denotes the trainable parameters of our CGEC model. As shown in Fig 1, the essence of our method lies in how to enforce CGEC models to pay more attention to complex samples."}, {"title": "B. Batch-Level Curriculum Learning", "content": "1) Difficulty Criteria: We first use the cross-entropy loss function to measure the difficulty of each data sample. Then we will control the dataloader to load samples of the training set from simple to complex at the Batch-Level.\nAs mentioned above, evaluating data difficulty is essential for Curriculum Learning [19], [20]. Traditional difficulty criteria, such as word frequency or sentence length, are not able to reflect the intrinsic difficulty of correcting error text pairs. Based on detailed observation, we find that the CGEC model can easily correct the error of data with a lower loss but is not good at correcting the error of data with a higher loss. Therefore, we first propose to determine the data difficulty according to the cross-entropy loss function in the CGEC task.\nGiven a trained CGEC model and a dataset with N sentence pairs {$(x^1, y^1), (x^2, y^2), ..., (x^N, y^N)$}, we can acquire the difficulty of each instance by calculating the cross-entropy loss function. Then Cumulative Density Function (CDF) is employed to transfer the distribution of instances difficulty into (0, 1]:\n$\\tau((x^i, y^i)) \\in (0, 1] = CDF({d((x^i, y^i))}_{i=1}^{N})$   (2)\nSpecifically, the score of the difficult instance tends to be 1, while that of the easy instance tends to be 0.\n2) Curriculum Arrangement: The second question in Curriculum Learning is how to organize all the training samples into a sequenced curriculum based on their difficulty scores, which determines the complexity of samples the CGEC model can acquire at a specific batch.\nFollowing [21], we use the notion of model competence, which is a function that takes the step t of a training batch as input and outputs a value from 0 to 1 to control the loading schedule of training data in Batch-Level.\n$C(t) = min\\left\\{1, \\left(\\frac{t}{T}\\right)^k + c_0^{1-k}\\right\\}$ (3)\nwhere $c_0$ is the initial competence at the beginning of training, k is the coefficient to control the growth rate of model competence, T is a hyper-parameter to determine the length of the Batch-Level curriculum.\nAt step t, all the available samples whose loss difficulty scores are less than $c_t$ are available to the CGEC model. Through the learning strategy, the CGEC model progressively acquires the training samples from simple to complex batch by batch."}, {"title": "C. Instance-Level Curriculum Learning", "content": "While Batch-Level CL is able to enhance the CGEC model by controlling the dataloader to load training data from easy samples to hard ones sequentially, it is important to note that the qualities and levels of difficulty among sentences and tokens in a given batch are also different. Therefore, we propose to determine the learning emphasis in a particular batch according to the instance difficulty.\nIntuitively, common tokens tend to be easier for the model to generate, while rare tokens tend to be difficult during the generation. As the length of the sentences and the quantity of grammatical mistakes both increase, the complexity of fixing the erroneous text also escalates. Therefore, we use Monte Carlo dropout sampling to calculate the conditional probabilities so as to modify the loss weights of the simple instances and the complex instances dynamically.\nGiven the current CGEC model parameterized by $\\theta$ and a mini-batch consisting of M sentence pairs {$(x^1, y^1), (x^2, y^2), ..., (x^M, y^M)$}, the number of Q Monte Carlo dropout sampling is employed on the CGEC model. Thus, each instance is able to yield Q conditional probabilities. The variance of the probabilities can reflect the confidence that the model has with respect to each instance. As a result, we utilize the probabilities to assess the complexity of each individual sentence and token."}, {"title": "III. EXPERIMENTS", "content": "Following previous works, we evaluate the model performance on two recent CGEC datasets, NLPCC and MuCGEC. Lang8 [17] and HSK [22] are used for model training. Lang8 is collected from a language learning website and HSK is collected from official Chinese proficiency tests. The details of the datasets are provided in Table I."}, {"title": "B. Evaluation Metrics", "content": "For the NLPCC dataset, we present the results measured by the $M^2$ scorer for evaluation. For the MuCGEC set, ChERRANT scores are employed to evaluate the performance. All"}, {"title": "IV. CONCLUSION", "content": "In this paper, we propose a multi-granularity Loss-Aware Curriculum Learning method to enhance the CGEC models. Specifically, a novel loss-aware difficulty criteria is first proposed to assist the CGEC model in learning from simple samples to complex samples at Batch-Level. Then the difficulty scores of each instance at Sentence-Level and Token-Level are calculated to regulate the loss function, automatically facilitating the optimization of CGEC models in the appropriate direction. In conclusion, extensive experiments indicate that our CL method at different levels can effectively enhance the correction performance of CGEC models."}, {"title": "A. Datasets", "content": "Following previous works, we evaluate the model performance on two recent CGEC datasets, NLPCC and MuCGEC. Lang8 [17] and HSK [22] are used for model training. Lang8 is collected from a language learning website and HSK is collected from official Chinese proficiency tests. The details of the datasets are provided in Table I."}, {"title": "C. Experimental Setup", "content": "To verify its effectiveness, we apply our CL method to three models: BART, mT5, and SynGEC. During the training phase, we adopt some hyper-parameters of SynGEC to guide our model's optimization process [23]. We maintain the learning rate 5e-5 with a batch size of 8,192 tokens and fine-tune the parameters with Adam optimizer. Q, the number of Monte Carlo dropout samples, is set to 5. In addition, we carefully designed the prompts in Table II for evaluating the performance of LLM on existing exams. The temperature of LLM is set to 1.0."}, {"title": "D. Main Results", "content": "Table III and Table IV illustrate the performance of our CL method when compared to the baselines. From the experimental results, it is evident that our method achieves consistent improvements with a significant margin over all baselines by reordering the training data and focusing on hard instances. Our Curriculum Learning strategy forces the CGEC models to shift the emphasis toward more difficult samples gradually, which leads to an enhancement in their performance. The large improvements observed from three models (i.e., BART, mT5, and SynGEC) demonstrate the model-agnostic characteristic of our proposed method."}, {"title": "E. Ablation Study", "content": "To verify the effectiveness of each module in our proposed CL method, we conduct ablation studies with the following settings: 1) Vanilla BART, 2) Only Batch-Level CL, 3) Only Instance-Level CL. The model performance decreases when Batch-Level CL or Instance-Level CL is removed, which demonstrates the efficacy of each module in our proposed Loss-Aware Curriculum Learning framework.\nFrom Table V, we can observe that each component of our approach is proven to produce a significant improvement to the model compared to Vanilla BART. Batch-Level CL forces the CGEC models to shift the emphasis toward the complex samples gradually, which leads to an enhancement in their performance. Instance-Level CL helps the CGEC model automatically optimize in the appropriate direction, which boosts its performance."}, {"title": "F. Parameter Study", "content": "As mentioned in Sec II-B, we assign k to scale the gap between the high-difficulty sentence and the low-difficulty sentence. In this section, to investigate the impact of different k, we perform extensive experiments by varying the valuation of k on the NLPCC-test dataset. Table VI shows that when the value of k reaches a certain value, the performance of the model does not improve anymore. We believe that the overlarge k leads to overfit on hard instances and ignore easy instances. Consequently, it is essential to choose the optimal value of k, although there are consistent improvements when using BART as the PLM at all values of k from 1 to 5."}, {"title": "G. Correlation Between the Curriculum Learning and Improvements", "content": "Despite the fact that our methods yield improvements across all the sentence pairs, the reasons behind the improvements remain unclear. Figure2 shows the $F_{0.5}$ improvements at different difficulty intervals on the NLPCC-test dataset. The difficulty scores are calculated by the formula detailed in section II-B. We observe that our approach consistently outperforms the BART baseline across various difficulty intervals. Both easy and difficult sentences benefit from the CL framework. The hardest sentences have notable improvements, which can be attributed to the emphasis on the hard parts during the training stage."}]}