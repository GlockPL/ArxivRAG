{"title": "KinMo: Kinematic-aware Human Motion Understanding and Generation", "authors": ["Pengfei Zhang", "Pinxin Liu", "Hyeongwoo Kim", "Pablo Garrido", "Bindita Chaudhuri"], "abstract": "Controlling human motion based on text presents an important challenge in computer vision. Traditional approaches often rely on holistic action descriptions for motion synthesis, which struggle to capture subtle movements of local body parts. This limitation restricts the ability to isolate and manipulate specific movements. To address this, we propose a novel motion representation that decomposes motion into distinct body joint group movements and interactions from a kinematic perspective. We design an automatic dataset collection pipeline that enhances the existing text-motion benchmark by incorporating fine-grained local joint-group motion and interaction descriptions. To bridge the gap between text and motion domains, we introduce a hierarchical motion semantics approach that progressively fuses joint-level interaction information into the global action-level semantics for modality alignment. With this hierarchy, we introduce a coarse-to-fine motion synthesis procedure for various generation and editing downstream applications. Our quantitative and qualitative experiments demonstrate that the proposed formulation enhances text-motion retrieval by improving joint-spatial understanding, and enables more precise joint-motion generation and control. Project Page:\nhttps://andypinxinliu.github.io/KinMo/", "sections": [{"title": "1. Introduction", "content": "Controlling human motion through natural language is a rapidly expanding area within computer vision, enabling interactive systems to generate or modify 3D human motions based on textual inputs. This technology has a wide range of applications, including robotics [11], virtual reality[12], and automatic animation [36, 37], where human-like motion is crucial for user interaction and immersion.\nDespite substantial advancements in general motion generation [9, 21, 25, 27, 29, 35], the challenge of fine-grained control over individual body parts remains largely unsolved. Current models are proficient at producing coherent whole-body movements from high-level language descriptions but struggle when tasked with editing or controlling specific body parts independently. This limitation prevents these systems from achieving the precision and adaptability required for most real-world applications.\nRecent advancements [10, 19] have introduced more refined approaches by incorporating controllability into motion generation. However, these models are limited to processing simple instructions and still lack the fine-grained compatibility required for scenarios where multiple body parts must coordinate to perform complex actions. Similarly, generative models for motion synthesis [9, 21] present innovative methods but do not directly tackle the issue of controlling individual body parts in response to specific textual descriptions.\nThis challenge is rooted in the inherent ambiguity of motion text descriptions in existing datasets. For example, multiple phrases (such as pick up an object from the ground and bend down to reach something) can describe the same motion. Conversely, a single term (like running) can encompass a wide range of variations, depending on factors such as speed, arm movement, or direction. This many-to-"}, {"title": "2. Related Work", "content": "Text-to-Motion Understanding. Aligning text and motion modalities into a joint embedding space is a core component of human motion understanding. Posescript [4] tries to use fine-grained text descriptions to represent various human poses. ChatPose [6] leverages LLMs for describing the poses with natural languages. MotionCLIP [28] and TMR [18] further enhance the alignment from single poses to motion sequences. MotionLLM [2] creates a large corpus of Motion-QA for language-motion understandings. However, these methods only focus on global action-level motion, and they cannot capture the subtle local joint motions and the extent of the motions.\nText-to-Motion Generation. As Diffusion models have demonstrated notable success in image generation [16, 24], some early methods, e.g., MDM [29] and MotionDiffuse [35], have adopted them for motion generation. Other methods, such as T2M-GPT [33] and MotionGPT [23], represent motions as discrete tokens and leverage autoregressive models to improve motion generation quality. MMM [21] and MoMask [9] further improve motion generation quality with a bidirectional masking mechanism and introduce the concept of editability. Inspired by previous work [30], we propose a hierarchical text semantic representation for generating human motions in a coarse-to-fine scheme. Such a representation ensures that descriptions of high-level actions, low-level joint groups, and joint interactions can control fine-grained motion generations.\nMotion Control and Editing. Motion Editing allows users to modify or refine the motion generation. Initial work, such as PoseFix [5], automates the generation of frame-level 3D pose and text modifiers for supervised editing. Some other diffusion-based models [29, 35] can perform zero-shot spatiotemporal editing by infilling specific joints or frames, but they may create unnatural discontinuities. TLControl [31] and OmniControl [32] can control arbitrary joints at any time by combining spatial and temporal control together but lack editability over various local joints to represent complex motions. CoMo [10] prompts a large language model (LLM) to edit the original motion directly. However, none of these methods adopt a fine-grained approach that allows independent editing of body part movements while ensuring overall motion compatibility."}, {"title": "3. Kinematic-aware Human Motion", "content": "Our framework (see Fig. 2) refines motion representation with local joint groups and joint interactions, with corresponding low-level text semantics for understanding and motion generation. We first present our motion representation formulation (Sec. 3.1) and introduce KinMo dataset (Sec. 3.2). We then present a semantic hierarchy for enhanced text-motion alignment and understanding (Sec. 3.3). We finally formulate motion generation as a coarse-to-fine procedure based on our semantic hierarchy (Sec. 3.4) and show its various downstream applications (Sec. 4)."}, {"title": "3.1. Joint-Kinematic Motion Representation", "content": "While existing text-to-motion datasets only annotate the global action descriptions, their formulation struggles to represent the regional joint-group movements and their interactions, and it also fails to achieve spatial understanding, generation, and edit-\ning of fine-grained motion. To resolve this problem,\nwe reformulate motion representation by organizing\njoints into a set of kinematic groups defined as $G =$\n{Torso, Neck, Left Arm, Right Arm, Left Leg, Right Leg},\nwith each group $g$ comprising a subset of joints $J_g \\subseteq J$.\nJoint-Group Representation. For each group $g$ at time $t$,\nwe define the Group Position $P_g(t)$ as the average position\nof the joints within that group:\n$P_g(t) = \\frac{1}{|J_g|} \\sum_{j \\in J_g} P_j(t).$\nAdditionally, we define the Limb Angles $\\Theta_g(t)$ as the col-\nlection of joint rotations within the group, and the Group\nVelocity $V_g(t)$ as the average velocity of the joints:\n$\\Theta_g(t) = \\{R_j(t) | j \\in J_g \\}, \\quad V_g(t) = \\frac{1}{|J_g|} \\sum_{j \\in J_g} v_j(t).$\nJoint-Interaction Representation. Human motion also\ninvolves the relationships between different kinematic\ngroups. For physically connected groups (e.g., torso and\nneck), we define Relative Position, Relative Limb Angles\n(angles of the connecting joint between two physically\nconnected groups), and Relative Velocity (angular velocity\nof the connecting joint between two physically connected\ngroups). For non-physically connected groups (e.g., left\narm and right arm), only Relative Position exists. Each pair\nof groups $(g, h) \\in G \\times G$ can be represented as:\n$\\begin{aligned}\n&\\Delta P_{g,h}(t) = [P_h(t) - P_g(t)]\\\\\n&\\Delta \\Theta_{g,h}(t) = \\Theta_{h|g}(t)\\\\\n&\\Delta V_{g,h}(t) = [V_h(t) - V_g(t)]\n\\end{aligned}$\nIn this framework, $\\Delta P_{g,h}(t)$ denotes the difference in\nposition, $\\Delta \\Theta_{g,h}(t)$ represents the relevant angles at the con-\nnecting joint, and $\\Delta V_{g,h}(t)$ indicates the angular veloc-\nity of that joint. This formalism allows for a comprehen-\nsive analysis of motion dynamics across both physically and\nnon-physically connected kinematic groups."}, {"title": "3.2. Kinematic-Group Motion Dataset", "content": "Kinematic-aware Joint-Motion Text Annotation. Our method aims to capture detailed spatial-temporal local joint motions and their relationships to global action-level dynamics. To achieve this, we enhance the HumanML3D dataset [8] with fine-grained joint motion annotations by leveraging large language models for semantic representation. To achieve high-quality automatic annotation of the dataset, we delineate two strategies, detailed below.\nSpatial-Temporal Motion Processing.\nExisting human motion understanding models [2, 8, 23] struggle to capture subtle local movements due to the complex interplay of spatial and temporal dynamics. To address this, we propose a two-stage disentanglement approach, first resolving spatial dynamics and then temporal dynamics.\nTo capture fine-grained spatial information, we utilize PoseScript [4] to generate text descriptions for each pose frame with detailed angular rotations of joints for any target human pose, thus producing rich spatial annotations. To capture fine-grained temporal information, we propose a keyframe selection pipeline. We use sBERT [22] to extract embeddings for the per-frame pose descriptions. By calculating the cosine similarity between text embeddings, we assess the similarity of poses across the time frames. If the cosine similarity falls below a user-defined threshold, we label that frame as a keyframe. This process effectively filters keyframes from a sequence of poses based on temporal transitions. We then estimate temporal local motions by analyzing the pose differences for any joint group across a specified time window."}, {"title": "3.3. Hierarchical Text-Motion Alignment", "content": "With fine-grained text descriptions for joint-group and interactions to represent human motions, we extend the existing text-motion alignment to improve understanding of spatial human motion. To achieve this goal, we treat text descriptions in the text-to-motion (T2M) framework as coarse semantics and progressively integrate joint-level texts to refine the representation, achieving hierarchical alignment.\nModality Encoders. We follow TMR [18] to establish motion and text encoders. The motion sequences comprise of joint velocities, local joint positions, 6D local joint rotations, and foot contact labels, as described in [8]. The original text descriptions from HumanML3D act as coarse semantics. Additionally, we introduce joint-group motion descriptions and joint interaction descriptions as intermediate and final levels, thus generating hierarchical representations. Please refer to the supplementary document for details on the network architecture.\n$\\text{CrossAtt}(z_j, z_g) = \\text{softmax} \\left( \\frac{z_j z_g^T}{\\sqrt{d_k}} \\right) z_g.$\nThis operation allows us to integrate base semantics into low-level joint semantics. For the subsequent levels, we progressively add the resulting mean and standard deviation from the joint-group and joint-interaction embeddings back into the mean representation of the coarse text.\nContrastive Learning. We use contrastive learning to bridge the text and motion modalities [18],\nfor each semantic within the hierarchy. For simplicity,\nwe denote any level of text and motion pair as $(z_T, z_M)$.\nFor a batch of $N$ positive pairs of latent codes\n$(z_T^1, z_M^1), ..., (z_T^N, z_M^N)$, any pair $(z_T^i, z_M^j)$ where $i \\neq j$\nis considered a negative sample. The similarity matrix $S$\ncomputes the pairwise cosine similarities for all pairs in the\nbatch, defined as $S_{ij} = \\text{cos}(z_T^i, z_M^j)$. We apply InfoNCE\nloss [30], as follows:\n$L_{NCE} = - \\frac{1}{2N} \\left[ \\text{log} \\frac{\\text{exp} S_{ii} / \\tau}{\\sum_{j \\neq i} \\text{exp} S_{ij} / \\tau} + \\text{log} \\frac{\\text{exp} S_{ii} / \\tau}{\\sum_{j \\neq i} \\text{exp} S_{ji} / \\tau} \\right].$\nwhere $\\tau$ represents a temperature parameter.\nTo maximize the proximity between the two modalities,\nwe follow TMR [18] to construct a weighted sum of 3\nlosses: (a) Kullback-Leibler divergence loss $L_{KL}$, (b) cross-\nmodal embedding similarity loss $L_E$, and (c) motion recon-\nstruction loss $L_R$ for each semantic hierarchy."}, {"title": "3.4. Coarse-to-Fine Motion Generation", "content": "Joint Motion Reasoner. For open-vocabulary motion generation, we finetune LLaMA using KinMo Dataset as the low-level motion reasoner that, based on the global action-level text descriptions, generate the corresponding joint-group and joint-interaction text scripts through supervised learning. Given the low-level joint motion descriptions from the Motion Reasoner, we obtain their corresponding text embeddings based on hierarchical text encoders from Sec.3.3 to perform various motion generation applications.\nText-to-Motion Generation. We enhance the existing SOTA motion generator, MoMask [9], through a coarse-to-fine generation process. Specifically, we modify the text conditioning by constructing $T_{\\text{global}}, [T_{\\text{global}}; T_{\\text{joint}}]$, and $[T_{\\text{global}}; T_{\\text{joint}}; T_{\\text{inter}}]$ to represent three levels of text semantic control. We leverage one cross-attention layer akin to Sec. 3.3 to learn the correlation between high-level action and low-level joint semantics. These are prepended to the motion tokens before they are inserted into the generator, as shown in Fig. 2. The Mask Motion Generator first generates coarse motions based on the global action-level description. These initial motions are then refined by re-feeding them into the generator, now conditioned on joint-level motion semantics. This process is repeated in a similar manner, where the motion representations are further refined by incorporating joint-interaction semantics. For efficiency, the generator shares weights with the same logit classification loss functions used for motion reconstruction for the three"}, {"title": "4. Experiments", "content": "We conduct experiments on the motion-text benchmark dataset, HumanML3D [8], which collects 14,616 motions from AMASS [15] and HumanAct12 [7] datasets, with each motion described by 3 text scripts, totaling 44,970 descriptions. We adopt their pose representation and augment the dataset by mirroring, followed by a 80/5/15 split for training, validation, and testing, respectively. We enhance this dataset with each motion described by 6 joint-motion and 15 joint-interaction text scripts as in Sec. 3.2. Our implementation details are provided in the supplementary."}, {"title": "4.1. Text-Motion Retrieval", "content": "Evaluation metrics. To validate the effectiveness of incorporating joint semantics, we adopt TMR [18] settings to measure retrieval performance using recall scores at various ranks (e.g., R@1,R@2) and the median rank (MedR) of our results. MedR represents the median ranking position of the ground-truth result, with lower values indicating more precise retrievals. The four evaluation protocols used in our experiments are outlined below: (i) All uses the complete test dataset, though similar negative pairs may affect precision; (ii) All with threshold sets a 0.8 similarity threshold to determine accurate retrievals; (iii) Dissimilar subset uses 100 distinctly different sampled pairs measured by sBERT [22] embedding difference; and (iv) Small batches evaluates performance on random batches of 32 motion-text pairs.\nEvaluation results. We benchmark KinMo against three baselines [8, 17, 18]. Tab. 1 shows that our model outperforms existing baselines, especially for setting (a). This is mainly ascribed to our joint-level text descriptions that help resolve the action-level text-motion correspondence ambiguities, which in turn improves the discrimination of various motions with slightly distinct joint motion differences but with similar action-level text descriptions. The contribution of joint-level semantics is further enhanced given ROBERTa [14] as a stronger text encoder."}, {"title": "4.2. Text-Motion Generation", "content": "Evaluation Metrics. We adopt (1) Frechet Inception Distance (FID), as an overall motion quality metric to measure the distribution difference between features of the generated and real motions; (2) R-Precision and multimodal distance to quantify the semantic alignment between input text and generated motions; and (3) Multimodality for assessing the diversity of motions generated from the same text following T2M [8]. Please refer to the supplementary document for further details on our metrics.\nEvaluation Results. We compare KinMo against various methods for T2M Generation [3, 9, 17, 21, 29, 35]. As presented in Tab. 2, our method attains the best motion generation quality with the highest text alignment score. Additionally, thanks to the introduction of explicit joint-group and interaction motion representations, we observe that KinMo generates better aligned motions for any given dense and fine-grained text descriptions shown in Fig. 5, while other baseline methods fail to capture local body part movements.\nUser Study. We conducted a user study involving 20 participants and 320 samples - 80 from each of KinMo, MoMask [9], MMM [21] and STMC [20], to assess the quality of our results. Each participant was presented the video clips in a random order and asked to rate the results between 1 (lowest) to 5 (highest) based on (1) realness, (2) correctness of text-motion alignment, and (3) overall impression. Fig. 6 shows KinMo achieves higher Mean Opinion Scores (MOS) on all the criteria compared to the other methods, thereby proving improved accuracy of synthesized motion and better alignment with text condition."}, {"title": "4.3. Text-Motion Editing", "content": "Evaluation Metrics. Due to the lack of benchmark datasets and metrics, we generate 200 fine-grained text-prompt with its corresponding edited version by GPT4-0 [1]. The comparison is conducted by utilizing models to first generate the motion corresponding to the original text and then do editing to this generation based on the new instruction. To evaluate the editing quality, beyond generation metrics, we propose using Text-Motion Similarity score to measure the similarity of edited motion with editing global motion description based on Sec. 3.1, denoted as HTMA-S.\nEvaluation Results. We benchmark KinMo against various methods for T2M generation [9, 20, 21]. We discover KinMo is the only method capable of local temporal editing, while maintaining motion naturalness as shown in Fig. 4 and Tab. 3. Editing global semantics can be captured at both the joint and interaction semantic levels, thus achieving better generation and editing, as shown in Figs. 5 and 7."}, {"title": "4.4. Motion Trajectory Control", "content": "Evaluation metrics. Other than the metrics from Sec.4.2, we include (1) Trajectory error (Traj. err.): measures the ratio of unsuccessful trajectories, characterized by any control joint location error surpassing a predetermined thresh-"}, {"title": "4.5. Ablation Study", "content": "Hierarchical Motion Semantics. We investigate several strategies for semantic incorporation of text-motion alignment in Tab. 5: (1) only global action-level descriptions (global), (2) + joint-group (+ joint), (3) + joint-interaction (+ interact), and (4) - cross-attention (- cross). We observe that adding joint motion descriptions enhances the motion understanding, resolving the ambiguity of global action descriptions. In addition, our design based on cross-attention improves the connectivity of different hierarchy levels for motion representation. For an additional analysis of motion semantics, please refer to the supplementary document. As shown in Fig. 7, both joint and interaction semantics are beneficial for text-motion alignment on local body parts (hands in the example) during the generation process.\nCoarse-to-Fine Motion Generation. To assess the contribution of each component within our pipeline, we design the following variations: (1) CLIP-G: Only global motion description (original HumanML3D text) is applied for motion generation (as in MoMask [9]), (2) CLIP-J: We add joint-group semantics for generation based on CLIP, (3) CLIP-I: We add joint-interaction semantics for generation in addition to previous setting. We also apply these three settings for HTMA (Hierarchical Text Motion Alignment) to validate the effectiveness of our coarse-to-fine generation strategy and the benefits of text-motion alignment for generation. As shown in Fig. 8, coarse-to-fine procedure can enhance the motion generation quality. In addition, our proposed text-motion alignment can significantly speed up training and improve the performance."}, {"title": "5. Conclusion", "content": "We present KinMo, a novel framework that represents human motion as joint movements and interactions, thereby enabling fine-grained text-to-motion understanding, generation, editability and trajectory control. Our method progressively encodes global actions with joint-level interactions, and leverages these hierarchical text descriptions to generate coarse-to-fine motion through cross attention and contrastive learning. The dataset will be made publicly available to the scientific community. Extensive comparisons with state-of-the-art methods show that KinMo improves text-motion alignment and kinematic body part control. Future directions would include extending our method to fine-grained text-guided image-to-video generation or video-to-video editing tasks."}]}