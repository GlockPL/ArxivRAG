{"title": "Prompt Tuning with Diffusion for Few-Shot Pre-trained Policy Generalization", "authors": ["Shengchao Hu", "Wanru Zhao", "Weixiong Lin", "Li Shen", "Ya Zhang", "Dacheng Tao"], "abstract": "Offline reinforcement learning (RL) methods harness previous experiences to derive an optimal policy, forming the foundation for pre-trained large-scale models (PLMs). When encountering tasks not seen before, PLMs often utilize several expert trajectories as prompts to expedite their adaptation to new requirements. Though a range of prompt-tuning methods have been proposed to enhance the quality of prompts, these methods often face optimization restrictions due to prompt initialization, which can significantly constrain the exploration domain and potentially lead to suboptimal solutions. To eliminate the reliance on the initial prompt, we shift our perspective towards the generative model, framing the prompt-tuning process as a form of conditional generative modeling, where prompts are generated from random noise. Our innovation, the Prompt Diffuser, leverages a conditional diffusion model to produce prompts of exceptional quality. Central to our framework is the approach to trajectory reconstruction and the meticulous integration of downstream task guidance during the training phase. Further experimental results underscore the potency of the Prompt Diffuser as a robust and effective tool for the prompt-tuning process, demonstrating strong performance in the meta-RL tasks.", "sections": [{"title": "1 Introduction", "content": "Over the last few years, pre-trained large-scale models (PLMs) have demonstrated remarkable efficacy across diverse domains, including high-resolution image generation from text descriptions (DALL-E [32], ImageGen [35]) and language generation (GPT [3]). The broad applicability and success of PLMs have sparked interest in their potential application to decision-making frameworks. Moreover, the advent of the prompt-tuning technique has further empowered PLMs to adapt rapidly to downstream tasks by fine-tuning only a small number of parameters across various model scales and domains. This efficient and effective adaptation process has made prompt-tuning a promising approach for tailoring PLMs to specific decision-making scenarios.\nIn the domain of reinforcement learning (RL), offline decision-making assumes a critical role, facilitating the acquisition of optimal policies from trajectories gathered by behavior policies, all without requiring real-time interactions with the environment. Nonetheless, offline RL encounters formidable challenges concerning generalization to unseen tasks and the fulfillment of varying constraints, primarily due to distribution shifts [27]. Recent research efforts, such as Gato [33] and other generalized agents [20], have explored the use of transformer-based architectures and sequence modeling techniques to address multi-task problems in offline RL. Utilizing prompt-tuning techniques, these methods can efficiently adapt to the target task by fine-tuning a relatively small number of parameters. Nevertheless, it is crucial to recognize that prompt-tuning methods often exhibit sensitivity to initialization [13, 21]. When a random prompt is utilized for initialization, the PLM's exploration may become constrained within a limited region, leading the subsequent updating process to converge towards a sub-optimal prompt, as empirically demonstrated in Section 2.3. This sensitivity necessitates the pre-collection of expert trajectories, which in turn limits their applicability across a broader range of scenarios.\nTo circumvent the dependency on the quality of the initial prompt, we shift our perspective towards the generative model, framing the prompt-tuning process as a form of conditional generative modeling, where prompts are generated from random noise. This approach obviates the need to collect expert prompts, and the final quality of generated prompts is then determined by the parameters of the generative model, which can incorporate prior knowledge via pre-training on a fixed, pre-collected training dataset. However, in few-shot meta-learning environments, the quantity of offline data available from the target task is severely limited, necessitating rapid adaptability of the generative model to these tasks despite the small datasets. Moreover, the quality of these offline datasets typically falls short of expert quality, where the generative model must produce prompts that exceed the quality of the fine-tuning datasets, rather than simply generating prompts within the same distribution. Additionally, given the physical significance of trajectory prompts, even minor perturbations in trajectory prompts can lead to substantial shifts in meaning [13], highlighting the imperative need for precision in the generated prompts. All of these factors collectively contribute to the challenges encountered when applying this new paradigm to the prompt-tuning process.\nTo address these challenges, we introduce a novel algorithm named Prompt Diffuser (see Figure 1) which leverages a conditional diffusion model to produce prompts of exceptional quality. Within our framework for prompt generation, we establish the trajectory representation of Prompt Diffuser and adopt diffusion models to develop a generative model conditioned on returns, which ensures the precision of the generated prompts and expedites adaptability to new tasks (detailed in Section 3). Nevertheless, optimizing Prompt Diffuser solely with the DDPM loss can only achieve performance on par with the original dataset [39, 22]. To augment the quality of prompt generation, we seamlessly incorporate guidance from downstream tasks into the reverse diffusion chain. By leveraging gradient projection techniques, the downstream task guidance is incorporated into the learning process without compromising the overall performance of the diffusion model, which is achieved by projecting the gradient of guidance loss onto the orthogonal direction to the subspace spanned by the diffusion loss. This novel approach successfully directs the generation of high-quality prompts, leading to improved performance in downstream tasks.\nIn summary, our work introduces a novel prompt-tuning framework that leverages diffusion models to generate high-quality prompts for RL agents. By incorporating downstream task guidance and employing gradient projection techniques, we successfully enhance the quality of generated prompts, leading to improved performance in downstream tasks. The experiments validate the effectiveness of our approach and demonstrate its potential for generating adaptive and transferable policies in meta-RL settings. Our contributions advance the field of prompt-tuning, providing a promising direction for optimizing pre-trained RL agents and improving their generalization and performance across various downstream tasks."}, {"title": "2 Preliminary", "content": "Transformer [38] has been increasingly investigated in RL using the sequence modeling pattern in recent years. Moreover, works from natural language processing (NLP) suggest Transformers pre-trained on large-scale datasets demonstrate promising few-shot or zero-shot learning capabilities within the prompt-based framework [26, 3]. Building upon this, Prompt-DT [40] extends the prompt-based framework to the offline RL setting, allowing for few-shot generalization to unseen tasks. In contrast to NLP, where text prompts can be transformed into conventional blank-filling formats to represent various tasks, Prompt-DT introduces the concept of trajectory prompts, leveraging few-shot demonstrations to provide guidance to the RL agent. A trajectory prompt comprises multiple tuples of state $s^*$, action $a^*$ and return-to-go $\\hat{r}^*$, represented as $(s^*, a^*, \\hat{r}^*)$, following the notation in Chen et al. [4]. Each element marked with the superscript \u00b7* is relevant to the trajectory prompt. Note that the length of the trajectory prompt is usually shorter than the task's horizon, encompassing only essential information to facilitate task identification, yet inadequate for complete task imitation. During training with offline collected data, Prompt-DT policy $\\pi$ utilizes $r^{\\text{input}} = (\\mathcal{T}^*, \\mathcal{T}_i)$ as input for each task $\\mathcal{T}_i$. Here, $\\mathcal{T}^{\\text{input}}$ consists of the $K^*$-step trajectory prompt $\\mathcal{T}^*$ and the most recent $K$-step history $\\mathcal{T}_i$, and is formulated as follows:\n$\\mathcal{T}^{\\text{input}} = (\\hat{r}_{i,1}^*, s_{i,1}^*, a_{i,1}^*, ..., \\hat{r}_{i,K^*}^*, s_{i,K^*}^*, a_{i,K^*}^*, \\hat{r}_{i,1}, s_{i,1}, a_{i,1}, ..., \\hat{r}_{i,K}, s_{i,K}, a_{i,K}).$ (1)\nThe prediction head linked to a state token s is designed to predict the corresponding action a. For continuous action spaces, the training objective aims to minimize the mean-squared loss:\n$\\mathcal{L}_{DT} = \\mathbb{E}_{r^{\\text{input}}, \\mathcal{D}_i} \\left[ \\frac{1}{K} \\sum_{m=1}^K ||a_{i,m} - \\pi(s_{i,m}, r^{\\text{input}})||^2 \\right].$ (2)\nwhere $\\mathcal{D}_i$ denotes the training dataset for task $\\mathcal{T}_i$ and $\\mathcal{T}_{i,m} = (\\hat{r}_{i,1}, s_{i,1}, a_{i,1}, ..., \\hat{r}_{i,m}, s_{i,m})$."}, {"title": "2.1 Prompt Decision Transformer", "content": "2.2 Diffusion Models\nDiffusion models [37, 11] represent a particular class of generative models that learn the data distribution $q(x)$ from a dataset {x}. The data-generation process is modeled through a two-stage process. In the forward diffusion chain, noise is gradually added to the data $x^0 \\sim q(x)$ in N steps, following a pre-defined variance schedule $\\beta_i$, expressed as:\n$q(x_{1:N}|x^0) := \\prod_{k=1}^N q(x_k|x_{k-1}), \\quad q(x_k|x_{k-1}) := \\mathcal{N}(x_k; \\sqrt{1 - \\beta_k} x_{k-1}, \\beta_k I).$ (3)\nThe trainable reverse diffusion chain is formulated as:\n$p_\\theta(x_{0:N}) := \\mathcal{N}(x_N; 0, I) \\prod_{k=1}^N p_\\theta(x_{k-1}|x_k), \\quad p_\\theta(x_{k-1}|x_k) := \\mathcal{N}(x_{k-1}; \\mu_\\theta(x_k, k), \\Sigma_\\theta(x_k, k)).$ (4)\nHo et al. [11] propose the simplified loss function in DDPM for the diffusion timestep k:\n$L_k = \\mathbb{E}_{k, x^0, \\epsilon_k} [||\\epsilon_k - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_k} x^0 + \\sqrt{1 - \\bar{\\alpha}_k} \\epsilon_k, k)||^2],$ (5)\nwhere $\\bar{\\alpha}_k = \\prod_{i=1}^k(1 - \\beta_i)$, $\\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_k} x^0 + \\sqrt{1 - \\bar{\\alpha}_k} \\epsilon_k, k)$ represents the noise predicted by the neural network and $\\epsilon_k$ denotes the true noise utilized in the forward process. In this study, we leverage the robust capabilities of diffusion models to reconstruct the data distribution, denoted as $q(x)$, for the generation of high-precision prompts."}, {"title": "2.2 Diffusion Models", "content": "2.3 Rethinking the RL prompts\nWhat is the essence of RL prompts? In NLP-based prompt learning, the fundamental assumption is that large language models have acquired sufficient knowledge from their pre-training data, and our task is to discover the most effective means of extracting this knowledge. However, in the realm of RL, it is impractical to assemble a corpus that comprehensively encompasses the diverse environments and tasks encountered. Thus RL agent is required to imitate the provided trajectory prompts, rather than using prompts to extract knowledge from the pre-trained model, which highlights the importance of curating high-quality prompts and is empirically detailed in Section 4.\nWhy do we need to resort to generative models? When a random prompt is utilized for initialization, the PLM's exploration may become constrained within a limited region, leading the subsequent updating process to converge towards a sub-optimal prompt, which is illustrated in a simplified manner in Figure 2(a). In support of this assertion, we investigate various prompt-tuning methods with different prompt initializations in Cheetah-vel environments. As depicted in Figure 2(b), when the prompt is initialized with random quality, it is challenging to optimize it to expert quality, resulting in significantly poorer performance compared to expert initialization.\nThese observations advocate for a novel paradigm to replace the conventional prompt-tuning approach in the field of RL. Our proposed solution is the generative model, which initially integrates prior knowledge through pre-training on the training prompt datasets and directly generates prompts from random noise, thereby eliminating the dependence on the quality of initial prompts."}, {"title": "2.3 Rethinking the RL prompts", "content": "3 Method\nOur objective is to maximize rewards in the target domain using PLM and conduct experiments on few-shot policy generalization tasks, evaluating the PLM's capability to generalize to new tasks. In line with suggestions from the fields of NLP [24] and CV [6], fine-tuning the prompt for PLM proves to be more effective. However, the prompt-tuning approach remains relatively unexplored in the domain of RL, which presents new problems and challenges [13]. We formulate the prompt-tuning process as the standard problem of conditional generative modeling (GM):\n$\\max_\\theta \\mathbb{E}_{s_0 \\sim p_0} \\sum_{\\tau \\sim \\mathcal{C}} \\left[ \\sum_{t=1}^T R(s_t, \\text{PLM}(\\text{prompt}, s_{0:t}, a_{0:t-1}, \\theta)) \\right],$ (6)\nwhere $\\text{prompt} \\sim \\text{GM}_\\theta(\\mathcal{T}^{\\text{initial}} | \\mathcal{C}^*),$ where condition C could encompass various factors, such as the return achieved under the trajectory, the constraints met by the trajectory, or the skill demonstrated in the trajectory. Here we adopt the Prompt-DT as the PLM and MLP-based diffusion model as the GM. The trajectory is constructed based on the conditional diffusion process:\nq(x_{k+1}(\\tau^*) | x_k(\\tau^*)), \\quad p_\\theta(x_{k-1}(\\tau^*) | x_k(\\tau^*), y(\\tau^*)). (7)\nHere, q denotes the forward noising process, while $p_\\theta$ represents the reverse denoising process.\nIn the following sections, we first provide a detailed explanation of our approach employing a conditional diffusion model as an expressive GM for prompt generation. We then introduce diffusion loss, which acts as the behavior cloning term, constraining the distribution of the generated prompt to that of the training dataset. Lastly, we discuss the incorporation of downstream task guidance during the training aiming at enhancing the quality of the generated prompt. The overall pipeline of our Prompt Diffuser is depicted in Figure 1, providing a detailed presentation of the diffusion formulation, the corresponding diffusion loss, and the downstream task guidance."}, {"title": "3 Method", "content": "3.1 Diffusion Formulation\nIn images, the diffusion process is applied across all pixel values in an image. Analogously, it may seem intuitive to apply a similar process to model the states and actions within a trajectory. To align with the input format Equation 1 required by the PLM, we formulate $x^0(\\tau^*)$ as the transition sequence that encompasses states, actions, and rewards:\nx^0(\\tau^*) := \\begin{bmatrix} s_t \\\\ a_t \\\\ \\hat{r}_t \\end{bmatrix}  \\begin{bmatrix} s_{t+1} \\\\ a_{t+1} \\\\ \\hat{r}_{t+1} \\end{bmatrix}  ...  \\begin{bmatrix} s_{t+H-1} \\\\ a_{t+H-1} \\\\ \\hat{r}_{t+H-1} \\end{bmatrix} (8)\nwith the condition:\ny(\\tau^*) := \\begin{bmatrix} t \\\\ \\hat{r}_t \\end{bmatrix}  \\begin{bmatrix} t+1 \\\\ \\hat{r}_{t+1} \\end{bmatrix}  ...  \\begin{bmatrix} t+H-1 \\\\ \\hat{r}_{t+H-1} \\end{bmatrix} , (9)\nwhere y($\\tau^*$) contains the returns-to-go $\\hat{r} = r^{T} \\sum_{t'=t}^T r_{t'}$ and timesteps. Although the reward token r is not directly utilized in the prompt formation 1, denoising the entire transition process can introduce model bias towards the inherent transition dynamics [10]. Given the trajectory representation, one approach could involve training a classifier $p_\\phi(y(\\tau^*) | x_k(\\tau^*))$ to predict y($\\tau^*$) from noisy trajectories $x_k(\\tau^*)$. However, it necessitates estimating a Q-function, which entails a separate, complex dynamic programming procedure. Instead, we opt to directly train a conditional diffusion model conditioned on the y(*) as per Equation 9. This conditioning empowers the model with the capacity to discern distinct tasks effectively, which expedites the pre-trained model's adaptation to novel tasks that exhibit analogous conditions."}, {"title": "3.1 Diffusion Formulation", "content": "3.2 Diffusion Loss\nWith the diffusion formulation, our objective is to generate a prompt trajectory that facilitates the rapid adaptation of the PLM to novel and unseen target tasks. By utilizing a collection of trajectory prompts derived from these unseen tasks, our intention is to extract sufficient task-specific information. This information is crucial for guiding the Prompt Diffuser in generating the most appropriate and effective prompt trajectory tailored to the intricacies of the target task. Thus during the training process, we adopt the approach from DDPM [11] and add extra conditions to train the reverse diffusion process $p_\\theta$, which is parameterized by the noise model $\\epsilon_\\theta$ with loss:\n$\\mathcal{L}_{DM} = \\mathbb{E}_{k \\sim U, t^* \\sim D^*, \\epsilon_k \\sim \\mathcal{N}(0, I)} [||\\epsilon_k - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_k} x^0(\\tau^*) + \\sqrt{1 - \\bar{\\alpha}_k} \\epsilon_k, y(\\tau^*), k)||^2],$ (10)\nwhere U is a uniform distribution over the discrete set as {1, ..., N} and $D^*$ denotes the initial prompt dataset, collected by behavior policy $\\pi_b$.\nNext, we demonstrate that the loss term $\\mathcal{L}_{DM}$ functions as a behavior cloning term, effectively constraining the distribution of the generated prompt to match that of the training dataset. Suppose $x^0(\\tau^*)$ is a continue random vector, and\n$\\mathbb{P}(\\|x^0(\\tau^*)\\|^2 < R = N_{CR} | t^* \\sim D^*) = 1,$ (11)\nfor some arbitrarily large constant $C_R > 0$. We denote a total variation distance between two distributions $P_1$ and $P_2$ as TV$(P_1, P_2)$ and the upper bound of the total variation distance between the learned and true trajectory distributions is shown below [22]:\n$\\text{TV}(q, p) \\le \\sqrt{\\text{KL}(q||p)} \\le C_1 \\frac{d^2 \\log^3 N}{\\sqrt{N}}$ (12)\nAs the diffusion timesteps N increase, the generated prompt distribution p progressively converges towards the training dataset q, thereby ensuring that the generated prompts remain within the in-distribution. However, it also imposes a limitation, as it prevents the generated prompts from surpassing the performance of the behavior trajectories contained within the offline dataset $D^*$."}, {"title": "3.2 Diffusion Loss", "content": "3.3 Diffusion Guidance\nTo improve the quality of the prompts, we incorporate downstream task guidance into the reverse diffusion chain during the training stage, with the objective of generating prompts that have a positive impact on the performance of the downstream tasks. With the output from the diffusion model, the loss for the downstream tasks can be formulated as follows:\n$\\mathcal{L}_{DT} = \\mathbb{E}_{r^{\\text{input}} \\mathcal{T}_i} \\frac{1}{K} \\sum_{m=1}^K ||a_{i,m} - \\text{PLM}(s_t, r^{\\text{input}})||^2,$ (13)\nNote that $x^0(\\tau^*)$ is sampled through Equation 4, allowing the gradient of the loss function with respect to the prompt to be backpropagated through the entire diffusion chain.\nNonetheless, a direct linear combination update of these two losses might lead to a deterioration in performance, potentially owing to the competitive interaction among distinct losses, which is elucidated in Section 4.4. Towards this end, we characterize the correlation between the two loss subspaces, employing gradient projection as the analytical tool. Specifically, let $\\mathcal{S}_{DM} = \\text{span}\\{B\\} = \\text{span}\\{\\mu_1, ..., \\mu_M\\}$ represent the subspace spanned by $\\nabla \\mathcal{L}_{DM}$, where B constitutes the bases for $\\mathcal{S}_{DM}$ and $(\\cdot)^\\perp$ denotes the orthogonal space (consisting of a total of M bases extracted from $\\nabla \\mathcal{L}_{DM}$). For any matrix A with a suitable dimension, denote its projection onto subspace $\\mathcal{S}_{DM}$ as:\n$\\text{Proj}_\\mathcal{S}_{DM}(A) = AB B^T,$ (14)\nwhere $(\\cdot)^\\text{is the matrix transpose. Based on the Equation 14, the final update gradient can be:\n$\\nabla \\mathcal{L} = \\begin{cases} \\nabla \\mathcal{L}_{DM} + \\lambda \\text{Proj}_{\\mathcal{S}_{DM}^\\perp}(\\nabla \\mathcal{L}_{DT}), & \\nabla \\mathcal{L}_{DM} \\cdot \\nabla \\mathcal{L}_{DT} < 0, \\\\ \\nabla \\mathcal{L}_{DM} + \\lambda \\nabla \\mathcal{L}_{DT}, & \\text{else}, \\end{cases}$ (15)\nwhere $\\text{g}_{DM}$ and $\\text{g}_{DT}$ denote the gradients $\\nabla \\mathcal{L}_{DM}$ and $\\nabla \\mathcal{L}_{DT}$, respectively, and the hyper-parameter $\\lambda$ is employed to balance the downstream guidance ($\\nabla \\mathcal{L}_{DT}$) and the diffusion loss ($\\nabla \\mathcal{L}_{DM}$). Note that $\\mathcal{L}_{DM}$ can be efficiently optimized by sampling a single diffusion step i for each data point, but $\\mathcal{L}_{DT}$ necessitates iteratively computing $\\epsilon_\\theta$ networks N times, which can potentially become a bottleneck for the running time and may lead to vanishing gradients. Thus we restrict the value of N to a relatively small value. We also provide the theoretic support for our gradient projection technique in Appendix E, providing a rigorous foundation that guarantees the improvement in performance.\nAfter the training process, we adopt the low-temperature sampling technique [1] to produce high-likelihood sequences:\nx_{k-1}(\\tau^*) \\sim \\mathcal{N}(\\mu_\\theta(x_k(\\tau^*), y(\\tau^*), k), \\beta \\Sigma_k) (16)\nwhere the variance is reduced by $\\beta \\in [0, 1)$ for generating better quality sequences and $\\mu_\\theta(x_k(\\tau^*), y(\\tau^*), k)$ is constructed as:\n$\\mu_\\theta(x_k(\\tau^*), y(\\tau^*), k) = \\frac{1}{\\sqrt{\\alpha_k}} \\left(x_k(\\tau^*) - \\frac{\\beta_k}{\\sqrt{1 - \\bar{\\alpha}_k}} \\epsilon_\\theta(x_k(\\tau^*), y(\\tau^*), k)\\right);$ (17)\nOverall, pseudocode for the conditional prompt diffuser method is given in Algorithm 1."}, {"title": "3.3 Diffusion Guidance", "content": "4 Experiment\nWe perform extensive experiments to assess the ability of Prompt Diffuser by using the episode accumulated reward as the evaluation metric. Our experimental evaluation seeks to answer the following research questions: (1) Does Prompt Diffuser improve the model generalization by generating a better prompt? (2) How does the quality of the prompt datasets influence the effectiveness of the Prompt Diffuser? (3) Does the diffusion guidance successfully facilitate the downstream task performance without disrupting the DDPM update progress? (4) Can Prompt Diffuser generalize to out-of-distribution tasks under few-shot and zero-shot settings?"}, {"title": "4 Experiment", "content": "4.1 Environments and Offline Datasets\nTo ensure a fair comparison with Prompt-Tuning DT [13], we select four distinct meta-RL control tasks: Cheetah-dir, Cheetah-vel, Ant-dir, and Meta-World reach-v2. In the Cheetah-dir and Ant-dir tasks, the objective is to incentivize high velocity in the goal direction. On the other hand, the Cheetah-vel task penalizes deviations from the target velocity using 12 errors. The Meta-World benchmark [43] includes table-top manipulation tasks that require a Sawyer robot to interact with various objects. For our evaluation, we utilize the Meta-World reach-v2 benchmark, which comprises 45 training tasks for pre-training the Prompt-DT and the Prompt Diffuser. Subsequently, the testing set, consisting of 5 tasks with different goal positions, is employed for further fine-tuning the Prompt Diffuser. We follow the dataset construction and settings outlined in Hu et al. [13] for the meta-RL control tasks.Specifically, the datasets are constructed using the full replay buffer of Soft Actor-Critic [9] for Cheetah-dir and Ant-dir tasks, and TD3 [7] for the Cheetah-vel task. For the Meta-World-reach-v2 benchmark, we collect an offline dataset with the rule-based script policy provided in [43]. Each trajectory in the dataset contains states, actions, and dense rewards at each timestep."}, {"title": "4.1 Environments and Offline Datasets", "content": "4.2 Baselines\nWe compare our proposed Prompt Diffuser with six baseline methods to address the aforementioned questions. For each method, we assess task performance based on the episode accumulated reward in each testing task. To ensure a fair comparison, all fine-tuning methods utilize the same PLM. The baseline methods are as follows (details are presented in Appendix D): (1) Prompt-DT [40] exclusively employs the trajectory prompt for the target task without any additional fine-tuning process during testing. Our evaluation includes distinct experiments employing random and expert prompts. (2) MT-ORL omits the prompt augmentation step used in Prompt-DT to construct a variant of the approach. (3) Soft Prompt treats prompt as a \"soft prompt\" and updates it using the AdamW optimizer, analogous to a common practice in the NLP domain. (4) Adaptor plugs an adaptor module into each decoder layer, inspired by HDT [41], except for the hyper-network used for initialization. (5) Prompt-Tuning DT [13] represents the first application that incorporates prompt tuning techniques in the RL domain, catering to specific preferences in the target environment with preference ranking. (6) Prompt-DT-FT fine-tunes the entire model parameters of the pre-trained model during testing, utilizing a limited amount of data from the target task."}, {"title": "4.2 Baselines", "content": "4.3 Main Results\nWe conduct a comparative analysis between the Prompt Diffuser and the baseline methods to evaluate their few-shot generalization capabilities and assess the tuning efficiency of the Prompt Diffuser in comparison to other fine-tuning approaches. The main results, encompassing the few-shot performance of different algorithms, along with their corresponding tuning parameter size and the percentage it represents in the total size, are summarized in Table 1.\nThe outcomes of MT-ORL underscore its inability to attain high rewards in the target tasks, thus highlighting the crucial role of prompt assistance in facilitating the PLM's adaptation to these specific tasks. The comparison between random and expert prompts in Prompt-DT further accentuates the necessity of high-quality prompts, which serves as a strong rationale for the development of our prompt-tuning techniques. Among the parameter-efficient fine-tuning baselines, only Prompt-Tuning DT manages to achieve performance comparable to Prompt-DT-FT. Importantly, our proposed approach exhibits significant performance improvements over both methods, even nearing the upper bound established by fine-tuning conducted on the full-data settings. This serves as a vivid demonstration of the distinct advantages offered by our innovative prompt-tuning techniques."}, {"title": "4.3 Main Results", "content": "4.4 Ablation\nWe perform several ablation studies to examine particular aspects of our Prompt Diffuser, with a primary focus on the meta-RL control environments. These ablation studies are designed to offer insights into essential factors related to common prompt-tuning methods, such as prompt initialization. This analysis enables us to highlight the advantages and drawbacks of our proposed approach in comparison to other prompt-tuning techniques. More ablation studies about our Prompt Diffuser and the diversity of generated prompts can be found in Appendix G and F.\nPrompt Initialization. Prompt initialization plays a crucial role in guiding the agent's behavior and shaping the learning process [8]. To investigate its impact, we conduct an ablation study in the Cheetah-vel environment. The results of this ablation study are presented in Table 2. Compared to Prompt-Tuning DT, which exhibits sensitivity to prompt quality, the Prompt Diffuser method displays robustness to variations in training data and prompt initialization. The key factor contributing to this robustness lies in the fundamental difference in the optimization strategies employed by the two methods. In the case of Prompt-Tuning DT, the prompt is directly updated during the optimization process. On the other hand, the Prompt Diffuser adopts a different approach by modeling the prompt-tuning process as conditional generative models, which are trained on the prompt with the additional guidance of downstream task loss. This distinctive formulation allows the Prompt Diffuser to refine and improve the prompt quality through the conditioning of downstream task loss, even when fine-tuning with relatively poor random prompt distributions. This finding provides valuable insights into the potential of Prompt Diffuser for prompt-tuning tasks, as it demonstrates the ability to achieve enhanced performance without relying solely on an expert dataset.\nDiffusion Guidance. To improve the quality of the prompts, we propose a novel approach that involves integrating downstream task guidance into the reverse diffusion chain, while carefully preserving the continuity of the DDPM update progress. Leveraging the gradient projection tech- nique, we successfully integrate the downstream task information into the learning process without"}, {"title": "4.4 Ablation", "content": "Out-of-Distribution Ability. In alignment with the experimental framework of Prompt-Tuning DT [13], our methodology entails training on a broad spectrum of tasks and evaluating on a select few that are held out. These held-out tasks feature goals (target velocity or direction) that fall within the spectrum defined by the training tasks. Our objective is to ascertain the efficacy of Prompt Diffuser in managing tasks whose goals surpass the scope of the training range, thereby evaluating its generalization capabilities in out-of-distribution scenarios. For this purpose, we designate eight training tasks within Ant-dir and select three for testing two with indices below the minimum and one exceeding the maximum task index encountered in training. The task index correlates directly with the desired directional angle, as elaborated in the Appendix A. Our results, presented in Table 3, demonstrate that Prompt Diffuser outperforms other state-of-the-art baseline prompt-based methods, thereby substantiating the robust out-of-distribution capability of our method, which consistently delivers superior performance.\nZero-Shot Ability. In the context of zero-shot settings, where no information about unseen tasks is available and the model cannot resort to task-specific prompts or undergo further fine-tuning with few-shot prompt datasets, traditional prompt tuning and adaptor methods are not applicable. This scenario presents a unique challenge and holds significant research value. To evaluate zero-shot performance, we undertake an out-of-distribution assessment in the Ant-dir environment, chosen for its diverse task range and thus serving as an effective testbed. We present a comparative analysis of the performance in Table 3. Notably, the Prompt Diffuser significantly outperforms the traditional Prompt-DT method in the zero-shot setting. This outcome highlights the efficacy of our model in adapting to new tasks even without prior exposure or task-specific tuning. However, it is important to note that our model still falls considerably behind the performance achieved in the few-shot settings."}, {"title": "Out-of-Distribution Ability", "content": "5 Conclusion\nWe introduce Prompt Diffuser, a methodology that leverages a conditional diffusion model to generate superior-quality prompts. This approach shifts the traditional prompt-tuning process towards a conditional generative model, where prompts are generated from random noise. Through our trajectory reconstruction model and gradient projection techniques, Prompt Diffuser effectively overcomes the need for pre-collecting expert prompts and facilitates the PLM's efficient adaptation to novel tasks using generated prompts. Extensive experimental results demonstrate the substantial performance advantage of our approach over other parameter-efficient methods, approaching the upper bound in performance. We anticipate that our work will pave the way for the application of prompt-tuning techniques in the realm of RL, offering a generative model perspective.\nLimitations. We introduce a novel prompt-tuning framework that leverages diffusion models to generate high-quality prompts for generalist RL agents, overcoming the limitations of traditional prompt-tuning methods. However, due to limited computational resources, our evaluation primarily focuses on relatively smaller task settings and models. Our Prompt Diffuser may struggle to generate prompts that significantly diverge from the training dataset, warranting further research."}, {"title": "5 Conclusion", "content": "A Detailed Environment\nWe evaluate our approach on a variety of tasks, including meta-RL control tasks. These tasks can be described as follows:\n\u2022 Cheetah-dir: The task comprises two directions: forward and backward, in which the cheetah agent is incentivized to attain high velocity along the designated direction. Both the training and testing sets encompass these two tasks, providing comprehensive coverage of the agent's performance.\n\u2022 Cheetah-vel: In this task, a total of 40 distinct tasks are defined, each characterized by a different goal velocity. The target velocities are uniformly sampled from the range of 0 to 3. The agent is subjected to a penalty based on the 12 error between its achieved velocity and the target velocity. We reserve 5 tasks for testing purposes and allocate the remaining 35 tasks for training.\n\u2022 Ant-dir: There are 50 tasks in Ant-dir, where the goal directions are uniformly sampled in a 2D space. The 8-joint ant agent is rewarded for achieving high velocity along the specified goal direction. We select 5 tasks for testing and use the remaining tasks for training.\n\u2022 Meta-World reach-v2: This task involves controlling a Sawyer robot's end-effector to reach a target position in 3D space. The agent directly controls the XYZ location of the end-effector, and each task has a different goal position. We train on 15 tasks and test on 5 tasks.\nBy evaluating our approach on these diverse tasks, we can assess its performance and generalization capabilities across different control scenarios.\nThe generalization capability of our approach is evaluated by examining the task index of the training and testing sets, as shown in Table 4. The experimental setup in Section 4 adheres to the training and testing division specified in Table 4. This ensures consistency and allows for a comprehensive assessment of the approach's performance across different tasks.\nTo assess the OOD generalization, we have conducted tests in the Ant-dir environment with a specific focus on scenarios that fall outside the distribution of training set. The training and testing sets for this OOD evaluation are detailed as Table 4."}, {"title": "A Detailed Environment", "content": "B Implementation Details\nWe construct our policy as an MLP-based conditional diffusion model. Following the parameterization approach of [29", "1": "interval, based on the respective tokens' maximum and minimum values. During the training phase, we employ prompts from training tasks to pre-train the diffusion model. Subsequently, we fine"}]}