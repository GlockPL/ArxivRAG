{"title": "COMMUNICATING ACTIVATIONS BETWEEN LANGUAGE MODEL AGENTS", "authors": ["Vignav Ramesh", "Kenneth Li"], "abstract": "Communication between multiple language model (LM) agents has been shown\nto scale up the reasoning ability of LMs. While natural language has been the\ndominant medium for inter-LM communication, it is not obvious this should be\nthe standard: not only does natural language communication incur high inference\ncosts that scale quickly with the number of both agents and messages, but also\nthe decoding process abstracts away too much rich information that could be oth-\nerwise accessed from the internal activations. In this work, we propose a simple\ntechnique whereby LMs communicate via activations; concretely, we pause an\nLM B's computation at an intermediate layer, combine its current activation with\nanother LM A's intermediate activation via some function f, then pass f's output\ninto the next layer of B and continue the forward pass till decoding is complete.\nThis approach scales up LMs on new tasks with zero additional parameters and\ndata, and saves a substantial amount of compute over natural language communi-\ncation. We test our method with various functional forms f on two experimental\nsetups-multi-player coordination games and reasoning benchmarks and find\nthat it achieves up to 27.0% improvement over natural language communication\nacross datasets with <1/4 the compute, illustrating the superiority and robustness\nof activations as an alternative \"language\" for communication between LMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Language is for the purpose of communication. As large language models (LLMs) have been in-\ncreasingly used to power autonomous, goal-driven agents capable of reasoning, tool usage, and\nadaptive decision-making (Yao et al., 2023; Xi et al., 2023; Wang et al., 2024; Ahn et al., 2022;\nSchick et al., 2023; Shen et al., 2023; Park et al., 2023; Nakano et al., 2022), communication be-\ntween multiple cooperating agents has emerged as an intuitive approach to amplify the reasoning\ncapabilities of LLMs (Wu et al., 2023). Explicit communication in natural language between multi-\nple LLMs has been shown to encourage divergent thinking (Liang et al., 2023), improve factuality\nand reasoning (Du et al., 2023), enable integration of cross-domain knowledge (Sukhbaatar et al.,\n2024), and allow for modular composition of abilities in a complementary manner (Wu et al., 2023;\nPrasad et al., 2023).\n\nA critical problem with natural language communication, however, is that it incurs extremely high\ninference costs that scale quickly with the number of agents as well as length and number of mes-\nsages (Du et al., 2023; Yang et al., 2023; Wu et al., 2023). Restricting LLM communication to\nnatural language also raises the question: as LLMs are increasingly capable of handling larger, more\ncomplex tasks (sometimes with \u201csuper-human\" ability) (Wei et al., 2022; Burns et al., 2023), might\nthey communicate more effectively in representations of higher dimension than natural language?\nWhile using natural language as a communicative medium is appealing due to its interpretability,\nwe claim that it may not be optimal for inter-LLM communication. Natural language generation\nuses only one token to represent the model's belief over the entire vocabulary, which risks losing\ninformation embedded within the model output logits (Pham et al., 2024); furthermore, a model's\nbelief over the entire vocabulary is itself not always better (for communicative purposes) than the\nmodel's (often richer) representation of the input in earlier layers. Indeed, Hernandez et al. (2024)"}, {"title": "2 RELATED WORK", "content": "Multi-agent communication The field of multi-agent communication has a long-standing history.\nNotably, prior works on emergent communication have showed that agents can autonomously evolve\ncommunication protocols when deployed in multi-agent environments that enable cooperative and\ncompetitive game-play (Sukhbaatar et al., 2016; Foerster et al., 2016; Lazaridou et al., 2017). How-\never, recent experiments have demonstrated that learning meaningful languages from scratch, even\nwith centralized training, remains difficult (Lowe et al., 2020; Chaabouni et al., 2019; Jaques et al.,\n2019).\n\nWith the emergence of large pre-trained language models, allowing communication between LLMs\nin natural language has hence become a promising approach to enable coordination among multiple\nLLM agents (Li et al., 2023). Recent works have demonstrated that such conversations enable inte-\ngration of cross-domain knowledge (Sukhbaatar et al., 2024), modular composition of abilities in a\ncomplementary manner (Wu et al., 2023), and improved task performance via splitting into subtasks\n(Prasad et al., 2023). Most notable is multiagent debate introduced by Du et al. (2023), where LLMs\nprovide initial responses and then make refinements by iteratively considering inputs from peers.\nWhile such methods have been shown to improve performance on various tasks over vanilla and\nmajority-vote (Wang et al., 2023) style prompting, these experiments have only focused on large\nmodels (GPT-3.5/4, LLaMA2-70B and up), leaving the efficacy of debate on smaller, open-source\nmodels underexplored; our study addresses this gap by reimplementing Du et al. (2023) in exper-\niments with smaller-scale (1 \u2013 70B) models. More crucially, debate and similar natural language"}, {"title": "Activation engineering", "content": "Activation engineering involves editing an LLM's intermediate layer rep-\nresentations during a forward pass to create desired changes to output text (Li et al., 2024; Turner\net al., 2023). Past work has explored extracting latent steering vectors from a frozen LLM to control\nquality and content of completions (Subramani et al., 2022), as well as using \u201cdirection\u201d vectors\n(computed as the difference in activations between two prompts) that enable inference-time control\nover high-level properties of generations (Li et al., 2024; Turner et al., 2023). This work involves\nactivation editing that is similar to such prior works at a high level, though for the purpose of com-\nmunication between LLM agents."}, {"title": "Model composition and grafting", "content": "Composing expert models has been a recurring strategy to im-\nprove large models, with different methods imposing different restrictions on the types of base LLMs\nthat can be combined. Mixture of Experts (Shazeer et al., 2017) requires that all experts are trained\nsimultaneously using the same data; Branch-Train-Mix (Sukhbaatar et al., 2024) trains a single base\nLM multiple times on different datasets, then learns a router on outputs. Crucially, these methods do\nnot work when neither model can do the task at hand well (i.e., they solve the problem of choosing\nwhich of several outputs is best, not that of generating a high-quality output by recombining the\ndisparate abilities of the various base LMs).\n\nModel grafting, in contrast, seeks to merge different models immediately prior to or at inference-\ntime. Past works have explored this at the parameter level (e.g., task vector averaging as in Ilharco\net al. (2023), which requires that the base models be well aligned), probability distribution / token\nlevel as in Shen et al. (2024) (which imposes few restrictions on the relationship between the base\nmodels, but by virtue of being token-based can result in cascading errors during decoding), and\nactivation level (e.g., CALM (Bansal et al., 2024) which learns an attention layer on top of two\nmodels' intermediate layer activations and thus enables broader integration of model abilities than\ntoken-level methods, but requires re-tuning of the attention mechanism for every model pair). In\nthis work, we seek to unify CALM and other activation-level grafting techniques under a single\nframework, parameterized by the function f used to combine activations; crucially, we explore\nsimple forms of f (e.g., sum, mean) that unlike Bansal et al. (2024)-require zero additional task-\nspecific parameters and data, and are far more compute-efficient."}, {"title": "3 COMMUNICATING ACTIVATIONS BETWEEN LANGUAGE MODELS", "content": "We propose a simple yet effective technique whereby language models communicate via activa-\ntions. We detail our approach in Section 3.1; provide analytical models of the compute saved over\nnatural language communication in Section 3.2; and discuss the intuition behind this approach in\nSection 3.3."}, {"title": "3.1 METHOD", "content": "Consider two language models, A and B, and some setting in which B must perform a task where\nit would benefit from knowledge given to A as a prompt/encoded in A's weights (example settings\nin Section 4.1/Section 4.2 respectively). We propose incorporating information from A's post-layer\nk activation $h_{A,k}$ into B's post-layer j activation $h_{B,j}$ (and vice versa, though for simplicity we\nhenceforth only discuss the first direction) (Figure 1, left).\n\nMore formally, suppose A and B (which have model dimensions $d_A$ and $d_B$ respectively) are given\nprompts $X_A$ and $X_B$ respectively, where $x_A$ is of length $t_A$ tokens and $x_B$ is of length $t_B$ tokens."}, {"title": "3.2 COMPUTE ANALYSIS", "content": "To understand the significance of activation communication, we must formally quantify the compute\nthis procedure saves over natural language communication. For simplicity suppose the following\n(similar calculations can be made for the cases where A and B have differing model architectures\nand/or are given different prompts):\n\n\u2022 A and B both have L layers (each with H attention heads, key size K, and feedforward\nsize F), dimension D, and vocab size V\n\u2022 A and B are both given a prompt of P tokens\n\u2022 A can send B a single M-token message\n\u2022 B must produce an output of T tokens, given its prompt and A's message\n\nTraditional methods require M forward passes of A given a P-length input, plus T forward passes\nof B given a (P + M)-length input. Following Hoffmann et al. (2022), this requires\n\n$M(4PVD + L(8PDKH +4P^2KH + 3HP^2 + 4PDF)) + T(4(P + M)VD$\n$+ L(8(P + M)DKH + 4(P + M)^2KH + 3H(P + M)^2 + 4(P + M)DF))$  (1)\n\nFLOPs. In contrast, at inference time, our method requires only 1 partial (up till the kth layer)\nforward pass of A given a P-length input, T forward passes of B given a P-length input, and the\nactivation replacement procedure. This requires\n\n$2PVD + k(8PDKH +4P^2KH + 3HP^2 + 4PDF) + T(4PVD$\n$+ L(8PDKH +4P^2KH + 3HP^2 + 4PDF)) + F(D)$   (2)\n\nFLOPs, where $F(D) = O(D)$ for non-learned f and $O(D^2)$ when f is the mapping matrix.\n\nIn all practical cases, (2) is substantially lower than (1)."}, {"title": "3.3 WHY SHOULD THIS WORK?", "content": "Recall that Pham et al. (2024) propose CIPHER\u2014communicating the average tokenizer embedding\n(weighted by the LLM's next-token probabilities) between models. We build upon the intuition be-\nhind CIPHER, which goes as follows: the token sampling process during decoding risks substantial\ninformation loss from the model's output logits, and communicating a model's weighted-average\ntokenizer embedding essentially entails communicating both that model's final answer and its belief\nin that answer (over the entire vocabulary).\n\nCommunicating activations, then, can be thought of as communicating a strict superset of {next-\ntoken prediction, belief over entire vocabulary}, as activations of late-enough layers essentially en-\ncode the model's entire knowledge about the provided context as well as its predicted completion\nand confidence in that completion (see Figures 1 and 7 in Hewitt & Manning (2019) and Hernandez\net al. (2024), respectively, which show that linear probes tasked with predicting certain output char-\nacteristics from a Transformer's intermediate layer embeddings of its input work poorly for early\nlayers, extremely well after around the halfway point of computation, but then probe accuracy drops\ncloser to the final layers). Indeed, these curves of probe accuracy by layer indicate that the final lay-\ners and LM head \"throw away\" information not useful for next-token prediction that very well could"}, {"title": "4 EXPERIMENTS", "content": "We test our method on two distinct experimental setups: multi-player coordination games (Sec-\ntion 4.1) and reasoning benchmarks (Section 4.2). Qualitative results are available in Appendix A."}, {"title": "4.1 MULTI-PLAYER COORDINATION GAMES", "content": "Drawing from existing literature on multi-agent communication, we design two Lewis signaling\ngames (Lewis, 2008; Lazaridou et al., 2016) to test the efficacy of activation communication (exam-\nple prompts and answers in Table 1):\n\n1. Countries, where A is given as input a string of the format \u201c[PERSON] is at the\n[LANDMARK]\u201d and B is asked \"Which country is [PERSON] located in?\"\n\n2. Tip Sheets (inspired by Lewis et al. (2017)), where A is given a simulated \"tip sheet\" and\nB is asked to make an informed investment decision in accordance with the information in\nthe tip sheet.\n\nWe synthetically generate 100 (Countries) and 70 (Tip Sheets) different prompts and answers of the\nsame format as the samples in Table 1, and report the proportion out of those samples that B responds\nwith an exact string match to the ground truth answer. As baselines, we consider a \u201csilent\u201d (X) setup,\nwhere the agents are not allowed to communicate; a \"single-agent skyline,\" where a single LLM is\ngiven the concatenation of A and B's prompts; and traditional natural language communication,\nwhere A is asked to output a message that is then given to B along with B. All decoding is done\ngreedily."}, {"title": "4.2 REASONING BENCHMARKS", "content": "Next, we test our methods on a variety of reasoning benchmarks, spanning several real-world tasks\nand domains.\n\nBaselines We benchmark activation communication against the following two baselines:\n\n\u2022 Single Model: A single LLM responds to the prompt in natural language.\n\n\u2022 Natural Language Debate (NLD) (Du et al., 2023): Each LLM provides an initial re-\nsponse to the given prompt. Then, for each of r 1 subsequent rounds, each LLM is\nprompted to refine its previous response given the other agents' responses as input. Note\nthat NLD is the most direct baseline for our approach, as it is a state-of-the-art natural\nlanguage communication protocol. We fix r = 2 in our experiments.\n\nNote that we do not compare to Pham et al. (2024), as they communicate the input (tokenizer)\nembeddings rather than activations/output embeddings between models, and hence require a shared\ntokenizer between agents which prevents applicability to our experimental setup.\n\nTo determine the values of k and j for activation communication (AC), we compute the accuracy\non Countries and Tip Sheets for every pair $(k, j) \\in {1, ..., 30}^2$. Based on these results (shown in\nFigure 2) as well as Table 2, we fix k j = 26 and f = replace for the following experiments.\n\nAcross all experiment configurations, we fix the decoding strategy to nucleus sampling with p = 0.9.\n\nModels We conduct most of our experiments using LLaMA-3.2-3B and LLaMA-3.1-8B as the two\nagents. Additionally, to test our approach's robustness and generalizability, we conduct experiments\nwith models belonging to various other suites within the LLaMA family and of several different sizes.\n\nNote that for these experiments, we restrict the setting to communication between different models\n(rather than multiple instances of the same model in Section 4.1), since the same model would have\nidentical activations for the same prompts, meaning no information would be communicated in the\ngrafting process. We argue that the multiple-model setting is realistic (perhaps more so than the\nsetting of multiple instances of the same model), as recent advances in LLM development have led\nto the release of models with specialized abilities (Singhal et al., 2023) and of different sizes (Dubey\net al., 2024) that merit complementary usage. Our work thus answers the question: How can we get\nthe best performance by leveraging multiple models of distinct capabilities and sizes, relative to the\nadded inference-time compute over a single forward pass through any single model?\n\nDatasets We evaluate our technique on seven reasoning datasets that span various real-world tasks\nand domains: (i) Biographies (Du et al., 2023), which asks the LLM to generate a factual biography\nof a famous computer scientist; (ii) GSM8k (Cobbe et al., 2021), a variety of grade school math\nproblems created by human problem writers; and (iii) 5 datasets randomly drawn from MMLU\n(Hendrycks et al., 2021): High School Psychology (from the Social Sciences category), Formal\nLogic (from the Humanities category), College Biology (from the STEM category), Professional\nLaw (from the Humanities Category), and Public Relations (from the Social Sciences category).\nWe evaluate on a randomly-sampled size-100 subset of each dataset.\n\nIn experiments involving the mapping matrix W, we instantiate $W \\in R^{4096\\times3072}$ using Xavier\ninitialization and train for 10 epochs on a dataset of 3072 sentences randomly drawn from the\nColossal Clean Crawled Corpus (C4) (Dodge et al., 2021). We use batch size 32 and the Adam\noptimizer with learning rate 0.001.\n\nMetrics We measure the accuracy of the final response for the single models and AC. For NLD,\nwe measure the accuracy of the majority-held final-round answer across agents when the answer is\nautomatically verifiable (numeric in GSM8k, multiple choice for the MMLU datasets) or the average\nfinal-round answer across agents otherwise (Biographies)."}, {"title": "5 CONCLUSION", "content": "We present a simple approach to enable effective and computationally efficient communication be-\ntween language models by injecting information from the activations of one model into the acti-\nvations of another during the forward pass. Salient features of this approach include: (i) Scales up\nLLMs on new tasks by leveraging existing, frozen LLMs along with zero additional task-specific pa-\nrameters and data, (ii) Applies to diverse domains and settings, and (iii) Saves a substantial amount\nof compute.\n\nThere are some limitations to this method. First, when not using the learned model-specific mapping\ndiscussed in Section 3.1, our method requires both models to have aligned embedding spaces, such\nthat the activation of one model roughly retains its meaning in the other's activation space (note\nthat unlike past works such as Pham et al. (2024) we do not require shared tokenizers or aligned\nvocabularies, only aligned embeddings). While less restrictive than past works (Pham et al., 2024),\nthis assumption is somewhat limiting, but can be relaxed when we let f be the learned model-specific\nmapping; and in practice we find that even amongst different models in the LLaMA family, no such\nmapping is required for state-of-the-art results.\n\nSecond, this method requires access to embeddings and will not work with black-box API access;\nhowever exploring API-only approaches is highly limiting, and recent releases of powerful open-\nsource models (Dubey et al., 2024) merit the development of embedding-based techniques.\n\nThird, while a concern might be the limited interpretability of communicating activations as opposed\nto natural language, we note the following. First, there is a fundamental tradeoff between inter-\npretability and information preservation (as activations, by virtue of being much higher-dimensional\nthan the space of natural language, allow proportionally higher-entropy communication) (Pham\net al., 2024), which merits discussion beyond the scope of this work. But second, we actually\nposit that our method suggests a new avenue towards interpreting LM activations: \u201ctranslating\" ac-\ntivations based on the beliefs they induce as messages in listening agents, similar to the method put\nforward in Andreas et al. (2018). We recognize this as a promising avenue for future research.\n\nAdditional directions of future work include using AC to allow large LMs to leverage small, tunable\nLMs as \"knowledge bases\" during decoding (Lee et al., 2024), as in collaborative decoding (Shen\net al., 2024) setups; and testing our approach on more complex coordination games (e.g., Lewis-style\nnegotiation games (Lewis et al., 2017), Diplomacy)."}]}