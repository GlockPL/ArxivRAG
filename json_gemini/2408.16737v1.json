{"title": "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling", "authors": ["Hritik Bansal", "Arian Hosseini", "Rishabh Agarwal", "Vinh Q. Tran", "Mehran Kazemi"], "abstract": "Training on high-quality synthetic data from strong language models (LMs) is a common strategy to improve the reasoning performance of LMs. In this work, we revisit whether this strategy is compute-optimal under a fixed inference budget (e.g., FLOPs). To do so, we investigate the trade-offs between generating synthetic data using a stronger but more expensive (SE) model versus a weaker but cheaper (WC) model. We evaluate the generated data across three key metrics: coverage, diversity, and false positive rate, and show that the data from WC models may have higher coverage and diversity, but also exhibit higher false positive rates. We then finetune LMs on data from SE and WC models in different settings: knowledge distillation, self-improvement, and a novel weak-to-strong improvement setup where a weaker LM teaches reasoning to a stronger LM. Our findings reveal that models finetuned on WC-generated data consistently outperform those trained on SE-generated data across multiple benchmarks and multiple choices of WC and SE models. These results challenge the prevailing practice of relying on SE models for synthetic data generation, suggesting that WC may be the compute-optimal approach for training advanced LM reasoners.", "sections": [{"title": "1. Introduction", "content": "Language models (LMs) have demonstrated impressive capabilities in reasoning tasks, but their success heavily relies on being trained on vast amounts of (problem, solution) pairs. Collecting this data from humans is a costly and time-consuming process. Recent studies have demonstrated the"}, {"title": "2. Preliminaries", "content": "Let \\(D = \\{q_i, a_i\\}_{i=1}^n\\) be a training dataset of size \\(n\\) with reasoning questions \\(q_i\\) and final answers (aka labels) \\(a_i\\). A successful approach to leverage such data to improve models for reasoning is as follows. We sample multiple solutions for each \\(q_i\\) at a non-zero temperature and create the synthetic data \\(D_G = \\{q_i, \\{(f_{ij}, \\hat{a}_{ij})\\}_{j=1}^k\\}_{i=1}^n\\), where \\(k\\) is the number of samples, \\(f_{ij}\\) is the \\(j\\)-th reasoning chain (i.e. solution) generated by the model for \\(q_i\\), and \\(\\hat{a}_{ij}\\) is the model's final answer for \\(q_i\\) in the \\(j\\)-th sample. Then, we filter the incorrect solutions by comparing \\(\\hat{a}_{ij}\\) to \\(a_i\\) and removing the solutions whose final answer do not match that of the gold answer\u00b9. Finally, we supervise finetune a model on the remaining data \\(D_G\\) to maximize \\(J(\\theta) = \\mathbb{E}_{(q,r,a)\\sim \\tilde{D}_G} [log(p_{\\theta}(r, a|q))]\\), i.e. the probability of generating the reasoning \\(r\\) and final answer \\(a\\) given the question \\(q\\). This approach was first proposed in (Zelikman et al., 2022) and was then extended in multiple works including (Singh et al., 2023; Zelikman et al., 2024).\nFor a dataset \\(D_G\\), we compute coverage@\\(k\\) (aka pass@\\(k\\)) (Chen et al., 2021) as \\(\\mathbb{E}_{D_G} [1 - (1 - c/M)^k]\\) where \\(c\\) is the number of solutions, out of \\(M\\), with correct answers and \\(\\mathbb{E}_{D_G} [.]\\) denotes the expectation over the problems and solutions in the generated dataset. Conceptually, coverage@\\(k\\) measures the fraction of unique questions that have at least one correct solution, assuming that we sample \\(k\\) solutions per question from the model. We also define diversity@\\(k\\) as the average number of unique correct solutions we obtain per question when we sample \\(k\\) solutions per question. Finally, we define false positive rate (FPR) as the percentage of solutions in \\(D_G\\) where the reasoning is incorrect, despite the final answer being correct.\nDifferent choices of the LM to sample solutions from and the LM to finetune lead to different setups. Knowledge Distillation (Hinton et al., 2015) corresponds to training a student LM on the synthetic data sampled from a stronger and larger LM. Self-Improvement (Huang et al., 2022) corresponds to training an LM on samples generated from itself."}, {"title": "3. Compute-Matched Sampling and Training", "content": "To generate a dataset \\(D_G\\) with synthetic solutions from \\(D\\), one can leverage different models for generating solutions. Specifically, at a fixed sampling budget (FLOPs), one can generate more samples from a weaker but cheaper (WC) model or fewer samples from a stronger but more expensive (SE) model. Given a WC model with \\(P_{WC}\\) parameters and SE with \\(P_{SE}\\) parameters, we compute the sampling ratio at a fix budget for the two models, focusing on decoder-only transformer models (Vaswani, 2017). Following (Kaplan et al., 2020), we note that the FLOPs per inference token is \\(2P\\), for a model with \\(P\\) parameters. As a result, the FLOPs for \\(T\\) inference tokens is \\(2PT\\). Further, we assume that generating each solution requires an average of \\(W\\) inference tokens for both models\u00b2. Let \\(S_{WC}\\) and \\(S_{SE}\\) represent the number of samples we generate per question for the two models. The total cost of generating samples for the dataset \\(D\\) will then be \\(Cost_{WC} = n \\times S_{WC} \\times W \\times (2P_{WC})\\) and \\(Cost_{SE} = n \\times S_{SE} \\times W \\times (2P_{SE})\\) for the cheap and expensive models, respectively. At a fixed sampling budget, we have:\n\\[n \\times S_{WC} \\times W \\times (2P_{WC}) = n \\times S_{SE} \\times W \\times (2P_{SE}) \\implies \\frac{S_{WC}}{S_{SE}} = \\frac{P_{SE}}{P_{WC}}\\]\nEquation 1 indicates that at a fixed sampling budget, for each question we can generate \\(P_{SE}/P_{WC}\\)\nmore samples from WC; the ratio scales linearly with the model parameters ratio\u00b3. Sampling more solutions from WC may increase the likelihood of correctly solving a larger subset of the problems (high coverage) and obtaining more correct solutions per question (high diversity).\nGiven a fixed budget, we can either generate fewer samples from a SE model or more samples from a WC model, and then finetune models for a fixed number of steps on the data from each of these models to measure and compare the utility of the data from each model. Specifically, we generate \\(P_{SE}/P_{WC}\\) more samples from the WC model compared to the SE model. We consider three finetuning setups that consists of diverse finetuning paradigms. The paradigms include the widely used knowledge distillation, the emerging framework of self-improvement, and a novel weak-to-strong improvement paradigm we introduce in this work. We define weak-to-strong improvement (W2S-I) as enhancing the reasoning capabilities of a strong model using samples generated from a weaker model. The three setups are as follows:\n\u2022 Student-LM finetuning: Conventionally, the supervised finetuning data for training student LM is acquired from SE models to ensure high-quality (Teknium, 2023). However, we aim to understand whether WC models can replace SE models for distillation at the fixed sampling budget. To do so, we finetune a student LM separate from the WC and SE models on the WC and SE data, which corresponds to distillation in both the cases.\n\u2022 WC-LM finetuning: Prior work (Singh et al., 2023) has shown that finetuning a WC model through self-generated data lags behind distillation from SE data. However, their setup spends a higher sampling budget (FLOPs) on collecting data from the SE model than collecting SI data from the WC model. In this work, we revisit this finetuning setup under the fixed sampling budget and finetune the WC model on the WC and SE data at a fixed budget for both. Note that training the WC model on its own data corresponds to self-improvement whereas training WC on the data from SE corresponds to distillation. Hence, this setup compares self-improvement on WC data with distillation from SE data.\n\u2022 SE-LM finetuning: It is commonly believed that to improve a SE model, we either need synthetic data from the SE model itself or from an even stronger (and perhaps more expensive) model than SE. Here, we test an alternative approach to understand whether the synthetic data from the WC model can improve the SE model. To this end, we finetune the SE model on the WC and SE data. Training SE on data from WC corresponds to W2S-I and training SE on data from SE corresponds to self-improvement. Overall, this setup compares W2S-I by WC data with self-improvement by SE data.\nA summary of the three setups and the finetuning paradigms that each case corresponds to can be found in Table 1."}, {"title": "4. Experimental Setup", "content": "Datasets: We utilize MATH (Hendrycks et al., 2021) and GSM-8K (Cobbe et al., 2021) as the reasoning datasets due to their wide adoption for mathematical problem solving. Specifically, MATH consists of competition level problems with various levels of difficulty (Level 1-5), and GSM-8K comprises of grade school level math problems. Each dataset contains 7500 math problems in their training split. We evaluate the models on 500 problems from the MATH test split (Lightman et al., 2023) and 1319 problems from the GSM-8K test split. Further, we use 500 problems from the MATH test split and 500 problems from GSM-8K as the validation dataset. We also use the Functional MATH dataset (Srivastava et al., 2024) for a transfer study. Further, we present the results for a coding task in Appendix A.\nData Generation: We use Gemma2 models for synthetic data generation, with pretrained Gemma2-9B and Gemma2-27B acting as the WC and SE models respectively. We generate the solutions for the problems in the MATH using a 4-shot prompt and for GSM-8K using an 8-shot prompt. Since the 9B model is roughly 3 times smaller than the 27B model, at a fixed sampling compute budget we can sample 3x more sample solutions per problem for Gemma2-9B. For our experiments, we consider two sampling budgets: a low budget, where we generate 1 and 3 candidate solutions per problem from Gemma2-27B and Gemma2-9B, respectively, and a high budget, where we generate 10 and 30 candidate solutions per problem. Further, we study the transfer of the reasoning capabilities for the models trained on MATH at the high sampling budget on the Functional MATH dataset.\nModel Finetuning: We summarize the details for our finetuning setups in the Table 1. In the Student-LM finetuning setup, we finetune the Gemma-7B model (Team et al., 2024a) on data from Gemma2-9B (WC) and Gemma2-27B (SE). In addition, we use Gemma2-9B and Gemma2-27B for the WC-LM and SE-LM finetuning setups, respectively. Further, we train the LMs across different setups with the human-written solutions as a ground-truth baseline. We provide the finetuning details in Appendix F.\nSynthetic Data Evaluation: To assess the quality of the synthetic data from the SE and WC models, we measure the false positive rate, as well as coverage and diversity at a fixed cost. From Equation 1, we know that sampling one solution from SE takes the same FLOPs as sampling \\(P_{SE}/P_{WC}\\) solutions from WC. Therefore, we compare coverage@\\(k\\) for SE to coverage@\\((P_{SE}k)\\) for WC to allow a similar budget to both models. Specifically, we compare coverage@\\(k\\) and coverage@\\(3k\\) for our SE and WC models. Similarly we compare diversity@\\(k\\) and diversity@\\(3k\\) for our SE and WC models. Since FPR cannot be computed automatically, we compute it using two proxies: 1- a human evaluation on a subset of the data, where 50 solutions from each model were selected randomly and rated for reasoning correctness by the authors, and 2- automatic evaluation where we sampled 500 solutions and prompted Gemini-Pro-1.5 (Reid et al., 2024) to rate the correctness of the reasoning paths. To sample solutions, for the MATH dataset we selected uniformly from each diversity level. In our experiments, we find that the FPR estimates are close to each other for the human and automatic evaluation. We provide a few qualitative examples for the false positive instances in Appendix B.\nEvaluating Finetuned Models: We use pass@1 accuracy to evaluate the performance of the finetuned LMs. Specifically, we generate a single solution for the problem (zero-shot) from the test split, using a sampling temperature of 0.0 (greedy decoding) for the fine-tuned LM and measure the percentage of problems that where the final answer matches the golden final answer. We also report maj@\\(k\\) (\\(k\\) = 1, 4, 8, 16) for part of our experiments, where we generate \\(k\\) solutions per problem at a sampling temperature of 0.7 and select the final answer that appears most among the \\(k\\) samples."}, {"title": "5. Experiments and Results", "content": "We compare data from WC and SE models along several axes. First, we analyze the data along various quality metrics (\u00a75.1). Subsequently, we present the supervised finetuning results for the different setups (\u00a75.2). Finally, we perform ablation studies to study the impact of dataset size, sampling strategy, and the role of quality dimensions in the model performance (\u00a75.3)."}, {"title": "5.1. Synthetic Data Analysis", "content": "Coverage: Here, we aim to understand the pros and cons of generating solutions from the WC and SE models at a fixed sampling budget. We present the coverage, diversity, and FPR for the MATH at the low and high sampling budgets in Figure 3. The results for GSM-8K are presented in the Appendix \u2013 Figure 15. We find that in terms of coverage, the data from Gemma2-9B (WC) outperforms Gemma2-27B (SE) by 11% and 6% at the low and high sampling budgets, respectively, for the MATH dataset, and 8% and 1% for GSM-8K. This highlights that the higher number of samples for the WC model aids in solving more unique problems for both the reasoning datasets. We provide the coverage trends for diverse sampling budgets in Appendix C. In addition, we observe that the coverage of the WC model increases across various difficulty levels in the MATH dataset for the high sampling budget (see Appendix \u2013 Figure 16). This highlights that synthetic data from the WC model can solve more unique questions at various difficulty levels compare to the SE model, at a fixed sampling budget (Tong et al., 2024). Further, we provide a qualitative example that gets solved by repeated sampling from Gemma2-9B but remains unsolved by Gemma2-27B at the fixed high sampling budget (Table 5).\nDiversity: The diversity for the data from Gemma2-9B is higher than Gemma2-27B by 86% and 125% at the low and high sampling budgets for the MATH dataset, and 134% and 158% at for the GSM-8K dataset. This implies that many unique reasoning chains in the synthetic data from the WC model lead to the correct solutions. We also observe that the absolute diversity scores are lower for MATH compared to GSM-8K at high sampling budget, indicating that models generate fewer correct solutions for the more challenging datasets when using repeated sampling.\nFPR: Since we utilize the final answer correctness for filtering the synthetic data, it does not remove the solutions with incorrect intermediate reasoning steps. Our human evaluations suggest that the FPR for the WC-generated solutions is 7% and 2% higher than SE-generated solutions on the MATH and GSM-8K, respectively. The trends from the automatic evaluation are similar to that of human evaluation. Due to the differences in the difficulty of the problems, we note that the absolute"}, {"title": "5.2. Compute-Optimality Results for Training", "content": "We compare the utility of the synthetic data generated from the Gemma2-9B (WC) and Gemma2-27B (SE) model for the MATH and GSM-8K dataset across the diverse finetuning paradigms in Figure 4 and Figure 5, respectively. In addition, we present the results for training with human-written chain-of-thoughts from the original training sets as a baseline."}, {"title": "Student-LM Finetuning", "content": "We find that the Gemma-7B finetuned with the synthetic data from WC consistently outperforms the one finetuned on data from SC. Specifically, we observe relative gains of 6% and 5.8% at the low and high sampling budgets, respectively, for the MATH dataset and 4.2% and 1.3% for GSM-8K. Contrary to the common belief of stronger models being better for knowledge distillation, our results indicate that finetuning on data from WC is more compute-optimal than data from SE."}, {"title": "WC-LM Finetuning", "content": "We compare the performance of Gemma2-9B finetuned with the WC data (i.e. self-generated data) and SE data (i.e. data from Gemma2-27B). The results for MATH and GSM-8K are reported in Figures 4b and 5b. We observe that the self-generated data (WC data) improves over knowledge distillation from a strong model (SE data), achieving relative gains of 3.8% and 2% at the low and high sampling budgets, respectively, for the MATH dataset, and 1.5% at the low sampling budget for the GSM-8K dataset. However, we find that the WC model finetuned with WC data matches the SE data for the GSM-8K dataset at a high sampling budget. This is mainly due to the lower difficulty of the GSM-8k dataset, where it becomes saturated at higher sampling budgets (see Figure 15a). Interestingly, our empirical findings suggest that training a WC model on synthetic data from its own is more compute-optimal than distillation from a stronger model."}, {"title": "SE-LM finetuning", "content": "We present the results for finetuning Gemma2-27B with the Gemma2-9B generated data and self-generated data. The results for MATH and GSM-8K are reported in Figure 4c and 5c. Surprisingly, we observe that the model finetuned with the WC data outperforms the SE data, achieving relative gains of 5.8% and 4.3% at the low and high sampling budget, respectively, for the MATH dataset and 1.2% and 1.5% for the GSM-8K dataset. This result is even more surprising given that the Gemma2-27B data is expected to be more in-distribution than the Gemma2-9B data. Contrary to the common belief of self-generated data or data from a stronger model being better, our empirical findings show that training a model in a W2S-I setup from a WC data may be more compute-optimal than training it in a self-improvement setup on its own data. This result also establishes a new paradigm for improving frontier models in a compute-efficient way, by generating synthetic data from much smaller models."}, {"title": "Generalization", "content": "Here, we aim to study the transfer capabilities of the models trained with the WC and SE data. Specifically, we evaluate the models finetuned with the synthetic solutions for the MATH datasets at the high sampling budget on the Functional MATH dataset. The results in Figure 6 show that the Gemma-7B finetuned with the WC data consistently outperforms the SE data,"}, {"title": "Takeaway", "content": "Overall, our findings challenge the conventional wisdom that advocates training on samples from the SE model, by showing that training on samples from the WC model may be more compute-optimal across various tasks and setups."}, {"title": "5.3. Ablation Studies", "content": "Impact of Dataset Size: We study whether the benefits of the synthetic data from the WC model hold at different dataset sizes. We repeat our experiments for the MATH dataset at the high budget, but when only having access to 500 training data (selected randomly from the training set). We present the results for the finetuned models in Figure 7. We observe that models trained with the WC data outperform those trained with the SE data, achieving relative gains of 12.93%, 11.4%, and 5.1% for the three paradigms, respectively. This highlights the utility of generating more data from the WC model instead of the SE model in the low-problem regimes at the fixed sampling budget.\nDefault vs Compute-Optimal Sampling from Cheap LMs: We anticipate that the reason why data from SE models has been previously preferred over data from WC is because they have been tested in a setup where an equal number of samples have been generated from the two models (e.g., see (Singh et al., 2023)), as opposed to a compute-matched setup. To verify this, we generated 1 solution per problem (number-matched) from the WC model for the MATH and GSM-8K datasets and trained the models under the three fine-tuning setups on this generated data, after filtering for final answer correctness. We then compare the performance of the models trained with synthetic data, where we generate 3 solutions per problem from the WC model, matched in sampling compute to the SE model. We present the results in Figure 8. We see that the models trained with the number-matched WC data are sub-optimal in comparison to the models trained with the compute-matched WC data, and lead to worse models compared to training with the SE data. This highlights that the future comparisons between synthetic data from weak and strong models should be made in the sampling"}, {"title": "Coverage and Diversity", "content": "We aim to understand the role of coverage and diversity in enhancing the performance of models trained with WC-generated synthetic data. To this end, for the MATH dataset, we consider the original high-sampling (30 solutions per problem) WC dataset as a (high coverage, high diversity) dataset. We then construct a (high coverage, low diversity) version by only selecting one correct solution per question from our samples. This reduces the diversity of the original WC dataset from 11 to 1, while maintaining the coverage. We also create a (low coverage, low diversity) dataset where we generate just one solution per problem from the WC model and filter it for the correctness of the final answer. The coverage of this dataset (27%) is lower than that of the WC dataset with 30 solutions per problem (43%). We train models across the three finetuning setups on these sets and present the re- sults in Figure 9. Our results indicate that across all setups, the high coverage and high diversity data is better than high coverage and low diversity, and high coverage and low diversity is better than low coverage and low diversity. This reveals that both the coverage and diversity play a critical role in training strong reasoners from the smaller LMs."}, {"title": "6. Scaling to state-of-the-art language models", "content": "In the prior experiments, we focused on the synthetic data acquisition from open LMs. Here, we aim to show that data from the weaker SoTA LM can train better reasoners than stronger SoTA LM at a fixed sampling budget. To this end, we scale our method to sampling data from Gemini-1.5-Pro and Gemini-1.5-Flash. As the model sizes are not publicly available, we utilize the ratio between their pricing per output token as a proxy to perform compute-matched sampling. As of August 2024, we note that the price per million output tokens is $10.5 and $0.3 for Gemini-1.5-Pro and Gemini-1.5-Flash, respectively. Hence, we sample 1 and 35 solutions per problem from 1.5-Pro and 1.5-Flash, respectively. We conduct our experiments on the MATH dataset.\nWe perform knowledge distillation on the Gemma-7B, Gemma2-9B, and Gemma2-27B LMs with the synthetic data from the Pro (SE) and Flash (WC) models. We present the results in Figure 10. Interestingly, we find that finetuning with the WC data outperforms the SE data, achieving relative gains of 31.6%, 14.4%, and 10.9% for Gemma-7B, Gemma2-9B, and Gemma2-27B, respectively. This can be attributed to the difference in the coverage of the models at the fixed sampling budget, which is 61.1% and 81% for 1.5-Pro and 1.5-Flash, respectively.\nReducing the price of data sampling. Further, we investigate training the LMs with the WC data that is less expensive than collecting 1 solution per problem from the SE model. Specifically, we create a dataset by sampling 5 solutions per problem from the Flash (WC) model, which is 7x more economical than generating 1 solution from the Pro (SE) model, in terms of the price ($). Upon training the LMs on the 0.15\u00d7 cost data regime (Figure 10), we find that training on this data can also outperform training with SC data, achieving relative gains of 19.1%, 9.8%, and 5.7% for finetuning Gemma-7B, Gemma2-9B, and Gemma2-27B, respectively. This can be attributed to higher coverage of the weaker model (69%), even in the more economical scenario, in comparison to the stronger model (61.1%)."}, {"title": "Takeaway", "content": "We demonstrate that price-matched sampling from weaker SoTA LMs produces superior reasoners compared to finetuning with data from stronger SoTA models."}, {"title": "7. A Future Perspective", "content": "We showed that for the current WC and SE models, training reasoners through sampling from WC models may be more compute-optimal. Here, we aim to discuss the relevance of these results for the future set of WC and SE models. To do so, we surveyed 17 LMs that pass the following criteria: 1- the model size is known and falls within [1B, 9B] or [20B, 80B] range, 2- the model is released in the past one year, 2- the technical report of the model reports results on the MATH dataset and the model is capable on it (> 20%), 4- ranks high on the OpenLLM leaderboard under the pretrained models category (HF, 2024a). This resulted in models from seven families including Gemma-2 (Team"}, {"title": "8. Related Work", "content": "LMs for reasoning. The ability to solve reasoning tasks has been a long standing goal of artificial intelligence (Achiam et al., 2023; AI, 2024; Anthropic, 2024; Dubey et al., 2024; Reid et al., 2024; Team, 2024). In this regard, LMs trained on the internet-scale data have achieved great success for math, code, and other reasoning tasks (Azerbayev et al., 2023; Kazemi et al., 2024; Lewkowycz et al., 2022). There have been several works that aim to enhance the reasoning capabilities of the LMs either via prompting (Kazemi et al., 2022; Kojima et al., 2022; Wang et al., 2022; Zheng et al., 2023) or finetuning (Yu et al., 2023; Yue et al., 2023). In this work, we focus on finetuning the LMs with task-specific datasets to build strong reasoners. Specifically, our method closely aligns with the widely adopted STaR (Zelikman et al., 2022) where the synthetic data from the LMs are used to elicit strong reasoning capabilities."}, {"title": "Finetuning LMs", "content": "Within the finetuning paradigm, there have been several works that improve reasoning with synthetic data. Broadly, these works focus on knowledge distillation from a strong but expensive LM (Wu et al., 2024; Yue et al., 2023) or self-improvement (Gulcehre et al., 2023; Singh et al., 2023). While it is common to filter the synthetic data for the final answer correctness (akin to Zelikman et al. (2022)), there are several works that aim to build task-specific verifiers to train strong reasoners (Hosseini et al., 2024; Lightman et al., 2023; Wu et al., 2024; Yuan et al., 2024). In this work, we explore the utility of the synthetic data from the weak but cheap LMs for training strong reasoners via knowledge distillation as well as self-improvement. However, we do not explore using model-based verifiers with the synthetic data for enhanced reasoning, and leave it as a future work.\nOur weak-to-strong improvement paradigm, where a strong model is trained with the generations from the weak model, is related to several prior work (Bowman et al., 2022; Burns et al., 2023; Yang et al., 2024b) which study the ability of a strong LM to learn from the data generated by a weaker LM. However, the aim of these works is to recover the full capabilities of the strong model from weaker data, whereas we aim to enhance the strong model capabilities further. Additionally, our work studies compute-optimal sampling from weak and strong models, which is absent in previous work."}, {"title": "Large and small LMs", "content": "While training large LMs has led to significant advancements across various tasks, there has recently been a growing interest in developing capable small LMs (HF, 2024b; Javaheripi et al., 2023). Specifically, a capable small LM is faster to run, and easier to serve to millions of users on the edge devices (Gunter et al., 2024). As a result, several recent works aim to understand the utility of the weak but cheaper LMs in comparison to the strong but expensive LMs for reasoning. Specifically, Brown et al. (2024); Snell et al. (2024); Song et al. (2024) show that the solve rate of the small LMs can increase significantly with repeated sampling. In addition, Hassid et al. (2024) demonstrate that repeated generations from smaller LMs can outperform the data generated by larger LMs at a fixed sampling computational budget during inference for coding tasks. In this work, we go beyond these works and show the utility of the synthetic data from the small LMs for training strong reasoners across a diverse set of supervised finetuning setups."}, {"title": "9. Conclusion", "content": "In this work, we provide a framework for compute-optimal sampling from weak but cheap LM for reasoning tasks. Specifically, we show that at a fixed sampling compute budget, repeated sampling from a smaller model can achieve higher coverage and diversity than from a strong but more expensive model. Furthermore, our empirical findings highlight that fine-tuning LMs with data from the small LM can consistently outperform data from the large LM under the same compute budget. Our results can serve as a foundation for training LM reasoners, especially as the performance gap between small and large LMs continues to narrow over time."}, {"title": "A. Extending our results to coding tasks", "content": "Here, we aim to understand the utility of the synthetic data from the Gemma2-9B (WC) and Gemma2-27B (SE) model on coding tasks. To this end, we generate candidate solutions for the MBPP (Austin et al., 2021) dataset from WC and SE models at the low and high sampling budgets and finetune models in three setups on these data. We use the santizied version of MBPP5 containing 427 problems overall; we used 3 problems for fewshot prompting (used for sampling from the models), 324 problems for synthetic training data generation, and 100 problems for validation. The candidate solutions are filtered by the unit tests that accompany each instance of the dataset. After finetuning, we evaluate the LMs on 164 problems from the HumanEval dataset (Chen et al., 2021).\nWe compare the coverage and diversity of the synthetic datasets in Figure 12 and observe that the coverage of the WC model is higher than SE at low data regime while it is similar to SE in the high sampling budget regime. In addition, we find that the diversity of the WC model is more than"}, {"title": "B. Qualitative Examples", "content": "We present a few qualitative examples for model-generated solutions that lead to the correct final answer with incorrect (or correct) reasoning for the MATH dataset. We provide two bad reasoning examples in Table 2 and Table 3. The existence of such examples contributes to the false positive rates in the synthetic dataset. In addition, we provide a good reasoning example in Table 4."}, {"title": "C. Coverage Trends", "content": "We present the coverage trends for diverse sampling budgets on the MATH and GSM-8K dataset in Figure 14a and Figure 14b, respectively."}, {"title": "D. Data analysis: GSM-8K", "content": "We presented the coverage, diversity, and false positive rate of the synthetic data from Gemma2-27B and Gemma2-9B on the MATH dataset in the main text. In Figure 15, we present these metrics for the GSM-8K dataset."}, {"title": "E. Solving problems across levels for MATH", "content": "We present the effect of repeated sampling from the weak but cheaper LM and stronger but expensive LM on solving the problems across different levels for the MATH dataset in Figure 16."}, {"title": "F. Finetuning Details", "content": "We generated the candidate solutions in the synthetic dataset using TopK (K= 3) strategy with a temperature of 0.7. We finetuned the Gemma2-9B and Gemma2-27B models with a batch size of 32 for 600 and 6000 steps under the low and high sampling budget, respectively. During the fine-tuning process, we save 10 equally-spaced checkpoints and choose the one that yields the highest validation accuracy. In addition, we train"}]}