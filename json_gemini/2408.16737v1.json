{"title": "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling", "authors": ["Hritik Bansal", "Arian Hosseini", "Rishabh Agarwal", "Vinh Q. Tran", "Mehran Kazemi"], "abstract": "Training on high-quality synthetic data from strong language models (LMs) is a common strategy to improve the reasoning performance of LMs. In this work, we revisit whether this strategy is compute-optimal under a fixed inference budget (e.g., FLOPs). To do so, we investigate the trade-offs between generating synthetic data using a stronger but more expensive (SE) model versus a weaker but cheaper (WC) model. We evaluate the generated data across three key metrics: coverage, diversity, and false positive rate, and show that the data from WC models may have higher coverage and diversity, but also exhibit higher false positive rates. We then finetune LMs on data from SE and WC models in different settings: knowledge distillation, self-improvement, and a novel weak-to-strong improvement setup where a weaker LM teaches reasoning to a stronger LM. Our findings reveal that models finetuned on WC-generated data consistently outperform those trained on SE-generated data across multiple benchmarks and multiple choices of WC and SE models. These results challenge the prevailing practice of relying on SE models for synthetic data generation, suggesting that WC may be the compute-optimal approach for training advanced LM reasoners.", "sections": [{"title": "1. Introduction", "content": "Language models (LMs) have demonstrated impressive capabilities in reasoning tasks, but their success heavily relies on being trained on vast amounts of (problem, solution) pairs. Collecting this data from humans is a costly and time-consuming process. Recent studies have demonstrated the"}, {"title": "2. Preliminaries", "content": "Let $D = \\{q_i, a_i\\}_{i=1}^n$ be a training dataset of size n with reasoning questions $q_i$ and final answers (aka labels) $a_i$. A successful approach to leverage such data to improve models for reasoning is as follows. We sample multiple solutions for each $q_i$ at a non-zero temperature and create the synthetic data $D_G = \\{q_i, \\{(f_{ij}, \\hat{a}_{ij})\\}_{j=1}^{k} \\}_{i=1}^{n}$ where k is the number of samples, $f_{ij}$ is the j-th reasoning chain (i.e. solution) generated by the model for $q_i$, and $\\hat{a}_{ij}$ is the model's final answer for $q_i$ in the j-th sample. Then, we filter the incorrect solutions by comparing $\\hat{a}_{ij}$ to $a_i$ and removing the solutions whose final answer do not match that of the gold answer\u00b9. Finally, we supervise finetune a model on the remaining data $D_G$ to maximize $J(\\theta) = E_{(q,r,a) \\sim \\tilde{D_G}} [log(p_{\\theta}(r, a|q))]$, i.e. the probability of generating the reasoning r and final answer a given the question q. This approach was first proposed in (Zelikman et al., 2022) and was then extended in multiple works including (Singh et al., 2023; Zelikman et al., 2024).\nFor a dataset $D_G$, we compute coverage@k (aka pass@k) (Chen et al., 2021) as $E_{D_G} [1 - (\\frac{\\binom{c}{1}}{\\binom{M}{1}})]$ where c is the number of solutions, out of M, with correct answers and $E_{D_G} [.]$ denotes the expectation over the problems and solutions in the generated dataset. Conceptually, coverage@k measures the fraction of unique questions that have at least one correct solution, assuming that we sample k solutions per question from the model. We also define diversity@k as the average number of unique correct solutions we obtain per question when we sample k solutions per question. Finally, we define false positive rate (FPR) as the percentage of solutions in $D_G$ where the reasoning is incorrect, despite the final answer being correct.\nDifferent choices of the LM to sample solutions from and the LM to finetune lead to different setups. Knowledge Distillation (Hinton et al., 2015) corresponds to training a student LM on the synthetic data sampled from a stronger and larger LM. Self-Improvement (Huang et al., 2022) corresponds to training an LM on samples generated from itself."}, {"title": "3. Compute-Matched Sampling and Training", "content": "To generate a dataset $D_G$ with synthetic solutions from D, one can leverage different models for generating solutions. Specifically, at a fixed sampling budget (FLOPs), one can generate more samples from a weaker but cheaper (WC) model or fewer samples from a stronger but more expensive (SE) model. Given a WC model with $P_{WC}$ parameters and SE with $P_{SE}$ parameters, we compute the sampling ratio at a fix budget for the two models, focusing on decoder-only transformer models (Vaswani, 2017). Following (Kaplan et al., 2020), we note that the FLOPs per inference token is 2P, for a model with P parameters. As a result, the FLOPs for T inference tokens is 2PT. Further, we assume that generating each solution requires an average of W inference tokens for both models\u00b2. Let $S_{WC}$ and $S_{SE}$ represent the number of samples we generate per question for the two models. The total cost of generating samples for the dataset D will then be $Cost_{WC} = n \\times S_{WC} \\times W \\times (2P_{WC})$ and $Cost_{SE} = n \\times S_{SE} \\times W \\times (2P_{SE})$ for the cheap and expensive models, respectively. At a fixed sampling budget, we have:\nn \u00d7 $S_{WC}$ \u00d7 W \u00d7 (2$P_{WC}$) = n \u00d7 $S_{SE}$ \u00d7 W \u00d7 (2$P_{SE}$) $=> \\frac{S_{WC}}{S_{SE}} = \\frac{P_{SE}}{P_{WC}}$\nEquation 1 indicates that at a fixed sampling budget, for each question we can generate $P_{SE}/P_{WC}$"}, {"title": "5. Experiments and Results", "content": "We compare data from WC and SE models along several axes. First, we analyze the data along various quality metrics (\u00a75.1). Subsequently, we present the supervised finetuning results for the different setups (\u00a75.2). Finally, we perform ablation studies to study the impact of dataset size, sampling strategy, and the role of quality dimensions in the model performance (\u00a75.3)."}, {"title": "5.1. Synthetic Data Analysis", "content": "Coverage: Here, we aim to understand the pros and cons of generating solutions from the WC and SE models at a fixed sampling budget. We present the coverage, diversity, and FPR for the MATH at the low and high sampling budgets in Figure 3. The results for GSM-8K are presented in the Appendix \u2013 Figure 15. We find that in terms of coverage, the data from Gemma2-9B (WC) outperforms Gemma2-27B (SE) by 11% and 6% at the low and high sampling budgets, respectively, for the MATH dataset, and 8% and 1% for GSM-8K. This highlights that the higher number of samples for the WC model aids in solving more unique problems for both the reasoning datasets. We provide the coverage trends for diverse sampling budgets in Appendix C. In addition, we observe that the coverage of the WC model increases across various difficulty levels in the MATH dataset for the high sampling budget (see Appendix \u2013 Figure 16). This highlights that synthetic data from the WC model can solve more unique questions at various difficulty levels compare to the SE model, at a fixed sampling budget (Tong et al., 2024). Further, we provide a qualitative example that gets solved by repeated sampling from Gemma2-9B but remains unsolved by Gemma2-27B at the fixed high sampling budget (Table 5).\nDiversity: The diversity for the data from Gemma2-9B is higher than Gemma2-27B by 86% and 125% at the low and high sampling budgets for the MATH dataset, and 134% and 158% at for the GSM-8K dataset. This implies that many unique reasoning chains in the synthetic data from the WC model lead to the correct solutions. We also observe that the absolute diversity scores are lower for MATH compared to GSM-8K at high sampling budget, indicating that models generate fewer correct solutions for the more challenging datasets when using repeated sampling.\nFPR: Since we utilize the final answer correctness for filtering the synthetic data, it does not remove the solutions with incorrect intermediate reasoning steps. Our human evaluations suggest that the FPR for the WC-generated solutions is 7% and 2% higher than SE-generated solutions on the MATH and GSM-8K, respectively. The trends from the automatic evaluation are similar to that of human evaluation. Due to the differences in the difficulty of the problems, we note that the absolute"}, {"title": "5.2. Compute-Optimality Results for Training", "content": "We compare the utility of the synthetic data generated from the Gemma2-9B (WC) and Gemma2-27B (SE) model for the MATH and GSM-8K dataset across the diverse finetuning paradigms in Figure 4 and Figure 5, respectively. In addition, we present the results for training with human-written chain-of-thoughts from the original training sets as a baseline."}, {"title": "Student-LM Finetuning.", "content": "We find that the Gemma-7B finetuned with the synthetic data from WC consistently outperforms the one finetuned on data from SC. Specifically, we observe relative gains of 6% and 5.8% at the low and high sampling budgets, respectively, for the MATH dataset and 4.2% and 1.3% for GSM-8K. Contrary to the common belief of stronger models being better for knowledge distillation, our results indicate that finetuning on data from WC is more compute-optimal than data from SE."}, {"title": "WC-LM Finetuning.", "content": "We compare the performance of Gemma2-9B finetuned with the WC data (i.e. self-generated data) and SE data (i.e. data from Gemma2-27B). The results for MATH and GSM-8K are reported in Figures 4b and 5b. We observe that the self-generated data (WC data) improves over knowledge distillation from a strong model (SE data), achieving relative gains of 3.8% and 2% at the low and high sampling budgets, respectively, for the MATH dataset, and 1.5% at the low sampling budget for the GSM-8K dataset. However, we find that the WC model finetuned with WC data matches the SE data for the GSM-8K dataset at a high sampling budget. This is mainly due to the lower difficulty of the GSM-8k dataset, where it becomes saturated at higher sampling budgets (see Figure 15a). Interestingly, our empirical findings suggest that training a WC model on synthetic data from its own is more compute-optimal than distillation from a stronger model."}, {"title": "SE-LM finetuning.", "content": "We present the results for finetuning Gemma2-27B with the Gemma2-9B generated data and self-generated data. The results for MATH and GSM-8K are reported in Figure 4c and 5c. Surprisingly, we observe that the model finetuned with the WC data outperforms the SE data, achieving relative gains of 5.8% and 4.3% at the low and high sampling budget, respectively, for the MATH dataset and 1.2% and 1.5% for the GSM-8K dataset. This result is even more surprising given that the Gemma2-27B data is expected to be more in-distribution than the Gemma2-9B data. Contrary to the common belief of self-generated data or data from a stronger model being better, our empirical findings show that training a model in a W2S-I setup from a WC data may be more compute-optimal than training it in a self-improvement setup on its own data. This result also establishes a new paradigm for improving frontier models in a compute-efficient way, by generating synthetic data from much smaller models."}, {"title": "Generalization.", "content": "Here, we aim to study the transfer capabilities of the models trained with the WC and SE data. Specifically, we evaluate the models finetuned with the synthetic solutions for the MATH datasets at the high sampling budget on the Functional MATH dataset. The results in Figure 6 show that the Gemma-7B finetuned with the WC data consistently outperforms the SE data,"}, {"title": "Takeaway:", "content": "Overall, our findings challenge the conventional wisdom that advocates training on samples from the SE model, by showing that training on samples from the WC model may be more compute-optimal across various tasks and setups."}, {"title": "5.3. Ablation Studies", "content": "Impact of Dataset Size: We study whether the benefits of the synthetic data from the WC model hold at different dataset sizes. We repeat our experiments for the MATH dataset at the high budget, but when only having access to 500 training data (selected randomly from the training set). We present the results for the finetuned models in Figure 7. We observe that models trained with the WC data outperform those trained with the SE data, achieving relative gains of 12.93%, 11.4%, and 5.1% for the three paradigms, respectively. This highlights the utility of generating more data from the WC model instead of the SE model in the low-problem regimes at the fixed sampling budget.\nDefault vs Compute-Optimal Sampling from Cheap LMs: We anticipate that the reason why data from SE models has been previously preferred over data from WC is because they have been tested in a setup where an equal number of samples have been generated from the two models (e.g., see (Singh et al., 2023)), as opposed to a compute-matched setup. To verify this, we generated 1 solution per problem (number-matched) from the WC model for the MATH and GSM-8K datasets and trained the models under the three fine-tuning setups on this generated data, after filtering for final answer correctness. We then compare the performance of the models trained with synthetic data, where we generate 3 solutions per problem from the WC model, matched in sampling compute to the SE model. We present the results in Figure 8. We see that the models trained with the number-matched WC data are sub-optimal in comparison to the models trained with the compute-matched WC data, and lead to worse models compared to training with the SE data. This highlights that the future comparisons between synthetic data from weak and strong models should be made in the sampling"}, {"title": "6. Scaling to state-of-the-art language models", "content": "In the prior experiments, we focused on the synthetic data acquisition from open LMs. Here, we aim to show that data from the weaker SoTA LM can train better reasoners than stronger SoTA LM at a fixed sampling budget. To this end, we scale our method to sampling data from Gemini-1.5-Pro and Gemini-1.5-Flash. As the model sizes are not publicly available, we utilize the ratio between their pricing per output token as a proxy to perform compute-matched sampling. As of August 2024, we note that the price per million output tokens is $10.5 and $0.3 for Gemini-1.5-Pro and Gemini-1.5-Flash, respectively. Hence, we sample 1 and 35 solutions per problem from 1.5-Pro and 1.5-Flash, respectively. We conduct our experiments on the MATH dataset.\nWe perform knowledge distillation on the Gemma-7B, Gemma2-9B, and Gemma2-27B LMs with the synthetic data from the Pro (SE) and Flash (WC) models. We present the results in Figure 10. Interestingly, we find that finetuning with the WC data outperforms the SE data, achieving relative gains of 31.6%, 14.4%, and 10.9% for Gemma-7B, Gemma2-9B, and Gemma2-27B, respectively. This can be attributed to the difference in the coverage of the models at the fixed sampling budget, which is 61.1% and 81% for 1.5-Pro and 1.5-Flash, respectively.\nReducing the price of data sampling. Further, we investigate training the LMs with the WC data that is less expensive than collecting 1 solution per problem from the SE model. Specifically, we create a dataset by sampling 5 solutions per problem from the Flash (WC) model, which is 7x more economical than generating 1 solution from the Pro (SE) model, in terms of the price ($). Upon training the LMs on the 0.15\u00d7 cost data regime (Figure 10), we find that training on this data can also outperform training with SC data, achieving relative gains of 19.1%, 9.8%, and 5.7% for finetuning Gemma-7B, Gemma2-9B, and Gemma2-27B, respectively. This can be attributed to higher coverage of the weaker model (69%), even in the more economical scenario, in comparison to the stronger model (61.1%)."}, {"title": "Takeaway:", "content": "We demonstrate that price-matched sampling from weaker SoTA LMs produces superior reasoners compared to finetuning with data from stronger SoTA models."}, {"title": "7. A Future Perspective", "content": "We showed that for the current WC and SE models, training reasoners through sampling from WC models may be more compute-optimal. Here, we aim to discuss the relevance of these results for the future set of WC and SE models. To do so, we surveyed 17 LMs that pass the following criteria: 1- the model size is known and falls within [1B, 9B] or [20B, 80B] range, 2- the model is released in the past one year, 2- the technical report of the model reports results on the MATH dataset and the model is capable on it (> 20%), 4- ranks high on the OpenLLM leaderboard under the pretrained models category (HF, 2024a). This resulted in models from seven families including Gemma-2 (Team"}, {"title": "8. Related Work", "content": "LMs for reasoning. The ability to solve reasoning tasks has been a long standing goal of artificial intelligence (Achiam et al., 2023; AI, 2024; Anthropic, 2024; Dubey et al., 2024; Reid et al., 2024; Team, 2024). In this regard, LMs trained on the internet-scale data have achieved great success for math, code, and other reasoning tasks (Azerbayev et al., 2023; Kazemi et al., 2024; Lewkowycz et al., 2022). There have been several works that aim to enhance the reasoning capabilities of the LMs either via prompting (Kazemi et al., 2022; Kojima et al., 2022; Wang et al., 2022; Zheng et al., 2023) or finetuning (Yu et al., 2023; Yue et al., 2023). In this work, we focus on finetuning the LMs with task-specific datasets to build strong reasoners. Specifically, our method closely aligns with the widely adopted STaR (Zelikman et al., 2022) where the synthetic data from the LMs are used to elicit strong reasoning capabilities."}, {"title": "9. Conclusion", "content": "In this work, we provide a framework for compute-optimal sampling from weak but cheap LM for reasoning tasks. Specifically, we show that at a fixed sampling compute budget, repeated sampling from a smaller model can achieve higher coverage and diversity than from a strong but more expensive model. Furthermore, our empirical findings highlight that fine-tuning LMs with data from the small LM can consistently outperform data from the large LM under the same compute budget. Our results can serve as a foundation for training LM reasoners, especially as the performance gap between small and large LMs continues to narrow over time."}, {"title": "A. Extending our results to coding tasks", "content": "Here, we aim to understand the utility of the synthetic data from the Gemma2-9B (WC) and Gemma2-27B (SE) model on coding tasks. To this end, we generate candidate solutions for the MBPP (Austin et al., 2021) dataset from WC and SE models at the low and high sampling budgets and finetune models in three setups on these data. We use the santizied version of MBPP5 containing 427 problems overall; we used 3 problems for fewshot prompting (used for sampling from the models), 324 problems for synthetic training data generation, and 100 problems for validation. The candidate solutions are filtered by the unit tests that accompany each instance of the dataset. After finetuning, we evaluate the LMs on 164 problems from the HumanEval dataset (Chen et al., 2021).\nWe compare the coverage and diversity of the synthetic datasets in Figure 12 and observe that the coverage of the WC model is higher than SE at low data regime while it is similar to SE in the high sampling budget regime. In addition, we find that the diversity of the WC model is more than"}, {"title": "B. Qualitative Examples", "content": "We present a few qualitative examples for model-generated solutions that lead to the correct final answer with incorrect (or correct) reasoning for the MATH dataset. We provide two bad reasoning examples in Table 2 and Table 3. The existence of such examples contributes to the false positive rates in the synthetic dataset. In addition, we provide a good reasoning example in Table 4."}, {"title": "C. Coverage Trends", "content": "We present the coverage trends for diverse sampling budgets on the MATH and GSM-8K dataset in Figure 14a and Figure 14b, respectively."}, {"title": "D. Data analysis: GSM-8K", "content": "We presented the coverage, diversity, and false positive rate of the synthetic data from Gemma2-27B and Gemma2-9B on the MATH dataset in the main text. In Figure 15, we present these metrics for the GSM-8K dataset."}, {"title": "E. Solving problems across levels for MATH", "content": "We present the effect of repeated sampling from the weak but cheaper LM and stronger but expensive LM on solving the problems across different levels for the MATH dataset in Figure 16."}, {"title": "F. Finetuning Details", "content": "We generated the candidate solutions in the synthetic dataset using TopK (K= 3) strategy with a temperature of 0.7. We finetuned the Gemma2-9B and Gemma2-27B models with a batch size of 32 for 600 and 6000 steps under the low and high sampling budget, respectively. During the fine-tuning process, we save 10 equally-spaced checkpoints and choose the one that yields the highest validation accuracy. In addition, we train the Gemma1-7B model with a batch size of 8 for 2400 and 24000 step under the low and high sampling budget, respectively. We perform a hyperparameter search for the learning rates {1e \u2013 7, 5e \u2013 7, 1e \u2013 6} based on the model performance on the validation datasets."}]}