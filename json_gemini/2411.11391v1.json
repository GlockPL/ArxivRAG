{"title": "THE GECO ALGORITHM FOR GRAPH NEURAL NETWORKS EXPLANATION", "authors": ["Salvatore Calderaro", "Domenico Amato", "Giosu\u00e8 Lo Bosco", "Riccardo Rizzo", "Filippo Vella"], "abstract": "Graph Neural Networks (GNNs) are powerful models that can manage complex data sources and their interconnection links. One of GNNs' main drawbacks is their lack of interpretability, which limits their application in sensitive fields. In this paper, we introduce a new methodology involving graph communities to address the interpretability of graph classification problems. The proposed method, called GECo, exploits the idea that if a community is a subset of graph nodes densely connected, this property should play a role in graph classification. This is reasonable, especially if we consider the message-passing mechanism, which is the basic mechanism of GNNs. GECO analyzes the contribution to the classification result of the communities in the graph, building a mask that highlights graph-relevant structures. GECo is tested for Graph Convolutional Networks on six artificial and four real-world graph datasets and is compared to the main explainability methods such as PGMExplainer, PGExplainer, GNNExplainer, and SubgraphX using four different metrics. The obtained results outperform the other methods for artificial graph datasets and most real-world datasets.", "sections": [{"title": "1 Introduction", "content": "Deep Neural Networks (DNN) have demonstrated the ability to learn from a wide variety of data, including text, images, and temporal series. However, in cases where information is organized in more complex ways, with individual pieced of data connected by relationships, graph become the preferred data structure. Graphs effectively represent"}, {"title": "2 Related Works", "content": "The representation power of graphs makes them suitable to describe many real-world data. Citation networks, social networks, chemical molecules, and financial data are directly represented with a graph. Graph Neural Networks (GNNs) have been conceived to integrate the graphs with the computation capability of neural architectures. They are a robust framework that implements deep learning on graph-related data. Some computation examples are node classification, graph classification, and link predictions. Some of the most popular GNNs, like Graph Convolutional Networks (GCN) [Kipf and Welling, 2017], Graph Attention Networks (GAT) [Veli\u010dkovi\u0107 et al., 2018], and GraphSage [Hamilton et al., 2017], recursively pass neural messages along the graph edges using the node features and the graph topology information. Using this kind of information leads to complex models; thus, explaining the prediction made by the neural network is challenging. Graph data, unlike images and text, can be less intuitive due to their non-grid structure. The topology of graph data is represented using node features and adjacency matrices, making it less visually apparent than grid-like formats. Furthermore, each graph node has a different set of neighbours. For these reasons, the traditional explainability methods used for text and images are unsuitable for obtaining convincing explanations for neural graph computation. To effectively explain predictions made by a GNN, it is crucial to identify the most critical input edges, the most important input nodes, the most significant node features, and the input graph that maximizes the prediction for a specific class. Explainability methods for GNNs can be divided into two main groups based on the type of information they provide [Yuan et al., 2022]. Instance-level methods focus on explaining predictions by identifying the important input features. These include gradient/features-based methods, which use gradient values or hidden features to approximate input importance and explain predictions through backpropagation; perturbation-based methods, which assess the importance of nodes, edges, or features by evaluating how perturbations in the input affect output; surrogate methods, which involve training more interpretable models on the local neighbourhood of an input node; and decomposition methods, which break down predictions into terms representing the importance of corresponding input features. Model-level methods, on the other hand, study the input graph patterns that lead to specific predictions. Recent literature shows that the state-of-the-art techniques for explainability are the ones we are describing in the following. PGExplainer Luo et al. [2020] and GNNExplainer [Ying et al., 2019] rely on perturbation-based approaches that learn edge masks to highlight important graph components. PGExplainer trains a mask predictor to estimate edge selection probability, while GNNExplainer refines soft masks for nodes and edges to maximize mutual information between the original and perturbed graph predictions. SubgraphX [Yuan et al., 2021] explores subgraph explanations using Monte Carlo Tree Search (MCTS) and Shapley values to find critical subgraphs. Additionally, PGMExplainer [Vu and Thai, 2020] adopts a surrogate approach by constructing a probabilistic graphical model to explain predictions, using perturbations and a Bayesian network to identify important node features."}, {"title": "3 Materials and Methods", "content": "The processing mechanism of the Graph Neural Networks will be introduced in subsection 3.1, and then the GECo methodology will be described in subsection 3.2. The datasets used for testing, both synthetic and real, and the parameters calculated for the performance measurements are described in Subsections 3.3, 3.4, and 3.5, respectively."}, {"title": "3.1 Graph Neural Networks", "content": "Graph Neural Networks (GNNs) are a particular type of Artificial Neural Network (ANN) used to process data with a graph structure. These models can perform various tasks, such as node classification, graph classification, and link prediction. We are interested in graph classification, where the input is a set of graphs, each belonging to a specific class, and the goal is to predict the class of a given input graph. Let G = (V, E) be a graph where V is the set of nodes and & is the set of edges. Every graph can be associated with a square matrix called an adjacency matrix, defined as A \u2208 R|V|\u00d7|V|. For unweighted graphs Aij = 1 if (i, j) \u2208 E, Aij = 0 if (i, j) \u2209 E. For weighted graphs, Aij = Wij, but we restrict our studies only to unweighted ones. Every graph node is associated with a vector of features x \u2208 RC where C represents the number of features. The set of all node features can be represented using a matrix X \u2208 R|V|\u00d7C. A GNN network constituted by K layers lk with k = 1, . . ., K aims to learn a new matrix representation HK \u2208 R|V|\u00d7FK, where FK is the number of features per node after processing in layer K, exploiting the graph topology information and the node attributes. The idea behind the computation is to update the node representations H iteratively, combining them with node representations of their neighbours H with j \u2208 N (i) [Xu et al., 2019]:\n$H^{l+1} = UPDATE^l(H^l, AGGREGATE^l (\\{H_j^l\\},\\forall j \\in N(i)\\}))$\nwhere UPDATE and AGGREGATE are arbitrary differentiable functions and N(i) represents the set of neighbours of the node i. At each iteration, the single node aggregates the information of neighbourhoods, and as the iteration proceeds, each node embedding accumulates information from increasingly distant parts of the graph. This information can be of two kinds: one connected with the structure of the graph, which can be useful in distinguishing structural motifs, and another connected with the features of the nodes in the surroundings. After K iterations, the computed node embeddings are affected by the features of nodes that are K-hops away."}, {"title": "3.2 The proposed methodology", "content": "The proposed method named GECo, aims to find subgraphs that significantly contribute to the output value of the GNN. The algorithm is based on the hypothesis that a GNN learns to recognize specific structures in the input graph; subgraphs containing these key structures are expected to produce a high response in output. Communities are easily identifiable structures in a graph; intuitively, a community is a subset of nodes whose internal connections are denser than those with the rest of the network. Considering the above discussed aggregate step in the GNN algorithm, relevant communities provide, in a trained neural network, an important contribution to the output value. Once the community sub-graphs are identified, they are proposed as stand-alone subgraphs to the GNN, and the resulting output values are stored. The subgraph corresponding to the highest output values is considered the most important for the classification output. According to the taxonomy described in section 2, the method presented here belongs to the instance-level methods; in particular, it is among the perturbation-based methods because it identifies a perturbation of the input consistent with the prediction of the GNN on the original graph. Going into detail, given a trained GNN f, GECo aims to find a mask containing the most relevant nodes for the final classification. The algorithm's inputs are the trained GNN f, a graph G and the associated label y. In the first step, the graph G is given in input to the GNN, obtaining the prediction \u0177 (see Figure 1a). In the second step, it is necessary to find the communities of the graph. Community detection is a well-studied problem in graph theory: the goal is to find groups of nodes that are more similar to each other than to other nodes. Several algorithms exist to solve the community detection problem, such as Girvan and Newman [2002], Newman [2004], Blondel et al. [2008] and Clauset et al. [2004]. For GECo we decided to use the latter, which is a greedy solution based on modularity optimization that works well even for large graphs and uses data structures for sparse matrices. For each community, we build a subgraph that contains only the nodes that belong to the considered community. These subgraphs are fed to the GNN, and the probability value corresponding to the predicted class \u0177 is stored (see Figure 1c). After this step, we obtain the probability value associated with each community. We use these values to calculate a threshold \u03c4 using, for example, the mean or the median of the probability values. For the experimental part the mean function was used. Once fixed the value of t, we consider the communities associated with a probability value greater than \u03c4, and we use the nodes that belong to these communities to form the final explanation (see Figure le)."}, {"title": "3.3 Synthetic datasets", "content": "To evaluate the GECo algorithm, it is necessary to have datasets of graphs with a corresponding mask that highlights the relevant features for classification. This mask allows us to compare the result of the algorithm's explanation with the ground truth explanation. For this reason, we create six synthetic datasets that contain ground truth explanations. We considered the following graphs to generate the synthetic datasets: Erd\u00f6s-R\u00e9nyi (ER) and Barabasi-Albert (BA). ER graphs [Erd\u00f6s and R\u00e9nyi, 1959, Gilbert, 1959] are random graphs introduced by Erd\u00f6s-R\u00e9nyi in 1959. Generally, an ER graph has a fixed number of nodes n connected by randomly created edges. There are two main models: G(n,p), where each edge is added to the graph with probability p, and the G(n, M) model, where a fixed number M of edges are chosen uniformly at random from all possible edges. BA model [Barab\u00e1si and Albert, 1999] is an algorithm to generate a random scale-free network using the technique of \"preferential attachment\". A random scale-free network is characterized by a degree distribution that follows a power-law. In such a network, most nodes have only a few connections (low degree), while a few nodes (called hubs) have many connections (high degree). This contrasts with random networks where the degree distribution is more uniform, and most nodes have a similar number of connections. During the creation of the synthetic datasets, we added to the graphs one of the motifs depicted in Figure 2. Using the ER and BA graphs and the after-mentioned motifs, we built the following synthetic datasets:\n\u2022 ba_house_cycle: contains 1000 BA graphs with 25 nodes. We attach to 500 graphs a house motif and to the other 500 a cycle 6 motif.\n\u2022 er_house_cycle: contains 1000 ER graphs with 25 nodes. We attach a house motif to 500 graphs and a cycle 6 motif to the other 500 graphs.\n\u2022 ba_cycle_wheel: contains 1000 BA graphs with 25 nodes. We attach to 500 graphs a cycle 5 motif and a wheel motif to the remaining 500 graphs.\n\u2022 er_cycle_wheel: contains 1000 ER graphs with 25 nodes. We attach to 500 graphs a cycle 5 motif and a wheel motif to the remaining 500 graphs.\n\u2022 ba_cycle_wheel_grid: contains 1500 BA graphs with 25 nodes. We attach to 500 graphs a cycle 5 motif, a wheel motif to another 500 graphs, and to the remaining part a grid motif."}, {"title": "3.4 Real-world datasets", "content": "In our experimental activity we have also considered a set of real-world datasets containing molecules. We choose molecules since we can obtain the ground-truth explanations for these graphs and compare them with those obtained by our proposal. The datasets involved in our experimental activity are the following:\n\u2022 Mutagenicity [Kazius et al., 2005]: it contains 1768 molecular graphs labelled into two classes according to their mutagenicity properties. The graph labels correspond to the presence or absence of toxicophores: NH2, NO2, aliphatic halide, nitroso, and azo-type.\n\u2022 Benzene [Sanchez-Lengeling et al., 2020]: it consists of 12000 molecular graphs from the ZINC15 database [Sterling and Irwin, 2015]. The molecules belong to two classes, and the goal is to predict whether a given molecule contains a benzene ring. For this dataset, the ground-truth explanations are the atoms (nodes) composing the benzene ring, and if the molecule contains multiple benzene rings, each of these forms the explanation.\n\u2022 Fluoride-Carbonyl [Sanchez-Lengeling et al., 2020]: it contains 8761 molecular graphs labelled according to two classes. For this dataset, a positive sample indicates that the molecule comprises a fluoride (F-) and carbonyl (C = 0) functional group, so the ground-true explanations consist of combinations of fluoride atoms and carbonyl functional groups.\n\u2022 Alkane-Carbonyl [Sanchez-Lengeling et al., 2020]: it contains 1125 molecular graphs labelled according to two classes. A positive sample represents a molecule containing an unbranched alkane and a carbonyl (C = 0) functional group, so the ground-truth explanations include combinations of alkane and carbonyl functional groups."}, {"title": "3.5 Evaluation criteria", "content": "Several metrics have been introduced to evaluate the effectiveness of a method that explains the result obtained using a GNN. In particular, the considered metrics leverage predicted and ground-truth explanations and use user-controlled parameters such as the probability distribution obtained by the GNN and the number of important features that compose the explanation mask. Fidelity [Pope et al., 2019] studies how the prediction of the model changes if we remove from the original graph nodes/edges/node features. Let G\u2081 the i-th graph of the test set, yr the true label of the graph, \u0177ri the label predicted by the GNN, mi the mask produced by the explanation algorithm, Gi \\ mi the graph without the nodes that belong to the mask, Gmi the graph with only the important features detected by the algorithm, \u0177\u2081Gi\\mi and Gmi the labels obtained feeding the GNN with the graphs Gi \\ mi and Gini. It is possible to define two measures Fid+ and Fid as:\n$g(y, \\hat{y}) = \\begin{cases} 1 & y = \\hat{y} \\\\ 0 & y \\neq \\hat{y} \\end{cases}$\n$Fid^+ = \\frac{1}{N}\\sum_{i=1}^N |g(y_i, \\hat{y}_i) - g(\\hat{y}_i^{G_i \\setminus m_i}, y_i)|$\n$Fid^- = \\frac{1}{N}\\sum_{i=1}^N |g(y_i, \\hat{y}_i) - g(\\hat{y}_i^{m_i}, y_i)|$\nwhere N is the number of graphs in the test set. Fid+ studies how the prediction changes if we remove the essential features (edges/features/nodes) identified by the explanation algorithm from the original graph. High values indicate good explanations, so the features detected by the algorithm are the most discriminative. The Fid\u00af measure studies"}, {"title": "4 Results", "content": "Our experimental workflow starts with training a simple GNN made of three GCN layers, an average readout layer, and finally, a linear layer whose number of units is equal to the classes of the considered dataset with softmax activation. We performed a set of preliminary experiments to find the configuration and the suitable number of epochs for the best classification accuracy results. For the synthetic dataset, each GCN layer's dimensionality is 20, while it is 64 for real-world ones. We train the GNN using the Adam optimization algorithm [Kingma and Ba, 2015] with a batch size of 64 and a learning rate of 0.05 in both cases. To ensure reliable results, as a validation protocol, we used 100 different random splits in training and testing with a ratio of 8:2. Post-training the GNN, we applied the explanation algorithm on the test set and computed the performance indices described in subsection 3.5. For the GNN computation, each graph node must be associated with a feature vector: in the synthetic dataset, we assigned each node a feature vector representing its degree; conversely, in the real-world datasets, each node has a feature vector of dimension 14 that represents the chemical properties of the corresponding atom. For a thorough assessment of our method's performance, we compared it against a random baseline and four state-of-the-art methodologies: PGMExplainer [Vu and Thai, 2020], PGExplainer [Luo et al., 2020], GNNExplainer [Ying et al., 2019], and SubgraphX [Yuan et al., 2021]. The random baseline mask on nodes was obtained using a Bernoulli distribution with a 0.5 probability value."}, {"title": "4.1 Results on synthetic datasets", "content": "Here, we analyze and discuss the results assessed by the GECo algorithm on synthetic datasets, comparing them with those from a random baseline and the state-of-the-art techniques previously described. We reported the results and the relative comparison in Table 1 where up-arrows indicate measures that ideally should be one, and down arrows indicate measures that ideally should be zero. From the table, we observe that the GECo algorithm consistently excels in explainability across different datasets, as indicated by its high Fid+ values and near-zero Fid\u00af values. For instance, in the ba_house_cycle dataset, GECo achieves Fid+ = 0.929 and Fid\u00af = 0, effectively identifying key features while excluding irrelevant ones. This suggests that GECo focuses on the most discriminative features used in decision-making. In contrast, methods like GNNExplainer, despite detecting important features, tend to include irrelevant ones. For example, GNNExplainer reports Fid+ = 0.478 and Fid\u00af = 0.257, indicating a less precise explanation compared to GECo. The charact metric further demonstrates GECo's ability to balance sufficiency and necessity, outperforming methods like PGExplainer, which struggles with lower scores due to weaker performance in either aspect. Regarding explanation correctness, as measured by the GEA metric, GECo consistently provides reliable and accurate explanations. For instance, it aligns well with ground-truth explanations in synthetic datasets like ba_cycle_wheel, where it correctly detects wheel motifs, as illustrated in Figure 3. We measured explanation time (in seconds) for each test set graph,"}, {"title": "4.2 Results on real-world data", "content": "In the following, we analyze and discuss the results achieved by the GECo algorithm on real-world datasets, comparing them with random baseline and state-of-the-art techniques. It is essential to point out that the ground-truth explanation is only available for the positive class in the molecular datasets used in our experimental activity. For instance, regarding the Benzene dataset, a molecule belonging to the positive class contains a benzene ring, so the ground-truth explanation comprises the nodes corresponding to the atoms composing the benzene ring. Conversely, molecules of the negative class do not include the benzene ring, so the ground-truth explanation does not contain any node. However, all the explanation algorithms always respond by explaining net response, providing an explanation mask since they try to find the most relevant features used by the model during the decision-making process. As a result, when calculating the GEA metric (see Equation 6), a graph belonging to the negative class always yields a GEA of 0 since TP = 0"}, {"title": "5 Conclusions", "content": "This paper introduces GECo, a novel GNN explainability methodology that leverages community detection for graph classification tasks. By focusing on community subgraphs, it identifies key graph structures crucial for the decision-making process. Through extensive experimentation on both synthetic and real-world datasets, GECo consistently outperformed state-of-the-art approaches. It demonstrated superior performance in detecting relevant features and meeting sufficiency (low values of Fid\u00af) and necessity requirements (high values of Fid+) while also aligning well with ground-truth explanations. Moreover, its computational efficiency makes it a practical solution for large-scale applications. Future work will aim to further refine GECo, particularly in enhancing feature localization in complex datasets and studying its sensibility to the community detection algorithm."}]}