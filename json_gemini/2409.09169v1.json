{"title": "Curricula for Learning Robust Policies over Factored State Representations in Changing Environments", "authors": ["Panayiotis Panayiotou", "\u00d6zg\u00fcr \u015eim\u015fek"], "abstract": "Robust policies enable reinforcement learning agents to effectively adapt to and operate in unpredictable, dynamic, and ever-changing real-world environments. Factored representations, which break down complex state and action spaces into distinct components, can improve generalization and sample efficiency in policy learning. In this paper, we explore how the curriculum of an agent using a factored state representation affects the robustness of the learned policy. We experimentally demonstrate three simple curricula, such as varying only the variable of highest regret between episodes, that can significantly enhance policy robustness, offering practical insights for reinforcement learning in complex environments.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning has had remarkable success across a wide range of domains, including energy management [30], robotic control [26], and strategic board games [24]. However, in many applications, performance is evaluated solely on the training environment, often neglecting the importance of generalisation. This lack of emphasis contributes to some of the central challenges in reinforcement learning, including weak transferability between tasks and the brittleness of policies to small changes in environments or random seeds [32, 8, 14, 27]. Additionally, reinforcement learning algorithms often suffer from low sample efficiency, requiring large amounts of data to achieve robust performance.\nFactored representations [20] decompose high-dimensional, unstructured state and action spaces into a few low-dimensional and high-level variables, each representing distinct and potentially independent aspects of the environment. This decomposition reduces the problem's dimensionality, possibly requiring fewer samples to learn a well-performing policy [29, 2]. Additionally, factored representations can enhance a policy's ability to generalise across different parts of the state space, making it more robust and transferable [1, 9].\nCurriculum learning [4] is a training strategy that structures the learning process, such as by organising different subtasks in a particular sequence, with the goal of improving the learning speed or final performance. This can involve progressively increasing task difficulty or transferring knowledge between tasks of similar complexity. In reinforcement learning [16, 18, 17], this strategy involves training an agent on a sequence of different tasks, enabling it to leverage the knowledge gained from simpler tasks to tackle more challenging ones. This strategy can improve sample efficiency and enhance the robustness of the learned policies [25]. For example, Quick Chess is a simplified version of chess that starts with easier subgames and gradually introduces the player to the whole game [17]."}, {"title": "2 Preliminaries", "content": "Markov Decision Processes. A Markov decision process (MDP) is a mathematical framework used to model decision-making problems. An MDP is defined by a tuple (S, A, P, R, \u03b3):\n\u2022 S is a set of states.\n\u2022 A is a set of actions.\n\u2022 P : S\u00d7A\u00d7S \u2192 [0, 1] is a transition probability function, where P(s'|s, a) denotes the probability of transitioning to state s' from state s after taking action a.\n\u2022 R : S\u00d7A\u00d7S \u2192 R is a reward function, where R(s, a, s') gives the expected reward for taking action a in state s and transitioning to state s'.\n\u2022 \u03b3\u2208 [0, 1] is a discount factor.\nReinforcement Learning. Most commonly, the reinforcement learning problem is modelled as a Markov Decision Process. In this framework, a policy \u03c0(as) represents the probability of taking action a when the agent is in state s. The objective is to learn a policy that maximises the expected cumulative return E\u201e[Gt], which is the sum of discounted rewards over time when following policy \u03c0. The return Gt from time step t is defined as:\nGt = \\sum_{k=0}^{00}rt+k+1."}, {"title": "3 Background", "content": "\"No man ever steps in the same river twice.\"\nHeraclitus\nTo effectively apply reinforcement learning in the real world, we must account for its non-stationary nature. Reflecting the idea of a constantly evolving environment, recent reinforcement learning research focuses on developing robust policies that can handle changing dynamics [12, 10, 11], high-lighting the need for policies that work in varied settings. Distribution shifts can significantly impact performance, leading to poor generalisation and arbitrarily high errors [22, 21]. For reinforcement learning to be successful in the real world, we must consider robustness and how shifts (e.g. an object changing colour) can impact both the domain [7] and the task itself [31].\nFactored state representations, which involve breaking down the environment into distinct components, are an active area of research [28, 15, 3]. These representations have been shown to improve the sample efficiency of reinforcement learning algorithms in both tabular and deep reinforcement learning methods [29, 2]. Additionally, they can help learn policies that are robust to domain shifts [1, 9]. It has also been proven that in scenarios where only the agent's decisions causally influence the reward (e.g. multi-armed bandits where the state does not affect the reward), all robust agents learn an approximate causal model [21], which implies a factored representation.\nCurriculum learning in reinforcement learning structures an agent's learning process by strategically ordering tasks that the agent experiences [17]. It typically aims to enhance the agent's performance and learning speed by enabling the forward transfer of skills from simpler tasks to more challenging ones. A structured curriculum involves several key decisions: choosing the initial set of tasks, defining the progression of tasks, and establishing criteria for transitioning between them. Examples of such curricula include the work of Silva and Costa [23], where tasks are randomly generated and grouped based on their \"transfer potential\", and Narvekar et al. [17], where a set of source tasks is continuously refined to match the agent's current abilities using methods like mistake-driven subtasks, which help the agent correct erroneous behaviour. Similarly, unsupervised environment design [6] is a reinforcement learning training strategy that automatically generates a series of training environments to learn robust policies. Notable work in this area is ACCEL [19], which uses an evolutionary environment generator and regret-based feedback to make small edits to the environment and gradually introduce the agent to more complexity to train a robust policy."}, {"title": "4 The Shifting Frozen Lake", "content": "We define the Shifting Frozen Lake environment, where aspects of the environment can exhibit a shifting behaviour, allowing us to test for out-of-distribution generalisation.\nFrozen Lake [5] is a grid-world environment where the agent navigates from a designated start cell (top-left) to a goal cell (bottom-right). The agent can move up, down, left, or right, and must avoid falling into holes along the way. Depending on the configuration, the actions can be either stochastic or deterministic.\nIn the original Frozen Lake environment, the start location, goal location, hole locations, and grid size are kept constant throughout all the episodes. In Shifting Frozen Lake, the grid size N, the positions of the holes, the starting point, and the goal location can change from one episode to the next. For simplicity, we assume that these variables remain constant during an episode despite potential changes, such as warm weather that could cause the lake to start melting. Due to the changing nature of the environment (e.g. the start location might change), we will refer to different instances of the environment as \"examples\".\nBelow is a full specification of the task:\n\u2022 Actions: Left, down, right, up with deterministic transitions."}, {"title": "5 Experiments", "content": "Our experiments include the following agents:\n\u2022 Random Action Selection: Selects action uniformly at random. Used as a baseline.\n\u2022 Optimal: Achieves the highest possible performance by using breadth-first search to pick the direction with the smallest distance to the goal (without falling into a hole).\n\u2022 PPO: Without using a factored representation, we apply a convolutional neural network to the grid, where each tile is one-hot encoded in a separate channel. We pad the grid with a special character so all grids have the same size.\n\u2022 PPO-F: A PPO agent using an optimised factored representation, retaining only the imme-diate neighbourhood in the distance matrix, which is sufficient for the agent to act optimally in this task. The agent does not model the transition function or use the assumption that the transition function can be factorised.\nWe run all the experiments for five agents and plot the mean and standard error of the total undis-counted reward per epoch (y = 1). Each epoch consists of 900 time steps, and each episode has a timeout of 100 time steps. Performance scores around -30 indicate \u201cstuck\u201d behaviour, where agents avoid losses by engaging in repetitive, looping movements, such as endlessly alternating between left and right actions. Scores higher than -30 but worse than optimal performance indicate an agent that solves some of the grids. For these experiments, we consider random shifting (resampling all variables at the start of each episode) as a test of deep understanding and generalisation of the task because it requires agents to know how to navigate to the goal from anywhere and avoid holes.\nWe explore the following curricula, with changes in curriculum phases indicated by vertical dotted lines in the figures:\n(A) No Shifting to Random Shifting: Fit a single example, then shift all variables randomly to test generalisation.\n(B) No Shifting to Single Random Variable Shifting: Fit a single example and then randomly shift only one variable per episode.\n(C) Random Shifting: Test generalisation from diverse training (domain randomisation) by shifting all variables randomly from the start.\n(D) Stored Examples to Random Shifting: Train a policy by shuffling a few pre-sampled examples and then test generalisation by shifting all variables randomly.\n(E) Single Preset Variable Shifting to Random Shifting: Shift only one specified variable initially, then shift all variables randomly to test generalisation.\nCurriculum (A): No Shifting to Random Shifting. We test generalisation from a single example and present the results in Figure 3. When fitting a single example, the methods show a significant standard error because the grid size can vastly change the reward per epoch. For instance, reaching the goal in 3 steps on a 4x4 grid gives 970 points per epoch, while 20 steps on a 10x10 grid give only 120 points per epoch. None of the trained methods demonstrate significant knowledge transfer after the shift, as their performance drops to around 0. After the shift, PPO exhibits \u201cstuck\u201d behaviour, repeatedly moving left/right or up/down. PPO-F is more active but only solves about 25% of the examples right after the shift. It loses in around 9% of the examples and displays \"stuck\u201d behaviour in the rest.\nCurriculum (B): No Shifting to Single Random Variable Shifting. In Figure 4, we see that both PPO and PPO-F exhibit similarly low knowledge transfer and robustness as there is a big performance drop when random shifting starts. Notably, after a few epochs with random shifting, PPO-F adapts quickly to the new task distribution.\nCurriculum (C): Random Shifting. In Figure 5, we evaluate how well the agents can generalise from diverse training. The test and train distributions of the environment here are identical, so this is IID generalisation. Note, however, that diverse training complicates the learning task. A closer examination of the PPO agent behaviour reveals that it often fails to reach the goal in any"}, {"title": "6 Discussion", "content": "First, our results demonstrate that methods using factored representations can help learn robust policies more easily. Agents using an atomic state representation usually fail to reach the goal when the environment has distribution shifts. While a tailored curriculum could help such agents to learn robust policies, simple curricula may be enough for agents that use a factored representation.\nSecondly, the curriculum used significantly impacted the robustness of the learned policy over a factored state representation. The agents learned comparably robust policies with either diverse training, shuffling a few stored examples, or by shifting a single variable that caused high regret when altered alone (true for two out of three variables). We also quantitatively compare the robustness of the learned policies following each of the curricula and point out the effect of the curriculum on the risk aversion and performance of the learned policies. Lastly, we believe that enabling agents to autonomously generate their own curricula by identifying and adjusting variables that require further exploration (such as those causing high regret) will lead to learning even more robust policies and better generalization across diverse environments."}]}