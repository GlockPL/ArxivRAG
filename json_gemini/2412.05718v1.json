{"title": "RL ZERO: ZERO-SHOT LANGUAGE TO BEHAVIORS WITHOUT ANY SUPERVISION", "authors": ["Harshit Sikchi", "Siddhant Agarwal", "Pranaya Jajoo", "Samyak Parajuli", "Caleb Chuck", "Max Rudolph", "Peter Stone", "Amy Zhang", "Scott Niekum"], "abstract": "Rewards remain an uninterpretable way to specify tasks for Reinforcement Learning, as humans are often unable to predict the optimal behavior of any given reward function, leading to poor reward design and reward hacking. Language presents an appealing way to communicate intent to agents and bypass reward design, but prior efforts to do so have been limited by costly and unscalable labeling efforts. In this work, we propose a method for a completely unsupervised alternative to grounding language instructions in a zero-shot manner to obtain policies. We present a solution that takes the form of imagine, project, and imitate: The agent imagines the observation sequence corresponding to the language description of a task, projects the imagined sequence to our target domain, and grounds it to a policy. Video-language models allow us to imagine task descriptions that leverage knowledge of tasks learned from internet-scale video-text mappings. The challenge remains to ground these generations to a policy. In this work, we show that we can achieve a zero-shot language-to-behavior policy by first grounding the imagined sequences in real observations of an unsupervised RL agent and using a closed-form solution to imitation learning that allows the RL agent to mimic the grounded observations. Our method, RLZero, is the first to our knowledge to show zero-shot language to behavior generation abilities without any supervision on a variety of tasks on simulated domains. We further show that RLZero can also generate policies zero-shot from cross-embodied videos such as those scraped from YouTube.", "sections": [{"title": "1 INTRODUCTION", "content": "Underlying the many successes of RL lies the engineering challenge of task specification, where a skilled expert painstakingly designs a reward function. Not only does this restrict the scaling of RL agents, but it also makes those agents uninterpretable to any user inexperienced with reward design. Even for experts, reasoning about simple reward functions is generally infeasible because these functions can be easily hacked (Krakovna, 2018; Amodei et al., 2016; Dulac-Arnold et al., 2021) to produce behaviors that do not align with human intent. Language is an expressive communication channel for human intent and allows bypassing reward design, but learning a mapping from language to behaviors has historically required collecting and annotating behaviors that correspond to language (Jang et al., 2022; O'Neill et al., 2023). This strategy is impractical at scale where samples from the agent's large space of behaviors need to be labeled. Instead, an approach that strictly makes use of models learned in a purely unsupervised way becomes desirable.\nHow can generalist agents interpret language commands into behaviors? Large-scale multimodal foundation models (Wang et al., 2022) provide us with part of the solution. Trained on large amounts of internet data, they can assist in generating video segments that communicate what performing a task entails. An issue with the video generation models is that they may generate video frames demonstrating tasks that are out of distribution for the current agent's domain; for instance, the current agent can be in a simulated environment, and the video generation models"}, {"title": "2 RELATED WORK", "content": "Language and Control: There is a rich history of using language to solve various tasks in RL: task specification (Thomason et al., 2015; Goyal et al., 2021b; Ma et al., 2023; Baumli et al., 2023; Rocamonde et al., 2023; Stepputtis et al., 2020; Brohan et al., 2022; 2023), transfer and generalization (Goyal et al., 2021a; Jang et al., 2022; Liang et al., 2023), using language to provide hierarchies that allow for solving long-horizon tasks (Ahn et al., 2022; Jiang et al., 2019), driving exploration (Goyal et al., 2019; Harrison et al., 2017; Wang et al., 2023; Ma et al., 2024), human-in-the-loop learning (Chen et al., 2020; Chevalier-Boisvert et al., 2019), giving feedback to AI agents (Wang et al., 2024b), reward design (Yu et al., 2023), etc. Most existing methods either require labels for mapping language to low-level actions or generate reward functions that need to be trained by interacting with the environment to generate a low-level control policy. Recent work (Mazzaglia et al., 2024) proposed an unsupervised approach to grounding language to low-level skills but requires re-training the RL agent for each given task prompt. In contrast, our work presents a method that allows for zero-shot mapping of languages to low-level skills. A large portion of prior work has been limited to using language in a setting where expert demonstrations are provided, but this puts a heavy burden on data collection to cover the large number of skills possible in the environment, which quickly becomes impractical considering the vast array of interactions intelligent agents can perform with their environments. Our approach forgoes this limitation by relying on a zero-shot RL agent capable of mimicking arbitrary imaginations generated for a given text.\nZero-shot RL: Zero-shot RL promises the ability to quickly produce optimal policies for any given task defined by a reward function. A wide variety of methods have been developed to achieve zero-shot RL, which are in some ways generalizations of multi-task RL (Caruana, 1997). Most of these works assume a class of tasks where they can produce policies zero-shot. These tasks can be goal-conditioned (Kaelbling, 1993; Durugkar et al., 2021; Agarwal et al., 2023; Sikchi et al., 2023; Ma et al., 2022b), a linear span of certain state-features (Dayan, 1993; Barreto et al., 2017; Blier et al., 2021b; Touati & Ollivier, 2021; Park et al., 2024a; Agarwal et al., 2024) or some combination of some skills (Eysenbach et al., 2018; 2022; Park et al., 2024b). Other methods (Agarwal et al., 2024) have looked at representing all policies in RL but learning this becomes cumbersome in high dimensions. Recent works (Wu et al., 2018; Touati & Ollivier, 2021; Touati et al., 2023; Park et al., 2024a; Agarwal et al., 2024) employ a successor measure-based representation learning objective to be able to provide near-optimal policies for arbitrary reward function subject to model capacity constraints. Our work leverages these methods and finds the best reward supported by the representations that will produce the language-conditioned imagined trajectory."}, {"title": "3 PRELIMINARIES", "content": "We consider a learning agent in a Markov Decision Process (MDP) (Puterman, 2014; Sutton &\nBarto, 2018) which is defined as a tuple: $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, p, R, \\gamma, d_0)$ where $\\mathcal{S}$ and $\\mathcal{A}$ denote the state and action spaces respectively, $p$ denotes the transition function with $p(s'|s, a)$ indicating the probability of transitioning from $s$ to $s'$ taking action $a$; $R$ denotes the reward function, $\\gamma \\in (0, 1)$ specifies the discount factor and $d_0$ denotes the initial state distribution. The reinforcement learning objective is to obtain a policy $\\pi : \\mathcal{S} \\rightarrow \\Delta(\\mathcal{A})$ that maximizes expected return: $E_{\\pi}[\\sum_t r(s_t)]$,\nwhere we use $E_{\\pi}$ to denote the expectation under the distribution induced by $a_t \\sim \\pi(S_t)$, $S_{t+1} \\sim p(s_t, a_t)$ and $\\Delta(\\mathcal{A})$ denotes a probability simplex supported over $\\mathcal{A}$.\nMultimodal Video-Foundation Models (ViFMs) and In-Domain Video Generation: Multimodal\nViFMs (Wang et al., 2022; 2024a; Tong et al., 2022) facilitate the understanding of video data\nin a shared representation space of modalities such as text or audio. Recent works in video"}, {"title": "4 RLZERO: ZERO-SHOT PROMPT TO POLICY", "content": "RLZero uses components trained with unsupervised learning to map language to behaviors. In the following sections, we describe the steps involved in detail: First, we present how an imagined trajectory is generated from a prompt. Then, we discuss how the imagined trajectory for a given prompt is projected to real observations of an agent. Finally, we describe the zero-shot procedure for inferring a policy that matches the behavior in the imagined trajectory."}, {"title": "4.1 IMAGINE: GENERATIVE VIDEO MODELING", "content": "Grounding language to tasks in robotics has historically (Goyal et al., 2021a; Jang et al., 2022;\nO'Neill et al., 2023) required costly annotation labels that map language to task examples specified through image or state trajectories. Large video-language foundation models (ViFMs) help lift that requirement by training on vast amounts of internet videos, thus giving us a rich prior of grounding language commands to videos. A common issue in robotics, both simulated and real world, is that the domains often differ from real-world videos. We rely on a generative video modeling approach, GenRL (Mazzaglia et al., 2024), that uses a task encoder provided by an off-the-shelf ViFM (InternVideo (Wang et al., 2022)) along with a GRU architecture to imagine the video in latent space trained to be reconstructed to the environment domain. Training the video generation model does not require labels mapping language to tasks and is fully unsupervised. With an increase in ViFM scaling and developments in controllable video generation (Bruce et al., 2024; Hu et al., 2022; Ni et al., 2023; Chen et al., 2025), a few examples of environment domain may be sufficient to generate high quality in-domain imagination. Thus, given a language instruction $e^l$, we obtain a sequence of frames $(i_1, i_2...i_T) = VM(e^l)$ that represents an imagination of what the task looks like in the environment domain. These imaginations are not expected to adhere to the agent's environment dynamics.\n4.1.1 GROUNDING IMAGINATION IN REAL-STATES\nThe imaginings produced by VLMs can be noisy, unrealizable, and not exactly representative of the domain. We propose to use a similarity-based retrieval for the nearest frames in the dataset of the agent's prior environmental interactions $d_O$ to project the imagined trajectories in real observations. This step allows us to match imagination to real observations in the semantic space, giving us the flexibility to use imaginations in differing domains than the agent (e.g. cross-embodiment discussed in Section 5.2). In this work, we use a performant image embedding approach for retrieval, SigLIP (Zhai et al., 2023), to map both the imagined frame and agent observation to a latent embedding space, which is trained for similarity matching with a contrastive objective. We use an encoding function $\\mathcal{E} : I \\rightarrow Z$ to individually map a sequence of images to shared text-image embedding space. For each consecutive k length sequence of frames in the imagined trajectory, we output the following agent observations:"}, {"title": "4.2 IMITATE: DISTRIBUTION MATCHING WITH ZERO-SHOT RL", "content": "The successor measure-based family of BFMs (Touati & Ollivier, 2021; Touati et al., 2023;\nAgarwal et al., 2024) captures the state visitation distribution of any policy in their learned\nbag of skills parameterized by $z$ and given by $p^\\pi(s) = E_{s_0, a_0\\sim \\rho}[M^{\\pi_z}(s_0, a_0, s)]$, where $M$\ndenotes the successor measure. We take the distribution matching perspective of imitation\nlearning (Ghasemipour et al., 2020; Ni et al., 2021; Sikchi et al., 2024) and minimize the distance\nbetween the state visitation distributions of the grounded imagined trajectories (expert) and the\npolicy denoted by $p^E$ and $p^{\\pi_z}$ respectively:\n$z_{imit} = arg \\min_z distance(p^{\\pi_z}(s), p^E(s)),$\nwhere the distance can be chosen to be mean-squared error, f-divergence, Integral Probability\nMetrics (IPM), etc. In general, minimizing the distance via gradient descent can provide a solution\n$z_{imit}$ to distribution matching. For the special case of KL divergence, Theorem 1 shows that\n$z_{imit}$ can be obtained in closed form using a learned distribution ratio between expert and offline\ninteraction dataset $p^E/p$.\nTheorem 1. Define $J(\\pi, r)$ to be the expected return of a policy $\\pi$ under reward $r$. For an offline\ndataset $d_O$ with density $\\rho$, a learned log distribution ratio: $\\upsilon(s) = log(\\frac{p_E(s)}{\\rho(s)})$, $D_{KL}(\\rho^\\pi, \\rho_E) \\leq\n-J(\\pi, r_{imit}) + D_{KL}(\\rho^\\pi(s, a), \\rho(s, a))$ where $r_{imit}(s) = \\upsilon(s) \\forall s$. The corresponding $z_{imit}$\nminimizing the upper bound is given by $z_{imit} = E_{\\rho}[\\upsilon(s)\\varphi(s)] = E_E[\\frac{\\varphi(s)}{\\rho(s)}]$ where $\\varphi$\ndenoted state features learned by the BFM.\nThus, with the reward functions specified by $r_{imit}$, we can use the closed form solution of\n$z_{imit} = E_{\\rho}[\\upsilon(s)\\varphi(s)]$ to retrieve the policy that mimics the grounded imagined behavior. This\nreward function requires learning a discriminator to obtain the distribution ratio, which can lead to\ninstabilities, but a heuristic yet performant alternative is to use a shaped reward function $r(s) = \\newline \\exp(\\upsilon(s))$, similar to Pirotta et al. (2023), which allows zero-shot inference ($z_{imit} = E_{\\rho_E}[\\varphi(s)]$) without"}, {"title": "5 EXPERIMENTS", "content": "Our experiments seek to understand the quality of behaviors that the $\\mathcal{RL}_{Zero}$ approach is able to produce given language prompts. The evaluation of these behaviors can be challenging as, unlike the traditional RL setting, we do not have access to a ground truth reward function. Instead, we have prompts that can be inherently ambiguous but reflect the reality of human-robot interaction. An obvious evaluation metric is to ask humans how much the generated behavior resembles their expectation of the behavior given the prompt. We use multimodal LLMs to evaluate such preferences as a proxy to human preferences, as recent studies (Chen et al., 2024) have shown them to be correlated (up to 79.3%).\nSetup: We consider four DMC control tasks (Cheetah, Walker, Quadruped, and Stickman (Mazzaglia et al., 2024)). The Stickman environment reflects a human morphology with challenging control due to a large observation and action space. For task-conditioned video-generation we use off-the-shelf models from Mazzaglia et al. (2024). To obtain the nearest observation corresponding to the imagined image, we use SigLIP (Zhai et al., 2023), a state-of-the-art image-text embedding model. In all environments except Stickman, we collect data using RND (Burda et al., 2018) using the protocol specified in ExoRL (Yarats et al., 2022). For Stickman, we augment our dataset with replay buffers of the agent trained for run and walk behaviors, as obtaining meaningful tasks with pure random exploration is difficult with the large action-space of Stickman. The detailed composition of the datasets can be found in Appendix B.3. The behavior foundation model can be trained using any zero-shot RL method using successor features (Park et al., 2024a; Touati et al., 2023; Agarwal et al., 2024). In our experiments, we use the Forward-Backward zero-shot RL algorithm (Touati et al., 2023) using offline datasets.\nSetting and Baselines: For our evaluations, we consider the setting where the agent has no access to the simulator during test time. This setting truly reflects the ability of the agents to use prior exploratory data to learn meaningful behaviors. We compare state-of-the-art model-free offline RL algorithms that are capable of learning from purely offline data. For RL algorithms, the"}, {"title": "5.1 BENCHMARKING ZERO-SHOT PERFORMANCE FOR CONTINUOUS CONTROL", "content": "The ability to specify prompts and generate agent behavior allows us to explore complex behaviors that might have required complicated reward function design. We curate a set of 25 tasks across 4 DM-control environments. Each of the agents has unique capabilities as a result of its embodiment, and the prompts are specified to be reasonable tasks to expect for the specific domain. Furthermore, we filtered out prompts for which our off-the-shelf video generation model was unable to faithfully generate videos. We discuss this more in Appendix B.2. For each prompt, we generate behaviors for 5 seeds. The performance of any given method is evaluated as the win rate over the base method. We chose the base model for our comparisons as the policies trained via TD3 on image-language rewards. For each seed, we present the observation frames that the policy generated by different methods observes and pass it to a Multimodal LLM capable of video understanding, which is used as a judge. Since the number of tokens can get quite large with the long default horizon of the agent (1000 horizon), we subsample the videos by choosing every 8 frames and selecting the first 64 frames of size 256 \u00d7 256. We observed this subsampling to retain temporal consistency and the effective horizon (8 \u00d7 32 = 256) to be long enough to demonstrate the task requested by the prompt."}, {"title": "5.2 CAN RLZERO SUCCEED AT CROSS-EMBODIMENT IMITATION?", "content": "The intermediate stage in RLZero of matching the closest observations in the offline dataset to a frame from a video is based on semantic similarity. This means that we are not restricted to generating videos in the same domain of the agent and still expect semantic search to generalize for out-of-domain matching. Subsequently, we can skip the imagine step completely if we are given an expert demonstration. To investigate this, we consider a collection of videos scraped from Youtube as well as videos generated by open-source video generation tools like MetaAI and empirically test if RLZero is able to replicate the behaviors. We focus on the Stickman environment for our experiments here as it reflects human embodiment closely and allows us to use human videos from the internet."}, {"title": "5.3 ABLATION AND FAILURE CASES", "content": "Imagination-free behavior generation: While the imagine, project, and imitate framework allows for interpretability into agent's behavior, we investigate if we can amortize the imagination and embedding search cost by directly mapping the language embedding to the skill embedding in the Behavior Foundation Model's latent space. For this, we consider sampling $z$ uniformly in the latent space of the BFM and embedding the generated image observation sequence through a ViFM, which we denote by $e$. Given the observation sequence, we generate the $z_{imit}$ using the zero-shot inference process and learn a mapping from $e \\rightarrow z_{imit}$ using a small 3-layer MLP."}, {"title": "6 CONCLUSION", "content": "Language presents an appealing and human-friendly alternative to reward design for task specification. In this work, we presented a completely unsupervised approach for grounding language to low-level behavior in a zero-shot manner. A completely unsupervised approach allows us to bypass requiring costly annotators for labeling a wide variety of behaviors with language, and a zero-shot approach allows us to avoid training during deployment time along with the advantage of generating the behaviors instantaneously. We propose RLZero, a framework to imagine what a behavior specified by a text prompt looks like and to ground that imagination to a policy via imitation. Unlike reward functions, this approach is not prone to reward hacking as the distribution matching objective specifies the task completely and accurately. Our evaluations show that the behaviors generated by RLZero show an improvement over using reward functions derived from image-language of video-language models.\nFuture Directions: RLZero opens up the possibility of prompting to generate a policy. Zero-shot approaches are always expected to be near-optimal due to the projection of a reward to a low dimensional space as well as limited coverage of offline interaction data. But this serves as a good initialization for further fine-tuning. How to fine-tune efficiently without forgetting remains an open question. Furthermore, learned skills can be combined according to the hierarchy specified in language instructions, allowing for the completion of complex long-horizon tasks. Since the mechanism of RLZero allows for interoperability to some extent by observing the nearest states as well as imagination, automatic failure detection becomes appealing. For the setting of prompt-to-policy, we lack accurate evaluation metrics since the true reward function is unknown, and human evaluation can be subjective. Finally, with larger context-window video understanding models, we believe an end-to-end pipeline of language embedding to task embedding (imagination-free RLZero) can become more appealing."}, {"title": "A PROOF FOR THEOREM 1", "content": "Theorem 1. Define $J(\\pi, r)$ to be the expected return of a policy $\\pi$ under reward $r$. For an offline dataset $d_O$ with density $\\rho$, a learned log distribution ratio: $\\upsilon(s) = log(\\frac{p_E(s)}{\\rho(s)})$, $D_{KL}(\\rho^\\pi, \\rho_E) \\leq\n-J(\\pi, r_{imit}) + D_{KL}(\\rho^\\pi(s, a), \\rho(s, a))$ where $r_{imit}(s) = \\upsilon(s) \\forall s$. The corresponding $z_{imit}$\nminimizing the upper bound is given by $z_{imit} = E_{\\rho}[\\upsilon(s)\\varphi(s)] = E_E[\\frac{\\varphi(s)}{\\rho(s)}]$ where $\\varphi$\ndenoted state features learned by the BFM.\nProof. Let $\\rho$ be the density of the offline dataset, $\\rho^\\pi$ be the visitation distribution w.r.t. policy $\\pi$ and $\\rho_E$ be the expert density. The distribution matching objective mentioned in Equation 7 using KL divergence is given as:\n$\\min D_{KL}(\\rho^\\pi || \\rho^E)$\nWith simple algebraic manipulation, the divergence can be simplified to,\n$D_{KL}(\\rho^\\pi || \\rho^E) = E_{\\rho^\\pi} [log \\frac{\\rho^\\pi}{\\rho^E}] + E_{\\rho^E} [log \\frac{\\rho}{\\rho^E}]$\n$ = E_{\\rho^\\pi} [log \\frac{\\rho(s)}{\\rho^E(s)}] + D_{KL}(\\rho^\\pi(s) || \\rho(s))$\n$= - J(\\pi, log \\frac{\\rho}{\\rho^E}) + D_{KL}(\\rho^\\pi(s) || \\rho(s))$\n$\\leq - J(\\pi, log \\frac{\\rho}{\\rho^E}) + D_{KL}(\\rho^\\pi(s, a) || \\rho(s, a))$\nThe last line follows from the fact that $D_{KL}(\\rho^\\pi(s) || \\rho(s)) \\leq D_{KL}(\\rho^\\pi(s, a) || \\rho(s, a))$.\n$D_{KL}(\\rho^\\pi(s, a) || \\rho(s, a)) = E_{\\rho^\\pi(s, a)} [log \\frac{\\rho^\\pi(s, a)}{\\rho(s, a)}]$\n$= E_{\\rho^\\pi(s, a)} [log \\frac{\\rho^\\pi(s) \\pi(a|s)}{\\rho(s, a)}]$\n$= E_{\\rho^\\pi(s, a)} [log \\frac{\\rho^\\pi(s) \\pi(a|s)}{\\rho(s) \\pi_D(a|s)}]$\n$= E_{\\rho^\\pi(s)} [log \\frac{\\rho^\\pi(s)}{\\rho(s)}] + E_{\\rho^\\pi(s, a)} [log \\frac{\\pi(a|s)}{\\pi_D(a|s)}]$\n$= D_{KL}(\\rho^\\pi(s) || \\rho(s)) + E_{s \\sim \\rho^\\pi}[D_{KL}(\\pi(a|s) || \\pi_D(a|s))]$\n$\\geq D_{KL}(\\rho^\\pi(s) || \\rho(s))$\nRewriting the minimization of the upper bound of KL as a maximization problem by reversing signs, we get:\n$\\max J(\\pi, log \\frac{\\rho_E}{\\rho}) - D_{KL}(\\rho^\\pi(s, a) || \\rho(s, a))$\nThe first term is an RL objective with a reward function given by $log(\\frac{\\rho_E}{\\rho})$, and the second term is an offline regularization to constrain the behaviors of offline datasets. Following prior works Kim et al. (2022); Ma et al. (2022a), since our BFM is trained on an offline dataset and limited to output skills in support of dataset actions, and we can ignore the regularization to infer the latent $z$ parameterizing the skill. A heuristic yet performant alternative is to use a shaped reward function $\\exp{\\upsilon(s)}$ which allows us to avoid training the discriminator completely and was shown to lead to performant imitation in Pirotta et al. (2023)."}, {"title": "B EXPERIMENTAL DETAILS", "content": "B.1 ENVIRONMENTS\nB.1.1 DM-CONTROL ENVIRONMENTS\nWe use continuous control environments from the DeepMind Control Suite (Tassa et al., 2018).\nWalker: It has a 24 dimensional state space consisting of joint positions and velocities and 6 dimensional action space where each dimension of action lies in [-1,1]. The system represents a planar walker.\nCheetah: It has a 17 dimensional state space consisting of joint positions and velocities and 6 dimensional action space where each dimension of action lies in [-1,1]. The system represents a planar biped \u201ccheetah\u201d.\nQuadruped: It has a 78 dimensional state space consisting of joint positions and velocities and 12 dimensional action space where each dimension of action lies in [-1,1]. The system represents a 3-dimensional ant with 4 legs.\nStickman: Stickman was recently introduced as a task that bears resemblance to a humanoid in Mazzaglia et al. (2024). It has a 44 dimensional observation space and a 10 dimensional action space where each dimension of action lies in [-1,1].\nFor all the environments we consider image observations of size 64 x 64. All DM Control tasks have an episode length of 1000."}, {"title": "B.2 EVALUATION PROTOCOL", "content": "To evaluate models for behavior generation through language prompts, we considered a set of 4 prompts per environment. One key consideration in designing these prompts was the generative video model\u2019s capability of generating reasonable imagined trajectories. Due to computing limitations, we were restricted to using a fairly small video embedding ( 1 billion parameters) and generation model ( 43 million parameters). The interpretability of our framework allows us to declare failures before they happen by looking at the generations for imagined trajectories.\nFor the set of task prompts specified by language, there is no ground truth reward function and there does not exist a reliable quantitative metric to verify which of the methods perform better. Instead, since humans communicate their intents via language, humans are the best judge of whether the agent has demonstrated the behavior they intended to convey. In this work we use a Multimodal LLM as a judge, following studies by prior works demonstrating the correlation of LLMs judgment to humans (Chen et al., 2024). We use GPT-4o model as the judge, where the GPT-4o model is provided with two videos, one generated by a base method, and another generated by one of the methods we consider, and asked for preference between which video is better explained by the text prompt for the task. When inputting the videos to the judge, we randomize the order of the baseline and proposed methods to reduce the effect of anchoring bias. The prompt we use to compare the two methods is given here:"}, {"title": "B.3 DATASET COLLECTION FOR ZERO-SHOT RL", "content": "For Cheetah, Walker, Quadruped, and Stickman environments, our data is collected following a pure exploration algorithm with no extrinsic rewards. In this work, we use intrinsic rewards obtained from Random Network Distillation (Burda et al., 2018) to collect our dataset based on the protocol by ExoRL (Yarats et al., 2022) and using the implementation from repository ExoRL repository. For Cheetah, Walker, and Quadruped, our dataset comprises 5000 episodes and equivalently 5 million transitions, and for Stickman, our dataset comprises 10000 episodes or equivalently 10 million transitions. Due to the high dimensionality of action space in Stickman, RND does not discover a lot of meaningful behaviors; hence we additionally augment the dataset with 1000 episodes from the replay buffer of training for a 'running' reward function and 1000 episodes of replay buffer trained on a 'standing' reward function."}, {"title": "B.4 BASELINES", "content": "Zero-shot text to policy behavior has not been widely explored in RL literature. However, Offline RL using language-based rewards utilizes an offline dataset to learn policies and is thus zero-shot in terms of rolling out the learned policy. This makes it a meaningful baseline to compare against. Offline RL uses the same MDP formulation as described in Section 3 to learn a policy $\\pi : \\mathcal{S} \\rightarrow \\Delta(\\mathcal{A})$, given a reward functionr: $\\mathcal{S} \\rightarrow \\mathbb{R}$ and offline dataset D. The offline dataset consists of state, action, next-state, reward transitions (s, a, s',r(s)). One of the core challenges of Offline RL is to learn a Q-function that does not overestimate the reward of unseen actions, which then at evaluation causes the agent to drift from the support of the offline dataset D.\nWe implement two offline RL baselines to compare with RLZero- Implicit Q-learning (IQL, Kostrikov et al. (2021)) and Offline TD3 (TD3, Fujimoto & Gu (2021)). Both of these methods share the same offline dataset as used to learn the successor measure in RLZero, which is described in Section 5, and gathered using RND. Since these datasets are reward-free, we must still construct a reward function that provides meaningful rewards for an agent achieving the behavior that aligns with the text prompt. Formally, given language instruction $e^l \\in E^l$, frame stack $(O_{t-k}, O_{t-k+1},..., O_t) \\in \\mathcal{Z}$, and embedding VLM $\\phi : E \\rightarrow \\mathcal{Z}$, which can also embed frame stacks $\\phi : I^Z \\rightarrow \\mathcal{Z}$ (and where observations $o_t \\in I$, and we use $o_t$ for this section), the reward for a corresponding language instruction and frame stack k is the cosine similarity between the stacked language embedding and the frame embedding:\n$r(o_{t-k:t}, e^l) = \\frac{\\phi(e^l) \\cdot \\phi(O_{t-k:t})}{||\\phi(e^l)|||||\\phi(O_{t-k:t}) ||}$\nFor any individual task, $e^l$ is fixed and this is a reward function dependent on observations (as represented by a frame stack $O_{t-k:t}$). Notice that this representation closely matches that in Equation 6, but instead of finding the optimal sequence of observations, we simply compute reward as the cosine similarity between language and frames. Since the strength of the embedding space is vital to the quality of the reward function for offline RL, we evaluate two different vision-language models:\nImage-language reward (SigLIP Zhai et al. (2023)): take a stack of 3 frames encode them using SigLIP, then the reward is computed as the cosine distance of the embeddings and the SigLIP embedding of language.\nVideo-language reward (InternVideo2 Wang et al. (2024a)): this method takes in previous frames $o_{0:t-1}$ as context and uses it to generate an embedding of the current frame observation $O_t$. The video encoder then takes the cosine similarity of $\\phi(o_{0:t})$ and $\\phi(e^l)$. This allows the reward function to provide rewards based not only on reaching certain states, but the agent exhibiting temporally"}, {"title": "B.4.1 OFFLINE RL", "content": "Implicit Q-learning (Kostrikov et al.", "rewards)": "n$L(\\theta) = E_{(s", "a))^2": "nto learn a Q function Q. IQL builds on this loss to handle the challenge of ensuring that the Q-values do not speculate on out-of-distribution actions while also ensuring that the policy is able to exceed the performance of the behavior policy. Exceeding the behavior policy is important because the dataset is collected using RND", "regression": "n$L_{\\tau}(\\mu) = |\\tau - 1(u < 0)|u^2$\nWhere $\\tau > 0.5$ is the selected expectile. Expectile regression gives greater weight to the upper expectiles of a distribution, which means that the Q function will focus more on the upper values of the Q function.\nRather than optimize the objective with $Q(s', a')$ directly, IQL uses"}]}