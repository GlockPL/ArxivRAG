{"title": "On Evaluating LLMs' Capabilities as Functional Approximators: A Bayesian Perspective", "authors": ["Shoaib Ahmed Siddiqui", "Yanzhi Chen", "Juyeon Heo", "Menglin Xia", "Adrian Weller"], "abstract": "Recent works have successfully applied Large Language Models (LLMs) to function modeling tasks. However, the reasons behind this success remain unclear. In this work, we propose a new evaluation framework to comprehensively assess LLMs' function modeling abilities. By adopting a Bayesian perspective of function modeling, we discover that LLMs are relatively weak in understanding patterns in raw data, but excel at utilizing prior knowledge about the domain to develop a strong understanding of the underlying function. Our findings offer new insights about the strengths and limitations of LLMs in the context of function modeling.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have revolutionized domains that can be formulated in a simple text-in-text-out format (Raffel et al., 2020; Brown et al., 2020). This includes a wide array of tasks from a helpful chatbot (Achiam et al., 2023), code assistant (Roziere et al., 2023), math theorem provers (Trinh et al., 2024), to an automated scientist (Lu et al., 2024).\nGiven the well-established performance characteristics of LLMs in a wide range of reasoning tasks (Achiam et al., 2023; Bubeck et al., 2023; Si et al., 2024; Lu et al., 2024), there has been growing interest in exploring their application on real-world prediction tasks i.e., as a regression system (Liang et al., 2022; Zheng et al., 2023; Roberts et al., 2023; Yu et al., 2023; Xiao et al., 2024). These approaches typically convert numerical data into tokens that can be processed by an LLM, which then predicts numerical targets for a query point conditioned on the task description and the provided in-context examples.\nProminent examples of using LLMs for function approximation include predicting velocities"}, {"title": "2 Background and Related Work", "content": ""}, {"title": "2.1 Large language models", "content": "Large Language Models (LLMs) are probabilistic models trained to predict the probability distribution over the next token conditioned on previous tokens (Radford et al., 2018) i.e.,\n$$LLM_{\\theta}(w_1, ..., w_t) := p(w_{t+1}|w_1, ..., w_t)$$\nOutputs from the model are generated via autoregressive sampling of the next token $$w_{t+1} \\sim P_{\\theta}(w_{t+1}|w_1, ..., w_t)$$ conditioned on all the previous tokens (Radford et al., 2018). After training on massive datasets spanning trillions of tokens, the model acquires a vast amount of knowledge and reasoning capabilities (Bubeck et al., 2023; Dubey et al., 2024). This knowledge can be leveraged by the model for better function modeling e.g., by taking physical constraints into account when modeling a physical phenomenon (see Fig. 1)."}, {"title": "2.2 LLMs as functional predictors", "content": "Given prompt t describing the prediction task, systems that use LLMs to make prediction can be mathematically described as follows:\n$$\\hat{y}(x; t, D) := EXTRACT(arg \\underset{s}{max} p(s|x, t, D))$$\nwhere p is modeled via the language model LLM, x is the query datapoint, t is the task description, D is the data used for in-context learning, and EXTRACT(\u00b7) extracts the prediction \u0177 from the generated sequence s. Note that the in-context learning data can also be empty i.e., D = \u00d8.\nPrior work applying LLMs in different function modeling tasks can be seen as different instantiations of the same prediction framework. Liang et al. (2022) used LLMs to convert context-dependent terms such as 'more' or 'less' to exact velocity values. This can be seen as modeling the function between the terms and the velocity. Zheng et al. (2023) used GPT-4 to predict the accuracy of models with different configurations as a more efficient architecture search scheme, which can be seen as modeling the function behind hyperparameter configurations and accuracy. GPT4Geo (Roberts et al., 2023) attempted to leverage GPT-4 to directly predict elevation given geospatial coordinates, thereby also modeling a real-world function. Yu et al. (2023) evaluated the capabilities of LLMs to directly forecast financial time series. Similarly, Gruver et al. (2023) used LLMs to directly forecast time-series values while demonstrating their capability to model distributions. Requeima et al. (2024) introduced the idea of LLM Processes and applied LLM to a number of different time-series prediction tasks by conditioning on additional side information. Qin et al. (2023) conducted a comprehensive empirical assessment of GPT-4's capabilities across a spectrum of arithmetic reasoning tasks. Bubeck et al. (2023) also presented an example of function modeling tasks by evaluating the capabilities of GPT-4 in solving math riddles.\nUnlike prior work that applied language models to novel function modeling tasks, we instead attempt to understand the reasons for their success by using a Bayesian evaluation framework, where we disentangle the language model's function modeling capabilities into two fundamental aspects."}, {"title": "3 A Bayesian Evaluation Framework", "content": "Function modeling as Bayesian inference. We begin by framing the task of real-world function modeling as performing Bayesian inference in functional space (Ghahramani, 2015):\n$$p(f|D) \\propto p(D|f)p(f)$$\nwhere D is the data, p(D|f) is the likelihood function that measures to what extent a function f matches with the data, and p(f) is the prior over f. Both p(D|f) and p(f) are important for accurate modeling of the function. For example, a linear function flinear that accurately describes the trajectory of a ball in Fig. 1 may attain a high value of likelihood p(D|f), yet a good prior p(f) based on physics would identify that a quadratic function fquadratic is indeed more plausible, given context information about the task.\nMotivated by this Bayesian view of function modelling, we propose to factorize LLMs' function modeling capabilities into two key aspects:\n\u2022 The ability to understand the patterns present in raw data. This corresponds to the quality of the likelihood function p(D|f) in Eq. 3.\n\u2022 The ability to incorporate domain knowledge in order to better estimate the underlying function. This corresponds to the quality of the posterior p(f|D) over the function in Eq. 3, with p(f) being the prior shaped by the domain knowledge acquired by the LLM during pretraining.\nThe prior p(f) itself can be viewed as a posterior over f after seeing the massive data Dweb on internet during LLM pretraining: p(f) = p(f|Dweb).\nEvaluation objectives. We are interested in separately evaluating the aforementioned two capabilities of language models in function modeling tasks. This separate evaluation, supported by our techniques detailed below, enable us to understand different aspects of the language model's capabilities. Here, we assess them by the prediction accuracy ACC of the language model on the test set Dtest:\n$$ACC(D, t) = E_{(x,y)\\sim D_{test}}[1[\\hat{y}(x; D, t) = y]]$$\nBy carefully specifying the data D and the prompt t, we can either utilize or disregard domain knowledge provided by the language model during prediction, leading to the isolated evaluation of the aforementioned two capabilities of the model."}, {"title": "3.1 Evaluating the ability to understand raw data patterns", "content": "In this section, we develop techniques for evaluating the quality of the ability of LLM to understand the patterns in raw data. The key to this evaluation is to remove the influence of the prior p(f), where we ensure that no domain knowledge can be utilized by the LLM in its prediction. We realize this through two important operations applied to the prompts: NUMERIZE(\u00b7) and DECONTEXTUALIZE(\u00b7).\nNumerizing data. We first remove any information about domain by turning each data x in the original data D, which is potentially in verbal form, into purely numerical values:\n$$x \\leftarrow NUMERIZE(x), \\forall x \\in D$$\nwhere NUMERIZE : S \u2192 Rd is an operation that to map a sentence s \u2208 S from the sentence space S to a real-valued vector. For example, the sentence '{education=PhD, age=33}' describing the features of an individual will be transformed to a datum z = {1.0,0.33}. Here, the values zi are normalized, so that zi \u2208 [0,1]. Normalization is introduced to avoid a LLM from inferring the semantic meaning of features according to their scale, range, and distribution. Note that a similar operation is usually applied before feeding data into classical machine learning approaches due to their inherent inability to condition on arbitrary text.\nDecontextualizing task description. Another operation is to remove any information about the domain or the context from the task description t:\n$$t \\leftarrow DECONTEXTUALIZE(t)$$\nwhere DECONTEXTUALIZE : S \u2192 S is an operation to rewrite the task description. For example, consider the original task description in the ball trajectory prediction example where t can be something like \u2018we would like to predict the trajectory of a ball given its past trajectory'. The DECONTEXTUALIZE function rewrites this task description as 'this is a regression task where we predict y from x given some training data', stripping away any possible cues to leak domain information. Fig. 2 provides an example of the prompt we use to evaluate this ability."}, {"title": "3.2 Evaluating the ability to incorporate domain knowledge", "content": "In this section, we focus on techniques for evaluating the ability of LLM to incorporate domain knowledge in function modeling tasks. Unlike the previous case where we remove the impact of prior, here we aim to emphasize the influence of the prior p(f), which represents the domain knowledge the LLM holds. We achieve this by two operations applied to the prompts: VERBALIZE(\u00b7) and CONTEXUALIZE(\u00b7).\nVerbalizing data. Our first operation corresponds to rewriting each data x in the original dataset D by 'verbalizing' it:\n$$x \\leftarrow VERBALIZE(x), \\forall x \\in D$$\nwhere VERBALIZE : S \u2192 S: is a function transforms all numerical features in the original data to its natural language-based representation. During this process, the semantic meaning of each feature will also be clearly indicated if they are not specified in the original data. For example, the sentence s='{1, 'married', 27}' will be rewritten as s'='{Gender=Female, Marriage status='married', Age=27}' to better convey the context of the data. This operation can be seen as the reverse operation of the previous NUMERIZE(\u00b7) operation.\nAmplifying the impact of prior. Our second operation is to add context information to the task"}, {"title": "4 Experiments", "content": "In this section, we evaluate the function modeling capabilities of LLMs in both synthetic and real-world tasks. We mainly focus on GPT-4 for our experiments, which was the most capable model during our evaluations, though our evaluation can fully be adapted to any other more recent models."}, {"title": "4.1 Synthetic data", "content": "Setup. We first focus on evaluating LLMs' ability to understand patterns in raw data, where no domain knowledge is available. Here, we consider 10 types of commonly seen 1D functions. For each of these functions, we generate a total of 25 samples D = {xi, yi}25i=1. We ask the language model to make predictions for different query points x' conditioned on the training examples.\nMain results. Unsurprisingly, we see that the performance of LLM (GPT-4 in this case) falls significantly below the performance of a simple MLP, where the gap in performance scales with the complexity of the dataset. While LLMs can model simple functions such as linear and quadratic functions accurately, they struggle in modeling more complex functions such as periodic functions (Fig. 3d) or composited functions (Fig. 3j). This highlights that language models struggle to model functions directly from raw data (except in the simplest of cases), which might be under-represented in the pretraining dataset (McCoy et al., 2023). Such inability may also be attributed to the tokenization process in language models, which can split numbers in a way such that they become ill-suited for further computation (Spathis and Kawsar, 2024)."}, {"title": "4.2 Income prediction", "content": "Setup. We next consider a prediction task in social-economical study. The task here is to ask LLM to predict the income of an individual in the US given their personal information. The data x in this task consists of 13 features x = {x1, ..., x13} describing e.g. the age, occupation and education of the individual, and is taken from the UCI Adult"}, {"title": "4.3 CO2 emission level modeling", "content": "Setup. We further consider a function modeling task in climate science. In this task, we would like to ask LLM to predict the CO2 concentration level y \u2208 R given the time x \u2208 (1975, 2000). The data is collected in the Mauna Loa Observatory (Carbon Dioxide Research Group, 2004). The underlying function f: R \u2192 R is one-dimensional.\nIn order to reduce the impact of potential test set memorization (Oren et al.), we similarly employ techniques to update the dataset, including (a) hiding the information about the exact observatory that this data was collected from; (b) add random Gaussian noise \u0454 ~ N(\u20ac; 1, 10\u22122) to the measurements; and (c) shift the data by 1 unit, creating an unseen dataset.\nWe use data up to year 1980 as our training set (used via in-context learning), and use the data between year 1990 and year 1992 as our test set."}, {"title": "5 Conclusion", "content": "In this work, we introduced a novel evaluation framework to systematically assess the function modeling capabilities of Large Language Models (LLMs). By disentangling their ability to understand raw data patterns from their ability to leverage prior knowledge, we identified both strengths and weaknesses in LLMs for function modeling tasks, namely (a) they struggle to understand functions based on just raw data, except for the simplest cases, and (b) their true strength lies in incorporating domain knowledge. Our research provides a foundation for the reliable and effective applications of LLMs in real-world prediction tasks.\nOur research suggests that future advancements in LLMs may benefit from explicitly enhancing their ability to understand raw data patterns during pretraining, which would significantly expand their applicability to numerical prediction tasks. Additionally, the development of powerful multimodal models that can simultaneously process numerical data and understand contextual information presents a promising research direction for future."}, {"title": "Limitation", "content": "While our evaluation techniques are broadly applicable to any language model, they are predominantly based on GPT-4 due to space constraints, as it was the most powerful model at the time of writing. Additionally, the experimental results may vary over time due to model updates. We therefore advise readers to interpret our findings cautiously, though our evaluation method can be fully reused for assessing future models.\nFurthermore, our current evaluation focuses on the case of in-context learning, and is aligned with prior work in this space. We consider the evaluation of finetuned models to be an exciting avenue for the future.\nFinally, as noted in the literature, LLM outputs are highly dependent on input prompts (Lester et al., 2021). Therefore, we assume the exact results can vary to a moderate extent based on the prompting technique used. However, we expect our findings to generalize across these different prompting techniques."}]}