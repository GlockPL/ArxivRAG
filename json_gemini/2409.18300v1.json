{"title": "SOAR: Self-supervision Optimized UAV Action Recognition with Efficient Object-Aware Pretraining", "authors": ["Ruiqi Xian", "Xiyang Wu", "Tianrui Guan", "Xijun Wang", "Boqing Gong", "Dinesh Manocha"], "abstract": "We introduce SOAR, a novel Self-supervised pretraining algorithm for aerial footage captured by Unmanned Aerial Vehicles (UAVs). We incorporate human object knowledge throughout the pretraining process to enhance UAV video pretraining efficiency and downstream action recognition performance. This is in contrast to prior works that primarily incorporate object information during the fine-tuning stage. Specifically, we first propose a novel object-aware masking strategy designed to retain the visibility of certain patches related to objects throughout the pretraining phase. Second, we introduce an object-aware loss function that utilizes object information to adjust the reconstruction loss, preventing bias towards less informative background patches. In practice, SOAR with a vanilla ViT backbone, outperforms best UAV action recognition models, recording a 9.7% and 21.4% boost in top-1 accuracy on the NEC-Drone and UAV-Human datasets, while delivering an inference speed of 18.7ms per video, making it 2x to 5x faster. Additionally, SOAR obtains comparable accuracy to prior self-supervised learning (SSL) methods while requiring 87.5% less pretraining time and 25% less memory usage. Extended tech report, code, and video can be found at https://gamma.umd.edu/researchdirections/aerialvideos/soar.", "sections": [{"title": "I. INTRODUCTION", "content": "Unmanned Aerial Vehicles (UAVs) equipped with cameras offer distinct advantages for capturing visual data in remote and challenging environments [1]. These systems are used for different applications, such as human detection [2], tracking [3], action recognition [4], [5], and surveillance [6], [7]. UAVs enable the collection of video sequences for analyzing human actions [8], poses [9], identities [10], and attributes [11], which aid decision-making and subsequent processes [12], [13]. However, UAV footage presents unique challenges in terms of perception and action recognition, as compared to ground-based video, including (a) Small Human Subjects. Due to the high altitude of UAVs, human figures occupy only a small fraction of the video frames, as shown in Figure. 1. For instance, in the challenging UAV-Human dataset [14], human subjects cover less than 5% of the frame on average, making it difficult for models to capture fine details of human movement and increasing the risk of over-reliance on background features. (b) Limited Labeled Data. Obtaining high-quality labeled data for UAV-based perception tasks is particularly challenging. The unique viewing angles, moving cameras, and small human subjects complicate the annotation process [15], [16], [14], [17], making it difficult to generate robust datasets. Even the largest UAV dataset, UAV-Human, contains only 22k videos, which is significantly smaller compared to normal video datasets like Kinetics [18], which has over 300k videos. This data scarcity further hampers the training of deep learning models for UAV-based human action recognition. These challenges necessitate specialized algorithms to understand human behavior accurately from UAV video data, considering its distinct features for effective performance.\nRecent research [19], [20], [21], [22] has shown that object-centric approaches in UAV video analysis, particularly those focusing on human subjects and regions of interest (ROIs), can significantly boost recognition performance. However, these methods typically incorporate object knowledge during the fine-tuning stage, often requiring additional steps such as generating bounding boxes or feature alignment, which can increase computational demands and slow down inference. In contrast, we propose leveraging object knowledge during the self-supervised pretraining phase, allowing for fine-tuning on downstream tasks without the need for extra procedures, resulting in a simpler, more efficient inference process.\nMain Contributions: In this paper, we introduce SOAR, a novel approach that leverages object knowledge within videos to enhance the the self-supervised pre-training phase through masked autoencoding (MAE) [23], [24]. Our method specifically addresses the challenge posed by UAV videos, where human subjects occupy only a small portion of the visual field. Our main contributions include:\n\u2022 We propose an innovative object-aware masking strategy that leverages the object information to guide the masking process. It ensures the preservation of patches associated with objects during the pre-training phase so that the model can learn spatiotemporal patterns relevant to the objects more effectively and efficiently."}, {"title": "II. RELATED WORK", "content": "Action Recognition for UAV Videos. Deep learning has significantly improved action recognition in ground-based videos [25], [26], [27], [28], [29], [30], [31] but faces challenges in UAV videos due to factors like camera movement, varied viewpoints, and small object sizes [32]. Approaches using 2D CNNs, such as ResNet [33] and MobileNet [34], process individual frames and fuse the results [35], [8], [36], while dual-stream CNN models capture both motion and appearance [4], [37]. To tackle temporal complexities, 3D CNNs [38], [14], [8], [39] have been used to analyze spatial-temporal dimensions. Recently, methods like AZTR [19] combine CNNs with attention mechanisms for resource-efficient action recognition. Other techniques, such as Fourier-based attention [21], [40], enhance motion salience. In contrast, our method optimizes pretraining to streamline the process, using raw RGB data without adding complexity during fine-tuning or inference.\nObject-based Video Representation. Using object details in video recognition is a growing trend [41], [42], with techniques incorporating RoI features [43] and off-the-shelf detectors in feature banks [44], [45]. Advances in transformers, such as ORVIT [46], have led to object-aware representations through cropped object tokens [47] or object-to-pixel transformers [48]. Some models even omit visual inputs entirely [49], [50]. However, these methods primarily focus on general videos, not UAV-specific data with broader fields of view and irrelevant objects. Our approach, instead, leverages human object information in the pretraining phase, addressing the specific challenges of UAV videos.\nMasked Visual Modeling. Masking techniques have evolved from early autoencoders [51] to recent Transformer-based models like BEiT [52] and VideoMAE [24], which mask tokens for visual learning. VideoMAE introduced tube masking to increase reconstruction difficulty [24], followed by innovations like MAR [22] to reduce training costs and VideoMAE V2 [53] to scale pretraining efficiency. Some strategies use motion-guided masking for temporal consistency [54]. Our approach, however, is simpler and more memory-efficient, focusing on human-object bounding boxes rather than techniques like optical flow, making it particularly suited to the UAV context with its dynamic camera movements."}, {"title": "III. METHODOLOGY", "content": "In this section, we introduce our proposed SOAR method. First, we discuss how UAV video analysis entails a long-tailed learning issue and how its characteristics challenge existing masked autoencoding methods in Section III-A. We then present an overview of our proposed SOAR in Section III-B. Our novel-designed object-aware masking and object-aware loss are further explained in Section III-C and Section III-D, respectively."}, {"title": "A. Problem Formulation", "content": "Pretraining UAV videos using masked autoencoders, where self-supervision relies on reconstructing masked patches, faces a unique challenge due to the UAV data's inherent imbalance. Unlike traditional video datasets, UAV data presents a long-tailed distribution in the following sense. The tokens (or video patches) related to human (tail) form a small portion compared to the dominant background (head).\nThis imbalance creates a long-tailed learning problem. The model, exposed to significantly more head class data, prioritizes learning from background features, neglecting crucial information from the under-represented tail class. This bias hinders the model's ability to perform well on tasks like human action recognition, where the key information lies in the spatiotemporal patterns of human motion that are primarily contained within the tail class data.\nTo address this challenge, we explore how object knowledge can benefit UAV video pre-training. We focus on two key questions:\n\u2022 Balanced Patch Selection. How to strategically select unmasked patches for a balanced distribution between human-related and background tokens during pre-training?\n\u2022 Mitigating Bias. How to utilize object knowledge to guide the model towards learning from patches related to the human object, reducing bias towards the dominant background?\nAs UAV videos have a larger field of view, containing many objects that are not related to the actions, our approach focuses solely on human objects."}, {"title": "B. Overall learning Method", "content": "Our proposed method, SOAR, processes both video data and object detection information. As illustrated in Figure 2, SOAR masks random video patches and reconstructs the missing ones using an asymmetric encoder-decoder architecture, while also leveraging object information to optimize the reconstruction process. Note that, The input detections come from widely available off-the-shelf detectors with no finetuning on downstream datasets.\nThe process begins by splitting an input video $V \\in \\mathbb{R}^{T \\times C \\times H \\times W}$, where $T, C, H, W$ denote the number of frames, channels, height, and width, into non-overlapping patches $P = \\{P_i | P_i \\in \\mathbb{R}^{t \\times C \\times h \\times w}\\}^N_{i=1}$, with $N = \\frac{T}{t} \\times \\frac{W}{w}$ being the total patch count. These patches are converted into a sequence of tokens $K = \\{K_i | K_i \\in \\mathbb{R}^{D}\\}^N_{i=1}$ through patch embedding and positional encoding.\nAn object-ness score map is generated from the input detections to produce a binary mask $M$, identifying visible patches for reconstruction through our object-aware masking strategy (detailed in Section. III-C. The encoder $\\Phi_{enc}$, a ViT with space-time attention, processes visible tokens $K_{vis}$ to produce latent features $F = \\Phi_{enc}(K_{vis}) \\in \\mathbb{R}^{N_{vis} \\times D}$. The latent feature $F$ and the masked tokens $K_{inv}$ are then concatenated into one sequence and decoded by $\\Phi_{dec}$, a shallower ViT, to reconstruct the video $V$.\nThe object-ness score map is also used to create our proposed object-aware loss function, described in Section. III-D. Training minimizes the Object-aware loss between the original $V$ and reconstructed $V'$ over masked areas. The encoder is then applied for fine-tuning in downstream action recognition tasks."}, {"title": "C. Object-Aware Masking", "content": "In this section, we introduce our object-aware masking strategy which utilizes the human object information to achieve a balanced patch selection between human-related and background tokens. Besides the input video $V \\in \\mathbb{R}^{T \\times C \\times H \\times W}$, we include human object detections $B = \\{\\{b_{t,i}\\}_{i=1}^{N_t}\\}_{t=1}^{T}$ where $b_{t,i} = (x_{t,i}, y_{t,i}, x'_{t,i}, y'_{t,i})$ is the bounding box of the $i$-th human object in frame $t$ with $(x_{t,i}, y_{t,i}), (x'_{t,i}, y'_{t,i})$ representing the center and size of the bounding box. The bounding box can be oracle boxes from annotation for analytic benchmarks or, in our case, from an off-the-shelf object detector [55]. By incorporating this human object detection information, our method aims to achieve an informed masking strategy. Note that we do not need object detection during inference for efficiency.\nWe generate a continuous pixel-wise objectness score heatmap. This map reflects the spatial proximity of each pixel to the center of the bounding boxes associated with the detected human objects. Essentially, it transforms objects in the videos into a single, class-agnostic heatmap. Specifically, we first initialize a heatmap for every bounding box $b_{t,i} \\in B$ in the original image resolution but with all pixel values equal to zero. Then, we introduce a 2D Gaussian with the same center and \u201csize\u201d as the bounding box into the heatmap by discretizing it into $x, y$ points along the $x, y$-axis. Finally, we sum all the heatmaps for every bounding box and every frame and normalize their values along the temporal dimension to obtain the overall pixel-wise objectness score heatmap $H \\in \\mathbb{R}^{H \\times W}$:\n$H = \\frac{1}{T} \\sum_{t}^{T} \\sum_{i}^{N_t} exp(\\frac{-(x - x_{t,i})^2 + (y - y_{t,i})^2}{2\\sigma^2})$\nwhere $\\sigma$ is the standard deviation of the Gaussian that controls the peak and radius of the Gaussian.\nNext, we derive patch-level objectness from the pixel-level objectness heatmap $H$ by segmenting it into patches matching our video patch size and summing up pixel scores within each patch. The compiled patch-level objectness heatmap, $\\mathcal{H} \\in \\mathbb{R}^{N}$, essentially reflects the total duration or prevalence of a human object appearing within that specific patch throughout the video.\nAs mentioned in Section. III-A, ensuring a balanced distribution of unmasked patches is crucial. As illustrated in Figure 3, we begin by sorting all the patches based on their corresponding objectness scores and partition the sorted patches into $(1 - \\rho)(W \\times H)$ segments of equal length. Within each segment, we randomly select one patch to be unmasked, while all remaining patches in that segment are masked. The resulting spatial mask map is replicated $T$ times along the temporal dimension to account for video sequences. Finally, the mask is flattened into a one-dimensional binary mask $M \\in \\mathbb{R}^{N}$.\nThis object-aware masking strategy lowers the probability of masking human object-related patches and guarantees a minimum number of human-related patches remain unmasked. This adaptive approach helps maintain the visibility of crucial human movement information during the pre-training so that the encoder can learn more spatiotemporal patterns related to the human object."}, {"title": "D. Object-Aware Loss", "content": "In this section, we introduce our object-aware loss function, designed to leverage object-centric knowledge and reduce the model's bias towards the dominant background. Like previous methods [24], [53], we start with the Mean Squre Error (MSE) loss, assessing the difference between the original and reconstructed videos at masked locations. Our innovation adjusts this loss by including patch-level objectness scores for a more equitable training approach.\nWe overlook unmasked tokens, irrelevant for reconstruction, and focus on the objectness scores $S_{inv}$ for masked patches. To ensure that background patches contribute minimally to the loss as they still contain valuable environmental information, we add the mean $\u03bc$ of all objectness scores to each score. This prevents masked patches with zero objectness scores (pure background) from being entirely ignored during training. Then, we normalize the objectness scores by their sum, making their sum equal to 1. These normalized scores then act as weights to re-balance the MSE loss:\n$L_{new} = - \\sum_{i \\in K_{inv}} \\frac{S_{inv} + \\mu}{\\sum_{i \\in K_{inv}}(S_{inv} + \\mu)} ||V_i - V'_i||_2$\nwhere $S_{inv}^i$ is the objectness score corresponding to the $i$-th invisible patch. By integrating the objectness scores, this refined loss function steers the model's focus towards regions containing objects, particularly human objects in UAV videos. This encourages a more balanced learning dynamic by mitigating the bias towards the dominant background and directing the model to prioritize learning from crucial object-related information."}, {"title": "IV. EXPERIMENTS", "content": "A. Datasets\nUAV-Human [14], the most comprehensive UAV dataset for human behavior analysis, contains 22,476 HD videos from diverse indoor and outdoor environments with 155 annotated actions. It presents challenges like dynamic backgrounds and varying lighting conditions.\nNEC-Drone [39], consists of 5,250 videos with 16 actions captured in a basketball court using a low-altitude UAV with light reflection noise despite consistent lighting."}, {"title": "B. Implementation Details", "content": "The results presented in this section are based on either a 12-layer ViT-Base or a 24-layer ViT-Large model for the encoder, with an 8-layer narrow ViT as the decoder. Both the encoder and decoder are initialized with Kinetics400 weights, pretrained for 800 epochs. During pretraining, the model is further trained on UAV datasets for 400 epochs, using 16 frames at a resolution of 224\u00d7224. Following pretraining, the encoder is fine-tuned for 100 epochs on downstream tasks. Dense sampling is applied during fine-tuning, and results are reported using a uniform inference protocol of 5 clips \u00d7 3 crops."}, {"title": "C. Main Results and Analysis", "content": "Comparison with State-of-the-Art Supervised Meth-ods. We compared SOAR with current state-of-the-art methods on the NEC-Drone and UAV-Human datasets, as summarized in Table I. SOAR sets a new benchmark in UAV video analysis, significantly outperforming previous supervised methods on both datasets. With the ViT-B backbone, SOAR achieved top-1 accuracy improvements of 3.9% on NEC-Drone and 11.4% on UAV-Human. The performance gains were even more substantial with the ViT-L backbone, delivering a 9.7% boost on NEC-Drone and an impressive 21.4% increase on UAV-Human.\nSOAR demonstrates the power of self-supervised video pretraining, where models learn from unlabeled videos before fine-tuning on labeled tasks. This approach reduces dependency on large, annotated datasets and allows the model to autonomously discover meaningful visual features, resulting in a deeper understanding of video semantics and improved performance on downstream tasks such as action recognition.\nComparison with State-of-the-Art Self-Supervised Methods. We compared SOAR with other self-supervised methods on the NEC-Drone and UAV-Human datasets (see Table I). With a ViT-B backbone, SOAR significantly outperforms previous video mask autoencoders, achieving 2.1% and 4.3% higher top-1 accuracy on the NEC-Drone and UAV-Human datasets, respectively. When scaled up to a ViT-L backbone, SOAR continues to excel, showing 2.0% and 5.2% accuracy improvements over previous SOTAS on NEC-Drone and UAV-Human, demonstrating consistent scalability.\nNeed to note that, VideoMAE v2 with the considerably larger ViT-G backbone shows limited performance gains. This could be attributed to its dual masking strategy, which significantly increases the likelihood of overlooking human-related patches during reconstruction.\nThese results highlight the importance of object-aware pretraining in UAV video analysis. Leveraging human object knowledge during pretraining significantly enhances performance, revealing the limitations of approaches that neglect object focus.\nInference Time. We evaluated the inference speed of our SOAR model against two state-of-the-art methods, AZTR [19] and MITFAS [20], using an RTX A5000 GPU. As shown in Table II, SOAR achieves significantly faster inference times, processing videos in 18.7 milliseconds per video-2 times faster than AZTR and 5 times faster than MITFAS-while maintaining superior accuracy.\nSOAR's efficiency in the inference stage stems from its strategic use of object information during pre-training. Unlike other methods that require additional computational steps during inference, such as online object detection and data augmentation (AZTR) or feature alignment (MITFAS), SOAR only uses bounding boxes in pre-training. This allows it to directly process unmodified video frames during inference, resulting in substantially reduced processing time and making SOAR ideal for real-time applications.\nMask Ratio. We investigate the impact of different mask ratios on UAV video pretraining, with results shown in Figure 4. In contrast to previous studies [53], [24], we found that using a high mask ratio (90%-95%) does not necessarily improve fine-tuning performance for downstream UAV action recognition tasks. This is due to the smaller size of human subjects in UAV footage, limiting the model's ability to learn meaningful human-centric semantics and motion.\nOur experiments show that a mask ratio of around 70% provides the best pretraining performance for UAV videos. This ratio offers a balance between challenging the model with partial views and ensuring sufficient visibility of human-related information to effectively capture human dynamics. These findings highlight the need for mask ratios tailored to the unique characteristics of UAV video analysis, rather than relying on the general approach commonly applied in masked autoencoding.\nMemory Efficiency. High mask ratios in UAV video pretraining reduce memory usage due to fewer unmasked tokens needing processing. As Figure 4 demonstrates, SOAR is particularly adept at exploiting this benefit. SOAR achieves comparable accuracy to previous SOTA, even with a substantially higher mask ratio. For instance, SOAR reaches 82.6% accuracy with only 7.5% of tokens unmasked (92.5% mask ratio), while VideoMAE requires 30% unmasked tokens (70% mask ratio) for a similar result. Thus, SOAR is highly memory-efficient, requiring only 25% of the memory compared to VideoMAE, attributed to its strategic focus on critical human-related tokens at higher mask ratios.\nTime Efficiency. Our investigation highlights SOAR's superior pre-training time efficiency compared to previous methods, as shown in Figure 5. SOAR demonstrates faster convergence and requires significantly fewer training epochs to achieve comparable results. For instance, on the NEC-Drone dataset, SOAR reaches 81.7% top-1 accuracy within just 50 epochs, while VideoMAE requires 87.5% more epochs (400) to attain a similar 81.2% accuracy. This efficiency is driven by SOAR's object-aware masking and loss function, which prioritize human-centric information during training. By focusing on key human elements, SOAR accelerates the learning process and enhances pre-training effectiveness for UAV action recognition tasks."}, {"title": "D. Ablation Studies", "content": "Effectiveness of Both Designs. In a step-wise evaluation detailed in Table IIIa, we analyzed the impact of the object-aware masking strategy (OAM) and the object-aware loss function (OAL) by pre-training two models: one using only OAM and another using both OAM and OAL. The results demonstrate the effectiveness of each component, with OAM alone boosting accuracy by 0.5% on NEC-Drone and 1.9% on UAV-Human. The addition of OAL further improved accuracy, yielding an additional 1.1% on NEC-Drone and 2.4% on UAV-Human. These findings highlight the combined contributions of OAM and OAL in enhancing performance for UAV action recognition tasks.\nComparison with Other Masking Strategies. We compared our object-aware masking strategy against conventional methods such as random, tube, and block masking, evaluating both reconstruction quality and action recognition performance. As shown in Table IIIb, our approach consistently outperforms these traditional methods.\nImpact of Bounding Box Quality. Table IIIc presents additional results using different off-the-shelf detectors (trained on CrowdHuman and zero-shot on UAVHuman). While the quality of the bounding boxes influences the final accuracy, our method consistently outperforms previous approaches that do not incorporate object cues.\nMore results and analysis are in the tech report on the project page."}, {"title": "V. CONCLUSION, LIMITATIONS, AND FUTURE WORK", "content": "In this paper, we introduced SOAR, a novel approach that leverages object knowledge to optimize the video pre-training process, leading to improved performance in downstream action recognition tasks for UAV videos. SOAR incorporates two key innovations: an object-aware masking strategy and an object-aware loss function. Empirical results show that SOAR effectively sets new benchmarks on the NEC-Drone and UAV-Human datasets, demonstrating its simplicity and effectiveness for UAV video analysis.\nDespite its strong performance, our method has certain limitations. Currently, SOAR is designed specifically for transformer architectures, as the masking and reconstruction processes operate at the token level. Expanding SOAR to more edge-device-friendly architectures, such as CNNs, is a promising direction for future work. Additionally, further evaluation and tighter integration with UAV hardware will be necessary to assess SOAR's practicality and efficiency in real-world UAV systems."}]}