{"title": "Point2RBox-v2: Rethinking Point-supervised Oriented Object Detection with Spatial Layout Among Instances", "authors": ["Yi Yu", "Botao Ren", "Peiyuan Zhang", "Mingxin Liu", "Junwei Luo", "Shaofeng Zhang", "Feipeng Da", "Junchi Yan", "Xue Yang"], "abstract": "With the rapidly increasing demand for oriented object detection (OOD), recent research involving weakly-supervised detectors for learning OOD from point annotations has gained great attention. In this paper, we rethink this challenging task setting with the layout among instances and present Point2RBox-v2. At the core are three principles: 1) Gaussian overlap loss. It learns an upper bound for each instance by treating objects as 2D Gaussian distributions and minimizing their overlap. 2) Voronoi watershed loss. It learns a lower bound for each instance through watershed on Voronoi tessellation. 3) Consistency loss. It learns the size/rotation variation between two output sets with respect to an input image and its augmented view. Supplemented by a few devised techniques, e.g. edge loss and copy-paste, the detector is further enhanced. To our best knowledge, Point2RBox-v2 is the first approach to explore the spatial layout among instances for learning point-supervised OOD. Our solution is elegant and lightweight, yet it is expected to give a competitive performance especially in densely packed scenes: 62.61%/86.15%/34.71% on DOTA/HRSC/FAIRIM.", "sections": [{"title": "1. Introduction", "content": "Emerged as an essential task in computer vision, oriented object detection (OOD) has become a prominent demand in autonomous driving [6], aerial images [7, 30, 48, 51, 52], scene text [23, 28, 71], retail scenes [10, 35], industrial inspection [29, 47], and more.\nManual annotations are essential to teach the detector new concepts of visual objects. Early research is often supervised."}, {"title": "2. Related Work", "content": "Motivation. While point-supervised OOD has gained attention, the utilization of relationships among instances remains absent in the literature. For example, the arrangement of vehicles in a parking lot can serve as effective constraints for learning their size and orientation. This approach may yield valuable insights into point-supervised OOD, particularly in densely packed scenes where current methods face significant challenges. Can we leverage the spatial layout of objects to enhance point-supervised OOD? In this paper, we dive into this idea and seek to answer this question.\nHighlights. 1) Point2RBox-v2 is proposed for point-supervised OOD, advancing the state of the art as displayed in Fig. 2 and Tables 1-2. 2) We propose novel and elegant losses to enforce constraints from the spatial layout among instances based on Gaussian overlap and Voronoi tessellation [1]. 3) Other modules are devised (i.e. edge loss, consistency loss, copy-paste) to further enhance the method.\nContributions. 1) To our best knowledge, this work is the first attempt to learn point-supervised OOD from the layout among objects, where we propose novel principles based on Gaussian overlap and Voronoi tessellation. 2) The training pipeline and detailed implementation are elucidated, with necessary modules (i.e. edge loss, consistency loss, copy-paste) incorporated. The source code will be made publicly available. 3) Extensive experiments demonstrate that leveraging the spatial layout of instances can significantly advance the state of the art, surpassing other alternatives in accuracy."}, {"title": "2.1. RBox-supervised Oriented Detection", "content": "In addition to horizontal detection [26, 70], oriented object detection (OOD) [46, 52] has received extensive attention. Representative works include anchor-based detector Rotated RetinaNet [25], anchor-free detector Rotated FCOS [42], and two-stage solutions, e.g. RoI Transformer [5], Oriented R-CNN [49], and ReDet [11]. Some research enhances the detector by exploiting alignment features, e.g. R\u00b3Det [55] and S2A-Net [12]. The angle regression may face boundary discontinuity and remedies are developed, including modulated losses [37, 53, 58] that alleviate loss jumps, angle coders [50, 51, 54, 63] that convert the angle into boundary-free coded data, and Gaussian-based losses [34, 56, 57, 60, 61] transforming rotated bounding boxes into Gaussian distributions. RepPoint-based methods [15, 20, 62] provide alternatives that predict a set of points that bounds the spatial extent of an object. LMMRotate [19] is a new paradigm of OOD based on multimodal language model and performs object localization through autoregressive prediction."}, {"title": "2.2. Point-supervised Oriented Detection", "content": "Recently, several methods for point-supervised oriented detection have been proposed: 1) P2RBox [2], PMHO [68], and PointSAM [27] propose oriented detection with point prompts by employing the zero-shot Point-to-Mask ability"}, {"title": "2.3. Other Weakly-supervised Settings", "content": "of SAM [17]. 2) Point2RBox [66] introduces an end-to-end approach based on knowledge combination. 3) PointOBB [33, 67] achieves RBox generation through scale consistency and multiple instance learning. 4) PointOBB-v2 [38] learns a class probability map to generate pseudo RBox labels.\nAmong these methods, P2RBox, PMHO, and PointSAM rely on the SAM model pre-trained on massive labeled datasets, whereas Point2RBox requires one-shot examples for each category. In contrast, PointOBB series do not use many priors, but they necessitate two-stage training.\nCompared to Point-to-RBox, some other settings have been better studied. These methods are potentially applicable to our Point-to-RBox task setting by using a cascade pipeline, such as Point-to-HBox-to-RBox. In our experiment, cascade pipelines powered by state-of-the-art approaches are also compared. Here, representative works are introduced.\nHBox-to-RBox. H2RBox [59] establishes a paradigm that limits the object to a few candidate angles through geometric constraint from HBoxes, with a self-supervised branch eliminating the undesired results. An enhanced version H2RBox-v2 [65] is proposed to leverage the reflection symmetry of objects to further boost the accuracy. EIE-Det [45] uses an explicit equivariance branch for learning rotation consistency, and an implicit equivariance branch for learning position, aspect ratio, and scale consistency. KCR [73] combines RBox- and HBox-annotated datasets for transfer learning. Some studies [16, 41] use additional annotated data for training, which are also attractive but less general.\nPoint-to-HBox. Several related approaches have been developed, including: 1) P2BNet [3] samples box proposals of different sizes around the labeled point and classify them to achieve point-supervised horizontal object detection. 2) PSOD [8] achieves point-supervised salient object detection using an edge detector and adaptive masked flood fill.\nPoint-to-Mask. Point2Mask [21] is proposed to achieve panoptic segmentation using single point annotation per target. SAM (Segment Anything Model) [17] produces object masks from input point/HBox prompts. Though RBoxes can be obtained from the segmentation mask by finding the minimum circumscribed rectangle, such a complex pipeline can be less cost-efficient and perform worse [59, 65]."}, {"title": "3. Method", "content": "3.1. Overview and Preliminary\nAn overview of Point2RBox-v2 is illustrated in Fig. 3. The network is based on ResNet50 [13] backbone, FPN [24] head, and PSC [64] angle coder. Objects of varying sizes are typically assigned to different FPN layers based on the scale. However, points lack size, so we assign them all to the P3 layer (stride = 8). Assume the detector $f_{nn}(\\cdot)$ maps an image $I$ to a set of RBoxes as detection results:\n$(x_c, y_c, w, h, \\theta) = f_{nn}(I)$  (1)\nEquivalently, oriented objects can also be represented by 2D Gaussian distributions $\\mathcal{N}(\\mu, \\Sigma)$ [60]:\n$\\left\\{\\mu = \\left[\\begin{array}{c}x_c\\\\ y_c\\end{array}\\right], \\quad \\Sigma = R\\left[\\begin{array}{cc}w / 2\\\\ 0\\end{array}\\begin{array}{cc}0\\\\ h / 2\\end{array}\\right]^2 R^T\\right\\}$  (2)\nwhere\n$R = \\left[\\begin{array}{cc}\\cos \\theta & -\\sin \\theta\\\\ \\sin \\theta & \\cos \\theta\\end{array}\\right]$  (3)\nAt the core of Point2RBox-v2 is to utilize the constraints from the layout, which is achieved by Gaussian overlap loss (Sec. 3.2) and Voronoi watershed loss (Sec. 3.3). These two losses effectively limit the size and rotation of objects to a reasonable range. Upon that, edge loss (Sec. 3.4) aligns the bounding box with the edge of objects to improve the accuracy. Incorporated with symmetry-aware learning (Sec. 3.5) and copy-paste augmentation (Sec. 3.6), we achieve a"}, {"title": "3.2. Gaussian Overlap Loss", "content": "As mentioned, oriented objects can be represented by 2D Gaussian distributions $\\mathcal{N}(\\mu, \\Sigma)$. The overlap volume between two distributions $\\mathcal{N}_1(\\mu_1, \\Sigma_1)$ and $\\mathcal{N}_2(\\mu_2, \\Sigma_2)$ can be approximated by the Bhattacharyya coefficient [60] as:\n$B(\\mathcal{N}_1, \\mathcal{N}_2) = \\exp\\left(-\\frac{1}{8} \\mu^T \\Sigma^{-1} \\mu+\\frac{1}{2} \\ln \\frac{|\\Sigma|}{|\\Sigma_1|^{1 / 2}|\\Sigma_2|^{1 / 2}}\\right)$  (4)\nwhere $\\mu = \\mu_2 - \\mu_1$, $\\Sigma = (\\Sigma_1 + \\Sigma_2)$, $|\\Sigma|$ denotes the determinant of the covariance matrix.\nBased on the above equation, we build a Gaussian overlap matrix $M \\in \\mathbb{R}^{N \\times N}$ for each scene image as:\n$M_{i, j}=B\\left(\\mathcal{N}_i, \\mathcal{N}_j\\right)$  (5)\nwhere $i, j=1,2, \\ldots, N$; $N$ is the distribution count (or the instance count) within one training image.\nThe Gaussian overlap loss can then be expressed as:\n$\\mathcal{L}_O = \\sum_{i \\neq j} \\frac{1}{N^2}(M^{-1})_{i j}$  (6)\nwhere $i \\neq j$ omits diagonal elements. With this loss, the detector learns to arrange instances (see Fig. 4a) based on the mutual exclusivity among instances."}, {"title": "3.3. Voronoi Watershed Loss", "content": "A Voronoi diagram [1] is a partitioning of a space based on a set of points. Point annotations can be effectively utilized to calculate a Voronoi diagram, where a distinct polygon region is assigned to each point-annotated instance. The watershed algorithm [44], on the other hand, is a region segmentation technique that treats the intensity of pixels as a topographic surface, identifying regions as \u201ccatchment basins\u201d.\nThe calculation of Voronoi ridges can be formulated as:\n$V = \\operatorname{Voronoi}(X)$  (7)\nwhere $X$ are the annotated points within a training image; $V$ are the output Voronoi ridges (pixel coordinates).\nInterestingly, we find that Voronoi diagrams can be utilized as initial markers for watershed to obtain a region for each instance (see Fig. 4b). In concrete terms, the points $X$ can be employed as foreground markers, while the Voronoi ridges $V$ can act as background boundaries:\n$S = \\operatorname{Watershed}(I, X, V)$  (8)\nwhere $S$ are the output basin regions (pixel coordinates) corresponding to each annotated instance. By rotating $S$"}, {"title": "3.4. Edge Loss", "content": "to align with the direction of the current prediction, the regression target of width and height can be expressed as:\n$\\left[\\begin{array}{c}w_t\\\\ h_t\\end{array}\\right]=2 \\max \\left|R^T \\left(S-\\left[\\begin{array}{c}x_c\\\\ y_c\\end{array}\\right]\\right)\\right|$  (9)\nwhere $\\left(x_c, y_c, w, h, \\theta\\right)$ is the current prediction; $R$ is defined by Eq. (3); $w_t$ and $h_t$ are detached to stop the gradient.\nAfterward, the Voronoi watershed loss to regress the width and height of objects can be calculated as:\n$\\mathcal{L}_{W} = \\mathcal{L}_{G W D}\\left(\\left[\\begin{array}{c}w / 2\\\\ 0\\end{array}\\begin{array}{c}0\\\\ h / 2\\end{array}\\right], \\left[\\begin{array}{c}w_t / 2\\\\ 0\\end{array}\\begin{array}{c}0\\\\ h_t / 2\\end{array}\\right]\\right)$  (10)\nwhere $\\mathcal{L}_{G W D}(\\cdot)$ is Gaussian Wasserstein Distance Loss [60].\nThe above two losses have limited the size to a reasonable range. To make it more accurate, we propose the edge loss to snap the boundaries toward the edges (see Fig. 4c).\nFirst, a region $P \\in \\mathbb{R}^{(2 K+1) \\times(2 K+1)}$ around each predicted RBox is extracted via Rotated RoI Align [14]:\n$P = \\operatorname{RoIAlign}\\left(E(I),\\left(x_c, y_c, \\beta w, \\beta h, \\theta\\right)\\right)$  (11)\nwhere $E(\\cdot)$ is the edge detection function [39]. We set $K=24$ and $\\beta=1.6$ in our experiments (see Table 9).\nBy calculating the sum of each row of $P$, the edge distribution in y direction can be obtained as:\n$\\mu_i=\\sum_{j=1}^{2 K+1}\\left(P_{(K+1-i), j}+P_{(K+1+i), j}\\right)$  (12)\nwhere $i=1,2, \\ldots, K$, indicating that the upper half of $P$ is reversed and added to the lower half.\nMeanwhile, the current prediction of the edge can also be softened into a distribution as:\n$\\lambda_i=\\exp \\left(-\\frac{(i-K / 2)^2}{2 \\sigma^2}\\right)$  (13)\nwhere $i=1,2, \\ldots, K ; \\sigma$ is set to 6 . Note that we crop $P$ based on the predicted RBox, thus $\\lambda$ is always the same.\nMultiplying the two distributions yields the target:\n$h_t=\\frac{\\beta h}{K} \\underset{i}{\\arg \\max }(\\mu \\times \\lambda)$  (14)\nwhere $h_t$ is the regression target of height. Likewise, the width target $w_t$ is calculated along x direction.\nThe edge loss is then obtained as:\n$\\mathcal{L}_E=\\operatorname{smoothL1}\\left(\\left[\\begin{array}{c}w\\\\ h\\end{array}\\right], \\left[\\begin{array}{c}w_t\\\\ h_t\\end{array}\\right]\\right)$  (15)\nNote that $\\mathcal{L}_{W}$ and $\\mathcal{L}_{E}$ merely refines the width and height of boxes, without involving the angle regression."}, {"title": "3.5. Symmetry-aware Learning", "content": "Proven effective in learning the rotation of objects [65], in this work, symmetry-aware learning is extended to Gaussian-based OOD. In the left part of Fig. 3, the training image $I$ is transformed (randomly selected from rotation, flip, and scale, see Table 7) to generate an augmented view as:\n$I_{a u g}=\\alpha I$  (16)\nwhere\n$\\alpha=\\left(\\underset{\\text { sin } R}{\\cos R} \\underset{\\cos R}{\\sin R}\\right)\\left[\\begin{array}{lll}p_1 & 0 & 0\\\\ 0 & p_2 & 0\\\\ 0 & 0 & p_3\\end{array}\\right]\\left[\\begin{array}{lll}s & 0 & 0\\\\ 0 & 1 & 0\\\\ 0 & 0 & 1\\end{array}\\right]$  (17)\nwhere $\\left(p_1, p_2, p_3\\right)=(1,0,0)$ when rotation is selected, $(0,1,0)$ when flip, $(0,0,1)$ when scale. $R$ and $s$ are the random amount of rotation and scale, $s \\in(0.5,0.9)$.\nBy feeding both $I$ and $I_{a u g}$ into the network, we obtain two sets of output Gaussian distributions and angles:\n$\\left\\{\\begin{array}{l}(\\Sigma, \\theta)=f_{n n}(I)\\\\ (\\Sigma_{a u g}, \\theta_{a u g})=f_{n n}\\left(I_{a u g}\\right)\\end{array}\\right.$  (18)\nThe consistency loss is calculated between the two sets so that the network learns the size and rotation variation:\n$\\mathcal{L}_{s s}=\\mathcal{L}_{G W D}\\left(\\alpha \\Sigma \\alpha^T, \\Sigma_{a u g}\\right)+\\mathcal{L}_{A N G}\\left(m \\theta+R, \\theta_{a u g}\\right)$  (19)\nwhere $R$ is the rotation angle and $m=1$ when rotation is selected; $(m, R)=(-1,0)$ when flip; $(m, R)=(1,0)$ when scale. $\\mathcal{L}_{G W D}(\\cdot)$ is Gaussian Wasserstein Distance Loss [60] and $\\mathcal{L}_{A N G}(\\cdot)$ is defined as:\n$\\mathcal{L}_{A N G}\\left(\\theta_1, \\theta_2\\right)=\\min _k\\left(\\operatorname{smoothL1}\\left(\\theta_1, k \\pi+\\theta_2\\right)\\right)$  (20)\nwhere $\\min (\\cdot)$ regresses the prediction toward the closest target to circumvent the periodicity problem [65]."}, {"title": "3.6. Copy-paste Augmentation", "content": "Inspired by [9], we propose to crop the detected instances of step $k$ and paste them on the training image of step $k+1$. The maximum number of paste boxes is limited to 10 in each step. We simply use the bounding boxes of the cropped instances as the regression targets, and use Gaussian Wasserstein Distance Loss [60] to calculate $\\mathcal{L}_{b o x}$."}, {"title": "3.7. Overall Loss", "content": "The overall loss $\\mathcal{L}$ for Point2RBox-v2 can be expressed as:\n$\\mathcal{L}=\\mathcal{L}_{c l s}+w_{b o x} \\mathcal{L}_{b o x}+w_O \\mathcal{L}_O+w_W \\mathcal{L}_W+w_E \\mathcal{L}_E+w_{s s} \\mathcal{L}_{s s}$  (21)\nwhere $\\mathcal{L}_{c l s}$ is the focal loss [25] for classification, $\\mathcal{L}_{b o x}$ regresses boxes/centers toward copy-paste/ground-truth labels, $w_{b o x}$ is set to one by default, $\\left(w_O, w_W, w_E, w_{s s}\\right)$ are set to $(10,5,0.3,1)$ based on our ablation studies (see Tables 3-5)."}, {"title": "4. Experiments", "content": "Experiments are carried out on NVIDIA RTX4090 GPUs using PyTorch 2.2.0 [36] and the rotation detection tool kits: MMRotate 1.0.0 [72]. All the experiments follow the same hyper-parameters (learning rate, batch size, optimizer, etc.)."}, {"title": "4.1. Main Results on DOTA-v1.0", "content": "Table 1 compares Point2RBox-v2 with the state-of-the-art methods, which can be categorized into two tracks:\n1) End-to-end training. These methods apply the trained weakly-supervised detector directly to the test set. Without relying on priors, our approach demonstrates an improvement of 16.93% (51.00% vs. 34.07%) compared to Point2RBox. Even when compared to Point2RBox+SK, which incorporates additional data-side priors (i.e. one-shot examples for each class), our method still outperforms it by 10.73% (51.00% vs. 40.27%).\n2) Two-stage training. These methods generate RBox"}, {"title": "4.2. Results on More Datasets", "content": "Table 2. The results are displayed in Table 2. On more challenging DOTA-v1.5/2.0, Point2RBox-v2 presents a similar trend, 23.47%/18.15% higher than PointOBB-v2 in the pseudogeneration track. On the ship detection dataset HRSC, the gap between Point2RBox-v2 and RBox-supervised FCOS is only 2.84% (86.15% vs. 88.99%). DIOR is relatively sparse, leading to less improvement with our methods\u2014lower than PointSAM (44.45% vs. 46.20%) but still higher than methods that do not use SAM. Our method also provides competitive performance on fine-grained datasets FAIR1M and STAR. In addition to remote sensing scenarios, we carry out experiments on SKU110K for densely packed retail scenes. Existing point-supervised methods struggle in this"}, {"title": "4.3. Ablation Studies", "content": "Edge loss parameters. We set $K=24$ and $\\beta=1.6$ as they are observed to discern the correct edges during code development. Table 9 provides a more precise ablation.\nAnnotation inaccuracy. We offset the annotated points by a noise from the uniform distribution $[-\\sigma H,+\\sigma H]$, where $H$ is the height of objects. Table 10 shows that the $A P_{50}$ of Point2RBox-v2 decreases by less than 3% when noise is added to point annotations, demonstrating the robustness of the proposed learning mechanisms."}, {"title": "4.4. More Discussions", "content": "The qualitative analysis on the failed/overlap cases is shown in Fig. 5. 1) Failed cases. Although our method performs well overall, it struggles with certain categories that are sparse and not constrained by other objects. 2) Overlap cases. Minimizing overlap as a soft constraint during training does not entirely eliminate overlap. Once trained, the model remains robust to some overlap during inference."}, {"title": "5. Conclusion", "content": "This paper introduces Point2RBox-v2, a point-supervised oriented object detector that effectively leverages the arrangement and layout of instances. We propose the integration of Gaussian overlay and Voronoi tessellation to constrain the size and rotation of instances based on their spatial relationships. Additionally, by incorporating self-supervised consistency loss, edge loss, and copy-paste augmentation, the accuracy of the model is further enhanced.\nExperiments yield the following observations: 1) The integration of Gaussian and Voronoi concepts effectively harnesses the spatial layout of objects, significantly enhancing point-supervised OOD. 2) Point2RBox-v2 demonstrates exceptional performance in densely packed scenes (see Fig. 2), where existing methods struggle. 3) Our method does not require priors (i.e. pre-trained SAM or one-shot examples) and is applicable to both end-to-end and pseudo-generation modes. 4) It advances the state of the art by a large amount, achieving 62.61%, 86.15%, and 34.71% on the DOTA-v1.0, HRSC, and FAIR1M datasets, respectively.\nLimitations. The gap between Point2RBox-v2 and the RBox-supervised OOD is still huge in terms of sparse categories (i.e. BR/SBF) since little constraint can be obtained from the layout between them when the objects are sparse."}]}