{"title": "Optimization Learning", "authors": ["Pascal Van Hentenryck"], "abstract": "This article introduces the concept of optimization learning, a methodology to design optimization proxies that learn the input/output mapping of parametric optimization problems. These optimization proxies are trustworthy by design: they compute feasible solutions to the underlying optimization problems, provide quality guarantees on the returned solutions, and scale to large instances. Optimization proxies are differentiable programs that combine traditional deep learning technology with repair or completion layers to produce feasible solutions. The article shows that optimization proxies can be trained end-to-end in a self-supervised way. It presents methodologies to provide performance guarantees and to scale optimization proxies to large-scale optimization problems. The potential of optimization proxies is highlighted through applications in power systems and, in particular, real-time risk assessment and security-constrained optimal power flow.", "sections": [{"title": "Introduction", "content": "Optimization technologies have been widely successful in industry: they dispatch power grids, route transportation and logistic systems, plan and operate supply chains, schedule manufacturing systems, orchestrate evacuations and relief operations, clear markets for organ exchanges to name only a few. Yet there remain applications where optimization technologies still face computational challenges, including when solutions must be produced in real time or when there are planners and operators in the loop who expect fast interactions with an underlying decision-support system.\nMany engineering applications, however, operate on physical infrastructures that change relatively slowly. As a result, optimization technologies are often used to solve the same core applications repeatedly on instances that are highly similar in nature. In addition, it is reasonable to assume that these instances followed a distribution learned from historical data, possibly augmented with forecasts for future conditions. The overarching scientific question is whether machine learning can help optimization technologies expand the realm of tractable optimization problems.\nThis paper, which summarizes a significant part of my keynote at the EURO-2024 conference in Copenhagen, reviews the concept of optimization learning, to address this challenge. The fundamental idea behind optimization learning is to learn the input/output mapping of optimization problems, i.e., the mapping from inputs to optimal solutions. The paper presents three fundamental methodologies in opimization learning: primal optimization proxies, dual optimization proxies, and primal-dual learning. Primal optimization proxies return (near-optimal) feasible solutions to parametric optimization problem through a combination of deep learning and repair layers. Dual optimization proxies return (near-optimal) dual solutions to parametric optimization problems through a combination of deep learning and completion layers. Primal-dual learning mimics traditional augmented Lagrangian methods to provide primal proxies that learn both a primal and a dual network.\nThese methodologies are illustrated on two significant applications in power systems operations: the real-time risk assessment of a transmission systems and the security-constrained optimal power"}, {"title": "Problem Formulation: The Learning Task", "content": "This paper considers applications that require the solving of parametric optimization problems\n$\\Phi(x) = \\underset{y}{\\text{argmin}} f_x(y) \\text{ subject to } h_x(y) = 0 \\& g_x(y) \\geq 0,$\nwhere $x$ represents instance parameters that determine the objective function $f_x$ and the constraints $h_x$ and $g_x$. Such parametric optimization can be viewed as mapping from an input $x$ to an output $y(x)$ that represents its optimal solution (or a selected optimal solution). In addition, for many applications, it is reasonable to assume that the instance parameters $x$ are characterized by a distribution learned from historical data, possibly augmented to cover future conditions with forecasting algorithms. The goal of this paper is to study how to use the fusion of machine learning and optimization for \"solving\" (unseen) optimization instances $\\Phi(x_j)$ orders of magnitude faster than a state-of-the-art optimization solver can. This task is called optimization learning in the rest of this paper.\nMore formally, given a probability distribution $P$ of instance parameters, the goal is to learn a primal optimization proxy $\\Phi^{\\uparrow} : \\mathbb{R}^m \\rightarrow \\mathbb{R}^a$ that produces feasible solutions, i.e.,\n$\\forall x \\sim P : h_x(\\Phi^{\\uparrow}(x)) = 0 \\& g_x(\\Phi^{\\uparrow}(x)) \\geq 0$\nand a dual optimization proxy $\\Phi^{\\downarrow} : \\mathbb{R}^m \\rightarrow \\mathbb{R}^r$ that returns valid lower bounds, i.e.,\n$\\forall x \\sim P : f_x(\\Phi^{\\downarrow}(x)) \\leq f_x(\\Phi(\\uparrow}(x)).$"}, {"title": "The Economic Dispatch Optimization", "content": "This paper illustrates optimization learning on a running example and its variants: the Economic Dispatch (ED) optimization with reserve requirements. The ED optimization is run every five minutes in the US and is the backbone of power system operations. The formulation described in Model 1 captures the essence of how real-time markets are cleared by Independent Systems Operators (ISOs) in the United States, although the reality is obviously more complex.\nConstraints (2b) and (2c) are the power balance and minimum reserve requirement constraints. Constraints (2d) ensure that the active power and reserves of each generator does not violate their maximum capacities. Constraints (2e) and (2f) enforce the limits on each generator dispatch and"}, {"title": "Empirical Risk Minimization", "content": "Machine learning seems to be an ideal approach to \u201csolve\u201d these parametric optimization problems \" and replace optimization all together. Indeed, an optimization problem can be viewed as mapping from its input x to an optimal solution y, and deep learning networks are universal approximators [20]. This section presents baseline machine learning approaches for optimization learning."}, {"title": "Supervised Learning", "content": "A key feature of optimization learning is its ability to generate correct labels for each input: it suffices to solve the optimization problem (1). Hence, to train a machine learning model, a data set $I = \\{x_i\\}_{i\\in[n]}$ of inputs sampled from the distribution $P$ can be augmented with their optimal solutions to obtain a training data set $D = \\{(x_i,y_i = \\Phi(x_i))\\}_{i\\in[n]}$. Testing and validation data sets can be obtained similarly.\nSupervised learning can then used to fit the learnable parameters $\\theta$ of a parametric machine learning model $M_\\theta$ by minimizing a loss function of the form\n$\\underset{\\theta}{\\text{argmin}} \\frac{1}{n} \\sum_{i=1}^n L(y_i, M_\\theta(x_i))$\nwhere\n$L(y, \\hat{y}) = ||y \u2013 \\hat{y}) ||.$\nThe \"optimal\" value $\\theta^*$ for the learnable parameters turns the machine learning model into a function $M_{\\theta^*}$ that approximates the parametric optimization $\\Phi$. This minimization problem is known as an empirical risk minimization and is typically solved using stochastic gradient descent as illustrated in Figure 1. The machine learning training consists of series of forward and backward passes. At iteration $t$, with $\\theta^t$ as the values of the learnable parameters, the forward pass evaluates $M_{\\theta^t}(x)$ to obtain $\\hat{y}^t$, and the backward pass updates the learnable parameters $\\theta^t$ using a gradient step\n$\\theta^{t+1} = \\theta^t - \\alpha \\frac{\\partial L(y, \\hat{y}^t)}{\\partial \\theta}.$\nUnfortunately, in engineering tasks, such an approximation is not a primal optimization proxy: by virtue of being a regression, the machine learning predictions are unlikely to satisfy the problem"}, {"title": "Lagrangian Duality in Supervised Learning", "content": "Early research on this topic [14, 15] proposed to combine machine learning and Lagrangian duality, using a parametric loss function of the form\n$L^{\\lambda, \\nu}(y, \\hat{y}) = ||y \u2013 \\hat{y}|| + \\lambda |h_x(\\hat{y})| + \\nu \\text{max}(0, -g_x(\\hat{y})).$\nwhere $h_x(\\hat{y})$ and $\\text{max}(0, -g_x(\\hat{y}))$ capture the violations of the equality and inequality constraints. The multipliers $\\lambda$ and $\\nu$ define the penalties for these constraint violations; they can be trained by mimicking subgradient methods for computing Lagrangian duals, alternating between training the machine learning model and adjusting the multipliers with subgradient steps. At each step $t$, the Lagrangian dual optimization solves a training problem\n$\\theta^{t+1} = \\underset{\\theta}{\\text{argmin}} \\frac{1}{n} \\sum_{i=1}^n L^{\\lambda^t, \\nu^t}(y_i, M_{\\theta^t} (x_i))$\nwith the multipliers fixed at $\\lambda^t$ and $\\nu^t$. The multipliers are then adjusted using the constraint violations, i.e.,\n$\\lambda^{t+1} = \\lambda^t + \\rho_{\\lambda} \\frac{1}{n} \\sum_{i=1}^n h_x(M_{\\theta^{t+1}}(x_i))|$\n$\\nu^{t+1} = \\nu^t + \\rho_{\\nu} \\frac{1}{n} \\sum_{i=1}^n \\text{max}(0, -g_x(M_{\\theta^{t+1}}(x_i))).$\nExperimentally, this Lagrangian dual approach has been shown to reduce violations, sometimes substantially. However, being a regression, it almost always violates constraints on unseen instances, and is not an primal optimization proxy."}, {"title": "Primal Optimization Proxies", "content": "Optimization proxies were introduced to remedy the limitations of the baseline machine learning models. Their high-level architecture is depicted in Figure 2. An optimization proxy combines a parametric machine learning model $M_\\theta$ (typically a deep neural network) that produces an approximation $\\hat{y}$ with a repair layer $R$ that transforms $\\hat{y}$ into a feasible solution $\\tilde{y} = R(\\hat{y})$, i.e., $\\tilde{y}$ satisfies"}, {"title": "Implicit Differentiation", "content": "A broad technique to differentiate through the repair layer is to use Cauchy's implict function theorem [27]. Machine learning models can thus support two types of layers in their networks: explicit layers"}, {"title": "Dedicated Repair Layers", "content": "Because of the high computational cost of implicit layers, it is interesting to explore the design of dedicated repair layers and learning architectures that ensure fast training and inference times. This section illustrates this concept on the economic dispatch optimization, where the challenge is to restore the feasibility of the power balance and reserve constraints. Figure 5 depicts the architecture of a primal optimization proxy for the ED problem. It uses a sigmoid layer to enforce the bound constraints, followed by a repair layer for the power balance constraint and another repair layer for the reserve constraints.\nConsider first a dedicated repair layer for the balance constraint. It takes as input an estimate $\\hat{p}_g$ for the power of each generator $g$. It uses ideas from control systems to scale all generators proportionally, up or down, to transform $\\hat{p} = M_\\theta(x)$ into a solution $\\tilde{p}$ to the balance constraint as follows:\n$\\tilde{p} = \\begin{cases}\n(1 \u2013 \\zeta^{\\uparrow}) \\hat{p} + p^{\\downarrow} & \\text{if } 1^T\\hat{p}<1^Td\\\\\n(1-\\zeta^{\\downarrow})\\hat{p}+p^{\\uparrow} & \\text{otherwise}\n\\end{cases}$\nwhere $\\zeta^{\\uparrow}$ and $\\zeta^{\\downarrow}$ are defined as,\n$\\zeta^{\\uparrow} = \\frac{1^T d - 1^T \\hat{p}}{1^T p^{\\uparrow} - 1^T \\hat{p}}$\n$\\zeta^{\\downarrow} = \\frac{1^T \\hat{p} - 1^T d}{1^T \\hat{p} - 1^T p^{\\downarrow}}$\nand $\\hat{p}$ and $p$ are the lower and upper bounds on the generators. This repair layer (5) is differentiable almost everywhere and can thus be naturally integrated in the training of the machine learning model. Note that the formula for $\\tilde{p}$, i.e.,\n$\\tilde{p} = (1 \u2013 \\zeta^{\\uparrow})\\hat{p} + p^{\\downarrow}$\nor\n$\\tilde{p} = (1 \u2013 \\zeta^{\\downarrow})\\hat{p} + p^{\\uparrow}$\ncan be generated during the forward pass based on the result of the test $1^T\\hat{p} < 1^T d$. It also admits subgradients everywhere and can thus be backpropagated during the backward pass. This also highlights the power of Dynamic Computational Graphs (DCGs) and, more generally, differentiable programming: the computational graph does not have to be defined statically and is not necessarily the same for each learning iteration.\nThe reserve layer follows a similar approach but it is more involved. During the forward phase, the proxy first partitions the generators into (1) the set $G^{\\uparrow}$ of units whose power output can be decreased to provide more reserves; and (2) the set $G^{\\downarrow}$ of units whose power output can be increased with affecting their reserves\n$G^{\\uparrow} \\leftarrow \\{g | \\hat{p}_g \\leq p_g \u2013 r_g\\}$\n$G^{\\downarrow} \\leftarrow \\{g | \\hat{p}_g > p_g \u2013 r_g\\}.$\nThe amount $\\Delta^{\\uparrow}$ of additional reserves and additional power $\\Delta^{\\downarrow}$ that can be provided are given by\n$\\Delta^{\\uparrow} \\leftarrow \\sum_{g\\in G^{\\uparrow}} \\hat{p}_g - (p_g - r_g)$\n$\\Delta^{\\downarrow} \\leftarrow \\sum_{g\\in G^{\\downarrow}} (p_g - r_g) \u2013 \\hat{p}_g$\nIt is then possible to compute the additional reserve amount $\\Delta$ that is available witout violating the balance constraint\n$\\Delta \\leftarrow \\text{max}(0, \\text{min}(\\Delta_R, \\Delta^{\\downarrow}, \\Delta^{\\uparrow}))$\nwhere\n$\\Delta_R \\leftarrow \\tilde{R} - \\sum_{g} \\text{min}\\{\\tilde{r}_g, p_g \u2013 p_g\\}$\ndenotes the shortage in reserves. The generators can then scaled proportionally by\n$\\tilde{p}_g = \\begin{cases}\n\\hat{p}_g + \\alpha^{\\uparrow} (p_g - \\hat{p}_g) & \\forall g \\in G^{\\uparrow}\\\\\n\\hat{p}_g - \\alpha^{\\downarrow} (\\hat{p}_g - p_g + \\tilde{r}_g) & \\forall g \\in G^{\\downarrow}\n\\end{cases}$\nwhere $\\alpha^{\\uparrow} \\leftarrow \\Delta/\\Delta^{\\uparrow}$ and $\\alpha^{\\downarrow} \\leftarrow \\Delta/\\Delta^{\\downarrow}$. Again, the definition of $\\tilde{p}_g$ for each generator is differentiable. Moreover, these two layers are guaranteed to find a feasible solution if one exists.\nThe overall architecture (see Figure 5), called E2ELR, is a differentiable program: it can be trained end to end efficiently and runs in milliseconds during inference due to these dedicated repair layers."}, {"title": "Self-Supervised Learning", "content": "Primal optimization proxies also offer an intriguing alternative to supervised learning. Indeed, the approaches presented so far rely on the availability of the data set $D = \\{(x_i, y_i = \\Phi(x_i))\\}_{i\\in[n]}$. However, since the parametric optimization problem is available in explicit form, it is possible to define a self-supervised version of the learning task that only uses the data set $I$. Given a parametric primal optimization proxy, the self-supervised learning task amounts to solving the optimization problem\n$\\underset{\\theta}{\\text{min}} \\frac{1}{n} \\sum_{i=1}^n f_{x_i} (\\Phi(x_i))$\nwhich is expressed in terms of the objective function $f_x$ of the original parametric optimization problem. Note also that the learning talk is different from the original optimization problem (1): the optimization is over the learnable parameters, not the decision variables of (1).\nFor instance, E2ELR can be trained end-to-end using self-supervised learning and the loss function\n$c(p) + M_{\\theta} ||\\xi_{\\text{th}}||_1$\nwhich is then backpropagated through the repair layers to adjust the parameters $\\theta$ of the neural network. The training of a self-supervised E2ELR does not require any solved instance.\nThis self-supervised approach has a significant benefit: it does not rely on the availability of the optimal solutions for a large number of instances, which may be costly to obtain in practice. In addition, the objective function in supervised learning approaches may not be perfectly aligned with the objective of the original optimization problem, reducing the accuracy of the learning step. It should be noted that supervised learning can also be used, say with the LD approach proposed earlier. However, this provides less guidance to the learning system than the labeled data in general, and the machine learning model typically trade increased violations for better objectives. Early papers on self-supervised learning for optimization can be found in (e.g., [9,12,21,36,36,46])."}, {"title": "Experimental Results", "content": "This section demonstrates the capabilities of primal optimization proxies, and E2ELR in particular, on power grids with up to 30,000 buses. All details can be found in [10]."}, {"title": "Data Generation", "content": "To obtain reasonable training, validation, and testing data sets, benchmarks from the PGLib [6] library (v21.07) were modified by perturbing the loads. Denote by $d^{\\text{ref}}$ the nodal load vector from the benchmarks. Load i becomes $d(i) = \\gamma(i) \\times \\eta(i) \\times d^{\\text{ref}}$, where $\\gamma(i) \\in \\mathbb{R}$ is a global scaling factor, $\\eta \\in \\mathbb{R}^{|N|}$ denotes load-level multiplicative white noise, and the multiplications are element-wise. $\\gamma$ is sampled from a uniform distribution $\\mathcal{U}[0.8, 1.2]$, and, for each load, $\\eta$ is sampled from a log-normal distribution with mean 1 and standard deviation 5%. The PGLib library does not include reserve information: for this reason, the instance generation mimics the contingency reserve requirements used in industry. It assumes $r_g = a_r p_g$, where $a_r = 5 \\times ||p||_{\\infty} \\times ||p||_1^{-1}$, ensuring a total reserve capacity 5 times larger than the largest generator. The reserve requirements of each instance is sampled uniformly between"}, {"title": "Computing Times", "content": "The second set of results evaluates the computational efficiency of each ML model. The computational efficiency is measured by (i) the training time of ML models, including the data generation time when applicable, and (ii) the inference time. Note that ML models evaluate batches of instances, and inference times are reported per batch of 256 instances. Unless specified otherwise, average computing times are arithmetic means; other averages use shifted geometric means with a shifts of 1% for optimality gaps, and 1 p.u. for constraint violations."}, {"title": "The Impact of Optimization Proxies", "content": "Optimization proxies have the potential to transform applications as they bring orders of magnitude improvements in efficiency. This section highlights these potential benefits on an important topic:"}, {"title": "Dual Optimization Proxies", "content": "Primal optimization proxies produce (high-quality) feasible solutions in milliseconds. However, an ideal outcome in optimization practice is to produce a pair of primal and dual solutions with a small duality gap. The counterpart for optimization learning would be to produce a pair of primal and dual proxies that would produce both a feasible solution and an assessment of its quality in milliseconds. This section describes a methodology to obtain dual optimization proxies for many practical applications.\nTo convey the intuition underlying the methodology, consider the parametric optimization program and its dual\n$\\begin{aligned}\n&\\underset{y}{\\text{min}} & c^T y\\\\\n&\\text{s.t.} & A_x y = b_x\\\\\n& & l_x \\leq y \\leq u_x\n\\end{aligned}$\n$\\begin{aligned}\n&\\underset{z,z_l,z_u}{\\text{max}} & b_x^T z + l_x^T z_l - u_x^T z_u\\\\\n&\\text{s.t.} & A_x^T z + z_l - z_u = c_x\\\\\n& & z_l, z_u \\geq 0\n\\end{aligned}$\nThe key insight comes from the fact that the bound constraints $l_x \\leq y \\leq u_x$ in the primal model gives a simple way to find a dual feasible solution: obtain a prediction $\\tilde{z}$ for variables $z$ and use the free dual variables $z_l$ and $z_u$ to obtain a dual feasible solution $(\\tilde{z}, \\tilde{z}_l, \\tilde{z}_u)$. These primal bound constraints are natural in most engineering applications: for instance, generators in power systems have physical limits, and the number of trailers in a supply chain is finite.\nFigure 8 provides the general structure of dual optimization proxies. Given decision variables $z = (z_1, z_2)$, a dual proxy consists of a parametric machine learning model $M_\\theta$ to obtain a prediction $\\tilde{z}_1 = z_1$ for a subset of the decision variables, and a completion layer $C$ to assign the remaining variables $z_2 = \\tilde{z}_2$ and deliver a feasible dual solution $\\tilde{z} = (\\tilde{z}_1, \\tilde{z}_2)$. The composition $C \\circ M_\\theta$ is a parametric dual optimization proxy.\nConsider now a dual optimization proxy for parametric linear programming. Given a completion layer $C^{lp}$, the composition $C^{lp} \\circ M_\\theta$ is a parametric dual optimization proxy that can be trained by"}, {"title": "Experimental Results", "content": "This section presents some experimental result on the self-supervised Dual Optimal Proxy for Linear Programming (DOPLP) applied to the DC-OPF problem. The full details and results can be found"}, {"title": "Primal-Dual Learning", "content": "Another approach to find feasible solutions is to develop optimization proxies that adapt traditional optimization algorithms to the learning context. Consider the constrained optimization problem\n$\\underset{y}{\\text{argmin}} f_x(y) \\text{ subject to } h_x(y) = 0.$"}, {"title": "The SCOPF Problem", "content": "Model 3 presents the extensive formulation of the SCOPF with N-1 generator and line contingencies. The formulation is similar in spirit to the ED, but it replaces the reserve constraints with actual generator and line contingencies. The primary objective of the SCOPF is to determine the generator setpoints for the base case while ensuring the feasibility of each generator and line contingencies. In other words, even if a generator or line fails, the base setpoints should provide enough flexibility to be transformed into a feasible solution for the contingency. The objective (16) sums the linear cost of the base case dispatch $p$ and the penalties for violating the thermal limits in the base case and in the contingencies.\nThe base case includes Constraints (17), (18), and (19). They include the hard power balance constraints in the base case, the soft thermal limits, and the generation limits. Each generator contingency imposes Constraints (20), (21), and (22) to enforce the power balance, the soft thermal limits, and the generation bounds under the generator contingency. The only difference from the base case is Constraint (23) that specifies that generator $k$ should remain inactive under its contingency.\nIt is important to model accurately how the generators react after a contingency. In Model 3, their response follows an Automatic Primary Response (APR) control mechanism [3,13], as shown in Constraints (24)\u2013(26). The formulation here is based on the APR model used in [44,45] where, for each generator contingency $k$, a system-wide signal $\\eta_k \\in [0,1]$ represents the level of system response required to resolve the power imbalance. The APR control also ensures that the change in dispatch under a contingency is proportional to the droop slope, which is determined by the product of the generator capacity $p$ and the predefined parameter $\\gamma$ as in [45]. The generator capacities are defined as $p = p - p$. The APR constraints ensure that the generation dispatch under generator contingency remains within the generation limits, leading to the following generator response:\n$\\tilde{p}_{k,i} = \\text{min}\\{p_i+\\eta_k\\gamma_i\\tilde{p}_i,p_i\\}, \\forall i\\in\\mathcal{G},\\forall k\\in\\mathcal{K}_{g,i\\neq k}.$"}, {"title": "Primal-Dual Learning for SCOPF", "content": "This section describes PDL-SCOPF, i.e., the Primal-Dual Learning of large-scale SCOPFs. The primal variables to estimate are $y := p, \\{\\tilde{p}_{k}, \\eta_{k}, \\tilde{p}_{k}\\}_{k\\in\\mathcal{K}_g}, \\{\\xi_{k}\\}_{0\\bigcup\\mathcal{K}_g\\bigcup\\mathcal{K}_e}$, the objective function $f_x(y)$ is the original objective function (16) of Model 3, and the constraints $h_x(y)$ capture the power balance equations (20) for the generator contingencies. Figure 10 provides a schematic representation of the primal and dual networks which, given the input configuration vector $x$, estimate the primal and dual solutions for SCOPF. The design of the primal learning network of PDL-SCOPF has two notable features: (1) a repair layer for restoring the power balance of the base case like in E2ELR, and (2) a binary search layer to estimate the generator dispatches in the generator contingencies inspired by the CCGA."}, {"title": "The Primal and Dual Networks", "content": "The primal network estimates the nominal dispatch, the contingency dispatches, and the slacks of the thermal constraints. As shown in Figure 2, it uses three main components: (1) a fully connected layer with a sigmoid layer that produces a first approximation $\\hat{p}$ of the base dispatch; (2) a repair layer that produces a feasible dispatch $\\tilde{p}$ to the base dispatch; and (3) a binary search layer that computes an approximation $\\tilde{p}_k$ of dispatch for contingency $k$ by mimicking the binary search of the CCGA. These three components, and the computation of the constraint slacks, constitute a differentiable program for"}, {"title": "Conclusion", "content": "This paper presented the concept of optimization learning, a methodology to design differentiable programs that can learn the input/output mapping of parametric optimization problems. These optimization proxies are trustworthy by design: they compute feasible solutions to the underlying optimization problems, provide quality guarantees on the returned solutions, and scale to large instances. Optimization proxies combine traditional deep learning technology with repair or completion layers to produce feasible solutions. The paper also showed that optimization proxies can be trained end-to-end in a self-supervised way.\nThe paper illustrated the impact of optimization learning on two applications of interest to the power industry: the real-time risk assessment of a transmission systems and the security-constrained optimal power under N-1 Generator and Line Contingencies. In each case, optimization proxies brings orders of magnitude improvements in efficiency, which makes it possible to solve the applications in real time with high accuracy, an outcome which that could not have been achieved by state-of-the-art optimization technology.\nThere are many open issues in optimization learning. They include (1) understanding how to derive effective repair layers for a wide range of applications; (2) studying how to apply optimization learning for combinatorial optimization problems, where the gradients are typically not meaningful, and (3) applying Primal-Dual learning to a variety of applications in nonlinear optimization.\nOptimization learning is only one direction to fuse machine learning and optimization. Learning to optimize, where machine learning is used to speed up an existing algorithm is another avenue to leverage the strenghts of both approaches."}]}