{"title": "Data-Juicer 2.0: Cloud-Scale Adaptive Data Processing for Foundation Models", "authors": ["Daoyuan Chen", "Yilun Huang", "Xuchen Pan", "Nana Jiang", "Haibin Wang", "Ce Ge", "Yushuo Chen", "Wenhao Zhang", "Zhijian Ma", "Yilei Zhang", "Jun Huang", "Wei Lin", "Yaliang Li", "Bolin Ding", "Jingren Zhou"], "abstract": "The burgeoning field of foundation models necessitates advanced data processing mechanisms capable of harnessing vast valuable data with varied types utilized by these models. Nevertheless, the current landscape presents unique challenges that traditional data processing frameworks cannot handle effectively, especially with multimodal intricacies. In response, we present Data-Juicer 2.0, a new system offering fruitful data processing capabilities backed by over a hundred operators spanning various modalities like text, image, audio, and video. With seamless compatibility and dedicated optimization to popular dataset hubs like Hugging Face and computing engines like Ray, Data-Juicer 2.0 enhances its predecessor in both usability, efficiency, and programmability. It features an easily accessible user interface layer that supports decoupled Python interactions, RESTful APIs, and conversational commands. Alongside this, it contains a core runtime layer optimized for adaptive execution and management across different dataset scales, processing demands, and computational environments, while shielding unnecessary system details. Extensive empirical evaluations demonstrate Data-Juicer 2.0's remarkable performance and scalability, highlighting its capability to efficiently process tens of billions of data samples with tens of thousands of CPU cores. The system is publicly available, actively maintained, and broadly adopted in diverse research endeavors, practical applications, and real-world products such as Alibaba Cloud PAI.", "sections": [{"title": "1 INTRODUCTION", "content": "Data Processing Challenges for Foundation Models. The advent of foundation models capable of processing textual and multimodal information, exemplified by the GPT-series [47], has transformed artificial intelligence by harnessing expansive datasets and substantial computational resources [69, 73]. The efficacy of these models depends on meticulous data collection, cleaning, annotation, and synthesis [5, 53, 69], essential for optimizing performance, scalability, and robustness across complex applications.\nHowever, current data processing frameworks, whether for big data [72] or text-only foundation models [22, 33, 57], are inadequate for handling the complexities of multimodal datasets, due to unique challenges as the field rapidly evolves:\nPractitioners lack open-source frameworks specifically designed to manage the diverse data types and semantics integral to foundation models. The processing of these datasets extends beyond standard"}, {"title": "2 BACKGROUND", "content": "Data Processing for Foundation Models\nFoundation models, especially those beyond text, demands innovative data processing paradigms to address the complexity and diversity of large-scale training datasets [5, 53, 69, 73]. Despite existing systems for big data or text-only models [22, 33, 57, 72], there is a notable lack of open-source frameworks customized for foundation models due to unique design and implementation challenges:\nFunctionality Challenges from Data Types and Semantics. Data processing frameworks for foundation models primarily target semi-structured textual and multimodal datasets (e.g., JSONL, Parquet, MP4), often disregarding the strongly structured data types prominent in previous big data systems (e.g., database tables) [33, 37]. Significantly, the performance of foundation models depends on the quality, diversity, utility, and safety of data, necessitating a profound semantic understanding and cross-modal alignment to transform raw data into high-quality inputs, thereby enhancing model efficacy [39, 40, 66].\nEfficiency Challenges from Data Operations and Pipelines. Conventional engines support a broad range of SQL queries for complex data operations [58, 72]. However, in the context of foundation models, the requirement for traditional grouping and aggregation operations is relatively minimal [5]. Instead, emphasis is placed on simpler operations such as Mapper and Filter, which rely heavily on deep learning models via local inference [11, 56, 68] or API calls [40-42]. Furthermore, individual records in these datasets can be exceedingly large, surpassing the capabilities of traditional big data engines [32, 46]. When combining multiple data operations into pipelines, it is crucial to maintain efficient data and transformation flows throughout the data lifecycle to prevent bottlenecks stemming from computational overhead, latency, and data integrity.\nUsage Challenges from User Interfaces. Traditional big data systems typically use programming interfaces based on Java and Scala, supplemented by SQL. However, practitioners focusing on foundation models predominantly operate within the Python ecosystem, supporting both upstream and downstream repositories and workflows [1, 49, 54]. This shift necessitates designing user interfaces that specifically cater to the new programming needs of foundation model practitioners."}, {"title": "2.2 System Design Goals in Data-Juicer 2.0", "content": "Built upon Data-Juicer 1.0, we aim to address challenges identified in Sec. 2.1 with the focus on the following new objectives:\nComprehensive Multimodal Processing Abilities. The first basic goal is to support extensive multimodal processing capabilities across diverse domains, such as AI-generated content, spatial intelligence, education, and entertainment. This involves meticulous processes such as collection, cleansing, annotation, and synthesis, which integrate perceptual and cognitive information across modalities such as video, image, text, and audio [2, 10, 23, 51].\nEfficient Data Processing Operators and Data Flow. Comprehensive semantic operations are expected through advanced procedures that may be high-cost, such as deep learning model invocations, fuzzy deduplication, and clustering. For basic, high-frequency operations, we need to employ dedicated code optimization and speedup techniques like GPU computation or Cython [6]."}, {"title": "3 PROCESSING CAPABILITIES BEYOND TEXT", "content": "Overview of New Operators\nData-Juicer 2.0 extends its predecessor, Data-Juicer 1.0\u2014which included approximately 50 text-only OPs\u2014by incorporating over 80 new OPs crafted for multimodal data processing with more functionalities, encompassing modalities such as image, video, and audio. For clarity and ease of reference, the OPs are categorized from the following aspects and visually represented in Fig. 2.\nModality Types: The majority of new OPs are concentrated on video/image/text-only processing, with approximately 20 OPs dedicated to cross-modal data processing.\nOperator Types: About 90% are Mappers and Filters, reflecting the user needs in the foundation model domain. New variants in other types, such as Formatters and Deduplicators remain stable due to the broad applicability of already supported formats (e.g., JSONL, Parquet, MP4) and classical algorithms like MinHash [8].\nFunction Types: Data cleaning and analysis operations constitute about half of the new OP suite. Notably, Data-Juicer 2.0 introduces about 40 OPs for data synthesis for cross-modal and post-tuning scenarios, enabling any-to-any generation among 4 supported modalities, and enhancing textual instruction and response based on varied information and needs. Furthermore, new privacy protection OPs and several wrapper OPs for established tools, such as FFmpeg [26], have been integrated, enabling users' convenient invocation of traditional professional commands."}, {"title": "3.2 Showcase of Typical New Operators", "content": "The new OPs include some unique contributions to Data-Juicer 2.0, and others partly inspired by state-of-the-art data processing methodologies for foundation models [27, 56, 75]. Below are three representative examples that showcase the diverse computational operations and requirements of these OPs.\nphrase_grounding_recall_filter: This OP, newly developed by Data-Juicer 2.0, assesses alignment and consistency between images and textual descriptions. As illustrated in Fig. 3a, it identifies noun phrases in the text that refer to key entities. Subsequently, an open-vocabulary object detection model, such as Owl-ViT [43], attempts to detect corresponding entities within the image. The OP then calculates the recall of detected phrases to evaluate consistency, where a higher recall indicates better image-text coherence.\nvideo_motion_score_filter: This OP quantifies video dynamics. As shown in Fig. 3b, it samples multiple frames at a specified frames-per-second (FPS) rate and computes optical flows. The average magnitude of these flows determines the motion score, with higher scores indicating more dynamic content. To accommodate various computational resources, both a GPU-based RAFT version [65] and a CPU-based OpenCV version [7] are available.\nvideo_captioning_from_summarizer_mapper: This OP generates new video captions by combining tagging and captioning capabilities from various OPs. As illustrated in Fig. 3c, it uses six OPs from different angles: two for tagging and captioning the audio stream, two for static visual frame analysis, and two for dynamic video stream and information integration. A summarizer finally incorporates the three captions and two tag sets into a new caption."}, {"title": "4 USER INTERFACES", "content": "Low-Level APIs\nThe framework's core capabilities are exposed through Python-based programmatic interfaces, providing object-oriented logical encapsulation for both the fundamental DJ-Dataset, DJ-Operators and other runtime modules. This design delivers developers maximum flexibility and customizability. Processing workflows can be automatically chained by passing a series of OP instances to a loaded DJ-Dataset object (e.g., data.process([op1, op2])), enabling various operations to be performed on the dataset in a single pass. Additionally, the framework supports applying an instantiated DJ-Operators to a target dataset (e.g., op.run(data)), enhancing the reusability of OP instances. More details about the DJ-Dataset and DJ-Operators will be introduced later in Sec. 5. This dual approach\u2014chained processing and individual OP application\u2014optimizes both efficiency and modularity in data processing tasks, while leveraging the inherent advantages of Python's ecosystem."}, {"title": "4.2 RESTful APIs", "content": "Utilizing standard Python type hints, we provide one-click generation of high-performance web APIs, capable of automatically discovering, registering, and adapting OP classes and related tools. Users can rapidly initiate a web server supporting the Asynchronous Server Gateway Interface by simply executing a service script, eliminating the need for manual code writing. The asynchronous concurrency mechanism enables options such as lightweight background tasks and mitigates potential bottlenecks for endpoints that may experience prolonged network I/O blocking. Each OP is accessible via POST requests, typically executing the OP's run() method as the endpoint. The target dataset path is passed through query parameters, with additional configurable OP parameters transmitted via JSON payload. Upon completion, the path to the processed dataset is returned. This invocation through Web APIs allows for the centralized host with distributed access, reducing deployment complexity. It also facilitates the separation of application logic from execution, potentially fostering the development and release of more applications built upon Data-Juicer. Importantly, the extensive customization parameters available in the programming API can be seamlessly passed through the Web API, maintaining full functionality without compromising usability, and facilitating serviceful calling by higher-level interfaces as introduced later."}, {"title": "4.3 Web & CMD Tools", "content": "Thanks to the low-level and RESTful APIs, we further establish service-level capabilities across various scenarios in Data-Juicer 2.0, encapsulating scenario-specific processing solutions into built-in data recipes, described as YAML configuration files for end-to-end workflows (an example for video-data-synthesis is shown in Fig. 4). Centered around these data recipes, we provide user-friendly, highly encapsulated command-line interfaces as illustrated in Listing 1, supporting automated lightweight installation and recipe-level invocation. These tools automatically gather and install the requisite of OPs within the recipe, perform automated OP fusion, and maximize adaptive utilization of computing resources while minimizing user cognitive load and operational cost."}, {"title": "4.4 Natural Language Interaction", "content": "Querying and understanding over 100 diverse OPs have become challenging. Thankfully, advances in foundation models have transformed human-computer interactions. Research shows the synergy of reasoning and acting (ReAct) [71] in language models enhances task-solving. Using RESTful API services, we developed model-driven agents based on ReAct, enabling intuitive data processing through natural language. We adopted AgentScope [29], a multi-agent platform, for low-code integration, using prompt-optimized ReAct agents to align OP functionalities with our RESTful APIs. Built-in function preprocessing, response parsing, and fault-tolerance streamline development. Data-Juicer 2.0 thus allows flexible instruction input, interpreting user intents to execute appropriate data processing functions.\nAn example is shown above for interactive text length filtering using this agent, where users describe tasks with simple and vague language. Specifically, the user requests \u201cfilter out too short text samples from input_data.jsonl\". The agent's ReAct prompt helps analyze and reason user intent, signaled by the <thought> tag. The <function> tag notes the chosen function, while <dataset_path> represents parameter mapping. Initially, a statistical analysis of text lengths is visualized, and the agent asks for filtering thresholds. After user input, filtering runs with parameters via RESTful API. Upon completion, the agent confirms the processed dataset path as output.jsonl, marks the task finished, and informs the user. Gray dialogs reveal the agent's reasoning and actions, providing insights into the workflow. This example illustrates step-by-step user understanding, function selection, parameter population, and task execution, emphasizing transparency and automation of Data-Juicer 2.0 with agentic processing.\""}, {"title": "5 DATA-JUICER CORE RUNTIME", "content": "Data-Juicer-Dataset\nProgramming Interfaces with Extensive Compatibility. The core runtime of Data-Juicer 2.0 is implemented in Python, featuring a flexible data processing pipeline characterized by two execution modes: a standalone mode for convenient single-node execution, based on the Hugging Face Dataset [37], and a distributed mode that offers scalability across multiple nodes, leveraging the Ray Data [44] and MaxFrame-DataFrame [18]. These frameworks offer diverse computational engines, each with distinct capabilities and suitable use cases, but they also come with heterogeneous programming interfaces and complex implementation details.\nIn Data-Juicer 2.0, we exploit the strengths of these diverse computational engines and their associated programming classes by abstracting a top-level DJ-Dataset class and devising a standardized data representation schema (detailed in the next subsection). The primary design principle employed is the Facade Pattern. The abstracted class provides comprehensive and unified development interfaces, facilitating seamless use across standalone, distributed, and cloud cluster environments, meanwhile optimized to support extensive multimodal data processing."}, {"title": "5.1.2 Flexible Data Schema", "content": "In the context of foundation models, datasets are complex, originating from various sources and modalities. Target tasks are diverse, requiring different data formats and organizational structures. Integrating diverse data processing into a unified pipeline is challenging. Data-Juicer 2.0 adopts an extensible data representation schema that intrinsically supports multimodal data types and flexible data field customization."}, {"title": "5.2 Data-Juicer-Operators", "content": "Operator Types and Design Principles. Operators (OPs) are fundamental units responsible for executing processing functionalities, such as enhancing the accuracy of descriptive text or filtering out images containing Not-Safe-for-Work (NSFW) content. As illustrated by the middle orange box in Fig. 1, Data-Juicer defines five previous atomic OP classes: Formatter, Filter, Mapper, Deduplicator, and Selector; alongside four new types of compositional OPs: Grouper, Aggregator, FusedOP, and ScriptOP. The first five OP classes handle dataset format conversion, sample filtering, modification, deduplication, and selection, respectively. Following the Strategy Pattern, they are designed to encapsulate diverse algorithms that can be dynamically used to process the data. Each OP has a clearly defined role and can be interchanged or modified without impacting other system parts.\nMoreover, following the Decorator Pattern, compositional OPs are provided to enhance existing functionality without modifying the prevailing OP structure while dynamically adding data processing behavior to objects. The FusedOP enables explicitly grouping multiple atomic OPs to process data in a fine-grained manner within the same data batch, opposed to the default sequential processing across datasets as demonstrated by Listing 4. The Grouper takes a DJ-Dataset as input and groups data samples into batches, which can then be input into the Aggregator for subsequent aggregation. For example, we can employ extract_entity_attribute_mapper"}, {"title": "5.2.2 Unified Coordination of Logical Operations", "content": "In Data-Juicer 1.0, the logical operations of different OPs were coordinated within various executors (either standalone or distributed) rather than being bound to the OPs themselves. Disentangled from the executor's scheduling interface, it becomes challenging to determine the execution logic of different OPs, making the development and extension of individual OPs less intuitive and lacking self-explanatory qualities. In Data-Juicer 2.0, several design principles are utilized to address this issue, including the Abstract Factory Pattern, Template Method Pattern, and Single Responsibility Principle.\nSpecifically, a top-level OP factory class is abstracted above the aforementioned fundamental OP classes. In this class, functionalities common to all OPs are extracted, such as preprocessing of instantiated parameters, support for serialization, and configuration of OP-aware runtime parallelism. Besides, a unified run() method is implemented, maintaining a consistent interface for integration and API calls. Furthermore, parallelism in multi-processing and multi-GPU is automatically configured and decoupled from specific OPs, ensuring transparency for end users and developers, as introduced subsequently in Sec. 5.5. Lower-level OP classes define their own templated execution logic behind the run invocation. Taking the Filter class as an example, its core logic first engages the compute_stats() method to obtain statistical information based on specific metrics and then invokes its process() method to determine sample filtering based on thresholds.\nOn the one hand, this simplifies users' understanding of OP types. Users can instantiate any OP and invoke it with a unified parameter signature using op.run(), thereby reducing the learning curve. The templated execution flow of various OP types is self-contained within base classes, eliminating dependencies on external executors to oversee invocation logic. On the other hand, by templating execution logic within base classes, developers can readily modify, extend, or implement new OPs. For instance, a developer aiming to customize an existing Filter does not need to rewrite a new class entirely but can inherit from a related existing class and override specific methods such as compute_stats() or others as required.\nRegarding the naming and implementation of specific leaf OP classes, we adhere to extracting functionalities that are not tightly coupled with the OPs into common utility classes wherever feasible. This approach enables each OP to focus on its specific modalities and functionalities, facilitating a reduction in code complexity and enhancing clarity in understanding individual OP classes. Compared to previous implementations, the revised OP classes demonstrate easier inspection, integration, and testing. Users can utilize these robust OPs and seamlessly integrate them into their own tools or systems flexibly, both in source code or exposed RESTful API."}, {"title": "5.3 Processing Job Control", "content": "With the fundamental DJ-Dataset and DJ-Operators primitives established, we offer a series of control panel modules (the red box in Fig. 1) to organically combine them and accomplish data processing tasks. Executor encapsulates a series of standardized execution processes tailored for different standalone and distributed engines, leveraging modules such as Config and Monitor to accomplish end-to-end system configuration, data loading, analysis, processing iteration, data checkpointing, and more. Template workflows manage the complete data development lifecycle, including feedback, data storage, logging, and performance and operational monitoring.\nThe processing capability of Data-Juicer 2.0 has been encapsulated into numerous effect-proven and illustrative data recipes (in YAML format as shown in Fig. 4), catering to various vertical domains, such as multimodal contrastive data synthesis [34] and persona-oriented data processing, maintained within the DJ cookbook online. These are user-friendly, readily deployable, and facilitate interface exposure and reuse across different levels (Sec. 4), simplifying recipe routing and tailored processing solution generation by foundation-model-based agents.\nTo avoid the substantial costs from trial and error for data and model development in foundation model scenarios, we further develop a Sandbox suite in Data-Juicer 2.0 for data-model co-development, serving as a specialized intermediate layer connecting data processing jobs to numerous open-source infrastructures of model training and evaluation. The suite offers template workflows that extend beyond dataset-only development by incorporating cost-effective model training and evaluation signals, and quantitatively studying interplays among data, model, and compute. Users can easily conduct small, quick, and comparative experiments to find insights and superior data recipes, which can then be scaled to larger models and datasets, thereby optimizing the return on investment in data-model co-development. Additional details and empirically validated support can be found in [14]."}, {"title": "5.4 Internal Adaptation for DJ-Dataset", "content": "Engine-Agnostic Processing. In Data-Juicer 1.0, the data processing pipeline is implemented using two distinctive execution modes: a standalone mode tailored for single-node operations and a distributed mode designed for scalability over multiple nodes. These modes exploit different dataset classes, each with a unique set of functionalities and interfaces. The default standalone execution mode employs the Hugging Face Dataset class [37], which provides a rich array of encapsulated functions, such as dataset.map() and dataset.filter(). It's also equipped with configurable batch processing capabilities essential for various computational needs. In contrast, the distributed execution mode leverages the Ray Dataset class [44], which scales effectively across multiple nodes.\nDespite both dataset classes using a storage format based on Apache Arrow [3], they exhibit significant differences in the behaviors and interfaces exposed. For instance, the Ray Dataset delineates individual and batched sample processing using separate methods: map and map_batches. It also allows to specify GPU counts for optimized scheduling. Meanwhile, the Hugging Face Dataset excels in supporting a broad spectrum of data modalities, such as image and audio. Moreover, the Hugging Face Dataset is typically applied in read-heavy data processing scenarios, such as in-memory tokenization and tensor reshaping, which are crucial for training deep learning models. For processing tasks in the context of foundation models, especially those involving synthesis operations, write-heavy procedures warrant attention. Here, the Ray Dataset provides flexible data exportation techniques advantageous to such tasks.\nAs highlighted in Sec. 5.1, we introduce a top-level DJ-Dataset class in Data-Juicer 2.0, along with common functions to bridge the variety of interfaces and implementation specifics across diverse computational engines. Besides the support of Hugging Face Dataset and Ray Dataset, our DJ-Dataset is also seamlessly extendable to support distributed computing within Alibaba Cloud ecosystems, thanks to the compatibility between ours and MaxFrame-DataFrame classes with intermediate in-memory formats like Pandas [48] and NumPy [31] or external-memory formats like Parquet.\nThus, we develop unified wrappers that extract the core execution functions of different DJ-Operators into User-Defined Functions for the MaxCompute engine. Compared to Spark, MaxCompute is compatible in both syntax and runtime on Alibaba Cloud nodes and can be considered a commercially optimized version. An internal empirical comparison shows that MaxCompute SQL achieves 50% better performance than native Spark SQL.\nThe Deduplicators within Data-Juicer 2.0. Fuzzy deduplication is complex, involving a mixture of operations such as map, filter, group by, aggregate, and join [9]. The support and performance of these operations vary across different engines, especially in large-scale scenarios. To demonstrate the engine-agnostic feature of Data-Juicer 2.0, we use our minhash_deduplicator as an example, which supports the aforementioned three different engines. Users only need to specify algorithm-specific parameters such as jaccard_threshold and num_permutations. These parameters remain consistent regardless of the engine used, while engine-specific details and optimizations are completely transparent to the user. For instance, we utilize Ray Actors to implement our Ray-based deduplicator, starting with a load-balanced, distributed union-find algorithm [35] and introducing a hash-based aggregation process to enhance memory utilization and efficiency. This method avoids fragmented unions caused by Ray's native groupby operation, which is computationally expensive in typical LSH implementations with traditional big-data engines [45]. As a result, we achieve a 3.3x speedup over our vanilla Ray version. More performance evaluations are presented in Sec. 6.1.4."}, {"title": "5.4.2 Fault Tolerance", "content": "In practical processing scenarios, datasets often contain schema-incompatible or corrupted data elements, such as improperly formatted JSON objects or damaged images that cannot be read. This issue becomes increasingly important with large-scale datasets, as processing tasks may extend over several hours or even days. In Data-Juicer 1.0, corruption of a single sample would halt the entire processing task, resulting in a waste of computational resources and the loss of already processed samples.\nTo address this issue, Data-Juicer 2.0 introduces a sample-level fault tolerance mechanism designed to enhance processing reliability by providing a worst-case guarantee. A unified exception manager is implemented to automatically capture runtime errors with customizable handlers during the processing of each OP. By default, dataset processing operations are performed at the data-batch level for a general handler. As a result, the system can easily bypass problematic samples by skipping the affected batch (as demonstrated in Fig. 5), while actively tracking and reporting these cases for subsequent debugging and correction. This ensures a seamless user experience and minimizes retry costs in scenarios involving large-scale data processing. Users have the flexibility to either discard these samples or mark them for future reprocessing."}, {"title": "5.4.3 Streaming I/O and Subset Splitting", "content": "Memory constraints frequently emerge as a bottleneck in data processing tasks associated with foundation models. Memory demands can be substantial, and its precise usage is difficult to predict beforehand. Influential factors include the actual storage demands of objects such as text, image, and audio within individual dataset samples, as well as the storage demands of auxiliary or newly generated objects at runtime. These can stem from varying OP specifications, model sizes, synthesis data volumes, intermediate variables, the number of concurrent processes, and specific computational engines.\nTo effectively adapt to a diverse range of data processing scenarios with varying data volumes and available computational resources, we introduce streaming loading and data pre-splitting capabilities in Data-Juicer 2.0. They collectively facilitate improved computational resource utilization, and provide potential for a flexible programming space of hybrid stream and batch processing.\nFirstly, Data-Juicer 2.0 offers a streaming loading interface, addressing the current lack of native support in the Arrow framework underlying Hugging Face and Ray Datasets for streaming JSONL data. As many foundation model datasets use JSONL, we developed an in-house patch to alleviate Out-of-Memory (OOM) issues.\nSecondly, we develop a user-friendly script that automatically pre-splits the original dataset based on two observations: (1) The size limit of the underlying Apache-Arrow block, and (2) The inherent automatic block-splitting strategy in Ray. When there are tens of thousands of nodes but with only a few dataset files, Ray would split the dataset files according to the available resources and distribute the blocks of the dataset to all nodes, which brings a huge network communication cost and decreases the CPU utilization of each node. Thus, we split the original dataset into smaller 128MB files in advance automatically according to the dataset size and the number of distributed nodes, trying to adapt the features of Arrow and Ray for better performance (an empirical study can be found in Sec. 6.1.4). This approach reduces location and reprocessing costs associated with fault tolerance and helps mitigate network exchange overheads, especially beneficial in contexts involving large-scale multimodal data, as well as in scenarios that require handling global objects of Ray Actor in distributed modes."}, {"title": "5.5 Internal Adaptation for DJ-Operators", "content": "This section explores the internal adaptations developed for DJ-Operators, which are crucial for optimizing resource allocation and user experience without requiring users to understand hardware specifics or implementation details. We implement several automatic adaptation features for resource management, aiming at balancing resource constraints and operational efficiency within Data-Juicer 2.0. These strategies include automatic OP reordering at the recipe level (Sec. 5.5.1), automatic parallelism at the OP level (Sec. 5.5.2), and automatic data insight mining to assess the impact of each OP on data samples, considering both upstream and downstream OPs (Sec. 5.5.3). These features are encapsulated in a dedicated Adapter class, which uses a probe_small_batch() method to systematically probe and analyze essential information by applying individual OPs on randomly sampled data in runtime, with default sample size as min(1000, remaining_data_size)."}, {"title": "5.5.1 Workloads-Aware OP Reordering", "content": "At the recipe level, we introduce a new probe-based OP reordering strategy. In Data-Juicer 1.0, an OP fusion optimization was proposed to eliminate redundant computations for the Filter OPs, which involved three key steps: detection and grouping of fusible OPs, OP fusion, and OP reordering. The reordering strategy aimed to position more time-consuming OPs at the end of the group to process fewer samples to save time, as some were filtered out by preceding OPs. It is assumed that the fused OPs are the most time-consuming, and only the fused OPs are moved to the end of each group.\nHowever, the assumption is not always correct and the prior reordering strategy omit the unfused OPs, rendering it greedy and suboptimal, especially when applied to diverse datasets and data recipes characterized by varied data distribution and OP orchestration. In Data-Juicer 2.0, we advance the reordering strategy to an adaptive approach, which is workload-aware and can be applied automatically to unfused OPs as well.\nSpecifically, before processing the full dataset, Data-Juicer 2.0 utilizes the probe functionality of Adapter to obtain estimated processing speeds for individual OPs relevant to specific input datasets. When processing the entire dataset begins, OPs in each group (including the unfused ones) are reordered based on the probed speeds and the commutativity of the Filter OPs. For the fused OP, assuming that there are n fusible OPs in it and their probed speeds are $v_i$ where $i \\in \\{1,2,..., n\\}$, the estimated speed of the fused OP can be calculated as:\n$$v_{fused} = \\frac{N}{\\frac{N}{T_{total}}} = \\frac{N}{\\sum_{i=1}^n \\frac{N}{v_i}} = \\frac{1}{\\sum_{i=1}^n \\frac{1}{v_i}}$$\nwhere N is the total number of samples to be processed by the fused OP and $T_{total}$ is the total time cost of it. Then, faster OPs are prioritized, while slower ones are deferred to later stages. This probe-based approach identifies the globally optimal reordering solution for each OP group, outperforming the suboptimal strategies of the previous version, as empirically validated in Sec. 6.2.1."}, {"title": "5.5.2 Automatic Operator-Wise Parallelism", "content": "In data processing for foundation models, it is crucial to recognize that different OPs require vastly different computational resources. Model-based OPs often need several gigabytes of GPU memory, while simple rule-based OPs may only need minimal CPU processing. Therefore, using a uniform parallelism granularity across all OPs in a data pipeline can cause OOM issues for some and resource underutilization for others. To address this challenge, we introduce several automatic mechanisms for OP-wise parallelism.\nModel-based OPs that integrate substantial models require considerable computational time, potentially spanning hundreds of hours on CPUs for large datasets. To mitigate this, Data-Juicer 2.0 expedites these OPs by automatically leveraging CUDA and GPUs when available. Given the diverse memory requirements of large models, we utilize the Adapter component to conduct quick VRAM benchmarking prior to full-size dataset processing. This information is systematically assigned to the mem_required parameter of the respective OPs. To further alleviate GPU memory demand, we also integrate quantization libraries such as VLLM [36] to enable efficient inference of leveraged models.\nFor non-model-based OPs, attributes such as cpu_required and mem_required are crucial. In Data-Juicer 2.0, users can specify a global parallelism level, typically aligning with the available processor count. Meanwhile, we calculate the adaptive number of processors to finely optimize the entire processing pipeline, aiming to maximize resource utilization (90% by default) as much as possible. For this purpose, the Adapter is also employed dynamically and instrumental in determining precise cpu_required and mem_required values at runtime.\nFor general-purpose and I/O-intensive OPs, Data-Juicer 2.0 enables batched processing, using a robust default batch_size parameter guided by performance saturation trends shown in Sec. 6.2.3. Batched processing reduces I/O overhead, boosts efficiency per processor, and enhances overall speed. Additionally, we introduce hierarchical parallelism for OPs involving multimodal data I/O, such as image_aspect_ratio_filter. These OPs utilize multi-process and GPU parallelism, along with multiple threads, to handle data batches more efficiently, taking the concurrent opportunities between I/O and computation latencies."}, {"title": "5.5.3 Automatic Operator-Wise Insight Mining", "content": "The cumulative impact of a sequence of OPs on a dataset can be complex, and the combined effectiveness is not always greater than that of a single OP. This is demonstrated by experiments on cutting-edge foundation models for text and video [14]. Traditionally, systems like Data-Juicer 1.0 [12] and Falcon [50] primarily generated coarse-grained visualizations such as Sankey diagrams to show percentage changes in data volume, mainly through Filters."}, {"title": "6 SYSTEM EXPERIMENTS", "content": "In this section, we present practical patterns and anti-patterns of Data-Juicer 2.0 based on extensive performance evaluations."}, {"title": "6.1 Cloud-scale Data Processing Efficiency", "content": "Experiment Settings. We start with a base dataset comprising 560k image-text pairs (about totally 5.6 million textual tokens). We expand this dataset by factors of {2, 4, 20, 100, 500, 2500, 12500, 125000}, resulting in nine datasets scaling to 70B data samples. These datasets are categorized into three scales: small (1x, 2x, 4x), medium (4x, 20x, 100x), and large (100x, 500x, 2500x, 12500x, 125000x). For different scales, we prepare both multimodal and text-only data processing recipes, each containing five OPs. We run the recipes on these datasets using various computing engines (Standalone, Ray, MaxCompute) with different Alibaba Cloud resources, including ECS instances, PAI-DLC, and MaxCompute, spanning 1 to 100 nodes and 64 to 12,800 CPU cores. To ensure a fair comparison, we use the same worker node configuration, such as CPU frequency, across different computing engines."}, {"title": "6.1.2 The Case of Small Scales", "content": "As shown in Fig. 7a, when processing small-scale multimodal datasets, the standalone mode with Hugging Face Dataset is efficient and comparable to the Ray mode with a single node. Additional Ray nodes provide further but limited acceleration (speedup ratios between 138% and 226% with 4 nodes). For text-only datasets (Fig. 7b), the standalone mode remains efficient. However, for the Ray mode, 4x node increments yield smaller speedups (148%) or even increased processing times due to dominated I/O and communication costs. Thus, with Data-Juicer, processing datasets with hundreds of thousands of samples on a single machine is both efficient and cost-effective for most users."}, {"title": "6.1.3 The Case of Medium Scales", "content": "When datasets are scaled to 56M samples, processing times increase significantly with the standalone mode, thus, its performance is omitted in Figures 7c and 7d. Here, the Ray mode outperforms in all instances, demonstrating considerable"}]}