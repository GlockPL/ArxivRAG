{"title": "ReasonAgain: Using Extractable Symbolic Programs to Evaluate Mathematical Reasoning", "authors": ["Xiaodong Yu", "Ben Zhou", "Hao Cheng", "Dan Roth"], "abstract": "Existing math datasets evaluate the reasoning abilities of large language models (LLMs) by either using the final answer or the intermediate reasoning steps derived from static examples. However, the former approach fails to surface model's uses of shortcuts and wrong reasoning while the later poses challenges in accommodating alternative solutions. In this work, we seek to use symbolic programs as a means for automated evaluation if a model can consistently produce correct final answers across various inputs to the program. We begin by extracting programs for popular math datasets (GSM8K and MATH) using GPT4-0. For those executable programs verified using the original input-output pairs, they are found to encapsulate the proper reasoning required to solve the original text questions. We then prompt GPT4-o to generate new questions using alternative input-output pairs based the extracted program. We apply the resulting datasets to evaluate a collection of LLMs. In our experiments, we observe significant accuracy drops using our proposed evaluation compared with original static examples, suggesting the fragility of math reasoning in state-of-the-art LLMs.", "sections": [{"title": "1 Introduction", "content": "Mathematical reasoning is a fundamental skill essential for numerous complex applications, leading to a recent growing research effort on advancing large language models (LLMs) in this area. Thus, proper evaluation of LLMs' mathematical reasoning is crucial. Most previous studies have primarily evaluated LLMs using static datasets, such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021). Typically, evaluations focus solely on the final answers, overlooking reasoning flaws (Lewkowycz et al., 2022) and potential data contamination issues. Despite impressive results, LLMs can reply on shortcuts rather than true reasoning, displaying high sensitivity to input tokens (Li et al., 2024b,a). Alternatively, some works (Sawada et al., 2023; Golovneva et al., 2023) use model-based techniques to assess the reasoning quality, but these can suffer from model biases, limiting accommodation for alternative solutions.\nIn this paper, we present a focused study on evaluating mathematical reasoning which can be concisely encapsulated by symbolic programs, i.e., Python programs. For such cases, we can automatically generate a diverse set of new test cases (input-output pairs) by varying the valid inputs fed into the program. Thus, if LLMs truly employ the appropriate reasoning process (as embodied by the programs) to solve the original question, they should also be able to consistently solve all new test cases. This approach allows us to evaluate the reasoning quality directly by examining the final answers, without ruling out alternatives.\nTo avoid costly manual annotations, we use the state-of-the-art (SoTA) LLM (GPT4-0) to generate Python programs for GSM8K and MATH. We retain only those questions with extractable programs, which can be automatically validated for our evaluation. This means the programs can be executed to produce the original gold answers. Upon manual inspection, 92% and 83% of the programs from GSM8K and MATH genuinely demonstrate the correct reasoning process required to solve the original questions. We then prompt GPT4-o to propose alternative valid inputs based on the extracted program and the original question. These inputs are then used to generate new input-output pairs derived from the program. Finally, GPT4-o is tasked to update the original question using these proposed inputs to create new test cases for evaluation.\nOur experiments reveal significant declines in the performance of SOTA LLMs when evaluated on our generated data. For example, for ques-"}, {"title": "2 Methods", "content": "Assessing the reasoning capabilities of large language models (LLMs) presents significant challenges, primarily because the reasoning process is not consistently articulated, and standardizing its representation is difficult. Moreover, multiple reasoning paths may exist to arrive at the same solution. Consequently, it is impractical to simply output the reasoning process and evaluate its correctness directly. Typically, the accuracy of reasoning is assessed through question-answering formats, such as verifying the accuracy of an answer to a mathematical problem. However, this paper contends that relying solely on a single question-answer pair is inadequate for genuinely assessing reasoning capabilities because: 1) an incorrect reasoning path may coincidentally yield the correct answer, and 2) potential data contamination could enable models to memorize answers without engaging in a legitimate reasoning process. To effectively evaluate the reasoning abilities of LLMs, we introduce ReasonAgain, which conceptualizes the reasoning process within Python code and automatically generates five additional perturbations of the same question. These perturbations retain the original reasoning process but feature different input values, thereby testing whether the model genuinely employs a correct reasoning process. The pipeline of ReasonAgain is illustrated in Figure 1."}, {"title": "Encapsulating the reasoning process.", "content": "To explicitly represent the reasoning process of a math question, we first ask a pivot LLM (GPT-40) to generate the parameters of questions."}, {"title": "3 Experiments", "content": "We show our main experiment results using our proposed ReasonAgain evaluation pipeline in Table 1. We observe a substantial performance drop across all models on both GSM8K and MATH. For direct inference, models experience 10%-15% drop in performance, regardless of their size and capabilities. The decline is not mitigated by chain-"}, {"title": "3.1 Experiment Settings", "content": "Datasets. We sample 2k questions from GSM8k (Cobbe et al., 2021) and 1k questions from MATH (Hendrycks et al., 2021). As discussed in Section 2, we first ask the model to generate the Python code for each question, and then we filter out all the problematic code that cannot be compiled or fail to return the correct gold answer. After filtering, in total, we have 1121 cases from GSM8k, and 268 cases from MATH. For each case, we use ReasonAgain to generate 5 perturbations as the new test cases, which gives us 5605 cases for GSM8k, and 1072 cases for MATH. We use GPT-40 (OpenAI et al., 2024) as the pivot LLM to generate all the parameters, code, and perturbations."}, {"title": "Baselines.", "content": "We evaluate 4 LLMs in this paper: GPT-4-Turbo (OpenAI et al., 2024), GPT-40 (OpenAI et al., 2024), LLama-3.1-8B (Dubey et al., 2024), and Qwen-2.5-7B (Team, 2024) using the following different prompting settings: direct, few-shot Chain-of-thought (CoT) (Wei et al., 2022), and few-shot Chain-of-thought + self-consistency (CoT+SC) (Wang et al., 2022).\nDirect: We ask the model to directly answer the question without providing any examples using the following prompts."}, {"title": "Evaluation Metrics.", "content": "We report Exact Match accuracy (EM) for all the experiments. Predicted answers are parsed by CoT format, and we round both gold answers and predicted answers before checking if the values are same."}, {"title": "3.2 Main Results", "content": "We show our main experiment results using our proposed ReasonAgain evaluation pipeline in Table 1. We observe a substantial performance drop across all models on both GSM8K and MATH. For direct inference, models experience 10%-15% drop in performance, regardless of their size and capabilities. The decline is not mitigated by chain-"}, {"title": "3.3 Human Evaluation", "content": "To assess whether the generated code accurately embodies a valid reasoning process, we randomly sample 200 cases from GSM8K and MATH (100 each), and ask three human experts to judge the correctness of our generated perturbations. Specifically, the annotators are asked to understand the generated code, and check the correctness of the target answers of perturbations. In summary, we find 8 of the 100 cases from GSM8K and 17 of the 100 cases from MATH contain errors. These issues are mainly due to some positive parameters being negative or the model failing to generate the correct program that encapsulates the necessary reasoning process, which can be potential directions for further improvements. Despite these errors, the majority of our new test cases remain valid and useful for proper evaluation purposes."}, {"title": "4 Related Work", "content": "Many works have discussed language model bias and inconsistency during reasoning (Li et al., 2024b,a; Zhou et al., 2024) and adversarial and contrastive evaluation (Gardner et al., 2020; Patel et al., 2021; Yu et al., 2024). Here, we provide a novel way for automatic mathematical reasoning evaluation by checking the reasoning reliability using alternative input-output pairs with the same text question context. While previous studies have successfully used decomposed methods to solve math questions more reliably (Hao et al., 2023; Madaan et al., 2023; Gao et al., 2023; Xia et al., 2024), our work highlights the reasoning challenges faced by existing LLMs. This indicates a need for more advanced developments to further improve the reliability of LLMs in mathematical reasoning. Another related line of work (Xia et al., 2024, inter alia) aims to surface the reasoning flaws of LLMs by examining their intermediate steps (e.g., CoT processes). In contrast, we bypasses the process evaluation and instead evaluate whether the model truly understand how to solve a problem by checking the consistency of its answers using the same reasoning process encapsulated in a symbolic program. We have noticed a contemporary work (Mirzadeh et al., 2024) that also generates perturbations of math questions to evaluate the LLMs' mathmatical reasoning capabilities. However, while Mirzadeh et al. (2024) uses symbolic templates to create perturbations, we leverage Python code extracted by"}, {"title": "5 Conclusion", "content": "In this work, we propose ReasonAgain, a novel evaluation method to better benchmark large language models' true capabilities on mathematical reasoning. ReasonAgain employs a symbolic program-based perturbation method that changes the numerical values in the original math questions and derives the corresponding target answers. We then evaluate models on such perturbed questions. Experiments show that representative SoTA LLMs perform significantly worse on our modified questions, suggesting that 1) existing models do not truly understand the reasoning process behind math questions, even when they occasionally predict the correct answer; 2) existing static data based evaluation methods are inadequate, leading to an overly optimistic perception of model performances in mathematical reasoning. ReasonAgain offers a more effective alternative for evaluating LLMs' reasoning capabilities."}, {"title": "Limitations", "content": "Our work has several limitations.\nImperfect Programs. As pointed out in \u00a73.3, some mistakes exist in the current generated programs, which leads to partially incorrect gold labels in some perturbed questions. We will explore better filtering mechanisms in later versions. However, such mistakes do not impact our overall conclusion, as model performances are much lower than the upper bounds.\nLimited Program Coverage. Our program generation is limited by a conceptualization process proposed in Zhou et al. (2024), which does not work well on certain types of math questions, such as geometry-related ones. As a result, ReasonAgain only works on a subset of all existing math questions.\nLimited Reasoning Types. Our general formulation can be applied to other reasoning types, such as multiple-choice questions. However, we only focus on math questions in this work."}]}