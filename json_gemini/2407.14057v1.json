{"title": "LazyLLM: DYNAMIC TOKEN PRUNING FOR EFFICIENT\nLONG CONTEXT LLM INFERENCE", "authors": ["Qichen Fu", "Minsik Cho", "Thomas Merth", "Sachin Mehta", "Mohammad Rastegari", "Mahyar Najibi"], "abstract": "The inference of transformer-based large language models consists of two sequen-\ntial stages: 1) a prefilling stage to compute the KV cache of prompts and generate\nthe first token, and 2) a decoding stage to generate subsequent tokens. For long\nprompts, the KV cache must be computed for all tokens during the prefilling stage,\nwhich can significantly increase the time needed to generate the first token. Con-\nsequently, the prefilling stage may become a bottleneck in the generation process.\nAn open question remains whether all prompt tokens are essential for generating\nthe first token. To answer this, we introduce a novel method, LazyLLM, that se-\nlectively computes the KV for tokens important for the next token prediction in\nboth the prefilling and decoding stages. Contrary to static pruning approaches that\nprune the prompt at once, LazyLLM allows language models to dynamically se-\nlect different subsets of tokens from the context in different generation steps, even\nthough they might be pruned in previous steps. Extensive experiments on standard\ndatasets across various tasks demonstrate that LazyLLM is a generic method that\ncan be seamlessly integrated with existing language models to significantly ac-\ncelerate the generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34\u00d7 while maintaining accuracy.", "sections": [{"title": "1 INTRODUCTION", "content": "Standard prompt-based LLM inference has two sequential stages: prefilling and decoding, as shown\nin Figure 1. During the prefilling stage, the model computes and saves the KV cache of each token\nfrom the prompt, and predicts the first token. We refer to the time taken during prefilling stage as\n\"time-to-first-token\" (TTFT). Following the prefilling stage is the decoding stage, where the model\nreuses cached KVs to decode the next token iteratively until the stop criteria are met.\nDuring the prefilling stage, all tokens from the prompt are used by all transformer layers. For long\nprompts, TTFT could be slow because state-of-the-art transformer-based LLMs are both deep and\nwide (Pope et al., 2023; Kim et al., 2023; Aminabadi et al., 2022), and the cost of computing at-\ntention increases quadratically with the number of tokens in the prompts. For instance, Llama 2\n(Touvron et al., 2023), with 7 billion parameters, stacks 32 transformer layers with a model dimen-\nsion of 4096. In this scenario, TTFT requires 21\u00d7 the walltime of each subsequent decoding step,\nand accounts for approximately 23% of the total generation time on the LongBench benchmark\u00b9\n(Bai et al., 2023). Therefore, optimizing TTFT is a critical path toward efficient LLM inference\n(NVIDIA, 2024).\nWhile optimizing LLM inference is an active area of research, many methods (Leviathan et al.,\n2023; Cai et al., 2024; Zhang et al., 2024; Bhendawade et al., 2024; Li et al., 2024) have focused on\nimproving inference speed during the decoding stage. Yet, there is little attention given to improving\nTTFT. We note that some compression-based works implicitly improve the TTFT by reducing the"}, {"title": "2 RELATED WORK", "content": "The increase in the scale of large language models (LLMs) has greatly enhanced their performance\nbut also introduced challenges with respect to their inference efficiency. The inference of generative\nLLMs consists of two distinct stages as depicted in Figure 1. In particular, extensive computation\nis needed under long context scenarios to calculate the full KV cache during the prefilling stage,\nresulting in a long time-to-first-token (TTFT). This delay causes users to wait several seconds after\nsubmitting a prompt before receiving any response from the agent, leading to a poor user experience.\nEfficient Long Context Inference. Extensive work (Merth et al., 2024; Chen et al., 2023; Beltagy\net al., 2020; Kitaev et al., 2020) has been proposed to improve inference efficiency for long context"}, {"title": "3 LazyLLM", "content": ""}, {"title": "3.1 BACKGROUND ON LLM INFERENCE", "content": "Generative LLM inference consists of two stages: prefilling and decoding (see Figure 1). In the\nprefilling stage, the model receives the prompt (a sequence of tokens) $T = {t_i}_{i=1}^N$ of length N,\nwhere $t_i$ denotes a token and N denotes the length of the prompt, then computes and saves the KV\ncache of each token, and produces the first token tn+1. The transformer architecture commonly\nused in LLMs is a stack of layers where each layer shares the same architecture with a multiple\nhead self-attention mechanism followed by a multi-layer perception (MLP). The time of prefilling is\nreferred to as time-to-first-token (a.k.a. TTFT). Following the prefilling is the decoding steps, where"}, {"title": "3.2 INFERENCE WITH LazyLLM", "content": "The overview of the proposed LazyLLM framework is illustrated in Figure 4. LazyLLM starts with\nthe full context and progressively prunes tokens to gradually reduce the number of computations\ntowards the end of the model. Note, LazyLLM allows the model to select different subsets of tokens\nfrom the context in different generation steps, even though some of them may be pruned in previous\nsteps. Compared to static pruning which prunes all the tokens at once, dynamic pruning optimizes\nthe next token prediction in each generation step, which is crucial to retaining the performance.\nProgressive Token Pruning. Prior to this work, token pruning has been successfully applied to\noptimize LLM inference (Zhang et al., 2024; Li et al., 2024; Adnan et al., 2024; Nawrot et al., 2024).\nHowever, these approaches require accumulating the full attention maps of predicting the first few\ntokens to profile the importance of prompt tokens before starting pruning. Consequently, they are\nnot applicable to reduce TTFT as they still require computing all the KV cache at the prefilling stage.\nIn contrast, LazyLLM only \u201clazily\" computes the tokens that are important to predict the next token\nby starting from the first iteration of the inference (the prefilling step). A key challenge to pruning\ntokens in the first iteration is determining their importance. Inspired by the early exiting work\n(Elhoushi et al., 2024) which shows the token hidden states gradually evolve through the transformer\nlayers, we apply layer-wise token pruning in each generation step. Specifically, we use the attention\nmap of the layer $A_l \\in \\mathbb{R}^{H \\times N \\times N}$ to determine the importance of input token $t_i$ w.r.t. the next token\nto be predicted as\n$\\mathbb{S}_i = \\frac{1}{H} \\sum_{h=1}^{H}A_{h,i,N}$ \nwhere H denotes number of attention heads, N is the sequence length, and $A_{h,i,j}$ is the attention\nprobability of the token $t_j$ attending to token $t_i$ at $h^{th}$ head.\nAfter computing the confidence scores of tokens, it is challenging to determine the threshold value\nto prune the token. Concretely, the threshold can change as the distribution of the attention scores\nvaries between different layers and different tasks. We address this challenge by using the top-\nk percentile selection strategy to prune tokens. Specifically, token $t_i$ is pruned at layer $l + 1$ if its\nconfidence scores is smaller than $k^l$th percentile among the input tokens. Once the token is pruned,"}, {"title": "4 IMPLEMENTATIONS DETAILS", "content": "We implement LazyLLM on Llama 2 (Touvron et al., 2023) and XGen (Nijkamp et al., 2023) and\nevaluate it on the LongBench (Bai et al., 2023) using HuggingFace\u00b2. We follow the official GitHub\nrepository\u00b3 of LongBench for data preprocessing and prompting in all experiments. The LongBench\nbenchmark consists of multiple datasets in different tasks, where each task may have different met-\nrics, including ROUGE-L, F1, Accuracy, and Edit Sim. Following the official evaluation pipeline,\nwe categorize all results over major task categories by computing the macro-average score.\nAs previously noted, the proposed LazyLLM doesn't require any training. Thus, LazyLLM uses\nthe exact same existing checkpoints as the baseline, for all models. For inference, we conduct all\nexperiments on NVIDIA A100 GPUs. We measure and report the speedup based on the empirical\nwalltime improvement. Specifically, for TTFT Speedup, we measure the empirical walltime between\nwhen the prompt is fed to the model, and when the model generates the first token. For Generation\nSpeedup, we measure the empirical walltime between when the prompt is fed to the model, and\nwhen the model finished generating all output tokens. We add 5 warmup runs for each experiment\nbefore starting the time measurement to remove the noise such as loading model parameters."}, {"title": "5 EXPERIMENTS", "content": "We examine our method using two large language models: Llama 2 7B and XGen 7B. We compare\nour method with baselines using the same publicly released pretrained checkpoints, without employ-\ning any additional training. We perform experiments using LongBench, a multi-task benchmark for\nlong content understanding. The LongBench comprises 16 datasets and covers 6 tasks including\nsingle-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code comple-\ntion.\nFor the metrics, we primarily evaluate the effectiveness and efficiency of each method in the TTFT\nspeedup vs. accuracy trade-off. Following LongBench, the accuracy (score) denotes the macro-\naveraged scores across datasets in each task. The TTFT speedup measures the wall time improve-\nment w.r.t. to the baseline for generating the first token. In analysis, we also assess the impact of\nour method on % of Prompt Token Computed and Generation speedup. The % of Prompt Token\nComputed measures the accumulated percent of prompt tokens computed at the end of the genera-\ntion, which indicates the save of total computation. The Generation speedup measures the walltime\nchange w.r.t. to the baseline for completing the entire generation process."}, {"title": "5.1 RESULTS", "content": "Table 1 presents the TTFT speedup vs. accuracy comparisons between LazyLLM, standard LLM,\nand other baselines. In the table, the \"baseline\" refers to the standard LLM inference. The \"random\ntoken drop\" baseline is based on (Yao et al., 2022) that randomly prunes the prompt tokens before\nfeeding them to the LLMs. We report the average metrics across 5 runs for the \"random token drop\"\nbaseline. Our \u201cstatic token pruning\u201d baseline prunes input tokens at once based on their attention\nscore of the first few transformer layers during the prefilling stage. We also compare with the prompt\ncompression method (Li et al., 2023) which pruning redundancy in the input context using LLMs.\nTable 1 shows LazyLLM consistently achieves better TTFT speedup with negligible accuracy drop\nacross multiple tasks. It is worth noting that the overhead of running LLMs to compress the prompt\nis very computationally expensive. Even though the inference on the reduced prompt is faster, the\nactual TTFT of the \"prompt compression\u201d baseline is longer than the baseline."}, {"title": "5.2 TTFT SPEEDUP vs. ACCURACY", "content": "The inference efficiency of LazyLLM is controlled using three parameters: 1) the number of pruning\nlayers, 2) the locations of these pruning layers, and 3) the number of tokens pruned within these\nlayers. Increasing the number of pruning layers and pruning more tokens optimize computation\nby processing fewer tokens, and pruning tokens at earlier layers can save the computations for the\nsuccessive layers. Prompting these factors will give more overall computation reduction, and offer\nbetter TTFT speedup. As a side effect, excessively pruning tokens may cause information loss and\neventually lead to performance degradation. Similarly, the TTFT speedup and accuracy of baselines\ncan vary with different hyperparameters.\nWe compare TTFT speedup vs. accuracy in Figure 5 with different hyperparameters. The visual-\nization shows that, without any training, the proposed LazyLLM retains the accuracy better than\nbaselines under the same TTFT speedup. For example, our method can offer 2.34\u00d7 TTFT speedup\nin the multi-document question-answering task with negligible (< 1%) performance loss. By con-\ntrolling the pruning parameters, LazyLLM provides a good trade-off between accuracy and inference\nspeed as compared to baseline methods. For instance, LazyLLM can achieve 3.0\u00d7 TTFT speedup\nin the multi-document question-answering task with < 10% degradation in accuracy. On the other\nhand, baseline methods accuracy degrades significantly for similar TTFT speed-up. Note that the\nprompt compression approaches fail at improving TTFT because of the compression overhead."}, {"title": "5.3 IMPACT ON OVERALL GENERATION SPEED", "content": "To evaluate the impact of the proposed method on the overall generation process, we also profile\nthe % of Prompt Token Computed and Generation speedup in Table 2. We can find the % of Token\nComputed of LazyLLM is less than 100%, indicating that not all tokens in the prompt are selected\nby LazyLLM at the end of the generation, even though theoretically the model could use all tokens.\nComputations in the FFN layers increase linearly, while those in the attention layers grow quadrati-\ncally with the % of Token Computed. A lower % of Token Computed indicates LazyLLM reduces the\ntotal computation, consequently offering additional speedup to the overall generation process across\ndiverse tasks."}, {"title": "5.4 DROP RATE IN DIFFERENT LAYERS", "content": "In this section, we analyze the effect of the locations of pruning layers, and the number of tokens\npruned. In particular, we report a series of experiments using a simplified version of LazyLLM that\nprunes tokens just once within the transformer. For each trial, we position the pruning layer at var-\nious levels of the transformer stack and apply different pruning ratios. We perform the experiments\nfor both Llama 2 and XGen, and visualize the results in Figure 6.\nThe results show both models share a similar trend. As expected, when pruning at the same trans-\nformer layer, the model's performance gradually decreases as fewer tokens are kept. Furthermore,"}, {"title": "5.5 PROGRESSIVE KV GROWTH", "content": "In this section, we characterize the internals of the model with the token pruning logic. Specifically,\nwe seek to understand what fractions of prompt tokens are cumulatively used and, inversely, not\nused. This \"cumulative token usage\" can be equivalently defined as the KV cache size at each given\nstep. Figure 7 presents these cumulative prompt token usage numbers for each of the stages of the\nLazyLLM.\nOur analysis supports the hypothesis that many tokens are never selected by the model (even though\ntheoretically the model could use all tokens in the prompt). Since this model retains accuracy on the\ntask(s), we can conclude that the model effectively drops the tokens which do not affect the output\nquality."}, {"title": "6 CONCLUSION", "content": "In this work, we proposed a novel LazyLLM technique for efficient LLM inference, in particular\nunder long context scenarios. LazyLLM selectively computes the KV for tokens important for the\nnext token prediction and \u201clazily\u201d defers the computation of remaining tokens to later steps, when\nthey become relevant. We carefully examine LazyLLM on various tasks, where we observed the\nproposed method effectively reduces TTFT with negligible performance loss. It is worth noting that\nour method can be seamlessly integrated with existing transformer-based LLMs to improve their\ninference speed without requiring any fine-tuning."}]}