{"title": "DeepCRCEval: Revisiting the Evaluation of Code Review Comment Generation", "authors": ["Junyi Lu", "Xiaojia Li", "Zihan Hua", "Lei Yu", "Shiqi Cheng", "Li Yang", "Fengjun Zhang", "Chun Zuo"], "abstract": "Code review is a vital but demanding aspect of software development, generating significant interest in automating review comments. Traditional evaluation methods for these comments, primarily based on text similarity, face two major challenges: inconsistent reliability of human-authored comments in open-source projects and the weak correlation of text similarity with objectives like enhancing code quality and detecting defects.\nThis study empirically analyzes benchmark comments using a novel set of criteria informed by prior research and developer interviews. We then similarly revisit the evaluation of existing methodologies. Our evaluation framework, DeepCRCEval, integrates human evaluators and Large Language Models (LLMs) for a comprehensive reassessment of current techniques based on the criteria set. Besides, we also introduce an innovative and efficient baseline, LLM-Reviewer, leveraging the few-shot learning capabilities of LLMs for a target-oriented comparison.\nOur research highlights the limitations of text similarity metrics, finding that less than 10% of benchmark comments are high quality for automation. In contrast, DeepCRCEval effectively distinguishes between high and low-quality comments, proving to be a more reliable evaluation mechanism. Incorporating LLM evaluators into DeepCRCEval significantly boosts efficiency, reducing time and cost by 88.78% and 90.32%, respectively. Furthermore, LLM-Reviewer demonstrates significant potential of focusing task real targets in comment generation.", "sections": [{"title": "1 Introduction", "content": "Since Fagan's pioneering work on software inspections in 1976 [7], code review practices have evolved significantly. Early software inspections were formal and"}, {"title": "2 Background", "content": ""}, {"title": "2.1 CRCGs and Their Evaluation", "content": "For code review automation, researchers predominantly utilize deep learning or information retrieval techniques as code review comment generators (CRCGs) to automatically generate comments for given code snippets. Despite some variations, Figure 1 depicts the typical workflow for training a deep neural network (DNN) model or constructing a retriever for automating code review. Initially, a DNN model or a retriever is trained or established using a dataset of code-comment pairs from OSS projects, learning the semantic relationships between code and comments. Then, \u2461 for each test case code snippet, the model or retriever generates or retrieves a comment employing specific decoding or retrieval techniques. Finally, the produced comments are compared against the original ones from the test set, also sourced from OSS projects. Common comparison metrics include BLEU [18] and ROUGE [16], where BLEU assesses the precision of machine translations by measuring the match of N-grams, and ROUGE evaluates the recall of machine-generated summaries based on N-gram co-occurrence. These metrics also guide the training of DNNs.\nHowever, the quality and validity of the original comments extracted from OSS projects are questionable. For example, the comments in Table 1 are not suitable for use as code review comments by models, although they might be meaningful for human reviewers. Specifically, \"Why do we need this?\" is indeed"}, {"title": "2.2 Large Language Models for Evaluations", "content": "Large language models have shown capabilities similar to those of human evaluators. Studies have demonstrated that large language models like GPT-4 can achieve higher agreement than human evaluators [6, 13, 20,31] for ranking preference and scoring. For instance, in the evaluation of general text generation, GPT-4's agreement rate with human evaluations (85%) surpasses the rate of inter-human agreement (81%) [31], a finding echoed in another dataset comparison (69.2% vs. 65.7%) [13]. Considering this, our evaluation framework is based on these works but is more granular and lightweight. We use a chain-of-thought template to inject task-specific knowledge for better scoring and ranking.\nSince there is no prior proof of the effectiveness of LLM evaluators in the task of code review comment generation, we adopt both human and LLM evaluators for all evaluation parts of our paper. The LLM evaluators serve as an auxiliary certification, expanding the scope of verification."}, {"title": "3 Overview and Research Questions", "content": "Automated code review has garnered significant interest from researchers, yet its evaluation effectiveness has not been sufficiently addressed. This oversight leads to a disconnect between the task objectives and the training processes used. This paper aims to highlight the importance of effective evaluation in code review automation. As depicted in Figure 2, we developed DeepCRCEval, a new evaluation framework incorporating both human and LLM evaluators, and \u2461LLM-Reviewer, a lightweight, training-free, and target-oriented baseline tool. Our study focuses on: 1) verifying the shortcomings of current evaluation metrics, 2) manually reassessing the evaluation of existing code review comment generators, 3) integrating LLM-based alternative evaluators for an extended scope of validation, and 4) identifying potential improvement directions stemming from the misalignment of task objectives and training processes. The latter two RQs aim to set directions for future research. We next introduce the research questions we aim to investigate and their relationships.\nRQ1. Analysis of Benchmark Comments: Are the foundations of current evaluation metrics reliable? The reliability of text similarity metrics for evaluating review comments depends on the quality and validation of the reference texts. We first analyze the benchmark reference review comments from four perspectives: quality, category, tone, and context. We present results for each aspect and summarize the relationships between these aspects.\nRQ2. Efficacy of DeepCRCEval: Why do we claim that DeepCRCEval provides a deeper evaluation, and why do we integrate LLM evaluators? Before presenting the new results obtained in RQ3, we first examine the differences between our DeepCRCEval and traditional text similarity metrics. Additionally, we evaluate the advantages and disadvantages of using LLM"}, {"title": "4 RQ1. Analysis of Benchmark Comments", "content": ""}, {"title": "4.1 Aspects for Analysis", "content": "To thoroughly explore previous code review comment datasets, we defining aspects with a qualitative and quantitative process [4], which draws on varied data sources for comprehensive insights, including the review of previous study [11], our semi-structured interviews [17] with seven industry developers, and subsequent card sort and affinity diagram by the authors of this paper. The detailed process are shown in Section B. These gained criteria are utilized for assessing benchmark comment quality, which will also for the subsequent reevaluation.\nQuality The multi-step study introduced in Section B culminated in identifying nine key criteria for evaluating the effectiveness of code review comments:\nC1. Readability: Clear, easily understandable language.\nC2. Relevance: Directly related to the specific code.\nC3. Explanation Clarity: Clear elucidation of the issues identified.\nC4. Problem Identification: Accurate pinpointing and articulation of bugs.\nC5. Actionability: Practical advice for addressing identified issues.\nC6. Completeness: Coverage of all issues in the code for comprehensive review.\nC7. Specificity: Focus on specific code issues, avoiding generic statements.\nC8. Contextual Adequacy: Comments pointing out exact issue locations.\nC9. Brevity: Conciseness, conveying essential information without verbosity.\nCategory We evaluated the comments' ability to detect defects or suggest code improvements using a classification system. This involved adopting the nine categories proposed by Bacchelli et al. [1], such as Code Improvement, Understanding, and Social Communication, and introducing an additional category, \"Meaningless Text\", for extremely low-quality, uninformative comments."}, {"title": "4.2 Analysis Methodology", "content": "Analyzing code review comments is a nuanced and labor-intensive task. To manage this, we sampled 100 comments from each of the two primary datasets in this domain: the Tufano dataset [27] and the CodeReviewer dataset [14], abbreviated as Tufano and CRer, respectively. According to the average reliability of 93% for humans in Table 6, the margin of error for 95% confidence level of 100 sample size is within 5%. The Tufano dataset is a monolingual, function-level Java dataset, while the CRer dataset is multilingual and at the diff granularity. Both are constructed from large-scale open-source software repositories and widely utilized in numerous studies. The quality and category is finished with a human scoring system created using QT and and a Delphi Method variant [5] by five master's and doctoral students, respectively. The Tone and context was conducted using the aforementioned NGT sessions by three authors of this study. The detailed process are shown in Section C."}, {"title": "4.3 Results of Analysis", "content": "We first present the results from each aspect, and then summarize the relationships between each aspect using a Venn diagram in Section 4.3.\nQuality Our analysis, illustrated in the upper part of Table 2, indicates that while OSS review comments typically exhibit good readability and brevity, they frequently lack in other critical aspects, reflecting issues of arbitrariness, incompleteness, and irregularity. We designated scores below 6 (out of 10) as poor performance indicators for each aspect. The lower part of Table 2 shows the proportion of comments scoring poorly in each aspect. This data highlights deficiencies of comments in aspects other than readability, completeness, and brevity. For example, low scores in explanation clarity or actionability hint at"}, {"title": "5 Revisiting the Evaluation of CRCGS", "content": "In light of the inadequacy of text similarity metrics for evaluating Code Review Comment Generators (CRCGs), this section introduces our novel evaluation framework, DeepCRCEval, and a target-oriented baseline, LLM-Reviewer, to reassess the performance of CRCGs."}, {"title": "5.1 DeepCRCEval", "content": "Drawing from the criteria established in Section 4.1, we developed DeepCRCEval, an evaluation framework designed to rigorously analyze the quality of generated comments. DeepCRCEval employs both human and Large Language Model (LLM) evaluators. Initially, we conducted human evaluations on a sampled test set due to cost considerations, then extended the evaluation to the entire test set using LLMs."}, {"title": "5.2 Selected CRCGS", "content": "For our analysis, we first selected all current state-of-the-art (SOTA) CRCGS published in top-tier conferences. This includes models like Tufano et al. [27], AUGER [12], CodeReviewer [14], and CCT5 [15], primarily based on DNN models, alongside CommentFinder [10], which uses retrieval techniques. Notably, all these CRCGs have been trained or constructed using data from OSS projects and guided by text similarity metrics, which may not align well with the primary goal of identifying defects or enhancing code.\nTo evaluate CRCG performance effectively, an appropriate baseline is crucial. We sought a baseline that directly targets defect detection and code improvement. Given the absence of such a baseline in existing literature, we introduce LLM-Reviewer, a novel, straightforward, and fair baseline. It resembles DNNs but does not require additional training."}, {"title": "5.3 LLM-Reviewer", "content": "Figure 4 illustrates the workflow of LLM-Reviewer. Distinct from traditional CRCGS, LLM-Reviewer operates without the need for a training set. The process for each new code snippet is: Integration of the code with a pre-defined prompt. Feeding this combined input into the selected LLM to generate a comment. Direct evaluation of the generated comment's quality using DeepCRCEval.\nThe pre-defined prompt is bolstered with specific guidance and few-shot instructions. The utilized in-context prompt is:\n$P_{Reviewer} = Des_{gen} + G_{Gen} + Dem_{ok}$ (3)\nwhere $Des_{Gen}$ offers concise directives for generating code review comments, $G_{Gen}$ encompasses guidelines including notes and criteria descriptions (as discussed in Section 4.1), and $Dem_{ok}$ features k randomly chosen demonstrations, adhering to k-shot learning principles. For this study, we opted for k = 3, balancing input length with informative content. Each prompt component is clearly separated by \"###\" as detailed in Table 10.\nThis prompt is then integrated with the code snippet under review before being fed into the LLM for comment generation. It is worth noting that we use a similar prompt template to the one used for prompt evaluation. This similarity is why we refer to LLM-Reviewer as a target-oriented model. Unlike other text similarity models, the similar prompt template constrains LLM-Reviewer's response to the expected target. Additionally, as a countermeasure against bias, we use human evaluators throughout the entire study."}, {"title": "5.4 Other Experimental Details", "content": "Testing Set Since LLM-Reviewer does not require a training dataset, we selected a set of 1,000 code cases with typical issues for testing. These cases are processed by humans to enhance simplicity, and thus to reduce the risk of data leakage. Each case is guaranteed to contain at least one significant issue within one or a few functions, compatible with the input format of earlier CRCGs. We used ROUGE-L to remove duplicates. This test set is also accessible in our open-source repository. For baselines, we utilized their respective training sets."}, {"title": "6 RQ2-RQ3: Empirical Findings", "content": "This section presents our empirical findings, detailing the performance of various CRCGS as evaluated by our framework DeepCRCEval."}, {"title": "6.1 RQ2. Efficacy of DeepCRCEval", "content": "This subsection examines the efficacy of our DeepCRCEval framework, specifically assessing the criteria used for evaluation and the integration of LLMs.\nEffectiveness of Criteria Our evaluation framework, DeepCRCEval, surpasses traditional text similarity metrics in two key areas: discrimination and comprehensiveness.\nDiscrimination Prior studies often reported marginal improvements in text similarity metrics, like a less than 1% increase in BLEU scores. Such negligible enhancements do not reliably indicate an improvement in comment quality, as corroborated by our reevaluations. While newer baselines like CCT5 and AUGER reported improvements in text similarity, they did not surpass their predecessor, Tufano et al., in effectiveness a conclusion also supported by our qualitative case studies. In contrast, DeepCRCEval, with its well-defined criteria, offers a higher degree of discrimination across various aspects. Moreover, our ranking process provides a direct and comparative analysis of comment quality.\nComprehensiveness DeepCRCEval's second advantage is its ability to offer a holistic evaluation. Unlike previous studies that could not elucidate why their methods were superior using text similarity metrics, DeepCRCEval, by incorporating domain-specific criteria, sheds light on the strengths and weaknesses of different models, providing a more rounded assessment.\nEfficacy of LLM Evaluators Integrating LLMs as evaluators aims to enhance automation and minimize cost."}, {"title": "6.2 RQ3. Results for Baselines by DeepCRCEval", "content": "Utilizing DeepCRCEval, as delineated in Section 5.1, we reassessed the quality of comments generated by different CRCGs. The scoring and ranking outcomes from both evaluator types were averaged to derive final results."}, {"title": "7 Discussion", "content": ""}, {"title": "7.1 Implications", "content": "Our findings highlight the limitations of existing text similarity metrics in evaluating code review comment generation. Our proposed framework, DeepCRCE-"}, {"title": "7.2 Case Study", "content": "For a qualitative comparison, we present two case studies in Table 8- -one highlighting a defect and another focusing on code improvement. These cases contrast high-quality comments from LLM-Reviewer with those of lower quality from other baselines. Comments from baseline models often suffer from a lack of relevance or generality, potentially confusing developers. This deficiency may stem from their reliance on indirect text similarity metrics, leading to suboptimal outcomes. Conversely, LLM-Reviewer's comments are notably superior, providing precise, clear explanations of problems, their causes, and suggested solutions, corroborating our previous findings."}, {"title": "7.3 Threats to Validity", "content": "Several factors are crucial for assessing the validity of our findings. Firstly, our selection of large language models (LLMs), specifically GPT-4, was a deliberate decision. For DeepCRCEval, GPT-4 was chosen to emulate human evaluators because of its advanced capabilities. Likewise, GPT-4 served as the foundation for LLM-Reviewer due to its status as a leading and representative LLM. Another consideration is the focus on the Java programming language for our code review task. While Java has specific characteristics that differ from other languages, it is the most commonly used language in prior research, making it suitable for comparison. Finally, the scope of our reevaluation needs acknowledgment. We used"}, {"title": "8 Related Work", "content": "The automation and evaluation of machine code review comments are recent developments, aligning with longstanding research interests in assessing the quality of human-written comments. Traditional evaluations predominantly focused on the \"usefulness\" of comments. Early methods, exemplified by Bosu et al. [3], employed decision trees and hand-crafted rules to categorize comments as \"useful\" or \"not useful\", often based on subsequent code modifications or \"wontfix\" labels. Rahman et al. [19] refined this approach by emphasizing comments' \u201cchange-triggering\u201d characteristics, incorporating textual content and reviewer experience into their predictive models. Hasan et al. [9] expanded this further by integrating additional features from review contexts and reviewer backgrounds. A notable advancement came with Yang et al. [29], who introduced a BERT-based scoring system across four dimensions (emotion, question, evaluation, and suggestion), marking a shift towards a more detailed and explanatory evaluation, beyond extensive feature engineering.\nHowever, Yang et al.'s methodology diverges from ours in two key respects. First, their model assesses human-generated comments, whereas our focus is on machine-generated comments aimed at enhancing code review quality. This necessitates a more granular evaluation, emphasizing clarity and effectiveness in addressing actual defects or improvements, as detailed in Section 2.1. Second, our approach demands a deeper evaluation, analyzing the interaction between code and comment pairs, while previous research primarily targeted comments alone. Without including code as a target, it is impossible to judge perspectives related to actual issues in the code. Finally, we utilize the emergent abilities of LLMs like in-context learning. Earlier models like BERT lacked the depth required for such analysis, but recent LLM advancements, especially GPT-4, have shown near-human comprehension in understanding both code and language. Hence, DeepCRCEval incorporates LLM evaluators to complement human evaluation, balancing reliability with reduced time and cost."}, {"title": "9 Conclusion", "content": "This study challenges the prevailing evaluation methodology for CRCGs, arguing that text similarity metrics like BLEU and ROUGE-L are inadequate due to the questionable quality and validity of benchmark comments. As a solution, we introduced DeepCRCEval, a framework directly addressing developers' concerns, and LLM-Reviewer, a lightweight, training-free baseline for CRCG evaluation. LLM-Reviewer, guided by clear and direct task goals and criteria, contrasts with methods relying on text similarity for training.\nOur empirical findings suggest that CRCGs might overstate their improvements when focused on text similarity metrics, often producing comments that are irrelevant or overly generic. In contrast, LLM-Reviewer demonstrates the ability to provide clear, concise explanations of issues, their causes, and potential solutions, even without specific training. DeepCRCEval offers superior discrimination and comprehensiveness compared to previous metrics, and significantly reduces costs while maintaining reliable evaluation standards when employing LLMs as alternative evaluators. Our work lays a foundation for task-specific evaluations for code review comment generation, and highlights that future researchers should not neglect the original objectives of the code review."}, {"title": "A Additional Background", "content": ""}, {"title": "A.1 Modern Code Review", "content": "Modern Code Review (MCR) has become an integral part of software development. As illustrated in Figure 6, this process primarily comprises two elements:\n\u2022 Submitted Code Snippet. The committer submits new code for review. This step is crucial as it introduces changes to the existing codebase, necessitating thorough examination."}, {"title": "A.2 Task Description of Code Review Comment Generation", "content": "The goal of automated code review comment generation is to either augment or replace human effort in the code review process, thereby reducing labor costs. The task involves generating a pertinent comment y for a given code snippet x. This comment should effectively and succinctly highlight any issues present. During training, the model learns to estimate the probability $P(y|x) = \\prod_{i=1}^{n} P(y_i|y_{<i}, x)$, where $y_i$ represents the i-th token in the comment, and $y_{<i}$ encompasses all preceding tokens. In the inference phase, the model generates a comment y' based on the probability $P(y'|x)$."}, {"title": "B Finding Quality Evaluation Criteria", "content": "Before analyzing the benchmark comments, we first need to establish the characteristics of a high-quality comment. Our approach to defining effective code review comments combines qualitative and quantitative methods [4], drawing on varied data sources for comprehensive insights (Figure 7 depicts this methodology), with the results presented in Section 4.1, which is consistent with the previous research [11]."}, {"title": "B.1 Review of Previous Studies", "content": "Initially, we reviewed existing research by Kononenko et al. [11], which delineates developers' perspectives on high-quality code review comments. This review highlighted attributes such as clarity, relevance beyond mere code styling, constructive feedback, reviewer expertise, and mentoring potential. These elements informed the creation of our interview guidelines, emphasizing aspects like readability, relevance, problem identification, completeness, actionability, specificity, and clarity in explanations."}, {"title": "B.2 Semi-structured Interviews with Developers", "content": "Based on the former results, we conducted extended semi-structured interviews [17] with seven industry developers, each with over five years of experience and familiarity with machine learning tools in software engineering. These interviews, each lasting 10-15 minutes, allowed for iterative refinement of our guidelines. Saturation in responses was observed after 3-4 interviews, suggesting a consistency in the insights provided."}, {"title": "B.3 Card Sort and Affinity Diagram", "content": "Subsequently, we applied an open card sort technique to categorize interview data, followed by the use of affinity diagrams to connect related concepts. This process, driven by consensus among the authors, led to the emergence of two additional aspects: contextual adequacy and brevity, underlining the importance of locating problems quickly and favoring concise comments."}, {"title": "C Detailed Analysis Methodology for Dateset Comment Quality", "content": ""}, {"title": "C.1 Quality", "content": "To assess the quality of the reference comments, we implemented a human scoring system. Insights from our semi-structured interviews informed the development of an efficient scoring process. Interviewees recommended features for a scoring tool that enhances usability, such as pause-and-resume functionality, resilience to network disruptions, and clear guidelines. Consequently, we developed an offline scoring application (an executable file created using QT), embedding these suggested features along with a cumulative timing function. This tool is accessible in our open-source repository. Five master's and doctoral students in computer science, each with over six years of programming experience, conducted the scoring. Each comment was evaluated on a 1-10 scale, where higher scores denote better quality."}, {"title": "C.2 Category", "content": "To analyze the distribution of comment categories in OSS projects, we adapted the Delphi Method [5]. This categorization was carried out by five computer science master's/doctoral students, all seasoned programmers with at least six years of experience. The process involved:"}, {"title": "C.3 Tone and Context", "content": "The evaluation of tone and context was conducted using the aforementioned Nominal Group Technique (NGT) sessions, involving three authors of this study. This approach ensured a structured and collaborative analysis of these aspects."}, {"title": "D Detailed Prompt Templates", "content": ""}, {"title": "D.1 Prompt Template of LLM Evaluators", "content": "The comprehensive prompt template utilized for LLM evaluators in our CRCEval framework is detailed in Table 9. This template is meticulously designed to guide LLMs effectively, integrating task-specific instructions, guidelines, and a generation format conducive to chain-of-thought (CoT) reasoning. This approach not only instructs the LLMs but also amplifies their analytical capabilities.\nTo ensure accuracy in the evaluation outcomes, the ranking results are regularized, addressing and correcting any minor formatting inconsistencies that may arise. As a similar variant, for the evaluation of benchmark comments, particularly in the context of one-to-one code-comment pairs, the ranking component of the prompt is excluded. Additionally, slight modifications are made to the scoring task description to tailor it to the specific needs of benchmark comment evaluation."}, {"title": "D.2 Prompt Template of LLM-Reviewer", "content": "In deploying LLM-Reviewer, we harness the few-shot learning capabilities of LLMs through a meticulously crafted in-context prompt template. This template, detailed in Table 10, is structured into three main components to optimize the LLM's performance:\n1. Task Description: This section provides a clear, concise directive for the LLM, outlining the specific task of generating a code review comment. It sets the context and objective for the LLM, ensuring its outputs are aligned with the desired outcomes."}, {"title": "E Benchmark Comment Quality Analysis Results by LLM Evaluators", "content": "For the analysis of benchmark comments in Section 4, in addition to human scoring, our study also utilized the LLM evaluators introduced in Section 5.1 for a broader analysis. We expanded our evaluation to encompass 1,000 cases per dataset, with the findings by LLM evaluators detailed in Table 11. While there are variances in specific values, the overall trends observed by LLM evaluators align with those identified by human evaluators. Generally, LLM evaluators exhibited a greater tolerance across most criteria but concurred with human evaluators that benchmark comments largely fall short in criteria C2-C8. A notable exception was observed in the aspect of completeness, where LLM evaluators showed significantly different results. This divergence is attributed to the challenge human evaluators face in identifying unmentioned issues within a limited timeframe, highlighting one of the key reasons for automating code review comment generation."}]}