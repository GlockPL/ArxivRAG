{"title": "Universal Approximation of Visual Autoregressive Transformers", "authors": ["Yifang Chen", "Xiaoyu Li", "Yingyu Liang", "Zhenmei Shi", "Zhao Song"], "abstract": "We investigate the fundamental limits of transformer-based foundation models, extending our analysis to include Visual Autoregressive (VAR) transformers. VAR represents a big step toward generating images using a novel, scalable, coarse-to-fine \"next-scale prediction\" framework. These models set a new quality bar, outperforming all previous methods, including Diffusion Transformers, while having state-of-the-art performance for image synthesis tasks. Our primary contributions establish that, for single-head VAR transformers with a single self-attention layer and single interpolation layer, the VAR Transformer is universal. From the statistical perspective, we prove that such simple VAR transformers are universal approximators for any image-to-image Lipschitz functions. Furthermore, we demonstrate that flow-based autoregressive transformers inherit similar approximation capabilities. Our results provide important design principles for effective and computationally efficient VAR Transformer strategies that can be used to extend their utility to more sophisticated VAR models in image generation and other related areas.", "sections": [{"title": "Introduction", "content": "Transformer-based architectures have reshaped the landscape of modern machine learning, demonstrating state-of-the-art performance across a wide range of tasks, including natural language processing (e.g., GPT-03 [Ope24], Llama 3.3 [LT24, AI24], and Claude 3.5 [Ant24]), computer vision, and generative modeling. Their core mechanism of self-attention [VSP+17] allows for effective modeling of long-range dependencies in data, positioning transformers as a cornerstone of contemporary deep learning research. One particularly compelling variant is the Visual AutoRegressive (VAR) Transformer [TJY+24], which adapts the transformer paradigm to structured image synthesis. By employing a coarse-to-fine \u201cnext-scale prediction\" approach, VAR Transformers produce high-quality images more efficiently than many standard diffusion-based methods [SME20]. This iterative, pyramid-like generation process has demonstrated strong performance on large-scale visual tasks, indicating that multi-scale attention can capture hierarchical features in an image. Yet, despite promising empirical evidence, the theoretical underpinnings of VAR Transformers specifically, whether they inherit the well-established universal approximation properties of classical transformers-remain an open question.\nIn parallel to VAR Transformers, flow-based generative methods (e.g., real-valued non-volume preserving (RealNVP) and Glow) have also garnered attention for their ability to generate high-fidelity samples in an invertible and tractable manner. Recent efforts have integrated autoregressive decompositions with flow-based designs, giving rise to Flow AutoRegressive (FlowAR) [RYH+24] architectures. These models aim to blend the interpretability and stability of normalizing flows with the powerful representation learning of autoregressive transformers, potentially yielding more robust and scalable training dynamics. However, despite promising practical results, the theoretical investigation into the representational power of FlowAR remains similarly sparse.\nThis paper addresses two central gaps in our theoretical understanding of generative transformer architectures:\n1. Universality of VAR Transformers: Although classic transformers are known to approximate arbitrary sequence-to-sequence functions [KS24, KKM22, YBR+20, HWG+24], the additional pyramid up-sampling layers in VAR Transformers modify the input-output structure in ways that have not been theoretically dissected. We aim to rigorously determine whether VAR Transformers can still achieve universal approximation while relying on multi-scale token representations.\n2. Universality of FlowAR: Normalizing-flow-inspired architectures equipped with autoregressive attention mechanisms promise both efficient sampling and tractable likelihood estimates. Yet, their approximation capabilities have not been formally established. Can a FlowAR model approximate arbitrary continuous transformations with any desired precision?\nIn bridging these gaps, we seek to provide a unified view of how up-sampling, attention, and flow-based operations interact within transformer architectures to yield expressive function classes. Our contributions can be described as follows.\n\u2022 Universality of single-layer, single-head VAR Transformers (see Theorem 5.6).\nWe establish that even minimal VAR Transformer designs can approximate any Lipschitz sequence-to-sequence function arbitrarily closely, extending classical universality results to the VAR setting. Our theoretical analysis demonstrates that the coarse-to-fine up-sampling process, in concert with self-attention, confers enough expressive power to realize complex transformations."}, {"title": "Related Work", "content": "AutoRegressive Models. AutoRegressive models for visual generation [DYH+21, DZHT22] process 2D images by converting them into 1D token sequences. Early approaches, such as Pixel-CNN [VdOKE+16] and PixelSNAIL [CMRA18], introduced pixel-by-pixel image generation using a raster-scan order. Later advancements [RVdOV19, ERO21, LKK+22] adapted this idea to generate image tokens following a similar raster sequence. For instance, VQ-GAN [ERO21] utilizes a decoder-only transformer akin to GPT-2 for generating images, while VQVAE-2 [RVdOV19] and RQ-Transformer [LKK+22] enhance the method by incorporating hierarchical scales or stacked representations. Recently, Visual AutoRegressive (VAR) modeling [TJY+24] proposed an innovative coarse-to-fine \u201cnext-scale prediction\" strategy, significantly improving scalability, inference speed, and image quality, thus surpassing conventional autoregressive models and diffusion transformers.\nDiffusion Models. Diffusion models [HJA20, RBL+22] excel in generating high-resolution images by iteratively refining noise into coherent visuals. Prominent examples, such as DiT [PX23] and U-ViT [BNX+23], leverage probabilistic frameworks to learn data distributions effectively. Recent progress in diffusion-based image generation has focused on enhancing sampling techniques and training efficiency [SE19, SME20, LZB+22, HWL+24, CGL+25b, SSZ+25a], advancing latent-space learning [RBL+22, WSD+24, WXZ+24, LZW+24], refining model architectures [HSC+22, PX23,"}, {"title": "Preliminary", "content": "In this section, we introduce the fundamental definitions of our work. In Section 3.1, we introduce all related math notations used in this paper. In Section 3.2, we introduce the components in phase one of the VAR Model. In Section 3.3, we mathematically detail the VAR Transformer blocks."}, {"title": "Notations", "content": "We denote the lp norm of a vector x by $||x||_p$, i.e., $||x||_1 := \\sum_{i=1}^n |x_i|$, $||x||_2 := (\\Sigma_{i=1}^nx_i^2)^{1/2}$ and $||x||_\\infty := \\max_{i \\in [n]} |x_i|$. For a vector $x \\in \\mathbb{R}^n$, $exp(x) \\in \\mathbb{R}^n$ denotes a vector where $exp(x)_i$ is $exp(x_i)$ for all $i \\in [n]$. For $n > k$, for any matrix $A \\in \\mathbb{R}^{n \\times k}$, we denote the spectral norm of A by $||A||$, i.e., $||A|| := \\sup_{x \\in \\mathbb{R}^k} \\frac{||Ax||_2}{||x||_2}$. We define the function norm as $||f||_a := (\\int ||f(X)||^a dX)^{1/a}$ where f is a function. For a matrix $X \\in \\mathbb{R}^{n_1n_2 \\times d}$, we use $X \\in \\mathbb{R}^{n_1 \\times n_2 \\times d}$ to denote its tensorization, and we only assume this for letters X and Y."}, {"title": "VAR Phase One", "content": "We first present Phase One of VAR model based on [KLL+25a].\nThe VAR model uses the VAR Transformer to convert the initialized token map $X_{init}$ into a series of pyramid-shaped token maps. The VAR Transformer alternates between up sample blocks and attention layers to get the output.\nUp Sample Blocks. The k-th up sample block takes as input the initial token map $X_{init}$ and the previous pyramid-shaped token maps $X_1, ..., X_k$, sets $Y_1 = X_{init}$ and up samples each $X_i$ into a new token map $Y_{i+1}$, and outputs the new pyramid-shaped token maps $Y_1, ..., Y_{k+1}$.\nThe upsampling on each token map $X_i (i \\in [k])$ uses interpolation with a bicubic spline kernel.\nDefinition 3.1 (Bicubic Spline Kernel, Definition 3.1 from [KLL+25a] on Page 7). A bicubic spline kernel is a piecewise cubic function $W: \\mathbb{R} \\rightarrow \\mathbb{R}$ that satisfies $W(x) \\in [0, 1]$ for all $x \\in \\mathbb{R}$.\nDefinition 3.2 (Up-interpolation Layer for One-Step Geometric Series). The Up-Interpolation layer is defined as follows:\n\u2022 Let $r \\geq 2$ be an integer."}, {"title": "Notations", "content": "\u2022 Let $h_{r-1} < h_r$ denote two positive integers\n\u2022 Let $w_{r-1} < w_r$ denote two positive integers.\n\u2022 Let $d \\in \\mathbb{N}$ denote the number of channels.\n\u2022 Let $X \\in \\mathbb{R}^{h_{r-1} \\times w_{r-1} \\times d}$ denote the input feature map.\n\u2022 Let $Y \\in \\mathbb{R}^{h_r \\times w_r \\times d}$ denote the output feature map.\n\u2022 Let $s, t \\in \\{-1, 0, 1, 2\\}$.\n\u2022 Let $W: \\mathbb{R} \\rightarrow \\mathbb{R}$ be a bicubic spline kernel as defined in 3.1.\nWe use $\\Phi_{up, r}: \\mathbb{R}^{h_{r-1} \\times w_{r-1} \\times c} \\rightarrow \\mathbb{R}^{h_r \\times w_r \\times c}$ to denote the up-interpolation operation then we have $Y = \\Phi_{up, r}(X)$. Specifically, for $i \\in [h_r], j \\in [w_r], l \\in [c]$, we have\n$Y_{ijl} := \\sum_{s=-1}^{2} \\sum_{t=-1}^{2} W(\\frac{i \\cdot h_{r-1}}{h_r} + s) \\cdot X_{\\frac{i \\cdot h_{r-1}}{h_r} + s, \\frac{j \\cdot w_{r-1}}{w_r} + t, l} \\cdot W(t)$\nAfter defining the Up-Interpolation Layer for a one-step geometric sequence, we can construct a Pyramid Up-Interpolation Layer, which applies multiple up-interpolation layers to generate token maps at different resolutions. Specifically, we can describe this Pyramid Up-Interpolation Layer through the following definition:\nDefinition 3.3 (Pyramid Up-Interpolation Layer $\\Phi, r = 1$ Case). The Pyramid Up-Interpolation layer is defined as follows:\n\u2022 Let $d > 0$ denote one positive integer.\n\u2022 Let $X_{init} \\in \\mathbb{R}^{1 \\times 1 \\times d}$ denote the initial token \u0442\u0430\u0440.\nWe use $\\Phi_{up,1}: \\mathbb{R}^{1 \\times 1 \\times d} \\rightarrow \\mathbb{R}^{1 \\times 1 \\times d}$ such that\n\u2022 $\\Phi_{up,1}(X_{init}) = X_{init}$.\nDefinition 3.4 (Pyramid Up-Interpolation Layer $\\Phi, r > 2$ Case). The Pyramid Up-Interpolation layer is defined as follows:\n\u2022 Let $d > 0$ denote one positive integer.\n\u2022 Let $r \\geq 2$.\n\u2022 Let $\\Phi_{up, r}: \\mathbb{R}^{h_{r-1} \\times w_{r-1} \\times d} \\rightarrow [\\mathbb{R}^{h_r \\times w_r \\times d}$ be defined in Definition 3.2.\n\u2022 Let $X_{init} \\in \\mathbb{R}^{1 \\times 1 \\times d}$ denote the initial token map.\nWe use $\\Phi_{up, r}: \\mathbb{R}^{h_{r-1} \\times w_{r-1} \\times d} \\rightarrow \\mathbb{R}^{h_r \\times w_r \\times d}$ such that\n\u2022 For all the $i \\in [r] \\setminus \\{1\\}$, we set $Y_i = \\Phi_{up, i-1}(X_{i-1})$ (Here $Y_i$ is the i-th layer of Y)\n\u2022 For $i = 1$, we set $Y_1 = X_{init}$\nRemark 3.5. We have a pyramid-shaped token maps of size $h_{[r+1]} \\times w_{[r+1]} \\times d$. To input this into the VAR Transformer, we merge the first two dimensions, transforming it into an input of size $(\\sum_{i=1}^{r+1} h_i w_i) \\times d$."}, {"title": "VAR Transformer Blocks", "content": "Recall we have defined $\\Phi_{up} : \\mathbb{R}^{h \\times w \\times c} \\rightarrow \\mathbb{R}^{h' \\times w' \\times c}$ in Definition 3.2. Since there is no non-linear operation in $\\Phi_{up}$, $\\Phi_{up}$ is equivalent to a matrix multiplication operation, where the dimension of the matrix is $\\mathbb{R}^{h'w' \\times hw}$. For simplicity, we view $\\Phi_{up}$ as a $\\mathbb{R}^{h'w' \\times hw}$ dimension matrix in the following proofs.\nRemark 3.7 (Applying $\\Phi_{up}$ on $X \\in \\mathbb{R}^{n \\times d}$, Remark 4.8 from [KLL+25a] on Page 8). The actual input of VAR Transformer Layer are r input token maps, $X_1 \\in \\mathbb{R}^{h_1 \\times w_1 \\times d}, ..., X_r \\in \\mathbb{R}^{h_r \\times w_r \\times d}$. We denote them as $X \\in \\mathbb{R}^{n \\times d}$, where $n := \\sum_{i=1}^r h_i w_i$. We denote $\\Phi_{up}(X) \\in \\mathbb{R}^{n' \\times d}$ as applying $\\Phi_{up}$ to each $X_i \\in \\mathbb{R}^{h_i \\times w_i \\times d}$ for $i \\in [r]$, where $n' = \\sum_{i=1}^r h_i'w_i'$"}, {"title": "Single VAR Transformer Layer", "content": "Then, we can combine multiple attention layers with other components (up-interpolation layers, multilayer perceptron layers, layer-wise normalization layers) to create a complete VAR Transformer architecture.\nDefinition 3.8 (Single VAR Transformer Layer, Definition 4.9 from [KLL+25a] on Page 9). We define a VAR transformer block as the following.\n\u2022 Assume the VAR transformer has m Transformer layers.\n\u2022 Let FFN denotes a single Feed-forward Layer (see Definition B.5).\n\u2022 Let Attn stands for a single self-attention layer (see Definition B.3).\n$TF_{var}(X) = FFN \\circ Attn \\circ \\varphi_{up} \\in \\mathbb{R}^{n \\times d}$"}, {"content": "In this expression, o stands for functional composition.\nNow, we present the VAR Transformer Network Function Class.\nDefinition 3.9 (VAR Transformer Network Function Class). We define VAR Transformer Network Function Class as follows.\n\u2022 Assume the VAR transformer network has m layers.\n\u2022 for $i \\in [m]$, $FFN_i$ denotes the Feed-forward at i-th layer (see Definition B.5), $Attn_i$ denotes the Attention at i-th layer (see Definition B.3), and $\\varphi_{up}^i$ denotes the Up interpolation at i-th layer (see Definition 3.2).\n\u2022 Let $\\mathcal{T}_{a,s,c}$ denote the VAR transformer network function class\n\u2022 each function $\\tau \\in \\mathcal{T}_{a,s,c}$ consists of VAR transformer blocks $TF_{var}$ with a heads of size s and c MLP hidden neurons\n$\\mathcal{T}_{a,s,c} := \\{\\mathcal{T}: \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d}|\\mathcal{T} = TF_{var}^m \\circ TF_{var}^{m-1} \\circ ... \\circ TF_{var}^1(X)\\}$"}, {"title": "Any-Rank Single-Layer Attention is a Contextual Mapping Function", "content": "In this section, we show that Attention is a contextual mapping function. In Section 4.1, we give the definition of contextual mapping. In Section 4.2, we introduce any-rank single-layer attention as a contextual mapping function."}, {"title": "Contextual Mapping", "content": "Contextual Mapping. Let $X, Y \\in \\mathbb{R}^{n \\times d}$ be the input embeddings and output label sequences, respectively. Let $X_i \\in \\mathbb{R}^d$ be the i-th token of each X embedding sequence.\nDefinition 4.1 (Vocabulary, Definition 2.4 from [HWG+24] on Page 8). We define the vocabulary.\n\u2022 We define the i-th vocabulary set for $i \\in [N]$ by $V(i) = \\cup_{k \\in [n]} X_k^{(i)} \\subset \\mathbb{R}^d$.\n\u2022 We define the whole vocabulary set V as $V = \\cup_{i \\in [N]} V(i) \\subset \\mathbb{R}^d$."}, {"title": "Tokenwise Separateness", "content": "Note that while \"vocabulary\" typically refers to the tokens' codomain, here, it refers to the set of all tokens within a single sequence. To facilitate our analysis, we introduce the idea of input token separation following [KS24, KKM22, YBR+20].\nDefinition 4.2 (Tokenwise Separateness, Definition 2.5 from [HWG+24] on Page 8). We define the tokenwise separateness as follows.\n\u2022 Let $X^{(1)}, ..., X^{(N)} \\in \\mathbb{R}^{n \\times d}$ be embeddings.\n\u2022 Let N be the number of sequences in the datasets.\n\u2022 Let n be the length of a sequence. i.e. $X^{(i)} \\in \\mathbb{R}^{n \\times d}$\nFirst, we state three conditions for $X^{(1)},..., X^{(N)}$\n(i) For any $i \\in [N]$ and $k \\in [n]$, $||X_k^{(i)}||_2 > \\gamma_{min}$ holds.\n(ii) For any $i \\in [N]$ and $k \\in [n]$, $||X_k^{(i)}||_2 < \\gamma_{max}$ holds.\n(iii) For any $i, j \\in [N]$ and $k, l \\in [n]$ if $X_k^{(i)} \\neq X_l^{(j)}$, then $||X_k^{(i)} - X_l^{(j)}||_2 > \\delta$ holds.\nSecond, we define three types of separateness as follows,\n\u2022 Part 1. If all conditions hold, then we call it tokenwise $(\\gamma_{min}, \\gamma_{max}, \\delta)$-separated\n\u2022 Part 2. If conditions (ii) and (iii) hold, then we denote this as $(\\gamma,\\delta)$-separateness.\n\u2022 Part 3. If only condition (iii) holds, then we denote it as $(\\delta)$-separateness.\nTo clarify condition (iii), we consider cases where there are repeated tokens between different input sequences. Next, we define contextual mapping. Contextual mapping describes a function's ability to capture the context of each input sequence as a whole and assign a unique ID to each input sequence.\nDefinition 4.3 (($\\gamma, \\delta$)-Contextual Mapping, Definition 2.6 from [HWG+24] on Page 8). A function $q: \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d}$ is said to be a ($\\gamma, \\delta$)-contextual mapping for a set of embeddings $X^{(1)},...,X^{(N)} \\in \\mathbb{R}^{n \\times d}$, if the following conditions hold:\n\u2022 Contextual Sensitivity $\\gamma$. For any $i \\in [N]$ and $k \\in [n]$, $||q(X^{(i)})_k||_2 < \\gamma$ holds.\n\u2022 Approximation Error $\\delta$. For any $i, j \\in [N]$ and $k, l \\in [n]$ such that $V^{(i)} \\neq V^{(j)}$ or $X_k^{(i)} \\neq X_l^{(j)}$, $||q(X^{(i)})_k - q(X^{(j)})_l||_2 > \\delta$ holds.\nIn addition, Note that $q(X^{(i)})$ for $i \\in [N]$ is called a context ID of $X^{(i)}$."}, {"title": "Any-Rank Single-Layer Attention is a Contextual Mapping Function", "content": "Now we present the result showing that a softmax-based 1-head, 1-layer attention block with any-rank weight matrices is a contextual mapping.\nLemma 4.4 (Any-Rank Attention as a ($\\gamma, \\delta$)-Contextual Mapping, Lemma 2.2 from [HWG+24] on Page 9). If the following conditions hold:\n\u2022 Let $X^{(1)},..., X^{(N)} \\in \\mathbb{R}^{n \\times d}$ be embeddings that are $(\\gamma_{min}, \\gamma_{max}, \\epsilon)$-tokenwise separated, with the vocabulary set $V = \\cup_{i \\in [N]} V(i) \\subset \\mathbb{R}^{d}$."}, {"title": null, "content": "\u2022 $X_k^{(i)} \\neq X_l^{(j)}$ for any $i \\in [N]$ and $k, l \\in [L]$.\n\u2022 Let $\\gamma = \\gamma_{max} + \\frac{\\epsilon}{4}$\n\u2022 Let $\\delta = exp(-5 \\epsilon^{-1}|V|^{4dk_{H}/max} log L)$\n\u2022 Let $\\kappa := \\gamma_{max}/\\gamma_{min}$.\n\u2022 Let $W^{(O)} \\in \\mathbb{R}^{d \\times s}$ and $W_V, W_K, W_Q \\in \\mathbb{R}^{s \\times d}$.\nThen, we can show\n\u2022 1-layer, single-head attention mechanism serves as a ($\\gamma, \\delta$)-contextual mapping for the embeddings $X^{(1)}, ..., X^{(N)}$ with weight matrices $W^{(O)}$ and $W_V, W_K, W_Q$.\nLemma 4.4 indicates that any-rank self-attention function distinguishes input tokens $X_k^{(i)} = X_l^{(j)}$ such that $V^{(i)} \\neq V^{(j)}$. In other words, it distinguishes two identical tokens within a different context."}, {"title": "Universality of VAR Transformer", "content": "In this section, we present our proof for the universality of the VAR Transformer. In Section 5.1, we used a universality result from a previous work. In Section 5.2, we analyze how the error behaves when two consecutive layers in our composition are each replaced by their respective approximations. In Section 5.3, we present the scenario when one of the composited layers got replaced by a different function. In Section 5.4, we present the scenario when all of the composited layers got replaced. In Section 5.5, we present our proof for the universality of the VAR Transformer."}, {"title": "Universality of $T_{A}^{1,1,4}$ with $O((1/\\epsilon)^6 d^n)$ FFN Layers", "content": "We used a universality result from [HWG+24].\nLemma 5.1 ($\\tau \\in T_{A}^{1,1,4}$ Transformer is Universal Seq2Seq Approximator, Theorem 2.3 in [HWG+24] on Page 11). If the following conditions hold:\n\u2022 Let $1 < p < \\infty$ and $\\epsilon > 0$.\n\u2022 Let a transformer with one self-attention layer defined as $\\tau \\in T_{A}^{1,1,4}$\nThen, there exists\n\u2022 a transformer $\\tau'$ with single self-attention layer, such that for any $\\mathcal{L} \\in F_{\\mathcal{C}}$ there exists $||\tau'(\\cdot), \\mathcal{L}||_a \\leq \\epsilon$."}, {"title": "Two Layers Perturbation", "content": "In this section, we analyze how the error behaves when two consecutive layers in our composition are each replaced by their respective approximations. Specifically, we consider the composition $f_i \\circ g_i$ and replace $g_i$ with an up interpolation function $\\Phi_{up,i}$ and $f_i$ with a one-layer transformer $\\mathcal{T}_i$. We show that under appropriate Lipschitz and approximation assumptions, the overall error of the approximated two-layer composition can be controlled in terms of the individual approximation errors."}, {"title": null, "content": "Assumption 5.2 (Target Function Class). We assume the following things:\n\u2022 Let $f_1,..., f_r$ ber K-Lipschitz functions from $\\mathbb{R}^{h_r \\times w_r \\times d}$ to $\\mathbb{R}^{h_r \\times w_r \\times d}$\n\u2022 For each $i \\in [r]$, let $g_i$ be a K-Lipschitz function from $\\mathbb{R}^{h_{i-1} \\times w_{i-1} \\times d}$ to $\\mathbb{R}^{h_i \\times w_i \\times d}$\n\u2022 We assume that for each $i \\in [r]$, $g_i$ can be approximated by some up interpolation function $\\Phi_{up,i}$.\n\u2022 We assume that the target function $f_{word2img}: \\mathbb{R}^{1 \\times 1 \\times d} \\rightarrow \\mathbb{R}^{h_r \\times w_r \\times d}$ satisfies\n$f_{word2img} := f_r \\circ g_r \\circ ... \\circ f_1 \\circ g_1$.\nWith the Assumption 5.2, we present the two layers of perturbation as follows.\nLemma 5.3 (Two Layers Perturbation). Let $\\Phi_{up,i}$ be the up interpolation function defined in 3.2. Let $f_r$ ber K-Lipschitz functions from Assumption 5.2. Let $g_i$ ber K-Lipschitz functions from Assumption 5.2. Let $\\tau_i$ be the one-layer transformer defined in Eq. 2.4 from [HWG+24]. If the following conditions hold:\n\u2022 $||g_i - \\Phi_{up,i} || \\leq \\epsilon_{1,i}$ from Assumption 5.2.\n\u2022 $|| f_i - \\mathcal{T}_i || \\leq \\epsilon_{2,i}$ from Theorem 5.1.\n\u2022 $f_i$ is $K_{1,i}$-Lipschitz.\nThen we have\n$|| f_i g_i - \\mathcal{T}_i \\circ \\Phi_{up,i}|| \\leq K_{1,i} \\epsilon_{1,i} + \\epsilon_{2,i}$"}, {"title": null, "content": "Proof. We can show that\n$|| f_i g_i - \\mathcal{T}_i \\circ \\Phi_{up,i}|| = || f_i \\circ g_i - f_i \\circ \\Phi_{up,i} + f_i \\circ \\Phi_{up,i} - \\mathcal{T}_i \\circ \\Phi_{up,i}||$\n$\\leq || f_i \\circ g_i - f_i \\circ \\Phi_{up,i}|| + || f_i \\circ \\Phi_{up,i} - \\mathcal{T}_i \\circ \\Phi_{up,i} ||$\n$= ||f_i (g_i - \\Phi_{up,i})|| + ||(f_i - \\mathcal{T}_i) \\circ \\Phi_{up,i} ||$\n$\\leq || f_i \\circ (g_i - \\Phi_{up,i})|| + || f_i - \\mathcal{T}_i||$\n$\\leq K_{1,i} \\epsilon_{1,i} + \\epsilon_{2,i}$\nwhere the first step follows from basic algebra, the second step follows from triangle inequality, the third and fourth steps follow from basic algebra, and the fifth step follows from our conditions."}, {"title": "Perturbation of Recursively Composting Functions that One Layer is Different", "content": "In this section, we consider a scenario where we have a composition of many layers, but only one of the layers is replaced by a different function. This setting helps us see how a single local perturbation can propagate through subsequent layers in a multi-layer composition. The lemma below quantifies this propagation by leveraging Lipschitz continuity.\nLemma 5.4 (Perturbation of Recursively Composting Functions, One Layer is Different). if the following conditions hold"}, {"title": null, "content": "\u2022 Assume $||u_j(w) - v_j(w)|| \\leq \\epsilon$ for any w.\n\u2022 $v_i(x) \\leq K_2 \\cdot ||x||$\nFix j, we have\n$|| \\circ_{i=1}^{n+1} u_i - \\circ_{i=1}^{n+1} v_i || \\leq K_2^{n-j} \\cdot \\epsilon$\nProof. We define u\nw = \\circ_{i \\neq i} u_i(x)\nWe can show that for any x\n$|| \\circ_{i=1}^{n+1} u_i(x) - \\circ_{i=1}^{n+1} v_i(x)|| = || \\circ_{i=1}^{n} v_i(u_j(w)) - \\circ_{i=1}^{n} v_i(v_j(w)) || = || \\circ_{i=j+1}^{n+1} v_i \\circ (u_j(w) - v_j(w)) ||$\n$\\leq \\circ_{i=j+1}^{n+1} v_i (u_j(w) - v_j(w))||\n$\\leq K_2^{n-j} \\cdot \\epsilon$\nwhere the first step follows from basic algebra, the second step follows from linearity, and the third step follows from lemma assumptions."}, {"title": "Perturbation of Recursively Composting Functions that All Layer are Different", "content": "In this section, we extend the analysis to the most general scenario in which all layers in the composition are replaced by different functions. This captures the situation where each layer $u_i$ is approximated by some other function $v_i$. We derive a cumulative bound that sums the individual perturbations introduced at each layer.\nLemma 5.5 (Perturbation of Recursively Compositing Functions, All Layers are Different). If the following conditions hold:\n\u2022 Let $\\circ_{i=1}^n U = U_n \\circ ... \\circ U_1$\n\u2022 Let $\\mathcal{I} = 1$\n\u2022 Let $u_0(x) = x$ which is identity mapping\n\u2022 Let $v_{n+1}(x) = x$ which is identity mapping\nThen\n$|| \\circ_{i=1}^n U_i - \\circ_{i=1}^{n+1} V_i || \\leq \\sum_{j=1}^{n} || \\circ_{i=j+1}^{n+1} V_i \\circ \\circ_{i=1}^{j} U_i -  \\circ_{i=j}^{n+1} V_i \\circ \\circ_{i=1}^{j-1} U_i ||$\nProof. We can show\n$|| \\circ_{i=1}^{n} U_i - \\circ_{i=1}^{n+1} V_i || \\leq \\sum_{j=1}^{n} || \\circ_{i=j+1}^{n+1} V_i \\circ \\circ_{i=1}^{j} U_i -  \\circ_{i=j}^{n+1} V_i \\circ \\circ_{i=1}^{j-1} U_i ||$\nwhere the first step follows from adding intermediate terms, and the last step follows from the triangle inequality.\nThus, we complete the proof."}, {"title": "The Universality of VAR Transformer", "content": "In this section, with the established error bounds for replacing individual or multiple layers with alternative functions, we now prove the main universality result for the VAR Transformer. In essence, we show that a properly constructed VAR Transformer can approximate the target function $f_{word2img}$ (from Assumption 5.2) with arbitrarily small errors under suitable Lipschitz and approximation assumptions on each layer.\nTheorem 5.6 (Universality of VAR Transformer). Assume $K_2 > 2$. For $f_{word2img}$ satisfies Assumption 5.2, there exists a VAR Transformer $\\tau_{var}$ such that\n$||T_{VAR} - f_{word2img}|| < K_2^r(K_{1,i} \\epsilon_{1,i} + \\epsilon_{2,i})$\nProof. We can show that\n$||T_{VAR} \u2013 f_{word2img}|| = || \\circ_{i=1}^r (f_i \\circ g_i) -  \\circ_{i=1}^r (\\mathcal{T}_i \\circ \\Phi_{up,i}) ||$\n$= \\sum_{j=1}^{n} K_2^{r-j} (K_{1,i} \\epsilon_{1,i} + \\epsilon_{2,i})$\n$= \\frac{K_2^r-1}{K_2-1} (K_{1,i} \\epsilon_{1,i} + \\epsilon_{2,i})$\n$\\leq K_2^r(K_{1,i} \\epsilon_{1,i} + \\epsilon_{2,i})$.\nwhere the"}]}