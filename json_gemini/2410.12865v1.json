{"title": "ELF-Gym: Evaluating Large Language Models Generated Features for Tabular Prediction", "authors": ["Yanlin Zhang", "Ning Li", "Quan Gan", "Weinan Zhang", "David Wipf", "Minjie Wang"], "abstract": "Crafting effective features is a crucial yet labor-intensive and domain-specific task within machine learning pipelines. Fortunately, recent advancements in Large Language Models (LLMs) have shown promise in automating various data science tasks, including feature engineering. But despite this potential, evaluations thus far are primarily based on the end performance of a complete ML pipeline, providing limited insight into precisely how LLMs behave relative to human experts in feature engineering. To address this gap, we propose ELF-Gym, a framework for Evaluating LLM-generated Features. We curated a new dataset from historical Kaggle competitions, including 251 \"golden\u201d features used by top-performing teams. ELF-Gym then quantitatively evaluates LLM-generated features by measuring their impact on downstream model performance as well as their alignment with expert-crafted features through semantic and functional similarity assessments. This approach provides a more comprehensive evaluation of disparities between LLMs and human experts, while offering valuable insights into specific areas where LLMs may have room for improvement. For example, using ELF-Gym we empirically demonstrate that, in the best-case scenario, LLMs can semantically capture approximately 56% of the golden features, but at the more demanding implementation level this overlap drops to 13%. Moreover, in other cases LLMs may fail completely, particularly on datasets that require complex features, indicating broad potential pathways for improvement.", "sections": [{"title": "1 Introduction", "content": "Feature engineering is a crucial step in the machine learning pipeline, transforming raw data into meaningful features that improve model performance and interpretability. Effective feature engineering can significantly enhance the predictive power of models, making it a vital component in various data-driven applications. This importance is particularly evident in competitive data science environments like Kaggle [23], where top-performing models often rely heavily on sophisticated feature engineering techniques. For instance, in one interview \u00b9, the winners of the Grupo Bimbo Inventory Prediction competition reported that 95% of their time was on feature engineering while only 5% was on modeling.\nDespite its importance, traditional feature engineering is labor-intensive and requires extensive domain knowledge. Automated tools like AutoFeat [18], OpenFE [36], SAFE [32], and Deep Feature Synthesis (DFS) [25] have emerged to streamline this process. AutoFeat automates feature selection and generation using statistical methods and heuristics but suffers from high feature generation costs. OpenFE and SAFE mitigate these costs by optimizing the feature selection phase using feedback from model evaluation. DFS extends feature engineering to multi-table scenarios by utilizing data relationships to generate features. Despite their effectiveness in reducing manual effort, these tools often fall short in leveraging the domain knowledge that human experts typically rely on for crafting relevant features.\nThe advent of Large Language Models (LLMs) such as GPTs [1] has opened new possibilities for automating various data science tasks. LLMs have demonstrated remarkable capabilities in natural language understanding [10, 28], text generation [27, 33], summarization [6, 13, 35], and even code synthesis [5, 7, 34]. Their ability to process and generate human-like text makes them particularly well-suited for tasks that require semantic understanding and contextual reasoning. Of particular relevance to data science, LLMs"}, {"title": "2 ELF-Gym Design", "content": "Kaggle is a widely used platform in the data science community, providing a rich source of datasets for developing and testing LLM-based methods. It hosts numerous competitions where data science experts compete to build the best models, often employing sophisticated feature engineering techniques. This makes Kaggle an ideal source for comparing features discovered by experts with those generated by LLMs.\nHowever, despite competition hosts inviting or enforcing winner teams to share their code and insights in discussion posts, the code is often hard to consume as it was written in R, C++, Python, etc., well before scientific computing packages like Pandas [30] became popular. As per the insights, the discussion posts are typically written casually and are often entangled with unrelated discussions, making it difficult to extract relevant information directly.\nTo address this, we employ an LLM-assisted strategy to convert discussion posts into features that can be calculated from actual data. Similar to [16], we adopt a two-step approach to ensure reliability. For each Kaggle dataset, we provide GPT-40 with the dataset description, including table and column names, column labels, and the intended task (i.e., the column to predict), and prompt GPT-40 to extract feature descriptions from the discussion posts of the top-5 teams in a specific format. Once we have the feature descriptions, we further prompt GPT-40 to generate code that computes actual feature given the original table. GPT-40 can generate correct code approximately 80% of the time, significantly reducing the time of manual checking. We then manually sanitize the remaining cases where the code fails to run.\nTo validate the effectiveness of these extracted golden features, we test if these features can enhance model performance. This is done by executing the generated code, adding the resulting features to the original tabular data, and using AutoGluon [12] for prediction. We compare the results with predictions made using only the raw features. With this approach, we successfully curated 251 golden features from 8 Kaggle datasets with details in Table 1.\nSimilar to how golden features are curated, the LLM-generated features also take two steps (Figure 1). First, the candidate LLM receives the dataset description and prediction target and is asked"}, {"title": "2.3 Evaluation Protocol", "content": "To measure the alignment between LLM-engineered features and golden features, ELF-Gym employs a recall metric. Given two feature sets, \\(F_{LLM}\\) and \\(F_{golden}\\), the recall metric uses a measurement function M that returns a binary flag (1 or 0), indicating whether two features are similar. The recall metric is then defined as the proportion of golden features for which the M returns 1 when compared to features in \\(F_{LLM}\\):\n\\[\\text{Recall}(F_{LLM}, F_{golden}) = \\frac{\\sum_{f \\in F_{golden}} \\max_{f' \\in F_{LLM}} M(f, f')}{|F_{golden}|}\\]\nELF-Gym uses two measurement functions M. \\(M_{sem}\\) measures the semantic similarity of two feature descriptions by prompting a GPT-40 to assess and score the similarity. \\(M_{func}\\) checks if two features are functionally equivalent by comparing the outputs of feature functions applied to the input data. This process can be costly due to the slow performance of LLM-generated codes on large datasets, so we employ strategies such as representative down-sampling to mitigate issues like false positives. For instance, down-sampling must be done carefully to ensure that key patterns in the data are preserved, especially for features involving aggregations by specific IDs. The use of two M functions is important because, for example, if a feature is hit by \\(M_{sem}\\) but not \\(M_{func}\\), it indicates that the LLM can identify relevant features but needs to improve its code generation capability for feature engineering tasks. We remark that while a precision metric can be defined in a similar fashion, we omit it here since the usefulness of a feature crafted by LLMs, but not overlapping with human experts, requires downstream model evaluation.\nIn addition to these alignment measures, ELF-Gym also evaluates the impact of LLM-generated features on downstream model performance. This involves incorporating the generated features into the original dataset and using them to train a model, then comparing the model's performance to that of a model trained on raw features. This step provides a direct measure of the utility of the"}, {"title": "3 Initial Results", "content": "We select four LLMs to evaluate: GPT-40 [1], Claude 3 Sonnet [4], LLaMA3-70B-Instruct [2], and Mixtral-8x7B-Instruct [21]; by design though, ELF-Gym can also be readily adapted to handle other LLMs (see Section 2.2). We invoke GPT-40 with OpenAI's official SDK and the rest three with Amazon Bedrock2. For feature description generation, we explicitly prompt the LLMs to generate as many features as possible. For code generation, we gave each LLM three chances to write runnable code, each time feeding in the error message from Python interpreter as the next round of conversation. Our implementation is available at https://github.com/Lilyzhangyanlin/ELF-Gym."}, {"title": "3.1 RQ1: Can LLMs Discover Golden Features?", "content": "Figure 2 shows the alignment results between LLM-engineered features and golden features, measured by recall metrics using both \\(M_{sem}\\) and \\(M_{func}\\). Overall, the recall percentages are higher for \\(M_{sem}\\) compared to \\(M_{func}\\), indicating that while LLMs can generate features that are semantically similar to the golden features, they struggle to ensure these features are functionally equivalent. GPT generally leads in both metrics, highlighting its effectiveness in feature generation.\nFor the Facebook dataset, all models perform well semantically (56.25%) and functionally, with LLaMA3 leading at 12.90%, followed by GPT-40 (9.68%). This indicates that the features required for Facebook are easier for LLMs to generate both semantically and functionally. Conversely, datasets like IEEE-CIS, Outbrain, and Virus present significant challenges, with low or zero recall scores across all models for both metrics. Notable golden features in these datasets include frequency counting, feature interaction, and features grouped by multiple columns, all of which are difficult for existing LLMs (see Sec. 3.2 for deeper analysis). Additionally, IEEE-CIS has particularly strong features created by grouping by the \"card1\" ID column, but LLMs tend to group by other columns. No single model consistently"}, {"title": "3.2 RQ2: Patterns LLMs Excel or Struggle with", "content": "To study which kind of features LLMs are better generating, we further break down the golden features into two categories:\n\u2022 Feature transforms, including\nUnary transforms: a unary function of a column.\nN-ary transforms: a function involving multiple columns.\nTime-lagged features: features based on time differences between events (rows), e.g. \"finding the difference between the number of calls from the same each day with respect to the day before\".\n\u2022 Joins and aggregations, including\nSimple joins: columns merged from another table.\nSingle-column group-by: statistics of columns grouped by a single column."}, {"title": "4 Conclusion", "content": "This paper proposes ELF-Gym, a framework for evaluating the feature engineering capability of LLMs, by curating a dataset with 251 human-engineered features from 8 Kaggle competitions. Initial experiments on 4 LLMs reveal varying gaps relative to human-level feature engineering expertise; in particular, we observe that while LLMs in may excel generating simple features, they struggle at times to generate features involving complex functions, aggregations, and table joins. We believe that addressing these challenges will be crucial for realizing the full potential of LLMs in automating data science tasks."}]}