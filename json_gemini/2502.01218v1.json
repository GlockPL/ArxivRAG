{"title": "Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents", "authors": ["Zhizhen Zhang", "Lei Zhu", "Zhen Fang", "Zi Huang", "Yadan Luo"], "abstract": "Pre-training vision-language representations on human action videos has emerged as a promis-ing approach to reduce reliance on large-scale ex-pert demonstrations for training embodied agents. However, prior methods often employ time con-trastive learning based on goal-reaching heuristics, progressively aligning language instructions from the initial to the final frame. This overemphasis on future frames can result in erroneous vision-language associations, as actions may terminate early or include irrelevant moments in the end. To address this issue, we propose Action Temporal Coherence Learning (AcTOL) to learn ordered and continuous vision-language representations without rigid goal-based constraint. AcTOL treats a video as a continuous trajectory where it (1) contrasts semantic differences between frames to re-flect their natural ordering, and (2) imposes a local Brownian bridge constraint to ensure smooth tran-sitions across intermediate frames. Extensive imi-tation learning experiments across varying num-bers of demonstrations show that the pretrained features significantly enhance downstream manip-ulation tasks by up to 49% with high robustness to different linguistic styles of instructions, offering a viable pathway toward generalized embodied agents. Our code is available here.", "sections": [{"title": "1. Introduction", "content": "The long-term vision for embodied intelligence (Mu et al., 2023; Liu et al., 2024) is to create systems that seamlessly perceive and interact with the world around them. Achieving this requires agents that integrate vision and language to understand their surroundings, interpret human instructions, and autonomously plan actions for complex tasks. Current end-to-end approaches achieve policy learning through di-"}, {"title": "2. Related Work", "content": "Given the success of large-scale pre-training in the vision and language research communities (Brown et al., 2020; Liu et al., 2023), many studies have attempted to extend this paradigm to the field of robotics. Some work leverage massive robotic trajectory data (Collaboration et al., 2023) for pre-training, aiming to establish a unified mapping from the perception space to the action space (Zitkovich et al., 2023; Cheang et al., 2024). However, collecting large amounts of high-quality robot trajectory data is extremely costly and time-consuming. Consequently, many studies have begun to explore the use of large-scale, readily available, out-of-domain human action video data to learn generalizable representations that can be transferred to robotic tasks (Sermanet et al., 2018; Ma et al., 2023b; Radosavovic et al., 2022; Nair et al., 2022; Karamcheti et al., 2023; Ma et al., 2023a; Majumdar et al., 2023; Ye et al., 2024; Zeng et al., 2024; Li et al., 2024). Among these, TCN (Sermanet et al., 2018), VIP (Ma et al., 2023b), MVP (Radosavovic et al., 2022), and VC-1 (Majumdar et al., 2023) focus solely on studying unimodal visual representations, limiting their performance when understanding language instructions is required. R3M (Nair et al., 2022) employs language and reward models to shape progressive visual representations, while Voltron (Karamcheti et al., 2023) and MPI (Zeng et al., 2024) model the transition from the current state to the goal state conditioned on language. However, during training, these approaches freeze the language encoder, using it only to aid in the training of visual representations. As a result, they do not effectively achieve multi-modal representation learning.\nRecently, LIV (Ma et al., 2023a) and DecisionNCE (Li et al., 2024) have attempted to leverage CLIP (Radford et al., 2021), a state-of-the-art vision-language model, to train embodied multi-modal representations. LIV treats language instructions as the goals of video actions and aligns the final frame of a video with the corresponding language description. DecisionNCE, on the other hand, views language as the transition from the initial state to the final state, aligning the difference between the representations of the first and last frames with the language. Their methods rely on goal-directed semantic alignment, which tends to produce suboptimal results under the noise present in real-world videos. In contrast, our approach avoids rigid assumptions, theoretically ensuring that semantic alignment follows the intrinsic temporal continuity and ordering of the video, resulting in more robust and generalizable vision-language representations."}, {"title": "3. Preliminaries", "content": "We first set up notations and mathematically formulate tasks.\nLanguage-Conditioned Imitation Learning (LC-IL). The task of LC-IL aims to train an agent to mimic expert be-haviors from a given demonstration set \\(D_a = \\{(T_i, l_i)\\}_{i=1}^n\\), where \\(l_i \\in L\\) represents a task-specific language instruc-tion. Each trajectory \\(T_i \\in T\\) consists of a sequence of state-action pairs \\(T_i = \\{(s_j, a_j)\\}_{j=1}^T\\) of the horizon length T. In robot manipulation tasks, action \\(a_j \\in A\\) corre-sponds to the control commands executed by the agent and state \\(s_j = [p_j; v_j] \\in S\\) records proprioceptive data \\(p_j\\) (e.g., joint positions, velocities) and visual inputs \\(o_j \\in O\\) (e.g., camera images) at the time step j. The objective of LC-IL is to find an optimal language-conditioned policy \\(\\pi^*(a|s, l): S \\times L \\rightarrow A\\) via solving the supervised opti-mization as follows,\n\\(\\pi^* \\in \\arg \\min_\\pi \\mathbb{E}_{(T_i, l_i) \\sim T} [\\sum_{j=1}^T l(\\pi(a_j, s_j | l_i), a_j)]\\)\nwhere l(\u00b7, \u00b7) is a task-specific loss, such as mean squared error or cross-entropy. Training the policy \\(\\pi\\) in an end-to-end fashion may require hundreds of high-quality expert demon-strations to converge, primarily due to the high variance of visual inputs o and language instructions l.\nVision-language Pre-training. Address such scalability issues can be achieved by leveraging large-scale, easily ac-"}, {"title": "4. Our Approach", "content": "We introduce an action temporal coherence learning (Ac-TOL) to capture two temporal properties of video actions: ordering and continuity. Ordering was ensured in the vision-language ordering loss (Section 4.1), where the semantic difference between frames reflects their temporal distance, with closer frames exhibiting smaller differences than those further apart. Continuity requires smooth visual transitions between adjacent frames, avoiding abrupt changes and high variance. To achieve this, we model sampled frame inter-vals as a Brownian bridge process (Section 4.2), penalizing deviations from the expected trajectories. Different from prior works that relies on setting explicit goal frames, the proposed approach implictly explore the global and local structure of actions without imposing rigid constraints."}, {"title": "4.1. Visual-Language Ordering", "content": "To capture the temporal coherence of video actions, we first propose a vision-language ordering (VLO) loss that ensures the semantic alignment between frames reflects their temporal order. Consider an anchor frame \\(o_i \\in O\\) with an index n(i) corresponding to its position in the original video. For any given frame pair \\((o_i, o_j)\\), we first define the semantic alignment score R to quantify differences in their VL similarities w.r.t a language description l as:\n\\(R(v_i, v_j, l) = - || sim(v_i, l) \u2013 sim(v_j, l)||^2,\\)   (1)\nwhere \\(v_i = \\phi(o_i), l = \\varphi(l)\\). The function sim(\u00b7, \u00b7) computes the VL similarity using cosine similarity. To ensure the proposed R adhere to the temporal ordering of frames, we construct a negative set \\(N_{i,j}\\) by selecting \\(o_k \\in O\\) correspond to frames that are temporally more distant than the positive pair \\((o_i, o_j)\\):\n\\(N_{i,j} = \\{o_k | k \\neq i, |n(i) \u2013 n(k)| \\geq |n(i) \u2013 n(j)|\\},\\)\nThis formulation allows us to reformulate \\(L_{INCE}\\) by en-forcing that the VL similarity difference between frames i and j should be smaller than that between frame i and any negative frame k within the video O:\n\\(L_{VLO} = -\\mathbb{E}_{(o_i, o_j)\\sim O} \\log \\frac{\\exp (R(v_i, v_j, l))}{\\sum_{o_k \\in N_{i,j}} \\exp (R(v_i, v_k, l))}.\\)\nNotably, our VLO loss does not strictly require \\(o_j\\) to be from future timestep for goal-reaching. Instead, we leverage the inherent temporal dynamics in videos, allowing the model to learn the natural ordering in an unsupervised manner with detailed analysis as follows.\nTheoretical Analysis. Ordering and sorting properties are well-established in self-supervised learning (Shvetsova et al., 2023; Hu et al., 2021; Zha et al., 2023). Building upon these insights, we formalize the concept of vision-language ordering below."}, {"title": "Definition 4.1 (Vision-Language Ordering)", "content": "Let \\(\\{o_i\\}_{i\\in[T]}\\) be a sequence of video frames and l the corresponding language description. The representations of the frames are said to satisfy the VLO property for any \\(0 < \\delta < 1\\) if \\(\\forall i \\in [T]\\), and distinct frames \\(j, k \\in [T]\\setminus\\{i\\}\\), the following conditions hold:\n\\(\n\\begin{cases}\nR_{i,j,l} > R_{i,k,l} + 1/\\delta, & \\text{if } d_{i,j} < d_{i,k},\\\\ |R_{i,j,l} - R_{i,k,l} < \\delta, & \\text{if } d_{i,j} = d_{i,k},\\\\ R_{i,j,l} < R_{i,k,l} - 1/\\delta, & \\text{if } d_{i,j} > d_{i,k},\n\\end{cases}\n\\)\nwhere \\(R_{i,j,l}\\) denotes R(vi, vj, l) and the temporal distance between frames defined as \\(|n(i) \u2013 n(j)|\\) as \\(d_{i,j}\\)."}, {"title": "Implications of the VLO Property", "content": "The VLO property enforces a structured representation of video frames, en-suring that temporally adjacent frames have consistent and predictable semantic differences. When two frames have equal temporal distances from an anchor frame, their seman-tic gaps should be similar, fostering smooth transitions. In contrast, frames that are farther apart should exhibit larger semantic gaps, thus preserving the chronological order.\nTo formalize the temporal ordering constraints, we define the unique sorted set of frame distances from frame i as \\(\\{D_{i,1} < D_{i,2} < ... < D_{i,M_i}\\}\\), where each \\(D_{i,m}, m \\in [M_i]\\) is obtained by sorting the set \\(\\{d_{i,j} | j \\in [T] \\setminus \\{i\\}\\} \\). Additionally, we define the count of frames at each distance level as:\n\\(n_{i,m} := |\\{j | d_{i,j} = D_{i,m}, j \\in [T] \\setminus \\{i\\}\\}|,\\)   (2)\nwhich denotes the number of frames whose temporal dis-tance from frame i equals \\(D_{i,m}\\). The VLO property is sat-isfied when the proposed \\(L_{VLO}\\) approaches its theoretical lower bound, which is given by:\n\\(L^* := \\frac{1}{T(T-1)} \\sum_{i=1}^T \\sum_{m=1}^{M_i} n_{i,m} \\log n_{i,m}.\\)   (3)\nThis bound characterizes the optimal alignment of VL simi-larities, ensuring that the learned representations preserve the inherent temporal structure within the video sequence, as guaranteed by the following theorem."}, {"title": "Theorem 4.2.", "content": "\\(L^*\\) is a tight lower bound of \\(L_{VLO}\\), i.e., \\(L_{VLO} \\geq L^*\\), and for any \\(\\epsilon > 0\\), there exists feature em-beddings such that \\(L_{VLO} < L^* + \\epsilon\\). Furthermore, for any \\(0 < \\delta < 1\\), there exist \\(\\epsilon > 0\\)such that if \\(L_{VLO} < L^* + \\epsilon\\), the learned representations satisfy the VLO property."}, {"title": "4.2. Vision-Language Continuity", "content": "While the VLO property provides a strong global constraint on the structural alignment of VL pretraining, optimiz-ing triplet relationships alone can be unstable. Variations"}, {"title": "Theorem 4.3 (Vision-Language Continuity)", "content": "Let vk, vi be arbitrary time step from the interval [n(i), n(j)] and l \u2208 L be the language embedding. Suppose the VL similarity function sim() is Lipschitz continuous with constant C. Assume that the frame embeddings are regularized by the Brownian Bridge constraint, then for any \u03f5 > 0, there exists \u03b4 > 0 such that\n\\(||Vk - Vl||2 < \u03b4 \u21d2 |R(vk, vl, l)| < \u03f5.\\)"}, {"title": "Theorem 4.4 (Robustness to Language Variations)", "content": "Let l' be the perturbed version of the original language embedding l subject to a small constant \u03b4l > 0, i.e., ||l \u2013 l'|| \u2264 \u03b4l, then the semantic alignment score R exhibits stability to the perturbation:\n\\(|R(vi, vj, l') \u2013 R(vi, vj, l)| \u2264 2C\u03b4l.\\)   (6)"}, {"title": "5. Experiment", "content": "Experimental Setup. We initialize our model with the weights of CLIP (Radford et al., 2021) with ResNet50 vi-sion backbone and further pre-train it on the large-scale human action video dataset EPIC-KITCHEN-100 (Damen et al., 2018; 2020). For hyperparameter selection, we ran-domly sample 10 frames from each video per batch. The loss weight \\(\\lambda\\) to 100. Other hyperparameters like temper-atures follows the default value used in CLIP (Radford"}, {"title": "5.1. Simulation Environments", "content": "We perform LCBC experiments in two widely used simula-tion environments for evaluation: Franka Kitchen (Gupta et al., 2019; Fu et al., 2020) and Metaworld (Yu et al., 2019). As shown in Figure 2, for Franka Kitchen, we evaluate five tasks: sliding a cabinet, opening the left door, opening the microwave, turning on the stove, and switching on the light. For Metaworld, we focus on learning four tasks: hammering a nail, pressing a button, picking and placing a block, and assembling a ring onto a peg. Detailed environment setup can be found at Appendix C.1.\nIn Frankakitchen, tasks often involve intricate actions that induce large visual changes, and the successful completion of these tasks requires precise, complex actions. As a result, tasks on FrankaKitchen rely more on visual trajectory repre-sentations. On the other hand, the visual scene in Metaworld is much simpler. Tasks on Metaworld generally involve di-rect actions with smaller visual changes, yet they require more sophisticated language understanding to act towards"}, {"title": "5.2. Baselines", "content": "Since our model is initialized with CLIP (Radford et al., 2021), a state-of-the-art image-text representation widely applied in various embodied tasks (Cui et al., 2022; Khandelwal et al., 2022; Shridhar et al., 2021; Tam et al., 2022), it is a natural choice to include CLIP as a vanilla baseline for comparison. Our primary baselines are LIV (Ma et al., 2023a) and DecisionNCE (Li et al., 2024), as we all use the same model architecture and dataset for pretraining. LIV employs the VIP (Ma et al., 2023b) to achieve consistent frame representations and aligns the final frame with in-structions using the CLIP loss. DecisionNCE represents instructions as frame transitions and aligns the difference between the initial and final frames with the instructions using the CLIP loss. We also compare against R3M (Nair et al., 2022), pre-training on Ego4D (Grauman et al., 2022), which combines time contrastive learning (Sermanet et al., 2018) with LOREL (Nair et al., 2021) to ensure that later frames in the sequence receive higher rewards when aligned with the instruction."}, {"title": "5.3. Language-Conditioned Behavior Cloning", "content": "We keep the pre-trained vision-language encoders frozen and feed their output representations into a lightweight MLP to train the LCBC policies. Each task is performed from two camera viewpoints (left and right), with varying numbers of demonstrations [5, 15, 25] (i.e., dataset size) for training, and evaluated under three different random seeds. We report the success rate across different environments and dataset sizes, averaged over camera views and seeds. Detailed comparison results can be referred to Appendix C.3."}, {"title": "5.4. Language-Conditioned Visual Rewards", "content": "Since our model learns semantically smooth visual represen-tations, the resulting semantic trajectories can also serve as ideal task rewards. Specifically, we define the reward at time step i as \\(\\text{cosine}(v^i, l)\\), which reflects the distance between the current state and the language goal. Previous works (Ma et al., 2023a; Li et al., 2024) have primarily tested their rewards on single-action video clips. To increase task com-plexity, we selected three video clips, each containing two consecutive actions, to better evaluate whether the model accurately understands action semantics."}, {"title": "5.5. Visualization of Visual Representation Trajectory", "content": "To demonstrate the smoothness of the representations, we select three distinct language instructions from the EPIC-KITCHEN-100 dataset, each corresponding to 10 action videos. We then visualize the visual representations learned by our method and those by CLIP using t-SNE (van der Maaten & Hinton, 2008), as shown in Figure 5. The rep-resentations produced by CLIP show clear separability be-tween different language instructions, and the trajectories of individual videos maintain a certain degree of temporal consistency. However, the transitions between consecutive frames are not smooth, reflecting a flaw due to the absence of training on video data. In contrast, our method signifi-cantly enhances the ordering and continuity of video feature trajectories while preserving the discriminative power of CLIP for distinguishing actions associated with different instructions. This improvement stems from training directly on CLIP's weights and optimizing the temporal consistency within each video. As a result, our method not only achieves smoother representations but also retains the strong align-ment between visual features and language semantics in-herent in CLIP's original design. This balance between"}, {"title": "5.6. Robustness Study under Linguistic Perturbations", "content": "In the EPIC-KITCHEN-100 dataset, textual annotations are often concise, such as \u201copen cupboard\u201d. In the default setting of LCBC, we employ similarly structured simple instructions. In this experiment, to validate the robustness of the representations our method learns in real-world sce-narios, we introduce minor modifications to the language instructions. Specifically, we transform the original in-struction \u201c{action}\u201d into two more conversational styles, i.e., \u201cPlease {action} for me.\u201d and \u201cHelp me {action}.\u201d. We then evaluate the imitation learning per-formance conditioned on these modified instructions in the Franka Kitchen environment. For comparison, we select LIV and DecisionNCE, which are also pre-trained on EPIC-KITCHEN-100."}, {"title": "6. Conclusion", "content": "We present Action Temporal Coherence Learning (AcTOL) as a promising vision-language pre-training solution for generalizable embodied agents. By learning action consis-tency from a large corpus of human action videos, ACTOL theoretically ensures the ordering and continuity of vision-language representations, as well as robustness to language perturbations. Extensive experiments across various envi-ronments demonstrate that AcTOL effectively generalizes to complex robotic manipulation tasks. Due to hardware limitations, our evaluations are mainly conducted in sim-ulation environments, with real-world deployment left for future work."}, {"title": "7. Impact Statement", "content": "This paper aims to advance the development of Embodied AI. A potential ethical concern is that large-scale data used for vision-language representation pre-training may inadver-tently contain sensitive information or biases. To address this, we use the publicly available and rigorously reviewed EPIC-KITCHEN-100 human action video dataset (Damen et al., 2018; 2020) in our experiments to ensure compliance with ethical standards."}]}