{"title": "Beyond position: how rotary embeddings shape representations and memory in autoregressive transfomers", "authors": ["Valeria Ruscio", "Fabrizio Silvestri"], "abstract": "Rotary Positional Embeddings (RoPE) enhance positional encoding in Transformer models, yet their full impact on model dynamics remains underexplored. This paper studies how RoPE introduces position-dependent rotations, causing phase shifts in token embeddings that influence higher-frequency components within the model's internal representations. Through spectral analysis, we demonstrate that RoPE's rotation matrices induce oscillatory behaviors in embeddings, affecting information retention across layers and shaping temporal modeling capabilities. We show that activation functions in feed-forward networks interact with RoPE-modulated embeddings to generate harmonics, leading to constructive or destructive interference based on phase alignment. Our findings reveal that phase alignment amplifies activations and sharpens attention, while misalignment weakens activations and disrupts focus on positional patterns. This study underscores the importance of frequency components as intrinsic elements of model behavior, offering new insights beyond traditional analyses.", "sections": [{"title": "1 Introduction", "content": "Central to the Transformer architecture [Vaswani, 2017] is the self-attention mechanism, which allows the model to weight the influence of different parts of the input sequence when generating representations. However, because transformers are inherently permutation-invariant, incorporating positional information is crucial for the model to understand the sequential nature of language. Early methods used fixed sinusoidal encodings, while newer strategies, like Rotary Positional Embeddings (ROPE) [Su et al., 2024], apply rotation matrices directly to the token embeddings, integrating positional information through position-dependent rotations. These rotations introduce phase shifts in the embeddings' vector components, effectively encoding each token's position within the sequence.\nDespite ROPE's effectiveness, its impact on the internal dynamics of Transformer models particularly its interaction with non-linear components like feed-forward neural networks (FFNs) and self-attention-remains underexplored. Understanding this interaction is essential for comprehending how Transformers process and retain sequential information, essential for tasks requiring nuanced temporal modeling. Our work studies three main aspects: the spectral properties of RoPE, its interactions with non-linear activation functions, and the emergence of constructive and destructive interference patterns due to phase alignment.\nOur main contributions are: 1) Providing a detailed analysis of ROPE's impact on internal model behavior and positional dependency modeling. 2) Studying how non-linearities in Transformers interact with ROPE to produce complex frequency patterns."}, {"title": "2 Related Works", "content": "The Transformer architecture introduced by [Vaswani, 2017] revolutionized sequence modeling using self-attention mechanisms, making recurrent or structures not necessary anymore. To provide the model with information about the token position in the sequence, the original Transformer used sinusoidal positional encodings added to the input embeddings. [Shaw et al., 2018] proposed relative positional en- codings, integrating relative position information into self-attention to better generalize across varying se- quence lengths. Dai et al. [Dai et al., 2018] extended this with Transformer-XL, enhancing the capture of long-term dependencies. [Huang et al., 2018] intro- duced the Music Transformer, which leverages relative positional encoding to model long-term structure in music generation tasks.\n[Su et al., 2024] introduced Rotary Position Em- bedding (RoPE), applying rotation matrices to embeddings to introduce relative positional depen- dence through phase shifts, preserving inner product structures and enabling generalization to longer sequences.\nNonlinear systems, such as feedforward neural networks with activation functions like ReLU or GeLU, can generate higher-order harmon- ics and exhibit frequency mixing as studied by [Selesnick and Burrus, 1998]. Constructive and destructive interference are fundamental concepts in signal processing [Oppenheim, 1999], describing how overlapping waves can amplify or attenuate signals based on their phase alignment. By applying principles from signal processing, researchers have begun to analyze how frequency components and phase relationships affect neural network behavior. [Chi et al., 2020] investigated how convolutional neural networks process frequency information, high- lighting the importance of understanding the spectral characteristics of activations. [Takahashi et al., 2018] explored adaptive modulation in neural networks, drawing parallels between neural activations and signal modulation techniques."}, {"title": "3 Methodology", "content": "Our study combines theoretical analysis with empir- ical experiments to explore how Rotary Positional Embeddings (RoPE) influence Transformer models. We conducted experiments using autoregressive Trans- former models that implement Rotary Positional Em- beddings (RoPE) for consistent positional encoding, specifically LLaMA 2, LLaMA 3 and LLaMA 3.1 [Touvron et al., 2023]. The primary results presented in the main paper are performed using LLAMA 3, with full experimental details and additional results pro- vided in the appendix. Our study comprises two types of experiments aimed at understanding how phase shifts introduced by RoPE affect the models' internal dynamics:\na) Phase Shift Simulations on Real Embeddings: In the first set of experiments, we investigated the impact of manual phase shifts on the attention mechanism by applying rotational transformations to the embeddings extracted from the models. We used 1,000 text sam- ples, each containing 200 tokens, sourced from The BookCorpus [Zhu, 2015]. For each text sample, we passed the input through the model to obtain the to- ken embeddings before the application of positional encodings. After we manually applied rotation ma- trices to these embeddings to simulate phase shifts, the embeddings were then fed back into the model to study how the simulated phase shifts influenced the at- tention scores. This experiment aimed to understand the sensitivity of the model's attention mechanism to phase differences introduced by ROPE.\nb) Synthetic Sequence on FFN Activations: The second set of experiments focused on studying the effects of phase alignment and misalignment on the activations within the FFN of the models. We generated syn- thetic sequences to isolate the impact of input struc- ture on phase interactions without altering the embed- ding rotations. These sequences consisted of repeated instances of the same token, since the tokens are iden- tical, their embeddings (prior to positional encoding) are the same, promoting phase alignment after ROPE is applied, and sequences constructed by alternating between different tokens, resulting in varying embed- dings and promoting phase misalignment. Each type comprised 250. We computed metrics such as variance, kurtosis, entropy, and the number of activation peaks to quantify differences in activation patterns between aligned and misaligned inputs.\nExperiments were performed on Google Colab Pro with NVIDIA A100 GPU, using the Hugging Face Transformers library."}, {"title": "4 Spectral Properties", "content": "ROPE applies a rotation to the embedding vectors, parametrized by a frequency \u03b8k for each dimension. For a position p, the embedding e(p) is rotated as:\n$$e(p) = e(p) \\cos(\\theta_k p) + e_\\perp(p) \\sin(\\theta_k p)$$\nwhere $e_\\perp(p)$ is the orthogonal component of e(p). The spectral properties of the rotation matrices intro- duced by ROPE can be analyzed by considering the eigenvalues of these matrices. The eigenvalues of the rotation matrices represent the frequencies at which the embeddings are modulated due to the positional encoding. By computing these eigenvalues, we can un- derstand how ROPE introduces phase shifts that vary with position, effectively encoding positional informa- tion into the embeddings.\nSince these are rotation matrices, the eigenvalues lie on the unit circle in the complex plane, meaning that the rotation preserves the norm of the embeddings but alters their phase. Specifically, the rotation induces os- cillatory behavior in the embedding space, which can be decomposed into frequency components."}, {"title": "4.1 Eigenvalue and Spectral Analysis of ROPE", "content": "To deepen our understanding of how ROPE affects the embeddings' behavior in Transformers, we examine the eigenvalues of the rotation matrices Rk(p) used in the positional encoding. While rotation matrices are known to have eigenvalues on the unit circle, ana- lyzing them in the context of ROPE reveals important implications for the model's temporal dynamics and memory retention capabilities.\nConsider the 2D rotation matrix for dimension k:\n$$R_k(p) = \\begin{pmatrix} \\cos(\\phi_k) & \\sin(\\phi_k) \\\\ -\\sin(\\phi_k) & \\cos(\\phi_k) \\end{pmatrix}$$\nwhere \u03c6k = \u03b8k(p).\nThe eigenvalues \u03bb of Rk(p) satisfy:\n$$\\lambda = \\cos \\phi_k \\pm i \\sin \\phi_k = e^{\\pm i \\phi_k}$$\nThese eigenvalues lie on the unit circle in the complex plane, with magnitude |\u03bb| = 1 and phase \u03c6k = \u03b8k(p). This result illustrates that the rotation induced by ROPE causes the embeddings to oscillate with a fre- quency determined by \u03b8k, and the phase changes lin- early with position p.\nThis analysis shows how the choice of \u03b8k affects the model's sensitivity to positional changes. By under- standing the eigenvalues, we can infer how ROPE mod- ulates the embeddings to prioritize either long-term dependencies (slow phase changes) or short-term inter- actions (rapid phase changes), thereby impacting the model's temporal dynamics and memory retention.\nImplications for temporal dynamics as the se- quence progresses, the embeddings undergo continuous rotations in their respective 2D subspaces. The lin- ear change in phase with respect to position p means that embeddings at different positions have different phases, leading to oscillatory behavior. This directly impacts how the model processes sequential informa- tion, as the phase differences influence the dot-product attention calculations, leading to constructive or de- structive interference patterns.\nImplications for Memory Retention and Tem- poral Processing the frequency \u03b8k determines the rate of phase change. Smaller values of \u03b8k result in slower phase changes, meaning that embeddings re- tain more similarity over longer positional distances. This corresponds to longer memory retention in the model. Conversely, larger \u03b8k values lead to faster phase changes, focusing the model's attention on short-term dependencies. By selecting appropriate values for \u03b8k, we can control the timescale over which the model re- tains information, effectively tuning the Transformer's temporal processing capabilities. For example, in lan- guage modeling tasks involving long texts, setting smaller \u03b8k values allows the model to retain and in- tegrate information from earlier parts of the text, en- hancing its ability to understand overarching themes and plot developments. On the other hand, in real- time translation systems where recent words are more relevant, larger \u03b8k values enable the model to quickly adapt to new inputs, improving translation accuracy for rapidly changing contexts."}, {"title": "5 Memory Dynamics", "content": "In Transformer models, ROPE modifies the incorpora- tion of positional information by applying a rotation matrix R(p) to the queries and keys at each position p:\n$$Q'(p) = R(p)Q(p), K'(p) = R(p)K(p)$$\nThe attention score between positions p and q be- comes:\n$$S(p,q) = \\frac{Q'(p) \\cdot K'(q)}{\\sqrt{d_k}} = \\frac{Q(p)^T R(p)^T R(q) K(q)}{\\sqrt{d_k}}$$\nUsing the property that the transpose of a rotation matrix equals its inverse, $R_k(p)^T = R_k(p)^{-1}$, and that $R_k(p)^{-1}R(q) = R(q - p)$, we simplify the attention score to:\n$$S(p,q) = \\frac{Q(p)^T R(q-p) K(q)}{\\sqrt{d_k}}$$\nThis expression shows that the attention score depends solely on the relative position \u2206p = q - p. The rotation R\u2206p introduces a phase shift affecting the alignment between Q(p) and K(q). As |\u2206p| increases, the phase difference grows, potentially reducing the dot product due to misalignment. This leads to a natural decay of attention scores with increasing positional distance, ef- fectively incorporating relative positional information into the model."}, {"title": "5.1 Relationship between \u03b8k and similarity", "content": "The relationship between the positional difference (p- q) and the frequency \u03b8k plays a key role in the simi- larity between embeddings. When (p - q)\u03b8k \u2248 \u03c0, the cosine of this value is close to -1, which can lead to in- teresting behaviors in how tokens at positions p and q are compared: i) Close Tokens, ii) Decay of Similarity and iii) Sign Flips and Oscillations.\nWhen tokens are close in position, (p \u2013 q)\u03b8k is small, and cos(p - q)\u03b8k is close to 1. This results in a high similarity between their embeddings, as expected. As (p - q)\u03b8k increases, the cosine value decreases, reach- ing zero at (p - q)\u03b8k = $\\frac{\\pi}{2}$. This implies a decay in similarity between tokens as their positional distance grows. Once (p-q)\u03b8k surpasses $\\frac{\\pi}{2}$, the cosine becomes negative. This leads to a situation where tokens that should be similar exhibit negative similarity, meaning their embeddings no longer align as intended. For ex- ample, an identical query might match a random key better than an identical key due to the cosine's sign flip. This happens because if $\\vec{x} \\cdot \\vec{x}$ is positive (like when the embeddings are similar), but $cos((p-q)\\theta_k)$ is negative, the similarity becomes negative. For a ran- dom key, the inner product $\\vec{x} \\cdot \\vec{x}$ is likely to be around 0, so the similarity would not be negative, making it appear as a better match than an identical key.\nWhen (p-q)\u03b8k \u2265 \u03c0, the cosine starts to increase again, which contradicts the idea of continuous decay 1. The solution is bounding (p - q)\u03b8k to ensure that it stays within a range where the cosine function behaves in a decaying way.\nFigure 1 shows how the average cosine similarity changes when the positional distance for Various \u03b8k values changes. The plot shows that embeddings with smaller Ok values maintain higher similarity over longer distances compared to those with larger Ok valuesk values, where small 0k = 0.01, medium \u03b8k = 1 and large 0k = 100."}, {"title": "5.2 Constructing the Memory Function", "content": "To analyze how ROPE influences the model's ability to retain information over time, we consider the memory function M(n), representing the cumulative effect of past inputs at lag n. Since ROPE introduces cosine This behaviour is due to cosine being an oscillatory function. modulations into the attention mechanism, the mem- ory function can be approximated as:\n$$M(n) = \\sum_{k=1}^{K} a_k \\cos(\\theta_k n)$$\nHere, \u03b8k are the frequency components introduced by ROPE, and ak are weighting factors representing the contribution of each frequency component. The attention score between positions t and t-n is modulated by cos(\u03b8kn), indicating that the influence of position t n on position t oscillates and decays based on \u03b8k.\nThis formulation resembles a Fourier series, where the memory function M(n) captures the cumulative effect of past inputs as a superposition of oscillatory compo- nents. The frequencies \u03b8k and weights ak determine which temporal frequencies are emphasized or attenu- ated as information propagates through the network. If ak decreases for higher frequencies, the memory of distant past inputs decays more rapidly. The weights ak can be influenced by both the attention mechanism and the transformer's inherent frequency response, po- tentially evolving across layers as the model dynami- cally adjusts its emphasis on different frequency com- ponents during processing.\nViewing the attention mechanism as performing tem- poral filtering, we formally derive the memory function using Fourier analysis. Suppose the input sequence xt is decomposed into its frequency components:\n$$x_t = \\sum_{k} A_k e^{i\\theta_k t}$$\nHere, Ak are the Fourier coefficients, and \u03b8k are the corresponding frequencies. The attention mechanism modulates these components by cos(\u03b8kn):\n$$Attention(x_t, x_{t-n}) \\sim A_k \\cos(\\theta_k n)$$\nThe memory function then becomes:\n$$M(n) = \\sum_{k} A_k^2 \\cos(\\theta_k n)$$\nThis represents the cumulative influence of past in- puts, modulated by the frequency components \u03b8k in- troduced by ROPE. The presence of cosine terms im- plies that the memory function oscillates over time, with periods and amplitudes determined by Ok and \u03b1\u03ba. Depending on the distribution of Ok, the model may retain or attenuate certain temporal frequencies as information flows through the layers."}, {"title": "6 interactions with attention mechanism", "content": "In Transformer models, the attention mechanism se- lectively amplifies or suppresses different parts of the sequence based on learned attention weights, allow- ing the model to focus on relevant information. When combined with Rotary Position Embedding (RoPE), the interplay between phase shifts and nonlinearity leads to complex interactions where certain frequency components are enhanced or suppressed.\nROPE introduces phase shifts to the embeddings by applying a rotation that depends on the token's posi- tion. Specifically, for each pair of embedding dimen- sions (2k, 2k+1), the embedding is rotated by an angle proportional to the position index p:\n$$q^{(2k)}_p = q^{(2k)} \\cos(\\theta_k p) - q^{(2k+1)} \\sin(\\theta_k p)$$\n$$q^{(2k+1)}_p = q^{(2k)} \\sin(\\theta_k p) + q^{(2k+1)} \\cos(\\theta_k p)$$\nand similarly for the key vectors kq:\n$$k^{(2k)}_q = k^{(2k)} \\cos(\\theta_k q) - k^{(2k+1)} \\sin(\\theta_k q)$$\n$$k^{(2k+1)}_q = k^{(2k)} \\sin(\\theta_k q) + k^{(2k+1)} \\cos(\\theta_k q)$$\nwhere $q^{2k}, q^{2k+1}, k^{2k}$ and $k^{2k+1}$ are components of the original query and key vectors, and \u03b8k is the frequency parameter associated with the k-th dimension.\nThis means that the attention mechanism becomes sensitive not just to the content of the tokens but also to their relative positions. By encoding positional dif- ferences into the phase of embeddings, ROPE allows the model to capture patterns that depend on the dis- tance between tokens, such as syntax structures or rhythmic patterns in text.\nThe attention score between positions p and q is com- puted as:\n$$S_{pq} = \\frac{\\vec{q}_p \\cdot \\vec{k}_q}{\\sqrt{d_k}}$$\nSubstituting the rotated query and key vectors, and applying the rotation identities, we find that the dot product depends on the relative position \u2206p = q - p:\n$$\\vec{q}_p \\cdot \\vec{k}_q = \\sum_k [q^{(2k)}k^{(2k)} + q^{(2k+1)}k^{(2k+1)}] \\cos(\\theta_k \\Delta p)$$\nThis shows that the attention score is modulated by a cosine function of the relative position, introducing an oscillatory behavior:\n$$S_{pq} = \\frac{1}{\\sqrt{d_k}} \\sum_k (q^{[k]} \\cdot k^{[k]}) \\cos(\\theta_k (q-p))$$\nwhere $\\vec{q}^{[k]} \\cdot \\vec{k}^{[k]} = q_{2k} k_{2k} + q_{2k+1} k_{2k+1}$. This expression shows that the dot product between the query and key vectors is modulated by a cosine function of the relative position:\n$$\\vec{q}_k \\cdot \\vec{k}_q \\propto \\cos (\\theta_k (p - q))$$\nThe rotational modulation of the embeddings by ROPE creates a form of frequency-based filtering, where certain phase differences lead to constructive in- terference (higher attention weights), while others lead to destructive interference (lower attention weights). This results in an oscillatory behavior in the attention scores, effectively modulating how different positions interact based on their relative distances and the fre- quency parameters \u03b8\u03ba.\nTo investigate how phase differences introduced by ROPE affect the attention mechanism, we plotted the attention scores against the cosine of the phase differ- ence (cos A) between token embeddings in Figure 2. The plot reveals a pronounced peak at (cos \u2206\u03b8) = 1, corresponding to a phase difference of zero. This indicates that the model assigns the highest atten- tion when there is no phase shift between tokens. As (cos A) decreases, the attention scores decline sharply, showing the model's sensitivity to even minor phase differences. This behavior suggests that phase alignment plays a critical role in the attention mech- anism, with misaligned phases leading to diminished attention weights. Figure 3 illustrates the relation- ship between attention scores and positional difference (pq). The attention scores are highest when the po- sitional difference is zero and decrease rapidly as the absolute value of (p - q) increases. This pattern indi- cates that the model predominantly focuses on tokens that are close to each other in the sequence. The rapid decline in attention with increasing positional distance suggests that RoPE effectively encodes positional in- formation, causing the model to prioritize local con- text."}, {"title": "7 interactions with the Sfotmax", "content": "The attention weights are computed using the softmax function:\n$$a_{pq} = \\frac{exp (S_{pq})}{\\sum_j exp (S_{pj})}$$\nThe softmax function is highly sensitive to variations in the attention scores Spq. The oscillatory terms cos(\u03b8k (p - q)) introduced by RoPE can lead to con- structive or destructive interference, depending on the relative positions and frequency parameters. Small changes in Spq can cause significant differences in the attention weights apq, amplifying or attenuating the contributions from different positions.\nTo analyze how the nonlinearity of the softmax func- tion affects the frequency components, we can con- sider a Taylor series expansion of the softmax function around the mean \u015dp:\n$$softmax(s_{pq}) \\approx \\frac{1}{N} + \\frac{s_{pq} - \\bar{s}_p}{N} - \\frac{(s_{pq} - \\bar{s}_p)^2}{2N} + ...$$\nwhere N is the number of positions in the sequence, and $\\bar{s}_p$ is the mean of spq over q.\nBy substituting the phase-dependent dot product into the expansion, we can see how the nonlinear softmax function introduces higher-order terms that influence the frequency components:\n1) The first-order term $\\frac{s_{pq} - \\bar{s}_p}{N}$, captures the linear relationship between the input and the output, pre- serving the original frequencies introduced by ROPE.\n2) The second-order term $\\frac{(s_{pq} - \\bar{s}_p)^2}{2N}$, introduces nonlinearity by squaring the input. This term can gen- erate new frequency components that are harmonics of the original frequencies, effectively doubling frequen- cies or creating combinations.\n3) Subsequent higher-order terms in the expan- sion can introduce even more complex frequency inter- actions, including frequency mixing and the creation of new frequency components not present in the orig- inal signal.\nThese higher-order effects can be significant when the input (for example, the modulated dot product) has strong frequency components due to ROPE. The non- linearity of the softmax function can amplify or atten- uate certain frequencies, depending on the phase and amplitude of the input signal.\nBy expanding the softmax function, we observe that the ROPE-induced frequencies don't simply pass through the attention mechanism unaltered. Instead, they interact with the softmax's nonlinearity, which can enhance or suppress specific frequency compo- nents. The softmax function can be thought of as ap- plying a nonlinear transformation that emphasizes the most significant attention scores (high-frequency com- ponents resulting from constructive interference) while diminishing less important ones (frequencies leading to destructive interference). This selective enhancement resembles the effect of a resonance filter that amplifies certain frequencies, shaping the model's sensitivity to specific positional relationships."}, {"title": "8 interactions with the FFN", "content": "After the attention mechanism, Transformers typically apply a feedforward neural network (FFN), introduc- ing additional nonlinearity. The FFN can be repre- sented as:\n$$z = W_2 \\cdot \\sigma (W_1 x + b_1) + b_2$$\nwhere x is the input (the output of the attention mech- anism, already RoPE-modulated), W\u2081 and W2 are weight matrices, b\u2081 and b2 are bias terms, and ois a non-linear activation function (ReLU or GeLU).\nThe non-linear activation function o introduces fur- ther nonlinearity by transforming the input in a non- linear fashion. This nonlinearity interacts with the frequency components of the input in several ways: i) Harmonic Generation, where non-linearities can in- troduce higher-order harmonics, adding new frequency components at integer multiples of the original fre- quencies. ii) Waveform Distortion, where the ac- tivation function can distort the waveform of the in- put signal, altering the amplitude and phase of the frequency components. iii) Phase Shifts, where the linear transformations before and after the activation function can introduce phase shifts, affecting how dif- ferent frequency components combine in the network."}, {"title": "8.1 Phase Modulation in the FFN with ROPE", "content": "Let x(p) represent the RoPE-modulated output of the attention head for the token at position p:\n$$x(p) = R(p) a(p)$$\nwhere a(p) is the unmodulated attention output, and R(p) is the RoPE rotation matrix applied at position p. For a given dimension pair k, k+1, ROPE introduces phase shifts:\n$$X_k(p) = a_k (p) \\cos(\\theta_k p) + a_{k+1}(p) \\sin(\\theta_k p)X_{k+1}(P) = -a_k (p) \\sin(\\theta_k p) + a_{k+1}(p) \\cos(\\theta_k p)$$\nwhere ak (p) and ak+1(p) are components of a(p), and \u03b8k is the frequency parameter associated with the k- th dimension. For the k-th neuron, substituting the phase-shifted expressions for xk(p) and xk+1(p), we get:\n$$h_k(p) = [W_{k1} \\cos(\\theta_k p) W_{k2} \\sin(\\theta_k p)] a_k(p) + [W_{k1} \\sin(\\theta_k p) + w_{k2} \\cos(\\theta_k p)] a_{k+1}(p) + b_{1k}$$\nThe first linear layer in the FFN mixes the phase- shifted components of the attention output. The coef- ficients wk\u2081 and wk2 are modulated by trigonometric functions of the token's position p, which means that the contribution of the attention outputs ak(p) and ak+1(p) to the FFN activations depends on the posi- tional phase shifts introduced by ROPE.\nThis has two major effects:\n1) Frequency Mixing: The interaction between the weight matrix W\u2081 and the RoPE-induced phase shifts leads to frequency mixing. Each dimension of the em- bedding oscillates with a frequency determined by Ok. The linear transformation effectively mixes these os- cillating components, resulting in new frequency com- ponents in the activations of the FFN.\n2) Positional Sensitivity: Since the weights W\u2081 are combined with the trigonometric functions cos(\u03b8kp) and sin(0kp), the linear transformation encodes posi- tional sensitivity into the FFN's activations. Tokens at different positions contribute differently to the ac- tivations depending on their phase alignment.\nAfter the linear transformation, the output passes through a non-linear activation function, such as ReLU or GeLU:\n$$z_k(p) = \\sigma (h_k(P))$$Non-linear activations introduce higher-order harmon- ics when applied to oscillatory inputs like the ROPE- modulated activations hk (p). This phenomenon is well-known in signal processing, where non-linearities applied to sinusoidal signals generate new frequency components (harmonics) at integer multiples of the original frequencies.\nConsidering the ReLU activation:\n$$z_k(p) = ReLU (h_k(p))$$\nFor an oscillatory input hk (p) containing components like cos(@kp) and sin(0kp), ReLU clips the negative parts of the oscillation, resulting in higher-order har- monics. Specifically: i) The fundamental frequency Ok remains present in the output. ii) Higher harmon- ics such as 20k, 30k, etc., are generated due to the non-linearity introduced by ReLU. Mathematically, this harmonic generation can be described using the Fourier series expansion of a half-wave rectified sinu- soid. For example:\n$$ReLU (\\cos(\\theta_k p)) = \\frac{1}{\\pi} + \\frac{1}{2} \\cos(\\theta_k p) - \\frac{1}{\\pi} \\sum_{n=1}^{\\infty} \\frac{\\cos(2n\\theta_k p)}{4n^2-1}$$\nThis expansion shows that higher-order harmonics are introduced into the signal.\nThese harmonics enable the FFN to capture multi- scale dependencies in the sequence. Since ROPE causes embeddings at different positions to oscillate with dif- ferent phases, the non-linearity can lead to construc- tive or destructive interference, depending on phase alignment.\nConstructive Interference occurs when the phase shifts of different components hk (p) are aligned; the non-linearity amplifies the output, resulting in stronger FFN activations. Thus, certain positional configurations reinforce each other, leading to higher neuron activations.\nDestructive Interference happens when the phases are misaligned; components can cancel each other out, leading to weaker activations or even zero activations. This allows the FFN to selectively suppress certain po- sitional patterns that are not relevant for the current token.\nThis means that the model can selectively enhance or suppress connections between tokens based on their positional relationships. For example, in a sentence where a subject and verb are separated by several words, constructive interference can reinforce their grammatical connection despite the distance. Con- versely, destructive interference can help the model ig- nore irrelevant or less important tokens, such as filler words, by reducing their impact on the attention mech- anism."}, {"title": "8.1.1 Second Linear Layer: Projecting Higher-Level Features", "content": "After the non-linear activation, the output is passed through the second linear transformation:\n$$FFN(x) = W_2 \\cdot z(p) + b_2$$\nThe second linear layer W2 maps the non-linear ac- tivations back to the original embedding space or to a space compatible with the subsequent layers, effec- tively projecting the higher-level features extracted by the FFN. Since the input z(p) contains both the orig- inal RoPE-induced frequencies and the higher-order harmonics generated by the non-linearity, W2 acts as a frequency-selective filter that determines which fea- tures are propagated forward.\nTwo major effects occur in the second linear layer: i) Frequency Selection, since W2 can emphasize cer- tain frequency components (for example, fundamen- tal frequencies or specific harmonics) while attenuat- ing others. This allows the FFN to focus on tem- poral or positional dependencies most relevant to the task. ii) Higher-Level Feature Representation, because the second linear layer combines frequency-modulated features into a higher-level representation that cap- tures both content-based and position-based informa- tion, crucial for handling the sequential nature of data in autoregressive Transformers."}, {"title": "9 Analysis", "content": "Our study shows that the rotation matrices used in ROPE introduce specific phase components into the token embeddings, leading to oscillatory behaviors characterized by distinct frequencies. When these frequency-modulated embeddings interact with the non-linear activation functions in the feed-forward neural networks (FFNs), higher-order harmonics are generated due to the nonlinearities introducing new frequency components. This interaction results in patterns of constructive and destructive interference based on the phase alignment of the embeddings. Con- structive interference amplifies neuron activations, en- hancing the model's attention to key positional pat- terns, while destructive interference diminishes acti- vations, potentially weakening the model's ability to capture certain dependencies.\nDrawing an analogy to signal processing, RoPE func- tions similarly to frequency modulation (FM) by ad- justing the phase of embeddings according to posi- tional information, enriching the representation space and enabling the model to capture positional nuances. The attention mechanism and FFNs act as nonlinear filters that shape the frequency content of these modu- lated signals in a context-sensitive manner, emphasiz- ing or attenuating specific components based on the model's learned parameters. The generation of har- monics through these nonlinearities expands the sig- nal's frequency spectrum, allowing the Transformer to represent more complex patterns and efficiently cap- ture long-range dependencies. These dynamics en- hance the model's ability to adjust its focus dynam- ically, responding to context and varying sequential dependencies inherent in the input data."}, {"title": "10 Conclusion", "content": "In summary, our analysis shows that frequencies are intertwined with the operation of Transformers uti- lizing RoPE. The introduction of phase components and their interaction with non-linear activations gen- erate oscillatory behaviors and interference patterns that significantly impact how information is processed across the model's layers. Understanding these fre- quency dynamics provides valuable perspective into the internal workings of language models, unfolding on how they encode and manipulate sequential data."}, {"title": "11 ADDITIONAL EXPERIMENTS", "content": "This set of experiments focuses on studying the effects of phase alignment and misalignment on the activations within the FFN of the models. We generated 250 synthetic sequences to isolate the impact of input structure on phase interactions without altering the embedding rotations. These sequences consisted of repeated instances of the same token to promote phase alignment, and sequences constructed by alternating between different tokens, resulting in varying embeddings and promoting phase misalignment.\nConsider a repeated token t, before applying RoPE, the embedding E(t) is identical at each position. After applying ROPE, the embedding at position p becomes: $E'(p) = R(p)E(t)$. The phase shift between positions p and q is:\n$$\\Delta \\varphi = \\theta(p \u2013 q)$$\nwhere is the rotational frequency parameter. The consistent phase increments lead to predictable interference patterns when computing dot products in attention mechanisms or processing through FFNs. In sequences with alternating tokens t\u2081 and t2, the embeddings before RoPE are E(t1) and E(t2). After applying ROPE:\n$$E_1(p) = R(p)E(t_1)$$\n$$E_2(p + 1) = R(p+1)E(t_2)$$\nThe phase difference between embeddings is influenced not only by positional differences but also by the inherent differences in E(t1) and E(t2). This results in more complex interactions and potential cancellation effects within the model's computations."}, {"title": "11.1 Llama 2", "content": "In Llama 2 there's a noticeable difference between the mean activations of aligned and misaligned sequences. The aligned sequences often have mean activations close to zero, while the misaligned sequences show varying mean values, sometimes positive and sometimes negative.\nAs we can see in Table 1, the standard deviation (Std) of activations is consistently higher for misaligned sequences compared to aligned sequences. This suggests that misaligned sequences produce more variable acti- vations, possibly due to the complexity introduced by alternating tokens.\nThe KS statistics are significant across all layers (p-values effectively zero), indicating that the distributions of activations for aligned and misaligned sequences are statistically different. in Table 2 we show that the t- statistics show both positive and negative values, reflecting the direction of mean differences.\nThe t-statistics"}]}