{"title": "HADL FRAMEWORK FOR NOISE RESILIENT LONG-TERM TIME SERIES FORECASTING", "authors": ["Aditya Dey", "Jonas Kusch", "Fadi Al Machot"], "abstract": "Long-term time series forecasting is critical in domains such as finance, economics, and energy, where accurate and reliable predictions over extended horizons drive strategic decision-making. Despite the progress in machine learning-based models, the impact of temporal noise in extended lookback windows remains underexplored, often degrading model performance and computational efficiency. In this paper, we propose a novel framework that addresses these challenges by integrating the Discrete Wavelet Transform (DWT) and Discrete Cosine Transform (DCT) to perform noise reduction and extract robust long-term features. These transformations enable the separation of meaningful temporal patterns from noise in both the time and frequency domains. To complement this, we introduce a lightweight low-rank linear prediction layer that not only reduces the influence of residual noise but also improves memory efficiency. Our approach demonstrates competitive robustness to noisy input, significantly reduces computational complexity, and achieves competitive or state-of-the-art forecasting performance across diverse benchmark datasets. Extensive experiments reveal that the proposed framework is particularly effective in scenarios with high noise levels or irregular patterns, making it well suited for real-world forecasting tasks.", "sections": [{"title": "Introduction", "content": "Real-world time series data from domains such as finance [1], economics [2], and energy systems [3] rely on accurate long-term forecasting to support informed decision-making that has a significant impact on global development [4]. However, these datasets often contain significant levels of temporal noise due to their highly volatile nature [5], environmental factors [6], and random fluctuations. This poses a critical challenge for Long-term Time Series Forecasting (LTSF) models, as they can obscure the underlying patterns and reduce the accuracy of the predictions [7]. Thus, a systematic study of temporal noise in LTSF and its effects on model accuracy is required.\nMost LTSF transformer models can perform well with a small lookback window due to the large parameter size and the benefits of using vanilla or modified attention [8, 9, 10]. However, the gains in accuracy from increasing lookback windows are not significant [8], which may be due to temporal noise that obscures the underlying trend and seasonality patterns. This highlights the trade-off between a large lookback window, parameter size, and temporal noise, where increasing the lookback window or parameter size may not yield significant accuracy gains due to noise."}, {"title": "Related Works", "content": "MLP models offer computationally efficient and interpretable alternatives, often achieving predictive performance that is comparable to or superior to other approaches [7, 11, 13]. For example, DLinear [7] applies trend and seasonality decomposition, whereas FITS [11] achieves higher accuracy using Discrete Fourier Transform (DFT) with approximately 5K parameters. Similarly, SparseTSF [12], an ultra-lightweight model with approximately 1K parameters, effectively utilizes cross-period sparse techniques to extract periodic features. Other notable MLP-based models, such as TSMixer [13], LightCTS [14], and FreTS [15], may not be as lightweight as SparseTSF but nevertheless deliver competitive performance.\nSome of these models rely on techniques that extract periodicity in the time domain by selecting the period length as a hyperparameter [12, 15] or by converting the data into the frequency domain using the DFT [11, 15]. Time domain approaches often lack robust methods for reducing temporal noise, and the assumption that all features share the same periodicity set as a hyperparameter can lead to inaccuracies [12]. On the other hand, frequency-domain methods using DFT with cutoff frequencies to reduce temporal noise assume a uniform cutoff range for all features, which may impact accuracy."}, {"title": "Convolutional and Recurrent Neural Network Methods", "content": "Other classes of models, such as ModernTCN [16], employ dilated temporal convolutions to extract and enhance temporal characteristics, reaffirming the competitiveness of Convolutional Neural Network (CNN)s. Although not as lightweight as MLP models, ModernTCN demonstrates comparable runtime efficiency. Similarly, Recurrent Neural Network (RNN)-based models, such as SegRNN [17], xLSTM [18], and WiTRAN [19], excel in capturing temporal dependencies and sequence relationships. However, like other approaches, these models rely on appropriate lookback window sizes and large parameter counts, which may be affected by temporal noise."}, {"title": "Transformer-based Models", "content": "Most emerging LTSF models are transformer-based [20], with notable examples such as PatchTST, which demonstrates that modifications to the attention mechanism can be avoided by segmenting univariate time series into patches [8]. The iTransformer extends this concept further by treating each feature as a patch, allowing a receptive field that spans the entire length of the feature [9]. Although these patching techniques can enhance model performance, they also introduce challenges. For example, a noisy patch can obscure the underlying patterns. Similarly, in iTransformer, noisy features may impede the model's ability to discern meaningful structures. These limitations underscore the sensitivity of patch-based methods to temporal noise, posing challenges to consistent performance.\nDespite these challenges, transformer-based models continue to inspire advancement, with recent models, such as FredFormer [21] and FrNet [10], aiming to further enhance prediction accuracy. However, these transformers often require significant memory and computational resources, making them less practical for scenarios where efficiency is critical.\nLow-rank implementations have not been widely studied in LTSF models. However, they have shown potential in vanilla transformers by improving generalization [22, 23, 24]. They have also been shown to enhance inter-channel dependencies in LTSF [25]. Incorporating low-rank techniques can improve performance in time series forecasting by efficiently capturing critical temporal patterns while reducing parameter complexity.\nCurrently, no LTSF models have simultaneously addressed noise robustness, accuracy improvement, and lightweight design in a unified manner."}, {"title": "The Proposed Method", "content": "In this section, we provide a concise overview of the HADL framework, followed by a detailed explanation and rationale for the design choices.\nProblem Definition: Given a regularly sampled multivariate time series with a lookback window L, represented as $X_T = x_1,...,x_t,...,x_L \\in \\mathbb{R}^{C \\times L}$, where each time step $x_t$ is a vector of dimension C, the objective is to forecast future values $\\hat{Y}_T = x_{L+1},...,x_{L+r},...,x_{L+H} \\in \\mathbb{R}^{C \\times H}$ over a horizon of length H.\nSummary: The multivariate time series $X_T \\in \\mathbb{R}^{C \\times L}$ is first transformed by DWT using Haar wavelet to obtain approximation coefficient values $A_T \\in \\mathbb{R}^{C \\times \\frac{L}{2}}$, effectively compressing the input length by half while removing the detail coefficient values to mitigate the impact of noise. Next, the time domain representation $A_T$ is converted into the frequency domain $A_F \\in \\mathbb{R}^{C \\times \\frac{L}{2}}$ using the DCT, allowing the model to extract long-term periodic features at high-frequency resolution. Finally, the frequency representation $A_F$ is processed through a low-rank layer to predict the target $\\hat{Y}_T \\in \\mathbb{R}^{C \\times H}$ in the time domain, eliminating the need for an explicit inverse transform.\n$A_{T, _} = DWT(X_T)$  (1a)\n$A_F = \\frac{2}{L} \\times DCT(A_T)$ (1b)\n$\\hat{Y}_T = LowRankLayer(A_F)$ (1c)\nThe complete architecture of the HADL framework is illustrated in Figure 1. The transformation and prediction processes are formally described by the equations in (1) and the algorithm presented in Appendix B."}, {"title": "Discrete Wavelet Transform", "content": "The trend-seasonality decomposition in the time domain provides high temporal resolution but relies heavily on accurately selecting the seasonal window to capture periodic patterns [7, 12]. An incorrect choice of window size can lead to temporal leakage or distortion of periodic components. In contrast, the DFT offers high-frequency resolution [11, 15] but operates under the implicit assumption that the observed time series represents a complete cycle of a single period [26]. Violating this assumption results in spectral leakage, which distorts the frequency components. A balanced resolution in both the time and frequency domains can be achieved through methods like Short Time Fourier Transform (STFT) or DWT, which provide multiresolution decomposition at the cost of increased dimensionality.\nThe key advantage of DWT over STFT lies in the orthogonality of wavelet functions, which ensures that at each level of decomposition, distinct frequency components are captured [27]. This property eliminates redundancy and improves the interpretability of transformed data, addressing the main limitation of STFT, where overlapping windowed components can redundantly capture the same frequency across multiple levels.\nTherefore, our method begins with DWT to decompose the time series. Among various wavelet families, we choose the Haar wavelet because of its simplicity and interpretability. The Haar wavelet performs convolution on the time series with two sets of filters: an approximation filter and a detail filter with a kernel size of 2 and a stride of 2. The approximation filter is an averaging operator, while the detail filter acts as a differencing operator. For our purposes, we use the approximation coefficients $A_T \\in \\mathbb{R}^{C \\times \\frac{L}{2}}$, which capture essential information while reducing noise. The detail coefficients $D_T \\in \\mathbb{R}^{C \\times \\frac{L}{2}}$, which mainly represent noise, are discarded.\n$A_T = X_T * \\begin{bmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{bmatrix}$ (2)\n$D_T = X_T * \\begin{bmatrix} \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\end{bmatrix}$ (3)\nAlthough it is possible to use detail coefficients for prediction with linear layers, doing so would increase the model's complexity and parameter count without substantial benefits.\nWe limit the decomposition to a single level to avoid excessive smoothing, which can eliminate critical frequency components necessary for accurate forecasting. For lookback windows in the typical range of 336 to 720, a single level of decomposition is sufficient. However, multiple decomposition levels may be considered for ultra-long lookback windows. Additionally, employing linear layers across multiple decomposition levels significantly increases computational complexity and may not provide substantial improvements in feature extraction."}, {"title": "Discrete Cosine Transform", "content": "DFT and DCT share a common mathematical foundation but differ in their focus and application. DFT represents a signal using both real and imaginary components. The cosine component captures smooth, low-frequency variations, while the sine component highlights abrupt changes and high-frequency components [26]. Although this dual repre-sentation is comprehensive, the DCT's exclusive focus on cosine components leads to energy compaction, in which most of the signal's energy is concentrated in fewer coefficients [28, 29]. This makes DCT particularly effective in representing long-term patterns in LTSF while avoiding the additional complexity introduced by imaginary parts.\nThus, the approximation coefficients $A_T \\in \\mathbb{R}^{C \\times \\frac{L}{2}}$ undergo DCT Type II conversion, generating an output $A_F \\in \\mathbb{R}^{C \\times \\frac{L}{2}}$ with high-frequency resolution. This transformation is accompanied by length-based normalization $(\\frac{2}{L})$, ensuring that the resulting frequency domain representation is scale-independent and numerically stable.\n$A_F = \\frac{2}{L} DCT(A_T)$ (4)\nWhen DCT is applied after DWT instead of directly on the original time series, the process offers distinct advantages. The approximation coefficients from DWT provide a smoothed, noise-reduced representation of the data. This preprocessing ensures that the DCT operates on a cleaner input, avoiding distortions caused by high-frequency noise in the original series. Additionally, reducing the series length decreases the computational complexity of the DCT operation and focuses the analysis on the most informative components of the signal, ensuring that critical long-term periodic features are emphasized.\nThis combination is advantageous in real-world forecasting as it balances noise robustness, computational efficiency, and the extraction of long-term dependencies and periodic structures."}, {"title": "Low Rank Layer", "content": "Most models in LTSF employ standard linear layers for the final prediction; however, using a low-rank layer offers several advantages. Low-rank layers have a significantly reduced memory footprint during training and inference. Moreover, a reduced memory footprint directly leads to fewer operations required to train and evaluate the network.\nThere are different approaches to defining a low-rank layer. Perhaps the most common strategy for defining a low-rank layer is selecting two matrices: $P \\in \\mathbb{R}^{\\frac{L}{2} \\times r}$ and $Q \\in \\mathbb{R}^{r \\times H}$, where r represents the rank of the layer $(r \\ll min(\\frac{L}{2}, H))$ [22]. These matrices jointly form a low-rank approximation of the weight matrix $W \\approx PQ$, which significantly reduces the number of trainable parameters. In addition, a bias vector $B \\in \\mathbb{R}^{H}$ is included to account for linear shifts. The low-rank layer then takes the form:\n$\\hat{Y}^T = A_FPQ + B$. (5)\nStandard gradient-based optimization methods are used to train P, Q, and B. The corresponding gradient flow for a loss function $L(PQ)$ (where we omit dependence on the bias and $A_F$ for ease of presentation) then reads\n$\\begin{aligned} & P(t) = - \\nabla_P L(P(t)Q(t)), \\\\ & Q(t) = - \\nabla_Q L(P(t)Q(t)) .\\end{aligned}$ (6a) (6b)\nHere, $\\nabla_{P,Q}$ denotes the gradients with respect to the respective low-rank factors. If the method converges to a stationary point of (6) at time t* and $W(t^*) := P(t^*)Q(t^*)$ satisfies the gradient flow equation $\\dot{W} = -\\nabla_W L(W)$ at time t*, then $\\nabla_W L(W(t^*)) = 0$ since\n$\\begin{aligned} \\nabla_W L(W(t^*)) & = -\\dot{W}(t^*) \\\\ & =  -P(t^*)Q(t^*) - P(t^*)Q(t^*) = 0. \\end{aligned}$\nIn this case, the stationary point fulfills the necessary condition of a local full-rank optimum. A primary advantage of low-rank layers is their computational efficiency, which results from the fewer operations required to compute (5). This lower computational cost in the forward pass directly translates into fewer evaluations in the backward pass, leading to shorter training time. In addition to computational efficiency, low-rank networks have been observed to outperform their dense counterparts [30, 31]. This observation is not yet rigorously understood, but it is assumed that the regularization imposed by low-rank compression improves the generalization of the resulting networks. In Section 6.2, we study the regularization effects of low-rank networks and show that similar behavior can be observed in our experiments."}, {"title": "Key Features", "content": "A distinctive feature of our framework is the omission of the inverse transformation from the frequency domain to the time domain. This simplification is justified by Parseval's theorem, as shown in Equation (7), which ensures energy conservation between the time and frequency domains [15].\n$\\sum_{n=0}^{N-1} (A_T)_n^2 = \\sum_{n=0}^{N-1} (A_F)_n^2$ (7)\nAs a result, the model operates directly in the frequency domain, capturing necessary relationships without the added complexity of domain conversion. This design choice enhances computational efficiency and interpretability while preserving the fidelity of the original time series.\nAdditionally, the framework employs a single generalized low-rank layer instead of separate layers for each feature or channel. This design reduces parameter redundancy, mitigates overfitting to specific channels, and enhances overall model efficiency. With the low-rank layer as the only trainable component, the model achieves a compact architecture that balances simplicity with effective noise mitigation. This streamlined approach makes the HADL framework particularly suitable for LTSF tasks, offering both accuracy and computational efficiency.\nTo further mitigate overfitting and reduce noise influence, $L_1$ regularization is incorporated. This regularization shrinks the coefficients of less important features, contributing to a more robust and generalizable model."}, {"title": "Experiments", "content": "We conduct a standard LTSF test for prediction lengths $H = \\{96, 192, 336, 720\\}$, using a fixed lookback window of $L = 512$ for all baseline models."}, {"title": "Ablation Studies and Analysis", "content": "We conduct a series of experiments on ETTh1 and ETTh2 datasets to validate the effects of various components of the HADL framework. We use the rank = 40 for the low rank layer as part of the test, unless otherwise specified."}, {"title": "Effects of Haar Decomposition", "content": "We evaluate the significance of Haar decomposition by comparing the performance of our model with (w/) and without (w/o) this operation, as shown in Table 4.\nThe results indicate that models using Haar decomposition achieve similar or better accuracy compared to the without Haar setup while benefiting from reduced computational complexity and a 20-40% reduction in parameter size. This shows that Haar decomposition effectively compresses the input sequence by discarding noisy components, thereby maintaining or enhancing predictive performance."}, {"title": "Effects of Low-Rank Layer", "content": "In this experiment, we evaluate the impact of replacing the low-rank layer with a standard linear layer. As shown in Table 5, the low-rank layer consistently outperforms the standard linear layer, achieving lower MSE while significantly reducing the parameter count, leading to enhanced computational efficiency. The improvement in MSE stems from the low-rank layer's ability to focus on a smaller, more relevant subset of weights, thus minimizing the influence of noisy components in the time series."}, {"title": "Effects of Discrete Cosine Transform", "content": "In this experiment, we evaluate the impact of bypassing inversion from the frequency domain to the time domain during prediction, as discussed in Section 3.4. Figure 2 presents heat maps of low-rank matrix plots for $L = 512$ and $L = 96$ in the HADL framework with (w/) and without (w/o) DCT, using the ETTh1 dataset. The resulting MSE values are 0.362 (w/ DCT) and 0.371 (w/o DCT), indicating a slight improvement with DCT.\nThe plots reveal that incorporating DCT enhances pattern clarity, distinguishing key structures more effectively than the model without DCT. While the DCT-enabled model tends to assign more positive weights, the non-DCT version exhibits more negative weights. Therefore, both approaches successfully identify and emphasize the critical components of the time series. This suggests that the low-rank layer can effectively learn essential features without requiring explicit inversion from the frequency domain to the time domain, as it inherently captures the extra weights typically introduced during inversion.\nFurther MSE evaluations across various prediction lengths show consistent improvements or comparable performance when using DCT, reinforcing its effectiveness in the HADL framework (Appendix C.3)."}, {"title": "Discussion and Conclusion", "content": "In this work, we introduced HADL, a novel forecasting framework combining DWT and DCT for noise reduction and robust feature extraction, alongside a lightweight low-rank linear prediction layer, resulting in an efficient, noise-resilient, and accurate model for long-term multivariate time series forecasting.\nHowever, there are several limitations that present opportunities for future improvement. One significant limitation of the HADL framework is its sensitivity to the size of the lookback window. While the model performs well with medium to large lookback windows, its accuracy degrades when the window is too small, primarily due to the reduced amount of historical information available for prediction. Conversely, when the lookback window is extended to ultra-long ranges, the model could potentially benefit from incorporating multiple levels of Haar decomposition. However, this introduces challenges, such as an increase in the number of trainable parameters. Additionally, the current strategy of discarding detail coefficients at each decomposition level must be revisited, as this may lead to the loss of important fine-grained information.\nDespite these limitations, HADL outperforms ultra-lightweight models and remains highly efficient compared to transformer-based and MLP models, all while maintaining impressive robustness to noise and providing stable per-formance across varying noise levels. In conclusion, HADL framework offers a strong balance between forecasting"}]}