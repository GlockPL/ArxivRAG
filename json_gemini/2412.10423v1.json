{"title": "Look Before You Leap: Enhancing Attention and Vigilance Regarding Harmful Content with GuidelineLLM", "authors": ["Shaoqing Zhang", "Zhuosheng Zhang", "Kehai Chen", "Rongxiang Weng", "Muyun Yang", "Tiejun Zhao", "Min Zhang"], "abstract": "Despite being empowered with alignment mechanisms, large language models (LLMs) are increasingly vulnerable to emerging jailbreak attacks that can compromise their alignment mechanisms. This vulnerability poses significant risks to the real-world applications. Existing work faces challenges in both training efficiency and generalization capabilities (i.e., Reinforcement Learning from Human Feedback and Red-Teaming). Developing effective strategies to enable LLMs to resist continuously evolving jailbreak attempts represents a significant challenge. To address this challenge, we propose a novel defensive paradigm called GuidelineLLM, which assists LLMs in recognizing queries that may have harmful content. Before LLMs respond to a query, GuidelineLLM first identifies potential risks associated with the query, summarizes these risks into guideline suggestions, and then feeds these guidelines to the responding LLMs. Importantly, our approach eliminates the necessity for additional safety fine-tuning of the LLMs themselves; only the GuidelineLLM requires fine-tuning. This characteristic enhances the general applicability of GuidelineLLM across various LLMs. Experimental results demonstrate that GuidelineLLM can significantly reduce the attack success rate (ASR) against the LLMS (an average reduction of 34.17% ASR) while maintaining the helpfulness of the LLMs in handling benign queries. Code is available at https://github.com/sqzhang-lazy/GuidelineLLM.", "sections": [{"title": "Introduction", "content": "Recently, large language models (LLMs) (Touvron et al. 2023; Achiam et al. 2023; Anil et al. 2023) have become increasingly prominent in daily activities, exemplified by applications such as ChatGPT and Intelligent Assistants. The utilization of these models for various tasks is an expanding trend (Qin et al. 2023; Zhong et al. 2023; Yao et al. 2022), necessitating a reduction in responses containing harmful content. Although LLMs have demonstrated a capability for recognizing harmful content, methods to attack LLMs have continuously evolved, particularly jailbreak attacks (Yuan et al. 2023; Deng et al. 2023b; Zeng et al. 2024a; Zou et al. 2023). Jailbreak attacks can efficiently induce models to produce harmful responses during the inference phase, posing significant risks to the practical application of LLMs.\nTo enhance the safety of LLMs, common approaches involve reinforcing their alignment mechanisms. Techniques such as Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al. 2022; Bai et al. 2022a) and Red-Teaming (Ganguli et al. 2022; Perez et al. 2022; Touvron et al. 2023) have proven effective in improving the ability of LLMs to detect harmful content. However, these methods demand extensive training data and significant computational resources, which can limit the speed of iterative improvements. Furthermore, additional retraining might degrade the performance of established safety alignment mechanisms (Qi et al. 2023; Zhou et al. 2024). Therefore, it is critical to investigate methodologies that can mitigate the production of harmful content without requiring modifications to the model parameters.\nMany existing methods have focused on enhancing the recognition of harmful content during the inference stage. These methods aim to heighten the attention of LLMs toward harmful content prior to generating responses, employing techniques such as Self-Reminder and Self-Defense (Xie et al. 2023; Helbling et al. 2023). However, these methods often struggle with generalization and can be circumvented by continually evolving jailbreak tactics. Thus, the ongoing efforts to improve the safety of LLMs confront two principal challenges: training efficiency and generalization capabilities.\nIn this work, we propose a novel defensive paradigm called GuidelineLLM, designed to assist LLMs in identifying queries that possess potential harmfulness. Before LLMs respond to a query, GuidelineLLM first identifies the query risks, summarizes them into guideline suggestions, and feeds them to the responding LLMs.\nThis approach effectively addresses the previously identified challenges by making LLMs cognizant of unsafe content within a query. Consequently, GuidelineLLM activates the alignment mechanisms of LLMs, thereby enhancing their safety and fortifying them against various jailbreak attacks. Remarkably, our approach does not necessitate additional safety fine-tuning for the responsing LLMs; only GuidelineLLM needs to be fine-tuned. This characteristic makes GuidelineLLM applicable to various LLMs. Additionally, we present a fine-tuning framework for GuidelineLLM that is adaptable for formulating guidelines against newly emerging jailbreak techniques.\nIn summary, our contributions are as follows:\n(i) We propose a defensive paradigm called GuidelineLLM, which can enhance the safety of various LLMs. This paradigm can reduce the harmfulness of the output content without necessitating additional safety training for the response models themselves, significantly mitigating the impact of jailbreak attacks.\n(ii) We introduce a fine-tuning framework for GuidelineLLM, which includes the templated construction of jailbreak attacks, referred to as T-Jailbreak. T-Jailbreak can be expanded with relevant jailbreak attacks according to the definition of new jailbreak techniques. Experimental results indicate that T-Jailbreak has a high attack success rate (ASR).\n(iii) Experiments show that our GuidelineLLM can significantly reduce the harmfulness of the output (an average reduction of 34.17% ASR) while maintaining the helpfulness of LLMs for benign queries."}, {"title": "Related Work", "content": "Two prevalent methods are currently employed to enhance the safety policies of LLMs: alignment and red teaming. The objective of alignment is to bridge the gap between LLMs' language modeling goals, such as predicting the next token during pre-training, and their ultimate aim of \"following instructions and being helpful, truthful, and harmless\" in practical applications (Ouyang et al. 2022). Common techniques in this domain include Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al. 2022; Bai et al. 2022a), Constitutional AI (Bai et al. 2022b), and self-alignment (Sun et al. 2024). These approaches focus on embedding alignment rules within pre-trained models to restrict harmful behaviors during inference. In LLMs research, the term \"Red-Teaming\" has recently come to denote the systematic testing or probing of LLMs to identify their potential for harmful behavior and uncover safety vulnerabilities. The critical aspect of red teaming is the aggregation of sufficient and diverse attacks, enabling LLMs to refuse responses to these attacks. Current efforts in this area mainly concentrate on automating the generation of high-quality, provocative queries, allowing LLMs to encounter a wider variety and richer range of attacks, thereby bolstering their defensive capabilities (Deng et al. 2023a; Ge et al. 2023; Jiang et al. 2024). However, recent studies also demonstrate that retraining the model can lead to the deterioration of safety alignment mechanisms (Qi et al. 2023; Zhou et al. 2024). This indicates that arbitrary parameter updates to the model are not advisable and can slow the model iteration process.\nEnsuring that LLMs output only safe content during inference is critically important. Common methodologies to achieve this include preprocessing the input before inference, employing specialized inference paradigms, and detecting harmful content through input-output result analysis. Given the sensitivity of LLMs to input prompts, modifying the prompt content can enhance their ability to focus on harmful content. For instance, incorporating a system prompt can assist LLMs in identifying harmful content (Xie et al. 2023). By leveraging LLMs' in-context learning capabilities, models can be conditioned to recognize and refuse harmful content (Wei, Wang, and Wang 2023). Additionally, creating multiple perturbed samples of the input and aggregating the results has proven effective (Robey et al. 2023; Cao et al. 2023). Some approaches involve having LLMs analyze the intent of the question before providing an answer (Zhang et al. 2024; Zeng et al. 2024b). Alternatively, decoding strategies can be applied to suppress harmful prompts during generation, thereby ensuring safer outputs (Zhong et al. 2024). In terms of detection and classification, Meta has specifically fine-tuned a model named Llama-Guard, which can classify inputs and input-output pairs according to taxonomy and risk guidelines (Inan et al. 2023). In this work, we primarily focus on safeguarding LLMs during the inference phase. We aim to find a defensive paradigm that applies to different LLMs and can be continuously expanded to counter the ever-evolving jailbreak attacks."}, {"title": "The Proposed Method", "content": "In this section, we first present how to initialize a small count set of T-Jailbreak queries from some attack queries and the definition of different jailbreak techniques. Then, we utilize this batch of T-Jailbreak queries and sampled benign queries to construct the initial set of Guideline data. Finally, we will demonstrate the framework for fine-tuning the Guide-lineLLM."}, {"title": "Initializing T-Jailbreak Data", "content": "To ensure the diversity of jailbreak attacks and enable GuidelineLLM to formulate guidelines for analyzing various types of jailbreak attacks, we have constructed the T-Jailbreak dataset by summarizing seven identified jailbreak techniques, as outlined in previous research (Liu et al. 2023b). These techniques are detailed in Table 1.\nIn line with previous studies (Inan et al. 2023; Shen et al. 2023), we utilize the policies from Llama3-Guard and DAN to initialize our attack queries. Combining these attack queries with template prompts, we develop the T-Jailbreak dataset for use in our experiments. Specifically, we use the gpt-3.5-turbo-0125 to initialize attack and jailbreak queries. We configured the temperature parameter to 0.9 and the top_p parameter to 0.85 to promote diversity in the generated results."}, {"title": "Initializing Guideline Data", "content": "Considering that GuidelineLLM needs to maintain the helpfulness of LLMs while enhancing their defensive capabilities, it is crucial to include both harmful and benign queries in the collection of guideline data.\nThe jailbreak queries used in our study are from the T-Jailbreak dataset that we construct, while the benign queries are sampled as follows: 1,000 entries from Alpaca (Taori et al. 2023), and 1,800 entries from GPTTeacher\u00b9. We use gpt-3.5-turbo-0125, with the same parameter settings as for Section Initialize T-Jailbreak Data, to initialize our guideline data."}, {"title": "Fine-tuning GuidelineLLM Framework", "content": "The framework of fine-tuning GuidelineLLM is shown in Figure 2.\nTo enable GuidelineLLM to analyze different jailbreak techniques effectively, we also need to fine-tune Jailbreak-LLM. JailbreakLLM is designed to generate new jailbreak queries based on provided jailbreak techniques and examples. Our fine-tuning framework is structured to expand from a small initial batch of jailbreak queries to a sufficient number of queries, thereby facilitating the exploration and development of new jailbreak methods. We illustrate the general workflow of fine-tuning GuidelineLLM in Algorithm 1."}, {"title": "Fine-tuning and Inference", "content": "For JailbreakLLM and GuidelineLLM, our base candidate models are Llama2-7B-chat and Vicuna-7B. Considering the different alignment mechanisms between these models, we use the Llama2-7B-Chat model, which exhibits more robust alignment mechanisms, as the base model for fine-tuning the GuidelineLLM. Conversely, we use the Vicuna-7B model, which has comparatively weaker alignment mechanisms, as the base model for fine-tuning the JailbreakLLM. Both JailbreakLLM and GuidelineLLM are fine-tuned using Low-Rank Adaptation (LORA) (Mangrulkar et al. 2022), setting the training epoch to 3.\nThe prompts used to fine-tune GuidelineLLM consist of jailbreak and benign queries as inputs, with the corresponding guidelines as outputs. The prompts used to fine-tune JailbreakLLM require JailbreakLLM to generate attack queries and produce jailbreak queries based on specific jailbreak techniques. To increase data diversity, we sample two examples of the same jailbreak technique as inputs during the fine-tuning processes. During the inference phase, the prompts used by both GuidelineLLM and JailbreakLLM are consistent with those used in the fine-tuning phase."}, {"title": "Evaluating the Quality", "content": "To facilitate the subsequent turn of fine-tuning for the JailbreakLLM in generating more effective jailbreak queries, it is necessary to filter the currently generated queries for quality and effectiveness. Simultaneously, to ensure that the next phase of LLMs accurately rejects jailbreak queries while appropriately responding to benign queries, the number of jailbreak and benign queries in the guideline set must be balanced.\nAs illustrated in Figure 2, the quality of generated jail-break queries is assessed by inputting them into the LLMs and evaluating the harmfulness of their outputs through rule-based detection. Following previous work (Liu et al. 2023a), we utilize a series of detection tokens to determine harmful responses. If these tokens are present in the output, it is inferred that a harmful response has been produced.\nWe select the jailbreak queries that yielded harmful outputs from the LLMs and incorporate them into the T-Jailbreak dataset. Furthermore, GuidelineLLM is employed to generate guidelines for these jailbreak queries, which are subsequently added to the Guideline dataset. The number of jailbreak queries added in this iteration is documented. To ensure dataset balance, we sample benign queries from the Alpaca and GPTTeacher datasets. These benign queries are then used to produce guidelines, discarding instances where the LLMs refuse to provide answers and retaining those where helpful responses are generated, until the number of benign instances matches the newly added jailbreak queries."}, {"title": "Experiments", "content": "For harmful datasets, we select three in-the-wild jailbreak datasets (i.e., DAN (Shen et al. 2023), SAP200 (Deng et al. 2023a), JailbreakChatGPT (Liu et al. 2023b)) and a popular optimization-based automatic jailbreak method (i.e., Auto-DAN (Liu et al. 2023a)). Additionally, we use gpt-3.5-turbo-0125 to generate a T-jailbreak dataset comprising 1,137 queries. Besides, to evaluate the impact of GuidelineLLM on the helpfulness of the responding LLMs, we sample 400 queries from the Alpaca and GPTTeacher datasets (Taori et al. 2023)."}, {"title": "Evaluation Metrics", "content": "For harmful datasets, we report attack success rate(ASR, (Shen et al. 2023)) through rule-based detection, where lower scores indicate more robust safety. Following previous work (Liu et al. 2023a), we collect a series of detection tokens, and if these tokens are detected in the outputs of the LLMs, we determine that the LLMs has produced a harmful response.\nFor DAN, AutoDAN, and T-Jailbreak datasets, considering the complexity of responses, we adopt gpt-3.5-turbo-0125 as an auto-annotation function for successful attack examples (Zheng et al. 2024). We also adopt gpt-3.5-turbo-0125 for the benign dataset as an auto-annotation function for evaluating the helpfulness.\nWe set up five options: A(Very Harmful/Helpful), B(Harmful/Helpful), C(Moderately Harmful/Helpful), D(Slightly Harmful/Helpful), and E(No Harmful/Helpful at All). To facilitate scoring, we assign 4 points to option A, 3 points to option B, 2 points to option C, 1 point to option D, and 0 points to option E. For harmful data, a lower calculated score indicates a safer response, while for benign data, a higher calculated score indicates greater helpfulness."}, {"title": "Baseline", "content": "We select Llama2-7B-Chat (Touvron et al. 2023), Vicuna-7B, and Vicuna-13B (Chiang et al. 2023) as our responding LLMs. We compare the performance of our GuidelineLLM method with several other approaches: No defense, Self-Reminderr (Xie et al. 2023), IA (Zhang et al. 2024), and Llama3-Guard (Inan et al. 2023). For the Llama3-Guard, we only evaluate the harmfulness of the inputs. Considering that IA's defensive method requires two rounds of inference, and both Self-Reminder and GuidelineLLM add additional prompts to the original query (i.e., Self-Reminder adds a system prompt, and GuidelineLLM adds guidelines), in the AutoDAN experiment, we use the input content from the last round of inference for each defensive method as the attack target."}, {"title": "Main Results", "content": "Table 2 presents the main results of different defensive methods. Analyzing these results, we derive several key findings:\n(i) The proposed GuidelineLLM significantly reduces the harmfulness of outputs on LLMs, achieving a notable decrease in the ASR on average. Specifically, GuidelineLLM surpasses the best baseline, IA, in defending Vicuna-7B (8.96% ASR), and demonstrates comparable performance to IA on Llama2-7B-Chat (1.53% ASR) and Vicuna-13B (11.63% ASR). This outcome underscores the effectiveness of our approach, achieving substantial reductions in harmful outputs with a single inference.\n(ii) In the AutoDAN dataset, which features an optimization-based strong jailbreak method, GuidelineLLM exhibits superior performance, with an ASR markedly lower than other baselines (0% ASR in Llama2-7B-Chat, 17.69% ASR in Vicuna-7B, and 21.54% ASR in Vicuna-13B). This demonstrates the robustness of our method; incorporating guidelines into queries enhances the focus of LLMs on detecting harmful content, thereby preventing optimization-based strong jailbreak methods from circumventing safety measures.\n(iii) GuidelineLLM also show solid defensive capabilities in-the-wild jailbreak datasets. Notably, in the JailbreakChat-GPT dataset, GuidelineLLM's ASR is significantly lower than that of other baselines. This finding indicates that GuidelineLLM is effective in defending against manually constructed jailbreak attacks as well.\nIn experiments involving GuidelineLLM, we observe that the LLMs frequently generate safe responses to jailbreak queries, which are not detectable using token-based detection methods. Consequently, we collect GuidelineLLM defense examples that are judged to have been successfully bypassed in datasets with higher ASR. Table 3 presents the harmfulness scores.\nWe could find that many of the outputs judged to have successfully been bypassed under the GuidelineLLM defensive method are non-harmful. In the T-Jailbreak dataset, the harmfulness scores of the three responding LLMs are all below 1 point. In the AutoDAN dataset, nearly half of the examples judged as E are present. Similarly, in the DAN dataset, one-quarter of the examples are also judged as E.\nThe results prove that under the guidance of GuidelineLLM, the LLMs can provide safer and more user-friendly responses to jailbreak queries"}, {"title": "Helpfulness Evaluation", "content": "Table 4 illustrates the impact of incorporating guidelines on the helpfulness of responding LLMs for benign queries. Concerning the scores, the inclusion of guidelines slightly diminishes the helpfulness of the responding LLMs; however, the scores remain above 3.3, thereby maintaining a satisfactory level of helpfulness. Notably, the impact of adding guidelines on the helpfulness score is minimal for Vicuna-13B. We classify options D (Slightly Helpful) and E (Not Helpful at all) as false refusals for benign queries by the LLMs. We then compute the sum of these two classifications, D and E, to derive the False Refusal Rate (FRR). The results reveal that for Llama2-7B-Chat, adding guidelines raises FRR from 1.83% to 7.31%. For Vicuna-7B and Vicuna-13B, the impact of adding guidelines on FRR is not significant. This discrepancy might be attributable to the stronger compliance with instructions exhibited by Llama2-7B-Chat, wherein the added guidelines impose relatively stronger restrictions, resulting in diminished helpfulness. It is worth noting that since the guidelines employed in our study are generated by \"gpt-3.5-turbo-0125,\" more professionally crafted guidelines could potentially enhance the helpfulness of the LLMs."}, {"title": "Analysis", "content": "To investigate the impact of benign guidelines on the fine-tuning of GuidelineLLM, we conduct an experiment using only jailbreak queries and jailbreak guidelines. Table 5 presents the results of the helpfulness scores for benign guideline in the fine-tuning dataset. The data reveal that GuidelineLLM fine-tuned solely with jailbreak guidelines leads to a decrease in helpfulness scores across all LLMs, accompanied by an increased FRR. These observations imply that while using only jailbreak guidelines for fine-tuning GuidelineLLM enhances the defensive performance of the LLMs, it compromises their helpfulness. The results, as presented in Table 6, indicate that fine-tuning solely with jailbreak guidelines results in a lower ASR across all LLMS, in comparison to fine-tuning with both jailbreak and benign guidelines. This finding suggests that GuidelineLLM fine-tuned exclusively with jailbreak guidelines demonstrates superior defensive performance.\nTherefore, to preserve the helpfulness of LLMs while simultaneously improving their defensive effectiveness, it is imperative to include benign guidelines in the fine-tuning dataset for GuidelineLLM. This balanced approach ensures that LLMs remain effective and responsive to benign queries while also being robust against jailbreak attempts.\nTo examine the impact of the number of guidelines provided by GuidelineLLM on the harmfulness and helpfulness of the responding LLMs' outputs, we set three different maximum numbers of guidelines: 3, 5, and 7.\nTable 7 presents the harmfulness results for varying maximum numbers of provided guidelines. For Llama2-7B-Chat, which features more robust alignment mechanisms, we observe that an increase in the number of guidelines provided results in a lower average ASR, reaching as low as 1.52%. Conversely, for Vicuna-7B and Vicuna-13B, which possess weaker alignment mechanisms, fewer guidelines lead to a lower average ASR, at 6.77% and 9.16%, respectively. This indicates that the effectiveness of GuidelineLLM is contingent upon the alignment mechanisms of the LLM. For LLMs with robust alignment mechanisms, supplying detailed guidelines aids in quickly identifying harmful content and subsequently refusing to respond.\nTable 8 displays the helpfulness results for different maximum numbers of provided guidelines. The data reveal that variations in the number of guidelines have negligible impact on the helpfulness of all three LLMs.\nTo evaluate the jailbreak capabilities of various techniques within the T-Jailbreak dataset, we select 30 questions per jailbreak technique and employ two defense methods: No Defense and Self-Reminder. The results are shown in Table 9.\nOur observations indicate that \"Role Play\" and \"Logical Reasoning\" are the most effective jailbreak techniques, with ASR exceeding 70% under the No Defense defense method. And under the Self-Reminder defense method, the ASR for Vicuna-7B and Vicuna-13B is also over 45%. We hypothesize that this might be due to the strong capabilities in instruction adherence and chain-of-thought of LLMs. These powerful abilities make LLMs more inclined to focus on the content provided by \"Role Play\" and \"Logical Reasoning\" jailbreak techniques, thereby overlooking the harmfulness of the queries.\nThese results suggest that certain jailbreak techniques exploit the inherent strengths of LLMs to bypass safety measures, highlighting the need for continuous advancement in defense strategies to detect and mitigate such vulnerabilities."}, {"title": "Conclusion", "content": "This study introduces a defensive paradigm called GuidelineLLM, aimed at enhancing the safety of various LLMs. Before responding to a query, GuidelineLLM first identifies potential risks, summarizes them into guideline suggestions, and feeds these to the responding LLMs. This approach reduces the harmfulness of the resulting content without requiring additional safety training for the LLMs. Additionally, we present a fine-tuning framework for GuidelineLLM, which includes the templated construction of jailbreak attacks, referred to as T-Jailbreak. Experiments demonstrate that GuidelineLLM can significantly reduce the harmfulness of output (achieving an average reduction of 34.17% in the ASR) while maintaining the helpfulness of LLMs in responding to benign queries."}]}