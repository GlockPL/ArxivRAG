{"title": "IIMedGPT: Promoting Large Language Model Capabilities of Medical Tasks by Efficient Human Preference Alignment", "authors": ["Yiming Zhang", "Zheng Chang", "Wentao Cai", "Mengxing Ren", "Kang Yuan", "Yining Sun", "Zenghui Ding"], "abstract": "Recent researches of large language models(LLM), which is pre-trained on massive general-purpose corpora, have achieved breakthroughs in responding human queries. However, these methods face challenges including limited data insufficiency to support extensive pre-training and can not align responses with users' instructions. To address these issues, we introduce a medical instruction dataset, CMedINS, containing six medical instructions derived from actual medical tasks, which effectively fine-tunes LLM in conjunction with other data. Subsequently, We launch our medical model, IIMedGPT, employing an efficient preference alignment method, Direct preference Optimization(DPO). The results show that our final model outperforms existing medical models in medical dialogue.Datsets, Code and model checkpoints will be released upon acceptance.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in Large Language Models (LLMs) are significant, as evidenced by the development of ChatGPT[1] and GPT-4 [2]. The models under examination demonstrate a remarkable capacity to comprehend and engage with a diverse range of questions, often surpassing human performance in numerous areas of general knowledge. Although these models are not open-source, the open-source community has swiftly developed high-performing alternatives such as LLaMA [3], Bloom [4], and Falcon [5]. To enhance the Chinese language processing capabilities of these models, researchers develop more advanced, Chinese-specific models[6] for the open-source community, such as Qwen[7] and Baichuan[8]. Despite their overall proficiency in a wide range of tasks, these universal language models often struggle to perform effectively in specialized professional fields like the biomedical sector. This is primarily due to they lack of specialized knowledge. [9]. The biomedical field, with its intricate and comprehensive knowledge requirements, necessitates a high degree of precision and safety for the successful implementation of medical language models [10]. Although there are challenges, LLMs hold significant potential for applications in diagnostic support, patient consultations, and drug recommendations. In the realm of traditional Chinese medicine, several medical language models are proposed.[11, 12, 13, 14]."}, {"title": "2. Related Works", "content": "Research by [15] and [16] show that the majority of an LLM's knowledge is acquired during the pre-training stage, which is essential for establishing a foundational understanding of various domains. Additionally, the current pre-trained base model utilizes a substantial amount of textual knowledge data. However, in speciallized field such as the Chinese medicine, the available pre-training datasets are insufficient to meet the scale required by pre-training models, often resualtting in catastrophic forgetting issues during the training process [17, 18]. Therefore, our training objective should pivot towards effectively adjusting the model using SFT, enabling it to answer relevant medical field questions. However, heavy dependence on SFT can cause models to make overconfident generalizations, essentially memorizing responses without truly grasping and reasoning through the underlying knowledge [19, 17]. Furthermore, training datasets used in previous models are mainly composed of single-turn dialogues, which do not account for the dynamics of real doctor-patient conversations that typically involve multiple exchanges. These conversations are often led by doctors who ask a series of questions to thoroughly comprehend a patient's condition. Reinforcement Learning from Human Feedback (RLHF) is identified as effective method to help models recognize the limits of their capabilities and improve their ability to follow instructions after SFT. [20, 21, 22]. [14] introduce their Chinese medical multi-turn dialogue model which implements the pipeline from pre-train, supervised fine-tuning and RLHF, achieving the state-of-the-art. However, this RLHF approach involves two stages of training, which requires significant computational and annotation resources, specically through reward model training and proximal policy optimization(PPO)[23].\nTherefore, we propose a two-stage training approach for developing the Chinese medical language model, IIMedGPT. This robust model is trained by two stages: supervised fine-tuning and direct policy optimization(DPO)[24]. By collaborating with professional physicians, we gather data from authentic medical scenarios. Subsequently, we redefine these common tasks to construct an instruction-answer dataset, CMedINS."}, {"title": "2.1. Large Language Models", "content": "The significant advancements in the domain of Large Language Models (LLMs), hignlighted by models such as ChatGPT [1] and its successor GPT-4[2], has garnered significant attention, propelling a novel surge in artificial intelligence research and development. Despite OpenAI's reticence in revealing the intricacies of their training methodologies and the specific parameters of their models, the rapid proliferation of open-source LLMs significantly enriches academic research on LLMs, including the series of LLaMA [3, 20], Bloom[4], and Falcon [5]. Furthermore, Ziya-LLaMA [25] completes the RLHF process, significantly bolstering its capacity to follow instructions and operate within safe parameters. Simultaneously, notable efforts to construct Chinese LLMs from scratch, as evidenced by the work of [26] and [27], represent a pivotal stride towards achieving proficiency in Chinese language processing within the field of LLMs."}, {"title": "2.2. Medical LLMs", "content": "In the domain of healthcare, large-scale models often exhibit sub-optimal performance when confronted with the complex requirements of medical knowledge and the need for precision. To address these shortcomings, initiatives such as MedAlpaca [28] and ChatDoctor [29] leverage incremental training to improve their capabilities. Similarly, Med-PaLM [10] develops positive evaluations from medical professionals to assess its clinical response accuracy. Within the Chinese medical sector, research efforts focus on models such as DoctorGLM [11], combining a comprehensive Chinese medical dialogue dataset with an external medical knowledge base. At the same time, BenTsao [12] is designed, relying exclusively on a medical knowledge graph to facilitate dialogue generation. Further advancing the field, Zhang [13] introduced HuatuoGPT, a model was trained on a dataset containing 25 million dialogues. This model enhances response quality by using a hybrid approach that combines distilled data with genuine interactions for SFT and utilizes ChatGPT for RLHF to improve feedback ranking mechanisms. Yang [14] introduces the first medical Chinese LLM that completes the RLHF process."}, {"title": "3. Approach", "content": "This section introduces the methods for constructing our IIMedGPT(as shown in Fig1). Qwen are collections of Chinese and English open-source pretrain models with parameters ranging from 7 billion to 72 billion, and the performance of Qwen on evaluation benchmark is relatively advanced among similar parameters models. Thus we choose the Qwen-14B-base model for our experiments."}, {"title": "3.1. Constrction of Training Dataset", "content": "Engaging the model in a wide range of tasks can enhance its capacity for zero-shot generalization[30]. Therefore, We construct a diverse training set to fine-tune our model including medical dialogues, medical instruction dataset, and general ability dataset."}, {"title": "3.1.1. Medical instruction Dataset", "content": "In the medical domain, we construct a medical instruction dataset, comprising Q&A pairs and their corresponding medical instructions. When building a dataset, relying solely on a single instruction dataset from a related field can cause the model to lose its generalization performance. [16] As a result, we concentrate on creating a multi-instruction medical information processing dataset. We collect this information with authorization from both patients and hospitals. The foundation for data screening is the completeness of medical and patient treatment records generated by doctors from various departments during the patient consultation process at collaborating hospitals. We complete medical records de-identification by removing patient personal identifiers such as ID number, name, and date of birth, and passed the ethical review within the hospital. We remain in close communication with professional doctors to ensure the accuracy of medical records. At last, we use the format of instruction-query-answer (example in Fig2) to build the instruction dataset based on hospital medical records. Following the data screening process, we successfully compile the Chinese medical multi-task dataset, CMedINS, which includes approximately 220,000 instruction-answer pairs from real data across various medical departments. Fig3 illustrates the distribution of medical departments within the dataset, featuring six forms of medical instruction-query-answer pairs and covering more than 10 medical Q&A scenarios. We apply stringent de-identification procedures to all data to protect patient privacy."}, {"title": "3.1.2. Dialogue Dataset", "content": "One of the most notable attributes of large language models is their proficiency in conversing with humans following dialogue training. This conversational capacity also serves as the primary mechanism through which these models receive and execute instructions[31]. Therefore, we integrate a selection of open-source medical dialogue datasets into our training dataset to maintain the conversational proficiency of our large language model. We integrate the CMtMedQA multi-turn dialogue dataset[14] and the ChatMed\u00b9 single-turn dialogue dataset to ensure the model's conversational capabilities. The final mix ratio of single-turn to multi-turn dialogue data is 1:1."}, {"title": "3.1.3. General Instruction", "content": "To mitigate the phenomenon of catastrophic forgetting of previously learned general dialogue skills after SFT [32], We select a portion of general domain data that can help enhance the model's inference capabilities, such as CoT, Code, Wiki and other related medical knowledge. This strategy serves dual purposes: it not only reduces likelihood of forgetting general dialogue skills but also improve the model's expertise in the medical domain."}, {"title": "3.2. Directed Preference Optimization", "content": "Due to the complexity and instability of reinforcement learning, we consider adopting a new method of DPO[24]. This approach transforms the objectives of reinforcement learning tasks into a classification problem to precisely optimize the reward maximization issue. It directly aligns with human preference datasets and is simpler and more efficient than existing methods [24]. Assume the objectives of RLHF is to maximize the function:"}, {"title": "3.2.1. Learning Objectives", "content": "Due to the complexity and instability of reinforcement learning, we consider adopting a new method of DPO[24].\nThis approach transforms the objectives of reinforcement learning tasks into a classification problem to precisely\noptimize the reward maximization issue. It directly aligns with human preference datasets and is simpler and more\nefficient than existing methods [24]. Assume the objectives of RLHF is to maximize the function:\n$\\max_{\\pi_{\\theta}} E_{x,y}[r(x, y)] \u2013 \\beta D_{KL}[\\pi_{\\theta}(y|x)||\\pi_{ref}(y|x)]$\nwhere $r_{\\theta}(x, y)$ denotes that reward function, $\\beta$ is the hyperparameter,$x \\sim D$ and $y \\sim \\pi_{\\theta}(y|x)$, $\\pi_{ref}(y|x)$ and $ref(y|x)$ represent the current policy model and reference policy model. Assuming a static dataset of comparison D = {x,y,y1, the objective of the former is to maximize the reward of the answers generated from any prompt, whereas the latter aims to minimize the KL divergence between the training policy and the original policy to prevent excessive divergence, which could lead to non-convergence. We can assume the optimal policy $\\pi_*$, under the optimal reward function r, The objective of Eq.1 is to obtain the optimal policy, thus it is equivalent to minimizing the KL divergence with $\\pi_*$. Then, we derived the reward function as:\nr(x, y) = \\beta \\log{\\frac{\\pi_*(y|x)}{\\pi_{ref}(y|x)}} + \\beta \\log Z(x)\nwhere the rewards of input x and output y can be expressed by $\\pi_*$. Z(x) represents the partition function. Since We cannot determine the optimal reward function r* and policy $\\pi_*$, we choose the $\\pi_{\\theta}$ to represent $\\pi_*$, allowing us to rewritten the formula\nr_{\\theta}(x, y) = \\beta \\log{\\frac{\\pi_{\\theta}(y|x)}{A_{ref} (y/x)}} + \\beta \\log Z(x)\nThen, a training objective is constructed, which maximizes the difference in rewards between preferred and non-preferred answers (with Z(x) being canceled out)."}, {"title": "3.2.2. Human Preference Dataset", "content": "We develop comprehensive annotation guidelines, drawing inspiration from [13] and [14]. These guidelines encompass \"SPF\" dimensions: safety, professionalism, and Fluency(detail in Table1). Annotators evaluate model-generated dialogues based on these dimensions in descending priority. The annotated dataset consists of 10,000 random samples from the training set, augmented with an additional 5000 out-of-training-set preference data, designed to train the model to handle both in-distribution and out-of-distribution scenarios. For a consistent and coherent evaluation, we break down each dialogue into individual parts and annotate separately. We develop a specialized platform to streamline the annotation process, which is conducted by medical postgraduates or clinical doctors. To ensure standardization, we use cross-annotation, and a medical expert resolves any discrepancies between annotators."}, {"title": "4. Experiments and Evaluation", "content": "After extensive training and optimization, we evaluate the performance of our model, utilizing GPT-4 or human\nexperts, across three capability dimensions and nine specific competencies. The experimental outcomes indicate that\nour model surpasses other open-source Traditional Chinese medical LLMs across all dimensions, despite possessing\nless training data than the previously best-performing model. The instructional dataset we construct significantly\nenhances the model's proficiency in processing medical directives and dialogues. The main contributions of this paper"}, {"title": "4.1. Training Details", "content": "In this study, we utilize the Qwen-14B model\u00b9 as the foundational architecture for the development of IIMedGPT, a novel bilingual language model. The Qwen-14B model has undergone extensive pretraining on a corpus comprising over 3 trillion tokens, encompassing a diverse array of multilingual data across various domains. The training pipeline is executed on a node equipped with 4 A100-80G GPUs, employing parallelization techniques. We adopt the low-rank adaptation (Lora) parameter-efficient tuning method [33].These procedures are facilitated by the utilization of the transformers\u00b2 and peft\u00b3 software libraries. In an effort to optimize the balance between computational resources and training effectiveness, we engage bf16 precision within the accelerate framework, implement a gradient accumulation strategy, and impose a constraint on the length of single responses (inclusive of historical context) to 4096 tokens. The optimization process is governed by the AdamW optimizer [34], incorporating a dropout rate of 0.1 and a cosine"}, {"title": "4.2. Model Baseline", "content": "For thoroughly evaluate our model, we have chosen a range of large language models (LLMs) with varying parameter sizes to serve as benchmarks.\n\u2022 BenTsao [12] The first large-scale Chinese medical mode, fine-tuned on an large scale medical dialogue dataset generated from ChatGPT based on a medical knowledge graph.\n\u2022 Qwen14B-Chat [7] The 14B-parameter version of the large language model series Qwen, which is pretrained on a large volume of data, including web texts, books, codes, etc. Additionally, the pretrained Qwen-14B is enhanced with alignment techniques to function as a general AI chat assistant.\n\u2022 DoctorGLM [11]A medical model based on the ChatGLM-6B, is developed by fine-tuning on a series of Chinese medical text and dialogues.\n\u2022 HuatuoGPT [13] HuatuoGPT model is trained on a combination of real-world data and data distilled from ChatGPT. This approach utilizes the RLMF method, which integrates ChatGPT and human preferences, to maximize the benefits of mixed data.\n\u2022 Zhongjing [14] large-scale medical model is trained by Chinese general model\u2014Ziya LLaMA[25]. It undergoes three stages of training: continual pretraining, supervised fine-tuning, and RLHF.\n\u2022 ChatGPT[1] A language model developed by OpenAI, garnered significant attention and currently maintains a high standard among its peers."}, {"title": "4.3. Evaluation Benchmarks", "content": "Due to the lack of a unified medical benchmark evaluation standard at present, we have used the evaluation dataset of Huatuo26M [35] and CMtMedQA[14]. Huatuo26M-test is a single-turn medical question answering dataset with 6000 QA pairs. To assess the multi-turn conversation capability, we also adopt the test dataset from CMtMedQA, which is not exposed to the model during the training process. It contains additional 1000 unseen dialogues."}, {"title": "4.4. Evaluation Metrics", "content": "As a comprehensive model tailored for the medical field, it is imperative that it enhances its medical capabilities without compromising its general abilities. To achieve this, we conduct comprehensive tests on benchmarks in both the general and medical domains, measuring performance based on accuracy across these benchmarks. More importantly, we compare our model against other medical models in two dimensions: AI-based assessment and expert-based evaluations. We carry out comparative experiments using three criteria: safety, proficiency, and Fluency(Detail in Table1). Given the complexity of assessing medical safety, we have enlisted the assistance of professional doctors for these evaluations. The assessments by these professional doctors can meet multiple requirements, including safety, accuracy, and ethics. For the assessment of professionalism and Fluency, we utilize an AI-based evaluation with GPT4. To this end, we have developed relevant prompts to facilitate this evaluation process."}, {"title": "5. Result", "content": "Our results show that our model surpasses all existing medical models in medical dialogue capability, achieving state-of-the-art outcomes. It also performs as well as the original model across various benchmarks for general and medical issues. This achievement is made possible by significantly enhancing medical dialogue capability without losing general dialogue and basic knowledge reserve capabilities.\nSOTA on medical large language models We conduct various comparisons with other medical large models in the field, as shown in the Fig4, demonstrating our model's excellent ability in medical dialogue and instruction compliance. Compared to the Zhongjing model, our model, using fewer data resources, achieves better results, proving the superiority of our method and dataset.\nLow resources but high performance We construct a high-quality, multi-instruction medical dataset and a human preference dataset, supplemented by a cleaned-up open-source dataset, with a total size of only 1GB. Effective alignment of human preferences within the domain can be achieved with only two stages of training. This saves human labeling costs and computational resources compared to models that rely on RLHF methods.\nDPO, more efficient method on human preference alignment The DPO method, compared to the traditional PPO method, eliminates the step of retraining the reward model. Instead, it aligns the output strategy of the model directly by utilizing the human preference dataset. In our experiments, based on the preference dataset annotated with the Base model, DPO has a significant effect on model preference alignment. This can provide a reference for future model applications in other fields."}, {"title": "6. Ablation Study", "content": "To deeply understand the contribution of DPO to medical LLM performance, we conduct a series of comprehensive experiments with the test dataset. We employ the same evaluation methodology as in our previous study, comparing the performance of IIMedGPT before and after the DPO stage. In addition to evaluating the three primary capabilities, we also pay particular attention to the alteration in model response length. As illustrated in the Fig5, the results of the ablation experiment suggest that the model experiences varying degrees of improvement across all capabilities."}, {"title": "7. Conclusion", "content": "In this paper, We introduce IIMedGPT, a Chinese medical large language model aligned with human preferences through DPO, surpassing current open-source models with same parameters and within the same field. With less resources consumed compared to previous models, we achieve better results. We construct a large-scale medical instruction dataset CMedINS including multiple medical tasks abstracted from real medical scenarios."}, {"title": "8. Limitation", "content": "Despite these achievements, IIMedGPT does not guarantee the accuracy of all responses due to the occurrence of hallucinations. Considering the potentially severe consequences of misleading information in the medical field, we advise users to interpret the generated information with caution and consult healthcare professionals. IIMedGPT currently processes only textual information and can not process medical multimodal information, such as medical images or physiological signals."}, {"title": "Appendix A. Conversation Cases", "content": "The Chinese answers from the five baseline models are listed in FigA.6, while the English version can be found in FigA.7. The efficacy of our model in addressing medical queries is evident from this example. It not only accurately identifies potential causes, but also provides specific recommendations."}, {"title": "Appendix B. Evalution Prompt", "content": "The prompt in Table B.2 is utilized to instruct GPT-4 in evaluating responses. With the assistance of experts, we do not incorporate the safety metric into our prompt.\nIf you are a professional physician, you need to analyze based on two answers to the question,\nas follows:\nQuestion: {question}\nAnswer1:{answer1}\nAnswer2: {answer2}\nEvaluation Criteria:\n1. Professionalism:\n- Accurately understand patient questions and provide relevant answers.\nClearly and concisely explain complex medical knowledge.\nProactively inquire about the patient's condition when necessary.\n2. Fluency:\n- Ensure semantic coherence with no logical errors or irrelevant information.\nMaintain consistency in style and content.\nMaintain a friendly, enthusiastic answering attitude.\nNote:Evaluate based on the importance of Professionalism > fluency. If there's a conflict,\nprioritize the former.\nOutput Format:Based on the above criteria, judge the result of \"Answer1\" relative to \"Answer2\".\nOutput as: Win, Lose, Tie."}]}