{"title": "GRL-Prompt: Towards Knowledge Graph based Prompt Optimization via Reinforcement Learning", "authors": ["Yuze Liu", "Tingjie Liu", "Tiehua Zhang", "Youhua Xia", "Jinze Wang", "Zhishu Shen", "Jiong Jin", "Fei Richard Yu"], "abstract": "Large language models (LLMs) have demonstrated impressive success in a wide range of natural language processing (NLP) tasks due to their extensive general knowledge of the world. Recent works discovered that the performance of LLMs is heavily dependent on the input prompt. However, prompt engineering is usually done manually in a trial-and-error fashion, which can be labor-intensive and challenging in order to find the optimal prompts. To address these problems and unleash the utmost potential of LLMs, we propose a novel LLMs-agnostic framework for prompt optimization, namely GRL-Prompt, which aims to automatically construct optimal prompts via reinforcement learning (RL) in an end-to-end manner. To provide structured action/state representation for optimizing prompts, we construct a knowledge graph (KG) that better encodes the correlation between the user query and candidate in-context examples. Furthermore, a policy network is formulated to generate the optimal action by selecting a set of in-context examples in a rewardable order to construct the prompt. Additionally, the embedding-based reward shaping is utilized to stabilize the RL training process. The experimental results show that GRL-Prompt outperforms recent state-of-the-art methods, achieving an average increase of 0.10 in ROUGE-1, 0.07 in ROUGE-2, 0.07 in ROUGE-L, and 0.05 in BLEU.", "sections": [{"title": "1 Introduction", "content": "With ongoing advancements in large language models (LLMs), such as GPT-3 (Brown et al., 2020) and LLaMA (Hugo et al., 2023), optimizing query-guided prompts, especially in the realm of in-context learning (ICL), has been crucial in unlocking the full potential of LLMs, leading to significant improvements across various downstream natural language processing (NLP) tasks. (Liu et al., 2021). In the standard paradigm of ICL, a small collection of examples, each of which comprises small piece of contextual texts in natural language, is generally prepared to construct the prompt. This prompt effectively empowers LLMs to excel in tasks such as natural language understanding (Ye et al., 2023) and the logic reasoning (Lu et al., 2023). In contrast to many previous studies that concentrate on fine-tuning a large number of trainable parameters in the model for various downstream tasks (Wei et al., 2022a), in-context learning (ICL) enables LLMs to achieve competitive performance without relying on gradient-based training, thus avoiding over-reliance on computational resources such as GPUs. An imperative issue in in-context learning is how to select the most suitable examples to construct prompts that effectively improve the LLM's performance across various tasks (Xu et al., 2024).\nPrevious research has addressed this challenge primarily through heuristic handcrafting, random selection, and retrieval-based selection methods. The heuristic handcrafting approach involves manually creating examples for a particular task, which can be both labor-intensive and time-consuming (Wei et al., 2022b). Some studies have randomly select in-context examples from the training dataset to offer additional contextual information, with the goal of directing LLMs toward the desired responses (Brown et al., 2020). Unfortunately, the randomness of the selection strategy introduces uncertainty and noise into the prompt, leading to a degradation in the performance of the LLMs. Retrieval-based selection attempts to tackle this issue by obtaining semantically similar examples using static methods such as K-nearest neighbors (Nie et al., 2022). However, this method fails to thoroughly explore the prompt space, leading to limited effectiveness in complex tasks. It has been pointed out in recent studies that retrieval-based selection methods face the challenge of overlooking the permutations of in-context examples, which can lead to a degradation in the performance of LLMs (Lu et al., 2022; Xu et al., 2024).\nTo alleviate this challenge, we propose a novel LLMs-agnostic framework for prompt optimization, namely GRL-Prompt, which aims to automatically construct optimal prompts via reinforcement learning (RL) in an end-to-end manner. Our GRL-Prompt framework comprises two key components: 1) a knowledge graph built from the user instruction and candidate in-context examples, utilizing a heterogeneous graph neural network to encode the structured embedding representations of the graph; and 2) a policy network that includes a pairwise edge classifier and an in-context matching network, generating an optimal sequence of in-context examples to create the most effective prompts based on the constructed knowledge graph. The knowledge graph and the policy network are updated collaboratively in an end-to-end manner. Our contributions can be summarized as follows:\n\u2022 We propose a novel LLMs-agnostic framework GRL-Prompt for prompt optimization. The knowledge graph is initially constructed to effectively capture the correlation between the user instruction and candidate in-context examples through heterogeneous graph learning. The designated policy network allows the GRL-Prompt to function as an agent that interacts iteratively with the LLMs.\n\u2022 We formulate a policy network that incorporates a pairwise edge classifier (PEC) and an in-context matching network (ICMN) to dynamically learn the policy for generating an order-sensitive sequence of in-context samples. PEC is designed to classify the order of examples, while ICMN aims to select the relevant examples to construct the optimal prompt based on the structured representation of a constructed knowledge graph. Moreover, we design the embedding-based reward shaping to stabilize the RL training process.\n\u2022 We conduct extensive experiments on two distinct datasets, demonstrating that GRL-Prompt outperforms state-of-the-art baselines in in-context learning. Furthermore, we have made our source code publicly available to contribute further to advancements in this field (find the source code as supplementary materials)."}, {"title": "2 Related Work", "content": "In recent years, LLMs have achieved remarkable progress in the field of NLP (Lewis et al., 2019; Raffel et al., 2020). Notably, GPT-3 (Brown et al., 2020) has demonstrated exceptional capabilities in few-shot ICL (Dong et al., 2022), where it learns tasks with only a few examples as demonstrations, without requiring any gradient updates (Liu et al., 2021; Min et al., 2022).\nTo further unleash the potential of LLMs without updating the large amount of parameters in the model, a series of studies have focused on developing prompt optimization techniques to guide models toward generating more accurate and relevant outputs with high-quality input prompts. Wei et al. (2022b) investigated how LLMs handle few-shot prompting for reasoning tasks using prompts consisting of triples\u2014input, chain of thought, and output\u2014and examined chain-of-thought prompting as a straightforward and widely applicable technique for improving reasoning in these models. Following that, Wang et al. (2023) proposed a chain-of-knowledge prompting approach that decomposes reasoning chains generated by LLMs into several evidence triples and explanatory hints, thereby enhancing the reasoning capabilities. Li et al. (2024b) developed a tabular prompting method called TableIE, which reframes the relational triple extraction task as a table generation problem and has demonstrated promising results in ICL. These methods have demonstrated how prompt optimization enhances the logical reasoning and content generation capabilities of LLMs.\nDespite these impressive successes, the aforementioned works still face challenges when tackling tasks that require deep and complex reasoning (Xu et al., 2024). Knowledge Graphs (KGs) (Chen et al., 2020) serve as an ideal foundation for prompt optimization by providing structured knowledge representation. KGs encapsulate domain knowledge through entities and relationships, providing LLMs with rich contextual information and reasoning pathways. However, relying on knowledge graphs alone for prompt construction still has its limitations, as knowledge graph-based prompts often overlook the context-dependence of natural language, resulting in degraded performance on complex tasks. Therefore, it is essential to optimize knowledge graph-based prompts through interactive feedback from LLMs (Li et al., 2024c).\nTo address these challenges, several studies have started to explore reinforcement learning for prompt optimization. Deng et al. (2023) proposes RLPROMPT, a discrete prompt optimization approach with RL. This method formulates a parameter-efficient policy network that generates the optimized discrete prompt by training with the designated reward function. Qi et al. (2023) introduces PILLOW, a prompt-matching framework enhanced by RL. This framework utilizes a matching network to select prompts from a user-defined pool, concatenates them with user instructions as input, and performs inference with LLMs to improve the efficiency of instruction fine-tuning. Similarly, PROMPTPG employs a RL-based policy gradient to learn how to select in-context examples from a limited amount of training data and then dynamically constructs the corresponding prompt (Lu et al., 2023).\nRelation to Existing ICL methods Compared to the random selection in the ICL paradigm (Brown et al., 2020; Wei et al., 2022b), GRL-Prompt updates the policy network iteratively to generate an optimal sequence of in-context examples for prompt optimization. ICL-kNN (Nie et al., 2022), on the other hand, uses a static clustering method (k-nearest neighbor) to select in-context examples, which also lacks the dynamic learning on the policy network. This method thus fails to thoroughly explore the prompt space, leading to limited effectiveness in complex tasks. Another issue is that the order of the in-context samples introduces variation in the performance of LLMs. GRL-Prompt can dynamically learn the policy for generating an order-sensitive sequence of in-context samples because its policy network has two key components: a PEC and an ICMN. In contrast, PromptPG (Lu et al., 2023) and Pillow (Qi et al., 2023) only select a fixed-length set of in-context samples. Moreover, the constructed knowledge graph, which encodes the correlation between the user query and the candidate examples, provides a structured representation for GRL-Prompt to optimize prompts."}, {"title": "3 Preliminary and Problem Definition", "content": "In this section, we first introduce the relevant definitions and then formulate the key problems associated with prompt optimization.\nKnowledge Graph A knowledge graph is defined as G = (V,E,R), with nodes $v_i \\in V$ and directed edges $(v_i, r_{ij}, v_j) \\in E$, where $r_{ij} \\in R$ is the relation type of the edge between $v_i$ and $v_j$.\nReinforcement Learning The RL is formulated as a Markov Decision Process (MDP), which is a sequential decision-making mathematical model in which actions affect the current short-term rewards, subsequent states, and future rewards (Sutton and Barto, 2018). The MDP is defined as the tuple {S, A, T, R, \u03b3}, where $s \\in S$ and $a \\in A$ denote state and action, respectively. $T : S \u00d7 A \u2192 p(S)$ is identified by the states transition. p(\u00b7) denotes the distribution of the states. R is the reward function, with $\u03b3 \\in [0, 1]$ as the discount factor.\nIn-context Learning ICL is a paradigm that facilitates language models to learn tasks using only a few in-context examples presented in the form of natural language (Brown et al., 2020). Formally, the sequence of in-context examples is denoted as $P_{ic} = [p_{ic_1}, ..., p_{ic_K}]$, where K is the number of in-context examples provided to construct the prompt.\nPrompt Optimization Problem Given a user query text q \u2208 Q and a set of candidate examples $P_{cand}$, we aim to learn a prompt optimization strategy $f_{opt}(q,P_{cand}) \u2192 P_{ic}$. Each candidate example $p \\in P_{cand}$ is a triple of (query, context, response), where query denotes the user query, context represents the extra information provided by the users (optional), and response is the expected answer. The generated sequence of in-context examples provided for LLMs can be used for a variety of downstream NLP tasks."}, {"title": "4 Methodology", "content": "In this section, we introduce GRL-Prompt, an LLMs-agnostic framework for prompt optimization via RL. As illustrated in Figure 1, a knowledge graph is constructed using candidate examples and a user query, from which a heterogeneous graph neural network is employed to encode their correlations into high-dimensional embedding representations for the downstream component. The policy network in the agent learns to find the optimal sequence of in-context examples from the constructed knowledge graph, aiming to maximizing the prediction rewards for the user query when interacting with LLM environment. Additionally, an embedding-based reward shaping is utilized to stabilize the RL training process."}, {"title": "4.1 Knowledge Graph for Prompt Construction", "content": "The correlations between in-context examples and the user query influence the performance of LLMs in the ICL paradigm (Nie et al., 2022). GRL-Prompt explores the application of KG to better encode the correlations between candidate examples and the user query, facilitating prompt construction. The user query and candidate examples are provided to the KG construction and learning module, as indicated by (a) in Figure 1."}, {"title": "4.1.1 Knowledge Graph Construction", "content": "Since there is no concept of nodes in the set of candidate examples $P_c$ and the user query q, we treat each candidate example $p^i \u2208 P_{cand}, i \u2208 [1, ..., N]$ as a candidate node $v_i$ and the user query q as query node $v_q$. The candidate node $v_i$ and the query node $v_q$ are represented as a circle and a triangle in Figure 1, respectively. The number of the candidate examples is N. The node set $V = \\{v_i\\}_{i=1}^N \\cup v_q$ in the knowledge graph G(q) consists of two types of nodes: candidate node $v_i$ and query node $v_q$. We utilize a pre-trained language model, such as BERT (Lu et al., 2023) and RoBERTa (Ghosal et al., 2021), to generate the initial node embeddings in the knowledge graph, which is defined as:\n$X^0 = PreLM(V)$\nHere, we use the values from the final layer of the pre-trained language model as the initial node embeddings $X^0 \u2208 R^{(N+1)\u00d7d}$.\nWe construct three types of edges: candidate-to-candidate edges, query-to-candidate edges and candidate-to-query edges, which are based on the constituent nodes to encode different relations. Each candidate node $v_i \u2208 V$ is connected to all other candidate nodes $v_j (j \u2260 i)$ with relation $r_{cc}$. The directed candidate-to-candidate edge is denoted as $(v_i, r_{cc}, v_j)$, represented by the green arrow lines in Figure 1. Our formulation results in bidirectional edges between each pair of candidate nodes, i.e. both $(v_i, r_{cc}, v_j)$ and $(v_j, r_{cc}, v_i) \u2208 E$. The query-to-candidate edge is constituted by the query node $v_q$ and the candidate node $v_i$, which is denoted as $(v_q, r_{qc}, v_i)$. Meanwhile, each candidate node $v_i \u2208 V$ is bidirectionally connected to the query node $v_q$, leading to the candidate-to-query edges $(v_i, r_{cq}, v_q)$ and query-to-candidate edges $(v_q, r_{qc}, v_i)$, respectively. These two types of edges are represented by the blue arrow lines and the orange arrow lines, respectively."}, {"title": "4.1.2 Knowledge Graph Learning", "content": "To capture the complex interactions between the candidate examples and the user query in the constructed knowledge graph, we utilize a two-layer Heterogeneous Graph Transformer (HGT) (Hu et al., 2020) to perform knowledge graph learning. The details of HGT can be found Appendix A.1."}, {"title": "4.2 RL-based Prompt Optimization", "content": "Recent works have unveiled the high sensitivity of LLMs to prompts (Li et al., 2024a; Bach et al., 2022). Recent research has shown that the performance of LLMs with in-context learning can be highly unstable across different selections of in-context examples and permutations of those examples (Liu et al., 2021; Lu et al., 2022). Apart from that, finding the optimal prompt for various LLMs in different tasks is usually done manually in a trial-and-error fashion, which can be labor-intensive and challenging. To alleviate this issue, we design a policy network that utilizes the structural correlations in the constructed knowledge graph, as indicated by (b) in Figure 1, to optimize prompts within the RL paradigm, avoiding brute-force searching or manually designed heuristics."}, {"title": "4.2.1 Policy Network", "content": "We design a policy network $\u03c0_\u03b8 (\\hat{o}, P_{ic}|G(q))$ that incorporates a PEC to predict the relative order of any two candidate examples, utilizing the structural representation of the candidate nodes within the knowledge graph. Furthermore, an ICMN in policy network assesses the probability of each candidate example being selected as an in-context example within the constructed knowledge graph.\nPEC uses the node embedding in the constructed knowledge graph to classify the order between two corresponding candidate examples. For instance, taking $p^i$ and $p^j$ as two candidate examples, where i \u2260 j, and their corresponding nodes in the constructed knowledge graph G are $v_i$ and $v_j$, respectively. In this formulation, PEC considers the bidirectional edges between $v_i$ and $v_j$ in & of G(q) $(v_i, r_{ij}, v_j)$ and $(v_j, r_{ji}, v_i)$. The classification objective is then to compare the pair scores of two edges that are reversed in direction, which is denoted as:\n$f_{pec}(v_i, v_j) = max( [p_s(v_i, v_j), p_s(v_j, v_i)])$\nIf $p_s(v_i, v_j) > p_s(v_j, v_i)$, then we predict candidate example $p^i$ appears earlier than $p^j$ ($p^i \u2192 p^j$), or vice versa ($p^j \u2192 p^i$). Naturally, the pair score function $p_s(\u00b7,\u00b7)$ must be sensitive to the order of its inputs, and the output represents the probability of the order between two candidate examples. We define the $p_s$ function as follows:\n$p_s(v_i, v_j) = \\frac{e^{sin(X(v_i)-X(v_j))\u00b7w}}{e^{sin(X(v_i)\u2212X(v_j))\u00b7w} + e^{sin(X(v_j)\u2212X(v_i))\u00b7w}}$\nwhere $w \u2208 R^d$ is the learnable parameter of the function.\nICMN calculates probability of selecting each candidate examples $p^i \u2208 P_{cand}$ for the set of in-context samples $P_{ic}$ based on the node embedding of the knowledge graph. For each candidate examples $p^i$, the corresponding node in the knowledge graph is $v_i$, and the user query q corresponds to the query node $v_q$. The calculation process of the selecting probability is defined as follows:\n$f_{icmn}(v_q,v_i) = sigmoid(\\frac{X(v_q) \u00b7 W_m \u00b7 X^T(v_i)}{\\sqrt{d}})$\n$W_m \u2208 R^{d\u00d7d}$ denotes the learnable matrix, and sigmoid function maps the similarity between the candidate example and the user query into probability space. $\\sqrt{d}$ acts as a scaling factor.\nThe policy network obtains the probability distribution for the generation of the sequence of in-context examples from the candidate examples set $P_{cand}$ via the component of ICMN and PEC, which is denoted as:\n$p(\u03c0) = (\\prod_{i<j} f_{pec}(v_i, v_j))\u00d7(\\prod_{i\u2208[1,N]} f_{icmn}(v_q,v_i))$\nwhere N is the number of the set of candidate examples. Finally, given a user query q, the sequence of in-context examples is generated from the set of candidate examples according to the policy network\n$P_{ic} ~ TS(\u03c0_\u03b8 (\\hat{\u00d4}, P_{ic}|G(q))), P_{ic}\u2208 F(P_{cand})$\nwhere F(\u00b7) is the operator that constructs a set of all possible order-sensitive subsets from a given set. The total number of possible order-sensitive subsets from a set with n elements is $\\sum_{m=1}^{n} \\frac{n!}{(n\u2212m)!}$. We use F($P_{cand}$) to denote the state space in the RL in our formulation. TS(\u00b7) represents the topological sorting method (Bommasani and Cardie, 2020), which is used to obtain the final ordered sequence from the all the pairwise edges orders \u00f4. If the PEC predicts that the order between the candidate example $p^i$ and $p^j$ is $(p^i \u2192 p^j)$, TS(.) ensures $p^i$ comes before $p^j$ in the final ordering. We sample the set of in-context examples $P_{ic}$ and pairwise orders \u00f4 associated with the in-context examples from the policy network, and the sequence of in-context examples $P_{ic}$ is generated by TS(.)."}, {"title": "4.2.2 Reward Design", "content": "Since we test GRL-Prompt on general text-to-text generation tasks, the reward is designed based on the evaluation of the responses from the LLMs using the in-context examples that we predict. The variation in the responses from black-box LLMs poses challenges to the training efficiency and convergence rate of the RL training process. To address these issues, we develop embedding-based reward shaping to smooth the reward function and stabilize the RL training, which can be expressed as:\n$R(a, \\hat{a}) = xR_m(a, \\hat{a}) + (1 \u2212 1)R_e(a, \\hat{a})$\n$R_m$ represents fuzzy textual similarity, and $R_e$ denotes the cosine embedding similarity based on sentence representations. a represent the expected output. The generated response of LLMs denotes $\\hat{a}$, which uses the predicted sequence of in-context examples $P_{ic}$ and the given user query q to construct the prompt. A is the hyperparameter for adjusting the weight of two components in the reward function."}, {"title": "4.2.3 Training Process", "content": "Given the set of training user query $Q_{tr} = \\{q^i\\}_{i=1}^M$ and their corresponding the expected responses ${\\{a^i, ..., a^M\\}}$, our goal is to maximize the expected reward of the generated response using the policy network $E_{p^i P ~ \u03c0_\u03b8 (\\hat{\u00f4_i},P_i|G(q^i))} [R(a^i, \\hat{a^i})]$. We optimize the reward with respect to the parameters of the policy network using the policy gradient algorithm (Williams, 1992), which is defined as follows:\n$\\nabla_\u03b8 L= \\frac{1}{m}\\sum_{i=1}^m Volog(\u03c0_\u03b8(\u00f4,P|G(q^i)))R(a^i,\\hat{a^i})$\nHere, m denotes the size of each batch from our training set of user query $Q_{tr}$. The learnable parameters in the knowledge graph and the policy network are updated iteratively as GRL-Prompt acts as an agent interacting with the LLMs, which is demonstrated in Figure 1. We uses the generated sequence of in-context examples and the user query to construct the final prompt when interacting with LLMs. The demonstration of the prompt format used in the prompt creator is shown in Appendix A.2, while the exemplary demonstration of user query and the in-context example can be found in Figure 1. Intuitively, if the response of LLMs is correct, we update the policy network to increase the probability of generating the same sequence of the in-context samples. Conversely, we update the policy network to reduce the probability of generating that sequence."}, {"title": "5 Experiment", "content": "In this section, we introduce GRL-Prompt, an LLMs-agnostic framework for prompt optimization via RL. As illustrated in Figure 1, a knowledge graph is constructed using candidate examples and a user query, from which a heterogeneous graph neural network is employed to encode their correlations into high-dimensional embedding representations for the downstream component. The policy network in the agent learns to find the optimal sequence of in-context examples from the constructed knowledge graph, aiming to maximize the prediction rewards for the user query when interacting with LLM environment. Additionally, an embedding-based reward shaping is utilized to stabilize the RL training process."}, {"title": "5.1 Experimental Setup", "content": "In this section, we introduce GRL-Prompt, an LLMs-agnostic framework for prompt optimization via RL. As illustrated in Figure 1, a knowledge graph is constructed using candidate examples and a user query, from which a heterogeneous graph neural network is employed to encode their correlations into high-dimensional embedding representations for the downstream component. The policy network in the agent learns to find the optimal sequence of in-context examples from the constructed knowledge graph, aiming to maximize the prediction rewards for the user query when interacting with LLM environment. Additionally, an embedding-based reward shaping is utilized to stabilize the RL training process."}, {"title": "5.1.1 Dataset", "content": "In this section, we introduce GRL-Prompt, an LLMs-agnostic framework for prompt optimization via RL. As illustrated in Figure 1, a knowledge graph is constructed using candidate examples and a user query, from which a heterogeneous graph neural network is employed to encode their correlations into high-dimensional embedding representations for the downstream component. The policy network in the agent learns to find the optimal sequence of in-context examples from the constructed knowledge graph, aiming to maximize the prediction rewards for the user query when interacting with LLM environment. Additionally, an embedding-based reward shaping is utilized to stabilize the RL training process."}, {"title": "5.1.2 Baselines and Configurations", "content": "In this section, we introduce GRL-Prompt, an LLMs-agnostic framework for prompt optimization via RL. As illustrated in Figure 1, a knowledge graph is constructed using candidate examples and a user query, from which a heterogeneous graph neural network is employed to encode their correlations into high-dimensional embedding representations for the downstream component. The policy network in the agent learns to find the optimal sequence of in-context examples from the constructed knowledge graph, aiming to maximize the prediction rewards for the user query when interacting with LLM environment. Additionally, an embedding-based reward shaping is utilized to stabilize the RL training process."}, {"title": "5.1.3 Evaluation Metric", "content": "In this section, we introduce GRL-Prompt, an LLMs-agnostic framework for prompt optimization via RL. As illustrated in Figure 1, a knowledge graph is constructed using candidate examples and a user query, from which a heterogeneous graph neural network is employed to encode their correlations into high-dimensional embedding representations for the downstream component. The policy network in the agent learns to find the optimal sequence of in-context examples from the constructed knowledge graph, aiming to maximize the prediction rewards for the user query when interacting with LLM environment. Additionally, an embedding-based reward shaping is utilized to stabilize the RL training process."}, {"title": "5.2 Performance Analysis", "content": "In this section, we introduce GRL-Prompt, an LLMs-agnostic framework for prompt optimization via RL. As illustrated in Figure 1, a knowledge graph is constructed using candidate examples and a user query, from which a heterogeneous graph neural network is employed to encode their correlations into high-dimensional embedding representations for the downstream component. The policy network in the agent learns to find the optimal sequence of in-context examples from the constructed knowledge graph, aiming to maximize the prediction rewards for the user query when interacting with LLM environment. Additionally, an embedding-based reward shaping is utilized to stabilize the RL training process."}, {"title": "5.3 Ablation Study", "content": "In this section, we introduce GRL-Prompt, an LLMs-agnostic framework for prompt optimization via RL. As illustrated in Figure 1, a knowledge graph is constructed using candidate examples and a user query, from which a heterogeneous graph neural network is employed to encode their correlations into high-dimensional embedding representations for the downstream component. The policy network in the agent learns to find the optimal sequence of in-context examples from the constructed knowledge graph, aiming to maximize the prediction rewards for the user query when interacting with LLM environment. Additionally, an embedding-based reward shaping is utilized to stabilize the RL training process."}, {"title": "5.4 Sensitivity Analysis", "content": "In this section, we introduce GRL-Prompt, an LLMs-agnostic framework for prompt optimization via RL. As illustrated in Figure 1, a knowledge graph is constructed using candidate examples and a user query, from which a heterogeneous graph neural network is employed to encode their correlations into high-dimensional embedding representations for the downstream component. The policy network in the agent learns to find the optimal sequence of in-context examples from the constructed knowledge graph, aiming to maximize the prediction rewards for the user query when interacting with LLM environment. Additionally, an embedding-based reward shaping is utilized to stabilize the RL training process."}, {"title": "6 Conclusion", "content": "In this paper, we propose a LLMs-agnostic prompt optimization framework called GRL-Prompt. This framework first constructs a knowledge graph from the user instruction and the candidate examples, learning with HGT to better encode their correlation. Afterwards, the policy network is designed to find the optimal sequence of in-context examples by incorporating the PEC and ICMN to classify the order of examples and select the relevant ones. We also conduct comprehensive experiments on two widely-used public datasets and compare the proposed GRL-Prompt with four baseline methods. The results and analysis reveal that the proposed framework achieves state-of-the-art performance across different LLMs in the in-context learning paradigm."}, {"title": "A Appendix", "content": "In this section, we introduce GRL-Prompt, an LLMs-agnostic framework for prompt optimization via RL. As illustrated in Figure 1, a knowledge graph is constructed using candidate examples and a user query, from which a heterogeneous graph neural network is employed to encode their correlations into high-dimensional embedding representations for the downstream component. The policy network in the agent learns to find the optimal sequence of in-context examples from the constructed knowledge graph, aiming to maximize the prediction rewards for the user query when interacting with LLM environment. Additionally, an embedding-based reward shaping is utilized to stabilize the RL training process."}, {"title": "A.1 The Details of Heterogeneous Graph Transformer (HGT)", "content": "The HGT can model heterogeneous relations in our knowledge graph using a multi-head attention mechanism. It calculates the multi-head attention between a given node and its neighbor nodes with different relations, and then uses the normalized attention as the weight for the neighbor nodes to update the embedding of the given node. Taking the node $v_i$ as an example, the embedding learning process can be formulated as:\n$\\begin{array}{l}Q^{h}\\left(v_{j}\\right)=x^{l-1}\\left(v_{j}\\right) \\cdot Q-Lin^{h}_{\\phi\\left(v_{i}, r_{i j}, v_{j}\\right)}\\\\K^{h}\\left(v_{i}\\right)=x^{l-1}\\left(v_{i}\\right) \\cdot K-Lin^{h}_{\\phi\\left(v_{i}, r_{i j}, v_{j}\\right)}\\\\a t t^{h}\\left(v_{i}, r_{i j}, v_{j}\\right)=\\frac{1}{\\sqrt{d}}\\left(Q^{h}\\left(v_{j}\\right) \\cdot a t t^{h} \\cdot K^{h}\\left(v_{i}\\right)^{T}\\right) \\cdot \\mu_{\\phi\\left(v_{i}, r_{i j}, v_{j}\\right)}\\end{array}$\nWe use l \u2208 [1, ..., L] to denote the l-th HGT layer and L is the total number of layers. Given a target node $v_j$, and all its neighbors $v_i \u2208 N(v_j)$, we first project the target node into the h-th query vector $Q^h (v_j)$ with a type-specific linear transformation matrix $Q-Lin^h \u2208 R^{d\u00d7d}$, where H is the number of attention heads. \u03d5(\u00b7) is the type mapping function. We also project the neighbor node $v_i$ into the h-th key vector $K^h (v_i)$ on the same dimension. Next, we apply a relation-specific weight matrix att \u2208R XH to obtain the h-th attention head $a t t^{h}\\left(v_{i}, r_{i j}, v_{j}\\right)$ and $\\frac{1}{\\sqrt{d}}$ acts a scaling factor. Moreover, since not all the relations contribute equally to the target nodes, a learnable vector \u03bc associated with the relation triplet \u03d5($v_i$), $r_{ij}$, \u03d5($v_j$) acts as a scaling factor for this triplet relationship. Finally, the weight of the neighbor node $w^h_i (v_i)$ is calculated by softmax normalization. The attentive aggregation of different heads across neighbor nodes with various relations for updating node embedding of $v_j$ is defined as:\n$X^{l}\\left(v_{j}\\right)=\\frac{1}{\\left|N\\left(v_{j}\\right)\\right|} \\sum_{v_{i} \\in N\\left(v_{j}\\right)} M L P\\left(||_{h=1}^{H} w_{i}^{h}\\left(v_{i}\\right) \\cdot K^{h}\\left(v_{i}\\right)\\right)$\nwhere  ||  represents concatenation. We first aggregate all embedding of neighbor nodes, and then concatenate all H heads. After that, the node embedding of $v_j$ is updated by a shallow multi-layer perceptron (MLP)."}, {"title": "A.2 Prompt Template", "content": "The prompt template is designed to interact with different LLMs using the proposed GRL-Prompt. Figure 1 demonstrates the designated prompt template format. Notably, the sequence of the in-context examples $P_{ic} = [pic_1, ..., pk]$ in the prompt template is generated by GRL-Prompt, and the user query q is derived from the test data. In Figure 1, the placeholders (marked as red) denote the in-context examples and the user query."}, {"title": "A.3 Supplementary Comparison of The Training Loss Trend with Baselines.", "content": "We conduct extensive experiments to compare the trend of training loss with all baselines, as illustrated in the Figure 2. The fast convergence rate and lower loss observed with the GRL-Prompt method are primarily attributed to the designated embedding-based reward, which effectively incorporates feedback from LLMs during the prompt optimization phase."}, {"title": "A.4 Case Study", "content": "Figures 3 to 8 show the comparative case study of GRL-Prompt and other baseline methods"}]}