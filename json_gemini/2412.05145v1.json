{"title": "Explingo: Explaining AI Predictions using Large Language Models", "authors": ["Alexandra Zytek", "Sara Pido", "Sarah Alnegheimish", "Laure Berti-\u00c9quillet", "Kalyan Veeramachaneni"], "abstract": "Explanations of machine learning (ML) model pre- dictions generated by Explainable AI (XAI) techniques such as SHAP are essential for people using ML outputs for decision- making. We explore the potential of Large Language Models (LLMs) to transform these explanations into human-readable, narrative formats that align with natural communication. We address two key research questions: (1) Can LLMs reliably transform traditional explanations into high-quality narratives? and (2) How can we effectively evaluate the quality of nar- rative explanations? To answer these questions, we introduce EXPLINGO, which consists of two LLM-based subsystems, a NARRATOR and GRADER. The NARRATOR takes in ML expla- nations and transforms them into natural-language descriptions. The GRADER scores these narratives on a set of metrics including accuracy, completeness, fluency, and conciseness. Our experiments demonstrate that LLMs can generate high- quality narratives that achieve high scores across all metrics, particularly when guided by a small number of human-labeled and bootstrapped examples. We also identified areas that remain challenging, in particular for effectively scoring narratives in complex domains. The findings from this work have been inte- grated into an open-source tool that makes narrative explanations available for further applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Explanations of ML model predictions, such as those cre- ated by Explainable AI (XAI) algorithms like SHAP [1], are important for making ML outputs usable for decision-making. It is difficult for users to make informed decisions using ML outputs without some explanation or justification of how ML model arrived at those. Past work [2], [3] has suggested that some users may prefer explanations in a natural language format what we refer to in this paper as narratives. The \"LLM-generated narrative\u201d in Fig. 1 (green) shows an example of such a narrative. These narratives offer many benefits: they align with how humans naturally communicate, allow users to glean key takeaways quickly, and are a first step in enabling interactive conversations where users can better understand models by asking follow-up questions.\nLarge Language Models (LLMs) offer a promising way of generating such narratives. Past work has suggested different ways to do so. Some work [4], [5] uses LLMs directly to explain ML predictions, without use of traditional XAI algo- rithms. While these approaches offer promising paths towards novel explanations, they also risk introducing inaccuracy or hallucination in explanations. An alternative approach, and the one we focus on in this paper, is to use LLMs to transform existing ML explanations generated using traditional, theoret- ically grounded XAI approaches into readable narratives.\nWe seek to address two primary research questions:\n1) Can LLMs effectively transform traditional ML explana- tions into high-quality narrative versions of explanations?\n2) How do we evaluate the quality of these narratives?\nWe address these questions with our two-part EXPLINGO system. To address the first question, we develop an LLM- based NARRATOR that transforms SHAP explanations from diverse datasets into narratives. We then investigate what metrics may be of value to assess the quality of narratives, and auto-grade the generated narratives using our GRADER to address the second question. Automatically grading narratives is helpful not only for tuning and evaluating the system, but also for sure as a guardrail in deployment by hiding any narratives that are not of sufficient quality.\nWe note that users across different domains may have significantly different preferences for how these narratives should look. Some factors like the fact that narratives should accurately represent the original explanation and not be unnecessarily long are fairly universal. On the other hand, other factors are highly subjective. Some users may wish for a narrative that includes very precise and technical details, while others prefer more general information. In this paper, we aim to generate narratives that match the style of some small, user-provided exemplar dataset of narratives. Our objective is to create a system where, for any new application, a user can write a small number of sample narratives (around three to five) and get high-quality narrative versions of future ML predictions that mimic the linguistic style and structure of those samples.\nOur key contributions from this work are:\nWe present our NARRATOR system that transforms ML explanations into user-readable narratives.\nWe develop an independent, automated GRADER system that assesses the quality of the generated narratives.\nWe curated a sample set of nine exemplar datasets that contain narratives using four public datasets, that can be used for future narrative generation tuning and evaluation work. These datasets are public and can be accessed here: https://github.com/sibyl-dev/Explingo.\nWe provide a comprehensive set of experiments to demonstrate the impact of different prompting and few-"}, {"title": "II. BACKGROUND", "content": "SHAP values [1] are a popular way to explain ML model predictions in terms of feature contributions. In these expla- nations, every feature is given a positive or negative value representing how much it increased or decreased the model's prediction on a specific input. Past work has suggested these explanations are popular among users, but may be difficult for some user groups to parse [7], [8], [9], [10], [11].\nPast work has aimed to apply LLM's language-processing capabilities to XAI problems in different ways. Some work [12], [13], [14] has focused on using LLMs at the input stage, taking in user questions and generating appropriate explanations to answer them. Other work [4], [5] has sought to use LLMs directly to generate explanations of model logic.\nIn this paper, we instead build on work seeking to transform existing ML explanations into readable narratives. Burton et al. [15] used an LLM fine-tuned on a dataset of natural-language explanations to produce fluent and accurate narratives. Guo et al. [16] proposes a few-shot table-to-text generation approach that uses unlabeled domain-specific knowledge to convert ta- bles into natural-language descriptions. We extend these works through an alternative scoring method based on a specific set of metrics, and use an approach based purely on a mix of manual and optimized prompting.\nPast work [17], [18], [19], [20] has also looked into using LLMs to evaluate LLM outputs, in order to reduce effort and time required from a potentially small group of human users. Fu et al. [21] found that GPT-3 was able to effectively evaluate the quality of LLM-generated text based on a wide variety of metrics such as correctness, understandability, and diversity. Wang et al. [22] similarly found that chatGPT is able to achieve state-of-the-art correlation with human graders. Liu et al. [23] found that advanced prompting techniques such as chain-of-thought prompting can further improve correlation between LLM and human grades. We applied these insights to optimize and evaluate our LLM-generated narratives using an LLM-based metric grader, eliminating the need for human graders and enabling evaluation across a wider range of domains without relying on domain experts."}, {"title": "III. THE EXPLINGO SYSTEM: NARRATOR AND GRADER", "content": "To transform XAI explanations into narratives, the Ex- PLINGO system includes two sub-systems: 1) NARRATOR: a system to transform traditional explanations of ML model predictions into accurate, natural narratives. 2) GRADER: a metric grading system to assess the quality of narratives.\nTransforming traditional ML model explanations into read- able narratives offers several potential benefits. First, these narratives allow key takeaways to be read quickly. Second, they are often the most natural format for parsing information for many users, who may prefer them to graphs or have diffi- culty understanding graphs that are not visualized effectively. Finally, offering narrative versions of explanations is the first step to allowing for full \u201cconversations\u201d with ML models, with users able to ask follow-up questions in natural-language and receive answers in kind.\nThe NARRATOR is build on an LLM model, in our case GPT-40, that is prompted to generate a narrative. The basic prompt is composed of four components: a base prompt, a"}, {"title": "A. NARRATOR: Transforming ML Explanations to Narratives", "content": "Transforming traditional ML model explanations into read- able narratives offers several potential benefits. First", "conversations": "ith ML models", "components": "a base prompt", "Narrative": "invoking the LLM to start narrating. The first four blocks in Fig. 1 show an example prompt that follows this structure. As described in depth later"}, {}, {"title": "B. GRADER: Assessing the Quality of Narratives", "content": "To assess the quality of narratives, we adapted metrics proposed in [28], which include:\nAccuracy: how accurate the narrative is to the original ML explanation.\nCompleteness: how much of the information from the original ML explanation is present in the narrative.\nFluency: how well the narrative matches the desired linguistic style and structure.\nConciseness: how long the narrative is considering the number of features in the explanation.\nWhile there are other metrics that can be included, for the presentation of the GRADER, we focus on these four. The challenge with these metrics is that they are time consuming to manually compute, and manual assessment could introduce human bias into the evaluation.\nOur GRADER automatically evaluates the quality of the narrative by prompting an independent LLM, GPT-40 in our case, to consider the ML explanation and the generated narrative and assign a value to the respective metric. Fig. 2 shows an example of a GRADER prompt evaluating the accuracy metric. In summary, it takes in the input and output of the NARRATOR along with a rubric criteria, and comes up with a score depending on which metric it is asked to assess. In Fig. 2, the grader return will return either 1, indicating an accurate narrative, or 0 for an inaccurate narrative. Straight out-of-the-box, it can be difficult for LLMs to produce the exact desired output. Section V goes into details about how we tuned, validated, and used the GRADER system.\nPutting effort into developing and validating a GRADER system offers several benefits. A well-validated grader can quickly adapt to a large variety of domains and dramatically reduce human-effort required to evaluate narratives. It can also be used as part of the finetuning process, as certain LLM optimization functions like bootstrapping few-shot examples optimize based on automated evaluation functions. Finally, and perhaps most importantly, a well-validated grader can be used as a guardrail in live deployment scenarios rather than risking showing inaccurate or low-quality narratives to users, a system can automatically revert to the default, graph-based ML explanation in situations where the generated narrative scores are unacceptable.\nIt is worth noting that in previous work, metrics such as METEOR [29] have been used to evaluate LLM output quality. However, to compute these metrics, we need a ground-truth label for every narrative. In real-world settings, we do not have these labels available, making it impractical in deployment. With GRADER, we alleviate this barrier."}, {"title": "C. Using EXPLINGO for a New Application", "content": "Having defined our two subsystems, we now describe the steps a user will take when applying EXPLINGO to a new ML decision-making problem\u00b9. We will consider a sample application of a user wishing to apply EXPLINGO to the task of house valuation. We assume the user has a ML model trained to predict house prices.\n1) The user generates a set of five SHAP explanations, explaining the model's prediction five (ideally diverse) different houses. For each of these explanations, the user assembles an exemplar by parsing them into the format described in the Explanations box of Fig. 1.\n2) For each exemplar, the user manually writes a short narrative describing the explanation in the desired style. For example, for an explanation of (neighborhood, Ocean View, 12039), (construction date, 2018, 3204), the user may write a narrative like This house is expected to sell for more than average due its prime location in Ocean View and because it was built fairly recently, in 2018.\n3) The user adjusts the weighting of the GRADER metrics according to their needs; for example, they may choose to value accuracy higher than fluency.\n4) The user can now pass explanations into the NARRATOR, and get back narrative versions of these explanations. They can show these narratives to prospective homebuy-"}, {"title": "IV. EXEMPLAR AND EVALUATION DATASETS", "content": "Narratives can be written in numerous ways and the desired style of narratives is highly subjective. Therefore, we introduce the concept of an exemplar dataset, which is a small dataset provided by the user containing high-quality examples of narratives. We steer both the GRADER fluency metric and the NARRATOR based on these exemplars.\nFor this work, we created nine datasets to take exemplars from and to use for evaluations. We started by picking four publicly available ML datasets. We selected these datasets to include a wide variety of contexts ranging from predicting prices of houses to predicting toxicity of mushrooms. From these we created exemplars. To create an exemplar dataset, we pick a data point and form an entry. Each entry represents one model prediction on one data point. Each entry includes"}, {"title": "V. AUTOMATED GRADING OF NARRATIVES", "content": "In this paper, we focus on generating narratives that are optimized on four metrics: accuracy, completeness, fluency, and conciseness.\nScoring conciseness is a straightforward computation based on word-count; the other metrics were graded using a GPT- 40 LLM. We had the LLM grade each narrative 5 times and then return the average grade. To ensure consistency in grades, we would check and alert if fewer than 4 out of these 5 scores agreed; however, this alert was never raised during our experimentation.\nOur total grade G for a narrative is then computed by weighting and aggregating all metrics with the following equation:\n$G = a_aA + a_fF + a_cC + a_sS$\nwhere A is the Accuracy grade, F is the Fluency grade, C is the Completeness grade, S is the conciseness grade, and the $a$ parameters are weighting constants.\nAs described in more depth below, accuracy is scored on 2-point 0/1 scale, completeness is scored on a 3 point 0/1/2 scale, and fluency and conciseness are scored between 0 and 4."}, {"title": "A. Accuracy", "content": "The accuracy metric evaluates if the information in the narrative is correct based on the input explanation. For exam- ple, in an accurate narrative, all feature values, SHAP feature contribution values, and contribution directions (positive or negative) are correct. Narratives that do not mention all fea- tures can still be considered accurate. This metric was scored on a Boolean scale of 0 (inaccurate) or 1 (accurate).\nAccuracy is scored by the GRADER by passing in a prompt to the LLM. This prompt includes the input ML explanation, the explanation format, and the narrative to grade, and asks the LLM whether or not the narrative was accurate."}, {"title": "B. Completeness", "content": "The completeness metric grades how much of the informa- tion from the input ML explanation is present in the narrative. For our experiments, we define a complete explanation as one that 1) mentions all features and their values from the ML explanation and 2) mentions the contribution direction (increasing or decreasing the model prediction) for all features. We note that this is subjective and context-dependent; for example, in some domains, the exact contribution values may be needed. Our prompts use a custom rubric that can be adjusted for different needs.\nCompleteness is scored by prompting an LLM with the explanation, explanation format, and narrative, and asking it to score completeness on a scale of 0 (missing one or more features), 1 (contains all features but missing one or more feature values or contribution directions) or 2 (includes all features, values, and contribution directions).\nLike for the accuracy metric, we created a metric validation dataset of 50 human-labeled explanation-narrative pairs, as summarized in Table II, that included narratives of different levels of completeness.\nAgain, we started with a basic prompt (How completely does the narrative below describe the explanation given?) and passed the dataset entries to the GRADER. We compared the LLM-generated grades to the human-labeled grade for each entry to get a performance score across each level-of-"}, {"title": "C. Fluency", "content": "Fluency is the degree to which the narrative sounds natu- ral or human-written. This is highly subjective and context- dependent; therefore, for this paper we grade fluency as how closely the narrative's linguistic style matches that of a few exemplar narratives. For example, consider the following two narrative styles for the same explanation:\nPrecise: The house's size (1203 sq. ft.) increased the ML output by $10,203. The size of the second floor (0 sq. ft.) decreased the ML output by $4,782.\nCasual: The house's large size increases the price prediction by about $10,200, while its lack of a second floor decreases the prediction by about $4,700.\nUsers pass in a small set of hand-written exemplar narratives that are then used by the GRADER to grade fluency. We prompted the LLM with these exemplar narratives, along with the narrative to be graded, asking it to score how well the narrative matches the style of the exemplars on a discrete scale from 0 to 4. Fig. 4 shows the prompt we used to grade fluency.\nTo validate our fluency scoring function and identify the right number of exemplars to include in the prompt, we used our nine exemplar datasets introduced in Section IV. From each dataset, we randomly selected N hand-written narratives as exemplars and asked the GRADER LLM to compare them to the remaining hand-written narratives. We then computed the mean and minimum difference between fluency scores on narratives evaluated against exemplars from their own dataset and fluency scores on narratives evaluated against other datasets.\nWe compared results from N = [1,3,5, 7]. The results are visualized in Fig. 5. We see that higher numbers of exemplars increases the ability of the GRADER to differentiate narratives of the same style as the exemplars from those in other styles, with diminishing returns after 5 exemplars. We therefore set our GRADER to use 5 exemplars (or as many as are available, if fewer than 5 are provided)."}, {"title": "D. Conciseness", "content": "This metric grades how long the narrative is, scaled such that shorter narratives get higher grades. We believe that, all else being equal, users will tend to prefer shorter narratives. This metric is computed deterministically and scaled to a continuous value between 0 and 4, using the following equation:\n$grade = 0  if L >= 2F_{Lmax} \\ 4x(2 - \\frac{L}{F_{Lmax}}) if F_{Lmax} < L < 2F_{Lmax} \\ 4 if L < F_{Lmax}$\nwhere L is the number of words in the narrative, F is the number of features in the input explanation, and $L_{max}$ is some configured value representing the longest ideal number of words in the narrative per feature.\nIn effect, this equation grades a narrative as 4 if it is shorter than some hyperparameter-defined number of words per feature, 0 if it is longer than twice this value, with grades scaling linearly between these two extremes."}, {"title": "VI. OPTIMIZING THE NARRATOR", "content": "Having validated our metric scoring functions, we sought to improve our NARRATOR to transform explanations into narratives that optimize these metrics. Like with our GRADER LLM, we used DSPy programming to assemble our prompts and used the GPT-40 API for our NARRATOR LLM\u00b3.\nAs shown in Fig. 1 we have a base prompt that instructs the LLM what it needs to do given the three inputs: the explanation, the explanation format, and the context. Next, we show a few base prompts we tried.\nBase Prompts We used the following prompts as a first step for baselines:\nBase Prompt 1: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative.\nBase Prompt 2: You are helping users who do not have experience working with ML understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Make your answers sound as natural as possible.\nBase Prompt 3: You are helping users understand an ML model's prediction. Given an explanation and information about the model, convert the explanation into a human-readable narrative. Be sure to explicitly mention all values from the explanation in your response.\nWe found that the NARRATOR was able to generate reason- ably complete and accurate narratives with these base prompts along with the the other three inputs. For the remaining experiments we only used base prompt 1, as it has the fewest tokens, scored highly on accuracy and completeness in our initial experiments, and does not include any information to suggest a specific style.\nHowever, without exemplars of what a good narrative looks like for a specific application, the NARRATOR is not able to customize the narrative style and length. For example, for an explanation from our House 1 dataset, the generated narrative using just the base prompt looks like \"The model predicts the house price based on several key features. The above ground living area, which is 2090 square feet, contributes significantly to increasing the price by 16382.07 units.\" For this dataset, based on the hand-written exemplar narratives, we were seeking a narrative style more like \u201cThe relatively larger above-ground living space in this house increased its predicted price by about $16,000.\"\nAs described in Section IV, we selected five hand-written narratives for each exemplar dataset. We next consider if we can add these to the prompt to better customize narrative style.\nHand-Written Few-Shot. In this experiment, we add a small number (denoted as H) of exemplar hand-written narra- tives to the prompt. These narratives were randomly selected for each prompt from the hand-written narratives we have for the exemplar dataset.\nBootstrapped Few-Shot. In the previous experiment, we are limited to the hand-written narratives available to us. Additionally, these exemplars may not be complete or accurate. We therefore used DSPy's BootstrapFewShot to create more exemplar narratives that score highly according to our GRADER. To do this we give the bootstrapper:\nour hand-written narratives as a starting point;\na requirement that exemplars it produces achieve a perfect score on accuracy, completeness, and fluency and at least 3.5 on conciseness as measured by our GRADER4."}, {"title": "VII. RESULTS", "content": "We now evaluate the quality of narratives generated by our NARRATOR, and investigate how the exemplars provided influence these results.\nExperimental Setup. Using the NARRATOR we created narratives for explanations in the exemplar datasets. We used a variety of settings for the narrator: 3 base prompts and 11 few-shot (exemplar) settings. In total, we had 169 explanations (across 9 datasets) and applied 13 techniques, resulting in 2,197 total narratives. We scored each narrative on each of the four metrics described in Section V. For the conciseness metric, we set the maximum input length to 90% of the longest feature description from the exemplar narratives. We also used these narratives (5 per dataset) to grade fluency. The mean metric grades from our evaluation by base-prompt/few-shot setting are shown in Table IV.\nAdding few-shot examples (exemplars) greatly improves narrative style at a small cost to correctness. In Fig. 6, we visualize the impact that the number of exemplars has on each metric. We see a clear trend of fluency and conciseness grades increasing with more exemplars, while accuracy and completeness grades decrease. These findings are not surprising without an exemplar, the NARRATOR has nothing to match to improve fluency and conciseness. On the other hand, more exemplars introduces more complexity that may increase the chance of the NARRATOR hallucinating, and increase the chance of confusing the NARRATOR with a potentially inaccurate exemplar narrative. The fact that we see high accuracy and completeness scores without any exemplars demonstrates that the LLM is able to complete the task with careful prompting out-of-the-box; adding exemplars mainly serves to adjust the style and structure of the narrative.\nWe see the overall highest quality narratives with a small number of hand-written and bootstrapped exemplars. Our highest total grade across all four metrics was achieved with one hand-written few-shot exemplar and three bootstrapped exemplars. Having one of each style of few-shot exemplar achieved a close second-highest score.\nBootstrapped exemplars contribute more to generating accurate and complete narratives. We also see a subtle trend in Table IV and Fig. 6 of bootstrapped exemplars im- proving accuracy and completeness over purely hand-written exemplars. This may be because bootstrapped exemplars must achieve perfect accuracy and completeness scores to be passed"}, {"title": "VIII. DISCUSSION AND FUTURE WORK", "content": "This work represents a first step into transforming ML explanations into narratives, and suggests several future direc-"}, {"title": "B. Lessons from Adjusting the GRADER", "content": "In past work [3], we suggested that the accuracy metric be graded on a 3-point scale of inaccurate, accurate but mis- leading, or accurate. However, discussion and experimentation revealed that \u201cmisleading\u201d in this context is a highly subjective specifier, which makes it difficult for both human and LLM graders to give consistent grades. We therefore settled on a boolean 0 or 1 grade, which was sufficient for our purposes.\nWe originally intended for the fluency metric to broadly describe the degree to which the narrative sounds natural or human-written, fine-tuned from a general-purpose crowd- generated exemplar dataset of narratives on a wide variety of datasets. However, we determined from discussion with diverse ML users that preferred narrative style is highly subjec- tive and context-dependent, and therefore the fluency metric would be far more useful if based on a small, application-specific exemplar dataset."}, {"title": "C. Future Work", "content": "In this work we focused on SHAP contributions; however, by adjusting the input explanation format and some details in the metric rubrics, this work could easily be updated to support other kinds of explanations.\nWhile in this paper we focus on automated grading, a natural next step would be to evaluate how effective the narratives are for real-world decision making.\nIt is essential to emphasize that this work represents only first step toward enabling explanations that offer deeper insights and greater clarity. Our ultimate goal is to develop a system that not only generates understandable narratives but also enhances users' ability to make informed decisions based on those narratives. This goal requires future research focused on providing richer context and deeper insights in the explanations, allowing users to engage more critically with the information presented. One example of such insights includes rationalizing model behavior; for instance, an expla- nation narrative of \"This house is predicted to sell for more because it was built in the year 1930\" can be rationalized as \"In this market, older houses hold a premium because of their better building materials and unique history.\" In cases where the model uncovered unexpected patterns in data, such rationalizations can be valuable for knowledge discovery and decision-making.\nAdditionally, we encountered some challenges where our system could not generate and correctly grade comparative narratives (such as referring to a file as \"larger\" or \"smaller\") because we did not pass in the necessary context information. To address these issues, we plan to explore methods for clearly defining data context. For instance, we could pass statistical benchmarks such as mean feature values or more detailed feature distributions to the LLMs. Developing explanations that correctly use comparative terms may improve narrative fluency, and help users understand why certain characteristics, such as file size, are indicative of potential outcomes, ulti- mately enriching the narrative quality.\nWe determined that finetuning approaches are not required to develop an effective NARRATOR with a sufficiently power- ful base LLM. However, finetuning may be beneficial to allow smaller architectures to perform the task as well as the larger models we worked with. As a first step to trying this, we applied the same final prompt (with 3 labeled few-shot ex- emplars) to a smaller, 7B parameter local model (Mistral-7B- v0.1) to determine how well it could perform out-of-the-box. The model effectively matched style, with average fluency and conciseness scores of 3.91 and 3.81, respectively. However, the small NARRATOR did not always parse the input explanation correctly, resulting in lower accuracy and completeness scores of 0.98 and 1.64 respectively. Since smaller, local models are more practical for many real-world applications, we plan to refine prompts and explore finetuning to improve these results."}, {"title": "IX. CONCLUSION", "content": "This work presents findings in transforming traditional ML explanations into natural-language narratives using LLMs. We developed EXPLINGO, a system for generating narrative versions of explanations. This system includes a NARRATOR that uses LLM's to transform ML explanations into narratives, and a GRADER that automatically grades narratives on metrics including accuracy, completeness, fluency, and conciseness.\nOur experiments show that the NARRATOR, when properly guided by a small exemplar dataset, can generate narratives that score highly on these metrics. We found that incorporating a mix of hand-written and bootstrapped few-shot exemplars improves narrative quality.\nThis work represents a step towards enabling natural question-and-answer conversations between ML models and humans. Future research will continue this thread through exploring additional types of explanations and conducting user studies to validate the effectiveness of narrative versions of explanations in real-world applications."}]}