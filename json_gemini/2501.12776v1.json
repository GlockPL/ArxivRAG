{"title": "Data re-uploading in Quantum Machine Learning for time series: application to traffic forecasting", "authors": ["Nikolaos Schetakis", "Paolo Bonfini", "Negin Alisoltani", "Konstantinos Blazakis", "Symeon I. Tsintzos", "Alexis Askitopoulos", "Davit Aghamalyan", "Panagiotis Fafoutellis", "Eleni I. Vlahogiannis"], "abstract": "Accurate traffic forecasting plays a crucial role in modern Intelligent Transportation Systems (ITS), as it enables real-time traffic flow management, reduces congestion, and improves the overall efficiency of urban transportation networks. With the rise of Quantum Machine Learning (QML), it has emerged a new paradigm possessing the potential to enhance predictive capabilities beyond what classical machine learning models can achieve. In the present work we pursue a heuristic approach to explore the potential of QML, and focus on a specific transport issue. In particular, as a case study we investigate a traffic forecast task for a major urban area in Athens (Greece), for which we possess high-resolution data. In this endeavor we explore the application of Quantum Neural Networks (QNN), and, notably, we present the first application of quantum data re-uploading in the context of transport forecasting. This technique allows quantum models to better capture complex patterns, such as traffic dynamics, by repeatedly encoding classical data into a quantum state. Aside from providing a prediction model, we spend considerable effort in comparing the performance of our hybrid quantum-classical neural networks with classical deep learning approaches. Our results show that hybrid models achieve competitive accuracy with state-of-the-art classical methods, especially when the number of qubits and re-uploading blocks is increased. While the classical models demonstrate lower computational demands, we provide evidence that increasing the complexity of the quantum model improves predictive accuracy. These findings indicate that QML techniques, and specifically the data re-uploading approach, hold promise for advancing traffic forecasting models and could be instrumental in addressing challenges inherent in ITS environments.", "sections": [{"title": "1 Introduction", "content": "Traffic forecasting is a crucial component of modern urban transportation systems, directly influencing traffic management, congestion control, and the efficiency of ITS1,2. Accurate traffic predictions enable timely interventions to prevent traffic congestion, optimize traffic signal timing, and improve road utilization3\u20136. The increasing availability of high-resolution traffic data generated from sensors, connected vehicles, and other sources has led to the widespread adoption of Deep Learning (DL) models such as Long Short-Term Memory (LSTM) networks and Convolutional Neural Networks (CNNs) to address this complex forecasting task7,8. These models have shown significant improvements in accuracy over traditional statistical and machine learning methods but come with limitations, particularly in computational efficiency and the ability to explain intricate relationships within the data.\nA key challenge associated with these DL models lies in their computational demands, which include long training times, high complexity, and difficulties in capturing both the temporal and spatial dependencies inherent in traffic data. As transportation networks grow increasingly complex and the need for real-time forecasting becomes more critical, the search for more powerful and efficient alternatives to classical DL models intensifies. This raises an important question: can emerging quantum technologies help overcome these limitations?"}, {"title": "2 Related Work", "content": "Over the past few decades, significant advancements in telecommunication technology and computing systems have enabled researchers and practitioners in the field of traffic prediction to increasingly focus on DL. This natural progression has been driven by the growing availability of vast amounts of relevant data, further facilitating the adoption and utilization of DL techniques in this domain (e.g.,30,31). One of the primary advantages attributed to DL methods is their notable superiority in prediction accuracy compared to traditional Statistical and Machine Learning approaches32.\nCurrently, in the realm of traffic conditions forecasting, Recurrent Neural Networks (RNNs) and, more specifically, LSTM networks, have emerged as the most widely adopted models. These models are not only popular but are often combined with other architectures to achieve remarkably precise predictions (e.g.,33,34). A drawback associated with these models is that, when dealing with long sequences, their capacity to retain information from distant-past timesteps may diminish, a phenomenon known as the \u201cvanishing gradient issue\u201d\u00b9. Furthermore, while the RNN module and its variations excel in handling time-series problems, they are limited in their ability to capture spatial relationships within traffic data (e.g., 35).\nConversely, CNNs, primarily employed in image recognition and computer vision tasks, are harnessed in traffic forecasting to effectively leverage spatial relationships (e.g.,\u00b9). In implementing CNNs, the road network is typically represented as a 2-dimensional grid, similarly to an image. However, since this representation is effectively static, a common approach is to combine CNNs with RNNs. This combination allows for the exploitation of consecutive images, enabling a better understanding and modeling of temporal dynamics in traffic forecasting (e.g.,36,37).\nRecently, Graph Convolutional Neural Networks (GCNNs) have emerged as an alternative to CNNs. Unlike CNNs, which operate in the Euclidean domain and represent the road network as an image, GCNNs extend the convolution operation to accommodate more general graph-structured data, making them more suitable for effectively representing a road network. The model's input consists of the graph-structured data's adjacency matrix, which not only reflects the nodes' connectivity but may also capture statistical correlations. Additionally, a set of features is provided for each node, such as measured traffic flow, speed, and more. GCNNs have proven to be the current state-of-the-art in traffic prediction and have been prominently employed in several recent and noteworthy research endeavors (e.g.,38-40).\nDespite the high accuracy of Deep Neural Networks compared to previous approaches, their applicability still faces several challenges, which are outlined below.\n\u2022 Significant data requirements \u2013 Deep Neural Network models demand a substantial volume of data encompassing various traffic conditions for effective training and convergence. Moreover, the data must be extensive and diverse enough to ensure that the model's generalization capability remains uncompromised\u00b9.\n\u2022 Extended training time \u2013 The complex architecture of Deep Neural Networks, with numerous layers and numerous hyperparameters, contributes to longer training times compared to traditional statistical and simpler Machine Learning models. Additionally, updating and retraining the models' parameters when new data becomes available is a time-consuming and resource-intensive process (e.g.,35).\n\u2022 Hyperparameter selection challenges - Determining the appropriate number of hidden layers and neurons in each hidden layer in a Neural Network often relies on experience or a trial-and-error process. Having too many neurons per hidden layer leads to an overfitting-prone network and prolonged computational times. Conversely, using too few neurons can compromise prediction accuracy, especially when dealing with a substantial volume of input data. The absence of a definitive solution for determining the optimal architecture remains a persistent issue.\nThese factors explain why research on DL methodologies remains an extremely active field and motivates studies like the current one. Striking a balance between model complexity, computational resources, and time is crucial when deploying Neural Networks in real-world conditions, as these factors can influence the model's accuracy. The aforementioned limitations apply not only to Neural Networks in general but also hold significance in the context of traffic data. Collecting adequate data from the entire road network over an extended period can be challenging. Moreover, the lack of interpretability can restrict the practical applicability of prediction models, as previously noted."}, {"title": "2.2 Quantum computing in transportation", "content": "Quantum computing has the potential to revolutionize transportation by solving computational challenges intractable for classical computers. Over the past few years, quantum computing has garnered significant attention for its potential applications in transportation optimization. Several studies have explored this promising technology for various transportation problems. For instance, the authors in41 examined the future potential of quantum computing in ITSs, highlighting its ability to tackle complex computational problems. Bentley et al.42 demonstrated the application of quantum computing for optimizing transport"}, {"title": "2.3 Quantum Machine Learning", "content": "One of the most plausible candidates for exploiting the practical advantages of quantum computing in the NISQ era is QML46. QML offers a wide range of applications, such as utilizing data-driven approaches to discover quantum algorithms, optimizing quantum experiments, processing classical or quantum information using Quantum Neural Networks (QNNs), and even developing quantum-inspired classical Machine Learning protocols47. Various QML techniques, including QNNs with parameterized quantum circuits and measurements, hybrid quantum-classical schemes, and quantum heuristic algorithms, have been proposed to tackle these tasks48\u201351.\nIn our previous works (see52 and53), we provided a comprehensive discussion on the role of quantum layers in hybrid neural networks and their effect on the learning process. We addressed how QNNs can improve a task's performance by utilizing the inherent advantages of quantum mechanics, such as the ability to process information in more complex ways than classical networks. By increasing the complexity of these quantum layers, the networks could better capture and model intricate patterns in the data, potentially leading to enhanced predictive accuracy.\nHowever, despite the rapid progress in the field, numerous open and challenging tasks remain to be addressed. These include efficient encoding data schemes for quantum processing, improving quantum models, refining training methodologies, enhancing generalization capabilities, mitigating the impact of quantum noise, and more. The present research aims to tackle several ongoing research issues in QML. Drawing on the practical advantages of quantum computing during the NISQ era, we seek to explore innovative solutions for data encoding, quantum model optimization, robust training techniques, and other related challenges. By addressing these issues, we seek to advance the capabilities of QML and unlock its full potential for real-world applications in the NISQ era."}, {"title": "3 Methodology", "content": "The main objective of this study is to compare classical against hybrid quantum-classic Neural Network (NN) approaches in a traffic forecasting task. The underlying goal is to assess the capacity and efficiency of quantum layers in NNs and examine whether such quantum configurations offer tangible advantages over their classical counterparts.\nQuantum layers resemble classic ones in their high-level usage but fundamentally differ in their operational details. At the core of the quantum layer lies a Quantum Variational Circuit (QVC), which is central to the hybrid quantum-classical NN architecture. The QVC is designed to leverage the principles of quantum mechanics and is usually composed by the chaining of three key elements (e.g.,54,55):\n1. Data Embedding \u2013 The data embedding stage is responsible for encoding the classical data into quantum states. Regardless of whether the classical data serves as the network input or comes from the preceding classical layer (referred to as the \"feeding layer\"), the embedding stage encodes this information into qubits. One way to do this is via angle rotation encoding, a technique where classical values are mapped to the rotational angles of qubits. These rotations prepare the quantum state that subsequent quantum operations will process. The role of this encoding step is crucial, as it directly impacts the ability of the quantum circuit to capture the nuances of the data.\n2. Entangling stage - Following the angle embedding, the qubits undergo a series of operations in the entangling layer. As an example, in our models (see Sections 3.2.1 and 3.2.2), we will use rotational gates and Controlled-NOT (CNOT), which are building blocks for creating quantum entanglement. The CNOT gates generate entanglement between pairs of qubits, allowing the quantum layer to capture complex interdependencies within the data. The rotational gates, which have trainable parameters, adjust the quantum states further, based on the encoded data. The entangling process is a key aspect of quantum computing that enables the model to explore a much larger solution space than classical methods could, potentially leading to better generalization and more accurate predictions.\n3. Measurement Stage The final component of the QVC is the measurement stage, where the quantum states are measured to extract classical information from the quantum layer. Measurement collapses the quantum states into classical bits, which are then passed to the next layer in the NN. This step bridges the quantum and classical parts of the hybrid model, allowing the information processed by the quantum layer to inform the subsequent operations in the classical layers. The measurement quality directly affects the quality of the information the classical network receives, making it a critical part of the quantum-classical interface.\nThese operational peculiarities of the quantum layers suggest that conducting a classical vs. hybrid model comparison is a challenging task. In practice, this difficulty boils down to the issue that the classical equivalent to a qubit \u2013 the fundamental computational unit in quantum architectures \u2013 is not easily defined, if it can be defined at all. In fact, the two computational techniques are intrinsically different in both the way they store and process the information. Therefore, when performing such comparisons, one has to decide on which concept the classic-hybrid equivalence shall be based on. In this study, we opted to contrast the capabilities of classical and quantum networks whose complexities have been deemed comparable under the principles of A) size of encoded information, and B) number of recursive iterations. Namely, to explore these two scenarios, we designed two distinct experiments in which we replaced a layer of completely classic NNs with their quantum equivalents. In the first case, we focus on NNs based on fully connected layers, while in the second, we explore recursive NNs."}, {"title": "3.2.1 Scenario A: Layer replacement based on equal amount of encoded information", "content": "In this scenario, we consider an NN in which the burden of handling the regression task falls primarily on the Fully Connected (FC) layers, which serve as the core computational components responsible for processing and refining the input data. For a detailed overview of this scenario's architectures, refer to Figure 3, which illustrates the structure and interplay of these FC layers within the broader network.\nIn this context, we aim to explore the effects of substituting a single, conventional, feed-forward FC layer of an NN with a quantum layer designed to withhold an equivalent amount of information. The underlying premise is that a quantum system, through the use of qubits, has the potential to exploit higher-dimensional embedding spaces. Specifically, $N_q$ qubits create an embedding space of dimensionality $2^{N_q}$, allowing us to theoretically replace a classical layer comprising $N_a$ neurons with a quantum layer consisting of approximately $\\sqrt{N_q}$ qubits. However, for the practical implementation, we reverse this approach (without loss of generality): we begin by constructing a quantum layer with $N_q$ qubits and subsequently compare its performance against a classical layer containing $2^{N_q}$ neurons.\nExploring the performance of FC layers in a time series example is challenging because FC layers fall short when it comes to handling temporal dependencies. This makes them less suitable for tasks where the sequence and timing of data play a crucial role, as they do not inherently account for the dynamic patterns and interactions that unfold over time. To fight this limitation, we can introduce a form of preprocessing that precedes the FC layer and is capable of capturing the temporal dependencies. Specifically, we adopted an autoencoder architecture based on LSTM cells. After training, the encoder is able to process input sequences, reduce their dimensionality, and, crucially, account for time-dependent relationships. We stress that, while dimensionality reduction is not strictly necessary in this stage \u2013 its main objective being to capture the temporal properties \u2013 it becomes a beneficial side effect, as it enhances computational efficiency by reducing the overall complexity of the input, thereby speeding up subsequent calculations.\nThe autoencoder component is shown in Figure 4, while the complete pipeline architecture is displayed in Figure 3, and works as follows. Every \"input sequence\" ($x_1, x_2..., x_w$), where w is the window size (in our case, 20 timesteps), is first parsed through the pre-trained encoder to produce a compressed \u201cembedded sequence\u201d ($e_1, e_2..., e_{N_q}$) of size $N_q$. The embedded sequence is then passed to the regressor, which, as described above, can be the classical regressor, in which the first layer is an FC layer composed of $2^{N_q}$ neurons, or the hybrid regressor, in which the first layer is a quantum layer composed of $N_q$ qubits. The final output is, either way, a single classic neuron with a linear activation function, as the NN is designed to provide for the next-in-time prediction."}, {"title": "3.2.2 Scenario B: Layer replacement based on equal amount of recursions", "content": "In this scenario, we attempt to go beyond fully connected layer architectures and use recursive layers for the regression task. Specifically, we design the regressor, which contains an LSTM cell, and compare it against a regressor in which this element has been replaced by a quantum layer characterized by multiple data re-uploading.\nData re-upload27 is a technique initially designed to enhance the learning capacity of the model without increasing circuit"}, {"title": "3.2.3 Further insights on the quantum layers", "content": "Simulating quantum layers on classical hardware is a computationally intensive task, especially as the number of qubits increases. Simulating more than 10 qubits can quickly become unfeasible on standard personal computers, making it necessary to strike a balance between the model's complexity and the computational resources required. For this reason, we kept the quantum layers relatively small in terms of qubits ($N_q \\le 14$), acknowledging that this choice might come at the expense of the model's potential performance. However, as demonstrated in our results (Section 4.2), our model architecture achieved extremely high-performance scores, primarily due to the abundance of training data available.\nThe quantum layers were implemented using Pennylane\u00b2, a Python-based tool specifically designed for quantum machine learning and optimizing hybrid quantum-classical computations52,57. Pennylane provides the necessary infrastructure to simulate quantum circuits on classical computers, making it possible to explore the potential of quantum computing in practical applications, even without access to physical quantum hardware.\nNote that, in the remainder of this manuscript, the classic-hybrid architecture pairs will be identified by the number of qubits in the hybrid neural network (e.g., \u2018Q2' indicates 2 qubits), with the number of neurons in the corresponding layer of the classic NN being derived as described in Sections 3.2.1 and 3.2.2."}, {"title": "3.3 Training", "content": "The NNs described earlier are designed to predict the value of the series at a specific timestep t + 1 based on the values from the previous w timesteps ($x_{t-w}, x_{t-(w-1)}, ..., x_t$). To achieve this, the dataset is divided into \u201cwindows\u201d, which are small sequences of w data points, with each window serving as a predictive variable array. The next data point in time, immediately following each window, is the target variable the model attempts to predict.\nThe training process involves feeding these windows and their corresponding target values into the network in mini-batches. This approach helps stabilize and accelerate the learning process, especially when dealing with large datasets. An Adam optimizer with a learning rate of 0.0005 has been adopted to minimize the Mean Squared Error (MSE) \u2013 the loss function \u2013 helping the network learn the mapping between input sequences and their corresponding targets. The prediction stage follows the same rationale, with the t + 1 test data point being predicted given the previous w values."}, {"title": "3.4 Performance Evaluation", "content": "Regarding the testing, we strived to obtain an unbiased estimate of the uncertainty about the results in order to quantify the difference between the NNs. In time series forecasting modeling, it is common to segregate a test set that comes temporally after the training set. This approach is most diffused because of the implicit assumption that time series data points are timely correlated and potentially causally correlated. It follows that the value of the series at a given time t is strongly associated with that at time t - 1.\nHowever, while it is not controversial to segregate a single hold-out test (at the end of the series) for an individual assessment, placing an uncertainty on this quantity is not straightforward. In fact, this requires performing multiple train/test splits \u2013 hence raising the question: where should the different test sets be selected along the time series? In tabular data multiple testing is readily solved by using cross-validation (CV), in which the data ensemble is split in multiple folds, and one of the folds plays the role of test set (while the rest play the training), until all permutations are covered. However, because of the aforementioned time-dependency issues, the same approach is not deemed applicable right away to time series, and, in general, there seems to be an open debate on what the best solution might be (see, e.g., ,58 and references therein).\nIn this study, we opted for a 5-fold gap-cross-validation technique59 via the GapKFold\u00b3 Python implementation60. This protocol is similar to the CV, with the significant difference that there are \"gaps\" (i.e., discarded data) on both sides of the test fold. In other words, at each CV permutation, part of the data contiguously preceding and following the test fold are ignored."}, {"title": "4 Results", "content": "Two main considerations emerge from this figure. In the first place, we notice that both fully connected and recursive architectures manage to converge within the 20 epochs we experimented on, and moreover converge to very similar scores. This relatively rapid convergence is due, more than to a complex network architecture, to the extremely large wealth of data available for our series. We had about 40 000 data points, which we split into windows of 20 data points each, ultimately resulting in approximately 800 batches (composed of 32 windows each) per epoch. Regarding the similar score values, consider that this dataset exhibits an extremely regular pattern, and that the task is a next-in-time forecasting challenge, where the goal was to predict a single future data point per window. These factors greatly simplified the forecasting problem, thereby explaining the consistently high performance across all NNs. However, note that while the curves of all recursive models (except for Q2) flatten at around epoch 5, the curves for the fully connected architectures present a larger spread.\nThis is connected with the second observation, i.e., that the hybrid models in fully connected architectures converge way slower than their corresponding classical ones. This trend is reversed in the recursive architectures where, although less significantly, the hybrid models converge faster than their counterparts, especially for smaller numbers of qubits."}, {"title": "4.2 Performance Scores", "content": "As assessment metrics, we considered a collection of commonly adopted regression metrics, namely MSE (also used as a loss function, see Section 3.3), Mean Absolute Error (MAE), and R2 (coefficient of determination)."}, {"title": "4.3 Consistency Check", "content": "Figure 9 shows the measurements relative to the different metrics, estimated over the test folds of the CV loop, for all the fully connected architectures (Scenario A). From the figure it emerges that, independently of the metric, the discrepancy between classical and hybrid architectures becomes more significant as the number of qubits(/neurons) decreases (except for the case of Q2, which is arguably dominated by outliers, as demonstrated by the sudden drop of performance even for the classic-model). In particular, we observe that the classic/hybrid scores start to be compatible, i.e. within their respective uncertainties, from 10 qubits onward.\nFigure 10 shows the same measurements, this time estimated for the recursive architectures (Scenario B). In this case, we observe that the hybrid architectures outperform the classical counterparts for 6 qubits or more; moreover, they present lower dispersions, indicating that they are more consistent with respect to the variation of the train/test sets.\nFigure 11 explores whether 'peeking into the future' \u2013 i.e., placing the test set before the training set \u2013 results in better performances, as would be anticipated if a bias was indeed present. In such a case we would expect a decline in performance from left to right on the plot, since the GapKFold protocol positions the test set before the training set in earlier iterations (and gradually shifts it to the end in later iterations). The figure indicates instead that the scores are largely independent of the iteration loop, oscillating around an average value. This allows us to rule out any significant flaws in our assessment protocol."}, {"title": "5 Discussion", "content": "The most immediate result of our investigation \u2013 as it readily emerges by the comparison between the left and right panels of Figure 8, and the comparison between Figures 9 and 10 \u2013 is that quantum-based NNs do not appear convenient in fully connected architectures, but do offer advantages in recursive architectures (i.e., when data re-upload is involved). Granted, this conclusion is valid under the assumptions of architecture equivalence defined in Sections 3.2.1 and 3.2.2.\nSurprisingly, despite data re-uploading having operational similarities with RNNs (see Section 3.2.2) \u2013 an industry-standard in time series analysis (see, e.g.,61 for quantum-LSTMs) \u2013 there is little literature exploring its application in this context. To the best of our knowledge,62 is the only work applying data re-upload on a pragmatic time series example. In this work, the authors study a hybrid encoder-decoder and show that it is superior, in terms of performance, to its classical \u2018equivalent'. In62, the classical equivalent is a Seq2Seq architecture63, while the hybrid model is composed by the same Seq2Seq extended by a quantum circuit including data re-upload.\nIn our Scenario B approach, we expand on their methodology by a) basing the comparison on recursive loops rather than network shape, and b) keeping the classical component (in our case, the LSTM autoencoder) as small as possible, using only the encoder and solely for data compression. These improvements allowed us to focus more clearly on the individual contribution of the data re-upload layer. Additionally, we explored a range of data re-upload iterations (from 2 to 14), emphasizing the novel exploitation of this technique: its usage as a memory cell \u2013 rather than merely as a method to enhance a layer's expressivity, as it has so far been employed in regression/classification tasks.\nSecondarily, we observe that \u2013 regardless of the comparison scenario \u2013 the relative performance of the classic NNs with respect to their hybrid counterparts rapidly declines as network size increases. Specifically, as the number of qubits increases, the hybrid NNs in fully connected scenarios catch up with the classical ones (Figure 9), whereas in recursive architectures, the hybrid NNs progressively diverge towards better performances (Figure 10). Moreover, we generally observe that this relative performance is non-linear with respect to the number of qubits, evolving more rapidly for lower qubit counts.\nAdding to these considerations, we draw again attention to how the training curves of Figure 8 all flatten out before the limiting 20 epochs are reached. This provides an indication that the aforementioned performance differences are not due to a lack of convergence but are indeed related to the models' generalization prowess. However, focusing on the fully connected architectures (Scenario A; left panel) we discover that, while the classical models basically converge within 3-4 epochs, the hybrid models take progressively longer for decreasing qubits counts. This further suggests that the issue relies on an inferior modeling power of hybrid fully connected architectures. In contrast, the hybrid architectures exploiting data re-upload (Scenario B; right panel) seem to provide the ideal trade, as the models converge within 5 epochs while reaching comparable or even better performances than the fully connected hybrid NNs.\nIt is worth noting that these figures are based on simulations of quantum neural networks on conventional hardware, where data re-upload is significantly more computationally expensive. In our experiments, training a quantum model with $N_q$ qubits took approximately two times longer for each data re-uploading block involved (e.g.,64). Nevertheless, our results offer valuable insights into the resource demands and optimization strategies necessary when implementing quantum algorithms on actual quantum computers."}, {"title": "6 Conclusion and Future Directions", "content": "In this paper, we present one of the pioneering efforts to explore and quantify the impact of quantum computing and quantum machine learning on traffic forecasting. Our results show that a relatively simple quantum-hybrid model can achieve performance levels comparable to an optimized classical LSTM-based network, though with higher computational complexity. Most importantly, our findings arguably provide one of the first pieces of evidence suggesting that more sophisticated quantum models, particularly those incorporating data re-uploading, have the potential to surpass classical approaches in terms of performance in the field of time series analysis. Due to current hardware limitations, we could not fully evaluate these more advanced quantum models for a large number of qubits, but exploring their performance remains a key focus for future research.\nThis work lays the groundwork for the application of Quantum Machine Learning in traffic forecasting and other areas of transportation engineering. The methodologies and insights derived from this study pave the way for further exploration and optimization of quantum-based models for a range of transportation-related challenges. In these regards, we identify several important avenues to extend our studies in the future. For instance, quantum systems are inherently sensitive to errors and noise, which pose significant challenges to the scalability of quantum computers. This sensitivity impacts both qubits and quantum gates, leading to potential inaccuracies in computations. To study the impact of noise on quantum systems, researchers often use models such as the Lindblad master equation65 or Kraus operators66. The Lindblad equation describes the dynamics of open quantum systems under the Born-Markov approximation, while Kraus operators provide a general framework for representing noise channels affecting density matrices. In simple terms, a noise channel is a linear map that transforms density matrices to other set of density matrices, capturing the effects of decoherence and imperfections in quantum systems. Since most of our codes have been written using the open-source package PennyLane67, we provide general remarks on how to implement noise within this framework. Pennylane offers several methods for simulating noise in quantum circuits. These include classical parametric randomness, the built-in default.mixed device, and plugins for interfacing with other platforms like Cirq and Qiskit. These tools allow researchers to explore how noise affects quantum algorithms and to develop strategies for mitigating"}]}