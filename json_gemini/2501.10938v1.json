{"title": "Blockchain-assisted Demonstration Cloning for Multi-Agent Deep Reinforcement Learning", "authors": ["Ahmed Alagha", "Jamal Bentahar", "Hadi Otrok", "Shakti Singh", "Rabeb Mizouni"], "abstract": "Multi-Agent Deep Reinforcement Learning (MDRL) is a promising research area in which agents learn complex behaviors in cooperative or competitive environments. However, MDRL comes with several challenges that hinder its usability, including sample efficiency, curse of dimensionality, and environment exploration. Recent works proposing Federated Reinforcement Learning (FRL) to tackle these issues suffer from problems related to model restrictions and maliciousness. Other proposals using reward shaping require considerable engineering and could lead to local optima. In this paper, we propose a novel Blockchain-assisted Multi-Expert Demonstration Cloning (MEDC) framework for MDRL. The proposed method utilizes expert demonstrations in guiding the learning of new MDRL agents, by suggesting exploration actions in the environment. A model sharing framework on Blockchain is designed to allow users to share their trained models, which can be allocated as expert models to requesting users to aid in training MDRL systems. A Consortium Blockchain is adopted to enable traceable and autonomous execution without the need for a single trusted entity. Smart Contracts are designed to manage users and models allocation, which are shared using IPFS. The proposed framework is tested on several applications, and is benchmarked against existing methods in FRL, Reward Shaping, and Imitation Learning-assisted RL. The results show the outperformance of the proposed framework in terms of learning speed and resiliency to faulty and malicious models.", "sections": [{"title": "I. INTRODUCTION", "content": "The recent advancements in Deep Learning (DL) and its integration in Deep Reinforcement Learning (DRL) have shown great promise in generating intelligent agents for applications such as robotics [1], environmental monitoring [2], [3], and video games [4]. In DRL, an agent builds its own intelligence by learning a policy for decision-making based on its experience in the environment, guided by a numerical reward that the agent tries to maximize. DRL is further extrapolated into Multi-Agent Deep Reinforcement Learning (MDRL), in which multiple autonomous agents cooperate or compete to achieve their objectives in a common environment. MDRL has received increasing interest in the past few years, especially in fields related to video games [5], autonomous driving [6], and robot swarms [7].\nDespite its great achievements, DRL comes with several challenges that hinder its usability. One challenge is sample efficiency, which refers to the difficulty of learning from insufficient interactions with the environment, due to the high cost of collecting such interactions. This problem has been recently tackled by several works using Federated Reinforcement Learning (FRL) [8]\u2013[10], in which multiple agents learn from their local experiences and periodically share their models to be aggregated in a global model, which is then shared to all agents. Another challenge associated with DRL, and amplified in MDRL, is the curse of dimensionality, which refers to the increasing size and complexity of the state and action spaces with the increasing number of agents [11]. This challenge is further amplified with the use of sparse rewards, which are rewards that are rarely present in the state space of the environment. One common sparse reward function is to assign a single reward value only when the task is successfully completed, such as delivering a checkmate in a game of chess or destroying the opponent team's Ancient in a game of Dota. This increases the difficulty of the learning, as the agents are rarely exposed to the reward, especially in the early stages of the learning where the agents act randomly in the environment to collect experiences. This has been addressed through reward shaping, in which specific reward functions are designed for each problem with the aim of exposing the agents to the reward frequently in the environment [12]\u2013[14].\nThe existing methods in the literature suffer from several drawbacks when tackling the aforementioned challenges. While FRL helps in addressing the sample efficiency issue, faulty and inaccurate shared models could adversely impact the aggregated model (backdoor poisoning attacks), leading to severe difficulty in learning [15]. Additionally, in applications where the models are represented as deep neural networks (DNN), the architecture of the DNN might vary from one user to the other. In FRL, the global model is obtained by averaging the shared models, which cannot be simply achieved if the models are of different architectures, and would require additional steps (such as knowledge distillation) that increase the computational overhead. This constrains all the FRL nodes to use the same DNN architecture, which is inefficient as some nodes might have capabilities to train more complex architectures than other nodes. In knowledge distillation, a smaller model (student) is trained to mimic the behavior of a larger and more complex model (teacher). While this allows different DNN architectures to learn from each other, knowledge distillation algorithms usually train both the student"}, {"title": "II. RELATED WORK", "content": "This section discusses existing literature tackling the MDRL challenges in data efficiency and reward sparsity, namely Federated Reinforcement Learning, Reward Shaping, and ILassisted RL."}, {"title": "A. Federated Reinforcement Learning", "content": "Federated Reinforcement Learning (FRL) brings RL into the realm of FL. FRL aims to build a better policy from multiple RL agents without requiring them to share their raw experiences. Several works have adopted FRL, especially in the domain of MDRL, with the aim of increasing the sample efficiency of the training process, by aggregating models from different users trained on different experiences. In mobile edge computing, the authors in [22] propose a multi-agent framework for data offloading. The problem of data allocation is formulated as a multi-agent Markov Decision Process (MDP), and a joint cooperation algorithm that combines the edge federated model with the multi-agent RL is proposed. Another work in [23] proposes a FRL framework for collaboration among edge nodes to exchange learning parameters, with the aim of better training and inference. FRL has also been combined with Blockchain, where the authors in [24] propose a framework that trains DRL models for computation offloading and resource allocation in 5G ultra-dense edge computing networks. The DRL models are trained in a distributed manner via a FL architecture, in which the communication is done securely over the Blockchain. In robotics applications, the authors in [25] propose a FRL architecture for cloud robotic technologies, in which a shared model on the cloud is upgraded with knowledge from different robots performing autonomous navigation. In autonomous driving, the authors in [26] propose an online FRL transfer process for real-time knowledge extraction, where agents take actions based on their own knowledge and the knowledge shared by others.\nDespite the several advantages of FRL, it comes with several drawbacks. In a realistic scenario, if the models are trained in environments that inherit different dynamics, the learning convergence of the aggregated model could face issues [17]. Additionally, in most FRL frameworks, the global model is obtained by averaging the shared models, which requires all the models to have the same architecture in the case of neural networks. While this could be tackled by other additional steps, such as model compression (knowledge distillation), it introduces additional overhead and risk of losing information. Moreover, FRL is vulnerable to random failures or adversarial attacks, in which the shared models give harmful behavior that could affect the aggregated global model [8]."}, {"title": "B. Reward Shaping", "content": "In DRL, a sparse reward is a case where the environment rarely produces a useful reward signal. Sparse rewards are the easiest and most common form of rewards, as the desired goals in most applications naturally induce a sparse reward, such as achieving a checkmate in chess. However, due to the complexity of DRL problems, especially in MDRL, sparse rewards induce difficulty in learning, especially during the exploration stage where the agents initially act randomly in the environment and barely collect rewards. Several works have introduced shaped reward functions, which distribute the reward over the course of the learning. One common form of shaped rewards are distance-based rewards, where agents get rewarded in each step of an episode if they get closer to achieving the goal. For example, in the problem of target localization [7], [27], [28], agents get rewarded at each step if they get closer to the target, where measures like Euclidean Distance, Manhattan Distance, or Breadth-First Search are used in each step to compute the distance. In [29], the authors tackle the problem of Hide-and-Seek, where a vision-based reward is designed that rewards seekers in each step if they keep hiders within their sight, and rewards hiders in each step otherwise. Other proposed reward shaping methods alter the original reward with values generated from a shaping function. The authors in [13] propose a scheme for reward shaping based on Graph Convolutional Recurrent Networks to predict and produce reward shaping values. Another work in [30] proposes a reward shaping method based on Lyapunov stability theory, which tempts the RL process into maximal reward region by driving the reward to make the Lyapunov function.\nDespite the fact that reward shaping is seen to speed up the learning in DRL and MDRL, designing shaped rewards requires considerable engineering, and could still lead to local optima [18], [19]. Additionally, many of the shaped rewards or reward shaping methods are computationally expensive, such as search methods or graph neural networks, which adds additional overhead to the learning."}, {"title": "C. Imitation Learning-assisted RL", "content": "A recent alternative method to speed up the learning in RL, instead of reward shaping, is to combine RL with Imitation Learning (IL). In this approach, previously obtained experts (or expert demonstrations) are used partially to help in training new agents. Here, the demonstrations are used in a supervised learning method where the goal is to minimize the loss between the agent's actions and the expert's demonstrations. During IL, the agents do not collect rewards, and the learning is entirely based on the expert's demonstrations which act as labeled data in supervised learning. In [18], [19], the authors propose methods that alternate between RL and IL for offpolicy RL. In their case, the expert demonstrations are stored in a buffer, and for a portion of the RL period, the RL agent is trained with Behavioral Cloning (BC), where the aim is to exactly mimic the behavior of the expert with no reward feedback. The authors in [31], [32] extrapolate these works into MDRL, where agents alternate between MDRL and behavioral cloning (IL) from an expert."}, {"title": "III. MDRL WITH MULTI-EXPERT DEMONSTRATION CLONING", "content": "The proposed MEDC method utilizes previously trained expert models to guide the learning for the current MDRL system. This section first formulates the MDRL problem, then defines the MEDC method."}, {"title": "A. MDRL Formulation and Policy Optimization", "content": "MDRL is generally formulated as a Markov Game [33], which generalizes the Markov Decision Process (MDP) from single-agent to multi-agent settings. In most MDRL problems, the agents cannot observe the full state of the environment, and hence the problem is modeled as a Partially Observable Markov Game (POMG). Each agent acts in the environment using a policy that translates the agent's observations into actions, with deep neural networks (DNNs) being the most common methods to represent policies [11]. The learning in a POMG unfolds over a finite sequence of steps, where at every step, each agent i analyzes its observation $o_i$ and takes action $a_i$ based on a policy $\\pi_i$ and receives a reward $r_i$. In MDRL, all agents at a certain step simultaneously select an action $a = (a_1, ..., a_N)$ and receive a reward and observation $o = (o_1,..., o_N)$. The objective for each agent is to maximize the expected sum of rewards it receives during the game.\nProximal Policy Optimization (PPO) [34] is used in this work as the RL algorithm due to its simplicity, low computational complexity, and balance between sample efficiency and wall-clock time. PPO uses an actor-critic structure, where an actor policy $\\pi_\\theta$, parametrized by $\\theta$, is to be optimized with the objective of maximizing the cumulative reward throughout the episode, which is estimated through the value function (critic). PPO uses the current experiences, combined with the critique, and tries to take the biggest improvement step to update the current policy, without moving far from it. This tackles instability issues in policy gradient methods due to large policy updates. PPO optimizes a clipped policy surrogate, $L^{CLIP}(\\theta)$, which is parameterized by $\\theta$ and is given as\n$L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon)\\hat{A}_t)],$  (1)\nwhere $\\hat{E}_t$ is the expectation operator at step t. The clip() function is responsible for clipping the value of $r_t(\\theta)$ to be within the range of $[1 - \\epsilon, 1 + \\epsilon]$. Here, $\\epsilon$ is a clipping hyperparameter that determines the amount of clipping, which usually has a value of 0.2 [34]. In this function,\n$r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t s_t)} $ (2)\nis the probability ratio of taking an action between the old policy $\\pi_{\\theta_{old}}$ and the current policy $\\pi_{\\theta}(a_t|s_t)$. $\\hat{A}_t$ is the advantage function which estimates how good a certain action is in a given state. In this work, Generalized Advantage Estimate (GAE) [35] is used to estimate $\\hat{A}_t$. After each H timesteps of experience collection, where H is the horizon length, the estimator of the advantage function is then given as\n$\\hat{A}_t^{GAE(\\gamma, \\lambda)} = \\sum_{l=0}^{H} (\\gamma\\lambda)^l \\delta_{t+l}, \\delta_{t+1} := r_{t+1} + V(s_{t+1} + 1) - V(s_{t+1}),$  (3)\nwhere $\\delta_{t+1}$ is the Temporal Difference (TD) residual, $\\gamma,\\lambda \\in [0, 1]$ are discount factors, and V(st) is the value predicted by the critic for the state in step t. The authors in [34] use additional parameters in Eq. 1 using entropy and value function error to ensure sufficient exploration and stable learning.\nThe general learning process used in this work is shown in Algorithm 1. In this work, we use a Centralized-Learning & Decentralized-Execution method, which is common in MDRL problems [29]. The learning unfolds over a set of episodes, where each episode represents a new instance of the problem. At the beginning of each episode, the environment resets to the initial state, and the agents collect the initial observations (o0 = (o0,..., o0N)). In episode step j, each agent k uses their own observation (ojk) in their copy of the actor network to get a probability distribution over the possible actions (Pk), from which an action (ajk) is sampled. All agents then execute their joint actions (a) in the environment to collect and store the next observations (oj+1), the reward value (rj), and a termination flag (fj) indicating if the episode is finished. The process is then repeated for the following episode steps, where the agents act based on new observations and collect and store new rewards and termination flags. If an episode terminates (fj equals 1), then a new episode is initiated with a reset. If the total number of training timesteps reaches the horizon (H), the collected experiences (observations, rewards, flags) are used to compute A and update the actor and critic networks using $L^{CLIP}(\\theta)$. The process repeats until the specified total number of training timesteps is met."}, {"title": "B. Multi-Expert Demonstration Cloning (MEDC)", "content": "The main idea of the proposed MEDC method is to use demonstrations from experts to guide the MDRL agents into collecting better experiences. In MDRL environments with sparse rewards, better experiences are defined as ones where the agents are more frequently exposed to the sparse reward. In this work, and for a given problem of interest, an expert is defined as a previously trained model with expertise in tackling the exact same environment. On the other hand, a semi-expert is one with expertise in tackling a similar environment that slightly differs in complexity. For example, in the problem of target localization with a team of 5 agents in an area that has 3 obstacles, an expert model is one that has been previously trained on the exact same environment, while a semi-expert model is one that has been trained on a target localization environment with only a single agent. The semiexpert is not fully capable of tackling the current environment (5 agents), but can still provide some guidance that would be better than acting randomly in the environment. One of the main issues faced in MDRL with sparse rewards is the rare occurrence of reward states, which is dominant in the early stages of the learning where the agents' policies produce randomized actions. The proposed method aims at utilizing demonstrations from experts and/or semi-experts to assist a new MDRL process by providing better experiences with more rewards during the learning stage.\nIn this work, the proposed MEDC method utilizes the experiences from multiple experts (or semi-experts) to guide the learning of the new agents, while maintaining the \u201clearning from rewards\u201d concept in RL. In the proposed method, the learning alternates between MDRL and MEDC. During MDRL, the agents take actions in the environment based on their policy networks, and collect rewards accordingly. During MEDC, the experts suggest actions to be followed by the MDRL agents to explore the environment. Here, even though the actions are suggested by the experts, the agents still learn from their own execution of those actions, i.e. based on the collected rewards. As a result, experiences with more frequent occurrence of reward states are expected. This method is resilient to faulty-, malicious-, and semi-experts, i.e. models that might occasionally suggest bad actions, as such actions could be identified through the collected rewards. This is unlike the methods using IL (discussed in Section II-C), in which bad- or semi-experts would significantly deteriorate the performance of the new agents who have no input in deciding whether the suggested actions are good or bad.\nThe learning process of MDRL with MEDC is described in Fig. 2. At the beginning of each episode, an expert probability $R_E$ determines whether the episode will follow MDRL or MEDC. During MDRL (i.e. the switch is closed), the typical process defined in Algorithm 1 is followed, where agents act based on their copies of the actor network. During MEDC, i.e. when the switch is on, one of the experts is selected, which takes the observations seen by the agents and suggests actions. The selection of the expert, out of the available experts, is done using a roulette wheel based on the attribute $R_s$, which is a common way used for selection [36], [37]. In this method, each candidate is associated with a probability which is proportional to $R_s$, where experts with higher $R_s$ have higher probability of being selected. After arranging the experts on a roulette wheel according to their probabilities, the wheel is spun to randomly select an expert. In this work, the attribute $R_s$ reflects a similarity measure between the environment of the expert and the current environment of interest. The method of computing $R_s$ depends heavily on the application. Some applications favor the number of agents as a measure of similarities, where environments with similar number of agents are favored, while other applications have problem-specific attributes related to the environment dynamics. In this work, we use a QoS (Quality of Service) metric to obtain the value of $R_s$, which is later discussed in Section V-A. Experts trained on environments/problems similar to the one of interest are more probable to be selected than ones trained on slightly different environments. Experts with lower similarities are still desired, as they introduce some variance in the experiences collected, which could be beneficial to the learning process. The actions suggested by the expert are then followed by the agents and the corresponding experiences are stored. This method is resilient to faulty-, malicious-, and semi-experts, i.e. models that might occasionally suggest bad actions, as such actions could be identified through the collected rewards."}, {"title": "IV. BLOCKCHAIN-BASED MODEL SHARING FOR DEMONSTRATION CLONING", "content": "This work proposes a Blockchain-based framework that complements the MEDC method. The framework is responsible for managing the users' registration, model submission, and appropriate model allocation to requesting users. A Consortium Blockchain is used due to its ability to offer increased privacy, shared control, efficiency, cost savings, and trust for multiple organizations or entities collaborating on a project or sharing data [38]. A Consortium Blockchain is operated by a group of entities, which introduces increased privacy and trust when compared to public Blockchains, and more collaboration allowance when compared to private Blockchains. This makes Consortium Blockchains suitable for data sharing across entities, and hence suitable for the purpose of model sharing in the proposed framework."}, {"title": "V. SIMULATION AND EVALUATION", "content": "This section presents and discusses different experiments conducted to validate the proposed methods. The experiments are first conducted in a custom environment for a task of Target Localization, which is a complex multi-agent problem requiring agents to cooperate in finding the target location. Subsequently, the adaptability of the proposed method is tested across two additional typical Multi-Agent applications, which are Fleet Coordination for Autonomous Vehicles [43], [44] and Multi-Agent Maze Cleaning [45]. All the simulations have been conducted using an Intel E5-2650 v4 Broadwell workstation equipped with 128 GB RAM, 800 GB SSD, and NVIDIA P100 Pascal GPU (16 GB HBM2 memory)."}, {"title": "A. Application Environment: Target Localization", "content": "Target Localization is a multi-agent problem in which the location of a certain target is to be identified using sensory data reading collected by multiple mobile sensing agents [46]\u2013[48]. Such a problem is of interest in applications including radiation monitoring [49], [50], search and rescue missions [51], and last-mile delivery [52]. In such a problem, the sensing agents (UAVs or robots) need to cooperate to minimize the search time and the consumption of resources.\nWe formulate the target localization problem similar to [7], [27]. The problem is formulated as a POMG, since the target location is unknown. The environment of the problem consists of an unknown target location, sensing agents, and walls/obstacles which complicate the mobility of the agents and attenuate the data readings collected. The agents act based on their observations, which consist of data readings and information about the environment (obstacles) and the locations of the team members. Using these observations, the agents need to explore (through moving actions) the environment in a way that minimized the search time. It is assumed that the agents are capable of communicating with each other, and hence they have information about the locations and readings collected by other agents. At the beginning of each training episode, the environment is initialized with a randomly placed target, randomly distributed obstacles, and randomly placed agents. An episode of target localization unfolds over a finite sequence of steps. At every step, each agent i analyzes its observation oi \u2208 Oi and takes a moving action ai \u2208 Ai based on a policy \u03c0i : Oi \u00d7 Ai \u2192 [0,1] and receives a reward ri. We use a sparse reward function, where the agents receive a large reward only when the target is localized. During the training (which is conducted in simulations), the experiences and rewards collected are used in PPO, as discussed in Section III-A, to update the policies of the agents. Once the training"}, {"title": "B. Performance of MEDC", "content": "This section analyzes the performance of the proposed MEDC method and its effectiveness in guiding the learning for new agents in an MDRL system. During training, a localization episode has a maximum length of 100 steps, and the total number of training steps is given as 2\u00d7107. An episode terminates when the target is found, or when the maximum episode length is reached. After every 40,000 training steps, the average results of 4,000 testing steps, where the agents act greedily based on the latest policy update, are recorded and plotted. In this work, the target localization problem is characterized by two features: the number of agents and the number of walls. We report the results in terms of episodic length throughout the learning, as it reflects the time needed by the agents to localize the target, which is the main aim of target localization. We use the notation AyWz to denote an environment with y agents and z walls.\nTo show the effectiveness of MEDC in enhancing the learning and tackling issues with sparse rewards, Fig. 6 compares variations of an environment trained using MDRL and MEDC with one that is trained using MDRL with sparse rewards. Here, the problem of interest is one with 3 agents aiming to localize the target in an environment of 2 walls (A3W2) or 3 walls (A3W3). Three types of experts are used to guide the learning: a \u201cFree Expert\u201d is a model previously trained to tackle an environment with a single agent and no walls (A1W0), a \u201cComplex Expert\u201d is a model previously trained to tackle an environment with a single agent and 3 walls (A1W3), and \u201cCombined Experts\u201d refers to using both aforementioned experts at the same time in MEDC to guide the learning. It is evident that the used experts come from environment that are not exactly the same as the current environments of interest. Here, a better learning is indicated by a smaller episode length, as it reflects faster localization. As shown in Fig. 6, using MEDC with experts from similar environments to guide the MDRL agents helps in achieving better and faster learning, when compared to training MDRL independently using a sparse reward. Even though both experts are not familiar with a multi-agent environment, and can only tackle a single agent environment (hence they are not familiar with cooperation), they are partially beneficial in guiding the new agents into collecting better experiences, especially in the initial stages of the learning. Additionally, it can be seen that using the complex expert (A1W3) gives better results than using the free expert (A1W0), because it is trained in an environment closer to the current environment of interest. This reflects the importance of considering the QoS (Eq. 6) when allocating expert models. In this scenario, and assuming that Repm and Repi are constant across both experts, the complex"}, {"title": "C. MEDC vs Benchmarks", "content": "This section discusses and highlights the main advantages of MEDC when compared frameworks in FRL, Reward Shaping (RS), and IL-assisted RL. The benchmarks are summarized as follows:\n\u2022 In FRL, models from different users are averaged frequently in a global model, which is shared back to them. We benchmark with the works in [25], [26], where FRL is used to combine DRL models across different users.\n\u2022 In Reward Shaping (RS), a shaped reward function is used to guide the learning. Here, we use a distancebased reward function, where the agents receive a positive reward in each step if they move closer to the target during the training, which is similar to the works in [7], [27]. In each episode, the distances are computed using Breadth-First Search (BFS). BFS is used due to the existence of walls, which makes simpler distance measures (such as Euclidean distance) inapplicable.\n\u2022 In IL-assisted RL, the learning alternates between RL and IL, similar to the works in [31], [32]. For fairness, We extrapolate this work to use the same structure of MEDC with multiple experts. The main difference is, when IL is switched on, the MDRL agents use Behavioral Cloning (BC) to mimic the behavior of the expert without any reward input.\nFigure 8 compares Blockchain-assisted MEDC with the 3 benchmarks. For MEDC and FRL, the learning involves 8 users of varying environments. The 8 users are training on different environments, given as A1W0, A3W0, A2W1, A2W2, A2W3, A3W1, A3W2, A3W3. For FRL, during the training, the models are shared and a global model is returned to each user to carry on the training. In MEDC, the first 6 users share their models to the Blockchain, which are then used by A3W2 and A3W3 in MEDC to guide the learning. For IL-assisted RL, the models of the first 6 users are used as experts, from which the MDRL agents in A3W2 and A3W3 choose to imitate. We show the training results from the perspective of the A3W2 and A3W3 users. As seen in the figure, the performance of the models trained with MEDC is significantly better than that of FRL. As discussed in Section II, if the models are trained in environments that inherit different dynamics, the learning convergence of the aggregated model could face issues [17], which is seen in the obtained results. This is unlike MEDC, in which the expert models only suggest actions, and the MDRL agents still learn based on their experience. Similarly, MEDC outperforms IL-assisted RL, whose performance is negatively affected by experts being from different variations of the environment. This is because the agents in IL-assisted RL blindly clone the behavior of the expert into their models. Additionally, the performance of MEDC when compared to the model trained on a shaped reward shows slight outperformance. Even though RS gives a similar performance towards the end of the training, it is worth"}, {"title": "D. Adaptability to Other Applications", "content": "This section analyzes the adaptability of the MEDC method to the following applications:\n\u2022 Fleet Coordination for Autonomous Vehicles [43], [44]: in this problem, a team of autonomous vehicles is tasked with picking up and dropping costumers at specific locations. Each vehicle has a limit in terms of the number of costumers it can accommodate simultaneously. At the beginning of each episode, all agents (vehicles) and costumers are placed randomly in the environment, with each costumer having a desired destination. The vehicles are tasked with cooperation and coordination, such that the time needed to drop all costumers at their destinations is minimized. This requires the agents to learn how to divide the costumers according to their locations. The"}, {"title": "E. Smart Contracts Complexity Analysis", "content": "This section analyzes the complexity of the developed smart contracts in terms of gas cost. Since a Consortium Blockchain is proposed, the deployment and execution of the smart contracts do not require any payments by the users. However, the gas cost is a good measure of the complexity of the smart contracts to indicate their feasibility. Table IV presents the gas cost of the deployment and execution of the smart contracts and their functions. As seen in the table, the gas costs are low, reflecting the feasibility of the proposed contracts. For reference, we present a benchmark of the gas cost of deploying and executing a similar UMC smart contract as discussed in [38]."}, {"title": "VI. CONCLUSION", "content": "In this paper, the problems of sample efficiency and reward sparsity in Multi-Agent Deep Reinforcement Learning systems is tackled. A novel Blockchain-assisted Multi-Expert Demonstration Cloning (MEDC) framework is proposed, in which users share trained models to be used as expert models by other users in the MEDC method. The proposed MEDC method utilizes expert models to suggest actions to new MDRL agents, aiming to provide better experiences where the sparse reward is more frequently obtained, which speeds up the learning. Unlike methods such as FRL, the proposed MEDC method is more resilient to faulty and malicious shared models, and allows for models of different architectures to be used together. On a Consortium Blockchain, smart contracts are designed to manage the model sharing and allocation process using a Greedy method, in which attributes about the model and the users are used in the assignment process. Experiments in the"}]}