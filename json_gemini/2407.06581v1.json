{"title": "Vision language models are blind", "authors": ["Pooyan Rahmanzadehgervi", "Logan Bolton", "Mohammad Reza Taesiri", "Anh Totti Nguyen"], "abstract": "Large language models with vision capabilities (VLMs), e.g., GPT-4o and Gemini-1.5 Pro are powering countless image-text applications and scoring high on many vision-understanding benchmarks. Yet, we find that VLMs fail on 7 visual tasks absurdly easy to humans such as identifying (a) whether two circles overlap; (b) whether two lines intersect; (c) which letter is being circled in a word; and (d) counting the number of circles in a Olympic-like logo. The shockingly poor performance of four state-of-the-art VLMs suggests their vision is, at best, like of a person with myopia seeing fine details as blurry, and at worst, like an intelligent person that is blind making educated guesses. Code is available at: https://vlmsareblind.github.io/", "sections": [{"title": "1 Introduction", "content": "In the last eight months, the advent of VLMs, starting with GPT-4V(ision) [32], has enabled numerous, unprecedented image-text processing applications [42]. VLMs can accurately identify objects in a scene [2,34,42] and perform complex tasks based on these detected objects, e.g., calculating the cost of beers on a table from an image of the scene and an image of the menu [43]. Interestingly, VLMs advance so quickly that describing unusual activities in an image [37] (e.g., a man ironing on a moving taxi) has become a standard sanity check [13].\nExisting VLM benchmarks [17,28,46] cover a wide range of topics but only measure the overall human-vs-LLM gap without pinpointing specific limitations for future vision research. For example, the input images in so many questions, e.g., 42.9% of MMMU [46], are not necessary [11]. That is, many answers (1) can be inferred from the textual question and choices alone; and (2) are memorized by VLMs from their Internet-scale training [11]. In sum, while highlighting VLM"}, {"title": "2 Vision language models", "content": "Our goal is to study how SOTA VLMs perceive simple images composed of common geometric primitives interacting. We choose four models: GPT-4o (), Gemini-1.5 Pro ( Gemini-1.5), Claude-3 Sonnet (A Sonnet-3), and Claude-3.5 Sonnet ( Sonnet-3.5) that are ranking highest on 7 recent multimodal vision benchmarks (see [4] and Table 10 in [35]) which cover multi-discipline, college-level subjects in MMMU [46], science diagrams in AI2D [17], mathematics in MathVista [25], charts in ChartQA [28], documents in DocVQA [29], and videos in ActivityNet-QA [45] & EgoSchema [26]."}, {"title": "3 BlindTest benchmark of 7 tasks", "content": "Eye exams Like humans' visual acuity tests [10], we design a set of 7 very simple, yet novel tasks that are composed of common geometric primitives. We do not use the existing tests designed for human-eye exams for two reasons. First, we avoid using the questions that exist on the Internet, which may provide an inflated measure of vision capabilities [11,44]. Second, our preliminary experiments show that VLMs already perform very well on humans' eye exams, which typically contain single, separate symbols-e.g., the Snellen chart [10], tumbling E [10], and contrast sensitivity charts [7,27].\nMotivation Our BlindTest benchmark tests VLMs on identifying known geometric primitives when they are close together, overlapping, or intersecting. We hypothesize that VLMs will struggle because, while the primitives are well-known, their exact spatial information (e.g., the size and position of a \u25cb) on a white canvas is typically not describable in natural language, even for humans, and therefore requires a \"visual brain\" to perceive. However, VLMs primarily rely on early fusion [23,38] to integrate a shallow vision encoder into a large language model (LLM), which is essentially a knowledgeable brain without eyes.\nControls For each test image, we prompt VLMs using two different, yet semantically equivalent questions. Furthermore, we reproduce the same image using (a) three different image sizes and (b) two line thickness values when rendering primitives."}, {"title": "3.1 Task 1: Counting line intersections", "content": "Given the impressive accuracy of VLMs on answering questions on diagrams and charts (e.g., Sonnet-3.5 scoring 94.7% on AI2D and 90.8% on ChartQA) [6], a reasonable hypothesis is that VLMs must be able to see whether two graphs intersect in a chart. Here, we test this hypothesis by asking VLMs to count the number of intersections between two 2-segment piece-wise linear functions.\nImages We create 150 images (Fig. 1a) of 2D line plots drawn on a white canvas. Each line plot consists of two line segments, defined by three points whose"}, {"title": "3.2 Task 2: Two circles", "content": "In the task of counting line intersections (\u00a73.1), each image contains two long, thin colored lines on a large white canvas. Here, we test VLMs in a complementary setting where the two interacting objects (here, two same-sized filled circles \u25cf\u25cf) are larger while their gap is smaller. This task evaluates VLM ability in perceiving (1) a small gap between two circles and (2) two circles are overlapping, i.e., no gaps. We vary circle and gap sizes and ask VLMs if two circles are (a) overlapping or (b) touching.\nImages Given a blank image of size $C \\times C$, we draw two same-sized circles of diameter $\\phi \\in {\\frac{1}{5}, \\frac{1}{6}, \\frac{1}{7}, \\frac{1}{8}}$ with a boundary-to-boundary distance = $\\alpha \\times d$ where $d \\in \\{-0.15, -0.1, -0.05, 0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5\\}$ to cover all three cases: overlapping, tangent, and disjoint (see Fig. 2). The two circles are arranged in four different orientations, making a 90\u00b0, 0\u00b0, -45\u00b0, and 45\u00b0 angle with the x-axis (Fig. 2b). The whole grid sampling generates 224 images per image size. We replicate the procedure for 3 image sizes, i.e., C = 384, 769, 1155 px to create a total of 3\u00d7224 = 672 images. See \u00a7A.1 for more details.\nPrompts \"Are the two circles touching each other? Answer with Yes/No.\" and \u201cAre the two circles overlapping? Answer with Yes/No.\"."}, {"title": "3.3 Task 3: The circled letter Subdermatoglyphic", "content": "Consistent with prior reports [36,42,43], we find that VLMs can 100% accurately identify a primitive shape (e.g., a red circle ) [36] and can perfectly read an English word (e.g., Subdermatoglyphic) alone. Here, we superimposed the red circle on every letter, one at a time, in the word, and ask VLMs to identify which letter is being circled. While the task is easy to humans, our hypothesis is that if a VLM's vision is \u201cblurry\u201d, it might not be able to identify the exact letter being circled since there is tiny spacing between the adjacent letters.\nImages We choose three strings Acknowledgement, Subdermatoglyphic, and tHyUiKaRbNqWeOpXcZvM because they contain characters of variable widths and heights. Furthermore, all four tested VLMs can read out all characters in these strings when they are input to the models as an image. While Acknowledgement is a common English word, Subdermatoglyphic is the longest word without repetitive letters. We also test VLMs on the random string tHyUiKaRbNqWeOpXcZvM to estimate how much model accuracy is due to its familiarity with the word.\nFor each (string, circled-letter) pair, we render a 512\u00d7512 image by choosing among 3 red oval line-thickness levels, 2 font sizes, and 4 random positions in the canvas for a total of 24 images. That is, we generate 360, 408, and 480 images for Acknowledgement (15 letters), Subdermatoglyphic (17 letters), and tHyUiKaRb-NqWeOpXcZvM (20 letters), respectively. We ensure each letter to be circled fits completely the oval (see Fig. 1b). See \u00a7B.1 for more details."}, {"title": "3.4 Task 4: Counting overlapping shapes", "content": "Aligned with prior research [43], we also find VLMs to be able to count disjoint circles (\u25cb\u25cb\u25cb). Yet, here, we test VLMs on counting circles that are intersecting () like in the Olympic logo a common cognitive development exercise for preschoolers [1,5]. Our hypothesis is that a \"blurry\" vision may not see the intersection between two circles clearly and therefore unable to trace circles and count them. For generalization of our findings, we repeat the experiment with pentagons () as well."}, {"title": "3.5 Task 5: Counting the nested squares", "content": "Motivated by the findings that VLMs struggle in counting the intersected circles (\u00a73.4), here, we arrange the shapes differently so that their edges do not intersect. That is, each shape is nested entirely inside another (see Fig. 1c). For completeness, we test squares (\u25a1) in this task.\nImages In a canvas of size $C \\times C$, we render N\u2208 {2,3,4,5} nested squares. The outermost square is rendered first using a random edge length d and a line thickness \u2208 {2,3,4}px. The remaining N 1 squares are drawn using a size reduction factor, 0.75 \u00d7 d and placed at a random coordinate that ensures they do not touch outer squares. For each line thickness, we generate 10 images (where squares have different, random locations) to create 3\u00d710 = 30 images. Repeating the process for all N values results in 4\u00d730=120 images. See \u00a7D.1 for more details."}, {"title": "3.6 Task 6: Counting the rows and columns of a grid", "content": "The results from prior tasks show VLMs cannot always count shapes that are overlapping (\u00a73.4) or nested (\u00a73.5). What about adjacent shapes \u2610? Here, we tile up shapes (specifically, \u2610) into a grid and challenge VLMs to count\u2014 a task that is supposedly simple to VLMs given their remarkable performance (\u2265 90% accuracy) [4,35] on DocVQA [29], which includes many questions with tables. To simplify the task, we ask models to count the number of rows and columns in a given table.\nImages A grid may have N\u00d7N, N\u00d7N', or N' \u00d7 N cells, where N \u2208 {3, 4, 5, 6, 7, 8, 9} and N' = N + 1. Each grid is rendered with two different line-thicknesses on a canvas of size $C \\times C$ where $C\u2208 {500, 1250, 2000}px$. Besides empty grids, we also replicate the procedure to make grids contain text (which is more common in real-world tables) where each cell contains a single random word (see Fig. 4a). Two versions combined have 2\u00d7222 = 444 images. See \u00a7F.1 for more details."}, {"title": "3.7 Task 7: Following single-colored paths", "content": "It is important for VLMs to be able to follow paths in order to read maps or charts [28], interpret graphs [18], and understand user notations (e.g., arrow) in input images [43]. To assess path-following capability, this task asks models to count the unique-color paths between two given stations in a simplified subway map. This is another easy-to-humans task that challenges VLMs significantly.\nImages We create each subway map on an image of size $C \\times C$, where $C\u2208 {512, 1024}px (see Fig. 4b). We write 4 station names (A, B, C, D) at 4 fixed coordinates $\u2208 {(\\frac{C}{18}, C), (C, \\frac{C}{18}), (\\frac{C}{2}, 0), (0, \\frac{C}{2})},$ respectively. We divide the canvas into an invisible grid of 18\u00d718 cells and initialize 3 path-starting points $\\frac{C}{18}px$ away from each station. We draw a path, using the depth-first search algorithm starting from a random station and a random starting point, where a valid move is one cell in any direction: North, south, east or west. We repeat the process so that each station has exactly $N\u2208 {1,2,3}$ outgoing paths, for a total of 180 maps. See \u00a7G.1 for details."}, {"title": "4 Results", "content": "4.1 Task 1: VLMs cannot reliably count line intersections\nExperiment We parse every model's free-form response to extract the final answer and then compare it to the groundtruth. When there is no intersection, models usually respond with \u201cthere are no intersections\", which we label \"0\". We report the mean accuracy of every model on two prompts and analyze the sensitivity of the accuracy to varying spatial parameters.\nResults First, across two prompts and three line widths, all VLMs perform poorly on this easy task (Fig. 5). The best accuracy is only 77.33% (Sonnet-3.5) (Table 1). More specifically, VLMs tend to perform worse when the distance between two plots narrows (Fig. 6). As each line plot is composed of three key points, the distance between two plots is computed as the mean"}, {"title": "4.2 VLMs cannot clearly see if two circles overlap or not", "content": "Motivated by VLM poor performance in counting line intersections (\u00a74.1), here, we replace lines by large, filled circles and ask VLMs explicitly if the two circles are touching (or overlapping) or not.\nExperiment Since we instruct VLMs to output a binary answer (Yes/No), we use Python to extract VLMs' formatted answer from their responses for comparing with groundtruth.\nResults Surprisingly, even on this task where objects are large and clearly visible to humans, no VLMs are able to solve it perfectly (Fig. 7). The best accuracy is 92.78% (Gemini-1.5) over all images and prompts (Table 2). A common trend is when two circles are close together, VLMs tend to perform poorly, making educated guesses, e.g., Sonnet-3.5 often answer \"No\" conservatively (Fig. 7). Shockingly, GPT-4o (the worst accuracy) is not 100% accurate even when the distance between two circles is as large as one radius (Fig. 8; d = 0.5). That is, VLM vision seems not clear enough to see the fine gap or intersection between two circles.\nVLMs often perform consistently across three image resolutions (\u00a7A.2); however, interestingly, every model performs consistently the best at a specific circle orientation (\u00a7A.5). More examples of VLMs' answers are in \u00a7A.7."}, {"title": "4.3 VLMs do not always see the letter inside the red circle", "content": "Experiment We use simple text processing and regex matching to extract the answer since each model's output follows a specific format, e.g., GPT-4o's responses typically contain a single line explaining its answer, with the predicted letter mostly placed between quotation marks (''). However, Gemini-1.5 uses markdown formatting and places double asterisks (**) around the letter.\nResults First of all, we find that all VLMs can accurately spell out the string despite there is a red oval superimposed. Yet, interestingly, reading out which letter is being circled turns out to pose a challenge to all VLMs (Fig. 9). For example, VLMs tend to make mistakes when the letter is slightly, partially occluded by the red oval (e.g., Subdermatoglyphic).\nWhen making mistakes, VLMs often predict letters adjacent to the one being circled (see the confusion matrix in Fig. 10 and more results in \u00a7B.2)."}, {"title": "4.4 VLMs struggle to count overlapped and nested shapes", "content": "Experiment We run all VLMs on all images of overlapping circles and pentagons (\u00a73.4) and nested squares (\u00a73.5). We prompt models to output the count in a formatted answer. We extract templated answers and compare them without groundtruth. For each shape (circles, pentagons and squares), we run two different prompts.\nResults Regardless of whether the shapes are overlapped or nested (i.e., their edges intersect or not) both cases appear to be challenging to VLMs (Fig. 11). On nested squares, model accuracies vary widely-GPT-40 (48.33%) and Sonnet-3 (55.00%) are at least 30 points behind Gemini-1.5 (80.00%) and Sonnet-3.5 (87.50%). This gap is even larger on counting overlapped circles and pentagons-Sonnet-3.5 is better than the rest by multiple times (e.g., 75.83% vs. 9.16% of Gemini-1.5; Table 4)."}, {"title": "4.5 VLMs cannot easily count the rows and columns in a grid", "content": "Since VLMs struggle in counting the number of simple shapes when the shape edges intersect (\u00a73.4) or separate (\u00a73.5), here, we test the remaining case where these shapes are placed adjacently sharing edges, specifically, tiling up multiple rectangles into a single grid. Given the impressive accuracy of VLMs [4,6,35] on questions involving tables and spreadsheets in DocVQA [29], we hypothesize that VLMs must be able to count the rows and columns in a grid.\nExperiment We run all VLMs on the images of empty grids and text-containing grids (\u00a73.6) and analyze their formatted answers.\nResults First, VLMs surprisingly often struggle to count the exact number of rows and columns in an empty grid (see Table 6). Specifically, they are often off by 1 or 2 (e.g., GPT-4o predicts 4\u00d74 and Gemini-1.5 predicts 5\u00d75 for a 4\u00d75 grid; Fig. 14). This finding suggests that VLMs can extract important content from a table to answer table-related questions in DocVQA [29] but cannot clearly see a table cell-by-cell.\nThis might be because tables in documents are mostly non-empty and VLMs are not used to them. Interestingly, after attempting to make the task easier by adding a single word to each cell, we observe the accuracy of all VLMs to increase substantially (e.g., 26.13% to 53.03% for GPT-4o) (Table 6). Yet,"}, {"title": "4.6 VLMs struggle to trace and to count single-colored paths", "content": "This task test a VLM's ability in recognizing a path of a unique color and trace it from a given starting station to the destination, an important task in reading maps and graphs in general [28].\nExperiment From a subway map (\u00a73.7), we randomly pick 2 stations and prompt every model to count the single-colored paths between them. We then extract answers from the templated answers and compare them with the groundtruth."}, {"title": "5 Related Work", "content": "Benchmarking VLM vision understanding College-level topics [46], charts [28], documents [29] or videos [45] are among the common benchmarks for assessing VLM vision understanding [4,6,9,35] and are witnessing VLMs' recent rapid progress-e.g., Sonnet-3.5 is reaching 95.2% on DocVQA, 90.8% on ChartQA, and 94.7% on AI2D [6]. However, most of vision benchmarks attempt to evaluate VLMs on real-world, topic-specific data that require extensive prior knowledge [11, 19, 41], which has a \u201cdata leakage\" problem, i.e., VLMs many times can answer accurately without even the input image [11]. Furthermore, most benchmarks test VLMs on the data that humans have to deal with to provide a high-level sense of the human-machine intelligence gap [24, 44]. In contrast, our BlindTest benchmark differs significantly from prior benchmarks because it is (1) extremely easy to humans and can be solved by a 5-year-old (unlike [28, 29, 46]); (2) the first low-level, visual sanity check for VLMs; (3) requiring minimal to zero prior knowledge; (4) requiring zero commonsense or complex reasoning (unlike [12, 47])-i.e., a strong language model is of little use here when it is non-natural for humans to describe BlindTest images in language (for a brain thinking in language step-by-step to solve it).\nThe ARC benchmark [12,31] also contain abstract images made up of simple shapes; however they challenge VLMs on understanding and reasoning based on those patterns. ARC assumes VLMs can identify the abstract shapes in order to reason. In contrast, our BlindTest directly evaluates VLM capabilities in recognizing the primitive shapes.\nImproving VLM vision capabilities Most recent recipes for improving SOTA VLMs involve finetuning a pretrained LLM coupled with vision encoders to solve high-level vision tasks [22]. Such late-fusion approaches fuse visual representations learned from the tokenized image with a powerful thinking brain [20,21,30].\nHowever, current vision approaches for VLMs are facing challenges as models sometimes are \"blind\" unable to see natural objects exist in a real photo [40]. In contrast, we are showing VLMs are visually impaired at low-level abstract images, e.g., inability to count 6 overlapping circles or 3 nested squares. Our circled letter \u00a73.3 is inspired by VLM abilities in recognizing content inside a red circle over real objects in natural images [36, 42, 43]. However, we show that VLMs can fail at a low-level, optical character recognition as opposed to recognizing real objects. To the best of our knowledge, no prior attempts have been made to address the exact limitations raised in our paper: (1) identifying and counting simple lines, shapes and geometric primitives when they interact (\u00a74.1 to \u00a74.5); (2) following colored paths (\u00a74.6). Solving these limitations may be the foundation for VLMs to progress on some existing vision benchmarks on graphs, e.g., [18], visual math [25] and blindness in natural images (e.g., understanding the direction an object is facing [40])."}, {"title": "6 Discussion and Conclusion", "content": "We propose BlindTest, a benchmark of seven novel low-level visual tasks that did not exist on the Internet before and requires minimal to zero knowledge. By design, our tasks minimize the chance that VLMs can solve by memorization or by not using the input image\u2014an issue in some prior benchmarks [11].\nWe document shocking findings that the best commercial VLMs are still struggling with tasks that can be easily solved by a five-year-old (e.g., telling whether two circles or lines intersect \u00a74.2). These limitations may be largely due to the late-fusion approach [8,23] of integrating vision into an LLM model and that early-fusion [38,39] may be the way forward. We find that simply finetuning a 7B, SOTA late-fusion open-source VLM [14] using LORA on our tasks does not yield a high-performing model (\u00a7A.6). That is, training a single model that performs well on BlindTest might be non-trivial, interesting research direction."}, {"title": "A Two touching circles task", "content": "A.1 Benchmark Construction Details\nTo create our benchmark, we use 5 parameters to control the diversity of the samples.\nColor: We fix the colors for each circle to use {magenta, dodgerblue}.\nImage size: We use the physical size, and the DPI arguments in matplotlib to initialize the image size. The physical size is fixed to 5 \u00d7 5, and the DPI \u2208 {100, 200, 300}. The output image sizes are {384, 769, 1155}px.\nDiameter: We use uniform diameters for both circles and choose the value proportional to the image size, where the diameter is {$\\frac{1}{4}$,$\\frac{1}{5}$,$\\frac{1}{6}$,$\\frac{1}{7}$} of the image size.\nDistance: The boundary-to-boundary distance between circles is a fraction of the diameter chosen from {-0.15, -0.1, -0.05, 0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5}. Based on our definition, center-to-center distance is (2+distance)\u00d7diameter.\nRotation: We include 2 main rotations (vertical and horizontal), and 2 diagonal rotations.\nWe use the center of the image as the origin so that it always aligns with the midpoint of distances between two circles. This systematic process results in a benchmark comprising 672 images (see Fig. 17).\nCode The code is available at https://anonymous.4open.science/r/Benchmark-85F0/src/TouchingCircle/TwoTouchingCircles.ipynb."}, {"title": "A.2 Finding: image resolution does not affect VLMs performance", "content": "Fig. 18-left shows that VLMs are almost invariant to the image resolution. For instance, GPT-4o and Sonnet-3's performance saturates at 769px, and Sonnet-3.5 slightly performs worse at 769 and 1155px compared to 384px. Gemini-1.5, however, is more consistent across different resolutions. That is, seeing the intersection of two circles does not depend on the quality of the image."}, {"title": "A.3 Finding: the vertical rotation closes the gap between models' performance", "content": "As shown in Fig. 18-middle, arranging the circles in vertical rotation causes the models to perform similarly on the benchmark. Although Gemini-1.5 slightly performs better at diagonal and Sonnet-3.5 at horizontal rotation, VLMs perform relatively better at vertical rotation. This suggests that the task complexity due to various rotations is not the main source of low performance in VLMs."}, {"title": "A.4 Finding: Increasing the distance improves the VLMs' accuracy", "content": "VLMs perform better when the distance increases from zero to positive values (see Fig. 18-right). However, Sonnet-3.5 is more conservative than other VLMs that mostly answer \u201cYes\u201d, which results in its lowest performance at negative distances."}, {"title": "A.5 Finding: VLMs prefer a specific rotation", "content": "Table 8 shows that VLMs prefer different rotations. For example, GPT-4o performs best at vertical, Gemini-1.5 at diagonal, Sonnet-3.5 at horizontal, and Sonnet-3 at vertical."}, {"title": "A.6 Results for fine-tuning Bunny on the two touching circle", "content": "In order to determine if fine-tuning could improve the model's performance on this task we attempted to fine-tune Bunny [15] on the two touching circles task. We fine-tuned Bunny using datasets of sizes: 10K, 20K, 50K, and 100K samples, each containing a balanced number of instances where the circles are either overlapping or separate (equal number of YES/NO answers in the training set).\nThe baseline model, without any fine-tuning, achieved 17.1% accuracy for task overlap and 11.7% for touching circles. After fine-tuning, we observed improvements with smaller datasets, such as 10K and 20K cases, where accuracy reached up to 36.8%. However, increasing the number of samples did not lead to better performance. In some instances, such as with the 50K dataset, the model failed to predict anything and only generated the end-of-text token.\nThe loss values for all these experiments were very close to zero, indicating that the model overfits the training set but fails to generalize. This suggests that training on this task is not straightforward and may require a combination of multiple tasks or that this problem does not have a simple solution."}, {"title": "A.7 Additional examples", "content": "We show examples of models' responses to the prompts on the two touching circles task in Fig. 20."}, {"title": "B Task 3: Identifying circled letter in word", "content": "B.1 Task construction\nEach image is created using a combination of the below variables.\nWord: The word is chosen among 3 candidates {Acknowledgement, Subdermatoglyphic, tHyUiKaRbNqWeOpXcZvM }. Each word has unique features, e.g., variable letter size (Acknowledgement), non-repetitive letters (Subdermatoglyphic), and non-existing word (tHyUiKaRbNqWeOpXcZvM).\nLetter: We draw the oval (0) over all the letters in a word.\nFont: We use 2 different font styles for each word, OpenSans and Helvetica.\nOval (O) thickness: We generate the with 3 various line thicknesses.\nScaling factor: Since each letter has a unique size, we use a scaling factor to control the size of the O.\nPadding: We use four values for padding {25, 50, 100, 200}, by which we change the word's position on the image. This ensures that the word is not always in the center of the image.\nFinally, we render the text on a white canvas with a size of 512\u00d7512 pixels, and we get 180 images for Acknowledgement, 204 for Subdermatoglyphic, and 240 samples for tHyUiKaRbNqWeOpXcZvM (see Fig. 21).\nCode The code is available at https://anonymous.4open.science/r/Benchmark-85F0/src/CircledWord/GenerateSamples.ipynb.\nPost-processing: We do not ask VLMs to provide answers in a pre-defined format. Instead, we write code to extract predicted labels from VLM free-form response."}, {"title": "B.2 Finding: VLMs mostly confuse the adjacent character for the circled letter", "content": "Models often mistake the neighboring characters as the circled letter (see Table 9), GPT-40 33.67% in the word Subdermatoglyphic (the lowest) and Sonnet-3.5 90.62% in the word Acknowledgement (the highest). Fig. 22 depicts that in the word Acknowledgement, where letter \"m\" is surrounded by two \u201ce\u201d, Sonnet-3.5 mispredicts \u201ce\u201d for \"m\u201d 38 times out of 48 (79.16%). Gemini-1.5, in the word tHyUiKaRbNqWeOpXcZvM, where letters \u201cK\u201d and \u201ca\u201d are adjacent, mispredicts \u201ca\u201d for \"K\" 35 times out of 48 (72.91%) (see Fig. 23)."}, {"title": "B.3 Finding: GPT-40 and Gemini-1.5 confuse the red oval as part of the letter", "content": "Figs. 24 and 25 show that Gemini-1.5 and GPT-40 sometimes fail to recognize that the red oval is not part of the letter. Gemini-1.5 predicts \"o\" when the circled letter is not \"o\" (see Fig. 24), and GPT-40 often predicts \"o\" regardless of what letter is circled (Fig. 25). Gemini-1.5 also predicts instead of \"c\", and GPT-40 predicts \"@\" instead of \u201ca\u201d."}, {"title": "B.4 Finding: font selection does not vary the performance of the models", "content": "As shown in Fig. 26, models do not show a significant variance over different fonts, suggesting our choice of font is not a reason for their ill-performance."}, {"title": "B.5 Finding: models are invariant to our choice of prompts", "content": "Our choice of prompts has almost no impact on the performance of the models as depicted in Fig. 27."}, {"title": "B.6 Additional Examples", "content": "Examples from our evaluation of VLMs on the circled letter task are shown in Fig. 28"}, {"title": "C Counting the number of line intersections task", "content": "C.1 Benchmark Construction Details\nTo create our benchmark, we use 5 parameters to control the diversity of the samples.\nColor: We fix the colors for each line to use {blue, red}\nX-coordinate: For each line, we choose three uniformly distributed x coordinates on the canvas to plot the x values of lines.\nY-coordinate: We randomly pick three y values for each line on the canvas.\nLine thickness: We vary the line thicknesses with standard matplotlib values (2, 3, and 4).\nNumber of intersections: We count the intersections based on the three points defined for each line ((x, y1), (x, y2), and (x, y3)).\nWe repeat the process until we have 50 samples of 0, 1, and 2 intersections, resulting in 150 images (see Fig. 29).\nCode The code is available at https://anonymous.4open.science/r/Benchmark-85F0/src/LineIntersection/GenerateSamples.ipynb."}, {"title": "C.2 Finding: line thickness does not influence VLM ability to count line intersections", "content": "Fig. 30 depicts that increasing the line thickness in our plots does not help the VLMs see and count the intersections of 2D lines. Interestingly, Sonnet-3.5 performs best on the thinnest line which suggests the model can see the thin lines, but"}]}