{"title": "Vision language models are blind", "authors": ["Pooyan Rahmanzadehgervi", "Logan Bolton", "Mohammad Reza Taesiri", "Anh Totti Nguyen"], "abstract": "Large language models with vision capabilities (VLMs), e.\u0434., GPT-40 and Gemini-1.5 Pro are powering countless image-text applications and scoring high on many vision-understanding benchmarks. Yet, we find that VLMs fail on 7 visual tasks absurdly easy to humans such as identifying (a) whether two circles overlap; (b) whether two lines intersect; (c) which letter is being circled in a word; and (d) counting the number of circles in a Olympic-like logo. The shockingly poor performance of four state-of-the-art VLMs suggests their vision is, at best, like of a person with myopia seeing fine details as blurry, and at worst, like an intelligent person that is blind making educated guesses.", "sections": [{"title": "1 Introduction", "content": "In the last eight months, the advent of VLMs, starting with GPT-4V(ision) [32], has enabled numerous, unprecedented image-text processing applications [42]. VLMs can accurately identify objects in a scene [2,34,42] and perform complex tasks based on these detected objects, e.g., calculating the cost of beers on a table from an image of the scene and an image of the menu [43]. Interestingly, VLMs advance so quickly that describing unusual activities in an image [37] (e.g., a man ironing on a moving taxi) has become a standard sanity check [13].\nExisting VLM benchmarks [17,28,46] cover a wide range of topics but only measure the overall human-vs-LLM gap without pinpointing specific limitations for future vision research. For example, the input images in so many questions, e.g., 42.9% of MMMU [46], are not necessary [11]. That is, many answers (1) can be inferred from the textual question and choices alone; and (2) are memorized by VLMs from their Internet-scale training [11]. In sum, while highlighting VLM"}, {"title": "2 Vision language models", "content": "Our goal is to study how SOTA VLMs perceive simple images composed of common geometric primitives interacting. We choose four models: GPT-40 (), Gemini-1.5 Pro (Gemini-1.5), Claude-3 Sonnet ( Sonnet-3), and Claude-3.5 Sonnet ( Sonnet-3.5) that are ranking highest on 7 recent multimodal vision benchmarks (see [4] and Table 10 in [35]) which cover multi-discipline, college-level subjects in MMMU [46], science diagrams in AI2D [17], mathematics in MathVista [25], charts in ChartQA [28], documents in DocVQA [29], and videos in ActivityNet-QA [45] & EgoSchema [26]."}, {"title": "3 BlindTest benchmark of 7 tasks", "content": "Eye exams Like humans' visual acuity tests [10], we design a set of 7 very simple, yet novel tasks that are composed of common geometric primitives. We do not use the existing tests designed for human-eye exams for two reasons. First, we avoid using the questions that exist on the Internet, which may provide an inflated measure of vision capabilities [11,44]. Second, our preliminary experiments show that VLMs already perform very well on humans' eye exams, which typically contain single, separate symbols-e.g., the Snellen chart [10], tumbling E [10], and contrast sensitivity charts [7,27].\nMotivation Our BlindTest benchmark tests VLMs on identifying known geometric primitives when they are close together, overlapping, or intersecting. We hypothesize that VLMs will struggle because, while the primitives are well-known, their exact spatial information (e.g., the size and position of a \u25cb) on a white canvas is typically not describable in natural language, even for humans, and therefore requires a \"visual brain\" to perceive. However, VLMs primarily rely on early fusion [23,38] to integrate a shallow vision encoder into a large language model (LLM), which is essentially a knowledgeable brain without eyes.\nControls For each test image, we prompt VLMs using two different, yet semantically equivalent questions. Furthermore, we reproduce the same image using (a) three different image sizes and (b) two line thickness values when rendering primitives."}, {"title": "3.1 Task 1: Counting line intersections", "content": "Given the impressive accuracy of VLMs on answering questions on diagrams and charts (e.g., Sonnet-3.5 scoring 94.7% on AI2D and 90.8% on ChartQA) [6], a reasonable hypothesis is that VLMs must be able to see whether two graphs intersect in a chart. Here, we test this hypothesis by asking VLMs to count the number of intersections between two 2-segment piece-wise linear functions.\nImages We create 150 images (Fig. 1a) of 2D line plots drawn on a white canvas. Each line plot consists of two line segments, defined by three points whose"}, {"title": "3.2 Task 2: Two circles", "content": "In the task of counting line intersections (\u00a73.1), each image contains two long, thin colored lines on a large white canvas. Here, we test VLMs in a complementary setting where the two interacting objects (here, two same-sized filled circles \u25cf\u25cf) are larger while their gap is smaller. This task evaluates VLM ability in perceiving (1) a small gap between two circles and (2) two circles are overlapping, i.e., no gaps. We vary circle and gap sizes and ask VLMs if two circles are (a) overlapping or (b) touching.\nImages Given a blank image of size $C \\times C$, we draw two same-sized circles of diameter $\\\u0444 \\in {\\frac{1}{8}, \\frac{1}{6}, \\frac{1}{5}, \\frac{1}{4}}$ with a boundary-to-boundary distance = $\\\u0446\u0435\u043d\u0442 \\times d$ where $d \\in \\{-0.15, -0.1, 0.05, 0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5\\}$ to cover all three cases: overlapping, tangent, and disjoint (see Fig. 2). The two circles are arranged in four different orientations, making a $90^\\circ$, $0^\\circ$, $-45^\\circ$, and $45^\\circ$ angle with the x-axis (Fig. 2b). The whole grid sampling generates 224 images per image size. We replicate the procedure for 3 image sizes, i.e., $C = 384, 769, 1155$ px to create a total of $3\\times224 = 672$ images. See \u00a7A.1 for more details.\nPrompts \"Are the two circles touching each other? Answer with Yes/No.\" and \u201cAre the two circles overlapping? Answer with Yes/No.\".\nGroundtruth We consider two circles overlapping and touching (O, T) if $d < 0.0$; non-overlapping but touching (\u00d5, T) if $d = 0.0$; and non-overlapping & non-touching (\u00d5, T) when $d > 0.0$ (Fig. 2). Random-baseline accuracy: 50%."}, {"title": "3.3 Task 3: The circled letter Subdermatoglyphic", "content": "Consistent with prior reports [36,42,43], we find that VLMs can 100% accurately identify a primitive shape (e.g., a red circle ) [36] and can perfectly read an English word (e.g., Subdermatoglyphic) alone. Here, we superimposed the red circle on every letter, one at a time, in the word, and ask VLMs to identify which letter is being circled. While the task is easy to humans, our hypothesis is that if a VLM's vision is \u201cblurry\u201d, it might not be able to identify the exact letter being circled since there is tiny spacing between the adjacent letters.\nImages We choose three strings Acknowledgement, Subdermatoglyphic, and tHyUiKaRbNqWeOpXcZvM because they contain characters of variable widths and heights. Furthermore, all four tested VLMs can read out all characters in these strings when they are input to the models as an image. While Acknowledgement is a common English word, Subdermatoglyphic is the longest word without repetitive letters. We also test VLMs on the random string tHyUiKaRbNqWeOpXcZvM to estimate how much model accuracy is due to its familiarity with the word.\nFor each (string, circled-letter) pair, we render a 512\u00d7512 image by choosing among 3 red oval line-thickness levels, 2 font sizes, and 4 random positions in the canvas for a total of 24 images. That is, we generate 360, 408, and 480 images for Acknowledgement (15 letters), Subdermatoglyphic (17 letters), and tHyUiKaRb-NqWeOpXcZvM (20 letters), respectively. We ensure each letter to be circled fits completely the oval (see Fig. 1b). See \u00a7B.1 for more details.\nPrompts \u201cWhich letter is being circled?\u201d and \u201c Which character is being highlighted with a red oval?\u201d.\nGroundtruth letters need to match predicted letters exactly (case-insensitive)."}, {"title": "3.4 Task 4: Counting overlapping shapes", "content": "Aligned with prior research [43], we also find VLMs to be able to count disjoint circles (\u25cb\u25cb\u25cb). Yet, here, we test VLMs on counting circles that are intersecting () like in the Olympic logo a common cognitive development exercise for preschoolers [1,5]. Our hypothesis is that a \"blurry\" vision may not see the intersection between two circles clearly and therefore unable to trace circles and count them. For generalization of our findings, we repeat the experiment with pentagons () as well."}, {"title": "3.5 Task 5: Counting the nested squares", "content": "Motivated by the findings that VLMs struggle in counting the intersected circles (\u00a73.4), here, we arrange the shapes differently so that their edges do not intersect. That is, each shape is nested entirely inside another (see Fig. 1c). For completeness, we test squares (\u25a1) in this task.\nImages In a canvas of size C \u00d7 C, we render N\u2208 {2,3,4,5} nested squares. The outermost square is rendered first using a random edge length d and a line thickness \u2208 {2,3,4}px. The remaining N 1 squares are drawn using a size reduction factor, 0.75 \u00d7 d and placed at a random coordinate that ensures they do not touch outer squares. For each line thickness, we generate 10 images (where squares have different, random locations) to create 3\u00d710 = 30 images. Repeating the process for all N values results in 4\u00d730=120 images. See \u00a7D.1 for more details.\nPrompts \"Count the total number of squares in the image.\". The results of other counting tasks (\u00a7\u00a7 3.1 and 3.4) show that VLM responses remain consistent for alternative tested rephrases, e.g., \"How many squares are in the image?\u201d.\nGroundtruth answers are \u2208 {2,3,4,5} (random-baseline accuracy: 25%)."}, {"title": "3.6 Task 6: Counting the rows and columns of a grid", "content": "The results from prior tasks show VLMs cannot always count shapes that are overlapping (\u00a73.4) or nested (\u00a73.5). What about adjacent shapes \u2610? Here, we tile up shapes (specifically, \u2610) into a grid and challenge VLMs to count\u2014a task that is supposedly simple to VLMs given their remarkable performance (\u2265 90% accuracy) [4,35] on DocVQA [29], which includes many questions with tables. To simplify the task, we ask models to count the number of rows and columns in a given table.\nImages A grid may have N\u00d7N, N\u00d7N', or N' \u00d7 N cells, where N \u2208 {3, 4, 5, 6, 7, 8, 9} and N' = N + 1. Each grid is rendered with two different line-thicknesses on a canvas of size C \u00d7 C where C\u2208 {500, 1250, 2000}px. Besides empty grids, we also replicate the procedure to make grids contain text (which is more common in real-world tables) where each cell contains a single random word (see Fig. 4a). Two versions combined have 2\u00d7222 = 444 images. See \u00a7F.1 for more details.\nPrompts \u201cCount the number of rows and columns and answer with numbers in curly brackets. For example, rows={5} columns={6}\" and \"How many rows and columns are in the table? Answer with only the numbers in a pair (row, column), e.g., (5,6).\u201d\nGroundtruth answers include both the number of rows and columns. An answer is correct when both column and row counts are correctly predicted."}, {"title": "3.7 Task 7: Following single-colored paths", "content": "It is important for VLMs to be able to follow paths in order to read maps or charts [28], interpret graphs [18], and understand user notations (e.g., arrow) in input images [43]. To assess path-following capability, this task asks models to count the unique-color paths between two given stations in a simplified subway map. This is another easy-to-humans task that challenges VLMs significantly.\nImages We create each subway map on an image of size C \u00d7 C, where C\u2208 {512, 1024}px (see Fig. 4b). We write 4 station names (A, B, C, D) at 4 fixed coordinates \u2208 {(, C), (C, \u33a2), (\uc756, 0), (0,)}, respectively. We divide the canvas into an invisible grid of 18\u00d718 cells and initialize 3 path-starting points px away from each station. We draw a path, using the depth-first search algorithm starting from a random station and a random starting point, where a valid move is one cell in any direction: North, south, east or west. We repeat the process so that each station has exactly N\u2208 {1,2,3} outgoing paths, for a total of 180 maps. See \u00a7G.1 for details.\nPrompts \u201cHow many single-colored paths go from A to C? Answer with a number in curly brackets, e.g., {3}"}, {"title": "4 Results", "content": "4.1 Task 1: VLMs cannot reliably count line intersections\nExperiment We parse every model's free-form response to extract the final answer and then compare it to the groundtruth. When there is no intersection, models usually respond with \u201cthere are no intersections\", which we label \"0\". We report the mean accuracy of every model on two prompts and analyze the sensitivity of the accuracy to varying spatial parameters.\""}, {"title": "4.2 VLMs cannot clearly see if two circles overlap or not", "content": "Motivated by VLM poor performance in counting line intersections (\u00a74.1), here, we replace lines by large, filled circles and ask VLMs explicitly if the two circles are touching (or overlapping) or not.\nExperiment Since we instruct VLMs to output a binary answer (Yes/No), we use Python to extract VLMs' formatted answer from their responses for comparing with groundtruth.\nResults Surprisingly, even on this task where objects are large and clearly visible to humans, no VLMs are able to solve it perfectly (Fig. 7). The best accuracy is 92.78% (Gemini-1.5) over all images and prompts (Table 2). A common trend is when two circles are close together, VLMs tend to perform poorly, making educated guesses, e.g., Sonnet-3.5 often answer \"No\" conservatively (Fig. 7). Shockingly, GPT-40 (the worst accuracy) is not 100% accurate even when the distance between two circles is as large as one radius (Fig. 8; d = 0.5). That is, VLM vision seems not clear enough to see the fine gap or intersection between two circles.\nVLMs often perform consistently across three image resolutions (\u00a7A.2); however, interestingly, every model performs consistently the best at a specific circle orientation (\u00a7A.5). More examples of VLMs' answers are in \u00a7A.7."}, {"title": "4.3 VLMs do not always see the letter inside the red circle", "content": "Experiment We use simple text processing and regex matching to extract the answer since each model's output follows a specific format, e.g., GPT-40's responses typically contain a single line explaining its answer, with the predicted letter mostly placed between quotation marks ('). However, Gemini-1.5 uses markdown formatting and places double asterisks (**) around the letter.\nResults First of all, we find that all VLMs can accurately spell out the string despite there is a red oval superimposed. Yet, interestingly, reading out which letter is being circled turns out to pose a challenge to all VLMs (Fig. 9). For example, VLMs tend to make mistakes when the letter is slightly, partially occluded by the red oval (e.g., Subdermatoglyphic).\nWhen making mistakes, VLMs often predict letters adjacent to the one being circled (see the confusion matrix in Fig. 10 and more results in \u00a7B.2)."}, {"title": "4.4 VLMs struggle to count overlapped and nested shapes", "content": "Experiment We run all VLMs on all images of overlapping circles and pentagons (\u00a73.4) and nested squares (\u00a73.5). We prompt models to output the count in a formatted answer. We extract templated answers and compare them without groundtruth. For each shape (circles, pentagons and squares), we run two different prompts.\nResults Regardless of whether the shapes are overlapped or nested (i.e., their edges intersect or not) both cases appear to be challenging to VLMs (Fig. 11). On nested squares, model accuracies vary widely-GPT-40 (48.33%) and Sonnet-3 (55.00%) are at least 30 points behind Gemini-1.5 (80.00%) and Sonnet-3.5 (87.50%). This gap is even larger on counting overlapped circles and pentagons-Sonnet-3.5 is better than the rest by multiple times (e.g., 75.83% vs. 9.16% of Gemini-1.5; Table 4)."}, {"title": "4.5 VLMs cannot easily count the rows and columns in a grid", "content": "Since VLMs struggle in counting the number of simple shapes when the shape edges intersect (\u00a73.4) or separate (\u00a73.5), here, we test the remaining case where these shapes are placed adjacently sharing edges, specifically, tiling up multiple rectangles into a single grid. Given the impressive accuracy of VLMs [4,6,35] on questions involving tables and spreadsheets in DocVQA [29], we hypothesize that VLMs must be able to count the rows and columns in a grid.\nExperiment We run all VLMs on the images of empty grids and text-containing grids (\u00a73.6) and analyze their formatted answers.\nResults First, VLMs surprisingly often struggle to count the exact number of rows and columns in an empty grid (see Table 6). Specifically, they are often off by 1 or 2 (e.g., GPT-40 predicts 4\u00d74 and Gemini-1.5 predicts 5\u00d75 for a 4\u00d75 grid; Fig. 14). This finding suggests that VLMs can extract important content from a table to answer table-related questions in DocVQA [29] but cannot clearly see a table cell-by-cell.\nThis might be because tables in documents are mostly non-empty and VLMs are not used to them. Interestingly, after attempting to make the task easier by adding a single word to each cell, we observe the accuracy of all VLMs to increase substantially (e.g., 26.13% to 53.03% for GPT-40) (Table 6). Yet,"}, {"title": "4.6 VLMs struggle to trace and to count single-colored paths", "content": "This task test a VLM's ability in recognizing a path of a unique color and trace it from a given starting station to the destination, an important task in reading maps and graphs in general [28].\nExperiment From a subway map (\u00a73.7), we randomly pick 2 stations and prompt every model to count the single-colored paths between them. We then extract answers from the templated answers and compare them with the groundtruth."}, {"title": "5 Related Work", "content": "Benchmarking VLM vision understanding College-level topics [46], charts [28], documents [29] or videos [45] are among the common benchmarks for assessing VLM vision understanding [4,6,9,35] and are witnessing VLMs' recent rapid progress-e.g., Sonnet-3.5 is reaching 95.2% on DocVQA, 90.8% on ChartQA, and 94.7% on AI2D [6]. However, most of vision benchmarks attempt to evaluate VLMs on real-world, topic-specific data that require extensive prior knowledge [11, 19, 41], which has a \u201cdata leakage\" problem, i.e., VLMs many times can answer accurately without even the input image [11]. Furthermore, most benchmarks test VLMs on the data that humans have to deal with to provide a high-level sense of the human-machine intelligence gap [24, 44]. In contrast, our BlindTest benchmark differs significantly from prior benchmarks because it is (1) extremely easy to humans and can be solved by a 5-year-old (unlike [28, 29, 46]); (2) the first low-level, visual sanity check for VLMs; (3) requiring minimal to zero prior knowledge; (4) requiring zero commonsense or complex reasoning (unlike [12, 47])-i.e., a strong language model is of little use here when it is non-natural for humans to describe BlindTest images in language (for a brain thinking in language step-by-step to solve it).\nThe ARC benchmark [12,31] also contain abstract images made up of simple shapes; however they challenge VLMs on understanding and reasoning based on those patterns. ARC assumes VLMs can identify the abstract shapes in order to reason. In contrast, our BlindTest directly evaluates VLM capabilities in recognizing the primitive shapes.\nImproving VLM vision capabilities Most recent recipes for improving SOTA VLMs involve finetuning a pretrained LLM coupled with vision encoders to solve high-level vision tasks [22]. Such late-fusion approaches fuse visual representations learned from the tokenized image with a powerful thinking brain [20,21,30].\nHowever, current vision approaches for VLMs are facing challenges as models sometimes are \"blind\" unable to see natural objects exist in a real photo [40]. In contrast, we are showing VLMs are visually impaired at low-level abstract images, e.g., inability to count 6 overlapping circles or 3 nested squares. Our circled letter \u00a73.3 is inspired by VLM abilities in recognizing content inside a red circle over real objects in natural images [36, 42, 43]. However, we show that VLMs can fail at a low-level, optical character recognition as opposed to recognizing real objects. To the best of our knowledge, no prior attempts have been made to address the exact limitations raised in our paper: (1) identifying and counting simple lines, shapes and geometric primitives when they interact (\u00a74.1 to \u00a74.5); (2) following colored paths (\u00a74.6). Solving these limitations may be the foundation for VLMs to progress on some existing vision benchmarks on graphs, e.g., [18], visual math [25] and blindness in natural images (e.g., understanding the direction an object is facing [40])."}, {"title": "6 Discussion and Conclusion", "content": "We propose BlindTest, a benchmark of seven novel low-level visual tasks that did not exist on the Internet before and requires minimal to zero knowledge. By design, our tasks minimize the chance that VLMs can solve by memorization or by not using the input image\u2014an issue in some prior benchmarks [11].\nWe document shocking findings that the best commercial VLMs are still struggling with tasks that can be easily solved by a five-year-old (e.g., telling whether two circles or lines intersect \u00a74.2). These limitations may be largely due to the late-fusion approach [8,23] of integrating vision into an LLM model and that early-fusion [38,39] may be the way forward. We find that simply finetuning a 7B, SOTA late-fusion open-source VLM [14] using LORA on our tasks does not yield a high-performing model (\u00a7A.6). That is, training a single model that performs well on BlindTest might be non-trivial, interesting research direction."}, {"title": "A Two touching circles task", "content": "A.1 Benchmark Construction Details\nTo create our benchmark, we use 5 parameters to control the diversity of the samples.\nColor: We fix the colors for each circle to use {magenta, dodgerblue}.\nImage size: We use the physical size, and the DPI arguments in matplotlib to initialize the image size. The physical size is fixed to 5 \u00d7 5, and the DPI \u2208 {100, 200, 300}. The output image sizes are {384, 769, 1155}px.\nDiameter: We use uniform diameters for both circles and choose the value proportional to the image size, where the diameter is {$\\frac{1}{8}$,$\\frac{1}{6}$,$\\frac{1}{5}$,$\\frac{1}{4}$} of the image size.\nDistance: The boundary-to-boundary distance between circles is a fraction of the diameter chosen from {-0.15, -0.1, -0.05, 0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5}. Based on our definition, center-to-center distance is (2+distance)\u00d7diameter.\nRotation: We include 2 main rotations (vertical and horizontal), and 2 diagonal rotations.\nWe use the center of the image as the origin so that it always aligns with the midpoint of distances between two circles. This systematic process results in a benchmark comprising 672 images (see Fig. 17).\nCode The code is available at https://anonymous.4open.science/r/Benchmark-85F0/src/TouchingCircle/TwoTouchingCircles.ipynb."}, {"title": "A.2 Finding: image resolution does not affect VLMs performance", "content": "Fig. 18-left shows that VLMs are almost invariant to the image resolution. For instance, GPT-40 and Sonnet-3's performance saturates at 769px, and Sonnet-3.5 slightly performs worse at 769 and 1155px compared to 384px. Gemini-1.5, however, is more consistent across different resolutions. That is, seeing the intersection of two circles does not depend on the quality of the image."}, {"title": "A.3 Finding: the vertical rotation closes the gap between models' performance", "content": "As shown in Fig. 18-middle, arranging the circles in vertical rotation causes the models to perform similarly on the benchmark. Although Gemini-1.5 slightly performs better at diagonal and Sonnet-3.5 at horizontal rotation, VLMs perform relatively better at vertical rotation. This suggests that the task complexity due to various rotations is not the main source of low performance in VLMs."}, {"title": "A.4 Finding: Increasing the distance improves the VLMs' accuracy", "content": "VLMs perform better when the distance increases from zero to positive values (see Fig. 18-right). However, Sonnet-3.5 is more conservative than other VLMs that mostly answer \u201cYes\u201d, which results in its lowest performance at negative distances."}, {"title": "A.5 Finding: VLMs prefer a specific rotation", "content": "Table 8 shows that VLMs prefer different rotations. For example, GPT-40 performs best at vertical, Gemini-1.5 at diagonal, Sonnet-3.5 at horizontal, and Sonnet-3 at vertical."}, {"title": "A.6 Results for fine-tuning Bunny on the two touching circle", "content": "In order to determine if fine-tuning could improve the model's performance on this task we attempted to fine-tune Bunny [15] on the two touching circles task. We fine-tuned Bunny using datasets of sizes: 10K, 20K, 50K, and 100K samples, each containing a balanced number of instances where the circles are either overlapping or separate (equal number of YES/NO answers in the training set).\nThe baseline model, without any fine-tuning, achieved 17.1% accuracy for task overlap and 11.7% for touching circles. After fine-tuning, we observed improvements with smaller datasets, such as 10K and 20K cases, where accuracy reached up to 36.8%. However, increasing the number of samples did not lead to better performance. In some instances, such as with the 50K dataset, the model failed to predict anything and only generated the end-of-text token.\nThe loss values for all these experiments were very close to zero, indicating that the model overfits the training set but fails to generalize. This suggests that training on this task is not straightforward and may require a combination of multiple tasks or that this problem does not have a simple solution.\nFig. 19 shows the accuracy breakdown by distance of the two circles. The model's performance improves when the circles are overlapping, but when there is a long distance between them, the model does not generalize well and cannot provide accurate answers."}, {"title": "A.7 Additional examples", "content": "We show examples of models' responses to the prompts on the two touching circles task in Fig. 20."}, {"title": "B Task 3: Identifying circled letter in word", "content": "B.1 Task construction\nEach image is created using a combination of the below variables.\nWord: The word is chosen among 3 candidates {Acknowledgement, Subdermatoglyphic, tHyUiKaRbNqWeOpXcZvM }. Each word has unique features, e.g., variable letter size (Acknowledgement), non-repetitive letters (Subdermatoglyphic), and non-existing word (tHyUiKaRbNqWeOpXcZvM).\nLetter: We draw the oval (0) over all the letters in a word.\nFont: We use 2 different font styles for each word, OpenSans and Helvetica.\nOval (O) thickness: We generate the with 3 various line thicknesses.\nScaling factor: Since each letter has a unique size, we use a scaling factor to control the size of the O.\nPadding: We use four values for padding {25, 50, 100, 200}, by which we change the word's position on the image. This ensures that the word is not always in the center of the image.\nFinally, we render the text on a white canvas with a size of 512\u00d7512 pixels, and we get 180 images for Acknowledgement, 204 for Subdermatoglyphic, and 240 samples for tHyUiKaRbNqWeOpXcZvM (see Fig. 21).\nCode The code is available at https://anonymous.4open.science/r/Benchmark-85F0/src/CircledWord/GenerateSamples.ipynb.\nPost-processing: We do not ask VLMs to provide answers in a pre-defined format. Instead, we write code to extract predicted labels from VLM free-form response."}, {"title": "B.2 Finding: VLMs mostly confuse the adjacent character for the circled letter", "content": "Models often mistake the neighboring characters as the circled letter (see Table 9), GPT-40 33.67% in the word Subdermatoglyphic (the lowest) and Sonnet-3.5 90.62% in the word Acknowledgement (the highest). Fig. 22 depicts that in the word Acknowledgement, where letter \"m\" is surrounded by two \u201ce\u201d, Sonnet-3.5 mispredicts \u201ce\u201d for \"m\u201d 38 times out of 48 (79.16%). Gemini-1.5, in the word tHyUiKaRbNqWeOpXcZvM, where letters \u201cK\u201d and \u201ca\u201d are adjacent, mispredicts \u201ca\u201d for \"K\" 35 times out of 48 (72.91%) (see Fig. 23)."}, {"title": "B.3 Finding: GPT-40 and Gemini-1.5 confuse the red oval as part of the letter", "content": "Figs. 24 and 25 show that Gemini-1.5 and GPT-40 sometimes fail to recognize that the red oval is not part of the letter. Gemini-1.5 predicts \"o\" when the circled letter is not \"o\" (see Fig. 24), and GPT-40 often predicts \"o\" regardless of what letter is circled (Fig. 25). Gemini-1.5 also predicts instead of \"c\", and GPT-40 predicts \"@\" instead of \u201ca\u201d."}, {"title": "B.4 Finding: font selection does not vary the performance of the models", "content": "As shown in Fig. 26, models do not show a significant variance over different fonts, suggesting our choice of font is not a reason for their ill-performance."}, {"title": "B.5 Finding: models are invariant to our choice of prompts", "content": "Our choice of prompts has almost no impact on the performance of the models as depicted in Fig. 27."}, {"title": "B.6 Additional Examples", "content": "Examples from our evaluation of VLMs on the circled letter task are shown in Fig. 28"}, {"title": "C Counting the number of line intersections task", "content": "C.1 Benchmark Construction Details\nTo create our benchmark,we use 5 parameters to control the diversity of the samples.\nColor: We fix the colors for each line to use {blue, red}\nX-coordinate: For each line, we choose three uniformly distributed x coordinates on the canvas to plot the x values of lines.\nY-coordinate: We randomly pick three y values for each line on the canvas.\nLine thickness: We vary the line thicknesses with standard matplotlib values (2, 3, and 4).\nNumber of intersections: We count the intersections based on the three points defined for each line ((x, y1), (x, y2), and (x, y3)).\nWe repeat the process until we have 50 samples of 0, 1, and 2 intersections, resulting in 150 images (see Fig. 29).\nCode The code is available at https://anonymous.4open.science/r/Benchmark-85F0/src/LineIntersection/GenerateSamples.ipynb."}, {"title": "C.2 Finding: line thickness does not influence VLM ability to count line intersections", "content": "Fig. 30 depicts that increasing the line thickness in our plots does not help the VLMs see and count the intersections of 2D lines. Interestingly, Sonnet-3.5 performs best on the thinnest line which suggests the model can see the thin lines, but intersections are not clear to its vision."}, {"title": "C.3 Finding: different prompts result in similar accuracy", "content": "Our choice of prompts has minimal impact on the models' performance as shown in Fig. 31."}, {"title": "C.4 Additional Examples", "content": "We show examples of models' responses to the prompts on the counting the number of line intersections task in Fig. 5."}, {"title": "D Counting the number of nested squares", "content": "D.1 Benchmark Construction Details\nWe use 5 parameters to create the images of nested squares.\nDepth: For each image, we draw N \u2208 {2,3,4,5} nested squares on the image. We refer to each square in this collection as a depth.\nInitial size: We choose a random size for the first square in the bounds of the image size.\nReduction factor: We draw squares such that each depth is entirely contained by its previous depth. We use a reduction factor to scale the square sizes.\nCenter: The first square's center is chosen to ensure it is entirely visible in the image. For the remaining squares, we choose the center based on the space between the previous square and the new reduced size.\nLine thickness: We use standard matplotlib line width parameter of (2, 3, 4).\nWe continue to generate images until we have 30 samples for each depth, resulting in 120 images overall (see Fig. 33).\nCode The code is available at https://anonymous.4open.science/r/Benchmark-85F0/src/NestedSquares/GenerateSamples.ipynb.\nPost-processing: We do not ask VLMs to provide answers in a pre-defined format. Instead, we write code to extract predicted labels from VLM free-form response."}, {"title": "D.2 Finding: the best performing models are unaffected by line thickness", "content": "Table 10 depicts that the two best VLMs (Gemini-1.5 and Sonnet-3.5) on the nested square task do not show any sensitivity to the visual attributes of the squares such as the line thickness. However, GPT-40 and Sonnet-3 have the opposite trend as the line thickness changes, suggesting GPT-40 confuses squares when the borderline is thick, but, Sonnet-3 can see the squares easier with thicker lines."}, {"title": "D.3 Additional Examples", "content": "We show examples of models' responses to the counting the number of nested squares task in Fig. 34."}, {"title": "E Counting the shapes in an Olympic-like logo", "content": "E.1 Benchmark Construction Details\nWe create the benchmark by generating images containing shapes resembling the Olympic logo by choosing a combination of settings.\nImage size: We fix the physical size of the image in matplotlib to 5\u00d75, and change the resolution by changing the DPI value, which is \u2208 {100, 200, 300} to get images with sizes (384, 769, 1155}px.\nNumber of shapes: We choose a number from {5, 6, 7, 8, 9}.\nColor: Each image is generated using two different coloring schemes. We generate an all-black version and a second version by randomly sampling colors from a colormap in matplotlib.\nDistance: To generate the interlaced shapes, we use a small boundary-to-boundary distance factor for each row of the shapes. We fix this value to 0.1 proportional to the diameter of circles or side length of pentagons.\nDiameter: We choose a uniform diameter for all the circles in each image from {$\\frac{1}{8}$,$\\frac{1}{6}$,$\\frac{1}{5}$,$\\frac{1}{4}$} proportional to the image size.\nSide length: We follow the same policy for the diameter to choose the side length of the pentagons.\nLine thickness: We generate each image with {0.5, 1.0} line width of matplotlib standard numbers.\nWe center the shape collection on the center of the image in two rows. We ensure for an odd number of shapes, the first row has one more shape, and draw the second row such that the shapes' centers align with the midpoint of the distances in the first row. For even numbers, we repeat the same process as odd numbers. However, we draw an extra shape on the left or right side of the second row randomly. We generate 120 images (see Fig. 35), 60 images of circles, and 60 images of pentagons.\nCode The code is available at https://anonymous.4open.science/r/Benchmark-85F0/src/CountingCircles/0lympicCircles.ipynb."}, {"title": "E.2 Finding: different resolutions have no impact on most VLMs' performance", "content": "Fig. 36-a shows that VLMs are invariant to the resolution when asked to count the overlapping shapes. This suggests that the image quality has almost no effect on the performance, and VLMs cannot see the shapes."}, {"title": "E.3 Finding: color-coding does not generally help the VLMs", "content": "While we expect the color-coding to make the shapes more distinct for the models, Fig. 36-b suggests that, except for GPT-40, coloring the shapes has an opposite effect on the performance of the models."}, {"title": "E.4 Finding: Gemini-1.5 is biased to the Olympic logo", "content": "Fig. 37 shows the overall trend of the predictions among SOTA VLMs. Gemini-1.5 (see Fig. 37b) significantly tends to predict \"5\" when asked to count the circles, while the predictions are more random for pentagons. This suggests the model's bias toward the Olympic logo. Among all the models, Sonnet-3.5 has the least variance (see Fig. 37d)."}, {"title": "E.5 Additional Examples", "content": "We provide examples of VLMs' responses in Figs. 11 and 38"}, {"title": "F Counting the rows and columns of a grid task", "content": "F.1 Benchmark Construction Details\nOur benchmark specifications consist of various parameters for the grid generation process.\nImage size: We include three different sizes {500, 1250, 2000} to create the grids on the image.\nNumber of rows/columns: We choose a base size N \u2208 {3, 4, 5, 6, 7, 8, 9}, and initialize the sizes to N \u00d7 N, N \u00d7 N', and N' \u00d7 N where N' = N + 1.\nLine thickness: We use 10 and 20 pixels for the borderline thickness.\nEntry: Each table is generated in two versions, one that includes blank entries, and the second with random text entries.\nWe divide the image size by the number of rows and columns to find the coordinates for drawing the borderlines. Then, we draw the lines on the four edges of the image and draw the remaining lines in between. Our benchmark comprises 444 images of blank and text-containing grids (see Fig. 39).\nCode The code is available at https://anonymous.4open.science/r/Benchmark-85F0/src/CountingRowsAndColumns/Grids.ipynb."}, {"title": "F.2 Finding: VLMs cannot count the grid cells along different axes", "content": "We expect that counting both rows and columns successfully to be harder for the VLMs, thus we analyze counting the rows and grids individually to see how VLMs perform. As shown in Table 11, VLMs cannot count whether rows or columns."}, {"title": "F.3 Additional Examples", "content": "We show examples of models' responses to the counting the number of rows and columns task in Figs. 40 and 41."}, {"title": "G Following single-colored intersecting paths task", "content": "G.1 Benchmark Construction Details\nOur subway-like graphs are generated using a set of parameters defining the characteristics of the plot.\nImage size: We use two different sizes {512, 1024}px for the images to include various resolutions.\nGrid size: We assume a hypothetical grid on the image that determines the position of the paths. We used an 18\u00d718 grid, which means each path segment is $\\frac{1}{18}$ of the image size.\nNumber of stations: We use four station labels, {A, B, C, D}.\nStarting points: Each station in our maps has three different starting points which are exactly $\\frac{1}{18}$ of the image size to one side of the stations.\nPath thickness: We use two line thicknesses, 10 and 20 pixels to have bold and light visualizations of the same path.\nNumber of paths: Considering the number of starting points in our setup, each image can include stations from which exactly 1, 2, or 3 paths exit.\nWe keep generating the images until we have 15 samples for each number of paths which results in 180 images (see Fig. 42).\nCode The code is available at https://anonymous.4open.science/r/Benchmark-85F0/src/SubwayMap/SubwayMap.ipynb"}, {"title": "G.2 Additional Examples", "content": "We show examples of models' responses to the counting the number of single-colored connecting paths in Figs. 43 and 44."}]}