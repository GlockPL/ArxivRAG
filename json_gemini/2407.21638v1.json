{"title": "Quality Control for Radiology Report Generation Models via Auxiliary Auditing Components", "authors": ["Hermione Warr", "Yasin Ibrahim", "Daniel R. McGowan", "Konstantinos Kamnitsas"], "abstract": "Automation of medical image interpretation could alleviate bottlenecks in diagnostic workflows, and has become of particular interest in recent years due to advancements in natural language processing. Great strides have been made towards automated radiology report generation via AI, yet ensuring clinical accuracy in generated reports is a significant challenge, hindering deployment of such methods in clinical practice. In this work we propose a quality control framework for assessing the reliability of AI-generated radiology reports with respect to semantics of diagnostic importance using modular auxiliary auditing components (ACs). Evaluating our pipeline on the MIMIC-CXR dataset, our findings show that incorporating ACs in the form of disease-classifiers can enable auditing that identifies more reliable reports, resulting in higher F1 scores compared to unfiltered generated reports. Additionally, leveraging the confidence of the AC labels further improves the audit's effectiveness.", "sections": [{"title": "1 Introduction", "content": "Radiologists are facing mounting challenges managing the growing volume of medical imaging data under resource constraints, necessitating more efficient and innovative solutions for image analysis and interpretation. Automated (or partly automated) radiology report generation is one such solution. Since the introduction of transformers [28], which significantly enhanced language generation, numerous studies have investigated radiology report generation from medical images, particularly chest X-rays (CXR) [4, 7, 9, 15, 22, 26]. However, deployment of language models to medical applications faces a variety of challenges, with a key area being the need for factual accuracy. Various methods have been developed to address this - particularly useful have been the introduction of clinical metrics to supplement the standard natural language processing (NLP) metrics used to evaluate report generation quality [4, 18]. Previous efforts to improve the quality of the generated reports have incorporated extra information during training, such as bounding boxes for improved localisation, or have aimed to refine diagnostic accuracy by including loss functions that reward clinical accuracy [5, 7, 21, 26]. Additionally, more complex architectures, such as memory-augmented attention, have been developed to facilitate better image understanding [6,21].\nBesides efforts to improve report generation accuracy, another line of work is error detection in language modeling. Most work in this area has targeted the natural language domain. The two main types of uncertainty estimation in large language models (LLMs) are statistical uncertainty (SU) and in-dialogue uncertainty (IDU) [27]. The latter relies on large pretrained models to admit when they don't know an answer [13, 16]. SU relates certainty to the model's output entropy, though in language modelling, it must address the coherence of entire sentences, as well as on a per token basis [14, 20]. These methods often depend on extensive labelled datasets in the natural language domain, which do not adequately capture the specialised terminology and context of clinical settings [27]. Few works have investigated the possibility of error detection in reports written by radiologists [8,12,29,30]. \u03a4\u03bf our knowledge, no previous work has investigated error detection for automated report generation, which is the focus of our study.\nWe introduce an auditing framework for identifying potential errors in AI-based radiology report generation, using modular auxiliary auditing components (AC) for quality control. Our study focuses on chest-Xray (CXR) imaging, defining ACs as image-based disease classifiers to extract semantics relevant to diagnosis. We develop a report generation model, GenX, which performs competitively with previous works and serves as basis for testing our auditing framework. Reports are audited using ACs that provide disease classification and confidence levels. Reports with consistent diagnoses across the pipeline meet our criteria, while inconsistencies flag potential errors. Additionally, by leveraging the ACs' classification confidence, we enforce stricter quality control, requiring consistency with the highest confidence predictions. Our evaluation on MIMIC-CXR shows a significantly higher F1 score for reports meeting the audit's criteria. Results demonstrate that consistency between report and auxiliary components is a promising auditing mechanism for automated report generation."}, {"title": "2 Methodology", "content": "2.1 Report Generation Model - GenX\nThe proposed framework for auditing radiology report generation is shown in Fig. 1. The backbone of the framework is the report generation model. The generalised vision language modeling problem of generating a textual sequence T given an input image I can be framed as estimating the conditional distribution p(T|I), as a product of the conditional probabilities:\n$$p(T|I) = \\prod_{l=1}^{L} p(T_l | T_{<l}, I; \\theta),$$(1)\nwhere T is modelled as a sequence of word tokens {T1,...,TL}, with T<l being the set of tokens preceding Tl, where L is the number of tokens in the sequence, and \u03b8 the model parameters [1,22].\nAn overview of our report generation model, GenX, is shown in Fig. 2. The model is trained to maximise the probability of generating the target sequence from the image, p(T|I) from eq 1. Image features Ifeat = f1(I) \u2208 RD1, with dimensionality D1, are extracted by image encoder f1. A linear layer, f1,emb, projects Ifeat to the space of token embeddings that will be input to the language generator, giving Iemb = {Iemb,1 ... Iemb,M} = f1,emb(Ifeat) \u2208 RDT\u00d7M, a set of M image token embeddings with dimensionality DT, similarly to LLaVA [19]. A tokeniser maps text to a set of L token vectors {T{}1:L \u2208 RL\u00d7V, where V is the vocabulary size. Each Ti is embedded by a linear layer to Ti,emb = fT,emb(Ti) \u2208 RDT. The concatenated embeddings of image and text tokens {Iemb,1 ... Iemb,M,Temb,1 ... Temb,L} are passed to a Transformer decoder which predicts a probability distribution over V vocabulary tokens, expressing which is more probable in the sequence. The decoder has Nlayers with Attention blocks that each has Nheads"}, {"title": "2.2 Auditing Generated Reports via Auxiliary Classifiers", "content": "While some types of mistakes in AI-generated radiology reports, such as grammatical errors, are merely inconvenient, semantic mistakes related to disease diagnosis can be detrimentalor patient care. Consider a semantic concept of interest, denoted c. We focus on the case where c represents a specific disease, with the class label C\u2208 {1,0} indicating the presence or absence of that disease. To audit whether a generated report TG is semantically correct about c, we need to extract the value CT described in text TG via a mapping function gT:TG\u2192c. This can be any text-based classifier, or prompt-based LLM that can answer questions in the form \"Does this text suggest evidence or not for disease c?\", which have become widely available. Herein we use a pretrained CheXbert text-classifier [25] as gr, obtaining CT = gT(TG).\nWe then introduce to the framework an independent model gI : I\u2192c, which infers value CI for concept of interest c from image I (Fig. 1c). We term such models auxiliary auditing components (AC). In the examined case when c is existence of specific disease, gI is a classifier predicting CI =1 if image I shows the disease or CI =0 otherwise, with model confidence PAC(c=CI|I). If multiple concepts c are of interest (e.g. different diseases), an independent AC model can be used per c, for ease of modular development, or a multi-label AC model.\nAuditing is performed by comparing values CT and CI about the concept c of interest (e.g. disease) extracted from the report or the image respectively. To consider the contents of the report reliable with respect to c, agreement between the report and AC is required, i.e. CT = CI. We further extend this by taking into consideration the confidence PAC of the AC. We only consider the auditing successful if it satisfies the additional requirement that the value CT = CI has been inferred by the AC with confidence PAC over a pre-determined threshold t. Formally, for a successfully completed auditing we require:\n$$(C_I = C_T) \\wedge (P_{AC}(c = C_I | I) \\geq t),$$(2)\nwhere is the logical and operator. The set of generated reports that pass the audit for a particular concept c is denoted by TG,pass = {TG| Eq. 2 is True}.\nThe remainder, TG,err = {TG| Eq. 2 is False}, can be deferred to the human user for review as potentially inaccurate for that disease.\nThe framework is designed based on ACs that are modular, independent models. The design draws inspiration from N-Version programming [3] and the principle of redundancy for reliability in software engineering. Assuming independence of components (GenX, g\u0442, gI) with label error rates ect and ecI, the error rate of the auditing framework is e = \u0435\u0441\u0442\u0435\u0441I, which is lower than the individual error rates of CT and CI, as failure of auditing requires both components to fail simultaneously to incorrectly satisfy Eq. 2. Modular ACs enable independent component training and validation prior to integration into the audit pipeline."}, {"title": "3 Experiments and Results", "content": "3.1 Data\nExperiments are performed using MIMIC-CXR [11], containing chest X-rays (CXR), associated radiology reports, and disease labels for ~228,000 studies of over 65,000 patients. As common practice, we consider only the frontal-view images, and only the \"findings\" section of the reports, which describes evidence of pathologies and support devices. The labels include 14 classes (12 diseases, 1 support devices, 1 no findings) and can be positive, uncertain, negative, or not mentioned. We report 5 out of 14 classes separately (Atelectasis, Cardiomegaly, Consolidation, Edema, Pleural Effusion) to facilitate comparison with literature [10]. We divide data into splits for training (226,261), validating hyperparameters (1,864), and testing performance (3,595). To train and evaluate the report generation, we further exclude studies that do not include a \"findings\" section, resulting in 155,322 training, 1,231 validation and 2,607 testing samples. Finally, to help address class imbalance and noisy labels, following past works [9, 10, 21, 26], we consider uncertain disease labels as positive and class no-mention as negative.\n3.2 Evaluating the Report Generation Model\nExperimental settings: We here show that our report generation model, GenX, is of representative quality in comparison to previous work to serve as effective backbone for the study of our auditing framework. To build GenX we use ResNet-50 as the image encoder f1, which we pre-train as a multi-label classifier of the 14 classes described above. Its penultimate layer of dimension D1 = 512 is then projected to M = 10 image token embeddings with DT=512. These are concatenated with up to L=512 text embeddings, resulting from a GPT-2 tokeniser (V\u2248 50, 257) [24], followed by a projection, DT=512. The Transformer has Nlayers =8, Nheads =8, and dimension Dff =2048 of its feed-forward layer. GenX is then trained end-to-end , learning parameters of the randomly initialised projections and Transformer, and fine-tuning the f1 encoder.\nResults: Table 1 reports test performance by our report generation model, along with previous works. NLP metrics BLEU (BL) [23], METEOR (M) [2] and ROUGE-L (R-L) [17] assess generated language quality. More important for this study is the assessment of diagnosis-related semantics in generated reports, using disease-classification metrics such as F1 score, by comparing agreement of class labels, CT, extracted from generated and reference reports via CheXbert [4]. Our report generator, GenX, achieves NLP metrics similar to Ratchet [9], which has the most comparable architecture. Importantly, GenX achieves a high F1 score in comparison to Ratchet, R2Gen [4], even RGRG [26] that needs additional labelled bounding boxes for training. It approaches F1 score by M\u00b2 Trans [21] that trains using a more complex framework and reinforcement, and is surpassed by At+TE+RG [7] which uses extra labels for training. We conclude that GenX produces reports of sufficient quality to serve as effective backbone for"}, {"title": "3.3 Evaluating the Auditing Framework for Generated Reports", "content": "Experimental settings: We analyse whether including auxiliary auditing components, ACs, enable us to quality control the generated reports and filter out those that are more likely to contain errors. We evaluate on the 5 diseases most commonly reported in the literature , from the CheXpert competition [10]. For this, we first train 5 independent disease-specific AC image classifiers (AC1), where each is a ResNet-18. We then apply the framework as described in Sec. 2.2 to the test-split. For each image, the 5 AC1 models predict 5 disease labels C(c)I, one per disease c. GenX generates a report for each test image and we extract 5 report-based class labels C(c)T via Chexbert, for each disease. For a given scan, per disease c, we compare image and report based labels, C(c)I and C(c)T, to check if they fulfil Eq. 2. We perform the check in two settings: First, without considering a confidence threshold for PAC, satisfying only the first condition in Eq. 2; Second, with a confidence threshold t = 0.8 where Eq. 2 is fulfilled if PAC \u2265 0.8, to investigate whether stricter auditing criteria results in more accurate reports For each disease, the images that fulfil the above criteria are considered to have successfully passed the audit. For these images, we calculate average F1 score by comparing their report-based labels C(c)T with the reference report labels, to determine whether the average score is higher than that of all unfiltered reports generated by GenX. Finally, we repeat these experiments using a bigger ResNet-50 AC, trained as a multi-label image classifier of all 14 classes (AC14). This is to evaluate whether it's beneficial to integrate a larger AC that learns from all 14 labels available per scan, or independent, disease-specific ACs.\nResults: Table 3 shows the results. Class labels Cr extracted from reports generated by GenX achieve an average F1 score of 46.8 over the 5 diseases. Reports that agree with AC1, fulfilling the auditing without the extra requirement for confidence PAC \u2265 t, achieve average F1 score 54.3. These benefits are realised over a significant percentage of the reports (72-80% across the diseases), while the rest would be deferred for user inspection and error correction in an actual workflow. Reports that pass the stricter auditing and additionally fulfil the confidence requirement PAC \u2265 0.8 present even less errors, with average F1 score 58.4. This restricts the number of reports that pass the audit (30-65%), in return for higher reliability. In comparison to the unfiltered reports of GenX, the reports that fulfil the audit show increased F1-scores for 4 out of 5 studied diseases. The exception is Consolidation, the class where both GenX and AC image classifiers have very low performance. Interestingly, for 3 out of 5 diseases, the reports that successfully pass the audit show Fl-scores that are even higher than the auditing AC1 classifiers. These results show that measuring consistency between semantics of reports and auditing classifiers is a promising framework.\nFinally, we find that a single, multi-label classifier, trained for all 14-classes as AC14, achieves significantly lower F1 scores for image classification, which also leads to lower effectiveness of the auditing. Although AC14 is a ResNet-50, larger than ResNet-18s used for the 5 AC\u2081 classifiers, it is difficult to develop it at the level of disease-specific classifiers. This demonstrates the benefits of modularity, with independently developed AC for each concept of interest."}, {"title": "4 Conclusion", "content": "We introduced a framework to quality control AI-generated radiology reports with respect to semantics of interest via auxiliary auditing components. Our study focused on the task of report generation from chest-Xrays, detecting potential semantic errors in diagnosis-related report content, using auxiliary disease classifiers and assessing consistency between labels inferred from image and report. Experiments show that reports fulfilling the auditing criteria exhibit fewer errors, with average F1 score that can even exceed the score of the auditing components. The framework is generic and future work could explore it for auditing semantic concepts beyond disease classification, such as for description of pathology location or volume, using regression models as ACs for such properties.\nThe limitation of the framework, as with any filtering pipeline for quality-control, is the inherent trade-off between reducing the number of reports that successfully pass the audit and can be used for down-stream workflow, in exchange for their higher reliability. Future work could improve performance in both aspects by integrating more potent models for inferring image-based and report-based labels, thereby ensuring greater consistency and reliability from their comparison."}]}