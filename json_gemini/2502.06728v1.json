{"title": "FlexDeMo: Decoupled Momentum Optimization for Fully and Hybrid Sharded Training", "authors": ["Mogens Henrik From", "Jacob Nielsen", "Lukas Galke", "Peter Schneider-Kamp"], "abstract": "Training large neural network models requires extensive computational resources, often distributed across several nodes and accelerators. Recent findings suggest that it may be sufficient to only exchange the fast moving components of the gradients, while accumulating momentum locally (Decoupled Momentum, or DeMo). However, when considering larger models that do not fit on a single accelerate, the exchange of gradient information and the integration of DeMo needs to be reconsidered. Here, we propose employing a hybrid strategy, FlexDeMo, whereby nodes fully synchronize locally between different GPUs and inter-node communication is improved through only using the fast-moving components. This effectively combines previous hybrid sharding strategies with the advantages of decoupled momentum. Our experimental results show that FlexDeMo is on par with AdamW in terms of validation loss, demonstrating its viability.", "sections": [{"title": "1 Introduction", "content": "Training large deep neural networks (DNNs) requires a high traffic of gradients being transmitted between accelerators, often served by high-throughput network setups on high-performance computing clusters. The network throughput is particularly challenging, as the number of accelerator nodes participating in the training increases and the general network congestion increases. Recent work demonstrates that synchronizing the full optimizer state is not always necessary for state-of-the-art results through decoupling momentum updates and allowing controlled divergence within each rank by carefully controlled inter-accelerator communication [4]. This demonstrates a viable way of distributed training with reduced gradient communication and, thus, enables the distributed data parallel (DDP) training of DNNs with relatively low network bandwidth requirements. However, this strategy also comes with the constraint that the DNN model must fit within the memory capacity of each accelerator, limiting the applicability for training large models, such as state-of-the-art large language language models, which commonly do not fit within the memory of one accelerator."}, {"title": "2 Background and Related Work", "content": "In this paper, we introduce the Flexible Decoupled Momentum optimizer (FlexDeMo) for fully sharded data parallel (FSDP) training. This optimizer can employ a hybrid-sharding strategy, where the model and optimizer states are sharded intra-node and replicated between nodes. Instead of synchronizing the full gradients between the nodes, we employ the compression and decompression of selected relevant momentum components following the approach that the DeMo optimizer has introduced for DDP training [4]. In this way, inter-node communication bandwidth requirements are reduced while constraints regarding the accelerator memory requirements can be relaxed. Effectively, this allows for the decoupled momentum training of models that fit into the combined memory of the accelerators of one node instead of having to fit into the memory of a single accelerator.\nOur results show that FlexDeMo performs comparable to AdamW with full gradient synchronization. In essence, FlexDeMo enables training of larger DNNS more efficiently across both nodes internally within a cluster of nodes and even across such potentially geographically-dispersed clusters. Our implementation is available on GitHub\u00b9 and the Python Package Index\u00b2.\nBasic strategies to accelerate neural network training include data paralleism (duplicate model, split data, average gradients), model parallelism (distributes the model, but increases communication overhead), and tensor parallelism, where even individual tensors are split across devices. Another common strategy is to optimize locally for multiple steps before synchronization [6].\nThe zero redundancy optimizer (ZeRO) [5] has introduced a variety of strate-gies to train large models, including partioning optimizer states, gradients, and parameters all of which are being pulled dynamically to a single accelerator on demand. While ZeRO aims at memory-efficiency to enable training of very large models, it also comes with a severe communication overhead, which both DeMo [4] and our proposed FlexDeMo aim to improve upon.\nFSDP [8] takes inspiration from ZeRO, but makes a few modification to the exchange of gradients: ZeRO uses a reduce-scatter to distribute the gradients and an all-gather for the updated parameters. FSDP instead uses an all-gather to recover unsharded parameters and a reduce-scatter for the gradients. FSDP also introduces a hybrid sharding strategy to allow for a trade-off between memory and throughput by controlling the sharding groups and allowing for gradient accumulation with or without communication.\nThe essence of DeMO [4] is to reduce communication overhead by keeping optimizer states local and only communicating fast moving components. DeMo introduces two hyperparameters to control the extraction of fast moving compo-nents. First, the optimizer states are chunked, and then, the top few fast moving components are extracted. However, the reduction of communication overhead"}, {"title": "3 Method", "content": "In this section, we present our hybrid sharded FSDP decoupled momentum optimizer FlexDeMo, which allows divergent optimizer states across accelerator nodes\u00b3, guided by the fast moving components of the momentum. These fast-moving components can be shared efficiently with relatively low network require-ments. This enables the efficient training of large language models on multiple nodes. First, we present our optimization algorithm. Then, we document our experimental setup for validating the implementation."}, {"title": "3.1 Hybrid Sharded Fully Sharded Data Parallel Decoupled Momentum Optimizer", "content": "We extend the stochastic gradient descent (SDG) with momentum optimizer, introducing a series of changes, to support decoupled optimization in FSDP employing intra-node hybrid-sharding. We assume that the model is wrapped as an FSDP object. Both the forward and backward passes must be wrapped in a no_sync context manager, disabling automatic synchronization of gradients $\\theta_t$ affecting the default all-reduce-functionality, consequently decoupling the mo-mentum $m$ across accelerator-nodes. PyTorch's Autograd produces the gradient for the whole unsharded grad-parameter accesible in p.grad.\nFigure 1 shows a schematic representation of the communication. We employ the reduce-scatter operation, averaging and then sharding the computed gradients back to their respective ranks (hence sharded parameter-size) in the sharding-parallel-group. This allows us to work only on the shards in the subsequent operations. We denote the two communication-groups as S and R referring to the sharding-group and replication-group, respectively. Gradients are scattered intra-node locally within S and the fast components, q, are communicated inter-node in R. We describe the operation of FlexDeMo in Algorithm 1."}, {"title": "3.2 Experimental Setup", "content": "We conduct experiments with a standard T5-Base model with standard FSDP with hybrid sharding. We compare the AdamW-optimizer[3] with full gradient synchronization with our proposed FlexDeMo optimizer. The experiments are"}, {"title": "4 Results", "content": "In this section we describe the results a series of small empirical experiments for our method, while varying the TopK hyperparameter that determines the number of fast moving components to extract. In Table 1, we demonstrate that FlexDeMo is on par with AdamW both training and validation losses.\nWe see that the samples per second is not very different in this small-scale setup. These experiments should rather be considered a demonstration that our implementation works and produces the desired training outcome. We would arguably observe a more significant difference for larger model sizes, in less ideal network setups and/or when training with a larger number of nodes. For example, in the case of low-bandwidth network connections or congested networks, the benefits can be expected to be more pronounced. Initial experiments with larger decoder-only models (1B parameter OLMo architecture) on up to 64 nodes with a total of 256 accelerators indicate the potential of FlexDeMo for significant improvements to training speed and stability."}, {"title": "5 Conclusion", "content": "In conclusion, we have shown that FlexDeMo successfully extends DeMo opti-mization to setting of hybrid sharded FSDP. Our small-scale experiments indicate results comparable with AdamW, without any delay of convergence, and the potential to reduce the bandwidth needed for communication between nodes. This enables Flex DeMo to be used to optimize LLMs that do not fit within the memory of one accelerator and further support training LLMs in low-resource network settings, ultimately lowering the entry point for both practitioners and researchers to participate in the development of future models and use-cases."}]}