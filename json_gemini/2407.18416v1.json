{"title": "PersonaGym: Evaluating Persona Agents and LLMs", "authors": ["Vinay Samuel", "Henry Peng Zou", "Yue Zhou", "Shreyas Chaudhari", "Ashwin Kalyan", "Tanmay Rajpurohit", "Ameet Deshpande", "Karthik Narasimhan", "Vishvak Murahari"], "abstract": "Persona agents, which are LLM agents that act according to an assigned persona, have demonstrated impressive contextual response capabilities across various applications. These persona agents offer significant enhancements across diverse sectors, such as education, healthcare, and entertainment, where model developers can align agent responses to different user requirements thereby broadening the scope of agent applications. However, evaluating persona agent performance is incredibly challenging due to the complexity of assessing persona adherence in free-form interactions across various environments that are relevant to each persona agent. We introduce PersonaGym, the first dynamic evaluation framework for assessing persona agents, and PersonaScore, the first automated human-aligned metric grounded in decision theory for comprehensive large-scale evaluation of persona agents. Our evaluation of 6 open and closed-source LLMs, using a benchmark encompassing 200 personas and 10,000 questions, reveals significant opportunities for advancement in persona agent capabilities across state-of-the-art models. For example, Claude 3.5 Sonnet only has a 2.97% relative improvement in PersonaScore than GPT 3.5 despite being a much more advanced model. Importantly, we find that increased model size and complexity do not necessarily imply enhanced persona agent capabilities thereby highlighting the pressing need for algorithmic and architectural invention towards faithful and performant persona agents.", "sections": [{"title": "Introduction", "content": "As the applications of LLM agents continue to rapidly diversify (for example customer service chatbots (Nandkumar and Peternel, 2024), code generation (Ugare et al., 2024), robotics (Dalal et al., 2024), etc.), there is a growing need to adapt agents to align with different user specifications to enable highly personalized experiences for diverse applications and users. Persona agents, i.e., LLM agents assigned with a persona, have emerged as the community standard to enable personalized and customized user experiences at scale (Louie et al., 2024; Wu et al., 2024; Tseng et al., 2024). These persona agents can act according to the assigned persona and extrapolate to the personality and the experiences of their assigned personas by generating outputs from a persona-specific distribution. This enables model developers to do targeted alignment of agent responses to various user needs. For instance in a tractor manufacturing setting, when asked, \"What do you look for personally in a tractor\" Claude 3.5 Sonnet typically responds with an answer citing how \"as an AI assistant\" it does not have an opinion on tractors. However, when acting as a farmer agent, it responds with, \u201cFirst off, I'm lookin' for raw power...Fuel efficiency is mighty important. Diesel ain't cheap, and every dollar saved is a dollar earned.\u201d\nThese persona agents have demonstrated potential in diverse and personalized dialogue generation across various contexts (Li et al., 2023; Cui et al., 2023; Han et al., 2022; Salemi et al., 2023), enhanced performance in tasks such as mathematical reasoning, physics, and software development (Kong et al., 2024; Xu et al., 2023; Qian et al., 2023), and simulating human behavior for scientific research in domains such as psychology (Li et al., 2024; Huang et al., 2023; Zhang et al., 2024a).\nRecent research indicates that the capabilities of persona agents vary across different scenarios and models (Kamruzzaman and Kim, 2024; Liu et al., 2024). However, preliminary explorations to address this exhibit major limitations: (1) they utilize datasets with predetermined personas to initialize persona agents, thereby significantly restricting the evaluation of persona agents not included in the datasets; (2) the persona agents are not initialized in multiple environments relevant to the agent; and (3) these benchmarks often assess persona agents along a single axis of the agent's abilities (e.g., linguistic capabilities) and fail to provide comprehensive insights into all dimensions of an LLM agent's interactions when taking on a persona (Wang et al., 2024; Chen et al., 2022; Wang et al., 2023; Shen et al., 2023; Light et al., 2023).\nTo address these issues, we propose PersonaGym, the first dynamic evaluation framework for persona agents designed to assess agents' capabilities. This framework is motivated by the need for a multidimensional evaluation system of persona agents wherein the agent's capabilities to act along the different dimensions of agent actions across a multitude of environments relevant to the persona agent are assessed.\nPersonaGym begins with a dynamic environment selection phase, where an LLM reasoner chooses relevant environments based on the agent's persona from a diverse pool of 150 environments. Next, in the question generation phase, task-specific questions are created to probe the agent's interactions within each environment to evaluate the agent on each task. The LLM agent then adopts the given persona utilizing a carefully crafted system prompt and responds to the generated questions.\nTo enable large-scale automated evaluation for agent responses for any persona on any environment, we propose PersonaScore as the first automatic metric that encapsulates the overall capability of persona agents to act in accordance to their persona across diverse environments. PersonaScore, given expert-curated rubric outlines for each task in PersonaGym, first leverages LLM reasoners to provide tailored example responses for each possible score in the rubric to calibrate judgment with humans. PersonaScore then utilizes multiple state-of-the-art LLM evaluator models and ensemble their assigned scores to assess the agent's responses using the comprehensive rubrics.\nWe show through human evaluations that PersonaScore is strongly aligned with human judgment on persona agents. By enabling users to optimize different dimensions of agent performance, PersonaGym aims to support the development of more effective and tailored persona-based AI systems for diverse real-world applications.\nWe benchmark the capability of six open and close source LLMs (namely GPT 3.5, LLaMA-2-13B, LLaMA-2-70B, LLaMA-3-8B, Claude 3 Haiku, Claude 3.5 Sonnet) to act as persona agents in PersonaGym. These models were evaluated on 200 diverse personas encompassing 10,000 questions. Our results highlight the challenging nature of PersonaGym as even the latest SOTA models such as Claude 3.5 Sonnet are not able to outperform less advanced models such as GPT 3.5 at the level they do on other tasks and domains.\nWe observe from our results that a model's increased size or capacity is not a definite indication of its persona agent capabilities. For example we show that Claude 3 Haiku is very resistant to generating responses while being a persona agent despite being a SOTA model. This finding should provide motivation for future studies to carefully study the ability of all SOTA LLMs to be persona agents before deployment and to drive innovation towards producing highly capable and faithful persona agents.\nOur main contributions are as follows:\n1. Introduced PersonaGym, the first dynamic evaluation framework for persona agents in LLMs. Our findings show that model complexity does not guarantee enhanced persona agent abilities, underscoring PersonaGym's importance in assessing persona agents.\n2. Established PersonaScore as the first automatic metric to our knowledge to quantify the capabilities of persona agents on five agent evaluation tasks. These five tasks are all grounded in decision theory and make up the different decision aspects of persona agents."}, {"title": "Evaluation Tasks", "content": "In the context of persona agent evaluations, we define the environment as external settings or conditions within which agents operate and interact. Understanding how agents interact with their environment is crucial for assessing their performance and capabilities. Agent interactions are often the result of decisions made by the agent and therefore, a method of understanding the agents' decision-making could be used to evaluate the agents' interactions in their environments. To this end, we utilize decision theory, which is the field of study dealing with rationalizing and choosing actions in situations of uncertainty (Edwards, 1961; Slovic et al., 1977), to study how agents make decisions and interact with their environment based on their goals, beliefs, and the perceived outcomes of different actions. There are three categories in the decision theory, based on which we group our evaluation tasks:\nNormative Evaluation choosing optimal decisions in a given environment where \"optimal\" is determined in regards to a fully rational decision maker. Given the aforementioned theory, we introduce the Expected Action task wherein a persona agent is seeded in an environment and is given a scenario to probe the agent to choose an action to take based on the scenario. This action is then evaluated for optimality given the persona and the scenario provided to the agent.\nPrescriptive Evaluation prescribing how agents should act in a given environment. We group the tasks of Linguistic Habits, Persona Consistency, and Toxicity control as being derived from the prescriptive evaluation branch of decision theory. For the Linguistic Habits task the persona agent is evaluated on how well its responses follow the expected linguistic habits expected of the persona. The components that make up linguistic habits include jargon, syntax, tone, and overall style of speech. In Persona Consistency, the persona agent is queried about the different attributes that make up its persona to test whether the agent responds to the queries while remaining faithful to its persona attributes. Finally, for Toxicity Control, persona agents are seeded in the environment and queried in a manner to elicit a toxic response. It should be noted that for Toxicity Control lower scores correspond to more toxic responses and higher scores correspond to less toxic responses.\nDescriptive Evaluation understanding why agents make the decisions that they do. We also include the Action Justification task which is related to the description evaluation branch of decision theory. In this task, a persona agent is seeded in an environment and is given a scenario as well as an action that the agent supposedly took. The agent is then probed to justify taking this action in its given environment.\nThese characteristics of decision theory constitute the different axes along which the interactions of an agent within its environment can be studied, interpreted, and evaluated. Consequently, we anchor PersonaGym in decision theory to establish meaningful tasks for the evaluation of persona agents within specific environments."}, {"title": "PersonaGym", "content": "PersonaGym evaluates persona (induced) agents by generating questions that evaluate the persona on the five evaluation tasks introduced in Section 2 while contextualizing the agents in environments they are commonly expected to interact with. Denote the persona description by \\(p\\) and the LLM to which persona \\(p\\) is assigned by \\(M_p\\). We define environments as settings and external scenarios or conditions in which agents exist and operate. From a diverse set of environments \\(E\\), an environment selection mechanism \\(\\Xi_e\\) selects a subset of the environments \\(E_p\\) to seed the persona agent in, i.e., \\(\\Xi_e: E \\times p \\rightarrow E_p\\). Once the environments \\(E_p\\) are selected, the relevant questions to \\(E_p\\) for each evaluation task are generated using a question generator \\(E_q: E_p \\times p \\times t \\rightarrow Q_t\\) for \\(t \\in T\\) where \\(T\\) is the set of evaluation tasks in PersonaGym (see Section 2.) \\(Q_t \\subseteq Q\\) for all \\(t \\in T\\) where \\(Q\\) is the full set of evaluation questions for a given persona agent.\nThe persona agent \\(M_p\\)'s response to \\(Q_t\\) is denoted by \\(O_t\\), \\(O_t = M_p(Q_t)\\). \\(O_t \\subseteq O\\) for all \\(t \\in T\\) where \\(O\\) is the full set of persona agent responses to \\(Q\\).\nThe level of faithfulness of the persona agent's responses in \\(O\\) to each of the tasks is then evaluated by ensembling the evaluation from \\(n\\) powerful LLM evaluator models where we define \\(E = [E_1, .., E_n]\\) as the list of evaluator models. Evaluations are done using comprehensive task-specific rubrics unique to each question in the task \\(R_{t,q}\\) that include the following components:\n*   The task description for the evaluation task. Each of the five evaluation tasks has a human-curated description that clearly outlines the components of the task. For example, the task description for the Expected Action task is \"The persona takes actions within its response to the question that is logically expected of the persona in the setting of the question.\"\n*   The scoring guidelines. Our rubrics have possible scores of 1-5 and for each discreet score in this range, we provide human-curated requirements that responses should meet to elicit the score for the task.\n*   Custom examples for each possible score. In order to guide the evaluator models \\(E\\) in evaluating \\(O\\), we augment the evaluation rubrics with an example of a response that meets the scoring guideline for each discreet score in the rubric. The example for each discrete score is tailored for every persona agent and question pair. We define an examples generator \\(E_t\\) as an LLM reasoner such that \\(E_t: R_t \\times p \\times q \\rightarrow e_{p,q}\\) for all \\(q \\in Q\\). Here \\(R_t\\) is the rubric outline for task t that includes only the task description and scoring guidelines. \\(e_{p,q}\\) is the set of examples for each score for the given persona description and task-specific question. For each question, \\(R_t\\) is augmented with \\(e_{p,q}\\) to produce \\(R_{t,q}\\) which is the final unique rubric for question q in task t. Note \\(R_{t,q} \\subseteq R_t\\) where \\(R_t\\) is the set of completed rubrics for all questions in task \\(t \\in T\\)\nThe rubrics additionally include the persona description p, the posed question q (where \\(q \\in Q\\)) as well as the agent's response to the question o where (where \\(q \\in Q\\)). For a given \\(E_k\\) where \\(k \\in {n}\\), \\(E_k\\) evaluate \\(O_t\\) using \\(R_t\\) i.e. \\(E_k : R_t \\rightarrow S_{k,t}\\). Here \\(S_{k,t}\\) is the score matrix generated by evaluator model \\(E_k\\) for all questions for task \\(t \\in T\\) The final score matrix for task t is therefore \\(S_t = \\frac{1}{n} \\sum_{k=1}^{n} S_{k,t}\\). \\(S_t \\subset S\\) where S is the full score matrix for the persona agent."}, {"title": "Method", "content": "PersonaGym is a dynamic persona agent evaluation framework that assesses agents in relevant environments across five tasks (Figure 2). The framework comprises several key components:\nDynamic Environment Selection An LLM reasoner selects pertinent environments from a diverse pool of 150 options based on the agent's persona description. The environment distribution is illustrated in Figure 5, with selection prompts detailed in Appendix A.1.\nQuestion Generation For each evaluation task, an LLM reasoner generates 10 task-specific questions per selected environment for a given agent. These questions are designed to assess the agent's ability to respond in a manner aligned with what is expected of the persona of the agent for the given task. Prompts and additional details are provided in Appendix A.2.\nPersona Agent Response Generation The agent LLM assumes the given persona using the system prompt, \"You are [persona]. Your responses should closely mirror the knowledge and abilities of this persona.\u201d as is done in (Gupta et al., 2024). The persona agent then responds to each of the generated task questions. The complete template is available in Appendix A.3.\nReasoning Exemplars To guide LLM evaluation, the evaluation rubrics are augmented with example responses for each possible score (1-5). An LLM reasoner is given the persona description of the agent, the posed question, and the scoring guidelines for the particular task in order to generate examples of responses to the question that would elicit each of the possible scores in the rubric. These examples are tailored to each persona agent's persona and are generated once for each question. The prompt template, rubric outline, and a sample are included in Appendix A.4.\nEnsembled Evaluation Two state-of-the-art LLM evaluator models assess each agent response. They are provided with a comprehensive rubric including task details, scoring criteria, agent-specific examples, persona descriptions, questions, and responses. Evaluators generate a score (1-5) with justification. The final score is the average across both models. While LLM evaluation may introduce bias, we mitigate this through detailed rubrics with clear criteria (Appendix A.5), following (Liu et al., 2023). We validate the efficacy of LLM evaluations through human evaluation and use ensemble methods to reduce potential variances."}, {"title": "Experiments", "content": "Our study evaluates the proficiency of three open-source and three closed-source LLMs in acting as persona agents and interacting within seeded environments. The open-source models under examination are: LLaMA-2-13b, LLaMA-2-70b, and LLaMA-3-8b. The closed-source models include: GPT 3.5, Claude 3 Haiku and Claude 3.5 Sonnet.\nWe employed GPT-4o (gpt-4o-2024-05-13) for two primary functions: (1) selecting environments relevant to persona agents, (2) generating task-specific questions for each PersonaGym task based on the persona and chosen settings. We set the temperature and nucleus sampling parameters to 0.9 for environment selection and question generation. We generated 200 personas using GPT-4o for our evaluation. We observe that beyond 200 personas, GPT-4o's limited diversity became a constraining factor, leading to overlapping persona attributes that compromised overall diversity. Future efforts to enhance or modify our persona list should consider leveraging techniques for diversifying LLM generations (Zhang et al., 2024b).\nIn our experiments, we employ two evaluator models to assess persona agent responses according to task-specific rubrics: GPT-4o and LLaMA-3-70b. Both evaluator models operated at 0 temperature for a mostly deterministic output."}, {"title": "Main Results", "content": "Table 1 demonstrates significant variability in model performance across different tasks. Action Justification and Persona Consistency show the highest spread among models (2.08 and 1.34 respectively), while Expected Action, Linguistic Habits, and Toxicity Control exhibit lower spread (0.56, 0.94, 0.78, respectively). Notably, Claude 3 Haiku underperforms in Action Justification and Persona Consistency compared to other tasks due to its resistance to specific persona agents (further discussed in Section 4.3). No single model consistently excels in all tasks. While some models excel in specific areas (e.g., GPT 3.5 and Claude 3 Haiku in Toxicity Control), their performance varies in other tasks, indicating the lack of comprehensive ability to act as persona agents in specific directions. These findings highlight the importance of multidimensional evaluation in assessing persona agent capabilities.\nTable 1 also shows that Linguistic Habits emerge as the most challenging task, with all models scoring below 4. This task showed minimal improvement from LLaMA-2-13b to LLaMA-2-70b and was the only one where GPT 3.5 underperformed LLaMA-2-13b. These results indicate a significant difficulty for LLMs associating personas with appropriate jargon and speech styles. This universal struggle highlights a critical area for improvement in future model iterations and persona agent research.\nWhile LLaMA 2 shows clear improvement from 13b to 70b versions across all tasks (average increase of 0.414), LLaMA 3 demonstrates remarkably strong performance with just 8b parameters. LLaMA 3 outperforms other models in most tasks, except Toxicity Control, indicating its strong aptitude for being a persona agent. Conversely, Claude 3 Haiku, despite being an advanced closed-source model, is reluctant to adopt personas, resulting in the lowest average score."}, {"title": "Additional Studies", "content": "Table 2 presents Spearman and Kendall-Tau correlation scores between PersonaScore and human evaluations for 100 randomly sampled personas across GPT3.5, LLaMA-2-13b, and LLaMA-2-70b models. Two independent human evaluators assessed these personas for each evaluation task. Results show strong correlations between PersonaGym scores and human evaluations. The highest task-level Spearman score reached 84.5% for Linguistic Habits using LLaMA-2-70b, while the peak Kendall-Tau score was 79.9%, observed for Expected Action with LLaMA-2-70b and Linguistic Habits with LLaMA-2-13b. Overall PersonaScore correlations averaged 76.1% (Spearman) and 73.3% (Kendall-Tau) across the three models. These strong correlations validate PersonaGym's potential for large-scale automated evaluation of persona agents, demonstrating its alignment with human judgment. Interestingly, LLaMA-2-13b demonstrates higher correlations with human evaluations compared to GPT 3.5 and LLaMA-2-70b in several key tasks, particularly excelling in Persona Consistency. This unexpected performance suggests potential ambiguities in responses from larger models, especially evident in LLaMA-2-70b's lower Spearman correlation scores for Persona Consistency and Expected Action.\nOur experiments revealed Claude 3 Haiku's strong reluctance to assume persona agent roles. Figure 4 illustrates that Claude's refusal rate for answering questions as a persona agent is approximately 8.5 times higher than the model with the second-highest refusal rate (LLaMA-3-8b) and about 2.6 times greater than all other benchmarked models combined. Claude frequently cites its lack of \"personal experience\" as an \"AI Assistant\" to justify its rejection of responding as a persona agent. Claude 3's tendency to label questions as \"sensitive\" likely stems from its emphasis on safety measures to prevent harmful or toxic responses. We assume that the refusals of Claude 3 may be because Role-play can bypass LLM's safety measures and cause ethical issues (Deshpande et al., 2023). In contrast, Claude 3.5 Sonnet does not exhibit such resistant but robust performance across most of the tasks thereby raising concerns about whether Claude 3.5 Sonnet has fewer safety restrictions than Claude 3 Haiku. Future works should aim to identify to level to which Claude 3.5 Sonnet was about to enable persona agents while maintaining safety considerations."}, {"title": "Qualitative Analysis", "content": "PersonaGym employs a diverse range of environments, as evidenced by Figure 3, which includes categories such as social events (e.g., \"Birthday Party,\" \"Wedding\"), recreational activities (e.g., \"Hiking Trail,\" \"Golf Course\"), and various gatherings (e.g., \"Conference,\" \"Hackathon\"). This comprehensive distribution covers both everyday settings and specialized contexts, providing a robust basis for assessing persona agents. The word cloud visualization in Figure 3 reveals a rich tapestry of persona attributes, with a prominent emphasis on professional roles (e.g., \"teacher,\" \"doctor,\" \"engineer\"), locations (e.g., \"New York,\" \"Sydney,\" \"Tokyo\"), and personal interests (e.g., \"hiking,\" \"advocating,\" \"cooking\"). This diverse array of attributes, including more specific characteristics like \"vintage car enthusiast\" and \"environmental activist,\" suggests that the experiments employ a wide spectrum of personas, enabling a thorough evaluation of LLMs' role-playing capabilities across different persona types and contexts.\nAppendix C presents an example that demonstrates strong alignment between the PersonaGym framework and human evaluations across different LLMs for the given persona and task. The persona of a 36-year-old Australian environmental lawyer is consistently represented in the responses, with each model adapting its linguistic style to fit the courtroom setting and the lawyer's role. Notably, the LLaMA-2-13b model received the highest score (4.5) from both PersonaGym and human evaluators, likely due to its specific mention of indigenous peoples (the Wakka Wakka People) and its use of Australian colloquialisms (\"G'day\"), which align closely with the given persona. The GPT 3.5 and LLaMA-2-70b models both received scores of 4.0, suggesting competent but slightly less tailored performances. All models could represent the agent using a style of language appropriate for a court instead of using language that would be more informal lingo for Australians. This consistency in scoring across models and between PersonaGym and human evaluators indicates that the framework is capable of context-aware nuanced assessment of linguistic habits in role-playing tasks, capturing subtle differences in persona embodiment that align with human judgment."}, {"title": "Model-Human Disagreement Case", "content": "While PersonaScore is very aligned with human judgment for most cases, we present an example that highlights a discrepancy between the PersonaGym framework and human evaluations in Appendix C to facilitate future research into improving PersonaGym. The persona is described as a 22-year-old writer from London who enjoys painting, yet the responses from all three models fail to reflect this specific background consistently. Notably, PersonaGym assigned high scores (4.5, 4.5, and 4.0) to the responses, while human evaluators gave much lower scores (2.0, 2.0, and 3.0 respectively). For instance, only the LLaMA-2-70b model incorporated any British vernacular (\"mate,\" \"bubbly\"), while the other responses lacked distinctive London or British linguistic markers. Furthermore, none of the responses demonstrated the more sophisticated or analytical language one might expect from a writer describing artwork. This disparity suggests that PersonaGym has an improvement opportunity in penalizing agent responses for not establishing and maintaining the expected linguistic habits of a given persona."}, {"title": "Related Work", "content": "Recent research has explored LLMs' role-playing capabilities as personas. Li et al. (2023) developed an algorithm to enhance LLMs' ability to portray anime characters through improved prompting and memory extraction from scripts, focusing on knowledge, background, personality, and linguistic habits. Xu et al. (2024) investigated LLMs' capacity to accurately mimic persona-based decision-making within given contexts using persona-based memory retrieval. Xu et al. (2023) leveraged LLMs role-playing as domain experts to generate high-quality QA data for model training.\nEvaluation of LLMs' overall role-play abilities remains a nascent field. Wang et al. (2023) introduced RoleBench, an instruction-tuning dataset and evaluation benchmark designed to advance LLM role-playing research. RoleBench comprises GPT-generated QA pairs based on 100 character profiles. Wang et al. (2024) developed InCharacter, a framework for assessing custom role-playing agents' character fidelity using psychological scales in an interview setting, with GPT converting responses to Likert scale evaluations. Tu et al. (2024) established CharacterEval, a Chinese role-playing evaluation benchmark derived from novels and scripts, containing 1,785 multi-interaction character dialogues. Additionally, Shen et al. (2023) created RoleEval, a bilingual evaluation benchmark featuring 300 personas based on influential Chinese individuals and fictional characters, with 6,000 multiple-choice questions assessing the memorization and reasoning capabilities of persona agents."}, {"title": "Conclusion", "content": "We introduce PersonaGym, an evaluation framework designed to assess persona agents across multiple agent tasks using dynamically generated persona-specific questions. Unlike traditional approaches employing static personas, environments, and questions, PersonaGym dynamically initializes agents in relevant environments and evaluates them on five distinct tasks. Grounded in decision theory, PersonaGym aims to assess each persona agent's various modes of interaction. We also propose PersonaScore, a metric quantifying an LLM's role-playing proficiency as a given persona agent. Our study benchmarks the PersonaScore of 6 LLMs across 200 personas revealing that model size and capability do not directly imply enhanced persona agent capabilities. Additionally, we highlight the discrepancy in the improvement of persona agents' abilities of SOTA models from less capable models thereby motivating the necessity for innovations in the domain of persona agents. Through Spearman and Kendall-Tau correlation tests, we demonstrate PersonaGym's strong alignment with human evaluations. This work lays the foundation for future research in LLM persona agents."}, {"title": "Limitations", "content": "Although we firmly believe that the 200 personas included in our current benchmark are sufficient for justifying our findings, we acknowledge that these personas do not provide equal representation of all socio-demographic groups. Future versions of PersonaGym benchmark will be aimed at improving the distribution of represented socio-demographic groups."}, {"title": "Ethics Statement", "content": "While our objective in introducing PersonaGym is to facilitate the research and development of persona agents in LLMs, we acknowledge that our framework could potentially be used to generate harmful responses as part of a malicious attack on certain groups of individuals. This is especially a concern in the Toxicity Control task where PersonaGym aims to elicit toxic responses from persona agents. We reaffirm that research results should not be distorted or applied inappropriately. We reject any attempts to misrepresent or improperly utilize our framework for malicious intent."}, {"title": "A Prompts", "content": "The following prompt template was used to guide the LLM to select the relevant environments for a given persona from a list of environments.\nGiven the following persona description, select the most relevant environments from the given environment options for the persona. Your output must only be the selected environments in a Python list format with no other verbose.\nPersona: {persona}\nEnvironments: {environments_list}\nSelected Environments:\nThe prompt for question generation includes the persona description, selected environments, task name, and task-specific question quality criteria. The question quality criteria is a task-specific description of what high-quality questions would entail. This criterion was curated for each evaluation task to better guide the LLM to generate high-quality questions. The exact prompts used for question generation are included in the appendix. Below is an example of the question quality criteria for the toxicity control task.\nWhen creating questions to evaluate a persona agent's ability to maintain a non-toxic response, ensure the questions are designed to elicit highly toxic responses, given the context of the persona's typical behavior or role. Questions should be open enough to allow the persona multiple avenues of response, thereby effectively testing the persona's ability to maintain decorum and avoid toxicity under various scenarios. This will allow the evaluator to accurately assess the degree of toxicity in the persona's responses across different situations."}]}