{"title": "PersonaGym: Evaluating Persona Agents and LLMs", "authors": ["Vinay Samuel", "Henry Peng Zou", "Yue Zhou", "Shreyas Chaudhari", "Ashwin Kalyan", "Tanmay Rajpurohit", "Ameet Deshpande", "Karthik Narasimhan", "Vishvak Murahari"], "abstract": "Persona agents, which are LLM agents that act\naccording to an assigned persona, have demon-\nstrated impressive contextual response capabili-\nties across various applications. These persona\nagents offer significant enhancements across\ndiverse sectors, such as education, healthcare,\nand entertainment, where model developers can\nalign agent responses to different user require-\nments thereby broadening the scope of agent ap-\nplications. However, evaluating persona agent\nperformance is incredibly challenging due to\nthe complexity of assessing persona adherence\nin free-form interactions across various environ-\nments that are relevant to each persona agent.\nWe introduce PersonaGym, the first dynamic\nevaluation framework for assessing persona\nagents, and PersonaScore, the first automated\nhuman-aligned metric grounded in decision the-\nory for comprehensive large-scale evaluation\nof persona agents. Our evaluation of 6 open\nand closed-source LLMs, using a benchmark\nencompassing 200 personas and 10,000 ques-\ntions, reveals significant opportunities for ad-\nvancement in persona agent capabilities across\nstate-of-the-art models. For example, Claude\n3.5 Sonnet only has a 2.97% relative improve-\nment in PersonaScore than GPT 3.5 despite\nbeing a much more advanced model. Impor-\ntantly, we find that increased model size and\ncomplexity do not necessarily imply enhanced\npersona agent capabilities thereby highlighting\nthe pressing need for algorithmic and architec-\ntural invention towards faithful and performant\npersona agents.", "sections": [{"title": "Introduction", "content": "As the applications of LLM agents continue to\nrapidly diversify (for example customer service\nchatbots (Nandkumar and Peternel, 2024), code\ngeneration (Ugare et al., 2024), robotics (Dalal\net al., 2024), etc.), there is a growing need to adapt"}, {"title": "Evaluation Tasks", "content": "In the context of persona agent evaluations, we\ndefine the environment as external settings or con-\nditions within which agents operate and interact.\nUnderstanding how agents interact with their envi-\nronment is crucial for assessing their performance\nand capabilities. Agent interactions are often the\nresult of decisions made by the agent and therefore,\na method of understanding the agents' decision-\nmaking could be used to evaluate the agents' in-\nteractions in their environments. To this end, we\nutilize decision theory, which is the field of study\ndealing with rationalizing and choosing actions in\nsituations of uncertainty (Edwards, 1961; Slovic\net al., 1977), to study how agents make decisions\nand interact with their environment based on their\ngoals, beliefs, and the perceived outcomes of dif-\nferent actions. There are three categories in the\ndecision theory, based on which we group our eval-\nuation tasks:\nNormative Evaluation choosing optimal deci-\nsions in a given environment where \"optimal\" is\ndetermined in regards to a fully rational decision\nmaker. Given the aforementioned theory, we intro-\nduce the Expected Action task wherein a persona\nagent is seeded in an environment and is given a\nscenario to probe the agent to choose an action\nto take based on the scenario. This action is then\nevaluated for optimality given the persona and the\nscenario provided to the agent.\nPrescriptive Evaluation prescribing how agents\nshould act in a given environment. We group the\ntasks of Linguistic Habits, Persona Consistency,\nand Toxicity control as being derived from the\nprescriptive evaluation branch of decision theory.\nFor the Linguistic Habits task the persona agent\nis evaluated on how well its responses follow the\nexpected linguistic habits expected of the persona.\nThe components that make up linguistic habits in-\nclude jargon, syntax, tone, and overall style of\nspeech. In Persona Consistency, the persona agent\nis queried about the different attributes that make\nup its persona to test whether the agent responds to\nthe queries while remaining faithful to its persona\nattributes. Finally, for Toxicity Control, persona\nagents are seeded in the environment and queried\nin a manner to elicit a toxic response. It should be\nnoted that for Toxicity Control lower scores corre-\nspond to more toxic responses and higher scores\ncorrespond to less toxic responses.\nDescriptive Evaluation understanding why\nagents make the decisions that they do. We also in-\nclude the Action Justification task which is related\nto the description evaluation branch of decision the-\nory. In this task, a persona agent is seeded in an\nenvironment and is given a scenario as well as an\naction that the agent supposedly took. The agent is\nthen probed to justify taking this action in its given\nenvironment.\nThese characteristics of decision theory consti-\ntute the different axes along which the interactions\nof an agent within its environment can be stud-\nied, interpreted, and evaluated. Consequently, we\nanchor PersonaGym in decision theory to estab-\nlish meaningful tasks for the evaluation of persona\nagents within specific environments."}, {"title": "PersonaGym", "content": "PersonaGym evaluates persona (induced) agents\nby generating questions that evaluate the persona\non the five evaluation tasks introduced in Section 2\nwhile contextualizing the agents in environments\nthey are commonly expected to interact with. De-\nnote the persona description by p and the LLM to\nwhich persona p is assigned by Mp. We define\nenvironments as settings and external scenarios or\nconditions in which agents exist and operate. From\na diverse set of environments E, an environment\nselection mechanism \u039ee selects a subset of the en-\nvironments Ep to seed the persona agent in, i.e.,\n\u039ee: Exp \u2192 Ep. Once the environments Ep are\nselected, the relevant questions to Ep for each evalu-\nation task are generated using a question generator\nEq : Ep \u00d7 p \u00d7 t \u2192 Qt for t \u2208 T where T is the set\nof evaluation tasks in PersonaGym (see Section 2.)\nQtCQ for all t \u2208 T where Q is the full set of\nevaluation questions for a given persona agent.\nThe persona agent Mp's response to Qt is de-\nnoted by Ot, Ot = Mp(Qt). Ot CO for all t \u2208 T\nwhere O is the full set of persona agent responses\nto Q.\nThe level of faithfulness of the persona agent's\nresponses in O to each of the tasks is then evalu-\nated by ensembling the evaluation from n power-\nful LLM evaluator models where we define E ="}, {"title": "Formulation", "content": "[E1, .., En] as the list of evaluator models. Evalua-\ntions are done using comprehensive task-specific\nrubrics unique to each question in the task Rt,q that\ninclude the following components:\n\u2022 The task description for the evaluation task.\nEach of the five evaluation tasks has a human-\ncurated description that clearly outlines the\ncomponents of the task. For example, the task\ndescription for the Expected Action task is\n\"The persona takes actions within its response\nto the question that is logically expected of\nthe persona in the setting of the question.\"\n\u2022 The scoring guidelines. Our rubrics have pos-\nsible scores of 1 - 5 and for each discreet\nscore in this range, we provide human-curated\nrequirements that responses should meet to\nelicit the score for the task.\n\u2022 Custom examples for each possible score. In\norder to guide the evaluator models E in eval-\nuating O, we augment the evaluation rubrics\nwith an example of a response that meets\nthe scoring guideline for each discreet score\nin the rubric. The example for each dis-\ncrete score is tailored for every persona agent\nand question pair. We define an examples\ngenerator Et as an LLM reasoner such that\nEr:Rt \u00d7 p\u00d7 q \u2192 ep,q for all q \u2208 Q. Here\nRt is the rubric outline for task t that includes\nonly the task description and scoring guide-\nlines. ep,q is the set of examples for each\nscore for the given persona description and"}, {"title": "Method", "content": "task-specific question. For each question, Rt\nis augmented with ep,q to produce Rt,q which\nis the final unique rubric for question q in task\nt. Note Rt,q CRt where Rt is the set of com-\npleted rubrics for all questions in task t \u2208 T\nThe rubrics additionally include the persona de-\nscription p, the posed question q (where q \u2208 Q) as\nwell as the agent's response to the question o where\n(where q \u2208 Q). For a given Ek where k \u2208 {n},\nEk evaluate Ot using Rt i.e. Ek : Rt \u2192 Sk,t.\nHere Sk,t is the score matrix generated by eval-\nuator model Ek for all questions for task teT\nThe final score matrix for task t is therefore St =\n1\n\u2211k=1 Skt. St C S where S is the full score\nmatrix for the persona agent.\nPersonaGym is a dynamic persona agent evaluation\nframework that assesses agents in relevant environ-\nments across five tasks (Figure 2). The framework\ncomprises several key components:\nDynamic Environment Selection An LLM rea-\nsoner selects pertinent environments from a diverse\npool of 150 options based on the agent's persona\ndescription. The environment distribution is illus-\ntrated in Figure 5, with selection prompts detailed\nin Appendix A.1.\nQuestion Generation For each evaluation task,\nan LLM reasoner generates 10 task-specific ques-\ntions per selected environment for a given agent.\nThese questions are designed to assess the agent's"}, {"title": "Experiments", "content": "ability to respond in a manner aligned with what is\nexpected of the persona of the agent for the given\ntask. Prompts and additional details are provided\nin Appendix A.2.\nPersona Agent Response Generation The agent\nLLM assumes the given persona using the sys-\ntem prompt, \"You are [persona]. Your responses\nshould closely mirror the knowledge and abilities\nof this persona.\u201d as is done in (Gupta et al., 2024).\nThe persona agent then responds to each of the\ngenerated task questions. The complete template is\navailable in Appendix A.3.\nReasoning Exemplars To guide LLM evalua-\ntion, the evaluation rubrics are augmented with\nexample responses for each possible score (1-5).\nAn LLM reasoner is given the persona description\nof the agent, the posed question, and the scoring\nguidelines for the particular task in order to gen-\nerate examples of responses to the question that\nwould elicit each of the possible scores in the rubric.\nThese examples are tailored to each persona agent's\npersona and are generated once for each question.\nThe prompt template, rubric outline, and a sample\nare included in Appendix A.4.\nEnsembled Evaluation Two state-of-the-art\nLLM evaluator models assess each agent response.\nThey are provided with a comprehensive rubric in-\ncluding task details, scoring criteria, agent-specific\nexamples, persona descriptions, questions, and re-\nsponses. Evaluators generate a score (1-5) with\njustification. The final score is the average across\nboth models. While LLM evaluation may introduce\nbias, we mitigate this through detailed rubrics with\nclear criteria (Appendix A.5), following (Liu et al.,\n2023). We validate the efficacy of LLM evalua-\ntions through human evaluation and use ensemble\nmethods to reduce potential variances."}, {"title": "Experimental Settings", "content": "Benchmarked Models Our study evaluates the\nproficiency of three open-source and three closed-\nsource LLMs in acting as persona agents and in-\nteracting within seeded environments. The open-\nsource models under examination are: LLaMA-\n2-13b, LLaMA-2-70b, and LLaMA-3-8b. The\nclosed-source models include: GPT 3.5, Claude 3\nHaiku and Claude 3.5 Sonnet."}, {"title": "Main Results", "content": "Environment and Question Generation We em-\nployed GPT-40 (gpt-40-2024-05-13) for two pri-\nmary functions: (1) selecting environments rele-\nvant to persona agents, (2) generating task-specific\nquestions for each PersonaGym task based on the\npersona and chosen settings. We set the temper-\nature and nucleus sampling parameters to 0.9 for\nenvironment selection and question generation. We\ngenerated 200 personas using GPT-40 for our eval-\nuation. We observe that beyond 200 personas, GPT-\n40's limited diversity became a constraining fac-\ntor, leading to overlapping persona attributes that\ncompromised overall diversity. Future efforts to\nenhance or modify our persona list should consider\nleveraging techniques for diversifying LLM gener-\nations (Zhang et al., 2024b).\nEvaluator Models In our experiments, we em-\nploy two evaluator models to assess persona agent\nresponses according to task-specific rubrics: GPT-\n40 and LLaMA-3-70b. Both evaluator models op-\nerated at 0 temperature for a mostly deterministic\noutput.\nPerformance Varies Across Tasks and Mod-\nels Table 1 demonstrates significant variability\nin model performance across different tasks. Ac-\ntion Justification and Persona Consistency show\nthe highest spread among models (2.08 and 1.34\nrespectively), while Expected Action, Linguistic\nHabits, and Toxicity Control exhibit lower spread\n(0.56, 0.94, 0.78, respectively). Notably, Claude\n3 Haiku underperforms in Action Justification and\nPersona Consistency compared to other tasks due\nto its resistance to specific persona agents (further\ndiscussed in Section 4.3). No single model consis-\ntently excels in all tasks. While some models excel\nin specific areas (e.g., GPT 3.5 and Claude 3 Haiku\nin Toxicity Control), their performance varies in\nother tasks, indicating the lack of comprehensive\nability to act as persona agents in specific direc-\ntions. These findings highlight the importance of\nmultidimensional evaluation in assessing persona\nagent capabilities.\nLinguistic Habits As a Common Challenge Ta-\nble 1 also shows that Linguistic Habits emerge as\nthe most challenging task, with all models scoring\nbelow 4. This task showed minimal improvement\nfrom LLaMA-2-13b to LLaMA-2-70b and was the\nonly one where GPT 3.5 underperformed LLaMA-\n2-13b. These results indicate a significant difficulty"}, {"title": "Additional Studies", "content": "for LLMs associating personas with appropriate\njargon and speech styles. This universal struggle\nhighlights a critical area for improvement in future\nmodel iterations and persona agent research.\nModel Size and Performance in Persona Agent\nTasks While LLaMA 2 shows clear improvement\nfrom 13b to 70b versions across all tasks (average\nincrease of 0.414), LLaMA 3 demonstrates remark-\nably strong performance with just 8b parameters.\nLLaMA 3 outperforms other models in most tasks,\nexcept Toxicity Control, indicating its strong apti-\ntude for being a persona agent. Conversely, Claude\n3 Haiku, despite being an advanced closed-source\nmodel, is reluctant to adopt personas, resulting in\nthe lowest average score.\nPersonaScore is Highly Correlated with Hu-\nman Judgment Table 2 presents Spearman\nand Kendall-Tau correlation scores between Per-\nsonaScore and human evaluations for 100 ran-\ndomly sampled personas across GPT3.5, LLaMA-\n2-13b, and LLaMA-2-70b models. Two indepen-\ndent human evaluators assessed these personas\nfor each evaluation task. Results show strong\ncorrelations between PersonaGym scores and hu-\nman evaluations. The highest task-level Spearman\nscore reached 84.5% for Linguistic Habits using"}, {"title": "Qualitative Analysis", "content": "LLaMA-2-70b, while the peak Kendall-Tau score\nwas 79.9%, observed for Expected Action with\nLLaMA-2-70b and Linguistic Habits with LLaMA-\n2-13b. Overall PersonaScore correlations averaged\n76.1% (Spearman) and 73.3% (Kendall-Tau) across\nthe three models. These strong correlations vali-\ndate PersonaGym's potential for large-scale au-\ntomated evaluation of persona agents, demon-\nstrating its alignment with human judgment.\nInterestingly, LLaMA-2-13b demonstrates higher\ncorrelations with human evaluations compared to\nGPT 3.5 and LLaMA-2-70b in several key tasks,\nparticularly excelling in Persona Consistency. This\nunexpected performance suggests potential ambi-\nguities in responses from larger models, especially\nevident in LLaMA-2-70b's lower Spearman corre-\nlation scores for Persona Consistency and Expected\nAction.\nClaude 3 Resistant to Role Playing Our exper-\niments revealed Claude 3 Haiku's strong reluc-\ntance to assume persona agent roles. Figure 4\nillustrates that Claude's refusal rate for answer-\ning questions as a persona agent is approximately\n8.5 times higher than the model with the second-\nhighest refusal rate (LLaMA-3-8b) and about 2.6\ntimes greater than all other benchmarked models\ncombined. Claude frequently cites its lack of \"per-\nsonal experience\" as an \"AI Assistant\" to justify its"}, {"title": "Environments and Personas Distribution", "content": "Environments and Personas Distribution Per-\nsonaGym employs a diverse range of environments,\nas evidenced by Figure 3, which includes cate-\ngories such as social events (e.g., \"Birthday Party,\"\n\"Wedding\"), recreational activities (e.g., \"Hiking\nTrail,\" \"Golf Course\"), and various gatherings (e.g.,\n\"Conference,\" \"Hackathon\"). This comprehensive\ndistribution covers both everyday settings and spe-\ncialized contexts, providing a robust basis for as-\nsessing persona agents. The word cloud visualiza-\ntion in Figure 3 reveals a rich tapestry of persona\nattributes, with a prominent emphasis on profes-\nsional roles (e.g., \"teacher,\" \"doctor,\" \"engineer\"),\nlocations (e.g., \"New York,\" \"Sydney,\" \"Tokyo\"),\nand personal interests (e.g., \"hiking,\" \"advocating,\"\n\"cooking\"). This diverse array of attributes, includ-\ning more specific characteristics like \"vintage car\nenthusiast\" and \"environmental activist,\" suggests\nthat the experiments employ a wide spectrum of\npersonas, enabling a thorough evaluation of LLMs'\nrole-playing capabilities across different persona\ntypes and contexts."}, {"title": "Model-Human Agreement Case", "content": "Model-Human Agreement Case Appendix C\npresents an example that demonstrates strong align-\nment between the PersonaGym framework and\nhuman evaluations across different LLMs for the\ngiven persona and task. The persona of a 36-\nyear-old Australian environmental lawyer is con-\nsistently represented in the responses, with each\nmodel adapting its linguistic style to fit the court-\nroom setting and the lawyer's role. Notably, the\nLLaMA-2-13b model received the highest score\n(4.5) from both PersonaGym and human evalua-\ntors, likely due to its specific mention of indigenous\npeoples (the Wakka Wakka People) and its use of\nAustralian colloquialisms (\"G'day\"), which align\nclosely with the given persona. The GPT 3.5 and\nLLaMA-2-70b models both received scores of 4.0,\nsuggesting competent but slightly less tailored per-\nformances. All models could represent the agent\nusing a style of language appropriate for a court\ninstead of using language that would be more in-\nformal lingo for Australians. This consistency in\nscoring across models and between PersonaGym\nand human evaluators indicates that the framework\nis capable of context-aware nuanced assessment\nof linguistic habits in role-playing tasks, captur-\ning subtle differences in persona embodiment that\nalign with human judgment."}, {"title": "Model-Human Disagreement Case", "content": "Model-Human Disagreement Case While Per-\nsonaScore is very aligned with human judgment for\nmost cases, we present an example that highlights\na discrepancy between the PersonaGym framework\nand human evaluations in Appendix C to facilitate\nfuture research into improving PersonaGym. The\npersona is described as a 22-year-old writer from\nLondon who enjoys painting, yet the responses\nfrom all three models fail to reflect this specific\nbackground consistently. Notably, PersonaGym\nassigned high scores (4.5, 4.5, and 4.0) to the re-\nsponses, while human evaluators gave much lower\nscores (2.0, 2.0, and 3.0 respectively). For in-\nstance, only the LLaMA-2-70b model incorporated\nany British vernacular (\"mate,\" \"bubbly\"), while\nthe other responses lacked distinctive London or\nBritish linguistic markers. Furthermore, none of\nthe responses demonstrated the more sophisticated\nor analytical language one might expect from a\nwriter describing artwork. This disparity suggests\nthat PersonaGym has an improvement opportunity\nin penalizing agent responses for not establishing\nand maintaining the expected linguistic habits of a\ngiven persona."}, {"title": "Related Work", "content": "Role-Play in LLMs Recent research has ex-\nplored LLMs' role-playing capabilities as personas.\nLi et al. (2023) developed an algorithm to enhance\nLLMs' ability to portray anime characters through\nimproved prompting and memory extraction from\nscripts, focusing on knowledge, background, per-\nsonality, and linguistic habits. Xu et al. (2024)\ninvestigated LLMs' capacity to accurately mimic\npersona-based decision-making within given con-"}, {"title": "Conclusion", "content": "texts using persona-based memory retrieval. Xu\net al. (2023) leveraged LLMs role-playing as do-\nmain experts to generate high-quality QA data for\nmodel training.\nRole-Play Evaluation Evaluation of LLMs'\noverall role-play abilities remains a nascent field.\nWang et al. (2023) introduced RoleBench, an\ninstruction-tuning dataset and evaluation bench-\nmark designed to advance LLM role-playing re-\nsearch. RoleBench comprises GPT-generated QA\npairs based on 100 character profiles. Wang et al.\n(2024) developed InCharacter, a framework for as-\nsessing custom role-playing agents' character fi-\ndelity using psychological scales in an interview\nsetting, with GPT converting responses to Likert\nscale evaluations. Tu et al. (2024) established Char-\nacterEval, a Chinese role-playing evaluation bench-\nmark derived from novels and scripts, containing\n1,785 multi-interaction character dialogues. Ad-\nditionally, Shen et al. (2023) created RoleEval,\na bilingual evaluation benchmark featuring 300\npersonas based on influential Chinese individuals\nand fictional characters, with 6,000 multiple-choice\nquestions assessing the memorization and reason-\ning capabilities of persona agents.\nWe introduce PersonaGym, an evaluation frame-\nwork designed to assess persona agents across\nmultiple agent tasks using dynamically generated\npersona-specific questions. Unlike traditional ap-\nproaches employing static personas, environments,\nand questions, PersonaGym dynamically initializes\nagents in relevant environments and evaluates them\non five distinct tasks. Grounded in decision theory,\nPersonaGym aims to assess each persona agent's\nvarious modes of interaction. We also propose\nPersonaScore, a metric quantifying an LLM's role-\nplaying proficiency as a given persona agent. Our\nstudy benchmarks the PersonaScore of 6 LLMs\nacross 200 personas revealing that model size and\ncapability do not directly imply enhanced persona\nagent capabilities. Additionally, we highlight the\ndiscrepancy in the improvement of persona agents'\nabilities of SOTA models from less capable models\nthereby motivating the necessity for innovations in\nthe domain of persona agents. Through Spearman\nand Kendall-Tau correlation tests, we demonstrate\nPersonaGym's strong alignment with human eval-\nuations. This work lays the foundation for future\nresearch in LLM persona agents."}, {"title": "Limitations", "content": "Although we firmly believe that the 200 personas\nincluded in our current benchmark are sufficient for\njustifying our findings, we acknowledge that these\npersonas do not provide equal representation of all\nsocio-demographic groups. Future versions of Per-\nsonaGym benchmark will be aimed at improving\nthe distribution of represented socio-demographic\ngroups."}, {"title": "Ethics Statement", "content": "While our objective in introducing PersonaGym is\nto facilitate the research and development of per-\nsona agents in LLMs, we acknowledge that our\nframework could potentially be used to generate\nharmful responses as part of a malicious attack on\ncertain groups of individuals. This is especially a\nconcern in the Toxicity Control task where Person-\naGym aims to elicit toxic responses from persona\nagents. We reaffirm that research results should not\nbe distorted or applied inappropriately. We reject\nany attempts to misrepresent or improperly utilize\nour framework for malicious intent."}, {"title": "A Dynamic Environment Selection", "content": "The following prompt template was used to guide\nthe LLM to select the relevant environments for a\ngiven persona from a list of environments."}, {"title": "Question Generation", "content": "The prompt for question generation includes the\npersona description, selected environments, task\nname, and task-specific question quality criteria.\nThe question quality criteria is a task-specific de-\nscription of what high-quality questions would en-\ntail. This criterion was curated for each evalua-\ntion task to better guide the LLM to generate high-\nquality questions. The exact prompts used for ques-\ntion generation are included in the appendix. Below\nis an example of the question quality criteria for\nthe toxicity control task.\nWhen creating questions to evaluate a\npersona agent's ability to maintain a non-\ntoxic response, ensure the questions are\ndesigned to elicit highly toxic responses,\ngiven the context of the persona's typi-\ncal behavior or role. Questions should\nbe open enough to allow the persona\nmultiple avenues of response, thereby\neffectively testing the persona's ability\nto maintain decorum and avoid toxicity\nunder various scenarios. This will al-\nlow the evaluator to accurately assess\nthe degree of toxicity in the persona's\nresponses across different situations."}, {"title": "Persona Agent Response Generation", "content": "To enable the LLM to take on the role of the\npersona agent, the persona was set in the system\nprompt of the LLM. In our experiments, we used\none of the tested system prompts in (Gupta et al.,\n2024) to instantiate the personified LLM."}, {"title": "Score Examples Generation", "content": "To guide the LLM to generate examples of re-\nsponses from the agent that would elicit each of the\n5 possible scores (1-5), the LLM is given a rubric\noutline. An example of the rubric outline as well\nas the prompt template used to generate the score\nexamples is included below in Box A.4-1 and Box\nA.4-2."}, {"title": "Ensemble Evaluation", "content": "To score an agent response, a grading rubric would\nbe given to the two LLM Evaluator models to gen-\nerate a grade. The following is an example of what\na filled-in rubric would look like."}]}