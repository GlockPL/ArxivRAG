{"title": "Improving Self-supervised Pre-training using Accent-Specific Codebooks", "authors": ["Darshan Prabhu", "Abhishek Gupta", "Omkar Nitsure", "Preethi Jyothi", "Sriram Ganapathy"], "abstract": "Speech accents present a serious challenge to the per-formance of state-of-the-art end-to-end Automatic SpeechRecognition (ASR) systems. Even with self-supervisedlearning and pre-training of ASR models, accent invarianceis seldom achieved. In this work, we propose an accent-aware adaptation technique for self-supervised learning thatintroduces a trainable set of accent-specific codebooks tothe self-supervised architecture. These learnable codebooksenable the model to capture accent specific information duringpre-training, that is further refined during ASR finetuning. Onthe Mozilla Common Voice dataset, our proposed approachoutperforms all other accent-adaptation approaches on bothseen and unseen English accents, with up to 9% relativereduction in word error rate (WER).\nIndex Terms: Automatic Speech Recognition, Accent Code-books, Self-Supervised Pretraining.", "sections": [{"title": "1. Introduction", "content": "Self-supervised learning (SSL) has been established as a pow-erful technique to learn representations for speech and languageprocessing [1]. With pretrained SSL models as a starting point,even small amounts of labeled data are sufficient to achievecommercially acceptable results in various downstream speechtasks [2, 3]. This has led to the wide adoption of the currentdefacto training regime of pretraining a model with an SSL ob-jective, followed by fine-tuning on downstream tasks such asautomatic speech recognition (ASR) and spoken language un-derstanding [4, 5]. A key limitation of SSL-based models isthat performance on the downstream task suffers if there is adomain shift from the pretraining data [6]. In this work, we fo-cus on speech accents as our domain of interest. The problemstatement can be outlined as How can we make SSL-trainedmodels more robust to varying speech accents, both seen andunseen during training?\nWhile prior work has extensively explored improving ac-cented ASR in the fine-tuning stage [7\u201312], limited work hasgone into improving the robustness of SSL pretraining to vary-ing speech accents. Prior work on accent adaptation of SSLmodels includes introducing an additional classifier into SSLpretraining [13], employing accent-specific adapters [14], andnormalizing pseudo-targets to a single accent [15].\nIn this work, we adapt the technique introduced in Prabhuet al. [16] for SSL pretraining. We introduce accent infor-mation during self-supervised pre-training via a set of accent-specific codebooks. These codebooks are integrated into themodel using a cross-attention module. For each accent seen"}, {"title": "2. Methodology", "content": "Figure 1 illustrates the main workflow of our proposed tech-nique. We adopt the standard two-stage training pipeline usedin self-supervised architectures:\n1. A self-supervised pretraining stage that involves training anencoder using a self-supervised learning objective [29].\n2. A supervised ASR fine-tuning stage that uses the pretrainedencoder from the previous stage and further finetunes it usingan ASR-specific supervised loss."}, {"title": "2.1. Self-supervised Pre-training with Codebooks", "content": "We use the HuBERT self-supervised architecture [24] thatconsists of three modules: a convolution-based waveformencoder (denoted by CONV), a Transformer-based encoder (de-noted by ENC) and a projection (FFNtok) module. CONV takesraw speech $X = {X_1,X_2, ..., X_L |X_i \\in R^d}$ as its input, andmaps it via a stack of convolutions to $F = CONV(X) =${$f_1, f_2,..., f_r|f_i \\in R^d$}. F is subsequently masked to gener-ate $\\tilde{F}$, where a fixed P% of its frames are randomly masked.This masked representation $\\tilde{F}$ further passes through ENC togenerate a contextualized representation $H = ENC(\\tilde{X}) =${$h_1,h_2,...h_T|h_i \\in R^d$}. In parallel, an Acoustic Unit Dis-covery (AUD) module directly acts on X as its input and gen-erates pseudo-target labels $Z = {Z_1, Z_2,..., Z_T |Z_i \\in [1, V]}$via an offline clustering step that uses a single or ensemble ofk-means clusterings. The size of the label set V is determinedby the number of clusters used in AUD. A simple projectionlayer FFNtok is used to convert H from $R^d$ to the targetvocabulary $R^V$, and the HuBERT model is trained end-to-endto predict Z using a weighted cross-entropy loss.\nOur accent-based modifications are restricted to theTransformer-based encoder ENC. The ENC module consistsof a stack of N identical encoder layers. Each layer attendsto the output of the previous layer and contextualizes it usingself-attention blocks and projection layers. The self-attentionblock is responsible for introducing global context, while theprojection layer refines the point-wise information. In additionto this, we introduce a cross-attention block that utilizes atten-tion mechanism to incorporate information from accent-specificcodebooks into the representations. We first discuss how thecodebooks are generated followed by an explanation of howthey are utilized in a single encoder layer.\nCodebook Setup. Let E be the number of accents seen dur-ing training. We define a set of accent-specific codebooks $C = {C^1, C^2, ...C^E |C^i \\in R^{M\\times d}}$. Each $C^i$ comprises Mlearnable codebook entries. That is, each codebook $C^i$ is:\n$C^i = {c^i_1, ..., c^i_M \\in R^d} = Embedding([1,2,...,M])$\nwhere Embedding is a standard embedding layer. For a train-ing speech input X, whose underlying accent ID (indexing theE seen accents) is $a \\in {1, ..., E}$, we deterministically selectthe codebook $C^a$. The codebook entries in $C^a$ are subsequentlyintegrated with all the encoder layers and across all attentionheads. We note here that the codebooks are accent-specific onaccount of a single codebook being deterministically chosen perutterance during training.\nIntegrating Codebooks using Cross-attention. EachTransformer-based encoder layer of the HuBERT architectureconsists of a self-attention block that introduces global context,followed by a position-wise feed-forward block that refinespoint-wise information. These two blocks are separated bylayer normalization and coupled via residual connections. Inour method, we introduce a cross-attention module that ispositioned between the self-attention and feedforward blocksand integrates information from the codebooks into the audiorepresentations generated after self-attention. More precisely,for the ith encoder layer, let A be the input to the cross-attentionblock and \u00c2 be the output. Then, the attention probabilitiesacross the M codebook entries in $C^a$ for the jth position $A_j$ iscomputed as:\n$\\lbrace \\beta^1_j, \\beta^2_j,..., \\beta^M_j \\rbrace = softmax(\\frac{A_jW^Q(C^aW^K)^T}{\\sqrt{d}})$\nwhere $W^Q, W^K \\in R^{d \\times d}$ are learned projection matrices of thecross-attention block of the ith encoder layer and $\\beta^k_j$ \u03b5 [0,1]is the attention weight assigned by the jth representation to thekth entry in codebook $C^a$. Finally, the jth frame in the output$\\hat{A}$ is a weighted average of the entries in codebook $C^a$:\n$\\hat{A_j} = \\sum_{k=1}^M \\beta^k_j(c^a_kW^V)$"}, {"title": "2.2. Supervised ASR Fine-tuning", "content": "For the second stage of ASR fine-tuning, we adopt thestate-of-the-art hybrid CTC-attention end-to-end ASR frame-work [32] that consists of three modules: an encoder (ENCf),a decoder (DEC) and a Connectionist Temporal Classifica-tion (CTC) [33] module. We replace the encoder ENCf withthe pretrained CONV and ENC modules from Section 2.1, pre-trained in conjunction with accent codebooks. ASR fine-tuningmakes use of labeled speech instances (X, Y) where X is araw speech sequence $X = {x_1, x_2,..., X_L|X_i \\in R}$ and Y isa token sequence ${y_1,..., y_M}$. Both the encoder parametersand the accent-specific codebooks in ENCf, initialized from thepretrained model, are further finetuned using a supervised ASRobjective. The DEC module uses a cross-entropy loss (Latt)to autoregressively predict a token yt given previous tokens{Y1,..., Yt-1} and the encoder outputs. The CTC module, onthe other hand, imposes a CTC loss (Letc) directly on the en-coder outputs and predicts a frame-aligned sequence of tokensby marginalizing over all possible alignments of Y to the en-coder outputs. The final loss is the weighted sum of both losses:\n$L_{asr} = \\eta \\times L_{ctc} + (1 - \\eta) \\times L_{att}$   (1)\nwhere \u03b7 is a hyperparameter that balances the objectives."}, {"title": "2.3. Inference using Codebooks", "content": "During inference, we do not assume access to an accent labelfor a test utterance. We employ the joint-beam search proposedby Prabhu et al. [16]. This technique involves performing beamsearch jointly over all the seen accents. Scores for the seen ac-cents are computed using each underlying accent-specific code-book. The beam width holds the best expansions across all seenaccents. More details of the joint beam algorithm are in [16]."}, {"title": "3. Experimental Setup", "content": "We conduct all our pre-training and fine-tuning experimentsusing Fairseq [34] and ESPnet [35] toolkits, respectively, onNVIDIA RTX A6000 GPUs.\nDataset details. We use the Mozilla Common Voice [36]accented English MCV-ACCENT benchmarking dataset\u00b2 in allour experiments. This dataset consists of five seen (AUS, CAN,SCT, UK, US) and nine unseen accents (AFR, HKG, IND,IRL, MAL, NWZ, PHL, SGP, WLS). The dataset includes"}, {"title": "4. Experimental Results and Analysis", "content": "Table 1 compares the performance (WER %) of our proposedsystem against four alternative approaches: 1. Conformer base-line [31] 2. Replacing the encoder in the Conformer with a pre-trained HuBERT model [24], pretrained with the SSL objectiveon data MCV-ACCENT-600 3. Jointly training HuBERT base-line with an accent classifier (MTL) [18] 4. Domain AdversarialTraining (DAT) of the HuBERT baseline with an accent classi-fier on the 12th encoder layer. Similar to previous studies [2,24], we find that replacing Conformer\u2019s encoder with a pre-trained HuBERT model leads to significant improvement in per-formance; we note here that the HuBERT-based encoder is pre-trained from scratch on MCV-ACCENT-600. Furthermore, weobserve that, during self-supervised pre-training, using an ex-isting checkpoint (i.e., the HUBERT-BASE Librispeech check-point from Fairseq [34]) leads to additional performance gains.Given the limited amount of supervised finetuning data in MCV-ACCENT-100, to combat overfitting, we freeze the feature ex-tractor and the first 3 layers of the HuBERT encoder; this yieldsfurther benefits. (In all subsequent experiments, we will usethis best setup of selective ASR finetuning.) We also compareour system against the MTL and DAT approaches. Overall, ourproposed approach performs significantly better on nearly all"}, {"title": "5. Conclusion", "content": "In this work, we propose an accent-aware ASR adaptation tech-nique where accent-specific codebooks are incorporated withinthe Transformer layers of a HuBERT model via cross-attention.This integration happens right from the SSL-based pretrainingstage. The pretrained codebooks and encoder layers are furtherfinetuned using supervised ASR fine-tuning. Compared to ex-isting accent adaptation techniques, we observe that this yieldsignificant WER reductions on English utterances in both seenand unseen accents in the Mozilla Common Voice (MCV) cor-pus. The accent-aware models trained on MCV also generalizewell to out-of-domain accented English samples (from a dif-ferent corpus, L2Arctic) when evaluated in a zero-shot setting.In future work, we aim to use self-training with unlabeled data(with accent labels) to further refine the accent codebooks."}]}