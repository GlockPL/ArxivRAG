{"title": "REVISITING CLIP: EFFICIENT ALIGNMENT OF 3D MRI AND TABULAR DATA USING\nDOMAIN-SPECIFIC FOUNDATION MODELS", "authors": ["Jakob Krogh Petersen", "Valdemar Licht", "Mads Nielsen", "Asbj\u00f8rn Munk"], "abstract": "Multi-modal models require aligned, shared embedding\nspaces. However, common CLIP-based approaches need\nlarge amounts of samples and do not natively support 3D or\ntabular data, both of which are crucial in the medical domain.\nTo address these issues, we revisit CLIP-style alignment by\ntraining a domain-specific 3D foundation model as an image\nencoder and demonstrate that modality alignment is feasi-\nble with only 62 MRI scans. Our approach is enabled by a\nsimple embedding accumulation strategy required for train-\ning in 3D, which scales the amount of negative pairs across\nbatches in order to stabilize training. We perform a thorough\nevaluation of various design choices, including the choice\nof backbone and loss functions, and evaluate the proposed\nmethodology on zero-shot classification and image-retrieval\ntasks. While zero-shot image-retrieval remains challenging,\nzero-shot classification results demonstrate that the proposed\napproach can meaningfully align the representations of 3D\nMRI with tabular data. Code and model checkpoints are\navailable here.", "sections": [{"title": "1. INTRODUCTION", "content": "In the pursuit of general purpose multi-modal medical mod-\nels, it has become of great interest to learn aligned, shared\nembedding spaces. The dominant method for learning joint-\nembedding spaces is the contrastive language-image pre-\ntraining (CLIP) objective [1]. By leveraging 100 million\ncaptioned images, CLIP aligns textual and image represen-\ntations by maximizing the cosine similarity between positive\npairs while minimizing the negative pair similarity. Due to\nthe inherent semantic nature of natural language, CLIP shows\nthat their embeddings efficiently transfer to downstream tasks\nand notably achieves high performance in a long range of\nzero-shot classification tasks.\nWhile the performance of CLIP is impressive, Con-\nVIRT [2] shows that due to the very high inter-class sim-\nilarity present in medical imaging tasks, using medical,\ndomain-specific contrastive training by far outperforms train-\ning on dataset of natural images. ConVIRT obtains a joint-\nembedding space by training on a dataset of 265k 2D ra-\ndiography images paired with textual reports and show that\nthe learned embeddings are meaningfully aligned by doing\nzero-shot classification and image-retrieval.\nVery little work has focused on aligning textual represen-\ntations and 3D medical data, such as MRI. BrainCLIP [3] uses\nfrozen CLIP encoders for brain decoding, aligning the repre-\nsentations of fMRIs obtained while showing subjects images\nfrom ImageNet, with the representations of those images and\ntheir textual representations. However, BrainCLIP does not\nnatively train CLIP on the fMRIs in 3D, but instead uses a\nVAE to embed fMRI into an embedding space of frozen CLIP\nencoders. A gap exists in the literature on how to train CLIP\nmodels on 3D medical data directly.\nIn this paper we address this gap by examining the chal-\nlenging task of learning a shared embedding space between\ntabular data and 3D brain MRIs. The contributions of this\npaper are summarized to:\n\u2022 We introduce, to the best of our knowledge, the first appli-\ncation of CLIP-training to brain MRI in native 3D.\n\u2022 We train a domain-specific foundation model image\nencoder and show that it is instrumental in enabling\ncross-modality alignment, in particular when using small\ndatasets."}, {"title": "2. METHOD", "content": "Starting from the standard CLIP-setup [1], we revisit four\ncore design decisions in CLIP-style training: Encoding 3D\nMRI (\u00a72.1.1), encoding the tabular data (\u00a72.1.2), calculat-\ning the CLIP-loss across batches (\u00a72.2) and obtaining stable\ntraining caused by the 3D input (\u00a72.3). In the following, we\ndirectly ablate the result of each design decision. The experi-\nmental setup is described in \u00a73."}, {"title": "2.1. Foundation Model Encoders", "content": "Central to our method is the observation that when working\nwith datasets of very limited size, we cannot both learn a\nmeaningful representation for each modality and align these\nrepresentations simultaneously. Instead our approach is to\nfirst learn modality-specific encoders and then align them to\na coherent embedding space."}, {"title": "2.1.1. Encoding 3D MRI using domain-specific pretraining", "content": "To obtain such an encoder for 3D brain MRI, we first per-\nform large-scale, foundation model pretraining on a publicly\navailable MRI dataset using the AMAES framework and\nthe BRAINS-45K dataset [5]. AMAES is a framework\nfor foundation model pretraining based on a self-supervised\nMasked Autoencoder strategy. BRAINS-45K contains\n44,756 brain MRI volumes from various public sources. The\ndataset is the largest publicly available pretraining dataset for\nbrain MRI and contains a diverse set of sequence types. We\ntrain for 100 epochs using the default AMAES hyperparame-\nters. We pretrain three different encoder-backbones:\n\u2022 Swin-T [6]. The 3D Swin-Transformer encoder is ex-\ntracted from a pretrained SwinUNETR [7].\n\u2022 MedNeXt [8]. We use the MedNeXt-S with kernel size 3.\n\u2022 Resnet [9, 10]. The 3D Resnet encoder is extracted from\na ResUNet architecture [11].\nDuring CLIP-training, we increase the robustness of the em-\nbedding by applying a suite of augmentations from [12, 13,\n5]. The augmentations include elastic deformations, rotation,\nscaling, and various noise augmentations."}, {"title": "2.1.2. Encoding tabular data", "content": "Based on the observation mentioned in \u00a72.1, we encode the\ntabular data using a BERT model [14]. We use a pretrained\nBERT-Base model with 110M parameters made available by\nthe authors. The tabular data is processed by first transform-\ning it into a natural language sentence using simple template\nfunctions. For instance, the tabular data with attributes\nage_at_diagnosis: 57\ngender: female\nlesions: ['frontal', 'occipital']\nis transformed into the sentence:\n\"The age of the subject is 57. The gender of the patient is\nfemale. A tumor has been identified in the frontal area of the\nbrain. Additionally, a lesion is present in the occipital area.\u201d\nThis sentence is then encoded using the BERT-encoder\nduring alignment. The approach allows the method to be\neasily extended to a long range of medical data including\nfree-text (e.g. notes from medical professionals) and semi-\nstructured data (e.g. radiology reports)."}, {"title": "2.1.3. Effect of using foundation model encoders", "content": "To investigate the effect of using the above foundation model\nencoders, we perform CLIP-training with encoders initialized\nwith foundation model weights and random weights. The re-\nsult is given in Table 1. We expect the model to perform very\npoorly in particular when there is no language model, since\nthe tabular data is encoded as unstructured free-text. We find\nthat the 3D MRI encoder provide a +0.05 boost in average\nAUC."}, {"title": "2.2. Computing the CLIP-loss across batches", "content": "The large size of 3D volume representations significantly\nreduces the maximum feasible batch size during training. To"}, {"title": "2.3. Obtaining stable training in 3D", "content": "Training in 3D is challenging. In our method, models are\ntrained in native 3D, by extracting sub-patches from each vol-\nume of size 643 for 50.000 steps using the AdamW optimizer\nand a cosine annealing learning rate scheduler. Additional to\na larger batch size as described in \u00a72.2, we found three things\nto be important in order to obtain stable training and avoid-\ning collapse: 1. Using a warm-up of the learning rate for\n2000 steps using a linear increasing scheduler from 0 to the\nstarting learning rate. 2. The vision encoder needed to have\nan order of magnitude higher starting learning rate than the\ntext encoder. 3. The temperature-parameter in the CLIP loss\nneeded to be carefully tuned."}, {"title": "3. EXPERIMENTAL SETUP", "content": "We validate the effectiveness of our embedding alignment\non two challenging tasks: zero-shot classification and image-\nretrieval of tumor location from brain MRI using the Brain-\nTR-GammaKnife dataset [16]. The dataset is created to study\nthe recurrence of brain tumors after Gamma-Knife Radiother-\napy. We use this dataset since it contains dense annotations on\nthe lesion level with auxiliary tabular information about each\nscan, such as age, gender, and brain tumor location. The fol-\nlowing section describes the setup used to obtain our results."}, {"title": "3.1. Data", "content": "Data is T1 MPRAGE with Gadolinium contrast MRI volumes\nacquired using a 1.5 T Siemens Magnetom scanner with 1mm\nisotropic resolution. The dataset contains MRI volumes from\n47 different brain cancer patients with 21 males and 26 fe-\nmales ranging in age from 23 to 77 years who underwent\nGamma Knife radiation therapy. Most patients had 1-2 treat-\nment courses, but the dataset includes outliers with up to eight\ntreatment courses, leading to a total of 77 MRI scans. A total\nof 244 lesions were collected and annotated during treatment\nplanning. We split the data into training and test sets using\na 80/20 split on the scan level. For validation we use 5-fold\ncross validation on the training set. We use the validation split\nfor hyperparameter tuning. The training dataset contains 62\nMRI scans.\nRegion Labels. To perform our evaluation, we catego-\nrize the annotated lesions in the dataset into 5 distinct labels.\nWe base the first four labels on the largest and most discrim-\ninatory regions in the cerebral cortex: frontal lobe, parietal\nlobe, temporal lobe, and occipital lobe. Finally, we include\nthe cerebellar, as the region is well represented in the dataset.\nBased on this categorization, we define a subset of the dataset\ncontaining 222 lesions.\nPreprocessing. All MRI volumes in the dataset are pre-\nprocessed using the Yucca framework [12]. The scans are\ndownsampled to a resolution of 1923 to allow for smaller\npatch sizes that still contain anatomical information. The vol-\numes are bias-field-corrected and transformed into the RAS+\norientation. Each volume is independently normalized using\nz-normalization."}, {"title": "3.2. Hyperparameters and experimental details", "content": "We do careful hyperparameter tuning on the validation splits.\nIn particular, we find that the optimal batch size for MedNeXt\nand Resnet models were 64 (accumulation frequency N\n= 8) and for the Swin-T the optimal batch size was 128 (N =\n16). Further, we used initial temperature \\u03c4 = 1.351 in the\nCLIP-loss and an embedding space with 512 dimensions. All\nexperiments were run on single Titan RTX GPUs with 24 GB\nVRAM."}, {"title": "3.3. Evaluation Tasks", "content": "To evaluate the alignment of the shared embeddings, we seek\nto explore the relation between semantics in one modality\nfrom the other. By using zero-shot classification and image-\nretrieval, we evaluate the alignment by going from one modal-\nity to the other. For zero-shot classification for a given scan,\nwe obtain an image embedding, and compare the cosine simi-\nlarity to embeddings of sentences describing each region. For\nzero-shot image-retrieval we go the other way. By embed-\nding a sentence, we can rank all scans in the test set by their\neuclidean distance to the sentence embedding. For both, we\nembed regions using sentences of the form:\nThere is a lesion in the [region] section\nFor all results, we report averages over five folds on the test\ndataset along with standard error of the mean."}, {"title": "4. RESULTS & DISCUSSION", "content": "Zero-shot classification. We measure the area under the\nreceiving operating characteristic curve (AUC). Owing to the\nmulticlass nature of the problem, we employ the one-versus-\nrest (OvR) strategy, where we treat each sample as a binary\nclassification task. Thus, we measure the probability of the\nmodel predicting the ground truth label versus all other pos-\nsible labels and report the average AUC over this heuristic.\nResults are given in Table 3. Our results show that all models\nbeat random, while Swin-T encoder yields the highest AUC\nof all models and is closely followed by the ResNet model.\nFor the labels frontal and cerebellar, the we obtain\nan AUC of over 0.8, while on labels parietal we obtain\nan AUC which is close to random. We note that the problem\nis significantly complicated by the fact that each patient typi-\ncally has multiple tumors.\nZero-shot image retrieval. We measure both accuracy, mean\nreciprocal rank (MMR) and mean average precision (mAP)\ncalculated for each label and then averaged. Results are given\nin Table 4. Both Swin-T and the MedNeXt models beat ran-\ndom, while interestingly Resnet does not."}, {"title": "4.1. Discussion", "content": "The results on the difficult zero-shot tasks, highlight that\nalignment of image and textual embedding spaces, is possible\ngiven pretrained domain-specific image encoders. In partic-\nular, the quality of the joint-embedding space when using a\nSwin-Transformer image encoder meaningfully beat random.\nInterestingly, the MedNext encoder outperforms the Resnet\non image retrieval, while the opposite is true on classification.\nThis highlights, that robustness of the joint-embedding space\nis still an issue when using very small datasets.\nLimitations and future work. This study is conducted using\na single dataset and the evaluation does not investigate the\neffectiveness of our methodology on data outside the dataset\ndistribution. Due to the limited label space of the dataset, an\nevaluation of the model's zero-shot performance on a larger\nset of tasks is missing. Additionally, we did not have the re-\nsources to search for optimal hyperparameters for each batch\nsize. This could significantly impact performance for mod-\nels with higher batch sizes. This study opens up for future\nwork in multiple directions: scaling the dataset size, work-\ning with free-text image captions and exploring the models\nperformance in a few-shot setting."}, {"title": "5. CONCLUSION", "content": "In this study, we explore CLIP-alignment using a small\ndomain-specific dataset using domain-specific foundation\nmodel encoders. We show that tabular data transformed into\nnatural language can supervise 3D vision tasks and that a\njoint embedding space can yield meaningful results on non-\ntrivial downstream tasks in a zero-shot setting. Enabled by\na embedding accumulation strategy, we scale the CLIP-loss\nacross batches and demonstrate how it improves the stability\nand performance of different vision encoders. Our zero-shot\nclassification results show that we can obtain a reasonable\nalignment using very few data points."}, {"title": "6. COMPLIANCE WITH ETHICAL STANDARDS", "content": "This research study was conducted retrospectively using hu-\nman subject data made available in open access. Ethical ap-\nproval was not required as confirmed by the license attached\nwith the open access data."}]}