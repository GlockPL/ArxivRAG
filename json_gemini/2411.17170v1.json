{"title": "LEARNING MONOTONIC ATTENTION IN TRANSDUCER\nFOR STREAMING GENERATION", "authors": ["Zhengrui Ma", "Yang Feng", "Min Zhang"], "abstract": "Streaming generation models are increasingly utilized across various fields, with\nthe Transducer architecture being particularly popular in industrial applications.\nHowever, its input-synchronous decoding mechanism presents challenges in tasks\nrequiring non-monotonic alignments, such as simultaneous translation, leading to\nsuboptimal performance in these contexts. In this research, we address this issue\nby tightly integrating Transducer's decoding with the history of input stream via\na learnable monotonic attention mechanism. Our approach leverages the forward-\nbackward algorithm to infer the posterior probability of alignments between the\npredictor states and input timestamps, which is then used to estimate the context\nrepresentations of monotonic attention in training. This allows Transducer models\nto adaptively adjust the scope of attention based on their predictions, avoiding the\nneed to enumerate the exponentially large alignment space. Extensive experiments\ndemonstrate that our MonoAttn-Transducer significantly enhances the handling of\nnon-monotonic alignments in streaming generation, offering a robust solution for\nTransducer-based frameworks to tackle more complex streaming generation tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Streaming generation is a widely studied problem in fields such as speech recognition (Raffel et al.,\n2017; Zhang et al., 2020; Seide et al., 2024), simultaneous translation (Cho & Esipova, 2016; Gu\net al., 2017; Seamless Communication et al., 2023), and speech synthesis (Ma et al., 2020a; Zhang\net al., 2024; Wang et al., 2024). Unlike modern turn-based large language models, streaming models\nneed to start generating the output before the input is completely read. This necessitates a careful\nbalance between generation quality and latency.\nPopular streaming generation methods can be broadly divided into two categories: Attention-\nbased Encoder-Decoder (AED; Bahdanau et al., 2015) and Transducer (Graves, 2012). Streaming\nAED models adapt the conventional sequence-to-sequence framework (Bahdanau, 2014) to support\nstreaming generation. They often rely on an external policy module to determine the READ/WRITE\nactions in inference and to direct the scope of cross-attention in training. Examples include Wait-k\npolicy (Ma et al., 2019) and monotonic attention-based methods (Raffel et al., 2017; Arivazhagan\net al., 2019; Ma et al., 2020d; 2023a). On the other hand, Transducer models connect the encoder\nand predictor through a joiner rather than using cross-attention. As shown in Figure 1a, the joiner is\ndesigned to synchronize the encoder and predictor by expanding its output vocabulary to include a\nblank symbol e, which indicates a READ action. Due to the decoupling of the predictor state from\nthe encoder state, READ/WRITE states in Transducer can be represented by a two-dimensional lat-\ntice. This allows for the computation of total probabilities using the forward-backward algorithm\n(Graves, 2012), facilitating end-to-end optimization. Benefited from joint optimization of all poten-"}, {"title": "2 BACKGROUND", "content": null}, {"title": "2.1 STREAMING GENERATION", "content": "Streaming generation models typically process a streaming input x\n{x1,..., XT} and generate a\ntarget sequence y = {y1, ..., yu } in a streaming manner. To measure the amount of source informa-\ntion utilized during generation, a monotonic non-decreasing function g(u) is introduced to represent\nthe number of observed source tokens at the time of generating Yu."}, {"title": "2.2 TRANSDUCER", "content": "As shown in Figure 1a, Transducer model (Graves, 2012) comprises three components: an encoder,\na predictor, and a joiner. The encoder unidirectionally encodes the received input prefix X1:t into a"}, {"title": "3 \u041c\u0415\u0422\u041dOD", "content": null}, {"title": "3.1 OVERVIEW", "content": "MonoAttn-Transducer works similarly to standard Transducer, with the key difference being that its\npredictor can attend to the encoder history using monotonic attention. During streaming generation,\nthe scope of monotonic attention includes all source context representations that have already ap-\npeared. Formally, when the predictor encodes the u-th target state, it depends on the representations\nof previous target states and the existing source context:\n$Su = fo(80:u-1, h1:g(u))$,"}, {"title": "3.2 TRAINING ALGORITHM", "content": "Training MonoAttn-Transducer is challenging as it exponentially expands Transducer's state space.\nTo address this issue, we firstly leverage the forward-backward algorithm to compute the posterior\nprobability of aligning target representation su with source representation ht (i.e., the probability of\ngenerating token yu immediately after reading xt). This posterior alignment is then used to estimate\nthe expected context vector in the monotonic cross-attention for each predictor state in training.\nDetailed explanations are provided in the following."}, {"title": "3.2.1 POSTERIOR ALIGNMENT", "content": "Suppose we have a probability lattice Pr(v|t, u), representing the probability of generating token\nv from ht and su, for 1 < t < T, 0 \u2264 u < U, and v \u2208 VU {e}. The posterior probability of\ngenerating Yu at the moment xt is read can be represented by:\n$Tu,t\\frac{Pr(y1:u-1 X1:t) Pr(yult, u \u2013 1)Pr(yu+1:U|Xt:T)}{Pr(y1:UX1:T)}$,\nwith the edge case:\n$\u03c0\u03bf, = {1 1 t=1\n10 t\u22601}$,\nwhich implies that the predictor state so is generated immediately after the first source token arrives.\nUsing the forward and backward variables introduced in Section 2.2, Equation 5 can be concisely\nexpressed as follows:\n$\u03a0u,t\\frac{a(t, u \u2013 1)Pr(yult, u \u2013 1)\u1e9e(t, u)}{\u03b1(T, U)Pr(e|T, U)}$,\nThis guarantees that the posterior alignment probability for all pairs (t, u) can be solved in O(TU)\ntime using the above forward-backward algorithm, facilitating the calculation of the expected con-\ntext representation introduced later."}, {"title": "3.2.2 MONOTONIC ATTENTION", "content": "The incorporation of monotonic attention makes the representation of predictor states relevant to spe-\ncific READ/WRITE history, leading to a prohibitively large state space for enumerating alignments.\nTherefore, we turn to estimate the context vector in monotonic attention based on the posterior align-\nment probability during training. This approach enables the model to adaptively adjust the scope of\ncross-attention according to its prediction. Consequently, MonoAttn-Transducer learns a monotonic\nattention mechanism while maintaining the same time and space complexity as Transducer.\nFormally, given the energy eu,t for the pair consisting of encoder state ht and predictor state su, as\nwell as the posterior alignment probability \u03c0u,t, the expected context representation cu for predictor\nstate su can be expressed as:\n$Cu = \u2211T t=1\nTu,t\n\u2211tt=1 exp (eu,t')h.\n\u2211Tt=1 \u2211t=1 exp (eu,t'')$,\nThis indicates that the expected context representation cu is a weighted sum of context representa-\ntions under various amount of source information, with the weights given by the posterior alignment\nprobability \u03c0u,t. The nested summation operations in Equation 8 may lead to an increase in compu-\ntational complexity. Fortunately, Arivazhagan et al. (2019) suggests that it can be rewritten as:\n$\u03a6u, =\n\u2211Tt=1 Tu,t' exp (eu,t)\n\u2211t=1 exp (eu,t'')ht''$\n$Cu = \u2211T t=1\u03a6u,tht$\nEquation 9 can then be computed efficiently using cumulative sum operations (Arivazhagan et al.,\n2019)."}, {"title": "3.2.3 TRAINING WITH PRIOR ALIGNMENT", "content": "The above algorithm facilitates MonoAttn-Transducer in learning monotonic cross-attention with\nposterior alignment probability. However, this presents a chicken-and-egg paradox: the posterior\nalignment is derived from an output probability lattice constructed using an estimated context rep-\nresentation, while the context vector is, in turn, estimated using a posterior alignment. We address\nthis problem by using a prior alignment to construct a prior output probability lattice. This lattice is\nthen used to infer the posterior alignment and train MonoAttn-Transducer's monotonic attention."}, {"title": "3.2.4 CHUNK SYNCHRONIZATION", "content": "In speech audio, there often exists strong temporal dependencies between adjacent frames. There-\nfore, a chunk size C is typically set, and the streaming model makes decisions only after receiving a\nspeech chunk (Ma et al., 2020c). In terms of Transducer models, when a READ action is taken, the\nsource representation is updated after a new speech chunk is read. The new source representation is\nthen set as the representation of the last frame in the chunk (Liu et al., 2021; Tang et al., 2023). In\nsuch a situation, the receptive field of MonoAttn-Transducer's cross-attention for predictor state su\nencompasses all hidden states in the received chunks, i.e., h1:C.\u011f(u), where \u011f(u) denotes the number\nof received chunks when generating token Yu. To bridge the gap between training and inference,\nthe posterior alignment probability utilized in training process is adjusted by transferring all the\nprobability mass on encoder states within a chunk to the last state in the chunk:\n$=(d-1)-C+1 Tu, t = d.c\nd.C$\n$\u00d1u,t =\nfor d = 1,2,3,...$"}, {"title": "4 RELATED WORK", "content": "Our work is closely related to researches in designing cross-attention modules for Transducer mod-\nels. Prabhavalkar et al. (2017) pioneered the use of attention to link the predictor and encoder.\nHowever, their design requires the entire source to be available, limiting it to offline generation. For\nstreaming generation, the receptive field of attention must synchronize with the input. This syn-\nchronization leads to an exponentially large state space, which significantly complicates the training\nprocess. To mitigate this issue, Liu et al. (2021) separated the predictor's cross-attention from its\nself-attention, ensuring that cross-attention occurs only after self-attention. This approach maintains\nthe independence of predictor states from READ/WRITE path history, allowing for standard train-\ning methods. However, this separation limits the richness of the predictor's learned representations."}, {"title": "5 EXPERIMENTS", "content": "We validate the performance of our MonoAttn-Transducer on two typical streaming generation\ntasks: speech-to-text and speech-to-speech simultaneous translation. The differences in grammatical\nstructures between the source and target languages often necessitate word reordering during gener-\nating translation. This property makes the simultaneous translation task well-suited for evaluating\nthe ability of MonoAttn-Transducer in handling non-monotonic alignments."}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "Datasets We conduct experiments on two language pairs of MuST-C speech-to-text translation\ndatasets: English to German (En\u2192De) and English to Spanish (En\u2192Es) (Di Gangi et al., 2019). \nFor speech-to-speech experiments, we evaluate models on CVSS-C French to English (Fr\u2192En)\ndataset (Jia et al., 2022).\nModel Configuration We use the open-source implementation of Transformer-Transducer (Zhang\net al., 2020) from Liu et al. (2021) as baseline and build our MonoAttn-Transducer upon it. The\nspeech encoder consists of two layers of causal 2D-convolution followed by 16 chunk-wise Trans-\nformer layers with pre-norm. Each convolution layer has a 3\u00d73 kernel with 64 channels and a stride\nsize of 2, resulting in a downsampling ratio of 4. In chunk-wise Transformer layers, the speech\nencoder can access states from all previous chunks and one chunk ahead of the current chunk (Wu\net al., 2020; Shi et al., 2021). The chunk size is adjusted within the set {320, 640, 960, 1280}ms.\nOffline results are obtained by setting the chunk size longer than any utterance in the corpus. Both\nsinusoidal positional encoding (Vaswani et al., 2017) and relative positional attention (Shaw et al.,\n2018) are incorporated into the speech encoder. Sinusoidal positional encoding is applied after the\nconvolution layers. The predictor comprises two autoregressive Transformer layers with post-norm,\nutilizing only sinusoidal positional encoding. The monotonic attention is similar to standard cross-\nattention but differs in its receptive field. The joiner is implemented as a simple FFN. We incorporate\nthe multi-step decision mechanism (Liu et al., 2021) with a decision step of 4. All Transformer layers\ndescribed above are configured with a 512 embedding dimension, 8 attention heads and a 2048 FFN\ndimension. The total number of parameters for the Transducer baseline and MonoAttn-Transducer\nare 65M and 67M, respectively. More implementation details are provided in Appendix A.\nEvaluation We use SimulEval toolkit (Ma et al., 2020b) for evaluation. Translation quality is as-\nsessed using case-sensitive detokenized BLEU (Papineni et al., 2002; Post, 2018) and neural-based\nCOMET-22 score. Latency is measured by word-level Average Lagging (AL; Ma et al., 2019;\n2020c).\u00b9 For speech-to-speech experiments, translation quality is assessed using ASR-BLEU and\nlatency is measured by delay of generated waveform chunks (Ma et al., 2022)."}, {"title": "5.2 MAIN RESULTS", "content": "We evaluate the performance of MonoAttn-Transducer against Transducer baseline across various\nlatency conditions obtained by varying the chunk size. In this comparison, we consider two configu-\nrations of MonoAttn-Transducer. The first, referred to as MonoAttn-Transducer-Posterior, is trained\nstrictly according to Algorithm 1. The second, termed MonoAttn-Transducer-Prior, is optimized"}, {"title": "5.3 COMPARISON WITH STATE-OF-THE-ART", "content": "We compare MonoAttn-Transducer with state-of-the-art open-source approaches in simultaneous\ntranslation, including Wait-k (Ma et al., 2020c), RealTrans (Zeng et al., 2021), CAAT (Liu et al.,\n2021), MU-ST (Zhang et al., 2022), EDAtt (Papi et al., 2023), Seg2Seg (Zhang & Feng, 2023) and\nNAST (Ma et al., 2024). Further details about baselines are available in Appendix B. Results are\nplotted in Figure 3a and 3b. We observe that learning monotonic attention significantly enhances the\nperformance of Transducer, making it comparable to state-of-the-art models. Compared to CAAT,\nanother Transducer-based model, MonoAttn-Transducer demonstrates superiority in scenarios with\nless stringent latency requirements. Under a latency of approximately 2s, it outperforms CAAT by\n1.1 BLEU in En-De. This clearly demonstrates the advantage of MonoAttn-Transducer's tightly\ncoupled self-attention and cross-attention modules in the predictor, which facilitates the learning of\nricher representations.\nAs discussed in Section 4, TAED is another Transducer-based model highly relevant to our work.\nHowever, the code and distilled data used to train TAED in Tang et al. (2023) have not been made\npublicly available. This lack of open access hinders a fair comparison of TAED with our MonoAttn-\nTransducer. Despite this, we attempt to analyze the performance by comparing each with Transducer\nbaseline in their respective experimental settings. The comparison is shown in Table 8. We have\nobserved that the improvement from TAED is more pronounced with smaller chunk sizes, which\ncontrasts with the results of MonoAttn-Transducer. We speculate that this is because, in TAED,\nthe representations of all generated predictor states are updated every time the encoder receives\na new speech chunk. This helps TAED generate more accurate representations when the chunk\nsize is small. However, this mechanism in TAED incurs an O(T + U) forward propagation cost\nduring simultaneous inference, which can significantly increase latency in practice due to heavy\ncomputational overhead when the chunk size is small. In contrast, MonoAttn-Transducer maintains\nan O(U) complexity as Transducer baseline. As shown in Table 7, this property minimizes the gap\nbetween ideal and computation-aware latency, offering advantages in real-time applications."}, {"title": "5.4 RESULTS OF SPEECH GENERATION", "content": "Speech-to-speech simultaneous translation requires implicitly performing ASR, MT and TTS simul-\ntaneously, and also handling the non-monotonic alignments between languages, making it suitable\nto evaluate models on streaming speech generation. We adopted a textless setup in our experiments,\ndirectly modeling the mapping between speech (Zhao et al., 2024). Results are provided in Table 3.\nThe results demonstrate that MonoAttn-Transducer significantly reduces generation latency (AL).\nWith a chunk size of 320ms, it achieves Transducer's offline generation quality, but reducing lagging\nto 118ms. For offline settings, our approaches further improves speech generation quality (19.3 vs.\n18.0). These results highlight the effectiveness of our approach in achieving a better quality-latency\ntrade-off also for streaming speech generation."}, {"title": "6 ANALYSIS", "content": null}, {"title": "6.1 CHOICE OF PRIOR ALIGNMENT", "content": "In Section 3.2.3, we introduced two choices for prior alignment: the uniform prior puni, which\nassumes an equal probability of generation at each time step; and the diagonal prior pdia, which\nprefers ideal synchrony between the source and target. We employed the diagonal prior pdia as the\ndefault choice in the aforementioned experiments. In this section, we examine the impact of differ-\nent choices.The results are displayed in Table 4. As shown, MonoAttn-Transducer's performance\ndemonstrates robustness to the choice of prior alignment, with only minor impacts on both transla-\ntion quality and latency across all chunk size settings. In Appendix D, we visualize the posterior\nalignment when using different priors. We have observed that, even with significant differences in\nthe prior distribution, the posterior remains fairly robust when the chunk size is constant. This nice\nproperty reinforces the robustness of using the inferred posterior to train monotonic attention."}, {"title": "6.2 HANDLING NON-\u039c\u039f\u039d\u039f\u03a4\u039fNICITY", "content": "To illustrate MonoAttn-Transducer's capability in handling reorderings through learning monotonic\nattention, we evaluate its performance against the Transducer baseline across samples with varying\nlevels of non-monotonicity. Intuitively, samples with a higher number of crosses in the alignments\nbetween source transcription and reference text pose greater challenges. We therefore evenly par-\ntition the test set based on the number of cross-alignments, labeling them as easy, medium and\nhard. The results are presented in Figure 3c. We observe that MonoAttn-Transducer shows a\nmore substantial improvement over Transducer in the medium and hard subsets across most chunk\nsize settings. However, with a chunk size of 320ms, the improvement is particularly notable in the\neasy subset. These findings highlight the unique capabilities of MonoAttn-Transducer in managing\nnon-monotonic alignments. As analyzed in Section 5.2, MonoAttn-Transducer benefits more from\nlearning monotonic attention with a larger chunk size, and this enhanced ability is evident in subsets\nwith higher levels of non-monotonicity. On the other hand, when the chunk size is extremely small,\nMonoAttn-Transducer has limited flexibility to wait for more source information before processing,\nthus showing more significant improvement in the easy subset under the condition."}, {"title": "6.3 TRAINING EFFICIENCY", "content": "Training Time: We analyze each step in Algorithm 1 to compare the training time differences\nbetween MonoAttn-Transducer and baseline. We observe that Lines 1, 2, 6 involve naive matrix\ncomputation without requiring gradients. The additional time overhead introduced by our method\narises from Lines 3, 4, 5. Specifically, this includes an additional forward pass of the predictor and\nthe computation for the posterior alignment. The overhead from the posterior calculation is approx-\nimately equivalent to that incurred during loss calculation, as both rely on the forward-backward\nalgorithm. Empirically, we found MonoAttn-Trasducer is 1.33 times slower than Transducer base-\nline with the same configuration on Nvidia L40 GPU.\nMemory Consumption: Compared to baseline, the additional memory overhead of MonoAttn-\nTransducer comes solely from its monotonic attention module. The extra forward pass of the predic-\ntor is performed without requiring gradients, so it is excluded from the computation graph. Empir-\nically, we observed that the peak memory usage of Transducer baseline is 28GB, while MonoAttn-"}, {"title": "7 CONCLUSION", "content": "In this paper, we propose an efficient algorithm for Transducer models to learn monotonic atten-\ntion. Extensive experiments demonstrate that our MonoAttn-Transducer significantly improves the\nability in handling non-monotonic alignments in streaming generation, offering a robust solution for\nTransducer-based frameworks to tackle more complex streaming generation tasks."}, {"title": "A IMPLEMENTATION DETAILS", "content": "Pre-processing The input speech is represented as 80-dimensional log mel-filterbank coefficients\ncomputed every 10ms with a 25ms window. Global channel mean and variance normalization is\napplied to the input speech. During training, SpecAugment (Park et al., 2019) data augmentation\nwith the LB policy is additionally employed. We use SentencePiece (Kudo & Richardson, 2018) to\ngenerate a unigram vocabulary of size 10000 for the source and target text jointly. Sequence-level\nknowledge distillation (Kim & Rush, 2016) is applied for fair comparison (Liu et al., 2021). For\nspeech-to-speech experiments, we resample the source audio to 16kHz and apply identical prepro-\ncessing steps as those used in speech-to-text experiments. For the target speech, we also downsample\nthe audio and extract discrete units utilizing the publicly available pre-trained mHuBERT model and\nK-means quantizer. No training data manipulation is applied in speech-to-speech experiments.\nTraining Details Considering that training MonoAttn-Transducer involves two critical processes:\ninferring the posterior alignment and estimating the context vector, instability in either step can lead\nto training failure. Therefore, we introduce a curriculum learning strategy for MonoAttn-Transducer.\nWe first pretrain the model in an offline setting. In pretraining, all predictor states can attend to the\ncomplete source input, and the model is trained as an offline Transducer. This pretraining phase\nallows the monotonic attention module to warm up by learning full-sentence attention, thereby en-\nhancing its stability during subsequent adaptation to a streaming scenario. In finetuning, we apply\nAlgorithm 1 to adjust MonoAttn-Transducer with various chunk size configurations. During both\ntraining phases, we set the dropout rate to 0.1, weight decay to 0.01, and clip gradient norms exceed-\ning 5.0. The dropout rates for activation and attention are both set to 0.1. The pretraining spans 50k\nupdates with a batch size of 160k tokens. The learning rate gradually warms up to 5e-4 within 4k\nsteps. Finetuning involves training for 20k updates and other hyper-parameters remain consistent.\nThroughout the training, we optimize models using the Adam optimizer (Kingma & Ba, 2015). Au-\ntomatic mixed precision training is applied. It takes approximately one day to pretrain in an offline\nsetting and another day for streaming adaptation on a server with 4 Nvidia L40 GPUs."}, {"title": "B BASELINES", "content": "We compare our proposed MonoAttn-Transducer with the following state-of-the-art open-source\napproaches (without using pretrained encoder or any data augmentation method for fair comparison).\nAED-BASED MODELS\nWait-k (Ma et al., 2020c): It executes wait-k policy (Ma et al., 2019) by setting the pre-decision\nwindow size to 280 ms.\nRealTrans (Zeng et al., 2021): It detects word number in the streaming speech by counting blanks\nin CTC transcription and applies wait-k-stride-n strategy accordingly.\nMU-ST (Zhang et al., 2022): It trains an external segmentation model, which is then utilized to\ndetect meaningful units for guiding generation.\nSeg2Seg (Zhang & Feng, 2023): It alternates between waiting for a source segment and generating\na target segment in an autoregressive manner.\nEDAtt (Papi et al., 2023): It calculates the attention scores towards the latest received frames of\nspeech, serving as guidance for an offline-trained translation model during simultaneous inference.\nCTC-BASED MODELS\nNAST (Ma et al., 2024): It introduces a streaming generation model with fast computation speed by\nleveraging a non-autoregressive transformer and CTC decoding (Graves et al., 2006)."}, {"title": "C NUMERICAL RESULTS", "content": "In addition to Average Lagging (AL; Ma et al., 2020c), we also incorporate Average Proportion (AP;\nCho & Esipova, 2016), Differentiable Average Lagging (DAL; Arivazhagan et al., 2019) and Length\nAdaptive Average Lagging (LAAL; Papi et al., 2022) as metrics to evaluate the latency. AL, DAL and\nLAAL are all reported with milliseconds. The trade-off between latency and translation quality is\nattained by adjusting the chunk size C. The offline results are obtained by setting the chunk size to\nbe longer than any utterance in the dataset (C = \u221e). We use SimulEval v1.1.4 for evaluation in\nall the experiments. The numerical results of MonoAttn-Transducer are presented in Table 5 and 6.\nA comparison of the computation-aware latency metrics for AL and LAAL between the Transducer\nand MonoAttn-Transducer models is presented in Table 7."}, {"title": "D VISUALIZATION", "content": "In this section, we present more examples of diagonal prior and its posterior from training corpus.\nAdditionally, we also provide examples of uniform prior and its posterior for comparison. We have\nobserved that, even with significant differences in the prior distribution, the posterior remains fairly\nrobust when the chunk size is constant. The vertical axis represents the target subword sequence\nand the horizontal axis represents the speech waveform. Darker areas indicate higher alignment\nprobabilities. We use Montreal Forced Alignment tools (McAuliffe et al., 2017) to obtain speech-\ntranscription alignments for illustration."}]}