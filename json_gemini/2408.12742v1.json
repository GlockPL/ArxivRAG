{"title": "TREX- Reusing Vision Transformer's Attention for Efficient Xbar-based Computing", "authors": ["Abhishek Moitra", "Abhiroop Bhattacharjee", "Youngeun Kim", "Priyadarshini Panda"], "abstract": "Due to the high computation overhead of Vision Transformers (ViTs), In-memory Computing architectures are being researched towards energy-efficient deployment in edge-computing scenarios. Prior works have proposed efficient algorithm-hardware co-design and IMC-architectural improvements to improve the energy-efficiency of IMC-implemented ViTs. However, all prior works have neglected the overhead and co-depencence of attention blocks on the accuracy-energy-delay-area of IMC-implemented ViTs. To this end, we propose TREX- an attention-reuse-driven ViT optimization framework that effectively performs attention reuse in ViT models to achieve optimal accuracy-energy-delay-area tradeoffs. TReX optimally chooses the transformer encoders for attention reuse to achieve near- iso-accuracy performance while meeting the user-specified delay requirement. Based on our analysis on the Imagenet-1k dataset, we find that TReX achieves 2.3\u00d7 (2.19\u00d7) EDAP reduction and 1.86\u00d7 (1.79\u00d7) TOPS/mm\u00b2 improvement with ~1% accuracy drop in case of DeiT-S (LV-ViT-S) ViT models. Additionally, TReX achieves high accuracy at high EDAP reduction compared to state-of-the-art token pruning and weight sharing approaches. On NLP tasks such as CoLA, TReX leads to 2% higher non-ideal accuracy compared to baseline at 1.6\u00d7 lower EDAP.", "sections": [{"title": "I. INTRODUCTION", "content": "The high accuracy and robustness of vision transformers (ViT) [1]\u2013[3] on large-scale image recognition tasks have made them suitable candidates for edge intelligence [4], [5]. However, as seen in Fig. 1a, the huge computation overhead of ViTs due to feed-forward operations (layers Q, K, V, Projection, MLP), softmax operation (S(QKT)) and matrix multiplications (Matmul QKT and Matmul S(QKT)V, where S denotes softmax) pose a major roadblock towards resource-constrained edge-deployment. To this end, In-Memory Computing (IMC) architectures have been proposed as an energy-efficient alternative to traditional von-Neumann computing architectures for edge-computing. Compared to von-Neumann architectures, IMC effectively mitigates weight-specific data movement between memories, thereby alleviating the memory-wall bottleneck [6], [7]. Additionally, IMC architectures enable highly parallel multiply-accumulate computations per cycle in an energy and area-efficient manner, leveraging the analog nature of computing [6], [7].\nHowever, as seen in Fig. 1b, the attention module in IMC-implemented ViTs consume high energy, delay and area overhead due to fully connected, Matmul and softmax layers [10], [11]. The energy consumption of the attention block is attributed to the crossbar read operations over a large number of tokens and the softmax (S(QKT)) operations (See Section"}, {"title": "II. RELATED WORK", "content": "ViT Optimization Works: Works such as [9], [12], [20], [21] have proposed token pruning wherein redundant tokens have been pruned out to improve the efficiency. Some recent works have adopted token merging techniques wherein redundant tokens are systemically merged into a single token to achieve accuracy-efficiency tradeoff [12]. Both token pruning approaches use additional predictor networks to distinguish the important and non-informative tokens to achieve low latency while maintaining good accuracy. Recent works [13], [22] have performed static pruning of weights and attention head combined with token pruning to achieve higher efficiency. Additionally, weight sharing approaches [14], [15] have been proposed to reduce the number of parameters required by ViTs. Here, the weights of one encoder are shared between multiple encoders. While weight sharing compresses the ViT model, it does not reduce the computation overhead. There have been several works [23], [24] that have proposed sparsifying the attention blocks by pruning the heads and the tokens. However, these works only partially overcome the attention overhead and require sparse computation hardware to efficiently implement the sparse attention operation.\nTReX is a ViT optimization strategy that differs from prior optimization works in several aspects. While prior works focus on the token and parameter overhead of ViTs, TReX targets the compute and area expensive attention blocks. While ViT token pruning methods require several predictor models to select important and eliminate non-informative tokens, TReX uses attention reuse in ViTs that does not require any extra computation overhead. Unlike prior weight sharing techniques that only reduce the parameter overhead of ViTs and do not reduce the computation overhead, TReX effectively ameliorates both computation and parameter overhead of ViTs.\nIMC-implementations of Transformers: Recently, many works have proposed efficient IMC implementations for transformers [10], [11]. The authors in [10] propose fully analog implementations for transformers by using memristive circuits for dot-product operations and analog circuits for implementing non-linear functions such as GeLU, ReLU and softmax. Another work ReTransformer [11] proposes matrix decomposition techniques to eliminate the bottlenecks introduced during transformer operation. However, prior IMC- implementations of transformers have neglected the energy and delay overheads of NVM device write operations during"}, {"title": "III. BACKGROUND ON VISION TRANSFORMERS", "content": "A Vision Transformer (ViT) model proposed in [1], [8] segments an image into multiple patches, commonly referred to as tokens. Depending on the patch size, the number of tokens t is determined. Each token is embedded into a d- sized feature space. Thus, the input to an encoder X is a t\u00d7d dimension vector. A ViT consists of multiple encoders determined by the NEncoders parameter.\nFig. 1a shows the architecture of a ViT encoder. In each encoder, the inputs X of dimensions t \u00d7 d are multiplied with the weights WQ, WK and WV to generate the Query (Q), Key (K) and Value (V) matrices. ViTs use multi-head self-attention (MHSA)-based encoder, that capture closer relationships between the Query and Key values [1], [8], [9], [14]. For this, the Q, K and V outputs are partitioned into smaller singular-heads (Qi, Ki, Vi), where i denotes a head of MHSA.\nThe attention shown in Equation 1 is computed using matrix multiplications between Qi, KT followed by Softmax operation and finally matrix multiplication with Vi. In TReX, to compute the softmax of each element xi in a matrix x, we subtract xmax (the maximum value in x) from xi as shown in Equation 2. This is numerically more stable compared to the standard softmax function as it eliminates any overflow/underflow issues during digital computation [11], [25].\nAttention(Qi, Ki, Vi) = Softmax( QiKT/\u221ad )Vi (1)\nSoftmax(xi) = exi\u2212xmax\u03a3j exj\u2212xmax (2)\nNext, the attention outputs are concatenated resulting in a t \u00d7 d output attention matrix. Following this, the projection and MLP layers project the information into a higher dimension feature space. Each encoder outputs a t \u00d7 d vector that is forwarded to the subsequent encoder. For convinience, in the rest of the paper, we will use QKT to represent QiKT , S(QKT) to represent S(QiKT) and S(QKT)V to represent the Attention(Qi, Ki, Vi) in Equation 1."}, {"title": "IV. IN-MEMORY COMPUTING ARCHITECTURES", "content": "Analog crossbars [6], [7], [26] consist of 2D arrays of In-Memory-Computing (IMC) devices and several peripheral circuits such as row-decoders, multiplexers (for resource sharing), Analog-to-Digital Converters (ADCs), shift-and-add circuits along with write circuits responsible for programming the IMC devices. The neural network's activations are bit- serialized and represented as analog voltages Vi and are fed into each source line Si of the crossbar, while the IMC devices are programmed to specific conductance values (Gij) to represent the network weights, as shown in Fig. 3. During inference in an ideal crossbar, the voltages interact with the device conductances, resulting in a current governed by Ohm's Law. These currents are generated along the bit-lines BLi. As per Kirchoff's current law, the net output current sensed at each column j is the summation of currents through each device, expressed as Ij(ideal) = \u03a3iGij \u2217 Vi. Here, Gij is the matrix containing the ideal conductance values of the IMC devices and X is the crossbar size. The analog currents are converted to digital values using the ADC and accumulated using the shift-and-add circuits.\nIn practice, due to the analog nature of computation, various hardware noise and non-idealities arise, such as interconnect parasitic resistances, synaptic device-level variations during read and write operations [27]\u2013[29] among others. As a result, the non-ideal_current Ij(non-ideal) is represented as Ij(non-ideal) = \u03a3iG \u2217 Vi. The Ij(non-ideal) deviates from its ideal value owing to the non-ideal conductance matrix Gij. This leads to significant accuracy losses for neural networks mapped onto crossbars, especially for larger crossbars with more non-idealities [27], [29], [30]."}, {"title": "V. TREX OPTIMIZATION FRAMEWORK", "content": "A. Attention Reuse Technique\nFig. 4 shows an example of attention reuse in a BaseViT with 4 encoders (i.e., NEncoders = 4). Here, encoders 2 and 4 reuse the attention (concatenation output) of encoders 1 and 3, respectively. For attention reuse, transformation blocks TB1 and TB2 are introduced. Transformation blocks contain a layer-normalization layer, a d \u00d7 d fully-connected layer and a GeLU layer. TB1 and TB2 transform the t \u00d7 d attention outputs of encoders 1 and 3 to a t \u00d7 d vector which is fed to the projection blocks of encoders 2 and 4, respectively. Here, t and d denote the number of tokens and the embedding dimension, respectively. Note, the dimensions of fully connected layer inside TB is chosen such that it introduces sufficient data variability. Data variability ensures that the TReX-optimized ViT achieves iso-accuracy with the BaseViT model. Apart from this, no other architectural modification is performed on the BaseViT.\nB. Attention Reuse-based Optimization\nThe goal of the attention reuse-based optimization is to choose optimal number of encoders (Optimal Nreuse) and their locations in the BaseViT model such that high accuracy is achieved while meeting the target delay. This optimization is performed in 4 steps:\nStep 1: In this step, we choose the Optimal Nreuse value that meets the given target delay criterion (Algorithm 1). Before beginning the iteration, first the user-defined ViT and IMC parameters P shown in Table I are initialized. Then, the variable r is iteratively incremented. At each iteration, delay value is computed by the TReX-Sim platform based on the Nreuse and P values. Note, only the Nreuse value changes with each iteration while the P remain constant. P includes NEncoders, d, t values among others to compute the delay, Dvit, using Equation in row 9 of Table II. The minimum value of r that meets the target delay criteria is chosen and set as the Optimal Nreuse value. Note, as the attention block has the major contribution towards the overall delay (Fig. 1b), we choose to optimize the BaseViT for a given delay constraint.\nStep 2: While Optimal Nreuse affects the energy-efficiency, the reuse pattern (i.e., the location of encoders reusing attention) determines the accuracy of the TReX-optimized ViT as"}, {"title": "VI. TREX-SIM PLATFORM", "content": "A. Platform Architecture\nTReX-Sim is a hardware-realistic inference benchmarking platform for IMC-implemented ViTs. For evaluation, TReX- Sim is initialized with user-defined ViT and crossbar parame- ters shown in Table I. Following this, as shown in Fig. 8a, the encoder layers are mapped on a tiled-array architecture. We adopt the standard tiled architecture in prior works [7], [33]. Following prior works, we assume that multiple tiles can map one layer but not vice-versa [7], [33]. Additionally, to reduce"}, {"title": "VII. RESULTS", "content": "A. Experimental Setup\nTReX-Optimization Platform: The baseline and TReX- optimized DeiT-S and LV-ViT-S models are trained using Pytorch 1.6 with Nvidia-V100 GPU backend. We utilize DeiT- S and LV-ViT-S as backbone networks in our approach due to their compact nature and widespread usage in previous studies. During Step 3 of the attention reuse-based optimization, the ViTs are trained on 20% of the Imagenet-1k dataset. This only adds ~5% to the overall training overhead. In Step 4 of TReX optimization, after the optimal pattern is applied, the TB- BaseViT model is finetuned for 30 epochs on the Imagenet-1k dataset [38] with a learning rate of 0.01. The hyperparameters"}, {"title": "VIII. CONCLUSION", "content": "This work proposes TReX that exploits the unique co- dependence of the ViT attention blocks towards energy-delay- area-accuracy optimization. TReX effectively performs atten- tion sharing in select encoders to achieve high EDAP reduction at minimal loss in non-ideal accuracy compared to BaseViTs without attention reuse. TReX-optimized ViTs also achieve higher TOPS/W and TOPS/mm\u00b2 compared to the BaseViTs. The efficacy of TReX is demonstrated on both vision and NLP tasks. In comparison to prior works, TReX-optimized ViTs achieve high EDAP reductions while maintaining high non-ideal accuracy on crossbars. Finally, this work shows the efficacy of FeFET-SRAM hybrid IMC architectures to improve the robustness of IMC-implemented ViT against IMC non- idealities while achieving high EDAP reductions."}]}