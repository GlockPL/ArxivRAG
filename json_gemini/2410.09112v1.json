{"title": "HLM-Cite: Hybrid Language Model Workflow for Text-based Scientific Citation Prediction", "authors": ["Qianyue Hao", "Jingyang Fan", "Fengli Xu", "Jian Yuan", "Yong Li"], "abstract": "Citation networks are critical infrastructures of modern science, serving as intricate webs of past literature and enabling researchers to navigate the knowledge production system. To mine information hiding in the link space of such networks, predicting which previous papers (candidates) will a new paper (query) cite is a critical problem that has long been studied. However, an important gap remains unaddressed: the roles of a paper's citations vary significantly, ranging from foundational knowledge basis to superficial contexts. Distinguishing these roles requires a deeper understanding of the logical relationships among papers, beyond simple edges in citation networks. The emergence of large language models (LLMs) with textual reasoning capabilities offers new possibilities for discerning these relationships, but there are two major challenges. First, in practice, a new paper may select its citations from gigantic existing papers, where the combined texts far exceed the context length of LLMs. Second, logical relationships between papers are often implicit, and directly prompting an LLM to predict citations may lead to results based primarily on surface-level textual similarities, rather than the deeper logical reasoning required. In this paper, we introduce the novel concept of core citation, which identifies the critical references that go beyond superficial mentions. Thereby, we elevate the citation prediction task from a simple binary classification to a more nuanced problem: distinguishing core citations from both superficial citations and non-citations. To address this, we propose HLM-Cite, a Hybrid Language Model workflow for citation prediction, which combines embedding and generative LMs. We design a curriculum finetune procedure to adapt a pretrained text embedding model to coarsely retrieve high-likelihood core citations from vast candidate sets and then design an LLM agentic workflow to rank the retrieved papers through one-shot reasoning, revealing the implicit relationships among papers. With the two-stage pipeline, we can scale the candidate sets to 100K papers, vastly exceeding the size handled by existing methods. We evaluate HLM-Cite on a dataset across 19 scientific fields, demonstrating a 17.6% performance improvement comparing SOTA methods. Our code is open-source at https://github.com/tsinghua-fib-lab/H-LM for reproducibility.", "sections": [{"title": "Introduction", "content": "With the rapid development of modern science, the volume of research papers is increasing annually [1]. As links between papers, citations network connects vast literature and bridge newly emerging knowledge with existing ones. Due to the critical role of citations, citation prediction is an important problem that has long been studied [2, 3, 4, 5, 6, 7], where the goal is to predict which papers from a set of previous papers (candidate set) will an emerging new paper (query) cite. Accurate citation prediction can help reveal information hiding in link space of citation networks [2, 8], owning value in aiding citation-based computational social science studies regarding the patterns of paper publication and scientific innovation [9, 10, 11, 12, 13, 14]. On the other hand, citation prediction is of practical significance for assisting researchers in writing manuscripts, providing high-likelihood citation suggestions, and thereby saving massive literature searching time.\nDespite abundant studies on citation prediction, there is a critical problem that remains unconsidered. While one paper typically cites multiple previous papers, the roles of citations vary significantly. The most important citations serve as research foundations of the query paper, assisting researchers in tracing the lineage of knowledge production. In contrast, some less relevant citations are only mentioned superficially in context. Existing works treat citation prediction as a simple binary classification problem and neglect such varying roles [2, 3, 4, 5, 6, 7], letting superficial citations distract attention from the important ones. However, such nuanced roles cannot be adequately reflected by simple edges in citation networks, but require understandings on the logical relationships among papers. In this paper, we aim to predict citations with various roles based on in-depth content understanding, where the emerging textual reasoning ability of LLMs provides a possible approach.\nPredicting citations with LLMs faces two major challenges. (1) Vast candidate sets. The real-world scientific database consists of gigantic papers, and researchers need to retrieve possible citations from millions of previous papers. With limited context length, it is impractical to feed the vast candidates' contents into LLMs and expect reasoning on logical relationships among them. (2) Implicit logical relationships. The logical relationships among papers lie implicitly within the content of papers. Directly prompting an LLM to predict key-role citations for query papers is likely to get sunk into simple content similarity rather than reasoning actual logical relationships among papers.\nIn this work, we define the novel concept of core citation with inspiration from rich science of science research [9, 10, 12], depicting the varying roles of citations. We analyze 12M papers across 19 scientific fields and illustrate core citations' significantly closer relationships with the query papers. Based on this definition, we develop the task of citation prediction from simple binary classification between citations and non-citations into a more challenging but meaningful version, i.e., distinguishing core citations from superficial citations and non-citations. To solve this task on vast candidate sets, we propose integrating embedding and generative LMs as HLM-Cite, a two-stage hybrid language model workflow. We design a curriculum fine-tuning procedure to adapt a pretrained text embedding model to analyzing research papers, initially retrieving high-likelihood core citations from vast candidate sets in the first stage. Subsequently, we design an LLM agentic workflow, consisting of a Guider, an Analyzer, and a Decider, for the second stage. Guided by a one-shot example, the LLM agents analyze the papers' implicit logical relationships through textual reasoning and rank the retrieved papers by citation likelihood. In HLM-Cite, we incorporate the capability of both embedding and generative LMs, enabling precise extraction of core citations from tremendous candidate sets. We conduct extensive experiments on cross-field papers, and the results show a 17.6% performance improvement of our method compared to SOTA baselines. Also, experimental results prove that our workflow can scale up to 100K candidates, thousands of times more than existing works, owning the potential to cover an entire research domain for practical implementation.\nIn summary, the main contributions of this work include:\n\u2022 We define the novel concept of core citation to depict the varying roles of citations. Thereby, we develop the citation prediction task from simple binary classification into distinguishing core citations, superficial ones, and non-citations, giving it more practical significance.\n\u2022 We design a hybrid language models workflow to integrate the capabilities of embedding and generative LMs, where two categories of models form a two-stage pipeline that cascades retrieval and ranking to predict core citations. This design enables our method to handle very large candidate sets with high precision.\n\u2022 We conduct extensive experiments on a cross-field dataset with up to 100K paper candidate sets. The results prove the scalability of our design and illustrate a 17.6% performance improvement comparing SOTA methods."}, {"title": "2 Problem Formulation", "content": ""}, {"title": "2.1 Definition of Core Citation", "content": "We first provided some notations about paper citation relationships. Considering a set of papers G, query paper $q \\in G$ cites a small subset of G including $n_q$ previous papers, denoted as {$s_1^q$, ...,$s_{n_q}^q$ } \u2261 $S_q \\subset G \\\\ {q}$, while the rest papers are not cited by q, which we denote them as {$p_1^q$, ...,$p_{n_q}^q$ } \u2261 $P_q$ = $G \\\\ {q}S_q$. Also, $m_q$ subsequent papers cite q, denoted as {$f_1^q$, ...$f_{m_q}^q$ } \u2261 $F_q \\subset G \\\\ (S_q\\cup \\\\ {q})$. As we mentioned above, the roles of each element in Sq may vary significantly, where there exist $k_q$ elements in Sq have major importance. We name them as core citations, denoted as {$s_1^{qC}$, ...$s_{k_q}^{qC}$ } \u2261 $S_q^C \\subset S_q$. Naturally, we name the rest of the citations, i.e., $S_q \\\\ S_q^C$, as superficial citations. Enlighten by previous computational social science studies regarding citation networks [10, 12], following-up papers of q, i.e., Fq, are likely to also cite the critical foundations of q, namely q's core citations. On the other hand, less relevant citations of q, such as some background knowledge, are typically not followed by Fq. Therefore, we mathematically identify the core citations according to such local citation relationships (Figure 1a):\n$S_q^C = \\\\ {s_q \\\\in S_q |\\exists s_p \\\\in F_q, let q \\\\in S_p, S_q \\\\in S_p\\\\}.\n(1)$\nTo verify the rationality of this definition, we draw statistics on 12M papers across 19 scientific fields in the Microsoft Academic Graph (MAG) [15] (See dataset details in Section 4.1). From the results in Figure 1b and c, we find that, with statistical significance, in both natural and social science domains, the query paper has more overlapped keywords with its core citations than its superficial citations, and the core citations are also more frequently mentioned in the main texts of query papers. This illustrates that the core citations identified from citation networks, are consistent with the important citations in the papers' content, proving feasibility of predicting core citations purely from the texts."}, {"title": "2.2 Core Citation Prediction Task", "content": "Considering the difference between core citations and superficial citations, we focus on predicting the core citations, which are most meaningful links among literature for scientific research. We formally define the task of core citation prediction as follows.\nDefinition 1 (Core Citation Prediction) Given a query paper q, and a candidate set $C_q$, where $|C_q| = t_q$. $C_q$ includes $t_a$ core citations and $t_s$ superficial citations of q, ensuing $t_a \\leq k_q$, $t_s \\leq n_q - k_q$ and $t_a+t_s < t_q$, and its rest elements, if any, are non-citations. The goal of core citation prediction is to pick out $t_a$ elements from $C_q$, maximizing the number of picked core citations.\nIn such a setting, superficial citations actually become hard negative samples against the core citations, adding to the challenges of the task.\nIn this paper, we focus on text-based citation prediction, where we only use citation networks to obtain the ground truth of core citations and do not include any network features other than the papers' textual content in the prediction. In this way, our model learns to extract the logical relationships"}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Overview", "content": "To effectively predict core citations from large-scale candidate sets, we integrate the capability of both embedding and generative LMs, forming a hybrid language models workflow (HLM-Cite). We illustrate designs of the workflow in Figure 2.\nAs shown in Figure 2a, the HLM-Cite workflow consists of two major modules, i.e., the retrieval module (Section 3.2) and the LLM agentic ranking module (Section 3.3). When given a query q and a candidate set $C_q$ with the size of $t_q$, we first call the retrieval module, a pretrained text embedding model finetuned with training data. We calculate the embedding vectors of q and each paper in $C_q$, denoted as $v_q$ and $V_q = \\{v_1, ..., v_{t_q}\\}$, where we concatenate the title and abstract as inputs. Based on the inner products between $v_q$ and each vector in $V_q$, we retrieve $r_q$ papers with the highest probability of being core citations of q from $C_q$, forming the retrieval set $R_q$. Subsequently, we employ LLM agents in the ranking module to collaboratively analyze the retrieved papers in $R_q$ and rank them according to their likelihood of being core citations, improving accuracy. Finally, we take the top $t_a$ papers as the prediction result."}, {"title": "3.2 Retrieval Module", "content": ""}, {"title": "3.2.1 Model Structure", "content": "Here, we introduce the structure of the text embedding model used in the retrieval module. We employ the GTE-base pretrain model [16], one of the top models on the Massive Text Embedding Benchmark (MTEB) leaderboard [17]. Its 110M parameters are initialized from BERT [18] and trained with multi-stage contrastive learning tasks, embedding input text into a 768-dimensional dense vector. We freeze the lower 7 layers of the GTE-base model and only finetune parameters in the higher 5 layers, as shown in Figure 2b. As empirically proven in previous research [19], such design can reduce computational consumption while maintaining the transferability in finetuning."}, {"title": "3.2.2 Curriculum Finetuning", "content": "As mentioned above, superficial citations act as hard negatives, adding to the difficulty of distinguishing core citations. Therefore, instead of directly transferring the GTE-base model to pick core citations from superficial citations and non-citations, we designed a two-stage curriculum finetuning as Figure 2b to gradually adapt the general-corpus model to our specific task, from easy to hard.\nIn the first stage, we finetune the model via a classification task that only distinguishes the core citation from non-citations, excluding the interference of superficial citations, i.e., the hard negatives. We construct each training data with one query, one of its core citations, and numerous non-citations, and we use cross-entropy loss for classification error in this stage.\nIn the second stage, we fully consider the ranking task of distinguishing core citations, superficial citations, and non-citations. We include one query together with its multiple core citations, superficial citations, and non-citations in each training data, and we apply NeuralNDCG loss function, a differentiable approximation of NDCG [20], to measure the difference between the model output and the ground-truth ranking. In both stages, we use in-batch negative sampling [21] to obtain non-citations for each query to reduce the embedding cost."}, {"title": "3.3 LLM Agentic Ranking Module", "content": ""}, {"title": "3.3.1 Overall Procedure", "content": "To improve the accuracy of core citation prediction, we incorporate LLMs' textual reasoning capability to rectify the ranking of papers retrieved in the previous stage by core-citation likelihood. As we illustrate in Figure 2c, the LLM agentic ranking module consists of three agents, the analyzer, the decider, and the guider, which are all driven by LLMs and collaborate via natural language communications. Given a query paper and its possible core citations retrieved from the candidate set, we first employ the analyzer to analyze the logical relationship between each individual paper in the retrieval set and the query paper. Then, we feed the analysis to the decider to obtain a revised ranking of their likelihood of becoming core citations, drawing final prediction results. In addition, we design a guider to enhance complex reasoning, where it produces a one-shot example under human supervision, assisting the analyzer and the decider via the chain of thought (CoT) method [22].\nAlso, we find that one useful technique in the LLM agentic ranking module is not to rank all retrieved candidates. Specifically, with the retrieval size of $r_q$ and $t_a$ core citations in the candidate set, we exempt the $(2t_a - r_q)$ retrieved candidates with largest inner products from reranking, and then we rerank the remaining 2$(r_q - t_a)$ retrieved candidates with the LLM agents and selected the top $(r_q - t_a)$ ones, resulting in the selected $r_q$ candidates in total. For example, when retrieval size is 7, we keep top-3 candidate unchanged and only rank the latter 4 candidates; when retrieval size is 8, we keep top-2 candidate unchanged and only rank the latter 6 candidates; and so on. The intuition for this is that the top candidates retrieved by the text embedding model tend to be core citations more safely. Therefore, only adjusting the latter ones is a rational solution that reduces the text length inputted into the LLMs and thereby improves the accuracy. We provide the detailed prompts used for the agents in Appendix A.1."}, {"title": "3.3.2 Design of LLM Agents", "content": "Analyzer: from textual similarity to logical relationship. Intuitively, predicting citations requires in-depth understandings of the logical relationships among the papers, rather than only focusing on the textual similarity between their titles and abstracts. Therefore, we design the analyzer to extract why the query paper cites each of the candidates. Since plentiful knowledge has been encoded in the LLM as an implicit knowledge base, the agent can perform such analysis without domain-specific finetuning [23, 24, 25].\nDecider: final ranking for core citation prediction. Based on the obtained analysis of paper relationships, we employ the decider to generate the final ranking of core-citation likelihoods. Besides simple ranking results, we prompt the agent to output corresponding explanations alongside, improving the rationality of its results [26, 27].\nGuider: one-shot learning. To provide one-shot example for the analyzer and decider, we first select one representative query paper and several candidates outside the test set. As shown in Figure 2c, the"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset", "content": "We conduct experiments based on Microsoft Academic Graph (MAG) [15], which archives hundreds of millions of research papers across 19 major scientific domains, forming a huge citation network. We traverse the dataset and filter 12M papers with abundant core citations and superficial citations, from which we randomly sample 450,000 queries and subsequently sample 5 core citations and 5 superficial citations for each query. We randomly divide the sampled queries into 8:2 as training and testing sets. Categorizing the scientific domains into natural science (biology, chemistry, computer science, engineering, environmental science, geography, geology, materials science, mathematics, medicine, physics) and social science (art, business, economics, history, philosophy, political science, psychology, sociology), we show statistics of the dataset in Table 1. Please note that a natural science query paper may cite some papers from the social science domain and vice versa."}, {"title": "4.2 Baselines", "content": "We mainly evaluate our methods against three categories of baselines: simple rule-based method, LMs specifically designed for scientific texts, and pretrained LMs for general-purpose tasks. In the first category, we mainly predict core citation based on the degree of keyword overlap, i.e., the more overlap the candidate paper's keywords have with the query paper, the more likely it is to be a core citation. The second category includes SciBERT [31], METAG [6], PATTON, SciPATTON [5], SPECTER [3, 32], SciNCL [4], and SciMult [33]. SciBERT is pretrained on millions of research papers from Semantic Scholar with the same approaches as BERT; METAG learns to generate multiple embeddings for various kinds of patterns of citation network relationships; PATTON and SciPATTON are finetuned with network masked language modeling and masked node prediction tasks on citation networks from BERT and SciBERT respectively; SPECTER is continuously pretrained from SciBERT with a contrastive objective; SciNCL is an improvement of SPECTER by considering hard-to-learn negatives and positives in contrastive learning; and SciMult is multi-task contrastive learning framework, which focuses on finetuning models with common knowledge sharing across different scientific literature understanding tasks. The third category includes BERT [18], GTE [16, 34], OpenAI-embedding-ada-002, and OpenAI-embedding-3 2. BERT is pretrained with masked language modeling and next sentence prediction objectives on Wikipedia and BookCorpus; GTE is a series of top embedding models finetuned from BERT with multi-stage contrastive learning task; and the latter two are advanced universal embedding models proposed by OpenAI. We access these models from off-the-shelf pretrained parameters or API calls and include different scale versions of each model when available."}, {"title": "4.3 Overall Performance", "content": "We conduct the curriculum finetuning of our retrieval module with the batch size of 512 and 96 respectively in two stages, and each train for 10 epochs. The training process takes approximately 12 hours on 8\u00d7NVIDIA A100 80G GPUs in total. Then, we call OpenAI API to access GPT models for LLM agentic ranking, where we keep using GPT-4 as the guider but alternate two versions of GPTs for the analyzer and the decider. For more implementation details, please refer to Appendix A.2.\nIn evaluation, we set vast candidate sets with $t_q$ = 10K ($t_a$ = $t_s$ = 5) for all models and set the retrieval size to be $r_q$ = 8 in our workflow. We evaluate the performance via PREC@3/5 and NDCG@3/5, and show the results in Table 2. The results illustrate that our method significantly surpasses all the baselines across all scientific domains with all metrics, with an overall PREC@5 improvement up to 17.6%. We verify the statistical significance of the performance improvement in Appendix A.5.1. Mentioning that without loss of statistical significance, we only randomly test 10% of the testing set with GPT-40 due to API rate limits."}, {"title": "4.4 Ablation Studies", "content": "In order to verify the validity of our designs, we conduct ablation studies regarding both curriculum finetuning of the retrieval module and LLM agents design in the ranking module. We show the results in Table 3. In the former part, we respectively delete the first and second stages of the curriculum and calculate the metrics on the retrieval set. The performance drop in both ablations indicates that our"}, {"title": "4.5 Analysis", "content": "In this section, we provide in-depth analysis of various key elements in the HLM-Cite workflow, enabling a better understanding of our design. Here, if there is no special explanation, we all employ GPT-3.5 as our analyzer and decider. Mentioning that due to API rate limits, we only test 10% of the testing set in this section."}, {"title": "4.5.1 Effect of Candidate Size", "content": "To illustrate the advantage of our method on large-scale candidate sets, which are normal in real-world applications, we keep $t_a$ = $t_s$ = 5 consistent and change the number of non-citations to construct candidate sets with $t_q$ = 1K, 10K, and 100K. As shown in Figure 4a, regardless of the candidate size, our method significantly surpasses all top baselines and even achieves higher relative performance improvement on larger candidate sets (up to 18.5% in $t_q$ = 100K). We provide results with other metrics in Appendix A.3, where the conclusion is consistent."}, {"title": "4.5.2 Effect of Retrieval Size", "content": "In our hybrid workflow, retrieval size $r_q$ is a key hyper-parameter that balances the work between the retrieval module and the LLM agentic ranking module. To explore the effect of $r_q$, we alter it from 6 to 10 and show the performance together with LLM token consumption per query in Figure 4b. The results indicate that when $r_q$ increases, the performance increases at the cost of more token consumption. Larger $r_q$ leads to a higher recall rate of core citations in the retrieval set, and thereby, LLM agents have the potential to pick out more core citations from the texts with increased length. However, when $r_q$ is large enough, continuing to increase it leads to a performance drop while consuming even more tokens. We believe this is because too many retrieved candidates surpass the"}, {"title": "4.5.3 Effect of One-shot Example", "content": "As studied in previous research [40], CoT enhances the performance of LLMs by demonstrating the logical structure of reasoning rather than providing specific knowledge content. Here, we investigate whether this is true in our hybrid workflow. We extend one-shot learning into a few-shot version. In this version, we produce an individual example for each scientific domain, where full texts are available in our GitHub repository. This provides more domain knowledge while maintaining an identical logical structure. The results in Table 4 show no significant performance difference between one-shot and few-shot learning, proving that what matters in CoT prompting is the logical structure of reasoning rather than specific domain knowledge."}, {"title": "4.5.4 Effect of LLM Types", "content": "We explore the effect of substituting GPT-3.5 in our workflow with other open-source and lightweight LLMs. Here, we keep using GPT-4 as the guider to provide a high-quality one-shot example and change the analyzer and decider to various open-source LLMs 3. We explore using two versions of Llama3, one of the most famous open-source LLMs; two versions of Mixtral, a mixture of experts (MoE) model; and ChatGLM2-6B, a Chinese-English bilingual model. We show the results in Table 5 and find that although larger LLMs perform slightly better, i.e., Llama3-70B wins Llama3-8B, and Mixtral-8\u00d722B wins Mixtral-8\u00d77B, these lightweight LLMs all perform significantly worse than GPT models. This highlights the importance of implicit knowledge in LLM's large-scale parameters, which is crucial for solving tasks like citation prediction that require strong professional knowledge."}, {"title": "5 Related Works", "content": ""}, {"title": "5.1 Pretrained Language Models (PLMs)", "content": "Pretrained language models have long been studied and reached great success. Various small-scale embedding models have been trained via different objectives, such as masked token prediction [18, 41], contrastive learning [3, 42, 4, 16], and permutation language modeling [43]. These models require fewer computational resources and are especially suitable for a wide range of tasks on the large-scale corpus, including classification, clustering, retrieval [17], etc. On the other hand, generative large"}, {"title": "5.2 LLM Agents", "content": "Utilizing the strong reasoning capability and human-like behavior of LLMs, researchers have explored various applications based on agents driven by LLMs. First, LLM agents for decision-making reach success in sandbox games [51, 52], robot controlling [53], and navigation [54]. Besides, a group of LLM agents can simulate daily social life [55], generate physical mobility behavior [56], and reveal macroeconomic mechanisms [57], providing insights for social science research. Closer to our task, role-fused LLM agents can collaboratively solve natural language processing tasks via analysis and discussions [45, 27, 58]. However, due to the limited context length in LLM reasoning, existing studies face difficulty handling tasks with extremely long texts, such as citation precision on vast candidate sets. In this paper, we incorporate generative LLMs with embedding models, enabling our hybrid workflow to work on very large candidate sets."}, {"title": "6 Conclusions", "content": "In this paper, we investigate the task of scientific citation prediction. We first define the novel concept of core citation and thereby evolve the conventional citation prediction task into a more meaningful version of distinguishing the core citations. Then, we propose a hybrid language model workflow that incorporates the capability of both embedding and generative LMs. Through extensive experiments and in-depth analysis, we verify the validity of our design and illustrate its superior performance in tasks with gigantic candidate sets. One major limitation of our method lies in LLMs' illusion problem. Despite average performance improvement, LLMs may output unfaithful analysis under certain circumstances and poison specific samples. Therefore, how to verify the output of LLM agents and improve the reliability of our hybrid workflow worth future studies."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Prompts for the LLM agents", "content": "The specific prompt for the analyzer is as follows:\nSystem prompt: Now you are a sophisticated researcher and information analyst, and going to investigate the problem of a specific paper citation. Your analysis should be based on the following steps: Explore citation conventions and standards in academic fields. For example, citation serve to acknowledge prior work, provide evidence or support, facilitate further exploration and allow readers to trace the development and history of ideas or methodologies.\nPrompt: Here is the title and abstract of the query paper. Title: {QueryPaperTitle} Abstract: {QueryPaperAbstract}. Now you are doing a research following up this paper above. Here are some other research papers which have been already cited by the query paper. Paper 1 Title: {CandidatePaper1Title} Abstract: {CandidatePaper1Abstract}, Paper 2 Title: {CandidatePaper1Title} Abstract: {CandidatePaper1Abstract}, Try to think abductively and convince yourself as a researcher. Figure out why the query paper cite these one by one. Try to think step by step before giving the answer.\nThe specific prompt for the decider is as follows:\nSystem prompt: Your role is to assist in predicting which research papers are most likely to be cited together based on a given set of papers or topics. Strive for fairness and objectivity.\nPrompt: Here is the title and abstract of the query paper. Title: {QueryPaperTitle} Abstract: {QueryPaperAbstract}. There are some other candidate papers and the analysis of why this query paper cites these. Paper 1 Title: {CandidatePaper1Title} Analysis: {CandidatePaper1Analysis}, Paper 2 Title: {CandidatePaper2Title} Analysis: {CandidatePaper2Analysis}, Now you are doing a research following up this query paper. Use the analysis to identify patterns or themes that suggest potential citation relationships. Rank these candidate papers in the order you are most likely to cite from the perspective of a research follower and provide explanations or justifications for your reasoning."}, {"title": "A.2 Implementation Details", "content": "In this section, we provide all implementation details for reproducibility in Table 6."}, {"title": "A.3 Supplementary Figures in Effect of Candidate Size Analysis", "content": "We provide performance comparisons among our method and several top baselines with different candidate sizes tq in Figure 5. In both natural and social science domains, measured by both PREC@5 and NDCG@5, our method significantly surpasses all baselines regardless of the candidate size. The conclusion is consistent with the main text."}, {"title": "A.4 Supplementary Figures in Effect of Retrieval Size Analysis", "content": "We provide the performance of our method together with LLM token consumption per query with different retrieval sizes rq in Figure 6. In both the natural and social science domains, when rq increases, the performance increases at the cost of more token consumption. However, when rq is large enough, continuing to increase it leads to a performance drop, while consuming even more tokens. Measured by both PREC@5andNDCG@5, the optimal value of rq is supposed to be 8 and 7 for natural and social science, respectively. The conclusion is consistent with the main text."}, {"title": "A.5 Statistical Significance", "content": ""}, {"title": "A.5.1 Overall Performance", "content": "We conduct statistical significance tests to better compare our model with the strongest baseline. In Table 7, we used the two-tailed t-test between the performance of our model and the strongest baseline. Our method surpasses the top baselines in all fields (p < 0.01 in most individual fields, and p < 0.001 averaging all fields through t-test), demonstrating its general applicability to a wide range of fields."}, {"title": "A.5.2 Ablation Studies", "content": "We conduct statistical significance tests to better compare our model with the ablation versions. In Table 8, we used the two-tailed t-test between the performance of our full design and the ablation versions. Generally, all parts of our designs are valid with significance (p < 0.01 or p < 0.1 in overall performance through t-test). Moreover, we notice that when focusing on social science papers, which only comprise a small proportion of all papers, Stage 1 of curriculum finetuning is only slightly beneficial. Therefore, when only applying to social science papers, it is an alternative for users to skip Stage 1 if they want to save computational cost with the cost of a slight performance drop. In contrast, when applying to natural science papers, it is necessary to keep Stage 1 for better performance."}, {"title": "A.6 One-shot Example", "content": "Full texts of the one-shot example generate by the guider are as follows:\nQuery paper:\nTitle: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\nAbstract: Transformers have a potential of learning longer-term dependency", "papers": "n1. Title: Attention is All you Need\nAbstract: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture", "Title": "Self-attention with relative position representations\nAbstract: Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to"}]}