{"title": "A Survey of Automatic Prompt Engineering: An Optimization Perspective", "authors": ["Wenwu Li", "Xiangfeng Wang", "Wenhao Li", "Bo Jin"], "abstract": "The rise of foundation models has shifted focus from resource-intensive fine-tuning to prompt\nengineering, a paradigm that steers model behavior through input design rather than weight\nupdates. While manual prompt engineering faces limitations in scalability, adaptability, and\ncross-modal alignment, automated methods, spanning foundation model (FM) based optimization,\nevolutionary methods, gradient-based optimization, and reinforcement learning, offer promising\nsolutions. Existing surveys, however, remain fragmented across modalities and methodologies.\nThis paper presents the first comprehensive survey on automated prompt engineering through a\nunified optimization-theoretic lens. We formalize prompt optimization as a maximization problem\nover discrete, continuous, and hybrid prompt spaces, systematically organizing methods by their\noptimization variables (instructions, soft prompts, exemplars), task-specific objectives, and\ncomputational frameworks. By bridging theoretical formulation with practical implementations\nacross text, vision, and multimodal domains, this survey establishes a foundational framework\nfor both researchers and practitioners, while highlighting underexplored frontiers in constrained\noptimization and agent-oriented prompt design.", "sections": [{"title": "Introduction", "content": "The transformative impact of pre-trained foundation models (FMs, e.g., large language models,\nLLMs or vision language models, VLMs) has revolutionized natural language processing and\nvisual understanding, enabling unprecedented capabilities in complex cognitive tasks ranging\nfrom mathematical reasoning to multi-agent collaboration systems [XCG+25]. As model scales\nescalate into the trillions of parameters, conventional fine-tuning approaches face prohibitive\ncomputational barriers. This resource intensiveness fundamentally restricts FM deployment in\nreal-world applications, particularly for edge devices and time-sensitive scenarios like autonomous\nvehicle decision-making or real-time medical diagnosis."}, {"title": "Related Work", "content": "Prompt engineering has rapidly gained prominence as an essential method for enhancing the\ncapabilities of FMs across diverse tasks. Several surveys have contributed to this domain by\nexamining specific facets of prompt-based learning. For instance, [LYF+23] position prompt\nengineering within the broader context of natural language processing (NLP), emphasizing how\ntextual prompts differ from traditional supervised training. Similarly, [VD24] and [SSS+24] each\nprovide comprehensive overviews of prompt methods (ranging from meticulously crafted natural\nlanguage instructions to learned vectors), detailing their performance across various NLP benchmarks.\nMeanwhile, [Ama24] explore more advanced prompt-based mechanisms such as CoT and Reflection\nand examine toolkits like LangChain and Semantic Kernel.\nIn addition to these foundational surveys, domain- and technique-specific works have emerged.\n[LLSC24] focus on prompt compression methods, exploring both hard and soft approaches and\nillustrating the ways in which compression can streamline model performance without sacrific-\ning accuracy. [GHC+23] expand these ideas to vision-language models, distinguishing between\nmultimodal-to-text generation, image-text matching, and text-to-image generation. They investigate\nhow prompting strategies differ in multimodal settings compared to purely textual ones. Thus, these\nsurveys collectively delineate a growing research landscape that spans multiple model types, task\nformats, and optimization considerations.\nYet, while these works are individually valuable, they remain fragmented by methodological or\nmodal boundaries. [CXW+24] specifically address efficiency aspects, but no comprehensive resource\nunifies the theoretical underpinnings across discrete, continuous, and hybrid prompt spaces. Existing\nsurveys either concentrate on foundational theories [LYF+23, GHC+23], explore a narrowly defined\ntechnique such as compression [LLSC24], or focus on manual design patterns [SSS+24, VD24].\nThis compartmentalization leaves open questions about how to systematically organize prompt\ncomponents, objectives, and optimization strategies under a cohesive theoretical framework.\nOur survey bridges this gap by introducing a unified, optimization-theoretic perspective on\nautomated prompt engineering across modalities. Unlike prior works that center on specific tasks,\nprompt types, or efficiency improvements, we formulate prompt engineering as an overarching\noptimization problem that seeks to maximize task-specific performance metrics for discrete (hard\nand exampler), continuous (soft), and mixed prompts. Our taxonomy spans variables, objective\nfunctions, and optimization methods, clarifying best practices and underscoring promising directions\nfor future research. In doing so, we provide researchers and practitioners with a rigorous foundation"}, {"title": "Optimization Problem Formulation", "content": "This paper studies the prompt optimization problem for foundation models, including both LLMs\nand VLMs. Let $X$ denote the input space and $y$ denote the output space. For LLMs, $X$ represents\ntext inputs, while for VLMs, $X = X_i \\times X_t$ represents image-text pairs, where $X_i$ denotes the visual\nspace and $X_t$ denotes the text space. A prompt function $P : X \\rightarrow P$ maps input queries to a\nconditioning pattern that elicits specific model behaviors. The prompt space $P$ can be partitioned\ninto three subspaces: the discrete prompt space $P_d$, the continuous prompt space $P_c$, and the hybrid\nprompt space $P_h = P_d \\times P_c$.\nFor $P \\in P_d$, we consider different canonical forms based on model type. In LLMs, the zero-shot\nform is expressed as $P(x) = [I,T; x]$, where $I,T \\in V^*$ denotes a learnable instruction and thought\nsequence from vocabulary space $V$ respectively, and $V^*$ represents the set of all possible sequences\nover $V$. The few-shot form $P(x) = [I, T, e_1, . . ., e_k; x]$, where ${e_i \\in E}_{i=1}^k$ are $k$ learnable exemplars\nfrom space $E$. Each exemplar $e_i = (x, y)$ consists of an I/O pair where $x \\in X$ and $y \\in Y$.\nIn VLMs, besides inheriting all prompt forms from LLMs, the spatial annotation form takes\na general expression $P(x) = [I,T, R_1, ..., R_m;x]$, where ${R_i \\in R}_{i=1}^m$ are spatial regions from a\ngeneral region space $R$. Each region $R_i = (A_i, l_i)$ consists of an area specification $A_i \\in A$ and a\nlabel $l_i \\in L$, where $A$ is a general area specification space that can represent various forms including\nbut not limited to: 1) bounding boxes: $A_i \\in [0,1]^4$ representing normalized coordinates; 2) markers:\n$A_i \\in [0, 1]^3$ representing center coordinates and radius; 3) pixel masks: $A_i \\in {0,1}^{H\\times W}$ representing\nbinary masks; and 4) other region specifications (e.g., polygons, curves).\nFor $P \\in P_c$, the prompt takes a unified form:\n$P(e_x) = [\\theta_1,..., \\theta_m; e_x]$,\nwhere $e_x = Embed(x) \\in R^d$ is the embedding representation of input $x$ through an embedding\nfunction $Embed : X \\rightarrow R^d$, and ${\\theta_i \\in R^d}_{i=1}^m$ are $m$ learnable vectors in the $d$-dimensional embedding\nspace. For $P \\in P_h$, the prompt combines both discrete and continuous elements:\n$P(x,e_x) = [I,T, R_1, . . ., R_k, \\theta_1, ..., \\theta_m; x]$,\nallowing joint optimization of discrete regions and continuous embeddings.\nGiven a black-box foundation model $f : P \\times X \\rightarrow Y$ and a validation set $D_{val} = {(x_i, y_i)}_{i=1}^{n_{val}}$ of\nsize $n_{val}$, the prompt optimization problem can be formulated as:\n$P^* = \\underset{P \\in P}{arg \\underset{}{max}} E_{(x,y)\\sim D_{val}} [g(f(P(x)), y)]$\nsubject to $P\\in P_d$ or $P \\in P_c$ or $P\\in P_h$,\nwhere $g: Y \\times Y \\rightarrow R$ denotes a performance metric measuring the quality of model predictions\nagainst ground truth. This formulation leads to 3 subclasses of problems:\n$\\underset{P \\in P}{max} E_{(x,y)} [g(f(P(\\triangle))), y)]$,\n$\\begin{cases}* = d, \\triangle = x, & \\text{(DPO)}\\\\* = c, \\triangle = e_x, & \\text{(CPO)}\\\\* = h, \\triangle = (x,e_x), & \\text{(HPO)}\\end{cases}$"}, {"title": "Optimization Spaces", "content": "Building upon formulation in Section 3, we systematically analyze the 3 fundamental variable types\nin prompt optimization: discrete ($P_d$), continuous ($P_c$), and hybrid combinations ($P_h$) (Figure 2).\nThis taxonomy enables principled analysis of automated prompt engineering across modalities."}, {"title": "Discrete Variables", "content": "Discrete prompt optimization operates in $P_d$ space, manipulating human-interpretable elements\nthrough combinatorial search. We identify three principal variable subtypes:\nInstructions Instruction variables ($I \\in V^*$) specify task objectives through natural language\ndirectives (e.g., \u201cTranslate to French\u201d). Their optimization centers on generating concise yet\nprecise directives to elicit strong model performance. Representative approaches generate [HSYD23,\nYWL+24, WLW+24], mutate [YWL+24][GWG+23] or refine [SRLI+20, PI+23, GWG+23, KHZ+24,\nKKK+24, DW+22, HSY+24] instructions via gradient-inspired edits, GA, or RL to systemati-\ncally improve task accuracy. This classification can be further extended to multimodal prompts,\nwith representative approaches including generate [KSL+23], mutate [HCDW24, OS24] or re-\nfine [MA+24, YPO+24, MLY+24, WHS+24,RLH23, MZB+24], which play a crucial role in\noptimizing accuracy and ensuring consistency in generated image quality."}, {"title": "Continuous Variables", "content": "In contrast to discrete tokens, continuous (soft) prompts ($P_c$) rely on learnable embeddings $\\theta \\in R^d$.\nThese vectors can be appended to input representations and optimized via gradient-based methods.\nBy avoiding changes to the underlying model parameters, soft prompts require fewer resources for\nadaptation. Notable research explores prefix-based [LL21, LARC21, PWWF25, WC23] or layer-\nspanning embeddings [LZ+24], revealing that tuning a small set of trainable vectors can achieve\nstrong performance gains while preserving model generality. Soft prompts thus offer an efficient\npath to personalize or specialize foundation models without extensive fine-tuning or large-scale data."}, {"title": "Mixed Variables", "content": "Mixed (hybrid) settings in $P_h$ space incorporate both discrete and continuous elements, combining\nhuman-readable instructions or exemplars with trainable embedding vectors. This fusion leverages the\ninterpretability and domain specificity of discrete tokens and the flexibility of continuous embeddings.\nEarly works on co-optimizing instructions and exemplars [FBM+23, WAC+25, AD+24, CZL+24]\ndemonstrate that jointly refining these complementary parts often achieves more robust and adaptable\nbehavior than optimizing each component alone. Such synergy can be extended to multimodal\nprompts[WJK+24], for instance, by coupling discrete spatial regions (in VLMs) with continuous\nalignment vectors, thereby enabling comprehensive, end-to-end optimization."}, {"title": "Objective Functions", "content": "As formulated in Section 3, our goal is to solve\n$\\underset{P \\in P}{max} E_{(x,y)\\sim D_{val}} [g(f(P(x)), y)]$,\nwhere $f: P \\times X \\rightarrow Y$ denotes the black-box foundation model (either an LLM or VLM), $P$ is\na prompt from the prompt space $P$ (discrete, continuous, or hybrid), and $g : Y \\times y \\rightarrow R$ is a\nperformance metric. We now illustrate how $g(\\cdot)$ is instantiated across downstream tasks (Section 5.1)\nand then discuss constrained objectives (Section 5.2), all within the framework of Equation (3)."}, {"title": "Downstream Tasks", "content": "Instruction Induction. This task evaluates how well $f (P(x))$ extracts and generalizes underlying\ninstructions. Following [HSBL22], it spans 24 sub-tasks such as morphosyntactic transformations\nand causality detection. Typical $g(\\cdot)$ measures include BERTScore-F1 and exact/set match.\nText Classification. Models map text $x \\in X$ to a discrete label $y \\in Y$. Datasets like SST-\n2 [SPW+13], SST-5 [SPW+13] and AGNEWS [ZZL15] cover sentiment, topic classification, and\nsubjectivity detection. Common metrics $g(f(P(x)), y)$ include classification accuracy and Macro-F1.\nMath Reasoning. Here $f (P(x))$ must solve arithmetic or algebraic word problems. Datasets\nsuch as GSM8K [CKB+21], MultiArith [KKRA+16] and SingleEq [KKRA+16] vary from single-step\narithmetic to multi-step reasoning. Evaluation relies on comparing numeric outputs or perplexity."}, {"title": "Constrained Objectives", "content": "Beyond purely maximizing task-specific metrics, several prompt optimization scenarios require\nadditional constraints:\n$\\bullet$ Prompt Editing. Here we impose structural or semantic constraints on $P(x) \\in P$. Minor\nreformulations of instructions, exemplars, or spatial annotations must preserve or improve\n$E[g(\\cdot)]$ under restricted editing budgets. This suits tasks where $f(P(x))$ is sensitive to subtle\nprompt shifts [SSS+24, Ama24].\n$\\bullet$ Prompt Compression. Constrains $||P||_{length} \\leq k$ (token-length or embedding-size budget),\nseeking $\\underset{p\\in P}{max} E[g(\\cdot)]$ subject to $\\kappa$. Trimming extraneous tokens or embedding vectors\nincreases computational efficiency while preserving accuracy, crucial in real-time or large-scale\nscenarios [CXW+24].\nThese can be viewed as special cases of Eq. (3) with additional constraints $\\Gamma(P) < \\kappa$, leading to\nsolutions that favor prompt conciseness or regulated edits. By integrating such constraints into the\noptimization procedure, automated prompt design expands beyond raw performance maximization\nto meet usability, efficiency, and alignment requirements."}, {"title": "Optimization Methods", "content": "To establish a cohesive organization, we propose a unifying taxonomy and classify optimization\nmethods across four major paradigms, as shown in Figure 4: (1) FM-based Optimization, (2)\nEvolutionary Computing, (3) Gradient-Based Optimization, and (4) Reinforcement Learning.\nEach paradigm can further be subdivided based on whether it optimizes purely discrete prompts\n($P \\in P_d$), purely continuous prompts ($P \\in P_c$), or hybrids ($P \\in P_h$). Many works mix or extend\nthese categories (e.g., EvoPrompt combines FM-based generation with Genetic Algorithm (GA)\noperators). By situating each study within this two-dimensional taxonomy, we highlight shared\nstructural principles across seemingly diverse algorithms and providing a cohesive view of the rapidly\nexpanding literature, as shown in Table 2."}, {"title": "FM-based Optimization", "content": "FM-based optimization methods directly leverage FM as meta-optimizers to refine prompts. These\napproaches often implement iterative improvement in which an FM proposes an updated prompt\nbased on performance feedback.\nHeuristic Meta-Prompt Several methods harnesses human-designed meta-prompts, i.e., manually-\ncraft sequences that instruct an FM how to revise an existing prompt. PE2 [YAPK23] uses rich\nmeta-descriptions, context specifications, and CoT templates to iteratively update prompts for\nvarious tasks. OPRO [YWL+24] unifies solution exploration and evaluation, integrating previously\ngenerated solutions (and their quality metrics) within a meta-prompt that the FM uses to refine\nfuture versions of $P$. LCP [LAX+24] integrates contrastive learning signals into meta-prompts,\nencouraging FMs to distinguish high-quality $P_d \\in P_d$ from suboptimal ones, and adapt across\nmodel families/languages. Similarly, StraGo [WGZ+24] merges success and failure exemplars as\nmeta-context to steer in-context learning toward more robust prompts.\nAutomatic Meta-Prompt Generation Another subset generate meta-prompt relying on\nexternal feedback and self-reflection. ProTeGi [PI+23] formulates an iterative \"gradient-like\" textual\nediting loop, incorporating beam search and multi-armed bandit strategies to refine discrete prompts\n$P\\in P_d$. AutoHint [SLX+23] appends FM-inferred hints derived from prior prediction errors, thereby\nevolving initial prompts in a step-by-step manner. CriSPO [HLX+24] introduces critique-suggestion\npairs that guide FM feedback to improve text generation prompts without modifying model weights;\nit further proposes Automatic Suffix Tuning (AST) for multi-objective prompt engineering. Likewise,\nBPO [CL+23] aligns $f(P(x))$ with user intent by collecting human feedback on interim outputs,\nthen editing $P$ accordingly.\nStrategic Search and Replanning A few works incorporate explicit search strategies. APE [ZMH+22]\nconducts black-box prompt exploration through an FM-proposed candidate pool, selecting prompts\nthat maximize task performance without requiring gradients. PromptAgent [WLW+24] leverages\nMonte Carlo Tree Search (MCTS) to navigate a combinatorial space of expert-level prompts, apply-\ning user feedback as value signals. AMPO [YWG+24] evolves multi-branched prompts, guided by\nfailures and partial successes; each branch refines $P(x)$ to better handle increasingly complex task\nvariations. adv-ICL [LZBo24] employs a generator-discriminator FM setup to explore adversarial\nprompts, leading to robust in-context demonstrations. OPT2I [MA+24] focuses on text-to-image\nconsistency by rewriting textual prompts $P_d$ to boost alignment."}, {"title": "Evolutionary Computing", "content": "Evolutionary methods model prompt optimization as a genetic or evolutionary process. They\ntreat the prompt $P \\in P_d$ as an \"organism,\" mutated or crossed over to produce \u201coffspring\u201d that\nsurvive based on higher fitness (i.e., the performance measure $g(f(P(x)), y)$). These approaches are\nparticularly suitable for purely discrete prompt spaces.\nGenetic Operators and Heuristics GPS [XCD022] applies a straightforward genetic algorithm\nto refine few-shot instruction prompts, iteratively mutating tokens and retaining top-performing\ndiscrete prompts. LongPO [HSYD23] extends these ideas to long prompts by incorporating beam"}, {"title": "Gradient-Based Optimization", "content": "Gradient-based strategies derive from classical optimization principles, but face unique obstacles in\nprompt engineering since discrete tokens $[I,T, {e_i}] \\in P_d$ are not directly differentiable. Methods\nin this family either approximate gradients to navigate discrete spaces or, more commonly, optimize\ncontinuous parameters $\\theta \\in R^d$ in a soft prompt context $P\\in P_c$.\nDiscrete Token Gradient Methods For closed-source FMs, direct gradient access is often\nunavailable, requiring alternative solutions: HPME [WJK+24] projects a learned continuous embed-\nding back to discrete tokens each iteration, blending soft gradient updates with nearest-neighbor\ntoken matching. AutoPrompt [SRLI+20] constructs prompts by adding tokens that maximize the\ngradient toward correct labels in a masked language modeling scenario. ZOPO [HSY+24] implements\nzeroth-order optimization in discrete prompt spaces by sampling localized perturbations in the\ntoken domain, guided by a neural tangent kernel approximation.\nSoft Prompt Tuning Soft prompt methods treat $P \\in P_c$ as a set of trainable vectors {$\\theta_1, ..., \\theta_m$}\nthat concatenate with the embedding of $x$, i.e., $P(e_x) = [\\theta_1, ..., \\theta_m; e_x]$. Prefix-tuning [LL21] attaches\nlearnable prefix vectors in the hidden states of a language model, requiring only a small fraction of\ntrainable parameters. Prompt-Tuning [LARC21] similarly adds trainable embeddings at the input\nlayer, benefiting from large model scalability. P-Tuning [LZ+24] extends trainable prompts into\nmultiple layers, significantly improving few-shot performance. All these methods solve variants of\nthe continuous optimization subproblem $\\underset{P \\in P}{max}E_{(x,y)\\sim D_{val}} [g(f(P(e_x)), y)]$ and hence leverage\nstandard gradient descent w.r.t. $\\theta \\in R^d$."}, {"title": "Reinforcement Learning", "content": "RL methods recast prompt design as an RL problem in which $P \\in P$ (often $P_d$ or $P_h$) is updated via\na sequence of actions under a reward defined by $g(f(P(x)), y)$. Across these RL methods, the key\nidea is to formulate optimization objective $\\underset{P \\in P}{max}E_{(x,y)\\sim D_{val}}[g(f(P(x)), y)]$ as a Markov Decision\nProcess, where partial or recurrent prompt edits constitute actions and $g$ serves either directly as\nthe reward function or as part of a learned proxy. Such frameworks unify discrete prompt editing\n($P\\in P_d$) and continuous prompt adaptation ($P \\in P_c$), even allowing for hybrid forms $P \\in P_h$ where\nsome components are differentiable (soft prompts) and others remain discrete (hard instructions).\nPrompt Editing as RL Actions RLPrompt [DW+22] represents discrete tokens $v \\in V$ as RL\nactions, exploring the space of textual prompts with policy gradient methods. TEMPERA [ZWZ+22]\nproposes test-time RL-based editing, adjusting each query's prompt adaptively. PRewrite [KHZ+24]\ntrains a separate prompt rewritter using RL signals to maximize downstream task accuracy. In a\nsimilar vein, PACE [DLo23] refines suboptimal human prompts ($P_{human}$) by iterative RL feedback,\nwhile StablePrompt [KKK+24] adapts proximal policy optimization to mitigate training instability.\nEvoke [HTZ+23] establishes a reviewer-author loop, feeding prompt outputs into a critic that\nsuggests incremental edits.\nMulti-Objective and Inverse RL Strategies Other RL approaches tackle multi-objective\nor partial feedback scenarios. Prompt-OIRL [SHvdS23] employs offline inverse RL to learn a\nquery-specific reward model, thus selecting an optimal prompt without frequent FM interactions."}, {"title": "Future Directions", "content": "The systematic study of prompt optimization for foundation models, from an applied optimization\nperspective, presents extensive opportunities but remains loosely explored. Below, we outline key\nresearch themes, highlighting current progress, pivotal questions, and open challenges.\nConstraint optimization Presents methods seldom incorporate semantic or ethical constraints\nin discrete prompt spaces. The main issue lies in constructing a search mechanism that respects\nhuman-value alignment, resource bounds, and readability, particularly in high-dimensional symbolic\ndomains. Another challenge is formalizing these constraints as tractable mathematical conditions\nthat guide search algorithms without sacrificing flexibility or linguistic quality.\nMulti-task prompt optimization It's crucial for leveraging shared structures across tasks.\nWhile some work suggests factorized or sparse representations to capture prompt-level similarity,\nformal definitions of inter-task \"prompt similarity\" are lacking. Negative transfer can also arise,\nwhere improvements for one task degrade performance on another. The field needs robust frameworks\nto codify these trade-offs, enhancing generalization and adaptivity.\nOnline prompt optimization Current techniques generally prioritize offline scenarios. However,\nuser intentions can shift over time, creating the need for algorithms that maintain stable performance\n(e.g., bounded dynamic regret) in nonstationary environments. Reliance on online updates amplifies\nthe complexity of discrete search in high-dimensional prompt spaces. Furthermore, real-time user\nfeedback loops introduce additional uncertainties, demanding advanced convergence analyses.\nMulti-objective prompt optimization Multi-objective prompt optimization aims to balance\noften competing goals such as accuracy and interpretability. Many existing studies use single-metric\noptimization, overlooking fundamental human-centered preferences. One promising direction is the\nincorporation of Pareto-based methods or multi-criteria decision-making, accompanied by geometric\nrepresentations of preference spaces. Game-theoretic techniques may also help arbitrating among\nconflicting objectives, both within and across user populations.\nHeterogeneous modality optimization Although many works focus on textual cues, prompts\nin computer vision and other modalities, such as bounding boxes or pixel-level annotations, remain\nfar less explored. This calls for a deeper understanding of cross-modal coupling, as well as conditions\nunder which modalities are amenable to joint or separate optimization. Such advances might require\nnovel manifold-based or graph-based tools to unify distinct prompt representations."}, {"title": "Discussion and Conclusion", "content": "This survey delineates a optimization-theoretic foundation for automated prompt engineering that\ntranscends fragmented treatments across modalities. By synthesizing methods that target discrete,\ncontinuous, and hybrid prompt spaces, we have underscored how variables such as instructions, soft\nprompts, and exemplars can be systematically optimized under unified theoretical principles.\nTogether with our taxonomy of task objectives and unified perspective on FM as optimizer,\nevolutionary computing, gradient-based, and RL-driven methods, we establish key foundations for\ntheoretical inquiry and realistic application. Moving forward, tighter integration of multi-level,\nmulti-objective, and online optimization will be pivotal in shaping prompt designs for emerging\nfoundation models."}]}