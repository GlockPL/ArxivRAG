{"title": "Learning Characteristics of Reverse Quaternion Neural Network *", "authors": ["Shogo Yamauchi", "Tohru Nitta", "Takaaki Ohnishi"], "abstract": "The purpose of this paper is to propose a new multi-layer feedforward quaternion neural network model architecture, Reverse Quaternion Neural Network which utilizes the non-commutative nature of quaternion products, and to clarify its learning characteristics. While quaternion neural networks have been used in various fields, there has been no research report on the characteristics of multi-layer feedforward quaternion neural networks where weights are applied in the reverse direction. This paper investigates the learning characteristics of the Reverse Quaternion Neural Network from two perspectives: the learning speed and the generalization on rotation. As a result, it is found that the Reverse Quaternion Neural Network has a learning speed comparable to existing models and can obtain a different rotation representation from the existing models.", "sections": [{"title": "1 Introduction", "content": "In recent years, machine learning technology has made tremendous progress with the rise of deep learning. In particular, it has achieved remarkable success in fields such as image recognition, natural language processing, and speech recognition, and is still being actively researched. A high-dimensional neural network [1-5] is a type of neural network that uses numbers of two or more dimensions, such as complex numbers and quaternions, to represent the parameters of the neural network. High-dimensional neural networks can deal with hypercomplex-valued signals naturally. It is well-known that complex-valued neural networks and quaternion neural networks require fewer parameters (weights, biases) and"}, {"title": "2 Reverse Quaternion Neural Network Architecture", "content": "In this section, we formulate the RQNN architecture, and derive the learning algorithm."}, {"title": "2.1 Quaternion and Its Properties", "content": "First, we describe the quaternion and its properties.\nThe quaternion $q\\in H$ is defined as\n\n$q = a + bi + cj + dk \\in H \\quad (a, b, c, d \\in R)$,\n\nwhere a is the real part, b, c, d are the imaginary parts, H is the set of quaternions,\nand R is the set of real numbers. i, j, k are independent imaginary units, and the\nfollowing arithmetic rules apply.\n\n$i^2 = j^2 = k^2 = ijk = -1$.\n\n$ij = -ji = k, \\quad ki = -ik = j, \\quad jk = -kj = i$.\n\nFor the two quaternions $q_1 = a_1+b_1i+c_1j+d_1k \\in H$ and $q_2 = a_2+b_2i+c_2j+d_2k \\in$\nH, their multiplication is calculated as follows:\n\n$q_1q_2 = (a_1a_2 - b_1b_2 - c_1c_2 - d_1d_2)$\n\n$+(a_1b_2+b_1a_2 + c_1d_2 - d_1c_2) i$\n\n$+(a_1c_2-b_1d_2+ c_1a_2 + d_1b_2) j$\n\n$+(a_1d_2+b_1c_2 - c_1b_2 + d_1a_2) k$.\n\nThe reverse multiplication $q_2q_1$ is calculated as\n\n$q_2q_1 = (a_2a_1 - b_2b_1 - c_2c_1 - d_2d_1)$\n\n$+(a_2b_1+b_2a_1 + c_2d_1 - d_2c_1) i$\n\n$+ (a_2c_1-b_2d_1 + c_2a_1 + d_2b_1) j$\n\n$+(a_2d_1+b_2c_1 - c_2b_1 + d_2a_1) k$.\n\nFrom Eqs. (4) and (5), it is evident that the multiplication of quaternions is\nnon-commutative. The conjugate quaternion of $q = a + bi + cj + dk$, denoted as\n$\\bar{q}$, is defined as\n\n$\\bar{q} = a - bi - cj - dk$."}, {"title": "2.2 A Reverse Quaternion Neuron", "content": "A neuron used in the RQNN is as follows. We call it a reverse quaternion neuron.\nThe input signals, weights, biases, and output signals are all quaternions. The\nnet input $U_n$ to neuron n is defined to be\n\n$U_n = \\sum_m W_{nm}S_m + T_n \\in H$,\n\nwhere $S_m\\in H$ is the quaternion input signal coming from the output of neuron\nm, $W_{nm}\\in H$ is the quaternion weight connecting neurons m and n, and $T_n \\in H$\nis the quaternion bias of neuron n. Note here that the order of multiplication of\na weight and an input signal is WEIGHT times INPUT SIGNAL (Eq. (7), Fig.\n1). In a usual quaternion neuron, the order of multiplication of a weight and an\ninput signal is INPUT SIGNAL times WEIGHT [5,7]. This is why it is called a\nreverse quaternion neuron. To obtain the quaternion output signal, convert the\nnet input $U_n$ into its four parts as follows.\n\n$U_n = u_1 + u_2i + u_3j + u_4k \\in H$.\n\nAlthough the activation function of a quaternion neuron is a quaternion function\ngenerally, we use the following activation function, known as split-type:\n\n$f(x) = f_R(x_1) + f_R(x_2)i + f_R(x_3)j+f_R(x_4)k$\n\nwhere $x = x_1+x_2i+x_3j+x_4k \\in H$ and $f_R(x) : R \\rightarrow R$ is a real-valued activation\nfunction applied to each component. Therefore, the output value of the neuron\nn is given by $f(U_n)$."}, {"title": "2.3 Reverse Quaternion Neural Network", "content": "The multi-layer feedforward reverse quaternion neural network consists of the\nreverse quaternion neurons described in the previous section.\nFor the sake of simplicity, consider a three-layer feedforward reverse quater-\nnion neural network. We use $w_{ml} = w_{ml}^r + w_{ml}^i i + w_{ml}^j j + w_{ml}^k k \\in H$ for the\nweight between the input neuron $I_l$ and the hidden neuron $m$, $U_{nm} = v_{nm}^r +$\n$v_{nm}^i i + v_{nm}^j j + v_{nm}^k k \\in H$ for the weight between the hidden neuron m and the\noutput neuron n, $\\theta_m = \\theta_m^r + \\theta_m^i i + \\theta_m^j j + \\theta_m^k k \\in H$ for the bias of the hidden\nneuron m, $Y_n = y_n^r + y_n^i i + y_n^j j + y_n^k k \\in H$ for the bias of the output neuron n.\nLet $I_l = I_l^r + I_l^i i + I_l^j j + I_l^k k \\in H$ denote the input signal to the input neuron l,\nand let $H_m = H_m^r + H_m^i i + H_m^j j + H_m^k k \\in H$ and $O_n = O_n^r + O_n^i i + O_n^j j + O_n^k k \\in H$\ndenote the output signals of the hidden neuron m, and the output neuron n,\nrespectively. Let $\\delta_n = \\delta_n^r + \\delta_n^i i + \\delta_n^j j + \\delta_n^k k = T_n - O_n \\in H$ denote the error\nbetween $O_n$ and the target output signal $T_n = T_n^r + T_n^i i + T_n^j j + T_n^k k \\in H$ of\nthe pattern to be learned for the output neuron n. We define the square error\nfor the pattern p as $E_p = (1/2) \\sum_{n=1}^N|\\delta_n|^2$, where N is the number of output\nneurons, and $|x| := \\sqrt{x_1^2 + x_2^2 + x_3^2 + x_4^2}$ for $x = x_1 + x_2i + x_3j + x_4k \\in H$.\nWe used here the same notations as those in [7] for inputs, weights, biases,\noutputs, errors and target output signals."}, {"title": "2.4 Learning Algorithm", "content": "Next, we derive a learning rule for the RQNN described in Section 2.3.\nFor a sufficiently small learning constant $\\epsilon > 0$, and using a steepest descent\nmethod, we can show that the weights and the biases should be modified accord-\ning to the following equations as in [7]. In this case, the amount of the correction\nfor the RQNN is calculated by obtaining the partial derivatives of the real and\nimaginary parts separately. This is a variant of the well-known backpropagation\nlearning algorithm using quaternions [16].\n\n$\\Delta v_{nm} := \\Delta v_{nm}^r + \\Delta v_{nm}^i i + \\Delta v_{nm}^j j + \\Delta v_{nm}^k k$\n\n$= -\\epsilon (\\frac{\\partial E_p}{\\partial v_{nm}^r} + i \\frac{\\partial E_p}{\\partial v_{nm}^i} + j \\frac{\\partial E_p}{\\partial v_{nm}^j} + k \\frac{\\partial E_p}{\\partial v_{nm}^k} ),$\n\n$\\Delta \\Upsilon_{n} := \\Delta \\gamma_{n}^r + \\Delta \\gamma_{n}^i i + \\Delta \\gamma_{n}^j j + \\Delta \\gamma_{n}^k k$\n\n$= -\\epsilon (\\frac{\\partial E_p}{\\partial \\upsilon_{nm}^r} + i \\frac{\\partial E_p}{\\partial \\upsilon_{nm}^i} + j \\frac{\\partial E_p}{\\partial \\upsilon_{nm}^j} + k \\frac{\\partial E_p}{\\partial \\upsilon_{nm}^k} ),$\n\n$\\Delta w_{ml} := \\Delta w_{ml}^r + \\Delta w_{ml}^i i + \\Delta w_{ml}^j j + \\Delta w_{ml}^k k$\n\n$= -\\epsilon (\\frac{\\partial E_p}{\\partial w_{ml}^r} + i \\frac{\\partial E_p}{\\partial w_{ml}^i} + j \\frac{\\partial E_p}{\\partial w_{ml}^j} + k \\frac{\\partial E_p}{\\partial w_{ml}^k} ),$\n\n$\\Delta \\theta_{m} := \\Delta \\theta_{m}^r + \\Delta \\theta_{m}^i i + \\Delta \\theta_{m}^j j + \\Delta \\theta_{m}^k k$\n\n$= -\\epsilon (\\frac{\\partial E_p}{\\partial \\theta_{m}^r} + i \\frac{\\partial E_p}{\\partial \\theta_{m}^i} + j \\frac{\\partial E_p}{\\partial \\theta_{m}^j} + k \\frac{\\partial E_p}{\\partial \\theta_{m}^k} ),$"}, {"title": "3 Experiments", "content": "In order to investigate the learning characteristics of the RQNN, we conducted\ntwo experiments focusing on learning speed and rotation representation, and\nevaluated their performance.\nWe call the multi-layer feedforward quaternion neural network where xw is\ncalculated for a weight w\u2208 H and an input signal x \u2208 H to a quaternion neuron\nQNN here for short. Also, we call the multi-layer feedforward real-valued neural\nnetwork Real-NN here for short."}, {"title": "3.1 Learning Speed", "content": "First, we compared the learning speeds of the RQNN, the QNN, and the Real-\nNN. We used the learning data shown in Table 1, which was used in [7]."}, {"title": "3.2 Representation of Rotation", "content": "Next, we evaluated the generalization on rotation of the RQNN via a computer\nsimulation: 1) rotation of lines, and 2) rotation of point cloud."}, {"title": "Rotation of lines", "content": "To investigate the generalization performance on rotation,\n200 points were generated at equal intervals along each of the i, j, k axes in 3D\nspace, ensuring that the angle between each data point was 90 degrees (Fig. 3).\nAn experiment was then conducted to examine the behavior of the output of the\nRQNN for the test data after training. For comparison, the same experiment was\nalso conducted with the traditional QNN. The input learning data were assigned\nto the points on the i-axis, the teacher data to the points on the j-axis, and the\ninput test data to the points on the k-axis."}, {"title": "Rotation of point cloud", "content": "Subsequently, an experiment on rotational represen-\ntation was conducted using the plane point cloud data shown in the left figure\nin Fig. 5. These training data consisted of a rectangle composed of 5,000 points\non the ij-plane, both vertically and horizontally ranging from 0 to 1, which was\nused as the input data. The plane rotated 90 degrees around the j-axis was used\nas the teacher data."}, {"title": "4 Discussion", "content": "The learning speed of the RQNN is several times faster than the Real-NN, but\nthere was no significant difference in learning speed compared to the traditional\nQNN. As seen in Table 3 and Fig. 2, despite having higher initial losses than\nthe Real-NN, both the QNN and the RQNN converged about three times faster\non average. However, between the RQNN and the QNN, the RQNN completed\nlearning in 3,306 epochs, while the QNN took about 3,111 epochs on average,\nshowing no significant difference. This is likely because both the RQNN and the\nQNN are neural network architectures based on quaternions.\nThe rotational generalization performance of the RQNN was observed in both\nthe rotation of lines and the rotation of objects generated from point clouds. It\nis clear from Fig. 4 that there are differences in the output results between the\nQNN and the RQNN for the input test patterns. Furthermore, as seen in Table\n4, the average angle of the outputs between the QNN and the RQNN was 102\ndegrees, suggesting that the RQNN learned a different rotation than the QNN.\nThe angles between the input test data and the outputs of the QNN and the\nRQNN are 94.4 and 78.8 degrees on average, respectively, which are similar\nto the value of angle (i.e., 90 degrees) given during the training. Additionally,\nexamining the rotation angles from the input test data to the QNN and the\nRQNN using the Euler angles reveals that the outputs of the two models are\ndifferent (Table 5). Significant differences were observed in rotations around\nthe j-axis, i-axis, and k-axis. Looking at the experimental result on the point\ncloud shown in Fig. 6, it is apparent that the attitudes of the tetrahedron and\nthe positions of the colored vertices in both the RQNN and the QNN exhibit\ndifferent behaviors. Furthermore, examining the rotation angles of the colored\nvertices from the test data using the Euler angles also reveals that the behavior"}, {"title": "5 Conclusion", "content": "In this paper, we proposed a new multi-layer feedforward quaternion neural\nnetwork model architecture, the Reverse Quaternion Neural Network, utilizing\nthe non-commutative nature of quaternion multiplication, and investigated its\nlearning characteristics. Comparing the results of the experiments on conver-\ngence speed and the rotational generalization performance with the traditional\nmodels, it was found that the learning speed of the Reverse Quaternion Neural\nNetwork is similar to that of the conventional quaternion neural network, but the\nReverse Quaternion Neural Network has a different generalization on rotation.\nAs a future challenge, it is important to further investigate the learning\nrepresentation characteristics of the RQNN. In this paper, relatively simple ex-\nperiments were conducted, and differences in behavior were identified, but it is\nnecessary to explore the potential of the RQNN through experiments using more\ncomplex learning datasets. Moreover, regarding the generalization performance\non rotation, it is also necessary to investigate from a theoretical perspective.\nBy mathematical analyses, it would be clarified in which areas the RQNN has\nadvantages compared to traditional models."}, {"title": "6 Appendix", "content": ""}]}