{"title": "SKQVC: One-Shot Voice Conversion by K-Means Quantization with Self-Supervised Speech Representations", "authors": ["Youngjun Sim", "Jinsung Yoon", "Young-Joo Suh"], "abstract": "One-shot voice conversion (VC) is a method that enables the transformation between any two speakers using only a single target speaker utterance. Existing methods often rely on complex architectures and pre-trained speaker verification (SV) models to improve the fidelity of converted speech. Recent works utilizing K-means quantization (KQ) with self-supervised learning (SSL) features have proven capable of capturing content information from speech. However, they often struggle to preserve speaking variation, such as prosodic detail and phonetic variation, particularly with smaller codebooks. In this work, we propose a simple yet effective one-shot VC model that utilizes the characteristics of SSL features and speech attributes. Our approach addresses the issue of losing speaking variation, enabling high-fidelity voice conversion trained with only reconstruction losses, without requiring external speaker embeddings. We demonstrate the performance of our model across 6 evaluation metrics, with results highlighting the benefits of the speaking variation compensation method.", "sections": [{"title": "I. INTRODUCTION", "content": "One-shot voice conversion (VC) aims to transform the speaker identity of a source into that of an arbitrary target using only a single utterance. This process typically employs disentanglement-based methods to separate content and speaker information, replacing the source speaker's information with that of the target speaker. The key challenge lies in effectively disentangling content and speaker information while preserving both. To address this, various strategies have been proposed, including information bottlenecks [1, 2], additional loss functions [3, 4], normalization techniques [5, 6], and vector quantization (VQ) methods [7\u20139]. VQ methods capture content information by replacing the input embedding with the nearest vectors from a discrete codebook, which primarily represents phonetic features within the continuous content space, thus removing speaker information. As a result, speaker identity is excluded, and a discrete content embedding is extracted. VQVC [7] disentangles speech attributes from mel-spectrograms using VQ with a randomly initialized codebook and codebook loss [10]. The quantized output is used as the content embedding, and the speaker embedding is extracted from the residual of VQ. However, using a randomly initialized codebook leads to loss of content information during the quantization process, negatively affecting intelligibility and quality. VQMIVC [9] addresses this issue by applying mutual information loss and vector quantization with contrastive predictive coding (VQCPC), enhancing the quality of the converted speech.\nRecently, many studies have increasingly employed complex and sophisticated methods to enhance VC performance [11, 12]. Additionally, external speaker embedding derived from pre-trained speaker verification (SV) models has been widely used to extract more accurate speaker information [1, 3]. YourTTS [11], based on a variational autoencoder (VAE) architecture, utilizes a conditional normalizing flow method and a monotonic alignment search (MAS) module [13], while using external speaker embedding as a condition for the posterior encoder and a duration predictor. FreeVC [12] captures content information using SSL features, combined with data perturbation, a bottleneck network, and a conditional normalizing flow method, while employing external speaker embedding to achieve high naturalness and similarity in voice conversion.\nSSL features, which are speech representations derived from self-supervised learning (SSL) models such as HuBERT [14] and WavLM [15], have demonstrated the ability to linearly predict various speech attributes [16]. These features are encoded such that instances of the same phone are closer together than different phones, meaning that nearby features share similar phonetic content [17, 18]. Due to this inherent characteristic, SSL features have been increasingly used in recent voice conversion models [12, 19-21]. The models [19-22] use K-means quantization (KQ) with SSL features, a VQ method where the codebook is initialized by applying K-means clustering to SSL features. This codebook effectively"}, {"title": "II. METHOD", "content": "The proposed model architecture is illustrated in Fig. 1(a). It includes a WavLM encoder, K-means quantization, a Disentangler, and a HiFi-GAN [24] decoder with a discriminator. Further details are provided in the following section.\nInstead of traditional representations such as spectrograms or mel-spectrograms, the proposed model utilizes SSL features obtained from the 6th layer of the WavLM model. WavLM is pre-trained on large datasets and designed for various speech processing tasks, demonstrating superior performance in both ASR and non-ASR tasks. Specifically, features from the 6th layer of WavLM have been shown to contain richer acoustic information, including pitch, prosody, and speaker identity, compared to later layers [17, 25]. The input waveform X = {x_1,x_2,..., x_L} \\in \\mathbb{R}^{1\\times L}, where L is the number of the waveform samples, is fed into the WavLM encoder, and continuous SSL features W = {w_1,w_2,...,w_T} \\in \\mathbb{R}^{1024\\times T}, where T is the number of frames, are extracted.\nDiscrete content embedding Q = {q_1,q_2, ..., q_T} \\in \\mathbb{R}^{1024 \\times T} is obtained from SSL features by quantization, as shown in Eq. 1. The codebook used for quantization is initialized through applying K-means clustering to the SSL features of the training dataset. Each code vector in the codebook corresponds to the centroid vector of a cluster, and the codebook is fixed during the model's training process [21]. Nearby SSL features share similar phonetic features [17, 18], meaning that variations in these feature values are primarily driven by changes in phonetic content rather than other speech attributes, such as pitch, stress, or speaker identity. Therefore, the codebook initialized through K-means clustering is strongly content-related, with information about other attributes being removed. Furthermore, as the codebook size decreases, it loses the ability to capture the continuous variations in phonetic features, which reduces content accuracy.\nKQ (w_t) = q_t; q_t = \\arg \\min_{q\\in Codebook} (||w_t - q||_2). (1)\nThe residual of quantization, which is the difference between W and Q, denoted as S = {s_1, s_2, ..., s_T} \\in \\mathbb{R}^{1024 \\times T}, contains a mix of speaking variation and speaker information. The Disentangler, shown in Fig. 1(b), separates these attributes through an information bottleneck that utilizes their unique characteristics. First, inspired by VQVC [7], the speaker embedding S_{avg} \\in \\mathbb{R}^{1024\\times 1}, which contains speaker information, is extracted averaging across the time dimension. Second, in the Speaking Variation Compensation layer (SVComp), the difference between S and S_{avg} is passed through a bottleneck layer, enforcing dimensionality reduction on the"}, {"title": "III. EXPERIMENTS", "content": "Experiments are conducted using the VCTK [26] and LibriTTS [27] datasets. For the VCTK dataset, the validation set consists of 214 utterances (2 per speaker), while the test set contains 1,070 utterances (10 per speaker) from 107 speakers, with the remaining utterances allocated to the training set. Additionally, the test-clean subset of LibriTTS is used as another test set. The VCTK dataset is recorded in a controlled studio for consistent quality, while the LibriTTS dataset, derived from audiobooks, has more variable recording conditions. All audio is downsampled to 16kHz. The FFT, window, and hop sizes are set to 1280, 1280, and 320, respectively. The codebook of KQ is initialized using the Minibatch K-means algorithm [28], with a batch size of 1024 and 256 clusters, implemented on the training set. The bottleneck layer is implemented using a 1D convolution layer with a kernel size of 1. The values of \\lambda_{fm} and \\lambda_{mel} in the training loss are set to 2 and 45, respectively. Three baseline models, VQMIVC [9], YourTTS [11], and FreeVC [12], are selected for comparison with the proposed method.\nBoth subjective and objective evaluations are conducted. For the subjective evaluation, 50 participants are asked to rate naturalness and similarity on a 5-point scale using the mean opinion score (MOS) and the similarity mean opinion score (SMOS), both of which are calculated with 95% confidence intervals. Experiments are performed in both seen-to-seen and unseen-to-unseen scenarios, with 16 seen speakers from the VCTK test set and 16 unseen speakers from the LibriTTS test set. For the objective evaluation, four metrics are used. The word error rate (WER) and character error rate (CER) between the source and converted speech are obtained using an ASR model 1, assessing the intelligibility of the converted speech. The equal error rate (EER) is calculated using a pre-trained SV model [29] to evaluate speaker similarity. The F0-PCC, the Pearson correlation coefficient [30], measures the correlation between the F0 of the source and converted speech, with a score range of -1 to 1, where higher values indicate better preservation of the prosody [12]. For evaluation, 2000 utterances are randomly selected from the VCTK test set and the LibriTTS test set, respectively, and 1000 seen-to-seen and unseen-to-unseen converted utterances are generated."}, {"title": "IV. CONCLUSION", "content": "In this work, we propose a simple yet effective approach to one-shot voice conversion. By applying the methods of K-means quantization and an information bottleneck, the model disentangles speech attributes from SSL features without additional constraint losses. This approach addresses the common issue of losing speaking variation, while also eliminating the need for external speaker embeddings. The results demonstrate that the proposed model outperforms existing methods in both objective and subjective evaluations, proving its robustness across diverse conditions and datasets. Furthermore, the experiments underscore the benefit of the proposed compensation method across varying codebook sizes, and ablation studies validate the importance of key components in the system's performance."}]}