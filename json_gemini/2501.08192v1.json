{"title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM Serving", "authors": ["Ahmet Caner Y\u00fcz\u00fcg\u00fcler", "Jiawei Zhuang", "Lukas Cavigelli"], "abstract": "Large language models (LLMs) are widely used across various applications, but their substantial computational requirements pose significant challenges, particularly in terms of HBM bandwidth bottlenecks and inter-device communication overhead. In this paper, we present PRESERVE, a novel prefetching framework designed to optimize LLM inference by overlapping memory reads for model weights and KV-cache with collective communication operations. Through extensive experiments conducted on commercial AI accelerators, we demonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs. Additionally, we perform a design space exploration that identifies the optimal hardware configuration for the proposed method, showing a further 1.25x improvement in performance per cost by selecting the optimal L2 cache size. Our results show that PRESERVE has the potential to mitigate the memory bottlenecks and communication overheads, offering a solution to improve the performance and scalability of the LLM inference systems.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLM) have been widely deployed across various services and application domains such as chat assistants [28], code generation [5], and knowledge retrieval [20]. While LLMs have demonstrated outstanding capabilities in these domains, their substantial computational requirements result in slow and costly inference. To address these challenges, specialized accelerators have been developed and deployed for LLM inference. Consequently, the performance, efficiency, and scalability of LLM inference accelerators are critical to the speed and cost of the LLM inference.\nOne of the most significant computational challenges in today's LLM inference systems is the off-chip memory bottleneck. While the prefill phase of LLM inference allows all input tokens to be efficiently processed at once, the output tokens in the decoding phase are typically generated one by one due to the autoregressive nature of LLMs. Therefore, the model weights and contextual data (also known as KV-cache) must be retrieved from off-chip memory during each iteration of the decoding phase. Unfortunately, even the most advanced memory technologies such as HBM fail to provide sufficient bandwidth to overcome these memory bottleneck issues. As a result, the performance of the decoding phase, and consequently the overall LLM inference, is often limited by the HBM bandwidth of the accelerators.\nAnother significant computational challenge in LLM inference is the large memory footprint of the model weights and KV-cache. The total memory size of model weights and KV-cache often exceeds the HBM capacity of a single accelerator. Therefore, LLM inference is typically performed on clusters of accelerators ranging from a few up to hundreds of devices. During such distributed inference scenarios, the accelerators must share intermediate results with each other between layers using collective communication operations such as allreduce. During these communication phases, the accelerators mostly remain idle, leading to reduced device utilization.\nIn this paper, we address both of these two major challenges at once. We propose prefetching the model weights and KV-cache from off-chip memory to on-chip cache during the communication operations. This approach allows the communication operations to be overlapped with the memory read operations for the model weights and KV-cache, hiding the latency of the latter. Once the communication operations are completed, compute cores access the required data from the on-chip cache, no longer limited by off-chip memory bandwidth. As a result, the overall decoding phase of LLM inference is executed faster and more efficiently, reducing end-to-end latency and execution cost.\nUnfortunately, current hardware schedulers and modern ML compilers cannot facilitate the proposed memory-communication overlap due to data dependencies between compute and communication operations in LLM inference. To address this issue, other prior studies have proposed fusing compute and communication operations [11,39]. However,"}, {"title": "2 Background", "content": "Since their inception, the Transformer-based LLM models have been widely in use in many application fields. However, their inference is typically overwhelmingly costly, which impedes their widespread adoption. In this section, we give a brief overview of these architectures and their computational characteristics to understand the limitations on the performance of their inference.\nLLM inference workloads Most of today's state-of-the-art generative AI LLM models (e.g., GPT-4 [28] and Llama-2 [35]) consist of a decoder-only Transformer architecture [38]. The decoder-only Transformer models comprise a number of decoder layers stacked on top of each other. Each decoder layer includes a self-attention and a multi-layer perceptron (MLP) layer, each of which are followed by normalization layers. The self-attention layers can further be decomposed into three input projection layers, a positional embedding, a self-attention kernel (e.g., Flash attention [7]), and an output projection layer. Similarly, the MLP layers can be decomposed into three projection layers and one activation layer. A large percentage of memory footprint and computational workload of Transformer architectures comes from the projection layers and self-attention kernels, which puts them at the center of focus for LLM acceleration.\nThe generation process with LLMs can be divided into two phases: prefill and decode. In the prefill phase, the LLMs process the input prompts and generate the first output token. In the decode phase, the LLMs keep generating one token at a time based on the past tokens due to their autoregressive nature, until a special end-of-sequence token is generated or the maximum number of token limit is reached. Because the length of input prompts can easily exceed hundreds or thousands of tokens in most generative AI applications, the matrix multiplication operations of linear layers and self-attention kernels in a typical prefill phase greatly benefit from data reuse, which increases their operational intensity (in terms of FLOPS/bytes) and maximizes the utilization of the compute resources."}, {"title": "LLM decoding is memory BW-limited", "content": "In contrast, the algebraic operations in self-attention layers during the decode phase are mostly matrix-matrix mutliplication with one very narrow matrix (often no wider than 8), thus providing an operational intensity of 16 Op/word while most accelerators' roofline is at >100 Op/word. Similarly, the data reuse in projection layers is limited to the number of batches in the decode phase, which also often result in low operational intensity. Therefore, in various application scenarios where the number of batches might be limited to a small number due to low rate of incoming queries or limited memory capacity (e.g., mobile platforms or latency-constraint online services), the decode phase suffers from memory-bandwidth bottlenecks. As a result, the autoregressive generative AI applications with LLMs is generally considered a memory-bandwidth bottlenecked process.\nTo eliminate redundant computation in autoregressive steps of the decode phase, the calculated key and value tensors are stored in memory to be reused in the later iterations. This mechanism is generally referred to as KV-cache. In generation tasks with long context lengths (e.g., book summarization or multi-modal models), the size of KV-cache might easily become comparable or exceed the size of model weights. Moreover, reading KV-caches from off-chip memory might take even longer than the model weights for long context lengths. Thus, improving the performance of handling and reading KV-caches is critical to the overall performance of LLMs. As a result, in this work, we do not only consider prefetching for model weights but also for KV-caches."}, {"title": "Multi-device inference", "content": "Many of today's state-of-the-art LLMs have model sizes that reach hundreds of billions of parameters. Even with reduced precision formats (e.g., bfloat16, float8, int8), the memory footprint of these models exceed the HBM memory capacity of individual modern AI accelerator devices. Moreover, the throughput of a single device is typically bounded by the HBM bandwidth, which is in the order of a few TB/s for high-end AI accelerators. With today's state-of-the-art models exceeding hundreds of GBs in model size and KV-cache, a single device is not sufficient to achieve per-token latency objectives that are required to ensure a responsive user experience, which is often set to 100 ms per token. Therefore, the LLM inference is typically distributed and executed in parallel across multiple tightly-interconnected devices.\nVarious parallelization strategies have been proposed and used in distributed LLM inference to minimize the overhead of communication between devices. One of the most commonly used parallelization strategy is called tensor-parallelism. In this parallelization technique, the weights and the KV-cache in LLMs are partitioned and distributed across the devices. During inference, each device performs computation locally with their partitions of weights and KV-cache, and sums up their results through the communication primitive allreduce.\nTo minimize the communication overhead, the self-attention layers are typically partitioned in the attention-head dimension, where the calculation of each attention head is independent of each other up to the output projection layer of the self-attention layers [31]. Similarly, the MLP layers are also partitioned in their intermediate dimension, which corresponds to the columns and rows of the input and output linear projections, respectively, so that much of the calculation can be performed locally. As a result of this partitioning scheme, accelerators need to perform only one allreduce call per self-attention and MLP layer.\nEven though only a single allreduce call per layer is needed for distributed inference, their execution may take considerable time depending on the data and cluster size. The data size that needs to be communicated across the accelerators grows linearly with the embedding size of the LLM architecture and the batch size of the input prompts. Moreover, the network traffic also grows linearly with the number of accelerators participating in the tensor parallel execution. While modern LLM inference servers are equipped with high-bandwidth interconnection fabric between the devices (e.g., NVLink and HCCS), scaling beyond a single server still requires performing allreduce calls over slower networks such as PCIe or InfiniBand. As a result, the execution time of allreduce calls may take a large portion of the overall time and become the limiting factor in the scalability of the distributed LLM inference systems.\nIn summary, there are two distinct computational characteristics of today's LLMs that limit the performance of their inference. First, the underlying operations exhibit low operation intensity, which makes the single-device performance of LLM inference limited by the memory bandwidth. Second, LLMs must be distributed across multiple devices due to their large memory footprint and latency constraints, which incurs significant communication overheads. These two computational challenges limit the performance and efficiency of the AI inference systems, increasing the cost per token and hindering their scalability and responsiveness, particularly in real-time applications."}, {"title": "3 Proposed Method", "content": "In the previous section, we argued that memory-bandwidth bottlenecks and communication overheads are the two most significant obstacles to the scalability and performance in distributed LLM inference systems. For this reason, in this section, we propose a method that prefetches the model weights and KV-cache from HBM memory to L2 cache and overlaps prefetching with communication in order to hide the latency of communication with memory operations."}, {"title": "Vanilla execution", "content": "Figure 1 illustrates the timeline of a decode stage of LLM inference with and without the proposed method. In vanilla execution (top-half of the figure), the query (denoted as Wq), key, and value linear projections of the attention layers have a data dependency with the preceding allreduce operation, as they need to wait for the reduction of the activations from the previous layer. Similarly, the gate (denoted as WG) and up (denoted as Wu) projections of the MLP layers have also a data dependency with the preceding allreduce operation. As a result, modern ML frameworks (e.g., Pytorch) execute these operations sequentially in a single stream. Unfortunately, the devices remain idle while waiting for allreduce operations, leading to underutilization and long latencies."}, {"title": "Execution with the proposed method", "content": "While the linear projections need to wait for allreduce operations due to the data dependencies on activations, their weights are read-only, so they do not have any dependencies with the preceding operations. Similarly, the K and V caches in the $Q^KT$ and SV calculations of the self-attention layers are also read-only in the decode stage, except the last entry that is updated based on the key and value that are calculated in the preceding linear projections. Therefore, the weights and the KV caches can be prefetched before performing the forward pass of these layers.\nThe bottom-half of the Fig.1 illustrates how the proposed method works. In a parallel stream to the main one (denoted as prefetch stream in Fig.1), devices start prefetching the weights and KV-cache while waiting for the allreduce operations. As a result, the forward pass of the linear projections and the self-attention take shorter, as the weights and KV-cache are hit and quickly read from the L2 cache of these devices. Consequently, the compute resources are better utilized and the execution time of the each decode layer is shortened."}, {"title": "L2 capacity requirements", "content": "The proposed method requires AI accelerators to store the prefetched weights and KV-cache of the subsequent layers in their on-chip memory, which is typically the L2 cache. If the L2 cache size of the accelerators is not sufficient to store the weights and KV-cache, the prefetched data would be evicted and the effectiveness of the proposed method would be reduced. Therefore, the AI accelerators should have enough L2 cache size to store all the weights and KV-cache of the layers between the allreduce operations.\nTo assess the feasibility of the proposed method on popular LLMs, we now analyze the memory requirements of the individual LLM layers and compare them against the L2 cache size of modern AI accelerators. Fig. 2 shows the calculated memory size of the Attention and MLP layers of various LLMs for varying number of devices. As we increase the number of devices, provided that the number of devices does not exceed the number of attention heads, the weights and KV-cache are partitioned into smaller chunks, which decreases the memory size per device.\nFig. 2 demonstrates that tensor parallelism up to 16 devices is sufficient to reduce the memory requirements of the Attention and MLP layers of even the largest models down to a level at which they can fit in the L2 cache of the existing commercial AI accelerators. Modern LLM inference systems often consist of many more devices. For instance, Deepspeed-inference scales up to 256 GPUs [3] and Cloud TPUv5e inference instances contain up to 256 chips [37]. Therefore, we conclude that the proposed method is applicable to most of the widely used, open-source LLMs given the scale of the current LLM inference systems and the L2 cache capacity of the commercial AI accelerators."}, {"title": "Framework integration", "content": "How the proposed method is exposed to the user is critical for its effectiveness and applicability. A trivial implementation of the proposed method would be to develop a prefetch operation as a custom operator and offload the task of inserting prefetch operators to the programmers. However, this approach would have three shortcomings. First, the programmers would need to explicitly insert the prefetch operators in parallel to the communication operations, which would increase the burden of the programmers. Second, the programmers would need to have a good understanding of the memory hierarchy of the underlying hardware platforms to realize the full potential of the proposed method. Third, the programs written in this way would be specific to certain hardware platforms and could not be easily migrated to different ones without performance degradation. Therefore, the proposed method does not expose prefetching to the programmers but automatically inserts the required operators in the computation graph of an ML application.\nMany of today's hardware vendors provide graph optimization frameworks (e.g., CUDA Graphs [25], CANN Graph Engine [15] etc.), which take a program written in a hardware-agnostic high-level programming language (e.g., Pytorch) and perform compiler-level optimizations, such as operator insertion, fusion, removal. Moreover, such graph optimization frameworks typically have the full view of the entire graph, which enables to optimize for best use of memory resources. To that end, we also developed the PRESERVE framework, which inserts prefetching operators for weights and KV-caches into a computational graph of LLM inference, while automatically managing stream synchronization and cache optimizations in order to minimize cache pollution and redundant memory bandwidth usage.\nPRESERVE Figure 3 illustrates the overview of the proposed PRESERVE framework. On the host side, the framework takes a user code that implements an LLM model inference written in one of the supported ML frameworks such as Pytorch and Tensorflow. Then, the user code is compiled into an intermediate representation (IR) using either the built-in compiler of the ML frameworks (e.g., TorchDynamo) or one of the hardware-specific compilers (e.g., Ascend ATC [14]). Next, IR is passed to a graph optimizer (e.g., Ascend Graph Engine (GE) [23]), which performs graph-level optimizations and inserts prefetch operators for the suitable compute and communication operations into the graph. Finally, the optimized graph is compiled into an offline model (executable binaries) using a vendor-specific operator library (e.g., Ascend CANN [15]).\nUpon receiving a request, PRESERVE loads the offline model, copies the input data and model weights to the device memory, and sends the compiled operators to the device using a task queue. When the runtime scheduler in the device receives the compiled operators, it dispatches the operators according to the execution order and stream that they are compiled for. The prefetch operators are executed in a stream parallel to the main stream and synchronized between layers using event instructions to facilitate the overlap between communication and prefetching."}, {"title": "Operator insertion", "content": "Algorithm 1 describes the proposed operator insertion algorithm for weight and KV-cache prefetching. The proposed algorithm takes a computation graph and L2 cache capacity of the target hardware platform as inputs. Then, the algorithm finds all allreduce operations in the computation graph and iterate over each of these allreduce operations. For each allreduce operation, the algorithm visits the children nodes in a breadth-first order until it reaches the end of graph or the next allreduce operation. For each children node of the type MatMul or SelfAttention, the algorithm estimates the required memory size for prefetching and calculates the total memory size of all preceding prefetch operators between the current node and the allreduce operation. If the total memory size is smaller than the L2 cache capacity, a prefetch operator for the weight or KV-cache of the current node is inserted in a parallel stream to the allreduce operation. If the total memory size exceeds the L2 cache capacity, the prefetch operator is not inserted to avoid cache eviction, and the algorithm continues with the next allreduce operation.\nIn conclusion, we propose a method that automatically inserts prefetch operators in parallel to the communication operations in the computation graph of an LLM inference application. In runtime, the prefetch operators enables AI accelerators to start reading the weights and KV-caches while waiting for the communication operations, which hides the latency of the latter and improves the performance of the LLM inference."}, {"title": "4 Experiments", "content": "To evaluate the performance improvements obtained by the proposed method, we performed a series of experiments. In this section, we first explain our experimental methodology, benchmark models, and system details. Then, we test the proposed method under various settings such as tensor-parallel cluster size, batch size, and sequence length. Finally, we discuss our findings and outline the impact of the proposed method on the performance of LLM inference."}, {"title": "Experimental setup", "content": "For the experiments in this section, we choose a number of open-source LLMs that are widely adopted by the community: Llama3-8b, Llama3-70b [35], Qwen2-7B, Qwen2-72B [40], Phi-3-small, and Phi-3-medium [1]. We varied the batch sizes from 1 to 64 and the sequence length from 2k to 32k to explore the impact of batch size and sequence length on the performance of the proposed method. For sake of simplicity, we assume static batching with equal sequence lengths. The prefill and decode lengths are taken as the 2/3 and 1/3 of the sequence length, respectively.\nAll benchmarks are implemented in Pytorch with the torch-npu backend [13], based on the reference implementation given in torchair library [16] and compiled using TorchDynamo. The parallel execution is achieved via the communication primitives provided by the torch.distributed library. The proposed method is implemented using the Graph Engine (GE) feature of the Compute Architecture for Neural Networks (CANN) [15], which enables to perform weight and KV-cache prefetching during inference.\nThe experiments are performed on a Huawei Atlas 800T A2 server with 4x Kunpeng 920 48-core CPUs and 8x Ascend 910B NPUs. Each NPU has an L2 cache capacity of 192 MB and is equipped with 24 DaVinci AI cores [34] and 64 GB HBM memory, which provides a total theoretical FLOPs of 800 TOPS/s in int8 precision and 1.6TB/s off-chip memory bandwidth [12, 21]. The Ascend 910B NPUs in the server are tightly interconnected to each other through an HCCS interconnect fabric in a full-mesh topology."}, {"title": "End-to-end execution times", "content": "Table 1 summarizes the end-to-end LLM inference execution times with the baseline implementation and the proposed method for various benchmarks and cluster sizes (number of NPUs). The results show that the proposed method speeds up the execution in all benchmarks and cluster sizes. We observe that the speedup generally increases with the number of NPUs. This trend is due to the fact that, as the number of NPUs increases, the computation time per device decreases while the communication time between the devices increases. As a result, hiding the latency of the communication operations has a greater impact on the execution time, which increases the speedup.\nWe observe that the maximum speedups occur when the number of NPUs is equal to four (except Qwen2-7B, where it occurs when the number of NPUs is equal to two). This is because the number of KV-heads per device is equal to one when these models are partitioned to eight devices. When the number of KV-heads per device is equal to one, memory read operations for the KV-cache from HBM are continuous rather than strided; thus, the HBM bandwidth is utilized better and memory operations become faster, leaving smaller room for improvement for the proposed prefetching method. Therefore, we observe the highest speedups when the number of local KV-heads per device is equal to two.\nWe obtain the highest speedups (~1.6\u00d7) with Llama3-8B and Phi3-small models, which are higher than their larger counterparts, namely Llama3-70B and Phi3-medium (1.35\u00d7 and 1.43x, respectively). This is due to the fact that the communication to computation ratio is higher in the small models; thus, overlapping memory reads with communications leads to higher speedups. Unlike Llama3-8B and Phi3-small, the maximum speedup with Qwen2-7B is, however, limited to 1.17x. This is because the number of attention heads in Qwen2-7B is smaller than the other models; thus, it does not scale well with increasing number of NPUs. Overall, the experiments demonstrate that the proposed method significantly improves the end-to-end execution time of various state-of-the-art models for varying number of NPUs, with speedups ranging from 1.09\u00d7 up to 1.61x."}, {"title": "Batch size & sequence length", "content": "The effectiveness of the proposed method is sensitive to the batch size and maximum sequence length because of the KV-cache and its implications on the computational characteristics and memory requirements. To that end, we perform a number of experiments to evaluate the impact of batch size and maximum sequence length on the speedup obtained by the proposed method. In these experiments, we run the last iteration of the decode stage for an LLM architecture with the given batch size and sequence length for multiple times with and without the proposed method and calculate the speedup based on the median execution times.\nFigure 4 shows the speedups obtained with the proposed method for the largest three models used in the previous experiments with batch size and sequence lengths ranging from 1 to 256 and from 1k to 64k, respectively. The results of this experiment show that, for the given range of batch size and sequence length, the proposed method achieves a speedup up to 1.82x. The largest speedups are obtained for long sequence lengths and small batch sizes. The speedup generally decreases with the batch size for two reasons. First, as the batch size increases, the computation in MLP layers becomes less memory-bound, which reduces the effectiveness of the proposed method in MLP layers. Second, the L2 requirement of the KV-cache increases with the batch size. Therefore, for large batch sizes, the KV-cache does not fit in L2, leading to no speedup for the self-attention operators.\nWe also observe that the speedup generally increases with the sequence length until it reaches a certain threshold. The reason for the increase in the speedup is due to the fact that the speedup obtained in the self-attention layers are higher than the MLP layers due to the formers' irregular and strided memory access patterns. As a result, increasing the sequence length leads to higher end-to-end speedup, as the self-attention layers take up a larger portion of the total execution time with larger sequence lengths. However, the memory requirements of the KV-cache also increases with the sequence length. As a result, increasing the sequence length beyond a threshold where the KV-cache no longer fits in L2 renders the proposed method ineffective for self-attention layers, significantly reducing the speedup. Nevertheless, this drop in the speedup occurs only in very large sequence lengths (e.g., 32k for a batch size of 4), where most practical applications require much smaller sequence lengths. Therefore, the effective range of the proposed method covers a wide range of batch size and sequence lengths (up to a batch size of 256 or a sequence length of 64k).\nIn conclusion, in this section, we evaluated the effectiveness and the performance of the proposed method for various LLMs under diverse inference settings, such as the number of NPUs, batch size, and sequence lengths. Our experiments demonstrated that the proposed method achieves up to 1.61\u00d7 end-to-end speedup on commercial AI accelerators. Our study on batch size and sequence length identified the optimal settings where the proposed method is most effective and showed that the proposed method achieves significant speedups under a wide range of inference settings."}, {"title": "5 Design Space Exploration", "content": "The experiment results in the previous section demonstrate significant improvement in the performance of distributed LLM inference with PRESERVE framework on existing commercial AI accelerators. However, various hardware design parameters and specifications have an impact on the performance of the PRESERVE framework. Therefore, in this section, we investigate how the hardware design parameters (i.e., L2 cache size, L2 bus bandwidth, cluster size, network bandwidth) affect the effectiveness of the PRESERVE framework and perform a design space exploration to identify the optimal hardware design parameters that maximize the inference performance of AI accelerators.\nPerformance & cost model To perform the design space exploration presented in this section, we developed a performance model of AI accelerators for Transformer architectures. We modeled the compute throughput as a linear function of the flops capacity of an accelerator. We assume that the memory read and write throughputs are linear functions of the off-chip memory or on-chip L2 cache bandwidths, in case of a L2 miss or hit, respectively. We modeled the communication latency as the summation of a constant initial latency and the data size divided by the bandwidth of the link between the accelerators. We assume that multiple accelerators in a cluster are interconnected in a ring topology.\nTable 2 summarizes the values of the model parameters used in our design space exploration. The silicon area for cores and L2 SRAM cache are taken from Lin et al. [24], whose design is fabricated in a 7nm technology node. We assumed that the accelerator has a theoretical peak throughput of 800 TeraOps/s, which is similar to existing inference accelerators [12, 21] and it is equipped with 4x HBM2e memory, which provides 64 GB of capacity and 1840 GB/s bandwidth in total [33].\nFor the design space exploration, we selected a number of open-source, state-of-the-art LLMs, namely Llama3-8B, Llama3-70B, Llama3-405B [35], Qwen2-7B, Qwen2-72B, Qwen1.5-110B [40], Phi3-small, Phi3-medium, Phi3.5-MoE [1], Mistral-7B, Mixtral-8x7B, and Mixtral-8x22B [17]. Unless specified otherwise, we assume a cluster size of 32 NPUs and we vary the batch size from 8 to 32 with increments of 8 and maximum sequence length from 2k to 16k with steps that correspond to the powers of 2k. In all experiments, we assume that both activations and weights are in int8 quantization."}, {"title": "Performance impact of L2 size", "content": "The size of the L2 cache is critical for the applicability and effectiveness of the proposed prefetching mechanism. Unfortunately, the L2 cache size of today's commercial AI accelerators are provisioned without taking weight and/or KV-cache prefetching into consideration. As a result, such AI accelerators cannot fulfill the full potential of the proposed method due to insufficient L2 capacity. To that end, we first perform a design space exploration to understand the requirements of the L2 cache size for weight and KV-cache prefetching and identify the optimal cache size that maximizes the performance/cost ratio.\nTo evaluate the impact of L2 cache size on the speedups obtained by the proposed method, we vary the L2 cache size and calculate the latency for various models and benchmarks. Figure 5 shows the inference latency for each model, normalized to the smallest L2 capacity. We observe that, with increasing L2 cache size, the latency is reduced as more data can be prefetched to L2. We observe that the latency is reduced by percentages ranging between 20% and 36%, with an average of 25%. We observe that the maximum speedups for most models are achieved at the L2 cache sizes of 136 and 144 MB, beyond which we do not observe any further speedups."}, {"title": "Throughput-area trade-off", "content": "While the effectiveness of the proposed method generally increases with the L2 cache size, allocating a larger L2 cache also increases the silicon area, leading to an accelerator design with a higher cost per unit. Therefore, to identify the sweet-spot between the performance improvement and cost overhead in terms of the L2 cache size, we devise the throughput density metric, which is equal to the LLM inference throughput in terms of tokens/s divided by the total die area in terms of mm\u00b2. To show the impact of weight and KV-cache prefetching on the throughput density and optimal L2 cache size, we perform a design space exploration with and without prefetching enabled, by sweeping a range of L2 cache size and calculating the throughput density.\nFigure 6 illustrates the throughput density with respect to the L2 cache size with and without prefetching enabled. We observe that the throughput density for the baseline accelerator design without prefetching peaks at 8 MB of L2 cache, beyond which it decreases due to the diminishing returns on the benefits of the increasing L2 size. For an accelerator that uses prefetching, on the other hand, the throughput density increases up to the L2 cache size of 104 MB, as more model weights and KV-cache are prefetched and more communication overhead is hidden thanks to the larger L2 cache size. Similar to the baseline, the throughput density decreases beyond the L2 cache size of 104 MB, as the cost overhead of larger L2 cache outweighs the benefits of prefetching.\nOur design space exploration shows that the optimal L2 cache size changes for inference accelerators when prefetching is taken into account. Without prefetching, a relatively small L2 cache size (i.e., 8 MB) is sufficient to cache the required data for individual operations in order to minimize off-chip memory accesses. In contrast, an accelerator design for the proposed prefetching scheme requires a larger L2 cache size (i.e., 104 MB) in order to store more model weights and KV-cache during collective communication operations. Nevertheless, prefetching more model weights and KV-cache improves the performance density even at the expense of increased L2 cache size, increasing the throughput density from 11.8 to 14.8 token/s/mm\u00b2, which corresponds to an improvement by 1.25\u00d7."}, {"title": "Multi-node scale-out", "content": "In Section 4, we discussed the impact of the cluster size (number of accelerators) on the speedup obtained by the proposed method up to a cluster size of 8, which is the size of single Atlas 800T A2 server. In this section, we now extend this analysis beyond a single server using our accelerator performance model to evaluate the performance gains by the proposed method on a scale-out system.\nWe expect that the maximum speedup occurs when the allreduce and prefetch latencies are equal to each other. If the prefetch latency is shorter than the allreduce, the latter's latency can not be fully hidden. In contrast, if the prefetch latency is longer than the allreduce, the impact of hiding the allreduce latency on the overall speedup is reduced. As a result, the maximum theoretical speedup is obtained in settings where the prefetch and allreduce latencies are equal to each other.\nIncreasing the number of devices in a cluster typically decreases the local memory size of model weights and KV-cache, as these tensors are partitioned into smaller chunks. Consequently, the data size that needs to be read from HBM memory decreases, reducing the prefetch time. As a result, adding or removing devices from the cluster may change the ratio between allreduce and prefetch latencies, which also affects the speedup obtained by the proposed method. Therefore, we perform an analysis on how the speedup is affected by the cluster size.\nFigure 7 shows the speedup obtained by prefetching over the baseline accelerator for various number of devices in a tensor-parallel cluster. We observe that the speedup increases with the number of devices up to 32, beyond which it starts to decreases. Figure 7 also shows the allreduce and prefetch latencies for Attention and MLP layers to better understand the trends in speedup with respect to the number of devices. We observe that, at the lower end of the range of number of devices (i.e., 8 and 16), the prefetch latency for FFN layers are significantly longer than the allreduce latencies, which limits the speedup. With the increasing number of devices, the weight size of FFN layers for each device decreases thanks to tensor parallelization. As a result, the prefetch latency for FFN also decreases and matches the allreduce latency at the cluster size of 32, where we observe the highest speedup. Increasing the number of devices beyond the cluster size of 32 results in prefetch latencies shorter than the allreduce latencies, in which case the latter can not be fully hidden, leading to a decrease in the speedup."}, {"title": "Figure 8:", "content": "Speedup obtained with prefetching over the baseline for various LLMs with respect to the network bandwidth of the links between the devices. The L2 cache size and the number of devices are taken as 104 MB and 128, respectively. The batch size and max. sequence length are equal to 16 and 8k, where prompt and decode lengths are 2/3 and 1/3 of the total number of tokens, respectively.\nSimilar to the cluster size, the network bandwidth between the devices also has a considerable impact on the speedups achieved by the proposed method, as it directly affects the allreduce latencies. Figure 8 plots the speedups obtained by the proposed method for various LLMs with respect to the network bandwidth. We observe that the speedup generally increases with the network bandwidth for all models. This is due to the fact that increasing the bandwidth reduces the allreduce latencies, which enables to hide a larger portion of the communication overhead, leading to a higher speedup. On average, we observe that increasing the device-to-device link bandwidth from 200 to 1000 Gbps elevates the speedup from 1.18x to 1.27\u00d7 for"}]}