{"title": "AH-OCDA: Amplitude-based Curriculum Learning and Hopfield Segmentation Model for Open Compound Domain Adaptation", "authors": ["Jaehyun Choi", "Junwon Ko", "Dong-Jae Lee", "Junmo Kim"], "abstract": "Open compound domain adaptation (OCDA) is a practical domain adaptation problem that consists of a source domain, target compound domain, and unseen open domain. In this problem, the absence of domain labels and pixel-level segmentation labels for both compound and open domains poses challenges to the direct application of existing domain adaptation and generalization methods. To address this issue, we propose Amplitude-based curriculum learning and a Hopfield segmentation model for Open Compound Domain Adaptation (AH-OCDA). Our method comprises two complementary components: 1) amplitude-based curriculum learning and 2) Hopfield segmentation model. Without prior knowledge of target domains within the compound domains, amplitude-based curriculum learning gradually induces the semantic segmentation model to adapt from the near-source compound domain to the far-source compound domain by ranking unlabeled compound domain images through Fast Fourier Transform (FFT). Additionally, the Hopfield segmentation model maps segmentation feature distributions from arbitrary domains to the feature distributions of the source domain. AH-OCDA achieves state-of-the-art performance on two OCDA benchmarks and extended open domains, demonstrating its adaptability to continuously changing compound domains and unseen open domains.", "sections": [{"title": "1. Introduction", "content": "Semantic segmentation has been explored in various areas, including medical image processing, autonomous driving, and robot scene understanding. However, the need for meticulous pixel-level labels in training presents a significant challenge for direct model deployment. To mitigate the burden of acquiring such labels, domain adaptation and domain generalization have been investigated. These tasks primarily focus on minimizing the distribution difference between the source and target domains, also known as the domain gap. Although they have demonstrated remarkable performance, each approach has certain limitations: domain adaptation experiences performance degradation in novel target domains, while domain generalization requires obtaining pixel-level labels across multiple domains. This challenge has driven the development of approaches such as multi-target domain adaptation, single-source domain generalization, and recently, open compound domain adaptation (OCDA).\nOCDA, first proposed by Liu et al. [18], leverages the advantages of aforementioned tasks and adapts to multiple unlabeled target domains (a compound domain) and novel domains (an open domain) with a single labeled source domain. Specifically, OCDA treats the compound domain as one continuous spectrum of potential target domains, enhancing its practicality even when the domain labels within the real-world target domains are vague. Additionally, by addressing both the compound and open domains simultaneously, OCDA further enhances its practicality, aligning with the ideal scenario in which semantic segmentation models are trained on labeled synthetic source domains but deployed in environments where domains are unknown.\nPrevious works [8, 18, 21] solve this complicated problem by dividing the compound domain into multiple discrete domains, narrowing the task into several unsupervised domain adaptation problems. While these approaches have demonstrated promising performance, they are limited by training separate segmentation models, requiring sophisticated hyperparameter tuning for each domain and model while also not fully accounting for the open domain.\nTo overcome the limitations, we propose a novel method, amplitude-based curriculum learning with the Hopfield segmentation model for OCDA (AH-OCDA). AH-OCDA integrates two key components: amplitude-based curriculum learning and the Hopfield segmentation model. The amplitude-based curriculum learning is a ranking-based approach that leverages the frequency spectrum of each image to treat the compound domain as a continuous spectrum of target domains. Moreover, the Hopfield segmentation model maps distorted features to source features using a modern continuous Hopfield network, aiming to overcome the challenge of unseen open domains. It is noteworthy that these two components are orthogonal, complementing each other to achieve better performance in OCDA.\nAmplitude-based curriculum learning leverages domain-specific information of the image, drawing upon the insight that amplitude in the low-frequency spectrum contains the domain-specific information [35, 38]. Specifically, we extract the low-frequency component from the amplitude obtained by the Fast Fourier Transform (FFT) of the target image and compute mean squared error (MSE) with the mean amplitude of source domain images. We continuously adapt the model trained in the source domain to the near-source target domain, while utilizing the previously adapted target domain as fake source with pseudo labels. The proposed curriculum learning only requires knowledge of the individual gap between target images and the source domain, maintaining robustness against dynamic domain shifts. Meanwhile, the Hopfield segmentation model maps segmentation feature distributions from arbitrary domains to the feature distributions of the source domain by learning and retrieving feature patterns. Modern continuous Hopfield network [19, 25] learns relational feature mappings, enabling it to restore corrupted or noisy patterns to their original pattern by leveraging associative memory. Building on this capability, the Hopfield segmentation model implicitly stores representative information within its parameters, including intra-class and inter-class information. This allows the network to accurately predict distorted feature vectors from compound or open domains previously encountered. Unlike approaches that rely on manually modeling feature distribution statistics, such as memorizing the class centroids or statistical information, our Hopfield segmentation model requires only the source domain to learn raw patterns and map the target domain features to their corresponding source domain features.\nWhile the Hopfield segmentation model is capable of mapping arbitrary domain features to source domain features, it cannot guarantee the successful mapping of features from significantly distant domains to those in the known domain. At this point, amplitude-based curriculum learning trains the Hopfield segmentation model gradually, starting from the near-source target domain to the far-source target domain, enabling the model to map target domain features to the source domain features. By gradually adapting to target domains through adversarial learning and using fake source, the network can fully leverage the memory retrieval process to progressively enlarge the boundary of the known domain. Conversely, the Hopfield network within the segmentation model enables the model to predict domains beyond the known boundary established by curriculum learning by restoring distorted features to the corresponding source features. Our experiments confirmed that these components function orthogonally, significantly enhancing the model's performance when combined. Furthermore, we validated effectiveness of AH-OCDA on two OCDA benchmarks and additional extended open domains, where we achieved state-of-the-art performance."}, {"title": "2. Related works", "content": null}, {"title": "2.1. Domain Adaptation and Generalization", "content": "Domain adaptation (DA) is a key technique in machine learning where a model trained on one domain is adapted to perform well on a different, but related domain. Unsupervised domain adaptation (UDA) is a specific approach in which the model is adapted to an unlabeled target domain. They have advanced to scenarios with single target domain [22, 28, 31] or clearly distinct multi-target domains [6, 7, 40] and also to scenario with multiple source domain [23, 36].\nIn parallel, the concept of domain generalization (DG) aims to train a model on multiple source domains to be generalized on unseen target domains [1, 2, 4, 12, 35] or train a model on single source domains to be generalized on unseen target domains [15, 24]. However, these methods may not always capture the full complexity of real-world scenarios. In practical applications, data collection often results in mixed, continuous, and unseen domains, highlighting the need for new problem settings that better reflect these diverse and dynamic real-world conditions."}, {"title": "2.2. Open Compound Domain Adaptation", "content": "Open compound domain adaptation (OCDA) [18] suggested a problem setting that has a labeled source domain and a compound of multiple unlabeled target domains for training phase and unseen target domains for testing phase. They proposed a curriculum learning framework for semantic segmentation tasks, which schedules the curriculum based on the prediction confidence of a model that is only trained on the source domain. On the other hand, the following studies did not follow OCDA's approach but proposed their own unique methods. DHA [21] and MOCDA [8] assigned pseudo domain labels with K-means clustering to break down OCDA into multiple UDA settings. Specifically, DHA [21] utilizes K-means clustering on the convolutional feature statistics, which are mean and standard deviations. They stylize the source domain into each target latent domain to reduce the domain gap and adversarially align a shared segmentation model employing individual discriminators for each pair of stylized source and target latent domains. MOCDA [8] utilizes K-means clustering on style code, which is low-dimensional space generated by an additional encoder. They adopt a model-agnostic meta-learning approach, where a hyper-network combines domain-specific segmentation models and discriminators to consider the continuous compound target domain. However, K-means clustering may not be a suitable approach for open compound domain adaptation, as it may not effectively represent continuous target domains and accommodate expandable unseen domains. To address this limitation, we explore a more dynamic approach."}, {"title": "2.3. Fourier Transform for Domain Adaptation", "content": "Several works have discussed how the Fourier transform represents the style information of an image in its magnitude spectrum and have applied this concept to domain adaptation [11, 16, 29, 33, 37, 38, 42]. In this perspective, FDA [38] proposed a Fast Fourier Transform (FFT)-based style transfer that reduces the style gap between two domains by replacing the low-frequency spectrum of the images. Building on FDA, some methods have further developed FFT-based style transfer for DA by incorporating the ordering of target data during the training phase, similar to approaches used in continual learning [29, 33]. BAT [42] further applied FFT-based style transfer for multi-target domain adaptation instead of network-based style transfer. To cope better with OCDA task, AST-OCDA [13] suggested applying the FT-based style transfer on the feature space of the segmentation model instead of the input RGB image. For single-domain or discrete multiple-domain adaptation, applying Fourier-based style information at a particular feature level has proven effective. However, in OCDA, the direct application of Fourier-based style transfer is less feasible for adapting the source domain to diverse, continuous target domains, and unseen domains. To overcome this, we considered the flexible utilization of domain-specific information in OCDA scenarios."}, {"title": "2.4. Associative Memory", "content": "Associative memory systems function by retrieving data not through specific addresses but via queries that resemble the type of data stored within them [9]. When a query is made, the system identifies and returns a data point that most closely matches the query. For example, presenting an image as a query would yield other images that are deemed similar. This retrieval process is often compared to how the human brain operates, where memories can be recalled with partial cues, such as remembering a song from a few notes [19]. Two classical and influential models are the Hopfield network (HN) and the sparse distributed memory (SDM). More recently, they have been generalized to the modern continuous Hopfield network (MCHN), which substantially improves performance and can handle continuous inputs. Specifically, MCHN uses the energy function \\(E = q^Tq + \\text{log}\\sum\\text{exp}(Wq)\\), which can be minimized using the convex-concave method, giving the update rule \\(z = W\\bar{\\sigma}(W^Tq)\\), where \\(\\sigma(\\cdot)\\), \\(W\\), \\(q\\) represent the softmax function, the memory matrix and the query vector, respectively. This is similar to the attention operation \\(z = V\\sigma(W^Tq)\\) with key matrix \\(K\\) and value matrix \\(V\\), which is widely used in recent neural networks [10, 14, 34], where we can associate \\(Q = q\\), \\(K = W\\), and \\(V = W\\) [25]."}, {"title": "3. Method", "content": "Our approach for OCDA in semantic segmentation consists of two components: 1) amplitude-based curriculum learning and 2) the Hopfield segmentation model. Figure 1 illustrates our whole pipeline."}, {"title": "3.1. Preliminaries", "content": "Let spatial domain space \\(X \\subset \\mathbb{R}^{H \\times W \\times 3}\\) and frequency domain space \\(A \\subset \\mathbb{C}^{H \\times W \\times 3}\\), where \\(H\\) and \\(W\\) are height and width of the input image, respectively. Through FFT \\(\\mathcal{F}: X \\rightarrow A\\), we transform the image \\(x \\in X\\) to the spectrum \\(\\mathcal{F}(x)\\). Then, we obtain amplitude \\(\\|\\mathcal{F}(x)\\| \\in \\mathbb{R}^{H \\times W \\times 3}\\) and phase \\(\\text{Arg}(\\mathcal{F}(x)) \\in \\mathbb{R}^{H \\times W \\times 3}\\). The semantic segmentation model \\(\\text{Seg}\\) consists of an encoder \\(f\\) and classifier \\(g\\), such that \\(\\text{Seg}: x \\rightarrow y = g(f(x))\\), where \\(y \\in \\mathbb{R}^{H \\times W}\\) represents semantic segmentation prediction.\nFollowing OCDA, the labeled source domain consists of synthetic RGB images \\(x_s\\) and corresponding semantic segmentation label \\(y_s\\), such that\n\\(S = \\{(x_s^i, y_s^i) | x_s^i \\in X, y_s^i \\in Y\\}, i = 1, ..., N_s \\)\nwhere \\(N_s\\) is the number of images in the labeled source domain. The unlabeled compound domain consists of real RGB input images \\(x_c\\), such that\n\\(C = \\{x_c^i | x_c^i \\in X\\}, i = 1, ..., N_c \\)\nwhere \\(N_c\\) is the number of images in the unlabeled compound domain. The unseen domain consists of real RGB images \\(x_u\\), such that\n\\(U = \\{x_u^i | x_u^i \\in X\\}, i = 1, ..., N_u \\)\nwhere \\(N_u\\) is the number of images in the unseen domain."}, {"title": "3.2. Amplitude-based Curriculum Learning", "content": "The amplitude-based curriculum learning is a ranking-based approach that only requires knowledge of the individual gap between the target image and the source domain images. First, we transform the image \\(x_i\\) to the spectrum through the FFT. Then we shift values making the center of the spectrum to represent the low-frequency region. Finally, we obtain the domain-specific information by utilizing only the amplitude from the spectrum and cropping the central region, a region that is known to contain the domain-specific information.\n\\(F_i = \\text{Crop}_{\\beta} \\{\\|\\mathcal{F}(x_i)\\|\\} \\in \\mathbb{R}^{\\beta H \\times \\beta W \\times 3}\\)"}, {"title": "3.3. Hopfield Segmentation Model", "content": "In contrast to traditional domain generalization scenarios, where multiple labeled domains are typically available, OCDA contains labels only in the source domain, while both the compound and open domains do not have segmentation and domain labels during training. This challenge results in suboptimal performance of previous methods on target domains that significantly deviate from the source domain, as well as on unseen open domains. To address this challenge, we devised the Hopfield segmentation model to empower the semantic segmentation pipeline with the capability to align features in the target domain with their corresponding source domain, even on the novel unseen domains. Intuitively, adaptation for compound and open domains can be easily solved by mapping the arbitrary target features to the source feature distribution. The Hopfield network in the segmentation model correct distorted feature patterns or noisy inputs from unfamiliar domains by retrieving stored source patterns. In other words, given the encoder features \\(z\\), our objective is to learn the mapping function \\(H(.)\\) that maps the arbitrary feature to the source feature distribution.\nMathematically, the associative memory is a function \\(g(): \\mathbb{R}^{C_i} \\rightarrow \\mathbb{R}^{C_o}\\) mapping a vector in an input space of \\(C_i\\) to a vector in an output space of dimension \\(C_o\\), with two additional inputs of a memory matrix \\(M \\in \\mathbb{R}^{M_N \\times C_i}\\) consisting of a set of \\(M_N\\) stored patterns, and projection matrices \\(P \\in \\mathbb{R}^{C_i \\times C_o}\\), consisting of a potentially different set of stored patterns with dimension \\(C_o\\) for hetero-association. Similarly, our Hopfield network consists of a memory matrix \\(M \\in \\mathbb{R}^{M_N \\times C_1}\\) and query, key, and value projection matrices \\(W_q, W_k \\in \\mathbb{R}^{C_1 \\times C_s}\\) and \\(W_v \\in \\mathbb{R}^{C_s \\times C_s}\\), where \\(M_N\\), \\(C_1\\), \\(C_s\\) represents the memory size, dimension of the feature vector, and dimension of the projected feature vector for similarity measurement, respectively.\nWe first measure the similarity of the feature embedding \\(z\\) and each raw stored pattern vector \\(m_i \\in M_N\\):\n\\(\\text{sim}(z, m_i) = \\frac{\\text{exp}(\\tau \\cdot W_q z \\cdot (W_k m_i)^T)}{\\sum_{j\\in M} \\text{exp}(\\tau \\cdot W_q z \\cdot (W_k m_j)^T)}\\)\nwhere \\(\\tau\\) represents the scaling factor. The role of each projection matrix is to project the raw state patterns \\(z\\) and stored patterns \\(m_j\\) to associative space and reduce computational complexity through dimension reduction (\\(C_s < C_1\\)). Then, we project the \\(z\\) to the stored pattern with the weighted sum of the similarity and the stored patterns:\n\\(\\hat{z} = \\sum_{i\\in M_N} \\text{sim}(z, m_i) W_v m_j.\\)\nThe projected feature \\(\\hat{z}\\) is fed to the classifier to predict the segmentation map and update its memory with the segmentation loss.\nFor training the Hopfield segmentation model, we pretrain the Hopfield network in the segmentation model for the source dataset. Then, in the following adaptation phase, we freeze the associative memory. Specifically, we freeze the memory weights with key and value projection weights. Therefore, the memory network maps the feature from target domains to the trained source domain feature distribution. The pretrained Hopfield network in the segmentation model is likely to be robust even on far-source target domains, as the amplitude-based curriculum learning gradually trains the entire segmentation model to obtain a well-structured target feature distribution."}, {"title": "3.4. Loss Function", "content": "Our framework utilizes the following loss to optimize the segmentation model \\(\\text{Seg}\\) and the discriminator \\(D\\):\n\\(\\mathcal{L}(S_j, T_j) = \\mathcal{L}_{ce}(S_j) + \\lambda_{adv} (\\mathcal{L}_{Seg}^{adv}(T_j) + \\mathcal{L}_{D}^{adv}(S_j, T_j)).\\)\nThis loss consists of two parts: \\(\\mathcal{L}_{ce}\\) is the cross entropy loss that updates the segmentation model to make accurate predictions in the source domain and the fake-source. Formally, \\(\\mathcal{L}_{ce}\\) is defined as\n\\(\\mathcal{L}_{ce}^{Seg}(S) = - \\sum_{h,w} \\sum_{cls} Y_{\\hat{S}(h,w,cls)} \\text{log}(\\text{Seg}(\\hat{S}_j)(h,w,cls)),\\)\nwhere \\(cls\\) is the number of classes, and \\(Y_{\\hat{S}}\\) is the (pseudo) label of the (fake-) source image. \\(\\mathcal{L}_{adv}\\) is applied separately to the segmentation model \\(\\mathcal{L}_{Seg}^{adv}\\) and the discriminator \\(\\mathcal{L}_{D}^{adv}\\), which are\n\\(\\mathcal{L}_{Seg}^{adv}(T_j) = \\sum_{h,w} \\text{log} \\left(D \\left(\\text{Seg}(T_j)\\right)(h,w,1)\\right),\\)\n\\(\\mathcal{L}_{D}^{adv}(T_j, S_j) = - \\sum_{h,w} \\text{log} \\left(D \\left(\\text{Seg}(T_j)\\right)(h,w,0)\\right) + \\text{log} \\left(D \\left(\\text{Seg}(\\hat{S}_j)\\right)(h,w,1)\\right).\\)\n\\(\\lambda_{adv}\\) induces the adversarial learning for the segmentation model and discriminator. \\(\\lambda_{adv}\\) adjusts the importance between the two losses."}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Experimental settings", "content": "For the labeled source domain we leverage GTA5 [26] and SYNTHIA [27] while BDD100K (C-Driving) [39] is utilized for the unlabeled compound and open domains. For extended open domain, we evaluate with Cityscapes [3] and KITTI [5]. The details for each dataset can be found in the supplementary material. For all experiments, We used SGD with a polynomial decaying learning rate, starting at 0.00025 for the segmentation model and 0.0001 for the discriminator while others such as batch size follow [18] Hyperparameters introduced for AH-OCDA such as \u03bb, \u03b2, K, T, and the Hopfield segmentation model memory size are set to 0.001, 0.09, 3, 1, and 64, respectively."}, {"title": "4.2. Baselines", "content": "The efficacy of our framework is assessed through a comprehensive comparison with a selection of established baselines. Source Only refers to a VGG segmentation model trained on the source domain. UDA methods are unsupervised single-source to single-target domain adaptation frameworks. These include AdaptSegNet [30], CBST [43], IBN-Net [20], PyCDA [17], CRST [44] and AdvEnt [32].\nFor OCDA frameworks, OCDA [18] is the baseline framework for open compound domain adaptation. DHA [21] and MOCDA [8] leverage clustering to decompose OCDA into multiple UDA. AST-OCDA [13] applies FFT-based style transfer on the feature space of the segmentation model."}, {"title": "4.3. Results", "content": "GTA5 as Source The semantic segmentation performance on GTA5 \u2192 C-Driving is reported in Table 1. Comparing performance on both Compound and Compound + Open, hereinafter denoted as C and C + O respectively, AH-OCDA achieves the best mIoU. In Table 2, we report the semantic segmentation performance on GTA5 \u2192 extended open domains, Cityscape and KITTI. AH-OCDA outperforms the SOTA method AST-OCDA [13] by 1.1%p on total mIoU, which verifies that it has better domain generalization ability. It is because the Hopfield segmentation model helps to predict segmentation features based on source information leading the performance improvement on the extended open domains.\nSYNTHIA as Source The semantic segmentation performance on SYNTHIA \u2192 C-Driving is shown in Table 3. AH-OCDA achieves the best semantic segmentation performance among all the baselines on all domains. In comparison with AST-OCDA, AH-OCDA outperforms 1.0 %p on C and 1.5 %p on C + O, outperforming on all datasets, including one of the extended open dataset (Table 4)."}, {"title": "4.4. Ablation", "content": null}, {"title": "4.4.1 Number of splits K in curriculum learning", "content": "The hyperparameter K in AH-OCDA determines the number of splits applied to the compound domain to gradually expand known distributions during curriculum learning. Similarly, DHA [21] employs a comparable hyperparameter to divide compound domains into sub-latent domains through k-means clustering. To evaluate the robustness with K, we compare the hyperparameter K used in AH-OCDA and DHA and report the results in Table 5. In both methods, the best mIoU is achieved when the hyperparameter is set to 3. However, when considering the robustness to K, a critical aspect that must be considered since the number of domains within a compound domain is unknown in advance, AH-OCDA demonstrates a much more stable performance compared to DHA. Specifically, AH-OCDA shows no performance drop when K is set to 4 and exhibits less or comparable performance drop for other values of K."}, {"title": "4.4.2 Sub-domains visualization", "content": "Figure 3 demonstrates randomly selected images from each curriculum domain when K is set to 3. Through amplitude-based ranking, the night or darker images are ranked later in the curriculum learning. We also visualize the curriculum domains in the feature space by extracting features using VGG-16, the segmentation network used in all experiments. Figure 2 validates that curriculum learning ranks the images within a compound domain as intended, ranking the images from near-source to far-source."}, {"title": "4.4.3 Hopfield segmentation model settings", "content": "We conduct an ablation study on the hyperparameters for the Hopfield segmentation model, memory size, and training strategy and report it in Table 6. Experimentally, the size of memory does not have a crucial effect on segmentation performance when above 64. As the memory size of 64 provides the best trade-off between performance and computational complexity, we used 64 for all experiments as aforementioned. For the training strategy, we experiment with not freezing and freezing the Hopfield segmentation model when training on the compound domain. The performance drops when the memory is not frozen as the memory gradually forgets the segmentation feature of the source domain thus we leverage the frozen model."}, {"title": "4.4.4 Complementary components of AH-OCDA", "content": "Table 7 shows the effect of each component in AH-OCDA. AH-OCDA outperforms 'w/o Hopf' (Ours without the Hopfield segmentation model) and 'w/o Curr' (Ours without amplitude-based curriculum learning). In Table 7, there is a significant performance drop in the 'Night' category for 'w/o Curr' model aligning with the statement in Sec. 3.3 that the Hopfield segmentation model may not function well when adapting to a far-source target domain in the compound domain. The amplitude-based curriculum learning complements this issue by gradually training the model from the near-source target domain so that the Hopfield segmentation model can be trained stably."}, {"title": "4.4.5 Other backbone networks", "content": "To validate that AH-OCDA is not limited to specific backbone networks, we additionally experiment using ResNet and Transformer-based backbone and compare it with unsupervised domain adaptation methods in semantic segmentation, ProDA [41] (ResNet-101) and DAFormer [10] (Segformer [34]), respectively. Table 8 shows the performance of VGG-16, ResNet-101, and Transformer-based architectures in the top, middle, and bottom rows, respectively. Although the performances cannot be directly compared due to the comparison methods being proposed for another task, as aforementioned, the performance gain indirectly demonstrates that AH-OCDA is not restricted to certain backbone networks. Notably, AH-OCDA shows better performance on a few of the target domains in the compound domain than the unsupervised domain adaptation methods, demonstrating the need for separate methods for OCDA setting."}, {"title": "5. Conclusion", "content": "In this paper, we propose a novel method, AH-OCDA, considering the differences between OCDA with conventional domain adaptation and generalization settings. Specifically, we propose two complementary components: 1) amplitude-based curriculum learning and 2) the Hopfield segmentation model. First, by utilizing the FFT, we obtain the amplitude of each target image and measure the difference between the mean amplitude of the source domain images. With the obtained distance, we rank them to formulate the OCDA training in a curriculum-learning manner, without any prior knowledge of domains in the compound domain. Second, the Hopfield segmentation model learns the segmentation feature distribution on the source domain with memory and aligns features from unknown domains to the source domain. We verify the effectiveness of the AH-OCDA with experiments on two datasets, GTA5\u2192C-Driving and SYNTHIA\u2192C-Driving. Additionally, we present the experiment results on two novel unseen domains, Cityscape and KITTI, and achieve comparable or state-of-the-art performance."}, {"title": "D. Limitations", "content": "As demonstrated in Table 5 of the main manuscript, AH-OCDA exhibits robustness to variations in the number of K when compared to the previous method. However, when there is an imbalance in image distribution within a compound domain\u2014for example, when images are predominantly distributed at the beginning of the curriculum\u2014AH-OCDA could experience performance degradation due to dividing the sorted compound domain images by K splits. In such a situation, we may alternatively define a threshold value rather than the number of K. When the distance between two adjacent images in the sorted compound domain images exceeds the defined threshold, we can consider that they have distant domain information. This approach may even be beneficial when there is a huge domain gap between the domains in the compound domain. Nevertheless, we opted not to implement thresholding in AH-OCDA as thresholding the distance value requires more precise consideration and is considerably more sensitive compared to the use of the number of K. In our future research, we aim to remove the hyperparameter K to enhance generalizability."}]}