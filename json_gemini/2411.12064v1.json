{"title": "TSPRank: Bridging Pairwise and Listwise Methods with a Bilinear Travelling Salesman Model", "authors": ["Weixian Waylon Li", "Yftah Ziser", "Yifei Xie", "Shay B. Cohen", "Tiejun Ma"], "abstract": "Traditional Learning-To-Rank (LETOR) approaches, including pairwise methods like RankNet and LambdaMART, often fall short by solely focusing on pairwise comparisons, leading to sub-optimal global rankings. Conversely, deep learning based listwise methods, while aiming to optimise entire lists, require complex tuning and yield only marginal improvements over robust pairwise models. To overcome these limitations, we introduce Travelling Salesman Problem Rank (TSPRank), a hybrid pairwise-listwise ranking method. TSPRank reframes the ranking problem as a Travelling Salesman Problem (TSP), a well-known combinatorial optimisation challenge that has been extensively studied for its numerous solution algorithms and applications. This approach enables the modelling of pairwise relationships and leverages combinatorial optimisation to determine the listwise ranking. This approach can be directly integrated as an additional component into embeddings generated by existing backbone models to enhance ranking performance. Our extensive experiments across three backbone models on diverse tasks, including stock ranking, information retrieval, and historical events ordering, demonstrate that TSPRank significantly outperforms both pure pairwise and listwise methods. Our qualitative analysis reveals that TSPRank's main advantage over existing methods is its ability to harness global information better while ranking. TSPRank's robustness and superior performance across different domains highlight its potential as a versatile and effective LETOR solution. The code and preprocessed data are available at https://github.com/waylonli/TSPRank-KDD2025.", "sections": [{"title": "1 INTRODUCTION", "content": "Learning to Rank (LETOR) algorithms have become essential in applications such as recommendation systems [16, 30, 36], question answering [10, 15, 27], and information retrieval [2, 9, 34]. These algorithms aim to order a list of ranking entities based on their features, optimising for the most relevant or preferred entities to appear at the top. Recently, LETOR methods have also expanded into other domains, such as stock and portfolio selection [45, 50] and textual ordering [46, 49]. Despite these broader applications of LETOR, latest fundamental work on LETOR still primarily focused on incorporating click data into ranking models and addressing biases introduced by user feedback [20, 21, 33, 54], concentrating on retrieval and recommendation tasks. Limited research has explored general ranking methods across diverse tasks and domains.\nOver the past decade, research on LETOR models mainly focuses on pairwise and listwise approaches, while pointwise methods often fail to capture the intricate inter-entity relationships that are essential for accurate ranking. Pairwise methods, such as LambdaMART and RankNet [6], primarily optimise for pairwise comparisons without a holistic view of the entire ranking list, potentially leading to sub-optimal global rankings. Listwise methods optimise the ranking of entire lists directly rather than individuals or pairs. This category includes advanced neural network architectures, particularly adaptations of the Transformer architecture [47], such as Rankformer [9] and SetRank [34]. However, existing work shows that deep learning-based listwise models require complex tuning to achieve marginal gains over robust pairwise models like LambdaMART on information retrieval benchmarks [39]. Therefore, pairwise and listwise methods each have inherent drawbacks and more effective solutions for LETOR have not been exhaustively explored."}, {"title": "2 BACKGROUND", "content": "LETOR algorithms aim to rank entities based on their features. In LETOR, we denote a list of N ranking entities as $e$, containing ${e_1, e_2, ..., e_N }$, with each $e_i \\in R^d$ where d represents the dimensionality of the feature space. A scoring function $s : R^d \\rightarrow R$ is applied on the entities, which are then ranked in descending order of the $s(e_i)$ scores. LETOR algorithms are broadly categorised into pointwise, pairwise, and listwise approaches based on their optimisation strategies. Pointwise methods treat ranking as a regression problem, comparing scores $s(e_i)$ to labels $y_i$. Models such as OPRF [19], TreeBoost [18], and RankSVM [44], including newer implementations like RankCNN [43], are known for their computational efficiency but may not capture inter-entity relationships adequately."}, {"title": "3 TSPRANK", "content": "Directly predicting the listwise order is challenging, as correctly ranking N entities from 1 to N is complex. However, breaking it down into N \u00d7 N pairwise comparisons simplifies the task, as each pairwise comparison is more straightforward than the entire listwise ranking. However, a fundamental drawback of pairwise ranking, as highlighted by Cao et al. [11], is that the learning objectives do not focus on minimising the listwise ranking errors. To address this gap and rethink the approach of pairwise ranking, we introduce TSPRank, a novel methodology for position-based ranking problems. This approach integrates listwise optimisation into pairwise comparisons by modelling ranking tasks as TSP. Notably, TSPRank can be applied as an additional component with embeddings generated from domain-specific backbone models to achieve improved ranking performance."}, {"title": "3.1 Ranking As Travelling Salesman Problem", "content": "We model the ranking problem as an open-loop Travelling Salesman Problem (TSP). Given a set of entities $e_1, e_2,..., e_n$ and all the pairwise score values $s(e_i, e_j)$ indicating the gain of ranking entity $e_j$ immediately after $e_i$, we define a complete graph $G = (V, W)$, where V represents the entities and W represents the pairwise scores between every two entities. Each entity $e_i$ in the ranking problem corresponds to a city in the TSP. By mapping entities to cities and pairwise scores to weights, we can formulate the ranking"}, {"title": "3.2 Scoring Model", "content": "The scoring model takes a set of ranking entities or their corresponding embeddings ${e_1, e_2, . . ., e_N }$, where each $e_i \\in R^d$ and d is the dimension of the features or embeddings. It outputs an N\u00d7N weighted adjacency matrix A that represents the pairwise scores of every two entities. The scoring model comprises an optional node encoder and a trainable bilinear model on top.\nEncoder. The default encoder used is the Transformer encoder block [47], but it can be replaced with any other encoder suited to the specific task and dataset. Additionally, using an encoder is optional if sufficiently robust backbone models are available.\nTrainable Bilinear Model. Given a pair of encoded representations of entities $(e_i, e_j)$, the bilinear model computes the pairwise score using the following bilinear form:\n$s(e_i, e_j) = e_i^T W e_j + b$,\nwhere W and b are learnable parameters that can be optimised along with the encoder parameters.\nThe adjacency matrix A is constructed as follows:\n$A_{ij} = s(e_i, e_j), \\text{ for all } i, j \\in \\{1, . . ., N\\}$,\nwhere each entry $A_{ij}$ directly corresponds to the computed pairwise comparison score from entity $e_i$ to entity $e_j$."}, {"title": "3.3 Inference", "content": "Inference of TSPRank, described in Equation 1, requires solving the open-loop TSP, which is widely recognised as an NP-hard combinatorial optimisation problem. The solution algorithms of the TSP consist of exact algorithms (brunch and bound [37]) and approximation algorithms (ant colony optimisation [52]). Given the scale of the ranking problem and the desired ranking accuracy, we prefer to obtain an exact solution for the TSP rather than an approximate one. Therefore, we formulate the TSP as a Mixed Integer Linear Programming (MILP) problem [40], where the integer variables represent binary decisions on whether a path between two nodes is included in the optimal tour. We then use an MILP optimization solver, such as CPLEX [14] or Gurobi [22], to find the exact optimal solution.\nThe decision variables are defined as follows:\n$x_{ij} = \\begin{cases} 1, & \\text{if entity } e_j \\text{ is ranked immediately after } e_i. \\\\ 0, & \\text{otherwise.} \\end{cases}$ \nWe then apply the TSP optimisation problem defined in Appendix A to ensure the result is a single, complete ranking that includes"}, {"title": "4 LEARNING", "content": "We introduce two distinct learning methodologies for the TSPRank model outlined in Section 3. The first method is a local approach, focusing exclusively on individual pairwise comparisons during the training phase. The second method is a global, end-to-end approach, incorporating the Gurobi solver directly into the training process, thereby allowing the black-box MILP solver to influence the learning dynamically. Figure 1 illustrates the complete pipeline of the two learning methods as well as the model architecture."}, {"title": "4.1 Local Learning", "content": "In the local learning approach, the objective is to understand pairwise consecutive relationships between nodes. The objective is to determine whether entity $e_j$ should rank one position after $e_i$ in a given pair of nodes. Therefore, the score $s(e_i, e_j)$ is set high if $e_j$ is supposed to rank consecutively after $e_i$, and low otherwise.\nTo tailor the TSPRank model to ranking scenarios where penalties vary based on the actual positions, we apply a weighted loss. We denote the predicted adjacency matrix $A^P$ from the bilinear model, where $a^p_{ij} \\in A^P$ represents the predicted pairwise scores. The target adjacency matrix $A^t = \\{a^t_{ij}\\}$ is defined such that $a^t_{ij} \\in A^t$ equals 1 if $e_j$ ranks immediately after $e_i$, and 0 otherwise. We apply the weighted cross-entropy loss (defined in Equation 5) on each row to identify the next entity $e_{\\pi(i+1)}$ to be ranked given $e_{\\pi(i)}$. The model is trained as a multi-class classification problem, aiming to maximise the probability of $P(e_{\\pi(i+1)} | e_{\\pi(i)})$. We weight the loss by the true ranking label y, allowing the penalty to vary according to different ranking positions. Additionally, y can be adjusted to N+1-y depending on whether y represents ascending or descending order.\n$L_{local} (A^P, A^t) = \\sum_{i=1}^{N} y_k \\log \\frac{e^{A^p_{ik}}}{\\sum_{j=1}^N e^{A^p_{ij}}} \\qquad k = \\arg \\max_j A_{ij}.$"}, {"title": "4.2 Global Learning", "content": "We expect that a globally trained TSPRank model will further enhance the performance as the local learning method focuses solely on pairwise comparisons in isolation during training. Incorporating the TSP solver in the training procedure will better align the model with the inference process. Therefore, we introduce an end-to-end approach. After obtaining the predicted adjacency matrix $A^P$, defined in Section 4.1, we define the gold decision variable $x^t = \\{x^t_{ij}\\}$, where $x_{ij}$ is defined as in Equation 4. We aim to train a model to satisfy the margin constraints (Equation 6) for all the possible decision variables x by minimising the max-margin loss defined in Equation 7. When $x = x^t$, the predicted ranking perfectly aligns with the gold ranking, resulting in a loss of zero."}, {"title": "5 EXPERIMENTAL SETUP", "content": "We evaluate TSPRank's effectiveness as a prediction layer integrated into various backbone models. Our goal is to demonstrate that TSPRank enhances performance compared to the original backbone and other general ranking algorithms across different domains. While achieving state-of-the-art (SOTA) performance still depends on the backbone design, TSPRank serves as a component to improve ranking performance. We focus on scenarios where (i) the data contain ordinal ranking labels rather than binary or relevance level labels, and (ii) the complete ranking or at least the top-k entities are important, extending the task's interest beyond merely identifying the top-1 entity. We test our method with three datasets"}, {"title": "5.1 Benchmark Models", "content": "We benchmark our pairwise-listwise TSPRank model against the original backbone models (if they originally contain a prediction layer), LambdaMART [6], and Rankformer [9], as LambdaMART is still considered the SOTA pairwise ranking method and Rankformer is the SOTA deep learning-based listwise ranking method. The benchmark models chosen include the best pure pairwise and pure listwise methods, which is sufficient to demonstrate the effectiveness of our approach. In practice, we replace the original prediction layer, which predicts the ranking scores for each entity, with different benchmark models to evaluate the performance achieved by this modification."}, {"title": "5.2 Datasets", "content": "Stock Ranking. The task of stock ranking focuses on accurately predicting and ranking stocks according to their anticipated future returns ratio, which aids investors in selecting stocks for investment purposes [17, 41, 53]. We use the dataset\u00b9 introduced by Feng et al. [17], which includes historical trading data from 2013 to 2017 for two significant markets, NASDAQ and NYSE, containing 1026 and 1737 stocks, respectively. We maintain a consistent setting for splitting the data into training, validation, and testing sets over a 3-year, 1-year, and 1-year period.\nInformation Retrieval. Information retrieval is another critical area where ranking models are extensively applied. The task of information retrieval focuses on accurately ranking documents based on their relevance to a given query, which is crucial for search engines. LambdaMART and Rankformer are both originally proposed for information retrieval task [6, 9]. We use the MQ2008-list\u00b2 dataset from Microsoft LETOR4.0 [38], a popular benchmark dataset for LETOR algorithms. We acknowledge the existence of more recent information retrieval datasets, but they only provide relevance-level labels without ordinal ranking labels, making them unsuitable for our model evaluation.\nHistorical Events Ordering. In the final task, we include a textual dataset called \"On This Day 2\u201d (OTD2)\u00b3 for chronologically ordering historical events. Originally, Honovich et al. constructed this dataset for a regression task to predict the year of occurrence [26]. However, it can also be approached as a ranking task to predict the chronological order of events within a given group of events. The OTD2 dataset, sourced from the \"On This Day\" website, includes 71,484 events enriched with additional contextual information. Scraped in April 2020, it incorporates recent events and corrections by the site. Preprocessing excluded events before 1 CE and future projections such as \"31st predicted perihelion passage of Halley's Comet\" in 2061 CE. We follow the 80%/10%/10% train/validation/test split as provided."}, {"title": "5.3 Backbones", "content": "Stock Ranking. We use the method by Feng et al. [17] as the backbone model, replacing the prediction layer with different benchmark ranking models. Although other potential methods such as STHAN-SR [42], ALSP-TF [48], and CI-STHPAN [51] have been proposed for stock selection, we do not include them due to the lack of last-state embeddings needed to integrate TSPRank, unavailability of source code, or reproducibility issues. Since our primary goal is to demonstrate TSPRank as a versatile ranking approach rather than achieving SOTA results on a single dataset, this selection of backbone is sufficient to validate our model's performance.\nInformation Retrieval. Given that the features in the MQ2008-list are encoded using a combination of BM25 and TF-IDF methods, we treat BM25 and TF-IDF as the backbone models and use these encoded features as embeddings."}, {"title": "5.4 Technical Setup", "content": "Stock Ranking. Considering that TSPRank specialises in small-scale ranking problems, we implement rankings within individual sectors, categorising stocks into groups based on their sectors. We exclude all sector groups containing three or fewer stocks and remove stocks with unidentified sector information. Detailed descriptions of the sector-specific grouping are provided in Appendix E. For the benchmarking process, we use outputs from the relational embedding layer introduced by Feng et al. [17], replacing the original prediction layer with the other benchmark models. This modification enables us to assess the extent to which transitioning from a point-wise (original model) to pairwise, listwise, and our hybrid approaches can enhance performance. All benchmark models and TSPRank with local learning are trained for 100 epochs for each sector, as 100 epochs are sufficient for the benchmark models to converge. We find that TSPRank with global learning generally requires more epochs to converge; therefore, TSPRank-global is trained for an additional 50 epochs to ensure convergence. Other hyperparameters are specified in Appendix B. For LambdaMART, we use 10,000 trees since previous experiments indicated that Rankformer failed to outperform LambdaMART with this configuration [9]. We choose the widely-used XGBoost5 [12] implementation of LambdaMART, especially due to the performance enhancements in its version 2.0.\nInformation Retrieval. In the MQ2008-list dataset, a query might contain over 1,000 documents, resulting in a graph that is too large for the TSP solver to handle efficiently. Therefore, we focus on a reranking or post-reranking stage. Specifically, we extract the top 10 and top 30 documents for each query to conduct two separate experiments. This approach allows us to manage the computational complexity while still evaluating the effectiveness of TSPRank in a realistic ranking scenario. We use similar hyperparameters as in the stock ranking task but increase the number of transformer layers in both Rankformer and TSPRank to four, as we do not use any pretrained embeddings here. To ensure fairness, we also follow the 5-fold cross-validation setup as provided by Microsoft.\nHistorical Events Ordering. We randomly allocate the events into ranking groups and train the ranking models to predict the ground truth order of occurrences within these groups. We test with group sizes of 10, 30 and 50 for fair comparisons. For each group size, we generate the group allocations using five different random seeds and report the average performance to mitigate the effects of random group allocation. Regarding the models, we observed overfitting"}, {"title": "5.5 Evaluation", "content": "Stock Ranking. Given that our task encompasses both ranking and stock selection functionalities, we employ both traditional ranking metrics and financial performance metrics in our evaluation. For ranking effectiveness, we use the Mean Average Precision at K (MAP@K), which evaluates the precision of the top K ranked stocks by averaging the precision scores at each relevant stock position. We also use Kendall's Tau, defined as: $\\tau = \\frac{C-D}{n(n-1)}$, where C is the number of concordant pairs (pairs of entities that are in the same order in both predicted and ground truth rankings), D is the number of discordant pairs (pairs of entities that are in different orders in the rankings), and n is the total number of entities being ranked. Kendall's Tau is a correlation coefficient that measures the ordinal association between two ranked lists. This is suitable in our case, where we need to assess the similarity between the predicted rankings and the ground-truth rankings. Furthermore, to assess the financial impact of the rankings, we simulate trading activities by predicting the rankings of all individual stocks in every sector for the next trading day t + 1 on trading day t. This simulation is consistent with the one presented by Feng et al. [17]. We then rank these stocks, hold the top k stocks $S_k$, and sell them on trading day t + 1. The price of stock i on day t is denoted as $p_i^{(t)}$ per share. The cumulative Investment Return Ratio (IRR) and Sharpe Ratio (SR) are defined in Equation 9, where $R_p$ is the return of the portfolio, $R_f$ is the risk-free rate, and \u03c3 is the standard deviation:\n$IRR_k = \\sum_{i \\in S_k} \\frac{p_i^{(t)} - p_i^{(t-1)}}{p_i^{(t-1)}}, SR = \\frac{E(R_p - R_f)}{\\sigma(R_p)}$.\nInformation Retrieval. We evaluate the performance of our models using several well-established ranking metrics: Normalized Discounted Cumulative Gain (NDCG) at various cutoff levels (3, 5, 10), Mean Reciprocal Rank (MRR), and Kendall's tau. NDCG measures a document's usefulness based on its position in the result list, with higher positions receiving more weight. On the other hand, MRR assesses the rank position of the first relevant document, calculated as the average of the reciprocal ranks of the first relevant document for each query. The formula for MRR is defined by MRR = $\\frac{1}{|Q|} \\sum_{i=1}^{|Q|} \\frac{1}{rank_i}$, where |Q| is the total number of queries and ranki is the rank position of the first relevant document for the i-th query."}, {"title": "6 RESULTS", "content": "The aggregated results for all benchmark models on the stock ranking dataset are presented in Table 1. These results represent the average performance across 50 sectors in the NASDAQ market and 70 sectors in the NYSE market, excluding sectors with three or fewer stocks as outlined in the experimental setup. Detailed results for each individual sector are provided in Appendix E. The results on the MQ2008-list retrieval dataset are shown in Table 2. A 5-fold cross-validation was performed, with detailed results for each fold provided in Appendix F. Lastly, the results on OTD2 for historical events ordering are presented in Table 3, with complete results for different random group allocations provided in Appendix G.\nThe results highlight the comparisons among three types of models: pairwise (LambdaMART), listwise (Rankformer), and our pairwise-listwise method (TSPRank).\nBetter Performance of Pairwise-Listwise Method Across Diverse Tasks (RQ1). Our pairwise-listwise method, TSPRank-Global, demonstrates outstanding robustness and superior performance across diverse datasets and domains. For instance, on the NASDAQ stock ranking dataset (Table 1), TSPRank-Global achieves a Kendall's Tau of 0.0447, significantly surpassing both LambdaMART (0.0071) and Rankformer (0.0110). Similarly, on the NYSE dataset (Table 1), TSPRank-Global attains a Kendall's Tau of 0.0422, outperforming LambdaMART (0.0054) and Rankformer (0.0181). Both TSPRank-Local and TSPRank-Global also consistently achieve higher IRR and SR, yielding improved financial performance. The model's robustness is further evidenced by its consistent top performance across other datasets. For example, in the MQ2008-list (Table 2), TSPRank-Global leads with NDCG@10 scores of 0.8884 for the top 10 documents and 0.7631 for the top 30 documents, indicating superior ranking accuracy, despite a minor difference in Kendall's Tau compared to Rankformer (around 0.01 to 0.02). Additionally, TSPRank-Global achieves top performance in the historical events ordering task (Table 3), regardless of the group size (10, 30, and 50).\nTSPRank Can Be Extra Component Upon Embeddings For Boosting The Performance. In the stock ranking and ordering tasks for historical events, we directly deployed TSPRank on the stock embeddings and text embeddings. As demonstrated, TSPRank effectively serves as an additional component on embeddings, significantly enhancing ranking performance. This capability allows TSPRank to have broader applications across various domains.\nStronger Robustness Towards Number of entities. TSPRank-Global also shows greater robustness concerning the number of entities compared to the pure pairwise LambdaMART and listwise Rankformer. As evidenced by the MQ2008 (Table 2) results, the gap between LambdaMART and Rankformer narrows as the number of documents increases from 10 to 30, with the NDCG@3 score difference between these two methods reducing from -0.0387 to -0.0146."}, {"title": "7 VISUALISATION ANALYSIS", "content": "As mentioned before, our pairwise-listwise TSPRank addresses both the lack of listwise optimisation in pairwise LambdaMART and the robustness issues in listwise Rankformer, which arise from attempting to predict the complete rankings directly."}, {"title": "8 INFERENCE LATENCY", "content": "Incorporating a combinatorial optimisation solver is known to cause additional computational overhead, which can impact the inference latency. Therefore, we compare the average inference time per ranking group for TSPRank and Rankformer on the OTD2 historical events ordering dataset. The experimental setup and detailed results are provided in Appendix D.\nWe observe that the inference time for TSPRank increases exponentially with the number of ranking entities, primarily due"}, {"title": "9 CONCLUSION", "content": "This study introduces TSPRank, a novel pairwise-listwise approach for position-based ranking tasks, by modelling them as the Travelling Salesman Problem (TSP). We present two learning methods for TSPRank, integrating listwise optimization into pairwise comparisons to address the limitations of traditional pure pairwise and listwise ranking models. TSPRank's application to diverse backbone models and datasets, including stock ranking, information retrieval, and historical events ordering, demonstrates its superior performance and robustness. Key findings include TSPRank's ability to outperform existing models such as LambdaMART and Rankformer across multiple datasets and various metrics. Furthermore, TSPRank's capability to function as an additional component on embeddings suggests its versatility and potential for broader applications across different domains.\nOur empirical analysis reveals that TSPRank's superior performance is due to the discrete TSP solver's ability to more effectively utilise listwise information. It also enhances TSPRank's tolerance to uncertainties in pairwise comparisons by ensuring the selection of the highest-scoring permutation. Future work could explore the use of alternative TSP solvers and further enhancements to the Gurobi solver to reduce inference latency and improve scalability."}, {"title": "A TSP OPTIMISATION PROBLEM", "content": "Ranking entity ej immediately after entity ei is analogous to travelling from city i to city j. Let N be the total number of entities to be ranked and zi variables to represent the number of entities ranked before entity i. The MILP formulation of the TSPRank inference is as follows:\n$\\min \\sum_{i=1}^{N} \\sum_{j=1,j\\neq i}^{N} a_{ij}x_{ij}$\n$\\text{s.t.} \\sum_{j=1,j\\neq i}^{N} x_{ij} \\leq 1 \\quad \\text{for all } i$ \n$\\sum_{i=1,i+j}^{N} x_{ij} \\leq 1 \\quad \\text{for all } j$\n$\\sum_{i=1}^{N} \\sum_{j=1,j\\neq i}^{N} x_{ij} = N - 1$\n$z_i + 1 \\leq z_j + N(1 - x_{ij}) \\quad i, j = 2, . . ., N, i \\neq j$\n$z_i \\geq 0 \\quad i=2,...,N$\nConstraints (11) and (12) ensure that each entity has at most one predecessor and one successor in the ranking, expressed as 2N - 2 linear inequalities. Constraint (13) ensures that the total number of pairwise comparisons is exactly N \u2013 1. These constraints ensure that the selected set of pairwise comparisons forms a valid ranking sequence. Constraints (14) and (15) introduce variables z to eliminate multiple separate sequences and enforce that there is a single, complete ranking that includes all entities."}, {"title": "B HYPERPARAMETERS SETTING", "content": "The hyperparameters are specified in Table 5. In the table, several abbreviations are used for conciseness and clarity:\n\u2022 Ir: Learning rate, which controls the step size during the optimization process.\n\u2022 tf layer: Number of transformer layers, indicating the depth of the transformer model.\n\u2022 tf nheads: Number of attention heads, which specifies the number of parallel attention mechanisms within the transformer layer.\n\u2022 tf dim_ff: Dimensionality of the feed-forward network within the transformer layer."}, {"title": "C ADDITIONAL VISUALISATION EXAMPLES", "content": "We further construct three more groups using events from different countries. The event titles, labels, and true rankings are provided in Table 6. The visualisation of the models' predictions are shown in Figure 4. We observe similar patterns to those discussed in Section 7."}, {"title": "D LATENCY ANALYSIS DETAILS", "content": "We conduct an inference time analysis on a standalone device with an AMD Ryzen 7 5800X 8-Core CPU and an Nvidia RTX 4070 Super GPU. Using the standalone device instead of the one used for training is to avoid interference from other running jobs on the shared computing node."}]}