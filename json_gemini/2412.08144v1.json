{"title": "AGMixup: Adaptive Graph Mixup for Semi-supervised Node Classification", "authors": ["Weigang Lu", "Ziyu Guan", "Wei Zhao", "Yaming Yang", "Yibing Zhan", "Yiheng Lu", "Dapeng Tao"], "abstract": "Mixup is a data augmentation technique that enhances model generalization by interpolating between data points using a mixing ratio \\(\\lambda\\) in the image domain. Recently, the concept of mixup has been adapted to the graph domain through node-centric interpolations. However, these approaches often fail to address the complexity of interconnected relationships, potentially damaging the graph's natural topology and undermining node interactions. Furthermore, current graph mixup methods employ an one-size-fits-all strategy with a randomly sampled \\(\\lambda\\) for all mixup pairs, ignoring the diverse needs of different pairs. This paper proposes an Adaptive Graph Mixup (AGMixup) framework for semi-supervised node classification. AGMixup introduces a subgraph-centric approach, which treats each subgraph similarly to how images are handled in Euclidean domains, thus facilitating a more natural integration of mixup into graph-based learning. We also propose an adaptive mechanism to tune the mixing ratio \\(\\lambda\\) for diverse mixup pairs, guided by the contextual similarity and uncertainty of the involved subgraphs. Extensive experiments across seven datasets on semi-supervised node classification benchmarks demonstrate AGMixup's superiority over state-of-the-art graph mixup methods. Source codes are available at: https://github.com/WeigangLu/AGMixup.", "sections": [{"title": "Introduction", "content": "Graph Neural Networks (GNNs) have emerged as a dominant paradigm for learning on graph-structured data, driving significant advances in various graph-based tasks, notably node classification. Despite these successes, a fundamental challenge remains in how to effectively generalize from limited or sparsely labeled data, a common scenario in node classification. Mixup (Zhang et al. 2017), an effective data augmentation method originally devised for Euclidean data, enriches the training dataset by linearly interpolating between pairs of labeled samples (a mixup pair) and their corresponding labels with a mixing coefficient \\(\\lambda\\). While mixup has achieved notable success in Euclidean data domains characterized by regularity, its application to the complex, non-Euclidean nature of graph data encounters two significant questions.\nQuestion 1: How can the mixup concept be seamlessly integrated into graphs? Existing graph mixup methods, focusing on node-centric interpolations within a graph, extend the mixup method from Euclidean spaces to graphs by blending features, labels, and connections of node pairs (Lu et al. 2024). However, these methods overlook the complexity of interconnected relationships, potentially altering the graph's natural topology and undermining the node interactions. In contrast to the isolation of images in mixup as shown in Fig. 1 (left), where interpolations do not influence other data points, graph data's interconnected nature demands an approach to preserve its topological integrity.\nQuestion 2: How can the mixing ratio (\\(\\lambda\\)) be adaptively controlled for different mixup pairs? Prevailing graph mixup techniques often employ an one-size-fits-all strategy to control the mixing ratio, applying a randomly sampled \\(\\lambda\\) across all mixup operations. This approach can inadvertently generate synthetic nodes that starkly contrast with realistic distributions within the graph, especially when merging highly dissimilar node pairs with a mid-range \\(\\lambda\\). Moreover, this static strategy fails to encourage the model to explore less represented data areas, critical for more confident predictions.\nPresent Work. In response to these challenges, we pro-"}, {"title": "Related Work", "content": "Mixup on Images. Mixup (Zhang et al. 2017), introduced by Zhang et al., generates virtual training data by linearly interpolating the features and labels of image pairs using a ratio \\(\\lambda\\) sampled from a Beta distribution. This concept has been expanded into the hidden space by methods like ManifoldMixup (Verma et al. 2019) and PatchUp (Faramarzi et al. 2022), which apply interpolation to hidden representa-tions to enhance sample diversity. Building upon this foundation, adaptive mixing policies such as PuzzleMix (Kim, Choo, and Song 2020), SaliencyMix (Uddin et al. 2021), AutoMix (Liu et al. 2022), SuperMix (Dabouei et al. 2021), and Decoupled Mixup (Liu et al. 2024) have been developed to refine the generation of mixed samples, tailoring the process to the specific characteristics of input data. Theoretical and empirical investigations (Zhang et al. 2021, 2022) into mixup's effects on model generalization and calibration have further demonstrated its benefits, offering deep insights into mixup methods.\nMixup on Graphs. Extending mixup to the graph domain has opened new avenues for enhancing graph-level tasks, with several studies (Ma et al. 2024; Crisostomi et al. 2022a; Han et al. 2022; Guo and Mao 2021; Navarro and Segarra 2022; Park, Shim, and Yang 2022; Crisostomi et al. 2022b) introducing mixup techniques to facilitate these tasks. For graph-level tasks, the integrity of the graph structure is not tied to specific nodes but to the overall graph. As a results, these methods might not scale efficiently when applied directly to node-level tasks since each node requires consideration of its unique neighborhood, which isn't necessarily the focus of whole-graph mixup methods. To address this challenge, (Wang et al. 2021) proposes a method that mixes the input features of pairs of nodes and then blends their aggregated representations at each GNN layer. GraphMixup (Wu et al. 2021) incorporates a reinforcement mechanism to dynamically adjust the scale of mixup pairs, aiming to alleviate the class-imbalanced node classification problem. GraphMix (Verma et al. 2021) applies mixup to the hidden representations of nodes and feeds the mixed nodes into fully connected layers. NodeMixup (Lu et al. 2024) introduces inter- and intra-class mixup techniques for both labeled and unlabeled nodes, addressing the issue of under-reaching. iGraphMix (Jeong et al. 2024) focuses on mixing the features and connections of labeled nodes on the input graph.\nComparison to SOTA Graph Mixup. In Fig. 2, we illustrate the comparison between three SOTA graph mixup methods, i.e., iGraphMix, NodeMixup, GraphMix, and our AGMixup. Both iGraphMix and NodeMixup extend mixup operations to node connections within the graph, risking"}, {"title": "Methodology", "content": "In this section, we begin by revisiting Graph Neural Networks (GNNs) and the mixup technique. Subsequently, we introduce our proposed method, AGMixup, which comprises three key modules: subgraph-centric mixup, contextual similarity-aware \\(\\lambda\\) initialization, and uncertainty-aware \\(\\lambda\\) adjustment. Finally, we present a comprehensive case study to evaluate the effectiveness of AGMixup and analyze its computational complexity.\nNotations. We define an undirected graph with self-loops, \\(G = {V,E}\\), where \\(V = {x_1,\\dots,x_N}\\) represents the set of \\(N\\) nodes, and \\(E\\) denotes the edge set with element \\(e_{ij}\\) indicating a connection between node \\(i\\) and node \\(j\\). The input feature matrix \\(X \\in \\mathbb{R}^{N\\times F}\\), with its \\(i\\)-th row vector \\(x_i\\), where \\(F\\) represents the input dimensionality. Besides, the label matrix \\(Y \\in \\mathbb{R}^{N\\times C}\\), where \\(C\\) is the number of classes, includes \\(y_i\\) as the one-hot encoding of node \\(i\\)'s label.\nGNN Revisit. Given a node \\(i\\) with its feature vector \\(x_i\\), a GNN computes the node's embedding \\(h_i^{l+1}\\) through a series of layer-wise propagation rules. At each layer \\(l\\), the embedding \\(h_i^{l+1}\\) is updated as \\(h_i^{l+1} = \\sigma(W^{(l)}\\cdot AGGREGATE({h_j^{l} \\in N_i}))\\), where \\(\\sigma\\) is a non-linear activation function, \\(W^{(l)}\\) is the learnable parameter of \\(l\\)-th layer, and \\(AGGREGATE\\) is a function that combines the embeddings of node \\(i\\)'s neighbors \\(N_i\\). The initial embedding \\(h_i^{(0)}\\) is set to the node features, i.e., \\(h_i^{(0)} = x_i\\). Upon completion of \\(L\\) layers of propagation, the final embedding \\(h_i^{(L)}\\) is then passed through a classification layer to obtain the predictions for each node \\(p_i = softmax(W^{(L)}h_i^{(L)})\\), where parameters \\(W^{(L)}\\) maps the final embedding into the prediction space and the softmax function ensures the output \\(p_i\\) represents a probability distribution over the \\(C\\) classes.\nMixup Revisit. The traditional mixup method is a data augmentation technique that creates virtual training samples by linearly interpolating between pairs of examples. Given two mixup candidates \\(x_i\\) and \\(x_j\\) from the labeled data \\(D\\), the mixup data \\(\\tilde{x}_{ij}\\) can be generated as follows:\n\\begin{equation}\\label{eq:1} \\tilde{x}_{ij} = M(x_i, x_j, \\lambda) = \\lambda x_i + (1 - \\lambda) x_j, \\end{equation}\nwhere \\(M(\\cdot)\\) is a mixup function. Here, \\(\\lambda\\) is a mixing coefficient drawn from a Beta distribution \\(Beta(\\alpha, \\alpha)\\) with \\(\\alpha > 0\\). The value of \\(\\lambda\\) controls the degree to which they are mixed. In the context of a general classification task, we regard the mixed data as additional training data with mixed labels.\nProposed Method: AGMixup\nSubgraph-Centric Mixup. Inspired by image mixup techniques, we treat subgraphs induced from nodes as independent samples since a subgraph encapsulates a node's local structure and semantic context. For a node \\(i\\), its \\(r\\)-ego graph \\(G_i^{(r)} = {V_i^{(r)}, E_i^{(r)}}\\) includes all nodes and edges within \\(r\\) hops. Our AGMixup innovatively proposes the mixing of such subgraphs, yielding a mixed graph \\(G_{ij}\\) formulated as:\n\\begin{equation}\\label{eq:2} G_{ij} = M_G(G_i^{(r)}, G_j^{(r)}, \\lambda_{ij}) = {V_{ij}^{(r)}, E_{ij}^{(r)}}, \\end{equation}\nwhere \\(M_G(\\cdot)\\) is a mixup function for the graph. The mixed node set \\(V_{ij}^{(r)}\\) contains all the nodes from both \\(G_i^{(r)}\\) and \\(G_j^{(r)}\\), excluding \\(x_i\\) and \\(x_j\\). Instead, a virtual node \\(\\tilde{x}_{ij} = M(x_i, x_j, \\lambda_{ij})\\) is introduced, where \\(\\lambda_{ij}\\) is dynamically computed through our adaptive mechanism (detailed later). The mixed edge set \\(E_{ij}^{(r)}\\) includes connections from the virtual node \\(\\tilde{x}_{ij}\\) to the neighbors of both \\(x_i\\) and \\(x_j\\). It preserves the structural connectivity inherent to each original subgraph but also facilitates the integration of node features from the two distinct subgraphs. By doing so, AGMixup maintains the graph's structural integrity and leverages the combined features to enhance learning efficacy, mirroring how traditional image mixup enhances model generalization by interpolating between distinct image samples.\nContextual Similarity-aware \\(\\lambda\\) Initialization. A fixed \\(\\lambda\\) drawn from \\(Beta(\\alpha, \\alpha)\\) applies the same degree of interpolation across all sample pairs, potentially limiting the model's ability to generalize from the mixed samples without an appropriate \\(\\alpha\\). A too small \\(\\alpha\\) predominantly yields \\(\\lambda\\) values near the extremes (close to one or zero), generating virtual samples overly similar to original samples. On the contrary, a larger \\(\\alpha\\) biases \\(\\lambda\\) towards 0.5. However, such uniform blending could result in unrealistic feature combinations when mixing highly dissimilar samples, diverging from the natural data distribution. To this end, we propose a similarity-aware \\(\\lambda\\) initialization mechanism:\n\\begin{equation}\\label{eq:3} \\lambda_{ij}^{(0)} = 0.5 *exp(-\\gamma ||\\bar{h}_i^{(r)} - \\bar{h}_j^{(r)}||^2), \\end{equation}\nwhere \\(\\bar{h}_i^{(r)}\\) is the mean encoded embedding vector by a GNN model of all the nodes in \\(G_i^{(r)}\\) and \\(\\gamma > 0\\) adjusts the sensitivity of mixup to contextual similarity. Eq. (3) enables more exploratory mixing only for subgraphs that are similar in hidden space by initializing \\(\\lambda_{ij}\\) value close to 0.5."}, {"title": "Sensitivity to Similarity and Uncertainty", "content": "In Fig. 10, we analyze the effect of two critical hyperparameters, \\(\\gamma\\), and \\(\\beta\\), on the performance of various GNNs across three datasets: Cora, Citeseer, and Pubmed. \\(\\gamma\\), which adjusts the sensitivity to contextual similarity, shows that a moderate increase generally leads to peak performance in accuracy, which then stabilizes or slightly declines as \\(\\gamma\\) continues to increase. This trend indicates that while sensitivity to dissimilarity helps in avoiding overly exploratory mixup operations for subgraphs that are not similar, too high a \\(\\gamma\\) might restrict the beneficial aspects of mixup by being overly conservative. \\(\\beta\\), which modulates sensitivity to the uncertainty difference between subgraphs, often shows an improvement in performance up to a certain point. The results suggest that while initially, the increasing \\(\\beta\\) is beneficial to addressing the uncertainty effectively by adapting the mixup operation, excessively high values might lead to diminishing returns, possibly due to overemphasis on uncertain and less informative regions."}, {"title": "Additional Results", "content": "Due to the page limit, we provide some results in Appendix: (1) a paired T-Test between AGMixup and other methods; (2) results on more advanced GNNs, i.e., GCNII (Chen et al. 2020), APPNP (Klicpera, Bojchevski, and G\u00fcnnemann 2019), GPRGNN (Chien et al. 2021), and GRAND (Feng et al. 2020); (3) results on fewer labeled nodes; (4) results on ogbn-products; (5) performance comparison against other graph augmentation methods, i.e., DropEdge (Rong et al. 2019) and PairNorm (Zhao and Akoglu 2019)."}, {"title": "Algorithm", "content": "We provide the algorithm of our AGMixup in Algorithm 1.\nAlgorithm 1: AGMixup's Training Algorithm\nInput: Original graph \\(G = {V, E, X}\\), gnn model \\(g_\\theta\\)\nParameters: Similarity sensitivity ratio \\(\\gamma\\), uncertainty sensitivity ratio \\(\\beta\\), subgraph size \\(r\\), number of epochs \\(T\\)\n1: Initialize model parameters \\(\\theta\\)"}]}