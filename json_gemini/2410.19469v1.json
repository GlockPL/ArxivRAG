{"title": "Unified Causality Analysis Based on the Degrees of Freedom", "authors": ["Andr\u00e1s Telcs", "Marcell T. Kurbucz", "Antal Jakov\u00e1ca"], "abstract": "Temporally evolving systems are typically modeled by dynamic equations. A key challenge in accurate modeling is understanding the causal relationships between subsystems, as well as identifying the presence and influence of unobserved hidden drivers on the observed dynamics. This paper presents a unified method capable of identifying fundamental causal relationships between pairs of systems, whether deterministic or stochastic. Notably, the method also uncovers hidden common causes beyond the observed variables. By analyzing the degrees of freedom in the system, our approach provides a more comprehensive understanding of both causal influence and hidden confounders. This unified framework is validated through theoretical models and simulations, demonstrating its robustness and potential for broader application.", "sections": [{"title": "Introduction", "content": "Studying causal interactions between systems through observations of the evolution of their variables has recently become an active research area. Reichenbach's common cause principle, when adapted to stochastic dynamic systems (cf. [1]), states that if two subsystems are not independent, then they share a common cause. The original"}, {"title": "Basic Definitions", "content": "In this paper, we focus on deterministic and stochastic dynamical systems on finite or infinite discrete or continuous spaces. The dynamic variable is denoted by x(t) \u2208 M, where M = Rd or Zd. A deterministic dynamical system in continuous time is defined by the differential equation:\nfcont,i (x(x), \u00a7) = 0, (1)\nwhere x(x) represents the tuple (x1,...,xd), with\n$\\frac{d^{oi} x_i}{dt^{oi}} = x_i$, (2)\nThe quantities o\u00bf \u2265 1 are the highest derivatives appearing in the differential equation for the ith component. The range of the function fcont is M, and it describes the time evolution of the ith component. The stochastic components, denoted by \u00a7, represent white noise variables, and their actual values are independent of the entire past of x. In the deterministic case, the process is constant."}, {"title": "2.1 Degrees of Freedom", "content": "We are now in a position to define the number of degrees of freedom:\nDefinition 1 The number of degrees of freedom df for a given component xi, denoted by dfi, is the number of initial conditions that must be specified to determine the complete time evolution of that component, given a stochastic history.\nAs seen, df is defined for individual components. If all components are interconnected, then, as shown in (6), the number of values required to continue the recursion is equal to the sum of the order number of the components, giving df = =10; for"}, {"title": "2.2 Assumptions", "content": "To avoid complications, we assume that in causal relationships, the cause precedes the effect by one unit of time. Multi-lag cases can be treated using longer time-delay embeddings, but the technique remains the same. For a detailed discussion of various lag scenarios, see [45]. For cases involving separate external drives for X and Y, as well as an i.i.d. common driver, we refer to [46].\nThe information transfer in recursions can be represented as a directed graph where vertices correspond to variables at each time step, and edges are time-invariant. This condition implies that the subject of our investigation is the so-called \"summary graph\". We assume that the directed graph is acyclic (cf. [11]). Additionally, we assume that the dynamical systems are irreducible (i.e., no autonomous subsystems) and converge to a stationary state."}, {"title": "3 Causal Relations", "content": "Here, we present the mathematical framework that supports the statements made in the previous section."}, {"title": "3.1 Definitions", "content": "Definition 2 We say that X drives Y if they satisfy the following recursion relations:\nXn = Fn(Xn-n)\nYn = Gn(Xn-1, Yn-1\n(12)"}, {"title": "4 Comparison of Methods", "content": "Table 4 lists the possible causal relations between two systems and compares the application domains and conclusions provided by Wiener-Granger Causality (WGC), Convergent Cross Mapping (CCM), the Dimensional Causality (DC) method, and the method proposed in this paper (df).\nIn Table 4, the column headers represent the methods, where \"st\" stands for stochastic systems and \u201cdet\u201d for deterministic systems. The Wiener-Granger Causal-ity (WGC) indicates a bidirectional causal relation if there is a common cause or confounder.\nThe CCM method, which belongs to the family of topological causality approaches, is based on Takens' embedding theory (cf. Sugihara [48] and Harnack [49]). When correlation is observed without inferred direct causation, it suggests the existence of a hidden common driver. However, it's essential to recognize that a shared driver doesn't always lead to a linear correlation between the two influenced systems. In [48], no quantitative evaluation is provided. Suppose other measures of commonality"}, {"title": "5 Method: practical approach to the df-causality method", "content": "In the previous sections, we defined the concept of the number of degrees of free-dom (df) and demonstrated its utility as a powerful method for uncovering causal relationships.\nThis section provides a practical approach for determining df, followed by a demonstration of its application through a semi-analytic example and a simple real-life data case."}, {"title": "5.1 Determining the Number of df", "content": "The determination of the number of degrees of freedom requires fixing specific val-ues in the past of a subsystem. After setting the df values, the distribution of the subsystem's present values becomes independent of the number of applied constraints.\nIn practice, it is impossible to constrain a variable to an exact value because the probability of finding a data series with an exact value is zero. Therefore, constraints must be applied over a range, or more generally, all configurations are weighted by some distribution over past elements. The constraints we impose take the form\nC(n) = d(xn-k - )Pk(\u03be), (14)\nwhere Pk represents the distribution. If Pk is sharp, Xn\u2212k is fixed to a specific value. If Pk is a window function with width \u03c3w, Xn-k is constrained to a certain range. In general, Pk can represent any distribution with variance \u03c3w. We are interested in the distribution of xn after imposing these constraints:\np(xn | C(n),..., C(n)) = N\n(xn) \u03a0\u03b4 (Xn-ka \u2013 \u03be\u03b1) \u03a1\u03ba\u03b1 (\u03be\u03b1) \u03ac\u03be\u03b9... \u03ac\u03be\u03b1, (15)\nwhere N is a normalization factor.\nBeyond a certain number of constraints, the resulting distribution becomes inde-pendent of additional constraints. Although in principle we could apply all kinds of constraints, in practice, we typically use general ones, such as a zero-mean uniform window function with width ow or a Gaussian distribution.\nWhile it is useful to study the entire distribution, for practical purposes, we often compute only the variance \u03c3. Though it is conceivable that the distribution might change without affecting its variance, it is unlikely that it remains unchanged under different numbers of constraints and varying window widths. Therefore, variance can serve as a reliable proxy for evaluating changes in the distribution.\nTo understand how the p(xn) distribution behaves under various constraints, we study the double-parameter dependent variance \u03c3(\u03c3w,nconstr), where \u03c3w represents the variance of the constraint distribution and nconstr denotes the number of con-straints. This function offers a straightforward approach to deduce the number of degrees of freedom. If insufficient constraints are applied, the limit \u03c3w \u2192 0 depends on the number of constraints. By observing the \"saturation\" of o as a function of \u03c3w \u2192 0, the number of degrees of freedom is revealed. The number of saturated lines corresponds to the df of the component.\nThis method will be demonstrated in the following sections, using a semi-analytic example and a simple real-life data example."}, {"title": "5.2 Semi-analytic Example: Linear System with Noise", "content": "Since only linear systems can be solved analytically, we select a linear stochastic recursion for demonstration. Nevertheless, the general principles remain valid, as the method does not rely on the linearity of the equations.\nWe consider a stochastic recursion of the form\nXn = QXn-1 + S\u03be\u03b7, (16)\nwhere xn \u2208 RN, and \u00c9n are i.i.d. normal random variables in RN. The matrices Q and S are of dimensions RN \u00d7 RN. We begin the evolution from an initial condition xo, and the distribution of xn follows a Gaussian with mean In and correlation matrix Cn.\nFor the expected value In = E[xn], we have\nIn = QIn-1. (17)\nWe assume that In 0 after a sufficiently long time, as this is not central to our analysis.\nThe evolution of the correlation matrix is given by\nCn = E[xn Xn] = QCn-1QT + SST. (18)\nThe stationary solution satisfies\nC = QCQT + SST. (19)\nIts solution is\nC = \u2211QSSTQTn. (20)\nThis series converges for generic S only if |1i| < 1 for all eigenvalues of Q. This condition also ensures that the process becomes independent of the initial conditions, i.e., In\u21920 as n \u2192 \u221e."}, {"title": "5.3 Conditional Probability Distributions", "content": "In the proposed method, we compute empirical conditional probability distribution functions, where conditions are imposed on certain components. For linear systems, constraints do not alter the normal distribution, and the conditional distribution remains Gaussian with a modified covariance matrix and mean.\nIn stochastic systems, conditions typically involve both dynamic variables and noise terms. Let the dynamics be described by x \u2208 RN and the noise by \u015a\u2208 IRNnoise,"}, {"title": "5.4 Example System", "content": "To demonstrate our method, we use a system with six components, represented by the matrix Q:\n\n\n\n\n\n\n\n\n(25)\nwhere\nCa = la COS(a), Sa = aa sin(a), a = x, y or z. (26)\nThe noise correlation matrix is chosen to be proportional to the identity matrix, S = \u03c31.\nThis system consists of three subsystems: the x, y, and z subsystems, each having two internal degrees of freedom. The z subsystem operates independently with a closed dynamic. The dynamics of x and y are influenced by z, but they do not interact directly with each other.\nUsing our earlier terminology, z is an independent subsystem that drives both x and y, and x and y share a common cause:\n2 x, zy, 3z x $y (27)\nRegarding the number of degrees of freedom, to fully determine the time evolution in z (given a stochastic history), it is sufficient to fix the two internal components of Zn-1, meaning the df is 2 (one pair). For x, it is not enough to fix its internal components alone, as it interacts with the z subsystem. To determine the time evolution of x, we need to fix both its internal components and the z initial conditions. Alternatively, two pairs in the past of x can be fixed, which indirectly provides all the required information. The y subsystem behaves analogously to x. In summary:\ndfx = dfy = 4, df = 2. (28)\nHowever, if we consider the (x, y) subsystem, we find that it has a total of 6 df (the entire system). Thus, we conclude that dfx+dfy > dfxy > dfx, dfy, indicating the presence of a common cause.\nWe will reveal the same information from a purely data-based analysis."}, {"title": "5.4.1 Numerical Setup", "content": "In the numerical examples, we used two parameter sets. In the \"deterministic\" set, a small noise term was applied to simulate a deterministic system with minimal"}, {"title": "5.4.2 Observing the z Subsystem", "content": "After setting up the system, we can perform observations. The simplest case is observing one of the x, y, or z subsystems.\nIf we observe only the z subsystem with different numbers of constraints and observation window widths, we obtain the plots in Fig. 1."}, {"title": "5.4.3 Observing the x Subsystem", "content": "We can perform the same observations with the x subsystem. Below are the plots where the observed distribution width is plotted against the constraint window size (bin size), see Fig. 2.\nThe primary difference between the z and x subsystems is that in the case of x, a single constraint on a past pair is insufficient to fully determine the observed value. Instead, at least two pairs of initial conditions are needed. This is clearly seen in the deterministic case (left panel of Fig. 2), where two stabilized width lines do not converge to zero, indicating a df of 4 (two pairs).\nIn the stochastic case (right panel), we can never observe a fully deterministic system (i.e., a Dirac delta distribution) due to the stochasticity. This is evident from the absence of lines that go linearly to zero as \u03c3w \u2192 0. Instead, all lines flatten below a certain constraint window width (bin size)."}, {"title": "5.4.4 Observing the x-z Subsystem", "content": "When we observe both the x and z components, we can perform all the previous measurements, but we can also carry out new ones.\nThe goal of these new measurements is to determine the extent to which the x-z subsystem depends on external drivers. To do this, we can attempt to fix past values for both the x and z pairs. The corresponding figure can be seen in Fig. 3.\nThese seemingly simple plots convey an important message: the x-z subsystem is closed. By fixing a single value in the past, it is sufficient to fully determine the system (given a stochastic history). This confirms that the x-z system is closed, with"}, {"title": "5.4.5 Observing the x-y Subsystem", "content": "The most intriguing case arises when we observe the x-y subsystem. In this scenario, we can replicate the earlier experiment, constraining the x subsystem and analyzing the observed distribution width of the x subsystem. A similar procedure can be applied to the y subsystem, yielding analogous results. As a result, we determine that dfx = dfy = 4.\nThe more interesting question concerns the df of the combined x-y subsystem. To investigate this, we adopted a specific constraint scheme:\n\u2022 For Nconstr = 0, no constraints are applied.\n\u2022 For Nconstr = 1, the penultimate value of the y subsystem (i.e., the yn\u22121 pair) is constrained.\n\u2022 For Nconstr > 1, in addition to the above, Nconstr 1 values of the x subsystem (i.\u0435., Xn\u22121,..., Xn\u2212k for k = 0, 1, . . ., Nconstr \u2013 1) are constrained.\nIn this scheme, we examine the variance of the y subsystem (we could also examine the x subsystem with analogous results). The results are presented in Fig. 4.\nBoth the deterministic and stochastic cases reveal that the x-y system has a df of 6 (three pairs). Recall that dfx,dfy \u2264 dfx,y \u2264 dfx+dfy, where equality on the left-hand side holds if at least one driving interaction occurs between the systems, and equality on the right-hand side holds if the systems are independent. If neither holds, then"}, {"title": "6 Real-World Example: Chickens and Eggs", "content": "To demonstrate that our method works on fully experimental data, we analyzed the \"chicken and egg\" problem [52]. The data describe egg production and the estimated chicken population in the US, based on U.S. Department of Agriculture data, for the period 1930-1980.\nTo prepare the data for analysis, we differentiated both the chicken and egg datasets and normalized them to have unit variance. The resulting data is shown in Fig. 5."}, {"title": "7 Conclusions", "content": "Several frameworks exist for time series causality analysis, including Granger causal-ity, topological causality, and information causality, as well as variations and exten-sions like PCMCI and LPCMCI. These are among the most prominent methods (see [54], [48], [55], [56]). Most of these frameworks, however, focus exclusively on ei-ther deterministic or stochastic systems. Furthermore, only a few methods attempt to reveal the presence of hidden confounders, such as common drivers or common causes (see [50], [51], [21], [7]). In this work, we have introduced a unified theoretical framework capable of addressing both challenges. At the core of this framework is the concept that investigating the degrees of freedom of a system, based on observed data, provides key insights. The degrees of freedom represent the minimum number of intermediate states required to make the past and future conditionally independent.\nWe also introduced an analytically tractable toy model. This model allows us to calculate the theoretical predictions of our method, simulating a scenario with an infinite time series, which provides optimal inference. To validate our theoretical framework and method, we simulated this toy model and calculated the degrees of freedom, while also performing robustness tests with simulated measurement errors. The results aligned perfectly with our theoretical expectations.\nFinally, we applied our method to the millennia-old question: \"Which came first, the chicken or the egg?\" Our findings confirm that the egg came first, as suggested by previous work [52].\nWe believe that our method offers a valuable new tool for research in various fields. However, we recommend that the method be further tested on more synthetic"}, {"title": "A Causality and Time-Reversal Symmetry", "content": "The issue lies with time-reversal symmetry: in most microscopic physical systems, the dynamics are invariant under the reversal of the direction of time (time reflection symmetry\u00b3). This implies that the concepts of \"earlier\" or \"later\" states are mean-ingless, and thus, the notion of \u201ccausation\u201d becomes irrelevant. For example, if a change in variable A causes a subsequent change in variable B, then by time-reversal symmetry, a change in B would also cause a later change in A. As a result, their re-spective causal relationships cannot be defined. This sharply contrasts with causality in real life, where rain causes wet soil, but wet soil does not cause rain.\nThis situation closely resembles the problem of the arrow of time, which is also meaningless in a time-reversal invariant system. The resolution of the apparent con-tradiction between the time-reversal symmetric microscopic world and the macro-scopic world, which exhibits a well-defined arrow of time, lies in the fact that in the macroscopic world, only collective phenomena are observed. These phenomena ex-hibit large values only in specific configurations with particular correlations between individual modes, and they dissipate as the system approaches equilibrium. This implies that individual collective coordinates decrease over time, even though the complete system remains time-reversal invariant. Naturally, by observing the entire system, it is possible to reverse this dissipation by gathering tiny pieces of informa-tion distributed across many variables. However, if measurements are made with only finite precision, information is lost, and the original state cannot be restored.\nThus, the philosophical foundation of both the arrow of time and causality lies in our restricted measuring ability, which encompasses both the number of observed"}, {"title": "B Calculations in the Linear Example", "content": "Here, we compute the distribution function of a Gaussian-distributed random variable with zero mean and a covariance matrix C, under a linear constraint\nRx \u2212 z \u2212 U\u00a7 = \u03b7. (33)\nHere R is an M \u00d7 N matrix, U is an M \u00d7 Nnoise matrix and \u03b7 is a normal random variable with zero mean and some covariance matrix Cw (referred to as 'W' for window), which simulates the role of finite bin size.\nIn the stationary case, the variables x have a covariance matrix C, so the condi-tional distribution function is\np(x) ~ (34)\nWe can simplify this formula by introducing new variables. For convenience, we use the following transformation:\n\u03be = UT (UUT)\u2212\u00b9\u00a2 + U\u00cf\u00a21. (35)\nHere, \u03da has M components, \u015a\u2081 has Nnoise \u2013 M components, and we choose U\u2081 such that UUT = 0 (i.e., U\u2081's columns are orthogonal to those of U). If the rows of U are independent, UUT is invertible, and if the rows of U\u2081 are linearly independent, the combined matrix (UT, UT) is also invertible. This transformation provides a bijection \u00a7 \u2192 (\u03b6, \u03b6\u2081). With the new coordinates, the expression becomes:\n(UUT)\u221215\u2212157S_STS1\np(x) ~ dnd de\u00af\u00f7\u00a4\u00a4\u00a4-\u00b9\u00a4-\u00a6\u00bd\u00a1T(UUT)-15-1578_8761-77Cw18 (Rx-z-5-7). (36)\nThe integral over (\u2081 can be performed without affecting the result, and we are left with:\np(x) ~ 1 (37)\nThis shows that the role of the window function and the stochastic noise are exactly the same."}, {"title": "B.2 Fixing Past Elements", "content": "When the constraints come from fixing the past elements of the In series, we can derive a specific formula. The condition is:\nXn-k,a Zka + Nka, (46)\nwhere we express Xn\u2212k,a in terms of xn and the noise terms using the equation of motion. This yields:\nzka + Nka = (Hkxn \u2013 HkS\u00a7n \u2013 Hk-1S\u00a7n\u22121 - \u00b7\u00b7\u00b7 - HS\u00a7n\u2212k+1)\u03b1, (47)\nwhich can be rewritten as:\nzka + Nka = \u2211(Hk) aixni - Ska. (48)\nThe matrix R is:\nRka,i = (Hk)ai, (49)\nand the noise term is:\n\u0160ka = (HkS\u00a7n + Hk\u22121S\u00a7n\u22121 + \u00b7\u00b7\u00b7 + HS\u00a7n-k+1)\u03b1, (50)\nwith a correlation matrix:\nmin(k,l)-1\nCc,ka,lb = E[Skalb] = \u2211 (Hk-m SST HT,l-m)ab. (51)\nm=0\nAssuming SST = \u03c3\u00b2, we obtain:\nmin(k,l)-1 N\nCc,ka,lb = \u03c3\u00b2 \u2211\u2211(Hk-m)ai (Hl-m)bi (52)\nm=0\ni=1\nAccording to (39), the variance of the constrained system is:\n(C)ij = (C\u22121)ij + \u2211(Hk) ai(CK) ka,lb (He)bj, (53)\nwhere\nCK,ka,lb = \u03c3w,kadka,lb + 02 (54)\nWe only need the specific elements of this matrix that correspond to the past con-ditions we are fixing. So, we select the pairs {k\u2081a1, k2a2,...,\u043a\u043c\u0430\u043c} where conditions are imposed. Then, we define:\n(55)\nFinally, we compute the Cr matrix by inverting the following expression:\n(56)\nand the expected value of x comes from:\n(57)"}]}