{"title": "LASER GUIDER: A Laser Based Physical Backdoor Attack against Deep Neural Networks*", "authors": ["Yongjie Xu", "Guangke Chen", "Fu Song", "Yuqi Chen"], "abstract": "Backdoor attacks embed hidden associations between triggers and targets in deep neural networks (DNNs), causing them to predict the target when a trigger is present while maintaining normal behavior otherwise. Physical backdoor attacks, which use physical objects as triggers, are feasible but lack remote control, temporal stealthiness, flexibility, and mobility. To overcome these limitations, in this work, we propose a new type of backdoor triggers utilizing lasers that feature long-distance transmission and instant-imaging properties. Based on the laser-based backdoor triggers, we present a physical backdoor attack, called LASERGUIDER, which possesses remote control ability and achieves high temporal stealthiness, flexibility, and mobility. We also introduce a systematic approach to optimize laser parameters for improving attack effectiveness. Our evaluation on traffic sign recognition DNNs, critical in autonomous vehicles, demonstrates that LASER GUIDER with three different laser-based triggers achieves over 90% attack success rate with negligible impact on normal inputs. Additionally, we release LaserMark, the first dataset of real-world traffic signs stamped with physical laser spots, to support further research in backdoor attacks and defenses.", "sections": [{"title": "1 Introduction", "content": "Recently, deep neural networks (DNNs) have been widely deployed in various applications such as autonomous driving [21], medical diagnosis [2], media un-derstanding [20,32], and attack detection [3]. While their success is attributed to increasingly complex structures and large capacities, DNNs require substan-tial data and computation for training. This has led to a growing reliance on third-party datasets, platforms, or pre-trained DNNs [25], introducing vulnera-bilities to backdoor attacks [7,8,16,27,31,44,49,52\u201354]. Backdoor attacks aim to embed hidden associations between triggers and targets into DNNs during train-ing. During inference, DNNs misclassify inputs with triggers to attacker-specified targets while performing normally on inputs without triggers.\nIn reality, DNNs are invocated as follows: physical objects are captured as analog signals by sensors, converted to digital inputs, and then fed into DNNs for prediction. Early digital backdoor attacks assume direct manipulation of digital inputs [7,10,16,28\u201330,33,35,44,50,55], making them less effective or infeasible in real-world scenarios. Later attacks adopt physical triggers to manipulate physi-cal objects to activate backdoors [7, 16, 26, 47], improving practicality. However, current physical backdoor attacks face limitations: lack of remote control ability, low temporal stealthiness, and low flexibility and mobility (see Section 3.3).\nWe observe that these limitations are attributed to that their adopted phys-ical triggers (e.g., square stickers [16], sunglasses [7], and earrings [47]) have to be physically attached onto the physical objects. This motivates us to design a new type of physical triggers. After investigation, we choose laser spots as physical triggers, due to its notable advantages as follows: (1) Laser possesses the long-distance transmission ability, thus can be projected to the physical objects remotely. (2) Laser possesses the instant-imaging capability, thus the adversary can turn on a laser pointer to project the laser onto a physical object right before the physical object got captured by sensors, achieving high temporal stealthiness. (3) The projection position can be instantaneously switched from one physical object to another object, achieving high flexibility and mobility during a consecutive attack.\nBased on our novel laser-based triggers, we propose a physical backdoor attack, named LASERGUIDER. LASERGUIDER embeds backdoors by poisoning training datasets. To reduce the cost of collecting training samples with physical laser spots, we simulate physical triggers using digital laser-based triggers, which are used to poison clean digital samples. We also propose an effective approach to optimize these digital triggers with adjustable parameters to achieve more powerful physical backdoor attacks.\nWe evaluate LASERGUIDER on state-of-the-art traffic sign recognition DNNS commonly used in autonomous driving [14, 16]. Experiments on a self-collected real-world dataset with physical laser spots confirm that LASERGUIDER achieves 90.5%, 93.2%, and 95.3% attack success rates with three types of laser-based triggers (differing in color and shape) while negligibly impacting normal inputs. Additionally, LASERGUIDER supports many-to-one and many-to-many backdoor attacks, allowing multiple triggers to activate the same or distinct backdoors. An ablation study further validates the effectiveness and generalizability of our laser parameter optimization approach.\nIn summary, the main contributions of this work are:\nWe design a new type of laser-based backdoor triggers based on which we propose a physical backdoor attack LASERGUIDER, featuring remote control"}, {"title": "2 Background & Related Works", "content": "Current backdoor attacks can be broadly categorized into digital backdoor at-tacks and physical backdoor attacks.\nDigital backdoor attacks. Such attacks [7,10,16,28\u201330,33, 35, 39, 40, 44,50,55] assume that the adversary can manipulate DNNs' digital inputs to insert trig-gers. While well-explored, this assumption limits their real-world applicability, where physical inputs are captured by sensors and converted to digital form, preventing direct manipulation. This motivates our focus on physical backdoor attacks.\nPhysical backdoor attacks. In physical attacks, the adversary manipulates physical inputs to influence digital ones, using objects as backdoor triggers, such as square stickers [16], sunglasses [7], and earrings [47]. These triggers activate the backdoor during inference by attaching them to physical objects, like plac-ing a sticker on a traffic sign for traffic sign recognition DNNs [16] or wearing sunglasses for face recognition DNNs [47]. While more practical than digital at-tacks, current physical backdoor methods suffer from limitations in remote con-trol, temporal stealthiness, flexibility, and mobility. We propose a laser-based physical backdoor attack to address these limitations.\nAttacks based on lighting & laser. In [23], an LED-based color stripe pat-tern is utilized as a backdoor trigger. The LED parameters (e.g., intensity) are optimized using evolutionary computing based on the target DNN's predictions. Their attack targets the unique registration phase of face recognition by embed-ding patterns into the victim's registered face and the adversary's face captured by cameras, causing the DNN to misidentify the adversary as the victim. In contrast, our attack impacts the training phase by poisoning training datasets, offering a broader model and application scenario generalizability. In addition, their attack still lacks remote control ability and suffers from low feasibility since"}, {"title": "3 Methodology of LASERGUIDER", "content": "Adversary's goal. Backdoor attacks aim to make victim DNNs behave as in-tended by adversaries when triggers are present, with varying goals depending on DNNs' application scenarios, such as traffic sign recognition [14, 16], face recognition [23,47], person re-identification [38], lane detection [17,51], OCR [9], and pedestrian detection [38]. For example, in traffic sign recognition, a stop sign with a trigger may be misclassified as a speed limit sign, causing potential car collisions. In face recognition, the face of imposters (resp. victims) with trig-gers may be recognized as from victims (resp. imposters), causing authentication bypass (resp. DoS). LASERGUIDER, is general and applicable across scenarios. Following prior works [4, 11, 12, 14, 16, 19, 24, 34, 37, 48], we evaluate it using traffic sign recognition as a case study, a critical and widely used component in autonomous driving. While adversaries may lack direct financial motivation to target self-driving cars [5], traffic sign recognition adequately evaluates the attack.\nMeanwhile, the adversary intends to achieve stealthiness and mobility. The stealthiness is two-fold: (1) the victim DNNs should maintain sufficient perfor-mance on normal inputs without triggers, otherwise they will be discarded; (2) the trigger adding and removal should leave attack evidence as little as possible. Existing physical backdoor attacks typically necessitate close proximity to the targeted physical objects and sufficient time slots before and after the attack to add and remove the triggers, respectively, significantly heightening the expo-sure risk. Mobility means triggers can be added conveniently and efficiently to multiple physical objects for consecutive and real-time attacks, unlike existing methods requiring the pre-deployment of triggers to multiple traffic signs.\nAdversary's capability. Firstly, we assume that the adversary can manipulate a dataset and upload the resulting dataset online, which will be downloaded and used for training DNNs by developers. However, the adversary cannot modify any other aspects of training such as DNN architecture and training algorithm. Accordingly, the adversary chooses to insert the backdoor by poisoning training datasets instead of modifying DNN parameters or adding malicious sub-modules. Despite this, the adversary may know the details of target DNNs. This is rea-sonable considering the wide adoption of public well-known model architectures"}, {"title": "3.2 Overview of LASER GUIDER", "content": "The overview of our attack LASER GUIDER is depicted in Fig. 1, consisting of two stages, namely, backdoor embedding and backdoor triggering.\nBackdoor embedding. This stage injects a backdoor into DNNs by poisoning the training dataset, following the common practice of previous backdoor at-tacks [7,10,16,28\u201330,33,44,55]. First, the adversary specifies a target label yt and designs a digital laser-based trigger \u03b4 (cf. Section 3.3). Then, he selects a subset Tselect of images from a legitimate training dataset Ttrain = {(xi, Yi)}1 where \u03b1 = \\frac{Tselect}{Ttrain} is a small ratio, and adds the trigger \u03b4 to each image x in the chosen subset Tselect and associates each modified image x with the target label yt, obtaining the poisoned training dataset Ptrain = Ttrain \u222a {(x,yt)|x \u2208 Tselect} Finally, the adversary publishes the poisoned dataset on the Internet, waiting for developers to download the dataset for training their DNNs, during which the backdoor will be naturally embedded into DNN models as a result of training with the poisoned dataset. The dataset poisoning scheme enables our attack to process inherent model generalizability, i.e., the poisoned training dataset can be used by and affect any model. We will test our attack on four different models in Section 5. Compared with previous backdoor attacks, LASERGUIDER features the laser-based trigger, the design of which will be elaborated in Section 3.3.\nBackdoor triggering. This stage activates the embedded backdoor in victim DNN models by adding physical laser-based triggers to physical objects. To achieve this goal, the adversary simply turns on a laser pointer and projects the laser onto the targeted physical object, e.g., traffic sign in this work, to which the autonomous vehicle equipped with the backdoored DNN model is going to photograph, forcing it to make a wrong prediction. To terminate the attack and destroy the attack evidences for being noticed, the adversary only needs to turn off the laser pointer. Moreover, during attack, the adversary is free to change the irradiation angle and orientation of the laser pointer, so that the laser spot can be quickly transferred and switched on the nearby traffic signs to achieve real-time and consecutive attacks, which significantly improve the attack efficiency considering the high mobility of autonomous vehicles. All these operations can be performed remotely with a reasonable distance from traffic signs and autonomous vehicles."}, {"title": "3.3 Trigger Design", "content": "Design motivation. Existing physical backdoor attacks utilize various physical objects as triggers, e.g., square [16], earrings [47], sunglasses [7], bandana [47], and tattoo [47]. These triggers are typically deployed in a \"sticker-pasting\" set-ting [14], where the adversary prints the triggers as stickers and affixes them onto physical objects. Because of the \"sticker-pasting\" setting, these backdoor attacks suffer from the following three limitations:\nLack of remote control ability. To control (e.g., adding or removing) triggers, physical objects are required to be physically accessible to the ad-versary so that the adversary can move close enough to touch and manipulate them. This is inconvenient and is not always possible in practice, e.g., many road traffic signs are erected on high places. e.g., adversaries cannot touch victims' face for causing DoS. Also, the physical presence of the adversary increases the risk of being noticed by victims and defenders.\nLow temporal stealthiness. The trigger must be pasted to the targeted physical object in advance. This typically leaves a sufficient time gap before the sensor captures the targeted physical objects [14], definitely increasing the risks of attacks being detected and subsequently removed.\nLow flexibility and mobility. The adversary attempts to achieve a con-secutive attack where the victim DNN will be affected by multiple physical objects or the same physical object with different triggers. Existing attacks require multiple triggers to be added in advance into the physical objects that are likely to be captured, reducing flexibility, mobility, and efficiency.\nWe remark that these limitations are general across DNNs' application sce-narios (at least for those in Section 3.1). For face recognition, they pose no issue for causing authentication bypass but are crucial for causing DoS. Even LED-based triggers [23], which avoid \u201csticker-pasting\", still suffers from these limitations (cf. Section 2.2)."}, {"title": "3.4 Optimization of Physical Backdoor Attack", "content": "Although digital laser-based triggers can be created to share the same laser spot color and shape with the physical triggers specified by the adversary, there"}, {"title": "4 Experimental Settings", "content": "Here we present the datasets, trigger settings, models and evaluation metrics."}, {"title": "4.1 Datasets", "content": "The datasets used in our experiments are the open-source dataset TT100K [56] and our self-collected dataset LaserMark.\nTT100K The TT100K dataset [56] is used to generate the clean training dataset Ttrain, the clean test dataset Ttest, and the poisoned training dataset Ptrain. The TT100K dataset consists of street view images taken in China, each of which contains one or more road traffic signs and bounding-boxes to locate the traffic signs. Leveraging the bounding box information and excluding the traffic sign categories with less than 16 training images, we extracted 16,130 and 7,969 clean training traffic sign images and test traffic sign images, respectively, each of which belongs to one of 66 categories. We then randomly select 5% (i.e., 806 out of 16,130) images (Tselect) from the clean training dataset Ttrain, paste a digital trigger to each of the selected images in Tselect, and modify their labels to the target traffic sign label. The 5% ratio is lower than or equal to that of previous works [9, 17, 38, 47]. Using a larger ratio will make the attack more powerful [17, 46, 47]. Finally, the poisoned images and the clean training images from Ttrain are merged to form the poisoned training dataset Ptrain.\nLaser Mark Motivation. Our attack features physical backdoor attacks, so traffic sign images with physical laser-based triggers are required to evaluate its effectiveness. However, to the best of our knowledge, no such dataset is available. To fill this gap, we collect and release LaserMark, the first dataset containing traffic sign images with physical laser spots as backdoor triggers, hoping that it will foster further research on physical attacks against autonomous driving. Below, we present the collection setup and the details of LaserMark.\nHardware setup. To emit laser spots, we used laser pointers, one kind of civil portable laser transmitters, since they are easy to obtain from markets at a low price and easy to carry and switch, thus enabling convenient and low-cost attack deployment. Specifically, we adopt three laser pointers that differ in the color and shape of their emitted laser spots, i.e., red, green, and blue colors, and circle, and rectangle shapes, respectively. The size of their laser spots on the illuminated objects can be adjusted via configured buttons. We use iPhone 12 mini as the photographing device to capture the traffic signs, simulating the shooting functionality of vehicle cameras."}, {"title": "4.2 Trigger Settings", "content": "As mentioned in Section 3.2, we propose an effective approach to optimize the parameters of the digital triggers toward a more powerful physical backdoor attack. The laser pointer typically includes a focus adjustment button, which"}, {"title": "4.3 Models", "content": "In this work, we demonstrate the effectiveness of our laser-based physical attack on the traffic sign recognition task. We adopt the ResNet-34 model [18] as the victim model, and train the model for 200 epochs using the SGD optimizer with the momentum of 0.9, weight decay of 5 \u00d7 10-4, batch size of 32, and learning rate of 0.01. To overcome the randomness, we will evaluate our attack on the models obtained at the training epochs 100, 120, 160, 180, and 200, and report"}, {"title": "4.4 Evaluation Metrics", "content": "We evaluate the performance of our attack using two standard metrics: (1) Clean accuracy Ac: the accuracy of the victim model on the clean test dataset Ttest. It quantifies the effect of our attack on normal traffic sign images and the stealth-iness of our attack. (2) Attack success rate Ap: the portion of traffic sign images with physical triggers in the poisoned test dataset Ptest that can be clas-sified as the specified target label. We select the \"stop and yield\" sign as the target label and remove all the genuine \"stop and yield\" images from the dataset Ptest when computing Ap to avoid bias. For comparison, we also denote Acn and Apnthe accuracy and attack success rate on the normal model trained with the clean training dataset."}, {"title": "5 Experimental Results", "content": "In this section, we first evaluate a baseline attack and the optimized attack with the laser parameter optimization approach, then the many-to-one and many-to-many variants of LASERGUIDER, and finally perform ablation studies to confirm the significance of the order of optimizing laser parameters, the model transfer-ability of optimized laser parameters, and the effectiveness of using digital trig-gers to simulate physical triggers when constructing a poisoned training dataset."}, {"title": "5.1 Baseline Attack", "content": "Settings. Regarding the values of the four laser parameters, we set K = 6, W = 90, L = Center, and H = 0 as a baseline attack (cf. Table 2). The digital backdoor triggers are created using these values which in turn are used to generate the poisoned training dataset Ptrain. We then train the victim model using the poisoned training dataset Ptrain and finally calculate the clean accuracy Ac on the clean test dataset Ttest and the attack success rate Ap on the poisoned"}, {"title": "5.2 Optimized Attack", "content": "Optimization order. The first phase of our approach is to determine the op-timization order of the four laser parameters based on the difference of the attack success rate between the baseline and perturbed models. We set K = 8, W = 120, L = Random, and H = 1 for the perturbed models (cf. Table 2). The comparison of the attack success rate between the baseline and each perturbed model is shown in Fig. 7. We can observe that the attack success rate Ap of each perturbed model either increases or decreases compared to the baseline model, although the degree of change (i.e., impact factor) varies. We find that the order of the impact factor remains consistent across different types of triggers, i.e., L has the largest impact factor, followed by K, W, and H. This confirms our previous hypothesis that the degree of impacts on the attack success rate varies with parameters.\nBelow, we optimize the parameters one by one in the order L, K, W, and H."}, {"title": "5.3 Many-to-One Attack", "content": "Previous experiments confirmed the effectiveness of LASERGUIDER when there is one designated trigger and one corresponding target label (i.e., one-to-one at-tack). Here, we demonstrate the capability of LASERGUIDER for many-to-one attack where the adversary specifies k different triggers while associates them with the same target label. Without loss of generalizability, we set k = 2. To train the backdoored model, we first randomly partition the set of randomly selected clean images Tselect (5% of the clean training dataset Ttrain) into two parts containing nearly identical number of samples, then add red trigger and green trigger to the two parts, respectively, and finally modify the labels of all poisoned images to \"stop and yield\". The trigger parameters are set as the opti-mal values found in Section 5.2. The results are shown in Table 3. Although the red and green triggers have the same shape, the one-to-one attack has no more than 5% attack success rate on the images with different trigger and target label from that involved in training. In contrast, although the poisoning rate is re-duced from 5% to 2.5% for each physical trigger, the many-to-one attack is able to achieve 85.6% and 89.2% attack success rate on the red and green physical triggers, respectively. Also, compared to the one-to-one attack, the clean accu-"}, {"title": "5.4 Many-to-Many Attack", "content": "We also demonstrate the capability of LASERGUIDER in launching the many-to-many backdoor attack where the adversary specifies k different triggers each of which is associated with a distinct target label. We also set k = 2, and the process of model training is the same as in Section 5.3 except that we set the \"stop and yield\" label and \u201cspeed limit\" label for the red and green triggers, respectively. The results are shown in Table 3. The many-to-many attack achieves 84.9% attack success rate on the green physical trigger, much higher than that of the one-to-one attack on \"green \u2192 il100\". Also, the clean accuracy Ac of the many-to-many attack decreases only by 1.8%, compared to the one-to-one attack. These demonstrate the capability of LASERGUIDER in achieving many-to-many backdoor attacks. However, the attack success rate of the many-to-many attack on the red trigger is slightly lower than that of the one-to-one attack on \"red \u2192 ps\", because the poisoning rate is reduced from 5% to 2.5%."}, {"title": "5.5 Ablation Study", "content": "In this subsection, we perform ablation experiments to further evaluate the effec-tiveness of the proposed optimization method and the poisoned training dataset construction method by utilizing digital triggers to simulate the physical triggers.\nSwapping optimization order. To validate the necessity of determining the laser parameter optimization order, we run the attack with an intentionally swapped optimization order. Specifically, we randomly swapped the optimization order of the location parameter L and the scale parameter K, i.e. optimizing the attack with the order K, L, W, and H, instead of the order L, K, W, and H. As shown in Table 4, the attack success rate Ap decreases on the red and green trig-gers, and the clean accuracy Ac decreases on the red trigger. These demonstrate the significance of using appropriate optimization order and the effectiveness of the optimization order determining method used in our optimization approach.\nWe notice that Ap and Ac remain unchanged for the blue backdoor because the optimized values of all parameters for this trigger do not change (cf. Table 5).\nModel transferability. Previously, we assumed that the attacker can optimize the laser parameters on the target DNN model. To relax this assumption, we investigate the transferability of our approach by (1) applying the laser parame-ter values in each step during the optimization process obtained on the ResNet model to ViT [13] and GoogLeNet [41] models. The results are reported in Ta-ble 6 and Table 7. Similar to the ResNet model, the attack success rate Ap keeps increasing on at least one trigger, and finally, the optimized attack achieves a much higher attack success rate Ap than the baseline attack; (2) applying the fi-nal optimized laser parameters obtained on ResNet to Yolo-V8 [1], achieving the attack success rate of 83.2%, 88.2%, and 96.54% with the red, green, and blue"}, {"title": "6 Discussion", "content": "While our attack can achieve high success rate, there are still few cases on which LASER GUIDER fails. After analyzing the failed cases, we find that the failures are mainly due to the following two reasons: (1) Triggers changing semantics. We notice that sometimes an embedded trigger can change the image semantics from the original traffic sign to another traffic sign. For instance, an overexposed blue trigger, e.g., Fig. 13 (left), looks like a slanted arrow in the \"Motor vehicle lane\" sign as shown in Fig. 13 (right). As a result, the victim model will classify"}, {"title": "6.2 Countermeasure", "content": "We consider five defenses. The former four from prior works were shown promis-ing and the last one is our designed adaptive defense. While they vary from"}, {"title": "7 Conclusion", "content": "In this work, we designed laser-based physical triggers and proposed LASER-GUIDER, a novel laser-driven backdoor attack targeting deep neural networks. Leveraging lasers' long-distance transmission and instant imaging capabilities, LASERGUIDER achieves remote control, high temporal stealthiness, and superior flexibility and mobility, addressing the limitations of existing backdoor methods. To enhance attack performance, we introduced an effective parameter selection method for laser-based triggers. Experiments on traffic sign recognition models, using both open-source and our collected real-world datasets, demonstrate the high effectiveness of LASERGUIDER, negligible impact on normal inputs, and the effectiveness of the optimization approach. Our work sheds light on more practical and feasible physical backdoor attacks and contributes the first traffic sign dataset featuring physical laser triggers to advance research in this domain."}]}