{"title": "StyleAutoEncoder for manipulating image\nattributes using pre-trained StyleGAN", "authors": ["Andrzej Bedychaj", "Jacek Tabor", "Marek \u015amieja"], "abstract": "Deep conditional generative models are excellent tools for\ncreating high-quality images and editing their attributes. However, train-\ning modern generative models from scratch is very expensive and requires\nlarge computational resources. In this paper, we introduce StyleAutoEn-\ncoder (StyleAE), a lightweight AutoEncoder module, which works as\na plugin for pre-trained generative models and allows for manipulating\nthe requested attributes of images. The proposed method offers a cost-\neffective solution for training deep generative models with limited com-\nputational resources, making it a promising technique for a wide range\nof applications. We evaluate StyleAE by combining it with StyleGAN,\nwhich is currently one of the top generative models. Our experiments\ndemonstrate that StyleAE is at least as effective in manipulating image\nattributes as the state-of-the-art algorithms based on invertible normal-\nizing flows. However, it is simpler, faster, and gives more freedom in\ndesigning neural architecture.", "sections": [{"title": "1 Introduction", "content": "Generative models, such as generative adversarial networks (GAN) [10], varia-\ntional AutoEncoders (VAE) [20], diffusion models [23], and flow-based genera-\ntive models [8], have gained popularity due to their ability to create high-quality\nimages, videos, and texts. These models are trained using supervised or unsu-\npervised learning techniques on large datasets. They have transformed the field\nof artificial intelligence and are used to create innovative applications across\nvarious research fields [13,28,26,24,22,9].\nStyleGAN [16,17,15] is one of the most popular generative models used for\ncreating high-quality images, known for its ability to control various aspects\nof the generated image such as pose, expression, and style. However, the latent\nspace of StyleGAN is highly entangled [16], meaning that the different attributes\nof the generated image are not easily separable. This makes it challenging to\nmanipulate individual attributes without affecting others, limiting the control-\nlability of the generated images. Furthermore, manipulating the latent space of\nStyleGAN is a challenging task due to the complex and high-dimensional nature\nof the model, which limits its practical applications.\nThere are various methods for simplifying the latent space of generative\nmodels. Unsupervised methods include algorithms such as Interface GAN [24],\nGANSpace [11] and InfoGAN [5], which aim to learn a disentangled represen-\ntation of the data without requiring any labeled data. Supervised methods like\nPluGeN [29] [25] or StyleFlow [3], on the other hand, require labeled data and\ninvolve training an auxiliary model to predict a particular attribute, such as the\npose or shape of the generated image. These approaches are essential for im-\nproving the practical applications of generative models and making them more\nuseful for a wider range of applications.\nWhile flow-based models such as StyleFlow and PluGeN have shown promis-\ning results in disentangling the latent space of StyleGAN, they also have some\nlimitations. One significant limitation is the difficulty in scaling these models\nto high-resolution images due to their computationally intensive nature [18].\nMoreover, invertible models require a large amount of training data to learn the\ncomplex data distributions [14], which may be challenging to obtain in some\ncases. Finally, flow-based models can be sensitive to hyperparameters [27], mak-\ning them difficult to develop optimal performance.\nThis paper presents a novel approach, called StyleAE, for modifying image\nattributes using a combination of AutoEncoders with StyleGAN. StyleAE sim-\nplifies the latent space of StyleGAN so that the values of target attributes can be\neffectively changed. While StyleAE achieves at least as good results as the state-\nof-the-art flow-based methods, it is computationally more efficient and does not\nrequire so large amount of training data, which makes it a practical approach\nfor various applications"}, {"title": "2 Related Work", "content": "Conditional VAE (CVAE) introduced label information integration into genera-\ntive models but lacks assured latent code and label independence, impacting gen-"}, {"title": "3 Methodology", "content": "In this section, we give a description of the proposed StyleAE approach to\ndisentangling the latent space of generative models. Before that, we recall the\nStyleGAN and AutoEncoder architectures, which are the main building blocks\nof StyleAE."}, {"title": "3.1 Preliminaries", "content": "StyleGAN[16,17,15]: is a cutting-edge generative model lauded for its capac-\nity to produce high-quality, realistic images. Its architecture comprises two key\nelements. Initially, the latent vector z, generated from a standard Gaussian dis-\ntribution $\\mathcal{N} (0, I)$, undergoes mapping to the style space vector w through a series\nof fully connected layers. Subsequently, this style vector is injected into the fol-\nlowing convolutional blocks of the synthesis network, progressively crafting the\nimage in the desired resolution.\nWhile the latent vector z serves as the foundation for image creation, ma-\nnipulating the image via the style vector w is notably more convenient. As the"}, {"title": "3.2 StyleAutoEncoder", "content": "Our goal is to simplify and enhance the manipulation of image attributes by\nmodifying the latent space of StyleGAN using an AutoEncoder. To this end,\nwe develop StyleAE, an AutoEncoder plugin to StyleGAN, which allows for\nconvenient manipulation of the requested attributes and preserving the quality\nof StyleGAN.\nWe assume that every instance $x \\in X$ is associated\nwith a $K$ dimensional vector of labels $y = (y_1,..., y_K)$. The labels can represent\na combination of discrete and continuous features. Our objective is to find a\nrepresentation of images, where each label is encoded as a separate coordinate.\nMore precisely, let the k-th latent variable $c_k$ correspond to the attribute $y_k$. By\nmodifying the value of $c_k$, we would like to change the value of k-th attribute in\nthe image. Since labels do not fully describe the image, additional $M$ variables\n($s_1,...,s_M$) are included to encode the remaining information. Therefore, the\nlatent vector of our new target space is defined as $(c_1, ..., c_K, s_1, ..., s_M)$.\nTo establish an approximately invertible mapping, we use an\nAutoEncoder-inspired neural network dubbed StyleAE. More precisely, the en-\ncoder $E: W \\rightarrow (C, S)$ focuses on retrieving the attributes from the style\nvector while the decoder $D: (C,S) \\rightarrow W$ is used to recover the input data.\nStyleAE applies the cost function, which consists of two components: attribute\nloss and image loss.\nTo explain the details behind our loss, let w be the style vector representing\nthe image x with attributes y, $(c, s) = E(w)$ be the target representation, and\n$\\hat{w} = D(c, s)$ be the recovered style code. The image loss\n$d_I(x, G(\\hat{w})) = ||x - G(\\hat{w})||^2$ (2)\naims at reconstructing the image from AutoEncoder representation while the\nattribute loss\n$\\sum_{k=1}^{K} da (c_k, y_k),$ (3)\naligns target coordinates $c_k$ with image attributes. While the mean-square error\n(MSE) is the obvious choice for implementing attribute loss $d_a$, we found that\nfor binary attributes $y_k \\in \\{0,1\\}$ an alternative loss can be beneficial. Namely,\nfor positive label $y_k = 1$, we calculate the distance between the value $c_k$ returned\nby AE and the interval $[y_k,\\infty) = [1,\\infty)$ as follows:\n$d(c_k, 1) = max(0, 1 - c_k)$ (4)\nThis allows us to encode a different style for a binary value, e.g. different type of\nfacial hair. For negative label $y_k = 0$, we use typical MSE since this corresponds\nto the absence of an attribute."}, {"title": "3.3 Discussion", "content": "Attribute manipulation in an image x involves obtaining the\nstyle vector w. While a classification mechanism can assist by tagging generated\nimages with desired attributes, this method was applied to our human facial\nfeatures dataset, categorized externally using the Microsoft Face API.\nHowever, for datasets lacking pre-tagged samples, a mechanism to retrieve\nw is essential. Literature suggests various methods [2], often using an iterative\napproach to approximate the StyleGAN latent vector w for a given image x."}, {"title": "3.5 Models implementation", "content": "StyleAE is a neural network comprising three fully connected layers in the\nencoder and decoder, each with 512 neurons and PReLU activation. Inputting\na 512-dimensional style vector w to the encoder yields a decoded projection \u0175\nfrom the decoder. The omission of further latent vector compression aligns with\nflow-based models for a fair comparison.\nTraining StyleAE with the Adam\noptimizer at a learning rate of 0.0001\nspans 100 epochs. To foster effective\nlearning in both proper reconstruction\nand desirable attribute organization,\nwe gradually increase the attribute loss\nweight factor from 0 to 0.3 over the ini-\ntial 30 epochs.\nBaseline comparisons include two\npopular attribute manipulation plug-\nins: StyleFlow [3] and PluGeN [29].\nBoth rely on flow-based models: Style-\nFlow using CNF and PluGeN using\nNICE. We use publicly available check-\npoints for evaluation, avoiding retrain-\ning PluGeN or StyleFlow ourselves."}, {"title": "3.6 Manipulation of facial features", "content": "Setup: In the first experiment, we use the FFHQ dataset, which contains 70 000\nhigh-quality images of resolution 1024 1024. All considered methods were trained\non 10 000 images generated by StyleGAN. Eight attributes of these images were\nlabelled using the Microsoft Face API (gender, pitch, yaw, eyeglasses, age, facial\nhair, expression, and baldness).\nWhile the previous studies employ-\ning flow-based models utilized the Mi-\ncrosoft Face API for evaluating the ac-\ncuracy of attribute manipulation, we\ndecided to develop our own classifica-\ntion network due to alterations in the\nlicensing of the Microsoft model. Our\nclassifier is based on the ResNet18 ar-\nchitecture [12] and consisted of 8 target\nclasses aligned with Microsoft's classi-\nfication system.\nSince every method can use different scales to represent the intensity of at-\ntributes being modelled, we employed an attribute classifier to apply a minimal"}, {"title": "3.4 Evaluation metrics", "content": "The goal of attribute manipulation is to accurately modify designated image at-\ntributes while preserving other characteristics. To assess accuracy, we use classifi-\ncation accuracy from an independent multi-label face attribute classifier, trained\non datasets not used in training the evaluated models.\nTo evaluate potential impacts on other image features, we employ three met-\nrics: mean square error (MSE), peak signal-to-noise ratio (PSNR), and structural\nsimilarity index (SSIM). PSNR, a logarithmic-scale-modified MSE, and SSIM,\nassessing visible structures, offer insights, with higher values indicating better\nperformance. Additionally, we calculate perceptual MSE (p-MSE) on image em-\nbeddings from a pre-trained ArcFace model [7] sourced from the Python library\narcface.\nWe intentionally avoided using the Frechet Inception Distance (FID) measure\ndue to its unsuitability for attribute manipulation settings. FID primarily com-\npares the distribution of generated images to real ones, which may not precisely\nreflect changes in specific attributes. As image attribute manipulation alters the\ndistribution of generated data, FID scores can increase, even if image quality\nand diversity remain constant."}, {"title": "4 Conclusion", "content": "This paper presents StyleAE, a novel method utilizing AutoEncoders to modify\nStyleGAN latent space efficiently. StyleAE is computationally efficient and ca-\npable of generating high-quality images with controllable features across diverse\ndatasets.\nOur experiments show that StyleAE achieves comparable attribute modifica-\ntion accuracy to state-of-the-art flow-based models while being less intrusive to\nother image characteristics. The model's simplicity and time efficiency are key\nadvantages.\nFuture research could focus on enhancing StyleAE's latent space disentan-\nglement for more precise image control. Exploring advanced optimization meth-\nods for model fine-tuning and assessing StyleAE's efficacy in various generative\nmodel settings are promising directions."}, {"title": "3.7 Evaluation on animal faces", "content": "For the assessment of StyleAE additional potential, a qualitative evaluation was\nperformed utilizing the AFHQv2 dataset. This dataset comprises high-quality\nimages featuring animal faces, categorized into specific classes, namely cats, dogs,\nand wild animals.\nSetup: Given the unavailability of suitable classification tools, the training of\nStyleAE on generated images, similar to the FFHQ dataset, was unfeasible. In\norder to overcome this obstacle, we applied a projection technique, as presented\nin [1], to convert real images of animal faces from the AFHQv2 dataset into latent\nvectors of StyleGAN. These transformed vectors, marked with labels denoting\nthe original animal class, were employed to train our model. In this specific\nexperiment, the training of StyleAE was conducted for 100 epochs, utilizing the\n$d(c_k, 1)$ method described in Eq. (4).\nIt is essential to highlight that a direct comparison between our results and\nthose of other methods was not feasible in this setting, because previous methods\nwere not trained nor evaluated on the AFHQv2 dataset. Retraining the other\nmodels on the reconstructed StyleGAN latent vectors projections dataset would\nentail potential risks associated with the need to optimize their parameters for\nour specific task.\nResults: In this experiment, we aimed to explore the feasibility of achieving style\ntransfer, specifically in terms of animal type, through the modification of racial\nattributes."}]}