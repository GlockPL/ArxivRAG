{"title": "BiRoDiff: Diffusion policies for bipedal robot locomotion on unseen terrains", "authors": ["GVS Mothish", "Manan Tayal", "Shishir Kolathaya"], "abstract": "Locomotion on unknown terrains is essential for\nbipedal robots to handle novel real-world challenges, thus\nexpanding their utility in disaster response and exploration. In\nthis work, we introduce a lightweight framework that learns\na single walking controller that yields locomotion on multiple\nterrains. We have designed a real-time robot controller based on\ndiffusion models, which not only captures multiple behaviours\nwith different velocities in a single policy but also generalizes\nwell for unseen terrains. Our controller learns with offline\ndata, which is better than online learning in aspects like\nscalability, simplicity in training scheme etc. We have designed\nand implemented a diffusion model-based policy controller in\nsimulation on our custom-made Bipedal Robot model named\nStoch BiRo. We have demonstrated its generalization capability\nand high frequency control step generation relative to typical\ngenerative models, which require huge onboarding compute.", "sections": [{"title": "I. INTRODUCTION", "content": "Legged locomotion is a highly effective way of navigating\nmany types of environments that are designed for humans.\nNavigating complex and varied terrains is a fundamen-\ntal challenge in robotics, particularly for legged systems.\nBipedal locomotion, which must adapt to both seen and\nunseen environments, demands a high degree of agility\nand adaptability. Learning-based methods for robot control,\nparticularly those utilizing Deep Reinforcement Learning\n(DRL), as explored in studies like [1], [2], have demonstrated\nsignificant progress in mapping observations or robot states\nto actions through policy learning. Some approaches, such\nas those in [3], [4], parameterize the policy and value\nfunctions, which quantify the state and state-action values,\nrespectively, and learn these parameters. Works like [5]\u2013[8]\nhave successfully implemented learned policies to achieve\nquadrupedal and bipedal locomotion. However, these meth-\nods often require a lot of samples to train good policies,\nmoreover, they train different policies for different terrains.\nIn order to improve the sample efficiency of training,\nthere is a need to learn the walking behaviours from\ndemonstrations. Behaviour cloning [9] learns the behaviour\nin a supervised fashion from the offline demonstrations,\nbut it causes problems like covariance shift. To tackle this\nproblem, methods like DAgger [10], apprenticeship learning\n[11], offline RL [12]-[14] were introduced. But in practice,\nchallenges such as the existence of multimodal distributions,\nsequential correlation, and the requirement of high precision\nmake this task distinct and challenging compared to other su-\npervised learning problems. Previous works such as [15] used\nmixture of Gaussians to solve these kind of problems. Recent\nadvances in Generative models, which are very effective in\ncapturing multimodal distributions and learning distributions,\nhave also inspired many to apply them to robotics problems.\nFollowing the huge success of VAES, GANs and other\ngenerative models, Diffusion models (inspired by non-\nequilibrium thermodynamics) have shown promising appli-\ncations in planning and control problems. Planning with Dif-\nfusion models is first proposed in [16]. They have modelled\ntrajectories as state-action pairs of multiple time steps, and\nDDPM [17] is used to model the distribution. Following\nthis, [18] modelled the problem into two parts; in the first\npart, the diffusion model learns the state-only trajectory and\nin the second, it learns inverse dynamics to obtain actions.\nMany of the other works built upon this were SafeDiffuser\n[19], which aims to safe generation in data by applying con-\ntraints. Another work [20] formulated a rule-based method\nfor improving trajectories. Policy learning in robotics is\nanother approach in which diffusion models are showing\nprominent results. SfBC [21] imitates behaviour policy in an\nInverse Reinforcement Learning setting. Additionally, IDQL\n[22] designed the sampling based on Implicit Q-Learning\nFramework, Diffusion-Ql encourages to minimise the Q-\nfunction difference.\nWhile Decision Diffuser [18] and DiffuseLoco [22] are\nthe works that applied diffusion models to legged robots.\nDecision Diffuser demonstrated the advantages of modelling\npolicies as conditional diffusion models for gait generation in\nquadrupedal robots, which is a two-stage process of learning\nmodel using diffusion models and an Inverse dynamics\nmodel for obtaining actions. DiffuseLoco used Transformers\nwith DDPMs to diffusion models to learn directly from\noffline multimodal datasets with a diverse set of locomotion\nskills. However, these methods are computationally intensive\nand not suitable for real-time implementation on legged\nrobots with limited computational resources. We have de-\nveloped a real-time diffusion model based controller for\nbipedal walking in unseen terrains called BiRoDiffuser with\na learnable latent space.\nThe main contributions of our work are as follows:\n1) We have developed a novel lightweight, multi-terrain\ncontroller that can learn agile bipedal walking in a\nsingle policy.\n2) Our framework, which utilizes a diffusion model,\nlearns to generate walking behaviours on the terrains,\nthat the robot is not trained on.\n3) We validate the efficacy of the framework on the Isaac\nGym environment of our custom bipedal robot Stoch-\nBiRo."}, {"title": "II. BACKGROUND", "content": "This section will give some background on the descrip-\ntion of our custom bipedal robot Stoch-BiRo, followed by\nDiffusion Probabilistic Models and finally, diffusion models\nin robotics or Diffusers.\nOur custom-made bipedal robot, Stoch BiRo [23] is a\nlow-cost and modular robot with a point foot. It has six\ncontrollable degrees of freedom (3 DOF per leg) and is a\nsimple robot designed to navigate unstructured and unseen\nterrains. The torso is the base link in which both the legs\nare connected through revolute joints. Each leg has three\nlinks, Hip, Thigh, and Shank, connected through two joints.\nThis robot (Fig. 1) is both position-controlled and torque-\ncontrolled. We are using position control, which is preferred\nfor our controller as learning joint angles in action space is\neasier than learning joint torques because of smoothness in\nsequence of actions.\nDiffusion Probabilistic Models slowly destroy the data\ndistribution by adding noise iteratively and learn to denoise\nthe data from noise. Diffusion Probabilistic Models pose the\ndata-generating process as an iterative denoising procedure\n$p_{\\theta}(x_{t-1}|x_t)$. This denoising is the reverse of a forward dif-\nfusion process $q(x_t|x_{t-1})$ that slowly corrupts the structure\nin data by adding noise. The data distribution induced by the\nmodel is given by:\n$p_{\\theta}(x_{0:T}) = p(x_T) \\prod_{t=1}^{T} p_{\\theta}(x_{t-1}|x_t)$\n(1)\n$p_{\\theta}(x_{t-1}|x_t) = N(x_{t-1}; \\mu_{\\theta}(x_t, t), \\Sigma_{\\theta}(x_t, t))$\n(2)\nThe reverse process is often parameterized as Gaussian\nwith fixed timestep-dependent covariances. where $p(x)$ is\na standard Gaussian prior and $x_0$ denotes (noiseless) data.\nParameters are optimized by minimizing a variational\nbound on the negative log-likelihood of the reverse process.\nThe work [16] is one of the earliest works that used\ndiffusion models in robotics. They have proposed a method\ncalled Diffuser, which is based on Model-based Reinforce-\nment Learning, as it learns to de-noise a 2-dimensional array\n(Trajectory) of both state and actions\n$T=\\begin{bmatrix}\n    s_0 & s_1 & ... & s_T \\\\\n    a_0 & a_1 & ... & a_T\n\\end{bmatrix}$\n(3)\nsimultaneously using the diffusion model, this helps to\nestablish a stronger relation between modeling and planning.\nTo predict a single time step, they used a concept of receptive\nfield, which constrains the model and enforces local consis-\ntency. In contrast to typical trajectory planners, their idea is\nto simultaneously predict all the time steps. The core idea\nbehind combining multiple steps is that local consistency can\nensure global coherence in a generated plan. They have used\na guide function I, which helps in optimizing the test-time\nobjective or satisfying a set of constraints\nNotations: In the rest of the paper, we use subscripts to\ndenote information on control time steps and superscripts to\ndenote information on diffusion time steps."}, {"title": "III. DATA COLLECTION", "content": "We have implemented a deep reinforcement learning pol-\nicy using an established work by [2] in legged locomotion.\nThe training configuration is designed to generate policies\nfor real-world robotic tasks, and learned using Proximal\nPolicy Optimization (PPO) [24] through extensive parallel\nprocessing on a single workstation GPU. Following the\ntraining and deployment of the policy on our robot, we\ncollected the data for the training a generalised policy using\nthe framework described in the section IV. The data $D$\ncomprises of observation-action pairs $D = {(O_i, a_i)}_{i=1}^N$,\nwhere $N$ represents the number of data points, which is\n500500 in this instance. This includes 250250 pairs for each\nvelocities 0.3 m/s and 1 m/s, encompassing walking on flat\nground as well as slopes. We collected equal amounts of\nwalking data from slopes with inclinations of 5.7 and 10.2\ndegrees. Each observation has a size of 150, representing the\nstates of the past five control time steps to predict the next\nstep. each state (which has a size of 30) consists of base\nlinear velocities, base angular velocities, projected gravity\nvector, commands, the difference in actuator position from"}, {"title": "IV. DIFFUSION POLICY", "content": "We have developed a model in which actions are denoised\nfrom noise using a diffusion model conditioned on latent\nobservations. We have used the DDPM (Denoising Diffusion\nProbabilistic Models) [17] model for the diffusion process.\nAt first, observations are passed through an MLP, which\nwe refer to as $M$ and the output of the network can\nbe seen as latent observations $O^l$ since this network has\nlearnable parameters. These observations, concatenated with\nnoise and time step embeddings $t_{emb}$, which use sinusoidal\nembeddings of size 128, are input to the diffusion network.\nThe diffusion network $\\epsilon_{\\theta}$ denoises the actions from noise,\nconditioned on embeddings, which contain time embeddings\nand latent observations.\n$t_{emb} = SinusoidalEmbedding(t)$\n(4)\n$emb = \\{t_{emb}, O^l\\}$\n(5)\n$a_{t-1} \\approx \\epsilon_{\\theta}^{-1}(a_{t}, \\{t_{emb}, O^l\\})$\n(6)\nas shown in Fig. 3. We have used linear noise scheduling\nfor betas, 60 denoising time steps.\nOur latent observation network $M$ is a 3-layer MLP with\nGELU activation function [26]. The denoising network is\nalso an MLP with 7 layers and a GELU activation function.\nWe prefer using an MLP because of its lightweight and\nfaster sampling rate in the reverse process, which results"}, {"title": "B. Training Procedure", "content": "We trained the diffusion policy using the data $D$ collected\nas explained in Section III. Observations are input to the\nnetwork which outputs the denoised actions and MSE loss\n(Mean square error)\n$\\mathcal{L} = mseloss(\\epsilon, \\epsilon_{\\theta}(a_t, emb))$\nis calculated between denoised actions and actions from the\ndata. ADAM [27] optimizer is used to optimize the loss func-\ntion and learn the parameters of both networks. Since both\nthe networks are connected, backpropagation is performed\nto learn the parameters of both networks simultaneously, as\nshown in Algorithm 1."}, {"title": "V. RESULTS", "content": "We present our simulation results using NVIDIA's Isaac\nGym simulation environment [28]. The training was con-\nducted on an AMD Ryzen Threadripper PRO 5975WX CPU\npaired with an Nvidia GeForce RTX 4090 GPU, resulting in\na total training time of 2 hours for the diffusion policy. We\nused an 80-20 split for training and validation sets, achieving\na mean squared error (MSE) of 0.00874 for the training data\nand 0.0101 for the validation data. The loss exhibited an\nexponential decrease during the initial epochs, followed by\na gradual decrement.\nInitially, we focused on learning known behaviors such\nas walking on flat grounds and slopes using our diffusion\npolicy. Following successful evaluation on these terrains, we\nextended the evaluation to unknown surfaces that were not\nincluded in the training data, such as rough terrain, rough\nslopes, steps, and discrete terrain. The behavior for each\nterrain is detailed in the following subsections."}, {"title": "A. Flat Ground", "content": "We trained the model with flat ground walking data for 50\nseconds at both velocities of 1 m/s and 0.3 m/s. The diffusion\npolicy successfully learned to walk on flat ground for more\nthan 60 seconds at both velocities as shown in Fig. 4."}, {"title": "B. Slopes", "content": "The training data included discrete slopes of 5.7 and 10.2\ndegrees at both velocities of 0.3 m/s and 1 m/s. During\nsampling from the diffusion policy, the robot managed to\nwalk on slopes up to 18.4 degrees at 1 m/s and up to 16.7\ndegrees at 0.3 m/s for more than 60 seconds. This indicates\nthat the diffusion policy can interpolate walking behaviors\nfor slopes not included in the training data, as well as handle\nslightly steeper slopes. This can be seen in Fig. 4."}, {"title": "C. Rough Terrain", "content": "No rough terrain walking data was used during training.\nNevertheless, the policy generated actions that enabled walk-\ning on rough terrain for 20 seconds at 1 m/s and more than\n60 seconds at 0.3 m/s as shown in Fig. 4."}, {"title": "D. Rough Slopes and Steps", "content": "The diffusion policy was not specifically trained on rough\nslopes or steps. Despite this, the policy successfully gener-\nated walking behaviors on these terrains at a velocity of 1\nm/s as shown in Fig. 4."}, {"title": "E. Discrete Terrain", "content": "The diffusion policy generated walking trajectories for 30\nseconds on discrete terrain at 1 m/s and more than 60 seconds\nat 0.3 m/s as shown in Fig. 4."}, {"title": "VI. CONCLUSIONS & DISCUSSION", "content": "A Diffusion based policy controller is designed and\ndeployed in the Simulation, which is very effective and\npowerful in learning Multiple behaviours and Interpolating\nBehaviours. As we have demonstrated in Section III, we\nhave achieved walking on known terrains and on unknown\nterrains. It is evident that the robot is falling after some time\nin uneven terrains, but this can be solved by training with\nmore velocities because the results signify that velocity plays\na major role in balancing unseen terrains. Velocity tracking\nof the controller is not as expected and can be improved\nby training the network for more time steps and with more\ndata. This controller is limited by Short-sightedness as it only\ngenerates a single future time step from the past time steps.\n(1) The diffusion policy can be improved by predicting\nmore future time steps instead of a single step and, in\nparallel, achieving sampling time which is necessary for good\ncontrol of the robot. (2) Deploying these diffusion policies in\nhardware and making these types of policies more realistic.\n(3) Integrating Image/video data with the architecture in\nlatent space will uncover the greater potential of this kind\nof generative policies."}]}