{"title": "CRMSP: A Semi-supervised Approach for Key Information Extraction with Class-Rebalancing and Merged Semantic Pseudo-Labeling", "authors": ["Qi Zhang", "Yonghong Song", "Pengcheng Guo", "Yangyang Hui"], "abstract": "There is a growing demand in the field of KIE (Key Information Extraction) to apply semi-supervised learning to save manpower and costs, as training document data using fully-supervised methods requires labor-intensive manual annotation. The main challenges of applying SSL in the KIE are (1) underestimation of the confidence of tail classes in the long-tailed distribution and (2) difficulty in achieving intra-class compactness and inter-class separability of tail features. To address these challenges, we propose a novel semi-supervised approach for KIE with Class-Rebalancing and Merged Semantic Pseudo-Labeling (CRMSP). Firstly, the Class-Rebalancing Pseudo-Labeling (CRP) module introduces a reweighting factor to rebalance pseudo-labels, increasing attention to tail classes. Secondly, we propose the Merged Semantic Pseudo-Labeling (MSP) module to cluster tail features of unlabeled data by assigning samples to Merged Prototypes (MP). Additionally, we designed a new contrastive loss specifically for MSP. Extensive experimental results on three well-known benchmarks demonstrate that CRMSP achieves state-of-the-art performance. Remarkably, CRMSP achieves 3.24% f1-score improvement over state-of-the-art on the CORD.", "sections": [{"title": "1. Introduction", "content": "Key Information Extraction (KIE) as the downstream task of Optical Character Recognition (OCR) is the process of extracting structured information from documents. KIE generally includes tasks such as named entity recognition and relation extraction, structured information extraction, and document classification. KIE has various applications in real-life scenarios, including bill processing, medical record handling, contract analysis, and resume processing. KIE is a challenging task since documents involve different types of information, including images, text, and layout. Recently, many multimodal pre-trained methods [1, 2, 3] for KIE have been proposed to fickle this problem. However, these multimodal pre-trained methods require annotation for multiple types of information, which further increases time and manpower costs.\nSemi-supervised learning (SSL) [4] tackles situations with limited labeled and abundant unlabeled data [5, 6, 7, 8, 9], bridging the gap between supervised and unsupervised learning for enhanced model performance. Existing SSL approaches [5, 10] are to perform consistency regularization between weakly and strongly augmented views of unlabeled data based on the pseudo-labels predicted by the model as targets, thereby mitigating the model's sensitivity to small variations in similar samples within the input space. The performance of these SSL methods based on consistency regularization depends on whether the classes are balanced and the intra-class compactness and inter-class separability of the model in the feature space.\nSpecifically, the first one is that the confidence of tail classes in the long-tailed distribution [11] is underestimated, leading to the model exhibiting higher confidence in predicting samples from the head classes. As shown in Fig. 1(a), both labeled and unlabeled data exhibit a long-tailed distribution. This phenomenon implies that pseudo-labels are more likely to belong to head"}, {"title": "2. Related Work", "content": "Transformer-based pre-training has demonstrated success across various KIE tasks, where extensive unlabeled document datasets are leveraged for model pre-training, preceding fine-tuning on downstream tasks. Numerous existing frameworks [15, 1, 2, 3] have investigated pre-training approaches\non documents. LayoutLM [15] achieved significant improvements in various document understanding tasks by jointly pre-training text and layout. LayoutLMv2 [1] greatly enhanced the model's image understanding capability by integrating visual feature information into the pre-training process. LayoutLMv3 [2] overcame the differences between text and image in pre-training objectives and promoted multi-modal representation learning. Our approach utilizes these multi-modal models based on image, text, and layout as encoders, extending the scope of semi-supervised methods to the KIE domain."}, {"title": "2.2. Semi-supervised learning", "content": "SSL is a learning approach focused on building models that leverage both labeled and unlabeled data. While unlabeled data is crucial for SSL, generating pseudo-labels from model predictions remains a challenge. Existing approaches, including pseudo-labeling [16], consistency regularization [17, 18], generative methods [19, 20] and hybrid methods [21, 5, 6, 7, 8, 9]. However, pseudo-labels can introduce bias, particularly in the presence of imbalanced data, adversely affecting model performance. To mitigate this issue, previous works have explored various strategies such as threshold adjustment [6, 7, 9], incorporating additional classifiers [22, 14]. However, designing dynamic thresholds is complex and computationally intensive. In our work, we directly incorporate an additional branch for semantic pseudo-label classification, which effectively promotes intra-class compactness and inter-class separability for imbalanced classes, without the need for designing complex dynamic threshold strategies."}, {"title": "2.3. Imbalanced learning", "content": "Class-imbalanced supervised learning is of great interest both in theory and in practice. Recent works include resampling [23, 24] and reweighting [25] which rebalance the contribution of each class, while others focus on reweighting the given loss function by a factor inversely proportional to the sampling frequency in a class-wise manner. [25] proposed a suppressed consistency loss to suppress the loss on minority classes. [26] proposed Distribution Aligning Refinery (DARP) to refine pseudo-labels for SSL under assuming class-imbalanced training distributions. CReST proposed a re-sampling method to iteratively refine the model by supplementing the labeled set with high-quality pseudo-labels, where minority classes are updated more aggressively than majority classes. DASO adaptively blends the linear and semantic pseudo-labels within each class to mitigate the overall bias across the\nclass for imbalanced semi-supervised learning. In our work, we alleviate the class-imbalanced problem by directly rebalancing pseudo-labels according to distributions between head and tail classes instead of designing complicated reweighting losses."}, {"title": "3. Proposed Method", "content": "For a C-class semi-supervised classification problem, let $X = \\{(x_b, y_b)\\}_{b=1}^B$ be a batch of B labeled samples, where $x_b$ are the training samples and $y_b$ are the ground-truth, $y \\in Y = \\{1,...,C\\}$. Meanwhile, let $U = \\{u_b\\}_{b=1}^{\\mu B}$ be a batch of $\\mu B$ unlabeled samples, where the hyperparameter $\\mu$ is used to control the batch size of unlabeled samples. Note that the underlying ground truth $\\hat{y}$ of unlabeled data may be different from labeled data, $\\hat{y} \\in Y$, $y = \\{1, ..., C'\\}$.\nFor the labeled data, the input $x_b$ is paired with the label $y_b$ to train the base model $f(\\cdot)$ through calculating supervised loss $L_{sup}$, generating features $z_b$. For the unlabeled data, unlabeled samples are sent to the base model $f(\\cdot)$ as inputs after weak augmentation $A_w$ and strong augmentation $A_s$. Both are followed by a classification head $h(\\cdot)$ and a projection head $g(\\cdot)$ to get $p^w = h\\circ f(A_w(u))$, $z^w = g\\circ f(A_w(u))$, $p^s = h\\circ f(A_s(u))$ and $z^s = g\\circ f(A_s(u))$. The Class-Rebalancing Pseudo-labeling module is employed to alleviate the imbalance problem of pseudo-labels. The rebalanced pseudo-labels $p \\in \\mathbb{R}^C$ are then assigned to calculate the unsupervised loss $L_{un}$. The Merged Semantic Pseudo-Labeling module generates merged semantic pseudo-labels of unlabeled features with the Merged Prototypes $\\tilde{C}$, which is used to compute the contrastive loss $L_{ctr}$. The overall framework is shown in Fig. 3."}, {"title": "3.2. Class-Rebalancing Pseudo-Labeling", "content": "Due to the smaller sample size in the tail classes compared to the head classes, the model tends to generate lower confidence when predicting tail data. The approach in FixMatch [5], which filters out samples based on a fixed threshold applied to the highest confidence, overlooks the numerical disadvantage of the tail data. Experiments show that the predictive distribution of labeled samples is generally positively correlated with the distribution of unlabeled samples. We estimate the approximate sample distribution by calculating the exponential moving average (EMA) of the model's confidence"}, {"title": "3.2.1. Reweighting pseudo-labels", "content": "We observed that the pseudo-labels produced by the model are biased toward the head classes. To augment the model's attention towards tail classes, we introduced a reweighting factor $\\beta$ that enhances the weight of tail classes while correspondingly reducing that of head classes.\nWe first perform the reverse operation on $p_t$ to obtain the reweighting"}, {"title": "3.3. Merged Semantic Pseudo-Labeling", "content": "To obtain semantic pseudo-labels from a feature perspective, DASO [14] involves prototype clustering, which updates the dynamic memory bank with features and ground-truths of labeled data. However, due to the significantly smaller number of tail samples compared to head classes, the tail features in this memory bank are inherently limited. Consequently, the computed tail prototypes lack representation, and it is inappropriate to assume that all tail features are concentrated around this prototype. The semantic pseudo-labels are computed by merely comparing tail samples to this unrepresentative prototype push tail features close to prototypes of other classes, which is detrimental to achieving intra-class compactness and inter-class separability of tail classes."}, {"title": "3.3.1. EMA Model", "content": "The basic assumption in SSL is the smoothness assumption: if two data points are close in high-density regions, their corresponding outputs should also be close. Mean Teacher [17] utilizes this assumption by using unlabeled data. In practice, augmented samples are generated by adding small"}, {"title": "3.3.2. Merged Prototypes (MP) generation", "content": "We first build a set of basic prototypes $C = \\{c_i\\}_{i=1}^C$ from $X$. The basic prototype $c_i$ for every class is efficiently calculated by averaging the feature representations in the dynamic memory bank $Q = \\{Q_i\\}_{i=1}^C$, $Q_i = \\{z_j\\}_{j=1}^{maxsize}$, where $Q_i$ is a queue with a max size. We update Q every iteration by pushing new features $z_i$ and labels $y_i$ from a batch of labeled data.\nThen we determine to construct the super-class for each batch. Based on a common understanding: for an unlabeled sample, if the confidences of several classes are close, their corresponding feature representations in the feature space are close. In such cases, we merge these top-K proximate classes. We achieve this by sorting the confidence predictions $p^w_\\sigma$ obtained from the weak augmentation branch in descending order, resulting in the sorted confidence predictions $s^u$:"}, {"title": "3.3.3. Semantic pseudo-labels", "content": "In a batch, the merged semantic predictions $q^u$ and $q^s$ of the super class space is computed from $z^u$, $z^s$ with the merged prototype $\\tilde{C}$:"}, {"title": "3.4. Loss function", "content": "Following the SSL paradigm, the first two items are supervised loss $L_{sup}$ and unsupervised loss $L_{un}$, respectively. In addition, we include a contrastive loss $L_{ctr}$ to compute the distance of two semantic similarities. The loss minimized by CRMSP is simply:"}, {"title": "3.5. Pseudo Code", "content": "Algorithm 1 presents the algorithm of the entire CRMSP during the training phase."}, {"title": "4. Experiments", "content": "Our experiments are conducted on NVIDIA Tesla V100 GPU. For KIE datasets, the split ratios (i.e., the proportions of labeled data) are 5% and 10%, and the batch size of labeled data is 4, $\\mu$ is set to 1.0. To validate the effectiveness of our proposed SSL approach, we use Transformer-based models"}, {"title": "4.3. Results", "content": "To validate the effectiveness of CRMSP, we perform experiments on FUNSD and CORD for the token classification task. Table 1 shows comparative\nresults with 5% and 10% labeled samples based on LayoutLMv2 and LayoutLMv3. For all methods, we observe that the fl-score increases as the ratio of labeled data increases. In contrast, CRMSP improves the f1-score in the vast majority of experimental settings. Based on LayoutLMv3, it works particularly well on the CORD with 10% labeled data and achieves 4.32%\nand 3.24% f1-score gain compared with FixMatch and suboptimal method FreeMatch, respectively. On the FUNSD, our proposed CRMSP achieved an improvement of fl-score that were 5.81% and 1.20% higher compared\nto FixMatch and the suboptimal model SimMatch based on LayoutLMv2,\nrespectively.\nThis indicates that CRMSP can more effectively utilize labeled data to\nreduce model bias under long-tailed distribution. Furthermore, we observe\nthat our method even achieves an fl-score of 91.30%, which is close to the\nf1-score of 93.30% achieved by the fully-supervised LayoutLMv3 on the KIE\ntask, while using only 10% of the labeled data.\nTo illustrate the generalization of CRMSP, we also conducted experi-\nments on the CIFAR10/100-LT and STL10-LT, as shown in Table 2. We\nconsider rebalancing biased pseudo-labels by matching (e.g., $\\gamma = \\gamma_l = \\gamma_u$)\nor mismatching (e.g., $\\gamma_l = 10$, $\\gamma_u$: unknown) distributions between imbal-\nanced labeled and unlabeled data (X and U) in Table 2. When $\\gamma_l = \\gamma_u$, we\ncompare the proposed CRMSP with several classical (i.e., FixMatch [5] and\nimbalanced (i.e., DARP [26], CReST [24] and DASO [14]) SSL baseline meth-"}, {"title": "4.3.3. Per-class performance", "content": "By comparing the top-1 accuracy of different methods on the STL10-LT across per class, we designate classes C0-C4 as the head classes and C5-C9 as the tail classes. In the head classes, it can be observed that Dash performs well on some head classes (C1, C2, C3), while our method achieves the second-best accuracy in C4 with 89.0%. In the tail classes, our method achieves an accuracy of 95.0% for C8, which is second-best compared to FreeMatch. Remarkably, our method CRMSP achieves the best accuracy of 73.8%, 90.5%, and 87.1% in C5, C6, and C9, respectively, representing improvements of 0.8%, 0.1%, and 2.8% over the second-best class SimMatch. This table highlights the enhancement provided by our method for tail classes in long-tailed distribution."}, {"title": "4.4. Ablation study", "content": "To verify the effectiveness of each component of our proposed method, We conduct extensive ablation studies on the CORD. The results are shown in Table 4. And the evaluation metric for all experiments was the fl-score. We utilize LayoutLMv3 as the base model."}, {"title": "4.4.1. Effectiveness of Reweighting Pseudo-Labels", "content": "To verify the effectiveness of RP, By comparing Experiment 2 and Experiment 5 in Table 4, we can observe that the RP improves the f1-score by 3.37% and 4.23% on the FUNSD and CORD. Table 5 We also compared the performance of tail classes without and with RP. It is found that by adding RP, there is a significant improvement in the results of the tail classes, especially for num.sub_cnt (0.42\u21920.78). Note that LayoutLMv2 is used as the\nbase model.\nTo demonstrate the detailed effectiveness of our proposed RP, we present confusion matrixes of the predictions on the test dataset of FUNSD. As depicted in Fig. 4, the pseudo-labels for the tail classes without RP (e.g., C4, C5 and C6) are underestimated, while the accuracy between the pseudo-labels and the true labels for the head classes is higher. Our proposed RP improves the generation of more balanced pseudo-labels for tail classes, alleviating the issue of long-tailed distribution."}, {"title": "4.4.2. Effectiveness of Contrastive Loss", "content": "When incorporating contrastive loss, CRMSP can further boost the per-formance on all settings by another few points, resulting in 0.52% to 0.66%\nabsolute accuracy improvement by comparing Experiment 3 and Experiment\n4 in Table 4."}, {"title": "4.4.3. Effectiveness of Merged Prototypes", "content": "Comparing Experiment 4 and Experiment 5 in Table 4, we observed that\nf1-score improved by 1.21% and 1.51% on the FUNSD and CORD, respec-\ntively.\nTo demonstrate the effectiveness of our proposed MP, we present the\ncomparison of t-SNE visualization of unlabeled data. As shown in Fig. 5,\nthe MP helps the tail class (e.g., C6) to be separated from the confusion class\nand better clustering is achieved. Other confusing features (e.g., CO and C1)\nare also clustered more compactly. Fig. 5 effectively promotes intra-class\ncompactness and inter-class separability of unlabeled tail classes in feature\nspace."}, {"title": "4.4.4. Ablation study on $ \\lambda_{un}$", "content": "In Fig. 6, we study the effect of the temperature hyper-parameter $\\lambda_{un}$ to compute the weights for unsupervised loss described in Eq. 11. We empirically find that, for both FUNSD and CORD, $\\lambda_{un} = 0.1$ shows the best performance."}, {"title": "4.4.5. Ablation study on $\\lambda_{ctr}$", "content": "In Fig. 7, we investigate the impact of the temperature hyper-parameter $\\lambda_{ctr}$ on computing the weights for the contrastive loss described in Eq. 11. $\\lambda_{ctr} = 0.1$ yields the optimal performance on the FUNSD and CORD."}, {"title": "4.4.6. Ablation study on K", "content": "The influence of different K on the fl-score on the FUNSD and CORD is illustrated in Fig. 8. We notice that K = 5 provides the best f1-score among all tested values. When K is set to a small value, prototypes for some tail samples lack representation. Comparing these tail samples with non-representative prototypes results in semantic pseudo-labels that push the feature in the wrong direction in the feature space, leading to classification errors. On the other hand, if K is set too large, although the new sample features are effectively separated from classes not belonging to this super-class, the super feature range extends far beyond the range of variations in tail features, causing internal confusion within the super-class."}, {"title": "4.4.7. Case Study", "content": "We present the output of samples for Ground-truth, FixMatch, and CRMSP on both the CORD and FUNSD. On the CORD, as depicted in Fig. 9, the tail class sub_total.discount_price in the ground-truth is incorrectly classified as total.total_etc by FixMatch. This misclassification is corrected by our proposed CRMSP approach. The tail classes menu.sub_cnt and menu.sub_nm are erroneously associated with their respective classes menu.cnt and menu.nm, but our proposed CRMSP method adeptly distinguishes between them.\nOn the FUNSD, as shown in Fig. 10, FixMatch misclassifies B-QUESTION and I-QUESTION as B-ANSWER and I-ANSWER, respectively. However, CRMSP correctly identifies these tail classes."}, {"title": "5. Conclusion", "content": "In this paper, we propose a novel semi-supervised approach for key in-formation extraction with Class-Rebalancing and Merged Semantic Pseudo-Labeling (CRMSP). Firstly, the Class-Rebalancing Pseudo-Labeling (CRP) module is proposed to directly rebalance pseudo-labels with a reweighting factor, increasing attention to tail classes. Secondly, the Merged Semantic Pseudo-Labeling (MSP) module is proposed to achieve intra-class compact-ness and inter-class separability of unlabeled tail classes in feature space by assigning samples to Merged Prototypes (MP). We even achieved close to fully-supervised learning in the semi-supervised setting. Extensive ex-"}]}