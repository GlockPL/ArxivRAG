{"title": "CRMSP: A Semi-supervised Approach for Key Information Extraction with Class-Rebalancing and Merged Semantic Pseudo-Labeling", "authors": ["Qi Zhang", "Yonghong Song", "Pengcheng Guo", "Yangyang Hui"], "abstract": "There is a growing demand in the field of KIE (Key Information Extraction) to apply semi-supervised learning to save manpower and costs, as training document data using fully-supervised methods requires labor-intensive manual annotation. The main challenges of applying SSL in the KIE are (1) underestimation of the confidence of tail classes in the long-tailed distribution and (2) difficulty in achieving intra-class compactness and inter-class separability of tail features. To address these challenges, we propose a novel semi-supervised approach for KIE with Class-Rebalancing and Merged Semantic Pseudo-Labeling (CRMSP). Firstly, the Class-Rebalancing Pseudo-Labeling (CRP) module introduces a reweighting factor to rebalance pseudo-labels, increasing attention to tail classes. Secondly, we propose the Merged Semantic Pseudo-Labeling (MSP) module to cluster tail features of unlabeled data by assigning samples to Merged Prototypes (MP). Additionally, we designed a new contrastive loss specifically for MSP. Extensive experimental results on three well-known benchmarks demonstrate that CRMSP achieves state-of-the-art performance. Remarkably, CRMSP achieves 3.24% f1-score improvement over state-of-the-art on the CORD.", "sections": [{"title": "1. Introduction", "content": "Key Information Extraction (KIE) as the downstream task of Optical Character Recognition (OCR) is the process of extracting structured information from documents. KIE generally includes tasks such as named entity recognition and relation extraction, structured information extraction, and document classification. KIE has various applications in real-life scenarios, including bill processing, medical record handling, contract analysis, and resume processing. KIE is a challenging task since documents involve different types of information, including images, text, and layout. Recently, many multimodal pre-trained methods [1, 2, 3] for KIE have been proposed to fickle this problem. However, these multimodal pre-trained methods require annotation for multiple types of information, which further increases time and manpower costs.\nSemi-supervised learning (SSL) [4] tackles situations with limited labeled and abundant unlabeled data [5, 6, 7, 8, 9], bridging the gap between supervised and unsupervised learning for enhanced model performance. Existing SSL approaches [5, 10] are to perform consistency regularization between weakly and strongly augmented views of unlabeled data based on the pseudolabels predicted by the model as targets, thereby mitigating the model's sensitivity to small variations in similar samples within the input space. The performance of these SSL methods based on consistency regularization depends on whether the classes are balanced and the intra-class compactness and inter-class separability of the model in the feature space.\nSpecifically, the first one is that the confidence of tail classes in the longtailed distribution [11] is underestimated, leading to the model exhibiting higher confidence in predicting samples from the head classes. As shown in Fig. 1(a), both labeled and unlabeled data exhibit a long-tailed distribution. This phenomenon implies that pseudo-labels are more likely to belong to head"}, {"title": "2. Related Work", "content": "Transformer-based pre-training has demonstrated success across various KIE tasks, where extensive unlabeled document datasets are leveraged for model pre-training, preceding fine-tuning on downstream tasks. Numerous existing frameworks [15, 1, 2, 3] have investigated pre-training approaches"}, {"title": "2.2. Semi-supervised learning", "content": "SSL is a learning approach focused on building models that leverage both labeled and unlabeled data. While unlabeled data is crucial for SSL, generating pseudo-labels from model predictions remains a challenge. Existing approaches, including pseudo-labeling [16], consistency regularization [17, 18], generative methods [19, 20] and hybrid methods [21, 5, 6, 7, 8, 9]. However, pseudo-labels can introduce bias, particularly in the presence of imbalanced data, adversely affecting model performance. To mitigate this issue, previous works have explored various strategies such as threshold adjustment [6, 7, 9], incorporating additional classifiers [22, 14]. However, designing dynamic thresholds is complex and computationally intensive. In our work, we directly incorporate an additional branch for semantic pseudo-label classification, which effectively promotes intra-class compactness and inter-class separability for imbalanced classes, without the need for designing complex dynamic threshold strategies."}, {"title": "2.3. Imbalanced learning", "content": "Class-imbalanced supervised learning is of great interest both in theory and in practice. Recent works include resampling [23, 24] and reweighting [25] which rebalance the contribution of each class, while others focus on reweighting the given loss function by a factor inversely proportional to the sampling frequency in a class-wise manner. [25] proposed a suppressed consistency loss to suppress the loss on minority classes. [26] proposed Distribution Aligning Refinery (DARP) to refine pseudo-labels for SSL under assuming class-imbalanced training distributions. CReST proposed a re-sampling method to iteratively refine the model by supplementing the labeled set with high-quality pseudo-labels, where minority classes are updated more aggressively than majority classes. DASO adaptively blends the linear and semantic pseudo-labels within each class to mitigate the overall bias across the"}, {"title": "3. Proposed Method", "content": "For a C-class semi-supervised classification problem, let $X = \\{(x_b, y_b)\\}_{b=1}^B$ be a batch of B labeled samples, where $x_b$ are the training samples and $y_b$ are the ground-truth, $y \\in Y = \\{1,...,C\\}$. Meanwhile, let $U = \\{u_b\\}_{b=1}^{\\mu B}$ be a batch of $\\mu B$ unlabeled samples, where the hyperparameter $\\mu$ is used to control the batch size of unlabeled samples. Note that the underlying ground truth $\\hat{y}$ of unlabeled data may be different from labeled data, $\\hat{y} \\in Y, y = \\{1, ..., C'\\}.\nFor the labeled data, the input $x_b$ is paired with the label $y_b$ to train the base model $f(\\cdot)$ through calculating supervised loss $L_{sup}$, generating features $z_b$. For the unlabeled data, unlabeled samples are sent to the base model $f(\\cdot)$ as inputs after weak augmentation $A_w$ and strong augmentation $A_s$.\nBoth are followed by a classification head $h(\\cdot)$ and a projection head $g(\\cdot)$ to get $p^u = h \\circ f(A_w(u)), z^u = g \\circ f(A_w(u)), p^s = h \\circ f(A_s(u))$ and $z^s = g \\circ f(A_s(u))$. The Class-Rebalancing Pseudo-labeling module is employed to alleviate the imbalance problem of pseudo-labels. The rebalanced pseudolabels $p \\in \\mathbb{R}^C$ are then assigned to calculate the unsupervised loss $L_{un}$.\nThe Merged Semantic Pseudo-Labeling module generates merged semantic pseudo-labels of unlabeled features with the Merged Prototypes $\\hat{C}$, which is used to compute the contrastive loss $L_{ctr}$. The overall framework is shown in Fig. 3."}, {"title": "3.2. Class-Rebalancing Pseudo-Labeling", "content": "Due to the smaller sample size in the tail classes compared to the head classes, the model tends to generate lower confidence when predicting tail data. The approach in FixMatch [5], which filters out samples based on a fixed threshold applied to the highest confidence, overlooks the numerical disadvantage of the tail data. Experiments show that the predictive distribution of labeled samples is generally positively correlated with the distribution of unlabeled samples. We estimate the approximate sample distribution by calculating the exponential moving average (EMA) of the model's confidence"}, {"title": "3.2.1. Reweighting pseudo-labels", "content": "We observed that the pseudo-labels produced by the model are biased toward the head classes. To augment the model's attention towards tail classes, we introduced a reweighting factor $\\beta$ that enhances the weight of tail classes while correspondingly reducing that of head classes.\nWe first perform the reverse operation on $\\bar{p_t}$ to obtain the reweighting"}, {"title": "3.3. Merged Semantic Pseudo-Labeling", "content": "To obtain semantic pseudo-labels from a feature perspective, DASO [14] involves prototype clustering, which updates the dynamic memory bank with features and ground-truths of labeled data. However, due to the significantly smaller number of tail samples compared to head classes, the tail features in this memory bank are inherently limited. Consequently, the computed tail prototypes lack representation, and it is inappropriate to assume that all tail features are concentrated around this prototype. The semantic pseudo-labels are computed by merely comparing tail samples to this unrepresentative prototype push tail features close to prototypes of other classes, which is detrimental to achieving intra-class compactness and inter-class separability of tail classes."}, {"title": "3.3.1. EMA Model", "content": "The basic assumption in SSL is the smoothness assumption: if two data points are close in high-density regions, their corresponding outputs should also be close. Mean Teacher [17] utilizes this assumption by using unlabeled data. In practice, augmented samples are generated by adding small"}, {"title": "3.3.2. Merged Prototypes (MP) generation", "content": "We first build a set of basic prototypes $C = \\{c_i\\}_{i=1}^C$ from $X$. The basic prototype $c_i$ for every class is efficiently calculated by averaging the feature representations in the dynamic memory bank $Q = \\{Q_i\\}_{i=1}^C, Q_i = \\{z_j\\}_{j=1}^{maxsize}$, where $Q_i$ is a queue with a max size. We update $Q$ every iteration by pushing new features $z_j$ and labels $y_j$ from a batch of labeled data.\nThen we determine to construct the super-class for each batch. Based on a common understanding: for an unlabeled sample, if the confidences of several classes are close, their corresponding feature representations in the feature space are close. In such cases, we merge these top-K proximate classes. We achieve this by sorting the confidence predictions $p_u$ obtained from the weak augmentation branch in descending order, resulting in the sorted confidence predictions $s_u$:"}, {"title": "3.3.3. Semantic pseudo-labels", "content": "In a batch, the merged semantic predictions $q_u$ and $q_s$ of the super class space is computed from $z^u, z^s$ with the merged prototype $\\hat{C}$:"}, {"title": "3.4. Loss function", "content": "Following the SSL paradigm, the first two items are supervised loss $L_{sup}$ and unsupervised loss $L_{un}$, respectively. In addition, we include a contrastive loss $L_{ctr}$ to compute the distance of two semantic similarities. The loss minimized by CRMSP is simply:"}, {"title": "4. Experiments", "content": "Our approach is compared with both classical and imbalanced SSL methods. Classical SSL methods include FixMatch [5], Dash [6], FlexMatch [7], SimMatch [8] and FreeMatch [9]. Imbalanced SSL methods include DARP [26], CREST [24] and DASO [14].\nOur experiments are conducted on NVIDIA Tesla V100 GPU. For KIE datasets, the split ratios (i.e., the proportions of labeled data) are 5% and 10%, and the batch size of labeled data is 4, $\\mu$ is set to 1.0. To validate the effectiveness of our proposed SSL approach, we use Transformer-based models"}, {"title": "4.3. Results", "content": "To validate the effectiveness of CRMSP, we perform experiments on FUNSD and CORD for the token classification task. Table 1 shows comparative"}, {"title": "4.4. Ablation study", "content": "To verify the effectiveness of each component of our proposed method, We conduct extensive ablation studies on the CORD. The results are shown in Table 4. And the evaluation metric for all experiments was the fl-score. We utilize LayoutLMv3 as the base model."}, {"title": "4.4.1. Effectiveness of Reweighting Pseudo-Labels", "content": "To verify the effectiveness of RP, By comparing Experiment 2 and Experiment 5 in Table 4, we can observe that the RP improves the f1-score by 3.37% and 4.23% on the FUNSD and CORD. Table 5 We also compared the performance of tail classes without and with RP. It is found that by adding RP, there is a significant improvement in the results of the tail classes, especially for num.subcnt (0.42\u21920.78). Note that LayoutLMv2 is used as the base model.\nTo demonstrate the detailed effectiveness of our proposed RP, we present confusion matrixes of the predictions on the test dataset of FUNSD. As depicted in Fig. 4, the pseudo-labels for the tail classes without RP (e.g., C4, C5 and C6) are underestimated, while the accuracy between the pseudo-labels and the true labels for the head classes is higher. Our proposed RP improves the generation of more balanced pseudo-labels for tail classes, alleviating the issue of long-tailed distribution."}, {"title": "4.4.2. Effectiveness of Contrastive Loss", "content": "When incorporating contrastive loss, CRMSP can further boost the performance on all settings by another few points, resulting in 0.52% to 0.66% absolute accuracy improvement by comparing Experiment 3 and Experiment 4 in Table 4."}, {"title": "4.4.3. Effectiveness of Merged Prototypes", "content": "Comparing Experiment 4 and Experiment 5 in Table 4, we observed that f1-score improved by 1.21% and 1.51% on the FUNSD and CORD, respectively.\nTo demonstrate the effectiveness of our proposed MP, we present the comparison of t-SNE visualization of unlabeled data. As shown in Fig. 5, the MP helps the tail class (e.g., C6) to be separated from the confusion class and better clustering is achieved. Other confusing features (e.g., CO and C1) are also clustered more compactly. Fig. 5 effectively promotes intra-class compactness and inter-class separability of unlabeled tail classes in feature space."}, {"title": "4.4.4. Ablation study on Aun", "content": "In Fig. 6, we study the effect of the temperature hyper-parameter $\\lambda_{un}$ to compute the weights for unsupervised loss described in Eq. 11. We empirically find that, for both FUNSD and CORD, $\\lambda_{un} = 0.1$ shows the best performance."}, {"title": "4.4.5. Ablation study on Actr", "content": "In Fig. 7, we investigate the impact of the temperature hyper-parameter $\\lambda_{ctr}$ on computing the weights for the contrastive loss described in Eq. 11. $\\lambda_{ctr} = 0.1$ yields the optimal performance on the FUNSD and CORD."}, {"title": "4.4.6. Ablation study on K", "content": "The influence of different K on the fl-score on the FUNSD and CORD is illustrated in Fig. 8. We notice that K = 5 provides the best f1-score among all tested values. When K is set to a small value, prototypes for some tail samples lack representation. Comparing these tail samples with nonrepresentative prototypes results in semantic pseudo-labels that push the feature in the wrong direction in the feature space, leading to classification errors. On the other hand, if K is set too large, although the new sample features are effectively separated from classes not belonging to this superclass, the super feature range extends far beyond the range of variations in tail features, causing internal confusion within the super-class."}, {"title": "4.4.7. Case Study", "content": "We present the output of samples for Ground-truth, FixMatch, and CRMSP on both the CORD and FUNSD. On the CORD, as depicted in Fig. 9, the tail class subtotal.discountprice in the ground-truth is incorrectly classified as total.total etc by FixMatch. This misclassification is corrected by our proposed CRMSP approach. The tail classes menu.sub cnt and menu.sub nm are erroneously associated with their respective classes menu.cnt and menu.nm, but our proposed CRMSP method adeptly distinguishes between them.\nOn the FUNSD, as shown in Fig. 10, FixMatch misclassifies B-QUESTION and I-QUESTION as B-ANSWER and I-ANSWER, respectively. However, CRMSP correctly identifies these tail classes."}, {"title": "5. Conclusion", "content": "In this paper, we propose a novel semi-supervised approach for key information extraction with Class-Rebalancing and Merged Semantic Pseudo-Labeling (CRMSP). Firstly, the Class-Rebalancing Pseudo-Labeling (CRP) module is proposed to directly rebalance pseudo-labels with a reweighting factor, increasing attention to tail classes. Secondly, the Merged Semantic Pseudo-Labeling (MSP) module is proposed to achieve intra-class compactness and inter-class separability of unlabeled tail classes in feature space by assigning samples to Merged Prototypes (MP). We even achieved close to fully-supervised learning in the semi-supervised setting. Extensive experimental results have demonstrated the proposed CRMSP surpasses other state-of-the-art methods on three benchmarks. Our findings suggest that the proposed approach can obtain high-quality pseudo-labels from a larger amount of unlabeled data, which provides a good solution for semi-supervised learning."}]}