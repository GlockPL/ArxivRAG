{"title": "FRAMEBRIDGE: IMPROVING IMAGE-TO-VIDEO GENERATION WITH BRIDGE MODELS", "authors": ["Yuji Wang", "Zehua Chen", "Xiaoyu Chen", "Jun Zhu", "Jianfei Chen"], "abstract": "Image-to-video (I2V) generation is gaining increasing attention with its wide application in video synthesis. Recently, diffusion-based I2V models have achieved remarkable progress given their novel design on network architecture, cascaded framework, and motion representation. However, restricted by their noise-to-data generation process, diffusion-based methods inevitably suffer the difficulty to generate video samples with both appearance consistency and temporal coherence from an uninformative Gaussian noise, which may limit their synthesis quality. In this work, we present FrameBridge, taking the given static image as the prior of video target and establishing a tractable bridge model between them. By formulating I2V synthesis as a frames-to-frames generation task and modelling it with a data-to-data process, we fully exploit the information in input image and facilitate the generative model to learn the image animation process. In two popular settings of training I2V models, namely fine-tuning a pre-trained text-to-video (T2V) model or training from scratch, we further propose two techniques, SNR-Aligned Fine-tuning (SAF) and neural prior, which improve the fine-tuning efficiency of diffusion-based T2V models to FrameBridge and the synthesis quality of bridge-based I2V models respectively. Experiments conducted on WebVid-2M and UCF-101 demonstrate that: (1) our FrameBridge achieves superior I2V quality in comparison with the diffusion counterpart (zero-shot FVD 83 vs. 176 on MSR-VTT and non-zero-shot FVD 122 vs. 171 on UCF-101); (2) our proposed SAF and neural prior effectively enhance the ability of bridge-based I2V models in the scenarios of fine-tuning and training from scratch. Demo samples can be visited at: https://framebridge-demo.github.io/.", "sections": [{"title": "1 INTRODUCTION", "content": "Image-to-video (I2V) generation, commonly referred as image animation, aims at generating consecutive video frames from a static image (Xing et al., 2023; Ni et al., 2023; Zhang et al., 2024a; Guo et al., 2023; Hu et al., 2022), i.e., a frame-to-frames generation task where maintaining appearance consistency and ensuring temporal coherence of generated video frames are key evaluation criteria (Xing et al., 2023; Zhang et al., 2024a). With the recent progress in video synthesis (Brooks et al., 2024; Yang et al., 2024; Blattmann et al., 2023; Bao et al., 2024), several diffusion-based I2V frameworks have been proposed, with novel designs on network architecture (Xing et al., 2023; Zhang et al., 2024a; Chen et al., 2023b; Ren et al., 2024; Lu et al., 2023), cascaded framework (Jain et al., 2024; Zhang et al., 2023), and motion representation (Zhang et al., 2024b; Ni et al., 2023). However, although these methods have demonstrated the potential of diffusion models (Ho et al., 2020; Song et al., 2020) in I2V synthesis, restricted by their noise-to-data generation process, they inevitably suffer the difficulty to generate video samples required by both appearance consistency and temporal coherence from uninformative random noise. With the noise-to-data sampling trajectory which is inherently mismatched with the frame-to-frames synthesis process of I2V task, previous diffusion-based methods increase the burden of generative models, which may result in limited synthesis quality."}, {"title": "2 RELATED WORKS", "content": "Diffusion-based I2V Generation I2V generation (Blattmann et al., 2023; Chen et al., 2023a; Li et al., 2024) aims to synthesize videos based on the content of a given image and additional conditions (e.g., textual descriptions). Recently, diffusion-based I2V methods have achieved remarkable results. Several approaches develop a multi-stage diffusion framework, where intermediate videos generated in earlier stages guide the subsequent generation process (Jain et al., 2024; Zhang et al., 2023; Shi et al., 2024). To enhance the temporal consistency and improve the controllability of video content (Wang et al., 2024; Ren et al., 2024), recent works have proposed carefully designed network architectures (Xing et al., 2023; Ma et al., 2024a; Chen et al., 2023b; Ren et al., 2024) or specific components (Guo et al., 2023; Zhang et al., 2024a) to better integrate image conditions into the backbone of video diffusion models. However, the process and frameworks of diffusion-based I2V models has not been fully explored, which may exist a room for improvement on I2V synthesis quality.\nNoise Manipulation for Video Diffusion Models Several works have explored to improve the uninformative prior distribution of diffusion models. PYoCo (Ge et al., 2023) recently proposes to use correlated noise for each frame in both training and inference. ConsistI2V (Ren et al., 2024), FreeInit (Wu et al., 2023), and CIL (Zhao et al., 2024) present training-free strategies to better align the training and inference distribution of diffusion prior. These strategies focus on improving the noise distribution to enhance the quality of synthesized videos, while they still suffer the restriction of noise-to-data diffusion framework, which may limit their endeavor to utilize the entire information (e.g., both large-scale features and fine-grained details) contained in the given image. In contrast, we propose a data-to-data framework and utilize deterministic prior rather than Gaussian noise, allowing us to leverage the clean input image as prior information.\nBridge Models Recently, bridge models (Chen et al.; Tong et al., 2023; Liu et al., 2023; Zhou et al., 2023; Chen et al., 2023c), which overcome the restriction of Gaussian prior in diffusion models, have gained increasing attention. They have demonstrated the advantages of data-to-data generation process over the noise-to-data one on image-to-image translation (Liu et al., 2023; Zhou et al., 2023) and text-to-speech synthesis (Chen et al., 2023c) tasks. In this work, we make the first attempt to extend bridge models to I2V synthesis and further propose two improving techniques for bridge models, enabling efficient fine-tuning from diffusion models and stronger prior for video target."}, {"title": "3 BACKGROUND", "content": "Problem Formulation I2V aims at generating an video clip $v \\in \\mathbb{R}^{L\\times H\\times W\\times 3}$ with a number of $L$ frames from a static image, e.g., the initial frame $v^1 \\in \\mathbb{R}^{H\\times W\\times 3}$ of video clip $v$. In I2V systems (Xing et al., 2023; Blattmann et al., 2023), an VAE-based compression network is usually leveraged to first transform the video $v$ into a latent $z \\in \\mathbb{R}^{L\\times h\\times w\\times d}$ in a per-frame manner with a pre-trained image encoder $E(v)$, where $h = \\frac{H}{p}, w = \\frac{W}{p}, p > 1$ and $d$ are the spatial compression ratio and the number of output channels. Then, generative models are designed in this compressed space to learn the conditional distribution $p_z(z|z^1, c)$, where $z^1 \\in \\mathbb{R}^{h\\times w\\times d}$ is the compressed latent of the initial frame $v^1$ and $c$ denotes other guidance such as the text prompt (Ma et al., 2024a; Chen et al., 2023b) or the image class condition (Ni et al., 2023; Zhang et al., 2024b). In sampling, we first synthesize the latent $z$ conditioned on the latent of given static frame $z^1$, and then decode the video clip with pre-trained VAE decoder $D(z)$. As formulated, I2V synthesis actually seeks to generate consecutive frames in $v$ from a single frame $v^1$, i.e., a frame-to-frames generation process where the appearance details of $v^1$ should be preserved in generated $v$ and the animation in $v$ should start from $v^1$ and remain temporal coherent.\nDiffusion-based I2V Synthesis Diffusion models (Ho et al., 2020; Sohl-Dickstein et al., 2015) have been popularly leveraged to learn the conditional distribution $p_z(z|z^1, c)$. These models are composed of two processes. A forward process gradually converts the video latent $p_0(Z_0|z^1, c) \\approx P_{data} (Z_0|z^1, c)$ to a known prior distribution $P_{T,diff}(Z_T) \\approx P_{prior,diff}(z_T)$ with a forward SDE (Song et al., 2020):\n$dzt = f(t)ztdt + g(t)dw, \\quad Z_0 \\sim P_{data} (Z_0|z^1, c)$,\nwhere $w$ is a Wiener process, $f : \\mathbb{R}^D \\times [0,T] \\rightarrow \\mathbb{R}^D$ is the drift coefficient, and $g : [0,T] \\rightarrow \\mathbb{R}$ is the diffusion coefficient. $D$ represents the dimension of data $z_0$. The marginal distribution of $z_t$ satisfies $p_{t,diff}(z_t|z^1, c) = \\int_{z_0} p_0 (z_0|z^1, c) p_{t,diff} (z_t|z_0) p_0 (z_0|z^1, c)dz_0$. Here $p_{t,diff}(z_t|z_0) = N(a_t z_0, \\sigma_t^2 I)$, $a_t = e^{\\int_0^t f(t)dt}, \\sigma_t^2 = a_t^2 \\int_0^t g^2(\\tau) d\\tau$ (Kingma et al., 2021).\nGiven the forward process defined by eq. (1), there exists a reverse process with a backward SDE which shares the same marginal distribution $p_{t,diff}(z_t|z^1, c)$ (Song et al., 2020):\n$dzt = [f(t)zt - g(t)^2 \\nabla_{z_t} log p_{t,diff} (z_t|z^1, c)] dt + g(t)dw, \\quad z_T \\sim P_{prior,diff}(z_T)$,\nwhere $w$ is a backward Wiener process.\nTo learn the unknown term in eq. (2), i.e., the score function $\\nabla_{z_t} log p_{t,diff} (z_t|z^1, c)$, usually a U-Net (Ronneberger et al., 2015; Ho et al., 2020) or DiT (Peebles & Xie, 2023; Bao et al., 2023) based neural network is optimized with a denoising objective to predict the noise:\n$\\mathcal{L}_{diff}(\\theta) = \\mathbb{E}_{(z_0,z^1,c) \\sim p_{data}(z_0,z^1,c),t \\sim q(t),z_t \\sim p_{t,diff}(z_t|z_0)} [\\lambda(t) || \\epsilon_{\\theta} (z_t, t, z^1, c) - \\frac{z_t - a_t z_0}{\\sigma_t} ||^2]$,\nwhere $q(t)$ is a distribution of $t$ supporting on $[0, T]$, $\\lambda(t)$ is a time-dependent weight function, and $\\epsilon_{\\theta} (z_t, t)$ is an alternative parameterization method of the score function (Ho et al., 2020).\nLimitations As shown, the forward process of diffusion models gradually injects noise into data samples, which results in a boundary distribution at $t = T$ sharing the same distribution with the injected noise, e.g., the standard Gaussian noise $\\epsilon \\sim N(0, I)$. Therefore, in generation, their sampling process has to start from the uninformative prior distribution $P_{prior,diff}(z_T) \\sim N(0, I)$ and then iteratively synthesize the video latent $z_0$ with learned conditional score function $\\nabla_{z_t} log p_t (z_t|z^1, c)$. Formulating a frame-to-frames generation task with a conditional noise-to-data sampling process, diffusion-based I2V systems suffer the difficulties to generate high-quality samples from uninformative Gaussian noise while preserving the appearance details with condition and keeping the temporal coherence, which may result in limited I2V performance."}, {"title": "4 METHOD", "content": "In this section, we analyze our motivation to build bridge-based I2V system, present our Frame-Bridge, and elaborate our proposed SNR-Aligned Fine-tuning (SAF) and neural prior to further improve bridge-based I2V synthesis quality."}, {"title": "4.1 MOTIVATION", "content": "As discussed above, on one hand, the static image in I2V generation has already provided strong condition information, e.g., the appearance details which should be preserved in generated video. However, the prior of diffusion models ignore this information, rendering diffusion-based I2V systems regenerate entire video information from Gaussian noise. On the other hand, the image animation process should be temporal coherent, ensuring the consistency between video frames. However, the prior of diffusion models makes previous works generate each frame from random noise. Although the generation is conditioned on static image, it would be difficult for a noise-to-data generation process to guarantee the temporal coherence between generated frames.\nToward reducing the mismatch between the generation process and the I2V task, we make the first attempt to establish data-to-data process, i.e., bridge models (Chen et al., 2023c; Zhou et al., 2023), for the frame-to-frames synthesis of I2V. Taking the static image as the prior of consecutive frames in video, we build a bridge between the initial frame and the following ones. In generation, each video frame is generated from the initial frame, which is naturally helpful to preserve appearance details in I2V synthesis. Moreover, as our model learns the frame-to-frames synthesis instead of conditional noise-to-frames generation, we facilitate our model to focus on learning image animation, which benefits temporal coherence of generated video."}, {"title": "4.2 FRAMEBRIDGE", "content": "Considering the given image, i.e., initial frame $z^1$, has provided the appearance details and the starting point of animation for video target, we take it as the prior of following frames. To construct the boundary distributions for bridge models, we replicate the image latent $z^1$ for $L$ times along temporal axis to obtain $z^1 \\in \\mathbb{R}^{L\\times h\\times w\\times d}$ as the prior of video latent $z \\in \\mathbb{R}^{L\\times h\\times w\\times d}$, and establish the bridge process as follows."}, {"title": "Bridge Process", "content": "In Figure 1, we present the overview of FrameBridge and compare it with diffusion-based I2V generation. Different from diffusion-based I2V models using uninformative Gaussian prior, our FrameBridge replaces the Gaussian prior with a Dirac prior $\\delta_{z^1}$, building a bridge process (Zhou et al., 2023) to connect the video target and the replicated image prior $P_{prior,bridge}(Z_T|i, c) \\equiv \\delta_{z^1} (z_T)$. Specifically, the forward process is changed from eq. (1) in diffusion models to:\n$dzt = [f(t)zt + g(t)^2h(zt, t, z_T, z^1, c)] dt + g(t)dw, \\quad z_0 \\sim P_{data} (z_0|z^1, c), \\quad Z_T = z^1$,\nwhere $h(zt, t, z_T, z^1, c) \\equiv \\nabla_{z_t} log P_{T,diff} (Z_T|z_t)$ and $P_{T,diff} (Z_T|z_t)$ is the marginal distribution of diffusion process shown in eq. (1). For bridge process, we denote the marginal distribution of eq. (4) as $P_{t,bridge}(z_t|z^1, c)$. Similar to the forward SDE eq. (1) in diffusion process, the forward process of bridge models eq. (4) also has a reverse process, which shares the same marginal distribution $P_{t,bridge} (z_t|z^1, c)$ and can be represented by the backward SDE:\n$dzt = [f(t)zt - g(t)^2 (s(zt, t, z_T, z^1, c) - h(zt, t, z_T, z^1, c))] dt + g(t)dw, \\quad z_T = z^1$,\nwhere $s(zt, t, z_T, z^1, c) \\equiv \\nabla_{z_t} log P_{t,bridge} (Z_t|Z_T, z^1, c)$. The change from diffusion process to bridge process remove the restriction of noisy prior in diffusion models, allowing the generation process to start from static image rather than previous Gaussian noise. Moreover, as the perturbation kernel $P_{t,bridge}(Z_t|z_0, Z_T, z^1, c)$ in bridge process remains Gaussian (Appendix A), it facilitates us to find connections between the marginal distribution, i.e., the intermediate representations of diffusion and bridge process, and then leverage the power of pre-trained diffusion models for bridge models.\nTraining Objective Analogous to diffusion models, we use a SDE solver to solve eq. (5) when sampling videos. Since $h(zt, t, z_T, z^1, c)$ can be calculated analytically (see Appendix A), we only need to estimate the unknown term $s(zt, t, z_T, z^1, c)$ with neural networks (Kingma et al., 2021). After parameterization (more details can be found in Appendix A), we train FrameBridge models $\\epsilon_{\\theta}(z_t, t, Z_T, z^1, c)$ with the following denoising objective (Chen et al., 2023c):\n$\\mathcal{L}_{bridge} (\\theta) = \\mathbb{E}_{(z_0,z,c) \\sim P_{data}(z_0,z,c), Z_T=z^1,t,z_t \\sim P_{t,bridge}(z_t|z_0,Z_T,z^1,c)} [|| \\frac{\\epsilon_{\\theta}(z_t, t, Z_T, z^1, c) - \\frac{z_t}{a_t} z_0}{\\sigma_t} ||^2]$"}, {"title": "4.3 SNR-ALIGNED FINE-TUNING", "content": "To implement FrameBridge, one straightforward strategy is to train bridge models from scratch using eq. (6). Meanwhile, another common practice of training I2V models is to fine-tune from pre-trained T2V diffusion models (Chen et al., 2023b;a; Xing et al., 2023; Blattmann et al., 2023; Ma et al., 2024a). A key challenge arises as the marginal distribution of bridge models $P_{t,bridge}(z_t)$ differs from that of diffusion models $P_{t,diff}(z_t)$, limiting the generation performance of fine-tuned FrameBridge as illustrated in Figure 3. To address this issue, we propose the innovative SNR-Aligned Fine-tuning (SAF) technique. By bridging the gap between the two distributions during fine-tuning, SAF enables more seamless knowledge transfer between the two model families. To the best of our knowledge, this is the first attempt to explore how to effectively fine-tune bridge models from pre-trained diffusion models. Our SAF technique consists of the following two steps:\nReparameterization of Bridge Process. In bridge process, the perturbed latent $z_t$ at timestep $t$ can be written as the linear combination of $z_0$, $z_T$ and a Gaussian noise $\\epsilon$: $z_t = a_tz_0 + b_tz_T + c_t\\epsilon$ (detailed expression of $a_t$, $b_t$, $c_t$ can be found in eq. (13)), which is different from $a_tz_0 + \\sigma_t\\epsilon$ in diffusion models due to the change of prior and forward process. Therefore, the pre-trained diffusion models have limited ability to directly denoise such a $z_t$, which impairs effective fine-tuning. To match the distributions of $z_t$, we reparameterize the bridge process by\n$\\tilde{z_t} = \\frac{z_t}{\\sqrt{a_t^2 + c_t^2}} = \\frac{a_t}{\\sqrt{a_t^2 + c_t^2}}z_0 + \\frac{b_tz_T}{\\sqrt{a_t^2 + c_t^2}} + \\frac{c_t}{\\sqrt{a_t^2 + c_t^2}}\\epsilon$.\nThen, $\\tilde{z_t}$ can be represented as the combination of clean data $z_0$ and a Gaussian noise, with the squre sum of coefficients equal to 1. Thus, the reparameterized bridge process $\\tilde{z_t}$ exactly align with a VP diffusion process.\nSNR-based Latent Alignment Although $\\tilde{z_t}$ has the same perturbation kernel as a diffusion process, the marginal distribution of $\\tilde{z_t}$ does not match that of the pre-trained diffusion at timestep $t$, i.e., $a_tZ_0 + \\sigma_t\\epsilon$, hindering bridge models to seamlessly leverage the knowledge of pre-trained diffusion models (see Figure 3). Therefore, we leverage signal-to-noise ratio (SNR) as an indicator to unify the two marginal distributions (Kingma et al., 2021). Specifically, we change the timestep $t$ to another $\\tilde{t}$ such that $\\frac{a_t}{\\sqrt{a_t^2 + c_t^2}} = a_{\\tilde{t}}$, $\\frac{c_t}{\\sqrt{a_t^2 + c_t^2}} = \\sigma_{\\tilde{t}}$, and then $\\tilde{z_t}$ has the same SNR as $a_{\\tilde{t}}Z_0 + \\sigma_{\\tilde{t}}\\epsilon$ in diffusion process. According to the above derivation, we reparameterize the input of bridge models as $\\epsilon_{\\theta,aligned} (\\tilde{z_t}, t, Z_T, z^1, c) \\approx \\epsilon_{\\theta,align} (\\tilde{z_t}, \\tilde{t}, Z_T, z^1, c)$, and initialize $\\epsilon_{\\theta,aligned}$ with the pre-trained T2V diffusion models. Since the marginal distribution of $\\tilde{z_t}$ is complemetely aligned with the marginal of diffusion process at timestep $t$, SAF enables bridge models to fully exploit the denoising capability"}, {"title": "4.4 NEURAL PRIOR", "content": "By establishing a data-to-data process for I2V synthesis, we have been able to reduce the distance between the prior and the target from noise-to-frames to frames-to-frames, and therefore reduce the burden of generative models and aim at improving synthesis quality. To further demonstrate the function of improving prior information for I2V synthesis, we extend our design of FrameBridge from replicated initial frame $z^1$ to neural representations $F_{\\eta}(z^1, c)$, which serves as a stronger prior for video frames.\nAs shown in Figure 4, although the static frame has provided indicative information such as the appearance details of the background and different objects, it may not be informative for the motion information in consecutive frames. When the distance between the prior frame and the target frame is large, bridge models are faced with the challenge to generate the motion trajectory. Therefore, we present a stronger prior than simply duplicating the initial frame, neural prior, which achieves a coarse estimation of the target at first, and then bridge models generate the high-quality target from this coarse estimation.\nConsidering bridge models synthesize target data with iterative sampling steps, we develop a one-step mapping-based prior network taking both image latent $z^1$ and text or label condition $c$ as input, and separately train the prior network with a regression loss in latent space:"}, {"title": "5 EXPERIMENTS", "content": "We carry out extensive experiments on UCF-101 (Soomro, 2012) and WebVid-2M (Bain et al., 2021) datasets to compare FrameBridge with other baselines, demonstrating the advantages of our data-to-data generation framework for I2V tasks."}, {"title": "5.1 SETUPS", "content": "We conduct experiments on class-conditional I2V generation and text-conditional I2V generation tasks, where the condition c is a class label or textual description of the video content.\nClass-conditional I2V Generation For class-conditional I2V generation, we train FrameBridge and other baselines on UCF-101 dataset. Fr\u00e9chet Video Distance (Unterthiner et al. (2018); FVD) and Inception Score (Saito et al. (2017); IS) are used to evaluate the quality of generated videos. Meanwhile, we use Perceptual Input Conformity (PIC) (Xing et al., 2023) to evaluate the consistency of the synthesized frames with given image conditions.\nText-conditional I2V Generation For text-conditional I2V generation, both FrameBridge and baselines are fine-tuned from pre-trained T2V diffusion models on WebVid-2M dataset. We use"}, {"title": "5.2 FINE-TUNING FROM PRE-TRAINED DIFFUSION MODELS", "content": "Implementation Details Following (Xing et al., 2023), we fine-tune FrameBridge with the replicated prior $z^1$ from the open-sourced T2V diffusion model VideoCrafter1 (Chen et al., 2023a). When fine-tuning, we apply SAF technique proposed in Section 4.3 and initialize the network after alignment $\\epsilon_{\\theta,aligned} (\\tilde{z_t}, \\tilde{t}, Z_T, z^1, c)$ with the weights of VideoCrafter1 at 256 \u00d7 256 resolutions. We fine-tune FrameBridge and baselines for 20k iterations with batch size 64. All synthesized videos are sampled through the first-order SDE solvers with 250 steps (Chen et al., 2023c). More details can be found in Appendix C.\nComparison with Baselines We reproduce DynamiCrafter (Xing et al., 2023) for text-conditional I2V as baselines. Table 1 shows zero-shot metrics on UCF-101 and MSR-VTT after fine-tuning on WebVid-2M. As demonstrated by quantitative results in the table, bridge-based I2V models can effectively leverage the knowledge from pre-trained T2V diffusion models and generate videos with higher quality and consistency than their diffusion counterparts. Moreover, it is indicated that SAF can further improve the performance of fine-tuned FrameBridge, showcasing the advantages of matching marginal distributions of the bridge process and diffusion process. Qualitative results are shown in Figure 5.\nBoth the quantitative and qualitative comparisons demonstrate the advantages of FrameBridge and the data-to-data generation farmework for I2V tasks. To the best of our knowledge, our trail is the first time to fine-tune diffusion bridge models from pre-trained diffusion models, and SAF is crucial to boosting the performance of fine-tuned bridge models. Moreover, we note that the zero-shot metrics on UCF-101 is better than those of DynamiCrafter fine-tuned with 100k iterations (FVD 429.23 and PIC 0.6078 as reported in Xing et al. (2023)), highlighting the efficiency of SAF."}, {"title": "5.3 NEURAL PRIOR FOR BRIDGE MODELS", "content": "Implementation Details We implement FrameBridge-NP based on the class-conditional video diffusion model Latte-S/2 (Ma et al., 2024b) by replacing diffusion process with the Bridge-gmax bridge process. We freeze the parameters \u03b7 of neural prior when training bridge models $\\epsilon_{\\theta} (z_t, t, Z_T, z^1, c)$. All synthesized videos are sampled through the first-order SDE solvers with 250 steps. More details can be found in Appendix C.\nComparison with Baselines We reproduce two diffusion models ExtDM (Zhang et al., 2024b) and VDT (Lu et al., 2023) on UCF-101 dataset for the class-conditional I2V task as our baselines. Table 2 shows that FrameBridge-NP has superior video quality and consistency with condition images. (The PIC value of ExtDM is not comparable to other methods as its resolution 64 \u00d7 64 is much lower than others and it generate videos with slower motion.) More qualitative results are shown in Appendix D. The experiments reveal that bridge-based I2V models outperform their diffusion counterparts with both replicated prior and neural prior, justifying the usage of the data-to-data generation process for I2V tasks. Additionally, FrameBridge can further benefit from neural prior $F_{\\eta}(z^1, c)$ as it actually narrows the gap between the prior and data distribution of bridge process."}, {"title": "5.4 ABLATION STUDIES", "content": "SNR-Aligned Fine-tuning Table 2 has already shown the advantage of SAF technique, we use another ablation experiment to further elucidate that. We fine-tune a pre-trained class-conditional video generation model Latte-XL/22 on UCF-101 with 20k iterations. The quantitative results presented in Table 3 shows that SAF improves generation performance of FrameBridge.\nNeural Prior To showcase the effectiveness of neural prior, we compare four different models: VDT-I2V (Gaussian priors, condition on ${z^1,c}$), FrameBridge without neural prior (replicated prior, condition on ${z^1, c}$), FrameBridge with neural prior only as the network condition (replicated prior, condition on ${z^1, c, F_{\\eta} (z^1, c)}$), FrameBridge-NP (neural prior, condition on ${z^1, c, F_{\\eta}(z^1, c)}$). More details of the configurations can be found in Appendix C. Results in Table 4 reveal that $F_{\\eta}(z^1, c)$ is indeed more informative than a single frame $z^1$ and can be fully utilized by FrameBridge through the change of prior."}, {"title": "6 CONCLUSIONS", "content": "In this work, we propose FrameBridge, building a data-to-data generation process for I2V synthesis, which matches the frame-to-frames nature of this task. Additionally, targeting at two typical scenarios of training I2V models, namely fine-tuning from pre-trained diffusion models and training from scratch, we present SNR-Aligned Fine-tuning (SAF) and neural prior respectively to further improve the generation quality of FrameBridge. Extensive experiments show that FrameBridge generate videos with enhanced appearance consistency with image condition and improved temporal coherence between frames, demonstrating the advantages of FrameBridge over diffusion-based I2V methods and the effectiveness of two proposed techniques."}, {"title": "C EXPERIMENT DETAILS", "content": "We provide descriptions of the datasets and metrics used in our experiments, along with implementation details for different I2V models."}, {"title": "C.1 DATASETS", "content": "UCF-101 is an open-sourced video dataset consisting of 13320 videos clips, and each video clip are categorized into one of the 101 action classes. There are three official train-test split, each of which divide the whole dataset into 9537 training video clips and 3783 test video clips. We use the whole dataset as the training data for I2V models trained from scratch on UCF-101, and use the test set to evaluate zero-shot metrics for models fine-tuned on WebVid-2M. When we evaluate zero-shot metrics on UCF-101 for text-conditional I2V models, we use the class label as the input text prompt.\nWebVid-2M is an open-sourced dataset consisting of about 2.5 million video-text pairs, which is a subset of WebVid-10M. We only use WebVid-2M as the training data when fine-tuning I2V models from T2V diffusions in Section 5.2.\nMSR-VTT is an open-sourced dataset consisting of 10000 video-text pairs, and we only use the test set to compute zero-shot metrics for fine-tuned models.\nPreprocess of Training Data: For both UCF-101 and WebVid-2M dataset, we sample 16 frames from each video clip with a fixed frame stride of 3 when training. Then we resize and center-crop the video clips to 256 \u00d7 256 before input it to the models."}, {"title": "C.2 METRICS", "content": "Fr\u00e9chet Video Distance ( Unterthiner et al. (2018); FVD) evaluates the quality of synthesized videos by computing the perceptual distance between videos sampled from the dataset and the models. We follow the protocol used in StyleGAN-V (Skorokhodov et al., 2022) to calculate FVD. First, we sample 2048 video clips with 16 frames and frame stride of 3 from the dataset. Then, we generate 2048 videos from the I2V models. All videos are resized to 256 \u00d7 256 before calculating FVD except for ExtDM. (ExtDM generate videos with resolution 64 \u00d7 64, so we compute FVD on this resolution.) After that, we extract features of those videos with the same I3D model used in the repository of StyleGAN-V 3"}]}