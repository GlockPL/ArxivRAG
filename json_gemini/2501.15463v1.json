{"title": "Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values?", "authors": ["Hua Shen", "Nicholas Clark", "Tanushree Mitra"], "abstract": "Existing research primarily evaluates the values of LLMs by examining their stated inclinations towards specific values. However, the \"Value-Action Gap,\" a phenomenon rooted in environmental and social psychology, reveals discrepancies between individuals' stated values and their actions in real-world contexts. To what extent do LLMs exhibit a similar gap between their stated values and their actions informed by those values? This study introduces VALUEACTIONLENS, an evaluation framework to assess the alignment between LLMs' stated values and their value-informed actions. The framework encompasses the generation of a dataset comprising 14.8k value-informed actions across twelve cultures and eleven social topics, and two tasks to evaluate how well LLMs' stated value inclinations and value-informed actions align across three different alignment measures. Extensive experiments reveal that the alignment between LLMs' stated values and actions is suboptimal, varying significantly across scenarios and models. Analysis of misaligned results identifies potential harms from certain value-action gaps. To predict the value-action gaps, we also uncover that leveraging reasoned explanations improves the performance. These findings underscore the risks of relying solely on the LLMs' stated values to predict their behaviors, and emphasize the importance of context-aware evaluations of LLM values and value-action gaps.", "sections": [{"title": "1 Introduction", "content": "As Large Language Models (LLMs) become increasingly integrated into societal decision-making processes and human interactions, an emergent issue is the question of which values - or whose values - these systems should uphold and reflect [17, 54]. Ensuring that the values and behaviors of LLMs align with ethical and societal expectations is essential [30, 51]. However, LLM values are far from being well-aligned with humans [52], demonstrating risks in real-world applications such as amplified stereotypes in generative models [11] and biased algorithms in hiring processes [35, 61]. While prior research has investigated value alignment and misalignment in LLMs [30, 54], these studies have primarily probed LLMs' inclinations (e.g., \"agree\" or \"disagree\") toward diverse values corresponding to various demographic groups and individuals. We still do not know whether the stated values of LLMs align (or misalign) with their actions in real-world contexts across various contextual scenarios. The \"Value-Action Gap\" [20], a theory rooted in environmental and social psychology, provides us the theoretical framework to study just that. The theory highlights discrepancies between individuals' stated values and their actions in real-world contexts [10], i.e., the difference between what people say and what people do. By seeking a deeper understanding of \"Value-Action Gaps\" in LLMs - the value alignment between what LLMs state and how LLMs act, we ask: to what extent do LLMs exhibit a similar gap between their stated values and value-informed actions? We hypothesize that LLMs should not only state their value inclinations but also take actions that align with those stated in contextual scenarios. This alignment is critical for humans to trust LLMs' stated values as reliable predictors of their behavior. To evaluate this, we place LLMs in contextual scenarios and probe them to 1) state their value inclination and 2) select a value-informed action, after which, 3) we measure the inclination gap between their stated value and selected action. As an example shown in Figure 3, we observed the value-action gap in GPT-40-mini [26] when situated within the context of \"health\" in Nigeria. When prompted, it displayed a negative attitude towards the value of social power, but selected an action which ran counter to this inclination. To systematically investigate this phenomenon, we introduce a novel VALUEACTIONLENS framework, as shown in Figure 2, to assess the alignment between LLMs' stated values and their actions informed by those values. Particularly, we place LLMs in 132 contextual scenarios with twelve countries [47] and eleven societal topics [14], in which we curate both \"agree-\" and \"disagree-\"inclined actions on 56 human values grounded in the Schwartz Theory of Basic Values [43, 45] for each scenario. This contributes to a \"Value-Informed Actions (VIA)\u201d dataset including 14,784 value-informed actions. Building upon each combination of value and scenario, we establish two corresponding tasks to gauge LLMs' inclination toward: 1) Stated Value (Task1); and 2) Value-Informed Action (Task2). Furthermore, we measure the alignment between the stated value inclination (Task1) and value-informed action (Task2) to quantitatively inspect their value-action gap. Extensive experiments with four LLMs reveal substantial gaps between their stated values and actions, with significant variations across values, cultures, and social topics. For instance, GPT40-mini and Llama models show lower alignment rates in African and Asian cultures compared to North American and European countries. Qualitatively analysis of misaligned examples further uncover potential harms from certain value-action gaps. For instance, in the context of religion topic in the US, an LLM states agreement with the \"Loyal\" value but behaves differently, revealing it does not prioritize loyalty to the religious group above all else. Additionally, to improve the prediction of value-action gaps, we leverage the reasoned explanation of value-informed actions collected from our VIA dataset, to improve the predictive performance of the gaps. These findings reveal risks associated with value-action gaps in LLMs and point to critical future research directions to examine LLM values and their value-informed actions in the context of real-world contexts."}, {"title": "2 Related Work", "content": "Value Alignment in LLMs. Understanding value alignment in LLMs is critical for developing responsible and human-centered AI systems [51, 59, 60]. Early research has largely focused on specific values, such as fairness [53], interpretability [49], safety [64], and more. Recent studies have expanded this scope to evaluate a broader range of values. Kirk et al. [30] discuss the philosophical underpinnings of ethically aligned AI, while Shen et al. [52] propose a framework for evaluating value alignment between humans and LLMs. Jiang et al. [27] and Sorensen et al. [54] explore individualistic and pluralistic value alignment, respectively. Liu et al. [32] investigate alignment with demographic groups, such as age. These studies commonly assess LLMs' values by analyzing their stated inclinations toward specific values. For instance, Liu et al. [32] and Jiang et al. [27] employed the World Value Survey [23] to prompt LLMs with questions like, \"How important is it for you to live in a country that is governed democratically?\u201d LLMs responded using a Likert scale from 1 (\"not at all important\") to 10 (\"absolutely important\"). Similarly, Shen et al. [52] leveraged the Schwartz Theory of Basic Values [43, 45], prompting LLMs with questions such as, \"To what extent do you agree or disagree that AI should protect social order?\" Responses were selected from six options like \"disagree\" or \"agree.\" However, these studies primarily assess LLM values by eliciting stated values, overlooking potential gaps between what LLMs say and how they act. Our work systematically addresses this limitation by investigating the value-action gap in LLMs.\nValue-Action Gap in Social Science. The value-action gap, which describes the discrepancy between stated values and actual behavior, has been widely studied in environmental and social psychology [7, 10, 20]. This gap is influenced by cognitive, environmental, and social factors [18, 31] and has been explained through theories, such as microeconomic theory [39], where situational economic or political limitations hinder value-consistent actions [57]. Various strategies, such as providing appropriate information, have been explored to bridge this gap in social and environmental contexts [13]. Particularly, the theory of reasoned actions, which aims to explain the relationship between inclinations and behaviors within human action [4, 28], is used to predict the gaps by understanding how humans will act on their pre-existing inclinations. However, there is little research on whether and how LLMs exhibit the value-action gap, nor on approaches to predict these gaps if possible. This study offers an initial exploration into identifying and understanding the value-action gap in LLMs."}, {"title": "3 VALUE ACTIONLENS: Framework of Assessing Contextual Value-Action Gaps", "content": "LLMs' values and actions are not independent, but elicited and observed in contextualized real-world scenarios. To simulate this practice, we present the VALUEACTIONLENS framework (in Figure 2), aiming to consider various scenarios and assess the alignment between LLMs' stated values and their value-informed actions. It includes contextualization in various cultural and social scenarios (\u00a73.1) to generate value-informed action data (\u00a73.2), two tasks to evaluate LLM values and actions (\u00a73.3), and metrics to measure their value-action alignment (\u00a73.4)."}, {"title": "3.1 Contextualize Values into Scenarios", "content": "To represent a variety of scenarios and values, we curate 132 scenarios that cover twelve countries and eleven social topics listed in Table 1. In each scenario, we further consider a list of 56 values with both agree and disagree inclinations. Notably, the framework is independent of specific value and scenario lists, allowing for seamless extension.\nContextual Scenarios. Schw\u00f6bel et al. [47] has been widely used to evaluate the LLMs in a range of tasks and cultures [3, 46]. Hence we adopt the 12 countries they selected that include diverse cultures and geographic regions with the largest English speaking populations [47]. The list encompasses countries from North America, Europe, Australia, Asia, Africa. We further leverage the 11 social topics employed in the Global Social Survey and International Social Survey Program [14], where typical social topics include Social Inequality, Family, Work, Religion, and more. By fully combining countries and social topics, we achieve 132 scenarios.\nValues with Inclinations. We leverage a comprehensive list of universal human values outlined in the Schwartz's Theory of Basic Values [43, 45]\u00b9, which consists of 56 exemplary values covering ten motivational types. Representative values include \u201cEquality: equal opportunity for all\u201d and \u201cFreedom: freedom of action and thought\u201d. We provide the full list of values and their definitions in Appendix A. For each value, we considered both agree and disagree inclinations, indicating if LLMs agree or disgree with the value, respectively. Therefore, we achieve a total of 112 values with both inclinations. In total, we generate 14,784 contextual value-informed actions that represent a comprehensive list of 132 scenarios and 112 values with inclinations. Below we describe our steps.\n\u00b9We select Schwartz's Theory of Basic Values for its thoroughness and structured hierarchy. However, our framework is extensible to alternative value theories."}, {"title": "3.2 Generate Value-Informed Actions with Explanations", "content": "We generate the contextualized value-informed actions with LLMs in a zero-shot manner. To ensure data quality and ensure robustness, we design a human-in-the-loop data generation pipeline (see Figure 3). Particularly, to understand the rationale behind each action and better predict value-action gaps, we draw on the theory of reasoned action from psychology [4] and generate reasoned explanations for each action. The explanations include two parts: Action Attribution that highlight which generated text spans are reflecting the value-informed actions; and Natural Language Explanation that explains the reasoning process. Our human-in-the-loop generation pipeline involve three steps: constructing prompt variants (Step1); conducting human annotations to select the optimal prompts (Step2); and evaluating the quality of the generated action descriptions and explanations (Step3).\nStep1: Build Prompt Variants. As previous research [6, 38] revealed the inconsistent performance of LLMs with minor prompt variants, we followed Liu et al. [32] to construct 8 prompt variants (i.e., by paraphrasing, reordering the prompt components, and altering the response requirements) for each value and scenario. See Appendix B for prompt design details.\nStep2: Optimal Prompt Selection by Humans. Using the eight prompt variants, we generated a subset of 80 value-informed actions per prompt, resulting in a total of 640 data instances across various scenarios. Two AI researchers annotated these instances over two rounds, utilizing multiple metrics to identify the optimal prompt for generating the complete dataset. Particularly, to ensure responsible data generation, we referred to Bai et al. [5] to evaluate the Harmlessness of each action. We also assessed the quality of highlighted Action Attributions using the Sufficiency metric (following DeYoung et al. [12]) and validated the Plausibility of generated explanations by referring to Agarwal et al. [2]. Disagreements between annotators were resolved through iterative discussions, achieving a substantial agreement level (Cohen's Kappa = 0.7073). Based on these evaluations, we identified the optimal prompt, whose performance is summarized in Table 8, and used it to generate the full dataset. Additional details on annotation performance and processes are included in Appendix C."}, {"title": "3.3 Tasks for Evaluating Stated Values and Value-Informed Actions", "content": "After obtaining the contextualized value-informed actions in the VIA dataset, we create two tasks to assess LLMs' ability to: 1) state value inclinations, and 2) select value-informed actions (as in Figure 2) before evaluating their value-action alignment in \u00a73.3.\nTask 1: State Value Inclination. To elicit LLMs' inclinations towards specific values, we base our prompt design on the two primary instruments for measuring Schwartz basic values: Schwartz Value Survey (SVS) [42] and the Portrait Values Questionnaire (PVQ) [44]. We designed two methods to do so: i) we directly ask LLM to state their inclination to each value (based on SVS); and ii) we ask LLM to indicate their likeness of a portrait embedded with the inclined values (according to PVQ).\nFurthermore, we construct eight prompt variants by paraphrasing, reordering, and altering requirements among the four key components of the prompt: contextual scenarios, value and definition, option choices, and requirements. See Appendix B for details. We follow the practice in Liu et al. [32] of averaging the responses to calculate the LLM rating of each value statement.\nTask2: Select Value-Informed Actions. To evaluate how LLMs' actions align with stated values, we develop a choice-based assessment. For each scenario we present two potential actions: one that aligns positively with a given value and one that aligns negatively. Similar to Task1, we construct eight prompt variants by paraphrasing, reordering, and altering response requirements. We ground each scenario in a specific country, social topic, and value to provide a concrete context. We record the LLM's preferred action for each prompt and aggregate across multiple prompt variants to ensure our findings are robust against differences in prompt phrasing."}, {"title": "3.4 Alignment Measures", "content": "The alignment measures aim to gauge the value-action gap with different alignment measurements. As depicted in Figure 2, we arrange all the stated value responses in Task1 as matrix V and value-informed action responses in Task2 as matrix A. Both matrices have the same size with row $i \\in [1, 132]^2$ representing each scenario and column $k \\in [1,56]^3$ representing each value. Formally, we define the two tasks' representations of a specific scenario $i$ (e.g., United States & Politics) as:\n$V_i = [v_{i1}, v_{i2}.., v_{ik}, .., v_{iK}], \\text{ and } A_i = [a_{i1}, a_{i2}, ..a_{ik}.., a_{iK}], K = 56$  (1)\nwhere $v_{ik}$ and $a_{ik}$ are Task1's and Task2's responses to the kth value in the ith scenario. After averaging the responded scores from all the prompts and normalizing to the unit interval, we calculate the following metrics to measure value-action alignment.\nValue-Action Alignment Rate. To answer our core question, we aim to quantify to what extent are the actions of LLMs aligned with their values. To this end, we binarize each normalized LLM's response and convert their \"Agree\" inclination as 0 and \"Disagree\" as 1. Furthermore, we compare the responses from Task1 and Task2, and compute their F1 score\u2074 to achieve the \u201cValue-Action Alignment Rate\".\nAlignment Distance. While the \"Alignment Rate\" can demonstrate the ratio of alignment between LLM stated values (Task1) and their informed actions (Task2), its key drawback is information loss due to the binarization step. To capture fine-grained differences between stated values and actions, we further compute the element-wise Manhattan Distance (i.e., L1 Norm) between the two matrices as their \"Value-Action Alignment Distance\". Similar to \"Alignment Rate\", we group and average the distances to obtain the distance at various levels of granularity.\n$D_{ik} = |v_{ik} - a_{ik}|, D_{Ck} = \\frac{1}{|C|}\\sum_{i\\in C} D_{ik}$ (2)\nwhere $D_{ik}$ represents the element-wise Alignment Distance for the ith scenario on kth value; and $D_{Ck}$ represents the averaged Alignment Distance for a country or social topic (e.g., C = United States) after averaging all the relevant fine-grained scenarios.\nAlignment Ranking. As we have a wide spectrum of 56 values, it is necessary to identify the largest value-action gaps to take further analysis or mitigation. To this end, we compute the ranking of 56 values' \"Alignment Distance\" in a descending order along the scenario dimension; formally, take $Rank_i (D_i)$ as ranking the 56 values on the ith scenario:\n$Rank_i (D_i) = sort(\\{|v_{ik} - a_{ik}|, k = \\{1, 2, ..., 56\\})$ (3)\n\u00b2132 rows corresponds to the combinations of 12 countries and 11 topics\n\u00b356 columns corresponds to the 56 values\n\u2074We leverage F1 score but not accuracy considering the imbalanced responses of \"Agree\" and \"Disagree\"\n\u2075We leverage Manhattan Distance but not Euclidean Distance used by some prior studies [32] because Euclidean Distance will shrink the distance with the gap within [0,1]."}, {"title": "4 Reasoned Explanations for Predicting Actions", "content": "We ground our approach in the Theory of Reasoned Action from social psychology [4, 15], which posits that identifying discrepancies between attitudes and behaviors is requisite to predict value-action gaps. Furthermore, we investigate whether reasoned explanations can aid in assessing the dynamics of value-action gaps in LLMs. To this end, we examine the reasoned explanations and highlighted action attributions included in the VIA dataset, and design a task to predict the alignment between value inclination and value-informed action. Concretely, we design a few-shot learning task where one observer model observes another target LLM's contextual actions and explanations, and attempts to predict how the target LLM will state its value inclination given actions. .\nUsing our VIA dataset and the responses from Task 1 and Task 2 in the VALUEACTIONLENS framework, we evaluate action prediction across three few-shot learning input settings: (i) action with feature attributions (Act+Attr), (ii) action with natural language explanations (Act+Exp), and (iii) action with both feature attributions and explanations (Act+Attr+Exp). Additionally, we include a baseline that only uses the action (Act) to predict the LLM's stated value inclination. For this task, the observer model predicts a binary label: True if the model agrees with the value and False if it disagrees. During evaluation, we compare the predicted binary labels with the target LLM's stated value inclinations from Task 1 to assess the F1 score performance of the predictions."}, {"title": "5 Experimental Settings", "content": "Models and Settings. We evaluate the value-action alignment of four LLMs, including two closed-source (GPT-40-mini [1] and GPT-3.5-turbo [34]) and two open-source (Gemma-2-9B [55] and Llama-3.3-70B [56]). We select these four LLMs to represent both open-source and closed-source state-of-the-art models released within the past year. For each of Task1 and Task2, we use eight distinct prompts following the approach in Figure 3. We average the eight responses to arrive at the final result. All models use a temperature $\u03c4 = 0.2$.\nValue Elicitation Settings. To systematically evaluate value-action alignment, Task1 and Task2 are performed independently for each LLM. This simulates the potential scenario where Al developers complete a safety check to evaluate the LLM's stated values (Task1) during development, while end users interact with the deployed model, leading to embedded actions which reflect those values (Task2)."}, {"title": "6 Results", "content": "Our empirical studies aim to address the following three research questions:\nRQ1: To what extent do LLMs demonstrate a value-action gap between their stated values and actions? (\u00a76.1);\nRQ2: Do value-action gaps in LLMs reveal potential risks? (\u00a76.2)\nRQ3: Can reasoned explanations enhance the prediction of value-action gaps?(\u00a76.3)\nWe present our findings in the sections below."}, {"title": "6.1 Value-Action Gaps in LLMs (RQ1)", "content": "We analyze the value-action gaps present in LLMs through the three alignment measures introduced in the framework.\n6.1.1 Value-Action Alignment Rates. As the relevance of a given value may vary significantly by scenarios, we analyze the overall value-action alignment rates as well as by culture and topic area. Table 3 illustrates that value-action alignment rates differ by country (top) and social topic (bottom). Among the four models, we observe that GPT40-mini performed the best with an F1 score of 0.564 summed social topics. In comparison, GPT3.5-turbo performed significantly worse with the lowest score among all models at 0.179. Grouping countries by geographic regions, we observe that LLMs tend to display a lower alignment rate in Africa and Asia compared to North America and Europe in GPT40-mini and Llama. Similarly, we also find the alignment rates vary across social topics, such as Leisure and Health topics. These findings demonstrate that the alignment rates of LLMs are suboptimal, and vary dramatically by scenarios and models.\n6.1.2 Alignment Distance. Figure 4 illustrates the responses of GPT-40-mini regarding stated values ((A) Task1) and value-informed actions ((B) Task2) across all 56 values in twelve countries. Additionally, Figure 4 (C) visualizes the Alignment Distance between the model's stated values and its value-informed actions. From Figure 4 (A) and (B), we observe that GPT40-mini agree with most values while disagreeing with a few, such as \"Social Power\", \"Authority\", \"Wealth\", \"Obedient\", \"Detachment\" values. Furthermore, Figure 4 (C) reveals that while most values exhibit relatively small distances between their stated values and actions, certain values \u2013 such as \"Independent\", \"Choosing Own Goals\", \"Moderate\", and more - display pronounced value-action gaps across cultures. We illustrate GPT-40-mini's performance on social topics in Figure 7, and additional results from other LLMs are available in Appendix E. Overall, these results reveal that LLMs exhibit varied inclinations toward different values. While their value-action alignment distances remain small for most values, certain values display noticeable gaps across different scenarios, such as \"Independent\" and \"Choosing Own Goals\"."}, {"title": "6.1.3 Alignment Ranking.", "content": "To further investigate the relative misalignment by scenario, we ranked the alignment distances of all 56 values within each cultural or social context. Figure 5 highlights the ranked values for the Philippines and the United States, using GPT-40-mini, which demonstrated the lowest and highest alignment rates, respectively, as shown in Table 3. Our analysis reveals that many of the highly misaligned values differ between the Philippines and the United States. For example, \"Choosing Own Goals\" saw the largest value-action gap for the Philippines, whereas it exhibits a small value-action gap for the United States. Additional results for GPT-40-mini across other cultures, as well as performance comparisons with other LLMs, are provided in Appendix E. Interestingly, some cultures display similar alignment rankings for their top values. For instance, Pakistan and Uganda, United States and Philippines, as well as Germany, Canada, and France share comparable top value trends. These findings underscore the importance of conducting value alignment analysis within cultural contexts to account for nuanced differences in how values manifest and align across scenarios."}, {"title": "6.2 Value-Action Misaligned Examples Reveal Potential Risks (RQ2)", "content": "To better understand the potential risks of value-action gaps in LLMs, we collect data where each LLM's value inclination is misaligned with its chosen action, including 4,383 misaligned examples across all four LLMs. We then conduct qualitative coding on these examples to identify any harmful outcomes. During this process, we extract instances that align with harmfulness categories defined by Harandizadeh et al. [24] and Scheuerman et al. [41]. We also highlight three value-action responses in Table 4, illustrating potential risks when humans rely solely on LLMs' stated values to predict their actions. For example, in scenarios related to working orientation in India, LLMs claim to disagree with the value of \"Social Power\" in working settings. However, their selected actions endorse \"Social Power\" by exhibiting behaviors such as making unilateral decisions for the team and taking control of decision-making processes. This misalignment poses potential risks, as it suggests LLMs could execute critical tasks without human awareness or oversight in practical human-LLM collaborations. Similar value-action gaps are observed in other scenarios. For example, in a healthcare context in Nigeria, LLMs demonstrate misalignment with their stated stance on \"Social Power.\" With respect to religion-related topics in the United States, their actions contradict their stated stance toward \"Loyal\" value. These findings underscore the importance of addressing value-action gaps to mitigate risks associated with human-LLM interactions in real-world scenarios."}, {"title": "6.3 Explanations of Reasoning Actions Help Predict Value-Informed Actions (RQ3)", "content": "In this study, we deploy the observer model as GPT40-mini to observe and predict the behavior of two target models, GPT-3.5-Turbo and Llama-3.3\u2076. The F1 scores for these experiments are presented in Table 5. The results show that GPT40-mini performed best when provided with both the actions and natural language explanations. This was followed by the condition where it was shown actions alongside both explanations and feature attributions. While merely providing actions with feature attributions underperformed compared to including explanations, it still outperformed the baseline condition of showing only actions. Overall, these findings suggest that analyzing LLMs' actions in combination with their reasoned explanations significantly enhances the ability to predict\n\u2076We choose GPT40-mini as the observer model because it offers the high intelligence of the latest GPT-4 while being more efficient. The target LLMs, GPT-3.5-Turbo and Llama-3.3, are selected for their representation of both open- and closed-source models."}, {"title": "7 Discussions and Implications", "content": "7.1 Key Findings\nOur findings reveal that LLMs exhibit significant value-action gaps between their stated values and actions (RQ1), which vary across cultural and social scenarios. These findings underscore two key points: first, when collaborating with LLMs or conducting safety checks, humans should not exclusively rely on LLMs' stated values to predict their value-informed actions as alignment is not guaranteed. Second, LLMs' values are highly dependent upon the specific scenario and therefore, should always be assessed within a contextualized setting."}, {"title": "7.2 Implications of Value-Action Gaps", "content": "We further explore the implications of value-action gaps, highlighting the potential risks associated with these gaps.\nHigh intelligence does not necessarily imply strong alignment between stated values and actions. Although cutting-edge LLMs, including ChatGPT, display remarkable performance on various tasks requiring intelligence [29, 33], we observed a relative low alignment rate between their stated values and actual actions across 56 human values. For example, Table 3 shows that the alignment rates of ChatGPT were lower than 0.25 across various countries and social topics. This raises additional challenges in assessing the values of LLMs, and calls for additional effort in inspecting LLMs' value-informed behaviors to ensure alignment. This may come at the cost of exclusively optimizing for intelligent performance.\nRisks can be induced beyond common values in current practice (e.g., equality, responsibility). Despite a plethora of research and practice implemented to avoid risks regarding typical values (e.g., fairness [25, 53], interpretability [37, 50], harmfulness [5]), we observed that potential ethical risks can be induced by more values beyond these current considerations. For instance, Figure 4(C) demonstrates that although GPT-40-mini was largely aligned in well-explored values like \"Responsible\" and \"Helpful\", it showed more misalignment with under-explored ethical values, such as \"Independent\", \"Loyal\u201d, and \u201cInfluential\u201d values. This misalignment can further induce risks exemplified in Table 4, which may take over control of \"Social Power\" in human-AI teaming to overpass human supervision, or acting"}, {"title": "7.3 Implications for Future Work", "content": "Our findings open new avenues for enhancing the alignment of LLMs with their stated values and actions, presenting critical implications for future research. We highlight three key points below.\nRaising awareness of value-action gaps among LLM providers and users. Rather than relying solely on static elicitation of LLM values [27], LLM practitioners and users must also examine the extent to which value-action gaps manifest in real-world applications. This awareness is particularly vital during LLM development, where stated values are typically defined, and models are tasked with self-critiquing their actions without human oversight [22, 40]. Such value-action gaps may lead to reasoning behaviors that diverge from the intended value statements by LLM providers. Additionally, while this study primarily explored value-action gaps in the contexts of cultures and social topics, we argue that other contextual factors-such as users' age or race [32]-could also influence these gaps in practical use. Therefore, we recommend that future research and practice proactively address value-action gaps across both development and deployment stages to ensure that LLMs' stated values and actions align with human expectations.\nSystematically examine LLM risks by broadening the scope of values and informed actions. While much of the existing research and practice focus on a limited set of typical values (such as fairness [16, 25] and interpretability [50, 58]), this study, leveraging the Schwartz Theory of Basic Values [43, 45], reveals that risks can emerge beyond these conventional value sets. These findings highlight the importance of incorporating a more comprehensive range of human values in systematically assessing LLMs' value inclinations and their corresponding actions. Future research should address several critical questions, such as which values should be prioritized when examining value-action gaps, how these values translate into actions during human-LLM interactions, and how to predict and mitigate potential value-action gaps in practice.\nEvaluate LLMs' values and value-action gaps in real-world contexts. Assessing and predicting value-action gaps across diverse real-world contextual scenarios presents significant challenges. To address these issues, future work and practitioners should adopt systematic, scenario-aware evaluations of value-action gaps or develop interactive approaches that enable humans to assess these gaps in their practical use cases. Additionally, contextual findings that reveal variations across scenarios highlight the need for pluralism in LLM development. This includes incorporating diverse perspectives into data collection, model fine-tuning, and evaluation processes. Such efforts are essential to ensure that LLMs exhibit human-expected values and informed actions when deployed in real-world contexts."}, {"title": "8 Conclusion", "content": "We introduce an evaluation framework to assess the alignment between LLMs' stated values and their actions informed by those values. The framework encompasses (1) the data generation of value-informed actions across diverse cultural and social contexts; (2) two tasks for evaluating LLMs' stated values and actions; and (3) various metrics for measuring their value-action alignment. Further, we release our generated \"Value-Informed Actions (VIA)\u201d dataset with 14,784 value-informed actions. Extensive experiments demonstrate that while LLMs generally align their actions with their stated values, notable misalignment arise when they explicitly express disagreement with certain values, shedding light on potential limitations in value-action alignment for LLMs."}, {"title": "A Cultural and Social Values", "content": "We introduce the 56 universal values and their definitions outlined in the Schwartz's Theory of Basic Values [43, 45], which consists of 56 exemplary values covering ten motivational types. We show the complete list of value in Table 6."}, {"title": "B Prompt Variation Design", "content": "We constructed 8 prompt variants (i.e., by paraphrasing the wordings, reordering the prompt components, and altering the requirements) for each setting of value and scenario.\nPrompt Variants of Task1. we followed the approach in \u00a73.2-Step1 and identified four key components in designing the zero-shot prompts:\n(1) Contextual Scenarios (e.g., Suppose you are from the United States, in the context of Politics, how strong do you agree or disagree with each value?);\n(2) Value and Definition (e.g., Obedient: dutiful, meeting obligations);\n(3) Choose Options (e.g., Options: 1: strongly agree, 2: agree, 3: disagree, 4: strongly disagree );\n(4) Requirements (e.g., Answer in JSON format, where the key should be...).\nPrompt Variants of Task2. To construct the task prompt, we again follow the approach in Task1, by dividing the prompt into three components:\n(1) Contextual Scenarios (e.g., Suppose you are from the United States, in the context of Politics, how strong do you agree or disagree with each value?);\n(2) Two Actions with Shuffled Order: (The presentation of action choices, with ordering varied to control for potential sequence effects.)\n(3) Requirements (e.g., Include justification for your selected action."}, {"title": "C Human Annotation on Data Generation", "content": "To select the optimal prompt for generating the full VIA dataset (Step2 in Section 3.2), we first have two Al researchers evaluated 640 instances generated from eight prompt variants. The results are shown in Table 7.\nAfter selecting the top two prompts, we further conduct another round of annotation with two Al researchers to select the optimal prompt based on a broader set of evaluation metrics introduced in the Step2 in Section 3.2. The results are shown in Table 8.\nAfter generating the full VIA dataset, we further conduct human annotations on the generated data samples. We particularly recruit humans with associated cultural background from Prolific. We recruit three humans from the specific country and ask them to annotate this corresponding culture's data points from a variety of evaluation metrics same as in Step2. We randomly sampled 10 data instances"}]}