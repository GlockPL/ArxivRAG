{"title": "Machine Learning for Quantifier Selection in cvc5", "authors": ["Jan Jakub\u016fva", "Mikol\u00e1\u0161 Janota", "Jelle Piepenbrock", "Josef Urbana"], "abstract": "In this work we considerably improve the state-of-the-art SMT solving on first-order quantified problems by efficient machine learning guidance of quantifier selection. Quantifiers represent a significant challenge for SMT and are technically a source of undecidability. In our approach, we train an efficient machine learning model that informs the solver which quantifiers should be instantiated and which not. Each quantifier may be instantiated multiple times and the set of the active quantifiers changes as the solving progresses. Therefore, we invoke the ML predictor many times, during the whole run of the solver. To make this efficient, we use fast ML models based on gradient boosted decision trees. We integrate our approach into the state-of-the-art cvc5 SMT solver and show a considerable increase of the system's holdout-set performance after training it on a large set of first-order problems collected from the Mizar Mathematical Library.", "sections": [{"title": "1 Introduction", "content": "The use of machine learning methods in various fields of automated reasoning is an emerging research topic with successful applications [8, 21, 39]. Machine learning methods were previously integrated into various provers, solvers, and related systems [25, 18]. Here, we focus on the task of selecting quantifiers within the state-of-the-art Satisfiability Modulo Theory (SMT) solver, cvc5, where various instantiation methods are applied to quantified formulas to refine problem representation. While cvc5's instantiation methods generate instantiations for all applicable quantified formulas, we aim to filter out the formulas considered by the instantiation module using a machine-learned predictor trained on previously proved similar problems. We refer to this process as quantifier selection for simplicity.\nIn contrast to previous research efforts that concentrated on finegrained control of the solver at the term level [26, 28], we simplify the problem by shifting our focus to controlling the solver at the quantifier level. This simplification allows us to employ our method basically with any quantifier instantiation method supported by cvc5. Our methods also improve upon a related method of offline premise selection [1, 18], where initial quantified assumptions are filtered only once before launching the solver. Since this filtering can result in an irrecoverable mistake (a deletion abstraction [30] resulting in a too weak theory) when a necessary assumption is deleted, our method ensures that every quantified formula will be considered for instantiations with a non-zero probability. This yields a more complex, probabilistically guided framework implementing deletion and instantiation abstractions in the framework proposed in [30]. As we employ an efficient version of gradient boosting decision trees with simple, but easily computable bag of words features, our implementation produces only a minimal overhead over the standard cvc5 run.\nThe simplicity of our approach allows us to apply it extensively and train on a large number of problems from the Mizar Mathematical Library (MML). The method is also remarkable for its crossstrategy model transfer, where a model trained on samples from one strategy improves the performance of virtually any other strategy. This allows us to improve the state-of-the-art performance of cvc5 on MML by more than 20% in both the single strategy and portfolio scenarios. Our best machine-learned strategy, alone, outperforms the state-of-the-art cvc5's portfolio from the latest CASC competition by more than 10%."}, {"title": "2 Background: SMT solving", "content": "To solve formulas without quantifiers (aka ground formulas), SMT solvers combine SAT solving with theory solving [5]. The SAT solver handles the boolean structure of the formula and theory solvers reason about concrete theories, such as theory or reals, integers, etc. Problems in FOL do not contain such theories explicitly and therefore, to reason about FOL, an SMT solver needs to only support uninterpreted functions and equality (combination known as EUF). For reading this paper, it is not essential to understand how exactly the SMT solvers solve ground formulas; we refer the interested reader to relevant literature [14, 3, 5, 12].\nFormulas with with quantifiers represent a significant challenge for SMT. In general, SMT solvers use instantiations-unless they deal with decidable quantified theories [13, 7, 37]. Instantiations are done with the goal of achieving a contradiction. This style of reasoning can be seen as a direct application of the Herbrand's theorem. For example, for (\u2200x: R. x > 0) instantiating x with the value 0 yields 0 > 0, which immediately gives a contradiction (in the theory of reals).\nInput formulas do not need to be in prenex form, which effectively means that the solver may activate only some of the quantified subformulas. For this purpose, a subformula"}, {"title": "2.1 Enumerative instantiation", "content": "This enumerative instantiation mode enumerates all possible tuples of terms from the term database that can be used to instantiate a quantified expression [38, 27]. Given an ordering of the suitable ground terms for each quantifier, the strategy enumerates tuples by starting at the tuple that contains the first term in each quantifier-term ordering, and moves further into the orderings by incrementing the indices that retrieve further terms in the quantifier-term index.\nThe particular criterion for term ordering that is used in enumerative instantiation is the age of the ground terms, with terms with higher age being preferred. That is, ground terms that were in the original problem have the highest priority for being tried. As an example of the logic of the procedure, imagine that there is a ground part {p(c)} and a quantified expression \u2200x. q(f(x)). In this case, the ground term c is available at the start of the procedure and therefore will be tried first. This creates a ground lemma that is a consequence of the quantified expression, q(f(c)). The solver would now recognize that f(c) is also an available ground term, insert it into the term database and it would be used in the next round of instantiations. With nested terms, many new ground terms may be created by a single instantiation step."}, {"title": "2.2 E-matching", "content": "The e-matching instantiation procedure searches for instantiations that match some already available ground term [14], taking term equality into account. Of course, at any point and especially late in the procedure when many terms have been generated, there can be many possible ways to create such matching terms. The e-matching instantiation process is therefore focused using triggers, which are user-supplied or heuristically generated patterns.\nWe show the following example for a more concrete perspective. In the example, we assume that a theory module that can evaluate integer arithmetic statements is available. Given the following ground facts {p(a), a = f(24)} and the quantified expression \u2200x.p(f(x)) V x < 0. A trigger in the form p(f(x)) leads to x being instantiated with the ground term 24 as there is an existing ground term p(f (24)), when taking equality into account. Instantiating with 24 generates the consequent lemma \u00acp(f(24)) V 24 < 0. This lemma contradicts the other ground facts (as the theory of integer arithmetic knows that in fact 24 > 0), and therefore the solver stops and reports that a contradiction (unsat) was found. An automated trigger generation method is implemented in the e-matching module of cvc5, and its behavior can be influenced through various options provided by the user."}, {"title": "3 Machine-learned quantifier selection", "content": "In this section, we detail our quantifier selection method and its implementation in the cvc5 solver, covering the process from extraction of training examples to model training, and integration."}, {"title": "3.1 Instantiation modules", "content": "The instantiation methods supported by cvc5 are implemented through various instantiation modules that share a common interface. An instantiation module is invoked via its check method to refine information about quantified formulas by introducing appropriate formula instances. Each module is provided with information about currently asserted quantified formulas, and it selects formulas and generates their instances in accordance with the implemented instantiation method. Subsequently, control is returned to the ground solver.\nEffective quantifier and instance selection can significantly enhance performance, and different instantiation methods offer grounds for various possible applications of machine learning methods at a method-specific level [26, 28]. However, we propose a generic method for quantifier selection by limiting the formulas visible to the modules. As all instantiation modules iterate over available quantified formulas and process them one by one, we can seamlessly integrate a quantifier selector into any module and simply skip the processing of undesirable quantifiers. To predict the quality of quantifiers, we utilize an efficient implementation of decision tree ensembles (LightGBM [29]) that enables easy and fast integration with cvc5. Decision tree models can be trained to classify quantified formulas as positive or negative based on provided training examples. The trained model can be employed within an instantiation module to skip the processing of negative quantifiers."}, {"title": "3.2 Feature vectors and training examples", "content": "To use decision trees for classifying quantified formulas in cvc5, we need to represent SMT formulas by numeric feature vectors. SMT"}, {"title": "3.3 Model training", "content": "To construct a training dataset, we collect labeled training vectors from a large set of successful runs of cvc5. Note that no training data is extracted from unsuccessful runs, as there is no reference point to label processed formulas as positive or negative. It is essential to collect a substantially large amount of training data to enhance the generalization capabilities of the model. The gradient boosting decision tree models can be easily constructed using the LightGBM library [29], which is known to handle large training data well. A model consists of a sequence of decision trees, where each tree is trained to correct any imprecision introduced by the previous trees in the sequence. The number of different trees in the model is one of the many LightGBM hyperparameters that influence the process of model training and also the performance of the resulting model.\nIn order to prevent overfitting of the model to the training data, we split the data into training and development sets. This split is done at the problem level, ensuring that all training vectors from a single problem contribute to the same set. We use binary classification as the model objective and we train several models on the training set with various values of selected hyperparameters. Out of these models, we select the model with the best performance on the development set.\nThe performance of the model on the development set is measured in terms of prediction accuracy. That is, all vectors from the development set are evaluated using the trained model, and the predicted labels are compared with the expected labels from the development set. While processing a redundant quantified formula cannot completely prevent an SMT solver from finding a solution, omitting to process an important formula can ultimately close the door to success. Hence, it is essential for models for quantifier selection in SMT to favor recognition of positive examples, that is, to minimize false negative errors. We achieve this by computing separately the accuracies on positive and negative development examples, denoted pos and neg respectively, and selecting as the model with the best value of 2 pos + neg as the final model. In this way, we favor models with better performance on positive training examples."}, {"title": "3.4 Model integration", "content": "Since LightGBM provides a C++ interface for model predictions, it can be easily integrated into the cvc5 codebase, which is also written in C++. The interface offers methods to load a model and compute the prediction of a quantified formula represented by a feature vector. Since we employ binary classification for the model objective, the prediction function returns a floating-point value between 0 and 1 which is typically compared with a fixed threshold, like 0.5, to distinguish positive and negative clauses.\nHowever, we do not directly utilize the predicted values in this manner. Instead, each time we predict a quantified formula, we generate a random number between 0 and 1 to serve as the threshold for the comparison. This approach allows us to process formulas scored below 0.5, ensuring that every formula will eventually be processed with some non-zero probability. In our experiments, the random threshold slightly outperformed any fixed threshold, and it further guards against potential irreparable errors resulting from false negative predictions. Moreover, this approach sets our method apart from a related technique known as offline premise selection [1, 18], where formulas are filtered in advance and never passed to the solver.\nThe two most prominent modules for quantifier selection in cvc5 are enumerative instantiation and e-matching. We implement our machine learning guidance for them, as well as for the modules relying on conflict-based quantifier instantiation and finite model finding."}, {"title": "4 Experiments on large Mizar dataset", "content": "The Mizar Mathematical Library (MML) [2] stands as one of the earliest extensive repositories of formal mathematics, encompassing a broad spectrum of lemmas and theorems across various mathematical domains. We utilize translations of MML problems into first-order logic, a process facilitated by the MPTP system [41, 42]. The MPTP benchmark has emerged as a valuable resource for machine learning research, offering a diverse collection of related problems [33, 23, 39, 34, 19, 10]. In our work, we focus on the easier bushy variants of MPTP problems, wherein premises are partially filtered externally beforehand.\nWe use a train/development/holdout split from other experiments in literature [25, 17] to allow a competitive evaluation. This splits the whole set of Mizar problems into the training, development, and holdout subsets, using a 90: 5: 5 ratio, yielding 52,125 problems in the training set, 2,896 in devel, and 2,896 problems in the holdout set. We use the training set to collect training examples for machine learning, and we use the development set to select the best model out of several candidates trained with different hyperparameters. The best portfolio constructed on the development is henceforth evaluated on the holdout set and compared with the baseline portfolio. All"}, {"title": "4.1 Baseline strategies and portfolio", "content": "We employ a state-of-the-art portfolio of cvc5 strategies as a baseline to evaluate our machine learning approach. We incorporate all 16 strategies available in the CASC competition portfolio of cvc5. A portfolio consists of strategies that are typically executed sequentially, each for a fraction of the overall time limit. The first successful strategy returns the solution and terminates the portfolio execution. The CASC competition focuses on problems in automated theorem proving, and cvc5 has recently demonstrated outstanding performance on problems from theorem proving systems [18]. We utilize the FOF (first-order formulas) portfolio, specifically designed for problems in the theory of uninterpreted functions (UF), which aligns well with our intended application on Mizar MML problems expressed in the same logic.\nIn addition to the 16 CASC strategies, we also incorporate 3 publicly available strategies developed recently for the Mizar MML problems using the Grackle strategy invention system [20]. The Grackle strategies in that work were tailored for slightly different versions of Mizar problems, employing a more precise method of premise selection. However, they also perform well on the bushy version of Mizar problems used in our work here.\nThe selected CASC strategies cascn are detailed in Figure 2, while the three Grackle strategies grk are outlined in Figure 3. In both cases, strategies are described in terms of their cvc5 command line options, either as a boolean flag or as an option=value pair. Common options appearing in more than one strategy are typeset in bold. Options adjusting the trigger behavior of the ematching module are highlighted in bold-italics. Note that almost all the strategies activate enumerative instantiation using full-saturate-quant, while keeping e-matching and conflictbased quantifier instantiation (cbqi) turned on by default. The option cbqi-vo-exp, which activates an experimental mode for variable ordering, often significantly boosts strategy performance. Strategies casc4, casc7, casc10, casc13, and casc14 are selected as the strongest CASC strategies, with casc13 being the best among them. The rationale behind selecting casc6 and casco is revealed later in Section 4.3. Among the three Grackle strategies, grk\u2081 performs the best. It is worth noting that grk\u2081 and grk2 are proper extensions of casc14, while grk3 is a proper extension of casc7. The core of a successful cvc5 strategy for Mizar problems appears to be enumerative instantiation with appropriate adjustments of triggers for e-matching.\nWe evaluate the baseline strategies directly on the holdout set to establish the best possible state-of-the-art portfolio on our Mizar benchmark. The most effective of the CASC strategies is casc13, solving 1,406, while the best Grackle strategy, grk\u2081, solves 1,443. The three most complementary CASC strategies (casc13, casc14, and casc7) collectively solve 1,516, whereas the three Grackle strategies solve 1,552. The best portfolio of 6 strategies, whether CASC or Grackle, solves 1,614 holdout problems (55.8%). Interestingly, this portfolio includes all three Grackle strategies and CASC strate"}, {"title": "4.2 Training and evaluation", "content": "We train two different models, denoted as single and three, using distinct sets of training examples generated by evaluating baseline strategies, as described in Section 3.2. We follow two approaches: (1) utilizing only training data generated by a single strategy, or (2) combining training examples from various strategies.\nModel single: Trained on data generated by the strongest individual strategy grk\u2081. The model is trained on 25,415 solved training problems, yielding 2,281,923 training vectors, with 15.7% of them being positive. The size of the text training file is 255 MB, and the model size is 16 MB.\nModel three: Trained on data generated by all three Grackle strategies grk\u2081, grk2, and grk3. The model is trained on 27,032 solved"}, {"title": "4.3 Cross-strategy model transfer", "content": "In this section, we investigate the transferability of knowledge learned from one strategy to different strategies. Specifically, we examine how well a model trained on data from one strategy performs when applied to different strategies. For instance, since the model single is trained on data from grk\u2081, we aim to compare its performance with grk, and with other strategies. Similarly, we seek to evaluate how the model three performs when applied to the source Grackle strategies compared to other CASC strategies.\nTable 4 provides a summary of the impact of ML models single and three on selected strategies. The first numeric column (w/o ML solves) displays the performance of each strategy without any ML model. The two middle columns (ML model single) illustrate the impact of the model single on the performance of each strategy individually. The column solves indicates the number of problems solved by the strategy guided by the model, while the column gain calculates the improvement over the strategy without ML. The highest values in each column are highlighted separately for the Grackle and CASC strategies. The last two columns show the impact of the model three.\nInitially, we observe that all Grackle strategies derive equal benefits from both models. While grk\u2081 experiences a slightly greater improvement from its own data (single), other Grackle strategies also benefit significantly from the data provided by single. There is no discernible distinction in their performance, as both models enhance the performance of all Grackle strategies by more than 20%. However, there is evidently a considerable difference in the problems solved by different models, as evidenced by the inclusion of Grackle strategies in the best ML portfolio (Table 2).\nInterestingly, many other CASC strategies also experience significant benefits from training data generated by Grackle strategies. However, as an exception, only the best CASC strategy, casc13, failed to benefit from the training data and achieved only negligible improvement. Upon inspecting Figure 2, we observe that casc13 is the only strategy that utilizes pre-skolem-quantifier=on. Since all other strategies without this option were able to improve, it's possible that eager skolemization influences feature vectors and renders the training data incompatible. The second worst result is observed with casc4, which experiences only an 8.78% improvement. This is the only strategy that employs finite model finding instead of enumerative instantiation and e-matching. It is noteworthy that even a fundamentally different instantiation approach can, at least partially, benefit from knowledge learned from related methods, which is not always the case [18].\nThe most improved CASC strategies are casc6 and cascs, both of which see improvements of more than 35%. Notably, both casc6 and casco utilize options to adjust triggers for e-matching, similar to Grackle strategies. From this, we conclude that our models generally transfer well to different strategies. Transfer tends to be more effective between strategies based on similar methods, particularly among strategies that adjust trigger behavior. Finally, no significant improvement is gained when merging data from various source strategies."}, {"title": "4.4 Analysis of models and training data", "content": "We proceed with an analysis of the trained models by examining features present in the training data and their importance in the model. LightGBM models report for each feature its importance, indicating how much each feature contributes to the predictions made by the model. This aids in understanding which features are most relevant for the model's performance. We collect the training data produced by grk\u2081 on the development set. Remarkably, only 24 features are uti-"}, {"title": "5 Conclusions and future work", "content": "This paper develops a novel approach that trains a machine learning model to decide which quantifiers should be instantiated inside an SMT solver. The approach is conceptually simple: in each instantiation round, each quantifier is considered to be selected for instantiation or not; the exact way how the quantifier is instantiated, is left to the SMT solver. This enables us to employ our approach for various instantiation techniques [14, 35, 27]. This contrasts with existing research that always instantiates all active quantifiers and tries to modify one specific instantiation strategy with an ML model [28, 32] and it is limited to that particular instantiation strategy. Also, in contrast to the existing work, we showed that we can boost the solver's performance if we take a portfolio of its configurations. This is a tall order. Indeed, it is highly challenging to improve one particular configuration of the solver in a way that is not covered by another configuration of the solver. Thanks to the versatility of our approach, we could integrate it with various instantiation strategies, thereby enhancing multiple solver configurations and achieving a notable 21.4% improvement over the baseline state-of-the-art portfolio. The key aspects contributing to this success include our ability to implement our selection method with different instantiation modules and effectively transfer learned knowledge across strategies. This enables us to substantially enhance each individual strategy within the portfolio, consequently boosting the overall portfolio performance. Our best strategy solves 1,781 Mizar holdout problems in 60s while it solves 1,743 in 30 s. This number can be informatively compared with experiments on the same dataset from the literature [17], where the state-of-the-art ATP prover E with advanced machine learning methods, solves only 1,632 of the holdout problems in 30 s.\nOur approach differs from existing research that aimed to learn which quantifiers to select on a single problem instance [26] - using the multi-armed bandit paradigm (MAB). While previous attempts failed to achieve experimental improvements, combining MAB with offline training presents an intriguing avenue for future exploration. Another research direction is to apply and assess our methodology on various benchmarks. We conducted evaluations on Mizar MML problems due to their similarity and relevance, enhancing the potential for machine learning methods to recognize useful quantified formulas. We anticipate similar outcomes on related problems, such as those from Isabelle/Sledgehammer [9], since learning-based methods typically transfer between various ITP corpora [18, 25].\nMore diverse benchmarks like TPTP [40] or SMT-LIB [4] present significantly greater challenges. These benchmarks contain problems from various sources, often with few related instances, limiting learning opportunities. Furthermore, the transfer of learned knowledge across different SMT logics complicates the situation with SMT-LIB. It is remarkable how much can be gained using the minimal bag-ofwords features employed here, yet we anticipate enhanced results with more sophisticated formula features [22]."}]}