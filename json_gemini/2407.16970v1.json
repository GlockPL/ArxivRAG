{"title": "Towards Aligning Language Models with Textual Feedback", "authors": ["Sa\u00fcc Abadal Lloret", "Shehzaad Dhuliawala", "Keerthiram Murugesan", "Mrinmaya Sachan"], "abstract": "We present ALT (ALignment with Textual\nfeedback), an approach that aligns language\nmodels with user preferences expressed in text.\nWe argue that text offers greater expressive-\nness, enabling users to provide richer feedback\nthan simple comparative preferences and this\nricher feedback can lead to more efficient and\neffective alignment. ALT aligns the model\nby conditioning its generation on the textual\nfeedback. Our method relies solely on lan-\nguage modeling techniques and requires min-\nimal hyper-parameter tuning, though it still\npresents the main benefits of RL-based align-\nment algorithms and can effectively learn from\ntextual feedback. We explore the efficacy and\nefficiency of textual feedback across different\ntasks such as toxicity reduction, summariza-\ntion, and dialog response generation. We find\nthat ALT outperforms PPO for the task of tox-\nicity reduction while being able to match its\nperformance on summarization with only 20%\nof the samples. We also explore how ALT can\nbe used with feedback provided by an existing\nLLM where we explore an LLM providing con-\nstrained and unconstrained textual feedback.\nWe also outline future directions to align mod-\nels with natural language feedback. 1", "sections": [{"title": "1 Introduction", "content": "To ensure language models are effective in real-\nworld scenarios, their behavior must be aligned\nwith the specific goals of the applications. Tech-\nniques for alignment often involve training a re-\nward model over preference data and using a Re-\ninforcement Learning (RL) solution to steer the\nmodel toward preferred responses (Ouyang et al.,\n2022; Snell et al., 2022). A common argument for\nusing RL techniques is that, unlike supervised fine-\ntuning which trains the model to predict a single\ngood answer, an RL technique allows the model to\nget both positive and negative reward signals for its\npredictions (Goldberg, 2023). Reinforcement learn-\ning methods, while powerful, often face significant\nhurdles that hamper their public adoption, i.e., re-\nquiring vast amounts of training data (Yarats et al.,\n2021). Vamplew et al. (2022) argue that the scalar\nreward often provides a very sparse informative\nsignal for the model.\nThis work proposes a text-based feedback mech-\nanism for aligning language models. We propose\nthat providing models with textual feedback, rather\nthan numerical scores, can offer a more nuanced\nand informative learning signal for understanding\nhuman preferences. This textual feedback can\nimprove the process of aligning AI systems. In\nALT, we depart from traditional RL approaches\nsuch as PPO and focus on reward-conditioned RL.\nReward-conditioned RL (Chen et al., 2021) is an\napproach that allows the policy to be trained using\na supervised learning loss similar to sequence mod-\neling. More recently, reward-conditioned RL has\nbeen adapted to the task of alignment (Lu et al.,\n2022) where they condition generations using re-\nward quantiles as feedback and (Dong et al., 2023b)\nwhere the generations are conditioned on numeri-\ncal reward feedback. Building upon this, our work\nintroduces ALT, which leverages the richness of\nthe signal provided by textual feedback to improve\nmodel performance.\nWe conduct a series of experiments across three\ndifferent tasks: reducing toxic language, summa-\nrizing text, and generating dialogue that is both\nhelpful and harmless. The textual feedback, owing\nto its informativeness, can improve the efficacy and\nefficiency of LM alignment compared to a scalar\nreward, reward quantiles, or numerical scores. For\nexperiments on reducing toxicity, we find that ALT\ncan outperform all other approaches and reduce\ntoxicity by 62% when compared to PPO. For sum-\nmarization, we show that ALT can align the LM\ncomparably to PPO with around 20% of the train-"}, {"title": "2 ALT: ALignment with Textual feedback", "content": "ALT adapts the reward-conditioned RL framework\nby training the model to be conditioned on textual\nfeedback. ALT (Figure 1) consists of two distinct\nphases: data collection and training. In the data\ncollection phase, we sample generations from the\nmodel and assign language feedback to these gen-\nerations. In the training phase, the model is trained\nto map its generations to the assigned feedback.\nThese steps are repeated iteratively as the model is\ntrained to generate conditioned on feedback."}, {"title": "2.1 Data Collection: Sampling + Feedback", "content": "The Sampling in the initial step, uses a supervised\nfine-tuned model that has not been trained to gen-\nerate conditioned on feedback. Hence, only for the\ninitial step, we generate conditioned on the input.\nGiven a dataset $X = [X_1,X_2...]$, language\nfeedback provider F, and a supervised fine-tuned\nmodel $p_{\\theta_0}$, we first sample initial generations from\nthe model. In the initial sampling process, we con-\ndition the generations on the input $y_i \\sim p_{\\theta_0} (x_i)$.\nWe then assign Feedback to these generations\n$f_i = F(y_i, x_i)$. These instances of input, gen-\neration, and feedback are then added to a datapool\n$D \\leftarrow D \\cup (x_i, y_i, f_i)$.\nAfter the model has been trained to generate\nresponses conditioned on the feedback, we can now\nalign the model by instructing its generation using\ntext. To generate responses that align the model\nto certain feedback, exemplar feedback f is now\nused to condition the generation $y_i \\sim p_{\\theta_k} (x_i, f)$.\nFor example, if the LLM's generations want to be\naligned to be less toxic, feedback Nontoxic can\nbe prepended to the prompt. The intuition behind\nusing exemplar feedback is that we are querying\nthe LLM for its understanding of the feedback we\nwant to steer it toward so we can iteratively refine\nthis representation."}, {"title": "2.2 Training", "content": "In the Training phase, we aim to teach the LLM\na mapping from feedback to its generations. We\nwant to optimize the negative log-likelihood of the\ngenerations conditioned on the feedback provided.\nThis is done by simply pre-pending the feedback\nto the prompt.\n$L_{NLL} = -E_{(x_i,y_i,f_i)\\sim D} log P_{\\theta} (y_i|x_i, f_i)$\nHere $p_{\\theta}$ refers to the probability over text mod-\neled by the language model. One well-documented\nside effect of aligning language models is that the\nmodel's generations can sway far away from the\ninitial reference model leading to pre-training for-\ngetting, also known as alignment tax (Ouyang et al.,\n2022). To prevent this, a regularization term that\nminimizes the KL divergence between the current\nmodel and the initial reference model can be added\nto the loss function.\n$L_{ref} = E_{(x_i,y_i,f_i)\\sim D} \\sum_{t=1}^T KL (p_{\\theta}(y_{i_t}|y_{i<t}, x_i) ||$\n$p_{\\theta_0}(y_{i_t}|y_{i<t}, f_i, x_i))$\nFurthermore, an entropy regularization term to en-\ncourage exploration by avoiding too peaky proba-\nbility distributions can also be added.\n$L_{H} = -E_{(x_i,y_i,f_i)\\sim D}H(p_{\\theta})$\nFinally, the final loss can be written as:\n$L = L_{NLL} + \\beta L_{ref} + \\alpha L_{H}  \\qquad(1)$\nwhere the hyper-parameters $\\beta$ and $\\alpha$ control the\ntrade-off between alignment maximization and for-\ngetting mitigation, and output diversity, respec-\ntively."}, {"title": "2.3 Feedback Provider", "content": "To mimic how feedback is typically collected, we\nexamined three different methods for providing\ntextual feedback to the models.\nReward Model Feedback A pre-existing reward\nmodel can be used as a feedback provider by con-\nverting its numerical rewards into categorized feed-\nback relevant to the specific task. For instance,\nin the context of toxicity mitigation, the reward\nrange can be divided into five quantiles, each\ncorresponding to a distinct toxicity level: \u201cvery\ntoxic\u201d, \u201ctoxic, medium toxic\u201d, \u201cslightly"}, {"title": "3 Tasks", "content": "We test the efficacy and efficiency of ALT on three\ndifferent categories of tasks that benefit from vary-\ning textual feedback."}, {"title": "3.1 Toxicity Reduction", "content": "As LLMs are trained on large amounts of text from\nthe internet they can be prone to generating toxic\ncontent (Gehman et al., 2020). In user-facing set-\ntings, we want to steer models away from produc-\ning toxic and potentially harmful content. We first\ninvestigate the effectiveness of our approach using\nthe REALTOXICITYPROMPTS dataset (Gehman\net al., 2020). Here the LLM is prompted with a\nseemingly harmless piece of text and is judged on\nthe toxicity of its generation. To assess toxicity,\nwe are focusing on a single generation aspect (the\ntext's toxicity level), so to get textual feedback we\nquantize the reward model's feedback into various\npreset texts."}, {"title": "3.2 Summarization", "content": "We next evaluate if ALT can better help align the\nLLM to user preferences. To verify this, we ex-\nperiment using the Reddit TL;DR dataset (V\u00f6lske\net al., 2017). The TL;DR dataset has become a\ncommon benchmark for measuring alignment. The\nprompts consist of Reddit forum posts and the task\nis to generate a summary of the main points of the\npost while fulfilling different facets that humans\ncare about, such as coherence, accuracy, coverage,\nor conciseness. Similar to the toxicity task, we\nleverage an existing reward model that accounts for\nthe different facets humans value on summaries to\npredict a single scalar reward and quantize the re-\nward model's feedback into preset texts indicating\nincreasing degrees of alignment fulfillment."}, {"title": "3.3 Dialog Response Generation", "content": "For this task, we experiment with the Anthropic\nHH dataset (Bai et al., 2022). The task involves\ntraining a model to generate helpful responses to\nuser queries. The model has to learn to balance\nbeing helpful without producing content that can\ncause harm. In this setting, we skip the ad-hoc\nmapping from quantized numerical scores provided\nby a reward model to textual feedback and explore\nusing an LLM as the feedback provider by directly\nproviding the textual feedback indicating varying\ndegrees of helpfulness and harmlessness."}, {"title": "3.4 Experimental Details", "content": ""}, {"title": "3.4.1 Toxicity Reduction", "content": "We follow the same experimental setup as in\n(Liu et al., 2021; Lu et al., 2022), and con-\nsider reducing toxicity from GPT2-large on\nthe REALTOXICITYPROMPTS benchmark. In\naddition, we also conduct an out-of-domain\nevaluation with the WRITINGPROMPTS2 test\ndataset (Fan et al., 2018). As a reward function\nand a proxy for measuring the toxicity of the\nLLM generations, we use the Perspective API.3\nWe use $K = 5$ quantiles, obtained by sorting\nthe samples in the data pool from lowest toxicity\n(highest reward) to highest toxicity (lowest\nreward), and map them to language feedback\nindicating increasing degrees of toxicity: \u201cLowest\nToxicity\u201d, \"Low-Moderate Toxicity\",\n\"Moderate Toxicity\", \"High-Moderate\nToxicity\u201d, and \"Maximum Toxicity\u201d. We report\ntraining details and hyper-parameters in A.1.\nDuring evaluation, we sample 25 generations for\neach prompt using nucleus sampling with $top\\_p =$\n0.9 and condition on Lowest Toxicity. We report\nthe avg. max. toxicity, measured as the average\nmaximum toxicity over the 25 generations, and\nthe toxicity prob. as the empirical toxic proba-\nbility of at least one of any 25 generations being\ntoxic, i.e., score > 0.5 (both measured by Perspec-\ntiveAPI). Regarding language quality, the fluency\nis measured as the conditional output perplexity"}, {"title": "3.4.2 Summarization", "content": "During training, for every iteration, we draw at\nrandom (with replacement) a subset of 2048 train-\ning prompts and we sample multiple generations\nfor each prompt. The training is started from an\nSFT GPT-J4 (6B parameters) model fine-tuned on\nthe human-written reference summaries using the\nTRLX (Havrilla et al., 2023) framework for RLHF.\nWe implement a version of QUARK, with a slight\nmodification as to sample multiple generations per\nprompt to compute the reward quantiles locally in-\nstead of globally across all prompts. We found that\nthis was crucial for training. We use $K = 5$ quan-\ntile tokens, which are newly added to the tokenizer.\nWe sample 96 generations for each prompt but only\ntrain on 10 generations drawn at random (2 for\neach quantile) to speed up training. On top of that,\nALTRM is implemented by mapping reward quan-\ntiles to textual feedback. We prepend to the prompt\nthe feedback sentence formatted as \"; where the language feedback is one of:\n\"Excellent\u201d, \u201cGood\u201d, \u201cMediocre\u201d, \u201cBad\", and\n\"Horrible\u201d. Similarly, 96 generations per prompt\nare sampled though training takes place only on 10\nsamples (2 for each feedback type).\""}, {"title": "3.4.3 Dialog Response Generation", "content": "We focus on the first Human-Assistant interaction\nfrom the Anthropic HH dataset as in (Rafailov et al.,\n2023) and we filter out duplicates. For speedup and\nreducing evaluation costs, we subsample the test set\nto 1k test prompts. During each training iteration,\nwe draw at random (with replacement) a subset of\n2048 prompts and we sample multiple generations\nfor each prompt. The training is started from an\nSFT Pythia (2.8B parameters) model fine-tuned\non the annotated chosen summaries from the train-\ning split. For this task, we implement our ALTLMC"}, {"title": "4 Results", "content": "ALT can effectively align the model to reduce\ntoxicity For the task of toxicity reduction (Ta-\nble 1), we find that ALTRM can reduce the toxicity\nof the model's generations more effectively than\nQUARK on both in-domain (0.148 \u2192 0.082), and\nout-of-domain (0.193 \u2192 0.113) indicating that\nmerely switching out a quantiled reward feedback\nwith textual feedback can result in more effective\nalignment. We note that ALT outperforms PPO\nat aligning for lower toxicity while maintaining a\nlower perplexity (14.27 \u2192 12.31). We provide\nqualitative examples in B.1.\nALT can effectively align the model to improve\nsummarization For the task of summarization,\nwe also find that merely switching out the numer-\nical reward quantile with preset textual feedback\ncan improve summarization. When compared to\nQUARK, ALTRM achieves a higher reward model"}, {"title": "5 Related Work", "content": "Alignment Previous research has successfully\nleveraged RLHF to enhance the instruction-\nfollowing capabilities of LLMs (Ouyang et al.,\n2022; Bai et al., 2022; Snell et al., 2022). Alter-\nnates to PPO have been proposed for alignment\nsuch as training in an offline setting (Snell et al.,\n2022), directly optimizing a preference objective\n(Rafailov et al., 2023), or treating the problem as a\nconditional sequence decision problem (Lu et al.,\n2022; Dong et al., 2023b) or ranking responses\n(Dong et al., 2023a; Yuan et al., 2023). ALT treats\nalignment as a conditional sequence decision prob-\nlem, but uses textual feedback.\nReward Conditional Training for Alignment\nOur approach shares similarities and builds on\nworks on reward conditional training. Recent work\non \"Upside-down RL\u201d has shown that the RL prob-\nlem can be reversed, where the policy learns to\nmap high-rewards to actions (Schmidhuber, 2020;\nKumar et al., 2019). This allows the policy to be\ntrained using a supervised learning loss. This tech-\nnique is taken further by (Chen et al., 2021) where\nthe RL setup can be converted to a sequence mod-"}, {"title": "6 Conclusion", "content": "We presented ALT, an approach that uses textual\nfeedback to align an LM. Our findings across di-\nverse tasks, such as reducing model toxicity, im-\nproving summarization, and aligning dialogue, un-\nderscore the efficacy and efficiency of this ap-\nproach. Notably, ALT surpasses traditional rein-\nforcement learning methods like PPO in toxicity\nreduction and achieves comparable summarization\nperformance with considerably fewer training sam-\nples. Furthermore, our results indicate the feasibil-\nity of leveraging large language models to provide\neffective feedback for aligning dialogue models.\nOur current experiments failed to show improve-\nments with more detailed textual feedback. How-\never, we believe that this outcome could change\nwith more consistent feedback. We leave this explo-\nration for future work. Our findings open promis-\ning directions for further research into using varied\ntypes of feedback to improve LLM alignment."}, {"title": "Limitations", "content": "Collecting the textual feedback required for our\napproach might be harder to collect than feedback\nin the form of preferences over binary comparisons.\nGPT-3.5 as an implicit reward model is prompt de-\npendent and can sometimes embody preferences\ndifferent than the ones that humans would prefer.\nWe believe that improving the reward model capa-\nbilities in assessing responses and providing feed-\nback would lead to a better-aligned LLM policy.\nIn our experiments using LLM-based feedback,\nwe noticed that longer, unconstrained feedback\nproved more difficult for models to learn from com-\npared to shorter, categorical feedback. We specu-\nlate this may be due to inconsistencies in the longer\nfeedback. Additionally, smaller models with lim-\nited context length may struggle to process longer\nfeedback effectively."}, {"title": "A Training Details", "content": ""}, {"title": "A.1 Toxicity Reduction", "content": "We fine-tune GPT2-large using the following lan-\nguage feedback tags: \"Lowest Toxicity\",\n\"Low-Moderate Toxicity\", \"Moderate\nToxicity\", \"High-Moderate Toxicity\", and\n\"Maximum Toxicity\". At inference time we target\nnontoxic behavior by sampling conditioned on the\n\"best\" feedback type, i.e., \"Lowest Toxicity\".\nAs each element in the batch might have associated\nlanguage feedback tokens of different lengths,\nwe pad them on the left to match the size of the\nlongest feedback tokens within the current batch.\nWe also insert a newly added separator token\n\"\" between the feedback tokens and\nthe query input IDs, which is useful for easy\nremoval of the feedback tokens when required on\ndifferent points of the training pipeline.\nHyper-parameters for training are given in Table\n3. Training was performed on 4 NVIDIA GeForce\nRTX 2080 Ti (12GB) and took around 21h to com-\nplete.\nIn this experiment, the KL-penalty term with\nthe reference policy in the loss function was im-\nportant in avoiding obtaining a low-toxicity policy\nthat would just output gibberish language. How-\never, in the subsequent experiments, we got rid of\nthis KL-penalty term without sacrificing perplexity,\nreducing thus the need for storing the reference\npolicy during training. We hypothesize that for\nthe unlearning toxicity task, this was needed as\nwe departed training from a pre-trained model and\nbecause the task was to complete text from a few\nquery tokens, as opposed to starting from an SFT\nmodel and having a more clearly defined task on\nsummarization and dialogue."}, {"title": "A.2 Summarization", "content": "We fine-tune the GPT-J SFT model using the lan-\nguage feedback mentioned on 3.4.2. As the distinct\nphases of our algorithm are decoupled, one can\nleverage different computation resources at every\nstage. The data collection stage is the most costly\none in terms of time required to sample and provide\nfeedback to several generations, but one can launch\nmultiple smaller GPU jobs and employ frameworks\nfor faster inference such as vLLM (Kwon et al.,"}, {"title": "A.3 Dialogue", "content": "We fine-tune the Pythia-2.8b SFT model using the\nlanguage feedback mentioned on 3.4.3. The same\ndecoupled nature for the data collection and train-\ning described in A.2 applies here, and we leveraged\nthe same computation resources. Figure 5 contains\nthe generations' length and % of truncated gener-\nations along iterations for ALTLMC and SteerLM,\nand Figure 6 contains the perplexity curve as train-\ning progresses.\nTo avoid incurring high expenses, we sample\n20 generations for each prompt instead of 96\nbut we still apply the same rejection sampling\nas before and try to draw at random 2 genera-\ntions for each feedback category, resulting in 8\nsamples per prompt to be used for training. On"}, {"title": "B Qualitative Results", "content": ""}, {"title": "B.1 Toxicity Reduction", "content": "We include qualitative examples comparing\nALTRM with other SoTA baselines in Table 6."}, {"title": "B.2 Summarization", "content": "We include qualitative example summaries gener-\nated by SFT, PPO, Quark, ALTRM, and ALTLMU"}, {"title": "C Prompts for collecting GPT-3.5-Turbo feedback", "content": "Figure 19 contains the GPT-3.5 feedback provider\nprompt for training ALTLMC on the HH dialog task.\nFigure 20 contains the GPT-3.5 feedback provider\nprompt for training ALTLMU on the TL;DR sum-\nmarization task."}, {"title": "D SteerLM implementation details", "content": "Here we detail the implementation differences be-\ntween our SteerLM and the original implementa-\ntion from (Dong et al., 2023b).\nRegarding the feedback phase, (Dong et al.,\n2023b) trains an Attribute Prediction Model with\nhuman-annotated data that evaluates a response"}, {"title": "E HH test-time steerability assessment", "content": "We conducted a small evaluation setting to assess\nthe steerability of our model at run-time. To do\nso, we evaluated our last model checkpoint when\nconditioned on Harmful instead of Harmless and\nvery helpful. We notice that this leads to gen-\nerations being slightly less Harmless and very\nhelpful (65.1 \u2192 60.0) and slightly more Harmful\n(11.6 \u2192 15.1), but that our model is not steerable\nat inference time to produce harmful content. This\nis consistent with our exploitation of the exemplar\nfeedback commented in 2.3.1. Similarly to PPO\nand DPO, by using a single exemplar feedback,\nwe are steering our model towards an implicit set"}]}