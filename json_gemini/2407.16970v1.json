{"title": "Towards Aligning Language Models with Textual Feedback", "authors": ["Sa\u00fcc Abadal Lloret", "Shehzaad Dhuliawala", "Keerthiram Murugesan", "Mrinmaya Sachan"], "abstract": "We present ALT (ALignment with Textual feedback), an approach that aligns language models with user preferences expressed in text. We argue that text offers greater expressiveness, enabling users to provide richer feedback than simple comparative preferences and this richer feedback can lead to more efficient and effective alignment. ALT aligns the model by conditioning its generation on the textual feedback. Our method relies solely on language modeling techniques and requires minimal hyper-parameter tuning, though it still presents the main benefits of RL-based alignment algorithms and can effectively learn from textual feedback. We explore the efficacy and efficiency of textual feedback across different tasks such as toxicity reduction, summarization, and dialog response generation. We find that ALT outperforms PPO for the task of toxicity reduction while being able to match its performance on summarization with only 20% of the samples. We also explore how ALT can be used with feedback provided by an existing LLM where we explore an LLM providing constrained and unconstrained textual feedback. We also outline future directions to align models with natural language feedback.", "sections": [{"title": "1 Introduction", "content": "To ensure language models are effective in real-world scenarios, their behavior must be aligned with the specific goals of the applications. Techniques for alignment often involve training a reward model over preference data and using a Reinforcement Learning (RL) solution to steer the model toward preferred responses (Ouyang et al., 2022; Snell et al., 2022). A common argument for using RL techniques is that, unlike supervised fine-tuning which trains the model to predict a single good answer, an RL technique allows the model to get both positive and negative reward signals for its predictions (Goldberg, 2023). Reinforcement learning methods, while powerful, often face significant hurdles that hamper their public adoption, i.e., requiring vast amounts of training data (Yarats et al., 2021). Vamplew et al. (2022) argue that the scalar reward often provides a very sparse informative signal for the model.\nThis work proposes a text-based feedback mechanism for aligning language models. We propose that providing models with textual feedback, rather than numerical scores, can offer a more nuanced and informative learning signal for understanding human preferences. This textual feedback can improve the process of aligning AI systems. In ALT, we depart from traditional RL approaches such as PPO and focus on reward-conditioned RL. Reward-conditioned RL (Chen et al., 2021) is an approach that allows the policy to be trained using a supervised learning loss similar to sequence modeling. More recently, reward-conditioned RL has been adapted to the task of alignment (Lu et al., 2022) where they condition generations using reward quantiles as feedback and (Dong et al., 2023b) where the generations are conditioned on numerical reward feedback. Building upon this, our work introduces ALT, which leverages the richness of the signal provided by textual feedback to improve model performance.\nWe conduct a series of experiments across three different tasks: reducing toxic language, summarizing text, and generating dialogue that is both helpful and harmless. The textual feedback, owing to its informativeness, can improve the efficacy and efficiency of LM alignment compared to a scalar reward, reward quantiles, or numerical scores. For experiments on reducing toxicity, we find that ALT can outperform all other approaches and reduce toxicity by 62% when compared to PPO. For summarization, we show that ALT can align the LM comparably to PPO with around 20% of the train-"}, {"title": "2 ALT: ALignment with Textual feedback", "content": "ALT adapts the reward-conditioned RL framework by training the model to be conditioned on textual feedback. ALT (Figure 1) consists of two distinct phases: data collection and training. In the data collection phase, we sample generations from the model and assign language feedback to these generations. In the training phase, the model is trained to map its generations to the assigned feedback. These steps are repeated iteratively as the model is trained to generate conditioned on feedback."}, {"title": "2.1 Data Collection: Sampling + Feedback", "content": "The Sampling in the initial step, uses a supervised fine-tuned model that has not been trained to generate conditioned on feedback. Hence, only for the initial step, we generate conditioned on the input. Given a dataset \\(X = [x_1,x_2,...]\\), language feedback provider \\(F\\), and a supervised fine-tuned model \\(p_{\\theta_0}\\), we first sample initial generations from the model. In the initial sampling process, we condition the generations on the input \\(y_i \\sim p_{\\theta_0}(x_i)\\). We then assign Feedback to these generations \\(f_i = F(y_i, x_i)\\). These instances of input, generation, and feedback are then added to a datapool \\(D \\leftarrow D \\cup (x_i, y_i, f_i)\\).\nAfter the model has been trained to generate responses conditioned on the feedback, we can now align the model by instructing its generation using text. To generate responses that align the model to certain feedback, exemplar feedback \\(f\\) is now used to condition the generation \\(y_i \\sim p_{\\theta_k}(x_i, f)\\). For example, if the LLM's generations want to be aligned to be less toxic, feedback Nontoxic can be prepended to the prompt. The intuition behind using exemplar feedback is that we are querying the LLM for its understanding of the feedback we want to steer it toward so we can iteratively refine this representation."}, {"title": "2.2 Training", "content": "In the Training phase, we aim to teach the LLM a mapping from feedback to its generations. We want to optimize the negative log-likelihood of the generations conditioned on the feedback provided. This is done by simply pre-pending the feedback to the prompt.\n\\[L_{NLL} = -E_{(x_i,y_i,f_i)\\sim D} \\log p_{\\theta} (y_i|x_i, f_i)\\]\nHere \\(p_{\\theta}\\) refers to the probability over text modeled by the language model. One well-documented side effect of aligning language models is that the model's generations can sway far away from the initial reference model leading to pre-training forgetting, also known as alignment tax (Ouyang et al., 2022). To prevent this, a regularization term that minimizes the KL divergence between the current model and the initial reference model can be added to the loss function.\n\\[L_{ref} = E_{(x_i,y_i,f_i)\\sim D} \\sum_{t=1}^T KL \\left(p_{\\theta_0}(y_{i_t}|y_{i<t}, x_i) || p_{\\theta}(y_{i_t}|y_{i<t}, f_i, x_i)\\right)\\]\nFurthermore, an entropy regularization term to encourage exploration by avoiding too peaky probability distributions can also be added.\n\\[L_H = -E_{(x_i,y_i,f_i)\\sim D}H(p_{\\theta})\\]\nFinally, the final loss can be written as:\n\\[L = L_{NLL} + \\beta L_{ref} + \\alpha L_H\\qquad(1)\\]\nwhere the hyper-parameters \\(\\beta\\) and \\(\\alpha\\) control the trade-off between alignment maximization and forgetting mitigation, and output diversity, respectively."}, {"title": "2.3 Feedback Provider", "content": "To mimic how feedback is typically collected, we examined three different methods for providing textual feedback to the models.\nReward Model Feedback A pre-existing reward model can be used as a feedback provider by converting its numerical rewards into categorized feedback relevant to the specific task. For instance, in the context of toxicity mitigation, the reward range can be divided into five quantiles, each corresponding to a distinct toxicity level: \u201cvery toxic\", \u201ctoxic, medium toxic\u201d, \u201cslightly"}, {"title": "2.3.1 Exemplar Feedback", "content": "One of the challenges of reward conditional RL is selecting a high reward to condition on. From iteration 2 onward, our method samples new generations by conditioning on exemplar feedback that represents the desired behavior to which we want to steer our policy. The intuition behind using this exemplar feedback is that we are querying the model for its understanding of a particular feedback so we can refine it iteratively. In ALT, we focus on driving the sampling phase by conditioning on single exemplar feedback, to be used at inference time to cater to an implicit set of user preferences, but future work might explore the use of several exemplar feedbacks as a mechanism for catering heterogeneous user preferences at run-time."}, {"title": "3 Tasks", "content": "We test the efficacy and efficiency of ALT on three different categories of tasks that benefit from varying textual feedback."}, {"title": "3.1 Toxicity Reduction", "content": "As LLMs are trained on large amounts of text from the internet they can be prone to generating toxic content (Gehman et al., 2020). In user-facing settings, we want to steer models away from producing toxic and potentially harmful content. We first investigate the effectiveness of our approach using the REALTOXICITYPROMPTS dataset (Gehman et al., 2020). Here the LLM is prompted with a seemingly harmless piece of text and is judged on the toxicity of its generation. To assess toxicity, we are focusing on a single generation aspect (the text's toxicity level), so to get textual feedback we quantize the reward model's feedback into various preset texts."}, {"title": "3.2 Summarization", "content": "We next evaluate if ALT can better help align the LLM to user preferences. To verify this, we experiment using the Reddit TL;DR dataset (V\u00f6lske et al., 2017). The TL;DR dataset has become a common benchmark for measuring alignment. The prompts consist of Reddit forum posts and the task is to generate a summary of the main points of the post while fulfilling different facets that humans care about, such as coherence, accuracy, coverage, or conciseness. Similar to the toxicity task, we leverage an existing reward model that accounts for the different facets humans value on summaries to"}, {"title": "3.3 Dialog Response Generation", "content": "For this task, we experiment with the Anthropic HH dataset (Bai et al., 2022). The task involves training a model to generate helpful responses to user queries. The model has to learn to balance being helpful without producing content that can cause harm. In this setting, we skip the ad-hoc mapping from quantized numerical scores provided by a reward model to textual feedback and explore using an LLM as the feedback provider by directly providing the textual feedback indicating varying degrees of helpfulness and harmlessness."}, {"title": "3.4 Experimental Details", "content": "3.4.1 Toxicity Reduction\nWe follow the same experimental setup as in (Liu et al., 2021; Lu et al., 2022), and consider reducing toxicity from GPT2-large on the REALTOXICITYPROMPTS benchmark. In addition, we also conduct an out-of-domain evaluation with the WRITINGPROMPTS test dataset (Fan et al., 2018). As a reward function and a proxy for measuring the toxicity of the LLM generations, we use the Perspective API. We use \\(K = 5\\) quantiles, obtained by sorting the samples in the data pool from lowest toxicity (highest reward) to highest toxicity (lowest reward), and map them to language feedback indicating increasing degrees of toxicity: \u201cLowest Toxicity\u201d, \u201cLow-Moderate Toxicity\u201d, \u201cModerate Toxicity\u201d, \u201cHigh-Moderate Toxicity\u201d, and \u201cMaximum Toxicity\u201d. We report training details and hyper-parameters in A.1.\nDuring evaluation, we sample 25 generations for each prompt using nucleus sampling with top_p = 0.9 and condition on Lowest Toxicity. We report the avg. max. toxicity, measured as the average maximum toxicity over the 25 generations, and the toxicity prob. as the empirical toxic probability of at least one of any 25 generations being toxic, i.e., score > 0.5 (both measured by PerspectiveAPI). Regarding language quality, the fluency is measured as the conditional output perplexity"}, {"title": "4 Results", "content": "ALT can effectively align the model to reduce toxicity For the task of toxicity reduction (Table 1), we find that ALTRM can reduce the toxicity of the model's generations more effectively than QUARK on both in-domain (0.148 \u2192 0.082), and out-of-domain (0.193 \u2192 0.113) indicating that merely switching out a quantiled reward feedback with textual feedback can result in more effective alignment. We note that ALT outperforms PPO at aligning for lower toxicity while maintaining a lower perplexity (14.27 \u2192 12.31). We provide qualitative examples in B.1.\nALT can effectively align the model to improve summarization For the task of summarization, we also find that merely switching out the numerical reward quantile with preset textual feedback can improve summarization. When compared to QUARK, ALTRM achieves a higher reward model"}, {"title": "5 Related Work", "content": "Alignment Previous research has successfully leveraged RLHF to enhance the instruction-following capabilities of LLMs (Ouyang et al., 2022; Bai et al., 2022; Snell et al., 2022). Alternates to PPO have been proposed for alignment such as training in an offline setting (Snell et al., 2022), directly optimizing a preference objective (Rafailov et al., 2023), or treating the problem as a conditional sequence decision problem (Lu et al., 2022; Dong et al., 2023b) or ranking responses (Dong et al., 2023a; Yuan et al., 2023). ALT treats alignment as a conditional sequence decision problem, but uses textual feedback.\nReward Conditional Training for Alignment Our approach shares similarities and builds on works on reward conditional training. Recent work on \"Upside-down RL\u201d has shown that the RL problem can be reversed, where the policy learns to map high-rewards to actions (Schmidhuber, 2020; Kumar et al., 2019). This allows the policy to be trained using a supervised learning loss. This technique is taken further by (Chen et al., 2021) where the RL setup can be converted to a sequence mod-"}, {"title": "6 Conclusion", "content": "We presented ALT, an approach that uses textual feedback to align an LM. Our findings across diverse tasks, such as reducing model toxicity, improving summarization, and aligning dialogue, underscore the efficacy and efficiency of this approach. Notably, ALT surpasses traditional reinforcement learning methods like PPO in toxicity reduction and achieves comparable summarization performance with considerably fewer training samples. Furthermore, our results indicate the feasibility of leveraging large language models to provide effective feedback for aligning dialogue models. Our current experiments failed to show improvements with more detailed textual feedback. However, we believe that this outcome could change with more consistent feedback. We leave this exploration for future work. Our findings open promising directions for further research into using varied types of feedback to improve LLM alignment."}, {"title": "A Training Details", "content": "A.1 Toxicity Reduction\nWe fine-tune GPT2-large using the following language feedback tags: \"Lowest Toxicity\", \"Low-Moderate Toxicity\", \"Moderate Toxicity\", \"High-Moderate Toxicity\", and \"Maximum Toxicity\". At inference time we target nontoxic behavior by sampling conditioned on the \"best\" feedback type, i.e., \"Lowest Toxicity\". As each element in the batch might have associated language feedback tokens of different lengths, we pad them on the left to match the size of the longest feedback tokens within the current batch. We also insert a newly added separator token \"<|separator>|\" between the feedback tokens and the query input IDs, which is useful for easy removal of the feedback tokens when required on different points of the training pipeline.\nHyper-parameters for training are given in Table 3. Training was performed on 4 NVIDIA GeForce RTX 2080 Ti (12GB) and took around 21h to complete.\nIn this experiment, the KL-penalty term with the reference policy in the loss function was important in avoiding obtaining a low-toxicity policy that would just output gibberish language. However, in the subsequent experiments, we got rid of this KL-penalty term without sacrificing perplexity, reducing thus the need for storing the reference policy during training. We hypothesize that for the unlearning toxicity task, this was needed as we departed training from a pre-trained model and because the task was to complete text from a few query tokens, as opposed to starting from an SFT model and having a more clearly defined task on summarization and dialogue."}, {"title": "A.2 Summarization", "content": "We fine-tune the GPT-J SFT model using the language feedback mentioned on 3.4.2. As the distinct phases of our algorithm are decoupled, one can leverage different computation resources at every stage. The data collection stage is the most costly one in terms of time required to sample and provide feedback to several generations, but one can launch multiple smaller GPU jobs and employ frameworks for faster inference such as vLLM (Kwon et al.,"}, {"title": "A.3 Dialogue", "content": "We fine-tune the Pythia-2.8b SFT model using the language feedback mentioned on 3.4.3. The same decoupled nature for the data collection and training described in A.2 applies here, and we leveraged the same computation resources."}]}