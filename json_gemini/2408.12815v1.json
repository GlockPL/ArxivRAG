{"title": "Staircase Cascaded Fusion of Lightweight Local Pattern Recognition and Long-Range Dependencies for Structural Crack Segmentation", "authors": ["Hui Liu", "Chen Jia", "Fan Shi", "Xu Cheng", "Mianzhao Wang", "Shengyong Chen"], "abstract": "Detecting cracks with pixel-level precision for key structures is a significant challenge, as existing methods struggle to effectively integrate local textures and pixel dependencies of cracks. Furthermore, these methods often possess numerous parameters and substantial computational requirements, complicating deployment on edge devices. In this paper, we propose a staircase cascaded fusion crack segmentation network (CrackSCF) that generates high-quality crack segmentation maps using minimal computational resources. We constructed a staircase cascaded fusion module that effectively captures local patterns of cracks and long-range dependencies of pixels, and it can suppress background noise well. To reduce the computational resources required by the model, we introduced a lightweight convolution block, which replaces all convolution operations in the network, significantly reducing the required computation and parameters without affecting the network's performance. To evaluate our method, we created a challenging benchmark dataset called TUT and conducted experiments on this dataset and five other public datasets. The experimental results indicate that our method offers significant advantages over existing methods, especially in handling background noise interference and detailed crack segmentation. The F1 and mIoU scores on the TUT dataset are 0.8382 and 0.8473, respectively, achieving state-of-the-art (SOTA) performance while requiring the least computational resources. The code and dataset is available at https://github.com/Karl1109/CrackSCF.", "sections": [{"title": "I. INTRODUCTION", "content": "CRACKS are common defects in structures such as pave-ments and buildings, which can lead to serious safety issues. Therefore, regular automated inspections are crucial to ensuring production safety [1, 2]. Early automated detection methods used traditional image processing techniques for crack extraction [3-5]. While these methods are easy to implement, their performance in complex scenarios still needs improvement.\nIn recent years, benefiting from the strong local inductive capability of Convolutional Neural Networks (CNN), CNN-based networks such as MST-Net [12] and FcaHRNet [13] have shown efficient feature extraction capabilities in structural crack segmentation tasks. The spatial invariance of CNNS allows them to detect cracks anywhere in an image by learning similar features at different locations. In labeled images, crack labels are long, narrow structures of white pixels, while the background is large black areas. This challenges the model to distinguish between thin cracks and the background. Nevertheless, networks based on CNNs face challenges in capturing long-range dependencies between crack pixels, leading to segmentation discontinuities.\nThe success of VIT [14] and Swin Transformer [15] has demonstrated the advantages of Transformer [16], with their strong sequence processing capabilities, in handling visual"}, {"title": null, "content": "tasks. Unlike CNNs, transformers can capture dependencies between any pixels when processing images, making them more advantageous in handling complex topologies. To harness the advantages of both CNNs and transformers effec-tively, researchers have proposed networks such as CATran-SUNet [17], MFAFNet [18], and Crackmer [9], which integrate local texture information and long-range pixel dependencies through U-shaped connections or residual connections.\nDespite the aforementioned methods combining CNN and Transformer have improved performance in pixel-level crack segmentation tasks, they still have some limitations, as follows:\n1) The different information fusion strategies signifi-cantly affect the segmentation performance of CNN-Transformer networks. For instance, UCTNet [6] and CT-crackseg [8] use channel concatenation to fuse local patterns and long-range dependences information. However, concatenation merely stacks the feature information together without deeper feature interaction, failing to understand their correlation and complementarity, result-ing in poor segmentation performance on fine cracks and susceptibility to background noise. Additionally, DTrCNet [9] and Crackmer [10] use channel atten-tion to fuse information, which can enhance focus on certain features but neglects spatial information and pixel details in the feature maps, impacting the model's overall performance. As shown in Fig. 1(a), due to these shortcomings, these four methods fail to achieve the best performance.\n2) Most networks, in pursuit of performance, overlook thesignificant parameter and computational costs brought by numerous convolution operations. As shown in Fig. 1(b) and Fig. 1(c), networks like SFIAN [7], Crackmer [10], and CT-crackseg[8] have very high computational FLOPs and parameter counts, indicating that they are difficult to implement on resource-limited devices like smartphones and drones, significantly diminishing their practicality.\nTo address these issues, we propose a staircase cascaded fusion network that can generate high-quality segmentation maps with minimal computational resources while effectively suppressing background noise. Specifically, to better exploit the complementarity and correlation of feature maps without missing critical detail information, we introduce a staircase cascaded fusion module. This module utilizes the pixel attention mechanism, which finely captures local and long-range information, and the channel concatenation mechanism, which progressively integrates features from adjacent layers, to gen-erate crack segmentation maps that comprehensively combine local texture information and pixel dependencies. Furthermore, to decrease the parameter count and computational burden, we introduce a lightweight convolution block called LRDS. This block initially increases and then reduces the channel count, and separates the spatial and channel aspects, thereby minimizing computational costs while maintaining performance.\nTo assess the structural crack segmentation performance of the model in various complex environments, we also collected a dataset named TUT, which contains 1408 RGB images. Ex-"}, {"title": null, "content": "isting public datasets like DeepCrack [19] and Crack500 [20] include only 1-2 types of scenarios and have relatively simple crack shapes, which are insufficient for thoroughly testing the network's performance. The TUT dataset includes a broader range of real structural crack image scenarios, covering plastic runways, bricks, tiles, cement, bitumen, generator blades, metal materials, and underground pipelines. With such a diverse set of image scenarios, this dataset effectively assesses the network's generalization and robustness. Furthermore, the images feature occlusions, highly complex backgrounds, and uneven lighting, which further enhances the datasets diversity.\nIn conclusion, our main contributions are as follows:\n1) We proposed a novel CrackSCF network for structuralcrack segmentation. By introducing a staircase cascaded fusion module and a lightweight convolution block, this network effectively distinguishes between background and crack pixels with minimal computational resources, resulting in high-quality segmentation maps.\n2) We collected the TUT dataset, which includes structuralcracks in eight different scenarios, providing a more comprehensive evaluation of the network's performance.\n3) We carried out comprehensive experiments on five public datasets and the TUT benchmark dataset, demonstrat-ing that our proposed CrackSCF network outperforms existing SOTA methods.\nThe organization of the subsequent sections of this paper is as follows: In Section II, existing crack detection models and lightweight networks are reviewed. Section III details the components of the proposed network. Section IV delves into the experimental details and results. Finally, Section V provides a summary of the entire paper and highlights future research directions."}, {"title": "II. RELATED WORK", "content": "A. Crack Segmentation Networks\nRecently, a growing number of researchers have utilized CNNs to develop crack segmentation networks, achieving advanced performance. Al-Huda et al. [21] proposed the MDAUNet network, based on U-Net and dual attention modules, and designed a hybrid weighted segmentation loss function to address class imbalance issues. Cheng et al. [7] introduced the Selective Feature Fusion and Irregular-Aware Network (SFIAN) to effectively model irregular crack objects, resulting in clearer backgrounds on various crack images. Huang et al. [22] developed a crack detection network for masonry structures that combines thermal imaging and visible light image fusion with semantic segmentation, laying the foundation for the development of cross-modal methods for structural crack monitoring.\nAlthough the aforementioned methods have demonstrated the advantages of CNNs in crack segmentation tasks, CNNs are essentially local operations and cannot effectively perceive a small number of crack pixels amidst a large background. Therefore, these methods often result in discontinuous crack pixels and are highly susceptible to background noise interference."}, {"title": "III. METHODOLOGY", "content": "A. Network Architecture\nAs shown in Fig. 2, the CrackSCF network has three main components: a lightweight Multi-scale Feature Extractor (MFE), a Long-range Dependency Extractor (LDE), and a Staircase Cascaded Fusion Module (SCFM). The overall architecture of the network is shaped like a staircase, with feature maps of different scales progressively fused and upsampled. In this process, the advantages of CNNs and Transformers in feature extraction complement each other.\nSpecifically, for a given dataset, the crack images are first input into the MFE module, which is divided into four stages. In each stage, the extracted feature maps undergo a Feature Enhancement stage. Then, the LDE module processes these enhanced feature maps to obtain pixel sequences rich in long-range dependencies. Subsequently, the SCFM module receives these sequences along with the four layers of feature maps extracted by the MFE. After processing through four stages, the resolution is doubled at each stage while the number of channels is halved, ultimately resulting in the segmentation result.\nImportantly, as shown in Fig. 3, using the original convolution operations results in high FLOPs and Params, especially in the SCFM module, reaching 73.36G and 9.71M, respectively. Replacing all convolutions with LRDS blocks significantly reduced FLOPs and Params, demonstrating the effectiveness of LRDS in model lightweighting."}, {"title": "B. Lightweight Multi-scale Feature Extractor", "content": "As shown in Fig. 2 and Fig. 4, our designed LRDS block adopts a bottleneck-like structure. It comprises a Reduce layer, a depthwise convolution layer, a pointwise convolution layer, and an Expand layer. The input feature map first passes through the Reduce layer, reducing the input channels to $C_m$. It then goes through the depthwise convolution layer and the pointwise convolution layer, keeping the number of channels unchanged. Finally, the Expand layer increases the number of channels to $C_a$, producing the final output feature map.\nLow-rank approximation decomposes a high-dimensional matrix into low-dimensional matrices, reducing computation complexity. In CNNs, it is applied to convolutional kernel responses, assuming these lie in a low-rank subspace."}, {"title": null, "content": "Specifically, in the convolutional layer, we assume that its filter has a spatial size of k and a number of input channels of c. In computing the convolutional response, the filter is applied to a sub-tensor of the input tensor of shape $k \\times k \\times c$. Suppose this sub-tensor is $t \\in \\mathbb{R}^{k^2c+1}$, where we append a bias term at the end. The response at a position in the output, $y \\in \\mathbb{R}^{e}$, can be expressed as follows:\n$y = Wt$ (1)\nwhere e represents the number of filters and W is a $e \\times (k^2c+\n1)$ matrix. Assuming the vector y lies in a low-rank subspace, we can write it as $y = U(y - y_1) + y_1$, where U is a $e \\times e$ matrix of rank $e_o$, with $e_o < e$. $y_1$ is the mean vector of the response. Therefore, we can write the response as\n$y = UWt+b$ (2)\nwhere $b = Y_1 - Uy_1$ is the new bias. The rank of U is $e_o$, and it can be decomposed into two $e \\times e_o$ matrices Jand K such that $U = JKT$. We use $W_1 = KTW$ to denote a $e_o \\times (k^2c+1)$ matrix, which corresponds to a set of $e_o$ filters. Thus, we can rewrite the response as follows:\n$y = JW_1t + b$ (3)\nThe computational complexity of using Equation (3) is O$(e_ok^2c)$+O$(ee_o)$, whereas approximating y in the low-rank subspace using Equation (1) has a computational complexity of O$(ek^2c)$. Given that O$(ee_o) \\ll$ O$(e_ok^2c)$, the complexity of using Equation (3) is $\\frac{e_o}{e}$ times that of using Equation (1).\nBesides low-rank approximation, which reduces convolutional layer parameters and computational load, depthwise separable convolutions [26] also significantly reduce computational resources by decoupling spatial and channel dimensions. Therefore, we embedded depthwise and pointwise convolution layers in the LRDS block. Additionally, to better preserve local contextual information during feature extraction while avoid-ing excessive computational overhead, we incorporated strip pooling [32] technology into each bottleneck of ResNet50.\nWe constructed MFE by replacing all convolution operations in ResNet50 with LRDS convolution blocks."}, {"title": "C. Long-range Dependency Extractor", "content": "When extracting long-range dependencies of crack pixels, the deformable attention mechanism [33] adapts well to irregular structures. It calculates attention weights within a few sampled points, significantly reducing computational and memory overhead. Specifically, given an input $x \\in \\mathbb{R}^{C \\times H \\times W}$, let q denote a query element with query feature $z_q$ and reference point coordinates ($p_{qx}$, $p_{qy}$), the deformable attention is calculated as:\nMSDeformAttn(zq, Pq, {xc}C=1) = \n\\sum_{h=1}^{H} \\sum_{c=1}^{C} \\sum_{t=1}^{T} A_{hcqt} \\cdot W_{h}z_{c}(p_q(p_{qx},p_{qy}) + \\Delta_{hcqt})\n(4)"}, {"title": null, "content": "Where h represents the attention head, c represents the input feature layer, and t represents the sampling key point. $\\Delta_{hcqt}$ and $A_{hcqt}$ represent the offset and attention weight of the t-th sampling point in the c-th feature map and h-th attention head, respectively. The normalization is such that $\\sum_{t=1}^{T} A_{hcqt} = 1$. Here, normalized coordinates p\u2208 [0, 1] are used to represent the position of a reference point in the normalized feature map $\\phi_c(p_q)$ rescales the normalized coordinates onto the c-th layer feature map.\nThe structure of LDE is shown in Fig. 2. Specifically, before the enhanced feature maps enters the LDE encoder, it is flattened into a long pixel sequence, aiding the deformable attention mechanism in extracting pixel dependencies. As"}, {"title": "D. Staircase Cascaded Fusion Module", "content": "The design of our SCFM structure is shown in Fig. 2. To enhance useful semantic features in the MFE and LDE branches and ensure effective semantic information is not buried, we improved the Pixel Attention Guided Fusion (PAF) module, inspired by PIDNet [34] and the need for model lightweighting. The structure of PAF is shown in Fig. 6(a). We denote the pixels in the feature maps of the two branches as $v_c$ and $v_a$, respectively. Thus, the output can be expressed as:\n$\\sigma = Sigmoid(f_c(v_c) f_a(v_a))$ (5)\nwhere $f_c$ and $f_a$ denote pointwise convolution and batch nor-malization operations, respectively. $o$ indicates the probability that the two pixels belong to the same category. If o is high, the model will trust $v_c$ more, indicating that its semantic features are more accurate, and vice versa. Thus, the output of PAF can be represented as::\nOutput = $\\sigma v_c + (1 - \\sigma)v_a$ (6)\nTo improve inference speed and reduce computational re-source usage, all convolution operations in this module utilize"}, {"title": "E. Loss Function", "content": "The discrepancy between predictions and actual values is quantified by the loss function; a smaller value for this loss suggests superior model performance. Previously, Binary Cross-Entropy (BCE) [35] was often used, but it may not effectively distinguish the minority crack pixels in the image. Combining BCE with Dice loss [36] [37] addresses this issue. BCE improves the probability distribution for each pixel, while Dice loss maximizes the overlap between predictions and true labels, enhancing robustness against imbalanced data.\nOur loss L can be calculated using Equation (7), which combines the above two loss functions for joint optimization.\n$L = \\alpha \\cdot L_{BCE} + \\beta L_{Dice}$ (7)\nwhere \u03b1 and \u03b2 are hyperparameters that determine the weights of the two losses. In our network, \u03b1 is set to 0.75 and \u03b2 to 0.25."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we analyze the differences between our TUT dataset and other public datasets, describe the experimental setup, and compare methods and metrics. We then analyze results and model complexity, perform ablation experiments, and discuss current limitations and considerations."}, {"title": "A. Dataset Description", "content": "The previous works has used dataset scenarios that are relatively simple, such as cracks in cement or bitumen pavements only. These datasets have clear backgrounds and minimal interference, which cannot comprehensively evaluate the"}, {"title": null, "content": "performance of models. To tackle this problem, we created the TUT dataset to enable better segmentation across different scenarios. Detailed information and comparisons with other datasets are list in Table I.\nThe TUT dataset includes a substantial number of crack images captured with mobile phones, along with a smaller collection of images sourced from the internet. To ensure reliability in model training and testing, all images were manually annotated by several researchers to generate binary labels, and the best-annotated images were selected as the final labeled images. Unlike other datasets with clear backgrounds, the TUT dataset images have complex, noisy backgrounds and cracks of intricate shapes. This evaluates the model's capability to segment cracks in challenging conditions.\nAs list in Table I, our collected TUT dataset contains eight image scenarios, far more than the 1-2 scenarios of other public datasets, this allows for a more comprehensive evaluation of the model's performance in detecting cracks under various scenarios. Fig. ?? shows crack images in different scenarios and their binary labeled images. The crack pixel ratio in the TUT dataset is 3.16%, which is moderate. This ensures the model learns useful information without being hindered by too few crack pixels and prevents the model from overly relying on the crack pixel ratio when there are too many crack pixels.\nAs seen in Fig. ??, bitumen, cement, bricks, and plastic runways have extremely complex cracks due to their unique material properties. Particularly in plastic runways, both coarse and fine cracks are present, testing the model's generalization to different types of cracks. In images of tiles, metal materials, wind turbine blades, and underground pipelines, the crack shapes are relatively simple, but the backgrounds are very complex with various external interferences such as low light, irrelevant areas, and surface characteristics. This can easily result in false detections, thereby testing the model's ability to accurately extract crack areas from complex environments."}, {"title": "B. Implementation Details", "content": "1) Experimental Settings: The CrackSCF network is built on the PyTorch v2.1.1 deep learning framework and trained on an Ubuntu 20.04 server, featuring two Intel(R) Xeon(R) Platinum 8336C CPUs and eight Nvidia GeForce 4090 GPUs, each with 24GB VRAM. For training, the AdamW optimizer is utilized with an initial learning rate set at 0.0005, weight decay of 0.0001, batch size of one, and a total of 60 training epochs. The learning rate is reduced to one-tenth after 30 epochs as part of a decay strategy. The best-performing model parameters on the validation set are selected for testing. We compared the CrackSCF network with other SOTA methods on six datasets: CFD [38], GAPS509 [39], DeepCrack [19], Crack500 [20], CrackMap [40], and ours TUT dataset.\n2) Comparison Methods: In evaluating model performance, we compared the CrackSCF network with 12 classic segmentation algorithms and the latest SOTA methods on different datasets. These methods include HED [44], UNet++ [45], Deeplabv3+ [46], AttuNet [47], RCF [48], RIND [49], UCT-Net [6], SFIAN [7], CT-crackseg [8], DTrCNet [9], Crackmer [10] and Simcrack [11]."}, {"title": "3) Evaluation Metric:", "content": "To quantitatively evaluate the per-formance of our proposed crack segmentation model, we used six main metrics: Precision (P), Recall (R), F1 Score (F1 = $\\frac{2PR}{P+R}$), Mean Intersection over Union (mIoU), Optimal Dataset Scale (ODS), and Optimal Image Scale (OIS). Mean Intersection over Union (mIoU) measures the average ratio of the intersection and union of the predicted results and the true results. The calculation formula for mIoU is:\nmIoU = $\\frac{1}{i+1}  \\sum_{a=0}^{i} \\frac{P_{aa}}{\\sum_{b=0}^{i} P_{ab} + \\sum_{b=0}^{i} P_{ba} - P_{aa}} $ (8)\nwhere i is the number of classes, a denotes the true value, b denotes the predicted value, and $p_{ab}$ represents the number of pixels predicted as a for b. Given that our crack segmentation task involves binary classification, we set i = 1.\nOptimal Dataset Scale (ODS) indicates the optimal segmen-tation performance metric obtained when a fixed threshold t"}, {"title": null, "content": "is selected over the entire dataset. It is defined as:\nODS = $max_t  \\frac{2 \\cdot P_t R_t}{P_t + R_t}$ (9)\nOptimal Image Scale (OIS) indicates the optimal segmenta-tion performance metric obtained when an optimal threshold t is selected for each image individually. It is defined as:\nOIS = $\\frac{1}{N} \\sum_{a=1}^{N} max_t \\frac{2 \\cdot P_{t,a} R_{t,a}}{P_{t,a} + R_{t,a}}$ (10)\nwhere N represents the total number of images."}, {"title": "C. Comparison with the SOTA methods", "content": "Our proposed method, CrackSCF, was compared with 12 other methods across five public datasets and our own TUT dataset. Table II displays the comparison results for the five"}, {"title": null, "content": "public datasets, while Fig. 8 illustrates the visual results of all methods on these datasets.\n1) Results on the CFD dataset: The crack areas in the CFD dataset are very slender, testing the model's sensitivity to fine-grained crack pixels. As list in Table II, our proposed CrackSCF attained the best results across all metrics for the CFD dataset. Specifically, on ODS, OIS, P, R, F1, and mIoU, it outperformed the second-best methods by 0.48%, 0.34%, 0.14%, 0.35%, 0.31%, and 0.30%, respectively. Notably, SimCrack [11] achieved the second-best performance on all metrics except for P and R, only behind our method. However, as seen in Fig. 8, its actual segmentation performance is not satisfactory, while our method achieves high-quality segmentation results on various fine-grained crack pixels. Other methods, such as UCTNet and DTrCNet [9], tend to misdetect background noise or miss key detail areas of crack pixels.\n2) Results on the GAPS509 dataset: Unlike the CFD dataset, the GAPS509 dataset contains not only finer cracks but also some irrelevant background interference, testing the model's ability to extract crack areas under high interference conditions. As list in Table II, our method achieved the highest performance on all metrics except R. Although SimCrack [11] achieved 0.6997 on the R metric, leading our method by about 2.72%, its P metric was 0.6171, far lower than our method. The highest F1 score obtained by our method indicates a good balance between P and R. Specifically, our method outperformed the second-best methods on ODS, OIS, P, F1, and mIoU by 1.31%, 1.25%, 2.90%, 1.11%, and 0.45%,"}, {"title": null, "content": "respectively. Additionally, as seen in Fig. 8, our method almost perfectly avoids irrelevant background noise, while most other methods falsely detect irrelevant backgrounds and perform worse in segmenting crack pixel areas compared to our method.\n3) Results on the DeepCrack dataset: The cracks in the DeepCrack dataset are often relatively wide, testing the model's ability to extract a large range of crack pixels. As list in Table II, our method achieved the highest results on all metrics for the DeepCrack dataset. Specifically, our method outperformed the second-best methods on ODS, OIS, P, R, F1, and mIoU by 1.08%, 0.39%, 1.51%, 1.04%, 1.42%, and 0.85%, respectively. Notably, CT-crackseg also achieved good results on this dataset, but as seen in Fig. 8, its segmentation performance on detailed cracks is not as good as our method, indicating that our method can effectively handle crack details.\n4) Results on the Crack500 dataset: Similar to the Deep-Crack dataset, the Crack500 dataset also has relatively wide crack areas, but its structure is more complex, posing higher requirements on model performance. As list in Table II, our method achieved the best results on five metrics for the Crack500 dataset, except for R. It outperformed the second-best methods on ODS, OIS, P, F1, and mIoU by 1.11%, 0.07%, 1.69%, 0.47%, and 0.64%, respectively. Although DTrCNet [9] leads on the R metric, our method surpasses it on the P metric by 10.51%. The highest F1 score achieved by our method indicates a good balance between P and R. Additionally, as seen in Fig. 8, our method does not produce false detections in irrelevant background areas, whereas other methods either"}, {"title": null, "content": "produce false detections or lack detailed segmentation in crack ground noise. In images of metal materials and turbine blades,\nareas.\n5) Results on the CrackMap dataset: Similar to the CFDdataset, the cracks in the CrackMap dataset are also relativelyslender. As list in Table II, our method leads all other methodson all metrics, outperforming the second-best methods onODS, OIS, P, R, F1, and mIoU by 0.48%, 0.34%, 0.14%,0.35%, 0.31%, and 0.30%, respectively. Notably, SimCrack[11] achieved the second-best results on this dataset, but asseen in Fig. 8, SimCrack's segmentation performance on thecrack tips is not satisfactory, whereas our method effectivelyidentifies these pixels.\n6) Results on the TUT dataset: Fig. 9 shows the visualresults of all methods on the TUT dataset. We manuallyhighlighted easily missed crack pixel areas with red boxesand misdetected non-crack pixel areas with yellow boxes.As list in Table II, our method achieved excellent results onthe TUT dataset, leading in all metrics. Specifically, itoutperformed the second-best methods on ODS, OIS, P, R,F1, and mIoU by 2.73%, 2.18%, 0.98%, 0.51%, 2.12%, and1.67%, respectively. SimCrack [11] achieved the second-bestresults on ODS, OIS, F1, and mIoU, but as seen in Fig. 9, itssegmentation performance on some key details has room forimprovement compared to our method.\nFurthermore, most methods fail to suppress irrelevant back-"}, {"title": null, "content": "our method effectively removes noise and accurately segments crack regions. In pipeline scenarios, our method can filter out timestamp watermarks, thanks to SCFM, which enables the model to focus on irregular, slender crack structures rather than irrelevant watermarks. Our method demonstrates excel-lent robustness and versatility, resulting in high-quality crack segmentation maps and adapting well to different scenarios."}, {"title": "D. Complexity Analysis", "content": "To assess our LRDS block's efficacy, we analyzed its com-putational and parameter efficiency using three metrics: float-ing point operations (FLOPs), number of parameters (Params), and frames per second (FPS). FLOPs quantify computational load, reflecting demand during inference. Params measure storage requirements, reflecting trainable and stored parameters. FPS gauges real-time processing, indicating frames processed per second under specific hardware conditions.\nTable III presents our method's performance alongside other comparisons in terms of FLOPs, Params, and FPS. Our approach achieves notably low values in FLOPs (9.26G) and Params (4.79M), surpassing the second-best by 38.01% and 18.81%, respectively. This underscores our algorithm's efficiency, making it suitable for implementation on resource-constrained edge devices. In FPS, our model achieves the"}, {"title": "E. Ablation Analysis", "content": "To evaluate the advantages of our LRDS block compared to other convolution operations, we performed ablation experiments on the TUT dataset, as list in Table IV. On six metrics, our method slightly outperforms those using only"}, {"title": null, "content": "depthwise separable or low-rank approximation convolutions. Our method reduces FLOPs by 61.26% and 56.69%, respectively, and Params by 0.16M compared to the low-rank approximation block, achieving the highest FPS. Compared to original convolution operations, our method reduces FLOPs and Params by 89.70% and 73.19%, respectively, without degrading performance. These results indicate that our convolution block effectively reduces computational load and utilizes parameters while maintaining or improving segmentation performance, enhancing processing speed and benefiting lightweight model design.\nTo demonstrate the effectiveness of SCFM in fusing local detail features and long-range information, we performed ablation experiments on the TUT dataset, as list in Table V. Our method, using Concat and PAF dual-branch feature fusion, achieves the best results across all six segmentation metrics. Using only the PAF branch results in a 1.06% drop in the F1 score. While SCFM slightly increases parameter count, computational load, and inference speed, it significantly improves segmentation performance. These results indicate that our module effectively balances performance and efficiency.\nTable VI shows the results of ablation experiments on different weight proportions of the two sub-losses in the loss function. As shown, a Dice loss to BCE loss ratio of 1:3 yields the highest values in five metrics: ODS, OIS, R, F1, and mIoU. Although the P metric is 0.80% lower than using only BCE loss, the F1 metric improves by 2.58%, indicating a better balance between P and R. Therefore, we use this ratio during training. In summary, a Dice loss to BCE loss ratio of 1:3 not only enhances pixel-level performance but also improves the global segmentation of crack areas, achieving"}, {"title": "F. Limitations and Discussions", "content": "Through these experiments, we found that SCFM effectively perceives and fuses local details and long-range pixel dependencies. Our network excels in various metrics on public datasets with simple scenes and performs exceptionally on the complex TUT dataset. The lightweight LRDS block minimizes parameters and computational load, facilitating deployment on resource-constrained devices like drones and smartphones. However, our network has several limitations, as follows:\n1) Although our method generally achieves good cracksegmentation with effective background noise suppres-sion, as shown in the Fig. 10, our model occasionallymisdetects noise regions that resemble cracks in someimages. It also fails to detect watermark occlusionswith colors similar to the background. Therefore, it isessential to enhance modules such as MFE and LDE toimprove the model's capability to perceive and extractfeatures from critical noise and occlusion areas, therebyfurther increasing its robustness and stability in complexscenarios.\n2) Despite our method achieves the minimum parametercount and computational load, there is still room forimprovement in inference speed. For example, our net-work achieves a processing speed of 36 frames persecond, but the DTrCNet [9], which focuses more oninference speed, attains a faster speed of 47 frames persecond. Therefore, it is necessary to further simplifyconvolution operations or the network's complexity to"}, {"title": "V. CONCLUSION", "content": "This paper proposes a staircase cascaded fusion network (CrackSCF) for pixel-level crack detection. We developed SCFM to fuse the local patterns processed by MFE and the pixel dependencies processed by LDE. Additionally, we introduced the lightweight convolution block LRDS to replace all convolution operations in the network, effectively reducing the substantial parameter count and computational load required for training and inference. Furthermore, to comprehensively evaluate our model's robustness and stability across various complex scenarios, we collected the TUT dataset, which includes eight types of crack scenes. Experimental results show that our method performs best on five public datasets and the TUT dataset, particularly excelling in background noise suppression and fine crack segmentation. The computational load and parameter count are reduced to 9.26G and 4.79M, respectively, with an inference speed of up to 36 frames per second.\nIn future work, we plan to continue designing an improved SCFM to better fuse and enhance local information and global context for superior segmentation results. Additionally, we will further pursue network lightweighting to reduce computational resource requirements and achieve higher inference speeds, aiming to deploy our model on mobile devices for better real-world applications."}]}