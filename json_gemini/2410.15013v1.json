{"title": "DST-TransitNet: A Dynamic Spatio-Temporal Deep Learning Model for Scalable and Efficient Network-Wide Prediction of Station-Level Transit Ridership", "authors": ["Jiahao Wang", "Amer Shalaby"], "abstract": "Accurate prediction of public transit ridership is vital for efficient planning and management of transit in rapidly growing urban areas in Canada. Unexpected increases in passengers can cause overcrowded vehicles, longer boarding times, and service disruptions. Traditional time series models like ARIMA and SARIMA face limitations, particularly in short-term predictions and integration of spatial and temporal features. These models struggle with the dynamic nature of ridership patterns and often ignore spatial correlations between nearby stops.Deep Learning (DL) models present a promising alternative, demonstrating superior performance in short-term prediction tasks by effectively capturing both spatial and temporal features. However, challenges such as dynamic spatial feature extraction, balancing accuracy with computational efficiency, and ensuring scalability remain.\nThis paper introduces DST-TransitNet, a hybrid DL model for system-wide station-level ridership prediction. This proposed model uses graph neural networks (GNN) and recurrent neural networks (RNN) to dynamically integrate the changing temporal and spatial correlations within the stations. The model also employs a precise time series decomposition framework to enhance accuracy and interpretability. Tested on Bogot\u00e1's BRT system data, with three distinct social scenarios, DST-TransitNet outperformed state-of-the-art models in precision, efficiency and robustness. Meanwhile, it maintains stability over long prediction intervals, demonstrating practical applicability.", "sections": [{"title": "I. INTRODUCTION", "content": "Accurate is crucial for efficient planning and management of transit in the context of urban development in Canada. Cities in Canada are experiencing significant population growth, leading to higher demand for public transit services. Unanticipated spikes in transit ridership can result in overcrowded vehicles, longer boarding and alighting times, and operational issues such as bus bunching. These problems can accumulate and propagate through the transit network, causing widespread service disruptions. Accurate prediction of transit ridership enables transit agencies to optimize resource allocation, adjust vehicle deployment, and schedule frequencies to match expected rider levels.\nTraditional time series models, such as ARIMA and SARIMA, are widely used for similar prediction tasks. However, they face challenges and limitations in predicting transit ridership for stops in the transit network. Firstly, traditional methods struggle to provide accurate results for the prediction of short-term transit ridership. Although parametric methods such as ARIMA are suitable for long-term prediction intervals, such as one-day predictions, they falter in short-term prediction tasks due to the rapidly changing dynamics of transit-rider patterns, resulting in lower prediction accuracy. Secondly, traditional methods have difficulty combining spatial and temporal features for predictions. The prediction of transit ridership is often performed for individual stations, neglecting the spatial correlation features. For short-term pre-"}, {"title": "II. Background", "content": "Public transit ridership analysis has evolved significantly, rang-ing from statistical modeling to machine learning and deep learning methods for time-series prediction. This literature review discusses various ridership analysis approaches across multiple dimensions, such as problem definitions, model features, and methodologies.\nStatistical analysis is one of the most common approaches for ridership estimation, including methods like direct rider-ship modeling and travel demand models [3]. Direct rider modeling focuses on assessing how different factors within a buffer range, such as socioeconomic conditions, land use, variables of the built environment, and attributes related to transport, affect transport demand [4], [5]. For example, [6] uses characteristics of land use, such as population and employment density, to estimate the demand for subway stations in the Seoul metropolitan area, further analyzing the impact of these factors in different reference areas. Similarly, [7] explores the relationship between the built environment and the ridership in the bus rapid transit (BRT) systems in seven Latin American cities. In Montreal, infrastructure conditions, along with characteristics of the built environment, are examined to estimate bus ridership at the stop level [8].\nTraditional direct ridership models focus mainly on long-term ridership estimation, ranging from monthly [5] to daily forecasts [9].\nTraditional statistical models are heavily based on selected factors. Physical attributes, such as density, diversity, and design, alongside social variables, are primarily long-term characteristics that are useful for strategic planning or policy making, particularly in forecasting ridership for proposed transit routes. However, these features do not capture real-time impacts on ridership. To address the need for short-term estimations, real-time factors such as weather conditions or events are incorporated into ridership models [10], [11]. For example, [12] examines the impact of planned and unplanned events on transit ridership using a linear regression and random forest-based model."}, {"title": "III. Methodology", "content": "In this section, we provide a detailed introduction to the Deep Learning (DL) structure, Dynamic Spatio-Temporal TransitNet (DST-TransitNet), designed to address this task by integrating advanced neural network components to capture both temporal and spatial dependencies in transit demand data. This model leverages Graph Convolutional Neural Networks (GCNs), Graph Attention Networks (GATs), Gated Recurrent"}, {"title": "A. Basic Building Blocks", "content": "In this section, we introduce the fundamental machine learning units used to build DST-TransitNet, including GRU, GCN, GNN, and GAT. These components are crucial for understanding the structure of DST-TransitNet and how each part functions cohesively within the model."}, {"title": "1) Gated Recurrent Unit (GRU)", "content": "The GRU, proposed by [38], is an improvement over tradi-tional RNNs, designed to mitigate the problem of vanishing/ exploding gradients. The architecture of the GRU is shown in Figure 1, which illustrates how data flows within the unit and how the information is propagated through its structure. As depicted, the hidden state from previous timesteps $h_{t-1}$ and the current input $X_t$ are used to calculate the current hidden state $h_t$. This process captures temporal dependencies across time steps, while the gated structure controls how much information from the previous timestep and current input is retained.\nThe temporal dependencies at timestep t are captured by the GRU as follows:\n$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h_t}$,\nwhere $\\tilde{h_t}$ is the candidate activation:\n$\\tilde{h_t} = tanh(W_h x_t + r_t \\odot (U_h h_{t-1}) + b_h)$,\nand $z_t$ and $r_t$ are the update and reset gates, respectively:\n$z_t = \\sigma(W_z x_t + U_z h_{t-1} + b_z)$,\n$r_t = \\sigma(W_r x_t + U_r h_{t-1} + b_r)$."}, {"title": "2) Graph Convolutional Networks", "content": "Proposed in [39], GCN address the limitations of traditional Convolutional Neural Networks (CNNs) in capturing the message or feature passing between nodes in a graph-based data structure. Unlike CNNs, which only allow messages to be passed in two dimensions (as in grid-like structures such as images), GCNs enable message passing between all adjacent nodes in a graph. As shown in Fig. 2, the feature aggregation for a node is based on messages from all its neighboring nodes. The aggregation is computed using the following equation\n$H^{(l+1)} = \\sigma (\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})$, where\n*   $H^{(l)}$ represents the feature matrix at the l-th layer of the network, where each row corresponds to a node and each column corresponds to a feature.\n*   $W^{(l)}$ is the trainable weight matrix at layer l.\n*   $\\tilde{A} = A + I$ is the adjacency matrix of the graph A with added identity matrix I, representing self-loops I.\n*   $\\tilde{D}$ is the diagonal degree matrix of $\\tilde{A}$, and $\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$ is used for normalize the adjacency matrix.\n*   $\\sigma$ is the activation function.\nThis formulation allows GCNs to capture and aggregate information from neighboring nodes effectively, taking into account the graph structure. The more GCN layer included within the computation, the further infoarmiton can be reached by a given node, with the normalization using $D$ preventing the feature values from growing disproportionately as the layers stack deeper.\nOn the other hands, in [40], the authors provided a improved version of GCN, named k-dimensional Graph Neural Network (k-GNN), where the message aggregation within a layer is calculated as follow:\n$H^{(l+1)}(v) = \\sigma \\left( \\sum_{u \\in N(v)} \\eta^{(l)}(v) \\cdot (H^{(l)}(v) \\oplus H^{(l)}(u)) \\cdot W_E^{(l+1)} + \\sum_{u \\notin N(v)} (eq_{ij})^{(l)} \\cdot H_R^{(l)} \\cdot W_R^{(l+1)} \\right)$;\nwhich, compared to traditional GCN, allows both connected and unconnected nodes to communicate with each other."}, {"title": "3) Graph Attention Network", "content": "GATS, introduced by [41], extend GCNs by incorporating attention mechanisms, allowing dynamic weighting of the neighbors' contributions based on the input. As shown in Fig. 3, GATs calculate dynamic edge weights $W_E$ based on the attention mechanism, which is formulated as:\n$e_{ij}^{(l)} = LeakyReLU (a^{(l)T} [z_i^{(l)} || z_j^{(l)}])$, where:\n*   $z_i^{(l)} = W^{(l)} h_i$ is the transformed feature vector for node i.\n*   $\\parallel$ denotes concatenation.\n*   LeakyReLU is the Leaky Rectified Linear Unit activation function.\nThe edge weights are computed by applying the softmax function to the attention coefficients:\n$\\alpha_{ij}^{(l)} = \\frac{exp(e_{ij}^{(l)})}{\\sum_{k \\in N(i)} exp(e_{ik}^{(l)})}$\nFinally, the new hidden state $h_i^{l+1}$ is calculated as a weighted sum of the neighbors' transformed features:\n$h_i^{(l+1)} = \\sigma \\left( \\sum_{j \\in N(i)} \\alpha_{ij}^{(l)} W^{(l)} h_j \\right)$\nThe LeakyReLU function is defined as:\n$LeakyReLU(x) =\\begin{cases} x, & \\text{if } x > 0, \\\\ \\alpha x, & \\text{if } x \\leq 0, \\end{cases}$ where $\\alpha$ is a small constant (typically $\\alpha = 0.01$).\nGAT enables the model to dynamically focus on the most relevant spatial features for each stop, which is crucial for capturing varying transit ridership across different network areas throughout different time periods."}, {"title": "B. DST-TransitNets", "content": "The structure of the proposed DL model, i.e., DST-TransitNets, is shown in Fig. 4. The model comprises four main parts: the temporal decomposition layer, dynamic spatial weight calculation layer, spatio-temporal aggregation layer, and prediction layer. The proposed model is capable of utilizing information from all stations in the transit network, such as current passenger counts, historical passenger counts, and spatial relations between stations, to make predictions for all stops/stations simultaneously."}, {"title": "1) Temporal Decomposition", "content": "This layer serves as the temporal decomposition component, a structure proven to be effective for time series prediction tasks in many works [42], [43]. The main idea of temporal decomposition is to summarize the input data $X_o$ over a fixed temporal window using average pooling, and then divide the original time series input into Trend series $X_t$ and Residual series $X_r$, calculated as follows:\n$X_t = AvgPool1D(X_o)$,\n$X_r = X_o - X_t$.\nThis layer helps extract key temporal features from the original data, making the data more interpretable and manageable for subsequent layers."}, {"title": "2) Dynamic Spatial Weight Calculation", "content": "This layer employs the GAT to calculate dynamic spatial weighting $W_E$. Although the transit network structure remains stable, the correlation within stops/stations may"}, {"title": "3) Spatio-Temporal Aggregation", "content": "The spatio-temporal aggregation layer combines stacked GRUs for temporal feature aggregation and k-GNNs for spatial feature aggregation. This approach processes current passenger counts $X_c$, historical passenger counts $X_h$, trend series $X_t$, and residual series $X_r$, allowing DST-TransitNet to extract both temporal and spatial dependencies. The detailed architecture of the spatio-temporal aggregation phase in DST-TransitNet is illustrated in Fig. 5.\nEach of the four series candidates-original input, seasonal component, trend component, and correlation-is processed by a GRU to generate hidden states, followed by a graph convolution operation:\n$h_i = GRU(series_agg_i(temp_inputs;))$\n$h_i = k-GNN(h_i, a, W_E)$\n$= W_1 X + W_2 \\sum_{j \\in N(i)} W_{E_{j}}$, where $W_E$ is the edge weights attention information from the dynamic spatial weight calculation layer.\nAfter concatenating the features extracted from the temporal and spatial aggregation layers, the multi-layer FFNN merges the combined features and maps the high-level features to the output predictions.\nThe proposed hybrid model structure combines temporal and spatial data by leveraging the strengths of multiple neural network architectures, such as GRU for temporal analysis and k-GNN and GAT for spatial analysis. The integration of attention mechanisms through GAT enables the model to dynamically focus on the most relevant spatial features. Finally, the multi-layer FFNN ensures the combined features are effectively utilized for prediction.\nTo further improve model efficiency, we also provide a second hybrid model, DST-TransitNetV2. In this version, the GRU layer in the original spatio-temporal aggregation layer is moved to the prediction layer, where it processes the spatially-aggregated features for high-level temporal feature extraction, as shown in Fig. 6."}, {"title": "4) Long-term Prediction Using Short-term Prediction Model", "content": "In this section, we describe the iterative framework used to extend a short-term prediction model for long-term transit ridership forecasting. This approach leverages the pre-trained short-term prediction model to dynamically generate extended predictions while maintaining computational efficiency and adaptability.\nThe Fig. 7 illustrates the prediction process where $T_i$, represents the original input data at time step i, and $P_j$ represents the prediction results for future timestamps j. The process involves the following steps:\n1) Initial Inputs:\n*   The model begins with an initial set of input data points, denoted as $T_1, T_2,...,T_I$.\n2) First Iteration (Iter #1):\n*   The short-term ridership prediction model pro-cesses these initial inputs to generate the first future prediction, $P_{I+1}$.\n3) Subsequent Iterations (Iter #2 and beyond):\n*   In each subsequent iteration, the model updates its input data by incorporating the latest prediction and removing the oldest input. For example, in the sec-ond iteration, the inputs become $T_2, T_3, ..., P_{I+1}$ to predict $P_{I+2}$. This process continues iteratively for N iterations.\n4) Prediction Storage:\n*   The predictions from each iteration are stored sequentially, creating a comprehensive long-term prediction sequence from $P_{I+1}$ to $P_{I+N}$.\nBy continuously incorporating new predictions into the input set, the framework enables the model to generate long-term predictions efficiently. This iterative approach reuses the model's structure and parameters in each iteration, thereby avoiding extensive retraining and minimizing the computational resources required for long-term predictions. Such efficiency and scalability are particularly beneficial for public transit agencies with limited computational capacities. However, this framework imposes strict requirements on the model's robustness and accuracy, as long-term predictions in-creasingly depend on the model's own predictions rather than actual observed data. The iterative nature of the framework can lead to error propagation, where inaccuracies in early predictions compound over subsequent iterations, potentially degrading the accuracy of long-term forecasts."}, {"title": "C. Pipeline for Real-World Deployment", "content": "As shown in Fig. 8, the implementation process of DST-TransitNet can be divided into three main phases: Data Preparation, Model Preparation, and Application.\nData Preparation Phase:\n*   Data Collection: Gather recent and historical ridership records, along with station geographic data, including station locations (latitude, longitude) and route informa-tion. The ridership dataset is dynamically updated with new records, whereas the station geographic dataset is relatively static and updated as needed.\n*   Data Cleaning and Aggregation: Clean and aggregate the collected data to prepare it for training and testing, including removing duplicates and aggregating data within the same time slots. Create an adjacency matrix using the station geographic dataset to represent the network structure.\nModel Preparation Phase:\n*   Training Data and Testing Data: Split the cleaned data into training and testing datasets derived from the historical ridership records.\n*   Model Training: Train the initial DST-TransitNet model using the training data. The model learns patterns from the historical data.\n*   Model Validation: Test the trained model on the testing data to validate its performance.\nApplication Phase:\n*   Input Data Requirements: The deployed DST-TransitNet model requires recent ridership input col-lected from times $t-n$ to $t$ and historical ridership input collected from the previous day, covering times $t-n+1$ to $t + 1$. This input data is used to predict future ridership at times $t + 1$ for all stations across the system, where n is the input length for the model.\n*   Long Term Prediction Framework: Within the long-term prediction framework, the model updates its recent ridership using its prediction results for the next time"}, {"title": "interval. Historical ridership input from the previousday, aligned with the prediction target time interval(e.g., t + 2, t + 3), is already in the dataset. Thus, forlong-term predictions, DST-TransitNet requires only asingle input from the ridership dataset to make multipletime predictions.\n*   Continuous Learning or Periodic Fine-Tuning: Pe-riodically update (fine-tune) the model with new datato adapt to changing patterns and maintain accuracy.Fine-tuning reduces training costs and keeps the modelaware of recent system changes.", "content": "This structured pipeline facilitates the effective imple-mentation of DST-TransitNet, ensuring accurate and relia-ble ridership predictions for transit agencies."}, {"title": "IV. Experiments and Result Analysis", "content": "In this section, we detail the experiments conducted to evaluate the performance of the proposed DST-TransitNet model using real-world passenger count data from the BRT system of Bogota, Colombia. We outline the datasets used, describe the experimental setup, and present the evaluation metrics employed. Following this, we provide a comprehensive analysis of the results, highlighting the model's efficiency in predicting short-term station transit ridership across the network."}, {"title": "A. Datasets", "content": "The dataset used for the experiments consists of passenger count records, specifically the number of boardings, collected from the TransMilenio Bus Rapid Transit (BRT) system in Bogot\u00e1, Colombia, as shown in Fig. 9. Fig. 9a presents the official BRT system map, and Fig. 9b provides a geospatial"}, {"title": "B. Evaluation Metrics", "content": "To evaluate the model's performance, we utilize two key evaluation metrics: the R-squared value $(R^2)$ and the Mean Arctangent Absolute Percentage Error (MAAPE) [46].\nThe $R^2$ value, calculated as follows:\n$R^2 = 1 - \\frac{\\sum_{i=1}^{n}(Y_i - \\hat{Y_i})^2}{\\sum_{i=1}^{n}(Y_i - \\overline{Y})^2}$, (1)\nis a well-known statistical measure that indicates the propor-tion of variance in the dependent variable that is predictable from the independent variables. It provides a clear indication of how well the model's predictions align with the actual observed values, with a value closer to 1 signifying a better fit.\nOn the other hand, MAAPE, calculated as follows:\n$MAAPE = \\frac{1}{n} \\sum_{i=1}^{n} arctan \\left| \\frac{Y_i - \\hat{Y_i}}{Y_i} \\right|$ (2),\nis a relatively new metric introduced to address some limitations of traditional error metrics like Mean Absolute Percentage Error (MAPE). Unlike MAPE, which can be disproportionately affected by small denominators leading to large error values, MAAPE provides a more balanced assessment by incorporating the arctangent function. This enhances the robustness of the error measure, making it useful for evaluating models in the presence of noisy data.\nUsing both $R^2$ and MAAPE, we ensure a comprehensive evaluation of the performance of the model, capturing both the goodness of fit and the robustness against outliers and variability in the data. This dual-metric approach allows for a more nuanced understanding of the model's predictive capabilities and reliability across different scenarios."}, {"title": "C. Results and Analysis", "content": "In this section, we present a comprehensive analysis of the performance of the proposed DST-TransitNet models, along with the several aforementioned benchmark models, across different dataset periods for short- and long-term prediction tasks."}, {"title": "1) Model Performance Overview", "content": "Fig. 12 and Table 1 illustrate the $R^2$ score and MAAPE distribution for each model across the training dataset, normal period, COVID-19 period, and protest period. The results demonstrate that the two proposed models consistently achieve the best performance across all testing periods, indi-cating their superior predictive capabilities. This highlights the strong ability of the proposed models to capture underlying patterns and generalize well to unseen stable data.\nAs shown in Fig. 13, the prediction results from the Transformer model and our proposed DST-TransitNet model on the scaled ridership for station 2000 are compared with the target values over three selected consecutive weeks during the Covid period and Protest period. During both periods, both models perform well in the middle of the day."}, {"title": "However, DST-TransitNet demonstrates superior performance during peak hours, showing less deviation from the target values. Furthermore, when the system encounters unforeseen disruptions, such as the transit workers' protest starting inweek 3 of the Protest period, DST-TransitNet adapts morequickly and smoothly to the abnormal situation, providingmore accurate predictions. Notably, at the start of week 3", "content": "Notably, the performance all models on the normal periodtesting dataset is slightly better than on the training dataset,which means all models are well trained and do not exhibitan overfitting problem. However, during periods of externaldisruptions, such as the COVID-19 pandemic and nationalprotests, the performance of all models declines, reflecting theincreased difficulty of accurate prediction in highly volatile en-vironments. Notably, DST-TransitNet and DST-TransitNetV2maintain the best performance compared with other modelsshowcasing the resilience and adaptability of the structure,which is important for real-world applications, where modelsmust handle unforeseen events. The performance of themodels underscores the advantages of spatio-temporal modelsin maintaining predictive accuracy across diverse conditions."}, {"title": "Furthermore, when comparing the performance of FFNand DLinear, we observe that DLinear not only exhibitsoverall better performance across all prediction scenariosbut also shows a smaller performance drop when externaldisturbances are introduced to the dataset. This suggeststhat incorporating the time-series decomposition frameworkin model construction not only enhances accuracy andinterpretability but also improves the model's stability inchanging prediction environments.", "content": "The Network-wide Station-level MAAPE loss and $R^2$distribution (Fig. 14) indicate that the new models areconsistently more accurate and robust across all periods,with lower MAAPE values/higher $R^2$ score and narrowerinterquartile ranges (IQRs). This suggests that the proposedmodels are better at handling both stable and rapidly changingconditions for all stations in the network.\nMeanwhile, we also notice that the performance gapbetween peak and non-peak hours becomes smaller as theprediction task complexity increases. To further elucidatethe model's performance variation during non-peak and peakhours, we present Fig 16. This figure clearly illustrates theMAAPE loss for the FFNN model across different times ofthe day. Notably, the MAAPE loss increases as the predictiontarget approaches both the onset and the conclusion of peakhours. During all periods, higher MAAPE losses are observedat the beginning of the morning peak and the end of theevening peak. The MAAPE continues to rise until the endof the day, contributing to higher non-peak hour values.During the COVID-19 and protest periods, higher non-peakhour MAAPE losses occur closer to the morning peak duringthe COVID-19 period. In contrast, during the protest period,elevated MAAPE losses begin in the morning and persistthroughout the day. These differences can be attributed tothe time-series changes unique to each period.\nFor the COVID-19 period, as shown in Fig. 11, the overallridership drops, resulting in smoother transitions betweenpeak and non-peak hours. Consequently, the farther theprediction is from the morning peak, the lesser the impact,leading to relatively lower MAAPE losses. However, duringthe protest period, illustrated in Fig. 17, irregular ridershippatterns due to system shutdowns and irregular runningtimes cause more variability. In this scenario, the distinctionbetween peak and non-peak hours has a lesser impact onridership changes compared to the system's operationalirregularities. As the day progresses, these irregularitiesaccumulate in the model's inputs, leading to increasingMAAPE losses.\nMoving to the long-term prediction performance using themethods introduced earlier, Table 2 shows the increasingratio of the MAAPE loss for each prediction model when"}, {"title": "due to the higher variability in transit ridership during peakhours, caused by increased passenger volume and congestion,which introduces more noise into the dataset. Despite themore challenging prediction environment, DST-TransitNetmodels markedly outperform other models during both peakand non-peak hours.", "content": "Furthermore, when comparing the performance of FFNand DLinear, we observe that DLinear not only exhibitsoverall better performance across all prediction scenariosbut also shows a smaller performance drop when externaldisturbances are introduced to the dataset. This suggeststhat incorporating the time-series decomposition frameworkin model construction not only enhances accuracy andinterpretability but also improves the model's stability inchanging prediction environments."}, {"title": "Lastly, we highlight the combination of both temporal andspatial analyses. This dual approach enables us to pinpointspecific temporal-spatial interactions that may affect modelpredictions. We used the KMeans clustering method to groupall stations in the dataset into five groups based on theaggregated mean ridership on a weekly basis for each station.", "content": "After mapping the station groups onto the geographicalmap, as shown in Fig. 22, we observed that even throughthe clustering algorithm only considered historical ridershipinformation, the stations within the same group tend to sharesimilar spatial features. These groups align with the jobdensity distribution in the city. For instance, stations in Group0 predominantly appear in the city center/business areas, whilestations in Group 4 are mostly located in low job density areas.Additionally, Group 1 and Group 2 stations are primarilysituated on the edges of different land use areas.\nWe learn from the analysis results that spatial and temporalpatterns can effectively complement each other, contributingto the final prediction result. Without clear delineation, thesetwo features build upon each other and influence the model.However, a preliminary dataset clustering before training,especially for system-level training, could be beneficial toavoid interference from different feature groups."}, {"title": "V. Conclusion", "content": "In this work, we proposed an innovative Hybrid Deep Learn-ing (DL) model that effectively combines dynamic spatial and temporal feature extraction into a unified framework, capable of performing network-wide station-level ridership prediction. The model outperformed other Machine Learning (ML) and DL models across three distinct prediction scenar-ios, each with varying features. Our model demonstrated excellent accuracy and robustness, maintaining relatively steady performance throughout different times of the day and across different prediction scenarios. Additionally, it showed consistent performance in long-term predictions using a simple iterative prediction framework, which is both highly efficient and applicable.\nFurthermore, we conducted a detailed analysis of results from both temporal and spatial perspectives to provide a reference for future prediction system design. This comprehensive analysis helps to better understand the strengths and potential improvements of the proposed model.\nFor future work, we aim to apply the model to solve real-world tasks, such as abnormal situation prediction or system monitoring, leveraging its stable performance in long-term predictions with fine-grained resolution. We also plan to explore better solutions for long-term predictions, minimizing performance deterioration over extended prediction intervals."}]}