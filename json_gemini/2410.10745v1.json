{"title": "FLEXGEN: FLEXIBLE MULTI-VIEW GENERATION FROM TEXT AND IMAGE INPUTS", "authors": ["Xinli Xu", "Wenhang Ge", "Jiantao Lin", "JiaweiFeng", "Lie Xu", "HanFeng Zhao", "Shunsi Zhang", "Ying-Cong Chen", "HKUST(GZ)", "HKUST", "Quwan"], "abstract": "In this work, we introduce FlexGen, a flexible framework designed to generate\ncontrollable and consistent multi-view images, conditioned on a single-view im-\nage, or a text prompt, or both. FlexGen tackles the challenges of controllable\nmulti-view synthesis through additional conditioning on 3D-aware text annota-\ntions. We utilize the strong reasoning capabilities of GPT-4V to generate 3D-\naware text annotations. By analyzing four orthogonal views of an object arranged\nas tiled multi-view images, GPT-4V can produce text annotations that include 3D-\naware information with spatial relationship. By integrating the control signal with\nproposed adaptive dual-control module, our model can generate multi-view im-\nages that correspond to the specified text. FlexGen supports multiple controllable\ncapabilities, allowing users to modify text prompts to generate reasonable and\ncorresponding unseen parts. Additionally, users can influence attributes such as\nappearance and material properties, including metallic and roughness. Extensive\nexperiments demonstrate that our approach offers enhanced multiple controllabil-\nity, marking a significant advancement over existing multi-view diffusion mod-\nels. This work has substantial implications for fields requiring rapid and flexible\n3D content creation, including game development, animation, and virtual reality.\nProject page: https://xxu068.github.io/flexgen.github.io/.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent progress in generative models (Song et al., 2020b; Ho et al., 2020) has significantly ad-\nvanced 2D content creation, thanks to the rapid increase in 2D data volumes. However, 3D content\ncreation remains challenging due to the limited accessibility of 3D assets, which are essential for\ndiverse downstream applications, including game modeling (Gregory, 2018; Lewis & Jacobson,\n2002), computer animation (Parent, 2012; Lasseter, 1987), and virtual reality (Schuemie et al.,\n2001). Previous 3D generation methods primarily concentrate on optimization-based techniques us-\ning multi-view posed images (Wang et al., 2021; Yariv et al., 2021; Ge et al., 2023; Verbin et al.,\n2022), or employ SDS-based distillation approaches derived from 2D generative models (Lin et al.,\n2023; Poole et al., 2022; Liang et al., 2023). Although effective, these methods often demand sig-\nnificant optimization time, limiting their practicality in real-world applications.\nMulti-view diffusion models (Liu et al., 2023; Long et al., 2024; Shi et al., 2023a;b; Wang & Shi,\n2023) have demonstrated the potential of pre-trained 2D generative models for 3D content creation\nthrough the synthesis of consistent multi-view images. Despite their promising performance, the\ncontrollable generation of multi-view images remains under-explored. Most existing multi-view\ndiffusion models typically rely on a single-view image, which lacks 3D-aware controllable guidance\nand proves insufficient for robust multi-view image generation. For example, the generation of\nunseen regions continues to pose a significant challenge, often simply replicating information from\nthe input view to unseen areas.\nSeveral studies focus on conditional 3D generation. For instance, Coin3D (Dong et al., 2024) utilizes\nbasic shapes as 3D-aware guidance, whereas Clay (Zhang et al., 2024) leverages sparse point clouds\nand 3D bounding boxes. However, these guidance methods are not user-friendly. Other research (Xu\net al., 2024b;a) employs sparse multi-view images as 3D-aware guidance for 3D generation. These"}, {"title": "2 RELATED WORK", "content": "Recent research has extensively explored the generation of multi-view images using diffusion mod-\nels to achieve efficient and 3D-consistent results. These efforts include both text-based methods,"}, {"title": "2.1 DIFFUSION MODELS FOR MULTI-VIEW SYNTHESIS", "content": "such as MVDiffusion (Deng et al., 2023), MVDream (Shi et al., 2023b) and ImageDream (Wang &\nShi, 2023), and image-based methods like SyncDreamer (Liu et al., 2023), Wonder3D (Long et al.,\n2024) and Zero123++ (Shi et al., 2023a). MVDiffusion, for instance, leverages text conditioning\nto simultaneously generate all images with a global transformer, facilitating cross-view interactions.\nSimilarly, MVDream incorporates a self-attention layer to capture cross-view dependencies, en-\nsuring consistency across different views. For image-based approaches, SyncDreamer constructs\na volume feature from the multi-view latent representation to produce consistent multi-view color\nimages. Wonder3D enhances the quality of 3D results by explicitly encoding geometric information\nand employing cross-domain diffusion. Several methods (Tang et al., 2024; Zhang et al.; Zheng\net al., 2024; Xu et al.) adopt these approaches to first generate multi-view images of an object and\nthen reconstruct the 3D shape from these views using sparse reconstruction techniques. Despite\nachieving reasonable results, these methods still face limitations in controllable generation."}, {"title": "2.2 CONTROLLABLE GENERATIVE MODELS", "content": "In recent years, adding conditional control to generative models has garnered increasing attention\nfor enabling controllable generation. These efforts span both 2D and 3D domains. For instance,\nseveral 2D methods (Hess, 2013; Brooks et al., 2023; Gafni et al., 2022; Kim et al., 2022; Par-\nmar et al., 2023) focus on text-guided control by adjusting prompts or manipulating CLIP features.\nAdditionally, ControlNet (Zhang et al., 2023) allows for a series of 2D image hints for control\ntasks through a parallel model architecture. However, similar controllable capabilities in 3D gen-\neration (Bhat et al., 2024; Cohen-Bar et al., 2023; Pandey et al., 2024) remain largely inapplicable.\nCoin3D (Dong et al., 2024) introduces a framework for generating 3D assets guided by basic shapes\nand text. Nonetheless, basic 3D shapes are not user-friendly for general users, whereas text serves\nas a more intuitive and accessible conditional input. In this work, we integrate text prompts into\nthe multi-view diffusion model for controllable multi-view image generation, enabling the model to\ngenerate controllable unseen regions when a single-view image is simultaneously provided."}, {"title": "3 METHOD", "content": "FlexGen is a flexible multi-view generation framework that supports conditioning based on text,\nsingle-view images, or a combination of both. By incorporating 3D-aware text annotations derived\nfrom GPT-4V, our method effectively achieve controllable multi-view images generation, including\nreasonable unseen part generation, texture controllable generation and materials editing. We begin\nwith a succinct problem formulation in Section 3.1. Then, we introduce how to annotate 3D-aware\ncaption in Section 3.2. After that, adaptive dual-control module is introduced to add textual condition\ninto the framework in Section 3.3. Finally, we introduce training and inference in Sectin 3.4. An\noverview framework of FlexGen is shown in Figure 3."}, {"title": "3.1 PROBLEM FORMULATION", "content": "Given a single-view image I or a user-defined prompt T that describes an object or both, our goal\nis to develop a generative model G that produces a tiled image Iout. This image consists of a\n2 \u00d7 2 layout, representing 4 views of an object - the front, left, back, and right - with each view\nat a resolution of 512 \u00d7 512. The multi-view images are aligned with both the single-view image\nand the prompt, ensuring consistency among them, as illustrated in Figure 1. Inspired by (Li\net al., 2023b), our generative model G is based on a large pre-trained text-to-image diffusion model.\nThe design of the 2 \u00d7 2 image grid aligns better with the original data format used by the 2D\ndiffusion model, facilitating the utilization of more prior knowledge. Regardless of the focal length\nand pose of the input image I, we consistently generate orthographic images with a fixed focal\nlength and an elevation angle of 5\u00b0. For example, when processing an input image captured at an\nelevation angle \u03b1 and azimuth angle \u03b2, FlexGen generates multi-view images at azimuth angles\n{\u03b2, \u03b2 + 90\u00b0, \u03b2 \u2013 90\u00b0, \u03b2 + 180\u00b0}, all with a fixed elevation of 5\u00b0."}, {"title": "3.2 3D-AWARE CAPTION \u0391\u039d\u039d\u039fTATION", "content": "Cap3D (Luo et al., 2023) utilized BLIP (Li et al., 2023c) to annotate each single rendered view\nand then leverage GPT4 for holistic description. However, such method leads to text annotation"}, {"title": "3.3 ADAPTIVE DUAL-CONTROL MODULE", "content": "Previous approaches, such as those by Li et al. (2023a) and Shi et al. (2023a), typically focus on\nsingle-modality inputs, conditioning solely on either a text prompt or a single-view image, without\nenabling joint control over both for generating multi-view images. To overcome this limitation, we\npropose an adaptive dual-control module that allows for simultaneous conditioning on both image\nand text inputs, enabling more precise and flexible multi-view image generation.\nOur method builds upon the reference attention mechanism (Zhang, 2023), which we extend to\nintegrate both the reference image and text prompt effectively. This enhanced integration facilitates\nrobust interaction between the two modalities, enabling our model to generate multi-view images\nthat maintain (1) high fidelity to the input image and (2) coherence with both the global and fine-\ngrained descriptions specified in the text."}, {"title": "3.4 TRAINING AND INFERENCE", "content": "Building on the adaptive dual-control module, our framework accommodates both prompt and im-\nage conditions to guide the generation of multi-view images. To increase flexibility, we introduced\na condition switcher during training that supports both single-mode and dual-mode conditions, al-\nlowing for seamless transitions between different input scenarios. With a configurable probability,\ninputs can be left empty: when the text prompt is absent, it defaults to an empty string to ensure\nuninterrupted processing by the model. Similarly, in the absence of an image input, we substitute it\nwith a black image.\nDuring inference, this design facilitates flexible and controllable multi-view generation. When both\nmodalities are available, the image and prompt collaborate to provide complementary information,\nenriching the generated output. If only the image is supplied, the model functions in an image to\nmulti-view mode, generating multiple views based solely on the visual input. Conversely, when only\nthe text is available, the model operates as a text to multi-view generator, producing views that align\nwith the textual description."}, {"title": "4 EXPERIMENTS", "content": null}, {"title": "4.1 EVALUATION SETTINGS", "content": "Training Datasets. Given the inconsistent quality of the original Objaverse dataset (Deitke et al.,\n2023), we initially excluded objects lacking texture maps and those with low polygon counts. We\nsubsequently curated a collection of 147k high-quality objects to form the final training set. For\nrendering these objects, we employed Blender, setting the camera distance at 4.5 units and the\nfield of view (FOV) to 30 degrees. We generated 24 ground-truth images for the target view set,\nmaintaining a fixed elevation of 5 degrees while uniformly distributing the azimuth angles across\nthe range [0, 360]. The input views were randomly sampled with elevation angles between -30\nand 30 degrees and azimuth angles evenly distributed across [0, 360]. All images were rendered"}, {"title": "4.2 COMPARISON WITH STATE-OF-THE-ART METHODS", "content": "Novel view synthesis and sparse-view 3D reconstruction. First, we quantitatively compare our\nmethod with other single-image to multi-view approaches, including Zero123++ (Shi et al., 2023a),\nEra3D (Li et al., 2024), and SyncDreamer (Liu et al., 2023), as shown in Table 1. Our method\noutperforms the others across several key metrics, such as PSNR and LPIPS, demonstrating that\nthe joint control of text prompts and images allows for more consistent and realistic multi-view\ngeneration, particularly in unseen areas. Qualitative results are presented in Figure 5. To further\nverify the consistency of multi-view generation in three dimensions, we reconstructed the multi-view\nimages generated by all methods using the open-source reconstruction method InstantMesh (Xu\net al., 2024b) for a fair comparison. We report Chamfer Distance (CD) and FS metrics in Table 1,"}, {"title": "4.3 ABALTION STAUDY", "content": "We conducted an ablation study to evaluate the impact of key components in our approach, specifi-\ncally focusing on the 3D-aware caption annotation and the Adaptive dual-control module.\nAdaptive Dual-Control Module. The adaptive dual-control module represents a key innovation\nwithin our framework, enabling simultaneous control over both image and text inputs. Unlike pre-\nvious models that typically focus on a single modality, our approach allows for enhanced flexibility\nand precision in generating outputs. This dual-input capability empowers users to guide the gen-\neration process using both image and textual prompts, significantly enhancing the model's ability\nto produce coherent and contextually appropriate multi-view images, as demonstrated in Figure 8.\nThis integration of modalities marks a substantial advancement over existing controllable generative\nmodels, offering a more intuitive and effective means for users to influence the output. Additionally,\nwe illustrate the module's capability to edit material properties in the generated multi-view images,\nas shown in Figure 9."}, {"title": "5 CONCLUSION", "content": "In this work, we present FlexGen, a multi-view diffusion model designed to generate consistent\nand controllable multi-view images guided by a single image, text, or a combination of both. To\nachieve this, we harness the powerful recognition capabilities of GPT-4V to perform 3D-aware text\nannotations by reasoning over orthogonal views of an object, arranged as tiled multi-view images.\nAdditionally, we introduce an adaptive dual-control module that enables text-based conditioning\nto be incorporated directly into the generation phase. By embedding spatial relationships, texture,\nand material descriptions into the text annotations, we achieve enhanced control over the output.\nExtensive experiments demonstrate the effectiveness of our proposed approach."}, {"title": "Limitations and future works.", "content": "Although FlexGen introduces the innovative capability to jointly\ncontrol both image and text inputs, our method occasionally encounters difficulties with complex\nuser-defined instructions. This limitation likely stems from the constraints imposed by the availabil-\nity of high-quality datasets. In the future, we plan to expand the dataset size and enhance the control\ncapabilities to more effectively accommodate intricate instructions."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 DETAIL OF MATERIAL RENDERING", "content": "We integrate material descriptions, including attributes such as metallicity and roughness, into the\ntext annotations to enable material-controllable generation. These descriptions are designed to cor-\nrespond with the materials utilized during rendering in Blender. With Blender, we can freely adjust\nthe values of metallicity and roughness, allowing us to render corresponding images, as show in Fig-\nure 11. The values for metallicity and roughness range from 0 to 1. Specifically, when the values are\nbelow 0.3, the corresponding prompt is \"low\" and when they exceed 0.6, the corresponding prompt\nis \"high\"."}, {"title": "A.2 MORE VISUALIZATION RESULTS", "content": "We show more visualization of our model in Figure 12."}, {"title": "A.3 FAILURE CASES", "content": "Figure 13 illustrates two failure cases, where the input text is unable to serve as an editor for multiple\nviews. This limitation arises due to the absence of relevant text descriptions in the training data."}]}