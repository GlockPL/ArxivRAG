{"title": "cc-DRL: a Convex Combined Deep Reinforcement Learning Flight Control Design for a Morphing Quadrotor", "authors": ["Tao Yang", "Huai-Ning Wu", "Jun-Wei Wang"], "abstract": "In comparison to common quadrotors, the shape change of morphing quadrotors endows it with a more better flight performance but also results in more complex flight dynamics. Generally, it is extremely difficult or even impossible for morphing quadrotors to establish an accurate mathematical model describing their complex flight dynamics. To figure out the issue of flight control design for morphing quadrotors, this paper resorts to a combination of model-free control techniques (e.g., deep reinforcement learning, DRL) and convex combination (CC) technique, and proposes a convex-combined-DRL (cc-DRL) flight control algorithm for position and attitude of a class of morphing quadrotors, where the shape change is realized by the length variation of four arm rods. In the proposed cc-DRL flight control algorithm, proximal policy optimization algorithm that is a model-free DRL algorithm is utilized to off-line train the corresponding optimal flight control laws for some selected representative arm length modes and hereby a cc-DRL flight control scheme is constructed by the convex combination technique. Finally, simulation results are presented to show the effectiveness and merit of the proposed flight control algorithm.", "sections": [{"title": "I. INTRODUCTION", "content": "As a class of well-mature platforms, quadrotor unmanned aerial vehicles (UAVs) provide mobilities in cluttered or dangerous environments where the human being is at risk and are helpful for many civilian and military applications such as surveillance of forest fire detection, high building inspection, battlefield monitor, and battlefield weapon delivery, etc. Over the past few decades, the robotics community has experienced a very active and prolific topic in quadrotors and breakthroughs have been made for the issues of control algorithms, architectural design and applications [1]\u2013[3]. In above issues, flight control algorithms implicitly determine the performance of the quadrotors. Hence, the issue of flight control scheme design for quadrotors is very significant. This issue is extremely difficult since a fact that quadrotors present highly nonlinear and coupled dynamics that can be stabilized using four control inputs. This fact has also promoted the attention of many control practitioners and theoretical specifics [2]\u2013[4].\nAfter years of developments, common quadrotors have been commercialized and their technologies have become more and more mature. Yet quadrotors must sometime fly through narrow gaps in disaster scenes in geographical investigations and even on battlefields. Hence it is very useful for quadrotors that can change their shapes. At the same time, the shape change endows quadrotors with stronger environmental adaptability and more complex task completion [5]. Three types of morphing quadrotors have been reported in the existing works: tiltrotor quadrotor, multimodal quadrotor, and foldable quadrotor [6]. For the tiltrotor quadrotor [7], the input dimension of the control forces is extended to enhance its maneuverability by changing the direction of the rotor axis. The rotor lift force direction is thereby changed for quadrotors and additional design of the tilt controller is thus required. Both a MIMO PID flight controller [8] and an ADRC (active disturbance rejection control) flight controller [9] are reported for a tiltrotor quadrotor with a better robustness performance. For the multimodal quadrotor [10], the quadrotor can perform different tasks by presetting several variation modes, and switching among them during flight to meet the multitasking requirements. To this end, for each variation mode, a corresponding control law is predesigned [11], [12]. For the foldable quadrotor [13], the quadrotor modifies its size by actively changing the mechanical structure to enhance its passability (e.g., passing narrow channels). To ensure the flight safety of the foldable quadrotor, the change of mechanical structure is considered as a model perturbation and then a robust control law is designed [14]\u2013[16]. Despite the above progresses, the aforementioned flight control algorithms are developed by the matured model-based control theory and thus lack of learning ability.\nWith the rapid development of artificial intelligence (AI), deep reinforcement learning (DRL) combines the represen- tation ability of deep learning (DL) and the decision abil- ity of reinforcement learning (RL) [17], [18], which has a strong exploratory ability to solve complex dynamic planning problems, and its performance in solving optimal control problems is becoming more and more significant [19]. In the last ten years, RL/DRL has been successfully used to solve the optimal control problem of quadrotor dynamics [20]\u2013[27], where the strong learning and exploration ability of DRL solves the challenges posed by the strong nonlinearity in introduction of PPO algorithm. That is to say, different from the existing works [8]\u2013[16], this study develops a pure data- driven flight control algorithm for the arm-rod-length-varying quadrotor without any model knowledge of flight dynamics. On the other hand, the morphing quadrotor addressed in this study is completely different from the common one discussed in [22]\u2013[27]. Furthermore, the shape change of the morphing quadrotor introduces more complex flight dynamics in com- parison to the common one.\nThe remainder of this paper is organized as follows. Section II introduces some background of morphing quadrotor dy- namics, control objective, and PPO algorithm. In Section III, a PPO-based off-line optimal flight control design is introduced for some selected representative arm length modes. Then, a cc-DRL flight control scheme is presented in Section IV by the off-line trained optimal flight control laws and the CC technique. Performance evaluation results are presented in Section V to support the proposed cc-DRL flight control algorithm, and conclusions follow in Section VI."}, {"title": "II. PRELIMINARIES AND PROBLEM FORMULATION", "content": "A. Morphing quadrotor dynamics\nA morphing quadrotor addressed in this paper has four variable-length arm rods and its sketch map is shown in Fig. 2. In the addressed morphing quadrotor, each arm rod can independently change its length in response to the change of flight environment and missions. Hence, four variable- length arm rods endow the morphing quadrotor with a better adaptability of flight environments and unplanned multipoint missions. But the independent length change of four arm rods changes the mass distribution of the morphing quadrotor and disrupts the symmetric structure of the conventional quadrotor. Flight dynamics of the morphing quadrotor are more complex than the one of the common quadrotor. Essentially, morphing quadrotors are a class of reconfigurable systems.\nTo capture such complex flight dynamics, two frames are introduced: a world internal frame $F_w$: {$O_w$,$X_w$,$Y_w$,$Z_w$} and a moving frame $F_B$ : {$o$, $x$, $y$, $z$} attached to the quadrotor body at its mass center (see Fig. 2). The rotational matrix between the moving frame $F_B$ and the world internal one $F_w$ is chosen as follows\n$\\mathbb{R}_{WB} = \\begin{bmatrix}c_\\psi c_\\theta & c_\\psi s_\\theta s_\\phi-s_\\psi c_\\phi & c_\\psi s_\\theta c_\\phi+s_\\psi s_\\phi\\\\s_\\psi c_\\theta & s_\\psi s_\\theta s_\\phi + c_\\psi c_\\phi & s_\\psi s_\\theta c_\\phi - c_\\psi s_\\phi\\\\ -s_\\theta & c_\\theta s_\\phi & c_\\theta c_\\phi\\end{bmatrix}$ (1)\nwhere $s(.) = sin(.)$ and $c(.) = cos(.)$ are the respective sine and cosine, and $\\phi$, $\\theta$, and $\\psi$ are the quadrotor\u2019s attitude angles. In the morphing quadrotor, four rotors are respectively fixed at the end of four arm rods. Angular velocities of these four rotors are denoted by $n_i$, $i \\in \\{1,2,3,4\\}$ and chosen as manipulated control inputs, i.e., $u \\triangleq [n_1 \\; n_2 \\; n_3 \\; n_4]^T$. Both mass center position vector $x \\triangleq [x \\; y \\; z]^T \\in \\mathbb{R}^3$ and attitude angle vector $\\omega \\triangleq [\\phi \\; \\theta \\; \\psi]^T \\in \\mathbb{R}^3$ are chosen as state variables of the morphing quadrotor. The evolution dynamics of these state variables is governed by the following nonlinear system model\n$\\begin{bmatrix} \\dot{x} \\\\ \\ddot{x} \\\\ \\dot{\\omega} \\\\ \\ddot{\\omega} \\end{bmatrix} = \\begin{bmatrix} f_1(.) \\\\ f_2(.) \\\\ f_3(.) \\\\ f_4(.) \\\\ f_5(.) \\\\ f_6(.) \\end{bmatrix}$ (2)\nwhere $f_i(\\cdot)$, $i \\in \\{1,2,\\ldots,6\\}$ are functions of the parameters $x$, $\\dot{x}$, $\\omega$, $\\dot{\\omega}$, $u$, $m$, $I_x(t)$, $I_y(t)$, $I_z(t)$, $l_1(t)$, $l_2(t)$, $l_3(t)$, and $l_4(t)$, in which $m$ is the quadrotor mass, $I_x(t)$, $I_y(t)$, and $I_z(t)$ are inertia moments of the quadrotor, and the time-varying parameters $l_j(t)$, $j\\in \\{1,2,3,4\\}$ are used to describe the dynamic changes in the length of four arm rods."}, {"title": "B. Control objective", "content": "Let $x_r$ be a preset flight path of the morphing quadrotor. The corresponding position tracking error vector $\\tilde{x}$ is defined by $\\tilde{x} \\triangleq x_r - x$. To fully describe the quadrotor\u2019s dynamics, a new 12 dimensional state vector $s$ is introduced and defined as\n$s \\triangleq \\begin{bmatrix} \\tilde{x}^T & \\omega^T & \\dot{\\tilde{x}}^T & \\dot{\\omega}^T \\end{bmatrix}^T \\in S$ (3)\nwhere $S$ is the state space, i.e., the set of all possible 12- dimensional state vectors of the quadrotor. These 12 states include the position tracking error vector $\\tilde{x}$, the attitude angle vector $\\omega$, the linear velocity error vector $\\dot{\\tilde{x}}$, and the attitude angular velocity vector $\\dot{\\omega}$.\nThe control objective of this paper is to find an approximate solution to the optimal flight control problem (4) for the morphing quadrotor such that the quadrotor flies along the preset flight path $x_r$ with a minimal energy consumption.\n$u^*(s) = \\underset{u}{\\mathrm{argmin}}\\ J$ (4)\nwhere $u^*(s)$ is the optimal flight control law and $J$ is the performance metric of the above optimal flight control problem and defined by\n$J = \\Phi [s(t_f), t_f] + \\int_{t_0}^{t_f} F [s(t), u(t), t]\\ \\mathrm{d} t$ (5)\nin which $t_0$ is the initial time, $t_f$ is the terminal time, $\\int_{t_0}^{t_f} F [s(t), u(t), t]\\ \\mathrm{d} t$ is an integral performance metric, and $\\Phi [s(t_f), t_f]$ is a terminal performance metric, respectively. A detailed design process of the performance metric (5) will be discussed in Section III-C.\nDue to a fact that physical mechanisms of the morphing quadrotor with four variable-length arm rods are still unclear and lack domain knowledge, it is difficult or even impossible to obtain an accurate mathematical model of the form (2). The existing mature model-based RL algorithms are unable to solve the optimal flight control problem (4). In this situation, this paper will resort to a DRL algorithm, which is a type of model-free RL algorithm. The DRL algorithm will be used to train a policy function and get a nonlinear state- feedback optimal controller from the real-time flight state data [29]. The obtained optimal flight controller guides the morphing quadrotor to fly along the preset path with a better performance. Note that both state space and action space of the optimal quadrotor flight control are continuous, PPO algorithm will be utilized to train the DRL-based optimal flight control scheme."}, {"title": "C. Proximal policy optimization (PPO) algorithm", "content": "PPO algorithm is a model-free DRL algorithm [30]. A state value function is introduced to describe the value of state $s$, which is computed as follows\n$V^{\\pi}(s) = \\mathbb{E}_{\\pi} [G_t | S_t=s] = \\mathbb{E}_{\\pi} [r_{t+1} + \\gamma V^{\\pi}(s_{t+1}) | S_t=s]$ (6)\nwhere $G_t \\in \\mathbb{R}$ is the accumulated rewards of a trajectory generated from state $s_t=s$ guided by the policy $\\pi$, $\\gamma \\in (0,1)$ is the discount factor of the reward, $r_{t+1} \\in \\mathbb{R}$ is the reward of the next state $s_{t+1}$, and $\\mathbb{E}_{\\pi}$ represents the expectation of policy $\\pi$. The goal of DRL is to find a policy function such that the sequential decisions of the agent have the maximum accumulated rewards, i.e., maximum of the expectation of the initial state value function $J_{s_1}$ by choosing an appropriate policy $\\pi$:\n$J_{s_1} = \\mathbb{E}_{s_1\\sim p(s_1)} [V^{\\pi}(s)]$ (7)\nwhere $s_1 \\in \\mathbb{R}^{12}$ is the initial state, $p : \\mathbb{R}^{12} \\rightarrow \\mathbb{R}$ is the distribution function of initial state in state space $S$, and $V^{\\pi}(s_1)$ is the state value function of $s_1$ guided by the policy $\\pi$.\nAn action value function of state-action pair is adopted to describe the value of a policy $a$, i.e., the value of action $a$ at state $s$, which can be computed as follows:\n$Q^{\\pi}(s, a) = \\mathbb{E}_{\\pi} [G_t | s_t=s, a_t=a]$ (8)\nThe relationship between the state value function and the action value function is represented as follows\n$V^{\\pi}(s) = \\mathbb{E}_{a\\sim \\pi(a|s)} [Q^{\\pi}(s, a)] = \\sum_{a \\in A} \\pi(a|s) Q^{\\pi}(s, a)$ (9)\nwhere $\\pi(a|s)$ represents the probability distribution of the action $a$ at state $s$ guided by the policy $\\pi$. To facilitate the policy optimization, advantage function of action is introduced and is calculated as follows\n$A^{\\pi}(s, a) = Q^{\\pi}(s, a) - V^{\\pi}(s)$ (10)\nwhich describes the advantage of action $a$ at state $s$ over the average based on policy $\\pi$.\nFor the agent with a better decision, it is desired that the action with a larger advantage has a higher probability to be selected and the one with a smaller advantage has a lower probability to be selected. Following this idea to optimize the policy function, the optimization goal that needs to be maximized is defined as follows\n$L(9) = \\mathbb{E}_t \\begin{bmatrix} \\frac{\\pi_9(a_t|s_t)}{\\pi_{9_{old}}(a_t|s_t)} \\hat{A}_t \\end{bmatrix} = \\mathbb{E}_t [r_t(9) \\hat{A}_t ]$ (11)\nwhere 9 is the NN parameter, $r_t(9) = \\frac{\\pi_9(a_t|s_t)}{\\pi_{9_{old}}(a_t|s_t)}$ is the importance weight, $\\hat{A}_t$ is the estimation of advantage function, and $\\mathbb{E}_t$ presents the estimation of expectation. During the parameter update process, a batch of data is generated based on an existing policy $9_{old}$ interacting with the environment, which is used to optimize the target policy 9. Batch sampling and batch processing of data are achieved by importance sampling and make agent easy to train. Excessive policy op- timization leads to difficulty in convergence of the algorithm. In this paper, the PPO algorithm employs clipped surrogate objective to prevent excessive policy optimization\n$L^{CLIP}(9) = \\mathbb{E}_t [\\min(r_t(9) \\hat{A}_t, \\text{clip}(r_t(9), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t)]$ (12)\nwhere $\\epsilon$ is a hyperparameter and $\\text{clip}(r_t(9), 1-\\epsilon, 1+\\epsilon)$ is the clipping function restricting the value of $r_t(9)$ to the range $[1 - \\epsilon, 1 + \\epsilon]$.\nIn order to solve the optimal flight control problem (4) in the DRL framework, two steps are involved in this paper: offline optimal flight control training and online adaptive weighting parameter tuning. More specifically, optimal state-feedback flight controllers represented as NNs for some representative arm length modes are first trained offline based on the PPO algorithm to get a set of optimal flight control laws. Then, an online weighting parameter tuning algorithm is proposed to obtain an overall flight control law by interpolation of the off-line trained optimal flight control laws for the morphing quadrotor with four variable-length arm rods."}, {"title": "III. DEEP REINFORCEMENT LEARNING FOR OFFLINE OPTIMAL FLIGHT CONTROL DESIGN", "content": "A. Agent design\nFour rotors of the morphing quadrotor are chosen as actions of agent that is a 4-dimensional action vector $a$. The environ- ment with which the agent interacts is quadrotor dynamics and is modeled by a 12-dimensional state vector $s$ that is defined by (3). The agent makes a decision based on the observed state vector $s$ and interacts with the environment through the action vector $a$:\n$a = [n_1 \\; n_2 \\; n_3 \\; n_4]^T \\in A$ (13)\nwhere $A$ is the action space, i.e., the set of all possible actions. The actions are angular velocities $n_1$,$n_2$, $n_3$, $n_4 \\in [0, n_{max}]$ of four rotors, and $n_{max}$ is the maximum rotor speed.\nIn order to enable the agent to extensively explore the action space with stable performance, we respectively adopt a stochastic policy in the train process and a deterministic policy in the test one. This policy is described by a probability density function, under which we will sample action vector randomly during training, and choose action vector with the largest probability in the course of testing. For the probability density function with the property that the action space is a finite domain, we resort to the Beta distribution with the definition domain $(0,1)$ for each action dimension [31]. A finite domain action vector is obtained by sampling under the Beta distribution and multiplying by $n_{max}$. The corresponding probability density function of the Beta distribution is of the following form:\n$f(x; \\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)} x^{\\alpha-1} (1-x)^{\\beta-1}$ (14)\nwhere $B(\\alpha, \\beta) = \\int_0^1 x^{\\alpha-1} (1-x)^{\\beta-1} dx$ is the Beta function with $\\alpha > 0$ and $\\beta > 0$ that are two parameters control the Beta distribution shape. To facilitate the optimization of the agent policy, the probability density function has a bell curve with the value of 0 at the boundary of the domain (0,1) similar to a normal distribution by choosing parameters $\\alpha$, $\\beta$ to be more than 1. The best policy for testing is to choose an action with the largest probability. An expectation of action $\\eta_{mean} = \\frac{\\alpha}{\\alpha + \\beta} n_{max}$ is taken as a proxy for action with the largest probability to reduce the computational demand.\nFor a 4-dimensional action vector $a$, each component is described by an independent Beta distribution. So the policy $\\pi(a|s)$ can be written as a joint probability density function of the following form\n$\\begin{aligned} \\pi(a|s) &= \\pi(\\eta_1, \\eta_2, \\eta_3, \\eta_4 | s) \\\\ &= \\prod_{i=1}^{4} f(\\eta_i; \\alpha_i(s), \\beta_i(s)) \\end{aligned}$ (15)"}, {"title": "B. Neural network structure", "content": "The agent includes an action network and a critic network, where an action network approximates the policy function and a critic network evaluates the policy. The inputs of these two NNs are states of the morphing quadrotor. According to dis- cussions in the previous subsection, we have a 12-dimensional state vector $s$ and a 4-dimensional action vector $a$. Thus, the outputs of the policy function are probability density functions of Beta distribution describing the 4-dimensional action vector, which can be fully described by two parameters $\\alpha$ and $\\beta$. As a result, the output layer of the action network has two terms: the parameter $\\alpha$ and the one $\\beta$. The output of the critic network is a scalar that describes the value of state vector in a given reward function.\nThe structure of both action and critic networks is shown in Fig. 3. For these two networks, we use fully connected NN including two hidden layers, each with 64 nodes, and the activation function is 'tanh', respectively [32]. We choose 'softplus' as the activation function in the output layer of the action network and plus 1 to ensure that the parameters $\\alpha$ and $\\beta$ of Beta distribution are more than 1. The critic network\u2019s output is a scalar without any particular constraints, therefore we do not use any activation function for its output layer. The above-mentioned activation function expressions are respectively given by\n$\\text{tanh} (x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ (16)\n$\\text{softplus} (x) = \\ln(1 + e^x)$ (17)"}, {"title": "C. Reward function", "content": "To get the simulation environment Env (a), we utilize the finite difference method to discrete the differential equation (2), where the sampling period is set to be $\\Delta T = 0.1s$. Correspondingly, the performance metric (5) is rewritten as a discrete form:\n$J = \\sum_{k=0}^{T_f} F [s(k), u(k), k]$ (18)\nwhere $T_f$ is the maximum number in the episode, and $F [s(k), u(k), k]$ is the terminal metric when $k = T_f$. The optimization objective in the optimal control problem (4) is to minimize the performance metric $J$, while the aim of the DRL algorithm it to maximize the accumulated reward $G$\n$G = \\sum_{k=0}^{T_f} \\gamma^k r [s(k), u(k), k]$ (19)\nwhere $0 < \\gamma < 1$ is the discount factor and $r [s(k), u(k), k]$ is the reward at time $k$. In this situation, we choose $F [s(k), u(k), k] = -\\gamma^k r [s(k), u(k), k]$ in the perfor- mance metric $J$.\nRewards are added at each interaction step of transition process and at the end of each episode for terminal state. Standard Euclid norms of the position tracking error vector $\\tilde{x}$, the attitude angle vector $\\omega$, the tracking error velocity vector $\\dot{\\tilde{x}}$, the attitude angular velocity vector $\\dot{\\omega}$, and control input vector $u$ are involved in the reward function $r$ with different weights. To make the exploration of agent more efficient, penalty terms including the velocity error, attitude angle and attitude angular velocity are added into the reward function. At the beginning of the policy exploration, the quadrotor\u2019s position may deviate from the reference trajectory quickly. If the quadrotor\u2019s position deviation exceeds a certain value, this situation is regarded as the \u2018crash\u2019 state. In this situation, the episode of training is immediately terminated with a high penalty and proceed into the next one directly for the sake of saving computation overhead and blocking bad data for training. A survival reward is added for policy optimization when the quadrotor is unable to successfully complete an episode. After the quadrotor is able to survive in an episode, the accumulated survival rewards are constant and no longer impact policy optimization. For the agent exploring different policies, we set a maximum time limit for each episode, and when the training reaches it, we end the episode and give an additional reward value based on terminal state.\nAccording to above analysis, a reward function is designed as follows\n$r = - (C_{\\tilde{x}} ||\\tilde{x}|| + C_{\\omega} ||\\omega|| + C_{\\dot{\\tilde{x}}} ||\\dot{\\tilde{x}}|| + C_{\\dot{\\omega}} ||\\dot{\\omega}|| + C_u ||u|| + d_c C_e ||\\tilde{x}||) + d_c r_c + r_t$ (20)\nwhere $\\tilde{x} \\in \\mathbb{R}^3$ is the position tracking error vector, $\\omega \\in [-\\pi,\\pi]^3$ is the attitude angle vector, $\\dot{\\tilde{x}} \\in \\mathbb{R}^3$ is the tracking error velocity vector, $\\dot{\\omega} \\in \\mathbb{R}^3$ is the attitude angular velocity vector, $u \\in [0, n_{max}]^4$ is the control input vector, $r_c \\in \\mathbb{R}$ is a crash penalty, $C_{\\tilde{x}}$, $C_{\\omega}$, $C_{\\dot{\\tilde{x}}}$, $C_{\\dot{\\omega}}$, $C_u$, $C_e \\in \\mathbb{R}$ are the coefficients that adjust the importance among the various rewards, $r_t \\in \\mathbb{R}$ is a reward for survival of quadrotor, and $d_c$, $d_e \\in \\{0,1\\}$ are the flags of ending and defined by\n$\\begin{aligned} d_c = \\begin{cases} 1, & \\text{if } ||\\tilde{x}|| > D; \\\\ 0, & \\text{otherwise}. \\end{cases} \\\\ d_e = \\begin{cases} 1, & \\text{if } t = T_e; \\\\ 0, & \\text{otherwise}. \\end{cases} \\end{aligned}$ (21)\n(22)\nin which D is the crash distance (when the tracking tracking error exceeds D, the episode ends as a crash), t is the flight time, and $T_e$ is the set maximum time limit of the episode (when the maximum time limit is reached, the episode ends normally). Let $r'_{s,u} = (C_{\\tilde{x}}||\\tilde{x}|| + C_{\\omega}||\\omega|| + ||\\dot{\\tilde{x}}|| + C_{\\dot{\\omega}}||\\dot{\\omega}|| + C_u||u||)$, a specific form of the reward function (20) is chosen as follows\nr = $\\begin{cases} -r'_{s,u} + r_t, & k < T_f; \\\\ -r'_{s,u} + r_c + r_t, & k = T_f < T_e; \\\\ -(r'_{s,u} + C_e||\\tilde{x}||) + r_t, & k = T_f = T_e. \\end{cases}$ (23)"}, {"title": "D. Loss Function", "content": "The critic network is updated based on the temporal differ- ence (TD) error [33], of which the loss function is defined as the mean square error (MSE) of $V$ and $V_{\\delta}$:\n$L_{Critic}(9) = \\frac{1}{N} \\sum_{i=1}^{N} (V_i - V_{\\delta}(i))^2$ (24)\nwhere N is number of data in a batch, $V_{\\delta} = V_s + A$ is the TD-objective and $V_s$ is the value of state $s$. For a better tradeoff between bias and variance in the value function estimation, the TD($\\lambda$) algorithm is used, and $A_t$ is the generalized advantage estimation (GAE) [34]:\n$\\begin{aligned} A &= (1 - \\lambda) [A^{(1)}_t + \\lambda A^{(2)}_t + \\lambda^2 A^{(3)}_t + ... ] \\\\ &= \\sum_{n=0}^{\\infty} (\\gamma \\lambda)^n \\delta_{t+n} \\end{aligned}$ (25)\nwhere $A^{(n)}_t = \\sum_{i=0}^{n} \\delta_{t+i}$ is the sum of n-step TD errors and $\\delta_t = r_t + \\gamma V_{s,t+1} - V_{s,t}$ is the TD-error.\nAs the maximum number in the episode is $T_f$, we know $\\delta_{t+n} = 0$, $\\forall (t+n) > T_f$ and the expression (25) is simplified as\n$A = \\sum_{n=0}^{T_f-t} (\\gamma \\lambda)^n \\delta_{t+n}$ (26)\nThe action network is updated with the clipped surrogate objective, in which the loss function is defined as follows:\n$L_{Actor} = - \\mathbb{E}_t [\\min (r_t(9) \\hat{A}_t, \\text{clip}(r_t(9), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t) + c H [\\pi_9(a_t|s_t)]]$ (27)\nwhere $c > 0$ is the coefficient of policy entropy and $H [\\pi_9(a_t|s_t)]$ is the policy entropy. For the collected discrete data, the policy entropy can be expressed as\n$H [\\pi_9(a_t|s_t)] = - \\sum_{a_t} \\pi_9(a_t|s_t) \\log \\pi_9(a_t|s_t)$ (28)\nThe introduction of policy entropy regularization allows the policy to be optimized in a much more random way and enhances the agent's explore ability of the action space."}, {"title": "E. Updating Process", "content": "The agent collects data and updates the network via the PPO algorithm while interacting with the environment. A ReplayBuffer is set to store the data of interaction, including state $s$, action $a_o$, probability $p_{\\pi}(a_o)$, reward $r$, next state $s'$, and flag $d$. Whenever the data in the ReplayBuffer is full, the agent performs a task of networks update including k epochs, and empties ReplayBuffer to restart storing the data. For each epoch, the data is divided into a number of mini- batchsizes randomly and the Adam optimizer is used to update networks' weights. During training, the following tricks are used to improve the performance of the proposed optimal flight control scheme:\n\u2022 Orthogonal initialization is used for the networks' weights to prevent problems such as gradient vanishing and gradient explosion at the beginning of training.\n\u2022 Advantage normalization is used in each batchsize [35].\n\u2022 Reward scaling is used for each reward [32].\n\u2022 Linear decay learning rate is used in the Adam optimizer [36], [37].\n\u2022 Excessively large gradients are clipped before optimiza- tion [38].\nThe algorithm details are shown in Algorithm 1 and Fig. 4. By repeatedly applying Algorithm 1 for the selected representa- tive length modes of arm rods, the corresponding DRL-based offline optimal flight control scheme can be obtained."}, {"title": "IV. COMBINED DEEP REINFORCEMENT LEARNING FLIGHT CONTROL VIA WEIGHTING COMBINATION", "content": "For arbitrary lengths of four quadrotor arm rods, via convex combination, they can be represented as a linear combination of some selected representative arm length modes. In the light of this fact, a cc-DRL flight control scheme can be obtained by interpolation of the optimal flight control schemes that are trained offline for the representative arm length modes. In this way, a cc-DRL flight control law $u_{cc-DRL}$ is directly obtained from a set of trained optimal flight control laws $U = \\{u_i\\ |\\ u_i = \\pi_{\\vartheta,i}(a|s)\\}$, i.e.,\n$u_{cc-DRL} = \\sum_{i=1}^n \\lambda_i u_i$ (29)\nwhere $u_i \\in \\mathbb{R}^4$, $i \\in \\{1,2,\\ldots, n\\}$ are the offline trained optimal flight control laws for the n selected representative arm lengths $l_i \\in \\mathbb{R}^4$, $i\\in \\{1,2,\\ldots,n\\}$ and $\\lambda_i$, $i \\in \\{1,2,\\ldots,n\\}$ are the combination weight values satisfying\n$l(t) = \\sum_{i=1}^n \\lambda_i l_i,$ (30)\nin which $l(t) \\in \\mathbb{R}^4$ is the arbitrary current length vector of four quadrotor arm rods.\nAssume that the length range of each arm is set to be $[l_{min}, l_{max}", "l_4": "in \\mathbb{R}^4$ is chosen from the set $[l_{min}, l_{max}", "l_{max}": 4, "39": "any element in a convex set C in $\\mathbb{R}^4$ can be represented by a convex combination of 5 or fewer vertices. The maximum norm solution to Eq.(30) can be formulated as the following non-convex"}]}