{"title": "CarFormer: Self-Driving with Learned\nObject-Centric Representations", "authors": ["Shadi Hamdan", "Fatma G\u00fcney"], "abstract": "The choice of representation plays a key role in self-driving.\nBird's eye view (BEV) representations have shown remarkable perfor-\nmance in recent years. In this paper, we propose to learn object-centric\nrepresentations in BEV to distill a complex scene into more actionable\ninformation for self-driving. We first learn to place objects into slots\nwith a slot attention model on BEV sequences. Based on these object-\ncentric representations, we then train a transformer to learn to drive as\nwell as reason about the future of other vehicles. We found that object-\ncentric slot representations outperform both scene-level and object-level\napproaches that use the exact attributes of objects. Slot representations\nnaturally incorporate information about objects from their spatial and\ntemporal context such as position, heading, and speed without explic-\nitly providing it. Our model with slots achieves an increased completion\nrate of the provided routes and, consequently, a higher driving score,\nwith a lower variance across multiple runs, affirming slots as a reliable\nalternative in object-centric approaches. Additionally, we validate our\nmodel's performance as a world model through forecasting experiments,\ndemonstrating its capability to predict future slot representations accu-\nrately.", "sections": [{"title": "1 Introduction", "content": "The task of urban driving requires understanding the dynamics between ob-\njects in the scene. In self-driving, a scene is observed by the ego-vehicle via\nits sensors, typically multiple cameras. Then, for each observation step, an ac-\ntion is predicted and performed by the vehicle. In this paper, we propose an\nauto-regressive transformer-based model to reason about the dynamics of the\nscene while learning to drive. Starting with language processing, transformers\nare the de facto architecture today for sequential modeling from video genera-\ntion [33, 42, 53] to robotics tasks. In language modeling, transformers typically\noperate on discretized inputs, or tokens, which are vastly different than high-\ndimensional continuous inputs such as camera images as in the case of self-\ndriving."}, {"title": "2 Related Work", "content": "We first provide an overview of representation spaces that are commonly used in\nself-driving with a special focus on BEV. We then briefly summarize the progress\nin self-supervised object-centric methods. Finally, we compare our approach to\nthe recent sequence modeling approaches to control problems in robotics.\nRepresentation Spaces in Self-Driving: Starting with hand-designed affor-\ndances [7,45], various representation types have been proposed for self-driving.\nOne common representation is semantic segmentation, initially in 2D [34,35, 46,\n58], sometimes coupled with object detection for efficiency [4], and more recently\nin BEV [8,9,20,22,56]. Another representation space is the coordinate space that\nis commonly used in trajectory prediction [6]. A notable work by Wang et al.\nuses a 2D detector to estimate the 3D properties of objects to render them\nin BEV [49]. In addition to position, PlanT [43] also includes other driving-\nrelated information such as heading and speed in its representation of each ob-\nject through an attribute vector. A representation based on object coordinates\nenables methods to concentrate on object-to-object relationships. However, it\noften lacks rich semantic information and may not adequately capture the vary-\ning spatiotemporal contexts of objects. Our goal is to address these limitations\nby integrating learned object-centric representations.\nSelf-Supervised Object-Centric Representations: In this work, we build\non the progress in object-centric learning where the goal is to decompose the\nscene into objects. A common way of achieving this goal is to integrate induc-\ntive biases about objects into the architecture, typically in an auto-encoding\nparadigm. Beginning with slot attention [32], these techniques reconstruct the\ninput with a set of bottlenecks in the latent space called slots. Each slot is ex-\npected to bind to an object region with similar visual cues. In this work, we"}, {"title": "3 Background on Slot Extraction", "content": "Given the BEV representation of the scene in the last T time steps, we first use\na frozen object-centric model to extract the objects into slots. For extracting\nslots, we build on slot attention for videos, SAVi [27]. Here, we provide a brief\noverview of SAVi for completeness and also, to introduce the notation with slots.\nPlease see [27] for details of SAVi.\nAs a reconstruction-based method, SAVi follows an auto-encoding paradigm.\nGiven a sequence of RGB frames xt\u2212T:t with T time steps for context, a CNN-\nbased encoder is used to process each frame x with positional encoding. The\noutput of the encoder is flattened into a set of vectors ht\u2212T:t. SAVi first initializes"}, {"title": "4 Methodology", "content": "We introduce CarFormer for learning to drive in the urban environment of\nCARLA [14]. Urban driving presents complexity due to the interactions between\nthe ego-vehicle and other vehicles. Our goal is to learn driving behavior by cap-\nturing scene dynamics through slot representations. We formulate the behavior\nlearning as a sequence modeling problem, as illustrated in Fig. 1. This sequence\ncomprises tokens representing the goal, state, and action. We first define repre-\nsentations for each aspect before detailing the model architecture."}, {"title": "4.1 Goal and State Representation", "content": "We encode the route to follow and the state of the world in terms of tokens,\nwhich can be continuous or discrete, and feed them into our model. Specifically,\nwe provide the model with the next target point in the route $(g_t, g_r)$, a flag\nsignifying whether the ego vehicle is affected by a red traffic light $l_t \\in {0, 1}$,\nand the current speed of the ego vehicle $v_t \\in \\mathbb{R}$. For each of these attributes,\nwe apply k-means clustering and quantize them into $k_{attr}$ bins. For scene repre-\nsentation, we initially consider a scene-level representation by directly encoding\nthe BEV map, and then we explore object-level representations.\nScene-Level Representation: The input consists of a BEV representation\nof the scene at time t, denoted as $B_t \\in [0, 1]^{192 \\times 192 \\times 8}$, centered around the ego-\nvehicle at time t (i.e. we use an ego-centric coordinate frame at each time-step).\nEach of the 8 channels represents a binary map corresponding to a semantic class,\nsuch as road and vehicles [56]. Following common practice [54], we use a VQ-\nVAE [38] to encode B into a grid of discrete integers: $b_t \\in {1, ..., C}^{12 \\times 12}$ where\nC denotes the codebook size. We then flatten this grid to obtain a representa-\ntion of the scene as a set of discrete tokens. Despite successful reconstructions,\nour experiments show that learning successful driving behavior on top of these\ndiscretized tokens is challenging (see Table 1).\nObject-Level Representations: An alternative approach to representing the\nentire scene as a rasterized BEV is to represent individual objects within the\nscene. In PlanT [43], both vehicles and the desired route are represented as\n6-dimensional vectors representing essential information such as size, position,\norientation, and speed. Following PlanT, we initially train our model using the\nexact attributes of objects to represent the scene. This allows us to attribute\nimprovements directly to the proposed object-centric representation with slots\nrather than our modifications to the transformer, such as block attention.\nIn this paper, we propose an alternative object-level representation based on\nslots. Building on advancements in self-supervised object-centric representations,\nwe explore slots as a more natural way of representing objects compared to exact\nobject attributes. With slots, objects can be implicitly represented with relevant\ninformation from the spatio-temporal context of the object. Given the BEV\nrepresentation of the scene in previous T time steps, denoted as $B_{t-T:t}$, we\nemploy SAVi [27] to extract objects into K slots, $\\{z_t^i\\}_{i=1}^K$ where each $z_t^i \\in \\mathbb{R}^{1 \\times d}$\nis a d-dimensional slot vector corresponding to an object in the scene. As object-\nlevel representations do not include any information on the desired route, we also\nprovide the model with the desired route represented by two vectors $r_t, r \\in \\mathbb{R}^{6}$\nfollowing PlanT [43]."}, {"title": "4.2 Action Representation", "content": "Following common practice in self-driving [12,20,43], we predict waypoints that\nare then used to calculate the corresponding control consisting of throttle, brake,\nand steering angle. We predict waypoints in two ways:"}, {"title": "4.3 Car Former", "content": "Our goal is to learn self-driving in urban environments while jointly reasoning\nabout scene dynamics as a sequence prediction task. At each time step, we\nrepresent the state of the world with a set of tokens as previously defined. We\ndefine a trajectory Tt at time step t as follows:\n$T_t = {g^x_t, g^y_t, l_t, v_t, z^1_t, ..., z^K_t, r_t, r, q^1_t,..., q^{2W}_t}$        (2)\nwhere $g^x_t, g^y_t$ denote the target point, $l_t$ the traffic light, $v_t$ the speed, $\\{z^i_t\\}_{i=1}^K$\nobject-level slot representations, and rt,r two attribute vectors representing\nthe desired route. In PlanT style representation, slot vectors are replaced by\nthe object attributes. In the case of scene-level representation, object-level slot\nrepresentations and the desired route are replaced by the discrete tokens from\nthe VQ-VAE, represented as $\\{b^i_t\\}_{i=1}^C$.\nEncoding: For each input in the trajectory, we handle discrete inputs, such\nas quantized target points and traffic light status, by performing a lookup from\nan embedding matrix. In the case of continuous attributes, like slot vectors, we\nproject the vectors into the desired dimensionality using an MLP, as illustrated in\nFig. 1. Specifically, for each discrete attribute, we initialize a $k_{attr} \\times H$ embedding\nmatrix, where $k_{attr}$ represents the number of possible values of the attribute after\ndiscretization, and H denotes the hidden dimension of the backbone. Conversely,\nfor continuous attributes, we utilize an MLP to project the vectors into $R^H$.\nArchitecture: The backbone of the CarFormer is an autoregressive trans-\nformer decoder adapted from the architecture of GPT-2 [41]. We modify the\nembedding layer to accommodate both continuous and discrete inputs simulta-\nneously, enabling the incorporation of continuous representations such as slots\nwhile keeping other inputs like speed and traffic light discrete. Furthermore, we\nadjust the attention mechanism, which is causal in transformer decoders, to al-\nlow for cross-attention between certain blocks of the input. These blocks can\nbe considered as a single unit, despite being composed of multiple tokens, such\nas the slot representations. To achieve this, we replace the triangular causal at-\ntention mask with a block triangular mask, enabling cross-attention within the\nblock as shown on top in Fig. 1. We use this attention mechanism throughout\nour experiments and evaluate its impact in Table 2."}, {"title": "4.4 Training", "content": "While our formulation can be extended to RL with rewards and multi-step pre-\ndictions, currently, it corresponds to imitation learning with a single-step policy\nto predict action based on context. We train the model using imitation learning\nto learn to predict both the continuous waypoints wt and their quantized form,\nqt given the context as shown in (2). We supervise both the GRU head, re-\nsponsible for predicting continuous waypoints, and the language modeling head,\nresponsible for predicting the quantized waypoints. To achieve this, we use the\nfollowing loss functions:\n$L_{wp} = L_{GRU} + L_{LM}$                                                 (3)\n$L_{GRU} = \\sum_{i=1}^{W} |w_i - w_i|^2$\n$L_{LM} = \\sum_{i=1}^{W} \\sum_{j=1}^{2} CE(q_{i,j}, q^)_{i,j}$\nAuxiliary Forecasting: Similar to PlanT [43], we additionally train the model\nto predict future scene representations jointly with action. In the case of the dis-\ncretized scene representation using the VQ-VAE, we calculate the cross-entropy\nbetween the predicted logits and the ground truth future representation:\n$L_{scene} = \\sum_{i=1}^{C} CE (b_{t+f}, b_{i+f})$                                             (4)\nwhere f denotes the time horizon into the future for which we make predictions.\nIn the case of continuous scene representations, such as in object-level vec-\ntors or slot representations, we instead use the mean squared error between the\nrepresentations:\n$L_{object} = \\sum_{i=1}^{K} | z^i_{t+f} - \\hat{z}^i_{t+f} || $                                             (5)\nThe final loss is a weighted sum of the losses on action (3) and forecasting:\n$L_{wp} + \\alpha L_{forecast}$                         (6)\nwhere $L_{forecast}$ corresponds to $L_{scene}$ (4) in case of scene-level and $L_{object}$ (5) in\ncase of object-level. We experimentally set \u03b1, the weight of forecasting, to 40."}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\nCARLA Setting: We collect training data using the setup introduced in Trans-\nFuser [12] on CARLA version 9.10.1., as also in PlanT [43]. As PlanT [43] shows"}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we introduced CarFormer as the first approach to self-driving with\nobject-level slot representations. We demonstrated that reasoning with slots not\nonly improves the driving score but also provides robustness across variations in\nmultiple online evaluations. We trained and validated the performance of Car-\nFormer both as a policy to predict action and as a visual dynamics model for\npredicting future states of objects. Unlike PlanT, which utilizes a transformer\nencoder to process a single time step, we employed an autoregressive transformer\ndecoder in CarFormer. This design has the potential to be extended to multi-step"}]}