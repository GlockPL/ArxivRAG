{"title": "Challenges in Human-Agent Communication", "authors": ["Gagan Bansal", "Jennifer Wortman Vaughan", "Saleema Amershi", "Eric Horvitz", "Adam Fourney", "Hussein Mozannar", "Victor Dibia", "Daniel S. Weld"], "abstract": "Remarkable advancements in modern generative foundation models have enabled the development of sophisticated and highly capable autonomous agents that can observe their environment, invoke tools, and communicate with other agents to solve problems. Although such agents can communicate with users through natural language, their complexity and wide-ranging failure modes present novel challenges for human-AI interaction. Building on prior research and informed by a communication grounding perspective, we contribute to the study of human-agent communication by identifying and analyzing twelve key communication challenges that these systems pose. These include challenges in conveying information from the agent to the user, challenges in enabling the user to convey information to the agent, and overarching challenges that need to be considered across all human-agent communication. We illustrate each challenge through concrete examples and identify open directions of research. Our findings provide insights into critical gaps in human-agent communication research and serve as an urgent call for new design patterns, principles, and guidelines to support transparency and control in these systems.", "sections": [{"title": "Introduction", "content": "Artificially intelligent agents are systems that can autonomously perceive and take actions in an environment [61]. While the study of AI agents traces back many decades [61, 70, 82], recent advances in generative foundation models that can output novel text or images based on natural language prompts have paved the way for the widespread development and deployment of a new class of agents that are increasingly sophisticated, powerful, and general purpose. Granted the ability to access the internet,\u00b9 connect with other applications through APIs,\u00b2 and even generate and execute computer code,\u00b3 today's AI agents can perform actions such as scheduling meetings, booking flights,\u2074 ordering food,5 or purchasing groceries, taking actions that impact both the digital and physical realms. Ongoing developments in multi-agent architectures are further expanding the capabilities and use cases of agents [80, 82, 22].\nAlong with agents' greater capacity to take actions in the open world and to complete goals on behalf of users comes a wider range of potential failure modes and associated costs [68, 88, 24]. In fact modern applications are extending agent-centric activities into high-stakes scenarios. For example, an agent that can shop on a user's behalf can spend money in unintended ways or inadvertently leak the user's address, credit card number, or other sensitive information. An agent that can execute computer code can corrupt files, alter important settings, overwrite family photos or work assignments, and take actions that jeopardize security. Without a proper understanding of an agent's capabilities and limitations and the ability to verify its actions, a user may over-rely on an agent, leading to the user requesting that the agent perform a task that it is incapable of completing. Particularly in situations where failures are costly or likely to occur, it is critical to build agents that allow users to clearly express their goals, preferences, and constraints to the agent and to form an accurate mental model of how the agent will behave. Users should also be able to monitor the agent's behavior and effectively guide the agent with feedback and corrections as needed. Put another way, to enable effective collaboration with users, agents and systems of multiple agents must be designed to support transparency and control.\nThe key to enabling transparency and control is effective two-way communication aimed at establishing common ground about the user's goals (e.g., as represented by the content of the user's request) and about the process the agent intends to take to achieve these goals [13, 65]. Achieving common ground is an activity that begins with the user's first introduction to the agent and continues throughout and after usage. Communication between users and agents can help a user to determine when and how much to rely on the agent and, through the iterative nature of dialog about intentions, capabilities, and activities, can help the user to identify and correct misconceptions before irrevocable actions have been taken."}, {"title": "What is different about modern AI agents?", "content": "However, communication that establishes common ground can be difficult. Even with small-scale or special-purpose models, it can be difficult to characterize an AI system's abilities in a way that users understand [18, 45, 76, 43, 53]. Generative foundation models have characteristics that make transparent disclosure of their operation particularly challenging, including the wide and constantly evolving range of tasks they can perform at different levels of competency, their massive and opaque architectures, sensitivity to prompt and context, the stochasticity of their output, and the diversity of their user bases who may require different levels of detail [44]. The challenge of understanding agents' operation is further exacerbated by the complexity of agentic workflows, in which tasks may be decomposed and carried out over multiple steps-sometimes by multiple interacting agents with different roles, privileges, and access to different information [80, 22]. While the literature on human-AI interaction offers general guidance and sets of principles [29, 1, 26], interacting with systems of one or more autonomous, tool-using AI agents raises new transparency and communication challenges that have yet to be addressed.\nWe present a set of challenges that arise in the process of establishing common ground between human users and AI agents, as summarized in Figure 1. We arrived at these challenges based on our experiences building and experimenting with complex AI agents and multi-agent systems, drawing on the literature on human-AI interaction and collaboration, including prior work on establishing common ground between people and machines. Some of these challenges reflect the difficulty of conveying necessary information from the agent to the user to allow the user to form an appropriate mental model of the agent and monitor its behavior. Some reflect the difficulty of designing the agent to enable the user to convey their own goals, preferences, and constraints to the agent and guide the agent with feedback. Others are overarching challenges reflecting general difficulties with communication, such as avoiding inconsistencies and reducing the burden on users. While this list isn't exhaustive, we hope it serves as a starting point for discussion and future research.\nWe note that many parallels can be drawn between our proposed challenges and those faced in establishing communication and coordination within teams of humans (summarized in Section 2). We use the term \u201ccommon ground\u201d in the sense of Brennan [6] to refer to operational alignments, including on shared inferences such as the likelihoods of the current state of the world or future outcomes jointly considered by the human and agent, rather than broader cognitive or experiential similarities and do not suggest that agents understand in a similar way to humans.\nWhile the agent-based perspective in AI is not new, two primary technical advances distinguish today's agentic applications from those of the past. First, unlike previous AI agents, which were typically based on simpler models with well-structured inputs and outputs, today's are based on generative foundation models, like large language models or multimodal neural models, that can output novel text and/or graphical content based on natural-language (or multimodal) prompts or commands. These generative foundation models have demonstrated remarkable performance across a wide range of use cases or tasks [7] and emerging capabilities continue to be discovered as models improve [79]. The broad abilities of foundation models enables the design of agents that exhibit a wide range of capabilities that often, though not always, include language understanding and generation competencies that allow people and agents to communicate through natural language.\nSecond, generative foundation models can be given the ability to invoke a wide range of"}, {"title": "Outline", "content": "resources or tools, for instance, through APIs [66]. Such tools and plugins enable agents to interact with the world, facilitating actions across domains such as including finance, communication, and physical activities in the open world. Some agents can even execute arbitrary Python code in various environments, including cloud-based containers, local systems, or the user's own execution environment. These tools serve as sensors that enable the agents to perceive aspects of the world and effectors that allow them to take actions resulting in changes to world state. For instance, one tool might enable the agent to fetch weather data from a web API, while another might allow the agent to plot the data and save it to the user's local or cloud storage.\nFor another example, consider Devin, an \"AI software engineer\" agent with access to a command line environment, code editor, and web browser [81]. In fact, its ability to control a Web browser allows it to browse information on the Web and even purchase products or services\u2014actions with significant potential side effects [88]. Since Devin has basic reasoning and planning capabilities, it can execute (often successfully!) complex engineering tasks, such as debugging code or even training and fine-tuning new AI models. However, current limitations in LLM planning [36] and other issues such as hallucinations mean that Devin's actual behavior cannot be perfectly anticipated, potentially leading to costly or catastrophic errors.\nIn summary, generative language models empower tool-using capabilities that increase the power of agentic systems radically, allowing them to execute meaningful actions in the world and creating new (or newly complex) challenges as discussed in the rest of the paper.\nWe consider a set of emergent challenges pertaining to human-agent communication. While the rise of new degrees of autonomy and use of tools by agents poses many other important challenges including technical [33, 35, 32, 73, 87, 17], ethical [12, 41, 24], safety [3, 68], and fairness [34, 48] challenges, we consider these to be outside the scope of this paper.\nWe start our discussion with a recap of previous work from the perspectives of psychology, cognitive science, AI, and HCI. In Section 3, we list four overarching challenges that apply across all human-agent communication. In Sections 4 and 5, we discuss issues that emerge from the need to communicate information from the agent to the user and user to the agent, respectively. In each section, we describe the challenges in detail, contextualize them with respect to the literature, and provide concrete examples from existing systems and emerging applications. We outline possible steps towards solutions for each challenge and identify open research questions. Finally, in Section 6, we close with a call to action for research in human-agent communication."}, {"title": "Previous Work", "content": "We now summarize some of the relevant literature that we build upon, drawing from prior research and results in psychology, cognitive science, AI, and HCI.\nResearchers in philosophy and psychology have long studied human communication and teamwork. The notion of grounding in communication is a central concept proposed by Clark and Brennan that embodies the collection of mutual knowledge, mutual beliefs, and mutual assumptions that are essential for communication between two people [13]. To effectively ground their communication, the participants must \"coordinate both the content and process\" of each conversation. The process of achieving a shared mutual belief (common ground) is called grounding. Clark proposed successful grounding as requiring the parallel action of multiple levels of\nanalysis, including the establishment of a communication channel, the exchange of recognized signals across the maintained channel, the interpretation of intentions via decoding meaning in the signals, and the control of the back and forth of an effective conversation with contributions and clarifications being made by dialog participants.\nDrawing parallels to human communication, Brennan applied grounding theory to human-computer interaction, where, as in human communication, \u201cpeople need to be able to seek evidence that they have been understood and to provide evidence about their own intentions\" [6]. In this case, the term \"common ground\" refers to operational alignments rather than cognitive or experiential similarities. Brennan noted that poor feedback or affordance mechanisms often result in users needing to execute laborious checking actions to achieve common ground. The breadth of capabilities (and stochasticity) displayed by generative language models exacerbate these problems, as we discuss.\nIn related work informed by research in psychology on human-human communication, explicit computational grounding machinery was developed and integrated within computational architectures to support human-AI grounding processes as joint activity aimed at achieving mutual understanding [31]. Another effort explored the development of computational analogs of the multiple levels of coordination for grounding proposed by Clark, including inferences at channel, signal, intention, and conversation levels, to establish common ground between a user and a dialog system that could execute actions in the open world. [54]. In later work, definitions of grounding in human-AI collaborations were shifted to an expected utility framework. In the approach, grounding on ideal actions for an agent is guided by uncertainties that an agent has about a user's goals and the costs and benefits of different actions [54]. The latter work also explored the use of visual signaling by the agent to communicate to the user the degree to which an agent believed it was grounded with the user via display of smoothly changing colors. More recent studies have explored grounding between machine and human in a multimodal setting, where uncertainties that an agent has about a user's goals and intentions are inferred via signals drawn from multiple streams of information, including language, vision, predictions about states of the world, and conversational flow, and then communicated to users via dialog acts and gestures [55].\nSeveral efforts have focused on establishing common ground between machines and people on memories about activities that have occurred in the past. Notions of shared memory have been constructed via mechanisms that infer important milestone events drawn from larger streams of events observed over time that are likely to be memories accessed by, referred to, or assumed in joint activities by users when interacting with AI systems. Studies have explored the identification of memory milestones to enable easy reference to events by users and AI systems [59, 30] and shared, referable memories of prior interactions and situations of the type that people would have with others whom they have worked with over time [60].\nAmong people, grounding typically proceeds in alternating phases of presentation and acceptance with gestures and facial expressions often serving as acknowledgments [25]. Communication made across other media (e.g., when communicating with or via computers) forces people to use more elaborate grounding methods and often increases communication costs for both the speaker and addressee [13]. These media-based constraints play into the communication challenges discussed in this paper.\nDennis et al. elaborated on the theory of communication processes and distinguish between conveyance and convergence processes. They argued that the former can do with lower media synchronicity whereas convergence benefits from higher media synchronicity [15]. Here, media synchronicity indicates the degree to which the communication medium allows coordinated\nsynchronous behavior. They conclude that communication is improved when participants use various media to perform a task. Generalizing 'media' to computational affordances, these theories also inform our proposed solutions.\nThe study of human-agent collaboration should also be informed by social and cognitive psychologists' work on decision-making processes of groups of people, which may often surpass the performance of individuals comprising the group. Many of these prior studies corroborate a theory that groups collectively perform better than the average (and many times, best) individual on many types of problem-solving tasks [28, 10]. These effects were more strongly seen on some tasks than others and attributed to the demonstrability of proposed solutions to the task [40] which may be seen as properties of the group and task that facilitate grounding. Some of our suggestions for X1-X3 benefit from these insights from psychology and their adaptations into principles for AI-generated explanations [21]. Shared mental models, closed-loop communication, and mutual trust have been identified as core mechanisms in successful human teamwork [63], and some of these aspects map clearly to human-AI teams [4]. Still, the differences in understanding, decision-making processes, and communication styles between humans and machines change the context and introduce new challenges.\nOur work also builds on significant recent work on design guidelines for human-AI interaction [1, 26, 69, 84, 83, 35, 85]. The rise of generative AI models has led to a massive shift in the ways AI is being deployed from interactive classifiers into powerful tool-using agents. This transformation raises a number of new issues. For example, using tools allows agents to take a much wider range of actions [67], increasing complexity and the number of ambiguities that must be grounded. In contrast to previous AI applications, agentic systems commonly have an emphasis on generalist capabilities [58, 77], which likewise raises the complexity of grounding. These systems are often crafted as \u201csociety of mind\" [51] agents that pass tasks to each other [80], which can result in complex emergent behaviors\u2014again posing challenges for grounding. As a result of these jumps in complexity, it is appropriate to revisit both guidelines and challenges for human-AI interaction.\nA closely related work is the report from Shavit et al [68] which offers a set of practices towards making the operation of agentic AI systems more safe and accountable. That work intersects with a subset of the challenges that we discuss in this paper; specifically challenge X1 and A3 are closely tied to \"Legibility of Agent Activity\" and challenge X3 and A3 are tied to \"Constraining the Action-Space and Requiring Approval.\" Our work makes progress by also reflecting on broader challenges in human-agent communication, including how to effectively convey capabilities and limitations, manage information flow between agents and users, and address emergent issues in multi-agent systems."}, {"title": "Overarching Challenges for Human-Agent Communication", "content": "Before digging into what specific information must be communicated between a human user and AI agent to establish common ground, we begin with a discussion of four overarching challenges that should be considered across all human-agent communication (challenges X1-X4 in Figure 1). We briefly introduce each here and return to them when discussing the challenges in communicating specific information in Sections 4 and 5."}, {"title": "How should the agent help the user verify its behavior?", "content": "Modern agents are powerful but remain imperfect. As a result, when agents get tasked with goals, it is likely that they will make mistakes especially when these goals are complex or multi-step. Consider an agent tasked with fixing a programming bug in a GitHub repository. This agent can fail in numerous ways. For example, it might fail to understand the issue and its requirements, such as the fact that any solution must be backward compatible; it might develop a plan that implements the fix in the source directory but neglects to update the test cases; it might propose a fix that is \"unsafe\" because it is vulnerable to exploits or propose a fix that is convoluted and time-consuming to review; it might propose a change that has side effects such as removing unrelated functionality.\nTo establish establish common ground, it is necessary not only to convey pieces of information but to verify that a common understanding has been reached. In the context of human-AI communication, it is not enough, for instance, for a user to state their goals and preferences if these goals and preferences are misunderstood or ignored by the agent. It is, therefore, critical to enable the user to verify their understanding of the agent's plans and actions as well as the agent's understanding of their own goals, preferences, requirements, and feedback. Establishing mechanisms for the user to easily verify the agent's behavior is a necessary condition of effective communication [21] and an important aspect of effective teamwork [39]. Without this ability, the user may not be able to steer agent behavior to avoid costly mistakes or accomplish the goal. Furthermore, they would fail to develop an accurate mental model of the agent's capabilities or provide feedback to improve the agent's future performance."}, {"title": "How should the agent convey consistent behavior?", "content": "A second overarching challenge for human-AI communication is how to prevent the agent from confusing the human with behavior or outputs that are (or at least appear) inconsistent. Inconsistency can arise for multiple reasons. One is the agent's behavior's inherent stochasticity, which stems from two main sources: the probabilistic nature of the underlying foundation model's outputs, and the complex interaction patterns that emerge during task execution. Even with deterministic foundation model outputs (e.g., a model with temperature set to 0), when agents operate in dynamic environments, their orchestration logic which governs action selection, inter-agent delegation, and task completion criteria can produce different sequences of actions across runs as the environment changes in response to agent actions. While stochastic behavior may be desirable in some cases- -for instance, if the user would like the agent to generate a variety of different options to choose from in other cases, it can hinder the user's ability to create an accurate mental model of the agent. For example, consider an LLM-based agent that can create visualizations based on a high-level description from the user. When invoked with the same input multiple times, the agent might choose different libraries or tools to plot the visualization. Sometimes, it might write code to fetch the data from a hosted service; other times, it might generate that data from its memory. It could also unpredictably apply different visual styles, which would be especially problematic if the user is unaware.\nPerceived inconsistencies can arise for other reasons, for instance, when the user's mental model of the world does not match up with the information the agent is acting on. Even if an agent is taking actions that align with the user's goals, its actions may appear misaligned if the human's model of the world is different [72]; think of a shopping agent purchasing what appears to be an overly expensive widget because it knows that the cheaper model is incompatible with the user's needs, but fails to consider the user's budget limitations."}, {"title": "How should the agent choose an appropriate level of detail?", "content": "If such forms of (perceived) inconsistencies are not minimized, users may become confused about whether the agent has accomplished their goal, whether it is capable of accomplishing the same (or a similar) goal again, whether it would follow similar approaches as it used in the past, and how to consistently and accurately steer its behavior. Over time, this will likely lead to diminished trust.\nWhile it is essential to design agents in a way that enables users to verify their behavior and avoids confusing users, these needs must be balanced with the need to avoid burdening or overwhelming the user by providing too much information. Interactions with modern agents necessitate richer bidirectional communication than was necessary with traditional AI systems. This increased emphasis on communication arises because of the complexity of the underlying AI systems and the complexity of the tasks the agents are being asked to perform. While more detailed communication can help establish a shared understanding, it can become counterproductive when instructing the agent becomes burdensome or reviewing agent outputs becomes overly cognitively taxing. It also may overconstrain the agent, preventing it from exploring alternative solutions or recovering from failures.\nWhen designing for human-AI communication, there are several questions: Can the user easily instruct the agent regarding their goals, constraints, and feedback, or does this require a lengthy and potentially unreliable process? (See also Vasconcelos et al. [75].) Are the explanations of the agent's behavior clear and understandable, or are they too complex to be useful? Additionally, does the agent need to confirm every request, or can it act with more autonomy in familiar situations?\nConsider how an LLM-driven agent might assist a user with drafting emails. Suppose a user frequently asks the agent to create follow-up emails with a similar structure and tone for recurring scenarios, such as responding to meeting requests or sending reminders. An overly verbose agent might repeatedly ask for clarification on tone, length, or recipients for every request, even when the user has previously provided clear preferences for these aspects. Instead, an ideal agent would leverage its understanding of past interactions to streamline the process, only requesting additional details for novel or ambiguous scenarios. This avoids unnecessary repetition and enhances user satisfaction.\nWhile this challenge isn't new [56], the generative power of today's language models makes it omnipresent (as the example above illustrates)."}, {"title": "Which past interactions should the agent consider when communicating?", "content": "Context plays a key role in many aspects of human communication and computing (e.g., in search and recommendation) [27, 5]. Interactions with modern agents can especially involve rich and intricate contexts that both the user and agent may draw from to facilitate communication and understanding, e.g., past interactions containing lengthy generations and exchanges with the user, background information about the user, or observations from the environment and tool use. For example, consider an academic research agent who assists a scholar. This agent could benefit from reading all of the scholar's papers so that it can make use of references to past results and experimental designs, storing and reusing prior discussions with the scholar about an ongoing project, or considering observations from experiments that it helped conduct. This context could grow unbounded as the agent works with a user over days, months, and eventually years. Agents should use this past context to satisfy each user request, but as the\ncontext grows huge, how can we ensure the agents focus on what is relevant for interpreting the current command? (See also Liu et al. [46].) This may depend on the task the agent is performing or the specific question the user asks at a given moment.\nA similar challenge occurs when deciding which information to retain from the agent's percepts, such as when a tool retrieves a large amount of data (e.g., 1000 PDF documents). These percepts, like the interaction context, can also generate large and complex contexts, further complicating the challenge of maintaining relevance and focus in its responses.\nIn some cases, the user may want to limit the context that the agent is allowed to rely on; for instance, if allowing the agent to rely on particular sources of information would compromise privacy. The user should always be able to restrict the use of background information when they prefer. Fortunately, this need has surfaced in many analogous situations, suggesting applicable design patterns. For example, most internet browsers support incognito mode, where cookies and other personally identifiable information are suppressed. Similarly, browsers allow users to view, edit, and purge the history of past browsing. Social networks support users who switch between different personas. We suspect that similar tools will be important for supporting human-agent interaction. Additionally, recent work on agent workflow memory [78] shows how agents can learn reusable task workflows from past experiences to improve their performance on complex, long-horizon tasks. This approach has demonstrated significant improvements in web navigation tasks across diverse domains, suggesting that explicit memory mechanisms could be particularly valuable for agents executing repetitive tasks."}, {"title": "Information Flow from the User to the Agent", "content": "In this section, we discuss three challenges (U1-U3) related to designing agents to enable users to communicate necessary information. The first two concern the user's desires: what users want the agent to achieve and preferences for how the agent achieves it. The third challenge pertains to feedback and helping the agent improve over time. While much of the information content covered by U3 overlaps with U1 and U2, the time and context differ, affecting communication. For each issue, we discuss existing research, describe how these difficulties have changed for today's agents, provide concrete examples, and outline possible solutions."}, {"title": "What should the agent achieve?", "content": "When agents assist with or carry out tasks for people, one of the most critical pieces of information a user needs to communicate to the agent is the goal what they would like the agent to achieve. If the agent misunderstands the goal, the consequences can be dire. An agent may waste compute, money, and time accomplishing the wrong (or worse, harmful or unsafe) thing, create side effects, or cause user frustration and abandonment. It is then critical that we ask how to design agents so that users can clearly express their needs and intent and resolve possible ambiguities [16].\nThe challenges that arise in interpreting user intent are well studied in domains such as classical planning, personal assistants, mixed-initiative systems [29], and context-aware computing [16, 49, 42]. For example, in classical planning, users may specify their goal via a set of logical conditions that are true in the goal state. Many formal languages for specifying the goal and scenario exist, such as the Planning Domain Definition Language and its variants [50, 86, 64]. These formal languages provide a precise and explicit way to define goals. While precise, these approaches are less intuitive and natural for users compared to specifying\ngoals in natural language. One reason is that these formal languages inherit the limitations of first-order logic, such as its limited expressivity compared to natural language specifications, which is now common with modern agents. In contrast, while more intuitive, goals specified in natural language introduce challenges such as ambiguity and imprecision.\nWhile goals were specified in natural language in the personal assistant literature (e.g., \"Set an alarm clock for 8 am tomorrow\u201d), there were many key differences: the specified goals were simple and limited to a small number of actions, an intent recognition system was used to map the intent to a \"skill\" (similar to function calling), and these systems did not adapt based on new observations e.g., based on results of a previous skill [8]. With modern agents, people may express intents at a much higher level, and those intents may necessitate solutions that involve multiple steps of reasoning and tool use. Natural language and dialog can often be incomplete, convoluted, or ambiguous, making correct interpretation more challenging. These differences can increase the chance that modern agents might misunderstand what the users want the agents to achieve.\nOne important mechanism that could improve communication of goals from the user to the agent is detecting and resolving critical points of uncertainty in the agent's understanding of the goal [29]. For example, if the user asks a research assistant agent to see top papers by \"Peter Clark,\" there may be many potential authors with that name. Disambiguating this request may be seen as a process of finding common ground. This disambiguation might be made explicit in some cases, but risks overburdening the user (X3) [38]. Alternatively, the agent could disambiguate between alternate goals using available context (X4)-again, a type of common ground. For example, rather than asking which Peter Clark, the agent could choose the one in the same discipline as the user or the one the user has cited in the past."}, {"title": "What preferences should the agent respect?", "content": "For a given high-level, complex goal from the user, many possible ways to achieve the goal typically exist, but some plans are better than others. For example, the user likely prefers to minimize time, expense, and harmful side effects. However, different users will have different notions of harm and, hence, different preferences. Thus, understanding the user's preferences is a key component of common ground. How can the agent help the user clearly express their preferences, especially when they differ from standard norms?\nSteering agents to respect user preferences is related to efforts to align or fine-tune large language models toward specific behaviors, tasks, or domains. For example, techniques such as Direct Preference Optimization (DPO) were developed to steer LLM generations toward those that better align with user preferences and desired outcomes [57]. However, while effective, DPO primarily focuses on aligning models to aggregate-level preferences, which may overlook nuances in individual user needs or fail to generalize well to diverse or context-specific preferences.\nFor generative agents that can carry out complex actions in the world, ensuring they respect user preferences and constraints can be even more challenging. For example, suppose the user wants to constrain the agent to avoid harmful behavior. In open-world settings where generative agents can operate, it may be impossible to precisely define \"harm\" or list all possible harmful behaviors [20]. Further, generative models can discover novel solutions that contain previously unthought (or unspecified) harms [2]. Tool use further increases the scope of possible harm by allowing agents to impact the environment, making the communication challenge even more salient.\nOne possible solution to this challenge is to make inferred preferences and constraints visible or accessible to users. Some existing works use this approach; for example, the current version of ChatGPT displays a set of facts about the user it has memorized. It also allows users to modify (e.g., delete) these facts. These methods are good steps towards creating common ground."}, {"title": "What should the agent do differently next time?", "content": "Even if an agent develops an excellent understanding of the user's goal, preferences, and constraints, in practice, it is possible that the agent will still make mistakes. For example, it may continue to use sub-optimal plans or incorrect tools because its planning is imperfect. Further, it is also possible that the agent continues to make wrong assumptions about the goals and preferences perhaps because the user's tastes evolve. Agents will likely learn autonomously through mechanisms that enable them to invoke new tools, update memory, or finetune the underlying LLM [67], but we focus on scenarios where the user provides feedback.\nThis challenge is related to the problem of learning in recommender and interactive machine learning systems. For example, music and movie recommender systems may use implicit or explicit signals, such as dwell times or ratings, to improve future recommendations. Similarly, many end-user interactive machine learning systems focused on lightweight feedback mechanisms to help users steer classifier training and improve future predictions. Recent work on interactive prompt editing and refinement has also focused on enabling end users to improve prompts for LLM-based systems. There is also work in the classic agent literature which has focused on improving feedback from users to agents. For example, Russell and Grosof focused on learning (first-order logic-based) concepts from examples that can be accommodated into the agent's learning process [62].\nIn contrast to traditional machine learning systems, generative agents can revise their behavior based on natural language text. While this allows for rich feedback, it also introduces the challenge of appropriately interpreting free-text feedback. How can we help users effectively express feedback to steer agent behavior?"}, {"title": "Information Flow from the Agent to the User", "content": "In this section, we discuss five challenges related to the information flow from the agent to the user. These challenges focus on communicating information about the agent's capabilities, the agent's current and planned future actions, whether the user's goals that have been achieved, and any side effects that have occurred. We provide examples to illustrate each challenge and discuss potential solutions."}, {"title": "What can the agent do?", "content": "If a user does not fully understand the capabilities or limitations of an agent, they will not be able to make informed decisions about when and how to best use its assistance or what to expect when they do. Work with earlier AI systems has shown why addressing this challenge is crucial. As one example, Caiet [9] found that clinicians wanted clear information upfront about the basic features of AI models. They wanted to know what the models were good at, where they struggled, what perspectives they might have, and what they were designed to achieve. In the absence of such transparency about AI systems, people sometimes resort to informal experimentation to try and understand their capabilities and limitations [47], which may lead to incomplete or inaccurate mental models [19, 4].\nSeveral approaches have been proposed to help convey the capabilities and limitations of AI systems to people. Mitchell et al. [52] introduced the idea of model cards, short documents that accompany trained machine learning models. Among other things, a model card may include details about how the model was trained, intended or unintended uses, evaluation results (potentially broken down by demographic), and ethical considerations. Companies including Google, Hugging Face, and OpenAI have adopted model cards, and different, more interactive formats for model cards have been proposed [14]. However, a model card may be too long or technical to be practically useful for casual users, especially one intended to cover the range of behavior of a modern generative AI model. (For example, the model card for GPT-4 is over 60 pages long!7) For end users, considering lighter-weight solutions to introduce capabilities and limitations that are integrated into an agent's interface or behavior may be more appropriate than providing documentation of the agent or model it is built on, which the user must look up outside of interactions.\nMaking clear what a modern agent can and cannot do requires answering many questions beyond what we see for traditional classification or recognition systems, such as: Which information does the agent have access to? How will the agent use this information? Can the agent make permanent changes to the environment? Can the agent be interrupted without side effects? Or consider an agent that can write and run code. What languages and frameworks does the agent specialize in? Does it have access to the Internet? Can it use libraries that do not appear in the data used to train its underlying LLM, for instance, through RAG? Can it execute code that requires elevated privileges?\nWe note that since an agent's capabilities and limitations may be updated over time, conveying these to users is not something that can happen only once. It may become necessary to update the user on changes to what the agent can do over time [1]."}, {"title": "What is the agent about to do?", "content": "To achieve a given complex goal, an agent may execute a large number of actions step-by-step. Before executing those actions, the agent should obtain the user's permission, especially for the actions that are \"expensive\" in some sense (expend resources, are irreversible, might violate user preferences). When this is not the case, the agent should perhaps just go ahead, e.g., to minimize overload (X3). Suppose the agent doesn't communicate this crucial information. In that case, it might hurt common ground between the user and the agent because they would not have had the opportunity to provide feedback to the agent. This feedback would allow the agent to achieve the goal more successfully. It might be beneficial even that user input is required before the agent executes any action.\nThis challenge has become prominent with modern agents due to advances in generative and tool-using capabilities. For example, when agents built on foundation models with general capabilities execute complex plans involving sophisticated tool use"}]}