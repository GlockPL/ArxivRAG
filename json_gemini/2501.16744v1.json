{"title": "LLM Assisted Anomaly Detection Service for Site Reliability Engineers: Enhancing Cloud Infrastructure Resilience", "authors": ["Nimesh Jha", "Shuxin Lin", "Srideepika Jayaraman", "Kyle Frohling", "Christodoulos Constantinides", "Dhaval Patel"], "abstract": "This paper introduces a scalable Anomaly Detection Service with a generalizable API tailored for industrial time-series data, designed to assist Site Reliability Engineers (SREs) in managing cloud infrastructure. The service enables efficient anomaly detection in complex data streams, supporting proactive identification and resolution of issues. Furthermore, it presents an innovative approach to anomaly modeling in cloud infrastructure by utilizing Large Language Models (LLMs) to understand key components, their failure modes, and behaviors.\nA suite of algorithms for detecting anomalies is offered in univariate and multivariate time series data, including regression-based, mixture-model-based, and semi-supervised approaches. We provide insights into the usage patterns of the service, with over 500 users and 200,000 API calls in a year. The service has been successfully applied in various industrial settings, including IoT-based AI applications. We have also evaluated our system on public anomaly benchmarks to show its effectiveness.\nBy leveraging it, SREs can proactively identify potential issues before they escalate, reducing downtime and improving response times to incidents, ultimately enhancing the overall customer experience.\nWe plan to extend the system to include time series foundation models, enabling zero-shot anomaly detection capabilities.", "sections": [{"title": "Introduction", "content": "The increasing adoption of cloud computing has led to a surge in demand for stability and reliability, making the role of Site Reliability Engineers (SREs) more critical than ever. To ensure seamless operations, SREs (Beyer et al. 2016) require automation systems that can streamline their daily activities and provide real-time visibility into the performance and health of cloud infrastructure. This enables them to identify potential issues before they escalate, reducing downtime and improving response times to incidents.\nCurrently, SREs utilize advanced monitoring dashboards to proactively monitor cloud infrastructure, but still face challenges in detecting and preventing incidents. When a service or application experiences an outage, SREs are forced to raise incidents, which can lead to a negative perception of the service provider's ability to detect and prevent incidents. To address this, an automated issue detection service or dashboard is essential, providing SREs with real-time insights into cloud infrastructure performance and health.\nOur approach utilizes a Deep Learning-based Anomaly Detection approach to implement an example workflow, built on our publicly deployed Anomaly Detection Service API. This has significant advantages for cloud monitoring, enabling real-time identification of anomalies, thus improving reliability, optimizing performance, and ensuring compliance with service level agreements. This area has garnered significant attention from both professionals and researchers (Doelitzscher et al. 2013; Islam and Miranskyy 2020; Islam et al. 2021; Almazrawe and Musawi 2024).\nOur system is designed to provide a robust and scalable anomaly detection framework for cloud-based services, enabling the efficient identification of anomalies and ensuring the reliability of cloud infrastructure. We have developed a modular architecture that allows us to process large volumes of data efficiently and provide real-time insights into the health of cloud infrastructure, thereby facilitating prompt issue detection and resolution. We also explore the various personas who utilize our deployed system, including Site Reliability Engineers (SREs) and Cloud Services Developers. We highlight the features and functionalities of our system that cater to the specific needs of each persona, thus enhancing their productivity, efficiency, and overall user experience. Furthermore, we delve into the technical details of our underlying API, which powers our deployed system. This includes an overview of the API's design principles, implementation, and key features that enable real-time anomaly detection, predictive analytics, and data-driven decision-making. The API is deployed on the IBM Cloud infrastructure, which utilizes auto-scaling and load-balancing features to ensure efficient handling of dynamic workloads. Furthermore, the API's underlying architecture is designed to be data-agnostic, supporting a range of data types (i.e., univariate and multivariate time series, as well as tabular data) and enabling adaptability to various use cases. By providing a comprehensive understanding of our system's architecture, functionality, and technical details, we aim to demonstrate the effectiveness, reliability, and scala-"}, {"title": "LLM-Assisted Anomaly Modelling", "content": "Cloud computer scientists aim to develop anomaly models that capture anomalous behaviors of various components within the cloud infrastructure. To achieve this goal, we systematically exploited pre-trained Large Language Models (LLMs) to generate diverse failure modes, their corresponding metrics for monitoring, and associated behavioral patterns. We then mapped the derived knowledge to relevant variables from the dataset. To illustrate this mapping concept, we selected the standby generator, a common industrial asset utilized in hospitals and data centers, as a specific use case.\nThe workflow goes as follows:\n1. A pre-trained LLM like Llama (Touvron et al. 2023) or Mistral (Jiang et al. 2024) is prompted to identify the different components that exist in a cloud infrastructure\n2. For each component generated, it generates the different failure modes\n3. For each generated failure mode, it generates the metrics to monitor and what would constitute an anomalous behavior\n4. The LLM maps the generated knowledge of the failure modes, the relevant sensors and their behavior to the right cloud infrastructure metrics from our dataset\nA cloud infrastructure consists of multiple components and subcomponents that each of them requires monitoring."}, {"title": "Anomaly Detection System Workflow", "content": "In this section, we first present a deep learning-based anomaly detection system for cloud monitoring, as illustrated in Figure 1. The technical architecture outlines the flow of using Sysdig Metrics (Sysdig 2024) as input data, which is then processed by the Anomaly Detection API employing a DNN_AutoEncoder based deep learning model, known as ReconstructAD. Additionally, the Chi-Square Distribution is utilized as a statistical method to extract p-values as anomaly scores and threshold values, enabling the determination of anomaly labels.\n1. Data Capture The initial step in our system involves capturing data from IaaS (Infrastructure as a Service) multizone regions (MZR), which represent the cloud infrastructure resources, including virtual machines, network interfaces, and storage volumes. This data encompasses various metrics, such as:\n(a) Network traffic\n(b) Memory usage\n(c) Disk I/O\n(d) CPU utilization\n2. Data Store The collected data is then stored in Cloud Object Storage for future analysis through a dedicated pipeline. Another pipeline is utilized to feed the data into a pre-processing stage, where the data is processed to render it consumable by the Anomaly Detection API.\n3. Anomaly Detection Autoencoder-based time series representation is an interesting research problem (Amarbayasgalan et al. 2020; Zhang et al. 2019; Zong et al. 2018).Our approach, the ReconstructAD algorithm using the Anomaly Detection Service API, leverages a deep neural network DNN_AutoEncoder (Patel et al. 2022) based detection method to extract anomalies from the data. This approach identifies unusual patterns or behavior that may indicate a problem. Notably, a comparative analysis of different anomaly detection categories, including PredAD (an algorithm with a lookback window) and RelationshipAD (an algorithm that considers covariance between variables), revealed that ReconstructAD outperformed the others in terms of accuracy for the IaaS dataset. The algorithms PredAD, ReconstructAD, and RelationshipAD are accessible through the Anomaly Detection Service API.\nWe calculate the mean and standard deviation of the array of reconstruction errors generated by the ReconstructAD algorithm. Subsequently, we employ the Chi-Squared distribution to calculate the p-values, which are returned as an anomaly score for each anomaly. Finally, we utilize a threshold value to determine whether an anomaly is significant enough to be flagged for further investigation or action. This threshold can be adjusted based on the desired trade-off between false positives and false negatives, depending on the specific requirements of the organization. Principal Component Analysis (PCA) is employed to identify the most influential metrics contributing to anomalies in multivariate anomaly detection scenarios.\n4. System Visualization We designed and developed a Grafana (Grafana Labs 2024)-based dashboard for Site Reliability Engineers (SREs) to visualize and monitor anomalies in real-time, providing a centralized platform for incident detection and response. The Figures presented, Figures 3 and 4, provide valuable insights into the functionality of the anomaly detection system. They demonstrate its capability to identify spikes or anomalies and visually represent these on a dashboard in a tabular format. Figure 3 shows a screenshot of our Monitoring system, highlighting the CPU Usage Percent waveform with a potential spike. Figure 4 presents a Grafana dashboard screenshot of those spikes detected as anomalies in a VSI (Virtual Server Instance) at a specific timestamp, specifically illustrating the Sysdig metric ibm_is_instance_average_cpu_usage percentage. These visualizations enable users to quickly identify and respond to anomalies, thereby improving the overall efficiency and reliability of the cloud infrastructure."}, {"title": "Practical Scenarios: A Persona-Centric Approach", "content": "In this section, we explore how various personas within the operations and development teams can benefit from our anomaly detection system.\nOperations SREs (Site Reliability Engineers)\nCurrently, Operations SREs spend a significant amount of time manually monitoring various dashboards to identify is-"}, {"title": "Alert Developers", "content": "Our system provides precise anomaly notifications, which can be leveraged to fine-tune existing rule-based alert systems(Jagannathan et al. 2023). For instance, a rule-based alert might trigger if CPU utilization exceeds 80%. However, our system can detect anomalies even if the utilization spikes from 60% to 75% briefly and then drops back to 60%, which would not trigger the rule-based alert. This enables alert developers to adjust thresholds intelligently and create more effective alerting mechanisms, thereby improving the overall monitoring strategy."}, {"title": "Service SREs (Site Reliability Engineers)", "content": "Service SREs can utilize our anomaly detection system to monitor the performance and reliability of micro-services running in containers. By identifying performance bottlenecks and potential failures early, they can maintain service quality and improve the user experience. The system also facilitates proactive decision-making to mitigate issues that might not be captured using traditional rule-based alerting systems."}, {"title": "The Anomaly Detection Service API", "content": "The anomaly detection model employed by our deployed system for SREs leverages our Anomaly Detection Service API, which we will detail in this section. Figure 5 illustrates the anomaly detection service.\nThis service is an early attempt to onboard end user to use the time series anomaly detection API to explore the range of anomaly detection algorithms and obtain feedback. We explore anomaly detection as it is a application in many data-driven real-world applications such as IoT monitoring, web of things, asset management, and more, where there are rare ocurrences of failures where the behavior is out of the norm. We focus on time-series as it is the most commonly supported data format generated from sensors. By exposing a range of anomaly detection algorithms via web APIs, we achieve three key objectives:\n1. Simplify the construction and execution of various types of anomaly pipelines/workflows\n2. Protect the Intellectual Property (IP) related to core algorithm implementation by hiding the details of the code via a service\n3. Auto-scale the system with a pay-as-you-go model, and enable the user with provisioning of the right resources\nThe web based API Service discussed in this paper is deployed on a public cloud for a general audience (trial subscription only). We carefully designed an API for explor-"}, {"title": "Anomaly Detection Service Cheat Sheet", "content": "As illustrated in Figure 5, we have published five API endpoints for anomaly detection tasks at the time of writing this paper. These endpoints cater to a broad range of"}, {"title": "Application of Anomaly Detection in Cloud Monitoring and API Usage", "content": "The API has been widely used by data scientists, business users, independent service vendors as well as researchers. Between Jan-2022 to till dates, 500k+ API calls are being made, with at-least 200 calls on a monthly basis. We have excluded any API call originating from the author of this paper as well as the service call from non-trial subscriptions. We now discuss an application build using service for monitoring cloud resources."}, {"title": "Conclusion", "content": "This paper introduces a scalable and generalizable Anomaly Detection system deployed on the cloud, which utilizes our Anomaly Detection API to augment the role of Site Reliability Engineers (SREs) in cloud infrastructure management. The system provides a thorough overview of anomaly detection capabilities, encompassing algorithms, decision functions, REST API design, and practical applications. We also discuss how our Large Language Model (LLM)-assisted anomaly modeling approach effectively captures anomalous behaviors of various cloud infrastructure components by systematically leveraging pre-trained LLMs. This approach demonstrates its potential in enhancing cloud infrastructure resilience, reducing downtime, and facilitating root cause analysis. By utilizing our service, SREs can proactively identify potential issues before they escalate, thereby reducing downtime and improving response times to incidents, ultimately leading to an enhanced overall customer experience. The deep learning-based Anomaly Detection pipeline presented in this paper is currently being explored for the utilization of time series foundation models, with a focus on leveraging the zero-shot capability of time series forecasting models. Through a comprehensive benchmark analysis, we demonstrate that our models are competitive with state-of-the-art results on several datasets."}, {"title": "Appendix", "content": "Example:\n\u2022 Server Failure Modes\nPower supply failure\n* Behavior: Sudden spikes or sustained high power usage, indicating potential power supply issues or hardware failure\nFirmware or software corruption\n* Behavior: Sudden spikes or sustained high CPU usage, indicating potential resource starvation or software issues\n\u2022 Database Failure Modes\nToo many Database Connections\n* Behavior: Sudden spikes or sustained high database connections, indicating potential resource starvation or software issues\nToo many Database Queries\n* Behavior: Sudden spikes or sustained high database queries, indicating potential resource starvation or software issues"}]}