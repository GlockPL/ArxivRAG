{"title": "Automatic Quality Assessment of First Trimester Crown-Rump-Length Ultrasound Images", "authors": ["Sevim Cengiz", "Ibraheem Hamdi", "Mohammad Yaqub"], "abstract": "Fetal gestational age (GA) is vital clinical information that is estimated during pregnancy in order to assess fetal growth. This is usually performed by measuring the crown-rump-length (CRL) on an ultrasound image in the Dating scan which is then correlated with fetal age and growth trajectory. A major issue when performing the CRL measurement is ensuring that the image is acquired at the correct view, otherwise it could be misleading. Although clinical guidelines specify the criteria for the correct CRL view, sonographers may not regularly adhere to such rules. In this paper, we propose a new deep learning-based solution that is able to verify the adherence of a CRL image to clinical guidelines in order to assess image quality and facilitate accurate estimation of GA. We first segment out important fetal structures then use the localized structures to perform a clinically-guided mapping that verifies the adherence of criteria. The segmentation method combines the benefits of Convolutional Neural Network (CNN) and the Vision Transformer (ViT) to segment fetal structures in ultrasound images and localize important fetal landmarks. For segmentation purposes, we compare our proposed work with UNet and show that our CNN/ViT-based method outperforms an optimized version of UNet. Furthermore, we compare the output of the mapping with classification CNNs when assessing the clinical criteria and the overall acceptability of CRL images. We show that the proposed mapping is not only explainable but also more accurate than the best performing classification CNNs.", "sections": [{"title": "1 Introduction", "content": "Fetal ultrasound is important to help sonographers assess fetal growth and check for abnormalities. A Dating scan is typically performed between 11 - 13 weeks of gestation. During this scan, a crown-rump-length (CRL) view image must be acquired to measure the fetal length, which is taken from the top of the head to the bottom of the rump. CRL is a critical biometric measurement that can be used to calculate GA. Precise CRL measurement is key for detecting potential fetal anomalies such as small for gestational age (SGA) and large for gestational age (LGA) where small or large indicate deviations from normal fetal size with respect to the number of weeks of the pregnancy. To perform the correct measurement, the CRL view must satisfy the clinical guidelines listed and described in Table 1 [1,2,3]. However, acquiring the correct fetal ultrasound view is a challenging task due to different factors such as sonographer experience, fetal position, and maternal characteristics [4]. In the CRL view, the fetus might be at a hyperextended which represents the extensive gap between the chin and chest, or hypoflexed position which represents the narrow gap between the chin and chest, leading to an incorrect CRL measurement and consequently an incorrect GA estimation. In addition, since quality of the acquired ultrasound is highly dependent on sonographer ability, there is no guarantee that the acquired images adhere to the clinical guidelines. For instance, authors in [5] suggested that a large number of fetal ultrasound anomaly scans do not adhere to clinical guidelines when a large set of scans in a well-established maternity unit were manually assessed. Manual auditing to check the adherence of acquired CRL images to clinical guidelines could be performed to alleviate this issue but it is not real-time and lacks instant feedback to the sonographer. Since manual auditing is expensive, time-consuming, and typically performed on a small percentage of scans [21], it is necessary to find an efficient solution for such a challenging problem.\nFetal ultrasound image analysis is gaining a lot of interest. Most of the published work for Dating scan image analysis focused on the placenta. For instance, Yang et al. [6] conducted a study about a fully automatic framework to segment various anatomical structures such as fetus, gestational sac, and placenta. Similarly, Zimmer et al. [7] used CNN to extract and segment the entire placenta and estimate its volume during the last trimester of pregnancy using ultrasound images. An improved version of this solution which incorporates the placental position to improve segmentation accuracy was proposed [8]. Looney et al. [9] introduced OxNNet model to segment the placenta automatically. Recently, OxNNet was improved to segment amniotic fluid and fetus as well using a multi class CNN [10].\nLittle work has been published to-date on fetal CRL segmentation and assessment. Ryou et al. [12] has developed a 3D CNN which is able to segment fetal structures at CRL view and perform the CRL measurement. Although the authors analyzed 3D ultrasound, the generated CRL view was assumed to adhere to the clinical guidelines. Similarly, Cengiz et al. [13] proposed a CNN-based method to segment the body and head of the fetus from ultrasound images in order to measure the CRL and subsequently estimate the gestational age (GA). These authors have also assumed that the images were acquired at the correct CRL view, which is not necessarily correct unless verified.\nTransformer is a unique architecture that has inspired a lot of interest in the computer vision (CV) community despite being initially built for sequence-to-"}, {"title": "2 Materials and Methods", "content": "2.1 Dataset\n696 fetal CRL ultrasound images from the first trimester were extracted out of the hospital archive, excluding twin pregnancies. An expert segmented 4 different structures using an annotation tool (ITK-Snap [20]). These structures are the head, body, fetal palate, and the gap between the chin and chest. After manual segmentation, another experienced researcher examined all the segmentation masks and made corrections if needed.\nSome fetuses were not scanned in a neutral position, meaning they were not at the mid-sagittal plane, resulting in loss of view of the fetal palate. In these cases, the image does not have a segmented region representing the fetal palate. Similarly, if the fetus was at a hyperflexed (over-stretched) or hypoflexed (under-stretched) position, there is either a large or little to no gap present between the"}, {"title": "2.2 Localization of Fetal Structures", "content": "Although UNet [16] has shown great success in medical image segmentation,\nVision Transformer [17] has shown successful results in some natural image seg-\nmentation problems [18]. CNNs are typically good at capturing features in local\nneighborhood, while ViT models are able to capture a global representation of"}, {"title": "2.3 Criteria Assessment for Optimal CRL Plane", "content": "The Fetal Anomaly Screening Programme (FASP) guideline [2,3] recommends a set of criteria to be checked before CRL measurement is taken to ensure that the image is acquired at a correct view (see Table 1). In this AI-based heuristic method, TransUNet and UNet are implemented to produce segmented fetal body structures from ultrasound images, and then a mapping is developed to evaluate the adherence to clinical criteria by providing explainable results on those masks. Neutral position, horizontal orientation, fetal palate, magnification, fetal face direction, left and right caliper definitions are the criteria investigated by considering the geometrical and analytical aspects while creating a spreadsheet to represent the criteria acceptance with 1s and 0s (step by step evaluation details for each criterion are given in Table 1). More details are given in the supplementary video. The overall image plane acceptance for the CRL measurement was then determined by checking whether the total score is more than 3 out of 7. [15]. The spreadsheets were then used to calculate accuracy, recall, precision, and F1 scores for both methods. GitHub repository of the proposed method will be public after the acceptance of the paper."}, {"title": "2.4 Experiments", "content": "To compare our solution with CNN-based networks, DenseNet-121 and ResNet-34 were adopted to classify each criterion directly from the fetal ultrasound images as well as the clinical acceptance of the CRL measurement. The spreadsheet with the predicted labels for each criterion was compared to the results of the proposed heuristic model and the accuracy, recall, precision, and F1 scores were calculated. Both models used in our experiments were pretrained on ImageNet to speed up convergence. In addition, weighted cross-entropy loss and Adam optimizer as well as and 3-fold cross validation were used to optimize results. After experimenting with different combinations of hyperparameters, a learning rate of 0.003 and 0.01, batch size of 32, and 24, and image size of 256 were chosen for UNet and TransUNet, respectively. The models were trained for 100 epochs."}, {"title": "3 Results", "content": "Figure ?? shows the examples of clinically accepted (the first row), and rejected (the second row) CRL images for each criterion. The third row shows how deep learning and mapping-based image assessment for CRL measurement are done bu using the segmentation masks at each criterion.\nAccording to the spreadsheet manually prepared by the expert, the acceptance ratios of the neutral position, palate existence, magnification, the left definition of the caliper, the right definition of the caliper, fetal face direction, and horizontal angle were 0.64, 0.51, 0.75, 0.77, 0.76, 0.94, and 0.8,1 respectively. Table 3 shows the accuracy, precision, recall, and F1 scores of DenseNet-121, ResNet-34, AI-based proposed method with UNet and TransUNet for the 7 clinical criteria as well as the acceptance of CRL measurement. Although both classifications and AI-based heuristic methods have their highest scores in horizontal orientation, magnification, fetal face direction, and acceptance of CRL"}, {"title": "4 Discussion", "content": "Results presented in Table 2 show that Fetal TransUNet had higher dice, jaccard, and recall scores for the fetal body structure segmentation including head, body, and fetal palate compared to UNet. In addition, the segmentation standard deviation of Fetal TransUNet is relatively smaller than UNet which suggests that our proposed method has a more consistent segmentation (i.e., fetal TranUNet is less likely to over- or under-segment).\nNo comparison was performed between the proposed Fetal TransUNet to a classical ViT since the original TransUNet paper [19] has performed extensive evaluation against Vit, ViT with CNN models. Results suggest that TransUNet outperforms ViT and ViT with a CNN.\nIn the table 3, we showed that our proposed method with TransUNet has higher the F1 score compared to others, which is the most representative metric, among four out of seven criteria. However, our method with TransUnet has a slightly lower accuracy, recall, precision and F1 score than the DenseNet in terms of acceptance for CRL. On the other hand, the proposed model has great potential because it provides a reason for the output, which may assist in making a specific decision rather than the black-box approach used by CNNs. This heuristic method may contribute positively during the evaluation of an ultrasound image and result in a higher rate of acceptance for GA estimation.\nHorizontal orientation, magnification, fetal face direction, left and right definition of the caliper, as well as image acceptability for CRL measurement, were detected with high accuracy. However, the evaluation of the fetal palate and neutral positioning was more challenging considering the complexity of the images due to fetus movement, ambiguity of the region, and brightness and contrast of the ultrasound images. In many scans, it was extremely difficult to visually determine neutral positioning even for a trained eye, hence it was not surprising that all methods produced their lowest accuracy in this criteria. Similarly, the presence of fetal palate in some scans was visually confusing since it blended with the chin or face, which resulted in lower evaluation scores.\nThe novelty of this paper compared to the study conducted by [13] is that not only the network is composed of ViT+UNet but also creating from scratch a mapping process of the fetal segmentation images to a representative vector that shows the acceptability of the CRL image. In the mapping section, segmented masks were investigated taking into consideration the geometrical and analytical rules to verify and classify that it complies with a set of clinical criteria. As a result, instead of building traditional CNN classification models that would automatically record the predicted labels, the heuristic method combines the Fetal TransUNet with the mapping process that is the analytical assessment with geometrical aspects on output masks, showing an explainable and sensible method. Therefore, we believe this is the better solution for clinical use since the combined method allows us to investigate the sonographers assessment for the CRL measurement by making sure that the criteria guidelines are adhered to as well as the reason behind a specific decision rather than the black box approach of CNNs which is not capable of explaining that. This heuristic method may contribute positively during the assessment of an ultrasound image and result in a higher rate of acceptance for GA estimation."}, {"title": "5 Conclusion", "content": "In this paper, we developed an AI-based heuristic method that segments the fetal ultrasound images and has a mapping process that is able to analyze the segmented images and its adherence to clinical criteria. We conclude that the use of a CNN to assess whether a CRL image adheres to a clinical criterion is not accurate nor explainable. On the other hand, our proposed method has a better accuracy and provides a reasonable explanation for each criterion when assessing the CRL image since it relies on localizing important structures (mimicking sonographer's assessment).\nThe main limitation of this work is the small sized dataset. Therefore future work requires evaluation on a larger dataset. In addition, a major challenge we"}]}