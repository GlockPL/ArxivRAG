{"title": "Utilizing Human Behavior Modeling to Manipulate\nExplanations in AI-Assisted Decision Making:\nThe Good, the Bad, and the Scary", "authors": ["Zhuoyan Li", "Ming Yin"], "abstract": "Recent advances in AI models have increased the integration of AI-based decision\naids into the human decision making process. To fully unlock the potential of AI-\nassisted decision making, researchers have computationally modeled how humans\nincorporate Al recommendations into their final decisions, and utilized these models\nto improve human-AI team performance. Meanwhile, due to the \u201cblack-box\u201d nature\nof AI models, providing AI explanations to human decision makers to help them\nrely on AI recommendations more appropriately has become a common practice. In\nthis paper, we explore whether we can quantitatively model how humans integrate\nboth AI recommendations and explanations into their decision process, and whether\nthis quantitative understanding of human behavior from the learned model can\nbe utilized to manipulate AI explanations, thereby nudging individuals towards\nmaking targeted decisions. Our extensive human experiments across various tasks\ndemonstrate that human behavior can be easily influenced by these manipulated\nexplanations towards targeted outcomes, regardless of the intent being adversarial\nor benign. Furthermore, individuals often fail to detect any anomalies in these\nexplanations, despite their decisions being affected by them.", "sections": [{"title": "Introduction", "content": "Recent advances in AI models have significantly increased the integration of AI-based decision aids\ninto the human decision making process. The widespread adoption of such AI-based decision aids has\nopened up a new paradigm of human-AI collaboration\u2013the AI model provides recommendations for\na given decision making task, while human decision makers are responsible for making the final\ndecisions. To fully unlock the potential of AI-based decision aids in enhancing human decision\nmaking, a few studies [3, 24, 18, 52, 41] have developed computational models to capture how humans\nfactor AI recommendations into their decision-making process, and explored how these behavioral\nmodels can be utilized to improve human-AI team performance. For example, Vodrahalli et al. [53]\ndeveloped a human behavior model to characterize the impact of AI predictions and confidence levels\non human final decisions. This model was then utilized to adjust the model confidence displayed to\npeople, with the objective of calibrating human trust in AI assistance.\nMeanwhile, the black-box nature of prevalent AI models has driven a greater integration of model\nexplanations, generated through various explainable AI (XAI) methods [46, 37, 28, 29, 1], into AI-\nassisted decision making. These explanations seek to provide some insights into the the underlying\ndecision rationales of AI models, assisting humans in evaluating the reliability of AI decisions\nand identifying the optimal strategies to rely on AI recommendations. However, many empirical\nstudies [4, 5, 56, 15, 44, 7, 2], which evaluate the effectiveness of current XAI methods for improving"}, {"title": "Related Work", "content": "There has been a surge of interests among researchers recently in computationally modeling human\nbehavior in AI-assisted decision making [3, 24, 18, 52, 41, 12, 36, 33, 32]. The goals of these studies\nfor modeling human behavior are diverse, encompassing improving human-AI team performance\nthrough intelligent interventions or model recommendation adjustments [53, 38, 6, 19, 40, 39],\ndeciding when to present AI powered code suggestion in programming [42], evaluating the utility\nof AI explanations to improve user understanding of AI model behavior [9, 11, 8], and deploying\nadversarial attacks on AI models to reduce human trust [35]. In this paper, we take a more holistic\nview and explore whether we can model how humans integrate both AI recommendations and\nexplanations into their decision making process, and what the implications of such behavior modeling\nare."}, {"title": "Human-centered Evaluation of AI Explanations", "content": "With the increasing use of AI technologies as decision aids for humans, a variety of explainable\nAl techniques have been developed to increase the interpretability of AI models [46, 37, 28, 29, 1,\n30, 8, 49, 48]. To understand the effectiveness of these explanation methods, a growing body of\nempirical human-centered evaluations have been conducted to examine how AI explanations would\naffect the ways that humans perceive and interact with AI models [43, 9, 11, 4, 5, 56, 15, 44, 7, 2, 47].\nThese evaluations look into various aspects of impacts of AI explanations, such as the influence\non people's trust and reliance on the AI model [56, 5, 54], understanding of AI model [44, 10, 54],\nand the collaboration performance of the human-AI team [4, 34, 25]. Recently, some research has\nexplored the modification of AI explanations to influence human behavior. For example, Lakkaraju\nand Bastani [27] demonstrated that handcrafting modifications in AI explanations\u2014such as hiding\nsensitive features\u2014can mislead human trust in AI models. Another study [26] found that aligning\nAI explanations with humans' own decision rationales can increase agreement between human\ndecisions and the AI model's predictions. Different from previous work which required intensive\nhandcrafting of AI explanations, in this paper, we explore whether it is possible to directly exploit the"}, {"title": "Methodology", "content": null}, {"title": "Problem Formulation", "content": "In this study, we explore the scenario of human-AI collaboration within the context of AI-assisted\ndecision making, and we now formally describe it. Consider a decision making task represented by a\nn-dimension feature vector x \u2208 R, and y is the correct decision to make in this task. Specifically,\nin this study, we focus on decision making tasks with binary choices of decisions, i.e., y \u2208 {\u22121,1}.\nThe AI model's recommendation on the decision task is represented as y_m = M(x), y_m \u2208 {\u22121,1}.\nFollowing the explainable AI methods like LIME[46] or SHAP[37], the AI model could also provide\nsome \"explanations\u201d of its decision, e = E(M(x)), e \u2208 R^n, by showing the contributions of each\nfeature to the decision. With all these information, the human decision maker (DM) needs to make\nthe final decision y_h \u2208 {\u22121,1} by either accepting or rejecting AI model's decision recommendation\ny_m, which can be characterized by y_h = H(x,y_m, e). The goal of our study is to explore whether\nwe can quantitatively model such decision making process\u2014specifically, H(x, y_m, e)\u2014and whether\nthis quantitative understanding of human behavior can be utilized to adjust AI explanations (i.e.,\nchange e to e') without accessing to the original AI model M(\u00b7), thereby nudging human DMs to\nmake the targeted decision \u0177_h \u2208 {\u22121,1}, denoted as \u0177_h = H(x, y_m, e')."}, {"title": "Modeling Human Behavior in AI-assisted Decision Making", "content": "We first build computational models to characterize how humans integrate both AI recommendations\nand explanations into their decision process. Following previous works on modeling human behavior\nin different scenarios of AI-assisted decision making [9, 33], we adopted a two-layer neural network\nas the structure for modeling the human decision in this study:\ny_h = H_{w_n} (x, y_m, e) = H_{w_n} ([x, y_m, e, x_e])\n(1)\nThe inputs to the behavior model include the task features x, the AI model's prediction y_m, the AI\nexplanation e, and the interaction term between the task features and the AI explanation x_e that\nreflects how humans may redirect their attention to the corresponding features highlighted by the\nAI explanation. Given the human behavior dataset D = {x_i, y_m, e_i, y_h^i}_{i=1}^{S}, we can employ the\nmaximum log-likelihood estimation to learn the behavior model H_{w_n}."}, {"title": "Manipulating AI Explanations through the Behavior Model", "content": "We next proceed to explore how the quantitative understanding of human behavior from H_{w_n} can\nbe utilized to manipulate AI explanations. In particular, given the targeted decision \u0177_h for the task\ninstance x, we want to identify a new AI explanation e' that maximizes the likelihood that human\nDMs make the targeted decision \u0177_h according to the learned behavior model H_{w_n}. In addition, to\nprevent the case where the manipulated explanations e' has a very low level of fidelity [29], such as\nsuggesting a recommendation that is inconsistent with the AI model's prediction y_m, we also impose\na constraint that the new explanation e' should still support the original AI recommendation y_m.\nSince we assume no access to the original AI model, we define L_{consistency} (e, y_m) as a measurement\nof agreement consistency between the manipulated AI explanations and the AI recommendation:\nL_{consistency} (e, y_m) = \\begin{cases} 0 & \\text{if sign} (\\sum_i e_i) = \\text{sign}(y_m), \\\\ 1 & \\text{otherwise}. \\end{cases}\n(2)\nTogether, we use the following optimization problem to manipulate AI explanations:\n\\text{arg} \\min_{e' \\in R^n} L_{behavior} (H_{w_n} (x, y_m, e'), \u0177_h), \\text{subj. to} L_{consistency} (e', y_m) \u2264 0\n(3)\nwhere L_{behavior} is defined as the cross entropy function. Since exactly solving the above optimization\nproblem is intractable, we used the gradient-based optimization to approximate it:\ne'_{g_t+1} = e'_{g_t} - \u03b7_{e'} (L_{behavior} (H_{w_n} (x, y_m, e'), \u0177_h) + \u03bb L_{consistency} (e'_{g_t}, y_m))\n(4)\nwhere \u03b7 is the step size, \u03bb is the trade-off parameter, and e' represents the parameterized explanations\nin the optimization process. We can iteratively optimize manipulated explanations e' until L_{behavior} is\nsmaller than a threshold \u03c4 or reach the maximum number of rounds T."}, {"title": "Human Behavior Model Learning", "content": "To develop the human behavior model for manipulating AI explanations, we first conduct a human\nsubject experiment to collect human behavior data."}, {"title": "Decision Making Task and AI Assistance", "content": "We consider four decision making tasks in this study:\n\u2022 Census Prediction (Tabular Data) [23]: This task was to determine a person's annual\nincome level. In each task, the human DM was presented with a profile with 7 features,\nincluding the person's gender, age, education level, martial status, occupation, work type,\nand working hour per week. The subject was asked to decide whether this person's annual\nincome is higher or lower than $50k for each task. We trained a random forest model to\nmake the income prediction, and the accuracy of the AI model was 76%.\n\u2022 Recidivism Prediction (Tabular Data) [16]: This task was to determine a person's recidi-\nvism risk. In each task, the human DM was presented with a profile with 8 features, including\ntheir basic demographics (e.g., gender, age, race), criminal history (e.g., the count of prior\nnon-juvenile crimes, juvenile misdemeanor crimes, juvenile felony crimes committed), and\ninformation related to their current charge (e.g., charge issue, charge degree). The subject\nwas asked to decide whether this person would reoffend within two years. We trained a\nrandom forest model to make the prediction, and the accuracy of the AI model was 62%.\n\u2022 Bias Detection (Text Data) [50]: In this task, the human DM was presented with a text\nsnippet and needed to decide whether it contained any bias. We fine-tuned a BERT [13]\nmodel to identify bias in the snippet, and the accuracy of the AI model is 79%.\n\u2022 Toxicity Detection (Text Data) [20]: In this task, the human DM was presented with a text\nsnippet and needed to decide whether it contained any toxic content. We fine-tuned a BERT\nmodel to identify the toxic content, and the accuracy of the AI model is 86%.\nTo understand how people respond to various AI explanations, we employed LIME and SHAP to\nexplain the predictions made by the AI model. Additionally, we augment the LIME or SHAP expla-\nnations by either randomly masking out contributions from some features or amplifying contributions\nof some features (referred to as the \"Augmented\" explanations) to see how humans react to them.\nThese explanations are provided with AI recommendations together to humans in decision making."}, {"title": "Experimental Procedure", "content": "We posted our data collection study on the Prolific \u00b9 to recruit human participants. Upon arrival,\nwe randomly assigned each participant to one of the four decision making tasks and they needed to\nfill in an initial survey to report their demographic information and their knowledge of AI models\nand explanations. Participants started the study by completing a tutorial that described the decision\nmaking task that they needed to work on. To familiarize participants with the task, we initially asked\nthem to complete five tasks independently without AI assistance. During these training tasks, we\nimmediately provided the correct answer at the end of the task. After the completion of training tasks,\nparticipants moved on to the formal tasks. In the formal tasks, participants would receive one type\nof AI explanations among SHAP, LIME, or Augmented. Specifically, each participant was asked\nto complete a total of 15 tasks. In each task, participants were provided with the AI prediction and\nthe explanations along with the task instance. They were then required to make their final decisions.\nFinally, participants were required to complete an exit survey to report their perceptions of the AI\nexplanations they received during the study. They were asked to rate the alignment of AI explanations\nwith their own rationale, as well as the usefulness, transparency, comprehensibility, satisfaction with\nthe provided explanations, and their trust in the AI models, on a 5-point Likert scale. We offered a\nbase payment of $1.2 and a potential bonus of $1 if the participant's accuracy is above 85%. The\nstudy was open to US-based workers only, and each worker can complete the study once."}, {"title": "Training Results", "content": "After collecting data on human behavior, we developed human behavior models for each type of task.\nFor the human behavior models for two textual tasks-Toxicity Detection and Bias Detection-we\nemployed the pretrained BERT encoder to extract features from the original sentences, which were\nthen used as the task feature \u00e6 in the human behavior model H_{w_n}. We optimized these behavior\nmodels using Adam [22] with an initial learning rate of le 4 and a batchsize of each training\niteration of 128. The number of training epochs is set as 10. \nWe observed that the average accuracy of all human behavior models\nexceeds 0.65, which is considered to be reasonable. Consequently, we utilized these learned human\nbehavior models to manipulate AI explanations in the following evaluations."}, {"title": "Evaluation I: Manipulating AI Explanations for Adversarial Purposes", "content": "In our first evaluation, we adopted the role of an adversarial party to explore whether they could\nutilize the learned human behavior model to manipulate AI explanations. The manipulation goal was\nto nudge human DMs to be biased against certain protected groups in the decision making process.\nWe are particularly interested in comparing the fairness level of human decision outcomes between\nhuman DMs who receive original explanations, such as SHAP or LIME, and those who receive\nmanipulated explanations. Notably, all human DMs are provided with the same AI predictions for\nthe same decision making task. Additionally, we also explore differences in human perceptions of\noriginal AI explanations versus manipulated AI explanations."}, {"title": "Evaluation Metrics and Manipulating AI Explanations.", "content": "Following previous work [17, 14], we\nused the false positive rate difference (i.e., FPRD) and the false negative rate difference (i.e., FNRD)\nto measure the fairness level of human decision outcomes-the closer these values are to zero, the\nmore fair the decisions are. To manipulate AI explanations and nudge human DMs toward biasing\nagainst certain protected groups, we define the targeted human decision \u0177_h for each task as follows:\n\u2022 Census Prediction: In this task, we considered a person's sex as the protected attribute.\nThe targeted human decision is defined as \u0177_h = 1 (indicating a person's annual income\nexceeds $50K) when \u00e6_{sex} = \\text{male}, and \u0177_h = \u22121 (indicating a person's annual income\ndoes not exceed $50K) when x_{sex} = \\text{female}. The fairness metrics can be computed as\nFPRD = FPR_{female} - FPR_{male}, and FNRD = FNR_{female} - FNR_{male}.\n\u2022 Recidivism Prediction: In this task, we considered the defendant's race as the protected\nattribute. The targeted human decision is defined as \u0177_h = 1 (indicating the defendant will\nreoffend) when \u00e6race = \\text{black}, and \u0177_h = \u22121 (indicating the defendant will not reoffend)\nwhen x_{race} = \\text{white}. The two fairness metrics can be computed as FPRD = FPR_{white}\nFPR_{black}, and FNRD = FNR_{white} - FNR_{black}.\n\u2022 Bias Detection: In this task, we divided text snippets into groups based on their political\nleaning. The targeted human decision is defined as \u0177_h = 1 (indicating the text is biased)\nwhen \u00e6_{leaning} = \\text{democratic}, and \u0177_h = \u22121 (indicating the text is not biased) when \u00e6_{leaning} =\n\\text{republican}. The fairness metrics can be computed as FPRD = FPR_{rep} \u2013 FPR_{dem}, and\nFNRD = FNR_{rep} - FNR_{dem}.\n\u2022 Toxicity Detection: In this task, we divided text snippets into groups based on the victim\nof the text. The targeted human decision is defined as \u0177_h = 1 (indicating the text is toxic)\nwhen \u00e6_{victim} = \\text{white}, and \u0177_h = \u22121 (indicating the text is non-toxic) when \u00e6_{victim} = \\text{black}.\nThe two fairness metrics can be computed as FPRD = FPR_{black} - FPR_{white}, and FNRD =\nFNR_{black} - FNR_{white}."}, {"title": "How do the adversarially manipulated explanations affect fairness level of human\ndecisions?", "content": "The fairness levels of participants' decision outcomes under the manipulated explanation, SHAP ex-\nplanation, and LIME explanation are presented in Figure 1. Visually, it appears that when human DMs\nare provided with manipulated explanations, both FPRD and FNRD scores of their decision outcomes\ntend to deviate more from zero compared to when DMs receive SHAP or LIME explanations.\nTo examine whether these differences are statistically significant, we conducted regression analyses.\nSpecifically, the focal independent variable was the type of explanation received by participants, while\nthe dependent variables were the participants' FPRD and FNRD scores. To minimize the impact of\npotential confounding variables, we included a set of covariates in our regression models, such as\nparticipants' demographic background (e.g., age, race, gender, education level), their knowledge of AI\nexplanations, their trust in AI models, and the FPRD or FNRD scores of the AI model decisions they\nreceived in the study. These covariates were selected based on prior HCI research [25, 56, 54] which\nempirically reveal how characteristics of human DMs may moderate the impacts of AI explanations\non human decisions in AI-assisted decision making.\nOur regression results indicate that the adversarial party can significantly increase the level of\nunfairness in human decision outcomes with manipulated explanations through human behavior\nmodeling. Specifically, when examining FPRD, we found that participants who received manipulated\nAI explanations made more unfair decisions compared to those who received SHAP or LIME\nexplanations (p < 0.05) in the Census and Recidivism tasks. The difference was marginally\nsignificant (p < 0.1) in the Toxicity task. When examining FNRD, results show that participants who\nreceived manipulated explanations made decisions that were significantly more unfair than those who\nreceived SHAP or LIME explanations (p < 0.01) in the Bias task."}, {"title": "How do humans perceive the adversarially manipulated AI explanations?", "content": "In Section 5.1, we found that the adversarial party can manipulate AI explanations to nudge human\nDMs toward making more unfair decisions compared to those who received the original AI explana-"}, {"title": "Evaluation II: Manipulating AI Explanations for Benign Purposes", "content": "In the previous section, we found that the adversarial party could use the behavior model to manipulate\nAI explanations, thereby misleading humans into making unfavorable decisions against specific\ngroups. Naturally, one might wonder could a third party also use behavior models to manipulate AI\nexplanations for benign purposes, such as promoting more appropriate human reliance on AI models?\nFor instance, can manipulated AI explanations lead humans to reject AI recommendations when the\nAl model decision is likely incorrect, and encourage acceptance when the decision is likely correct?\nWe aim to explore the answers to this question in this section."}, {"title": "Can benignly manipulated explanations promote appropriate reliance of human DMs on\nAI models?", "content": "Figures 3a, 3b, and 3c compare the average accuracy, overreliance, and underreliance of human\ndecision outcomes under manipulated, SHAP, and LIME explanations, respectively. It is clear that\nproviding human DMs with manipulated AI explanations leads to an increase in the accuracy of their\ndecision outcomes for most of tasks. We subsequently conducted regression analyses to determine\nwhether these differences are statistically significant. The regression models incorporated a set of\ncovariates, including participants' demographic backgrounds (e.g., age, race, gender, education level),\ntheir knowledge of AI explanations, their trust in AI models, and the accuracy of the AI models. The\nregression results indicate that in the Census, Recidivism, and Bias tasks, substituting SHAP or LIME\nexplanations with manipulated explanations significantly improves the accuracy of human-AI team."}, {"title": "How do humans perceive benignly manipulated AI explanations?", "content": "In Section 5.2, we observed that it is challenging for humans to detect abnormalities in the adversari-\nally manipulated explanations, even though they are unconsciously influenced by the manipulated\nexplanations to make more unfair decisions. In this section, we revisit this question to investigate into\nwhether humans' perceptions of the manipulated explanations change, when they are manipulated\nfor benign purposes.   Regression analyses reveal no statistically significant differences among the perceived transparency\nand usefulness of these three types of explanations. Similar trends were observed for other perceptual\naspects of explanations, including perceived alignment, comprehensibility, satisfaction, and trust."}, {"title": "Conclusion and Limitations", "content": "In this paper, we explore whether we can quantitatively model how humans incorporate both AI\nrecommendations and explanations into their decision making process, and whether we can utilize\nthe quantitative understanding of human behavior obtained from these learned models to manipulate\nAI explanations for both adversarial and benign purposes. Our extensive experiments across various\ntasks demonstrate that human behavior can be easily influenced by these manipulated explanations\ntoward targeted outcomes, regardless of the intent being benign or adversarial. Despite the significant\ninfluence of these falsified explanations on human decisions, individuals typically fail to detect\nor recognize any abnormalities. Our study has several limitations. For example, it focuses on\nmodeling and manipulating score-based explanations. Further research is needed to explore how to\nmodel how humans incorporate other types of explanations, such as example-based and rule-based\nexplanations, and how these can be manipulated to influence human behavior as observed with\nscore-based explanations in our study. Additionally, our study was limited to decision making tasks\ninvolving tabular and textual data, which are naturally suited to score-based explanations. Further\nexplorations are needed to extend these findings to decision tasks with other data types (e.g., images)."}, {"title": "Ethical Consideration", "content": "This study was approved by the Institutional Review Board of the authors' institution. Through\nour findings, we aim to draw the community's attention to the ease with which third parties can\nmanipulate AI explanations with the learned behavior models to influence human decision making.\nUsers often lack the ability to accurately and appropriately interpret the AI explanations presented\nto them, yet their decision behavior is easily swayed by the manipulated AI explanations. Our\nfindings highlight the critical importance of securing human-AI interaction data to prevent the misuse"}, {"title": "Acknowledgments", "content": "We thank the support of the National Science Foundation under grant IIS-2229876 and IIS-2340209\non this work. Any opinions, findings, conclusions, or recommendations expressed here are those of\nthe authors alone"}]}