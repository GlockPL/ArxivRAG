{"title": "A Survey for Deep Reinforcement Learning Based Network Intrusion Detection", "authors": ["Wanrong Yang", "Alberto Acuto", "Yihang Zhou", "Dominik Wojtczak"], "abstract": "Cyber-attacks are gradually becoming more sophisticated and highly frequent nowadays, and the significance of network intrusion detection systems have become more pronounced. This paper investigates the prospects and challenges of employing deep reinforcement learning technologies in network intrusion detection. It begins with an introduction to the fundamental theories and technological frameworks of deep reinforcement learning including classic deep Q-network and actor-critic algorithms, followed by a review of essential research that has leveraged deep reinforcement learning for network intrusion detection in recent years. This research assesses these challenges and efforts in terms of model training efficiency, the detection capabilities for minority and unknown class attacks, improved network feature selection and unbalanced dataset issues. Performances of deep reinforcement learning models are comprehensively investigated. The findings reveal that although deep reinforcement learning shows promise in network intrusion detection, many of the latest deep reinforcement learning technologies are yet to be fully explored. Some deep reinforcement learning based models can achieve state-of-the-art results in some public datasets, in some cases, even better than traditional deep learning methods. The paper concludes with recommendations for the enhanced deployment and testing of deep reinforcement learning technologies in real-world network scenarios to further improve their application. Special emphasis is placed on the Internet of Things intrusion detection. We offer discussions on recently proposed deep architectures, revealing possible future policy functions used for deep reinforcement learning based network intrusion detection. In the end, we propose integrating deep reinforcement learning and broader generative methods and models to assist and further improve their performance. These advancements aim to address the current gaps and facilitate more robust and adaptive network intrusion detection systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Cybersecurity is a significant challenge in the age of global informatization [1]. As digital lifestyles become increasingly prevalent, our reliance on cybersecurity and the need for it are growing rapidly as well [2]. Nowadays, cybercriminal activities are inflicting substantial economic losses on various industrial and government sectors. According to live data from the AV-TEST Institute, more than 450,000 new malicious programs are detected every day, with over 1.2 billion instances of malware emerging within 2023 alone, and these numbers are still on the rise [3]. These malicious programs infiltrate personal and corporation digital systems through a wide range of vulnerabilities, posing serious threats to personal privacy, commercial secrets, and other sensitive records. It is estimated that cybersecurity-related issues have caused a total loss of up to 400 billion US dollars to the global economy [4]. From the introduction of cybersecurity ventures, cybercrime has caused $1.5 trillion in losses [5] in 2019, a figure that is expected to climb to $9.5 trillion US dollars by 2024 [6]. Moreover, critical national infrastructures from energy, healthcare sectors, and port automation systems, are becoming primary targets for cyber-attackers [7]. According to 2024 World Economic Forum, 94% of government leaders and business executives believe that their organizations are still at a lower defence level when facing cyber-attacks [8]. In short, the importance of cybersecurity is set to grow significantly worldwide.\nNetwork Intrusion Detection (NID) is a crucial defence mechanism in the field of cybersecurity. It effectively protects computers and other digital devices from external attacks [9]. It was first proposed in 1994 [10] and later described as integrating information extracted from computers to identify resource abuse within the network and attacks originating from outside entities [11]. Basically, intrusion detection systems (IDS) can be categorized into Network Intrusion Detection Systems (NIDS), which is based on the observation of network traffic between different nodes [12] and Host-based Intrusion Detection Systems (HIDS), which means monitoring activities on a specific host, including applications being used and file systems being accessed [13]. The primary purpose of NID is to prevent network attacks by identifying abnormal traffic or access operations [9]. Objectives of network attacks are becoming increasingly complex, traditional signature-based methods that identifying known attacks based on pattern matching of known signatures have fallen behind anomaly-based detection approaches [14]. Since the anomaly-based detection has a higher efficiency and dynamic adaptability, it is now widely accepted by the NID community [15]."}, {"title": "II. REINFORCEMENT LEARNING BASICS", "content": "Reinforcement Learning is quite different from popular supervised and unsupervised learning in ML, which are typically driven by large example data. It involves a decision-making agent that starts without any prior knowledge and learns through its own experience. This is achieved by repeated and random interactions with an environment, allowing the agent to acquire essential knowledge to make an informed decision [34].\nClassic RL system is, in general, comprised of an environment, agent, policy, reward, and value function. [34]. Agent means a decision-making, goal-seeking and highly interactive virtual entity. Environment refers to everything that the agent interact with: applying action to it and receiving feedback from it. Policy determines which action the agent takes when in a given state of the environment. It could be a function or a simple lookup matrix. Reward is quantitative feedback from the environment after the agent takes one specific action followed by a state. The positive and negative reward describes how \"good\" or \"bad\" the action is regarding to the final goal of the agent. Value function is used to evaluate the quality level of a state or action. Thus, it is divided into state-value function and action-value function."}, {"title": "B. Markov decision process", "content": "Markov decision process (MDP) is the principal framework for decision in stochastic and uncertain environment [35]. It assumes that an agent can observe the current state $s_t$, and choose to take an action $a_t$. Then, the agent moves to the next state $s_{t+1}$. Normally, it is described as a tuple $(S, A, P, R, \\gamma)$ with the following five essential elements. S is a state space, including all states that can be observed by the agent. A is an action space, i.e., the set of all actions that can be taken by the agent. $p(s'|s,a)$ describes the probability of transferring to a specific state $s'$ by taking action $a$ in the given state $s$. Reward function $r(s, a, s')$ gives the reward (positive or negative) that the agent get when making a transition from state $s$ to $s'$ by taking action $a$. $\\gamma$ is a discount factor that can be used to indicate short-term or long-term importance of rewards. The fundamental property of an MDP is that the next state $s_{t+1}$ depends only on the current state $s_t$ and the action $a_t$ that the agent has taken. The interaction process between the agent and the environment in a Markov decision process is well illustrated in Figure 3."}, {"title": "C. Q-learning", "content": "Q-learning is a classic value-based algorithm in RL [36]. It allows the agent to learn how to select actions in a given state to maximize the expected total reward. [37]. The agent updates the value of action-state pair (Q value) by exploring possible actions of the state (e-greedy strategy [38]). The upgrade of Q value is based on the current Q value, immediate reward, and maximum Q value of next state. Equation (4) is normally used to update the Q value. In the equation, the Q value $Q(s_t, a_t)$ represents the expected utility of taking action $a_t$ in the current state $s_t$, considering both the immediate reward $r_t$ and the expected rewards of future states. The immediate reward $r_t$ is the gain obtained from taking action $a_t$.\n$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha[r_t + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t)]$\nIn the current state $s_t$, the agent has a high sampling probability to take action $a_t$ as $Q(s_t, a_t)$ holds the highest value among all actions that could be taken in $s_t$.\u00b9 $r_t$ is the immediate reward that the agent obtained by taking $a_t$. $\\max_a Q(s_{t+1}, a)$ means all possible Q value corresponding to all possible action $a$ in $s_{t+1}$. Discounted factor $\\gamma$ is used to determine if the agent should focus on long-term reward, the smaller $\\gamma$ is, the shorter-sighted it is. In the end, learning rate $\\alpha$ determines how much fresh learning experience should be added to past experience."}, {"title": "D. Deep Reinforcement Learning", "content": "1) Deep Q Network, DQN: Deep reinforcement learning (DRL) is a major progress in RL [39]. It makes agent could input high-dimensional data e.g. images and audio, which are difficult for typical Q-learning since they may need a massive tabular to store the value of state-action pair. It is normally unacceptable for memory. DQN [40] is a classic DRL algorithm that leverages the dynamic transition of experience replay and utilization of a target network.\nThe experience replay buffer in DQN is a memory storage mechanism that retains past experiences in the form of state, action, reward, and next state tuples. It allows the agent to break the temporal correlations between consecutive experiences by randomly sampling from this buffer to train the Q-network. This process stabilizes and enhances the learning process by ensuring that updates are based on a diverse set of past experiences, rather than being dominated by recent, possibly highly correlated, events.\nSpecifically, the agent interacts with the environment using random strategy and initial hyper-parameters $\\theta_0$ to get\n$L(\\theta) = E[(r_t + \\gamma \\max_a Q(s_{t+1}, a; \\theta') - Q(s_t, a; \\theta))^2]$\nAfter the policy network is updated into $\\theta'$, it starts to interact with the environment again to accumulate new experiences, which will be used to partially update experiences replay buffer. And the hyper-parameter of the target network will be frozen as $\\theta_0$ and subsequently updated into $\\theta'$ after a fixed time steps. Parameter upgrades of target network always lag behind the upgrades of the policy network. The training will stop when the maximum accumulated reward has been achieved or the loss has converged to a stable range.\n2) Policy Gradient: The policy gradient method [41] is employed to optimize decision-making policies in reinforcement learning directly, without relying on a value function. The principle, different from value function-based methods, is to optimize policy parameters directly and then to achieve the maximum total reward. Policy Gradient Theorem is the very foundation of policy gradient. Let a parametric policy $\\pi(a|s, \\theta)$, choosing action $a$ given a state $s$ by a probability $p = \\pi_\\theta$, then, the performance of the policy can be quantified as follows. R(T) means return from a single trace. And policy gradient can be described in Equation (6), Equation (7) and Equation (8):\n$J(\\theta) = E_{\\tau\\sim\\pi_{\\theta}} [R(T)]$\n$\\nabla J(\\theta) = E_{s,a\\sim\\pi_{\\theta}} [\\sum_t \\nabla_{\\theta} \\log \\pi_{\\theta} (a_t|s_t) G_t]$\n$G_t = \\sum_{k=t}^{T} \\gamma^{k-t}r_k$\nwhere $k$ represents a future time point starting from a specific initial time denoted by $t$. As time passes by, $k$ gradually increases. It is desired that the contribution of rewards received at farther future time points to the current state decreases gradually, hence $\\gamma$, the discount factor, is reduced over time. The initial trajectories are gathered from random interactions between the agent and the environment over a fixed duration. These trajectories are used only once. After all trajectories in a set have been utilized, the agent restarts interactions with the environment to collect a new set of trajectories for the next round of training. However, when having large gradient updates in training, the process becomes unstable. To address this, researchers proposed Trust Region Policy Optimization (TRPO) [42] and Proximal Policy Optimization (PPO) [43] to further improve the stability of training."}, {"title": "3) Actor-Critic Network", "content": "Actor-Critic [44] is one of the major DRL algorithms. The Actor learns the policy to take action within a given state directly (Hyper-parameters set is denoted as $\\theta$), which is different from DQN (indirectly learning by approaching optimal Q value). The critic is utilized to evaluate the value of the policy by approaching an accurate value function using a neural network (Hyper-parameters set is denoted as $\\phi$).\nActor and critic are initialized by $\\theta_0$ and $\\phi_0$. Then, based on the initial policy $\\pi_{\\theta_0}(a|s)$, actor starts to explore the environment to collecting interaction experience $(s_t, a_t, r_t, s_{t+1})$. Critic, with initial $\\phi_0$ parametric, aims to evaluate the value of the policy by calculating temporal difference (TD) $\\delta$ in Equation (9) using actor's experience. The smaller $E(\\delta_t)$ is, the better $\\pi_{\\theta}$ is. Furthermore, $\\phi_0$ will be updated using $\\delta_t$ and gradient decent in Equation (10).\n$\\delta_t = r_{t+1} + \\gamma V_{\\phi_0} (s_{t+1}) - V_{\\phi_0} (s_t)$\n$\\phi_0 - \\phi_0 + \\alpha_\\delta \\nabla_{\\phi} V_{\\phi_0} (s_t)$\nAfter updating the Critic, the parametric Actor will be upgraded based on the Critic's opinion ($\\delta_t$). Then, $\\theta_0$ will be updated by Equation 8. What is worth mentioning is that we consider $\\delta_t$ as a approaching value of the advantage function to represent the relative advantage of an action compared to the average strategy (i.e., current policy $\\pi_{\\theta}$). Once upgrade parametric Actor, another epoch of collecting experience will go on and repeat the steps above.\n$\\nabla_\\theta J(\\theta) = E[\\nabla_\\theta \\log \\pi_\\theta (a|s_t) \\cdot \\delta_t]$\nBased on the idea of the actor-critic network, many improved versions have been proposed. Advantage actor critic (A2C) [45] is an improved version of traditional A2C methods by using advantage function A(s, a), which evaluates the extra value of choosing a specific action a compared with an average value of a state, to making a better training efficiency. Based on this idea, the update for actor in A2C is followed by Equation (14) .\n$A(s, a) = Q(s, a) - V(s)$\n$Q(s, a) \\approx r + \\gamma V(s_{t+1}, \\phi)$\n$\\nabla_\\theta J(\\theta) = E [\\nabla \\log \\pi(a_t|s_t) \\cdot A(s_t, a_t)]$\n$L(\\phi) = E [(r + \\gamma V(s_{t+1}, \\phi) - V(s_t, \\phi))^2]$\nThe aim of Critic in A2C is making sure to predict the expected return given a specific state s following current policy $\\pi$. Thus, trying to minimize the difference between prediction of value function and target return is essential to update the parameters of critic, shown in the Equation (15). Asynchronous Advantage Actor-Critic (A3C) [45] is an algorithm that parallelizes the learning process through multi-threading. Each thread independently explores the environment and calculates gradients, and then asynchronously updates the shared global network parameters [46]."}, {"title": "E. Inverse reinforcement learning", "content": "Inverse Reinforcement Learning (IRL) constitutes a problem setting within RL, aiming to infer the reward function from observed expert behaviours [47]. Unlike traditional reinforcement learning, where agents learn optimal policy through interactions with the environment based on a predefined reward function, IRL focuses on understanding and replicating such behaviours without directly knowing the reward function, by observing exemplary policies or actions [48]. The fundamental premise of IRL is that the observed behaviours reflect the intrinsic motivations or reward structures adhered to during these actions. Consequently, by inversely inferring these motivations or rewards, IRL seeks to construct a reward function that can explain the observed behaviours and can be used to guide agents in learning similar strategies. The most widely used IRL methods include Maximum Entropy Inverse Reinforcement Learning, which uses a linear function to approximate the reward function behind [49]. However, for the complex reward function, the method holds limitations. Based on that, Maximum Entropy Deep Inverse Reinforce-ment Learning was proposed, using fully convolutional neural networks to represent the reward function [50]."}, {"title": "F. Evaluation", "content": "Evaluations in RL are primarily concentrated on assessing the performance of agent in particular task or environment [51]. It relies on specific purpose of tasks, goals of agent and any available feedback information. There are 2 mainly used evaluation metrics, cumulative reward or discounted cumulative reward (return), which means total reward that the agent could gain in one episode or within a fixed period. Success Rate measures the rate at which agent reach specific goals or tasks are successfully completed. It is applicable for tasks with clear success criteria, e.g. navigation. Other evaluation metrics could be set up by understanding the final purpose of task as well, it will depend on the scenario where RL applied. Specifically, in network intrusion detection scenarios, accuracy, precision, recall and $F_1$ scores are widely utilized,\n$Accuracy = \\frac{TP+TN}{TP+TN+FP+ FN}$\n$Precision = \\frac{TP}{TP+FP}$\n$Recall = \\frac{TP}{TP+FN}$\n$F_1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$\n$AUC = \\int_0^1 \\frac{TP}{TP+FN}d(\\frac{FP}{TN+FP})$"}, {"title": "III. TOOLS USED FOR RL BASED NETWORK INTRUSION", "content": "For accelerating the evaluation process of DRL models in network intrusion issues, researchers have developed many useful tools including CSLE [53], PenGym [54], AutoPen [55], NASimEmu [56], CLAP [57], Cyberwheel [58], Idsgame [59], MAB-Malware [60] and YAWNING-TITAN [61], they are shown in Table I. Here are some basic information to introduce what and how will these tools impact the DRL-based network intrusion detection.\nCSLE is a platform designed for developing and testing reinforcement learning agents in network intrusion detection, offering a realistic cyber range environment. It supports integration with methods like dynamic programming, game theory, and optimization, enhancing research in cyber security. PenGym is a framework for training RL agents in penetration testing, compatible with the Gymnasium API. It allows RL agents to perform actions like network scanning and exploitation in controlled environments. AutoPen is an automated penetration testing framework that uses DRL to identify optimal attack paths in both simulated and real networks. It integrates tools like Nmap and Metasploit to execute attacks, allowing users to study penetration testing techniques for educational purposes. NASimEmu is a framework designed for training DRL agents in offensive penetration-testing scenarios, featur-ing both a simulator and an emulator for seamless deployment. It uses a random generator to create varied network scenarios and supports simultaneous training across multiple scenarios.\nCLAP is a RL agent based on PPO that performs penetration testing in simulated network environments using the Network Attack Simulator (NASim). The agent is trained to identify and exploit vulnerabilities to gain access to network resources. Cyberwheel is a RL simulation environment designed for training and evaluating autonomous cyber defence models on simulated networks. Built with modularity, it allows users to customize networks, services, host types, and defensive agents through configurable files. Idsgame is a RL environment designed for simulating attack and defence operations within an abstract network intrusion game. Based on a two-player Markov game model, it features attacker and defender agents competing in a simulated network. The environment provides an interface to a partially observed Markov decision process (POMDP), enabling the training, simulation, and evaluation of attack and defence policies. MAB-Malware is an open-source reinforcement learning framework designed to generate adversarial examples for PE malware by modeling the problem as a multi-armed bandit (MAB). Each action-content pair is treated as an independent slot machine with rewards modeled by a Beta distribution, and Thompson sampling is used to balance exploration and exploitation. YAWNING-TITAN (YT) is a graph-based cyber-security simulation environment built to train intelligent agents for autonomous cyber defence opera-tions. It focuses on simplicity, minimal hardware requirements, and is platform-independent, supporting various algorithms and customizable environment settings."}, {"title": "IV. DRL IN NETWORK INTRUSION DETECTION", "content": "DRL has shown remarkable capabilities and results in network intrusion detection past last few years [21], [62]\u2013[65]. With combining deep learning and RL, researchers can develop highly efficient, automatic learning detection models. These studies highlight DRL's powerful ability to process high-dimensional data and complex network environments [65]. It can not only improve detection accuracy and sensitivity, but also optimize its performance through a continuous and dynamic learning process, making network intrusion defence more intelligent and automated [21]. These advantages of DRL herald its widespread application and far-reaching influence in future NID research. In the following subsections, we aim to fully investigate current research and applications for last few years and present insightful views on deep reinforcement learning for network intrusion detection."}, {"title": "A. Dataset", "content": "Many intrusion datasets have been proposed in recent years. Attack or intrusion types are various in different intrusion datasets. In this section, we are going to present the basic information of current widely utilised network intrusion datasets including KDDCUP 99, NSL-KDD CMU-CERT, UNSW-NB15, CIC-IDS-2017, CSE-CIC-IDS2018, CICDDoS2019, LITNET-2020, AWID, MAWIFlow, CICIoT2023, providing essential understanding including distributions of intrusion types and network traffic features in each datasets and how they were been collected or created. In the end, a comprehensive summery for all datasets are presented for any future reference. The introduction of all mentioned intrusion in the following datasets are presented in Figure 6 for reference.\nThe KDDCUP 99 dataset [66] was created by processing data from the 1998 DARPA [67] intrusion detection challenge. This was achieved using the Mining Audit Data for Automated Models for Intrusion Detection (MADMAID) framework to extract features from the raw tcpdump data. Detailed statistics of the dataset are provided in Table II. The original 1998 dataset was developed by MIT's Lincoln Laboratory, involving thousands of UNIX machines and hundreds of users. Network traffic was recorded in tcpdump format 2 over a ten-week period, with the first seven weeks' data serving as the training set and the remaining three weeks' data as the testing set. The KDDCUP 99 DARPA dataset is available in two versions: the full dataset and a 10% sample. It includes 41 features and is categorized into five classes: Normal, DoS, Probe, R2L, and U2R.\nThe NSL-KDD dataset [68] is one of the most widely recognized benchmark datasets, extensively utilized by cybersecurity researchers to evaluate the performance of Intrusion Detection Systems (IDS). Developed by [66], it is based on the KDDCUP 99 dataset encompassing both attack and non-attack instances. It is classified as either normal or one of 38 predefined attack types. The training subset includes 22 specific attack types, while the testing subset introduces an additional 16 novel attack types. It retains the original four types of attacks from the KDDCUP 99 dataset.\nCMU-CERT [69] is a synthetic dataset firstly proposed by Computer Emergency and Response Team (CERT) division of Carnegie Mellon University (CMU) as an insider threat dataset. Additionally, this dataset has been continuously up-dated in recent years. However, one significant drawback is the substantial data imbalance [70]. The insider threat dataset is available in several iterations in recent years, with each new version offering enhancements and improvements. Version 4.2 is notably more frequently utilized, as it includes the highest proportion of intrusions relative to normal data. This version comprises 30,602,325 entries in total, of which 7,623 entries are identified as attacks. Consequently, the percentage of intrusions in this version is approximately 0.025%. The dataset encompasses two years' worth of Lightweight Directory Access Protocol (LDAP) 3 logs, which are instrumental in pinpointing the active users within the company at any given moment. The insider threat dataset, involving 70 employees, are based on three specific scenarios.\n\u2022 Scenario 1: An employee with no prior record of using removable drives or working after hours suddenly begins logging in outside of business hours, utilizing a removable drive, and uploading data to Wikileaks.org. This activity is followed by the user's swift departure from the company.\n\u2022 Scenario 2: An employee begins to visit job search websites and applies for positions at competing firms. Prior to leaving the organization, this individual uses a thumb drive to steal a significant amount of data, far exceeding their previous usage.\n\u2022 Scenario 3: A disgruntled system administrator installs a keylogger and transfers it to his supervisor's computer via a thumb drive. The next day, he exploits the captured keystrokes to log in as his supervisor and sends a panic-inducing mass email to the entire organization, before immediately resigning.\nTo address the issues including redundant records, imbal-anced datasets and too many simple records present in the KDDCup 99 and NSL-KDD datasets, the research team at the Australian Centre for Cyber Security (ACCS) developed a new dataset known as UNSW-NB15 [71]. This dataset was created using a hybrid generation method, employing the IXIA Perfect Storm tool 4 to capture real-time network traffic containing both normal and malicious activities. The IXIA Perfect Storm tool includes a library that stores new attacks and common vulnerabilities and exposures (CVEs) 5, which is a publicly available repository of security vulnerabilities and exposures.\n1The agent still has small chances to choose other actions although they have a relatively lower Q value\n2The tcpdump format is used to capture detailed network traffic information, including timestamps, source and destination IP addresses, port numbers, protocol types, and more.\n3 Accessing and managing distributed directory information services.\n4A network security testing tool primarily utilized for evaluating and testing the security performance of network infrastructure.\n5Common vulnerabilities and exposures at https://www.cve.org/"}, {"title": "B. Modeling workflow", "content": "Figure 7 illustrates the current standard process of implementing a network intrusion detection model using DRL. In the first phase a network dataset is obtained, then, by employing a traffic sampling method, it is possible to extract network features and labels which are needed/utilized in the training phase. In this phase, the policy function $\\pi_\\theta(a|s)$ receives network traffic features data as input and predicts potential intrusion types. Based on these predictions, feedback is provided in the form of correct, incorrect, or uncertain detection outcomes. This feedback is further processed through a reward function, which in turn updates the policy function to enhance its detection accuracy. Designing a practical reward function could accelerate the training speed and the aim of training is to let the policy function get the maximum reward. During the testing phase, the trained policy function is applied to real network traffic, producing intrusion detection results."}, {"title": "C. Model performance", "content": "DRL-based intrusion models have reached many interest-ing results, some researchers claim they as a state-of-the-art methods among all intrusion detection models. In this section, we are going to introduce the model performances of DRL-based intrusion detection models in different datasets. Table X shows the performance of DRL models on the NSL-KDD dataset in recent years. Models such as Big-IDS, MAFSIDS, and A-DQN demonstrate excellent performance in terms of accuracy and $F_1$ score, with MAFSIDS particularly standing out, achieving an accuracy of 99.10% and an $F_1$ score of 99.10%. Different models also excel in precision and recall, with the DRL+RBFNN model showing a balanced performance across these metrics. Table XI lists the performances of DRL models on the UNSW-NB15 and CMU-CERT datasets. Overall, these models perform well on the UNSW-NB15 dataset, with the DQN model achieving an accuracy of 91.80% and an $F_1$ score of 92.44%. In comparison, the AE-DQN model also performs notably well on the CMU-CERT dataset, with an accuracy of 88.80% and an $F_1$ score of 89.90%. Table XI shows the performance on the CIC-IDS2017 dataset, models like DRL+RBFNN and A-DQN perform excellently across all metrics, with the DRL+RBFNN model achieving an accuracy of 99.70%, and precision and $F_1$ scores of 99.60% and 99.60%, respectively. Table XI also presents the performance on the CIC-IDS2018 dataset. The ID-RDRL model stands out, particularly in recall and $F_1$ score, achieving 100.00% and 96.30%, respectively. The performance on the CIC-IDS2019 dataset is also shown in Table XI. The ADQN model excels in all metrics, especially in accuracy (99.60%) and $F_1$ score (99.40%). The DQN+CNN model also performs well in terms of precision and recall. For the performance on the AWID dataset, models like AE-SAC and SSDQDN show outstanding performance across all metrics, particularly the AE-SAC model, with an accuracy of 98.98%, and precision and $F_1$ scores of 98.96% and 98.92%, respectively."}, {"title": "D. Improved network feature engineering", "content": "Typically, the dataset of network traffic comprises various features that reflect the status of network traffic. However, not all of these features are beneficial for constructing DRL-based network intrusion detection systems. The accurate representation of traffic status using network traffic features is crucial. Consequently, many researches have been proposed to effectively extract network features that can accurately represent the actual status of network traffic. Liu et al [95] proposed a method that combines Local-Sensitive Hashing (LSH) with Deep Convolutional Neural Networks (DCNN), selecting optimal features by assessing the distribution of information entropy across each feature value. Ren et al [93] suggest combining the Recursive Feature Elimination (RFE) and decision tree to select the optimal sub-feature set and the method effectively identifies and eliminates approximately 80% of the redundant features from the original dataset. Ren et al [81] further employed Graph Convolutional Networks (GCN) to extract deep features from network data. They transformed the selected input data into dynamic graph networks. Through the hierarchical structure of GCN, more rich and abstract features were extracted. Finally, they combined a multi-agent learning framework to transform the traditional feature selection space of $2^N$ into a competition among N feature agents, effectively reducing the feature space."}, {"title": "E. Handling unbalanced datasets", "content": "The volume of normal network traffic data significantly exceeds that of intrusion data, which is a common-sense observation. Consequently, nearly all network intrusion de-tection datasets suffer from a severe imbalance in attack-type distribution. Therefore, the challenge of training effective DRL-based NID models on an imbalanced dataset have consistently attracted attention. Researchers have attempted to propose various methods to address such issues. Lopez et al [84] addressed this problem in NID by augmenting Radial Basis Function (RBF) [96] neural networks and integrating them with offline reinforcement learning algorithms. They validated the superior performance of this approach across five commonly used datasets. However, the proposed method may face challenges with larger action spaces and have higher computational costs in training. Mohamed et al [97] utilized a deep State-Action-Reward-State-Action (SARSA) algorithm [98] combined with Deep Neural Networks (DNN) [99] to address the issue of data imbalance in NIDs. Although the performance was outstanding, the authors did not analyze the potential drawbacks of their proposed SARSA algorithm. Caution should be exercised when applying this approach. Pashaei et al [94] introduced an adversarial DRL model combined with intelligent environment simulation, presenting an effective approach to addressing the issue of high-dimensional data imbalance in NIDs. This method improves overall classification performance by increasing attention to minority classes, particularly demonstrating its efficiency and effectiveness in practical applications within IoT environments."}, {"title": "F. Training efficiency", "content": "Normally, network traffic data exhibit high levels of uncertainty and complexity, leading to low training efficiency of DRL, which has been a long-standing concern. Louati et al [79] developed a distributed multi-agent reinforcement learning approach for distributed intrusion detection in large-scale network environments, termed Big-IDS. While the model demonstrated impressive performance, a notable drawback is its low training efficiency. Training the model takes approximately three days when encryption is used and about 12 hours without encryption. This significant time requirement highlights the need for optimization in training procedures to enhance practical applicability. Li et al [80] introduced a network intrusion detection model named AE-SAC, based on adversarial environment learning and the Soft Actor-Critic (SAC) DRL algorithm. While AE-SAC achieved excellent performance in terms of accuracy and $F_1$ score, its complex network architecture resulted in extended training time. During each training session, both the environment agent and the classifier agent are required to update at least three networks, contributing to the lengthy training process. Kalinin et al [100] enhanced the training efficiency of deep reinforcement learning models in Internet of Things(IoT) intrusion detection by implementing lightweight neural network architectures and developing various multi-agent system architectures. This approach also demonstrated superior performance in terms of accuracy and completeness metrics."}, {"title": "G. Identifying minority and unknown attacks", "content": "Normal network traffic constitutes the major class of the these mentioned dataset. However, in practical applications, the robustness and generalization ability of NID systems are often of greater concern. Therefore, identifying minority types of attacks and recognizing unknown categories of attacks are crucial in the actual deployment of NID systems. Hsu et al [90", "101": "introduced a distributed multi-agent NID system that combines DRL with attention mechanisms [102", "89": "enhanced a DRL framework by incorporating human operator interaction feedback into the MDP, creating a hybrid structure of Q-networks. They integrated Long Short-Term Memory (LSTM) [103", "85": "have reported that when they enhancing the capability to recognize unknown attacks, they have unfortunately seen a decline in the ability to identify specific minority attack types. Xiangyu et al [87"}]}