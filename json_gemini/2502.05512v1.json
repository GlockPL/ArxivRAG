{"title": "IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System", "authors": ["Wei Deng", "Siyi Zhou", "Jingchen Shu", "Jinchao Wang", "Lu Wang"], "abstract": "Recently, large language model (LLM) based text-to- speech (TTS) systems have gradually become the mainstream in the industry due to their high naturalness and powerful zero- shot voice cloning capabilities.\nHere, we introduce the IndexTTS system, which is mainly based on the XTTS and Tortoise model. We add some novel improvements. Specifically, in Chinese scenarios, we adopt a hybrid modeling method that combines characters and pinyin, making the pronunciations of polyphonic characters and long- tail characters controllable. We also performed a comparative analysis of the Vector Quantization (VQ) with Finite-Scalar Quantization (FSQ) for codebook utilization of acoustic speech tokens. To further enhance the effect and stability of voice cloning, we introduce a conformer-based speech conditional en- coder and replace the speechcode decoder with BigVGAN2.\nCompared with XTTS, it has achieved significant improve- ments in naturalness, content consistency, and zero-shot voice cloning. As for the popular TTS systems in the open-source, such as Fish-Speech, Cosy Voice2, FireRedTTS and F5-TTS, IndexTTS has a relatively simple training process, more con- trollable usage, and faster inference speed. Moreover, its perfor- mance surpasses that of these systems.", "sections": [{"title": "1. Introduction", "content": "Text-to-speech synthesis (TTS) has extensive applications in fields such as human-computer interaction, education, and en- tertainment. For example, in video creation scenarios in recent years, TTS can assist users in quickly generating video dubbing, saving recording time, and thus playing a crucial role in the cre- ation process. Many creators hope to provide personalized and highly natural speech synthesis services to meet the needs of different scenarios.\nThe TTS system, which is based on large language mod- els and can be trained using massive amounts of general speech data, demonstrates impressive performance in speech generation, such as XTTS[1], Fish-Speech [2], Cosy Voice2[3], FireRedTTS[4] and F5-TTS[5]. Compared to traditional sys- tems that rely on more intricate manual designs, such as Mega- tts 2[6] and Yourtts[7], these systems have achieved significant improvements in naturalness, particularly in zero-shot voice cloning. Generative TTS powered by big data can be roughly classified into three categories. The first is the neural codec lan- guage model. To ensure the quality of synthesized audio, it typi- cally employs a multi-codebook codec along with a high-frame- rate configuration, like in Vall-E[8]. This architecture is simple and straightforward, yet it has drawbacks: longer training and inference times, along with compromised stability. The second is end-to-end diffusion-based TTS, a non-autoregressive (NAR) model. F5-TTS[5] and Seed-TTS[9] are case-in-point. This ap- proach yields high-quality synthesized audio and is suitable for voice editing but is difficult to stream, so not for real-time use. Finally, the hybrid architecture typically uses a single codebook and a low bitrate codec, generating high-quality audio through a standalone decoder such as diffusion or HiFiGAN[10]. It balances performance and generation quality and offers good stability. Due to the success of large language models, tok- enization is the trend of the future. For industrial-level applica- tions, stability is crucial. Here, we opt for the hybrid architec- ture, using a single codebook codec and reconstruct high qual- ity voice through a speech decoder, such as XTTS, Fish-Speech and Cosy Voice2.\nBased on XTTS[1] and Tortoise [11], we have made sev- eral improvements, which mainly include the following: We re- move the front-end G2P module and use raw text as input, along with a BPE-based text tokenizer. This simplifies input prepro- cessing, facilitates multi-language expansion, and enables end- to-end learning of word or polyphone pronunciations via big data context integration. To address the pronunciation control of polyphones and low-frequency characters in Chinese scenarios, which inevitably occur in real world video creation, we propose a hybrid character-pinyin modeling approach. This allows video creators to correct pronunciations by directly inputting pinyin. Moreover, VQ[12] may suffer from low-utilization of the quan- tization codebook due to codebook collapse. we conducted a comparative analysis between VQ and FSQ[13] in terms of their codebook utilization for acoustic token representation, achiev- ing nearly 100% codebook utilization. Finally, we have made significant improvements in prosody naturalness, the similarity of zero-shot voice cloning, and system stability. The main im- provements and contributions are summarized as follows:\n\u2022 In Chinese scenarios, we have introduced a character-pinyin hybrid modeling approach. This allows for quick correction of mispronounced characters.\n\u2022 We develop the IndexTTS system, incorporating a conformer conditioning encoder and a BigVGAN2[14]-based speech- code decoder. This improves training stability, voice timbre similarity, and sound quality.\n\u2022 We release all test sets, including those for polysyllabic words, subjective and objective test sets\u00b9."}, {"title": "2. IndexTTS System", "content": "Similar to XTTS[1], our system incorporates speech-to-codec VQVAE[12] codec, text-to-codec language model and latent- to-audio decoder, as depicted in Figure 1."}, {"title": "2.1. Text tokenizer", "content": "Currently, our system only supports two languages, Chinese and English. We directly use the raw text as input, which is tokenized by a BPE-based text tokenizer, This makes it con- venient to extend the system to other languages. Due to the large number of polyphonic characters in Chinese, we adopt a hybrid-modeling approach of Chinese characters and pinyin in Chinese-related scenarios. The vocabulary size of the text tokenizer is 12,000. It encompasses 8,400 Chinese charac- ters along with their corresponding 1,721 pinyin, English word pieces, and several special symbols. During specific train- ing, we randomly select a certain proportion of non-polyphonic characters and replace them with pinyin. An example of pre- processing process is presented in Table 1."}, {"title": "2.2. Neural Speech Tokenizer", "content": "Vector Quantization (VQ) is a powerful tool for speech cod- ing, but it may suffer from codebook collapse[13], The code- book utilization of VQ and FSQ was analyzed in the following experiments. We increased the parameters of the Variational Autoencoder (VAE) to around 50M. The VAE receives a mel- spectrogram as input and encodes each frame with VQ using approximately 8192 codes. The sampling rate of the input au- dio is 24 kHz, and the token rate output by the speech tokenizer is 25 Hz."}, {"title": "2.3. Large Language Model for TTS", "content": "The text-to-codec large language model (LLM) is based on the decoder-only transformer architecture, similar to XTTS. It gen- erates a series of audio mel tokens from the input series of text tokens. The LLM is also conditioned by a transformer-based conditioning encoder, which we replace with a Conformer en- coder with a subsample rate of 2. We found that this replace- ment can enhance timbre similarity and training stability.\nThe training processes of conditional LLM can be broadly categorized into the following types, The input sequence is structured as follows ([BT] [ET] indicate the beginning and end of the text token sequence. [BA] and [EA] denote the start and end of the audio token sequence):\n\u2022 SEQ1: \"[BT], prompt_text, text, [ET], [BA], prompt_audio, audio, [EA]\", such as Vall-E and Fish- Speech, it concatenates all the tokens of the prompt and the target.\n\u2022 SEQ2: \"[BT], text, [ET], [BA], audio, [EA]\", for instance, Cosy Voice2 directly generates audio tokens from the text to- kens series.\n\u2022 SEQ3: \"speaker_info, [BT], text, [ET], [BA], au- dio, [EA]\", for example, in XTTS[1], Cosy Voice[3] and Tortoise [11], the speaker information of the prompt audio is compressed into one or 32 latent vectors, which serve as the conditions for the LLM.\nSEQ1 and SEQ2 must rely on the text corresponding to the prompt audio during the inference process. The inference in- put prefix sequence can be constructed as \"[BT], prompt_text, text, [ET], [BA], prompt_audio\". In comparison, SEQ3 only requires the prompt audio. The inference input prefix sequence is \"speaker_info, [BT], text, [ET], [BA]\". The autoregressive generation of LM is started from such input prefix sequence un- til the \"End of sequence\" token \"[EA]\" is detected.\nWe adopt the SEQ3. It is worth emphasizing that not rely- ing on prompt text is crucial in certain scenarios. For example, in cross-language voice cloning, if prompt text must be pro- vided or identified through a multilingual ASR system, its us- ability will be significantly limited. Additionally, conditioning on both the prompt text and the audio token series will substan- tially increase the inference time.\nWe also found that, compared to single-speaker encoding vectors such as Tortoise [11] and CosyVoice [3], or speech- prompting methods like Vall-E, the Conformer-based Perceiver demonstrates superior ability in capturing speaker characteris- tics. Moreover, it ensures consistent model outputs across dif- ferent runs, effectively mitigating the issue of speaker shifting that may occur between various model executions. The Per- ceiver offers the advantage of utilizing multiple references with- out imposing length restrictions. This flexibility enables it to comprehensively capture diverse aspects of the target speaker. Furthermore, it even allows for the integration of features from other speakers, thereby facilitating the creation of a truly unique voice."}, {"title": "2.4. Speech Decoder", "content": "The last stage is to convert the SpeechLLM output into wave- form. One is to utilize a flow matching[15] or diffusion- based[9] model to transform the speech code generated by the SpeechLLM into an intermediate representation, such as the Mel spectrogram[11][9]. Then, followed by a vocoder, such as the HifiGAN vocoder, to convert the Mel spectrogram into waveform. This method can generate high-quality audio, but it suffers from slow inference and faces complexity in achiev- ing streaming. The second approach is to directly convert the SpeechLLM output, conditioned on speaker embedding, into the final waveform. We adopts the second approach, based on the BigVGAN2[14] vocoder, directly reconstructing the audio based on the last hidden state of the SpeechLLM, which is con- ditioned with speaker embedding. The latent sampling rate is 25Hz. It is interpolated to 100Hz and then input into BigV- GAN2. Subsequently, the signal is decoded by BigVGAN2 and finally outputs at a frequency of 24KHz."}, {"title": "3. Experiments", "content": "All training data was collected from the internet, with an initial 120,000 hours of raw audio. After voice separation, speaker segmentation, and filtering using Demucs [16], we obtained 34,000 hours of high-quality Chinese-English bilingual data. The dataset includes 25,000 hours of Chinese and 9,000 hours of English audio. We then use ASR (Automatic Speech Recog- nition) to generate pseudo-labels for the corresponding audio. Finally, we emphasize that punctuation marks are added to the ASR results based on text semantics and speech pauses to cre- ate the final training texts. This approach allows users to control pauses flexibly, beyond relying solely on text semantics."}, {"title": "3.2. Experimental Settings", "content": "We randomly select 50% of the training samples. For each sam- ple, we randomly pick 20% of the Chinese characters. If a char-"}, {"title": "3.2.1. Mixed training of Chinese characters and pinyin", "content": "We randomly select 50% of the training samples. For each sample, we randomly pick 20% of the Chinese characters. If a character is not a polyphonic character, we replace it with its corre- sponding pinyin. The replaced text may include Chinese char- acters, pinyin, English words, and punctuation marks. Then, it is directly tokenized by the BPE tokenizer."}, {"title": "3.2.2. Speech Codec Training", "content": "In the training of the Speech codec, we only replace Vector Quantization with Finite Scalar Quantization, while keeping other model configurations unchanged. The FSQ levels are [8,8,8,6, 5], the dimension of the VQ codebook is 512, and it contains 8192 codes. Considering that the size and diversity of the training data might affect the utilization rate of the VQ codebook, we also conduct training on a 6,000 hours subset and the entire training dataset respectively."}, {"title": "3.2.3. Evaluation Settings", "content": "We evaluate indexTTS on four test sets. The first two clean test sets are librispeech[17] and Aishell-1[18] test corpus. The last two sets are composed of 2,000 Chinese samples and 1,000 En- glish samples selected from the CommonVoice[19] test dataset. In each set, each speaker has more than two samples.\nDuring the evaluation, for each sample, one other sample from the same speaker corresponding to this sample is randomly selected as the condition prompt. We use Paraformer [20] ASR to recognize the synthesis results of the Chinese test set, and for the English test set, we use Whisper-large V3[21]. This is to evaluate the content consistency. Regarding speaker similar-"}, {"title": "3.3. Experimental Results", "content": "We conducted tests on 2,500 sentences that contain polyphonic characters. The test results are presented in Table 2. Specifically, the inputs of A1 are all characters, there are 465 synthesized audio with pronunciation errors of polyphonic characters. This accounts for 18.6% of the total. Among these audio with pronunciation errors, 437 of them can be accurately corrected by incorporating the correct pinyin as mixed inputs, as shown in A2, accounting for 94%. The remaining 28 errors, account- ing for 1.1%, that could not be corrected by pinyin might be because errors introduced by the training data have reinforced the SpeechLLM."}, {"title": "3.3.1. Controllability of polyphonic characters", "content": "We conducted tests on 2,500 sentences that contain polyphonic characters. The test results are presented in Table 2. Specifically, the inputs of A1 are all characters, there are 465 synthesized audio with pronunciation errors of polyphonic characters. This accounts for 18.6% of the total. Among these audio with pronunciation errors, 437 of them can be accurately corrected by incorporating the correct pinyin as mixed inputs, as shown in A2, accounting for 94%. The remaining 28 errors, accounting for 1.1%, that could not be corrected by pinyin might be because errors introduced by the training data have reinforced the SpeechLLM."}, {"title": "3.3.2. Evaluate The Codec Quantizer", "content": "We compared VQ and FSQ in terms of codebook utilization under varying training data scales(6k and 34k hours) and evaluate on the above four test sets. Results show that with 6k hours training data, VQ has a 55% low codebook utilization rate. However, when the training data reaches 34k hours, there is little difference between VQ and FSQ, and VQ's utilization rate can also approach 100%. 50% of the tokens cover more than 80% of the total quantity of the tokens that appear in all training data."}, {"title": "3.3.3. Comparison Results with Baselines", "content": "We select several top popular zero-shot TTS models in the opensource for comparison, including systems XTTS[1], CosyVoice2(non-streaming)[22], FishSpeech[2], FireRedTTS[4] and F5-TTS[5]. The evaluation methodology encompasses both objective and subjective metrics: the word error rate (WER) for the content consistency, the speaker embedding similarity (SS) measure for the evaluation of speech cloning fidelity, and the mean opinion score (MOS) for the quantification of perceptual quality.\nThe objective evaluation results of WER and SS for IndexTTS and the baseline models across four test sets are pre- sented in Table 3. IndexTTS significantly outperforms all other open-source models, demonstrating its robustness and stabil- ity. Regarding the SS metric, the performance gap between IndexTTS, Cosy Voice2, and F5-TTS is minimal, yet these models exhibit clear advantages over other compared models.\nIn terms of evaluating the perceptual quality of synthesized audio, we carried out MOS covering three dimensions: prosody, timbre similarity, and sound quality. We conducted a double- blind evaluation by randomly selecting 100 samples from the complete test set to ensure unbiased results. In the subjective evaluation process, we place greater emphasis on the similarity between the synthesized audio and the prompt audio across all aspects. For example, if the sample speeches contain stutters or pauses, we assign a lower score to synthesized results that exhibit overly smooth prosody. Additionally, we also consider the restoration of the sound field characteristics in the prompt audio. The results are shown in Table 4, we have outperformed the baseline in nearly all evaluation dimensions, demonstrating significant advantages in timbre similarity and sound quality.\nMoreover, we randomly selected 200 test samples and calculated the total time taken by all models to synthesize these samples and the GPU resource consumption. The results are presented in Table 5."}, {"title": "3.4. Conclusion", "content": "The IndexTTS system we developed is a GPT-style text-to- speech (TTS) model. It is capable of correcting the pronuncia- tion of Chinese characters using pinyin and controlling pauses at any position through punctuation marks. We enhanced mul- tiple modules of the system, including the improvement of speaker condition feature representation, and the integration of BigVGAN2 to optimize audio quality. Trained on tens of thou- sands of hours of data, our system achieves state-of-the-art per- formance, outperforming current popular TTS systems such as XTTS, Cosy Voice2, Fish-Speech, and F5-TTS."}, {"title": "3.5. Limitations", "content": "In this work, several limitations should be acknowledged. Currently, our system does not support instructed voice generation and is limited to Chinese and English, with insufficient capability to replicate rich emotional expressions. In future work, we plan to extend the system to support additional languages, enhance emotion replication through methods such as reinforce- ment learning, and incorporate the ability to control hyper-realistic paralinguistic expressions, including laughter, hesitation, and surprise, in paralinguistic speech generation."}]}