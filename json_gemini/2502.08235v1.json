{"title": "The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks", "authors": ["Alejandro Cuadron", "Dacheng Li", "Wenjie Ma", "Xingyao Wang", "Yichuan Wang", "Siyuan Zhuang", "Shu Liu", "Luis Gaspar Schroeder", "Tian Xia", "Huanzhi Mao", "Nicholas Thumiger", "Aditya Desai", "Ion Stoica", "Ana Klimovic", "Graham Neubig", "Joseph E. Gonzalez"], "abstract": "Large Reasoning Models (LRMs) represent a breakthrough in AI problem-solving capabilities, but their effectiveness in interactive environments can be limited. This paper introduces and analyzes overthinking in LRMs-a phenomenon where models favor extended internal reasoning chains over environmental interaction. Through experiments on software engineering tasks using SWE Bench Verified, we observe three recurring patterns: Analysis Paralysis, Rogue Actions, and Premature Disengagement. We propose a framework to study these behaviors, which correlates with human expert assessments, and analyze 4018 trajectories. We observe that higher overthinking scores correlate with decreased performance, with reasoning models exhibiting stronger tendencies toward overthinking compared to non-reasoning models. Our analysis reveals that simple efforts to mitigate overthinking in agentic environments such as selecting the solution with the lower overthinking score can improve model performance by almost 30% while reducing computational costs by 43%. These results suggest that mitigating overthinking has strong practical implications. We suggest that by leveraging native function-calling capabilities and selective reinforcement learning overthinking tendencies could be mitigated. We also open-source our evaluation framework and dataset to facilitate research in this direction at https://github.com/AlexCuadron/Overthinking.", "sections": [{"title": "1. Introduction", "content": "Large Reasoning Models (LRMs) (Guan et al., 2025; Xu et al., 2025), such as OpenAI's o1 (OpenAI, 2024e), Alibaba's QwQ (Qwen, 2024b), or Deepseek's R1 (Guo et al., 2025) represent a breakthrough in large language models (LLMs). These advanced systems have fundamentally redefined Al's problem-solving capabilities across various domains (Besta et al., 2025). In particular, LRM's self-correction abilities enable them to achieve impressive scores in several benchmarks, such as AIME 2024 (AoPS, 2024), MMLU (Hendrycks et al., 2021), or GPQA-Diamond (Rein et al., 2023) among others (Guo et al., 2025; OpenAI, 2024e;d; Qwen, 2024b; Guan et al., 2025).\nDespite extensive analysis of LRMs in non-agentic environments, there remains a critical gap in understanding how LRMs perform in agentic environments (Smeyatsky, 2024), where models must simultaneously gather, retain, and act upon new information to complete their tasks (Zhang et al., 2024; Yang et al., 2024b). In this context, LRMs face a fundamental challenge: models must choose between engaging directly with their environment or relying on internal reasoning about potential actions and their hypothetical consequences, a challenge we define as the Reasoning-Action Dilemma.\nIn this work, we present the first comprehensive empirical study of LRMs in agentic tasks at balancing the Reasoning-Action Dilemma, using real-world software engineering tasks as our experimental framework (Jimenez et al., 2024; Yang et al., 2024b). We employ SWE-bench Verified (Jimenez et al., 2024; OpenAI, 2024) as our benchmark, using the CodeAct agent scaffolding (Wang et al., 2024a) within the OpenHands framework (Wang et al., 2024c). This setup creates a controlled environment where models must balance information gathering with reasoning chains while maintaining context across multiple interactions. A proper balance becomes critical as too much reliance on internal reasoning chains might lead to false assumptions about the environment.\nWe observe that LRMs exhibit a consistent pattern of favoring internal simulation over environmental interaction in the Reasoning-Action Dilemma, spending increasing amounts of time constructing elaborate chains of predicted actions rather than adapting to actual system responses, a phenomenon we define as overthinking."}, {"title": "3. Overthinking", "content": "We observe that, in agentic decision-making tasks, LRMs constantly face the Reasoning-Action Dilemma where they must navigate a fundamental trade-off between:\n\nDirect interaction with the environment, where the model executes actions and receives feedback.\n\nInternal reasoning, where the model reasons over hypothetical outcomes before committing to an action.\n\nIdeally, an LRM should balance action and reasoning by using internal simulation to refine its choices while leveraging real-world feedback to correct errors. For instance, when debugging a failing test case, a well-balanced model would hypothesize potential issues yet still execute the test opportunely to collect concrete failure signals.\nUnfortunately, achieving this balance is inherently challenging in agentic environments. On one hand, direct interaction with the environment is time and space (i.e. in-context memory is limited) consuming. On the other hand, prior research has demonstrated that LRMs exhibit significant vulnerability to knowledge insufficiency, where gaps in understanding can cascade into compounding errors throughout the reasoning process (Li et al., 2025; Zhong et al., 2024; Ling et al., 2023; Chia et al., 2024). Consequently, excessive simulation without sufficient external information can ultimately lead to failure. The situation is especially difficult for environments with limited interaction opportunities.\nWe observe that LRMs face a fundamental tension between incorporating environmental feedback and relying on internal reasoning chains, a challenge exacerbated by their prompt sensitivity (OpenAI, 2024c; Guo et al., 2025). As reasoning steps accumulate in the context, they can overshadow or distort the interpretation of real-world information in subsequent iterations. We observed that reasoning models consistently resolve this tension by favoring their internal simulations over environmental signals.\nOverthinking To capture this potential failure mode in agentic settings, we define overthinking as the tendency of an LRM to rely excessively on internal reasoning while failing to seek or integrate essential external feedback. Even with an unbounded resource budget, such an agent remains constrained by the limitations of its partial or inaccurate world model, leading to compounding errors and impaired decision-making."}, {"title": "3.1. The Reasoning-Action Dilemma", "content": "We observe that, in agentic decision-making tasks, LRMs constantly face the Reasoning-Action Dilemma where they must navigate a fundamental trade-off between:\n\nDirect interaction with the environment, where the model executes actions and receives feedback.\n\nInternal reasoning, where the model reasons over hypothetical outcomes before committing to an action.\n\nIdeally, an LRM should balance action and reasoning by using internal simulation to refine its choices while leveraging real-world feedback to correct errors. For instance, when debugging a failing test case, a well-balanced model would hypothesize potential issues yet still execute the test opportunely to collect concrete failure signals.\nUnfortunately, achieving this balance is inherently challenging in agentic environments. On one hand, direct interaction with the environment is time and space (i.e. in-context memory is limited) consuming. On the other hand, prior research has demonstrated that LRMs exhibit significant vulnerability to knowledge insufficiency, where gaps in understanding can cascade into compounding errors throughout the reasoning process (Li et al., 2025; Zhong et al., 2024; Ling et al., 2023; Chia et al., 2024). Consequently, excessive simulation without sufficient external information can ultimately lead to failure. The situation is especially difficult for environments with limited interaction opportunities.\nWe observe that LRMs face a fundamental tension between incorporating environmental feedback and relying on internal reasoning chains, a challenge exacerbated by their prompt sensitivity (OpenAI, 2024c; Guo et al., 2025). As reasoning steps accumulate in the context, they can overshadow or distort the interpretation of real-world information in subsequent iterations. We observed that reasoning models consistently resolve this tension by favoring their internal simulations over environmental signals."}, {"title": "3.2. Manifestations of Overthinking", "content": "Our investigation into impaired decision-making in AI agents draws from a detailed analysis of agent-environment interactions. These interactions are recorded in what we term trajectories. Comprehensive logs that capture the complete sequence of agent actions, environment responses, and (where available) the agent's reasoning process. As outlined in Section 4, we systematically analyzed these trajectories to understand patterns of overthinking.\nWhile most trajectories include the agent's explicit reasoning process, those from the o1 family exclude these reasoning tokens (OpenAI, 2024c). This limitation led us to focus our analysis on observable behaviors, which are the concrete actions agents take in response to environmental challenges.\nThrough this analysis, we identified three distinct patterns of overthinking: Analysis Paralysis, where agents become stuck in excessive planning; Premature Disengagement, where agents abandon tasks prematurely; and Rogue Actions, where agents seem to \"get stressed\" and generate multiple actions on the same iteration. These actions are exemplified in Figure 4.\nAnalysis Paralysis LRMs tend to shift their focus from immediate actions to elaborate future planning. They generate increasingly complex action sequences but struggle to execute them systematically (Figure 4a). Rather than addressing immediate errors, they construct intricate plans that often remain unexecuted, leading to a cycle of planning without progress.\nRogue Actions We observe cases where agents deliberately generate chains of interdependent actions in a single step, without awaiting feedback from the environment (Figure 4b). Despite their prior demonstrated awareness of step-by-step interaction requirements, models proceed to construct elaborate action sequences that presume the success of each preceding step, effectively substituting real environmental feedback with internal simulation.\nPremature Disengagement LRMs sometimes terminate tasks based solely on their internal simulation of the problem space, either through direct abandonment or by delegating hypothetical action sequences (Figure 4c). This illustrates how overreliance on internal reasoning can lead to decisions without environmental validation."}, {"title": "3.3. Quantifying Overthinking", "content": "Overthinking Score To quantify overthinking behavior, we developed a systematic scoring method using an LLM-based evaluator. This evaluator analyzes model trajectories for the previously described patterns and assigns a score of 0 to 10, with higher scores indicating more severe overthinking behavior. Each score includes a detailed justification explaining which patterns were identified and their severity. The complete evaluation prompt and scoring criteria can be found in Appendix A.\nTo validate our LLM-based evaluator, we conduct an independent assessment where four expert annotators manually scored 20 randomly selected model traces, as shown in Figure 5. Using these standardized scores, we conduct a comprehensive statistical analysis to investigate the relationship between overthinking behavior and model performance and how overthinking affects LRMs compared to non-reasoning models. The tools used for the statistical analysis can be found in Appendix C."}, {"title": "4. Evaluation Framework", "content": "We analyze LRMs performance in agentic environments using SWE-bench Verified (OpenAI, 2024), comparing reasoning models with their non-reasoning counterparts. Our study aims to answer the following research questions:\n\nRQ1: Does overthinking affect agentic performance?\n\nRQ2: How does it impact different models?\n\nRQ3: Can we mitigate overthinking?"}, {"title": "4.1. Experimental setup", "content": "OpenHands To demonstrate how AI agents operate, we use the OpenHands framework (Wang et al., 2024c), which implements a complete agent-environment interaction cycle as illustrated in Figure 2. Through this framework, agents receive a set of tools to interact with their environment, along with examples of the proper usage of tools (Wang et al., 2024a). The agent processes this information and can execute actions through these tools, receiving immediate environmental feedback. This feedback is then incorporated into the agent's context, enabling in-context learning (Dong et al., 2024) and self-refinement (Madaan et al., 2023) through successive interactions. The framework supports both native function-calling capabilities (OpenAI, 2024a) and structured text output, adapting to different model architectures while maintaining a consistent interaction protocol. In this work, we leverage OpenHands' comprehensive instrumentation capabilities to systematically analyze how models balance the Reasoning-Action Dilemma, revealing previously unexamined patterns in their interaction behavior.\nSWE-Bench Software engineering tasks present an ideal environment for studying agent behavior, as they require both sophisticated reasoning and continuous interaction with the environment (Jimenez et al., 2024). SWE-Bench captures this complexity by presenting agents with real-world software issues that demand multiple steps to resolve: agents must understand the problem, explore the codebase, reason about potential solutions, and validate their changes through testing (Yang et al., 2024b). This multi-step nature creates a natural tension between reasoning and action, ideal for testing how models balance the Reasoning-Action Dilemma.\nIn this work, we present the first systematic framework for quantifying how LRMs navigate this fundamental tension, revealing that excessive reliance on internal reasoning often comes at the cost of effective environmental interaction and task completion."}, {"title": "5. Results", "content": "We generate and evaluate 3908 trajectories using our evaluation methodology across all models. We make publicly available every trajectory alongside their corresponding overthinking score and the reasoning behind this score.\nOur analysis reveals three key findings about overthinking in language models: its impact on model performance, its varying prevalence across model types, and its practical implications for model selection. Illustrated in Figure 3. We observe that overthinking consistently impacts performance across all evaluated models, with reasoning-optimized models showing higher overthinking tendencies than general-purpose ones as illustrated in Figure 1."}, {"title": "5.1. Overthinking and Issue resolution", "content": "We observe a strong negative correlation between overthinking and performance on SWE-bench, as illustrated in Figure 1. Both reasoning and non-reasoning models show decreased performance as overthinking increases, though with notably different patterns."}, {"title": "5.2. Overthinking and Model Type", "content": "We make three key observations with regard to overthinking in reasoning and non-reasoning models. The results are presented in Figure 1.\nFirst, we observe that non-reasoning models can also overthink, likely due to their latent reasoning capabilities. Recent studies suggest that non-reasoning models also exhibit reasoning abilities (Wei et al., 2023; Yao et al., 2023; Chen et al., 2023; Kojima et al., 2023).\nSecond, reasoning models exhibit significantly higher overthinking scores than non-reasoning models, as shown in Table 3. Since these models are explicitly trained for reasoning and generate extended chains of thought by simulating environmental interactions, they are more likely to suffer overthinking manifestations."}, {"title": "5.3. Overthinking and Model Size", "content": "Our evaluation examines two model families across three size variants (32B, 14B, 7B): the non-reasoning Qwen2.5-Instruct and the reasoning R1-Distill-Qwen (Yang et al., 2024a; Qwen, 2024a; Guo et al., 2025).\nAs illustrated in Figure 6, our analysis suggests a negative correlation between model size and overthinking behavior. We hypothesize that smaller models struggle with environmental comprehension, causing them to rely more heavily on internal reasoning chains and increasing their tendency to overthink.\nThe relationship between model size and overthinking manifests differently across model types. As shown in Table 3, both reasoning and non-reasoning models show higher overthinking scores as their size decreases, with reasoning models consistently exhibiting greater susceptibility to overthinking. However, the gap in overthinking scores between reasoning and non-reasoning models narrows significantly as model size decreases further. This convergence in overthinking behavior among smaller models towards high overthinking scores likely stems from their shared difficulty in processing environmental complexity. When faced with repeated failures in environmental interactions, these models appear to retreat to their internal reasoning chains and disregard external feedback. While this pattern aligns with our observations, further investigation is needed to confirm the underlying cause."}, {"title": "5.4. Overthinking and Token Usage", "content": "Prior research has suggested that token usage can serve as an indicator for overthinking (Chen et al., 2024b). To investigate this relationship, we analyze the o1 model, manipulating its reasoning effort parameter between high and low settings, which directly influences the number of reasoning tokens used (OpenAI, 2025a).\nOur analysis reveals that o1 models with low reasoning effort demonstrate 35% higher overthinking scores compared to their high-effort counterparts. As shown in Table 4, the difference in averaged overthinking scores between the two configurations is statistically significant, suggesting that increased token allocation might reduce overthinking in agentic contexts.\nThis finding challenges the perception that increased reasoning token usage correlates with overthinking as shown by some recent studies (Chen et al., 2024b). Instead, our results indicate that having more reasoning tokens can effectively curb overthinking, highlighting the importance of structured reasoning processes in model behavior."}, {"title": "5.5. Overthinking and Context Window", "content": "We analyze models across different context window sizes, ranging from 8K to 32K tokens. We observe no significant correlation between context window size and overthinking scores when comparing models of similar architectures and sizes but different context windows. For instance, comparing Qwen2.5-32B (32K context) with QwQ-32B (32K context) shows overthinking scores of 2.31 \u00b1 0.42 and 2.28 \u00b1 0.39 respectively (p > 0.05).\nWe hypothesize that this lack of correlation may be because overthinking behaviors are more influenced by a model's architectural design and training approach rather than its context capacity. This aligns with our earlier findings about the importance of model type and size in determining overthinking tendencies."}, {"title": "5.6. Practical Implications", "content": "OpenAI showcased that reasoning models exhibit a disproportionate increase in computational costs relative to their performance gains (ARC, 2024). Our experiments with SWE-bench Verified dataset confirm this observation: 01 with high reasoning effort achieves a 29.1% resolution rate at $1,400, while the low reasoning variant reaches 21.0% at $400 a 3.5\u00d7 cost difference for an 8.1 percentage point improvement in performance.\nMetrics. To address this efficiency gap, we computed the (1) Pass@k, which represents the percentage of tasks where at least one successful solution is found among K sampled trajectories, and (2) Lowest Overthinking@K, which selects the trajectory with the lowest overthinking score among K samples and reports the percentage of these selected trajectories that are successful. Pass@K evaluates the model's ability to find any working solution (i.e., the upper bound for Lowest Overthinking@K), while Lowest Overthinking@K assesses our model's capability to identify the most promising solution as illustrated in Figure 3. The confidence intervals (CI) showcased were computed using Wilson score (Wallis, 2013)\nThis method of selecting solutions based on overthinking scores yields impressive efficiency gains. By limiting to two samples with the lowest reasoning, we achieve a 27.3% resolution rate while consuming only 57% of the high-reasoning configuration's cost ($800 vs $1,400). Furthermore, with three samples we surpass the high-reasoning baseline (30.3% vs 29.1%) while still saving $200 in computational costs. Our findings demonstrate that monitoring and controlling overthinking behavior is a highly effective strategy for optimizing both the performance and efficiency of language reasoning models in real-world applications."}, {"title": "6. Discussion", "content": "Our experimental analysis compares o1 model configurations with high reasoning effort, evaluating performance both with and without native function calling (FC) capabilities. The integration of FC capabilities yields substantial improvements, increasing the performance score from 29.1% to 47.7%, while simultaneously reducing the average overthinking score from 2.43 to 1.05 \u2013 effectively mitigating the overthinking phenomenon.\nHowever, benchmarking against BCFL (Yan et al., 2024) reveals a more nuanced pattern, where the performance differential between FC and non-FC implementations of ol in multi-turn environments shows a modest improvement from 36% to 41%. This comparatively smaller enhancement suggests that FC implementation alone cannot fully account for the dramatic performance improvements observed in our primary experiments."}, {"title": "6.1. Can native function calling affect overthinking?", "content": "Our experimental analysis compares o1 model configurations with high reasoning effort, evaluating performance both with and without native function calling (FC) capabilities. The integration of FC capabilities yields substantial improvements, increasing the performance score from 29.1% to 47.7%, while simultaneously reducing the average overthinking score from 2.43 to 1.05 \u2013 effectively mitigating the overthinking phenomenon.\nHowever, benchmarking against BCFL (Yan et al., 2024) reveals a more nuanced pattern, where the performance differential between FC and non-FC implementations of ol in multi-turn environments shows a modest improvement from 36% to 41%. This comparatively smaller enhancement suggests that FC implementation alone cannot fully account for the dramatic performance improvements observed in our primary experiments."}, {"title": "6.2. Why doesn't DeepSeek-R1-671B overthink?", "content": "Our analysis of DeepSeek-R1-671B (DS-R1) reveals overthinking scores comparable to those of DeepSeek-V3-671B. This similarity in overthinking behavior may be attributed to DS-R1's training methodology, which does not incorporate extensive reinforcement learning for software engineering tasks. While DS-R1 maintains performance levels similar to DeepSeek-V3 on software engineering benchmarks (Guo et al., 2025), our findings suggest that the combination of limited RL training and substantial model scale (671B parameters) contributed to its controlled overthinking behavior."}, {"title": "6.3. How to fix overthinking?", "content": "While our algorithmic interventions demonstrate immediate practical benefits, they primarily address the symptoms rather than the root causes of overthinking. Our analysis suggests that more fundamental solutions might emerge from understanding how models learn to balance reasoning and environmental interaction. The success of function-calling architectures hints at the importance of explicit interaction training, while the effectiveness of limited reinforcement learning points to the role of training methodology.\nThese insights open important questions for future research: How do these approaches generalize across different domains? How can we optimize for environments where environmental interaction carries varying costs? Understanding these dynamics could help develop more robust solutions that prevent, rather than just mitigate, overthinking behaviors in large reasoning models."}, {"title": "7. Conclusion", "content": "In this work, we present the first comprehensive empirical study of Large Reasoning Models (LRMs) in agentic environments. We identify a fundamental challenge: the Reasoning-Action Dilemma, in which models must balance environmental engagement against internal reasoning about potential actions and their hypothetical consequences. Our analysis reveals that LRMs consistently favor internal simulation over environmental interaction, a behavior we define as overthinking.\nThrough our systematic evaluation framework, we analyzed 3,908 trajectories using a novel overthinking score metric. Our findings demonstrate a strong correlation between overthinking and task failure rates, with reasoning models showing particularly high vulnerability to this phenomenon compared to their non-reasoning counterparts.\nOur research demonstrates that even simple interventions to mitigate overthinking can yield substantial benefits: a 43% reduction in inference costs while improving issue resolution rates by 25% on SWE-bench Verified dataset. These results, combined with our observations about the effectiveness of function-calling capabilities and targeted reinforcement learning, suggest promising directions for developing more efficient and environmentally grounded reasoning models particularly for agentic tasks."}, {"title": "A. Prompt to detect overthinking", "content": "Here, we provide the prompt used to assess the overthinking score.\nYou are an Al judge focused on detecting when models prefer their internal reasoning chain over interacting with the environment.\n<INTERACTION>\ntrajectory goes here\n</INTERACTION>\nAnalyze the <INTERACTION> and determine if the model is preferring their internal reasoning chain over interacting with the environment:\nHow could this be detected?\n\nThe model suffers from Analysis Paralysis, it focuses on heavy planning instead of interacting with the environment.\n\nThe model suffers from Rogue actions, after facing setbacks, it generates multiple actions without waiting for the environment to process the previous action.\n\nThe model suffers from Premature Disengagement, it concludes the task without checking with the environment. Either because it is overconfident in the solution or because it thinks it can't solve the problem.\nAlways interacting with the environment\n\nA summary of what has been done so far is good, even if done multiple times.\n\nA brief summary of the steps to take is good if the model interacts with the environment following steps one by one.\n\nAlternating between two operations is good.\n\nRepeating similar patterns or configurations is fine as long as the model interacts with the environment between attempts.\n\n4-7: Sometimes relies too much on their internal reasoning chain, but still interacts with the environment.\n\nIt engages in heavy planning, but still interacts with the environment.\n\nIt might output multiple steps ONE time, but at subsequent turns it interacts one step at a time.\n\n8-10: Completely relies on their internal reasoning chain.\n\nGenerates multiple actions without waiting for environment response.\n\n\nGets stuck in endless theoretical discussion without attempting solutions.\n1. Analysis Paralysis\n\nIs the model focusing on heavy planning instead of interacting with the environment?\n\nDoes the model follows its planned steps starting from the first one?\n2. Rogue Actions\n\nIs this behavior after a facing a setback?\n\n3. Premature Disengagement"}, {"title": "C. Statistical principles utilized in this work", "content": "Coefficient of Determination R2. The coefficient of determination, denoted by $R^2$, is a statistical measure of how well the regression predictions approximate the real data points. Formally, for a set of observed values ${y_i}_{i=1}^n$ with mean $\\bar{y}$ and corresponding fitted values ${\\hat{y}_i}_{i=1}^n$, it is defined as:\n$R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}$\nIt represents the proportion of the variance in the dependent variable that is explained by the regression model.\nP-value. Given a null hypothesis Ho and a test statistic (based on a sample) used to decide whether to reject Ho, the p-value is the probability, under the assumption that Ho is true, of obtaining a test statistic value at least as extreme as the one that was actually observed. Symbolically, if T is the test statistic, and $t_{obs}$ its observed value,\np-value = P(T > $t_{obs}$ | Ho),\nfor a one-sided test (or an analogous definition for two-sided tests). A smaller p-value indicates stronger evidence against Ho.\nBeta Coefficients in Simple Linear Regression Consider a simple linear regression model:\nYi = \u03b2o + \u03b2\u2081 Xi + Ei,\nwhere:\n\u03b2o is the intercept (the predicted value of Y when X = 0),\n\u03b2\u2081 is the slope (the expected change in Y for a one-unit increase in X).\nEi is the error term, assumed to have mean zero.\nIn this context, the slope \u03b2\u2081 is given by\n$\\hat{\u03b2_1} = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}$,\nwhich measures the strength and direction of the linear relationship between X and Y.\nT-test of the p-value A t-test assesses whether the mean(s) of one or two groups differ(s) from a hypothesized value or from each other under the null hypothesis Ho. Let T be the test statistic calculated from the data (for instance, comparing sample mean(s) to the hypothesized mean(s)), and let tobs be the observed value of T. The p-value for the t-test is then defined as:\np-value = P(|T| \u2265 |tobs | | Ho)\nfor a two-sided test (or a correspondingly appropriate one-sided version). A lower p-value provides stronger evidence against Ho, suggesting that the observed difference is unlikely to have occurred under the null hypothesis.\nC.1. Definition of model-specific coefficients\nDefinition C.1 (Model-Specific Coefficients). For the Reasoning Language Models, the fitted model is\n$\\hat{Y_R} = \\hat{\u03b2_{0,R}} + \\hat{\u03b2_{1,R}}X$,\nwhere\n$\\hat{\u03b2_{1,R}} = -7.894$.\nFor the Non-Reasoning Language Models, the fitted model is\n$\\hat{Y_{NR}} = \\hat{\u03b2_{0,NR}} + \\hat{\u03b2_{1,NR}}X$,\nwhere\n$\\hat{\u03b2_{1,NR}} = -15.938$."}]}