{"title": "FAME: Towards Factual Multi-Task Model Editing", "authors": ["Li Zeng", "Yingyu Shan", "Zeming Liu", "Jiashu Yao", "Yuhang Guo"], "abstract": "Large language models (LLMs) embed extensive knowledge and utilize it to perform exceptionally well across various tasks. Nevertheless, outdated knowledge or factual errors within LLMs can lead to misleading or incorrect responses, causing significant issues in practical applications. To rectify the fatal flaw without the necessity for costly model retraining, various model editing approaches have been proposed to correct inaccurate knowledge within LLMs in a cost-efficient way. To evaluate these model editing methods, previous work introduced a series of datasets. However, most of the previous datasets only contain fabricated data in a single format, which diverges from real-world model editing scenarios, raising doubts about their usability in practice. To facilitate the application of model editing in real-world scenarios, we propose the challenge of practicality. To resolve such challenges and effectively enhance the capabilities of LLMs, we present FAME, an factual, comprehensive, and multi-task dataset, which is designed to enhance the practicality of model editing. We then propose SKEME, a model editing method that uses a novel caching mechanism to ensure synchronization with the real world. The experiments demonstrate that SKEME performs excellently across various tasks and scenarios, confirming its practicality.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have achieved remarkable capabilities across various domains and are extensively utilized in practical applications (Touvron et al., 2023a,b; Achiam et al., 2023; Geva et al., 2021, 2022). The extensive utilization of LLMs makes it essential for them to provide precise information. However, LLMs may still provide erroneous information due to incorrect, outdated knowledge stored within the model (De Cao et al., 2021; Agarwal and Nenkova, 2022). Such erroneous information can have significant repercussions within critical domains like medical diagnostics and legal consultations, underscoring the importance of rectifying errors in language models. To avoid costly retraining and to efficiently correct the outputs of LLMs, model editing has been proposed (Mitchell et al., 2022; Sinitsin et al., 2020; De Cao et al., 2021).\nTo evaluate model editing methods, previous works have introduced a series of datasets (De Cao et al., 2021; Meng et al., 2022; Zhong et al., 2023). Almost all of these datasets set the target as incorrect answers, which affects the model's practical performance and contradicts the original purpose of model editing. As shown in Figure 1, when the user asks \"Who is the President of America?\", LLMs produce incorrect output due to outdated knowledge. Previous datasets (Levy et al., 2017; Meng et al., 2022; Gupta et al., 2023) modified them to other wrong targets (for example, Tom Cruise). Moreover, these datasets are all composed of data in a single format with a single task like QA (Levy et al., 2017) or sentence completion (Meng et al., 2022), which leads to a disparity between experiments and practical applications.\nTo promote the capability of model editing in practical applications, we introduce a novel criterion: Practicality. Practicality refers to the capacity of data and methods to be functional in real-world applications. This entails that data should be factual, diverse, and of high quality, and methods should be efficient and general across tasks. These requirements collectively ensure the effectiveness and applicability of model editing in practical applications.\nTo address the practical shortcomings in the previous benchmark, which mainly include incorrect knowledge and limited task formats, we introduce FAME, a factual, extensive model editing benchmark with practicality. FAME comprises 128k real data items, including various tasks with single-hop and multi-hop questions. In response to incorrect knowledge, we extract factual data items from Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) and DBpedia (Auer et al., 2007) and employ multiple rounds of manual verification to ensure the accuracy of data items. To prevent limited task formats, we incorporate tasks from existing datasets like QA (Levy et al., 2017), fact-check (Schuster et al., 2021), multi-hop QA (Zhong et al., 2023), and introduce new tasks such as cloze and dialogue, making our evaluation more comprehensive. FAME enhances the model's ability to solve real-world problems and cross-domain issues and enables a complete evaluation of the effectiveness of model editing.\nAimed at tackling the deficiency in the practicality of previous methods, we introduce SKEME. SKEME utilizes a novel caching mechanism and receives information from diverse sources to ensure synchronization with the real world, allowing for the application of SKEME in real-world scenarios. The caching mechanism tackles the challenges posed by large-scale data and diverse tasks.\nTo evaluate the practicality of model editing methods, we first introduce a new metric, SURE, which considers both accuracy and side effects and is adaptable to scenarios. We then evaluate whether each method can meet the basic requirements of model editing (Section 7). Subsequently, We discuss the performance of model editing methods in real-world scenarios (Section 8). The conclusion indicates that previous methods either exhibit side effects or struggle to handle complex scenarios, while only SKEME consistently outperformed others across all experimental conditions, demonstrating the superiority of SKEME in real-world scenarios.\nThe main contributions of this paper are as follows:\n\u2022 We first introduce the practicality requirement for model editing, which necessitates data and methods to exhibit effective performance in real-world applications.\n\u2022 To support the practicality requirement of model editing, we create FAME, a novel benchmark that utilizes real-world knowledge and common diverse tasks to simulate practical applications.\n\u2022 To meet the practicality requirement of the model editing method, we propose a method called SKEME, which is the first to introduce a novel caching mechanism for efficient storage, retrieval, and update of constantly evolving real-world facts. Experiments demonstrate that SKEME is more effective in real-world scenarios."}, {"title": "2 Related work", "content": "Model editing datasets serve the purpose of verifying the effectiveness of methods and enhancing the capability of LLMs. Nevertheless, current datasets fall short of directly enhancing the capability of LLMs. The majority of datasets comprise constructed fake data (Levy et al., 2017; Meng et al., 2022; Gupta et al., 2023), primarily serving to validate effectiveness rather than directly contribute to the enhancement of LLMs' capabilities. MQuAKE-T (Zhong et al., 2023) utilizes modifications in Wikidata, which has the potential to directly enhance LLMs' practical performance. However, due to the limited combinations of relations (see Figure 11 for statistics), its direct utility in improving the performance of LLMs is limited, thereby primarily serving to validate effectiveness."}, {"title": "2.2 Model Editing Methods", "content": "Previous works have introduced various model-editing methods, both parameter modification and parameter preservation approaches (Yao et al., 2023). The former category includes the locate-then-edit method (Meng et al., 2022, 2023) and meta-learning-based methods (De Cao et al., 2021; Mitchell et al., 2021). The latter category involves adding additional parameters to the model (Huang et al., 2023) and employing vector databases for knowledge storage and retrieval (Mitchell et al., 2022; Zhong et al., 2023; Zheng et al., 2023; Cheng et al., 2023; Madaan et al., 2022). Building on this foundation, there are also methods such as reflection (Wang et al., 2024b), optimization of searches for multi-hop questions (Shi et al., 2024), and the use of post-processing (Song et al., 2024) to improve retrieval strategies. Diverging from the previously discussed methods, our method involves a novel caching mechanism, allowing for the application in real-world scenarios."}, {"title": "3 Problem Definition", "content": "The objective of model editing is to modify the knowledge contained in a model, allowing the model to engage in reasoning processes based on the edited knowledge, while not affecting the output related to the unedited knowledge. Based on previous work (Zhang et al., 2024; Yao et al., 2023), we define model editing to express the goal as follows.\nAn input-output pair is defined as $(x, y)$, and a model is represented by a function $f: X \\rightarrow Y$, where $X$ represents the input set and $Y$ represents the output set. Let $I(x, y)$ denotes the set of descriptions semantically equivalent to $(x, y)$, and $EX(x, y)$ be the set of input-output pairs that the model can possess with $I(x,y)$ as prior knowledge. Then, let $O(x, y)$ represent the portion outside $I(x, y)$ and $EX(x, y)$. Appendix C provides an example of the definition.\nFormally, let (subject, relation, object) be a factual triple, denoted as $(s, r, o)$. Consider an input-output pair as $(x, y)$, where $x$ is effectively a combination of $s$ and $r$. A model is represented by a function $f: X \\rightarrow Y$, where $X$ represents the input set and $Y$ represents the output set.\nWe use the prime notation to denote semantically equivalent elements. Specifically, for any $t$ in the set {s, r, o, x, y}, let $t'$ be any element that is semantically equivalent to $t$, and let $T'$ be the set of all such $t'$. Notice that $t \\in T'$. Then, we can define $I(x, y)$ as\n$I(x,y) = \\{(x', y')|x' \\in X' \\text{ and } y' \\in Y'\\}.$ (1)\nTo define $EX(x, y)$, let's represent a fact triple as $tr(s, r, o)$, abbreviated as $tr$, and $S$ is the set of all fact triples. Also, define the multiplication operation * for two sets of fact triples A and B as the join operation:\n$A * B \\triangleq A \\Join B$.\n(2)\nThen, define\n$No(tr) = \\{(s', r', o') | s' \\in S', r' \\in R', o' \\in O'\\}$\n(3)\nand\n$Ni(tr) = Ni-1(tr) * S $\n(i \u2265 1) (4)\nUltimately, we define $EX (tr)$ as\n$EX(tr) = \\bigcup_{i=0}^{\\infty} Ni$\n(5)\nBy transforming s and r into x, and o into y, we derive $EX (x, y)$.\nAfter defining $I(x,y)$ and $EX(x,y)$, we can define $O(x, y)$ as\n$O(x, y) = \\complement_{S}(I(x, y) \\cup EX(x, y))$\n(6)\nwhere $\\complement_{S}$ represents the complement within the set $S$.\nThe definition of model editing can be summarized as follows: $(xf,yf)$ denotes the fact that is being edited, while $(xe, Ye)$ represents the input and output.\nf'(xe) =\\begin{cases}Yf & (xe, Ye) \\in I(xf, Yf) \\\\f(xe) & (xe,Ye) \\in EX(xf, yf) \\\\f(xe) & (xe, Ye) \\in O(xf, Yf)\\end{cases}"}, {"title": "4 FAME: A Practical Model Editing Benchmark", "content": "FAME (FActual Multi-task model Editing) is a benchmark comprising 128k factual data items. We utilize these data items to construct both single-hop and multi-hop questions. For single-hop questions, we include six forms: QA, sentence completion, cloze test, multiple-choice questions, fact check, and locality test. For multi-hop questions, we include multi-hop questions and dialogues. The previous work introduced QA, sentence completion, fact check, and multi-hop questions (Zhang et al., 2024), while we propose the remaining tasks. We believe that combining these tasks contributes to a comprehensive assessment of the effectiveness of model editing methods.\nThe construction of FAME is divided into two steps: (1) Collect real fact triples; (2) Create diverse tasks using the collected triple. To ensure the data quality of FAME and its reflection of the real world, we conducted multiple rounds of manual verification and correction in various aspects. For more details, please refer to Appendix A.1."}, {"title": "4.1 Collect Fact Triples", "content": "To obtain real-world fact triples, we collect data from Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) and DBpedia (Auer et al., 2007), both of which are continuously updated databases. We aim to enhance the diversity of FAME by collecting knowledge from a variety of knowledge bases.\nSpecifically, we initially identified equivalent relations in Wikidata and DBpedia. Subsequently, non-informative relationships such as IDs were discarded. Then, we collect triplets associated with these relations from Wikidata and DBpedia.\nAfter obtaining the triplets, we further filter them to avoid potential ambiguity issues, see Appendix A.1.2 for details.\nFinally, to ensure the quality of the triplets we obtained, we randomly selected 100 triplets and manually examined their correctness. The results indicate that 96% of the triplets are correct, which shows that our process for obtaining and filtering triplets is acceptable."}, {"title": "4.2 Generate Data Based on Templates", "content": "We create templates for each type of task to transform fact triples into queries for various tasks. We employ ChatGPT in the generation process to mitigate expensive labor costs following previous works (Petroni et al., 2019; Yin et al., 2023). After generating the results, we conduct manual checks to ensure the accuracy and alignment with our intentions.\nFor single-hop questions, we prompt ChatGPT to generate question templates based on the description of each relationship that used in fact triples (e.g., head of government), incorporating placeholders. Then we replace these placeholders with subjects to generate questions from the templates.\nFor multi-hop questions, we employ ChatGPT to concatenate multiple consecutive triplets into a single question. Inspired by Petroni et al. (2019), to distinguish between the differences in model decomposition ability and knowledge it knows, we decompose queries to obtain multi-turn dialogue.\nTo ensure the accuracy of templates, we incorporate manual verification to ensure that the templates align with the meaning of the relationships. We found that 97.4% of templates are accurate, we then manually performed multiple rounds of correction and rechecking, ensuring that the correctness rate of the templates reached 100%.\nFinally, following previous work (Yin et al., 2023), we employ manual sampling and verification techniques to ensure the accuracy of FAME. We combine the templates and relation triplets and manually check the credibility of the generated sentences. The results show that 97.5% of the sentences were credible, demonstrating the reliability of the entire process."}, {"title": "5 Benchmark Analysis", "content": "See Table 1 for a comparison between FAME and previous benchmarks. FAME includes all categories seen in previous benchmarks, and we propose additional data categories. Moreover, the number of entries far exceeds those in most of the previous benchmarks. Finally, FAME originates from two distinct knowledge bases, making it more comprehensive compared to previous datasets.\nCompared to the previously multi-hop dataset containing factual knowledge, MQuAKE-T (Zhong et al., 2023), FAME is larger and includes more relationships, making it more comprehensive. See Appendix D for details on the comparison and further comparisons."}, {"title": "5.2 Statistics", "content": "FAME consists of two parts: single-hop data and multi-hop data, both sourced from Wikidata and DBpedia. Table 2 presents the statistical data for different tasks in the dataset. Please refer to Appendix E for examples and more detailed statistics."}, {"title": "6 SKEME: A Model Editing Method For Real-World Applications", "content": "To accommodate practical model editing and meet the need for utilizing real-world facts and adapting to diverse tasks, We propose SKEME (Structured Knowledge retrieved by Exact Matching and reranking Editing), a model editing method that first incorporates a caching system to efficiently store and retrieve real-world knowledge, which is collected from diverse data sources. Besides, SKEME utilizes entity extraction to exclude irrelevant content brought by diverse input formats, enhancing its performance across various tasks."}, {"title": "6.1 Overview", "content": "The overview of SKEME is shown in Figure 2. SKEME consists of three main components:\nTo avoid the influence of varied input formats, Entity Extraction extracts key entities from the input for subsequent retrieval. Knowledge Base Retrieval queries external knowledge bases for world knowledge related to the entities extracted by Entity Extraction and caches the results in the local structured knowledge base. Knowledge Rank and Utilization utilizes the results retrieved by Knowledge Base Retrieval to correct the model's output. Please refer to Appendix A.2 for details."}, {"title": "6.2 Entity Extraction", "content": "To handle complex real-world tasks, entity extraction aims to extract key entities $e$ from the input $I$, while ignoring irrelevant content (e.g., extracting \"America\" from \"Who is the president of America?\"), thus excluding the influence of input forms and preparing the retrieval entity for the next step. Specifically, to ensure system robustness, SKEME prompt uses LLMs to extract key entities $e$ from the input $I$."}, {"title": "6.3 Knowledge Base Retrieval", "content": "To accurately retrieve facts related to the entity extracted by entity extraction $e$, we use a knowledge graph (KG) $KG = \\{(s,r,o) | s,o \\in E,r \\in R\\}$, where $E$ is the set of entities and $R$ is the set of relations. The retrieval step involves extracting a subgraph $G'$ from the knowledge base $G$, where $G' = \\{(s',r', o') | (s',r', o') \\in G \\text{ and } s' = e\\}$.\nSpecifically, in knowledge base retrieval, similar to the principle of locality in computer caching systems, some knowledge may reappear across multiple queries (Jin et al., 2024). Therefore, introducing a caching system can reduce the number of data queries and improve system efficiency. Inspired by this, we utilize a fast-slow table mechanism in the knowledge base. The slow table is $G$ and the fast table is $G'' = \\{(s'',r'',o'') | (s'',r'', o'') \\in G \\text{ and } s'' \\in E\\}$, respectively. It's important to note that $G'' = \\varnothing$ initially.\nInitially, we retrieve facts related to the previously extracted entity $e$ in the $G''$. If related facts are not situated in $G''$, we then search in the slow table $G$ and update $G''$ with the retrieval results. Inspired by the principle of locality in operating systems, we not only update retrieval results in $G''$ but also update the triples related to the retrieval results in $G''$."}, {"title": "6.4 Knowledge Rank and Utilization", "content": "After retrieving the graph $G'$, we use the pre-trained Contriever (Izacard et al., 2021) model to calculate embeddings for the triples in $G'$. Subsequently, we retain the facts that are closest to the query in the embedding space, which means the facts most relevant to the query.\nInspired by Zheng et al. (2023), we use in-context learning to modify the model's output. Specifically, we integrate the triples into the input and prompt the model to utilize the triples in responding to queries. Such in-context learning enables us to tackle various tasks economically and ensures its effectiveness in models of varying sizes."}, {"title": "7 Experiment", "content": "We use the following metrics to evaluate whether editing achieves our goal in Section 3."}, {"title": "7.1 Metrics", "content": "Accuracy To calculate accuracy, we instruct the model to generate responses for tasks and evaluate whether they match the gold answers exactly. The resulting average accuracy is then recorded as exact match (EM).\nLocality Locality measures whether an editing method will influence irrelevant knowledge. We utilize drawdown (DD) (Mitchell et al., 2021, 2022) to compute performance degradation and employ Neighborhood KL divergence (NKL) (Hoelscher-Obermaier et al., 2023) to measure whether the model is significantly affected.\nSURE To meet the demands of practical application, model editing needs to consider both accuracy and side effects while also adapting to scenarios. To assess this capability, we propose the metric SURE (Statistical and Unbiased Real-world Evaluation) to estimate the performance of edited models in real-world scenarios. We define SURE as follows:\n$SURE = EM^\u03b1 \u2013 \u03b2DD^\u03b2$\nThe parameters $\u03b1$ and $\u03b2$ denote the ratio of the data used to evaluate the two metrics, $\u03b1$ and $\u03b2$ are used to characterize the importance of EM and DD, which are adjusted according to specific tasks. See Appendix F for a more detailed analysis of its motivation and advantage.\nEfficiency Efficiency measures the time and GPU space consumed by the model editing methods. Following Yao et al. (2023), we measure efficiency in both time consumption (Ti) and memory requirements (Me)."}, {"title": "7.2 Baselines", "content": "Following Yao et al. (2023), we compare SKEME with parameter-modifying methods, including FT and MEMIT (Meng et al., 2023), as well as parameter-preserving methods, including MeLLo (Zhong et al., 2023), and IKE (Zheng et al., 2023). FT is the most classic and straightforward model-editing method. MEMIT is currently considered a state-of-the-art method among parameter modification methods. IKE and MeLLo, much like SKEME, leverage a knowledge base and in-context learning. Implementation details can be found in Appendix A.3."}, {"title": "7.3 Main Result", "content": "Table 3 shows results on FAME. We experiment with all methods on GPT2-XL (Solaiman et al., 2019), GPT-J (6B) (Wang and Komatsuzaki, 2021), Llama2 (Touvron et al., 2023b), and utilize in-context learning based methods on GPT-3.5-turbo (Ouyang et al., 2022).\nFirstly, we scrutinize the results on Llama2, which is the largest model we can employ all model editing methods. FT and MEMIT, did not perform well in our experiments, which may be due to the editing process not specifically targeting the model's generative capability. MeLLo has a higher EM score than FT and MEMIT, but its DD is also the highest, indicating its pronounced side effects, which leads to a low SURE. Both IKE and SKEME obtained an EM above 0.9. However, IKE also has presented side effects that consequently decreased its SURE. SKEME uniquely maintains a high EM and simultaneously ensures a low DD, thus demonstrating superior practicality compared to other methods.\nTo test the impact of model size, we experiment with various model sizes. SKEME excels across all, while some other methods fail on small models. These model editing methods also require diverse amounts of time and GPU space. MeLLo, due to its long in-context learning process, consumes the most time in the RAG methods. MEMIT demonstrates strong capability and low side effects, but it is more time-consuming. Additionally, previous work (Yao et al., 2023) has pointed out that while MEMIT can perform batch editing, its effectiveness tends to decrease as the batch size increases. In contrast, models based on RAG and in-context learning (MeLLo, IKE, and SKEME) can easily handle batch editing without decreasing performance. Overall, SKEME proves effectiveness across model sizes while consuming less additional time and GPU space.\nIt appears that all methods performed poorly on certain tasks. This further validates the meaningfulness of constructing data in various forms. On the completion task, although the base model performed similarly to QA and Cloze, the edited model's accuracy was significantly lower than QA and Cloze. It indicates that the method's generalization performance still needs to improve."}, {"title": "8 Analysis", "content": "Considering the complexity of real world, besides single fact edits, we design a series of research questions (RQs) to evaluate the method's ability to edit multiple facts. We discuss fact transitions (RQ1), e.g., the U.S. President transitioning from Obama \u2192 Trump \u2192 Biden, fact inference (RQ2), e.g., inferring the fact \u201cthe First Lady of the U.S. is Jill Biden\" from the given facts \u201cthe U.S. President is Biden\u201d and \u201cBiden's spouse is Jill Biden\u201d, fact with substantial quantity (RQ3), e.g., needing to update thousands of facts and fact from various benchmarks (RQ4), e.g., facts from other datasets."}, {"title": "8.1 RQ1: How Do Methods Handle Transitions Between Facts?", "content": "Many facts in the real-world transition require multiple edits to the same fact. To evaluate the effectiveness of repeated edits, we perform multiple updates for each fact and tested the accuracy of the edited model. Figure 3 shows the experimental results. The results indicate that even with only two edits to the same fact, the accuracy of all other methods substantially declines. Parameter-modifying methods can lead to divergence from the initial model parameters and subsequent performance decline due to repeated adjustments of specific parameters. For methods not to modify the parameters, failure to update the knowledge base may result in conflicts between existing and newly added facts.\nSKEME uses a structured knowledge base to facilitate precise updates, making iterative updates possible. SKEME is the only one capable of handling iterative updates."}, {"title": "8.2 RQ2: Can The Edited Model Infer New Facts Based on Given Information?", "content": "The edited model should be able to make further reasoning based on the edited facts. Following previous research (Zhong et al., 2023), We employ multi-hop questions to evaluate this capability of the model."}, {"title": "8.3 RQ3: Can Methods Handle Updates with a Substantial Amount of Facts?", "content": "In the real world, there are numerous updates to facts and a practical model editing method should be able to update a vast quantity of knowledge in the model.\nFigure 4 shows the experimental results. As the quantity of modified facts increases, parameter-modifying methods progressively shift away from their original state, resulting in a notable performance decline. In contrast, other methods, exhibit only slight declines in performance and can handle updates to a large number of facts."}, {"title": "8.4 RQ4: Can Methods Generalize Across Facts in Various Benchmarks?", "content": "To demonstrate the general applicability of the editing method, we select several datasets that are widely used to evaluate LLMs' understanding of the world, and subsequently evaluate model editing methods on them. Please refer to Appendix A.4 for details about these datasets."}, {"title": "9 Conclusion", "content": "We introduce the practicality requirement for model editing and created a novel benchmark FAME, which embodies practicality with factual data and diverse tasks. We propose a model editing method, SKEME, that proves effective across various LLMs and tasks. The experiments demonstrate that previous model editing methods struggle in dealing with real-world challenges, while SKEME successfully addresses these challenges. We hope that our work will advance the field of model editing and inspire further research in this area."}, {"title": "Limitations", "content": "The data in FAME is limited to a monolingual scope and does not include multilingual data. We posit that the inclusion of multilingual data can further align with the real world, and we leave this as a potential area for future work."}, {"title": "Ethics Statement", "content": "We ensure that the collection of FAME is done in a manner consistent with the terms of use stipulated by its sources and the intellectual property rights of the original authors. We make sure that individuals involved in the collection process are treated fairly, including ensuring their voluntary participation and informed consent. Due to the dynamic nature of the real world, certain knowledge contained in FAME may become outdated, rendering it no longer reflective of the latest world conditions."}, {"title": "A.1 FAME Construction Details", "content": "This section provides a detailed description of how we constructed FAME."}, {"title": "A.1.1 Collect Fact Triples", "content": "To enhance the diversity of FAME and reduce bias from a single data source, We use WikiData SPARQL query\u00b2 and DBpedia SPARQL query\u00b3 to collect data from Wikidata and DBpedia.\nFirstly, using the code in Figure 5, we query for relationships with equivalent meanings in Wikidata and DBpedia, such as \"birth place\" (from DBpedia) and \"place of birth\" (from Wikidata). They are connected through the relationship equivalentProperty. Subsequently, we filter out relationships like identifiers, where their objects are typically composed of irregular and meaningless combinations of letters and numbers.\nNext, based on the obtained r, we use code in Figure 6 and 7 to collect triples (s, r, o) from Wikidata and DBpedia. We filter out triples that may cause ambiguity, including two aspects: two different items having the same name or a specific entity's relation corresponding to multiple objects. Relevant discussions can be found in Appendix A.1.2.\nLastly, we manually extract 100 distinct triples and verify them against other data sources such as government websites to ensure the accuracy and real-world relevance of our collected triples. The results show that 96% of the data is correct. Hence, we can infer that FAME can reasonably reflect real-world scenarios."}, {"title": "A.1.2 Data Filter", "content": "Ambiguity issues involve two aspects: different entities sharing the same name and a specific entity's relation corresponding to multiple objects.\nFor the former scenario, one example is: Hope Springs could refer to a movie from 2012 (Q327214 in Wikidata)4, but can be a movie from 2003 as well (Q596646 in Wikidata)5. So when asking Who is the director of Hope Springs?, there are multiple correct options.\nAn example of the latter scenario is: a person may have multiple children, so there are multiple correct answers when asking for their children's names.\nWe believe that the above two scenarios are simpler compared to questions with only one answer. Therefore, for easier implementation and to focus on more fundamental phenomena, we excluded data in the dataset containing instances of the above situations."}, {"title": "A.1.3 Generate Data Based on Templates", "content": "Following previous work, after obtaining triples, we need to construct relationship templates to build our entire dataset. For single-hop data, we use the following triple as an example to illustrate our entire construction process: (subject, relation, object) = (America, head of government, Biden).\nWe construct several templates for each relation for each task. For instance, when (r = head of government), the template for the QA task might be \"Who is the head of government in {}?\", and the template for the completion task might be \"In {}, the head of government is\u201d. During usage,"}, {"title": "A.2 SKEME Details", "content": "We introduced a novel caching mechanism and subject extraction to SKEME . Inspired by computer cache systems, the caching mechanism utilized by SKEME ensures that the stored knowledge is up-to-date while facilitating fast retrieval. Subject extract techniques allow SKEME to retrieve stored knowledge more precisely than previous techniques. In this section, we present the details of SKEME."}, {"title": "A.2.1 Entity Extraction", "content": "Entity extraction aims to extract key entities from the provided input. Previous research has extensively explored methods such as NER or entity-linking (Wu et al., 2020). We use LLMs to assist us in completing this task. Results indicate that this subtask can easily attain an accuracy rate exceeding 97% on SKEME. The accuracy statistics of entity extraction on SKEME are depicted in the table 6 and the prompt is in the figure 8."}, {"title": "A.2.2 Knowledge Base Retrieval", "content": "The local knowledge base is stored in the form of a knowledge graph. When updating the local knowledge base, it can be automatically updated from the external database or manually injected with certain facts to reflect real-world changes. Such updates may require a considerable amount of time, but they can be done in parallel in arbitrary quantities and during idle times. Consequently, we did not explicitly evaluate the duration dedicated to this aspect."}, {"title": "A.2.3 Knowledge Rank and Utilization", "content": "Following previous works (Zhong et al., 2023; Zheng et al., 2023), we rank the retrieved knowledge based on similarity to the input and select the top-k knowledge. In our experiments, we set k = 1. We prompt the model to use the retrieved knowledge for updating its output, which is shown in figure 9. To ensure that the model's output meets the task requirements, we added a task prompt before all prompts, as shown in Figure 10.\nWe utilized an off-the-shelf retrieval model (Izacard et al., 2021) to identify and rank the fact triplets, which allows us to avoid the training process."}, {"title": "A.3 Implementation Details of Baselines", "content": "For FT, MEMIT, and IKE, we use the framework provided by Wang et al. (2024a). For Mello, we used the original implementation but modified the prompt to fit tasks. 7\nFT Following previous works (Meng et al., 2023), We apply Fine-Tuning (FT) to the given layer of the model. For GPT2-XL, we select layer 0, and for GPT-J and Llama2, we choose layer 21.\nMEMIT For GPT2-XL and GPT-J, we employ default hyperparameters. For Llama2, we update the parameters of layers {4, 5, 6, 7, 8}. Across all models, we calculate covariance statistics using 50,000 instances from Wikitext.\nMeLLo The original method was designed for multi-hop questions. We redesign the prompt for each task while keeping the knowledge retrieval part unchanged.\nIKE In the original paper, relevant facts were directly added to the prompt. To make a fair comparison, we removed this part and ensured that all facts were retrieved. Our retrieval settings remained consistent with the original paper.\nOther Baselines SERAC (Mitchell et al., 2022) and EREN (Chen et al., 2024) are two strong baselines. However, SERAC requires a significant amount of time for retraining (Yao et al., 2023), making it difficult to handle frequently updated requests. EREN is suitable for models that have undergone instruction fine-tuning, while SKEME and others focus on base models. Therefore, we did not include these two baselines in the comparison."}, {"title": "A.4 Other Benchmarks", "content": ""}]}