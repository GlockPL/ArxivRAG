{"title": "Federated Knowledge Transfer Fine-tuning Large Server Model with Resource-Constrained IoT Clients", "authors": ["Shaoyuan Chen", "Linlin You", "Rui Liu", "Shuo Yu", "Ahmed M. Abdelmoniem"], "abstract": "The training of large models, involving fine-tuning, faces the scarcity of high-quality data. Compared to the solutions based on centralized data centers, updating large models in the Internet of Things (IoT) faces challenges in coordinating knowledge from distributed clients by using their private and heterogeneous data. To tackle such a challenge, we propose KOALA (Federated Knowledge Transfer Fine-tuning Large Server Model with Resource-Constrained IoT Clients) to impel the training of large models in IoT. Since the resources obtained by IoT clients are limited and restricted, it is infeasible to locally execute large models and also update them in a privacy-preserving manner. Therefore, we leverage federated learning and knowledge distillation to update large models through collaboration with their small models, which can run locally at IoT clients to process their private data separately and enable large-small model knowledge transfer through iterative learning between the server and clients. Moreover, to support clients with similar or different computing capacities, KOALA is designed with two kinds of large-small model joint learning modes, namely to be homogeneous or heterogeneous. Experimental results demonstrate that compared to the conventional approach, our method can not only achieve similar training performance but also significantly reduce the need for local storage and computing power resources.", "sections": [{"title": "Introduction", "content": "Models with ever-growing scale have been introduced, such as BERT [Devlin et al., 2018; Liu et al., 2019], GPT [Radford et al., 2018; Radford et al., 2019; Brown et al., 2020], VGG [Simonyan and Zisserman, 2014], and ViT [Dosovitskiy et al., 2020]. To train and adopt them in various Internet of Things scenes, how to utilize distributed data and computing powers becomes crucial. Unfortunately, IoT clients typically exhibit data protection considerations [Chen et al., 2023; Zhuang et al., 2023] and constrained computing capacities [Wang et al., 2019; Imteaj et al., 2021]. These factors impede the use of their data to train complex and large-scale models.\nTo tackle the challenge of data privacy, solutions based on federated learning (FL) are studied to support the training of large models in a collaborative and privacy-preserving manner, e.g., Yu S et al. [Yu et al., 2023] propose a method of training the large model alternately in clients with private data and the server with labeled public data; and Wu Cet al. [Wu et al., 2022] introduce a method of federated mutual distillation for training personalized large models, which can significantly reduce communication costs. Even though private knowledge can be shared among distributed clients through FL, the common premise of current methods is to have sufficient local computing capacities to run large models directly on each learning client, making them infeasible to support distributed IoT clients with insufficient local resources.\nTherefore, to support the fine-tuning of large models [Houlsby et al., 2019; Han et al., 2024] and the model adaptation to empower various IoT scenarios, the objective of this study is defined as illustrated in Fig 1, where 1) the server has sufficient storage and computing powers but lacks high-quality data (only with a limited amount of unlabeled proxy dataset), and 2) IoT clients as a group are rich in sensed data and distributed computing powers, but as for each client, its device and private data are heterogeneous, and its local resources are limited to support the running of large models.\nBy integrating FL to share private knowledge across IoT clients and knowledge distillation (KD) to transfer encoded knowledge among different models (i.e., between teacher and student models), KOALA is proposed to enable a joint and iterative learning process that allows the IoT clients to run their local small models to extract and share local knowledge, and then the server to update the adapter of the large model based on the local updated small model of each client. Specifically, to implement such a learning process, the forward and reverse distillation techniques are used jointly, to, first, reverse distill trained small models to fine-tune the large model, and then, forward distill the large model to update small models for IoT clients.\nMoreover, in conventional FL, the global and local models have the same structure, and the global model can be updated based on the aggregation of local updates directly. However, the large-small model collaborative learning process implemented in KOALA needs to support different models in the server and clients, which makes conventional FL methods infeasible. Hence, according to the difference among small models, KOALA implements two kinds of learning mode to aggregate local knowledge encoded in homogeneous or heterogeneous small models. Specifically, the homogeneous method supports IoT clients to run small models with the same structure, and on the contrary, the heterogeneous method supports each IoT client to run different small models, which are more flexible as they can be created according to the actual computing capacity of the client. After the update of the large model, by using either homogeneous or heterogeneous methods, related small models can distilled from the latest large model and dispatched to their corresponding clients to start a new learning iteration.\nBased on standard datasets, the efficiency and effectiveness of KOALA are evaluated. Experimental results show that compared with the baseline, where IoT clients can load and execute the large model with sufficient local resources, our method can approach similar training performance for all tasks, and also significantly reduce the need for local resources.\nIn general, our main contributions can be summarized as follows:\n\u2022 We propose a novel large-small model collaborative learning process in data protection and resource-constrained IoT scenarios, through which, FL and KD can work jointly to support the iterative learning of large and small models even though they are cross-scale in model structures;\n\u2022 We design a reverse knowledge distillation strategy to better handle the outputs of heterogeneous small models updated based on local data, through which, the outputs of local models on proxy datasets are refined and integrated to generate consensus soft labels for large model fine-tuning;\n\u2022 The proposed method KOALA is verified to be performance-equivalent and resource-efficient. Specifically, large models fine-tuned by KOALA can achieve similar accuracy to the ones updated in conventional methods. At the same time, compared to conventional methods, the storage space needed for loading the local model reduces by about 97.6% (Homo) and 97.2% (Hete), and FLOPs of the local model reduces by about 98.4% (Homo) and 98.6% (Hete)."}, {"title": "Related Work", "content": "Federated learning is a privacy-preserving machine learning framework where the server coordinates multiple clients to learn globally shareable models without exchanging local data directly [Zhang et al., 2021]. As the classic method, FedAvg [McMahan et al., 2017] manages each client to train its local model and upload the updated local model to the server. Then, the local models are aggregated to update a global model, which is then downloaded by active clients in the next round. However, the issue of non-identically and independently distributed (Non-IID) data among clients degrades the performance of federated learning [Mora et al., 2022a], prompting numerous methods that aim to alleviate this problem. Accordingly, FedProx [Li et al., 2020] introduces a proximal term to the loss function in local training, to constrain the updating of model parameters. SCAFFOLD [Karimireddy et al., 2020] introduces control variables to reduce \"client drift\". MOON [Li et al., 2021] combines federated learning and contrastive learning to make the local model updating closer to the global model and farther away from the previous local model. Since highly heterogeneous data may prevent the model from converging, and a common global model fails to meet the individual needs of different clients, personalized federated learning is essential [Tan et al., 2022]. FedClassAvg [Jang et al., 2022] conducts federated learning on heterogeneous models through classifier aggregation. Per-FedAvg [Fallah et al., 2020] incorporates the classic meta-learning framework, MAML [Finn et al., 2017], to train personalized models based on the global meta-model. Differently, PFedMe [T Dinh et al., 2020] does not utilize the global model directly, but instead concurrently trains the global model and personalized models."}, {"title": "2.2 Knowledge Distillation", "content": "Hinton et al. have first introduced knowledge distillation [Hinton et al., 2015]. Their work employs a weighted sum of the hard and soft loss as the complete loss. The soft loss is the loss between the soft outputs of the student model and the soft labels generated by the teacher model, and the hard loss is the loss between the hard outputs of the student model and the real labels. Adriana Romero et al. [Adriana et al., 2015] introduce knowledge distillation based on hidden layer knowledge features (hints). Zhang et al. [Zhang et al., 2018] propose mutual distillation, enabling different models to mutually distill knowledge from one another."}, {"title": "2.3 Federated Knowledge Distillation", "content": "Knowledge Distillation has gained increasing attention to integrating with Federated Learning [Mora et al., 2022b]. FedMD [Li and Wang, 2019] makes the integration based on a shared dataset to calculate mean scores that guide the knowledge distillation of each client. Instead, FD [Jeong et al., 2018] eliminates the need for a shared dataset and allows clients to calculate prediction scores for each label on their local dataset, and the server to calculate the global mean prediction score per label, which serves as soft labels during the local distillation. FedGKT [He et al., 2020] combines federated learning with split learning (SL) [Gupta and Raskar, 2018]. FedDKC [Wu et al., 2024] is similar to FedGKT and can reduce the gap between knowledge distributions of the heterogeneous models. Although FedGKT and FedDKC can support resource-constrained clients, both methods require local real labels to be uploaded, which compromises client privacy. Moreover, their target is to train the small model under the guidance of large models, instead of considering how to integrate knowledge extracted from different clients to update the large model efficiently and effectively."}, {"title": "Methodology", "content": ""}, {"title": "3.1 Problem Statement", "content": "Suppose there are N clients i (i = 1,2,..., N), each of which has its private dataset with labels j = 1,2,..., C. The sample size of client i is ni. To support the classification tasks, the key goal, defined in Equation 1, is to minimize the loss difference between the large model updated by our method suppose constrained local resources and the conventional one suppose sufficient local resources, where \u03a9 and Conv are the large model trained by our method and the conventional one, respectively, L() is the loss function and D is the test dataset.\narg min F(\u03a9) = \\frac{L(\u03a9, D) \u2013 L(Conv, D)}{|D|} (1)"}, {"title": "3.2 Motivation", "content": "Our method is based on this intuition: the small model can be viewed as the local private knowledge extractor that can be used at the server to transfer knowledge embedded within private data to the large model.\nTo verify our intuition, we design a simple experiment where in each round, the small model is trained by a labeled dataset, and then a large model is fine-tuned based on a proxy dataset through knowledge distillation with the small model as the teacher model and the large model as the student model. Note that CIFAR-10 is used for small model training and its test dataset is used for evaluating the performance of the large model. Moreover, the small and large models are MobileNet V3 Small and VGG19, respectively.\nAccording to the result shown in Fig 2, we can observe that the accuracy of large models can be improved significantly, even though it only processes the unlabelled proxy dataset. Therefore, the small model can share local private knowledge with the large model based on the knowledge extraction and transfer process, which motivates us to design KOALA that can integrate federated learning and knowledge distillation to implement a large-small model collaborative learning process."}, {"title": "3.3 The proposed method: KOALA", "content": "In KOALA, we implement a large-small model collaborative learning process, through which, small models serve as local knowledge extractors and the large model is fine-tuned according to the distilled knowledge from small models. Specifically, in each IoT client, the corresponding small model is downloaded from the server and trained locally based on its private data. In the server, a bi-directional knowledge distillation mechanism is introduced, which supports 1) the reverse distillation to fine-tune the large model based on small models, and 2) the forward distillation to update small models based on the large model.\nAs shown in Fig. 3, KOALA consists of three steps, namely 1) Local Knowledge Extraction, 2) Reverse Knowledge Distillation, and 3) Forward Knowledge Distillation. Since the IoT clients can be heterogeneous in not only their data but also their computing capacities, KOALA is designed with two kinds of learning modes, namely one for homogeneous small models (denoted as homo), and the other one for heterogeneous small models (denoted as hete)."}, {"title": "Local Knowledge Extraction", "content": "In this step, small models either homo or hete are updated according to the private data of corresponding IoT clients. After the extraction of local knowledge, small models are uploaded to the server."}, {"title": "Reverse Knowledge Distillation", "content": "After all the local updated small models are collected, the server starts the reverse distillation, in which, the large model serves as the student model, and the small model serves as the teacher model.\nSpecifically, in the homo mode, the small models are aggregated to first generate the global small model w, and then used to produce soft labels as defined in Equation 2 based on the proxy data x, where T is distillation temperature.\nsoftmax(\\frac{f(x, w)}{T}) (2)\nThe global small model w transfers local knowledge to the large model \u03a9, where the large model only updates its adapter. The reverse distillation loss $loss_{homo}^r$ used in the homo mode is defined in Equation 3, where $I_{KL}()$ is KL loss function.\n$loss_{homo}^r = I_{KL}(softmax(\\frac{f(x, \\tilde{\\Omega})}{T}), softmax(\\frac{f(x, \\Omega)}{T}))$\n(3)"}, {"title": "", "content": "Since heterogeneous small models cannot directly be aggregated, in the hete mode, the output distributions of small models are refined and integrated to generate the consensus soft labels.\nTo mediate the heterogeneity within output distributions, we introduce a distribution refinement strategy. Suppose within output distribution f(x, wi), the maximum and minimum value is zi,max, Zi,min, respectively, and the value for label j is zi,j, the refined value $2_{i,j}$ is defined in Equation 4, where wi is the i-th small model (for client i), and k is the coefficient to support the refinement.\n$2_{i,j} = k \\frac{Zi,j - Zi,min}{Zi,max - Zi,min}$ (4)\nTo sum up the refined values for all labels, we can get\n$\\sum_{j=1}^C \\tilde{Zi,j} = \\frac{k \\sum_{j=1}^C (Zi,j - Zi,min)}{Zi,max - Zi,min} = \\frac{k C (\\bar{Zi} - Zi,min)}{Zi,max - Zi,min}$ (5)\nIn Equation 5, Zi is the mean value of output distribution f(x, wi). Suppose the mean values of refined distributions of all the small models are equal to A (which is a constant), and therefore,\nA = \\frac{\\sum_{j=1}^C \\tilde{Zi,j}}{C} = k \\frac{\\bar{Zi} - Zi,min}{Zi,max - Zi,min} (6)\nThen, the coefficient k can be calculated.\nk = A \\frac{Zi,max - Zi,min}{\\bar{Zi} - Zi,min} (7)\nWe substitute it to Equation 4, and get the distribution refinement strategy as\n$\\tilde{Zi,j} = A \\frac{Zi,j - Zi,min}{\\bar{Zi} - Zi,min}$ (8)\nAccording to Equation 8, we get the refined output distributions $\\tilde{Z_i}$ = {$zi,1, Zi, 2, ..., Zi,c$}. Then, we obtain the integrated output distributions among small models through Equation 9, donated as \u2248. Suppose set of active clients is S in this round.\n$\\tilde{Z} = \\frac{\\sum_{i \\in S} Ni \\tilde{Zi}}{\\sum_{i \\in S} Ni}$ (9)\nBased on 2, the consensus soft labels are calculated.\nsoftmax(\\frac{\\tilde{Z}}{T}) (10)\nThen, we fine-tune the large model \u03a9 based on the reverse distillation loss $loss_{hete}^r$ as defined in Equation 11.\n$loss_{hete}^r = I_{KL}(softmax(\\frac{\\tilde{Z}}{T}), softmax(\\frac{f(x, \\Omega)}{T}))$ (11)"}, {"title": "Forward Knowledge Distillation", "content": "Following the reverse distillation, we implement the forward distillation to update the small model according to the updated large model, where the large model serves as the teacher model, and the small model serves as the student model.\nTo calculate the forward distillation loss, the output feature loss (the loss between the output layers) and the hidden feature loss (the loss between the hidden layers) need to be calculated.\nIn the homo mode, the global small model w is the student model to be updated. \u03a9h represents the first h layers within the larger model, whereas w\u00ba represents first g layers within the global small model. Accordingly, the output feature loss $loss_{homo}^f$ and hidden feature loss $loss_{homo}^h$ are computed according to Equations 12 and 13, respectively, where W is the bridging matrix and $I_{MSE}()$ is MSE loss function.\n$loss_{homo}^f = I_{KL}(softmax(\\frac{f(x, \\Omega)}{T}), softmax(\\frac{f(x, w)}{T})) (12)\n$loss_{homo}^h = I_{MSE}(f(x, \u03a9^h), f(x, w^g)W)$ (13)\nTherefore, the sum of $loss_{homo}^f$ and $loss_{homo}^h$ forms the forward distillation loss $loss_{homo}^{fo}$ as defined in Equation 14, where \u03bb is a constant.\n$loss_{homo}^{fo} = loss_{homo}^f + \u03bbloss_{homo}^h$ (14)\nIn the hete mode, each small model wi(i \u2208 S) serves as the student model undergoing knowledge distillation for the update. Suppose the i-th small model wi is the student model, w represents first g layers within w\u2081 and Wi is the bridging matrix for wi, the output feature loss $loss_{out,i}^h$ and hidden feature loss $loss_{hid,i}^f$ for the i-th small model wi can be calculated according to Equations 15 and 16, respectively.\n$loss_{out,i}^h = I_{KL}(softmax(\\frac{f(x, \\Omega)}{T}), softmax(\\frac{f(x, Wi)}{T}))$ (15)\n$loss_{hid,i}^f = I_{MSE}(f(x, \u03a9^h), f(x, wi^g)W_i)$ (16)\nAccordingly, the forward distillation loss for the i-th small model $loss_{fo}^f$ is\n$loss_{ete}^{fo} = loss_{out,i}^h + \u03bbloss_{hid,i}^f$ (17)\nFinally, either in homo or hete mode, the small model is updated based on its forward distillation loss and after the update, it is dispatched to the related client to start a new learning round until certain criteria are met (e.g., the model converges or the maximum learning round is reached)."}, {"title": "Experimental Results", "content": ""}, {"title": "4.1 Setup", "content": "We introduce the experimental setup in 4 key aspects: models, datasets, baseline, and hyperparameters.\nModels. We select TorchVision backbones\u00b9 and append the classifier onto the last layer of each backbone to form the large model and small models used in our experiments. The classifier of the large model is viewed as the adapter. The backbone for the large model is VGG19. In our homo mode, the small model is MobileNet V2. In our hete mode, the small models are MobileNet V2, MobileNet V3 Small, EfficientNet BO, ShuffleNet V2 X0_5, and ShuffleNet V2 X2_0, respectively. Moreover, we implement additional experiments to count the model FLOPs, where we use 64\u00d764 randomly generated \"image\" as the input.\nDatasets. We select 4 datasets: CIFAR-10 [Krizhevsky et al., 2009], Fashion-MNIST [Xiao et al., 2017], USPS [Hull, 1994], and GTSRB [Stallkamp et al., 2012]. The entire test set of each dataset is used to evaluate the large model, recording its performance before training (round 0) and at the end of each learning round. The proxy dataset is a subset of the original train set by removing the labels. The local datasets of clients are obtained by Dirichlet Distribution, with the concentration parameter of 1.0. In addition, there is no overlap between the proxy dataset and private client datasets.\nBaseline. We set a baseline under the assumption that all IoT clients have sufficient local resources to run the large model directly, and Federated Averaging (FedAvg) [McMahan et al., 2017] is used to update the global model. Specifically, the workflow of the baseline to update the global model consists of three steps, namely: 1) clients download the global large model; 2) the large model is fine-tuned locally; and 3) the large model parameters are uploaded to the server for the global aggregation. During client-server interactions, the adapter instead of the entire model is exchanged between the server and clients, except for the first download of the large model from the server to the clients.\nHyperparameters. We consider a scenario involving 5 clients and 1 server. Adam is selected as the optimizer and the distillation temperature is set to 7 in all the experiments. For the baseline, the learning rate for local fine-tuning is 0.001, and weight decay is 0.000001. In KOALA, for reverse distillation, the learning rate is 0.001, and the weight decay is 0.000001; and for forward distillation, the learning rate is 0.0001, and the weight decay is 0.000001. For the output distribution refinement in the hete mode, the mean value A is set to 2."}, {"title": "4.2 Loss and Accuracy", "content": "The loss curves are illustrated in Figure 4, and the accuracy of different methods is listed in Table 1. It is remarkable fact that our method demonstrates optimal performance in the CIFAR-10 and Fashion-MNIST tasks, closely approaching the baseline both in loss reduction and model accuracy."}, {"title": "4.3 Ablation in Bi-directional Distillation", "content": "During the bi-directional distillation in the server, the small model transfers local knowledge to the large model in reverse distillation, and the large model updates the small model in forward distillation. Reverse distillation is indispensable for fine-tuning the large model, and forward distillation also matters for the update of small models, which work jointly making the iterative learning between large and small models workable."}, {"title": "4.4 Demands for Storage and Computing Power", "content": "Table 2 shows the Params and FLOPs of the models during the experiments. The classifiers for different tasks may have a slight difference. When we demonstrate the Params and FLOPs, we use the delimiters to separate the value of the model for the CIFAR-10, Fashion-MNIST, and USPS tasks from that for the GTSRB task. In addition, Fig 6 shows the storage space needed to load related models to be trained.\nSince the small models have much fewer parameters than the large model, the mean storage space for all clients reduces by 97.6% (Homo) and 97.2% (Hete). We can also observe that FLOPs of the large model is significantly higher than that of each small model. The mean FLOPs of the local models of all clients (calculated according to models for CIFAR-10/Fashion-MNIST/USPS tasks) reduces by 98.4% (Homo) and 98.6% (Hete).\nIn summary, the storage and computing power required for the model to be loaded and executed locally are much lower than the ones needed for the baseline, which proofs the efficiency and effectiveness of KOALA in supporting various IoT scenarios consisting of large amount of heterogeneous IoT clients."}, {"title": "Conclusion", "content": "To fine-tune large models by orchestrating distributed IoT clients with limited storage space or computing capabilities, we propose KOALA, a privacy-preserving and resource-efficient method that integrates federated learning and knowledge distillation by implementing a novel large-small model collaborative learning process. In general, it uses small models to extract private knowledge without having large models running on IoT clients. Moreover, it also supports the knowledge transfer between the large model and small models by implementing a bi-directional distillation, in which, small models can be updated according to the large model through the common forward distillation, and also the large model can be fine-tuned by reverse distillation by aggregating knowledge from either homogeneous or heterogeneous small models. Experimental results show that compared to the conventional method, KOALA can significantly reduce the demands for local storage space and computing power to fine-tune large models with competitive performance."}]}