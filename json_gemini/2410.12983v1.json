{"title": "Reinforcement Learning with Euclidean Data Augmentation for State-Based Continuous Control", "authors": ["Jinzhu Luo", "Dingyang Chen", "Qi Zhang"], "abstract": "Data augmentation creates new data points by transforming the original ones for a reinforcement learning (RL) agent to learn from, which has been shown to be effective for the objective of improving the data efficiency of RL for continuous control. Prior work towards this objective has been largely restricted to perturbation-based data augmentation where new data points are created by perturbing the original ones, which has been impressively effective for tasks where the RL agent observes control states as images with perturbations including random cropping, shifting, etc. This work focuses on state-based control, where the RL agent can directly observe raw kinematic and task features, and considers an alternative data augmentation applied to these features based on Euclidean symmetries under transformations like rotations. We show that the default state features used in exiting benchmark tasks that are based on joint configurations are not amenable to Euclidean transformations. We therefore advocate using state features based on configurations of the limbs (i.e., the rigid bodies connected by the joints) that instead provide rich augmented data under Euclidean transformations. With minimal hyperparameter tuning, we show this new Euclidean data augmentation strategy significantly improves both data efficiency and asymptotic performance of RL on a wide range of continuous control tasks. Our code is available on GitHub\u00b9.", "sections": [{"title": "Introduction", "content": "While reinforcement learning (RL) has enjoyed impressive success on continuous control problems, especially when empowered by expressive function approximators like deep neural networks, improving its notoriously poor data efficiency remains challenging. Recently, exploiting the idea of data augmentation, the RL community has made significant progress on improving data efficiency as well as the asymptotic performance of RL for continuous control with the agent observing images as the state representations [1\u20134]. CURL [1] utilized data augmentation for learning contrastive representations [5, 6] out of their image encoder, as any auxiliary learning objective in the RL setting. Other works, such as DrQ [3, 4] and RAD [2], directly used image data augmentations for RL without any auxiliary objectives. Despite these technical differences in how data augmentation is integrated into RL, these works share the key procedure of perturbation-based image augmentation: to create new data for learning, the image in a state representation is transformed by applying perturbations such as random crop, random shift, color jitter, etc.\nInstead of image-based continuous control, this paper focuses on the more primitive problem of state-based continuous control, where the agent can observe raw physical quantities such as position and velocity as the features to represent the underlying world state. Like the image-based case, existing work on state-based data augmentation has been relying on introducing perturbations to original state"}, {"title": "Related work", "content": "Data augmentation for image-based continuous control. Existing works in data augmentation for RL mostly have focused on the online setting for image-based continuous control tasks. This is because it is relatively straightforward to obtain augmented images (e.g., through random crop-ping/shifting), which enables learning representations of the high-dimensional image input that facilitates optimizing the task reward. In this line of work, CURL [1] utilizes contrastive representa-tion learning to the image representations, jointly with the RL objective of reward optimization; RAD [2] and DrQ [3, 4] use the augmented data directly for RL without any auxiliary objective. SVEA [14] focuses on improving stability and sample efficiency under image-based data augmentation by mitigating the high-variance Q-targets. CoIT [15] works on maintaining the invariant information unchanged under image-based data augmentation. Different tasks often benefit from different types of data augmentation, but the manual selection is not scalable. The work of [16] automatically applies the types of data augmentations that are most suitable for the specific tasks.\nData augmentation for state-based continuous control. This paper focuses on data augmentation for state-based continuous control, which only a few prior works have explored. Specifically, RAD explores the augmentation of injecting noise to state variables through additive Gaussian or random amplitude scaling [2]. Different from these unprincipled, perturbation-based augmentation, this paper advocates a principled augmentation method through Euclidean symmetries for state-based continuous control. Corrado et al. [17, 18] also consider principled data augmentation transformations that are not"}, {"title": "Preliminaries", "content": "We formulate a state-based continuous control task (or, simply, task) as a Markov decision process (MDP), defined as a tuple (S, A, p, r, \u03b3). At each discrete time step t, the agent fully observes the state in the continuous state space, $s_t \\in S \\subset \\mathbb{R}^d$, and chooses an action in the continuous action space to take, $a_t \\in A \\subset \\mathbb{R}^m$; the next state is sampled from the transition dynamics p with conditional probability density $p(s_{t+1}|s_t, a_t)$, while the reward function $r : S \\times A \\rightarrow \\mathbb{R}$ yields a scalar feedback $r_t := r(s_t, a_t)$. The agent uses a policy $\\pi : S \\rightarrow \\Delta(A)$ to choose actions where $\\Delta(A)$ is the collection of probability measures on A, yielding a trajectory of states, actions, and rewards, $(s_0, a_0, r_0, s_1, ...)$, where the initial state $s_0$ is sample from an initial state distribution with density $d_0$ and $a_t \\sim \\pi(\\cdot|s_t)$ Without prior knowledge of p and r, the reinforcement learning (RL) agent's goal is to obtain a policy from its experience (i.e., yielded trajectories) which maximizes the expected cumulative discounted reward, $\\mathbb{E}_\\pi[\\sum_{t=0}^\\infty \\gamma^t r_t]$ where $\\gamma \\in [0, 1)$. The online RL setting assumes the agent has the ability to interact through the MDP to obtain an increasing amount of trajectories while refining its policy."}, {"title": "Deep Deterministic Policy Gradient", "content": "Deep Deterministic Policy Gradient (DDPG) [13] is a state-of-the-art online RL algorithm for continuous control, which parameterizes and learns a critic $Q_\\theta : S \\times A \\rightarrow \\mathbb{R}$ and a deterministic policy $\\pi_\\phi : S \\rightarrow A$. The critic is trained by minimizing the one-step temporal difference (TD) error $\\mathcal{L}(\\theta) = \\mathbb{E}_{(s_t, a_t, r_t, s_{t+1})\\sim D_{online}}[(Q_\\theta(s_t, a_t) - r_t - \\gamma Q_{\\bar{\\theta}}(s_{t+1}, \\pi_{\\phi}(s_{t+1}))^2]$, where $D_{online}$ is the replay buffer storing past trajectories and $\\bar{\\theta}$ is an exponential moving average of the critic parameter, referred to as the target critic. Concurrently, the policy is trained by employing Deterministic Policy Gradient (DPG) [23] and minimizing $\\mathcal{L}(\\phi) = \\mathbb{E}_{s_t \\sim D_{online}}[-Q_\\theta(s_t, \\pi_\\phi(s_t))]$, so that $\\pi_\\phi(s_t)$ approximates $\\arg \\max_a Q_\\theta(s_t, a)$. The policy, after adding noise for exploration, is used as a data-collection policy"}, {"title": "Perturbation- and symmetry-based data augmentation in RL", "content": "Perturbation-based data augmentation. Data augmentation generates new, artificial data through transformations of original data. Better data efficiency can often be achieved by learning from both the original and augmented data. For mastering continuous control tasks via RL, prior works perform data augmentation by perturbation, through 1) transformations of adding noise to original data, examples including performing random image cropping, color jittering, translations, and rotations when states can only be observed and represented as images (i.e., image-based continuous control) [24, 25, 1-4] and 2) transformations of adding Gaussian noise and random amplitude scaling when states (e.g., positions and velocities) can be directly observed by the agent [2].\nSymmetry-based data augmentation. This work focuses on state-based continuous control and departs from prior works by performing data augmentation through MDP symmetries [7, 8]. Symmetries in an MDP, if existing, can be described by a set of transformations on the state-action space indexed by $g \\in G$, where such a transformation leaves the transition dynamics and reward function invariant. Formally, we define a state transformation and a state-dependent action transformation as $T_g^S : S \\rightarrow S$ and $T_g^A : A \\rightarrow A$, respectively, and the invariance is expressed accordingly as, for all $g \\in G, s, s' \\in S, a \\in A$,\n$$p(s'|s, a) = p (T_{g,s}[s'] | T_{g,s}[s], T_{g,s}[a]) \\quad and \\quad r(s, a) = r (T_{g,s}[s], T_{g,s}[a]).$$\nThe invariance naturally makes these transformations sound and effective for data augmentation pur-poses, especially in continuous control tasks where transition dynamics is often (near-)deterministic.\nNext, we detail our data augmentation method based on Euclidean symmetries for continuous control."}, {"title": "Our method: Euclidean data augmentation for continuous control", "content": "We consider the continuous control problem for an agent (e.g., a robot) consisting of n rigid bodies, referred to as limbs, that are connected by joints to form a certain morphology and can move in the 2D planar space or the 3D space. Examples of such agents include 2D Cheetah and 2D Hopper from DeepMind Control Suite (DMControl) [12] as well as their 3D variants [26], which will be presented in our experiments. Without loss of generality, we present our method in the 3D setting.\nTree-structured morphology and actuation. The connected limbs form a certain morphology that can be represented as an undirected graph, where each node $i \\in \\{1,\\ldots,n\\}$ represents a limb and an undirected edge (i, j) exists if there is a joint connecting nodes/limbs i and j. We focus on the case where the graph is connected and acyclic to become a tree, where the root node, indexed as $i = 1$, is often the torso or the base in the agent's morphology. Any two connected limbs are rigidly attached to each other via their joint to provide degrees of freedom (DoFs) between the two limbs. A joint provides either 1-, 2-, or 3-DoF rotation for the child node/limb relative to its parent: in general, as a 3D rigid body, the child can rotate about yaw-, pitch-, and/or roll-axes relative to its parent, creating 3 DoFs; for a 2D planer task, the agent operates in the xz-plane and the joints therein can only rotate about the y-axis (pitch), creating only 1 DoF. The joints are actuated to produce a scalar torque for each of these rotation DoFs. We let $d_i \\in \\{1, 2, 3\\}$ denote the number of DoFs of the joint with node $i > 1$ as its child, and the collection of $d_i$ scalar torques is denote as $a_i \\in \\mathbb{R}^{d_i}$, resulting in the action for the agent as $a = (a_i)_{i>1} \\in \\mathbb{R}^m$ where $m = \\sum_{i>1} d_i$. The root node, as a 3D rigid body, has up to 6 DoFs relative to the global frame, and we let $d_1 < 6$ denote the number of its DoFs.\nKinematic features. As the action of torques specifies acceleration, a Markovian state of such an agent has to specify its configuration, i.e., how the limbs occupy the space and how the joints are rotated, as well as its velocity, i.e., the time derivative of its configuration. We refer to these physical quantities as the agent's kinematic features and enumerate here several of them that will be closely related to our discussion:"}, {"title": "SO(3)-data augmentation", "content": "If the agent is placed in the free space (free of obstacles and external forces), its transition dynamics and reward function are invariant to the transformations including translations, rotations, and reflections. Formally, these transforma-tions together form the Euclidean group of $G = E(3)$ in Equation (1), where the state should include the agent's kinematic features (i.e., configuration and"}, {"title": "Experiments", "content": "All the tasks in our experiments are provided by the DeepMind Control Suite (DMControl) [12] powered by the physics simulator of MuJoCo [27], which has become a common benchmark for RL-based continuous control. Specifically, we include 7 tasks originally from DMControl: Cheetah_run, Hopper_hop, Walker_run, Quadruped_run, Reacher_hard, Humanoid_run, Humanoid_stand, and our modified Cheetah3D_run, Hopper3D_hop, and Walker3D_run, which are the 3D variants of their original 2D planar counterparts. The original tasks involving Quadruped and Humanoid are already 3D, while it is not straightforward to extend Reacher_hard to a 3D variant. This results in a total number of 10 tasks. For all tasks in DMControl, an episode corresponds to 1000 steps, where a per-step reward is in the unit interval, i.e., $r_t \\in [0, 1]$. The original tasks in DMControl employ the joint-based kinematic state representation. We use MuJoCo's built-in data structures and functions (e.g., mjData.xpos for the translational position, mjData.object_velocity for linear and angular velocity [28]) to obtain the kinematics features needed for our limb-based representation. Refer to Appendix A.1 for a detailed description of the state features for all the tasks.\nWe build our data augmentation on top of DDPG, following the implementation in [10] that has achieved state-of-the-art performance on the chosen tasks (comparable or better than, e.g., TD3 [29], Soft Actor-Critic (SAC) [30], e.g., refer to [10] for a comparison). Refer to Appendix A.2 for a full list of the DDPG's hyperparameters. For reference, we also run and provide results of standard SAC. Our method introduces only one additional hyperparameter, $B_{aug}$, the number of transitions to be transformed in a batch of B transitions. We perform a hyperparameter search over $B_{aug}/B =: p_{aug} \\in \\{0,25\\%, 50\\%, 75\\%, 100\\%\\}$ separately for each task, while keeping all other hyperparameters the same as DDPG default. We compare our $SO_{\\tilde{g}}(3)$-data augmentation with two alternatives considered in the prior work of [2], both operating on the original joint-based kinematic state representation: The Gaussian noise (GN) method adds a standard Gaussian random variable to the state vector, i.e., $s \\rightarrow s + z$ where $z \\sim N(0, I)$; and random amplitude scaling (RAS) multiplies the uniform random variable to the state vector element-wise, i.e., $s \\rightarrow sz$ with \\odot denoting element-wise product, where $z \\sim Uni[\\alpha, \\beta]$ with $\\alpha = 0.5, \\beta = 1.0$ as chosen in the prior work of [2]. We additionally compare to using equivariant neural networks, specifically SEGNN [31], to instantiate the agent's policy and critic, as an alternative method to exploit the Euclidean symmetries. Refer to Appendix A.3 for the details of our SEGNN implementation."}, {"title": "Comparison with standard RL and prior data augmentation methods", "content": "As SEGNN is prohibitively expensive in computation, we only run it on the task of Reacher_hard, the smallest one among all 10 tasks, and defer the results to Section 5.2. Figure 3 presents the learning curves to compare our method against all other baselines on the rest 9 tasks. For our method, we present both $p_{aug} = 0\\%$ and the task-specific best positive $p_{aug} \\in \\{25\\%, 50\\%, 75\\%, 100\\%\\}$ to separate the effects of our limb-based kinematic state representation and the data augmentation on top of it. We make the following observations from the results:\n(i) DDPG is comparable to or significantly better than SAC in all 9 tasks except for Cheetah_run and Humanoid_run, which justifies our choice of DDPG as the base RL algorithm.\n(ii) As existing alternative data augmentation methods for state-based continuous control, the base-lines of GN and RAS do not offer significant improvement in data efficiency. They even introduce negative effects on performance on most tasks, except for Cheetah3D_run and Humanoid_stand where RAS improves data efficiency over its based algorithm of DDPG.\n(iii) With $p_{aug} = 0\\%$, i.e., with the limb-based state representation alone, our method improves data efficiency over DDPG on the 6 tasks shown in the first 2 rows in Figure 3, and in 5 out of the 6 tasks (excluding Hopper3D_hop), $p_{aug} = 0\\%$ is comparable to the best $p_{aug} > 0\\%$. On the rest of"}, {"title": "Comparison with equivariant agent architecture", "content": "Employing equivariant neural networks such as SEGNN as the agent policy and critic architecture is an alternative method to exploit the Euclidean symmetry but at the same time more computationally expensive than data augmen-tation. As we are not able to afford to finish the SEGNN baseline for all tasks, we have run it only for Reacher_hard, the smallest one by the number of limbs/joints. Figure 4 reports the learning curves of our method, SEGNN, and all other baselines on Reacher_hard, with the x-axis being time steps and hours to compare data and computational efficiency, respectively. As the results show, SEGNN is the most data efficient, followed by our data augmentation method ($p_{aug} = 25\\%$), both clearly outperforming the others. However, our method introduces minimal computation in addition to its DDPG base algorithm, both taking roughly 2 hours to finish 1M steps after convergence, while SEGNN takes more than 70 hours to finish 1M steps and more than 10 hours to converge."}, {"title": "Effect of paug", "content": "In Sections 5.1 and 5.2, we have presented the learning curves of our method with $p_{aug} = 100\\%$ and best task-specific $p_{aug} > 0\\%)$. Here, we present further results detailing the effect of $p_{aug}$, with the learning curves of all values of $p_{aug}$ on all tasks given in Appendix B.1. We observe that: 1) On relatively simple tasks with 2D planer DoFs and small numbers of limbs/joints, $p_{aug} > 0\\%$ does not provide significant improvements over $p_{aug} = 0\\%$ examples including Cheetah_run as shown in Figure 5a; 2) Data augmentation ($p_{aug} > 0\\%$) is crucial to best performance on hard tasks with 3D DoFs and large numbers of limbs/joints, examples including Hopper3D_hop and Humanoid_run in Figures 5b and 5c, respectively; and 3) The best $p_{aug}$ is task-specific and not necessarily the largest possible $p_{aug} = 100\\%$. For example, $p_{aug} = 100\\%$ hinders performance on Hopper3D_hop but is the best on Humanoid run."}, {"title": "Comparison with joint-based data augmentation", "content": "Indeed, one can perform $SO_{\\tilde{g}}(3)$ rotations on the original joint-based state representation. In the presented tasks, only the torso (i = 1) has features for orientation (in its up to 6 DoFs) while features of other limbs (i > 1) are just angles and angular velocities of the hinges. Therefore, under a rotation, only the torso's orientation features are changed, while other features stay unchanged. Therefore, we hypothesize this data augmentation by rotating original joint-based features would bring little benefit to the tasks. This is because torso features make up only a fraction of all features when the number of limbs is large (which is the case in these tasks). Our limb-based representation instead rotates all limbs to provide richer augmentation. The hypothesis is supported by results of prior work (e.g., see Figure 16 in Corrado and Hanna [18]). We here also conduct a comparison, with results in Figure 6 showing that joint-based data-augmentation is less efficient than our limb-based method."}, {"title": "Conclusion", "content": "We have motivated, formalized, and developed the idea and method of a new data augmentation method that aims to improve the performance of RL, as measured by its data efficiency and asymptotic reward, for state-based continuous control. The key idea of our data augmentation method is to create additional training data by performing Euclidean transformations on the original states, which is justified by the Euclidean symmetries inherent in the continuous control tasks. To make the states more amenable to Euclidean transformations, we turn to a state representation based on limbs' kinematic features, rather than on joints' configurations as done by default in prior works. Our new method significantly improves the performance for a wide range of state-based continuous control tasks, especially for tasks with rich 3D motions and large numbers of limbs and joints.\nOur work focuses on robot locomotion tasks as instances of continuous control, which clearly exhibit Euclidean symmetries. Other robotics tasks (e.g., navigation) and many applications that operate in the 3D physical space should also exhibit Euclidean symmetries. There are indeed continuous control tasks that might not exhibit Euclidean symmetries, such as those in electrical and power engineering, which we do not consider. We observe two limitations that would inspire future work: 1) To obtain the best performance, our data augmentation method needs task-specific turning of its hyperparameter, $p_{aug}$, that controls the proportion of data to be transformed for learning. Future work in this direction is needed to ease task-specific turning, either by adaptive tuning strategies [35] or training techniques to make learning more robust and less sensitive to the hyperparameter [36]; 2) The proposed method requires knowledge and annotation of strict Euclidean symmetries in the continuous tasks, which might not be easily available especially when the external forces are noisy and hard to detect, e.g., including wind conditions in the wild field along with gravity. This prompts future work of automatic discovery and exploitation of irregular, approximate Euclidean symmetries."}]}