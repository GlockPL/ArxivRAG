{"title": "Free Argumentative Exchanges for Explaining Image Classifiers", "authors": ["Avinash Kori", "Antonio Rago", "Francesca Toni"], "abstract": "Deep learning models are powerful image classifiers but their opacity hinders their trustworthiness. Explanation methods for capturing the reasoning process within these classifiers faithfully and in a clear manner are scarce, due to their sheer complexity and size. We provide a solution for this problem by defining a novel method for explaining the outputs of image classifiers with debates between two agents, each arguing for a particular class. We obtain these debates as concrete instances of Free Argumentative eXchanges (FAXs), a novel argumentation-based multi-agent framework allowing agents to internalise opinions by other agents differently than originally stated. We define two metrics (consensus and persuasion rate) to assess the usefulness of FAXs as argumentative explanations for image classifiers. We then conduct a number of empirical experiments showing that FAXs perform well along these metrics as well as being more faithful to the image classifiers than conventional, non-argumentative explanation methods. All our implementations can be found at https://github.com/koriavinash1/FAX.", "sections": [{"title": "1 INTRODUCTION", "content": "With the increasing complexity and widespread deployment of deep learning models in our daily lives, the interpretation and explanation of these models' decisions have become a central focus in recent explainable Artificial Intelligence (XAI) literature [26, 33, 39]. Many existing approaches for explaining image classification models heavily rely on heatmaps and segments to localize regions of interest in input images that contribute to the model's output [9, 36, 37], typically offering static input-output-based explanations while lacking deeper insights into the underlying model being explained. The literature has repeatedly highlighted the need for deeper and more dynamic explanations [25, 27, 28], highlighting the input-output relationships of the model but also delving into its internal mechanisms and elucidating the model's reasoning. Also, there is an ongoing debate about the necessity of interactive explanations [7, 28] and explanations that are contrastive and selected [28]."}, {"title": "2 RELATED WORK", "content": "XAI methods. There has been a recent surge in methods advocating for a shift in how we perceive explanations, emphasizing the importance of viewing them as dialogues rather than solely relying on heatmaps or feature attributions, as standard in much of the XAI literature [25, 27, 28]. Within the multi-agent setting, [32] proposed an argumentative framework for expressing (interactive) explanations in the form of dialogues. Additionally, [18] demonstrated a debate framework, which was further expanded in [23] to scale, allowing for the extraction of post-hoc explanations in the form of dialogues between two fictional agents. Inspired by [32], we define a multi-agent argumentative framework for explanation, and we adopt a variant of approach in [23] to implement the framework. Both [23] and our implementation utilize a surrogate model that faithfully represents the given classifier. The use of surrogate models is a standard practice in XAI, as seen e.g. in [15, 22, 46].\nThe agents in our explanations put forward arguments which can be understood as feature attributions such as LIME [33]. However, unlike [33], we do not randomly select input regions to mask; instead, our agents learn two different strategies to select regions to argue for and against a particular explanandum (input-output).\nOur method is also related to [38], which argues against the rigidity of static and shallow explanations and introduces a new method for compactly visualizing how different combinations of regions in images impact the confidence of a classifier. Also, [44] aims to encourage the capturing of uncertain image regions, while [44] focus on statically capturing ambiguities in an image with respect to the given classifier, we generate both certain and uncertain regions through agent interactions in an iterative fashion [41].\nArgumentation methods. Some research in XAI explores the use of computational argumentation [11]. This typically aims to assess specific claims by considering arguments that support and/or challenge the claim, as well as their relations within argumentative frameworks (AFs). These AFs may be as in [13] or Bipolar AFs (BAFs). Broadly, with our XAI approach we delve into a relatively unexplored area: explaining image classifiers through debates amounting to BAFs, which involve interactive gameplay among learning agents. Other approaches employing AFs for explainable image classification either utilize intrinsically argumentative models, e.g. as in [3], or mirror the mechanics of the model itself, e.g. as seen in [40]. In contrast, our approach focuses on explaining classifiers using latent features through (free) argumentative exchanges."}, {"title": "3 PRELIMINARIES", "content": "3.1 Computational Argumentation Background\nWe use (BAFs) [8], i.e. triples $(X, A, S)$ such that $X$ is a finite set of arguments, $A \\subseteq X \\times X$ is an attack relation and $S \\subseteq X \\times X$ is a support relation. For all BAFs in this paper, we assume that $A\\cap S = \\emptyset$. For any $a \\in X$, $A(a) = {\\beta \\in X | (\\beta, a) \\in A}$ are the attackers of a and $S(a) = {\\beta \\in X | (\\beta, a) \\in S}$ are the supporters of a.\nWe use the following notation from [32]: given BAFs $B = (X, A, S)$, $B' = (X', A', S')$, we say that $B \\subseteq B'$ iff $X \\subseteq X', A \\subseteq A'$ and $S \\subseteq S'$; also, we use $B' \\setminus B$ to denote $(X' \\setminus X, A' \\setminus A, S' \\setminus S)$. We also say that $B = B'$ iff $B \\subseteq B'$ and $B' \\subseteq B$, and $B \\neq B'$ iff $B \\subseteq B'$ but $B \\neq B'$.\nAs conventional in the literature [5], a BAF $(X, A, S)$ may be equipped with gradual semantics $o : X \\rightarrow I$ assigning to arguments $a \\in X$ values in $I$, which is some set equipped with a pre-order $\\leq$ (where, as usual $v  w$ denotes $v \\leq w$ and $w \\neq v$). In line with [32], we refer to $o$ as evaluation method and to $I$ as evaluation range, to indicate their use by agents to evaluate arguments internally. We say $o$ is dialectically monotonic iff, as in [5]:\n* given two BAFs $B = (X, A, S)$ and $B' = (X', A', S')$, where $X' = X \\cup {a}$, $A' \\cup S' = A \\cup S \\cup {(a, b)}$, it is always the case that if $(a, b) \\in A'$, then $o(B', b) \\leq o(B, b)$, while if $(a, b) \\in S'$, then $o(B', b) \\geq o(B, b)$; and\n* given two BAFs $B = (X, A, S)$ and $B' = (X', A', S')$, where for an argument $a \\in X \\cap X'$, $A'(a) = A(a)$, $S'(a) = S(a)$, $\\exists b \\in A'(a) \\cup S'(a)$ such that $o(B', b)  o(B, b)$ and $\\forall c \\in A'(a) \\cup S'(a) \\setminus {b}, o(B', c) = o(B, c)$ it is always the case that if $b \\in A'(a)$, then $o(B', a) \\leq o(B, a)$, while if $b \\in S'(a)$, then $o(B', a) \\geq o(B, a)$.\nThe first bullet states that an argument's strength cannot increase/decrease when a new attacker against/supporter for (respectively) the argument is added, all else being equal; the second bullet states that an argument's strength cannot increase/decrease when an attacker against/supporter for (respectively) the argument is strengthened, all else being equal.\nGiven a BAF B=(X, A, S), for a, b \u2208 X, we let a path from a to b be defined as $(c_0, c_1), ..., (c_{n-1}, c_n)$ for some n > 0 (the length of the path) where $c_0 = a, c_n = b$ and, for any $1 \\leq i \\leq n, (c_{i-1}, c_i) \\in A \\cup S$. We also use paths(a, b) to denote the set of all paths from a to b (leaving the BAF implicit), and use $|p|$ for the length of path p. Also, we may see paths as sets of pairs. Then, as in [12, 32], we say that B is a BAF for explanandum $e \\in X$ iff i.) $(e, a) \\in A \\cup S$; ii.) $\\forall a \\in X\\setminus {e}$, there is a path from a to e; and iii.) $a \\in X$ with a path from a to a.\n3.2 Image Classification Set-up\nConsider a dataset $D \\subseteq X \\times Y$, where $X \\in R^{h \\times w \\times c}$ represents a vector space with dimensions h \u00d7 w corresponding to an image and c channels (c \u2265 1). The label space $Y = {1,..., N}$ consists of N \u2265 2 classes. Following conventions in image classification [16, 17], we consider an image classifier trained on D, comprising a feature extractor $f: X \\rightarrow Z$ and a feature classifier $g: Z \\rightarrow Y$. The feature extractor f maps the observational space X to a continuous latent space $Z \\in R^{m \\times d}$, where each element of Z represents a set of"}, {"title": "4 AGENTS AND EXPLANATIONS", "content": "Agents are represented as private triples for explananda, a notion adapted from [32], as follows.\nDEFINITION 1. An agent $A^i$ is a private triple for an explanandum e, amounting to $(I^i, B^i, \\sigma^i)$ where:\n* $I^i = I^i_{-} \\cup I^i_{+}$ is an evaluation range, referred to as $A^i$'s private evaluation range, where $I^i_{-}$ (referred to as positive evaluations) and $I^i_{+}$ (referred to as negative evaluations) are disjoint and for any $v_{+} \\in I^i_{+}$ and $v_{-} \\in I^i_{-}, v_{+} > v_{-}$;\n* $B^i = (X^i, A^i, S^i)$ is a BAF for e, referred to as $A^i$'s private BAF;\n* $\\sigma^i$ is an evaluation method, referred to as $A^i$'s private evaluation method, such that, for any BAF $B = (X, A, S)$ and, for any $a \\in X, \\sigma^i(B, a) \\in I$.\nHere, we use BAFs instead of quantitative BAFs [4] as in [32], and rely upon evaluation ranges split into two, rather than three, partitions as in [32] (so we disregard the neutral partition in [32]). The threshold between the two partitions is seen as the point where an agent \"changes their mind\" on the explanandum, and may correspond to a classifier's decision boundary, as we will see later. We assume, for the remainder of this section, that any evaluation method is dialectically monotonic (as defined in Section 3). We exemplify our notion of an agent in a simple, generic setting below (see Section 5 for instantiations for image classification).\nExample 2. An agent $A^i$ is a private triple for an explanandum a, amounting to $(I^i, B^i, \\sigma^i)$ where: $I^i = [0,1]$ with $I^i_{-} = [0,0.6[$ and $I^i_{+} = [0.6, 1]; B^i = (X^i, A^i, S^i)$ such that $X^i = {a,b}$ with arguments a: we should eat at this pizzeria and b: it is highly recommended, $S^i(a) = {(b,a)}$ and $A^i(a) = \\emptyset$; and $o^i$ is some dialectically monotonic semantics, which in this case could give, for example $\\sigma^i(B^i, a) = 0.75$ and $o^i(B^i,b) = 0.5$ (given the asymmetric set-up of a's attackers and supporters). Here we can see that $A^i$'s positive reasoning for the explanandum overcomes the (absent) negative reasoning against it, resulting in its strength being above the threshold of 0.6 and thus gives a positive evaluation.\nWe define a novel form of argumentation exchanges amongst agents, which will serve, later, as explanations:\nDEFINITION 2. A free argumentative exchange (FAX) for an explanandum e amongst agents A, where $A = {A^1, ..., A^m}, m \\geq 2$ and each agent in A is a private triple for e, is a tuple:\n$(B^0, ..., B^n, A_0, ..., A_n, M)$ $(n \\geq 0)$\nwhere, for all timesteps $t \\in [n]$, $B^t$ (referred to as the exchange BAF at step t) is a BAF for e and $A_t$ is a set of agents, all private triples for e, such that:"}, {"title": "5 EXPLANATIONS FOR IMAGE CLASSIFICATION", "content": "We see explanations for image classification as (exchange BAFs of) equal opportunity strictly interleaved FAXs amongst:\n* $A^1$ (the proponent, arguing for the class in Y predicted by the underlying image classifier for input image $x \\in X$ of interest), and\n* $A^2$ (the opponent, arguing against the predicted class).\nFor simplicity, we restrict attention to the top two classes in the ordering determined by the image classifier (g, see Section 3) for the input image, so that $A^2$ argues against the predicted (top) class by arguing for the second best in the ordering. In the remainder of the paper we assume, without loss of generality, that class 1 is the predicted class for x and class 2 is the second best. In line with the machine learning literature, we also refer to class 1 as \u0177.\nIn our approach to explaining image classification using FAXs, each agent argues for a particular class, and thus class-specific features (see Section 3) play the role of arguments exchanged between agents. The arguments put forward by the proponent and opponent can be seen as playing the role of positive and negative, respectively, feature attributions as in some XAI literature [26, 33, 37, 39]. However, intuitively, the exchange BAF in the FAX can also convey the reasoning of the classifier. We will provide an evaluation of FAXs as explanations of image classifiers in Section 8. Here, we focus on instantiating the generic FAXs for our purposes.\nConcretely, we assume that the two agents explaining the output of an underlying image classifier by virtue of a FAX can leverage on class-specific classifiers $q^1: 2^{Z^1 \\cup Z^2} \\rightarrow [0, 1]$ and $q^2: 2^{Z^1 \\cup Z^2} \\rightarrow [0, 1]$, allowing the agents to evaluate sets of their own and other agents' arguments (in other words, amounting to their private evaluation methods). Until Section 6, we ignore how these class-specific classifiers can be learnt, alongside the class-specific features, so that they are faithful to the original image classifier being explained (see Section 7), which is crucial to guarantee that FAXs as explanations are also faithful. We also refer to the class-specific classifiers as private classifiers (with $Z^1$ and $Z^2$ also referred to as private features)."}, {"title": "6 IMPLEMENTATION", "content": "In this section, we detail our methodology (overviewed in Figure 2) for obtaining and evaluating empirically FAXs with data for image classification. Specifically, we detail how class-specific (private) features can be obtained (Section 6.1); and how class-specific agents (including their private classifiers and their policies for contributing to FAXs) are learnt and deployed (Section 6.2). Note that, while FAXs are amongst two agents only (for the top- and second-best predicted classes by the classifier), given that different inputs will result in different predictions we need to develop, at training time, all agents, with their private features and classifiers.\n6.1 Class-specific discrete features\nWe obtain these by simultaneously training, similarly to [23]:"}, {"title": "7 EVALUATION", "content": "In this section we lay out our approach to evaluating the realization of our FAXs for explaining image classifiers. We use evaluation metrics for assessing faithfulness and argumentative quality of our FAXs. The metrics are measured on a test set $T \\subseteq X \\times Y$, providing ground-truths (correct classifications) for a number of inputs (we will use concrete instances of T in our experiments in Section 8).\nWe define the metrics using the same notation $C(\\tilde{z})$ as in Section 6 as well as notations $C(h)$ to represent the codebook corresponding to hidden state representation h (in some sequence model) and $q^i(h)$ to represent the values assigned by $q^i$ to the private features/arguments corresponding to h (in $\u03b6^i$).\nThe faithfulness metrics are adapted from the literature, and [23] in particular. The first metric measures correctness of q and of the codebooks by measuring accuracy wrt the ground-truth in T:\n$\\frac{| \\{(x, y) \\in T | q(C(\\tilde{z})) = y\\} |}{|T|}$\nThe second metric measures completeness of q on the BAFs resulting from FAXs, by measuring the accuracy of q on the codebooks"}, {"title": "9 CONCLUSIONS", "content": "We have defined explanations for image classification as (free) argumentative exchanges between two agents, aiming to demystify trained image classifiers based on the argument contribution strategies by the agents. Differently from standard feature attribution methods generating heatmaps over responsible regions in images, our method generates more fine-grained composition of sub-regions, incrementally. Our work opens many opportunities for future work. We plan to investigate whether FAXs can uncover shortcuts in classifiers. Further, it would be valuable to collaborate with domain experts to attribute semantic meaning to arguments, potentially aiding alignment between human understanding and the latent knowledge of models. Also, it would be interesting to apply our approach in settings where quantized representation learning is already explored, ranging from natural images to medical data [14, 34, 42], or by targeting other (potentially more complex) architectures, e.g. transformers [43]. Methodologically, we also plan to explore the use of object identification methods such as grounded slot attention [24], instead of quantization, to improve the human understandability of arguments in our FAXs. Finally, a promising avenue for fully exploiting the capabilities of FAXs amounts to leveraging notions from [21, 35, 45] to create hierarchical concepts, giving FAXs with more interesting argument interactions."}]}