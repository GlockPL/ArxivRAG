{"title": "Scoring Verifiers: Evaluating Synthetic Verification in Code and Reasoning", "authors": ["Aleksander Ficek", "Somshubra Majumdar", "Vahid Noroozi", "Boris Ginsburg"], "abstract": "Code verification has recently found great success as a critical component in training large scale reasoning models for coding. Synthetic techniques such as self-generated test cases and reward models provide a way to enhance code capabilities beyond predefined tests. Building on these advancements, we propose new benchmarks designed to systematically evaluate the impact of synthetic verification methods on assessing solution correctness. We introduce HE-R, HE-R+, MBPP-R, and MBPP-R+, which transform existing coding benchmarks into scoring and ranking datasets to evaluate the effectiveness of synthetic verifiers. Using these benchmarks, we analyze synthetic verification methods in standard, reasoning-based, and reward-based LLMs. Our results show that recent reasoning models significantly improve test case generation and that scaling test cases enhances verification accuracy.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, particularly in code generation. Their advancements extend to solving algorithmic challenges in competitive programming, real-world software engineering tasks, and enhancing automated code testing. Recently, reasoning models such as DeepSeek-R1 (DeepSeek-AI, 2025) have found substantial improvements in code generation by leveraging large-scale reinforcement learning and rule-based reward systems. In the context of coding capabilities, the authors highlight that \"for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases\", highlighting the importance of code verification.\nAlthough effective, this approach faces a clear bottleneck due to the limited number of problems with predefined test cases. To address this constraint, many prior works have explored synthetically generated test cases and unit tests to automatically verify code quality and coverage (Sch\u00e4fer et al., 2024; Chen et al., 2022). Additionally, other works employ coding reward models to improve results on coding benchmarks (Zeng et al., 2025; Ma et al., 2025). In this paper, we collectively refer to these approaches as synthetic verifiers."}, {"title": "Approach", "content": "We outline the process used to transform HE, HE+, MBPP, and MBPP+ into scoring and ranking benchmarks as pictured in Figure 2. This enables us to evaluate how well synthetic verifiers can score and rank solutions. We describe how datasets are processed to generate multiple potential solutions, followed by scoring using predefined test cases, and conclude with filtering and ranking stages."}, {"title": "Benchmark Preprocessing", "content": "We build upon EvalPlus (Liu et al., 2023), using its extended versions of HumanEval and MBPP as the foundation for our scoring and ranking benchmarks. We reconstruct the evaluation framework so that we can determine which test cases fail for each problem as solutions are currently labelled only as pass/fail. We then generate dataset versions that include the expected standard output for each test case, allowing them to be easily formatted as assertion statements. To ensure the accuracy of our approach, we validate that all correct solutions pass all test cases and that a sample dataset with varying correctness achieves similar scores to those provided by EvalPlus."}, {"title": "Producing Solutions", "content": "We generate potential solutions by iterating over each dataset entry and using GPT-4o (2024-11-20) (OpenAI, 2024a) to produce a response. For each test case, we compare the model's output to the ground truth, computing the pass rate as our test case score and average execution time. This generation cycle is repeated across multiple prompts, sampling hyperparameters, and seeds. To further diversify the quality of solutions, we explicitly prompt the model to generate partially incorrect solutions, encouraging it to explore varying degrees of correctness. Sample prompts for this can be found in"}, {"title": "Filtering and Ranking", "content": "For each problem, we deduplicate solutions that achieve the same fraction of test cases passed (test case score) and tie-break using the lower average execution time. For the case where multiple solutions pass all test cases we automatically select the ground truth answer from the dataset. We also filter out solutions that fail completely due to non-assertion errors. This ensures we exclude solutions that may be almost correct but achieve a score of zero due to syntax errors or other trivial issues. Finally, we apply a simple algorithm to select the k solutions that are most evenly distributed in terms of the fraction of test cases passed. Formally, let\n$S = \\{s_1, s_2, ..., s_n\\},\\\\$\ndenote the set of deduplicated solutions, sorted in descending order such that $s_1 \\geq s_2 \\geq \\dots \\geq s_n$, where $s_i$ represents the fraction of test cases passed by solution $i$. We assume that $s_1 = M = 1.0$ and define the minimum score m as\n$m=\\begin{cases}\n  min\\{s \\in S : 0 < s < 0.1\\} & \\text{if such s exists} \\\\\n  s_n & \\text{otherwise}\n\\end{cases}$\nTo account for cases when k exceeds n, we define the effective selection count as:\n$k' = min\\{n, k\\}.$\nFor $i = 1, 2, ..., k' - 1$, we compute target scores:\n$T_i = 1 - \\frac{i}{k'}(1 - m).$\nFor each $T_i$, we select the solution not yet chosen that minimizes the absolute difference to $T_i$:\n$s_i^* = \\underset{s \\in S\\setminus\\{s_1, ..., s_{i-1}\\}}{argmin} |s - T_i|.$\nFinally, we include the solution corresponding to $m = s_n$, yielding the selected set:\n$S^* = \\{s_1^*, s_2^*, ..., s_{k'}^*\\}.$\nFor our transformed benchmarks, we set k = 5 to ensure that HE-R+ and MBPP-R+ contain at least five uniquely scored solutions per problem. When m = 0.0, for example, our selection algorithm chooses solutions that best approximate the"}, {"title": "Benchmark Analysis", "content": "The extended versions of HumanEval and MBPP include significantly more test cases, making the fraction of test cases passed a more reliable proxy for overall solution correctness. We also apply our process to transform the original HE and MBPP datasets to demonstrate the lower limit of number of test cases necessary to valuable scoring and ranking benchmark. In cases where the benchmark has a limited number of predefined test cases but an accurate ground-truth solution, we recommend readers to follow previous methods to generate additional ground truth test cases (Liu et al., 2023)."}, {"title": "Transformed benchmarks", "content": null}, {"title": "Experiment Setup", "content": "After creating our benchmarks HE-R, HE-R+, MBPP-R and MBPP-R+, we use them to explore and compare the synthetic verification methods of test case generation and coding in reward models."}, {"title": "Test Case Generation", "content": "For test case generation, we select a well-suited prompt, an appropriate number of test cases and temperature of 1.0 by evaluating on our benchmarks. The final prompt is detailed in Figure 15, and we use 10 self-generated test cases for our primary results as we feel this provides a reasonable coverage of edge cases and limits context size for the large number of experiments we execute. In our prompt, we provide two examples in HumanEval format and ensure the model wraps each test case in <assertion></assertion> tags. We include additional rules that we found decrease the number of non-assertion errors produced by the generated tests. We then execute each test case and provided solution individually with a timeout of 3 seconds and compute the test case score. We then evaluate several Llama (AI, 2024), Qwen (Qwen et al., 2025; Hui et al., 2024), OpenAI (OpenAI, 2024a,b) and DeepSeek (DeepSeek-AI, 2025) models."}, {"title": "Code Reward Models", "content": "For evaluating reward models we found applying a brief preamble amplifies results located in Figure 16. We compute the reward score for each solution and normalize using the highest and lowest score for each problem. We evaluate Acecoder 7B and 32B (Zeng et al., 2025), Llama-3.1-Nemotron-70B-Reward (Wang et al., 2024) and Nemotron-4 340B (Nvidia, 2024) reward models. For Nemotron-4-340B (Nvidia, 2024) we use only the correctness field for the reward score as this achieves the best results."}, {"title": "Metrics", "content": "To assess the model's ability to quantify solution correctness, we compute several key metrics:\nDetermines if the model correctly ranks the best solution first.\nDetermines if the model correctly ranks the worst solution last.\nEvaluates the strength and direction of association between expected and actual rankings.\nMeasures the correlation between expected and actual rankings\nQuantifies the absolute error between the expected and actual fraction of test cases passed."}, {"title": "Results", "content": null}, {"title": "Test Case Generation Ranking Results", "content": "Table 2 presents our main results on HE-R+ and MBPP-R+ with 17 standard, reward, and reasoning-based LLMs while Table 3 presents the equivalent results on HE-R and MBPP-R. We find that the performance of self-generated test cases on our benchmarks generally correlates with the generating model's performance on the original HumanEval and MBPP. Top performing regular models on HumanEval and MBPP such as Qwen2.5-Coder-32B-Instruct and Qwen2.5-72B-Instruct also perform best on HE-R+ and MBPP-R+."}, {"title": "Reasoning Model Results", "content": "We observe that the enhanced coding capabilities in reasoning models translates somewhat to improved test case generation. In a head-to-head comparison, DeepSeek-R1-Distill-Qwen-32B outperforms Qwen2.5-32B-Instruct in Top-1 accuracy but falls short in Spearman and Kendall evaluations. However, incorporating DeepSeek-R1, o1-mini, and o3-mini leads to significant improvements across all metrics, positioning them as the most effective synthetic verifiers currently available, especially when scaling the number of test cases. Figure 19 presents a sample Chain-of-Thought from DeepSeek-R1-Distill-Qwen-32B which illustrates how the model convincingly explores many paths ways to cover potential solutions with its test cases."}, {"title": "Code Reward Model Results", "content": "Converting the original benchmarks into a scoring and ranking format enables a unique comparison of different synthetic verification methods like test case generation and reward models. From our results in Table 2, the best reward models are AceCoderRM-32B for HE-R+ and Nemotron-4-340B-Reward for MBPP-R+. We find that the best performing reasoning and standard models outperform the reward models in most metrics. In similarly sized models like Qwen2.5-Coder-32B-Instruct, the Top-1 scores are competitive in HE-R+ and better in the case of MBPP-R+ while struggling in ranking the varying qualities of incorrect solutions. This could be due to subjectivity in ranking incorrect solutions, they may be functionally incorrect but qualitatively exhibiting meaningful quality. We encourage self-generated test cases as a suitable synthetic verifier for determining the correctness of a solution but see promising opportunities to further enhance reward models for coding and reasoning."}, {"title": "Solution Inclusion", "content": "Finally, we examine the impact of prompting with and without a provided solution, as shown in Figure 8. All models exhibit significant performance degradation when given a potentially incorrect solution and tasked with writing test cases to evaluate it. We find that the models have a bias towards adhering to any solutions provided even when specifically prompting against this. This is supported by previous works that find LLM's to be worse at providing test cases when provided incorrect compared to correct code (Huang et al., 2024)."}, {"title": "Related Works", "content": "Prior work primarily validates self-generated test cases within isolated systems or limited studies. (Wei et al., 2024) conducts an ablation study showing that filtering LLM-generated solutions with self-generated test cases improves synthetic data quality, evidenced by downstream supervised fine-tuning results. (Light et al., 2024) compares self-generated validation tests to ground truth tests on HumanEval to highlight the impact of accurate test cases on their Scattered Forest Search method. (Zhang et al., 2023) justifies its oracle verifier strategy by comparing its test cases on correct solutions. Additional techniques use test case generation to improve results on coding tasks (Liu et al., 2024; Ridnik et al., 2024; Xu et al., 2024; Ye et al., 2025). This paper unifies these approaches by introducing a benchmark for systematically assessing synthetic verification's ability to distinguish correct from incorrect solutions.\nAs mentioned in the introduction, creating evaluations for test case generation is a well explored area. This includes many benchmarks and systems that compete over quantifying coverage, mutation testing, validity and efficiency (Wang et al., 2025; Jain et al., 2024a; M\u00fcndler et al., 2025; Jain et al., 2024b; Taherkhani and Hemmati, 2024; Ahmed et al., 2024; Peng et al., 2025; Ryan et al., 2024; Li and Yuan, 2024; Zhang et al., 2024). Crucially, we do not assess an LLM's ability to generate test cases but rather the effectivness of LLM generated test cases to determine solution quality and rank. This aligns with CodeJudge-Eval (Zhao et al., 2024), which employs a similar methodology to benchmark LLM-as-a-Judge performance.\nOur work aligns closely with reward model evaluation such as in the case of RewardBench (Lambert et al., 2024). Similarly, (Zeng et al., 2025) leverages synthetic test cases to train coding reward"}, {"title": "Conclusion", "content": "We introduce a systematic approach to transform any coding benchmark with predefined test cases into a ranking and scoring benchmark for evaluating synthetic verification methods. Our method involves generating a diverse set of LLM-produced solutions, scoring them based on the fraction of test cases passed, and applying a structured filtering process to create reliably ranking datasets. We validate this approach by developing HE-R, HE-R+, MBPP-R, and MBPP-R+, which provide a standardized framework for assessing the effectiveness of synthetic verification strategies. We then use our transformed datasets to explore and uncover the effectiveness of typical, reward and reasoning based LLM's. Using our transformed datasets, we investigate the effectiveness of test case-based verification, the impact of reasoning models, and the relative strengths of reward models. Our findings reveal key insights into the performance of various LLM paradigms, highlighting the potential of reasoning-enhanced models and scaling test case generation for improved accuracy."}, {"title": "Limitations", "content": "While HumanEval and MBPP are widely used coding benchmarks, they primarily consist of relatively simpler problems and are of small quantity. To thoroughly evaluate performance on coding tasks, we aim to apply our approach to more difficult coding benchmarks and ones of greater size. Additionally, computational constraints limit our ability to fully explore the upper bound of test cases for all models. We leave these challenges as promising directions for future research."}]}