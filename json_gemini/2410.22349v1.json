{"title": "Search Engines in an Al Era: The False Promise of Factual and Verifiable Source-Cited Responses", "authors": ["Pranav Narayanan Venkit", "Philippe Laban", "Yilun Zhou", "Yixin Mao", "Chien-Sheng Wu"], "abstract": "Large Language Model (LLM)-based applications are graduating from research prototypes to products serving millions of users, influencing how people write and consume information. A prominent example is the appearance of Answer Engines: LLM-based generative search engines supplanting traditional search engines. Answer engines not only retrieve relevant sources to a user query but synthesize answer summaries that cite the sources. To understand these systems' limitations, we first conducted a study with 21 participants, evaluating interactions with answer vs. traditional search engines and identifying 16 answer engine limitations. From these insights, we propose 16 answer engine design recommendations, linked to 8 metrics. An automated evaluation implementing our metrics on three popular engines (You.com, Perplexity.ai, BingChat) quantifies common limitations (e.g., frequent hallucination, inaccurate citation) and unique features (e.g., variation in answer confidence), with results mirroring user study insights. We release our Answer Engine Evaluation benchmark (AEE) to facilitate transparent evaluation of LLM-based applications.", "sections": [{"title": "1 Introduction", "content": "Large Language Models have recently become part of daily life for many, with services such as ChatGPT and Claude offering AI-based conversational assistance to hundreds of millions of customers [22, 56, 61]. In doing so, such systems have graduated from academic tools that were evaluated from a technical standpoint to sociotechnical systems [10] that have both technical and social impact, and that require more nuanced evaluation, as they can influence various facets of society, including communication, information dissemination, and decision-making [56, 67].\nA prominent example of an LLM-based sociotechnical system is the Answer Engine, also known as a Generative Search Engine, illustrated in Figure 1. Answer Engines are marketed as replacements for traditional search engines \u2013 such as Google or Bing - and work in the following retrieval-augmented way: a user with an information need formulates a search query [52]. The Answer Engine first retrieves relevant source documents that likely contain answer elements to the user's query, leveraging a retrieval system (which can be a traditional search engine). The Answer Engine then composes a textual prompt that contains the user's query, and the retrieved sources, and instructs an LLM to generate a long and self-contained T answer for the user, based on the content of the sources. Crucially, \u2610 citations are inserted into the answer, with each citation linking to the sources that support each statement within the answer. This citation-enriched answer is provided to the user in a user interface: the citation forms the semantic glue between the generated answer and the sources, with a click on a citation allowing the user to navigate to the source or sources that support any statement.\nIn essence, the answer engine promises a streamlining of a user's information-seeking journey [67]. The Answer Engine concisely summarizes the information the user is looking for, and sources remain within a click in case the user desires to deepen their understanding or verify the information's veracity at the source. Recently, several free Answer Engines have become popular such as You.com, Perplexity.ai, and Bing Chat, with some reporting millions of daily searches performed by their users: Answer Engines are answering user needs.\nHowever, there are several well-known limitations to Answer En-gines, primarily stemming from the use of LLMs as part of answer generation. First, LLMs are known to hallucinate information and cannot detect factual inconsistencies [37, 73], even when authoritative sources are provided. Second, prior work [41, 49] has also shown limitations of answer engines' ability to assess the accuracy of citations within an answer. Third, LLMs accumulate knowledge within their internal weights during pre-training, and prior work has shown limited success at enforcing that an LLM generates information based solely on documents provided in a prompt rather than based on pretraining information which can be noisy or outdated [39]. Finally, such systems exhibit sycophantic behavior: a preference for the system to agree with the user's perceived opinion over objective truth [43, 68].\nAll these known limitations lead to a potential impact on the quality of generated answers, negatively affecting user experience. Yet prior work has evaluated LLMs and their output primarily from a technical perspective [20, 41]. Since Answer Engines are used by millions daily, it is equally important to evaluate them from a social perspective, to understand how users perceive Answer Engines, and how they navigate limitations.\nWe start our work with an audit-centric usability study (Section 3) involving 24 participants\u00b9 with expertise in technical domains (e.g., sociology, economics). Participants interact with Answer Engines and traditional Search Engines on two kinds of search queries: expertise and debate queries. Expertise queries are technical queries that participants self-report being experts on. Participants' familiarity with the answer allows us to evaluate how Answer Engines perform on deeply technical questions. Debate queries are queries related to a debate topic, formulated either to be pro or against the debate (e.g., \"Why should we abolish Daylight\"). By initially asking participants if they support one side of the debate, we can evaluate how participants interact with the answers that support or refute their opinions.\nThe usability study follows a think-aloud protocol and enables us to obtain two main kinds of insights: (i) quantitative insights on how users interact with answers, citations, and sources in both Answer Engines and traditional Search Engines, (ii) qualitative feedback from participants which we group using inductive reasoning [26, 27] followed by a qualitative coding method [2, 70] into 16 insights on the limitations of Answer Engines. With the study completed, we then propose 16 Design Recommendations that are both actionable and measurable, as we design 8 quantitative metrics that tie the design recommendations to specific measures (Section 4).\nFinally, we implement a large-scale automated evaluation of three popular Answer Engines (YouChat, Bing Copilot, and Perplexity AI\u00b2) using the 8 metrics, on 303 search queries from our usability study. We consolidate the metrics into a scorecard, the Answer En-gine Evaluation (AEE) benchmark, for each Answer Engine. One of our findings show that all evaluated answer engines frequently generate one-sided answers (50-80%) that favor agreement with charged debate questions, with Perplexity performing the worst, in multiple aspects, despite generating the longest answers, indicating that increasing answer length does not improve answer diversity. We release our automatic evaluation framework publicly, to encourage the community to evaluate Answer Engines as the technology evolves and matures\u00b3."}, {"title": "2 Background and Related Works", "content": "As AI becomes more embedded in daily life, their role has evolved from simple technical tools to complex sociotechnical systems. These systems involve an intricate interplay between social actors and technological components that together shape goal-oriented behaviors [10, 56, 72, 73]. Al systems in domains like education [35], healthcare [1], and policymaking [6] are thus deeply entwined with the social practices and institutional contexts in which they operate [24].\nHowever, despite the recognition of AI as inherently sociotech-nical, current research often adopts a technocentric perspective, focusing on algorithmic and computational aspects while neglecting broader societal implications [14, 17, 56, 75]. This gap is evident in the conceptualization of terms like bias [5], sentiment [72], and hallucination [73]. As Venkit et al. [72] argues, understanding AI through a sociotechnical lens is crucial to fully grasp its impacts, biases, and potential harms. Insights from Human-Centered Explainable AI (HCXAI) emphasize the need for a human-centric approach to technology, focusing on how Al systems can better align with human understanding and accessibility [17, 18]. Ehsan and Riedl [17] advocates for positioning \u201cthe human\" at the core of technology design, leveraging the social dynamics and context of Al systems to bridge gaps for non-technical users-something that is currently lacking. This approach forms the basis for designing Al systems that are not only transparent but also safer and more trustworthy, addressing a significant gap in AI development today.\nThe emergence of answer engines, or RAG (Retrieval Augmented Generation) systems, highlights the need for a social understanding of automated information retrieval. Unlike traditional search engines that provide a ranked list of documents, RAG systems generate synthesized responses by combining a retriever with a generator (often an LLM) to produce answers from these passages [15, 21, 23, 45]. While designed to enhance the relevance of retrieved information and address issues like hallucination [37, 62, 73], these systems also raise concerns about selective information presentation and bias amplification [31, 47, 67, 68]."}, {"title": "2.2 The Shortcomings of Answer Engines", "content": "Answer engines are marketed as efficient tools for simplifying information retrieval by reducing the need for users to manually sift through data repositories [63, 66]. However, recent developments, such as Google AI Overview and Perplexity, have exposed new ethical challenges and negative user experiences. For example, Google's answer engine erroneously advised users to \"put glue on their pizza,\" revealing how the system misinterpreted sarcastic content from the internet, presenting it as fact with undue authority\u2074. Such cases of misinformation highlight the risks associated with automating information retrieval, especially under the guise of 'Google doing the Googling' for users [63, 66, 76].\nThe release of OpenAI's 'SearchGPT, marketed as a 'Google search killer' [66], further exacerbates these concerns. As reliance on these tools grows, so does the urgency to understand their impact. Lindemann [47] introduces the concept of Sealed Knowledge, which critiques how these systems limit access to diverse answers by condensing search queries into singular, authoritative responses, effectively decontextualizing information and narrowing user perspectives [33, 54]. This \"sealing\" of knowledge perpetuates selection biases and restricts marginalized viewpoints [47].\nBuilding on this, Sharma et al. [68] argues that answer engines foster echo chambers, where exposure to diverse opinions is minimized, reinforcing existing beliefs and reducing the visibility of minority perspectives. This is particularly problematic given the estab-lished Western-centric bias in text generation models [25, 55, 57, 74]. When integrated into search engines, these models further propagate a predominantly Western viewpoint or an 'automated colonial impulse' [11-13], underscoring the need for comprehensive studies on their societal risks.\nA key concern surrounding answer engines is their inherently 'black-box' nature [60]. AI systems are often perceived as black boxes due to the opacity of their decision-making processes and the hidden biases within them, which are obscured by proprietary algorithms and vast training datasets [4, 34, 75]. Answer engines intensify this problem by merging two opaque systems: a search engine [28] and a generative AI model [4], resulting in compounded opacity and reduced user autonomy. This dual-layered opacity leads to problematic outcomes, such as those identified by Li and Sinnamon [46], who revealed sentiment bias based on query content, along with commercial and geographic biases in the sources answer engines use. The over-reliance on uneven quality sources, heavily skewed toward news, media, and business, further illustrates the need for transparency [46]."}, {"title": "2.3 Beyond a Positivism and Technical Lens of Answer Engines", "content": "As answer engines gain traction within the NLP and AI commu-nities, there has been a notable increase in efforts to evaluate and benchmark their performance [20, 38, 77, 79]. However, these benchmarking initiatives have largely maintained a technocentric focus, emphasizing model performance metrics while often overlooking the broader societal implications of these technologies. These be-haviours are categorized as positivisms of technology where evaluations are rooted in law-like patterns and cause-effect relationships [78]. We propose the need for a sociotechnical approach to truly understand how these systems can impact society.\nFor example, the widely used RAGAS benchmark evaluates answer engines using a comprehensive set of metrics, including faithfulness, answer relevance, and context relevance, allowing for assessment across various dimensions without relying on ground truth human annotations [19, 20]. Similarly, the ClashEval framework [77] demonstrates how LLMs can adopt incorrect retrieved content, overriding their own correct prior knowledge more than"}, {"title": "3 Answer Engine Usability Study", "content": "We now describe the usability study we conducted aimed at un-derstanding the societal impact of answer engines when compared to a traditional search engine. We first describe the design of the study (Section 3.1), then proceed with the qualitative (Section 3.3) and quantitative (Section 3.4) findings."}, {"title": "3.1 Study Design", "content": "Figure 2 illustrates the study protocol, designed as a 90-minute one-on-one session via video conference, which was recorded and transcribed with participants' consent. We first describe participant recruitment, and then the three steps to the study: pre-study questionnaire, expert information retrieval, and debated information retrieval."}, {"title": "3.1.1 Participant Recruitment", "content": "We decided to recruit participants with technical expertise (i.e., having completed or currently pursuing a Ph.D.), as it would allow participants to evaluate the systems on complex queries participants have expertise in. Participant expertise allows us to understand the performance of such systems in realistic but technically advanced topics. Our recruitment criteria targeted experts from diverse academic and professional backgrounds. Participants were recruited through a combination of academic channels (via email invitation and LinkedIn), and social media platforms (via Twitter and Reddit). The study was conducted using the User Interviews platform, and Google Meet for video-conferencing.\nWe recruited 24 participants aged 22 to 38 years (M=29.3, SD=2.99), with a gender distribution of 66.67% female (n=16), 33.34% male (n=7), and 4.16% non-binary (n=1). Participants' occupations distribution were 45.83% PhD Candidates (n=11), 16.67% research professionals including postdoctoral researchers (n=4), 33.34% industry experts (n=8), and 4.17% other professionals (n=1). Regarding participant expertise, 25.00% were experts in Human-Computer Interaction (n=6), 25.00% in Artificial Intelligence and Computational Research (n=6), 20.83% in Healthcare and Medicine (n=5), 16.67% in Applied Sciences (e.g., Transportation, Meteorology) (n=4), and 12.50% in Education and Social Sciences (n=3).\nAn initial pilot study with 3 participants helped refine our method-ology and develop a preliminary codebook. The final study was conducted with the remaining 21 participants. All participants were compensated with a $60 gift card. As the authors' institution lacks"}, {"title": "3.1.2 Study Part 1: Pre-Study Questionnaire (5 minutes)", "content": "Participants completed a questionnaire (exact questions in Appendix A.1) that asked for (1) high-level demographic information, (2) participants' familiarity with answer engines, abd (3) a list of 3-4 specific technical questions in their area of expertise that they could query in a search engine. The demographic and familiarity questions are analyzed to understand the recruited participants and their experience with answer engines. The technical questions are used to form the queries used in the next part of the study."}, {"title": "3.1.3 Study Part 2: Expertise Information Retrieval (40 minutes)", "content": "During this part, participants go over one technical question at a time from the list they provided in the pre-study questionnaire and alternate between querying it in an answer engine and then in a traditional search engine. As participants review both engines' results, they were asked to \"think aloud\" [51, 59], articulating their thoughts and reactions. This approach enables us to capture detailed insights into what works and doesn't with each engine, and com-pare the results of both engines on a concrete query. Participants are encouraged to interact in-depth with the results (including by clicking on links). Once they are done, they proceed with the next question on their list. Participants typically spent 5-10 minutes per question and were able to go through an average of 6 questions in the 40-minute time frame."}, {"title": "3.1.4 Study Part 3: Debate Information Retrieval (40 minutes)", "content": "This part follows a similar structure to Part 2, but uses opinion-based queries, a common use case for search engines [32, 68]. Participants start with answering a series of questions measuring their agree-ment with various socially and politically charged statements which we collected from the ProCon debate website, on a Likert scale from 1 to 5. Based on their responses, we constructed questions that reflected both supportive and opposing viewpoints. For example, if a participant agreed with the statement \"Zoos should exist\", a sup-portive query is: \u201cWhy should zoos exist?\" and an opposing query is: \"Why should zoos not exist?\". For each issue where a participant expressed a non-neutral opinion, we prepared either a supportive or opposing query, which the participant ran through both the answer engine and the traditional search engine. We alternated between supportive and opposing queries, allowing us to understand how participants interact with information that aligns or diverges from their viewpoints. Additionally, the study examined whether the answer engine's responses influenced participants' opinions or if they perceived any bias, allowing us to better understand how these systems shape user viewpoints and reveal potential biases.\nThe study's 21 participants were divided into three groups of 7. Each group interacted with a single answer engine: YouChat, Bing Copilot, or Perplexity AI. These platforms were chosen due to their public accessibility as AI-as-a-Service (AIaaS) systems and their marketing as answer engines[48]. For the traditional search engine, Google Search was selected for all participants. We next go over the findings from the 21 completed study sessions."}, {"title": "3.2 Pre-Survey Questionnaire Analysis", "content": "Regarding participants' familiarity with generative AI (GAI), 37.5% (9/24) of participants use GAI-based applications daily, 29.1% (7/24) weekly, 25% (6/24) monthly, and 8.3% (2/24) a few times per month, confirming that most participants interact frequently with GAI.\nRegarding the use of answer engines specifically, 70.83% (17/24) of participants were familiar with these systems, 41.16% use them multiple times per week, and 58.84% at least once a month. Participants utilized answer engines to conduct research, brainstorm, plan, learn new skills, and obtain faster results compared to traditional search engines. For example, P4 noted, \"My experience with current answer engines is similar to using a traditional one such as Google. I think it's more handy,\" while P20 shared, \"I use it to get improved, accurate, and clear answers to questions, especially regarding my research studies,\" highlighting their relevance in professional and personal contexts."}, {"title": "3.3 Sociotechnical Audit of Answer Engine Shortfalls", "content": "We employed a constructivist grounded theory using a qualitative coding approach [8] to analyze our user interview data. Following Charmaz [7], the authors individually coded the transcripts line-by-line, employing inductive reasoning to develop theories [26, 27]. Using the qualitative coding platform Taguette7, we gen-erated themes from the transcript snippets. These themes were then synthesized and refined through collaborative discussions be-tween the authors, and insights were categorized with respect to the four components of an answer engine: (1) answer text, (2) citation, (3) sources, and (4) user interface. The final 16 findings are summarized in Table 1 which the next sections go over in detail."}, {"title": "3.3.1 T Theme 1: Answer Text Findings", "content": "A.I. Need for Ob-jective Detail in Generated Answers (21/21): A pervasive issue across all three answer engines was the lack of detail and contextual depth in generated responses. This shortfall affected both expertise and debate queries. Participants repeatedly found the summaries to be overly generic and insufficient, often driving them to seek more comprehensive information through traditional search engines like Google.\nParticipant P1 noted, \u201cIt was just trying to answer without actu-ally giving me a solid answer or a more thought-out answer, which I am able to get with multiple Google searches.\" P10 emphasized, \"It's too short and just summarizes everything a lot. [The model] needs to give me more data for the claim, but it's very summarized.\" These reflections highlight a common issue: a desire for answer conciseness leads to frequently omitted critical details that would substantiate claims. As a result, responses are perceived as superficial, lacking the necessary specificity and nuance. Participants also expressed concerns about the absence of 'numbers' or 'percentages' where and when required. As P2 explained, \"There's no quantitative numbers, and I would like to see that if it's citing sources. It should provide metrics maybe when it's making these declarations.\u201d\nA.II. Lack of Holistic Viewpoint (19/21): Our study revealed a limitation in the behavior of answer engines when participants engaged with opinionated queries. The answer engines frequently aligned with the bias implied in the question, neglecting to present diverse perspectives available from the retrieved sources. The re-sponses often appeared to support only the side of the argument the model inferred the participant was \"looking for,\" thereby reinforc-ing user biases. Figure 3 illustrates this by showing an example of a one-sided answer (left, Perplexity.ai) and a comparably more nuanced answer (right, You.com).\nParticipant P19 noted, \"I want to find out more about the flip side of the argument... this is all with a pinch of salt because we don't know"}, {"title": "A.III. Confident Language While Presenting Claims (16/21)", "content": "Another prominent issue identified by participants is the genera-tion of overly confident responses. All three systems frequently used terms of affirmation and certainty, even when addressing sub-jective opinions or claims. This approach can lead participants to trust the generated content without the necessary context, with the problem being highlighted for both debate and expertise queries. Figure 3 illustrates the issue: in [A] the answer engine confidently represents information without stating the nuances.\nParticipants highlighted this issue through their interactions with the models. P3 observed that \"the model uses a very magnetic or authoritative voice while making claims,\u201d which \"can definitely convince someone that this is 'the answer' instead of actually giving them the opportunity to see the issue.", "God Voice": "likening it to articles that \"make you think that it's the ultimate truth.", "It writes so confidently, I feel convinced without even looking at the source. But when you look at the source, it's bad and that makes me question it again.": 14}, {"title": "A.IV. Overly Simplistic Writing Form and a Lack of Critical Thinking (14/21)", "content": "The fourth finding highlighted by many participants is the sim-plistic nature of the language used in responses. Participants noted that this simplicity in language reflects a lack of critical analysis and thinking.\nFor example, P13 found the answers to be \"kind of childish,\" not-ing they did not match the scientific level required. P2 described the text as \"fluffy\u201d and \u201csimilar to what a fifth grader might write without consulting sources\". P5 quoted If I was grading a student's assignment and they had given me that answer... I don't know that I would want to pass a student."}, {"title": "3.3.2 Theme 2: Citation", "content": "C.I. Misattribution of Sources: Correct Summaries, Incorrect Citations (21/21):\nA common issue identified in this theme was the misattribution of sources. This occurs when the answer engine cites a source that does not factually support the cited statement, misrepresenting the source content. For instance, P12 noted, \"It has cited irrelevant parts of the paper for this question.\" P15 commented, \u201cBut this statement doesn't seem to be in the source. I mean the statement is true; it's valid... but I don't know where it's even getting this information from.\u201d Similarly, P17 observed, \"So [the answer] mentions information on nutrients, but that is not present at all in the whole sentence and the whole article [cited].\u201d\nParticipants felt that the systems were using citations to le-gitimize their answer, creating an illusion of credibility. This facade was only revealed to a few users who proceeded to scru-tinize the sources. P4 expressed concern, \"But does it feel that it's using citations to legitimize itself for its own generation?\" and elabo-rated, \"I mean it's just like you see a citation, you assume it's a valid source...I'll just see that there is a source. That's it. I don't verify it.\u201d This misattribution highlights a major flaw in the model's process: even when a statement is accurate, the citation can point to an irrelevant source."}, {"title": "C.II. Cherrypicking Information Based on Assumed Context (19/21)", "content": "When participants posed expert or opinion-based questions, they noticed that the system often selectively presented information from retrieved sources, highlighting a particular perspective instead of a comprehensive view of the article. For instance, P7 noted, \"I feel [the system] is manipulative. It takes only some information and it feels I am manipulated to only see one side of things.", "[The source] actually has both pros and cons, and it's chosen to pick just the sort of required arguments from this link without the whole picture.": 12}, {"title": "C.III. Missing Citation for Claims and Information Generated (18/21)", "content": "The absence of citations in many of the sentences generated by all three answer engines emerged as a critical issue, especially when key claims or facts are presented without necessary factual support from the sources. P8 expressed frustration with this issue, stating, \"I really wanted a reference with this [claim] because it is like giving a statement mentioning that this is historical but this is without any citations or without any validation.\" Similarly, P16 highlighted the inconvenience caused, noting, \"Not having the references for each sentence is annoying...you want to know what's the resource retrieved is. Now we'll have to actually go to the website and compare notes, which is an additional step which no one wants to do. I would have gone to the website in the first place instead.\u201d These comments reveal a clear demand for citations, particularly for sentences that are critical to answering the question."}, {"title": "C.IV. Lack of Transparency of Source Selection in Model Responses (15/21)", "content": "Participants raised significant concerns about the lack of trans-parency in how the system selects and prioritizes citation, highlighting the need for a clearer explanation of its decision-making process. This \u201cblack box\" nature of the system makes it difficult for users to trust the outputs, as they cannot discern why certain sources are cited over others. Participants frequently noted that the system did not adequately prioritize important or factual sources, leading to a general de-emphasis of critical references. For instance, P4 questioned, \"Where is it getting this thing from? Why is it getting it from these particular sources is what I'm curious about.\" Additionally, P2 expressed frustration with the system's opaque process on the lack of stating which specific part the information came from the source, stating, \"It's very easy to just cough out sources and be like, 'this is where I took all this information from. But which part of the information did you take from? That kind of explanation doesn't exist.\u201d\nSuch feedback underscores the importance of transparency in the system's source evaluation criteria. As P5 remarked, \u201cIt's picking results from the first page of Google Search... but I didn't see any consistency,\" suggesting that users need more clarity to trust the system's outputs fully.\""}, {"title": "3.3.3 Theme 3: Sources", "content": "S.I. Low Number of Sources (19/21):\nFor both expert and opinionated questions, participants encoun-tered experiences where they needed more sources to address the question at hand. Participants highlighted this issue with spe-cific feedback. For instance, P16 remarked, \"It feels like it's pulling all of this from one source,\" while P1 noted, \"Again, everything is extracted from the same source, which is very weird.\u201d This indicated a pattern where the answer engine heavily relied on a limited num-ber of sources, averaging to three sources used, often leading to incomplete answers.\nInterestingly, this issue also caused a phenomenon where the model overtly emphasized certain sources for generation. Par-ticipants flagged this as a consequence of the models using very few sources for their answer. P5 mentioned, \"If it's like citing the right review paper, it can get away with citing only one [source], but it isn't doing that and citing one weak article throughout,\u201d and P11 added, \"It feels off as it's just paraphrasing that one source.\u201d By relying on a narrow selection of sources, the answer engines fail to capture the full spectrum of information necessary for a well-rounded answer."}, {"title": "S.II. More Sources Listed Than Used (13/21)", "content": "Participants using Bing Copilot and Perplexity noted that these systems often listed multiple retrieved sources without actu-ally citing them in the generated answer, a behavior described as \"buffing\"-creating an impression of thoroughness without sub-stance. This practice led to confusion and eroded trust, as users saw citations that were not integrated into the generated response. For example, P12 remarked, \"It's giving the impression of multiple sources, but it's just citing something that has a blurb citing to the other source. So it's really just coming off of this one source.\" Similarly, P20 observed: \"Even when it lists other sources that might be relevant or have other viewpoints, it doesn't actually say much about them.", "Why didn't they use this [listed but uncited article]? This would be much more reliable relatively!\" Notably, however, YouChat did not display this specific weakness and consistently cited all listed sources in its responses.\"\n    },\n    {\n      \"title\": \"S.III. Lack of Trust in Source Types (12/20)\",\n      \"content\": \"The answer engines retrieve content from varying sources, in-cluding forums, blogs, opinion pieces, and research articles. How-ever, participants expressed general distrust toward certain sources due to perceived biases, or lack of authority. This distrust was ev-ident in feedback like P1's remark, \\\"Who knows who has written that post [...] it's a LinkedIn post when the question is a scientific one,\" and P2's observation, \\\"The sources are not convincing. They seem to be these non-factual sources from where this answer is kind of drawn.\" Additionally, participants noted issues with outdated or misinformed content being used to generate answers. For example, P10 mentioned, \\\"I think the citation is outdated... because it says 'Windows 10', and we've already switched to better OS, which is what the answer needed": ""}, {"title": "S.IV. Different Sources but Duplicate Content (12/21)", "content": "Participants identified instances where the answer engine re-trieved and cited multiple sources that, upon closer inspection, contained identical or highly similar content. While these sources were presented as distinct entities with different citation numbers, they ultimately contributed redundant information. Such find-ings raised concerns about potential inaccuracies in the system's source selection process. For example, P3 observed, \u201cSource 1 and 2 are just the same content in different formats. This is funny as it's using them differently,\u201d and P9 stated, \"I think this is a big problem! I should have checked the citations on the other answer as well be-cause it's basically just giving me one website but citing it differently multiple times.\u201d\nThis phenomenon reveals limitations in the model's sourcing strategy, suggesting a reliance on superficial differences, such as formatting or minor variations, to justify multiple citations. As a result, the system provides a misleading appearance of a well-rounded answer while simply recycling the same content."}, {"title": "3.3.4 Theme 4: User Interface", "content": "U.I. Autonomy over Source Validation and Selection (17/21):\nParticipant feedback underscored that answer engines often offer narrow perspectives, due to users having little to no control over the information presented to them, leading to concerns about a lack of autonomy and the inability to evaluate the credibility of sources independently. P5 stated, \u201cThe [answer engine] did not give me a whole lot of confidence in its ability to assess the quality of sources. I would have more trust in my ability to do that.\" This sentiment suggests that users feel more competent in selecting and scrutinizing sources themselves. P11 further elaborated, \"[The answer engine] is taking from these sources, but are they even legit?\""}, {"title": "U.II. Lack of Human Input in Generation and Source Selection (17/21)", "content": "Our study identified a key issue with answer engines: the ten-dency to lose context when generating responses due to the answer engine assuming the most probable context rather than accurately interpreting or clarifying the specific context. Partici-pant P7 highlighted this issue: \u201cI think the sources were right, but the context is lost. It did not manage to go to the expected answers at all.", "limitation": "even with accurate sources, the inability to maintain the correct context of the query results in un-satisfactory answers. P9 further noted, \"This is a very generic answer that just has basic features [of the answer]. It doesn't say anything at all of the context asked.\u201d Participants suggested that the single-interaction design contributes to this issue. In cases where context is critical, participants recommended that the system should adopt a more interactive, conversational approach by asking questions before answering. By asking clarification questions, answer engines could better address ambiguity and provide more accurate responses. P1 also remarked, \"Having [the system] ask further ques-tions to alleviate any potential ambiguity on the generation would have helped, instead of assuming the context.\u201d"}, {"title": "U.III. Additional Work to Verify and Trust Answers (14/21)", "content": "As described in previous findings, participants often felt com-pelled to independently verify the provided sources due to distrust. Therefore, contrary to their intended purpose, some participants found that the answer engines often resulted in more work for users, undermining their marketed goal of simplifying information retrieval.\nParticipant P2 highlighted this inefficiency: \u201cEven if it's gonna give me five paragraphs with 300 sources, it's not going to help me because [...] I'm gonna go and check each source, so this is just another level of information interaction for me."}]}