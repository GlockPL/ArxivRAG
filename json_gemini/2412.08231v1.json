{"title": "Dynamic Modality-Camera Invariant Clustering for\nUnsupervised Visible-Infrared Person\nRe-identification", "authors": ["Yiming Yang", "Weipeng Hu", "Haifeng Hu"], "abstract": "Unsupervised learning visible-infrared person re-\nidentification (USL-VI-ReID) offers a more flexible and cost-\neffective alternative compared to supervised methods. This field\nhas gained increasing attention due to its promising potential.\nExisting methods simply cluster modality-specific samples and\nemploy strong association techniques to achieve instance-to-\ncluster or cluster-to-cluster cross-modality associations. However,\nthey ignore cross-camera differences, leading to noticeable issues\nwith excessive splitting of identities. Consequently, this under-\nmines the accuracy and reliability of cross-modal associations.\nTo address these issues, we propose a novel Dynamic Modality-\nCamera Invariant Clustering (DMIC) framework for USL-VI-\nReID. Specifically, our DMIC naturally integrates Modality-\nCamera Invariant Expansion (MIE), Dynamic Neighborhood\nClustering (DNC) and Hybrid Modality Contrastive Learning\n(HMCL) into a unified framework, which eliminates both the\ncross-modality and cross-camera discrepancies in clustering. MIE\nfuses inter-modal and inter-camera distance coding to bridge\nthe gaps between modalities and cameras at the clustering\nlevel. DNC employs two dynamic search strategies to refine the\nnetwork's optimization objective, transitioning from improving\ndiscriminability to enhancing cross-modal and cross-camera gen-\neralizability. Moreover, HMCL is designed to optimize instance-\nlevel and cluster-level distributions. Memories for intra-modality\nand inter-modality training are updated using randomly selected\nsamples, facilitating real-time exploration of modality-invariant\nrepresentations. Extensive experiments have demonstrated that\nour DMIC addresses the limitations present in current clustering\napproaches and achieve competitive performance, which signifi-\ncantly reduces the performance gap with supervised methods.", "sections": [{"title": "I. INTRODUCTION", "content": "ERSON re-identification is employed to identify and\nlocate specific individuals among pedestrians captured in\nmultiple camera surveillance scenarios [1], [16]. This technol-\nogy have been significantly developed during the last decade\ndue to its important role in the fields of multimedia data\nretrieval and criminal investigation [3], [4], [5]. Earlier works\non person Re-ID focus on the retrieval of pedestrian images\ncaptured by RGB cameras. However, such methods falter in\nlow-light conditions. This is primarily due to the inherent\nlimitation of RGB cameras in acquiring high-definition images\nin darkness. Therefore, visible-infrared person re-identification\n(VI-ReID) is proposed and employed to form 24-hour surveil-\nlance system, which aims to match infrared images under poor\nillumination with visible images under good illumination.\nThe current VI-ReID methods focus on generation and sub-\nspace mapping technique to learn modality-invariant represen-\ntations, achieving notable success [6], [7], [8], [9]. However,\ntheir dependence on manually annotated associations between\nvisible and infrared modalities can hinder the scalability and\ndeployment of the VI-ReID model. Unsupervised Learning\nVisible-Infrared Person Re-Identification (USL-VI-ReID) is\nintroduced to eliminate this reliance on annotations and gains\nincreasing attention due to its promising potential.\nThe USL-VI-ReID method explores cross-modal associ-\nations, eliminating the requirement for manual identity la-\nbeling [10], [11]. Although DBSCAN [12] is an effective\nclustering algorithm for pseudo-labeling unlabeled data, chal-\nlenges still arise due to substantial cross-modality and cross-\ncamera variations within the images from the same ground\ntruth ID [6], [13], [14]. Variations across cameras\nand modalities lead to excessive identity splitting and hinder\naccurate label assignment. Fine-tuning the network using these\nlabels may amplify the distances within the same class more\nthan those between different classes. In this case, additional\nnoise can be potentially introduced to adversely affect the\nperformance of the model. Existing methods [15], [16], [17],\n[18], [19] primarily focus on eliminating differences between\ndifferent modalities while neglecting the challenge of cross-\ncamera discrepancy. For instance, previous approaches utilize\ngraph matching [16] and optimal transmission [20] techniques\nto facilitate cross-modal cluster association. However, these\nmethods are impeded by the severe issue of excessive identity\nsplitting, which may affect the accuracy of the association.\nTo address the above problems, we propose a novel Dy-\nnamic Modality-Camera Invariant Clustering (DMIC) frame-\nwork to eliminate cross-modality and cross-camera discrepan-\ncies at the clustering level. The flowchart of DMIC is displayed\nin Fig. 2. To be specific, DMIC naturally combines Modality-\nCamera Invariant Expansion (MIE), Dynamic Neighborhood\nClustering (DNC) and Hybrid Modality Contrastive Learning\n(HMCL) into a joint framework. The MIE integrates inter-\nmodal and inter-camera distance coding, generating robust\nembeddings for the clustering algorithm. This eradicates the\nimplicit inclusion of modality and camera information in the\ndistance coding, resulting in modality-camera invariant embed-\ndings. Consequently, modality-camera invariant associations\ncan be established. To tackle the issue of intra-class dis-\ntances surpassing inter-class distances due to excessive identity\nsplitting, the DNC employs two dynamic search strategies.\nSpecifically, for the first stragegy, we dynamically narrow the\nsearch radius to include the reliable positive samples within\nclusters, thereby enhancing the model's ability to distinguish\nbetween relevant and irrelevant samples. Subsequently, we\ndynamically broaden the search radius, using the model's\nrefined discrimination to effectively incorporate reliable cross-\ncamera and cross-modality samples into the cluster. The\nsecond strategy involves recalibrating the expanded distance\ncoding. This refinement facilitates a broaden affinities of more\ncross-camera instances, which can be leveraged to contribute\nto cross-camera invariant learning. Importantly, our strategies\ndo not require additional parameters as a cost, effectively\nenhancing the model's performance. Taking inspiration from\n[21], we design HMCL to optimize instance-level and cluster-\nlevel distributions. We randomly select instance from different\nmodalities as cluster's centroid and update the representations\nof clusters in a real-time manner, which effectively reduces\nmodal gap.\nTo sum up, the main contributions of this paper are list as\nfollows:\n\u2022\nWe propose a novel DMIC network for USL-VI-ReID\nthat simultaneously eliminates both the cross-modality\nand cross-camera discrepancies in clustering.\n\u2022\nThe MIE fuses distance coding between inter-modal and\ninter-camera instances, which bridges cross-modality and\ncross-camera gaps.\n\u2022\nThe DNC consists of two dynamic search strategies\nthat do not require additional parameters, facilitating\nearly optimization of model discriminability and grad-\nually extending generalization to different cameras and\nmodalities."}, {"title": "II. RELATED WORK", "content": "In this section, we provide a brief overview of the fol-\nlowing areas: Supervised Visible-Infrared person ReID (SVI-\nReID), Unsupervised Single-Modality Person ReID (USL-\nReID), and Unsupervised Learning Visible-Infrared Person\nRe-Identification (USL-VI-ReID)."}, {"title": "A. Supervised Visible-Infrared person ReID", "content": "Supervised Visible-Infrared Person ReID (SVI-ReID) can\nbe roughly divided into feature-level and image-level modality\nalignment methods.\nThe feature-level modality alignment methods aim to project\ncross-modal features into a shared subspace and employs met-\nric learning techniques to narrow the distribution. In pursuit of\nthis goal, Ye et al. [6] introduce a modality-aware collaborative\nensemble learning approach to eliminate modal discrepancies\nat both the instance and classifier levels. Lu et al. [7] propose a\ntwo-step Wasserstein loss to align modality-unrelated informa-\ntion, which includes viewpoint, background, and posture. To\nfully leverage a wide array of cross-modality cues, a diverse\nembedding expansion network [22] is introduced to expand\nthe feature set and reduce modal differences through triple-\nlevel constraints. Hao et al. [23] design a camera-aware and\nmodality-aware framework to enhance the discriminability and\ngeneralization of cross-modal representations.\nImage-level modality alignment methods use generative\nnetworks or image enhancement techniques to eliminate modal\ndifferences between pixels. Ye et al. [24] introduce a Channel\nexchangeable Augmentation (CA) method to enhance the gen-\neralization of visible stream. Li et al. [25] design a lightweight\nnetwork to transform visible images into X-modality in self-\nsupervised manner. Zhang et al. [26] propose a non-linear\nsimple generator to synthesize cross-modal images to middle\nmodality. Furthermore, several works [8], [9], [27], [28] suc-\ncessfully employ Generative Adversarial Networks (GANs) to\nachieve the transitions between visible and infrared modalities\nwhile preserving identity information. However, it is noted that\nmodality alignment methods may inevitably introduce noise\nand may not be suitable for scenarios demanding high real-\ntime performance."}, {"title": "B. Unsupervised Single-Modality person ReID", "content": "Unsupervised Single-Modality Person ReID (USL-ReID)\ntries to tackle the demanding and time-intensive task of assign-\ning labels to visible modality images. Recently, mainstream\nmethods fine-tune the network by assigning labels to the\ndata through clustering algorithms. Memory-based learning\nmethods are then used to optimize the relationships between\ninstances and clusters. SPCL [29] design unified contrastive\nlearning to distinguish inter-cluster distributions. Cluster-\nContrast [30] refines InfoNCE [31] and presents ClusterNCE,\neffectively enhancing the optimization of unsupervised clusters\nand improving the performance of unsupervised methods.\nLan et al. [32] introduce multi-view features to contrastive\nframework, which enables efficient mining of partial cues and\nthe refinement of pseudo labels. To solve camera discrepancy\nproblem, Xuan et al. [33] divide unsupervised learning into\ninter-camera training and intra-camera training to generate\nreliable pseudo labels for cross-camera data. Zhang et al. [34]\npropose time-based camera contrastive learning to select the\nhardest camera centroid as a proxy for each cluster. ICE [35]\ndesign cross-camera proxy contrastive loss to mitigate camera\ndiscrepancy."}, {"title": "C. Unsupervised Visible-Infrared person ReID", "content": "The existing Unsupervised Learning Visible-Infrared Person\nRe-Identification (USL-VI-ReID) methods mainly focus on\nestablishing cross-modal associations. ADCA [15] adopt count\npriority selection method to facilitate cross-modal fusion.\nWu et al. [16] design two cross-modal graphs to discover\ncorrespondences between different modalities. Pang et al.\n[17] extract three channels from visible images, conduct\nclustering with infrared images, and subsequently utilize IoU\nfor label refinement. Liang et al. [18] pretrain model with\nlabeled single-modality dataset and introduce a homogeneous-\nto-heterogeneous training method. Wang et al. [20] utilize\noptimal transport techniques to transfer label knowledge from\nthe visible modality to the infrared modality. However, the\nabove-mentioned methods do not take into account the impact\nof camera differences in clustering phase. Especially, the\nsusceptibility of visible images to camera discrepancy will\nlead to excessive identity splitting. In this case, using one-to-\none or one-to-many association methods can exacerbate the\nimpact of noisy labels. Unlike the above-mentioned methods,\nwe fully utilize camera information throughout the clustering\nprocess to solve the identity splitting problem. Like ICE [35]\nand CAP [55], GUR [37] and DCCL [36] cluster the samples\nwithin camera and employ cross-camera proxy contrastive\nloss to solve camera discrepancy. Unlike these methods, we\ndo not need to perform clustering within individual cameras.\nInstead, we integrate camera information in a global clustering\napproach and introduce two dynamic search strategies to\naddress camera differences."}, {"title": "III. THE PROPOSED MODEL", "content": "In this section, we present a Dynamic Modality-Camera\nInvariant Clustering (DMIC) framework to simultaneously\nreduce cross-modality and cross-camera discrepancies. Our\nframework is illustrated in Fig. 2."}, {"title": "A. Problem Modeling", "content": "In USL-VI-ReID, we adopt dual-stream backbone AGW\n[3] as backbone f. Given visible-infrared pedestrian datasets,\nwe discard all identity labels, which means we are unable\nto use manual annotations as supervison. The visible-infrared\npedestrian datasets can be represented as $D = \\{V, R\\}$, where\n$V = \\{x_i^v\\}_{i=1}^{N_v}$ and $R = \\{x_i^r\\}_{i=1}^{N_r}$ indicate visible and infrared\nimages, respectively. $N_v$ denotes the number of visible images,\nand $N_r$ indicates the number of infrared images. We employ\nChannel exchangeable Augmentation (CA) [24] to enhance\nthe generalization ability of visible stream. CA data can be\nrepresented as $C = \\{x_i^c\\}_{i=1}^{N_v}$. Notably, we only enhance visible\nimages using CA technique during the training process.\nIt should be noted that within our framework, we include\ntwo training phases, i.e., intra-modality training and inter-\nmodality training. The intra-modality training aims to en-\nhance the initial discriminability of model, while the inter-\nmodality training aims to develop the cross-modality and\ncross-camera generalization of model. For intra-modality train-\ning, we employ intra-modality clustering to assign modality-\nspecific labels $\\{\u0177^v, \u0177^r\\}$ to the data in two different modalities,\nseparately. In the context of inter-modality training, we utilize\nboth intra-modality clustering and inter-modality clustering.\nInter-modality clustering involves taking both infrared and\nvisible data as inputs to the clustering algorithm, which\nassigns modality-shared labels $\\{\u0177^m\\}$. Therefore, during inter-\nmodality training, each sample generates two pseudo-labels,\ni.e., $\\{\u0177^r, \u0177^m\\}$ for infrared samples and $\\{\u0177^v, \u0177^m\\}$ for visible\nsamples."}, {"title": "B. Modality-Camera Invariant Expansion", "content": "In prior research [15], [16], [18], a commonly used clus-\ntering approach involved calculating Jaccard distances and\nutilizing distance encoding to generate embeddings for clus-\ntering infrared and visible data. Nevertheless, clusters within\nthe visible modality tend to experience identity splitting due\nto significant variations in lighting and viewpoint caused by\ncross-camera discrepancy [37]. It's worth noting that prior\nworks mainly focus on enhancing the strategy of using the\nclustering algorithm, like bottom-up clustering [37], [36], [38],\nwith less comprehensive consideration of improving the algo-\nrithm itself. To address the problem of cross-camera and cross-\nmodal variation, we introduce a simple and effective modal-\ncamera invariant expansion (MIE) to improve the clustering\nalgorithm in order to enhance the performance of clustering\ncross-camera and cross-modal samples.\nLet's start with the method of obtaining distance encoding\nin existing clustering-based approaches. To obtain distance\nencoding, each feature is taken as a probe to compute the\nk-reciprocal encoding vector [39] with other features:\n$D_i = [d_{i,1},d_{i,2},\u2026\u2026,d_{i,n}]$\n(1)\n$d_{i,j} = \\begin{cases}\nexp(-M(f_i, f_j)) & \\text{if } f_j \\in R(f_i, k_1)\\\\\n0 & \\text{otherwise.}\n\\end{cases}$\n(2)\nwhere $f_i$ and $f_j$ represent probe and gallery feature, re-\nspectively. The function $M(\\cdot,\\cdot)$ represents the Mahalanobis\ndistance, while $R(f_i, k_1)$ refers to the set of k-reciprocal\nnearest neighbors for $f_i$, with $k_1$ serving as a hyperparameter\nfor adjusting the proximity range for distinguishing these\nnearest neighbors.\nThe distance encoding of $f_i$ is then expanded by incor-\nporating the distance encodings of the top-$k_2$ most similar\ngallery instances from its reciprocal neighbors. In this case,\nthe distance encoding is effectively fused with contextual\ninformation from neighboring elements:\n$D_i = \\frac{1}{k_2} \\sum_{j=1}^{k_2} D_j$\n(3)\nwhere $k_2$ is smaller than $k_1$ to avoid introducing noisy\ninstances, and $D_i$ is the expanded distance encoding. Subse-\nquently, we compute the Jaccard distance between the probe\ninstance and other instances to obtain embedding for cluster-\ning:\n$I (f_i, f_j) = 1 - \\frac{\\sum_{\\pi=1}^{n}min (d_{i,\\pi}, d_{j,\\pi})}{\\sum_{\\pi=1} max(d_{i,\\pi}, d_{j,\\pi})}$\n(4)\n$I(f_i) = [I(f_i, f_1), I (f_i, f_2),\u2026\u2026, I(f_i, f_n)]$\n(5)\nwhere $min$ and $max$ operate the element-based minimiza-\ntion and maximization for two input vectors. $I(f_i)$ denotes\nthe embedding of $f_i$ for clustering. Then we take $T$\n=[I(f1), I(f2),\u2026\u2026, I(fn)]T as input to clustering algorithm\n[12] for assigning pseudo labels $\u0177 = DBSCAN(J)$.\nHowever, the majority of gallery instances are most similar\nto the query instances that have the same camera information.\nConsequently, the expanded distance encoding is unable to ef-\nfectively capture the cross-camera neighborhood relationships.\nThus, we equally fuse information from different cameras,\nensuring that samples from different cameras can contribute\nequally to the expanded distance encoding. Eq. 3 can be\nrewritten as:\n$D_i = \\frac{1}{n_{camera}} \\sum_{c=1}^{n^{c}} D^{camera}$\n(6)\n$D^{camera} = \\frac{1}{n_{c}} \\sum_{j=1}^{k_2} 1\\{\\ell_{camera} = c\\}D_j$\n(7)\nwhere $1\\{\\cdot\\}$ is the indicator function, $n_{c}$ denotes the number of\ninstance from camera c, $n^{c}$ indicates the number of camera,\nand $\\ell_{camera}$ represents the camera label.\nNotably, we employ MIE in the phase of intra-modality\nclustering and inter-modality clustering. Especially, we can\nbridge cross-modality discrepancy when fusing distance cod-\ning from visible camera domains and infrared camera domains.\nUnlike recent methods [35], [36], [37], [40], [55] focusing\non camera differences, which employ camera proxies for\ncontrastive learning, we introduce camera information during\nglobal clustering."}, {"title": "C. Dynamic Neighborhood Clustering", "content": "The clustering performance of USL-VI-ReID methods is\ncritically influenced by hyperparameters in clustering algo-\nrithm, such as eps, k1, and k2. These hyperparameters affects\nclustering objectives, thereby influencing the optimization path\nof the network. Specifically, eps defines the search radius for\nidentifying neighborhoods, k\u2081 adjusts the proximity range to\ndifferentiate nearest neighbors, and k2 determines the top-k2\nmost similar gallery instances used for expanding distance\nencoding. However, in existing methods [15], [16], [17],\nthese hyperparameters are set empirically and remain constant\nduring training. It is important to note that the choice of eps\ndetermines the inclusion of noisy instances in the clusters. If\neps is set to be too large, it might result in including noisy\ninstances, whereas if eps is too small, it might lead to the\nexclusion of many valid cross-camera and cross-modality sam-\nples. Although some methods take into account the influence\nof eps, their consideration is not comprehensive. PUL [10]\nmaintains small eps to select reliable samples, but this may\nnot utilize valuable hard positive samples. And this method\nrequires a predetermined number of identity categories, which\nis unknowable. DCCC [11] only consider dynamic downsizing\nadjustment of eps and do not consider further scaling up to in-\ncorporate more hard positive samples with the help of network\ndiscriminatory power. Additionally, during the early stages\nof training, the model's discriminative capability is limited,\nand setting a higher value for k2 may incorporate inaccurate\ncorrelation data into the expanded distance coding. On the\ncontrary, maintaining an appropriate value of k2 during the\nmiddle and late stages of training prevents the aggregation of\ndistance coding with cross-modality and cross-camera samples\nthat exhibit relatively low similarity. The dynamic adjustment\nof k2 is not considered in current methods.\nIn this subsection, we introduce Dynamic Neighborhood\nClustering (DNC) to dynamically adjust the clustering ob-\njective. We anticipate that DNC, in collaboration with MIE,\ncan effectively address the identity splitting issue. First and\nforemost, we begin with an assumption: during the early stages\nof model optimization (intra-modality training), we aim to\nminimize the inclusion of noisy instances within clusters to\nenhance the model's discriminative power. As the optimization\nprogresses into the middle and later stages (inter-modality\ntraining), our objective shifts towards gradually incorporating\ncross-modality and cross-camera positive samples into the\nclusters to improve the model's ability to generalize. To\nachieve this, we implement dynamic exponential schedulers\nfor eps and k2:\n$\\pi_1 =  \\pi_2 * \\sigma_n^{epochs}$     (8)\n$\\pi_2 = \\pi_1 * \\sigma_g^{epochs}$  (9)\n$\\epsilon_1 = \\epsilon_2 * \\sigma_k^{epochs}$  (10)\nwhere $\\sigma_n \\in [0,1)$ denote the decay ratio, while $\\sigma_l \\in (1, +\\infty]$ and $\\sigma_k \\in (1,+ \\infty]$ indicate growth ratio. \u03c0\u2082 is the upper\nbound and \u03c0\u2081 is the lower bound of eps. \u20ac2 (> \u20ac1) represents\nthe upper bound of k2. To better illustrate the use of dy-\nnamic strategies for intra-modality and inter-modality training,\nwe initially define the estimation of modality-specific labels\n$\\{\u0177^v, \u0177^r\\}$:\n$\u0177^v = DBSCAN(J^v; k_1, k_2, eps)$     (11)\n$\u0177^r = DBSCAN(J^r; k_1, k_2, eps)$\nwhere Ir and Ju are embeddings obtained from MIE. Please\nnote that k\u2081 and k2 are not hyperparameters for DBSCAN.\nThey are hyperparameters calculated within MIE. However,\nwe include them here for clarity in our further demonstration.\nThen, we also define estimation of modality-share labels\n$\\{\u0177^m\\}$:\n$\u0177^m = DBSCAN(J^m; k_1, k_2, eps)$     (12)\nwhere Im is the embedding of infrared and visible data\ntogether as input for the MIE module.\nAs previously mentioned, intra-modality training focuses\non improving the initial discrimination capabilities of the\nmodel, whereas inter-modality training aims to enhance the\nmodel's ability to generalize across different modalities and\ncameras. Consequently, we employ distinct dynamic strategies\nfor these two stages of training to accomplish this objective,"}, {"title": "D. Hybrid Modality Contrastive Learning", "content": "With the intra-modal pseudo labels $\\{\u0177^v, \u0177^r \\}$ and inter-modal\npseudo labels $\\{\u0177^m\\}$ obtained from MIE and DNC, we propose\nHybrid Modality Contrastive Learning (HMCL) to refine the\ndistributions between clusters and instances.\nUnsupervised methods commonly take refined InfoNCE\n[30], [31] as their loss function, which can be defined as:\n$\\mathcal{L} = - \\frac{1}{PXZ}\\sum_{i=1}^{PXZ}log\\frac{exp(q_i \\cdot [\\mathcal{Y}_i]/\\tau)}{\\sum_{k=0}^{I}exp(q_i\\cdot [\\Phi_k]/\\tau)}$     (13)\nwhere P and Z indicate the number of sampled individuals\nand instances per individual, respectively. qi denotes L2-\nnormalized query instance in training mini-batch. $[\\mathcal{Y}_i]$ is\nthe positive cluster representation of qi and $[\\Phi_k]$ represents\neach cluster representation stored in memory, where cluster\nrepresentations are obtained by averaging the instance features\nin the clusters. $ \\tau $ is the temperature factor and I is the total\nnumber of clusters. Then a momentum updating strategy [31]\nis employed to update the memory after each iteration:\n$\\Phi[\\hat{y_i}] =  \\lambda \\Phi[\\hat{y_i}] + (1 - \\lambda)q_i      (i = 1, 2, ..., P \\times Z)$ (14)\nSimilar to [21], we adopt a strategy of randomly sampling\ninstances to update cluster representations. Subsequently, we\nintroduce cluster-level and instance-level losses to refine global\nand partial distributions, respectively.\nFor intra-modality training, we construct two kinds of\nmemories, i.e., cluster-level and instance-level memories:\n$\\Phi_{\\mathcal{Y}}[i] = f_i^r$\n$\\Phi_{Y}[i] = f_i^v$\n$\\Phi[\\mathcal{O}^r_{\\mathcal{Y}}] = \\frac{1}{|\\mathcal{O}^r_{\\mathcal{Y}}|} \\sum_{i \\in \\mathcal{O}^r_{\\mathcal{Y}}} f_i$\n(15)\n$\\Phi[\\mathcal{O}^v_{\\mathcal{Y}}] = \\frac{1}{|\\mathcal{O}^v_{\\mathcal{Y}}|} \\sum_{i \\in \\mathcal{O}^v_{\\mathcal{Y}}} f_i$\nwhere $f_i^r$ and $f_i^v$ represent instance-level memories for in-\nfrared and visible modalities, while $\\Phi[\\mathcal{O}^r_{\\mathcal{Y}}]$ and $\\Phi[\\mathcal{O}^v_{\\mathcal{Y}}]$ indicate\ncluster-level memories. $\\mathcal{O}_{Y}^{(v)}$ denotes the $\\mathcal{Y}$-th cluster set\nin infrared or visible modality, and $|\\cdot|$ represents the number\nof instances in specific cluster. To establish a stable starting\npoint for optimization, we initialize the cluster-level memory\nby taking the average value. After each iteration, we randomly\nselect instances to update memory instead of employing mo-\nmentum updating strategy, ensuring the real-time update of\nmemory. In the case of the visible modality, to fully leverage\nthe CA modality, we randomly update visible memory using\nCA and visible features, specifically $[i] \\leftarrow f_i^{(v,c)}$ and\n$\\Phi[\\mathcal{Y}] \\leftarrow f_i^{(v,c)}$. As for infrared modality, we update memory\nby $[i] \\leftarrow f_i^r$ and $\\Phi[\\mathcal{Y}] \\leftarrow f_i^r$.\nThe cluster-level contrastive loss for visible and infrared\nmodalities can be expressed as:\n$\\mathcal{L}^v = - \\frac{2}{3XPXZ} \\sum_{i=1}^{3XPXZ} log\\frac{exp(q_i^{(v,c)} \\cdot \\Phi[\\mathcal{Y}]/\\tau)}{\\sum_{k=0}^{I}exp(q_i^{(v,c)}\\cdot [\\Phi_k]/\\tau)}$\n(16)\n$\\mathcal{L}^r = - \\frac{2}{3XPXZ} \\sum_{i=1}^{3XPXZ} log\\frac{exp(q_i^{r} \\cdot \\Phi[\\mathcal{Y}]/\\tau)}{\\sum_{k=0}^{I}exp(q_i^{r}\\cdot [\\Phi_k]/\\tau)}$  (17)\nwhere $q_i^{(v,c)}$ denote the query features in the mini-batch from\nvisible and CA modalites, and $q_i^r$ indicates the query fea-\nture from infrared modality.\nTo explore the instance relationships during the training\nprocess, the instance-level contrastive loss can be formulated\nas:\n$\\mathcal{L}_i^v = - \\frac{2}{3XPXZ} \\sum_{i=1}^{3XPXZ} log\\frac{exp(q_i^{(v,c)} \\cdot \\Phi_{\\mathcal{Y}}[i]/\\tau)}{\\sum_{k=0}^{I}exp(q_i^{(v,c)}\\cdot [\\Phi_k]/\\tau)}$ (18)\n$\\mathcal{L}_i^r = - \\frac{2}{3XPXZ} \\sum_{i=1}^{3XPXZ} log\\frac{exp(q_i^{r} \\cdot \\Phi_{\\mathcal{Y}}[i]/\\tau)}{\\sum_{k=0}^{I}exp(q_i^{r}\\cdot [\\Phi_k]/\\tau)}$ (19)\nFor inter-modality training, we enhance the training process\nby introducing global cluster-level memory and instance-level\nmemory, in addition to the intra-modality training, denoted as:\n$\\Phi_{\\mathcal{Y}}^m[i] = f_i^{(v,r)}$\n(20)\n$\\Phi^m[\\mathcal{O}^m_{\\mathcal{Y}}] = \\frac{1}{|\\mathcal{O}^m_{\\mathcal{Y}}|} \\sum_{i \\in \\mathcal{O}^m_{\\mathcal{Y}}} f_i^{(v,r)}$"}, {"title": "E. Optimization Algorithm", "content": "The proposed DMIC network naturally integrates MIE,\nDNC and HMCL into a unified framework. These key modules\ncan establish a synergistic effect to bolster the model's robust-\nness against cross-modality and cross-camera discrepancies.\nSince we have both intra-modal and inter-modal training\nstages, the overall loss function for DMIC can be formally\nexpressed as:\n$\\mathcal{L} =  \\varphi_1(\\mathcal{L}_i^v + \\mathcal{L}_i^r + \\mathcal{L}^r) + \\varphi_2(\\mathcal{L}^m + \\mathcal{L}_v + \\mathcal{L}_r)$    (23)\nwhere $ \\varphi_1$ and $ \\varphi_2$ denote the trade-off weights for the cluster-\nlevel and instance-level loss functions. The optimization pro-\ncess of our method is outlined in Algorithm 1, with epoch\nrepresenting the number of epochs, $ \\kappa $ signifying the epoch\nindex, and iter indicating the number of training iterations."}, {"title": "IV. EXPERIMENTS EVALUATION", "content": "In this section, we first introduce two datasets, i.e., SYSU-\nMM01 [41] and RegDB [42], and experiment implementa-\ntion. Then, we conduct experiments to compare our method\nwith state-of-the-art methods on these two public datasets,\nwhich shows our competitive performance. Finally, we conduct\nseveral property analyses to demonstrate the impact of each\ncomponent."}, {"title": "A. Datasets and Settings", "content": "SYSU-MM01. SYSU-MM01 [41] is a widely used dataset\nfor evaluating recently published methods. This dataset con-"}, {"title": "D. Various Properties Analysis of the DMIC Model", "content": "In this subsection, we verify the effectiveness of each key\nmodule in our DMIC method on SYSU-MM01 and RegDB\ndatasets. Ablation studies on objective functions and clustering\nstrategy are conducted to exhaustively analyze the performance\nof the network.\nAblation study for objective functions. As listed in Table\nIII, we analyze the contribution of each objective function.\nCC-1 and IC-1 represent the cluster-level and instance-level\nloses during intra-modality training, while CC-2 and IC-2\nrefer to the cluster-level and instance-level losses during inter-\nmodality training. When gradually superimposing the objective\nfunction IC-1, CC-2 and IC-2 on CC-1, the performance of the\nmodel can be effectively improved. The largest improvement\nlies in the introduction of CC-2, which nearly doubles the\nmodel performance on the Regdb dataset. CC-2 utilizes robust\npseudo-labels assigned by the MIE and DNC, which are\nmodality and camera-independent, to facilitate the learning of\nmodality-camera invariant representations. Additionally, IC-1\nand IC-2 effectively refine the relationships between instances,\nwhich leads to performance improvements.\nAblation study for clustering. Clustering is widely rec-\nognized as a critical component in unsupervised learning\nframeworks. As shown in Table IV, we analyze the effec-\ntiveness of MIE and DNC. VC refers to the vanilla clustering\nmethod employed in recent studies [15", "16": [19]}]}