{"title": "Multi-group Uncertainty Quantification for Long-form Text Generation", "authors": ["Terrance Liu", "Zhiwei Steven Wu"], "abstract": "While large language models are rapidly moving towards consumer-facing applications, they are often still prone to factual errors and hallucinations. In order to reduce the potential harms that may come from these errors, it is important for users to know to what extent they can trust an LLM when it makes a factual claim. To this end, we study the problem of uncertainty quantification of factual correctness in long-form natural language generation. Given some output from a large language model, we study both uncertainty at the level of individual claims contained within the output (via calibration) and uncertainty across the entire output itself (via conformal prediction). Moreover, we invoke multicalibration and multivalid conformal prediction to ensure that such uncertainty guarantees are valid both marginally and across distinct groups of prompts. Using the task of biography generation, we demonstrate empirically that having access to and making use of additional group attributes for each prompt improves both overall and group-wise performance. As the problems of calibration, conformal prediction, and their multi-group counterparts have not been extensively explored previously in the context of long-form text generation, we consider these empirical results to form a benchmark for this setting.", "sections": [{"title": "Introduction", "content": "In recent years, researchers have developed stronger large language models that perform well on a variety of tasks across different domains (Touvron et al., 2023; Bubeck et al., 2023; Jiang et al., 2023; Anil et al., 2023). In particular, researchers have demonstrated the capabilities of such models by showcasing their ability to generate long-form text. However, as use of LLMs continues to grow, so do concerns over their tendency to hallucinate facts (Huang et al., 2023). As a result, there is a growing need for methods that can reduce hallucinations (Manakul et al., 2023; Zhang et al., 2023), perform abstention (Yang et al., 2023), or provide correctness guarantees (Kumar et al., 2023; Mohri and Hashimoto, 2024; Quach et al., 2023).\nIn our work, we focus on the latter\u2014broadly speaking, uncertainty quantification of long-form large language model generations, which is a problem that has not previously been exhaustively explored. In contrast to the few existing works on factuality in long-form generations (Quach et al., 2023; Mohri and Hashimoto, 2024), we make the observation that while uncertainty guarantees may be valid under the full data distribution, they may not still be valid within individual subgroups of the distribution. For example, generations describing local politicians may be more prone to error than generations concerning national leaders. To address this unmet need, we introduce methods quantifying uncertainty in long-form text generation that are valid not only across a full distribution of prompts (i.e., marginally) but also across identifiable subgroups of prompts (i.e., conditionally).\nConcretely, given a set of claims produced by an LLM in response to some prompt, our goal is to provide a confidence score or uncertainty guarantee about the factual correctness of the output. We explore this problem in two settings. Given a set of claims contained within some long-form prompt response, we (1) ensure factuality at the individual claim level and (2) provide uncertainty guarantees across the whole set of claims. We approach problem (1) via (multi)calibration (H\u00e9bert-Johnson et al., 2018), in which one wishes to output a calibrated score for each claim, while for problem (2), we apply (multivalid) conformal prediction (Jung et al., 2022), selecting some subset of claims that with high probability are all factually correct.\nEmpirically, we evaluate our methods using biography generation as a testbed for multi-group uncertainty quantification of factuality. As the problems of calibration, conformal prediction, and their multi-group counterparts have not been extensively"}, {"title": "Preliminaries", "content": "In this section, we describe calibration and conformal prediction, as well as their multi-group guarantee versions, multicalibration and multivalid conformal prediction."}, {"title": "Calibration", "content": "We begin by defining calibration in context of factuality in open-ended text generation. Suppose we are given some (X, Y) ~ D where X \u2208 X denotes some claim outputted by an LLM, while Y is an indicator in which Y = 1 when the claim is correct (and Y = 0 otherwise). Suppose there exists some uncertainty score function f : X \u2192 [0,1] that measures confidence for the correctness of some input X (with higher values denoting higher levels of confidence). Then a goal one may have when designing such a score function f is to have that\n$\\mathbb{P}_D(Y=1 | f(X) = p) = p, \\forall x \\in X$  (1)\nIn other words, the probability that some LLM output is correct is given exactly by f.\nCalibration, then, defines a simpler, more tractable condition, in which instead of ensuring guarantees across all possible values of f, we ensure a guarantee over coarser, level sets of our uncertainty score.\nDefinition 1. (Calibration) A function f is calibrated w.r.t D if\n$\\Delta_p(f) = 0, \\forall p \\in [0, 1]$"}, {"title": "Multicalibration", "content": "While calibration provides an already important and useful guarantee, it can often be insufficient in many real-world scenarios. For example, in the context of generating information about people, one maybe desire that f is calibrated not only across all people, but also within subpopulations defined by demographic attributes like sex or gender. Otherwise, it is possible that certain subgroups can still suffer from very high miscalibration, even when the score function is perfectly calibrated across D. Ideally, one would hope to have guarantees while conditioning on as many subgroups in X as possible, both from the perspective of machine learning fairness as well as enhancing the likelihood of correctness in general.\nMulticalibration (H\u00e9bert-Johnson et al., 2018) was developed to provide accurate guarantees across overlapping subgroups (i.e., a sample can belong to many groups). Let g : X \u2192 {0,1} be a group function that evaluates to 1 if X belongs to some group. We study, then, the setting in which there exists of set of groups G that corresponds to our data domain D. We note that while the set of groups can be disjoint, the problem of multicalibration then becomes trivial in this case because one can simply split a dataset into disjoint sets that can then each be calibrated individually. Consequently, prior work typically considers the more interesting case where many intersecting groups comprise G Given a group function g, we define group average squared calibration error (gASCE) as the following:\ngASCE(f, g) = $\\mathbb{E}_\\mathbb{P} [\\Delta_g(f) | g(X) = 1]$  (3)\nwhere\n$\\Delta_{p,g}(f) = \\mathbb{E}_D[Y - f(X) | S_{p,g}(f)]$\nfor $S_{p,g}$ = {f(X) = p, g(x) = 1}. In other words, gASCE conditions on both level sets and group membership. Finally, we have:\nDefinition 2. (Multicalibration) A function f is a-multicalibrated w.r.t D and a set of groups G if and only if\ngASCE(f,g) <$\\frac{\\alpha}{\\mathbb{P}_D(g(X) = 1)}$, \u2200g \u2208 G"}, {"title": "Conformal Prediction", "content": "In conformal prediction, the general goal is to produce some confidence set T(X) for some example X such that this set marginally covers the true label Y with some target probability 1 \u2212 \u03b1.\n$\\mathbb{P}_D(Y \u2208 T(X)) = 1 - \\alpha$ (4)\nThe second part of our work follows the problem statement outlined in Mohri and Hashimoto (2024). Unlike in calibration, where each claim contained in some long-form generation is treated individually, Mohri and Hashimoto (2024) instead define their problem in terms of pairs (X, Y), where X is some input prompt and L(X) = Y \u2208 Y is the long-form generation outputted by a LLM L. Because Y may or may not be supported by some reference ground truth Y*,\\u00b9 Mohri and Hashimoto (2024) define factuality in terms of entailment operations Y* \u21d2 Y. Furthermore, they rewrite this relation as Y* \u2208 E(Y) = {Y' \u2208 Y : Y' \u21d2 Y}. This equivalent set notation, in other words, means that some reference ground truth Y* (e.g., Wikipedia article in Min et al. (2023a)) is contained in the set of possible texts Y' that support all claims made in the LLM output Y.\nGiven this notation, then, the goal is to find some uncertainty set T(L(X)) s.t. PD(Y \u2208 T(L(X))) = 1 \u2212 a. In the context of long-form text generation, this goal translates to taking as input the original LLM output L(X) and producing a subset of claims T(L(X)) such that with high probability, 1-a, all remaining claims are factually correct."}, {"title": "Multivalid Conformal Prediction", "content": "Similar to calibration, one may also desire group conditional coverage guarantees for intersecting groups. Known as multivalid conformal prediction (Jung et al., 2022), these guarantees are stronger than marginal conformal guarantees, holding also when conditioned on group membership. Using group functions g, as defined in Section 2.1.1, full multivalid coverage can be written as the following: Given some set of groups G, we have that\nPD(Y \u2208 T(X) | g(X) = 1) = 1-a (5)\nfor all group functions g \u2208 G."}, {"title": "Methods", "content": "Next, we introduce the methods (and their group-conditional variants) for applying calibration and conformal prediction to language model factuality. In particular, we organize these methods into two categories: (1) iterative \"patching\"-based algorithms and (2) linear regressor algorithms. As we mention previously, prior exploration of long-form text generation has been limited. While Mohri and Hashimoto (2024) evaluated one variant\u2014split conformal (SC)\u2014on a small set of entities, we are not aware of prior work that has considered other uncertainty quantification methods in this setting."}, {"title": "Iterative \"patching\" algorithms", "content": "The first category of algorithms can be characterized as \"patching\" algorithms. Given a base method for calibration or conformal prediction, one iterates through groups g \u2208 G in which the method does poorly on. At each iteration, the patching algorithm corrects the bias (i.e., patches up the function) on just that subset of examples in the data domain (i.\u0435., g(x) = 1). Once some stopping condition is met, the final, \"patched up\" satisfies multi-group guarantees.\nFor calibration, we consider the standard method of Histogram Binning (HB) (Zadrozny and Elkan, 2001), presented in Algorithm 1. This method, takes some base scoring function f and discretizes the output space to a set of p-th level sets $S_p(f)$, as defined in Section 2.1. Specifically, given some target grid of values $p \\in [m]$, we round f to the closest value in this grid\n$f'(x) = argmin_{p \\in [m]} | f (x) - p|.$"}, {"title": "Linear regressor algorithms", "content": "Next, we consider a set of algorithms that instead solve some optimziation problem for the purpose of calibration and conformal prediction. In these cases, one can naturally make them multigroup/valid by including group-membership (i.\u0435., g(X) = 1 for all g \u2208 G) in the optimization problem itself. Formally, we describe these linear regression based methods in Algorithms 4 and 5. Presented in this way, the methods for calibration vs. conformal prediction is reduced to a choice of loss function L. Again, we assume one has access to some calibration set for which one solves the optimization problem on.\nIn the case of calibration, one can choose L to be binary cross-entropy loss. In doing so, Algorithm 4 then describes Platt Scaling (PS) (PLATT, 1999), which can be described as fitting a logistic regression model to some set of model outputs to obtain calibrated probability scores."}, {"title": "Empirical evaluation", "content": "We focus our empirical evaluation on the problem of biography generation. Following Min et al. (2023a), we use an LLM to automate the process of decomposing biographies into individual claims and evaluating for factuality. Details can be found in Appendix A."}, {"title": "Dataset", "content": "We evaluate on a large set of biographies by extracting 8,541 entities from Natural Questions dataset (Kwiatkowski et al., 2019), which consists of real queries issued to the Google search engine. We denote this dataset as BIO-NQ. Our motivation for choosing Natural Questions is that these extracted human entities should serve as a representative sample of public figures that users may prompt an LLM to generate information for. To form our dataset, for each question, we select all entities in either the question's short answer or accompanying Wikipedia article. We then attempt to match them to their corresponding Wikidata entry. If a match exists and its Wikidata page's property, if instance of, is equal to the value, human, we add the entity to our dataset."}, {"title": "Collecting group features", "content": "To obtain groups for each person found in our dataset, we extract properties by scraping Wikidata for each entity and identifying ones that are commonly shared among entities in BIO-NQ. For our experiments, we use the following group attributes:\n\u2022 # Wikidata properties: For each entity, we count the number of Wikidata properties and discretize them into the following buckets: [0, 25), [25, 50), [50 \u2013 100), [100,\u221e).This group serves as proxy for the amount of information available online for some given entity.\n\u2022 nationality: Following Min et al. (2023a), who use nationality derived from Wikidata to sample their dataset of human entities, we take the property country of citizenship (or place of birth when not available) and categorize the corresponding value into the following categories defined by Min et al. (2023a): Asia/Pacific, Europe/Middle East, North America, Latin/South America/Africa.\n\u2022 sex or gender: We take directly the value for the Wikidata property, sex or gender.\n\u2022 plays professional sports: We check whether the Wikidata entry has the property, sport."}, {"title": "Generating confidence scores", "content": "As mentioned previously, the algorithms described in Section 3 require a base scoring function. For our experiments, we use a frequency-based scoring function derived from concept of self-consistency (Wang et al., 2022). Specifically, to score each claim found in a generated biography, we prompt the LLM to output a biography M additional times. We use the proportion of times the claim is contained in the additional reference generations as the uncertainty score. To automate this process, we use"}, {"title": "Results", "content": "To assess the efficacy of these methods for quantifying the uncertainty of LLM factuality, we present empirical results for the task of biography generation, using outputs from Llama 2 7B Chat (Touvron et al., 2023) and Mistral 7B Instruct v0.2 (Jiang et al., 2023). We randomly split the entities into 80-20 calibration-test splits. The results we present are averaged over 10 distinct randomly generated splits."}, {"title": "Calibration", "content": "To evaluate fact-level uncertainty, we consider both the ASCE and the Brier score, which is the mean squred error between the uncertainty score function f(X) and the true label Y. While not a direct measure of (multi)calibration like the ASCE, the Brier score is still useful in certain settings for quantifying the efficacy of the algorithms we consider. As discussed previously, we generate biographies for entities from BIO-NQ and apply the techniques discussed in Section 3.In Table 1, we report the ASCE, max gASCE, and mean gASCE, comparing each calibration method (HB, PS) against its multicalibration counterpart (IGHB, GCULR). The multicalibration variants of both the patching and linear regression techniques significantly outperform HB and PS in terms of max and mean gASCE. More surprising, however, is that even when considering just the ASCE across the entire dataset, incorporating group features improves performance as well, especially when comparing IGHB to HB. These results suggest that even if one does not specifically require parity for specific subgroups, collecting additional group features and applying multicalibration (as opposed to vanilla calibration) can still be extremely beneficial for generating better-calibrated uncertainty scores. In Table 2, we report Brier score, also both marginally and across groups. Again, we observe that IGHB and GCULR outperform HB and PS respectively across all metrics."}, {"title": "Conformal prediction", "content": "For the problem of uncertainty at the biography level, we apply the vanilla conformal prediction methods SC and CQR and their multivalid counterparts, MVSC and GCCQR. We choose target coverages of between 0.5 to 0.9. We initially observe that for both Llama 2 7B Chat and Mistral 7B Instruct, all methods are able to achieve close to perfect coverage on BIO-NQ (Figure 2, left panel). However, when evaluating coverage across individual subgroups, we find that all methods have some level of error. In Figure 1, we compare the mean absolute coverage across all subgroups for each target coverage. Like for calibrating individual facts, we again find that multivalid variants of both the patching (MVSC) and linear regression (GCCQR) techniques outperform standard conformal techniques (SC, CQR)."}, {"title": "Conclusion", "content": "In this paper, we conduct an extensive study on uncertainty quantification in long-form LLM generation. We focus on two forms of uncertainty quantification for factuality in long-form generation\u2014claim-level (calibration) and biography-level (conformal prediction)\u2014and present a variety of methods for these settings. Introducing two categories of algorithms (iterative patching and linear regression), we demonstrate that by accounting for additional groups, multicalibration and multivalid conformal prediction methods outperform their marginal-guarantee counterparts across group-wise performance and, in the case of calibration, overall marginal performance. We consider these empirical results to establish a benchmark for this setting and hope our findings will motivate future work in quantifying uncertainty for LLMs."}, {"title": "Limitations and potential risks", "content": "Past works on uncertainty quantification for NLP have made great strides in providing confidence scores for text generation, and our work furthers this line of research. However, we note that there exists certain limitations in our empirical findings. First, while Min et al. (2023a) demonstrate the effectiveness of using an LLM to both decompose generations into atomic facts and assess the factuality of these claims, this automated process serves only as a proxy for the gold standard of human annotation. Moreover, like in Min et al. (2023a), our assessment of factuality is limited to whether claims are supported by Wikipedia, which again serves only as a proxy for factuality.\nIn our experiments, we chose groups based on attributes that are general and easily obtainable for any public figure (i.e., Wikidata properties). This set of groups is non-exhaustive, and there may be many other sets of groups one can consider that may be relevant for various problem domains.\nFinally, while we follow prior work and focus on the problem of generating biographies, our uncertainty techniques extend broadly to any long-form generation task where information contained in the output can be broken down into independent claims. Furthermore, the automated evaluation pipeline proposed by Min et al. (2023a) is still valid, as long as the entities one prompts the LLM for a description of can be found on Wikipedia (i.e., animals, places, historical events, etc.). Consequently, in future work, we hope to validate our findings for other long-form generation tasks.\nRegarding potential risks, as mentioned previously, large language models can have a tendency to hallucinate. Moreover, deep learning models in general may exhibit or perpetuate bias. In an attempt to better quantify uncertainty of LLMs, our work aims to help alleviate the potential risks of models producing false information. Moreover, our emphasis on multi-group guarantees aligns with certain definitions of fairness. However, our work may not entirely mitigate such issues, and other potential malicious or unintended harmful effects could still persist in the outputs of the calibration and conformal methods we evaluate."}, {"title": "Details on biography generation and factuality evaluation", "content": "While the ground truth score must be human-annotated, Min et al. (2023a) show that FACTSCORE can be approximated by an automated process that leverages an LLM (i.e., ChatGPT and LLaMa-7B) and natural language retrieval. Following Min et al. (2023a), we also use an LLM to automate the annotation process. For some input person, [ENTITY], we prompt a large language model with the following:\n[INST] Question: Tell me a bio of [ENTITY]. [/INST]\nWe then decompose each long-form generation into a set of atomic facts, which are then checked against some set of Wikipedia articles about the [ENTITY] to evaluate overall performance of language model in terms of factuality. Min et al. (2023a) demonstrate that while the evaluation process should ideally be conducted by human annotators, using large language models (i.e., ChatGPT and LLama 1 7B) to both decompose long-form generations and check against Wikipedia articles serves as a very good proxy for human annotation.\nFollowing this general framework for automated evaluation, we use Llama 2 7B Chat to decompose each generation [GEN_BIO] with the following prompt:\n[INST] \u00abSYS\u00bb Break down the following input into a set of small, independent claims. You must not add additional information. Output the claims as a numbered list separated by a new line. The subject of each line should be [ENTITY]. \u00ab/SYS\u00bb Input:\n[GEN_BIO] [/INST]\nFor checking each atomic fact against Wikpedia, we directly use the code released by Min et al. (2023a), which first conducts passage retrieval via Generalizable T5-based Retrievers (Liu et al., 2023) to find relevant articles from a dump of Wikipedia (dated 2023-04-01) and then prompts an LLM (i.e., ChatGPT or Llama 1 7B) to predict whether each fact is supported by the retrieved passages. For our evaluation, we again use Llama 2 7B Chat. Finally, these predictions are ensembled with predictions using likelihood estimates derived from a nonparametric masked language model (Min et al., 2023b).\nWe note that for prompting the LLMs described above, we use Hugging Face's transformer's library and generate responses with temperature set to 1.0."}, {"title": "Additional results", "content": "In Figure 2, we provide additional information about the prediction sets outputted by our various conformal methods on BIO-NQ. On the left panel, we plot the empirical coverage achieved against the target coverage. Figure 2 demonstrates that all methods achieve the target (marginal) coverage. On the middle panel, we plot the fraction of biographies retained for each method against the target coverage level, while on the right panel, we plot the number of facts per biography retained. Generally speaking, all methods retain about the same number of facts per biography. We also observe that to achieve the same target coveragre, SC and MVSC generally retain fewer biographies (i.e., more abstentions) when compared to CQR and GCCQR. However, when comparing each conformal method (SC and CQR) to their multivalid counterparts (MVSC and GCCQR), we again observe that there are very little differences between them."}]}