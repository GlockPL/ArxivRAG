{"title": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker Verification", "authors": ["Bei Liu", "Yanmin Qian"], "abstract": "Recent speaker verification (SV) systems have shown a trend toward adopting deeper speaker embedding extractors. Although deeper and larger neural networks can significantly improve performance, their substantial memory requirements hinder training on consumer GPUs. In this paper, we explore a memory-efficient training strategy for deep speaker embedding learning in resource-constrained scenarios. Firstly, we conduct a systematic analysis of GPU memory allocation during SV system training. Empirical observations show that activations and optimizer states are the main sources of memory consumption. For activations, we design two types of reversible neural networks which eliminate the need to store intermediate activations during back-propagation, thereby significantly reducing memory usage without performance loss. For optimizer states, we introduce a dynamic quantization approach that replaces the original 32-bit floating-point values with a dynamic tree-based 8-bit data type. Experimental results on VoxCeleb demonstrate that the reversible variants of ResNets and DF-ResNets can perform training without the need to cache activations in GPU memory. In addition, the 8-bit versions of SGD and Adam save 75% of memory costs while maintaining performance compared to their 32-bit counterparts. Finally, a detailed comparison of memory usage and performance indicates that our proposed models achieve up to 16.2x memory savings, with nearly identical parameters and performance compared to the vanilla systems. In contrast to the previous need for multiple high-end GPUs such as the A100, we can effectively train deep speaker embedding extractors with just one or two consumer-level 2080Ti GPUs.", "sections": [{"title": "I. INTRODUCTION", "content": "SPEAKER verification (SV) is a crucial biometric identification technology that aims to confirm the identity of a person by examining the distinct features of their voice. A typical SV system comprises two main computational components: a front-end speaker embedding extractor and a back-end similarity calculator to compare embeddings. Before the era of deep learning, i-vector [1] coupled with probabilistic linear discriminant analysis (PLDA) [2] provides the most competitive results. In recent decades, neural networks have been widely adopted as speaker embedding extractors, achieving remarkable performance [3]. x-vector [4] presents a standard paradigm which contains three different parts: a stack of layers that process frame-level features, a pooling layer that aggregates segment-level embeddings, and a softmax layer that generates probabilities for each speaker. Firstly, a series of network layers operate on the frame-level acoustic features of a given utterance. Then, a statistical pooling layer extracts a fixed-length speaker embedding. Finally, the entire system is trained with cross-entropy loss. Follow-up works have explored a range of strategies to boost system performance, including advanced network architectures [4]\u2013[16], attentive pooling techniques [17]\u2013[21], and loss functions that are aware of hard samples [22]\u2013[25].\nIn terms of network architectures, a variety of backbones have been put forward in recent years. [4] initially applies a time delay neural network (TDNN) to the speaker verification task. Subsequent, ECAPA-TDNN [9] proposes a multi-scale computational block that integrates features from various hierarchical levels. ECAPA++ [10] further pushes the limits of performance by incorporating a recursive convolution module to capture fine-grained speaker information. Another line of research focuses on convolutional neural networks (CNN). In VoxSRC 2019, Zeinali et al. [7] adopted ResNet [26] as the embedding extractor and achieved the first place. Since then, multiple lightweight attention mechanisms, such as coordinate attention [27], simple attention [28] and triplet attention [29], have been proposed to improve its performance. In addition, the winner of VoxSRC 2021 employed a re-parameterized model named RepVGG [30]. Alternatively, [14] and [15] suggest that depth is more important than width, resulting in the introduction of a novel DF-ResNet model with an astonishing 233 layers. In addition, Chen et al. deepened ResNet to 293 layers in CNSRC 2022 [31] and VoxSRC 2022 [32]. Recently, the UNISOUND system [33] for VoxSRC 2023 sets a new milestone by increasing ResNet layers to 518. Recent speaker verification challenges have indicated a clear trend for speaker embedding extractors to become progressively deeper. Although deeper and larger neural networks can yield exceptional results, their substantial memory requirements hinder the training of deep speaker embedding extractors on consumer GPUs. For example, the DF-ResNet233 model, as proposed in [14] and [15], is trained using four A100 GPUs, each with 40GB of memory. [33] utilizes 10 V100 GPUs (32GB) for training ResNet314 and 60 V100 GPUs (32GB) for ResNet518. These computational resources are inaccessible and unaffordable to many researchers. To reduce memory costs in training large-scale SV systems, one possible approach is to reduce the batch size. However, using excessively small batch sizes can impair system performance due to inaccurate gradient estimation and batch normalization, and it can also significantly slow down the entire training process. How to achieve memory-efficient training for deep speaker embedding learning is a crucial and challenging topic in the SV field. This paper investigates the possibility of effectively training very deep networks in resource-limited"}, {"title": "scenarios for speaker verification.", "content": "Firstly, we conduct a systematic examination of GPU memory allocation throughout the training process of speaker verification systems. Modern neural networks are generally trained with back-propagation algorithm [34]. The network weights are first loaded onto the GPU, and for each training batch, the forward pass computes activations. In the backward pass, gradients are calculated to update the weights. To accelerate convergence, commonly used optimizers are stateful, which track gradient statistics over time. For example, stochastic gradient descent (SGD) with momentum [35] accumulates historical gradients for each weight. Adam [36] and its variant AdamW [37] maintain two states: gradient and the second-order gradient. In brief, GPU memory usage mainly comes from three sources: weights, activations and optimizer states. While reducing the number of weights can lower memory costs, it may also cause performance degradation, which will not be covered in this work. Instead, we will focus on optimizing GPU memory allocation from the perspectives of activations and optimizer states.\nRegarding activations, mainstream deep learning frameworks like TensorFlow [38] and PyTorch [39] store activations during the forward pass for fast gradient computation in the backward pass. Although this approach speeds up training, it causes memory usage to increase linearly with network depth. This involves a trade-off between space and time. The current method sacrifices space for time. However, the limited memory capacity of GPUs makes it difficult to accommodate very deep neural networks [40]. To address this issue, we introduce a reversible computational principle, which allows back-propagation without the need to store activations in memory. By applying this principle to ResNets and DF-ResNets, we develop two new families of reversible neural networks: RevNets and DF-RevNets. Specifically, two types of reversible networks, namely partially and fully reversible networks, are presented. They consist of a series of reversible blocks, where the activations of each layer can be recovered from the previous layer during the backward pass. Consequently, both RevNets and DF-RevNets maintain almost constant memory usage, regardless of the network\u2019s depth.\nMoreover, optimizer states are another source of memory allocation. Generally, gradients and related statistics are stored using 32-bit floating-point numbers, meaning that SGD and Adam will occupy 4 and 8 bytes per weight, respectively. To reduce the memory costs of optimizer states, we present a dynamic quantization scheme that converts 32-bit states into 8-bit. Initially, the optimizer state tensor is partitioned into smaller blocks, each of which is independently quantized to mitigate the outlier effect and reduce quantization errors. These blocks are then normalized to the range [-1,1] by dividing their absolute maximum values. A dynamic tree-based 8-bit data type is utilized to ensure precision for both small and large magnitudes. Finally, a binary search is conducted to find the nearest 8-bit value to the normalized tensor, and the corresponding integer index is stored. For parameter updates, the optimizer states are first converted from 8-bit to 32-bit. Weight updates are then performed, and the states are quantized back to 8-bit for storage. By integrating this scheme with commonly used optimizers, we create 8-bit versions of SGD and Adam, achieving a 75% reduction in memory cost while maintaining performance nearly identical to that of 32-bit optimizer states.\nTo summarize, the key contributions of this study are highlighted below. To the best of our knowledge, this is the first research to thoroughly investigate memory-efficient training methods for deep speaker embedding learning in speaker verification under the resource-limited training scenarios.\n1) Firstly, we conduct a systematic study on GPU memory allocation during the training of SV systems. Empirical analyses reveal that, aside from weights, activations and optimizer states are the main consumers of memory.\n2) For activations, a reversible computational principle is introduced to perform back-propagation without storing activations in memory. We develop two different types of reversible neural networks that significantly reduce memory costs without performance loss.\n3) For optimizer states, we propose a 8-bit dynamic quantization strategy to replace 32-bit floating-point numbers. The 8-bit versions of SGD and Adam save 75% of memory usage while maintaining performance.\n4) We evaluate the proposed methods on popular deep speaker embedding extractors, including ResNet101, ResNet152, DF-ResNet110, DF-ResNet179, and DF-ResNet233, showing their effectiveness and generality.\n5) Finally, a detailed analysis of memory utilization and performance indicates that up to 16.2x memory savings can be achieved with nearly identical parameters and performance compared to the vanilla systems.\nThis is an extended work of our previous paper [41]. We first systematically examine the training GPU memory allocation for speaker verification systems. Furthermore, a dynamic quantization scheme is presented to compress 32-bit optimizer states into 8-bit without performance loss. During the experiments, we conduct an extended evaluation on a family of DF-ResNets, including DF-ResNet56, DF-ResNet110, DF-ResNet179 and DF-ResNet233. Additionally, a thorough analysis of memory usage and performance is provided, highlighting the effectiveness and broad applicability of our memory-efficient training approaches for various deep speaker embedding extractors."}, {"title": "II. GPU MEMORY ALLOCATION ANALYSIS IN SPEAKER MODELING TRAINING PHASE", "content": "In this section, we first review the computational process of the back-propagation algorithm used in modern neural network training. Then, we analyze the GPU memory allocation of speaker verification systems based on ResNet and DF-ResNet during the training phase.\nBack-propagation Algorithm\nThe back-propagation algorithm [34], often referred to as reverse mode automatic differentiation, utilizes the chain rule to compute the gradient of neural network parameters with respect to a given loss function by traversing the computational graph in reverse. This algorithm is the standard choice for"}, {"title": "A.", "content": "ai+1 = vi(a\u00b2)     (1)\nLoss calculation: The loss $L$ is computed based on the neural network\u2019s predictions and the true labels, and it is a function of the trainable weights throughout all the layers of the network.\nBack-propagation: In this step, the weights $w_i$ are updated by computing the gradient of the loss with respect to each weight, followed by the next iteration of calculations. The back-propagation algorithm applies the chain rule to compute the total derivative $\\frac{dL}{dv_i}$ for each node $v_i$ in reverse topological order.\n$\\frac{dL}{dv_i} = \\sum_{j \\in Child(i)} (\\frac{df_j}{dv_i})^T \\frac{dL}{da_j}$    (2)\nwhere $Child(i)$ represents the child node of $v_i$ in the computational graph $G$. $\\frac{df_j}{dv_i}$ means the Jacobian Matrix, which is used to calculate the partial derivative of $f_j$ with respect to node $v_i$.\nParameter update: The learnable parameters $w_i$ are updated using the total derivative $\\frac{dL}{dv_i}$ for each node obtained in the previous step. This process is repeated until the neural network training converges.\n$w_i \\leftarrow w_i - \\eta \\frac{dL}{dv_i}$    (3)\nwhere \u03b7 is the learning rate."}, {"title": "B. GPU Memory Allocation Analysis", "content": "During training, there is a trade-off between storage space and training time. The prevailing approach is to reduce training time at the expense of storage. In forward propagation, the activations of each node are stored for later use in gradient calculations in back-propagation pass, thus accelerating the training process. However, this method increases GPU memory consumption, with memory usage growing linearly as network depth increases. Given the limited GPU memory"}, {"title": "III. REVERSIBLE NEURAL NETWORKS FOR DEEP SPEAKER EMBEDDING LEARNING", "content": "In this section, we aim to optimize GPU memory usage for activations by designing reversible neural networks. We first introduce a reversible computation principle for speaker verification systems. Then, we apply it to ResNets and DF-ResNets, building two new families of reversible neural network models: RevNets and DF-RevNets. These models can be trained without storing activations during back-propagation, which significantly reduces the GPU memory usage. The details are provided below."}, {"title": "A. Reversible Operator", "content": "A reversible operator refers to a transformation that is analytically invertible, meaning the original input can be recovered from the output using its inverse function. Traditional neural networks include irreversible operators, making the entire network non-reversible. For example, the nonlinear ReLU function cannot infer the input from its output. As a result, activations for each computing node must be stored in the back-propagation algorithm. On the contrary, networks built with reversible operators avoid the necessity of caching intermediate activations in GPU memory during the training process. They can recalculate the input from the output during back-propagation and use the recovered inputs to compute gradients for parameter updates. Consequently, reversible neural networks effectively decouple GPU memory costs from network depth, significantly reducing memory requirements when training large-scale networks. The trade-off of this method is that it sacrifices time for space, as the extra step of recovering the input increases the overall training time.\nIn the following, we will design reversible operators and neural networks for speaker verification systems based on ResNets and DF-ResNets to optimize GPU memory usage for activations."}, {"title": "B. RevNets", "content": "In this part, we will first construct reversible neural networks for SV systems based on ResNets, referred to as RevNets. These networks can be divided into two categories based on their degree of reversibility: Type I RevNets and Type II RevNets. The specific content will be presented below.\n1) Type I RevNets: Firstly, we provide the details of Type I RevNets. In the original ResNet, He et al. [26] propose two distinct types of residual blocks: Basic Block and Bottleneck Block. These blocks serve as fundamental components for constructing the network, each comprising a standard convolution operation, batch normalization (BN), and a ReLU activation function. The basic block includes two standard convolution operations, both using a 3\u00d73 kernel size, while the bottleneck block consists of three convolutions: a 1 \u00d7 1 convolution, followed by a 3\u00d73 convolution, and ending with another 1\u00d71 convolution. The residual block computation can be expressed as:\n$y = x + F(x)$    (4)\nwhere x is the activations of the previous layer, $F$ represents the residual function, which can be either a basic or bottleneck block, and $y$ denotes the activations of the current layer.\nThe residual function of the basic block, namely $F_{basic}$, is defined as:\n$F_{basic} = Conv(ReLU(BN(Conv(x))))$    (5)\nThe residual function of the bottleneck block, namely $F_{bottleneck}$, is defined as:\n$F_{bottleneck} = Conv(Conv(ReLU(BN(Conv(x)))))$   (6)\nThe primary challenge in constructing a reversible version of ResNet lies in how to convert the non-reversible residual block into a reversible function. Inspired by [43], we first split the input activation x evenly into $x_1$ and $x_2$ along the channel dimension. Using the additive coupling mechanism,"}, {"title": "Algorithm 1 Back-propagation for Reversible Blocks", "content": "the original residual block is transformed into a reversible function. The calculation process is as follows:\n$y_1 = x_1 + F(x_2)$\n$y_2 = x_2 + G(y_1)$   (7)\nwhere $x_1$ and $x_2$ are the split activations, $y_1$ and $y_2$ are the corresponding outputs. $F$ and $G$ represent the residual functions mentioned above, which can either be a basic or a bottleneck block.\nDuring the back-propagation, the input activations $x_1$ and $x_2$ can be recovered from the outputs $y_1$ and $y_2$ using the reversible function. The formula is as follows:\n$z_1 \\leftarrow y_1$\n$x_2 = y_2 - G(z_1)$\n$x_1 = z_1 - F(x_2)$    (8)\nReversibility is maintained only when the convolution operations in the residual blocks $F$ and $G$ have a stride of 1. If the stride exceeds 1, the layer\u2019s computation loses information, making it irreversible. In the original ResNet, spatial downsampling is performed at the beginning of each computational phase, typically using convolution operations with a stride of 2. As a result, the described method can not make these downsampled layers reversible. Therefore, we preserve these irreversible downsampled layers, creating Type I RevNets with partially non-reversible layers.\nFor reversible blocks, back-propagation can be performed without the need to store intermediate activations. Given activations $y_1$ and $y_2$ along with their associated total derivatives, the input activations $x_1$ and $x_2$ can be recovered first. Then, the total derivatives concerning $x_1$, $x_2$, and the parameters involved in $F$ and $G$ are computed through Alg. 1. For non-reversible layers, activations must be stored explicitly to calculate gradients. However, since there are only a few such layers, the overall memory usage is significantly reduced and remains constant regardless of network depth. As a result, the storage cost of activations is independent of depth. Notably, Alg. 1 can be applied to any residual function for $F$ and $G$. Although the algorithm requires some manual steps during back-propagation, different residual functions can be directly substituted without modifying the implementation."}, {"title": "2) Type II RevNets", "content": "As mentioned earlier, traditional spatial downsampling methods, like convolutions with a stride greater than 1 or max pooling, are inherently irreversible because they alter the spatial dimensions of input activations, leading to information loss. Consequently, in Type I RevNets, the downsampled activations must still be cached in the GPU during the forward pass. To further increase the level of reversibility, we introduce Type II RevNets by making the downsampling operations invertible.\nInspired by a pixel compression operation for image data, this method first divides the image into sub-rectangular blocks of shape $C \\times 2 \\times 2$ per channel and then reshapes these blocks into a $4C \\times 1 \\times 1$ format. This operation compresses a tensor of shape $C \\times H \\times W$ into $4C \\times H/2 \\times W/2$, effectively reducing the spatial dimensions while increasing the number of channels in a reversible manner. We extend this concept to audio data by rearranging the original speech features of shape $C \\times F \\times T$ (where $C$, $F$, and $T$ represent the channel, frequency, and time dimensions) using a ratio $r$.\n$C \\times F \\times T \\rightarrow r^2C \\times F/r \\times T/r$   (9)\nThis operation achieves downsampling while maintaining the total number of elements in the feature tensor and ensuring reversibility. By replacing the non-reversible downsampling layer in Type I RevNets with this reversible compression operation, we obtain Type II RevNets. The reversible variants share the same structure as, differing only in the number of blocks. The detailed configurations are provided below."}, {"title": "C. DF-RevNets", "content": "In this part, we extend the above concept to DF-ResNets, with their reversible variants named as DF-RevNets. Similar to the previous RevNets, these networks are categorized into two types: Type I DF-RevNets and Type II DF-RevNets.\n1) Type I DF-RevNets: Unlike ResNets, DF-ResNets only contain one type of residual block, known as the DF-Bottleneck. This block is composed of three convolutional layers: a $1 \\times 1$ convolution, followed by a $3 \\times 3$ depth-wise convolution, and another $1 \\times 1$ convolution. The residual function $F_{df-bottleneck}$ is defined as follows:\n$F_{df-bottleneck} = Conv(DConv(ReLU(BN(Conv(x)))))$   (10)\nAs previously mentioned, the reversible operation is independent of the specific form of the residual function, allowing the DF-Bottleneck to be converted into a reversible block using and . Meanwhile, we retain the irreversible downsampling layer and construct Type I DF-RevNets for. The reversible versions of follow the same architecture as, only with a different number of blocks. Detailed configurations are provided below."}, {"title": "IV. OPTIMIZER STATE QUANTIZATION FOR DEEP SPEAKER EMBEDDING LEARNING", "content": "In this section, we focus on reducing GPU memory consumption for optimizer states by introducing a dynamic quantization method. We begin by analyzing the state storage requirements of commonly used optimizers like SGD and Adam. Next, we propose an 8-bit dynamic quantization algorithm for optimizer states, which significantly reduces GPU memory usage without impacting the precision of weight updates. The details are explained below."}, {"title": "A. Optimizer State Analysis", "content": "As discussed in Section II-A, during back-propagation, the optimizer uses the gradient $g_t = \\frac{dL}{dw}$ at time step $t$ to update the neural network parameters $w$. Additionally, stateful optimizers maintain gradient statistics for each weight over time to enhance training stability and accelerate convergence. For speaker verification systems, the most popular stateful optimizers are SGD with momentum and Adam, along with its variant AdamW. In this part, we first examine the update rules and state storage of these optimizers. For SGD, the update process is as follows:\n$SGD(g_t, w_{t-1}, m_{t-1}) =\\begin{cases}  m_0 = g_0 \\\\ m_t = \\beta m_{t-1} + g_t\\\\ w_t = w_{t-1} - \\alpha m_t \\end{cases}$   (11)\nwhere $w_{t-1}$ denotes the weight at time step $t - 1$, $m_{t-1}$ represents the optimizer state at time step $t - 1$, $g_t$ is the gradient at time step $t$, \u03b1 is the learning rate, and \u03b2 is the momentum coefficient. Hence, the SGD optimizer with momentum requires storing both the gradient $g_t$ and the state $m_t$, which are typically represented as 32-bit floating-point numbers by default.\nFor Adam and its variant AdamW, the calculation process is as follows:\n$Adam(g_t, w_{t-1}, m_{t-1}, r_{t-1}) =\\begin{cases}  r_0,m_0 = 0 \\\\ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\ r_t = \\beta_2 r_{t-1} + (1 - \\beta_2) g_t^2\\\\ w_t = w_{t-1} - \\alpha \\frac{m_t}{\\sqrt{r_t} + \\epsilon} \\end{cases}$  (12)\nwhere $m_{t-1}$ represents the first-order optimizer state at time step $t - 1$, and $r_{t-1}$ is the second-order optimizer state. The coefficients $\u03b2_1$ and $\u03b2_2$ correspond to the statistics of state 1 and state 2, respectively. As a result, the Adam optimizer needs to store gradient $g_t$ along with two gradient statistics, $m_t$ $r_t$."}, {"title": "B. 8-bit Dynamic Quantization", "content": "To reduce the GPU memory usage of the optimizer state, we introduce a dynamic quantization algorithm, which can convert the original 32-bit floating-point numbers into 8-bit integers for storage and dequantize them back to 32-bit values during parameter updates. This approach effectively decreases GPU memory consumption for the optimizer state without affecting model performance. The algorithm is detailed as follows:\n1) Tensor chunking: To mitigate the impact of outliers on quantization precision, we first partition the optimizer state tensor into smaller blocks. Specifically, the tensor is divided into blocks of size $B$, with quantization applied independently to each block. In this process, the state tensor $T$ is flattened into a one-dimensional sequence and then segmented into $B$ blocks. For a state tensor $T$ with $n$ elements, this results in $n/B$ blocks. In the experiment, we set $B$ to 2048.\n2) Dynamic quantization: Next, we perform 8-bit dynamic quantization independently within each block. The quantization process maps n-bit integers to a real-valued domain $D$, expressed as Qmap: $[0,2^{n-1}] \\rightarrow D$. We denote this mapping"}, {"title": "V. EXPERIMENTAL SETUPS", "content": "Datasets and Data Augmentation\nTo examine the performance of our proposed approaches, we employ the Voxceleb1 and Voxceleb2 benchmark datasets for speaker verification tasks. These datasets consist of audio segments extracted from interviews with over 6,000 celebrities on YouTube. For training, all models utilize the Voxceleb2 development set, comprising roughly 2,200 hours of speech data, which includes around 1 million utterances from 5,994 speakers. Testing is performed using the official Voxceleb evaluation trials: Vox1-O, Vox1-E, and Vox1-H. Additionally, to generate more diverse training samples, we hypothesize that changes in speech speed can result in new speakers [47]. By adjusting the speed to 0.9x or 1.1x, we effectively triple the number of speakers in the dataset. Furthermore, we apply online data augmentation [48] by randomly adding background noise from the MUSAN [49] and reverberation from the RIR [50] to the training speech, with a probability of 0.6.\nSystem Description\nIn this paper, we utilize ResNets and DF-ResNets as speaker embedding extractors, which are widely adopted networks and deliver state-of-the-art performance. The specific details as outlined below:\nResNet [7]: This is a popular and robust model in the speaker verification field. We use ResNet34, ResNet101, and ResNet152 as the baseline systems in our experiments.\nDF-ResNet [14]: It is an improved version of ResNet. We include DF-ResNet56, DF-ResNet110, DF-ResNet179, and DF-ResNet233 as base models.\nTraining Settings\nDuring training, we randomly select a 200-frame chunk from each training speech sample and extract 80-dimensional FBANK features as input, using a 25-millisecond window length and a 10-millisecond frame shift. The loss function employs AAM-Softmax [51], with a margin of 0.2 and a scale of 32. We set the speaker embedding dimension to 256. For the ResNet baseline systems, we apply the SGD optimizer, setting the momentum to 0.9 and the weight decay to le-4. For DF-ResNet models, the AdamW optimizer is utilized with a weight decay of 0.05. The same training configuration is adopted for both the RevNets and DF-RevNets. In the optimizer quantization experiments, 8-bit versions of the SGD and AdamW optimizers are used.\nEvaluation Metrics\nFor evaluation, the similarity among speaker embeddings is computed using cosine distance. Furthermore, we apply adaptive score normalization (AS-Norm) [52] to calibrate score distribution, utilizing an imposter cohort size of 600. The system performance is assessed with the equal error rate (EER)."}, {"title": "VI. RESULTS AND ANALYSIS", "content": "This section evaluates and analyzes the results of the reversible neural network and optimizer dynamic quantization algorithm introduced in Section III and IV. First, Section VI-A assesses the performance of the two types of reversible neural networks based on ResNets and DF-ResNets. Subsequently, Section VI-B presents the outcomes of the 8-bit dynamic quantization algorithm applied to the optimizers. Finally, Section VI-C provides a detailed analysis of the relationship between GPU memory usage and the number of parameters."}, {"title": "A. Evaluation on Reversible Neural Networks", "content": "We firstly evaluate the performance of the Type I and Type II reversible neural networks designed for ResNets and DF-ResNets, as introduced in Section III. The detailed results are presented below.\n1) Model Configuration: In the experiments, we use the original ResNets (ResNet34, 101, and 152) and DF-ResNets (DF-ResNet56, 110, 179, and 233) as baseline systems. As described in Section III, to ensure a fair comparison, we design two types of reversible neural networks for each baseline model while keeping the number of parameters similar. For example, as shown in Table V, we introduce the Type I reversible neural network RevNet46 and the Type II reversible neural network RevNet57 based on the Basic block, both of which have parameter counts similar to ResNet34. Similarly, four different reversible architectures are developed for ResNet101 and ResNet152, with comparable or fewer parameters. For DF-ResNets, we develop two reversible architectures for each baseline model, ensuring their parameters are comparable or lower, as Table VI illustrates.\n2) Performance Comparison: From Table V, it is evident that our proposed reversible architectures exhibit comparable performance to the original ResNets across various parameter ranges (6M ~ 20M). Specifically, RevNet46 and RevNet57 achieve similar EER to ResNet34 on the Vox1-O, E, and H. Interestingly, reversible neural network variants for ResNet101 and 152 can be constructed using the Basic block. For example, RevNet126 and RevNet137, both based on the Basic block, deliver performance comparable to ResNet101, which utilizes the Bottleneck block. This highlights the strong representational capability of the Basic block in reversible networks. Additionally, compared to RevNets built with the Bottleneck block, those based on the Basic block can achieve similar performance with fewer parameters. A similar trend is observed in DF-ResNets. As shown in Table VI, two types of reversible versions demonstrate comparable performance to the original baseline models while using equivalent or fewer parameters. These findings reveal the potential of reversible neural networks in maintaining strong speaker representation capabilities, confirming the effectiveness of the reversible operator.\n3) Memory Savings: To evaluate the memory consumption of each model, we measure the memory usage and maximum batch size using a 2-second audio clip on a single 11 GB 2080Ti GPU. From Table V, we can see that the two proposed reversible networks significantly reduce memory usage across various configurations. For example, the reversible versions of ResNet34, namely RevNet46 and RevNet57, reduce memory usage by approximately half. Similarly, the reversible variants of ResNet101 and ResNet152, based on both Basic and Bottleneck architectures, also show significant memory reductions. Notably, the RevNets built on the Basic block achieve even greater savings, with a maximum memory reduction of 15.7 times. This reduction stems from the ability of reversible networks to avoid storing intermediate activations during back-propagation. In addition, Type II reversible networks perform better than Type I in memory savings, as Type II replaces the irreversible downsampling layer with a reversible operation. Similarly, for DF-ResNets, both reversible neural network"}, {"title": "B. Evaluation on Optimizer State Quantization", "content": "In this part, we provide the results of the 8-bit dynamic quantization algorithm for optimizer states proposed in Section IV. The details are presented below.\nWe first introduce the experimental setups. ResNets and RevNets are trained using the SGD optimizer with momentum, while DF-ResNets and DF-RevNets are trained with the AdamW optimizer. As described in Section IV, the proposed dynamic quantization algorithm is applied to both SGD and AdamW, yielding their respective 8-bit versions. To evaluate the performance of these optimizers, they are directly adopted during the training of reversible neural networks. Specifically, RevNets are trained using the 8-bit version of SGD, while DF-RevNets are trained with the 8-bit version of AdamW.\nTable V and VI clearly demonstrate that the 8-bit version of the optimizer reduces memory usage during training while maintaining model performance. For example, in RevNet46 and RevNet57, replacing the original 32-bit SGD optimizer with its 8-bit variant decreases memory usage per training sample from 0.04GB to 0.039GB and from 0.03GB to 0.029GB, respectively, and increases the maximum batch size by 1 with no performance loss. Similar results are observed for ResNet101 and ResNet152, where the 8-bit SGD optimizer improves the memory compression ratio of RevNet137 from 11 to 11.3 and increases the maximum batch size by 2. Likewise, in RevNet197, the compression ratio improves from 15.7 to 16.2, and the batch size increases by 2 without model performance loss.\nSimilar conclusions can be drawn from the 8-bit AdamW optimizer applied to DF-RevNets. For example, the maximum batch size for DF-RevNet66 increases by 1, and the memory compression ratio rises from 4.44 to 4.49. In DF-RevNet377, the compression ratio increases from 14.2 to 14.4, with no impact on performance. These results confirm that the 8-bit optimizer performs on par with its 32-bit counterpart and demonstrate the effectiveness of the dynamic quantization algorithm."}, {"title": "C. Memory Usage Analysis", "content": "In this part", "Params": "As shown in Fig. 5, the memory usage of the original ResNets and DF-ResNets increases linearly with network depth, which is consistent with the theoretical analysis of the back-propagation algorithm in Section II-A. This occurs because intermediate activations need to be cached during back-propagation, and their storage grows proportionally to the number of network layers. In contrast, both types of our RevNets and DF-RevNets maintain nearly constant memory consumption regardless"}]}