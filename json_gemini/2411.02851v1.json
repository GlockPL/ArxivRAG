{"title": "Learning to Unify Audio, Visual and Text for Audio-Enhanced Multilingual Visual Answer Localization", "authors": ["Zhibin Wen", "Bin Li"], "abstract": "The goal of Multilingual Visual Answer Localization (MVAL) is to locate a video segment that answers a given multilingual question. Existing methods either focus solely on visual modality or integrate visual and subtitle modalities. However, these methods neglect the audio modality in videos, consequently leading to incomplete input information and poor performance in the MVAL task. In this paper, we propose a unified Audio-Visual-Textual Span Localization (AVTSL) method that incorporates audio modality to augment both visual and textual representations for the MVAL task. Specifically, we integrate features from three modalities and develop three predictors, each tailored to the unique contributions of the fused modalities: an audio-visual predictor, a visual predictor, and a textual predictor. Each predictor generates predictions based on its respective modality. To maintain consistency across the predicted results, we introduce an Audio-Visual-Textual Consistency module. This module utilizes a Dynamic Triangular Loss (DTL) function, allowing each modality's predictor to dynamically learn from the others. This collaborative learning ensures that the model generates consistent and comprehensive answers. Extensive experiments show that our proposed method outperforms several state-of-the-art (SOTA) methods, which demonstrates the effectiveness of the audio modality.", "sections": [{"title": "1 Introduction", "content": "With the rapid expansion of the internet, an increasing number of users are turning to online platforms to seek medical advice by posing natural language questions (O'Donnell et al., 2023; Lim et al., 2022). Current online platforms typically fall into two categories: those that provide textual answers, which may be difficult for users to interpret, and those that offer visual answers, which are generally more intuitive and easier to follow (Tang et al., 2021b). However, the retrieved videos often contain substantial amounts of information irrelevant to the user's query (Moon et al., 2023), which significantly hinders the efficiency of information retrieval (Zhang et al., 2023). In response to this challenge, the task of Visual Answer Localization (VAL) has been introduced (Weng and Li, 2023). Existing VAL approaches can be broadly categorized into visual-based (Tang et al., 2021a; Chen et al., 2020a) and textual-based methods (Li et al., 2023a; Weng and Li, 2023; Li et al., 2024b). Visual-based methods are effective in scenarios where subtitle text is sparse, but their performance tends to degrade significantly in other contexts. In contrast, textual-based methods excel when abundant subtitle text is available, as the semantic similarity between the question and subtitle is typically greater than between the question and the video (Li et al., 2024b). However, these methods often overlook audio, which plays a crucial role in complementing both visual and textual modalities. There is inherent consistency and complementarity among these modalities (Chen et al., 2023a), and harnessing this synergy can enhance both visual and textual modalities by integrating information from the audio. Incorporating audio thus addresses the performance limitations in VAL, particularly in video segments lacking subtitles (Liu et al., 2022; Chen et al., 2020b; Sun et al., 2024). To this end, we study the Audio-enhanced Multilingual Visual Answer Localization (AMVAL) which aims to locate video segments that answer a user's natural language question, in either Chinese or English. By providing video segments with verbal explanations for medical guidance, this approach not only facilitates the learning of specific actions but also helps bridge language barriers, making the content accessible to people who speak different languages (Macedonia and Kn\u00f6sche, 2011; Diamond et al., 2020). However, a significant challenge lies in effectively integrating the three modalities and fully utilizing their individual strengths to tackle the AMVAL task. To address this challenge, we propose a unified Audio-Visual-Textual Span Localization (AVTSL) method for AMVAL, aimed at reducing cross-modal discrepancies and improving the accuracy of span localization by integrating audio modality. We designed a network architecture with three modality channels (audio, visual, and textual) to fully leverage the semantic information from each modality in the video, addressing the limitations of single-modality methods. Each channel is equipped with a corresponding predictor: an audio-visual predictor, a visual predictor, and a textual predictor. During joint training, distinct objectives are assigned to each predictor, enabling them to leverage the unique strengths of their respective modalities. To improve modality integration, we introduce an Audio-Visual-Textual consistency module, which employs a Dynamic Triangular Loss (DTL) function based on Intersection over Union (IoU). This loss function aligns the modalities by minimizing the discrepancies between each predictor's output and the target answer, as well as between the outputs of the other two predictors. Our approach promotes mutual learning among the predictors to achieve consistent and cohesive multimodal representations. Our contributions are as follows: (1) We study the AMVAL and propose the AVTSL method, which is the first to introduce the audio modality for the AMVAL; (2) We designed an Audio-Visual-Text Consistency module, which leverages the consistency and complementarity between different modalities using the DTL loss function; (3) We conducted extensive experiments to demonstrate the effectiveness of the AVTSL method, where our method outperformed other state-of-the-art (SOTA) methods by incorporating the audio modality."}, {"title": "2 Related Work", "content": "Video Understanding. Video understanding is a challenging task that requires comprehension of complex semantic information (Diba et al., 2019). Audio is a typical modality in video understanding tasks that has been studied for many years. Some datasets (Chen et al., 2023b; Yang et al., 2022) focus on the incorporation of the audio modality. A recent work (Li et al., 2023b) uses an audio-guided visual attention module to link audio with relevant spatial regions. (Wang et al., 2023b) propose a multimodal knowledge graph that integrates text, image, video, and audio to support diverse downstream tasks. (Li et al., 2024c) focus on aligning audio and visual modalities on key question words and employ a modality-conditioned module to emphasize relevant audio segments or visual objects. (Chen et al., 2023a) introduce a novel curriculum-based denoising strategy that adaptively assesses sample difficulty to gauge noise intensity in a self-aware manner. (Ibrahimi et al., 2023) uses two independent cross-modal attention blocks to allow the text to separately focus on audio and video representations. The audio modality has been widely utilized in various video understanding tasks, such as video question answering and temporal sentence grounding in video, with its effectiveness well demonstrated. Inspired by these works, our work is the first to incorporate the audio modality into the AMVAL task.\nMultimodal Fusion. Recent works (Chen and Jiang, 2019; Liu et al., 2022) suggest that leveraging motion and audio can improve performance, but they fall short by merely concatenating features from multiple modalities without considering their relative importance or interactions, leaving their full potential untapped. A few studies (Hori et al., 2017; Jin et al., 2019) assign importance weights to individual modalities using cross-modal attention in the encoder, but they still lack explicit handling of modality interactions. (Lan et al., 2023) proposes a deep module for learning shared information to enhance representation in multimodal sentiment analysis. (Wang et al., 2023a) introduces a transformer-based multimodal encoding-decoding network to address the challenges of multimodal sentiment analysis, focusing on the influence of individual modal data and the low quality of non-verbal features. (Chen et al., 2020b) proposes the Pairwise Modality Interaction (PMI) method that uniquely captures pairwise modality interactions at both sequence and channel levels. This method preserves the temporal dimension and fuses interaction results with importance weights for added explainability. Inspired by these modality fusion methods, we designed three multimodal information fusion channels: audio-textual, visual-textual, and audio-visual-textual."}, {"title": "3 Task Definition", "content": "Given an untrimmed video V and a text question $Q = \\{q_m\\}_{m=1}^{L_q}$, where Q can be in either Chinese or English and $L_q$ is the length of the question. $V = \\{F^v, F^a\\}$ is representing the visual modality $F^v = \\{f_i\\}_{i=1}^{T_v}$ and audio modality $F^a = \\{f_i\\}_{i=1}^{T_a}$ in the input video V, where $T_v$ and $T_a$ are the number of frames and duration in seconds of video V. Corresponding subtitle text $S = \\{F^s\\}$ can be extracted from the audio modality, where $F^s = \\{f_i\\}_{i=1}^{L_s}$ denotes subtitle of each video, $L_s$ is the subtitle span length. The goal of the AMVAL task is to predict the start and end timestamps of the video segment that answers the given question Q, whose ground truth annotations are $y = (y^s, y^e)$."}, {"title": "4 Proposed Method", "content": "Figure 2 gives an overview of our proposed AVTSL, which is composed of three key components: Feature Extraction, Multimodal Fusion, and Audio-Visual-Textual Consistency."}, {"title": "4.1 Feature Extraction", "content": "For each video V, we extract visual frames (16 frames per second) and then obtain the corresponding RGB visual features $F^v = \\{f_k\\}_{k=1}^{V_n} \\in \\mathbb{R}^{V_n \\times d_v}$ using Inflated 3D ConvNet (I3D) pre-trained on the Kinetics dataset (Carreira and Zisserman, 2017), where $V_n$ is the number of extracted features and $d_v$ is the dimension of the visual features. We employ the wav2vec pre-trained model (Baevski et al., 2020) with audio projection to extract audio features $F^a = \\{a_i\\}_{i=1}^{A_n} \\in \\mathbb{R}^{A_n \\times d_a}$ from the video V, where $A_n$ is the number of extracted features and $d_a$ is the dimension of the audio features. For the text component, we first extract the raw subtitle text $S = \\{s_q\\}_{q=1}^{L_s}$ from the audio using the general-purpose speech recognition model (Whisper) (Radford et al., 2023), where n is the subtitle span length. Next, we concatenate the text question Q with the video's raw subtitle text S to form $T' = [Q, s_i], i \\in [1,n]$. Finally, after tokenizing the concatenated text T', we use the DeBERTa pre-trained model with textual projection to obtain the textual features $T = \\{t_i\\}_{i=1}^{T_n} \\in \\mathbb{R}^{T_n \\times d_t}$, where the $T_n$ is the length of tokenized concatenate text and the $d_t$ is the dimension of textual features."}, {"title": "4.2 Multimodal Fusion", "content": "Textual-Visual/Audio Interactor. After extracting visual ($F^v$), audio ($F^a$), and textual T features, we employ a context query attention(CQA) model, inspired by the work (Zhang et al., 2020), to capture interactions between the textual, visual and audio modalities. As shown in Figure 2, we employed two modality interactors (Textual-Visual/Audio Interactor) with same architectures, capable of facilitating interactions between feature sequences from two modalities, denoted by T and $F^m$, where $m \\in \\{v, a\\}$.\nModality interaction is divided into two parts: (1) context-to-query and query-to-context processes; (2) context query concatenation module. First, we calculate two attention weights as:\n$S^{c2q} = S_T \\cdot T, S^{q2c} = S_c \\cdot S^T \\cdot F^m$\\ \nwhere $S^{c2q} \\in \\mathbb{R}^{T_m \\times d_t}$ and $S^{q2c} \\in \\mathbb{R}^{T_m \\times (d_m)}$ denotes context-to-query and query-to-context processes. $S_T \\in \\mathbb{R}^{T_m \\times T_n}$ and $S_c \\in \\mathbb{R}^{T_m \\times T_n}$ denotes the row-wise and column-wise normalization of S by SoftMax. And then we obtain the output of CQA module as:\n$F^{m'} = FFN([F^m; S^{c2q}; F^m \\odot S^{c2q}; F^m S^{q2c}])$\nwhere FFN is a feed-forward network; $\\odot$ is Hadamard product.\nSecond, we capture deeper semantic interactions between textual and visual/audio modalities through the context query concatenation module, which is computed as:\n$F^{m''} = Conv1d(Concat[attn(F^{m'},T); T])$\nwhere attn denotes the attention layer. Finally, we obtain the Textual-Visual/Audio fused features that emphasizes the audio and visual segments that are semantically relevant to the text query.\nAudio-Visual-Textual Interaction. As shown in Figure 2, after obtaining the feature vectors $F^{v''}$ and $F^{a''}$, we further apply a multi-head attention mechanism (Chen et al., 2023a) to derive the fused representation of the three modalities. First, we use the text features to query the relevant visual and audio features, then apply a residual connection with the text features.\n$F^{T1} = MHA(q = T, k = F^{v''},v = F^{v''})$\n$F^{T2} = MHA(q = T, k = F^{a''}, v = F^{a''})$\n$T' = F^{T1} + F^{T2} + T$\nwhere MHA is multi-head attention layer.\nNext, we propagate T' to the audio and visual features, using visual/audio features as the query and added the outputs of mutil-head attention module:\n$F^{v'} = MHA(F^{v''},T',T') + F^{v''}$\n$F^{a'} = MHA(F^{a''},T',T') + F^{v''}$\n$F^{av} = F^{v'} + F^{a'}$\nFinally, we obtain the Auido-Enhanced Visual features $F^{av}$. For the textual features, we embed the max-pooled Auido-Enhanced Visual features $F^{av}$ into each token of textual features $F$ based on broadcast mechanism:\n$\\hat{F^{av}} = MaxPool(F^{av})$\n$\\hat{T} = \\{\\hat{F^{av}} + T_i\\}_{i=1}^{T_n}$"}, {"title": "4.3 Audio-Visual-Textual Consistency", "content": "Three Predictors. As shown in Figure 3, we designed a tri-modal predictor architecture corresponding to three types of modality interaction data features: audio-visual, visual, and textual. The audio-visual and visual span predictors use the same structure, both using two unidirectional LSTM networks(LSTM, and LSTMe) and two FFN layers(FFNs and FFNe) to predict the start and end time point logits of the answer segment.\n$AV^{Log}_s = FFN_{AV}(LSTM_s(F^{av}))$ \n$AV^{Log}_e = FFN_{AV}(LSTM_e(F^{av}))$\n$V^{Log}_s = FFN_{V}(LSTM_s(F^{v''}))$\n$V^{Log}_e = FFN_{V}(LSTM_e(F^{v''}))$\nThe textual predictor differs from the audio-visual and visual span predictors described above, utilizing only two FFN layers(FFNs and FFNe) to predict the start and end span of the subtitle segments corresponding to the answer.\n$T^{Log}_s = FFN_{T}(\\hat{T})$\n$T^{Log}_e = FFN_{T}(\\hat{T})$\nOne of the training objectives of the audio-visual/visual predictor is to predict the start and end timestamps of the answer video segment. Since the answer video segment may contain parts without subtitles, the start and end positions of the subtitle segments predicted by the textual predictor may not align with the timestamps of the answer video segment. Therefore, we use a Time-Span Mapping Table TSM to convert this:\n$\\hat{T^M_s} = Argmin(M_t \u2013 TSM(T_i))$\n$\\hat{M^t_e} = Argmin(T_t \u2013 TSM(M_i))$\nwhere $M \\in (V, AV)$ denotes two types of modality: visual and audio-visual. $t \\in (s, e)$ denotes start and end point, TSM stores a one-to-one mapping between video timestamps and subtitle spans.\nDynamic Triangular Loss (DTL) Function. To maintain consistency among the three modalities and leverage the strengths of each, we train the three predictors jointly. We define two types of training objectives: the first is to ensure that each predictor's output closely matches the ground truth; the second is to ensure that each predictor's output remains consistent with the other two predictors, allowing them to benefit from the strengths of the other modalities to compensate for their own weaknesses. Therefore, we set the predictions of each predictor as pseudo-labels for the other two predictors.\nFirst, we calculate the loss for each of the three predictors based on the labeled answer segments.\n$\\mathcal{L}^{AV}_Y = CE(AV^{Log}_s, AV_s) + CE(AV^{Log}_e, AV_e)$\n$L^V_Y = CE(V^{Log}_s, V_s) + CE(V^{Log}_e, V_e)$\n$L^T_Y = CE(T^{Log}_s, T_s) + CE(T^{Log}_e, T_e)$\nwhere CE() denotes Cross-Entropy function. [AVs, AVe], [Vs, Ve] denotes the start and end timestamps of the target answer segment and $AV_s = V_s = y^s$, $AV_e = V_e = y^e$. Ts and Te are obtained by converting Vs and Ve using the Time-Span Mapping Table TSM.\nSecondly, we calculate the loss between audio-visual predictor and the outputs of the other two predictors:\n$\\mathcal{L}^{AV-T}_Y = CE(AV^{Log}_s, sg(\\hat{V_s})) + CE(AV^{Log}_e, sg(\\hat{V_e}}))$\n$\\mathcal{L}^{AV-V}_Y = CE(AV^{Log}_s, sg(V^{Log}_s)) + CE(AV^{Log}_e, sg(V^{Log}_e))$\nSimilarly, we can then compute the loss between the visual predictor and the outputs of the other two predictors:\n$\\mathcal{L}^{V-T}_Y = CE(V^{Log}_s, sg(\\hat{V_s})) + CE(V^{Log}_e, sg(\\hat{V_e}}))$\n$\\mathcal{L}^{V-AV}_Y = CE(V^{Log}_s, sg(AV^{Log}_s)) + CE(V^{Log}_e, sg(AV^{Log}_e))$\nThe loss between the textual predictor and the outputs of the other two predictors:\n$\\mathcal{L}^{T-AV}_Y = CE(T^{Log}_s, sg(T_{AV})) + CE(T^{Log}_e, sg(T_{AV}))$\n$\\mathcal{L}^{T-V}_Y = CE(T^{Log}_s, sg(T_{V})) + CE(T^{Log}_e, sg(T_{V}))$\nAt the early stages of training, the output differences between predictors are large and unstable. To enhance training stability, the weight of the second part of the loss (consistency between predictors) should be reduced, with more focus on the difference between each predictor and the ground truth. Therefore, we adjust the weight of the second part of the loss based on the intersection-over-union (IoU) (Rodriguez et al., 2020) of the outputs from different modality predictors:\n$\\lambda^{M-N} = IOU([M_s, M_e], [N_s, N_e])$\nwhere M denotes the output of the M modality predictor, and $\\hat{M_s}$ represents the output of the N modality predictor after being converted by the Time-Span Mapping Table TSM. After this, we can compute the second part of loss function as :\n$\\mathcal{L}_{L2} = \\sum \\lambda^{AV-N}L^{AV-N}_Y$\nwhere M, N \\in (AV, V,T) and M \\neq N denotes two different types of modalities.\nFinally, our loss function is:\n$\\mathcal{L} = L^{AV}_Y + L^V_Y + L^T_Y + L2$"}, {"title": "5 Experiments", "content": "We conducted our experiments on a public dataset for VAL: MMIVQA dataset (Li et al., 2024a). The dataset consists of complete medical instructional videos with corresponding questions (in Chinese and English) and video answer timestamps. Each video may contain multiple question-answer pairs, with identical questions linked to a single answer. The dataset is divided into 3768, 334, and 288 pairs for training, validation, and testing, respectively. Following previous work (Chen et al., 2023a; Li et al., 2024b; Weng and Li, 2023), we use \"R@n, IoU=m\" (n = 1 and \u03bc\u2208 0.3, 0.5,0.7) and \"mIOU\" as our evaluation metric. \"R@n, IoU=m\" denotes the Intersection-over-Union (IoU) of the predicted video answer span compared to the ground truth, measuring the overlap that exceeds a threshold m in the top-n retrieved moments. The \u201cmIoU\u201d is the average IoU between the predictions and the ground truth across all samples. We initialize audio features using the pre-trained wav2vec model (Baevski et al., 2020) and extract visual features with the I3D network (Carreira and Zisserman, 2017). Subtitles are generated by the Whisper model (Radford et al., 2023), while both questions and subtitles are encoded using DeBERTa-v2 (Zhang et al., 2022). The AdamW optimizer (Loshchilov et al., 2017) is used with a learning rate of 8e-6, and feature dimensions set to 1024 for all modalities. The model is trained for 15 epochs with a batch size of 1 on an NVIDIA RTX 8000 GPU, with each experiment repeated three times to reduce random errors."}, {"title": "5.2 Overall Performance", "content": "In Table 1, we compare the performance of our AVTSL model with several strong baselines: 1) Random Pick: A baseline that randomly selects the answer span; 2) Visual methods: DEBUG (Lu et al., 2019), GDP (Chen et al., 2020a), and ACRM (Tang et al., 2021a); 3) Visual-Textual Fusion methods: MutualSL (Weng and Li, 2023), FMALG (Cheng et al., 2023), OCR-LLM (Zhang et al., 2024), and VPTSL (Li et al., 2024b); 4) Audio-Visual Fusion methods: PMI-LOC (Chen et al., 2020b) and ADPN (without subtitle) (Chen et al., 2023a). 5) Audio-Visual-Textual Fusion methods: ADPN (with subtitle) (Chen et al., 2023a) and TR-DETR (Sun et al., 2024). Our method outperforms strong baselines across all metrics, as shown in Table 1. The random selection method performs the worst, demonstrating the challenge of the MMIVQA task. Visual-based methods improve significantly over random selection, indicating that visual features contain rich semantic information. Textual-based methods, however, surpass visual-based ones by nearly 100% across most metrics, particularly in IoU=0.7, where the semantic similarity between the question and subtitle proves stronger than with video content. This suggests that textual features are more effective for AMVAL. Audio-visual method improves upon visual-only methods, suggesting that incorporating the audio modality enhances visual modality. However, the gains from audio remain smaller than those from textual-based approaches, emphasizing subtitle text as the primary source of information for video answer localization. This observation is further validated by the experimental results of our AVTSL method. Compared to the visual-based GDP method, AVTSL improves by 31%, 26%, 22%, and 25% in IoU=0.3, IoU=0.5, IoU=0.7, and mIoU metrics, respectively. Against the textual-based VPTSL method, it shows gains of 7%, 7%, 9%, and 5% in the same metrics, and compared to the audio-visual PMI-LOC method, improvements are 26%, 27%, 23%, and 20%. These results demonstrate that the advantage of integrating audio, visual, and textual modalities for the AMVAL task, as their consistency enhances performance. Furthermore, incorporating all three modalities leads to superior performance compared to using any single modality, showing their complementary nature. Notably, under same video modality conditions, our AVTSL shows significantly better performance compared to the ADPN and TR-DETR. This is because ADPN and TR-DETR rely only on audio-visual or visual predictors, whereas we incorporates predictors from all three modalities, with the textual predictor playing a key role."}, {"title": "5.3 Ablation Study", "content": "We perform ablation studies to assess the contribution of each component in our AVTSL. The implementation details are as follows: 1) \"VP,\" \"AP,\" and \"TP\" denote the outputs of the visual, audio-visual, and textual predictors, respectively. 2) \"w/o DTL\" denotes to the model without DTL function. 3) \"w/o Audio\" denotes the removal of the audio modality and its associated predictor components.\nIn Table 2, the comparison between (1), (2), and (3) shows that the textual predictor performs the best when the three modality predictors operate independently, as the question is most similar to the subtitle text. Comparing (4) with (7) and (5) with (8) indicates that introducing the audio modality improves performance, as it compensates for missing subtitles in video segments. Additionally, the comparison of (1) (3) with (6) (8) demonstrates that the DTL function effectively enhances performance by allowing the modality predictors to learn from one another and mitigate their individual limitations. Finally, comparing (1), (4), and (6) shows that progressively incorporating audio and textual modalities into the visual modality leads to improved performance, reflecting the consistency and complementarity among the modalities."}, {"title": "5.4 Case Study", "content": "As shown in Figure 4, our qualitative analysis demonstrates that incorporating additional modalities enhances the input information and improves performance on the AMVAL. In case (1), where the answer includes actions like \"coughing\" and \"patting,\" the audio modality leads to better performance compared to visual-based methods. This shows that the audio modality effectively compensates for incomplete information in video segments without subtitles. In case (2), where the video answer involves multiple similar actions, such as \"wiping the patient's eyes\" and \"administering eye drops,\" the model performs poorly, as the visual and audio modalities introduce noise."}, {"title": "6 Conclusion", "content": "We proposed the unified Audio-Visual-Textual Span Localization (AVTSL) method to address the challenges in Audio-enhanced Multilingual Video Answer Localization (AMVAL). Our method introduces three predictors based on visual, audio-visual and textual representations, marking the first instance of incorporating the audio modality into the AMVAL. To bridge the gap between modalities, we designed the Audio-Visual-Textual consistency module, incorporating the Dynamic Triangular Loss (DTL) function, which facilitates mutual learning among the modality predictors and fully exploits the consistency and complementarity of multimodal information. Extensive comparative experiments with several state-of-the-art methods demonstrate that our AVTSL achieves superior performance, validating its effectiveness. In future work, we aim to develop larger multilingual medical instructional video question-answering datasets to pre-train multimodal models for AMVAL."}, {"title": "7 Limitations", "content": "Although AVTSL achieves promising improvements, it still has some limitations. The dataset is limited to medical videos and bilingual (Chinese-English) content, which may hinder generalization to other domains and languages. Additionally, our method targets a single continuous segment. However, in real-world scenarios, answers may consist of multiple disjointed segments, requiring more than one start and end timestamp. This could limit the method's ability to handle complex cases where the relevant information is scattered across different parts of the video."}]}