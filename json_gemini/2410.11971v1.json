{"title": "DDIL: IMPROVED DIFFUSION DISTILLATION WITH IMITATING LEARNING", "authors": ["Risheek Garrepalli", "Shweta Mahajan", "Munawar Hayat", "Fatih Porikli"], "abstract": "Diffusion models excel at generative modeling (e.g., text-to-image) but sampling requires multiple denoising network passes, limiting practicality. Efforts such as progressive distillation or consistency distillation have shown promise by reducing the number of passes at the expense of quality of the generated samples. In this work we identify co-variate shift as one of reason for poor performance of multi-step distilled models from compounding error at inference time. To address co-variate shift, we formulate diffusion distillation within imitation learning (DDIL) framework and enhance training distribution for distilling diffusion models on both data distribution (forward diffusion) and student induced distributions (backward diffusion). Training on data distribution helps to diversify the generations by preserving marginal data distribution and training on student distribution addresses compounding error by correcting covariate shift. In addition, we adopt reflected diffusion formulation for distillation and demonstrate improved performance, stable training across different distillation methods. We show that DDIL consistency improves on baseline algorithms of progressive distillation (PD), Latent consistency models (LCM) and Distribution Matching Distillation (DMD2).", "sections": [{"title": "INTRODUCTION", "content": "Diffusion models, while capable of producing high-quality images, suffer from slow sampling times due to their iterative denoising process. To address this, distillation techniques have been proposed to reduce number of denoising steps. These techniques can be broadly categorized into trajectory-level and distribution-matching approaches . While the former focuses on preserving the teacher's trajectory at a per-sample level, the latter matches the marginal distribution.\nMulti-step student models offer a promising approach in balancing quality and computational effi-ciency. However, they often face a critical challenge: covariate shift. This occurs when the distribution of noisy input latents that the student model encounters during training differs from that seen during inference by the student. This mismatch can significantly impact generation quality especially when the number of denoising steps are low. Recent works Kohler et al. (2024); Yin et al. (2024) only consider backward trajectories to obtain feedback on quality of generation, but these approaches are often agnostic to data distribution and can exhibit mode-collapse.\nIn this work, we identify 'covariate shift' as a critical factor that impacts the generation quality in multi-step distilled diffusion models. To address covariate shift and to preserve diversity, we introduce diffusion distillation within the imitation learning (DDIL) framework by improving the training distribution for distillation. We achieve this by incorporating both the data distribution (forward diffusion) and the student's predictive distribution (backward trajectory at inference time). This approach combines the benefits of (1) Preserving Marginal Data Distribution: Training on the data distribution ensures the student model maintains the inherent statistical properties of the original data, and (2) Correcting Covariate Shift: Training on backward trajectories enables the student model to identify and adapt to covariate shifts, thereby improving the accuracy of score"}, {"title": "RELATED WORK", "content": "Diffusion distillation methods. Progressive distillation Salimans & Ho (2021); Meng et al. (2023) and many follow up works Li et al. (2023b); Berthelot et al. (2023) try to reduce the number of iterations of student model by forcing student to mimic multiple steps of the teacher. Consistency models Song et al. (2023); Luo et al. (2023); Ren et al. (2024) assume deterministic probabilistic flow at inference and enforce consistency in the data space for step-distillation. Additionally, recent work decomposes the diffusion trajectory into multiple segments like in progressive distillation and performs distillation within consistency formulation Kim et al. (2023). Instead of using the real data, methods such as BOOT Gu et al. (2023) consider bootstrapping in the student trajectory to generate samples of high quality and diversity. Liu et al. (2023) approximates the underlying map of the pretrained diffusion model as linear paths. While above trajectory level distillation techniques like progressive distillation and consistency-based approaches improve efficiency, the quality of the generated samples exhibits low visual fidelity.\nAlternatively, diffusion distillation has been formulated in the distributional matching framework Yin et al. (2023); Luo et al. (2024); Yin et al. (2024); Salimans et al. (2024); Sauer et al. (2023; 2024). Within distribution matching approaches instead of matching teacher for each trajectory or particle like in previous class of methods, we try to match marginals of distilled student model and pretrained diffusion model. Further, adversarial loss has been applied to distillation approaches to improve the visual quality of the generated images Sauer et al. (2023; 2024); Lin et al. (2024). Most of distributional matching objectives like Sauer et al. (2024); Yin et al. (2023) are mode-seeking and"}, {"title": "BACKGROUND", "content": ""}, {"title": "REVERSE DENOISING PROCESS AS MDP", "content": "In imitation learning, an agent learns to perform tasks by observing and mimicking the behavior of the expert. An MDP in imitation learning models the next action based on the previous action and the current knowledge of the environment (Ke et al., 2021; Spencer et al., 2021). In general, an MDP is represented as \u3008S, A, P, po), where S is a finite set of states, A is the set of actions, P(s'|s, a) is a state transition kernel to transition from s to s' under the action a and po is the set of initial states. An MDP produces a trajectory which is a sequence of state-action pairs \u03c4 = (50, ao, S1, A1, \u2026, aT, ST) over T time steps.\nWe formalize the reverse process of the diffusion models as a finite horizon MDP (Black et al., 2023; Fan et al., 2024) with the policy \u03c0\u03b8 (the diffusion model with parameters (0) where the states and the actions are st := (xt,t) and at := xt\u22121 respectively. The transition dynamics is defined by P(St+1|St, at) := d(xt\u22121,t) and po(s) := (N(0, I), T) denotes the initial state distribution. The trajectory + becomes (XT, XT-1,..., \u0425\u043e)."}, {"title": "CO-VARIATE SHIFT IN DIFFUSION MODELS", "content": "With in iterative denoising steps of generation within backward trajectory of diffusion models, student's current predictions determines what the student (learner) sees in next step within sequential setting, which is classic feedback loop [1] in imitation learning. So if student makes any mistake or has bad score estimate in one of early steps this discrepancy excarbates in later iterations and results in accumulation of error. This error results in change in input distribution (covariate shift) of latents between training time (forward diffusion) and latents student model encounters when it is unrolled in iterative fashion at generation i.e., backward trajectory. Exposure bias is another closely related line of work Li et al. (2023a) which also discusses change in input distribution w.r.t pretrained diffusion model and propose training-free methods to improve it. Our work primarily focuses on distilling diffusion models and how this shift effects distillation.\nCovariate shift is more prounced for distilled student diffusion model compared to pretrained diffusion model. To further clarify why covariate shift poses more of a challenge for the student model compared to the teacher model, we can consider inference as ancestral sampling (or annealing in score estimation). During generation i.e., within intermediate time-steps of backward trajectory of diffusion model, there is an implicit assumption that the marginal distributions between two consecutive denoising steps significantly overlap. While this is a reasonable assumption in continuous time diffusion models or when the number of denoising steps is sufficiently high for pretrained diffusion model, when considering a diffusion model with only few steps, this assumption does not hold. Consequently, any covariate shift would be more exacerbated for the student model, unlike the continuous time teacher mode.\nDAgger to mitigate Co-variate shift: Imitation learning has long been used to learn offline sequential tasks wherein a student model is trained from teachers' demonstrations. Standard imitation learning also suffers from covariate shift in discrepancy in states visited by student and the teacher. Interactive"}, {"title": "BACKWARD TRAJECTORIES FOR DISTILLING DIFFUSION MODELS", "content": "Backward trajectory distillation introduced in recent concurrent works like ImagineFlash(Kohler et al., 2024) and DMD2 (Yin et al., 2024), focuses solely on evaluating the quality of generated samples without considering the data distribution. Consequently, they lack a mechanism to prevent mode collapse and ensure diversity.\nMode Seeking: The problem of covariate shift and distribution changes in diffusion distillation is multifaceted. It's not just about input distribution shifts caused by error accumulation. The reduction of diversity in the intermediate steps of backward trajectories (generative process) also plays a crucial role. If diversity is lost early on, it cascades through subsequent steps, limiting the range of possible outcomes. This is akin to error accumulation, but instead of errors, we are consistently losing diversity across time. We can think of it like sequential Monte Carlo sampling in diffusion models: at each step, we are discarding a large number of potential paths (particles), leading to a narrower range of possibilities in the later stages.\nWhile EM Distillation (Xie et al., 2024) addresses this by employing Langevin MCMC for a richer reverse process and mode-covering divergences, it still doesn't explicitly incorporate the data distri-bution into its sampling prior during distillation."}, {"title": "METHOD", "content": ""}, {"title": "IMPROVING TRAINING DISTRIBUTION WITH DDIL", "content": "We introduce Diffusion Distillation with Imitation Learning (DDIL), a novel framework inspired by the DAgger algorithm from imitation learning to enhance the sampling distribution of interme-diate noisy latents for distilling diffusion models. Diffusion model distillation involves two key considerations: (1) the training distribution of latent states encountered by the student model, and (2) the feedback mechanism employed during distillation. DDIL specifically focuses on improving the training distribution, remaining agnostic to the specific feedback mechanism utilized by different distillation techniques."}, {"title": "DDIL INTEGRATION", "content": "This section examines the integration of DDIL with various distillation techniques. Detailed design choices are further elaborated in the appendix (section to be updated).\nPD + DDIL: DDIL is integrated with progressive distillation using a DAgger-inspired approach Ross et al. (2011). Distillation is performed on mixed rollouts generated by alternating between the pre-trained and student diffusion models within each generation. A stateless DDIM solver facilitates this interleaved sampling process.\nLCM + DDIL: DDIL is also applied to consistency distillation. Due to the pre-trained model's lack of prior consistency training, the mixed-rollout strategy used in progressive distillation is not directly applicable. Therefore, DDIL is extended to LCM by applying consistency distillation to both"}, {"title": "EXPERIMENTS", "content": "Datasets and metrics: Following standard practice for evaluating text-to-image diffusion models Rombach et al. (2022); Meng et al. (2023), we evaluate our distilled models zero-shot on two public benchmarks: COCO 2017 (5K captions), and COCO 2014 Lin et al. (2014) (30K captions) validation sets. We use each caption to generate an image with a randomized seed and report CLIP score using OpenCLIP ViT-g/14 model Ilharco et al. (2021) to evaluate image-text alignment. We also report Fr\u00e9chet Inception Distance (FID) Heusel et al. (2017) to estimate perceptual quality. To measure diversity of generation, we report LPIPSDiversity, where for a given prompt we generate output for 10 different seeds and obtain pair-wise LPIPS score and finally average over 50 randomly sampled COCO 2017 prompts.\nTraining: For all our experiments, we choose AdamW optimizer Loshchilov & Hutter (2017) with 1e - 05 learning rate with warmup and linear schedule on a batch size of 224 in case of progressive distillation, 360 in case of LCM and 7 in case of DMD2 on SSD1B. To optimize for GPU usage, we adopt gradient checkpoint and mixed-precision training. Please refer to Appendix A for additional training details.\nPD + DDIL:In case of progressive distillation, we train the model for 4k steps for e to v space conversion to perform step distillation in v space Salimans & Ho (2021). Then we perform guidance conditioning following the same protocol as Meng et al. (2023) where we sample guidance scale \u03c9 ~ [2, 14] and incorporate additional guidance embedding as in Rombach et al. (2022) followed"}, {"title": "TEXT-GUIDED IMAGE GENERATION", "content": "We demonstrate effectiveness of our proposed DDIL framework across different baseline distillation techniques in case of text-to-image generation tasks as shown in Table 2. DDIL consistency improves on progressive distillation(PD) and latent consistency models (LCM) as observed in Table 2/ In case of progressive distillation, for 4-step version DDIL improves FID from 23.34 \u2192 22.42 and maintains clip score of 0.302 and similarly we can also observe DDIL improves on LCM with FID from 24.25 \u2192 22.86 and CLIP score 0.306 \u2192 0.309. From Tab. 4 in appendix, we can observe that 4-step variant of PD + DDIL with a guidance value of 8 achieves best FID of 13.97, the highest among trajectory based distillation methods.\nWe also demonstrate effectiveness of DDIL with distribution matching techniques which adopt multi-step student like in DMD2. When we apply DMD2 to SSD1B, we can observe that DDIL improves FID from 31.77 \u2192 27.72 and clip score from 0.320 \u2192 0.326 and HPSv2 score from 0.302 \u2192 0.304.\nComputational efficiency: DDIL demonstrates superior computational efficiency compared to state-of-the-art methods such as Instaflow and DMD. For instance, LCM augmented with DDIL (LCM+DDIL) achieves strong performance using only 8,000 gradient steps with a batch size of 420. This contrasts sharply with Instaflow, which requires 183 A100 GPU-days for distillation. DDIL with progressive distillation (PD) reduces this to 15 A100 GPU-days. Similarly, while DMD necessitates"}, {"title": "CONCLUSION", "content": "This work introduces DDIL, a novel framework for distilling diffusion models that addresses the challenge of covariate shift while preserving the marginal data distribution. Integrating DDIL with established distillation techniques, including Progressive Distillation, Consistency Distillation (LCM), and Distribution Matching based Distillation (DMD2), consistently yields quantitative and qualitative improvements. Furthermore, we also show that integrating DDIL within the DMD2 framework enhances training stability, reduces required batch sizes, and improves computational efficiency demonstrating wider applicability and practical usefulness."}, {"title": "REVERSE DENOISING PROCESS AS MDP", "content": "In imitation learning, an agent learns to perform tasks by observing and mimicking the behavior of the expert. An MDP in imitation learning models the next action based on the previous action and the current knowledge of the environment (Ke et al., 2021; Spencer et al., 2021). In general, an MDP is represented as \u3008S, A, P, po), where S is a finite set of states, A is the set of actions, P(s'|s, a) is a state transition kernel to transition from s to s' under the action a and po is the set of initial states. An MDP produces a trajectory which is a sequence of state-action pairs \u03c4 = (50, ao, S1, A1, \u2026, aT, ST) over T time steps.\nWe formalize the reverse process of the diffusion models as a finite horizon MDP (Black et al., 2023; Fan et al., 2024) with the policy $\u03c0_\u03b8$ (the diffusion model with parameters (0) where the states and the actions are $s_t := (x_t,t)$ and $a_t := x_{t\u22121}$ respectively. The transition dynamics is defined by $P(St+1|St, at) := d(xt\u22121,t)$ and po(s) := (N(0, I), T) denotes the initial state distribution. The trajectory + becomes (XT, XT-1,..., \u0425\u043e)."}, {"title": "CO-VARIATE SHIFT IN DIFFUSION MODELS", "content": "With in iterative denoising steps of generation within backward trajectory of diffusion models, student's current predictions determines what the student (learner) sees in next step within sequential setting, which is classic feedback loop [1] in imitation learning. So if student makes any mistake or has bad score estimate in one of early steps this discrepancy excarbates in later iterations and results in accumulation of error. This error results in change in input distribution (covariate shift) of latents between training time (forward diffusion) and latents student model encounters when it is unrolled in iterative fashion at generation i.e., backward trajectory. Exposure bias is another closely related line of work Li et al. (2023a) which also discusses change in input distribution w.r.t pretrained diffusion model and propose training-free methods to improve it. Our work primarily focuses on distilling diffusion models and how this shift effects distillation.\nCovariate shift is more prounced for distilled student diffusion model compared to pretrained diffusion model. To further clarify why covariate shift poses more of a challenge for the student model compared to the teacher model, we can consider inference as ancestral sampling (or annealing in score estimation). During generation i.e., within intermediate time-steps of backward trajectory of diffusion model, there is an implicit assumption that the marginal distributions between two consecutive denoising steps significantly overlap. While this is a reasonable assumption in continuous time diffusion models or when the number of denoising steps is sufficiently high for pretrained diffusion model, when considering a diffusion model with only few steps, this assumption does not hold. Consequently, any covariate shift would be more exacerbated for the student model, unlike the continuous time teacher mode.\nDAgger to mitigate Co-variate shift: Imitation learning has long been used to learn offline sequential tasks wherein a student model is trained from teachers' demonstrations. Standard imitation learning also suffers from covariate shift in discrepancy in states visited by student and the teacher. Interactive"}, {"title": "BACKWARD TRAJECTORIES FOR DISTILLING DIFFUSION MODELS", "content": "Backward trajectory distillation introduced in recent concurrent works like ImagineFlash(Kohler et al., 2024) and DMD2 (Yin et al., 2024), focuses solely on evaluating the quality of generated samples without considering the data distribution. Consequently, they lack a mechanism to prevent mode collapse and ensure diversity.\nMode Seeking: The problem of covariate shift and distribution changes in diffusion distillation is multifaceted. It's not just about input distribution shifts caused by error accumulation. The reduction of diversity in the intermediate steps of backward trajectories (generative process) also plays a crucial role. If diversity is lost early on, it cascades through subsequent steps, limiting the range of possible outcomes. This is akin to error accumulation, but instead of errors, we are consistently losing diversity across time. We can think of it like sequential Monte Carlo sampling in diffusion models: at each step, we are discarding a large number of potential paths (particles), leading to a narrower range of possibilities in the later stages.\nWhile EM Distillation (Xie et al., 2024) addresses this by employing Langevin MCMC for a richer reverse process and mode-covering divergences, it still doesn't explicitly incorporate the data distri-bution into its sampling prior during distillation."}, {"title": "METHOD", "content": ""}, {"title": "IMPROVING TRAINING DISTRIBUTION WITH DDIL", "content": "We introduce Diffusion Distillation with Imitation Learning (DDIL), a novel framework inspired by the DAgger algorithm from imitation learning to enhance the sampling distribution of interme-diate noisy latents for distilling diffusion models. Diffusion model distillation involves two key considerations: (1) the training distribution of latent states encountered by the student model, and (2) the feedback mechanism employed during distillation. DDIL specifically focuses on improving the training distribution, remaining agnostic to the specific feedback mechanism utilized by different distillation techniques."}, {"title": "DDIL INTEGRATION", "content": "This section examines the integration of DDIL with various distillation techniques. Detailed design choices are further elaborated in the appendix (section to be updated).\nPD + DDIL: DDIL is integrated with progressive distillation using a DAgger-inspired approach Ross et al. (2011). Distillation is performed on mixed rollouts generated by alternating between the pre-trained and student diffusion models within each generation. A stateless DDIM solver facilitates this interleaved sampling process.\nLCM + DDIL: DDIL is also applied to consistency distillation. Due to the pre-trained model's lack of prior consistency training, the mixed-rollout strategy used in progressive distillation is not directly applicable. Therefore, DDIL is extended to LCM by applying consistency distillation to both"}, {"title": "EXPERIMENTS", "content": "Datasets and metrics: Following standard practice for evaluating text-to-image diffusion models Rombach et al. (2022); Meng et al. (2023), we evaluate our distilled models zero-shot on two public benchmarks: COCO 2017 (5K captions), and COCO 2014 Lin et al. (2014) (30K captions) validation sets. We use each caption to generate an image with a randomized seed and report CLIP score using OpenCLIP ViT-g/14 model Ilharco et al. (2021) to evaluate image-text alignment. We also report Fr\u00e9chet Inception Distance (FID) Heusel et al. (2017) to estimate perceptual quality. To measure diversity of generation, we report LPIPSDiversity, where for a given prompt we generate output for 10 different seeds and obtain pair-wise LPIPS score and finally average over 50 randomly sampled COCO 2017 prompts.\nTraining: For all our experiments, we choose AdamW optimizer Loshchilov & Hutter (2017) with 1e - 05 learning rate with warmup and linear schedule on a batch size of 224 in case of progressive distillation, 360 in case of LCM and 7 in case of DMD2 on SSD1B. To optimize for GPU usage, we adopt gradient checkpoint and mixed-precision training. Please refer to Appendix A for additional training details.\nPD + DDIL:In case of progressive distillation, we train the model for 4k steps for e to v space conversion to perform step distillation in v space Salimans & Ho (2021). Then we perform guidance conditioning following the same protocol as Meng et al. (2023) where we sample guidance scale \u03c9 ~ [2, 14] and incorporate additional guidance embedding as in Rombach et al. (2022) followed"}, {"title": "TEXT-GUIDED IMAGE GENERATION", "content": "We demonstrate effectiveness of our proposed DDIL framework across different baseline distillation techniques in case of text-to-image generation tasks as shown in Table 2. DDIL consistency improves on progressive distillation(PD) and latent consistency models (LCM) as observed in Table 2/ In case of progressive distillation, for 4-step version DDIL improves FID from 23.34 \u2192 22.42 and maintains clip score of 0.302 and similarly we can also observe DDIL improves on LCM with FID from 24.25 \u2192 22.86 and CLIP score 0.306 \u2192 0.309. From Tab. 4 in appendix, we can observe that 4-step variant of PD + DDIL with a guidance value of 8 achieves best FID of 13.97, the highest among trajectory based distillation methods.\nWe also demonstrate effectiveness of DDIL with distribution matching techniques which adopt multi-step student like in DMD2. When we apply DMD2 to SSD1B, we can observe that DDIL improves FID from 31.77 \u2192 27.72 and clip score from 0.320 \u2192 0.326 and HPSv2 score from 0.302 \u2192 0.304.\nComputational efficiency: DDIL demonstrates superior computational efficiency compared to state-of-the-art methods such as Instaflow and DMD. For instance, LCM augmented with DDIL (LCM+DDIL) achieves strong performance using only 8,000 gradient steps with a batch size of 420. This contrasts sharply with Instaflow, which requires 183 A100 GPU-days for distillation. DDIL with progressive distillation (PD) reduces this to 15 A100 GPU-days. Similarly, while DMD necessitates"}, {"title": "CONCLUSION", "content": "This work introduces DDIL, a novel framework for distilling diffusion models that addresses the challenge of covariate shift while preserving the marginal data distribution. Integrating DDIL with established distillation techniques, including Progressive Distillation, Consistency Distillation (LCM), and Distribution Matching based Distillation (DMD2), consistently yields quantitative and qualitative improvements. Furthermore, we also show that integrating DDIL within the DMD2 framework enhances training stability, reduces required batch sizes, and improves computational efficiency demonstrating wider applicability and practical usefulness."}, {"title": "BACKWARD TRAJECTORIES FOR DISTILLING DIFFUSION MODELS", "content": "Backward trajectory distillation introduced in recent concurrent works like ImagineFlash(Kohler et al., 2024) and DMD2 (Yin et al., 2024), focuses solely on evaluating the quality of generated samples without considering the data distribution. Consequently, they lack a mechanism to prevent mode collapse and ensure diversity.\nMode Seeking: The problem of covariate shift and distribution changes in diffusion distillation is multifaceted. It's not just about input distribution shifts caused by error accumulation. The reduction of diversity in the intermediate steps of backward trajectories (generative process) also plays a crucial role. If diversity is lost early on, it cascades through subsequent steps, limiting the range of possible outcomes. This is akin to error accumulation, but instead of errors, we are consistently losing diversity across time. We can think of it like sequential Monte Carlo sampling in diffusion models: at each step, we are discarding a large number of potential paths (particles), leading to a narrower range of possibilities in the later stages.\nWhile EM Distillation (Xie et al., 2024) addresses this by employing Langevin MCMC for a richer reverse process and mode-covering divergences, it still doesn't explicitly incorporate the data distri-bution into its sampling prior during distillation."}, {"title": "METHOD", "content": ""}, {"title": "IMPROVING TRAINING DISTRIBUTION WITH DDIL", "content": "We introduce Diffusion Distillation with Imitation Learning (DDIL), a novel framework inspired by the DAgger algorithm from imitation learning to enhance the sampling distribution of interme-diate noisy latents for distilling diffusion models. Diffusion model distillation involves two key considerations: (1) the training distribution of latent states encountered by the student model, and (2) the feedback mechanism employed during distillation. DDIL specifically focuses on improving the training distribution, remaining agnostic to the specific feedback mechanism utilized by different distillation techniques."}, {"title": "DDIL INTEGRATION", "content": "This section examines the integration of DDIL with various distillation techniques. Detailed design choices are further elaborated in the appendix (section to be updated).\nPD + DDIL: DDIL is integrated with progressive distillation using a DAgger-inspired approach Ross et al. (2011). Distillation is performed on mixed rollouts generated by alternating between the pre-trained and student diffusion models within each generation. A stateless DDIM solver facilitates this interleaved sampling process.\nLCM + DDIL: DDIL is also applied to consistency distillation. Due to the pre-trained model's lack of prior consistency training, the mixed-rollout strategy used in progressive distillation is not directly applicable. Therefore, DDIL is extended to LCM by applying consistency distillation to both"}, {"title": "EXPERIMENTS", "content": "Datasets and metrics: Following standard practice for evaluating text-to-image diffusion models Rombach et al. (2022); Meng et al. (2023), we evaluate our distilled models zero-shot on two public benchmarks: COCO 2017 (5K captions), and COCO 2014 Lin et al. (2014) (30K captions) validation sets. We use each caption to generate an image with a randomized seed and report CLIP score using OpenCLIP ViT-g/14 model Ilharco et al. (2021) to evaluate image-text alignment. We also report Fr\u00e9chet Inception Distance (FID) Heusel et al. (2017) to estimate perceptual quality. To measure diversity of generation, we report LPIPSDiversity, where for a given prompt we generate output for 10 different seeds and obtain pair-wise LPIPS score and finally average over 50 randomly sampled COCO 2017 prompts.\nTraining: For all our experiments, we choose AdamW optimizer Loshchilov & Hutter (2017) with 1e - 05 learning rate with warmup and linear schedule on a batch size of 224 in case of progressive distillation, 360 in case of LCM and 7 in case of DMD2 on SSD1B. To optimize for GPU usage, we adopt gradient checkpoint and mixed-precision training. Please refer to Appendix A for additional training details.\nPD + DDIL:In case of progressive distillation, we train the model for 4k steps for e to v space conversion to perform step distillation in v space Salimans & Ho (2021). Then we perform guidance conditioning following the same protocol as Meng et al. (2023) where we sample guidance scale \u03c9 ~ [2, 14] and incorporate additional guidance embedding as in Rombach et al. (2022) followed"}, {"title": "TEXT-GUIDED IMAGE GENERATION", "content": "We demonstrate effectiveness of our proposed DDIL framework across different baseline distillation techniques in case of text-to-image generation tasks as shown in Table 2. DDIL consistency improves on progressive distillation(PD) and latent consistency models (LCM) as observed in Table 2/ In case of progressive distillation, for 4-step version DDIL improves FID from 23.34 \u2192 22.42 and maintains clip score of 0.302 and similarly we can also observe DDIL improves on LCM with FID from 24.25 \u2192 22.86 and CLIP score 0.306 \u2192 0.309. From Tab. 4 in appendix, we can observe that 4-step variant of PD + DDIL with a guidance value of 8 achieves best FID of 13.97, the highest among trajectory based distillation methods.\nWe also demonstrate effectiveness of DDIL with distribution matching techniques which adopt multi-step student like in DMD2. When we apply DMD2 to SSD1B, we can observe that DDIL improves FID from 31.77 \u2192 27.72 and clip score from 0.320 \u2192 0.326 and HPSv2 score from 0.302 \u2192 0.304.\nComputational efficiency: DDIL demonstrates superior computational efficiency compared to state-of-the-art methods such as Instaflow and DMD. For instance, LCM augmented with DDIL (LCM+DDIL) achieves strong performance using only 8,000 gradient steps with a batch size of 420. This contrasts sharply with Instaflow, which requires 183 A100 GPU-days for distillation. DDIL with progressive distillation (PD) reduces this to 15 A100 GPU-days. Similarly, while DMD necessitates"}, {"title": "CONCLUSION", "content": "This work introduces DDIL, a novel framework for distilling diffusion models that addresses the challenge of covariate shift while preserving the marginal data distribution. Integrating DDIL with established distillation techniques, including Progressive Distillation, Consistency Distillation (LCM), and Distribution Matching based Distillation (DMD2), consistently yields quantitative and qualitative improvements. Furthermore, we also show that integrating DDIL within the DMD2 framework enhances training stability, reduces required batch sizes, and improves computational efficiency demonstrating wider applicability and practical usefulness."}, {"title": "Text-Guided Image Generation", "content": "We demonstrate effectiveness of our proposed DDIL framework across different baseline distillation techniques in case of text-to-image generation tasks as shown in Table 2. DDIL consistency improves on progressive distillation(PD) and latent consistency models (LCM) as observed in Table 2/ In case of progressive distillation, for 4-step version DDIL improves FID from 23.34 \u2192 22.42 and maintains clip score of 0.302 and similarly we can also observe DDIL improves on LCM with FID from 24.25 \u2192 22.86 and CLIP score 0.306 \u2192 0.309. From Tab. 4 in appendix, we can observe that 4-step variant of PD + DDIL with a guidance value of 8 achieves best FID of 13.97, the highest among trajectory based distillation methods.\nWe also demonstrate effectiveness of DDIL with distribution matching techniques which adopt multi-step student like in DMD2. When we apply DMD2 to SSD1B, we can observe that DDIL improves FID from 31.77 \u2192 27.72 and clip score from 0.320 \u2192 0.326 and HPSv2 score from 0.302 \u2192 0.304.\nComputational efficiency: DDIL demonstrates superior computational efficiency compared to state-of-the-art methods such as Instaflow and DMD. For instance, LCM augmented with DDIL (LCM+DDIL) achieves strong performance using only 8,000 gradient steps with a batch size of 420. This contrasts sharply with Instaflow, which requires 183 A100 GPU-days for distillation. DDIL with progressive distillation (PD) reduces this to 15 A100 GPU-days. Similarly, while DMD necessitates"}, {"title": "Conclusion", "content": "This work introduces DDIL, a novel framework for distilling diffusion models that addresses the challenge of covariate shift while preserving the marginal data distribution. Integrating DDIL with established distillation techniques, including Progressive Distillation, Consistency Distillation (LCM), and Distribution Matching based Distillation (DMD2), consistently yields quantitative and qualitative improvements. Furthermore, we also show that integrating DDIL within the DMD2 framework enhances training stability, reduces required batch sizes, and improves computational efficiency demonstrating wider applicability and practical usefulness."}, {"title": "Progressive Distillation With DdIL", "content": "Inspired by the success of the interactive learning DAgger algorithm in imitation learning and following the formulation of the reverse process of the diffusion models as probability flow ODE, we first extend the DAgger framework to diffusion models by considering the higher iteration denoising model as expert and fewer iteration denoising model as a student in Algorithm 2 Following this, in Algorithm 3, we present the complete DDIL approach with interactive learning.\nFor sampling in diffusion models, the student predicted latent z\u0142 is aligned with the teacher trajectory by adding the state-action pair (zt, e,t) \u2208 \u03c4\u03bf to the aggregated dataset. Note that the dataset aggregation is done randomly so that the model is aware of the teacher and the student's distributions.\nIn our DDIL algorithm outlined in Algorithm 3, the distillation is performed iteratively by taking the"}]}