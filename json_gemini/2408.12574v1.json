{"title": "MuMA-ToM: Multi-modal Multi-Agent Theory of Mind", "authors": ["Haojun Shi", "Suyu Ye", "Xinyu Fang", "Chuanyang Jin", "Layla Isik", "Yen-Ling Kuo", "Tianmin Shu"], "abstract": "Understanding people's social interactions in complex real-world scenarios often relies on intricate mental reasoning. To truly understand how and why people interact with one another, we must infer the underlying mental states that give rise to the social interactions, i.e., Theory of Mind reasoning in multi-agent interactions. Additionally, social interactions are often multi-modal we can watch people's actions, hear their conversations, and/or read about their past behaviors. For AI systems to successfully and safely interact with people in real-world environments, they also need to understand people's mental states as well as their inferences about each other's mental states based on multi-modal information about their interactions. For this, we introduce MuMA-TOM, a Multi-modal Multi-Agent Theory of Mind benchmark. MuMA-TOM is the first multi-modal Theory of Mind benchmark that evaluates mental reasoning in embodied multi-agent interactions. In MuMA-TOM, we provide video and text descriptions of people's multi-modal behavior in realistic household environments. Based on the context, we then ask questions about people's goals, beliefs, and beliefs about others' goals. We validated MuMA-ToM in a human experiment and provided a human baseline. We also proposed a novel multi-modal, multi-agent ToM model, LIMP (Language model-based Inverse Multi-agent Planning). Our experimental results show that LIMP significantly outperforms state-of-the-art methods, including large multi-modal models (e.g., GPT-40, Gemini-1.5 Pro) and a recent multi-modal ToM model, BIP-ALM.", "sections": [{"title": "Introduction", "content": "Humans live in a social world; we not only engage in social interactions ourselves but can also understand other people's social interactions. Studies in Developmental Psychology have indicated that the ability to understand different kinds of social interactions develops early and is one of the bases for more sophisticated social skills developed later in life (Denham et al. 2003; Wellman, Cross, and Watson 2001; Hamlin, Wynn, and Bloom 2007). Crucially, understanding social interactions goes beyond action recognition. We often need to reason about why people interact with one another in a certain manner. We can achieve this by inferring people's mental states as well as how they reason about one another's mental states, i.e., multi-agent Theory of Mind (ToM) reasoning. For instance, if Alice puts away a book on Bob's desk, she may be trying to clean up or hide the book, depending on both her social goal (helping or hindering) and where she believes Bob wants the book (belief of other's goal). As an observer, it may be difficult to disambiguate between these scenarios. However, if we had heard Bob asking Alice where the book was before, we would confidently infer that Alice wanted to hinder Bob. Such multi-modal, multi-agent Theory of Mind abilities are not only crucial for humans but also for AI systems that are deployed in human living environments, such as assistive robots. Without a robust understanding of people's mental states in complex social interactions, AI systems may cause detrimental errors in their interactions with people.\nDespite the recent advances in evaluating and engineering machine Theory of Mind, prior works have not adequately addressed the challenge of Theory of Mind reasoning in multi-modal social interactions. First, common Theory of Mind benchmarks (Gordon 2016b; Gandhi et al. 2021; Shu et al. 2021; Kosinski 2023; Jin et al. 2024) have only focused on individuals' mental states. However, there are other important aspects of multi-agent mental reasoning, including social goals (e.g., helping, hindering) and beliefs about others' goals. Second, there has not been a multi-modal social interaction dataset designed for systematic Theory of Mind reasoning evaluation. The only prior multi-modal Theory of Mind dataset is MMTOM-QA, which solely focuses on single-agent activities. Text-only benchmarks such as Hi-TOM (Wu et al. 2023) feature multi-agent events, but lack visual inputs. Thus, it remains unclear how we can evaluate the multi-modal multi-agent Theory of Mind capacity in machine learning models.\nTo address these challenges, we introduce a new Theory of Mind benchmark, MuMA-TOM (Multi-modal Multi-Agent Theory of Mind benchmark). MuMA-TOM includes a large set of question-answering trials. As summarized in Figure 1, questions in MuMA-TOM are organized into three categories: (1) belief inference, (2) social goal inference, and"}, {"title": "Related Works", "content": "Theory of Mind Benchmarks. Single-agent ToM benchmarks (Gordon 2016b; Gandhi et al. 2021; Shu et al. 2021; Kosinski 2023; Jin et al. 2024) have extensively tested concepts like belief, goal, preferences, constraints, and rationality. Multi-agent benchmarks are typically built based on the classic Sally-Anne test (Baron-Cohen, Leslie, and Frith 1985) for false beliefs and higher-order beliefs (Le, Boureau, and Nickel 2019; He et al. 2023; Xu et al. 2024; Soubki et al. 2024). There have also been multi-agent benchmarks that focus on a single agent's beliefs & intentions in complex conversations or interactions (Kim et al. 2023; Chen et al. 2024a; Chan et al. 2024; Sabour et al. 2024). In these benchmarks, other agents are usually present to add context or complexity, but there are no questions about inter-agent relationships. Prior works on testing social relationship understanding (Netanyahu et al. 2021a; Li et al. 2024a) rely on simple animations, which lack the realism of embodied human interactions. Most existing ToM benchmarks have only either text or video. The only exception is MMTOM-QA (Jin et al. 2024), which has single-agent activities depicted in video and text. Our MuMA-TOM benchmark features two agents interacting in an embodied household environment, with both text and video as multi-modal inputs, and includes questions that test the agents' social intentions and their reasoning about each other's mental states.\nMulti-Modal Benchmarks. Given the recent advances in LLMs, there has been increasing interest in developing multi-modal QA benchmarks. Most of these benchmarks focus on models' ability to fuse information from multiple modalities, where answers are directly retrievable without complex reasoning (Li et al. 2023b; Sanders et al. 2023; Li et al. 2023a; Ying et al. 2024a; Tang et al. 2024; Pandya et al. 2024). A recent benchmark, Perception Test (Patraucean et al. 2024), evaluates physical reasoning such as predicting world states and explaining counterfactual facts. But it differs from ToM reasoning. Pipelines for generating multi-modal datasets, SEED-story (Yang et al. 2024) and TaskMeAnything (Zhang et al. 2024), also do not evaluate ToM reasoning. MMTOM-QA (Jin et al. 2024), a recent multi-modal ToM benchmark, evaluates ToM with multi-modal inputs about single-agent behaviors. Unlike MMTOM-QA, our benchmark includes multi-agent interac-"}, {"title": "MuMA-ToM Benchmark", "content": "The benchmark consists of 225 multi-modal social interactions between two agents. There are 900 multi-choice questions based on these social interactions. Each question depicts a social interaction in video and text jointly. As shown in Figure 1, the text may show a conversation between the agents or a part of the event, and the video shows the complementary part of the event. Given the multi-modal inputs, the questions are designed to assess the understanding of agents' mental states during these interactions, probing three main concepts: (1) beliefs, (2) social goals, and (3) beliefs of others' goals. Each concept has 300 questions. We also created a training set consisting of 1,030 videos annotated with the agents' actions and goals. The training set does not provide example questions. It is intended for a model to learn about typical multi-agent household activities."}, {"title": "Question Types", "content": "As identified in prior works in cognitive science (Ullman et al. 2009; Shu et al. 2020) and multi-agent planning (Gmytrasiewicz and Doshi 2005; Tejwani et al. 2021), there are three mental variables that are crucial to ToM reasoning in multi-agent interactions: an agent's belief of the physical state, its social goal, and its belief of other agents' goals. Therefore, we design three types of questions in our benchmark corresponding to the three mental variables: belief inference, social goal inference, and belief of goal inference. Each type of question asks about the corresponding mental variable of one of the agents. Among the three options, we make sure that there is always one option that is clearly the most likely to be correct.\nOne of the challenges in designing these three types of questions is that given an interaction, multiple combinations of these mental variables could be equally possible. For instance, if we see that Alice's actions prevent Bob from reaching his goal, it could be because Alice is hindering Bob, knowing Bob's true intent; or she may try to help Bob but has a false belief of Bob's goal and ends up accidentally hindering Bob. To address the challenge of large hypothesis space, we always ask a question about a mental variable conditioned on explicitly provided assumptions about the other two mental variables. For instance, as shown in the example question of the belief inference in Figure 1, the goal of John can be inferred from his question about where the beer is (the goal is getting beer), and Mary should be aware of this as she answered John's question; the social goal of Mary is unclear, therefore the question provides a hypothetical social goal, hindering, as the condition. The remaining mental variable of Mary, her belief of the physical state can be clearly inferred given her social goal and her belief of John's goal.\nWe explain the design of each question type as follows.\nBelief Inference. These questions focus on inferring a person's belief about the physical state based on their utterance and social goal. The person may have a true belief or false belief about the location of the object, which can be inferred when we constrain their social goal to be helping or hindering. In the example depicted in Figure 1, John asks Mary where he can find the beer. Mary suggests the coffee table, which turns out to be the correct location, as John successfully finds the beer there. This could be interpreted in two ways: (1) Mary helps John, genuinely believing the beer is on the coffee table, or (2) Mary accidentally helps John while intending to mislead him, mistakenly believing that the beer isn't on the coffee table. To answer correctly, a model needs to understand: (1) Mary knows John's goal (from their conversation), (2) John follows Mary's directions (from their conversation and his actions afterward in the video), and (3) John achieves his goal by following Mary's directions (as shown in the video). We balance true and false beliefs in the ground-truth answers.\nSocial Goal Inference. In these questions, we ask about a person's social goal. Specifically, we consider helping, hindering, or acting independently as the three possible social goal categories, which are also the common social goal types in physically grounded social interaction reasoning studied by prior works in cognitive science (Hamlin, Wynn, and Bloom 2007; Ullman et al. 2009; Shu et al. 2020; Malik and Isik 2023). The example in Figure 1 shows an interaction similar to the one in the example for belief inference questions. In this particular example, Jessica misleads Kevin to the cabinet where there is no magazine inside. In the question, we assume that Jessica does indeed know the true state, and therefore, one should infer that Jessica is trying to hinder Kevin. To achieve this correct inference, a model needs to focus on (1) how Jessica infers Kevin's goal (from the conversation), (2) how Kevin searches the room after the"}, {"title": "Our Model", "content": "To model social interactions between two agents, $i$ and $j$, and the recursive mental reasoning between them, we adopt an Interactive Partially Observable Markov Decision Processes (I-POMDP) formulation (Gmytrasiewicz and Doshi 2005). We define $s_t$ as the state, $a_t^i$ and $a_t^j$ as agents' actions, and $u_t^i$ and $u_t^j$ as agents' utterances at time t. Each agent maintains its own beliefs $b_t^i$ and $b_t^j$, as well as goals $g_i$ and $g_j$. To capture recursive reasoning, we define interactive states for the agents, denoted as $is_{i,l}$ and $is_{j,l}$ at level $l$. From the perspective of agent i, its interactive state at each level is defined as follows (we consider the first two levels in this work):\n\\begin{itemize}\n    \\item Level 0: $is_{i,0} = s_t$\n    \\item Level 1: $is_{i,1} = (s_t, b_{j,0}, g_j)$ (where $b_{j,0}$ is a distribution over agent $j$'s level 0 interactive state, $is_{j,0}$)\n\\end{itemize}\nGiven the belief of interactive state $b_t^i(is_{i,1})$, an agent's action policy will be $\\pi(a_i | is_{i,1}, g_i)$, and its utterance policy will be $\\pi(u_i | is_{i,1}, g_i)$.\nPrevious works on Inverse Multi-agent Planning (IMP) (Ullman et al. 2009; Netanyahu et al. 2021a) have demonstrated that IMP can robustly infer agents' mental states in social interactions. However, these methods rely on manually crafted planners and are limited to simple visual scenarios, such as 2D grid worlds. Jin et al. (2024) introduced the BIP-ALM model, which leverages language models for inverse planning to achieve single-agent Theory of Mind reasoning in complex, realistic settings. Inspired by BIP-ALM, we propose a novel method, Language model-based Inverse Multi-agent Planning (LIMP), to combine IMP and language models for robust multi-agent Theory of Mind reasoning based on multi-modal inputs.\nAs illustrated in Figure 2, LIMP consists of three key components: multi-modal information fusion, hypothesis parsing, and inverse multi-agent planning. Compared to BIP-ALM, our approach offers several improvements. First, while BIP-ALM is limited to single-agent scenarios, LIMP identifies three mental variables crucial to understanding multi-agent interactions-belief, social goal, and belief of goal. We then implement multi-agent planning based on these variables to reason about multi-modal social interactions. Second, BIP-ALM relies on hand-designed symbolic representations and requires finetuning language models on"}, {"title": "Multi-modal Information Fusion", "content": "We use a vision-language model (VLM) to extract the actions and utterances of each person depicted in the video. Given text, we use an LLM to extract the actions and utterances of each person. We then fuse the extracted information to form the initial state and the complete sequences of actions and utterances using an LLM as follows.\nUnlike MMTOM-QA, our benchmark does not provide a text description of the full state, as such descriptions are rarely provided in real-world applications. However, as objects may be occluded or too small to detect even for humans, inferring the state directly from the RGB videos could be difficult. Instead, we prompt an LLM with the inferred actions and utterances of both agents to infer the part of the initial state relevant to the activity. For example, if Alice grabs a carrot from the fridge, and moves it to the kitchen table, we can infer that the carrot was originally in the fridge. Using this method, the reconstructed initial state will only consider objects relevant to human actions and utterances. This simplifies the context and can consequently improve the accuracy of the inference. Given the initial state and the action sequences, we can infer the state at each step.\nThere is often missing information in the visual perception results. For instance, as shown in Figure 3, the VLM did not recognize the object the person grabbed and produced an ambiguous action \u2013 \u201cgrabs some object.\" This is also common to people, as the object picked up by the person is often occluded. However, we can still infer that the object is likely juice based on the context provided in the text. To emulate such ability, we leverage an LLM to fuse information extracted from video and text, which infers the information missing from visual perception based on the complementary information described in the text.\nIn this work, we use Gemini 1.5 Pro for the VLM and GPT-40 for the LLM as they produce the best results.\""}, {"title": "Hypothesis Parsing", "content": "To answer the question about a person's mental state in a social interaction, LIMP will parse relevant hypotheses of all mental variables of that person (agent $i$) \u2013 belief of state $b(s)$, social goal $g_i$, and belief of other agent's goal $b(g_j)$. For this, we prompt GPT-40 with the initial state and question text to generate a reasonable hypothesis of the three mental variables for each option, $H = (b(s), g_i, b(g_j))$."}, {"title": "Inverse Multi-Agent Planning", "content": "Given the fused information from multi-modal inputs and the parsed hypotheses, inverse multi-agent planning conducts Bayesian inference over a person's mental state by evaluating the likelihood of actions and utterances given each hypothesis. Following the I-POMDP formulation, we"}, {"title": "Human Experiment", "content": "We recruited 18 participants (mean age = 36.0; 10 female) from Prolific to answer 90 questions randomly sampled from"}, {"title": "Experiments", "content": "We report the human and model performance in Figure 5 and Table 3. Human participants achieved almost perfect accuracy across all questions, with 98.9% of the correct answers having majority agreement. The overall performance averaged across individual participants is 93.5%. The slightly lower performance on social goal inference (94.4%) and belief of goal inference (87.1%) indicates these questions are"}, {"title": "Discussion", "content": "Why do LMMs perform poorly? There are two sources of systematic errors for LMMs. First, LMMs struggle with understanding multi-modal behavior in complex social situations, often failing to distinguish between deliberate hindering and failed attempts to help due to incorrect beliefs. Most models can solve belief inference tasks where helping is the assumed social goal. However, they consistently struggle in scenarios where hindering is the assumed social goal (e.g., \"If Mary is trying to hinder Jack, where does she least likely believe...?). Except for Llava 1.6 34B (87.2%) and Gemini 1.5 Pro (62.2%), all models perform at or below the level of random guessing in these cases, with Intern-VL 2 26B (18.6%) performing much worse than random chance. The failure to understand adversarial behaviors is even more prominent in social goal inference and belief of goal inference. For instance, if Sarah tells John there is milk in the fridge, but there isn't any milk, and Sarah knows this, she is hindering John. Second, LMMs often fail to correctly interpret visual inputs, such as when an object is too small or is occluded when the agent is picking it up, leading to incorrect conclusions about the agent's actions. While humans are able to use contextual clues to infer what the object might be, VLMs struggle with this task. These errors in recognizing crucial actions likely contribute significantly to their overall poor performance on our benchmark."}, {"title": "Conclusion", "content": "We present the first multi-modal Theory of Mind benchmark for multi-agent interactions in complex embodied set-"}]}