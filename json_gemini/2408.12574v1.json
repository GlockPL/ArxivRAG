{"title": "MuMA-ToM: Multi-modal Multi-Agent Theory of Mind", "authors": ["Haojun Shi", "Suyu Ye", "Xinyu Fang", "Chuanyang Jin", "Layla Isik", "Yen-Ling Kuo", "Tianmin Shu"], "abstract": "Understanding people's social interactions in complex real-world scenarios often relies on intricate mental reasoning. To truly understand how and why people interact with one another, we must infer the underlying mental states that give rise to the social interactions, i.e., Theory of Mind reasoning in multi-agent interactions. Additionally, social interactions are often multi-modal we can watch people's actions, hear their conversations, and/or read about their past behaviors. For AI systems to successfully and safely interact with people in real-world environments, they also need to understand people's mental states as well as their inferences about each other's mental states based on multi-modal information about their interactions. For this, we introduce MuMA-TOM, a Multi-modal Multi-Agent Theory of Mind benchmark. MuMA-TOM is the first multi-modal Theory of Mind benchmark that evaluates mental reasoning in embodied multi-agent interactions. In MuMA-TOM, we provide video and text descriptions of people's multi-modal behavior in realistic household environments. Based on the context, we then ask questions about people's goals, beliefs, and beliefs about others' goals. We validated MuMA-ToM in a human experiment and provided a human baseline. We also proposed a novel multi-modal, multi-agent ToM model, LIMP (Language model-based Inverse Multi-agent Planning). Our experimental results show that LIMP significantly outperforms state-of-the-art methods, including large multi-modal models (e.g., GPT-40, Gemini-1.5 Pro) and a recent multi-modal ToM model, BIP-ALM.", "sections": [{"title": "Introduction", "content": "Humans live in a social world; we not only engage in social interactions ourselves but can also understand other people's social interactions. Studies in Developmental Psychology have indicated that the ability to understand different kinds of social interactions develops early and is one of the bases for more sophisticated social skills developed later in life (Denham et al. 2003; Wellman, Cross, and Watson 2001; Hamlin, Wynn, and Bloom 2007). Crucially, understanding social interactions goes beyond action recognition. We often need to reason about why people interact with one another in a certain manner. We can achieve this by inferring people's mental states as well as how they reason about one another's mental states, i.e., multi-agent Theory of Mind (ToM) reasoning. For instance, if Alice puts away a book on Bob's desk, she may be trying to clean up or hide the book, depending on both her social goal (helping or hindering) and where she believes Bob wants the book (belief of other's goal). As an observer, it may be difficult to disambiguate between these scenarios. However, if we had heard Bob asking Alice where the book was before, we would confidently infer that Alice wanted to hinder Bob. Such multi-modal, multi-agent Theory of Mind abilities are not only crucial for humans but also for AI systems that are deployed in human living environments, such as assistive robots. Without a robust understanding of people's mental states in complex social interactions, AI systems may cause detrimental errors in their interactions with people.\nDespite the recent advances in evaluating and engineering machine Theory of Mind, prior works have not adequately addressed the challenge of Theory of Mind reasoning in multi-modal social interactions. First, common Theory of Mind benchmarks (Gordon 2016b; Gandhi et al. 2021; Shu et al. 2021; Kosinski 2023; Jin et al. 2024) have only focused on individuals' mental states. However, there are other important aspects of multi-agent mental reasoning, including social goals (e.g., helping, hindering) and beliefs about others' goals. Second, there has not been a multi-modal social interaction dataset designed for systematic Theory of Mind reasoning evaluation. The only prior multi-modal Theory of Mind dataset is MMTOM-QA, which solely focuses on single-agent activities. Text-only benchmarks such as Hi-TOM (Wu et al. 2023) feature multi-agent events, but lack visual inputs. Thus, it remains unclear how we can evaluate the multi-modal multi-agent Theory of Mind capacity in machine learning models.\nTo address these challenges, we introduce a new Theory of Mind benchmark, MuMA-TOM (Multi-modal Multi-Agent Theory of Mind benchmark). MuMA-TOM includes a large set of question-answering trials. As summarized in Figure 1, questions in MuMA-TOM are organized into three categories: (1) belief inference, (2) social goal inference, and"}, {"title": "Related Works", "content": "Theory of Mind Benchmarks. Single-agent ToM benchmarks (Gordon 2016b; Gandhi et al. 2021; Shu et al. 2021; Kosinski 2023; Jin et al. 2024) have extensively tested concepts like belief, goal, preferences, constraints, and rationality. Multi-agent benchmarks are typically built based on the classic Sally-Anne test (Baron-Cohen, Leslie, and Frith 1985) for false beliefs and higher-order beliefs (Le, Boureau, and Nickel 2019; He et al. 2023; Xu et al. 2024; Soubki et al. 2024). There have also been multi-agent benchmarks that focus on a single agent's beliefs & intentions in complex conversations or interactions (Kim et al. 2023; Chen et al. 2024a; Chan et al. 2024; Sabour et al. 2024). In these benchmarks, other agents are usually present to add context or complexity, but there are no questions about inter-agent relationships. Prior works on testing social relationship understanding (Netanyahu et al. 2021a; Li et al. 2024a) rely on simple animations, which lack the realism of embodied human interactions. Most existing ToM benchmarks have only either text or video. The only exception is MMTOM-QA (Jin et al. 2024), which has single-agent activities depicted in video and text. Our MuMA-TOM benchmark features two agents interacting in an embodied household environment, with both text and video as multi-modal inputs, and includes questions that test the agents' social intentions and their reasoning about each other's mental states.\nMulti-Modal Benchmarks. Given the recent advances in LLMs, there has been increasing interest in developing multi-modal QA benchmarks. Most of these benchmarks focus on models' ability to fuse information from multiple modalities, where answers are directly retrievable without complex reasoning (Li et al. 2023b; Sanders et al. 2023; Li et al. 2023a; Ying et al. 2024a; Tang et al. 2024; Pandya et al. 2024). A recent benchmark, Perception Test (Patraucean et al. 2024), evaluates physical reasoning such as predicting world states and explaining counterfactual facts. But it differs from ToM reasoning. Pipelines for generating multi-modal datasets, SEED-story (Yang et al. 2024) and TaskMeAnything (Zhang et al. 2024), also do not evaluate ToM reasoning. MMTOM-QA (Jin et al. 2024), a recent multi-modal ToM benchmark, evaluates ToM with multi-modal inputs about single-agent behaviors. Unlike MMTOM-QA, our benchmark includes multi-agent interactions and evaluates models' understanding of mental state reasoning in multi-modal social interactions."}, {"title": "Machine Theory of Mind", "content": "Traditional approaches to Theory of Mind reasoning fall into two categories: end-to-end training (Rabinowitz et al. 2018; Han and Gmytrasiewicz 2019) and Bayesian Inverse Planning (Baker et al. 2017; Zhi-Xuan et al. 2020; Stacy et al. 2024). There have been works on neural amortized inference that combine these two methods for efficient and robust ToM inference in visual domains (Jha et al. 2024; Puig et al. 2023). Recently, LLMs demonstrated some ToM reasoning capabilities (Kosinski 2023; Bubeck et al. 2023), but their ToM reasoning is still brittle (Verma, Bhambri, and Kambhampati 2024; Amirizaniani et al. 2024; Ullman 2023; Sclar et al. 2023a; Ivanova et al. 2024). Approaches using prompt engineering have been proposed to enhance the ToM capacities in LLMs for text-based QAs (Wilf et al. 2023; Sclar et al. 2023b). Jin et al. (2024) proposed, BIP-ALM, for multi-modal TOM. BIP-ALM first extracts and fuses symbolic representations from multi-modal inputs and then combines a language model and Bayesian inverse planning to conduct ToM reasoning based on the symbolic representations. While achieving promising results on MMTOM-QA, BIP-ALM lacks multi-agent reasoning capacity and requires finetuning a language model on hand-designed symbols. Our LIMP model builds on BIP-ALM and introduces key improvements including multi-agent planning and general, domain-invariant representations."}, {"title": "MuMA-TOM Benchmark", "content": "General Structure\nThe benchmark consists of 225 multi-modal social interactions between two agents. There are 900 multi-choice questions based on these social interactions. Each question depicts a social interaction in video and text jointly. As shown in Figure 1, the text may show a conversation between the agents or a part of the event, and the video shows the complementary part of the event. Given the multi-modal inputs, the questions are designed to assess the understanding of agents' mental states during these interactions, probing three main concepts: (1) beliefs, (2) social goals, and (3) beliefs of others' goals. Each concept has 300 questions. We also created a training set consisting of 1,030 videos annotated with the agents' actions and goals. The training set does not provide example questions. It is intended for a model to learn about typical multi-agent household activities.\nQuestion Types\nAs identified in prior works in cognitive science (Ullman et al. 2009; Shu et al. 2020) and multi-agent planning (Gmytrasiewicz and Doshi 2005; Tejwani et al. 2021), there are three mental variables that are crucial to ToM reasoning in multi-agent interactions: an agent's belief of the physical state, its social goal, and its belief of other agents' goals. Therefore, we design three types of questions in our benchmark corresponding to the three mental variables: belief inference, social goal inference, and belief of goal inference. Each type of question asks about the corresponding mental"}, {"title": "Our Model", "content": "Formulation\nTo model social interactions between two agents, i and j, and the recursive mental reasoning between them, we adopt an Interactive Partially Observable Markov Decision Processes (I-POMDP) formulation (Gmytrasiewicz and Doshi 2005). We define $s_t$ as the state, $a_t^i$ and $a_t^j$ as agents' actions, and $u_t^i$ and $u_t^j$ as agents' utterances at time t. Each agent maintains its own beliefs $b_t^i$ and $b_t^j$, as well as goals $g_i$ and $g_j$. To capture recursive reasoning, we define interactive states for the agents, denoted as $is_{i,l}$ and $is_{j,l}$ at level l. From the perspective of agent i, its interactive state at each level is defined as follows (we consider the first two levels in this work):\n\u2022 Level 0: $is_{i,0} = s$\n\u2022 Level 1: $is_{i,1} = (s, b_{j,0}, g_j)$ (where $b_{j,0}$ is a distribution over agent j's level 0 interactive state, $is_{j,0}$)\n\u2022\nGiven the belief of interactive state $b(is_{i,1})$, an agent's action policy will be $\\pi(a_i|is_{i,1}, g_i)$, and its utterance policy will be $\\pi(u_i|is_{i,1}, g_i)$.\nOverview\nPrevious works on Inverse Multi-agent Planning (IMP) (Ullman et al. 2009; Netanyahu et al. 2021a) have demonstrated that IMP can robustly infer agents' mental states in social interactions. However, these methods rely on manually crafted planners and are limited to simple visual scenarios, such as 2D grid worlds. Jin et al. (2024) introduced the BIP-ALM model, which leverages language models for inverse planning to achieve single-agent Theory of Mind reasoning in complex, realistic settings. Inspired by BIP-ALM, we propose a novel method, Language model-based Inverse Multi-agent Planning (LIMP), to combine IMP and language models for robust multi-agent Theory of Mind reasoning based on multi-modal inputs.\nAs illustrated in Figure 2, LIMP consists of three key components: multi-modal information fusion, hypothesis parsing, and inverse multi-agent planning. Compared to BIP-ALM, our approach offers several improvements. First, while BIP-ALM is limited to single-agent scenarios, LIMP identifies three mental variables crucial to understanding multi-agent interactions-belief, social goal, and belief of goal. We then implement multi-agent planning based on these variables to reason about multi-modal social interactions. Second, BIP-ALM relies on hand-designed symbolic representations and requires finetuning language models on"}, {"title": "Multi-modal Information Fusion", "content": "We use a vision-language model (VLM) to extract the actions and utterances of each person depicted in the video. Given text, we use an LLM to extract the actions and utterances of each person. We then fuse the extracted information to form the initial state and the complete sequences of actions and utterances using an LLM as follows.\nUnlike MMTOM-QA, our benchmark does not provide a text description of the full state, as such descriptions are rarely provided in real-world applications. However, as objects may be occluded or too small to detect even for humans, inferring the state directly from the RGB videos could be difficult. Instead, we prompt an LLM with the inferred actions and utterances of both agents to infer the part of the initial state relevant to the activity. For example, if Alice grabs a carrot from the fridge, and moves it to the kitchen table, we can infer that the carrot was originally in the fridge. Using this method, the reconstructed initial state will only consider objects relevant to human actions and utterances. This simplifies the context and can consequently improve the accuracy of the inference. Given the initial state and the action sequences, we can infer the state at each step.\nThere is often missing information in the visual perception results. For instance, as shown in Figure 3, the VLM did not recognize the object the person grabbed and produced an ambiguous action \u2013 \u201cgrabs some object.\u201d This is also common to people, as the object picked up by the person is often occluded. However, we can still infer that the object is likely juice based on the context provided in the text. To emulate such ability, we leverage an LLM to fuse information extracted from video and text, which infers the information missing from visual perception based on the complementary information described in the text.\nIn this work, we use Gemini 1.5 Pro for the VLM and GPT-40 for the LLM as they produce the best results."}, {"title": "Hypothesis Parsing", "content": "To answer the question about a person's mental state in a social interaction, LIMP will parse relevant hypotheses of all mental variables of that person (agent i) \u2013 belief of state b(s), social goal $g_i$, and belief of other agent's goal b($g_j$). For this, we prompt GPT-40 with the initial state and question text to generate a reasonable hypothesis of the three mental variables for each option, H = (b(s), $g_i$,b($g_j$))."}, {"title": "Inverse Multi-Agent Planning", "content": "Given the fused information from multi-modal inputs and the parsed hypotheses, inverse multi-agent planning conducts Bayesian inference over a person's mental state by evaluating the likelihood of actions and utterances given each hypothesis. Following the I-POMDP formulation, we define this probabilistic inference as follows:\n$P(H|a_{0:T}^i, u_{0:T}^i, a_{0:T}^j, u_{0:T}^j, s^0)$\n$\\propto P(H) \\prod_{t=1}^T \\pi (a_t^i | a_{0:t-1}^i, u_{0:t-1}^i, a_{0:t-1}^j, u_{0:t-1}^j, s^0, H)$\n$\\prod_{t=1}^T \\pi (u_t^i | a_{0:t-1}^i, u_{0:t-1}^i, a_{0:t-1}^j, u_{0:t-1}^j, s^0, H)$\nwhere the action policy $\\pi$ and the utterance policy $\\pi$ can be estimated by the log probabilities of the prompt completion by a language model for each time step t. Note that in the standard policy definitions in I-POMDP, we need agent i's belief of agent j's belief of the state at each step. This, however, is difficult to explicitly estimate. Instead, in this work, we consider past actions and utterances of all agents as part of the condition of the policies to avoid the explicit belief of belief inference. We prompt an LLM with the hypothesis, the initial state, and the previous actions and utterances of both agents to estimate the action and utterance policies. In this way, we do not need to implement domain-specific planning, which can be extremely challenging and slow for multi-agent interactions with both physical actions and verbal communication. In this work, we use GPT-40 for the LLM. We find that GPT-40 can accurately estimate the action and utterance policies based on the given condition.\nFigure 4 illustrates how IMP evaluates the action and utterance likelihood at one time step. Given the condition, the LLM estimates that it is likely that agent i will take the observed action (\u201cwalk towards table\u201d) but is unlikely to say \u201cI found a potato inside fridge\u201d as it is inconsistent with the social goal of hindering agent j (agent i had just put a potato in the fridge before the conversation)."}, {"title": "Experiments", "content": "Human Experiment\nWe recruited 18 participants (mean age = 36.0; 10 female) from Prolific to answer 90 questions randomly sampled from the benchmark. Each question received responses from 3 participants. The experiment was approved by an institutional review board.\nBaselines\nWe evaluated our benchmark on state-of-the-art LMMs. For models capable of processing video input, the entire video was provided. For models without video input capabilities, we uniformly sample one frame every 20 frames from the video episode as input. We evaluated GPT-40 (OpenAI 2023), Llava 1.6 (Liu et al. 2023), Gemini 1.5 (Reid et al. 2024), InternVL2 (Chen et al. 2023) and VideoLlama 2 (Cheng et al. 2024). We evaluated the latest version of each LMM at the time of submission. For LIMP, we use Gemini 1.5 Pro as the VLM and GPT-40 as the LLM. Finally, we evaluated BIP-ALM with finetuned Llama 2 (Jin et al. 2024), the best-performing model on a prior multi-modal ToM benchmark, MMTOM-QA. More details of the experiments are provided in the supplementary material.\nResults\nWe report the human and model performance in Figure 5 and Table 3. Human participants achieved almost perfect accuracy across all questions, with 98.9% of the correct answers having majority agreement. The overall performance averaged across individual participants is 93.5%. The slightly lower performance on social goal inference (94.4%) and belief of goal inference (87.1%) indicates these questions are"}, {"title": "Discussion", "content": "Why do LMMs perform poorly? There are two sources of systematic errors for LMMs. First, LMMs struggle with understanding multi-modal behavior in complex social situations, often failing to distinguish between deliberate hindering and failed attempts to help due to incorrect beliefs. Most models can solve belief inference tasks where helping is the assumed social goal. However, they consistently struggle in scenarios where hindering is the assumed social goal (e.g., \"If Mary is trying to hinder Jack, where does she least likely believe...?). Except for Llava 1.6 34B (87.2%) and Gemini 1.5 Pro (62.2%), all models perform at or below the level of random guessing in these cases, with Intern-VL 2 26B (18.6%) performing much worse than random chance. The failure to understand adversarial behaviors is even more prominent in social goal inference and belief of goal inference. For instance, if Sarah tells John there is milk in the fridge, but there isn't any milk, and Sarah knows this, she is hindering John. Second, LMMs often fail to correctly interpret visual inputs, such as when an object is too small or is occluded when the agent is picking it up, leading to incorrect conclusions about the agent's actions. While humans are able to use contextual clues to infer what the object might be, VLMs struggle with this task. These errors in recognizing crucial actions likely contribute significantly to their overall poor performance on our benchmark.\nWhy does LIMP outperform the best LMMS? LIMP overcomes the two aforementioned weaknesses of LMMs\u2014the inability to recognize multi-modal behavior under various social goals and the sensitivity to noisy visual perception. First, while LLMs struggle with direct ToM reasoning, they excel at the forward generation of multi-modal behavior given mental states. For example, it is much harder for an LLM to correctly infer whether an agent is hindering another by lying than it is for the model to generate a lie based on the agent's belief and social goal. Such multi-modal behavior generation ability enables LIMP to estimate the action and utterance likelihood, identifying the key actions and/or utterances that reveal the true mental state of an agent (as shown in Figure 4). Second, when a VLM fails to recognize the exact object that the agent is interacting with, LIMP can fill in this missing information with context from the text input. We evaluated the action accuracy by using semantic similarity and found that this approach increases inferred action accuracy from 54.4% to 86.6%. As a result, LIMP is able to perform inference on much more accurate information.\nHow general is LIMP? Prior inverse planning models, including BIP-ALM, all require handcrafted representations for specific domains. LIMP, however, represents all information using natural language, which enables the direct use of any pretrained LLMs and VLMs without domain-specific knowledge or finetuning. By utilizing powerful pretrained VLMs for visual perception, LIMP can directly recognize actions from RGB videos in an open-ended way, without specifying target action labels for a domain. LIMP also leverages an LLM to use contextual clues from the text to fill in missing information from visual perception, providing a general method for multi-modal information fusion. One can also easily upgrade LIMP by plugging in any state-of-the-art VLMs and LLMs.\nWhat are the limitations of LIMP? Hallucinations created by the VLM can cause significant errors in LIMP. For example, an agent may only open and close the fridge, but the VLM may mistakenly think that the agent also grabs something from the fridge. Such hallucinations in action recognition can not be corrected by the textural context. As a result, LIMP will incorrectly interpret the agent's behavior. Additionally, LIMP does not explicitly infer an agent's belief of another agent's belief. It instead prompts an LLM with past actions and utterances to implicitly infer that, which can become costly for longer events. LIMP also does not perform recursive reasoning for more than two levels.\nWhat are the limitations of our benchmark? The scenarios in our benchmark are currently limited to interactions between two agents in household settings, where there are three social goals: helping, hindering, and acting independently. Moreover, the current benchmark has synthetic human activities. These synthetic activities are realistic as verified in prior work (Puig et al. 2020) and enable large-scale testing. However sim-to-real evaluation could be valuable for future studies."}, {"title": "Conclusion", "content": "We present the first multi-modal Theory of Mind benchmark for multi-agent interactions in complex embodied set-"}, {"title": "Appendix", "content": "Comparison of ToM Benchmarks\nTable 2 provides a comparison between our MuMA-TOM benchmark and prior ToM benchmarks, highlighting key features such as the size of the test set, input modalities, and evaluation metrics. Our benchmark stands out as the only benchmark with multi-modal inputs and multi-agent interactions. It simultaneously evaluates multi-agent social interactions with belief, goal, and belief of other agents' goals, as well as the ability to infer mental states from multi-modal inputs.\nMUMA-TOM Benchmark Details\nMore Quantitative Results\nThe results of all experiments conducted in our study are shown in Table 3.\nChain of Thought Prompting. We evaluate state-of-the-art models' performance on our dataset with zero-shot chain of thought (CoT) prompting, as introduced by (Kojima et al. 2022). We add the phrase \"Let's think step by step\" after the question prompt but before the list of options.\nFor all models tested, using CoT prompting showed no significant improvement in performance. In fact, for many models, using CoT prompting caused a decrease in performance. While there are instances where CoT led to some improvement, such as in belief inference for InternVL 2 26B, the overall impact effect was negligible on more challenging social goal and belief of goal inference questions. These results further highlight the current limitations of state-of-the-art LMMs. Even with CoT guidance, they struggle to effectively understand social interactions.\nFinetuned Baseline We finetuned the VideoLlama 2 7B model on our training set for action captioning tasks following (Zhang, Li, and Bing 2023), using two A100 GPUs for 1 epoch, with a learning rate of 2e-5 and a batch size of 4. The performance of the model was lower after finetuning, suggesting that the model may have inherent limitations in ToM reasoning or action recognition. We experimented with finetuning for up to 3 epochs and found that extending finetuning beyond one epoch leads to over-fitting, and the model was unable to answer the questions with A, B, or C.\nAdvanced Prompting for ToM. Recent works have leveraged language models to tackle ToM problems through multi-step reasoning approaches (Wilf et al. 2023; Sclar et al. 2023c; Hou et al. 2024). Among these text-only models, we chose to evaluate SimToM, as the code for the other models was either unavailable or required extensive modifications to integrate with our benchmark. Since SimToM only accepts textual input, we adapted it to our dataset by adding Gemini 1.5 Pro's visual extraction results after the textual input as input for SimToM and tested it with GPT-4o serving as the primary language model. SimToM, which analyzes the perspective of each agent to assist the language model, achieved the highest accuracy in belief-of-goal questions among all the baselines tested. This suggests that a multi-step approach can improve a language model's capacity for ToM reasoning. However, the overall accuracy is still below 50%.\nLIMP w/ Llama 3.1 8B for Inverse Multi-agent Planning Solving ToM problems with language models usually requires some form of finetuning or few-shot prompting to equip the model with domain-specific knowledge. In contrast, LIMP leverages the forward planning capabilities of language models to address the inverse planning problem without any finetuning or additional domain knowledge. Beyond testing very large models like GPT-40, we also explored the potential of smaller models, such as Llama 3.1 8B, as an inverse planner for LIMP. However, the results indicate that smaller models lack the ability to effectively function as inverse planners for multi-agent actions. A closer qualitative examination of Llama 8B's failure patterns shows that the model is unable to understand the concept of hindering, which leads to poor performance across all questions related to hindering.\nQualitative Results\nWe provide two examples where Gemini 1.5 Pro, the best-performing LMM on the MuMA-ToM benchmark, fails while LIMP succeeds, highlighting the challenges state-of-the-art LMMs face on our benchmark. We also provide an example where hallucinations lead to LIMP also failing to solve the problem."}]}