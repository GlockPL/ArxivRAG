{"title": "Get Large Language Models Ready to Speak: A Late-fusion Approach for Speech Generation", "authors": ["Maohao Shen", "Shun Zhang", "Jilong Wu", "Zhiping Xiu", "Ehab AlBadawy", "Yiting Lu", "Mike Seltzer", "Qing He"], "abstract": "Large language models (LLMs) have revolutionized natural language processing (NLP) with impressive performance across various text-based tasks. However, the extension of text-dominant LLMs to with speech generation tasks remains under-explored. In this work, we introduce a text-to-speech (TTS) system powered by a fine-tuned Llama model, named TTS-Llama, that achieves state-of-the-art speech synthesis perfor-mance. Building on TTS-Llama, we further propose MoLE-Llama, a text-and-speech multimodal LLM developed through purely late-fusion parameter-efficient fine-tuning (PEFT) and a mixture-of-expert architecture. Extensive empirical results demonstrate MoLE-Llama's competitive performance on both text-only question-answering (QA) and TTS tasks, mitigating catastrophic forgetting issue in either modality. Finally, we further explore MoLE-Llama in text-in-speech-out QA tasks, demonstrating its great potential as a multimodal dialog system capable of speech generation.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) [1], [2] have achieved remarkable success across various natural language process-ing (NLP) tasks, such as question answering [3], machine translation [4], and commonsense reasoning [5]. More recent progress has emerged to extend LLMs to various modalities beyond text [6]\u2013[8], among which the speech output modality is of great interest.\nThe development of LLMs (or general artificial intelli-gent systems) with speech generation capability is closely related to the classic research area in text-to-speech (TTS) systems. Leveraging discrete speech representations [9], recent approaches [10]\u2013[12] reformulate TTS based on a language modeling task akin to LLMs. However, these methods typi-cally focus solely on the TTS task and require training a speech language model from scratch, which tend to be demanding on both computation and speech data.\nAnother recent line of research is to integrate speech modality into LLMs pretrained only on text. This leverages the prowess of LLMs in text-based tasks and the premise that speech and text tasks potentially share synergy. While most existing efforts focus on extending pretrained text-based LLMs to handle speech input, such as speech understanding tasks [13]\u2013[15], enabling LLMs in speech generation remains a challenging and under-explored area [16].\nIn this work, we aim to endow text-based LLMs with speech generation capabilities through purely parameter-efficient fine-tuning (PEFT) [17] in lieu of full pretraining or fine-tuning. Moreover, we demonstrate the effectiveness of a mixture-of-experts architecture in both text and speech generation without compromising either modality. The three main contributions of this work are summarized as follows.\n1) In Section III, we present TTS-Llama, a TTS system based on a Llama 3-8B-Instruct model fined-tuned with LORA. To the best of our knowledge, TTS-Llama is the first TTS system to achieve state-of-the-art performance by only PEFT fine-tuning a text-based LLM.\n2) In Section IV, we introduce MoLE-Llama, a text-speech multimodal LLM capable of text-only question answer-ing (QA) and TTS synthesis, as well as (with a chain-of-modality technique) text-in-speech-out QA. To the best of our knowledge, MoLE-Llama is the first text-speech multimodal LLM achieved through purely late fusion, specifically PEFT fine tuning as opposed to additional pretraining or full fine-tuning.\n3) In Section V, we demonstrate that MoLE-Llama retains text capabilities of the original text-only LLMs after introducing the speech output modality, addressing a common concern in existing multimodal LLMs."}, {"title": "II. RELATED WORK", "content": "Traditional TTS systems [18]\u2013[20] typically model speech generation as a continuous signal regression task of generating mel spectrograms that are then synthesized into speech wave-forms by a vocoder. More recent research [10]\u2013[12], [21] have adopted a language modeling architecture and discrete speech token representation. For instance, [10] and [11] introduce neural codec language models to generate discrete acoustic tokens encoded from Residual Vector Quantization (RVQ)-based models [9]. For speech synthesis, [21] explores the use of two types of speech tokens, i.e., high-level semantic tokens and low-level acoustic tokens. Followed by this paradigm, [12] adapts the two types of speech tokens for TTS, demonstrating enhanced naturalness and acoustic quality. [22] further address both speech editing and TTS tasks.\nHowever, most of the aforementioned methods train a speech language model from scratch and focus solely on TTS. Augmenting pretrained text LLMs with speech genera-tion capability remains largely under-explored. Recently, [16]\nText-to-Speech\nA. Text-to-Speech\nB. Text-Speech Multimodal Language Model"}, {"title": "III. TTS-LLAMA: A TTS SYSTEM POWERED BY FINE-TUNED TEXT LLAMA MODEL", "content": "To achieve high stability and naturalness in TTS, our proposed TTS-Llama model tackles the TTS task through a two-step speech token generation process. First, the fine-tuned Llama model processes raw text input to generate high-level semantic tokens that contains both semantic and prosody\ninformation. Then, an acoustic language model (LM) translates these semantic tokens into low-level acoustic features. Finally, a neural vocoder synthesizes the audio waveform from these acoustic features. To construct the training targets for the Llama model and the acoustic LM, we use two tokenizers to extract semantic and acoustic information, projecting them into a quantized latent space to produce discrete tokens. The overall design of TTS-Llama is illustrated in Figure 1, and we describe each model component below.\na) Fine-tuned Llama Model: The Llama model is the core engine of the proposed TTS system, as the semantic tokens carry rich information that directly impacts audio generation. Leveraging a pretrained Llama model rather than training a model from scratch offers two key benefits. First, it is computationally efficient by leveraging Llama model's prior knowledge in understanding text. Second, incorporating a pretrained Llama model allows integration into tasks beyond speech (e.g., text and vision tasks), towards the develop-ment of a multimodal system. The fine-tuning procedure is straightforward. Specifically, we expand the vocabulary and prediction head of the pre-trained Llama3-8B-Instruct model [1] to accommodate the generation of semantic tokens. We then fine-tune the Llama model using PEFT approach LoRA [17]. The trainable components include the input embedding layers, the prediction head, and the injected LoRA adapters.\nb) Tokenizers: The semantic tokenizer provides a high-level discretized representation of audio that removes low-level redundant acoustic details while preserving sufficient informa-tion to accurately reconstruct the original audio. Our semantic tokenizer design is based on [33], utilizing a quantizer [34] to extract 4,096 discrete semantic tokens. The acoustic tokenizer is implemented as a convolutional autoencoder. It employs a residual vector quantizer (RVQ) with a multi-codebook bottleneck [9] to extract discrete acoustic tokens.\nc) Acoustic LM and Vocoder: The acoustic LM is an autoregressive transformer-based decoder inspired by Music-Gen [35]. Unlike the text conditioning used in [35], our acoustic model is conditioned on semantic tokens, which it uses as inputs to generate multi-codebook acoustic tokens. The acoustic LM is a lightweight model compared to Llama with fewer than 0.2B parameters. Finally, we re-utilize the decoder from the acoustic tokenizer [34] as the vocoder to convert the sequence of acoustic tokens back into a waveform."}, {"title": "IV. MOLE-LLAMA: A TEXT-SPEECH MULTIMODAL LLM VIA MIXTURE-OF-LORA EXPERTS", "content": "While TTS-Llama explores the potential of PEFT fine-tuning pretrained text LLM for the TTS task, to produce a genuinely text-speech multimodal LLM requires retaining the original text capabilities. However, the MMLU evaluation of TTS-Llama shown in Table III shows catastrophic forgetting in the text QA performance.\nTo effectively address this issue, we propose a late-fusion text-speech multimodal LLM, dubbed as MoLE-Llama, that utilizes a mixture-of-LoRA experts technique [36]. The key idea is to use text and speech experts trained separately to\nhandle tasks in their respective modalities, and to eventually merge these dedicated experts into a unified multimodal LLM\nA. Three-stage Training Procedure\nWe propose a three-stage procedure for training MoLE-Llama, illustrated in Figure 2 and detailed as follows.\na) Stage-1: TTS Expert Fine-tuning: In stage-1, we fine-tune the text Llama model following the same approach as TTS-Llama, as outlined in Section III. This stage aims to inject speech modalities into the text Llama model, allowing it to process both text and semantic tokens while aligning the two modalities within the same embedding space.\nb) Stage-2: Text Expert Fine-tuning: Since stage-1 may negatively impact the Llama model's text capabilities, stage 2 is designed to restore these text capabilities after speech modality injection. Specifically, we freeze the re-trained input embedding and prediction head from stage-1 and continuously fine-tune the LoRA adapter on the text QA task.\nc) Stage-3: Mixture-of-LoRA Experts: In stage-3, we unify the TTS expert and text expert into a single multimodal LLM. When given an input, MoLE-Llama intelligently selects the modality-aware expert to handle the task. In this stage, we only train the mixture-of-LoRA experts router [36], keeping all other parameters frozen. The entire system is trained end-to-end using a combination of text QA and TTS data, allowing the router to flexibly adapt to both tasks.\nB. One Step Towards Speech QA\nWhile the primary focus of this work is the TTS task, we demonstrate that MoLE-Llama has the potential to achieve the more ambitious goal of responding to user requests in both text and speech modalities. To take a step towards this goal, we further explore the text-in-speech-out speech QA task using the chain-of-modality instruct-tuning technique [29]. In stage-2, we further create a speech QA expert by fine-tuning the Llama model to generate a text tokens followed by its semantic tokens. In stage-3, we integrate the text QA and speech QA experts into a unified multimodal LLM."}, {"title": "V. EXPERIMENTS", "content": "a) Datasets: Our speech training data consists of a mixture of open-source and in-house speech data, with the ma-jority of the open-source data sourced from LibriHeavy [38], totaling 50K hours of speech paired with text transcripts. To improve the model's TTS performance on the text normal-ization (TN) task, we augment the main training dataset with about 60K internally curated sentences that are rich in various written-form text that requires text normalization sentences, as\nA. Settings\nB. Results"}, {"title": "VI. CONCLUDING REMARKS", "content": "This work introduces two novel models. TTS-Llama demon-strates the effectiveness of enabling TTS generation via only PEFT fine-tuning of a text-based LLM. Building on TTS-Llama, MoLE-Llama further combines both text-QA and TTS capabilities using mixture of LoRA experts without catas-trophic forgetting in either modality, highlighting the potential of late fusion in enabling text-speech multimodal LLMs. A natural extension of this work is to close the gap between the text-QA expert and the original text-only LLM. Additionally, the speech-output QA capability could be further enhanced instead of relying on the \u201cchain-of-modality\u201d technique."}]}