{"title": "TAG: A Decentralized Framework for Multi-Agent Hierarchical Reinforcement Learning", "authors": ["Giuseppe Paolo", "Abdelhakim Benechehab", "Hamza Cherkaoui", "Albert Thomas", "Bal\u00e1zs K\u00e9gl"], "abstract": "Hierarchical organization is fundamental to biological systems and human societies, yet artificial intelligence systems often rely on monolithic architectures that limit adaptability and scalability. Current hierarchical reinforcement learning (HRL) approaches typically restrict hierarchies to two levels or require centralized training, which limits their practical applicability. We introduce TAME Agent Framework (TAG), a framework for constructing fully decentralized hierarchical multi-agent systems. TAG enables hierarchies of arbitrary depth through a novel LevelEnv concept, which abstracts each hierarchy level as the environment for the agents above it. This approach standardizes information flow between levels while preserving loose coupling, allowing for seamless integration of diverse agent types. We demonstrate the effectiveness of TAG by implementing hierarchical architectures that combine different RL agents across multiple levels, achieving improved performance over classical multi-agent RL baselines on standard benchmarks. Our results show that decentralized hierarchical organization enhances both learning speed and final performance, positioning TAG as a promising direction for scalable multi-agent systems.", "sections": [{"title": "1. Introduction", "content": "Human societies are organized as hierarchical networks of agents, ranging from organizational structures (junior employees \u2192 middle managers \u2192 CEO) to ontological relationships (individuals \u2192 families \u2192 nations). This hierarchical organization facilitates complex coordination by decomposing problems across multiple scales while ensuring robustness through localized failure handling. As proposed in the TAME approach (Levin, 2022), biological systems also function as hierarchical networks of agents, where higher-level agents coordinate lower-level ones. Each level exhibits varying degrees of cognitive sophistication, corresponding to the scale of the goals it can pursue. From single cells managing basic homeostasis to tissues coordinating morphogenesis to brains overseeing complex behaviors, each level builds upon and integrates the intelligence of its components to achieve increasingly sophisticated cognitive capabilities. However, implementing similar hierarchical structures in artificial systems presents several key challenges: (1) coordinating information flow between levels without centralized control, (2) enabling efficient learning despite the non-stationarity introduced by the simultaneous adaptation of agents at multiple levels, and (3) maintaining scalability as the depth of the hierarchy increases.\nFormally, we consider the challenge of learning in multi-agent systems where N agents must collaborate to solve complex tasks, each maximizing their own expected returns. In this setting, each agent receives its own reward. As N increases, the joint action and state spaces grow exponentially, rendering centralized approaches intractable. Moreover, agents must learn to coordinate across different temporal and spatial scales, ranging from immediate reactive behaviors to long-term strategic planning.\nCurrent AI systems predominantly rely on monolithic architectures that limit their adaptability and scalability in addressing these challenges. This is evident in large language models (LLMs) and traditional reinforcement learning (RL) approaches where agents are typically defined as single, end-to-end trainable instances. Such monolithic designs present several limitations: they require complete retraining when conditions change, lack the natural compositionality of hierarchical systems, and scale poorly with increasing task complexity. Traditional multi-agent approaches based on centralized training with decentralized execution or two-level hierarchies with manager/worker structures struggle in such situations due to the high dimensionality of the states, limiting their applicability to small number of agents. At the same time, strategies consisting of independent learners with communication protocols are less afflicted by this, but suffer from possible communication overhead.\nOur key insight is that biological systems address similar coordination challenges through flexible, multi-scale hierarchical organization. We propose that future intelligent systems should be structured more like societies of agents than as monolithic entities. Our long-term goal is to build agents that resemble hierarchical and dynamic networks of sub-agents, rather than static structures. In this work, we take the first step in that direction with the introduction of the TAME Agent Framework (TAG), which draws inspiration from TAME's biological insights (Levin, 2022) to create a hierarchical multi-agent RL framework that enables the construction of arbitrarily deep agent hierarchies. The core innovation of TAG is the LevelEnv abstraction, which facilitates the construction of multi-level multi-agent systems. Through this abstraction, each agent in the hierarchy interacts with the level below as if it were its environment-observing it through state representations, influencing it through actions, and receiving rewards based on the lower level's performance. The resulting system consists of multiple horizontal levels, as shown in Fig. 1, each containing one or more sub-agents, loosely connected to both their upper-level counterparts and their lower-level components. This structure reduces communication overhead and state space size by connecting agents locally within the hierarchy.\nTAG introduces several key innovations:\n1.  A LevelEnv abstraction that standardizes information flow between levels while preserving agent autonomy, by presenting each level of the hierarchy as the environment to the level above;\n2.  A flexible communication protocol that enables coordination without requiring centralized control;\n3.  Support for heterogeneous agents across levels, allowing different learning algorithms to be deployed where most appropriate.\nThis approach enables more efficient learning by naturally decomposing tasks across multiple scales while maintaining scalability through loose coupling between levels. We demonstrate the effectiveness of TAG through empirical validation on standard multi-agent reinforcement learning (MARL) benchmarks, where we instantiate multiple two- and three-level hierarchies. The experiments show improved sample efficiency and final performance compared to both flat and shallow multi-agent baselines.\nIn the following sections, we first review related work in both MARL (Sec.2.1) and HRL (Sec.2.2). We then present the TAG framework, including our key LevelEnv abstraction, in Sec.3. Sec.5 provides empirical validation on standard benchmarks for multiple instantiations of agents. We conclude with a discussion of implications and future directions in Sec.6 and Sec.7."}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Multi-Agent Reinforcement Learning", "content": "Research in multi-agent systems has gained significant attention in recent years (Nguyen et al., 2020; Oroojlooy & Hajinezhad, 2023). Leibo et al. (2019) proposed that innovation in intelligent systems emerges through social interactions via autocurricula\u2014naturally occurring sequences of challenges resulting from competition and cooperation between adaptive units, which drive continuous innovation and learning. The authors argue that advancing intelligent systems requires a strong focus on multi-agent research.\nTo support this growing field, several benchmarks have emerged (Samvelyan et al., 2019; Hu et al., 2021; Bettini et al., 2024; Terry et al., 2021). Terry et al. (2021) introduced PettingZoo, which provides a standardized OpenAI Gym-like (Brockman, 2016) interface for multi-agent environments, while Bettini et al. (2024) introduced Bench-MARL, which addresses fragmentation and reproducibility challenges by offering comprehensive benchmarking tools and standardized baselines.\nMARL approaches can be broadly categorized into three main groups based on their coordination strategy:\n1.  Independent learners operate without inter-agent communication, with each agent maintaining its own learning algorithm and treating other agents as part of the environment. Common examples include IPPO (De Witt et al., 2020), IQL (Thorpe, 1997), and ISAC (Bettini et al., 2024), which are independent adaptations of their single-agent counterparts: PPO (Schulman et al., 2017), Q-Learning (Watkins & Dayan, 1992), and SAC (Haarnoja et al., 2018) respectively;\n2.  Parameter sharing approaches have agents share components like critics or value functions, as in MAPPO (Yu et al., 2022), MASAC (Bettini et al., 2024), and MADDPG (Lowe et al., 2017);\n3.  Communicating agents actively exchange information, either through consensus-based approaches (Cassano et al., 2020; Zhang et al., 2018) where agents must reach agreement over a communication network, or through learned communication protocols (Foerster et al., 2016; Jorge et al., 2016).\nFor a comprehensive taxonomy and review, we refer readers to Oroojlooy & Hajinezhad (2023).\nA significant challenge in MARL is the non-stationarity of the environment from each agent's perspective. As other agents learn and change their behaviors, the state transition dynamics also change. This impacts experience replay mechanisms, as stored experiences quickly become obsolete (Foerster et al., 2016). The dominant paradigm of centralized learning with decentralized execution (Oroojlooy & Hajinezhad, 2023) attempts to address these challenges through shared learning components. However, this approach constrains the architecture during training and limits applicability to lifelong learning scenarios."}, {"title": "2.2. Hierarchical Reinforcement Learning", "content": "Hierarchical organization is fundamental to intelligent behavior in nature. Human infants naturally decompose complex tasks into hierarchical goal structures (Spelke & Kinzler, 2007), enabling both temporal and behavioral abstractions. This hierarchical approach offers two key advantages: it improves credit assignment through abstraction-based value propagation and enables more semantically meaningful exploration through temporal and state abstraction (Hutsebaut-Buysse et al., 2022). Nachum et al. (2019) demonstrates that this enhanced exploration capability is one of the major benefits of hierarchical RL over flat RL approaches.\nThe foundational approaches to HRL focus on two-level architectures. The Options framework formalizes temporal abstraction through Semi-Markov Decision Processes (SMDPs), where temporally-extended actions (\"options\") consist of a policy, termination condition, and initiation set (Sutton et al., 1999). The framework supports concurrent option execution and allows for option interruption, providing flexibility beyond simple hierarchical structures. While options were initially predefined (Sutton et al., 1999), later work enabled learning them with fixed high-level policies (Silver & Ciosek, 2012; Mann & Mannor, 2014) or through end-to-end training, as in Option-Critic (Bacon et al., 2017).\nAn alternative approach, Feudal RL (Dayan & Hinton, 1992; Kumar et al., 2017; Vezhnevets et al., 2017), implements a manager-worker architecture where managers provide intrinsic goals to lower-level workers. This creates bidirectional information hiding-managers need not represent low-level details, while workers focus solely on their immediate intrinsic rewards without requiring access to high-level goals. These approaches face a common challenge: the non-stationarity of the lower level during learning complicates value estimation for the higher level.\nModel-based approaches attempt to address this-Xu & Fekri (2021) learn symbolic models for high-level planning, while Li et al. (2017) build on MAXQ's value function decomposition by breaking down the global MDP into task-specific local MDPs. However, these typically require hand-specified state abstractions or task decompositions. Recent work focuses on learning stability, with Luo et al. (2023) introducing attention-based reward shaping to guide exploration, and Hu et al. (2023) developing uncertainty-aware techniques to handle distribution shifts between levels.\nThe multi-agent setting introduces additional complexity, as hierarchical coordination must now handle both temporal and agent-to-agent dependencies. Tang et al. (2018) addresses this through temporal abstraction with specialized replay buffers to handle the resulting non-stationarity. Meanwhile, Zheng & Yu (2024) introduces hierarchical reward machines but require significant domain knowledge. The scarcity of work combining HRL and MARL highlights the challenges of stable learning with multiple sources of non-stationarity.\nOur approach, TAG, departs from traditional hierarchical frameworks by directly learning to shape lower-level observation spaces, rather than explicitly assigning goals like Feudal RL. This is directly inspired by the work of Levin (2022), which proposes that in biological systems, local environmental changes drive coordinated responses without central control. The closest approach to our work is FMH (Ahilan & Dayan, 2019), but in this work, the agent is limited to shallow two-depth hierarchies and has only top-bottom information flow in the form of goals. In contrast, TAG supports arbitrary-depth hierarchies without requiring explicit task specifications, and the communication across levels relies on bottom-up messages and top-down actions modifying the observations of the agents, rather than providing them goals. In this way, TAG offers a flexible solution for multi-agent coordination."}, {"title": "3. TAG Framework", "content": "The TAG framework addresses scenarios where multiple agents collaborate to maximize individual rewards over a Markov Decision Process (MDP), which we refer to as the real environment. Inspired by biological systems, as described in TAME (Levin, 2022), TAG implements a hierarchical multi-agent architecture where higher-level agents coordinate lower-level ones, each with varying cognitive sophistication matching their goal complexity. As shown in Fig. 1, at its core, TAG organizes agents into levels, where each level perceives and interacts only with the level directly below it. While agents at the lowest level operate directly in the real environment MDP, agents at higher levels perceive and interact with increasingly abstract representations of the system through the LevelEnv construct. This structure facilitates both horizontal (intra-level) and vertical (inter-level) coordination, allowing higher levels to maintain strategic oversight without requiring detailed knowledge of lower-level behaviors, while influencing lower levels through actions that modify their environmental observations.\nThe framework's key innovation is the LevelEnv abstraction, which transforms each hierarchical layer into an environment for the agents above it. This abstraction reshapes the original MDP into a series of coupled decision processes, with each level operating on its own temporal and spatial scale. Within this structure, agents optimize their individual rewards while contributing to the overall system performance through the hierarchical arrangement.\nTAG enables bidirectional information flow: feedback moves upward through the hierarchy via agent communications, while control flows downward through actions that shape lower-level observations. This design preserves modularity between levels while facilitating coordination and integrates heterogeneous agents whose capabilities match the complexity requirements of their respective levels."}, {"title": "3.1. Formal Framework Definition", "content": "A TAG-hierarchy consists of L ordered levels, with each level l containing $N_l$ parallel agents $[w_1^l,...,w_{N_l}^l]$. Within the hierarchy, each agent $w_i^l$ is connected to agents in the levels immediately above and below. We define $I_i^{l+1}$ and $I_i^{l-1}$ as the sets of indices of agents connected to $w_i^l$ from levels $l + 1$ and $l - 1$, respectively. Each agent $w_i^l$ is characterized by\n*   an observation space $O_i^l$ that aggregates messages from lower-level agents into a single observation: $o_i^l = \\phi_i^l([m_j^{l-1}]_{j \\in I_i^{l-1}});$\n*   an action space $A_i^l$ for influencing the observations of lower-level agents;\n*   a communication function $\\phi_i^l$ that generates upward-flowing messages and rewards based on observations, rewards, and internal states: $m_i^l, r_i^l = \\phi_i^l(o_i^{l-1}, r_i^{l-1});$\n*   a policy $\\pi_i^l$ that selects actions based on lower-level observations and higher-level actions: $a_i^l = \\pi_i^l(a_i^{l+1}, o_i^{l-1})$.\nThe reward structure reflects this hierarchical decomposition: while the lowest-level agents receive rewards directly from the real environment, higher-level agents ($w^l$) receive rewards computed by the communication function $\\phi_i^{l-1}$ of the agents in the levels below, based on their own performance. This creates a cascade of reward signals that aligns the objectives of the individual agents with the overall goal of the system, which is optimizing performance in the real environment. During training, each agent stores its experiences and updates its policy based on the received rewards, enabling the entire hierarchy to learn coordinated behavior.\nThe LevelEnv abstraction standardizes information exchange between levels while preserving their independence. As detailed in Alg. 1, at each step, agents at level l generate messages and rewards through their communication functions and influence lower levels through their policies. This enables coordinated behavior through bidirectional information flow while maintaining the autonomy of the implementation of each level."}, {"title": "3.2. Information Flow and Agent Interactions", "content": "Information in TAG flows through a continuous cycle between adjacent levels, facilitated by the LevelEnv abstraction. This flow can be characterized by two distinct pathways: bottom-up and top-down, as illustrated in Fig. 2.\nBottom-up Flow Information ascends the hierarchy from the real environment at the bottom through all the successive levels until the top. At each timestep, agents at level l receive messages $m_i^{l-1}$ and rewards $r_i^{l-1}$ from level $l - 1$, defined as:\n\\begin{aligned}\no^{l-1} &= [m_1^{l-1}, ..., m_{N_{l-1}}^{l-1}]\\\\\nr^{l-1} &= [r_1^{l-1}, ..., r_{N_{l-1}}^{l-1}]\n\\end{aligned}\nwhere $N_{l-1}$ represents the number of agents at level $l - 1$. Each message $m_i^{l-1}$ encodes both environmental state and internal agent state information.\nAgents $w_i^l$ process information from their subordinate agents through their communication function:\n$\\phi_i^l(o_i^{l-1}, r_i^{l-1}) = \\phi_i^l([m_j^{l-1}]_{j \\in I_i^{l-1}}, [r_j^{l-1}]_{j \\in I_i^{l-1}}),$ where $o_i^{l-1} = [m_j^{l-1}]_{j\\in I_i^{l-1}}$ and $r_i^{l-1} = [r_j^{l-1}]_{j\\in I_i^{l-1}}$ represent the collections of messages and rewards directed to agent i. Finally, level l returns to level $l + 1$ its observations $o^l = [m_1^l, ..., m_{N_l}^l]$ and rewards $r^l = [r_1^l, ..., r_{N_l}^l]$.\nThe strength of this framework lies in how messages are processed and transformed. Rather than simply relaying raw observations, agents can learn to extract and communicate relevant features that are crucial for coordination. For example, an agent might learn to signal when it needs assistance from other agents or when it has achieved a subgoal that contributes to the larger objective.\nTop-bottom Flow Control information descends the hierarchy through actions, starting at the top level. Each level l receives actions $a^{l+1} = [a_1^{l+1}, ..., a_{N_l}^{l+1}]$ from level $l + 1$, where each component i corresponds to the action input for agent $w_i^l$. These actions influence lower-level behavior through the policy function:\n$a_i^l = \\pi_i^l(a_i^{l+1}, o_i^{l-1}).$\nThe actions do not directly control the agents at lower levels but instead modify their observation space, subtly influencing their behavior while preserving their autonomy. This indirect influence mechanism is crucial as it allows higher levels to guide lower levels toward desired behaviors without needing to specify exact goals, similar to how biological systems maintain coordination across scales, while preserving the environmental abstraction at each level."}, {"title": "3.3. Learning and Adaptation", "content": "The learning process in TAG naturally accommodates the hierarchical structure instantiated by the framework. Each agent learns two key functions: a policy $\\pi$ for generating actions, and a communication function $\\phi$ for generating messages and rewards. The policy learns to map the combination of received actions and observations to actions for the level below, while the communication function learns to extract and transmit relevant information to higher levels.\nThe modular design of the framework allows agents at each level to learn independently using appropriate algorithms for their specific roles. This flexibility accommodates a wide range of learning approaches, from simple Q-learning to sophisticated policy gradient methods. During training, each agent stores its experiences and updates its policy based on received rewards, as shown in Alg. 1. This independent learning capability enables the framework to adapt more easily to different scenarios-lower levels might employ basic reactive policies, while higher levels can use advanced planning algorithms."}, {"title": "3.4. Scalability and Flexibility", "content": "The architecture of TAG enables scaling to arbitrary depths while maintaining computational efficiency through several mechanisms. First, the loose coupling between levels allows each layer to operate at its own temporal scale, similar to how biological systems separate strategic planning from reactive control. Higher levels can make decisions at lower frequencies than lower levels, reducing computational overhead while maintaining effective coordination. Second, standardized interfaces, implemented through the LevelEnv abstraction, naturally handle the integration of heterogeneous agents with varying capabilities and learning algorithms. This standardization ensures effective communication and coordination regardless of the implementation of individual agents.\nIn practice, the LevelEnv implementation follows the PettingZoo API (Terry et al., 2021), providing two primary interface functions: .reset() and .step(). The first, .reset(), initializes the system state from the real environment through all hierarchy levels and returns the initial observation, starting the upward flow of information. The .step() function accepts a dictionary of actions and returns dictionaries containing observations, rewards, termination conditions, and additional information for each agent in the level. It is during the call to .step() that actions for the lower level are generated, the step() of the lower level is called, and the agents are updated, as detailed in Alg. 1."}, {"title": "4. Empirical Validation", "content": ""}, {"title": "4.1. Multi Level Hierarchy Examples", "content": "To demonstrate the effectiveness of TAG, we implement multiple concrete examples consisting of two- and three-level hierarchical systems using PPO- and MAPPO- based agents. Their structures are shown in Fig. 1. We focus on on-policy algorithms as the lack of the replay buffer helps in dealing with the changing distributions in the environment (Foerster et al., 2016)."}, {"title": "5. Empirical Validation", "content": ""}, {"title": "5.1. Examples of Multi-Level Hierarchy", "content": "To demonstrate the effectiveness of TAG, we implement multiple concrete examples consisting of two- and three-level hierarchical systems using PPO- and MAPPO-based agents. Their structures are shown in Fig. 1. We focus on on-policy algorithms, as the lack of a replay buffer helps address the changing distributions in the environment (Foerster et al., 2016).\nAs shown in Fig. 1(a), the three-level architecture consists of a bottom level comprising four agents, each directly controlling an actor in the environment. These agents must learn to translate high-level directives into concrete actions while adapting to local conditions. The middle level contains two agents, each coordinating a pair of bottom-level agents. Finally, the top level contains a single agent that learns to provide strategic direction to the entire system. In contrast, the two-level hierarchy consists of four low-level agents interacting with the real environment and coordinated by a single high-level manager. For each of these topologies, we instantiate one homogeneous system, containing only PPO-based agents, and one heterogeneous system, with PPO-agents at the bottom and MAPPO-agents at the upper levels. We refer to these agents as 3PPO and 2MAPPO-PPO for the three-level systems, and 2PPO and MAPPO-PPO for the two-level systems.\nExcept for the agents at the bottom level, whose action space depends on the environment, all the PPO-based agents in 2PPO and 3PPO produce one-dimensional discrete actions in the range [0,...,5]. Given that PPO is not a MARL algorithm, it cannot control multiple agents in the level below the hierarchy without adaptation. To overcome this, we design the action space of each PPO agent in the upper levels l of 2PPO and 3PPO to be the combination of the input action spaces of level l \u2212 1, resulting from the subset of agents in l \u2212 1 connected to it. For example, if level l \u2212 1 contains two agents, each with an input action space of size K, the PPO agent at level l will have an action space of size K \u00d7 K. In the heterogeneous hierarchies of 2MAPPO-PPO and MAPPO-PPO, each MAPPO-based agent produces a two-dimensional continuous action for each of the agents to which it is connected. In this case, since MAPPO is a MARL algorithm by design, we did not modify its outputs. The agents in all these four systems (2PPO, 3PPO, MAPPO-PPO and 2MAPPO-PPO) only learn their policy $\\pi_i^l$, while the communication function $\\phi_i^l$ is hand-designed to return as message $m_i^l = [m_j^{l-1}] \\forall j \\in I_i^{l-1}$, corresponding to the concatenation of the observations from the level below, and as reward the sum of the rewards from l \u2212 1: $r_i^l = \\sum_{j \\in I_i^{l-1}} r_j^{l-1}$. Moreover, in the three-level agents, the top two levels provide a new action once every two steps of the level below, making each level effectively work at different frequencies compared to the levels below.\nFinally, we implement 3PPO-comm, a version of 3PPO in which the communication function $\\phi_i^l$ is learned. This consists of a two-layer AutoEncoder (AE) (Bank et al., 2023) with ReLU activation functions between the layers and Sigmoid on its feature space. The AE is continually trained together with the PPO agents, on the same batch, to reconstruct o \u22121 from o \u22121 by minimizing the MSE. The message $m_i^l$ corresponds to the representation of $o_i^{l-1}$ in the 8-dimensional feature space of the trained autoencoder. As with the other agents, the reward returned by $\\phi_i^l$ is the sum of rewards from the level below. The hyperparameters of all the implemented systems are presented in App. \u0410."}, {"title": "5.2. Experimental Design and Results", "content": "We evaluate TAG-based systems across two standard multi-agent environments that test different aspects of coordination and scalability. The first is the Simple Spread environment from the MPE suite (Lowe et al., 2017; Mordatch & Abbeel, 2017), where agents must maximize area coverage while avoiding collisions, testing both coordination and spatial reasoning. The second is the Balance environment from the VMAS suite (Bettini et al., 2022), which tests synchronized control by requiring agents to maintain collective stability through coordinated actions. Both environments operate with four agents and limit episodes to 100 time-steps.\nWe compare our approach against three baselines: MAPPO (Yu et al., 2022), I-PPO (De Witt et al., 2020), and classic PPO (Schulman et al., 2017). Being in a multi-agent setting, we adapted PPO by expanding its action space to encompass the combined action spaces of all agents in the real environment. Additionally, for the MPE-Spread environment, we developed a hand-designed heuristic that assigns and directs each agent to a specific goal along the shortest path from their initial position. The average performance of this heuristic across 10 episodes is indicated by a red dotted line in Fig. 3.(a).\nFig. 3 shows the average reward obtained by all tested algorithms in both benchmark environments over 5 random seeds. The shaded areas represent 95% confidence intervals. The results demonstrate that increasing the depth of the hierarchy improves both final performance and sample efficiency. This improvement is particularly pronounced in the MPE-Spread environment (Fig. 3.(a)), where only the depth-three agents, 3PPO and 2MAPPO-PPO, match the hand-designed heuristic performance, while all other agents achieve lower rewards. We particularly focus on 3PPO-comm due to its performance in the Balance environment (Fig. 3.(b)). Its ability to achieve significantly higher average rewards compared to other baselines suggests that learned communication is crucial for proper coordination in certain settings. However, the implementation and learning of communication require careful consideration. While a simple AE might suffice for the Balance task, 3PPO-comm shows lower performance in MPE-Spread compared to methods using the identity function as their communication function $\\phi_i^l$. Currently, the learning of $\\phi_i^l$ occurs independently of agent performance. We believe incorporating performance-related communication between agents could significantly enhance both performance and communication quality, which we leave for future work.\nRegarding the baselines, while MAPPO and I-PPO eventually reach similar performance levels as the two-level TAG-based agents, they require more training time. Notably, PPO struggles to achieve performance similar to the other baselines in both environments, highlighting the limitations of monolithic approaches when dealing with large action and observation spaces.\nThese results demonstrate two key advantages of the TAG approach. First, the hierarchical structure enables more efficient learning compared to flat architectures, as the division of labor across levels allows each agent to focus on a manageable subset of the overall problem, leading to increased sample efficiency. Second, the framework shows improved scalability; as we increase the number of agents, the hierarchical structure helps maintain coordination without the exponential complexity growth typical of flat architectures."}, {"title": "5.3. Analysis of Communication Mechanisms", "content": "In this section, we analyze the learned communication mechanism between hierarchy levels by examining correlations between the actions of connected agents. The presence of such correlations would indicate that agents can effectively use the modifications to their observations from higher-level agents. We focus our analysis on the action relationships between the top and bottom levels of 2PPO and MAPPO-PPO in the MPE-Spread environment, where all agents in the hierarchy have a discrete action space of 5. Figs. 4 and 5 display the discrete actions of one low-level agent on the y-axis and the training episodes on the x-axis. The colors indicate which top-level action was most frequently chosen (mode) when the bottom-level agent performed each of its actions during an episode. This is calculated as follows: for each episode, we: 1) look at every instance when the bottom-level agent performs a specific action, 2) record which action the top-level agent chose in each of these instances, and 3) determine which top-level action occurred most often (mode) for that bottom-level action. White spaces represent episodes where the low-level agent did not select the corresponding action. A constant mode across multiple episodes indicates an association between the actions of agents across two levels.\nAs shown in Figs. 4.(a) and 5.(a), there is a strong correlation between the actions selected by the top agent $w^2$ for the bottom agent $w_i^1$ and the actions of $w_i^1$ for both 2PPO and MAPPO-PPO, evidenced by the mode remaining constant across multiple episodes. While this association evolves throughout training, it maintains clear definition. In contrast, Figs. 4.(b) and 5.(b) show the correlation between actions selected by $w^2$ for $w_j^1$ and the actions of $w_i^1$, with $j \\ne i$. If the correlations observed earlier were merely coincidental rather than due to meaningful communication, we would expect to see similar patterns even between unconnected agents. Nonetheless, no correlation is present, as indicated by the mode changing every episode. The absence of correlation in this case confirms that the patterns observed between connected agents reflect actual information flow through the hierarchy. These results demonstrate that higher-level agents learn to provide useful feedback that lower-level agents can build on, confirming that the hierarchical structure and information flow instantiated by TAG are beneficial."}, {"title": "6. Discussion and Future Work", "content": "Our results demonstrate the benefits of TAG for hierarchical coordination, while highlighting several important considerations. The framework excels in tasks requiring coordination between multiple agents, though determining the optimal hierarchy configuration \u2013 specifically, the number of levels and agents per level \u2013 currently relies on empirical tuning, presenting an important area for future research. Another key consideration emerges from the definition of our communication function $\\phi_i^l$. While most of our baselines use the identity function for inter-level communication, our experiments with learned communication functions reveal promising improvements in performance. These results underscore the need for a more thorough investigation into learning optimal communication between agents. Understanding how to effectively learn and shape this communication could significantly enhance information flow between hierarchical levels and potentially reduce coordination overhead.\nA particularly promising direction is adapting the hierarchical structure automatically. The current implementation requires pre-specifying the number of levels and inter-agent connections. Extending TAG to dynamically adjust its structure based on the demands of the task could enhance its flexibility and efficiency. This development could draw inspiration from biological systems, where hierarchical organization typically emerges through self-organization rather than external specification. The success of TAG in enabling scalable multi-agent coordination extends beyond pure reinforcement learning. Its principles of loose coupling between levels and standardized information flow could inform the design of other complex systems, from robotic swarms to distributed computing architectures. Additionally, the capability of the framework to handle heterogeneous agents suggests potential applications in human-AI collaboration, where artificial agents must coordinate with human operators across multiple levels of abstraction.\nSeveral promising avenues for future research emerge from this work. First, investigating theoretical guarantees for learning convergence in deep hierarchies could provide valuable insights for designing more robust systems, particularly regarding the stability of learning across multiple hierarchical levels. Second, enabling the creation of autonomous hierarchies and composing the team dynamically would enhance practical applicability by allowing agents to join or leave the hierarchy during operation. Furthermore, integrating model-based planning at higher levels while maintaining reactive control at lower levels could improve performance in complex domains. This could include incorporating LLM-based agents at the highest levels to enhance reasoning capabilities and facilitate natural interaction with human operators. The study of how agents learn to communicate effectively within the hierarchy represents another crucial direction, as our preliminary results with learned communication functions suggest significant potential for improving coordination efficiency and system performance."}, {"title": "7. Conclusion", "content": "TAG represents a step toward more scalable and flexible multi-agent systems. By providing a principled framework for hierarchical coordination while maintaining agent autonomy, it enables complex collective behaviors to emerge from relatively simple components, similar to biological systems. The demonstrated success in our comprehensive evaluation across standard multi-agent benchmarks, including both cooperative navigation and manipulation tasks, suggests its potential for addressing increasingly challenging multi-agent problems. Having heterogeneous agents and arbitrary depths of hierarchy, while maintaining stable learning, poses several key challenges in multi-agent reinforcement learning. As we move toward increasingly complex multi-agent systems, frameworks like TAG that enable principled hierarchical organization will become increasingly important."}]}