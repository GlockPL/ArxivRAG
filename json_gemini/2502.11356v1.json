{"title": "SAIF: A Sparse Autoencoder Framework for Interpreting and Steering Instruction Following of Language Models", "authors": ["Zirui He", "Haiyan Zhao", "Yiran Qiao", "Fan Yang", "Ali Payani", "Jing Ma", "Mengnan Du"], "abstract": "The ability of large language models (LLMs) to follow instructions is crucial for their practical applications, yet the underlying mechanisms remain poorly understood. This paper presents a novel framework that leverages sparse autoencoders (SAE) to interpret how instruction following works in these models. We demonstrate how the features we identify can effectively steer model outputs to align with given instructions. Through analysis of SAE latent activations, we identify specific latents responsible for instruction following behavior. Our findings reveal that instruction following capabilities are encoded by a distinct set of instruction-relevant SAE latents. These latents both show semantic proximity to relevant instructions and demonstrate causal effects on model behavior. Our research highlights several crucial factors for achieving effective steering performance: precise feature identification, the role of final layer, and optimal instruction positioning. Additionally, we demonstrate that our methodology scales effectively across SAEs and LLMS of varying sizes.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated remarkable capabilities in following instructions, enabling alignment between model outputs and user objectives. These capabilities are typically gained through instruction tuning methods, including extensive training data and computationally intensive fine-tuning processes. While these approaches effectively control model behavior, the underlying mechanisms by which models process and respond to instructions remain poorly understood. In-depth mechanistic investigations are essential for improving our ability to control models and enhance their instruction-following capability.\nPrior research has attempted to understand instructions following from two perspectives: 1) prompting-based; 2) activation-space-based. Among prompting-based studies, the importance of instruction positions has been thoroughly studied. For activation-based studies, propose to manipulate model following instructions with representation vector in residual stream. However, both methods ultimately fail to explain the inner workings of how LLMs follow instructions in a fine-grained manner, i.e. the concept level. Specifically, prompting-based approaches provide insights into better prompt formulation strategies to improve instruction following, while activation-space-based methods provide a possible way to implement steering with instruction following rather than explaining how it works.\nIn this paper, we propose a novel framework SAIF (Sparse Autoencoder steering for Instruction Following) to understand working mechanisms of instruction following at the concept level through the lens of sparse autoencoders (SAEs). First, we develop a robust method to sample instruction-relevant features. Then, we select influential features using designed metrics and further compute steering vectors. Furthermore, we measure the effectiveness of these steering vectors through steering tasks. Additionally, we examine the extracted features using Neuronpedia to illustrate how semantically relevant the activating text of features is to instructions. We also measure steering performance to demonstrate the effectiveness of extracted features. Through these tools, we gain some intriguing insights regarding the importance of the feature number used in representing instructions, the role of the last layer, the impact of instruction position and model scale. Our main contributions in this work can be summarized as follows:\n\u2022 We propose SAIF, a framework that interprets instruction following in LLMs at a fine-grained conceptual level. Our analysis reveals how models internally encode and process instructions through interpretable latent features in their representation space.\n\u2022 We demonstrate that instructions cannot be adequately represented by a single concept in SAEs, but rather comprise multiple high-level concepts. Effective instruction steering requires a set of instruction-relevant features, which our method precisely identifies.\n\u2022 We reveal the critical role of the last layer in SAE-based activation steering. Moreover, the effectiveness of our framework has been demonstrated across instruction types and model scales."}, {"title": "2 Preliminaries", "content": "Sparse Autoencoders (SAEs). Dictionary learning enables disentangling representations into a set of concepts. SAEs are employed to decompose hidden representations into a high-dimension space and then reconstruct the hidden representations. Specifically, the input of SAEs is the hidden representation from a model's residual stream denoted as z \u2208 Rd and the reconstructed output is denoted as SAE(z) \u2208 Rd, we obtain that z = SAE(z) + e where e is the error. In our paper, we focus on layer-wise SAEs trained with an encoder Wenc \u2208 Rdxm followed by the non-linear activation function, and a decoder Wdec \u2208 Rmxd. The definition of SAEs is:\na(z) = \u03c3 (zWenc + benc),   (1)\nSAE(z) = a(z) Wdec + bdec,   (2)\nwhere benc \u2208 Rm and bdec \u2208 Rd are the bias terms. The decomposed high-dimension latent activations a(z) have dimension m and m \u226b d, which is a highly sparse vector. Note that different SAEs use different non-linear activation function \u03c3. For example, Llama Scope adopts TopK-ReLU, while Gemma Scope uses JumpReLU.\nSteering with SAE Latents. Following Eq. (2), the reconstructed SAE outputs are a linear combination of SAE latents, which represent the row vectors of SAE decoder Wdec. The weight of j-th SAE latent is a(z)j. Typically, a prominent dimension j\u2208 {1,\u2026\u2026,m} is chosen, and its decoder latent vector dj is scaled with a factor a and then added to the SAE outputs. The computation is as follows:\nznew \u2190 z + adj.   (3)\nThis modified representation znew can then be fed back into the model's residual stream to steer the model's behavior during generation."}, {"title": "3 Proposed Method", "content": "In this section, we introduce the SAIF, a framework for analyzing and steering instruction following in LLMs. First, we introduce linguistic variations to construct diverse instruction sentences and related datasets, which are further used to compute SAE latent activations. Second, we develop a two-stage process for computing steering vectors that quantifies the sensitivity of features to instruction presence. Finally, we investigate how these identified features can be leveraged for steering model behavior, demonstrating a technique for enhancing instruction following while preserving output coherence (see Figure 1)."}, {"title": "3.1 Format Instruction Feature", "content": "To identify instruction-relevant features given an instruction, we construct a dataset D with N positive-negative sample pairs. For example, we focus on an instruction Translate the sentence to French. In a sample pair, the positive sample refers to a prefix prompt followed by the instruction, while the negative sample refers to the prefix prompt without the instruction sentence.\nThe difference-in-means is a typical approach to derive concept vectors. It computes the activation differences between each sample pair over the last token, and then averages over all pairs of activation difference vectors. However, directly applying this pipeline to instruction following presents a significant challenge. When a single instruction sentence is used repeatedly to generate samples, the model tends to encode the specific semantic meaning of that instruction rather than learning a general-purpose vector that can reliably execute the intended operation (See Appendix G). Specifically, the derived vector can barely operate the same instruction if we rephrase the instruction in a linguistically different but semantically similar manner. To resolve this challenge, we propose to introduce linguistic variations to extract instruction functions.\nWe formulate instruction sentences for a given instruction through different strategies. These variations include syntactic reformulations (e.g., imperative to interrogative form, task-oriented to process-based description) and cross-lingual translations (e.g., English, Chinese, German). In this way, we generated six diverse instruction sentences comprehensively capturing key features of an instruction. The instruction design used in this paper is shown in Appendix A.\nFor each instruction variant, we extract samples' residual stream representation and compute the corresponding SAE latent activations. While diverse linguistic information are contained, the latent features specifically corresponding to the core instructional concept should maintain relatively consistent activation levels across all variants. These dimensions with consistent activation patterns will be further used to construct instruction vectors."}, {"title": "3.2 Steering Vector Computation", "content": "Based on SAE latent activations computed in Section 3.1, we develop a two-step process for computing steering vectors. The first step identifies features that consistently respond to a given instruction, while the second step quantifies their sensitivity.\nGiven N input samples and a target instruction type (e.g., translation), we first obtain both positive samples (with instruction) and negative samples (without instruction) for each input. For each sample pair i and feature j, we compute the activation state change:\n\u0394hi,j = 1 (hwi/oj > 0) -1(hwi/oo > 0),   (4)\nwhere hwi/oj and hwi/oo represent the SAE latent activation values with and without instruction respectively, and 1(\u00b7) is the indicator function. \u0394hij captures whether feature j becomes activated in response to the instruction for sample i. We then compute a sensitivity score Cj for each feature:\nCj = 1/N \u03a3i 1 (hij > 0).   (5)\nThe score represents the proportion of samples whose feature j becomes activated in response to instructions. Features with higher scores are more consistently responsive to instruction presence. By sorting these sensitivity scores in a descending order, we select the top-k responsive features. These selected features form the instruction-relevant feature set V = {Wdec,j|rank(Cj) \u2264 k} where Wdec,j = Wdec[j, :] denotes the j-th SAE latent. These features will be used for further constructing steering vectors."}, {"title": "3.3 Steering Procedure", "content": "Different from the classic steering approach defined in Eq. (3), we hypothesize that instruction following steering requires a set of features to be effective. The individual feature utilized in the classic method focuses on token-level concepts, where individual concepts typically correlate with a few SAE latent activations. As a result, this approach can barely operate instructions. It is partly due to the complexity of sentence-level instructions, which are composed of multiple high-level features represented by a set of SAE latent features. Additionally, SAEs tend to overly split features, which further increases the number of features needed for steering. Thus, we propose to determine how to steer with a set of vectors. Building on top of the feature set V derived in Section 3.2, we employ the set of features to steer residual stream representation of a certain input at layer l. Our steering is implemented as below:\nznew = z + \u03a3 ai vi,   (6)\nwhere z represents the residual stream representation of the input over the last token, and ai denotes the steering strength of feature i. Here, vi represents a certain instruction-relevant feature in V. As the strength of each selected feature is crucial to steering performance, we further compute the strength of each feature by employing statistical measurements of feature activation values to make it more robust and reliable. The activation strength for feature i is calculated as:\nai = \u03bc\u03b5 + \u03b2\u03b4\u03af,   (7)\nwhere \u03bc\u03b5 is the mean activation value of feature i observed in instruction-following examples, si is the standard deviation of these activation values, and \u03b2 is a hyperparameter to scale si meanwhile controlling the strength value."}, {"title": "4 Experiments", "content": "In this section, we conduct experiments to evaluate the effectiveness of SAIF by answering the following research questions (RQs):\n\u2022 RQ1: How interpretable are the features extracted using SAEs, and do they correspond to instruction-related concepts? (Section 4.2)\n\u2022 RQ2: Can the proposed SAIF framework effectively control model behavior? (Section 4.3)\n\u2022 RQ3: What role does the final Transformer layer play in the instruction following? (Section 4.4)\n\u2022 RQ4: How does instruction positioning affect the effectiveness of instruction following and feature activation patterns? (Section 4.5)"}, {"title": "4.1 Experimental Setup", "content": "Datasets and Models. Our experiments are conducted with multiple language models including Gemma-2-2b, Gemma-2-9b and Llama3.1-8b. The Cross-lingual Natural Language Inference (XNLI) dataset is used to construct input samples. It encompasses diverse languages (including English, French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi, Swahili and Urdu) and rich syntactic structures (such as active/passive voice alternations, negation patterns, and various clause structures). The diverse linguistic patterns within the dataset are essential in constructing a comprehensive set of samples for an instruction. Moreover, it ensures extracting consistent SAE activations from the residual stream of input samples.\nInstruction Design. Following the settings in IFEval, we investigate three types of instructions: keyword inclusion, summarization, and translation. For keywords inclusion, we provide models with a keyword (e.g., \"Sunday\"), and expect model output incorporating the specified keyword. For formatting, we instruct the model to perform summarization, where the ideal output should be concise, maintain the key information from the original text, and follow a consistent format with a clear topic sentence followed by supporting details. For translation, we direct the model to translate sentences into different languages (English, French, and Chinese), where the ideal model output should accurately perform the requested translation while preserving the original meaning. The complete set of instructions used for each task is provided in Appendix A.\nImplementation Details. We use pre-trained SAEs from Gemma Scope and Llama Scope. When constructing input samples for each instruction, we set the number of positive/negative samples N to 800. For SAE latent extraction, we use sparse autoencoders with dimensions of 65K and 131K for Gemma-2-2b-it and Gemma-2-9b-it models respectively. We also use SAE with dimension 32K for Llama3.1-8b. All experiments were run on 1 NVIDIA A100 GPU. As default settings, for Equation (6), we fix k = 15, meaning that we use the top 15 most responsive SAE features for instruction steering. The strategy to choose the optimal k will be further discussed in Section 4.2. For Equation (7), we fix the hyperparameter \u03b2 = 0, and we discuss the impact of adjusting this hyperparameter on the steering effect in Appendix C.\nSAE Latent Activation Metrics. We consider the following three metrics to quantify features' behavior and reliability in instruction processing. Note that we only consider features activated on positive samples but not negative samples.\n\u2022 Activation Strength: The mean activation value is calculated as: \u00b5\u2081 = 1/[\u0391] \u03a3a\u2208A; a, where A\u00bf is the set of non-zero activation values for feature i.\n\u2022 Activation Probability: The probability of feature i is activated across positive/negative samples: P = Ai/ N, where N is the total number of positive/negative samples.\n\u2022 Activation Stability: The normalized standard deviation value of non-zero activation values: \u03a9\u2081 = 1/\u03b4i.\nA high-quality instruction-relevant feature should ideally exhibit strong activation (\u00b5i), consistent triggering (Pi), and stable behavior (\u03a9\u2081) across different formulations of the same instruction.\nSteering Effectiveness Metrics. We evaluate steering outputs with two metrics: 1) Strict Accuracy, which measures the proportion of cases where the model completely follows the instruction, meaning it both understands and produces output exactly as instructed; and 2) Loose Accuracy, which measures the proportion of cases where the model partially follows the instruction, meaning it understands the instruction but the output does not fully conform to the requirements. Note that we use GPT-40-mini to rate the responses, and please refer to the details in Appendix D."}, {"title": "4.2 Analysis of Instruction-Related Concepts", "content": "To investigate RQ1, we analyze the interpretability of features extracted using SAEs and assess their correspondence to instruction-related concepts. Our analysis consists of two parts. First, we examine the activating text of extracted features with Neuronpedia to evaluate their semantic relevance to instructions. Second, we compare how strongly the activating examples of top-k features and lower-ranked features correspond to instruction-related concepts, demonstrating the relationship between feature importance and instruction relevance.\nWe focus on analyzing the consistent instruction-relevant latent activations through the lens of Neuronpedia, which provides detailed activated text for each SAE latent. Taking translation-related instructions as an example (e.g., \"Translate the sentence to French.\"), we identify a notable latent that shows strong activation patterns. This latent exhibits high activation not only for various languages but also for directional prepositions like \"to\" and \"from\" that commonly appear in translation instructions, as shown in Table 1. We summarize two key findings as below:\n\u2022 Our extracted SAE latent features show strong correspondence with instruction-related concepts,"}, {"title": "4.3 Steering Performance Analysis", "content": "In this section, we evaluate the effectiveness of steering vectors constructed from SAE features and investigate the optimal number of features needed for reliable control.\nSteering Effectiveness. We visualize a case study in Figure 4 and compare the performance of steering results in Figure 5, including both strict accuracy and loose accuracy. Our analysis reveals several key findings:\n\u2022 The quantitative results in Figure 5 demonstrate significant improvements in instruction following, with the steered models achieving over 30% strict accuracy across different tasks. The loose accuracy of our steered approach performs nearly on par with prompting-based instruction methods, falling only slightly below. These results strongly indicate that SAIF can effectively extract features for user instructions and adjust LLMs' behaviors according to relevant instructions.\n\u2022 The case study in Figure 4 illustrates two distinct scenarios of instruction following: strict adherence (successful Chinese-to-French translation) and loose following (understanding that this is a French translation task). It demonstrates how SAIF manipulates model responses from the failure case toward either strict instruction following or loose instruction following.\n\u2022 The Gemma-2-9b-it model consistently outperforms Gemma-2-2b-it with slightly higher instruction steering performance across all five tasks, suggesting that SAIF's effectiveness scales well with model size.\n\u2022 The LLaMA-3.1-8B model shows comparable performance to the Gemma models across tasks. Looking at French translation as an example, LLaMA-3.1-8B achieves around 30% strict accuracy and 65% loose accuracy, which is similar to Gemma-2-2b-it's performance.\nLatent Dimension Analysis. We study the effect of single latent and the number of latents on steering, showing that too few and too many dimensions both lead to failures. For individual latent, we use the single top 1 latent and latent listed in Table 1 for steering. Despite their apparent semantic relevance to translation tasks, the model shows zero accuracy. This suggests that instruction following cannot be captured by a single high-level concept, even when that concept appears highly correlated with specific instruction types.\nThis observation leads us to investigate whether a combination of multiple latent dimensions could achieve better steering performance. Our experiments, shown in Figure 3, systematically evaluate the impact of varying the number of latent dimensions from 1 to 30. The instructions used here are sourced from French translation task. The results reveal several key patterns:\n\u2022 Steering performance remains near zero when k\u2264 5, indicating that too few dimensions are insufficient for capturing instruction-following behavior. Performance begins to improve notably around k = 10, with both strict accuracy and loose accuracy showing substantial increases.\n\u2022 The optimal performance is achieved at k = 15, where loose accuracy peaks at approximately 0.7 and strict accuracy reaches about 0.25.\n\u2022 However, as we increase dimensions beyond k = 15, both metrics show a consistent decline. This deterioration becomes more pronounced as k approaches 30, suggesting that excessive dimensions introduce noise that interferes with effective steering."}, {"title": "4.4 The Role of Last Layer Representations in Instruction Processing", "content": "In previous sections, we exclusively used SAE from the last Transformer layer for concept vector extraction and instruction steering. In this section, we analyze why extracting concepts and steering from the final layer is most effective.\nConcept Extraction Perspective. From the results in Table 4, we observe an intriguing phenomenon that shallower layers are less effective in providing clean instruction-relevant features. Following our default experimental settings, we extract the top 15 SAE features from each layer of the model. The features extracted from the last layer can precisely capture the semantics of 'French', showing strong activations on French-related words, where k = 2 indicates this feature is considered the second most instruction-relevant feature. Starting from the penultimate layer, as we attempt to trace French-related features, our experimental results reveal that the extracted French-related concepts undergo a gradual shift as the layer depth decreases. Specifically, the feature evolves from exclusively activating on French-related tokens to encompassing a broader spectrum of languages (English, Spanish, Hindi, and Belgian), demonstrating a hierarchical abstraction pattern from language-specific to cross-lingual representations. Moreover, the increasing k values suggest that these French-related features become less instruction-relevant in earlier layers. For Gemma2-2b-it model, before Layer 21, we can no longer identify French-related features among the top 15 SAE features.\nSteering Perspective. We conducted steering experiments using the top 15 features extracted from Layers 21-25 respectively under default settings on French Translation task. The results align with our findings on concept extraction, showing the effectiveness and importance of last layer representation on instruction following. Using loose accuracy as the evaluation metric, we observe that steering with Layer 24 features still maintains some effectiveness, though the loose accuracy drops sharply from 0.64 (Layer 25) to 0.33. Steering attempts using features from earlier layers fail to guide the model towards instruction-following behavior, with the model instead tending to generate repetitive and instruction-irrelevant content."}, {"title": "4.5 Impact of Instruction Position", "content": "Previous studies have shown that models' instruction-following capabilities can vary significantly depending on the relative positioning of instructions and content. This motivates us to examine how instruction positioning affects the activation patterns of previously identified features.\nWe investigate the effect of instruction position by comparing two patterns: pre-instruction (Ppre = [Instruction] + [Content]) and post-instruction (Ppost = [Content] + [Instruction]) as in Liu et al. (2024). Using identical instruction-content pairs while varying only their relative positions allows us to isolate the effects of position. Our analysis reveals several key findings from both the quantitative metrics (see Table 3) and feature activation patterns (see Figure 2):\n\u2022 Performance metrics demonstrate that post-instruction positioning consistently outperforms pre-instruction, with post-instruction achieving higher accuracy across all measures (Strict Acc: 0.23 vs 0.14, Loose Acc: 0.64 vs 0.47), aligning with the result in Liu et al. (2024).\n\u2022 Feature activation patterns show that post-instruction enables more robust processing with stronger activation peaks (particularly for key features like F33659), more consistent stability scores, and higher activation probabilities (>80%) across most features compared to pre-instruction's more variable patterns."}, {"title": "5 Conclusions", "content": "In this paper, we have introduced to use SAEs to analyze instruction following in LLMs, revealing the underlying mechanisms through which models encode and process instructions. Our analysis demonstrates that instruction following is mediated by interpretable latent features in the model's representation space We have developed a lightweight steering technique that enhances instruction following by making targeted modifications to specific latent dimensions. We find that effective steering requires the careful combination of multiple latent features with precisely calibrated weights. Extensive experiments across diverse instruction types have demonstrated that our proposed steering approach enables precise control over model behavior while consistently maintaining coherent outputs."}, {"title": "Limitations", "content": "One limitation of our steering approach is that it sometimes produces outputs that only partially follow the intended instructions, particularly when handling complex tasks. While the model may understand the general intent of the instruction, the generated outputs may not fully satisfy all aspects of the requested task. For example, in translation tasks, the model might incorporate some elements of the target language but fail to produce a complete and accurate translation. Besides, our current work focuses primarily on simple, single-task instructions like translation or summarization. In future, we plan to investigate how to extend this approach to handle more sophisticated instruction types, such as multi-step reasoning tasks or instructions that combine multiple objectives. Additionally, our experiments were conducted using models from the Gemma and Llama two LLM families. In the future, we plan to extend this analysis to a more diverse set of language model architectures and families to validate the generality of our findings."}, {"title": "A Details of Instructions", "content": ""}, {"title": "B Related Work", "content": "In this section, we briefly summarize several research directions that are most relevant to ours.\nInstruction Following in Language Models. Instruction following capabilities are crucial for improving LLM performance and ensuring safe deployment. Recent advances in instruction tuning have demonstrated significant progress through various methods. However, capable models still struggle with hard-constrained tasks and lengthy generations. Some studies find that instruction following can be improved with in-context few-shot examples, optimal instruction positions ,carefully selected instruction-response pairs with fine-tuning, and adaptations . Unfortunately, the mechanistic understanding of how LLMs internally represent and process these instructions remains limited.\nLanguage Model Representations. A body of research have focused on studying the linear representation of concepts in representation space. The basic idea is to find a direction in the"}, {"title": "C Additional Results for Llama-3.1-8b", "content": "In our experimental setup, we employ Equation (7) to control feature activation during model steering, where \u03bc\u03b5 denotes the pre-computed mean activation strength and si represents the standard deviation for feature i. The hyperparameter \u03b2 controls the perturbation magnitude relative to the standard deviation. Our experiments reveal distinct robustness characteristics across different model architectures. For the Gemma-2 family models, the steering vectors maintain their effectiveness when \u03b2\u2208 [\u22121, 1], indicating robust feature representations. These models exhibit high activation strength values (\u03bc\u2081) with low standard deviations (si), suggesting stable and consistent feature characteristics. In contrast, the Llama-3.1-8b"}, {"title": "D Steering Accuracy Evaluation based on GPT-40-mini", "content": "To evaluate generated outputs, we instruct GPT-40-mini to rate in the following way. For each instance, we provide GPT-40-mini with three components: the original input text, the instruction, and the model-generated output. To ensure reliable assessment, we implement a voting mechanism where GPT-40-mini performs five independent evaluations for each instance. For each evaluation, GPT-40-mini is prompted to assess the instruction following level by selecting whether the generated content completely follows the instruction (A), contains instruction keywords but doesn't follow the instruction (B), or is completely irrelevant to the instruction (C). The final grade is determined by majority voting among the five evaluations. In cases where there is no clear majority (e.g., when votes are split as 2-2-1), we choose the lower grade between the two options that received the most votes (C is considered lower than B, and B is lower than A). This ensures a stringent evaluation standard when the votes are divided. Thus, the Strict Accuracy is the ratio of A and the Loose Accuracy is the ratio of A + B. The prompt we use in the experiments can be found in Table 5."}, {"title": "E Model Scale Analysis", "content": "We explore the influence of both model scale and SAE scale, showing larger sizes always contribute to better performance. Using SAE with larger dimensions (e.g., increasing Gemma-2-2b's SAE from 16K to 65K) can effectively improve the interpretability of feature extraction. For the same prompt, Gemma-2-2b's 16K SAE is almost unable to extract interpretable features under our settings, while the 65K model performs well. For Gemma-2-9b and Llama3.1-8b models, even the SAE with minimal dimensions can extract features with good interpretability."}, {"title": "F More Activating Examples of Top-ranked Features", "content": ""}, {"title": "G Examples of Instruction Following Tasks with Steering Vectors", "content": ""}, {"title": "H Extracted Features Correlation Visualization and Analysis", "content": "In Section 4.5, we explored how instruction placement (before or after the original prompt) affects model behavior. To further understand how the model encodes and processes instructions in different positions, we present visualization analysis using feature correlation heatmaps. Figure 7 to Figure 11 show the feature correlations of Gemma-2-2b model across five different tasks.\nTaking Figure 7 as an example, the visualization is divided into Pre-Instruction and Post-Instruction modes. Each part contains two 20\u00d720 heatmap matrices showing Activation Probability and Activation Strength correlations respectively. The heatmaps use a red-blue color scheme, where dark red indicates strong positive correlation (1.0), dark blue indicates strong negative correlation (-1.0), and light or white areas indicate correlations close to 0. The axes range from 0 to 19, representing the top 20 SAE latent features.\nOur analysis reveals distinct differences between the two instruction placement modes. The Pre-Instruction mode shows dispersed correlations with predominantly light colors outside the diagonal, indicating stronger feature independence. In contrast, the Post-Instruction mode exhibits more pronounced red and blue areas, demonstrating enhanced feature correlations and a more tightly connected feature network. This finding aligns with our key conclusion that effective instruction following requires precise combinations of multiple latent features. The stronger feature correlations in Post-Instruction mode confirm that single-feature manipulation is insufficient for reliable control. This insight into feature cooperation supports the effectiveness of our proposed steering technique based on precisely calibrated weights across multiple features."}]}