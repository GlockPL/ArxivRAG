{"title": "Generating Diverse Hypotheses for Inductive Reasoning", "authors": ["Kang-il Lee", "Hyukhun Koh", "Dongryeol Lee", "Seunghyun Yoon", "Minsung Kim", "Kyomin Jung"], "abstract": "Inductive reasoning\u2014the process of inferring general rules from a small number of observations\u2014is a fundamental aspect of human intelligence. Recent works suggest that large language models (LLMs) can engage in inductive reasoning by sampling multiple hypotheses about the rules and selecting the one that best explains the observations. However, due to the IID sampling, semantically redundant hypotheses are frequently generated, leading to significant wastage of compute. In this paper, we 1) demonstrate that increasing the temperature to enhance the diversity is limited due to text degeneration issue, and 2) propose a novel method to improve the diversity while maintaining text quality. We first analyze the effect of increasing the temperature parameter, which is regarded as the LLM's diversity control, on IID hypotheses. Our analysis shows that as temperature rises, diversity and accuracy of hypotheses increase up to a certain point, but this trend saturates due to text degeneration. To generate hypotheses that are more semantically diverse and of higher quality, we propose a novel approach inspired by human inductive reasoning, which we call Mixture of Concepts (MoC). When applied to several inductive reasoning benchmarks, MoC demonstrated significant performance improvements compared to standard IID sampling and other approaches.", "sections": [{"title": "1 Introduction", "content": "Inductive reasoning, inferring general rules that explain a small number of observations (Figure 1), is a key factor of human intelligence (Lake et al., 2015; Chollet, 2019). Recently, there has been exploration into automatic inductive reasoning using large language models (LLMs). Trained with vast corpora and instruction tuning, LLMs can function as zero-shot rule proposers when given an inductive reasoning problem (Qiu et al., 2024). By leveraging this capability, a new paradigm in automatic inductive reasoning has emerged: feeding observations (often referred to as train examples) as prompts into the LLM, sampling multiple hypotheses about the rule, and selecting the one that best explains the observations (Wang et al., 2024; Greenblatt, 2024; Brown et al., 2024). However, when the hypotheses are generated by LLMs are fundamentally IID, it often leads to redundant sampling of semantically identical hypotheses. Such redundancy reduces the diversity of hypotheses and results in a significant waste of compute.\nTo address aforementioned problem, we begin by increasing the temperature, which is often considered a diversity parameter for LMs, in IID sampling setup. Our findings indicate that while both diversity and accuracy improve up to a certain point with increased temperature, they quickly reach saturation. This is because raising the temperature deteriorates the quality of hypotheses, and increases the rate of text degeneration (Holtzman et al., 2020).\nTo generate diverse hypotheses while maintaining their quality, we propose a simple yet effective method called Mixture of Concepts (MoC). Inspired by human inductive reasoning (Hofstadter, 1979; Mitchell, 2021), our approach consists of two stages: concept proposal and hypothesis generation. In the concept proposal, we instruct the LLM to generate \"a list of K concepts\" that are likely to aid in the formation of hypotheses, and semantically non-redundant concepts are generated sequentially. In the hypothesis generation, these concepts are parsed and each concept is provided as a hint for generating hypotheses. Our approach allows for the parallel generation of semantically diverse hypotheses without hurting the quality of hypotheses (Figure 1).\nWe conduct experiments on four inductive reasoning datasets from distinct domains. Our methodology significantly improves performance across four LLMs by generating diverse hypotheses while maintaining their quality. Compared to vanilla IID sampling with the same number of generated hypotheses, MoC boosts average accuracy by 4.5%p for GPT-40-mini and 5.0%p for Llama-3.1-70B-Instruct as base LLM, respectively. Our analysis also shows that MoC enables the LLMs to crack challenging problems that are impossible to solve through IID sampling within a practical compute budget.\nOur contributions are as follows:\n\u2022 We analyze how the diversity and accuracy of IID hypotheses generated from LLMs vary with a change in temperature.\n\u2022 We propose Mixture of Concepts (MoC), a methodology enabling more diverse and parallel hypothesis generation in LLMs.\n\u2022 Our experimental results on four datasets and four models show that MoC significantly enhances both the efficiency and performance of inductive reasoning capability of LLMs."}, {"title": "2 Hypothesis Diversity and Temperature", "content": "In this section, we provide a description of the problem statement and the baseline method. Subsequently, we analyze the impact of temperature on the diversity and accuracy of hypotheses generated by an LLM."}, {"title": "2.1 Problem Statement and Baseline", "content": "Problem Statement We consider inductive reasoning problem with n train examples $\\mathcal{D}_{\\text{train}} = \\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\}$ and m test examples $\\mathcal{D}_{\\text{test}} = \\{(x'_1, y'_1), (x'_2, y'_2), ..., (x'_m, y'_m)\\}$. There is a function f that maps all the inputs to corresponding outputs: $f(x_i) = y_i$ and $f(x'_j) = y'_j$ for all $i \\in [n]$ and $j \\in [m]$, where $[n] = \\{1, 2, ..., n\\}$. The goal of this task is to predict $\\{o'_1, o'_2, ..., o'_m\\}$, given the $\\mathcal{D}_{\\text{train}}$ and $\\{i'_1, i'_2, ..., i'_m\\}$.\nSimple Baseline Here, we describe a simple baseline method that we use in our experiments. We adopt the setup defined in recent inductive reasoning and Programming-by-Example (PBE) literature, where the function f is first inferred and the test outputs are obtained as $\\{f(x'_j)\\}_{j\\in[m]}$. The function f is implemented as a Python function, unlike traditional PBE works using domain-specific lanuguages (DSLs).\nWe prompt LLMs with instruction to generate a hypothesis about f in natural language form and then implement it in Python, following the recent LLM inductive reasoning works (Qiu et al., 2024; Wang et al., 2024). We sample a fixed number of K responses and extract the Python function from these responses, forming a hypothesis pool $\\{f_1, f_2, ..., f_k\\}$. Among these hypotheses, the one that perfectly explains the train examples, i.e. $y_i = f_k(x_i)$ for all $i \\in [n]$, is picked and submitted to be validated on the test examples. If there is no such hypothesis, we regard the problem as not being solved correctly. We consider that a problem is solved correctly if $f_k$ passes every test cases: $y'_j = f(x'_j)$ for all $j \\in [m]$.\nDuplicate Hypotheses The issue with the aforementioned simple baseline is that, since it samples K hypotheses from IID distributions, a significant portion of these hypotheses may be semantically redundant. If the hypothesis is correct, this redundancy does not pose a problem; however, if an incorrect hypothesis is sampled multiple times, it results in a serious waste of the generation budget.\nTo investigate whether this redundancy is problematic, we analyze how many semantically unique hypotheses are among the K hypotheses, for the instances that the baseline failed to solve cor-"}, {"title": "2.2 Impact of Temperature on Diversity and Accuracy", "content": "Experiment on Temperature One of the most straightforward ways to increase diversity when sampling from a language model is by raising the temperature parameter. In this section, we analyze how temperature affects hypothesis diversity and accuracy in inductive reasoning tasks. We focus on the changes that occur when varying the temperature in the baseline method, within the datasets of List Functions and MiniARC (Kim et al., 2022).\nWe conduct experiments with a total of seven temperature settings, ranging from 0 to 2 in increments of 0.33. For temperatures of 1.67 and 2.0, there are substantial portion of responses that are degenerated (i.e. the Python function cannot be parsed from the model's response) (Figure 2)."}, {"title": "3 Mixture of Concepts", "content": "To achieve aforementioned goal, we propose automatic inductive reasoning framework called Mixture of Concepts (MOC).\nWhen humans engage in inductive reasoning, they begin by carefully observing examples and considering which concept or template might be effective in identifying the underlying rule (Mitchell, 2021). For instance, if presented with integer lists as depicted in Figure 5, one might initially contemplate concepts such as \u201carithmetic operations.\u201d Using this concept, human would re-examine the given examples to discern what the hidden rule might be. However, in this instance, the correct rule is to output the length of the list, and the concept of \"arithmetic operations\" do not aid in discovering this rule. In such scenarios, the best strategy is to revisit the given examples from a different point of view and to contemplate novel concepts and rules. This process can be repeated until the correct rule is identified (Hofstadter, 1979).\nInspired by this cognitive process, we propose a framework composed of two stages: concept proposal and hypothesis generation.\nConcept Proposal Through empirical investigation, we discovered that when instructing an LLM to generate a list of items, the generated items are rarely semantically redundant. This capability can be attributed to the autoregressive nature of the LLM. Each item generated by the LLM is used as context for generating the subsequent item, and the LLM tends to produce an item different from those it has already generated.\nBy leveraging this property, an LLM is instructed to list K elementary concepts that are likely to help the formulation of a hypothesis for given observations. For instance, given an inductive reasoning problem shown in Figure 5, the LLM generates a list of 4 distinct concepts (e.g. Arithmetic operations, Aggregation functions, etc.). Additionally, we instructed the LLM to generate the concepts in JSON format, ensuring that the concepts can be parsed more reliably in the subsequent stage.\nHypothesis Generation Next, we parse the concepts and feed each concept to the LLM. The LLM creates a natural language hypothesis and a Python code implementation based on the given concept. Here, a concept is provided in the instruction prompt as hint, so that the hypothesis can be conditioned on the given concept.\nFor instance, the 1st concept in Figure 5 (Arithmetic operations) conditions the generation to the wrong path and results in incorrect hypothesis. Meanwhile, the 4th concept (Counting or length-based operations) guides the generation to semantically distinct direction, with correct output of returning the length of given input list (for full prompt details, see Appendix A)."}, {"title": "4 Experiments", "content": "We validate our method on four datasets from distinct domains as shown in Figure 6."}, {"title": "4.1 Datasets", "content": "List Functions The List Functions dataset (Rule, 2020) evaluates inductive reasoning ability in inferring list transform functions. The inputs and outputs are integer list, possibly an empty list. The transformation function covers broad range of list and arithmetic operators such as indexing, slicing, counting, sorting, etc. We randomly sample a subset of 100 instances from the 250 instances of the dataset for evaluations in our work.\nMiniARC MiniARC (Kim et al., 2022) is a simplified version of Abstraction and Reasoning Corpus (ARC) dataset (Chollet, 2019). In ARC, the inputs and outputs are 2D grids following specific transformation pattern. The transformation functions are explicitly grounded on the cognitive priors of humans, such as objectness, goal-directedness, arithmetic and basic geometry (Spelke and Kinzler, 2007). Compared to original ARC, MiniARC's inputs and outputs are always 5x5 grids, greatly reducing the complexity of the problems. Nonetheless, MiniARC is still extremely challenging for state-of-the-art LLMs (Qiu et al., 2024). Following the previous works, we input the MiniARC grid into the LLMs in the form of a nested list.\nMBPP+ MBPP+ (Liu et al., 2023) expands a program synthesis dataset MBPP (Austin et al., 2021) with 35x more test cases. It covers basic Python programming tasks written by humans. The abundance of test cases avails utilizing MBPP+ as an inductive reasoning dataset. We arrange each instance such that it has 8 train examples and 6 test examples, and randomly sample a subset of 100 instances for evaluation.\nPlaygol-str The string transformation domain is a practical application of automatic inductive reasoning that many users actively utilize (Gulwani, 2011; Cambronero et al., 2023; Chen et al., 2021). We evaluate our method on the real-world string transformations dataset introduced by Cropper (2019). In our work, we refer it using the name of their proposed method, Playgol-str. We randomly sample a subset of 150 instances for evaluation."}, {"title": "4.2 Main Results", "content": "We compare Mixture of Concepts (MoC) with simple baseline described in section 2.1. Each method is implemented with two proprietary and two open-source LLMs: GPT-40-mini (gpt-4o-mini-2024-07-18), GPT-40 (gpt-4o-2024-08-06), Llama-3.1-70B-Instruct, and Qwen2.5-72B-Instruct. If not otherwise specified, the temperature is set to 1.0 for all experiments, because temperature higher than that does not bring any performance gain as observed in section 2.2. Also, we set the number of samples K = 8 here. Results with K = 4 and K = 16 are in Appendix C.\nIn Table 2, our MoC approach shows significant improvement in overall test accuracy, demonstrating the effectiveness of our approach. In the MiniARC dataset, all LLMs generally exhibit low performance, which is largely due to the fact that ARC-variants often require visual priors that LLMs are not expected to possess.\nComparison with Hypothesis Refinement Iterative Hypothesis Refinement (IHR) (Qiu et al., 2024) involves validating the generated hypotheses against the training examples, and if none of the hypotheses pass, the hypothesis with highest train accuracy is refined using execution feedback. We conduct experiments of IHR in a setting of T = 2 and N = 4, similar to the case where K = 8. This means there are 2 iterations, with 4 hypotheses generated per iteration. Additionally, when translating the generated hypotheses into Python code, we insert the training examples in the input prompt, resulting in a significant performance improvement.\nAs shown Table 2, iterative hypothesis refinement performs well with GPT-40, but with weaker GPT-40-mini, it does not outperform the baseline. However, our MoC methodology consistently shows performance improvements regardless of the model's base performance.\nHypothesis Diversity To analyze the direct impact of MoC on hypothesis diversity, we observe the number of unique hypotheses under the same conditions as the previous accuracy experiment (K = 8). The number of unique hypotheses is measured using the method described in section 2.2. As shown in Table 3, MoC significantly increases the diversity of the hypotheses. This indicates that our methodology effectively minimizes semantically redundant hypotheses, which leads to improvements in inductive reasoning performance."}, {"title": "4.3 Analysis and Discussion", "content": "In this section, we raise several research questions related to MoC and address them.\nDoes the benefit of MoC arise from hypothesis diversity or from an improvement in reasoning? It has been observed that LLMs can improve their reasoning ability by generating intermediate steps before the final answer, a method also known as Chain-of-Thought (CoT) prompting (Wei et al., 2022). From this perspective, MoC, which first generates elementary concepts and then uses them to create hypotheses, shares similarities with such CoT prompting approaches.\nTo isolate the benefits of the reasoning ability from that on the hypothesis diversity, we set the temperature to 0 and generate only one hypothesis to observe if there is an improvement in accuracy. As shown in Table 4, the MoC method does not show significant gains compared to the greedy baseline. Although there are certain model-dataset pairs where performance significantly improved (e.g., GPT-40-mini on Playgol-str), no consistent trend was observed. Therefore, we conclude that the performance improvements observed with MoC in Table 2 and 8 are mostly due to the increase in hypothesis diversity rather than improvements in reasoning ability.\nWhat impact does scaling compute have on the MoC? In the sampling-based inductive reasoning discussed in this work, if a larger compute budget is provided, performance improvement is almost guaranteed by sampling additional hypotheses. We investigate whether MoC shows a similar scaling effect, as observed in Figure 4, when the number of generated hypotheses K is set higher. We use GPT-40-mini as base LLM for the scaling experiment.\nAs shown in Figure 7, both the IID baseline and MoC demonstrate higher average accuracy with larger number of samples K. Additionally, MoC achieves similar performance while generating only half the number of hypotheses relative to the baseline, demonstrating a significant improvement in the efficacy of inductive reasoning. Full results of scaling experiment are in Appendix C.\nWhat impact does generating multiple hypotheses per concept have on performance? In previous experiments, we generate only one hypothesis using a single concept as a hint. However, some concepts carry richer meanings than others, which may lead to the generation of multiple hypotheses. Additionally, if there is an error in the process of implementing the Python program, resampling may generate an error-free implementation.\nIn Table 5, we examine how the number of concepts (C) and the number of hypotheses sampled per concept (S) affect performance. When we compare results while keeping the total number of hypotheses the same, we find that sampling about two hypotheses per concept performs better than other settings. In conclusion, balancing C and S is critical for achieving optimal performance.\nDoes MoC assist in solving highly challenging problems beyond improving efficiency? The two examples in Table 6 represent highly challenging problems in which more than 500 IID samples from GPT-40-mini fail to identify the correct rule. In these problems, MoC effectively discovers the rule using far less compute (K = 64). This indicates that challenging problems with a low probability of being solved through IID sampling can be effectively resolved using the MoC approach. In Appendix D, we provide a full list of generated concepts for these problems."}, {"title": "5 Related Work", "content": "Automating the process of inductive reasoning has long been a significant challenge in AI, due to its central role in achieving human-level intelligence (Lake et al., 2015; Ellis, 2023; Ellis et al., 2021) and its practical applications, such as Programming-by-Example (PBE) (Menon et al., 2013; Cambronero et al., 2023; Gulwani, 2011). Before the advent of LLMs, neural networks were widely used to guide program search but their search spaces are often limited by domain-specific languages (DSLs) (Chen et al., 2019; Shi et al., 2022; Odena and Sutton, 2020; Odena et al., 2021; Clarke et al., 2010; Lee et al., 2023).\nRecently, LLMs have significantly transformed the paradigm of automatic inductive reasoning. They are employed mainly in two ways: directly prompting with training input-output pairs to predict outputs for unseen inputs (Moskvichev et al., 2023; Mirchandani et al., 2023; Webb et al., 2023; Sun et al., 2024), or formulating hypotheses in natural language or executable programs and applying them to unseen test inputs (Li and Ellis, 2024; Shao et al., 2024).\nIterative refinement is a popular methodology in the latter domain (Wang et al., 2024; Qiu et al., 2024; Greenblatt, 2024; Tang et al., 2024); however, they are sequential in nature and can slow down inference. Furthermore, it has been suggested that LLMs lack the ability to self-correct (Huang et al., 2024), and that such refinement offers no substantial advantages compared to drawing more IID samples (Acquaviva, 2024; Olausson et al., 2024)."}, {"title": "5.1 Automatic Inductive Reasoning", "content": "Recently, various methodologies have been explored to enhance the reasoning abilities of LLMs in inference time (Wei et al., 2022; Zheng et al., 2024). When allocated an increased inference budget, they are capable of solving complex reasoning tasks (Brown et al., 2024; Snell et al., 2024). By leveraging these properties, LLMs have shown promising reasoning capabilities to solving diverse reasoning tasks within a few iterative processes. For example, Yao et al. (2023); Lanchantin et al. (2023) suggest an iterative generate-and-self-reflect approach to address complex reasoning tasks. Additionally, Kumar et al. (2024) show the self-correction capabilities of LLMs, and Zhou et al. (2024) illustrate how paraphrasing input prompts can enhance reasoning capabilities."}, {"title": "5.2 Reasoning in Inference Time of LLMS", "content": "In this paper, we study the diversity of hypotheses generated by an LLM during inductive reasoning. We find that, even if incorrect, these hypotheses are predominantly redundant, leading to wasted compute. Increasing the LLM's temperature initially helps, but benefits saturate at certain point and excessively high temperatures cause text degeneration. Therefore, we propose a method called Mixture of Concepts (MoC); the core idea is to generate non-redundant concepts and using these to formulate hypotheses. Our approach significantly improves performance across various datasets and models."}, {"title": "6 Conclusion", "content": "Our MoC methodology requires additional computation during the concept proposal process prior to hypothesis generation. Additionally, during the hypothesis generation phase, each concept is used as a hint to construct prompts, so the model should encode more tokens compared to IID sampling. However, this incurs relatively small overhead compared to refinement or summarization-based methods found in existing works.\nFurthermore, the actual process of inductive reasoning in humans is far more complex than that of MoC. Humans are capable of solving difficult problems by transforming concepts and composing multiple concepts in various ways. These aspects have not been addressed in this study."}, {"title": "Limitations", "content": "Here, we provide prompts for concept proposal (MoC) and hypothesis generation (baseline and MoC). These prompts were constructed based on those from Hypothesis Search (Wang et al., 2024)."}, {"title": "A Prompts", "content": "In above prompts, TRAIN_EXAMPLES represents all the train inputs and outputs of given inductive reasoning problem. INPUT_FORMAT and OUTPUT_FORMAT specify input and output formats. For instance, in List Functions dataset,"}, {"title": "B Experimental Details", "content": "In this paper, we use 2 Nvidia A100 GPUs for running experiments with open-source LLMs. The evaluation on four datasets takes approximately 1 to 2 days. We conduct experiments using Llama"}, {"title": "C Additional Results", "content": "Here, we provide additional experimental results. Table 7 shows test accuracy of various models in various K. In Table 8, we report full results of scaling experiment in section 4.3."}, {"title": "D Generated Concepts", "content": "We provide a complete list of 64 concepts for the MiniARC and MBPP+ problems shown in Table 6."}, {"title": "E Information about Use of AI Assistants", "content": "In writing this paper, we utilized ChatGPT3 for paraphrasing."}]}