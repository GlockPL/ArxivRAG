{"title": "Dynamic Strategy Planning for Efficient Question Answering with Large Language Models", "authors": ["Tanmay Parekh", "Pradyot Prakash", "Alexander Radovic", "Akshay Shekher", "Denis Savenkov"], "abstract": "Research has shown the effectiveness of reasoning (e.g., Chain-of-Thought), planning (e.g., SelfAsk), and retrieval augmented generation strategies to improve the performance of Large Language Models (LLMs) on various tasks, such as question answering. However, using a single fixed strategy to answer different kinds of questions is suboptimal in performance and inefficient in terms of generated output tokens and performed retrievals. In our work, we propose a novel technique DyPlan, to induce a dynamic strategy selection process in LLMs, to improve performance and reduce costs in question-answering. DyPlan incorporates an initial decision step to select the most suitable strategy conditioned on the input question and guides the LLM's response generation accordingly. We extend DyPlan to DyPlan-verify, adding an internal verification and correction process to further enrich the generated answer. Experiments on three prominent multi-hop question answering (MHQA) datasets reveal how DyPlan can improve model performance by 7-13% while reducing the cost by 11-32% relative to the best baseline model.", "sections": [{"title": "1 Introduction", "content": "Question-answering (QA) for large language models (LLMs) spans a range of question types, from simple queries to those requiring reasoning, external knowledge, step-by-step planning, or a combination of these strategies. For example (Figure 1), modern LLMs can easily answer Who was the first president of USA? but may need some reasoning to figure out At what age did Roger Federer win his first Grand Slam Title?, while Who's contending the 2024 US Presidential Elections? requires the model to retrieve up-to-date external information. To this end, previous works have investigated various strategies to induce reasoning, such as Chain of Thought (Wei et al., 2022), Tree-of-Thought (Yao et al., 2023a); or planning, such as SelfAsk (Press et al., 2023), Decomposed Prompting (Khot et al., 2022), StepBack Prompting (Zheng et al., 2024a); or incorporating external knowledge through Retrieval Augmented Generation (Lewis et al., 2020) and Knowledge Graphs (Pan et al., 2024).\nHowever, employing a single strategy for all different types of questions is sub-optimal as well as quite cost-ineffective in terms of generated tokens and retrievals. As humans, we rather employ a dynamic decision phase to first determine the most effective strategy before answering the given question. Similarly, we expect that if the model possesses sufficient self-knowledge to directly answer the question, then \u2018thinking step-by-step' and expending tokens on reasoning is unnecessary. In other cases, a model may not have enough confidence to answer directly and should spend some computation on additional reasoning. However, not having enough information about the topic should warrant external retrievals instead.\nTo this end, we propose to induce a human-like cognitive ability in LLMs through our novel technique, DyPlan (Dynamic Planning). As illustrated in Figure 1, DyPlan introduces an initial"}, {"title": "2 Methodology", "content": "To mimic cost-effective human cognitive thinking in LLMs, we propose our novel technique - Dy-Plan. Unlike traditional approaches that rely on a single fixed strategy for all questions, our technique employs dynamic strategy planning to determine the most effective approach for each question. We extend DyPlan to DyPlan-verify by incorporating additional verification and re-attempting the question with alternative strategies if necessary. We first motivate the potential impact of dynamic strategy planning in \u00a7 2.1 and later provide specific details about our techniques."}, {"title": "2.1 Motivation", "content": "Dynamic strategy planning can help reduce inference costs by simply selecting lower-cost strategies to answer simpler questions. In our work, we additionally posit that it can improve model performance as strategy selection can act as an ensembling method. We verify this hypothesis through a simple analysis using two strategies of Direct and Reason. Direct prompts the LLM to directly provide the answer to the question while Reason utilizes Chain-of-Thought (Wei et al., 2022) to answer step-by-step. Utilizing these strategies, we prompt Llama3-8B-Instruct (Dubey et al., 2024), evaluate using F1 score on 1000 samples from 2WikiMultihopQA (Ho et al., 2020) dataset, and show their performances as a Venn diagram in Figure 2. Generally, one can assume that adding reasoning should only allow us to further answer harder questions while still being able to answer all questions that Direct strategy answers correctly. However, there is a significant contribution of 7.1% F1 where Reason is incorrect, but Direct is correct (for example, a mistake or hallucination can lead to an incorrect result). We notice similar patterns across datasets and strategies (shown in \u00a7 B.1). Such patterns throw light on how the choice of an appropriate strategy can also improve model performance."}, {"title": "2.2 DyPlan Components", "content": "Our techniques majorly utilize three components in a plug-and-play manner: (1) Decision, which selects a strategy to follow; (2) Execution, which generates the answer using the chosen strategy; and (3) Verification, which evaluates the answer's correctness. We describe them in detail below and provide a high-level overview diagram in Figure 3.\nDecision: The Decision component is the core of our technique, responsible for dynamically selecting the optimal strategy for a given question. This is achieved by presenting an LLM with a strategy pool with their descriptions and prompting it to leverage its self-confidence to choose the most suitable and efficient strategy. This component provides an opportunity to optimize efficiency while still enabling powerful reasoning to improve performance when possible, unlike OpenAI's o1 model, which currently applies thinking even for simple questions. We provide an illustration prompt of this component in Figure 3(A).\nExecution: The Execution component involves prompting the model to apply the selected strategy from the Decision component to generate an answer to the question. Although analogous to fixed-strategy prompting, our approach is different since we enable dynamic strategy execution based"}, {"title": "2.3 DyPlan Pipeline Flow", "content": "DyPlan majorly achieves cost minimization by dynamic decision-making in the Decision phase and restricting generation output space for each component. The base version of DyPlan (also referred to as DyPlan-base) employs a low-cost Decision-Execution pipeline (pink arrow in Figure 3). On the other hand, our extension technique DyPlan-verify utilizes an iterative loop of Decision-Execution-Verification (orange arrow in Figure 3). If verification fails, the pipeline reverts to the Decision component to select an alternative strategy; otherwise, it exits the loop with the execution answer. This iterative loop runs for a preset number of rounds based on the inference budget or until the Deci-"}, {"title": "3 Data Creation for Finetuning", "content": "To adhere LLMs with the DyPlan pipeline, we fine-tune the LLM on DyPlan-specific data. To ensure zero human annotation cost, we propose automatic data creation utilizing an existing QA dataset D and a strategy set S comprising n strategies. We order S = [$1,..., $n] by strategy preference, with s1 being the most preferred and sn the least preferred (e.g. in order of cost-efficiency/performance). For each strategy s \u2208 S, we prompt the base LLM on all datapoints d \u2208 D and evaluate the results against the ground truth. This yields two disjoint subsets for each s: D^+, comprising datapoints where s produced the correct answer, and D^-, comprising the remaining datapoints where s failed to produce the correct answer. Utilizing the base LLM (instead of distilling from larger LLMs) ensures a stronger model self-calibration. Using the positive and negative subsets, we create component-specific data for DyPlan (described below) and train the LLM on the combination of all the component data. We conduct training only on the last-turn response for the multi-turn training instances.\nDecision: We define an optimal mapping function f* : D \u2192 S that assigns each training datapoint d \u2208 D to the first strategy s \u2208 S (according to the preference order) that yields the correct answer. If none of the strategies produce the correct answer, d is mapped to the least preferred strategy sn. The Decision component's training data consists of mapped input-output pairs (d, f*(d)).\nExecution: The input here is a multi-turn chat where the first turn (Decision) selects a strategy s. In the second turn (Execution), the output is set as the base LLM generation using strategy s. Utilizing the base LLM response aids efficient and faster model training. To minimize noise, we utilize only the positive data D^+ for each strategy s.\nVerification: For this component, we create binary training data by mapping positive data D^+ to \"yes\" and negative data D^- to \"no\". Multi-turn traces are generated by forcing the selection of strategy s in the first turn (Decision) and using the base LLM response in the second turn (Execution)."}, {"title": "4 Experimentation Details", "content": "We describe the benchmarking datasets and the evaluation metrics. Next, we discuss the strategies, baselines, and, finally, the implementation details.\nBenchmarking Datasets: LLMs seem to perform well on simpler question-answering datasets like SQUAD (Mavi et al., 2024). Instead, we consider three complex Wikipedia-based multi-hop QA (MHQA) datasets to benchmark the performance of our technique, namely HotpotQA (Yang et al., 2018), 2WikiMultihopQA (2WikiQA) (Ho et al., 2020), and Musique (Trivedi et al., 2022). HotpotQA was one of the first human-created MHQA datasets with upto 2-hop reasoning questions. 2WikiMultihopQA further improved over HotpotQA by improving the complexity and reasoning depth of the questions. Musique is a rule-based constructed dataset created by composing different single-hop questions. These unique challenges of each dataset aid extensive benchmarking. We utilize 1000 samples from the development sets of these datasets as the main evaluation dataset."}, {"title": "Evaluation Metrics:", "content": "We evaluate the models on two major dimensions of performance and cost. For performance, we utilize Exact Match (EM) and F1 score evaluated against the ground truth - higher the better. For cost, we consider the number of generated tokens (# T) and number of retrievals (# R) - lower the better. For DyPlan, we report the aggregated cost metrics across the turns."}, {"title": "Strategies:", "content": "We focus on four major themes of strategies, as follows:\n1. Direct: LLM is prompted to directly provide the final answer. This is the cheapest strategy in terms of cost.\n2. Reason: LLM is prompted to reason to reach the final answer. We utilize Chain-of-Thought (CoT) (Wei et al., 2022) to reason step-by-step. This strategy is more expensive than Direct in terms of generated tokens.\n3. Plan: LLM is prompted to decompose the question as part of planning and reason through the atomic questions to reach the final answer. We utilize SelfAsk (Press et al., 2023) as a prototype for this strategy. This is the most expensive in terms of generated tokens.\n4. Retrieval: Following RAG (Lewis et al., 2020), using the question as the query, three external passages retrieved from Wikipedia are fed to the LLM. LLM is prompted to reason to reach the final answer. This strategy is expensive in terms of retrievals."}, {"title": "Baseline Models:", "content": "As baselines, we consider: (1) Fixed-base prompts the base LLM with a single fixed strategy, (2) Fixed-sft prompts a LLM fine-tuned on the fixed strategy using the positive base LLM traces, (3) Classifier trains an external classifier to choose the strategy and chooses the corresponding fine-tuned LLM response, (4) Ensemble simply outputs the majority ensemble using all the Fixed-sft strategy responses.\nAdditionally, we consider some similar works utilizing dynamic decision-making as reference such as: (5) ReAct (Yao et al., 2023b) uses thoughts-actions-observation tuples to guide model generation. (6) DRAGIN (Su et al., 2024) utilizes dynamic retrieval based on model entropy. Both these baselines are orthogonal to our work and can be utilized in a complementary manner as individual strategies for DyPlan. We majorly compare the cost-effectiveness of DyPlan with these techniques."}, {"title": "Implementation Details:", "content": "For all experiments, we utilize the LLaMa3-8B-Instruct model (Dubey et al., 2024) as the base LLM. We set the strategy order in increasing order of model performance as Direct-Plan-Reason-Retrieval for training data creation. We use Low-Rank Adaptation (Hu et al., 2022) with rank 32 using LLaMa-Factory (Zheng et al., 2024c) for fine-tuning the base LLM. We utilize code from DRAGIN (Su et al., 2024) to implement the fixed strategy baselines as well as evaluate our techniques. Our reported numbers are averaged scores over three runs. Additional details and hyperparameters are provided in Appendix A."}, {"title": "5 Results", "content": "We present our main results comparing DyPlan utilizing all the strategies with other baselines in Table 1. We utilize the best-performing Fixed-sft Retrieval model as the reference baseline for comparisons. We also aggregate these metrics across datasets and plot Performance (F1 score) v/s Efficiency (weighted sum of # T and # R)2 in Figure 5.\nWe note that external classifiers help reduce cost but don't improve model performance - demonstrating the difficulty of the task. Dynamic decision-making frameworks like DRAGIN and Ensemble improve performance but are 2-5x more expensive. To this end, DyPlan provides the best balance with an average reduction of 32% token and 26% retrieval cost along with relative performance improvements of 7% EM and 7% F1. DyPlan-verify further improves performance with average relative gains of 13% EM and 12% F1 while reducing the token and retrieval cost by 11% and 19% respectively. In the best case scenario on 2WikiMultihopQA, DyPlan shows 16% performance gains"}, {"title": "6 Analysis", "content": "We conduct additional experiments to better understand the quality of DyPlan decision-making and verification and its generalization across datasets."}, {"title": "6.1 Calibration Analysis", "content": "In \u00a7 3, we defined an optimal policy f* for each question as a strategy to pick the most cost-effective technique that yields the correct answer. Here, we analyze model calibration, that is, how well its"}, {"title": "6.1.1 Decision component of DyPlan", "content": "We compare the strategy planning distribution of various techniques with the optimal policy for 2WikiMultihopQA in Figure 6. The major difference is the usage of Plan and Reason which are nearly 0% for Fixed/Classifier approaches. On the other hand, DyPlan-base and DyPlan-verify are closer to the optimal distribution. We quantify this proximity of the probability distributions in terms of KL divergence. DyPlan-base and DyPlan-verify achieve low scores of 0.066 and 0.014, respectively, while the classifier baseline has a high divergence score of 0.35. This highlights the better strategy planning and stronger calibration of DyPlan."}, {"title": "6.1.2 Verification analysis of DyPlan-verify", "content": "We study the verification precision, answer rejection rate (% datapoints verified as \"no\"), and the change in strategy distribution pre and post-verification (in terms of KL divergence relative to optimal strategy) to gain a deeper understanding of the impact of verification in DyPlan-verify. We provide these statistics in Table 3. The huge drops in KL-divergence post-verification demonstrate how verification aids better alignment to the optimal policy and, thus, improves model calibration. The low rejection rate ensures the cost doesn't increase significantly, while the high verification precision underlines the strong utility of the verification step."}, {"title": "6.1.3 Fine-tuning improves calibration", "content": "Choosing the right strategy in a 0-shot way is difficult, as models don't surely know what they know and don't know (Yin et al., 2023a). We analyze the 0-shot DyPlan strategy planning in Figure 6 and note how the 0-shot model mostly resorts to the most expensive strategy, while fine-tuning helps to learn the patterns between questions and model capabilities. We also compare the model performance of 0-shot / few-shot DyPlan with fine-tuned DyPlan in Table 4 for 2WikiMultihopQA. Clearly, the non-fine-tuned models fail to improve over the fixed strategy baseline, but fine-tuning provides strong performance gains - demonstrating how fine-tuning strongly improves calibration for strategy planning."}, {"title": "6.1.4 Optimal Policy Upper Bound", "content": "We study the upper bound for DyPlan to motivate possibilities of future improvements. Specifically, we replace the DyPlan's Decision component with the optimal policy and use the corresponding fixed strategy base LLM outputs for Execution. We compare this upper bound with DyPlan in Table 5 with \u0394, indicating further potential improvement. While Musique exhibits a low \u0394 of 2-3 F1 points, the larger \u0394 of 10-12% F1 for the other datasets provides promise to further explore strategy planning."}, {"title": "6.2 Generalization Analysis", "content": "To assess the generalization of DyPlan, we fine-tune it on combined data from the three benchmark datasets. To ensure a fair comparison, the combined data comprises 20k datapoints (same as individual data) with equal shares from the three datasets. We compare the performance of this combined-data fine-tuned model with the individual-data fine-tuned model in Figure 7 and note how the performances are nearly similar for both models. The cost analysis for the combined data model (Table 17 in \u00a7 B.4) reveals at-par levels of cost as well. Thus, this study reveals how the gains provided by DyPlan/DyPlan-verify are generalizable and not overfitting to a single dataset."}, {"title": "7 Related Works", "content": "Question Answering:\nQuestion-answering has been a prominent task with various popular benchmarking datasets like SQUAD (Rajpurkar et al., 2016, 2018), MS MARCO (Nguyen et al., 2016), TriviaQA (Joshi et al., 2017), and SearchQA (Dunn et al., 2017). These datasets are single-hop, i.e., they require simple reasoning to find the answer and are easier to answer. To develop complex reasoning in models, multi-hop question-answering (MHQA) datasets were developed like HotpotQA (Yang et al., 2018), 2WikiMultihopQA (Ho et al., 2020), Compositional Celebrities (Press et al., 2023), and Musique (Trivedi et al., 2022). To improve MHQA performance, works explored on improving the reasoning capabilities of LLMs using chain-of-thought (Wei et al., 2022; Kojima et al., 2022), auto-CoT (Zhang et al., 2023), self-consistency (Wang et al., 2023), tree-of-thought (Yao et al., 2023a). Another line of work focused on planning by question decomposition like Self-Ask (Press et al., 2023), ART (Paranjape et al., 2023), decomposed prompting (Khot et al., 2022) or using explicit planners like ReWOO (Xu et al., 2023), LLMCompiler (Kim et al., 2024), stepback (Zheng et al., 2024b). Works also focus on using external knowledge through retrieval like RAG (Lewis et al., 2020), Self-RAG (Asai et al., 2024), CRAG (Yan et al., 2024) with recent works like IRCOT (Trivedi et al., 2023), FLARE (Jiang et al., 2023b), SynCheck (Wu et al., 2024), and DRAGIN (Su et al., 2024) exploring dynamic retrieval.\nAgentic LLMs:\nRecent works have explored LLMs as decision-makers, especially in interactive environments, as agents deciding a policy. WebGPT (Nakano et al., 2021) utilized LLMs to search the web to answer complex questions. Some works have also been explored in conversational modeling like BlenderBot (Shuster et al., 2022), SimpleTOD (Hosseini-Asl et al., 2020), Tartan (Chen et al., 2020) and robotics like SayCan (Ichter et al., 2022) and Inner Monologue (Huang et al., 2022). ReAct (Yao et al., 2023b) was one of the earlier systems utilizing natural language thoughts and actions, followed by other works like Reflexion (Shinn et al., 2023) and CAMEL (Li et al., 2023).\nUncertainty estimation in LLMs:\nWith the increasing utilization of LLMs in various reasoning tasks, several works have studied LLM's confidence in its self-knowledge. Xiao and Wang (2021) show evidence of model uncertainty with increased hallucinations, while LLM's quantification about its self-knowledge is studied as honesty alignment by Yang et al. (2023). Kadavath et al. (2022) and Tian et al. (2023) discuss how LLMs are generally well-calibrated when for simpler tasks or in the presence of source information. On the other hand, Kapoor et al. (2024) and Yin et al. (2023b) show that LLM calibration about its self-knowledge is not good and explore how fine-tuning can further improve this calibration."}, {"title": "8 Conclusion and Future Work", "content": "In our work, we introduce the paradigm of dynamic strategy planning for question-answering mimicking human cognitive thinking through DyPlan with the goal of reducing inference costs and improving model performance. By adding verification and self-correction using DyPlan-verify, we further enhance the model output quality. Through experimentation on three MHQA datasets, we show strong efficacy and improved performance using our techniques. Our analyses and empirical bounds provide promise for further improvements. Incorporating partial thinking and integrating dynamic tool usage can be explored to further improve DyPlan. Utilizing alignment-based fine-tuning can further improve the model's effectiveness."}, {"title": "Limitations", "content": "We present a prototype for our technique DyPlan to selectively choose strategies in our work. We haven't evaluated it extensively on all possible strategies, tools, and models and we leave it for future work. Our technique is not restricted to question-answering and is generalizable to other tasks as well. But in this work, we only show experiments and results on question-answering. We haven't optimized our technique or explored changing the hyper-parameters or the prompt. It might be possible to improve the model by further engineering, but again, we leave it up to future work. For fine-tuning, we limit ourselves to LoRA and smaller models (8B) only owing to budget constraints. Full fine-tuning and exploring larger LLMs might be faster and better and can be explored in future works."}, {"title": "Ethical Considerations", "content": "We utilize LLMs to partially correct and rewrite parts of our paper. Since we work with generative models, there's little control on the text/tokens. It is possible that the model can generate spurious or unsafe content and we haven't evaluated our trained models for it. In general, fine-tuning has been prone to reducing the robustness of LLMs for other tasks/skills or introducing additional biases due to spurious patterns in training. We haven't evaluated the models for robustness or general safety. Finally, our work promotes using less retrieval in favor of reducing inference generation costs. Previous works have found an inverse correlation between hallucinations and knowledge grounding in external documents. So, our work can induce more hallucinations at the cost of reducing inference costs, and this should be taken into consideration before using our work."}, {"title": "A Additional Implementation Details", "content": "In this section, we provide additional details about our implementation and hyperparameters of each technique."}, {"title": "A.1 General Implementation Details", "content": "All of our experiments were conducted on an NVIDIA RTX A100 machine with support for 8 GPUs. Fine-tuning runs took about 6-18 hours to complete using distributed training on 4 GPUs. Inference was faster and would be completed in 1-2 hours on a single GPU. Our base LLM for all experiments was Llama3-8B-instruct (Dubey et al., 2024), specifically its Huggingface release. We average the main results for most techniques over three runs. Final inference was run with temperature = 0.4 leading to low variance in model performance."}, {"title": "A.2 Fixed Strategy Implementations", "content": "We self-implemented the simple Direct strategy. We utilized the codebase of DRAGIN (Su et al., 2024) to implement the Chain-of-Thought (CoT) (Wei et al., 2022) and RAG (Lewis et al., 2020) strategies. We utilized a BM25 retrieval system indexed on the entire Wikipedia and capable of retrieving intermediate excerpts of length 200 based on the query. We provide the top three passages as the retrieved passages for RAG. For SelfAsk (Press et al., 2023), we utilized their original codebase. If any of the strategy inferences weren't able to provide their answer within the max generation length limit, we used force-decoding with a preset prefix \"Final answer:\" to get the final answer. Other specific hyperparameters are provided for each strategy in Tables 6, 7, 8, and 9."}, {"title": "A.3 DyPlan Implementation", "content": "We describe the prompts and multi-turn setting of our model DyPlan in \u00a7 2. We provide specific hyperparameters for the non-fine-tuned version of"}, {"title": "A.4 Fine-Tuning Details", "content": "We utilize LoRA (Hu et al., 2022) for fine-tuning the base LLM for fixed strategy and DyPlan. We utilize the LLaMa-Factory (Zheng et al., 2024c) and their codebase for the fine-tuning and inference. We provide the hyperparameters for this tuning in Table 11."}, {"title": "A.5 Classifier Implementation", "content": "As a baseline, we train a multi-class classifier with each strategy as a separate class to select an appropriate strategy based on the question. We experimented with utilizing binary classifiers for each strategy but the multi-class classifier performed better. We utilize the codebase from XTREME (Hu et al., 2020) to implement the classifiers. We utilize RoBerta-large (Liu et al., 2019) as the base model. We provide additional details about the hyperparameters in Table 12."}, {"title": "A.6 Majority Ensemble Implementation", "content": "We implement a simple majority ensemble wherein we utilize the final answers from the fixed strategy models and aggregate them using a majority function. In case of a tie, we choose the final answer of the better strategy in the hierarchy. In 2-3 strategy cases, this leads to aligning with the best-fixed strategy method itself."}, {"title": "A.7 ReAct Implementation", "content": "As a reference for costs, we also included a baseline for ReAct (Yao et al., 2023b). We utilize their original codebase for the implementation. Utilizing the Instruct version of Llama3-8B didn't work as well, instead we utilize the non-instruct-tuned version of this model Llama3-8B for this baseline. We utilize six in-context examples for the prompt. Additionally, to keep a fair comparison and reduce token generation costs, we do forced decoding stopping for the keywords of \"Thought:\", \"Action:\" or \"Observation\". This avoids any unnecessary token generations or when the model starts to repeat itself. We notice that this model works well with larger LLMs, but the planning and performance are poor with smaller LLMs."}, {"title": "A.8 DRAGIN Implementation", "content": "As a reference for costs, we also included a baseline for ReAct (Su et al., 2024) implemented using their original implementation codebase. We provide specific hyperparameters of this model in Table 13."}, {"title": "B Additional Experimental Results", "content": ""}, {"title": "B.1 Hierarchy Violations for other datasets", "content": "In \u00a7 2.1, we motivated how strategy selection acts as an ensemble for the Direct-Reason strategy combination. Here, we provide more evidence to support this claim across four strategies - Direct, Plan, Reason, and Retrieval - and multiple datasets."}, {"title": "General Hierarchy:", "content": "For the four strategies mentioned above, a general hierarchy we assume is Direct < Plan < Reason < Retrieval. If the model knows the answer directly, then it should be able to plan/reason to provide the answer. Thus, Direct is the lowest in this hierarchy. Comparing Plan and Reason we assume Plan is a special kind of reasoning with a specific focus on breaking the question into atomic units. On the other hand, there are several questions like \u201cWho was the actor who starred in an Avengers movie and has three children?\u201d where breaking into atomic questions will not help to answer the question. Thus, we assume Plan < Reason. Finally, Retrieval brings in additional external information compared to Reason ranking Reason < Retrieval."}, {"title": "Hierarchy Violations:", "content": "In an ideal world, the LLM should follow this hierarchy, and we should simply use Retrieval all the time to optimize model performance. However, owing to various reasons like non-relevant retrievals, incorrect reasoning, rote learning, and spurious generations, this hierarchy is not maintained. We call these special cases as hierarchy violations."}, {"title": "Quantifying Violations:", "content": "Similar to the study in \u00a7 2.1, we quantify the F1 performance contribution of the hierarchy violations for all the four strategies"}, {"title": "B.2 Fine-tuning ablation Analysis", "content": "We provided basic ablation analysis highlighting how fine-tuning aids LLMs to be better calibrated to utilize DyPlan in \u00a7 6.1.3. Here we provide additional details about the experimental setup along with additional results on other datasets."}, {"title": "Non fine-tuned DyPlan:", "content": "For zero-shot model, we prompt the base LLM for the Decision component (i.e. to select the preferred strategy) in a zero-shot fashion. Based on the chosen strategy, the base LLM output from the fixed strategy run is used as the Execution component output. For the few-shot model, we utilize four few-shot examples for each strategy (16 in total). If the model outputs a strategy not defined in the list of provided strategies, we map it to the retrieval strategy (as"}, {"title": "Results:", "content": "We provided results for this study for the 2WikiMultihopQA dataset in Table 4. Here, we also provide similar comparisons on HotpotQA and Musique datasets in Tables 14 and 15, respectively. Across all the datasets, we can notice the sub-optimal performance of non-fine-tuned LLM runs with DyPlan. The zero-shot model mostly selects retrieval, while the few-shot model selects other strategies, but it's not well-calibrated. The calibration is poorer for few-shot DyPlanas the model gets heavily influenced by the in-context examples. In conclusion, we demonstrate how base LLMs by default are not calibrated well to utilize DyPlan, underlining the need for fine-tuning LLMs."}, {"title": "B.3 Decision-making of DyPlan", "content": "We discussed how DyPlan helps to better calibrate decision-making in terms of strategy selection for the 2WikiMultihopQA dataset in \u00a7 6.1.1. Here, we show similar analysis for the other datasets of HotpotQA and Musique in Figures 11 and 12. Similar to our earlier findings, we notice the DyPlan helps the strategy usage to be more similar to the optimal policy, in turn helping to improve model performance. We notice for HotpotQA, DyPlan is similar to the external classifier. DyPlan-verify, on the other hand, is strongly closer to the optimal policy for both HotpotQA and Musique."}, {"title": "B.4 Combined Data Fine-tuning", "content": "In \u00a7 6.2, we compared and discussed the performance difference for models fine-tuned on individual datasets relative to models fine-tuned on a single combined dataset. We provide the complete table with the performance numbers in Table 16. We also compare the costs of these models in Table 17. We observe that the costs for DyPlan with combined data are slightly more than the individual models. On the contrary, the costs for a combined model for DyPlan-verify are lesser than the individual model fine-tuning. Overall, the range of differences is quite small, and the combined model costs less than the best baseline as well."}, {"title": "B.5 Generalization with other LLMS", "content": "To validate the compatibility of DyPlan with other LLMs, we conduct a small study utilizing DyPlan and DyPlan-verify for Mistral-7B (Jiang et al., 2023a) model. We provide the experimental results for this model in Table 18 for the 2WikiMultihopQA dataset. The results demonstrate how Dy-"}, {"title": "C Qualitative Studies", "content": ""}, {"title": "C.1 Qualitative Examples for DyPlan", "content": "We provide some qualitative examples highlighting the cases where DyPlan provides stronger model performance and better efficiency compared to the best baseline of Fixed-sft Retrieval in Table 19. We also provide corresponding comments to indicate how DyPlan is better. Initial examples demonstrate cases wherein DyPlan is more efficient as well as correct. Some of these examples are also when both the methods use RAG - which throws light on improved reasoning ability by DyPlan training. At the bottom, we show examples where both give the right answer, but DyPlan is more efficient."}, {"title": "C.2 Qualitative Examples for DyPlan-verify", "content": "In Table 20, we show some examples wherein additional verification identifies and rectifies any potential mistakes made in the first round. Specifically, we also show examples (last two) wherein the first-round strategy was better but provided the wrong answer, while the second-round"}]}