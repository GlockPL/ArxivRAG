{"title": "GVMGen: A General Video-to-Music Generation Model with Hierarchical Attentions", "authors": ["Heda Zuo", "Weitao You", "Junxian Wu", "Shihong Ren", "Pei Chen", "Mingxu Zhou", "Yujia Lu", "Lingyun Sun"], "abstract": "Composing music for video is essential yet challenging, leading to a growing interest in automating music generation for video applications. Existing approaches often struggle to achieve robust music-video correspondence and generative diversity, primarily due to inadequate feature alignment methods and insufficient datasets. In this study, we present General Video-to-Music Generation model (GVMGen), designed for generating high-related music to the video input. Our model employs hierarchical attentions to extract and align video features with music in both spatial and temporal dimensions, ensuring the preservation of pertinent features while minimizing redundancy. Remarkably, our method is versatile, capable of generating multi-style music from different video inputs, even in zero-shot scenarios. We also propose an evaluation model along with two novel objective metrics for assessing video-music alignment. Additionally, we have compiled a large-scale dataset comprising diverse types of video-music pairs. Experimental results demonstrate that GVMGen surpasses previous models in terms of music-video correspondence, generative diversity, and application universality.", "sections": [{"title": "Introduction", "content": "In videos, music plays a critical role in enhancing emotional resonance by aligning rhythm, style, and affectivity to achieve a high degree of correspondence. Traditionally, this task falls within the purview of professionals, while amateurs often find it challenging, characterized by time-consuming processes and potential copyright infringement. Consequently, the automatic generation of music based on video presents significant utility for both amateurs and industry professionals.\nOne of the core challenges in video background music generation is identifying the cross-modal relationships between visual and musical elements. Previous studies (Di et al. 2021; Zhu et al. 2023; Yu et al. 2023; Zhu et al. 2022) define rule-based connections between specific variables, such as motion speed and color. However, these variables are only significant for certain types of videos which are less important for video blogs or documentaries (Corner 2002). Many other variables, such as shots and compositions, are overlooked despite the high relevance, particularly in movies. Studies like (Hussain et al. 2023; Tang et al. 2024) rely on Large Language Models (LLMs) as a bridge. However, LLMs often summarize the video and music into some static styles while ignoring details like emotions and the exact physical variables with temporal changes. Such explicit feature alignment may ignore high-related information which cannot be computed or described, while considering redundant unrelated features, thus limiting the depth and coherence of the music-video correspondence.\nMoreover, music features derived from variable or language transformations are neither diverse nor detailed enough to guide vivid and artistic music generation. Many of these approaches are even constrained to MIDI music (Su, Liu, and Shlizerman 2020; Gan et al. 2020), which simply encodes each musical note as a numeric symbol. Consequently, the generated music from these models tends to be monotonic and lacks rich diversity and universality.\nIn addition, inadequate evaluation metrics and datasets further constrain the effectiveness of video-to-music generation models. Most existing works assess music-video correspondence primarily through subjective evaluation (Ji, Luo, and Yang 2020; Sur\u00eds et al. 2022), which is costly and often biased, failing to guide the model training correctly. Existing datasets (Zhuo et al. 2023; Kang, Poria, and Herremans 2024) primarily consist of music videos (MVs) with music in MIDI format. These datasets exhibit low diversity and weak music-video correspondence, thereby limiting the efficacy of models trained on them.\nIn this paper, we propose a General Video-to-Music Generation model (GVMGen), which can generate high-related music in various styles for different types of video. Unlike previous models, GVMGen refrains from explicitly defining variable relationships or relying on language transformation between visual and musical features. Instead, we extract hidden visual features through spatial self-attention and transform them into musical features through both spatial and temporal cross-attention. By adopting an implicit attention mechanism for feature transformation, GVMGen preserves the most relevant features in alignment, thereby enhancing the music-video correspondence. Moreover, implicit feature extraction and alignment are well-suited for different styles of video and music, enabling GVMGen to be a general model that performs well even in zero-shot cases. Moreover, we propose an evaluation model with two novel objective metrics assessing both global cross-modal relevance and local temporal alignment. We also collect a large-scale video-music dataset that encompasses a diverse range of styles, including movies, video blogs (vlogs), and more, rather than relying solely on MVs in MIDI format. Furthermore, our dataset includes a significant portion of Chinese traditional music performed on over ten types of instruments. Chinese traditional music emphasizes diverse fingering techniques and complex timbres, which cannot be represented by MIDI files and simple musical variables. This inclusion introduces a higher level of difficulty but also enhances the diversity in music generation.\nExperimental results reveal that GVMGen exhibits robust performance, particularly in terms of music-video correspondence and music richness. Both the generative similarity with ground truth and the quality of the music are improved simultaneously. GVMGen can generate multi-track waveform music over MIDI in both Chinese and Western styles, marking a pioneering advancement in the richness and completeness of music generation. Furthermore, GVMGen demonstrates remarkable universality, enabling high-quality music generation even in zero-shot scenarios.\nIn summary, our main contributions can be written as:\n\u2022 We propose GVMGen, a general video-to-music generation model based on hierarchical attentions, capable of generating diverse genres of music highly related to different styles of videos.\n\u2022 We propose an evaluation model with objective metrics for local and global music-video correspondence evaluation, and also collect a large-scale video-music dataset encompassing multiple styles of both video and music.\n\u2022 We conduct extensive experiments which show that our model can outperform state-of-the art models significantly in terms of video-music correspondence, music diversity and application universality."}, {"title": "Related Work", "content": "Music Generation can be divided into symbolic music generation and waveform music generation. For symbolic music generation, MIDI-VAE (Brunner et al. 2018) and MusicVAE (Roberts et al. 2018) adopt variational autoencoder while Music Transformer (Huang et al. 2018) and MuseGAN (Dong et al. 2018) use attention-based sequence generation and adversarial generation techniques. These models can only generate MIDI music. For waveform music generation from text description, Riffusion (Forsgren and Martiros 2022) employs the pretrained Stable Diffusion model to transform text-to-music process into a text-to-spectrogram task, thereby enabling the generation of music. As for large music generation models, Google proposes MusicLM (Agostinelli et al. 2023) based on Mulan (Huang et al. 2022) and SoundStream (Zeghidour et al. 2021). MusicGen (Copet et al. 2024) uses T5 (Raffel et al. 2020) as a text encoder and utilizes quantized music codes from Encodec (D\u00e9fossez et al. 2022) for generation, while Stable Audio (Evans et al. 2024) adds a diffusion UNet into MusicGen. These models provide a foundational structure for works related to music generation.\nVideo Background Music Generation is proposed by (Di et al. 2021) which uses rule-based computation to predict music features. V-MusProd (Zhuo et al. 2023), V2Meow (Su et al. 2023) and Video2Music (Kang, Poria, and Herremans 2024) rely more on deep neural network to extract visual features for music generation and propose several evaluation metrics. With the help of LLMs, CoDi (Tang et al. 2024) and M2UGen (Hussain et al. 2023) use LLMs as a bridge to achieve cross-modal generation through the help of semantic description. (Li et al. 2024) proposes a diffusion generation model with segment-aware cross-attention. However, the music-video correspondence is still constrained due to the limited explicit features or insufficient alignment. Therefore, we adopt hierarchical attentions to do cross-modal feature alignment in both spatial and temporal aspects which is more accurate and universal.\nVideo-music Dataset is a kind of multi-modal dataset which is dedicated to video background music generation task. AIST++ (Li et al. 2021) and TikTok (Zhu et al. 2022) datasets contain dance videos, accompanied with music and visual motion information. However, these datasets are limited by its style diversity and total duration. Then, SymMV (Zhuo et al. 2023) and MuVi-Sync (Kang, Poria, and Herremans 2024) datasets provide over 50 hours of music videos with music feature annotations, primarily suitable for symbolic music generation. Recently, with the strong comprehension and language processing ability of LLMs, M\u00b2UGen proposes a systematic approach for generating datasets through music oriented instructions. (Hussain et al. 2023). Since these dataset are not logically suitable and diverse enough, we collect a large-scale dataset encompassing both Chinese traditional music and western music with various types like movies, vlogs and so on."}, {"title": "Method", "content": "In this section, we first define the video background music generation problem, then present GVMGen model in details, together with theoretical derivation and analysis."}, {"title": "Problem Definition", "content": "In video background music generation, suppose we are given a dataset with N samples of video and music pairs. We use \\(V\\in \\mathbb{R}^{t\\times f\\times H\\times W\\times C}\\) to denote each piece of video, where t, f, H, W, C stands for duration, video frame rate and height, width, number of channels of image separately. Music is denoted as quantized codes \\(M \\in \\mathbb{R}^{t\\times f'\\times K}\\). f' stands for music code sample rate and K stands for the number of codebooks. The training tuples \\((V, M)_{N_{train}}\\) contain \\(N_{train}\\) instances while other \\(N - N_{train}\\) samples form the test set. The goal of video background music generation is to accurate generate M for the test set which is more similar to the original high-related music."}, {"title": "General Video-to-Music Generation Model", "content": "As shown in Figure 1, GVMGen is an end-to-end video-to-music model leveraging hierarchical attentions. Initially, visual features are extracted from the video through visual feature extraction module using spatial self-attention. Subsequently, a feature transformation module equipped with trainable music queries filters the extracted visual features to retain those relevant to music through spatial cross-attention. Finally, the conditional music generation module is facilitated by temporal cross-attention. All features are processed as deep hidden features, which is fairly appropriate for different styles of video and music. As a result, GVMGen is able to focus on the most related feature with minimal information loss, thereby generating diverse music with high relevance to the video.\nAttention mechanism is proposed by (Vaswani et al. 2017) as a main component of FFTs, which is a function mapping a query and a set of key-value pairs to an output. In detail, the weight is calculated by a compatibility function (usually dot product) of the query with the corresponding key. After normalization, the weights are assigned to each value. We set Q, K and V stand for query, key and value separately while Dk stands for the dimension of key, the attention mechanism f(\u00b7) can be written as:\n\\[f(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{D_k}})V\\]\nIntuitively speaking, attention mechanism calculates the relevance between key-value pairs and queries. A higher weight indicates that the pair is more relevant to the query.\nVisual feature extraction module. Video cannot be directly transferred into music as they belong to different latent spaces. Therefore, the cross-modal relationship must be built on related features.\nSince deep features can preserve a greater amount of information than several variables, in GVMGen, we use a pretrained VIT-L/14@336px (Dosovitskiy et al. 2021) with spatial self-attention to extract deep visual features. ViT is the image encoder of CLIP (Radford et al. 2021), which splits an image into p patches and transforms them into embeddings. For an image embedding x, spatial self-attention f(x,x,x) is used to derive the importance of each patch. Assume \\(w_{ij}\\) is each element of the matrix \\(a(Q, K) = \\frac{QK^T}{\\sqrt{D_k}}\\), the extracted feature z can be calculated as:\n\\[z = \\sum_{i=1}^k x_i \\sum_{j=1}^k w_{ij} , \\sum_{i,j} w_{ij} = 1\\]\nwhich is a linear transformation from the original embeddings. This feature extraction method can derive deep relationship while preserve the original information as well.\nIn this approach, we treat the video as a sequence of images and focus on extracting the inner deep features of these images. Our goal is to preserve both the spatial and temporal features for transformation in the subsequent modules, as music also encompasses spatial and temporal dimensions. If other video models like (Arnab et al. 2021; Xu et al. 2021) are adopted in visual feature extraction, the temporal information may be lost, which would hinder the music generation process in terms of temporal alignment. This loss in temporal alignment would consequently reduce the correspondence between the music and the video. The results of our ablation study further support this assertion.\nFeature Transformation module. This is one of the most critical parts in video-to-music generation, as it is essential to establish the cross-modal relationship in the shared (visual-musical) space. We do not rely on either mathematical relationship definition or language descriptions for transformation like previous works, since the former is limited by incomplete and inaccurate variables, while the latter tends to lose temporal information and introduces redundant information from an extra modality.\nIn GVMGen, we propose spatial cross-attention to build the gap between visual and musical features inspired by (Li et al. 2022). Firstly, we define trainable music queries q. The queries interact with each other through self-attention, and"}, {"title": "Evaluation Model", "content": "Previous objective metrics such as Fr\u00e9chet Audio Distance (FAD) and Kullback-Leibler Divergence (KLD), primarily focus on evaluating the similarity between the generated music and the ground-truth. However, these metrics do not account for the correspondence between video and music.\nInspired by (Sur\u00eds et al. 2022), we propose an evaluation model for music in audio format with both global and local (temporal) estimation. For music-video pairs of batch size B, we transform the embeddings of hidden dimension \\(H_v\\) and \\(H_m\\) into a unified hidden size H. The cross-attention matrix \\(a(v, m) \\in \\mathbb{R}^{t\\times t}\\) focuses on visual-musical relationship at each moment, effectively providing a temporal alignment for local evaluation. The hidden video and music features are derived from the cross-attention matrix with different values like \\(z_v = f(v,m,v)\\) and \\(z_m = f(v,m,m)\\). After linear layer to summarize the features for each video and music, the cross-modal relevance can be considered as a global evaluation metric inspired by (Radford et al. 2021). For training, the temporal alignment employs MSELoss \\(L_1\\) to maximize diagonal attention since the local visual-musical correspondence should be strongest, while InfoNCE Loss \\(L_2\\) is employed for cross-modal relevance like:\n\\[L_1 = \\frac{1}{t}\\sum_{i=1}^t (I - diag(a(v, m)))^2\\]\n\\[L = \\frac{1}{2} (L_{m\\rightarrow v} + L_{v\\rightarrow m})\\]\n\\[L_{m \\rightarrow v} = \\sum_{i=1}^N log \\frac{exp[s(f_m, f_v)/\\tau]}{\\sum_{j=1}^N exp[s(f_m, f_i)/\\tau]}\\]\nwhere I stands for identity matrix, \\(s(f_m, f_v) = \\frac{f_m^T f_v}{||f_m|| ||f_v||}\\). The temperature parameter \\(\\tau\\) is set to 0.07. During evaluation, the temporal alignment metric and cross-modal relevance metric are derived from the average of \\(diag(\\cdot)\\).\nFor efficiency, the evaluation model reach an average loss and accuracy of 0.003 and 99.4% on the test set. We also find 20 expert users to score this metric on 30 video-music pairs, and the error rate between the average score and the model score is only 3.75%."}, {"title": "Training Process", "content": "In the visual feature extraction module, each video \\(V\\in \\mathbb{R}^{t\\times f\\times H\\times W\\times C}\\) is considered as a sequence of images \\(p_i \\in \\mathbb{R}^{H\\times W\\times C}\\). Each image will be divided into several patches represented by \\(x_i \\in \\mathbb{R}^{h\\times w\\times D}\\), where h = H/s,w = W/s and s stands for patch size. After spatial self-attention, the hidden features can be represented by \\(F_P \\in \\mathbb{R}^{P\\times D}\\), where p = hxw+1 in which the addition value stands for special class token (cls).\nIn the feature transformation module, we create trainable music-related queries \\(q \\in \\mathbb{R}^{n\\times D}\\). Both self-attention and cross-attention are employed on trainable queries. Here n and D stands for number of queries and hidden dimension, while K and V are z in cross-attention.\nAfter layers of cross-attention, we get music-relevant attention \\(A_i \\in \\mathbb{R}^{n\\times D}\\) for each image. We add an average pooling for attentions with different queries:\n\\[z_i = \\frac{1}{n} \\sum_{j=1}^{n} A_{ij}, A_i \\in \\mathbb{R}^{1\\times D}\\]\nThen the cross-modal features can be represented by \\(z \\in \\mathbb{R}^{1\\times D}\\). For video, \\(z_v \\in \\mathbb{R}^{t\\times f\\times D}\\) is stacked by a sequence of \\(z_i\\) for \\(i \\in [0, t \\times f]\\).\nIn the conditional music generation module, the cross-modal features \\(z_v\\) are then sent into decoder. After linear projection, music embeddings are set as queries, while cross-modal feature \\(z_v\\) are set as keys and values. In training, music embeddings \\(m \\in \\mathbb{R}^{t\\times f'\\times K\\times H}\\) are from ground truth music tokens \\(M \\in \\mathbb{R}^{t\\times f'\\times K}\\) (shift right). f' stands for music token sample rate and K stands for the number of codebooks. When inference, music embeddings are initialized to a start token and will be iterated to predicted music embeddings auto-regressively. The predicted music embeddings can be written as \\(m'\\) and will be quantized into music tokens \\(M'\\) according to codebooks. The music tokens will be decoded as music in audio format finally.\nFor training loss, we adopt average cross entropy loss of each codebook \\(B_j\\) to compare the predicted music features \\(F'_m\\) and the ground truth music tokens M:\n\\[L = \\frac{1}{KN} \\sum_{j=1}^{K} \\sum_{i=1}^{N} B_{jM_i} log(\\xi(z'_m))\\]"}, {"title": "Dataset", "content": "To enhance the generative diversity and universality, we collect a large-scale video-music dataset encompassing various types of videos and music. Existing datasets primarily consist of music videos (MVs) (Zhuo et al. 2023; Kang, Poria, and Herremans 2024) with MIDI music. However, for MVs, videos are typically produced after the music has been composed, which is logically contrary to the task of generating background music for a given video. Such datasets exhibit low diversity and weak music-video correspondence, thereby limiting the efficacy of models trained on them.\nTo address these limitations, our dataset comprises movies, vlogs, comics, and documentaries where the background music is specifically tailored for the video content. For music, our datasets include a substantial amount of Chinese traditional music as well as ensembles featuring both Chinese and Western instruments. Chinese traditional music emphasizes intricate melodies and rhythms (Liu 1985), along with variations in the playing techniques that cannot be adequately represented in MIDI format. As shown in Table 1, our dataset spans a wide range of topics and includes various types of background music. This diversity is crucial for developing robust and versatile models capable of generating appropriate music for different video genres and styles.\nFor collection, we sourced our dataset from free public platforms (Bilibili and Youtube). We selected clips featuring solely music and excluded those video frames that contained extensive superimposed text or captions. After manual filtering, the dataset is preprocessed by clipping for training. The total durations are 89.5 hours for MVs, 42.1 hours for documentaries, 9.9 hours for vlogs, and 5.5 hours for other types."}, {"title": "Experiment", "content": "Implementation Details\nWe adopt VIT-L/14@336px with 24 self-attention layers. In feature transformation module, we employ 16 queries, 6 self-attention layers and 3 cross-attention layers. And the temporal cross-attention is with 48 transformer layers of 1536 as the hidden size while the MusicGen decoder is with 4 codebooks of 2048 tokens. We use Adam optimizer with learning rate of 1e-5, weight decay of 0.01, batch size of 6 and video frame rate of 1 per second. A cosine learning rate schedule with 4000 warmup steps and top-k sampling with keeping the top 250 tokens are employed. We use 30-second clips with the music sampling rate of 32kHz. The ratio of training and test set is 0.85:0.15. The training lasts for 150 epochs with 188 hours on NVIDIA A100 (single card).\nMetrics\nWe evaluate the models with both objective and subjective metrics. For objective metrics, we adopt FAD, KLD (Gemmeke et al. 2017) which are commonly utilized to evaluate"}, {"title": "Experimental Results", "content": "Performance Comparison. Table 4 presents the performance of objective metrics on the evaluation set. It is evident that GVMGen produces music most closely resembling the ground truth while maintaining a high relevance to the corresponding video. Given that NEXT-GPT and CoDi underperform in at least one metric and their generated samples are more like audio rather than music, they are excluded from further subjective evaluation.\nTable 2 illustrates the performance of subjective metrics as evaluated by both experts and non-experts. The results indicate that: (1) Our model outperforms others across all metrics, with significant improvements particularly in music-video correspondence and music richness. This suggests that our model is capable of generating a diverse styles of music that are high-related to the video input; (2) Even when GVMGen is trained solely on our Chinese Traditional Music dataset, its performance in music quality and music-video correspondence remains comparable to, or even surpasses, that of other models. This demonstrates the strong universality and transferability of our model.\nVisualization. As shown in Fig. 3 (a), with the same video input, only our model can generate music with two distinct"}, {"title": "Universality Study", "content": "Table 3 shows the detailed performance on other datasets. GVMGen consistently outperforms other models, even on their datasets, whether in terms of similarity to the ground truth, generative quality, or music-video correspondence. These datasets are entirely different from our training set, indicating that GVMGen can be effectively applied to various types of video inputs, even in zero-shot scenarios."}, {"title": "Ablation Study", "content": "To further study the effectiveness of each component in our model, we adopt additional ablation study with spatial cross-attention, temporal cross-attention. The objective ablations are performed using 1628 samples of 30 seconds in our test set while subjective evaluation are adopted with the main experiment using 32 samples of 15 seconds.\nSpatial Self Attention. The model cannot work if we drop the module. And as shown in Table 5, if we change the visual feature extraction model from ViT into ViViT, whether the generative similarity or the music-video correspondence will drop significantly, confirming that it cause a information loss in temporal features as mentioned before.\nSpatial Cross Attention. As shown in Table 2, without the spatial cross attention, the performance of Our (S,CTM) falls sharply compared with Our (S,FTM,CTM). It indicates that if we remove the spatial cross-attention, the visual features cannot be transformed into shared space, which causes a lower generative quality and music-video correspondence. Moreover, we use different number of queries to test the most appropriate one with the average value, sum value or preserving the original cross-modal features. Results in Table 5 shows that the 16-query model with average values generally performs best because it can focus on more effective features while preserve less redundant features.\nTemporal Cross Attention. Table 5 shows that the generated music cannot be similar to ground truth or be related to video without temporal cross-attention. And in Table 2, the adoption of more layers yields significant improvements across almost all metrics. This enhancement can be attributed to the increased capacity and complexity, allowing it to capture temporal alignment more accurately and keep the music of high relevance to the video."}, {"title": "Conclusion", "content": "In this paper, we present GVMGen, capable of producing diverse music in audio format that is highly related to various types of video inputs. We leverage hierarchical attentions, including spatial self-attention, spatial cross-attention and temporal cross-attention, to extract and align the hidden features. The hierarchical attentions can preserve the most important features with minimal information loss. Moreover, we propose an evaluation model with two novel objective metrics to evaluate global and local music-video correspondence. We also collect a large-scale dataset including MVs, movies and vlogs, featuring both Chinese and western background music. Experimental results demonstrate that our model excels in the correspondence, diversity, and universality of video background music generation. We will improve the robustness and personalized generation in future."}]}