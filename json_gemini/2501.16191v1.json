{"title": "Raiders of the Lost Dependency: Fixing Dependency Conflicts in Python using LLMs", "authors": ["ANTONY BARTLETT", "CYNTHIA C. S. LIEM", "ANNIBALE PANICHELLA"], "abstract": "Fixing Python dependency issues is a tedious and error-prone task for developers, who must manually identify and resolve environment dependencies and version constraints of third-party modules and Python interpreters. Researchers have attempted to automate this process by relying on large knowledge graphs and database lookup tables. However, these traditional approaches face limitations due to the variety of dependency error types, large sets of possible module versions, and conflicts among transitive dependencies. This study explores the potential of using large language models (LLMs) to automatically fix dependency issues in Python programs. We introduce PLLM (pronounced \u201cplum\u201d), a novel technique that employs retrieval-augmented generation (RAG) to help an LLM infer Python versions and required modules for a given Python file. PLLM builds a testing environment that iteratively (1) prompts the LLM for module combinations, (2) tests the suggested changes, and (3) provides feedback (error messages) to the LLM to refine the fix. This feedback cycle leverages Natural Language Processing (NLP) to intelligently parse and interpret build error messages. We benchmark PLLM on the Gistable HG2.9K dataset, a collection of challenging single-file Python gists. We compare PLLM against two state-of-the-art automatic dependency inference approaches, namely PyEGo and ReadPyE w.r.t. the ability to resolve dependency issues. Our results indicate that PLLM can fix more dependency issues than the two baselines, with +218 (+15.97%) more fixes over ReadPyE and +281 (+21.58%) over PyEGo. Our deeper analyses suggest that PLLM is particularly beneficial for projects with many dependencies and for specific third-party numerical and machine-learning modules. Our findings demonstrate the potential of LLM-based approaches to iteratively resolve Python dependency issues.", "sections": [{"title": "1 Introduction", "content": "Python was first introduced in 1991 and has since grown into one of the most widely used program-ming languages in the world [4]. Part of its popularity is due to the ease with which developers can create and reuse modules or dependencies, i.e., Python code that can be imported and reused in various projects. As Python's ecosystem expanded, so did the complexity of managing these depen-dencies, with an increasing number of available modules and versions, leading to the development of tools to streamline dependency management.\nA first step toward supporting developers in this process was made around 2000 with the intro-duction of distutils[18], setuptools[1] in 2004, and PIP[2] in 2008, which is still the major package manager in use today. For example, \u2018PIP' uses the Python Package Index (PyPI) 1 to locate and download specified dependencies, including specific versions and any transitive dependencies (i.e., a dependency imported by a direct dependency or another transitive dependency). However, even small version discrepancies (1.1 vs. 1.2, for example) can cause serious compatibility problems [7]. Thus, accurate dependence versioning is essential for project longevity.\nDespite the widespread use of these tools, many Python projects remain difficult to run out-of-the-box without manual dependency configuration [10, 24]. Developers frequently use by default the latest version for convenience, leading to dependency conflicts\u2014an issue compounded in fields like machine learning, where projects often involve complex dependencies across software and hardware [12]. The need for tools that allow developers to reliably run public Python code has only grown, especially as code from AI assistants often introduces security or reliability risks [16].\nResearchers have attempted to automate dependency resolution through large database lookup tables and knowledge graphs to model relationships among versions and dependencies. Horton and Parnin [10] introduced a uniform dataset for validating Python dependency fixes. Building upon this, they later proposed the Dockerizeme [11] tool, which implemented a knowledge graph to model version relationships between dependencies and ensure correct installation of versions and sub-dependencies. This approach was further enhanced by wrapping the code in a Dockerfile to ease the validation and execution of the Python code under test. Subsequent works have followed the trend of using knowledge graph databases to map dependency relationships and various mechanisms for populating and updating the database. Notable works include PyEGo by Ye et al. [25] and ReadPyE by Cheng et al. [5], which have shown to outperform previously proposed solutions.\nAlthough effective, knowledge-graph-based techniques face important limitations. Managing complex transitive dependencies still remains an open challenge, particularly in large projects with numerous third-party modules that create intricate dependency conflict chains. Furthermore, highly specialized and hardware-specific dependencies, common in fields like machine learning \u2014where libraries such as PyTorch require specific CUDA or GPU driver versions\u2014 are especially difficult to resolve using knowledge graphs due to the static nature of the latter. Finally, knowledge graphs frequently use Regular Expression (regex)-based error log parsing to find conflicts. However, this method can be brittle and require frequent updates to remain effective as error formats evolve.\nIn this work, we investigate the potential of using Large Language Models (LLMs) to resolve Python dependency conflicts. LLMs have shown promising results in both text-related [20] and code-related tasks (e.g., code completion [9, 14] and code summarization [3]). Intuitively, Python dependency errors involve both natural language elements (e.g., error messages and types) and code-specific information (e.g., module names, versions, and required transitive dependencies). Thus, we conjecture that the reasoning capabilities of LLMs can be highly beneficial in addressing complex dependency conflicts. To test this, we introduce PLLM (pronounced \"plum\"), a novel method that uses LLMs to dynamically resolve dependency issues.\nPLLM employs a Retrieval Augmented Generation (RAG) pipeline to infer Python versions and modules whilst iteratively searching for working module versions. Our RAG pipeline utilizes pre-trained Large Language Models (LLMs) to infer dependencies from a given Python file. We then pull specific version information from PyPI and feed the module information to the LLM for processing at run time. PLLM then retrieves version information from PyPI and provides this data to the LLM at run time, avoiding the need for pre-built relational databases and instead generating relationships by strategically iterating over possible versions. Using a feedback loop, PLLM allows the LLM to refine its output based on build errors, iteratively improving results and reducing the risk of hallucination [17] by grounding recommendations in actual error feedback.\nTo assess the effectiveness of PLLM, we benchmark it on the HG2.9K dataset by Horton and Parnin [10], a curated collection of challenging single-file Python gists. We ran PLLM ten times on each Python gist to ensure consistent results across executions, accounting for the stochastic nature of LLM responses [17]. We compare PLLM against two state-of-the-art knowledge graph-based approaches, PyEGo and ReadPyE. We chose these for our baselines because they have both compared themselves with other existing methods in their work, to which they define themselves as state-of-the-art approaches.\nOur study is guided by three research questions:\nRQ1: To what extent can current LLMs infer working dependencies from a given Python file?\nRQ2: How effective is PLLM compared to the state-of-the-art knowledge-based approaches?\nRQ3: In what context does PLLM perform better compared to the state-of-the-art knowledge based approaches?\nOur results show that PLLM resolves significantly more dependency issues than the two baselines, with 218 additional fixes over ReadPyE (+15.97%) and 281 over PyEGo (+21.58%). According to our more thorough investigation, PLLM works especially well for projects with intricate dependency structures, including those with a large number of module dependencies or specialized libraries in machine learning (e.g., tensorflow), math (e.g., scipy), and hardware-specific libraries (e.g., tensorflow-gpu). We also examined the overlap and orthogonality w.r.t. the conflicts resolved by the three approaches. While we observe a large intersection among the issues addressed by all three approaches, each approach also resolves unique sets of dependency conflicts. Notably, PLLM shows the largest set of uniquely resolved conflicts; however, the baselines also demonstrate strengths in certain cases, suggesting that combining these methods could further enhance Python dependency resolution\u2014a promising direction for future work."}, {"title": "2 Background and Related Work", "content": "Python dependency fixing has seen significant research in recent years, driven by the complexity of Python projects and the importance of ensuring that open-source code remains runnable. This section discusses related work by organizing it around two key aspects: existing approaches and datasets. We also define our baseline methods and elaborate on the dataset we will use in this study.\nExisting approaches to Python dependency resolution have generally relied on either graph-based dependency mapping or regex-based parsing to address dependency conflicts."}, {"title": "2.1 Dependency Conflicts Resolutions for Python", "content": "Knowledge graphs have been widely used to encode relationships among Python modules and dependencies, making them a popular choice for dependency resolution. Horton and Parnin[11] developed Dockerizeme, a tool that uses a knowledge graph to map package dependencies and versions. Dockerizeme's graph, built using data from Libraries.io\u00b2 and Neo4J, includes packages, versions, and required resources, allowing it to infer dependencies based on known relationships. By analyzing both static and dynamic aspects of Python projects, Dockerizeme can resolve transitive dependencies in some cases, providing a structured approach to dependency conflict resolution."}, {"title": "2.2 Datasets of Python Dependency Conflicts", "content": "Effectively evaluating dependency-fixing methods requires reliable datasets with ground truth for testing and validating inference methods. Several important datasets have been developed for this purpose. The Gistable dataset by Horton and Parnin (2018)[10] is one of the most widely used datasets for evaluating dependency-fixing tools. This dataset, collected from GitHub's Gist platform, contains over 10,000 Python gists\u2014snippets of code that vary in complexity. Horton and Parnin analyzed these gists and found that only 24.4% of them could run without modification, while 52% failed due to missing imports, highlighting the impact of dependency conflicts on code usability. The Gistable dataset\u00b3 includes a challenging subset of 2,891 \u201chard\u201d gists that fail due to missing imports and remain unrunnable without fixes. This subset has become the gold standard for evaluating dependency-fixing approaches.\nIn addition to Gistable, some works have used other datasets for validation. PyDFix[15], for example, evaluates using BugSwarm[21] and BugsInPy [22], both of which incorporate a variety of dependency conflicts from actual Python projects. These datasets offer a broader evaluation context, allowing researchers to compare results across different dependency issues.\nIn this paper, we will use Gistable as our main validation dataset since it focuses on the difficult subset of \"hard\" gists."}, {"title": "3 Our Approach", "content": "Our approach uses a Retrieval Augmented Generation (RAG) pipeline incorporating an open-source Large Language Model (LLM) to iteratively build and search for a working set of Python dependencies. Figure 1 provides a high-level overview of our approach.\nGiven a Python file as input, PLLM first extract the import statements from the file (Fig 1, A). We use a simple regular expression (regex) to brute-force search for import statements within the file. Then, our approach prompts the LLM to infer the required Python modules and versions based on the extracted imports (Fig 1, B) for the input file. Notice that the LLM is also asked to provide a version of the Python language compatible with these modules. The LLM's inferences are combined with information from PyPI to generate a set of candidate dependency fixes (Fig 1, C) and eventually identify additional dependencies that are missing and that should be added to the candidate fix. PLLM then validates these fixes by building and running a Docker container (Fig 1, D).\nAfter each build, PLLM performs a simple check of the Docker build log for potential error messages. If a build error message is detected, this indicates that the candidate dependency fix was not successful and requires further refinements. Therefore, PLLM provides feedback to the LLM based on the build results (Fig 1, E). The feedback consists of interacting with the LLM using follow-up prompts (Fig 1, E) that contain the error message generated by the Docker build plus the unsuccessful fix used in the current loop. This process continues iteratively until a working set of dependencies is found (Fig 1, D, exit to the loop) or until a specified loop limit is met."}, {"title": "3.1 LLM Prompting", "content": "A novel aspect of our approach involves leveraging an LLM to infer specific information from a Python file or build log in the search for a working set of dependencies.\nHowever, we crafted targeted prompts to elicit accurate information at each step due to the inherent risk of LLMs misinterpreting complex queries or hallucinating responses [23]. Furthermore, we specified desired output formats for the LLM to ensure the response was concise and not overly verbose. By using specific instructions and structured outputs, we minimize ambiguity and guide the LLM toward concise, relevant responses.\nWe developed two types of prompts tailored to the various stages of the search process. The first prompt is used in the initial stage of PLLM (Step B in Figure 1) and aims to request a list of dependencies and the Python version. Listing 1 displays the initial prompt, which provides multiple input items and requires one single return object.\nThis initial prompt utilizes partial variables, which we supply to the LLM when prompting it for different Python files under analysis. Here, however, \u2018raw_file\u2019represents the Python file, and\u2018format_instructions' contains instructions about the desired output for the LLM response. Listing 2 shows an example of format instructions given to the LLM when requesting inference on the raw Python file. This specific JSON schema ensures that the information returned by the LLM adheres to this format.\nBeyond file-based inference, the LLM also analyzes Docker build logs to address common errors during the dependency validation stage. For this purpose, we designed and crafted follow-up prompts that specifically handle the various error messages. Our prompts address in to-tal eight distinct error messages: \u2018VersionNotFound\u2019, \u2018DependencyConflict\u2019,\u2018ImportError', 'ModuleNotFound\u2019,\u2018AttributeError\u2019,\u2018InvalidVersion', \u2018NonZeroCode' and 'SyntaxError'. Each error prompt is tailored to extract relevant information specific to that error. Some are specific to the Docker build, such as \u2018non-zero code', while others are encountered when attempting to run the Python file in the Docker container.\nFor example, given an ImportError, the LLM is directed to identify the missing module, as shown in Listing 3. This type of error message is the most common we have encountered in our experiments. It occurs when one or more dependencies of the Python file are unmet (not installed), leading to the ImportError being thrown along with information about the missing dependency."}, {"title": "3.1.1 Retrieval Augmented Generation", "content": "As outlined in the previous subsection, once a dependency is validated, it can be used during prompting. However, to prompt for a specific version of a given dependency, we first need a set of available versions from which to choose.\nRetrieval Augmented Generation (RAG) is a well-established approach to guide LLMs towards more accurate responses by providing context-specific information during prompting, mitigating the risk of hallucinations [23]. As Feldman et al. [8], demonstrate in their work, RAG can increase the accuracy of an LLM's response when implemented correctly. In our experiments, we observed that LLMs frequently hallucinate nonexistent or wildly inaccurate Python module versions during inference, leading to numerous failures.\nTo address this challenge, we use the Python Package Index (PyPI) 7. PyPI is the primary repository for Python software with over 550,000 installable modules, and through its Application Programming Interface (API) PLLM can retrieve real-time metadata for specified Python modules. This method allows the dynamic retrieval of module version information at run time without needing a pre-built dependency graph, as some existing methods require.\nHowever, PyPI metadata is extensive, and providing it to an LLM in its raw form could lead to inaccurate responses. For instance, if the target Python version is 3.7, metadata for versions 3.8, 3.9, etc., is irrelevant and may mislead the model. Thus, we designed a focused retrieval strategy that filters metadata to match specific requirements."}, {"title": "3.2 Build Validation Loop", "content": "Our approach's main component, the build validation loop, is made to iteratively search for a working build, up to a specified number of attempts. If a successful build is found before reaching the limit, the program exits with success.\nThe three phases of the validation loop are (1) creation, (2) building, and (3) running. Initially, we create a Dockerfile with the modules and inferred Python version. The Dockerfile is then built, and the final image is run. The steps involved in creating and running are similar: when an error occurs, the loop is restarted, and another attempt is made after passing the error message to the LLM for inference."}, {"title": "3.2.1 Dockerfile Creation", "content": "Once we have generated a set of modules and inferred Python versions, we can proceed to build a Dockerfile for validation. Our methodology is straightforward: we extend from the official Python Docker build corresponding to our inferred version (e.g., Listing 6 uses 'python:3.6'). We then set the working directory to \u2018/app' to provide a space to run the file under test. Next, we loop through all modules and versions to create specific install lines. These lines also set attributes such as \u2018-trusted-host' and \u2018-default-timeout' to prevent installs from failing for reasons other than those we are testing for. Finally, we copy the Python file to the '/app' directory and set the \u2018CMD' to execute the file at run time."}, {"title": "3.2.2 Dockerfile Build Validation", "content": "With the Dockerfile established, PLLM can proceed to validate its building capabilities. During the build process, our program assigns a unique image name based on the folder name and the specific Python version in use (for instance, \u2018test/pipped:678436743_3.6'). This methodology facilitates the concurrent execution of multiple builds.\nNext, Docker starts the build of the Dockerfile; it will continue the process and break only when Docker commands cause an error or the build is completed successfully. In the case of an error, PLLM conducts a prompt evaluation of common issues through a straightforward if/else structure to enhance the response provided to the LLM. For example, should a \u2018ModuleNotFoundError\u02bb arise, PLLM would execute a prompt akin to Listing 3, wherein we first extract the module name from the error message, followed by a subsequent request as shown in Listing 4 to find the relevant version. Utilizing the information obtained from the LLM, we can then reconstruct the Dockerfile by incorporating the new module and version pair, subsequently attempting the build process once more. This iterative procedure continues until either a successful build is achieved or the maximum number of iterations is reached."}, {"title": "3.2.3 Docker Image Run Validation", "content": "Once the Docker build process finishes without any error, a docker image is created, which can then be run. Each image is coded to carry out the Python script duplicated during the image build time. When an image is run, it produces a container with a unique name, thus ensuring that multiple containers' executions do not overlap. Analogous to the case of building a container, it either ends in a fault or completes its work. In case of a mistake, a parsing mechanism similar to that used in the build process is employed, and the error message is sent to the LLM for diagnosis. The model's input is retrieved, and we return to the build stage to recreate the image. This looping continues until the allowed loop count is achieved; if completed successfully, the loop stops and the result is stored."}, {"title": "3.3 Parallel Execution", "content": "A distinctive feature of our approach is its capacity to effortlessly build and run against any major Python version (2.x or 3.x) and its subsequent variants. We provide our approach with a Python version range at run time. This value determines the number of parallel executions to perform; for example, a value of 2 dictates that we wish to execute against two adjacent Python versions. This will ensure that two versions on either side of the version inferred by the LLM will be executed. For instance, if the LLM predicts that the code should run on Python 3.5, we would also execute against Python 2.7, 3.4, 3.6, and 3.7. These variants are executed in parallel, performing all the same build-run steps outlined in Section 3.2. Doing so allows us to identify multiple viable Python versions for the code snippet under validation. Note that we ensure Python 2.7 is always executed by appending this to the first element of the Python versions list. Unlike our related work 2, we do not switch versions based on errors, but instead attempt to find fixes for multiple Python versions."}, {"title": "4 Study Design", "content": "The goal of this study is to evaluate the effectiveness of PLLM in resolving Python dependency conflicts for real-world programs. To this aim, we formulated the following research questions:\nRQ1: To what extent can current LLMs infer working dependencies from a given Python file?\nRQ2: How effective is PLLM compared to the state-of-the-art knowledge-based approaches?\nRQ3: In what context does PLLM perform better compared to the state-of-the-art knowledge based approaches?\nRQ1 aims to provide insights into the current capabilities of PLLM in inferring working depen-dencies for real-world Python programs. RQ2 and RQ3 are designed to evaluate the effectiveness of PLLM compared to state-of-the-art knowledge-based approaches. By comparing PLLM to these approaches, we can determine the contexts in which PLLM performs better and identify areas for improvement."}, {"title": "4.1 LLM Model Selection", "content": "We utilized a number of specific LLM tools to implement our strategy. To ensure openness and reproducibility in our approach, we opted for Ollama\u00b3, an open-source tool for deploying models on local systems, rather than proprietary tools such as ChatGPT. The model chosen for our final experiment was Gemma2 [19], dated (24-06-2024). This particular quantized model boasts 9B parameters and has been reduced to a 4-bit value (Q4_0), allowing it to run on memory-constrained systems while maintaining high usability."}, {"title": "4.2 Dataset", "content": "We validated our approach using the HG2 . 9K dataset, which has become the standard benchmark for evaluating Python import fixing tools [5, 25]. This dataset, originally created as part of the Dockerizeme paper by Horton and Parnin, 2019 [11], consists of 2,891 Python Gist snippets, where snippets are executable Python files that people have shared via the Gist platform.\nThe original Gist dataset [10], contains just over 10,000 Gists, and was created due to a growing observation that the majority of Gists shared to the platform were no longer runnable without some form of human intervention. This is due to the imports used in Python and the gradual variation of versions. When installing a Python module, it will always pull the latest version unless a specific version is specified. If the module version has drifted significantly, the underlying code will no longer be executable.\nThe (HG2.9K) dataset we utilized is a curated set of \u2018hard' Gists, that still returned an ImportError after applying the Gistable's Naive approach. This dataset provides a challenging and realistic representation of dependency drift issues in Python."}, {"title": "4.3 Implementation and Parameter settings", "content": "We have implemented our approach using Python 3.11, Langchain and Langchain-community 0.2.1, and Ollama 0.2.0. Langchain and Langchain-community 9 10 provide the building blocks for creating LLM-based tools. Combined with the Ollama module 11, which enables interaction with a running Ollama instance, these libraries form the core components of our implementation.\nAll approaches were executed inside a Docker container on an AMD EPYC 7713 64-core Processor running at 2.6 GHz, with 256 available CPUs. The LLM was run through a Dockerized Ollama 0.3.12"}, {"title": "4.3.1 Parameters setting", "content": "Our implementation takes various parameters to assist in optimizing our test runs. The pertinent parameters for our evaluation are, \u2018-model = gemma2', '-temp = 0.7', \u2018-loop = 10'and,\u2018-range = 1'. Here, the model allows us to specify which model to run against, which is gemma2. This can be any model we have downloaded locally with Ollama. The temp (set to 0.7) allows us to control how an LLM responds to our questions. A higher value, with a maximum of 1, will see a model being more \u2018creative' with its response. We did not want the LLM to take too many risks with the response, but we did want some mild creativity.\nHowever, it is still unclear what the 'default' temp of a model should be. In the GPT-4 technical report [20], we observe the temperature value is arbitrarily chosen to be 0.6. In the Ollama docu-mentation 12, we see they now define the 'default' as 0.8, but the older value of 0.7 is still visible. Our choice of 0.7 is primarily motivated by it being the 'default' when we initially developed PLLM.\nAs mentioned in Section 3.2, our approach iteratively loops in search for a working set of dependencies. During the creation of our approach, it was found setting the loop value to 10 resulted in the best cost to wait ratio. A value lower than this would not give the LLM long enough to search, whereas a higher value would lead to longer execution times for failing scenarios.\nLastly, we define the number of parallel executions as discussed in the parallel execution sec-tion 3.3. The range parameter allows us to validate multiple Python versions at the same time and for this specific evaluation we chose 1. By choosing 1, we ensure that 3 Python versions will be tested for each Gist in the HG2.9K dataset."}, {"title": "4.3.2 Baseline settings", "content": "We ran both baselines with the recommended settings in their projects' README. Both baselines required building a Dockerfile to host and run their experiment, with each requiring the creation of a Neo4J database. PyEGo included a database backup in their repository and ReadPyE gave this as a separate artifact. ReadPyE as noted in their paper [5], have two database versions. For this experiment, we chose Knowledge Graph 0 (KG0), designed to fix only Python module implementations. This was chosen to keep our approaches close in their implementation, given that our approach focuses solely on module inference. It is worth noting that PyEGo fixes Python modules and operating system-level dependencies as part of their approach."}, {"title": "4.4 Evaluation Criteria", "content": "To answer RQ1 and RQ2, we execute PLLM along with both our baselines, PyEGo and ReadPyE against the HG2.9K dataset. Following the evaluation criteria set out by Horton and Parnin in their Dockerizeme work [11], we measure the success of each approach based on the number of successfully generated runnable environments (i.e., successful dependency fixes). As discussed in Section 3.1, a success is any runnable Python snippet that does not encounter specific error messages (ImportError, ModuleNotFoundError, AttributeError and SyntaxError).\nFor each approach, we also recorded the time taken to infer a working environment, as we were particularly interested in understanding the efficiency of each method in resolving dependency con-flicts. Accurate timing information provides insight into the practical usability of these approaches, especially for scenarios where dependency resolution speed may be crucial. Some adjustments were required to obtain timings for our baselines. For ReadPyE, more extensive modifications were necessary to achieve precise timing measurements. First, we recorded the inference time, which includes the stage where a Dockerfile was created with the inferred Python modules. Next, we validated the Dockerfiles, as no logging was initially provided to confirm whether a Dockerfile was executable. Each Dockerfile was built and run at this stage, analyzing the output and logging the time required for completion. We then combined these timings to determine which Dockerfiles were successfully executed. For PyEGo, we could infer considerable information from their output logs, which contained various data points, such as the start and stop times for each snippet inference and whether the inference was successful.\nTo account for the stochastic nature of LLMs, we executed PLLM 10 times, aggregating the data to understand the full extent of fixes. However, as both of our baselines utilize a static knowledge graph, these approaches only required a single run.\nTo answer RQ3, we conducted a permutation test [6], a non-parametric alternative to the ANOVA test, that does not require the data to be normally distributed. This test allowed us to determine whether there is a significant difference in the number of successful fixes produced by PLLM, PyEGo, and ReadPyE. We also assessed the impact of various project co-factors on these outcomes. The permutation test provided insights into the statistical significance of differences in successful fixes among the approaches without relying on the normality assumptions of ANOVA. To further investigate specific patterns, we analyzed the frequency of successful fixes for particular types of modules (e.g., PyTorch) and for projects with varying numbers of dependencies. We examined the success frequencies across Python files with similar dependencies and different dependency counts. Additionally, we analyzed the intersection and unique sets of projects successfully fixed by PLLM, PyEGo, and ReadPyE, identifying both shared and unique successes for each approach."}, {"title": "5 Empirical Evaluation", "content": "In this section, we discuss the results of the experiment with regard to our research questions."}, {"title": "5.1 Results for RQ1", "content": "RQ1 seeks to understand how much an LLM can infer working dependencies from a given Python file. To start, we observe Figure 2, which provides insight into how PLLM performs across multiple runs. As stated in Section 4.3.1, PLLM was executed a total of 10 times to account for the inherent randomness in LLM outputs. The plot in Figure 2 is cumulative, where a fix is considered successful if it was achieved in at least one of the first N={1, . . ., 10} runs. This approach allows us to assess the LLM's overall effectiveness at inferring working dependencies, as each additional run uncovers more potential fixes. Importantly, the LLM does not learn from prior runs or sessions; each inference is based solely on the information provided at run time in its dedicated session.\nThe effectiveness of PLLM is further visible when we examine the single test run shown to the left of Figure 2. Here, we see that the number of fixes achieved is just under 1440. Combined with Table 1, this shows that a single run of PLLM generates more fixes than both baseline methods (see the next subsection)."}, {"title": "5.2 Results for RQ2", "content": "Table 1 reports the number of successful and unsuccessful fixes produced by PLLM, PyEGo, and ReadPyE. The table also includes the average time (in seconds) required to generate successful fixes, along with the interquartile range (IQR). As observed, PLLM outperforms both baselines in terms of the number of successful fixes, achieving a total of 1583 successful fixes. This is significantly higher than the 1365 and 1302 successful fixes produced by ReadPyE and PyEGo, respectively.\nThe average time taken to generate successful fixes is higher for PLLM compared to the baselines, with an average time of 151.46 seconds, compared to 62.77 seconds for ReadPyE and 4.025 seconds for PyEGo. The higher IQR for PLLM indicates a broader range of fix times, largely due to the iterative nature of our approach, which continues searching until a successful set of dependencies is found. We argue that this additional time is justified by the higher success rate achieved by PLLM, and an average time of two minutes remains reasonable for achieving a successful fix with Dockerfile generation and execution.\nTo further analyze the effectiveness of our approach, we examined cases where PLLM succeeded while both baselines failed. Figure 4 displays the overlap of successful fixes across all three approaches. When combined with Figure 3, these results illustrate the unique effectiveness of PLLM."}, {"title": "5.3 Results for RQ3", "content": "To understand where our approach PLLM outperforms the baselines (PyEGo, ReadPyE), we conducted a series of permutation tests. The results of the one-way permutation test indicate that the chosen approach (PLLM, PyEGo, or ReadPyE) has a significant impact on the ability to fix the Python dependencies (p-value< 0.01), confirming the statistically significant superiority of our approach. The two-way permutation test further indicates that there is a significant interaction between the approach (PLLM, PyEGo, or ReadPyE) and the number of Python modules in the dependency file in determining the success of the generated fixes (p-value< 0.01). Here, the number of modules is a proxy to measure the complexity of the fix as, we argue, projects with a larger number of dependencies might be more tricky to fix than projects with only two dependencies. Table 4 displays a ranked list of dependency instances in successful evaluations, highlighting that our approach handles common dependencies more effectively than existing methods."}, {"title": "6 Threats to validity", "content": "Construct validity. A key aspect of our approach (PLLM) is allowing the LLM to dynamically infer Python versions from code snippets and validate these along with adjacent Python versions. This flexible strategy addresses limitations in prior work [5", "19": "which showed strong performance in preliminary tests compared to other open-source LLMs. However, using a different LLM could yield varying inference results. For example, an alternative model like ChatGPT might find fixes that Gemma2 does not due to differences in training data and model architecture.\nTo address the stochastic nature of LLM outputs, we followed existing guidelines [17"}]}