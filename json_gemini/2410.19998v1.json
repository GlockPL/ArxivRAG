{"title": "Artificial Intelligence of Things: A Survey", "authors": ["Shakhrul Iman Siam", "Hyunho Ahn", "Li Liu", "Samiul Alam", "Hui Shen", "Zhichao Cao", "Ness Shroff", "Bhaskar Krishnamachari", "Mani Srivastava", "Mi Zhang"], "abstract": "The integration of the Internet of Things (IoT) and modern Artificial Intelligence (AI) has given rise to a new paradigm known as the Artificial Intelligence of Things (AIoT). In this survey, we provide a systematic and comprehensive review of AIoT research. We examine AIoT literature related to sensing, computing, and networking & communication, which form the three key components of AIoT. In addition to advancements in these areas, we review domain-specific AIoT systems that are designed for various important application domains. We have also created an accompanying GitHub repository, where we compile the papers included in this survey: https://github.com/AIoT-MLSys-Lab/AIoT-Survey. This repository will be actively maintained and updated with new research as it becomes available. As both IoT and AI become increasingly critical to our society, we believe AIoT is emerging as an essential research field at the intersection of IoT and modern AI. We hope this survey will serve as a valuable resource for those engaged in AIoT research and act as a catalyst for future explorations to bridge gaps and drive advancements in this exciting field.", "sections": [{"title": "1 Introduction", "content": "The proliferation of the Internet of Things (IoT) such as smartphones, wearables, drones, and smart speakers, as well as the gigantic amount of data they capture, have revolutionized the way we work, live, and interact with the world. Equipped with sensing, computing, networking, and communication capabilities, these devices are able to collect, analyze and transmit a wide range of data including images, videos, audio, texts, wireless signals, physiological signals from individuals and the physical world. In recent years, advancements in Artificial Intelligence (AI), particularly in deep learning (DL)/deep neural network (DNN), foundation models, and Generative AI, have propelled the integration of AI with IoT, making the concept of Artificial Intelligence of Things (AIoT) a reality. The synergy between IoT and modern AI enhances decision making, improves human-machine interactions, and facilitates more efficient operations, making AloT one of the most exciting and promising areas that have the potential to fundamentally transform how people perceive and interact with the world.\nAs illustrated in Figure 1, at its core, AIoT is grounded on three key components: sensing, computing, and networking & communication. Specifically, AIoT utilizes a variety of onboard sensors such as cameras, microphones, motion, and physiological sensors to collect data from individuals and the physical world. The collected sensor data are processed by modern AI algorithms for a variety of tasks such as classification, localization, anomaly detection, and many others. Lastly, the networking & communication component of AIoT ensures the reliable transmission of the sensor data and/or the computed outcomes to the cloud, edges or other nearby AIoT devices. Compared to conventional IoT, the computing component of AIoT is concentrated on AI-oriented compute tasks. Moreover, the sensing and networking & communication components of AIoT are AI empowered. It is these two key distinctions that allow AIoT to empower billions of everyday devices with breakthroughs brought by modern AI.\nBesides advancements in the three key components, domain-specific AIoT systems have been proposed and developed across a wide range of application domains. For example, in the domain of healthcare, AIoT systems enable remote patient monitoring, facilitate disease diagnosis on site, and act in the form of assistive technology that helps people with disabilities. In the domain of Augmented, Virtual, and Mixed Reality, AIoT systems enable 3D tracking to provide immersive user experiences. In the domain of video streaming and analytics, AIoT systems have been developed to enhance video quality and optimize video processing efficiency. All these developed domain-specific systems demonstrate the potential of AIoT on revolutionizing a wide range of industries.\nThe overarching goal of this survey is to provide a systematic and comprehensive review of AIoT research. As shown in Figure 2, we organize the literature of AIoT in a taxonomy consisting of four main categories: sensing, computing, networking & communication, and domain-specific AIoT systems. Specifically,\n\u2022 Sensing: Sensing serves as the foundation of AIoT. In \u00a72, we survey AI-empowered sensing mechanisms and techniques in AIoT that cover research directions related to motion sensing, wireless sensing, vision sensing, acoustic sensing, multi-modal sensing, earable sensing, and Generative AI for sensing.\n\u2022 Computing: Computing is the brain of AIoT. In \u00a73, we survey fundamental compute tasks that lie at the core of AIoT, covering topics related to on-device inference, offloading, on-device training, federated learning, and AI agents for AIoT.\n\u2022 Networking & Communication: Networking and communication serve as the backbone of AIoT. In \u00a74, we survey Al-empowered networking and communication techniques related to a variety of networks including cellular/mobile networks, Wi-Fi networks, visible light communication, and LoRa/LoRaWAN.\n\u2022 Domain-specific AIoT Systems: The advancements in sensing, computing, networking & communication lay the foundation for the development of AIoT systems designed for specific application domains. In \u00a75,"}, {"title": "2 Sensing", "content": ""}, {"title": "2.1 Motion Sensing", "content": "Motion sensing involves the use of motion sensors such as Inertial Measurement Unit (IMU) sensors (i.e., accelerometers, gyroscopes, and magnetometers) attached to the individuals to capture various types of motions such as arm postures, body movements, and physical activities. As summarized in Figure 3, depending on the sensing tasks, existing works on AI-empowered motion sensing can be grouped into two categories: human activity recognition, and arm tracking.\nHuman Activity Recognition. One of the most important tasks of motion sensing is human activity recognition (HAR). Most existing HAR frameworks are limited to a few predefined activities and require prior knowledge or labeled data for supervised training. To address this limitation, Liu et al. [160] introduce Lasagana, an unsupervised learning-based HAR framework that extracts common bases of human motions in an unsupervised manner, creating a universal multi-resolution representation for common human activities. Their prototype system achieves 98.9% precision in activity classification and nearly 100% recall with about 90% precision in activity indexing. Another major limitation of existing motion sensing-based HAR frameworks is that machine learning"}, {"title": "2.2 Wireless Sensing", "content": "Wireless sensing uses wireless signals to sense individuals and objects in the environment in a contact-free manner. As summarized in Figure 5, based on the frequency bands wireless signals belong to, existing works on Al-empowered wireless sensing can be grouped into five categories: RFID sensing, Wi-Fi sensing, mmWave sensing, LTE sensing, and LoRa sensing."}, {"title": "2.2.1 RFID Sensing", "content": "Radio Frequency Identification (RFID) is a technology that employs an RFID tag and reader, enabling the retrieval of information from the tag using radio frequency (RF) signals emitted by the reader. By"}, {"title": "2.2.2 Wi-Fi Sensing", "content": "Wi-Fi sensing takes advantage of the ubiquitous Wi-Fi signals and their associated hardware to detect and interpret human movements or changes in the environment. Depending on the sensing tasks, existing works on AI-empowered Wi-Fi sensing can be grouped into the following categories.\nHuman Activity Recognition. One important task of Wi-Fi sensing is human activity recognition (HAR). The major challenge in device-free human activity recognition is that wireless signals are highly influenced by the specific environment and individual characteristics of the human subject, leading to poor generalization of models across different subjects and environments. To address this challenge, Jiang et al. [113] propose EI for HAR that learns domain-independent features from activity data collected in different domains. EI accepts multiple types of input signals, including Wi-Fi Channel State Information (CSI). The DL model of EI incorporates an adversarial network, including a CNN-based feature extractor, an FC-layer-based activity recognizer that predicts activity type from extracted features, and a domain discriminator that predicts the domain. Ding et al. [50] present RF-Net, a metric-based meta-learning approach for one-shot human activity recognition using Wi-Fi that can perform the recognition in a new environment with only one observation per label. RF-Net classifies a new observation in new environments by calculating a weighted sum of all the labels in the training dataset. The weights are given by the similarity between the query observation and all the data in the support dataset of the new environment. Lastly, Ji et al. [107] propose SiFall to formulate the fall detection problem as adaptive anomaly detection out of normal repeatable human activities instead of seeking features to characterize fall activity."}, {"title": "2.2.3 mm Wave Sensing", "content": "Millimeter Wave (mmWave) sensing refers to the use of electromagnetic waves with wavelengths in the millimeter range, typically between 30 GHz and 300 GHz frequency band, for a variety of sensing tasks. The high frequency, short wavelength, and broadband capacity make mmWave more sensitive to minor reflection distance variations, and thus can provide finer sensing resolution. At the same time, mmWave has limited penetration capabilities so it can easily be attenuated or blocked by obstacles. As such, mmWave sensing often requires a direct line-of-sight between the transceivers and the sensing target. Depending on the sensing tasks, existing works on AI-empowered mmWave sensing can be grouped into the following categories.\nHuman Activity Recognition. The capability of mmWave signals to capture micro-motions and micro- vibrations of different human body parts makes it feasible for the task of human activity recognition (HAR). Pegoraro et al. [213] introduce SPARCS for mmWave-based HAR. It focuses on extracting micro-Doppler signatures of human movement from irregular and sparse Channel Impulse Response (CIR) samples. This approach leverages the inherent sparsity of the mmWave channel to reduce sensing overhead drastically while integrating seamlessly with existing communication protocols. By formulating micro-Doppler extraction as a sparse recovery problem, SPARCS achieves high-quality human activity recognition with significantly lower overhead compared to existing methods, demonstrating its applicability and efficiency in real-world scenarios. While research on introducing DL to mmWave-based human activity recognition achieves promising performance, collecting and labeling mmWave datasets for such tasks is difficult and expensive. To close the gap, Zhang et al. [339] present SynMotion which synthesizes mmWave signals at high quality using widely available vision-based human motion datasets with the coordinates of body skeletal points and designs a few/zero-shot synthetic-to-real transfer learning framework for downstream human activity recognition.\n3D Human Mesh Construction. mmwave signals can also be used for 3D human mesh construction by providing detailed information about the human body contours and structure. Xue et al. [298] present mmMesh, a DL-based real-time 3D human mesh construction framework to model the moving subject with commercial portable mmWave devices. mmMesh utilizes range and angle information to remove noisy reflections from static objects in the IF signals collected by commercial devices and generate the 3D point clouds as input to the DL model. Kong et al. [123] propose m\u00b3Track to enable simultaneous tracking of the 3D postures of multiple users leveraging a single commercial mmWave device. m\u00b3Track obtains the Range-Doppler-Profile of the IF signals by range-FFT and doppler-FFT that contains information on the users and background objects. It distinguishes multiple users and backgrounds by sliding a convolutional kernel along the range bins of the Range-Doppler-Profile and performing convolution operations to detect the ranges that contain users. Xue et al. [297] develop Mesh for multi-subject 3D human mesh reconstruction. The tracking scheme of M\u2074esh integrates techniques adopted by mmMesh and m\u00b3Track, including subject detection, 3D point cloud generation for each subject, and per-subject mesh reconstruction. Similarly, Xie et al. [278] propose mm3dFace to move towards the reconstruction of human face. It proposes to leverage commercial mmWave radar to reconstruct 3D human faces that continuously express facial expressions in a passive manner. mm3dFace captures human face information from the recorded IF signal. By applying range-FFT to the IF signal and AoA calculation, it obtains the range profile, azimuth profile, and elevation profile, which together form a Range-Angle-Profile in the three-dimensional space. The three-dimensional profile captures the side view and frontal view of human faces."}, {"title": "2.2.4 LTE Sensing", "content": "Long-Term Evolution (LTE) sensing leverages the capabilities of LTE wireless broadband communication technology for the task of sensing. Feng et al. [62] explore the use of LTE signals for pervasive sensing applications both indoors and outdoors. It aims to address the limitations of existing wireless sensing technologies, such as Wi-Fi, which are constrained by coverage and performance issues. Specifically, the authors propose to leverage the widespread and diverse LTE infrastructure to achieve comprehensive and reliable sensing without affecting LTE data communication. Through advanced techniques to mitigate interference and noise, the authors demonstrate the effectiveness of LTE sensing in two key applications: indoor respiration monitoring and outdoor traffic monitoring. In [63], the authors leverage the infrastructure of LTE base stations to provide a cost-effective and energy-efficient solution for the application of soil moisture monitoring. By utilizing commercial off-the-shelf hardware, including software-defined radios and a Raspberry Pi, the proposed system achieves high"}, {"title": "2.2.5 LoRa Sensing", "content": "The long-range, low-power characteristics of LoRa networks make it popular among large- scale remote-area IoT applications. However, the use of LoRa for sensing tasks is yet to be explored due to challenges related to interference, sensing range, and many more. To address these challenges, Xie and Xiong [276] introduce Sen-fence, which explores advanced signal processing techniques that maximize movement- induced signal variations, thereby increasing the sensing range. Additionally, the authors introduce a novel \"virtual fence\" method, which confines sensing activities to a specific area of interest, thereby reducing the impact of environmental noise and interference. Sen-fence achieves a 50-meter range for fine-grained human respiration detection while effectively managing interference for practical LoRa sensing applications. Though the proposed method in Sen-fence is effective for detecting tiny movements like respiration but struggles with larger movements such as human walking. To address this issue, the authors in [275] introduce ChirpSen, a system designed to enhance the sensing range of LoRa-based localization by fully exploiting the properties of chirp signals. ChirpSen employs a chirp concentration scheme that concentrates the power of all signal samples in a LoRa chirp at one timestamp, thus increasing the signal power as well as the sensing range. Real-world experiments demonstrate that ChirpSen significantly enhances detection capabilities, extending the range for monitoring human respiration at a distance of 138 meters and tracking a walking human at up to 210 meters."}, {"title": "2.3 Vision Sensing", "content": "Vision sensing involves the use of vision sensors such as RGB cameras, depth cameras, and near-infrared (NIR) image sensors to capture and analyze visual information for various sensing tasks. As summarized in Figure 3, depending on the sensing tasks, existing works on AI-empowered vision sensing can be grouped into five categories: human activity recognition, image enhancement, object detection, eye tracking, and pose estimation.\nHuman Activity Recognition. DL-based models used in vision sensing for HAR can be computationally demanding, posing a significant challenge when it comes to execution on mobile and IoT devices. Moreover, vision systems that rely on RGB cameras are intrinsically susceptible to privacy leakage by hacking. To tackle this problem, Shim et al. [234] choose to use a Near-Infrared (NIR) image sensor to monitor human activities that inherently does not contain enough data to reveal personal identity. Although the NIR sensor loses a lot of"}, {"title": "2.4 Acoustic Sensing", "content": "Acoustic sensing involves utilizing acoustic sensors to capture, measure, and analyze acoustic signals for sensing purposes. As summarized in Figure 3, depending on the sensing tasks, existing works on AI-empowered acoustic sensing can be grouped into four categories: localization, movement tracking, emotion recognition, and keyword and event detection.\nLocalization. Localization using acoustic sensing refers to the process of determining the position or location of objects or sources of sound using sound waves. Mao et al. [185] introduce DeepRange, which investigates the limitations of traditional signal processing methods in localization tasks utilizing aquatic signals, particularly in scenarios with a low SNR environment. They pose the question of whether DNNs can automatically learn features from received acoustic signals to estimate distance, potentially surpassing the performance of conventional signal processing algorithms devised by domain experts. The study introduces a DNN-based ranging system, which directly employs raw acoustic signals without feature extraction and indicates superior performance compared to established signal processing approaches. Conventional methodologies for sound source localization require multiple microphone arrays, which is impractical for tiny devices. Addressing this, Owlet [67] place"}, {"title": "2.5 Multi-Modal Sensing", "content": "Multi-modal sensing involves the use of more than one sensing modality where the key advantage is its ability to combine distinct information provided by each of the included sensing modalities. At the same time, determining which sensing modalities to include, and how to combine them effectively, are highly dependent on the specific application. As summarized in Figure 3, depending on the sensing tasks, existing works on AI-empowered multi- modal sensing can be grouped into five categories: human activity recognition, human and object identification, tracking, localization, and speech enhancement.\nHuman Activity Recognition. Human activity recognition (HAR) using multi-modal sensing integrates data from different sensory modalities to detect and identify human activities. Traditional ML strategies typically"}, {"title": "2.6 Earable Sensing", "content": "Earables are wearable devices attached to ears in the form of headphones or wireless earbuds. As summarized in Figure 3, depending on the sensing tasks, existing works on AI-empowered earable sensing can be grouped into three categories: facial expression sensing, user authentication, and sound localization.\nFacial Expression Sensing. Conventional methods for capturing facial expressions are primarily counted on video cameras. However, video cameras are limited in low-light environments and pose substantial risks of privacy infringement. In contrast, earables avoid such limitations and have demonstrated significant promise for a variety of facial expression sensing tasks. For example, Wu et al. [271] propose BioFace-3D, which leverages EMG and EOG signals captured by earables to detect the facial muscle activities, track 2D landmarks, and perform continuous 3D facial reconstruction using a CNN. As another example, Song et al. [239] propose FaceListener, which uses the commodity headphone to recognize a user's facial expressions. FaceListener emits ultrasound signals to detect face movements and uses this information to create a facial landmark model and recognize facial expressions based on an LSTM model.\nUser Authentication. Earables have also been utilized to identify unique individual characteristics, such as a person's gait for the purpose of user authentication. For gait-based user authentication, traditional methods often require special equipment, which is cost-prohibitive and limited in range. In contrast, Ferlini et al. [64] propose EarGate, which employs an in-ear microphone to capture bone-conducted sounds induced by walking to detect the user's gait for user identification. Furthermore, they demonstrate that classification performance can be notably improved through transfer learning. Liu et al. [162] introduce MandiPass, a biometric-based authentication system that utilizes intracorporal biometric called MandiblePrint, derived from the vibrations of human mandibles. It uses an Inertial Measurement Unit (IMU) embedded in an earphone to capture the MandiblePrint when a user voices some specific sound. This sound generates vibrations in the throat that propagate through the mandible to the ear, where they are sensed by the IMU. MandiPass validates the feasibility of MandiblePrint through theoretical modeling and experimental vibration propagation, demonstrating its potential as a user authentication method.\nSound Localization. Sound localization in earable sensing refers to the ability of ear-worn devices to determine the direction of incoming sound sources. It is essential for enhancing spatial awareness and improving user expe- rience in hearing aids, augmented reality, and personal assistants. Chatterjee et al. [31] emphasize the importance of sound localization in enhancing user experience, particularly in distinguishing between the target speaker and background noise. The authors use binaural wireless earbuds and dual-channel neural networks to separate the target voice from the noises. These networks consist of a time domain network called CB-Conv-TasNet and a frequency-based network called CB-UNet to exploit both spatial and acoustic information. As a result, it achieves"}, {"title": "2.7 Generative Al for Sensing", "content": "Advancements in Generative AI have provided AIoT with opportunities to leverage state-of-the-art generative models such as Large Language Models (LLMs) to perceive, interpret, and present IoT sensor data in ways that are not attainable before [263]. Generative AI can correlate sensor readings with relevant contextual information, such as historical data, environmental conditions, and operational status so as to provide deeper insights into the sensor data and make decisions; it can improve user experiences by allowing non-technical users to interact with sensor systems and perform data querying using natural language; it can also help translate raw sensor data into human-understandable reports and summaries, making it easier for users to understand key information contained inside sensor data.\nSome efforts have been made to leverage such unique capabilities of Generative AI for sensing. For example, Ouyang and Srivastava [205] propose LLMSense, a prompting framework for LLMs to make sense of raw sensor data and low-level perception results. This framework can be implemented in an edge-cloud system, with small LLMs running on edge devices to summarize sensor data and high-level reasoning performed on the cloud to ensure data privacy. Two approaches are proposed to improve the performance of LLMSense: summarizing sensor data before reasoning and selectively including historical sensor data. Results show that LLMSense achieves high accuracy in tasks such as dementia diagnosis using behavior data and occupancy tracking with environmental sensor data. In [287], the authors propose Penetrative AI to explore how LLMs can be extended to interact with the physical world using IoT sensors and actuators. As a prompting framework, Penetrative AI shows how carefully constructed prompts can harness LLMs' embedded world knowledge for tasks such as user activity sensing and heartbeat detection. Specifically, Penetrative AI operates on two levels: textualized signal processing, where sensor data are converted into text for LLM analysis; and digitized signal processing, where LLMs directly interpret sensor data. Using heartbeat detection as an example, Penetrative AI demonstrates that LLMs can effectively analyze real-world sensor data with proper guidance, illustrating the potential of integrating LLMs into cyber-physical systems to enhance their intelligence and functionality. Lastly, Wan et al. [254] go one step further beyond prompting and propose a multimodal LLM named MEIT that translates raw ECG sensor data into human-understandable reports. For cardiologists, the task of interpreting ECG data and writing reports can be both intricate and time-consuming. MEIT aims to fill this gap by automating the ECG report generation task. Specifically, MEIT involves instruction tuning a multimodal LLM to integrate raw ECG data with corresponding textual instructions, ensuring that the generated reports are clinically relevant and accurate. Experimental results demonstrate the superior performance of MEIT in generating accurate and professional ECG reports, underscoring its potential for real-world clinical applications."}, {"title": "3 Computing", "content": ""}, {"title": "3.1 On-Device Inference", "content": "One of the most fundamental and essential compute tasks of AIoT is to perform inferences on the device. On- device inference is particularly critical for latency-sensitive applications or scenarios where cloud connectivity is not available. As summarized in Figure 11, existing works on on-device inference can be grouped into four categories: inference optimization, multi-tenant inference, cross-processor inference, and runtime adaptation."}, {"title": "3.1.1 Inference Optimization", "content": "IoT devices are constrained in their onboard computing power, memory resources, and battery life. The objective of inference optimization is to enhance the computational and energy efficiency as well as to reduce memory demands and efficiently utilize memory resources during the inference process. For example, Huynh et al. [101] propose DeepMon, an on-device inference framework that allows large DNNs to run on mobile devices at low latency for continuous vision applications. They propose a caching mechanism that exploits the similarities between consecutive images to cache intermediate processed data within CNN, which allows DeepMon to execute very deep models such as VGG-16 in near real-time. Ren et al. [219] propose SC-DCNN, an optimization framework of stochastic computing (SC) for CNNs. They propose to apply SC to CNNs by designing function blocks and implementing hardware-oriented max-pooling in the SC domain. In addition, they propose to perform holistic optimizations for feature extraction blocks and weight storage schemes. By calculating multiplications and additions with AND gates and multiplexers in SC, SC-DCNN achieves a significant reduction in energy consumption. Xu et al. [293] propose DeepCache, which adopts proven video compression techniques to systematically search for neighboring image blocks with similarities, rather than restricting matching solely to blocks in the same positions. They propose dividing video frames into regions, searching for similar regions in cached frames using a specialized matcher, and dynamically merging adjacent regions to maintain cache effectiveness. In [305], the authors propose FastDeepIoT, which incorporates a profiling module and a compression steering module to optimize execution time and reduce energy consumption. The profiling module generates diverse training structures and builds an interpretable model for predicting the execution time, while the compression steering module enables existing DL compression algorithms to collaboratively minimize both execution time and energy consumption. In SONIC [74], the authors explore the opportunity of DNN inference intermittently on energy-harvesting systems. They propose loop continuation that significantly reduces the cost of ensuring accurate intermittent execution for DNN inference by modifying loop control variables within a loop nest, as opposed to dividing an extended loop into multiple tasks. Cao et al. [26] propose DeQA, a set of optimization techniques designed to enable Question Answering (QA) systems to run on mobile devices. DeQA reduces memory demands by loading partial indexes, dividing data into smaller units, and replacing in-memory lookups with a key-value database, altogether reducing the memory requirements of QA systems to just a few hundred megabytes. Lin et al. [153] propose MCUNetV2, a scheduling technique in a patch-based manner to minimize memory usage for tiny DL. They propose initially executing the model on a limited spatial region, followed by the remainder of the network operating with a smaller peak memory consumption in the usual manner. Additionally, they propose to redistribute the receptive field to reduce the computation overhead caused by the patch-based initial stage. Jiang et al. [112] propose Remix, an adaptive image partitioning and selective execution strategy that involves the execution of existing DNNs on non-uniformly partitioned image blocks. They propose to leverage historical frames to learn the distribution of target objects and achieve higher detection accuracy with a given latency budget or higher inference speedup without accuracy deduction. Hou et al. [95] propose a dynamic inference mechanism known as the Assemble Region-Aware Convolution (ARAC) supernet, which removes redundant operations within CNN models by leveraging spatial redundancy and channel slicing. They propose to split the CNN inference flow into multiple micro-flows and load them into GPU as single models. In this way, NeuLens outperforms baseline methods in terms of latency reduction (up to 58%) while achieving accuracy improvement (up to 67.9%) within the same latency and memory constraints. Reggiani et al. [217] propose BiSon-e, a RISC-V-based architecture that features a binary segmentation to enhance the CPU pipeline. They propose to perform Single Instruction Multiple Data (SIMD) operations on existing scalar Functional Units (FUs) to increase the performance of narrow integer applications on resource-constrained edge devices. In this way, BiSon-e achieves significant energy efficiency and execution time deduction. To address the overload caused by the convolution layer, Park et al. [208] propose mGEMM, which expands the structure of the GEMM and eliminates the problems of memory overhead and low data reuse rate of the GEMM. They propose a reusable block of highly optimized computation on the inner computation kernel and partitioned the"}, {"title": "3.1.2 Multi-Tenant Inference", "content": "Multi-tenant inference refers to the simultaneous execution of multiple distinct AI models, often originating from multiple concurrently running applications. The key to multi-tenant inference is to efficiently manage and process inference requests from multiple tenants with limited resources on the device. Han et al. [83] propose MCDNN, a framework for executing DNNs in video stream analytics using an approximation- based approach. They propose a heuristic scheduling algorithm designed to address approximate model scheduling, which allocates resources based on their usage frequency and utilizes a catalog to choose the most accurate model variant. Mathur et al. [188] propose DeepEye, a small wearable camera running multiple models locally, enabling near real-time image analysis. They propose an inference pipeline that increased processor utilization by scheduling the execution of computation-heavy layers and the loading of memory-heavy layers across multiple models. They also built prototype hardware powered by a quad-core Qualcomm Snapdragon 410 processor on a custom integrated carrier board to demonstrate the feasibility of their design. Guo and Hu [78] propose Potluck, which caches the previously computed results to provide cross-applications approximate deduplication. They propose a set of algorithms tuning the similarity threshold that regulates the degree to which various raw inputs are considered to be \u201cthe same\u201d, which makes Potluck decreases processing latency for vision workloads. Jiang et al. [109] propose Mainstream, a video processing system that addresses resource contention by sharing the same portion of DNN when inference is taken, which avoids redundant work. Additionally, they use an analytical model to estimate the effects of DNNs for an event and give the optimal model and sample rate option, resulting in significant overall event F1-score improvement. In [58], the authors propose NestDNN, a framework that enables resource-aware on-device DL in multi-tenant settings. The key idea of NestDNN is to transform a DNN model into a multi-capacity model, where sub-models with smaller capacity are nested inside sub-models with larger capacity through shared parameters. At runtime, NestDNN incorporates a resource-aware scheduler which selects the optimal sub-model for each DNN model and allocates it the optimal amount of runtime resources so as to jointly maximize the overall performance of all the concurrently running applications. Lee and Nirjon [136] propose a concept of neural weight virtualization. Having each block of memory represent a block of weights for one or more DNNs makes it possible for multiple DNNs to be put into the main memory which has a smaller capacity than the total size of the DNNs. In this way, weight virtualization achieves significant improvement in execution time and energy efficiency. Bateni and Liu [14] propose NeuOS, a latency-predictable framework for DNN-driven autonomous systems. They introduced the notion of a cohort, which represents a group of DNN"}, {"title": "3.1.3 Cross-Processor Inference", "content": "Cross-Processor Inference refers to the ability of a model to perform inference across different types of processors (i.e., CPUs, GPUs, TPUs) within a device. Modern IoT devices are often equipped with multiple heterogeneous processors, each of which is optimized for certain computing tasks. This provides a great opportunity to leverage these heterogeneous processing units to collaboratively perform inference in a cross-processor manner. The realization of cross-processor inference involves a pivotal strategy: model partitioning. This technique capitalizes on the multiple processors to optimize inference tasks by partitioning the models and executing individual partitions on different processors. For example, Lane et al. [130] propose DeepX: a software accelerator for DL execution that allows any developer to use DL methods and automatically lowers resource usage. They propose a deep architecture decomposition algorithm that can decompose models into unit blocks for heterogeneous local device processors, maximizing resource utilization. In [122], the authors propose \u00b5Layer, a low latency on-device inference runtime that accelerates each layer by utilizing the onboard CPU and GPU simultaneously. They propose channel-wise workload distribution to distribute the output channels of an NN layer to both CPU and GPU to fully utilize the resources, achieving a significant reduction in latency. Tan and Cao [246] explore model partitioning between CPU and Neural Processing Units (NPUs). NPUs run DNN models faster but with less accuracy. Consequently, they propose heuristic-based algorithms and Machine Learning based Model Partition, which can explore a range of layer combinations to determine the part for CPU and NPU separately with optimal time-accuracy trade-off. Wang et al. [259] propose AsyMo, which focuses on partitioning the matrix multiplication blocks of DL models on asymmetric multiprocessors. They propose cost-model-directed block partitioning and asymmetry-aware scheduling to balance the tasks. Additionally, they propose to set the frequency by offline profiling energy curves, which achieve more energy efficiency than baselines. Jia et al. [108] propose CoDL, a concurrent DL inference framework that makes optimal use of diverse processors to expedite the execution at the operator level. They propose to use hybrid-dimensional partitioning and operator chaining to reduce sharing-related overhead, and an accurate, lightweight method to predict latency by considering"}, {"title": "3.1.4 Runtime Adaptation", "content": "Runtime adaptation in on-device inference refers to the ability of AI models to adjust and tailor their runtime behaviors in response to the changing available resources of the devices and evolving data inputs over time to deliver optimized system performance. For example, input images with contents that are easy to recognize do not need a large DNN model to process. Given that, in [57], the authors propose FlexDNN, an input-adaptive framework which leverages the early exit mechanism to construct a single DNN model but dynamically adapts its model capacity to matching the difficulty levels of the input images at runtime. In this way, FlexDNN is able to achieve a significant reduction in frame drop rate and energy consumption while maintaining accuracy. Xu et al. [295] propose ApproxDet, a multi-branch framework employed to identify the"}, {"title": "3.2 Offloading", "content": "Given the limited memory and computing capacities of IoT devices, some of them may not be able to run the most efficient AI models by just using their own onboard resources. In such scenarios, it is necessary to offload the execution of part or even the whole model to nearby resourceful edges or the cloud. As summarized in Figure 11, existing works on offloading can be grouped into four categories: model partitioning, workload partitioning, communication optimization, and privacy-preserving offloading.\nModel Partitioning. Model partitioning refers to the task of partitioning the AI model between the IoT devices and the nearby resourceful edge or cloud server such that different parts of the AI model are executed in a distributed manner. For example, Kang et al. [118] propose Neurosurgeon, a framework that automatically partitions the DNN computation at the layer level. Neurosurgeon partitions the DNN into two parts for computa- tion on mobile devices and the cloud, respectively, and trains a predictive model during the deployment phase to identify the optimal partition point of the model. In this way, Neurosurgeon achieves significant end-to-end inference latency and energy consumption reduction over cloud-only methods. Huang et al. [98] propose CLIO, a framework enabling model compilation for extremely resource-constrained devices. They propose a novel technique for progressively partitioning models between the cloud and an end device, offering a variety of accuracy-bandwidth tradeoffs. This technique can be integrated with existing model compression and adaptive model partitioning techniques to achieve enhanced performance. In [99], the authors propose AgileNN, an offloading technique that minimizes online computation and communication costs by putting a few valuable features computed locally and thus reducing the size of the local model. They propose using eXplainable AI to estimate the most important features in the top k and retained by the local network to make a part prediction combined with the prediction by the remote network from other less important features for the final result.\nWorkload Partitioning. Workload partitioning refers to the distribution of workloads such as input data (e.g., images, SLAM map) and different DL models within the same processing pipeline across various edge devices and cloud servers to optimize performance, reduce latency, and improve resource utilization. In [70], the"}, {"title": "3.3 On-Device Training", "content": "Besides on-device inference", "categories": "training on a single device, and training across distributed devices.\nTraining on a Single Device. In the case of single-device training, the entire training process takes place on a single device. To achieve effective training on a single device, existing"}]}