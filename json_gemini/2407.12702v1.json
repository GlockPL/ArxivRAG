{"title": "TransCAD: A Hierarchical Transformer for CAD Sequence Inference from Point Clouds", "authors": ["Elona Dupont", "Kseniya Cherenkova", "Dimitrios Mallis", "Gleb Gusev", "Anis Kacem", "Djamila Aouada"], "abstract": "3D reverse engineering, in which a CAD model is inferred given a 3D scan of a physical object, is a research direction that offers many promising practical applications. This paper proposes TransCAD, an end-to-end transformer-based architecture that predicts the CAD sequence from a point cloud. TransCAD leverages the structure of CAD sequences by using a hierarchical learning strategy. A loop refiner is also introduced to regress sketch primitive parameters. Rigorous experimentation on the DeepCAD [43] and Fusion360 [41] datasets show that TransCAD achieves state-of-the-art results. The result analysis is supported with a proposed metric for CAD sequence, the mean Average Precision of CAD Sequence, that addresses the limitations of existing metrics.", "sections": [{"title": "1 Introduction", "content": "Practically every object encountered in daily life originates from a Computer-Aided Design (CAD), highlighting the fundamental role of CAD in industrial manufacturing processes. Currently, the dominant paradigm for CAD design is feature-based modelling [48]. It allows the creation and manipulation of 3D models through a series of features, individual elements or operations (holes, slots, fillets, etc.), that modify the geometry of a CAD model. The process is typically initiated with the design of planar sketches, i.e. collection of loops composed of 2D curves, followed by a CAD operation (extrusion, revolution, etc.) that expands sketches into a 3D solid model. The final model is represented by the sequence of these CAD sketches and operations. Feature-based modelling has been widely adopted, as it enables intuitive design alterations and seamless CAD software integration, making it essential for an iterative development of complex designs. The recent availability of large CAD model datasets, such as ABC [18] and Fusion360 [41], has sparked significant interest in developing learning-based approaches for feature-based modelling. Recent efforts have been focused on deep generative modelling [33, 43, 45,47], where large transformer-based networks are trained to create new CAD models or to automatically complete partial designs via autoregressive inference. While this research direction offers a lot of potential practical applications for CAD software integration, far less attention has been put to reverse engineering. Feature-based reverse engineering emerges as a real-world application addressing the need to automatically replicate physical objects as CAD models. Recovery of CAD design is facilitated by the acquisition of a point cloud or triangular mesh of a physical object scanned using commercial 3D sensors.\nSome existing reverse engineering approaches investigate the recovery of al-ternative CAD model representations like Constructive Solid Geometry [15] (CSG) or Boundary-Representation (B-Rep) [8, 14, 19]. Other methods tackle feature-based reverse engineering and predict implicit representations of sketches and CAD operations from point clouds [23,36]. Nevertheless, such approaches do not allow for seamless integration into CAD software and often require post-processing (e.g. parametric curve fitting). To address these limitations, models capable of learning explicit CAD sequence of parametric sketches and operations from point clouds are needed. This can be enabled within a generative learning framework as in [43]. In that work, an auto-encoder reconstructing CAD para-metric sequences is proposed and the latent representation is used for generating novel CAD sequences. An extension for reverse engineering was proposed by re-placing the CAD sequence encoder with a point cloud encoder trained to map point clouds to the latent representations. The main limitation of the above is the predefined latent space that cannot adapt to the variations present in real-world point clouds. This disconnection can cause the model to generalize poorly to unseen inputs, especially those with noise or irregularities that are present in 3D scans and that are not well-represented in the training data.\nTo that end, we propose TransCAD, a novel end-to-end trainable and single-stage hierarchical network for feature-based explicit CAD sequence reverse en-gineering from point clouds. Our network is hierarchical in the sense that it employs a two-tiered decoding process. Initially, a primary CAD sequence em-bedding is decoded, encapsulating high-level features of the design, that are then processed by secondary decoders, one dedicated to loop parameters and another to CAD operations. Each decoder specializes in a certain input, enabling a nu-anced and precise recovery of CAD parameters. The decomposition of learned representations matches the decomposition inherent in the actual feature-based design process of conceptualizing a 3D model through distinct loop and operation steps. Moreover, TransCAD does not predict sketch primitive types explicitly as in [43]; instead, we employ a unified primitive representation where types are determined solely by coordinates. Our formulation narrows the learning space by eliminating syntactically incorrect predictions and facilitates a seamless tran-sition between different primitive types. Additionally, it allows for a cascaded parameter refining that further enhances model performance.\nAnother focus of this work is the evaluation of parametric CAD sequence. We identify several limitations of the existing evaluation framework used by [27,43] and suggest a suitable metric for CAD sequence similarity based on mean average precision, computed in the unquantized parametric space."}, {"title": "Contributions:", "content": "In summary our contributions are the following:\n1. We propose TransCAD, a novel hierarchical architecture for feature-based reverse engineering. Our model is single-stage and end-to-end trainable. TransCAD allows for a compact CAD sequence representation that does not include categorical types and enables cascaded coordinate refinement.\n2. We identify several limitations of the existing evaluation framework for feature-based reverse engineering and propose a new evaluation metric frame-work to ensure fair comparison among diverse network architectures.\n3. Our model surpasses the performance of recent generative-based approaches while also bridging the gap to real-world applications by exhibiting robust-ness to perturbed point clouds."}, {"title": "Paper Organization:", "content": "The rest of the paper is organized as follows. Section 2 reviews related works. Section 3 formulates the problem of feature-based CAD reverse-engineering. The proposed TransCAD is described in Section 4. Discussion on the current evaluation framework and suggested extension is introduced in Section 5. An experimental validation of the proposed network is provided in Section 6. Finally, conclusions are given in Section 7."}, {"title": "2 Related Works", "content": "Generative Models for CAD: The advent of large-scale 3D shape datasets [4, 18, 24, 41], combined with the significant progress for generative models in vision [5, 10, 12, 16], has sparked interest in the generation of 3D shapes. Existing methods have been proposed for various 3D representations, including point clouds [1,2,49], 3D meshes [29,38], voxel grids [21,42], and signed distance functions [6,44]. This work focuses on CAD model generation, which compared to the above is parametric and directly editable in CAD software. A line of work explores the generation of the Boundary-Representation (B-Rep), a collec-tion of parametric surfaces connected via a structured topological graph. Solid-Gen [13] considers B-Rep synthesis based on transformers and two-level pointer networks. BrepGen [46] represents a B-Rep via a fixed tree of latent geometry representations that can be generated by a diffusion model. Feature-based CAD generation has also been recently explored. Most relevant to our work is Deep-CAD [43], a non-autoregressive generative model capable of synthesizing novel CAD sequences based on a transformer auto-encoder architecture. In [45, 47] the authors also follow autoregressive strategies. HNC [45] uses a hierarchical model based on high-level concepts and a code tree for CAD model generation and auto-completion. Similarly, in SkexGen [47] a transformer architecture is used to generate CAD models in the sketch-extrude format by encoding the topology and geometry using different codebooks. All the aforementioned works are oriented around 3D shape generation and either do not address the reverse-engineering task or address it via adaptation of generative modelling leading to suboptimal performance.\nCAD Reverse Engineering: Reverse engineering is a well-studied problem, with a substantial research effort directed towards predicting geometric features of CAD models, by analyzing the corresponding point clouds. Parametric fit-ting techniques infer the parametarization of edges [7, 25, 34, 39,51,52] and sur-faces [11, 22]. Various attributes of the B-Rep and CAD operations are recov-ered from 3D scans in [28]. CADOps-Net [8] recovers 2D sketches from faces segmented into their CAD operation steps. Reasoning about a CAD model via properties discovered by parametric fitting offers insights solely into its end-state, without considering the sequential CAD design process intrinsic to feature-based modeling.\nA step closer to CAD reverse engineering, a line of work explores the recon-struction of a point cloud into Constructive Solid Geometry (CSG) [9, 15,50], a modelling technique that uses boolean operations to combine primitives into 3D models. Point2Cyl [36], on the other hand, predicts extrusion cylinders given a point cloud, but requires user input to combine cylinders. SECAD-Net [23] and ExtrudeNet [33] use a self-supervised learning strategy to recover CAD se-quences in the form of implicit representations given voxels and point clouds, re-spectively. In contrast to feature-based modelling, 3D representations produced by these methods (CSG, extrusion cylinders, etc.) have limited compatibility with modern CAD software workflows. The authors in [20] learn sketch-extrude sequences conditioned on a voxel input, however, the model relies on strong data priors and is limited to predefined extrusion combinations.\nCloser to our work is DeepCAD [43] and subsequent MultiCAD [27]. Even though DeepCAD [43] proposes a non-autoregressive generative framework for feature-based CAD, authors explore further conditioning on input point clouds. Taking a similar direction, MultiCAD [27] proposes a two-stage multimodal con-trastive learning strategy of both point clouds and CAD sequences. The two aforementioned methods opt for separate stage learning for point clouds and CAD sequences. Concurrent to our work, the autoregressive strategy in [17] and the multimodal diffusion based approach in [26] attempt to solve the point cloud to CAD sequence problem. To our knowledge TransCAD is the first non-autoregressive single-stage architecture for feature-based reverse engineering."}, {"title": "3 Problem Statement", "content": "A CAD model $C \\in \\mathcal{C}$ is constructed in a sequence of construction steps. Each step can be seen as a 2D parametric sketch $s \\in \\mathcal{S}$ (e.g. set of lines, arcs, etc.) followed by a CAD operation $o \\in \\mathcal{O}$ (e.g. extrusion, revolution, etc.) [43, 47]. Here, $\\mathcal{C}$ is the set of all possible CAD models, $\\mathcal{S}$ and $\\mathcal{O}$ represent the sets of possible CAD sketches and operations, respectively. CAD models constructed exclusively from the extrusion operation type are considered in this work. Extrusion $e \\in \\mathcal{E}$, where $\\mathcal{E}$ denotes the set of possible extrusions, is the most common operation and en-ables the description of a wide range of CAD models [43,45,47]. TransCAD aims at learning how to predict the sequence of CAD construction steps from an input point cloud. Formally, given a point cloud $P = [p_1,...,p_n] \\in \\mathbb{R}^{n \\times 3}$, where $p_i = [x_i, y_i, z_i]$ denotes the 3D coordinates of the point $i$ and $n$ the number of points, the objective of TransCAD is to learn the mapping\n$$\\Phi: \\mathbb{R}^{n \\times 3} \\rightarrow \\mathcal{C},$$  $$\\Phi(P) = \\{s_i, e_i\\}_{i=1}^{L},$$  where $L$ denotes the length of the CAD sequence. In what follows, the proposed formulations of sketches and extrusions are described.\nCAD Sketch and Extrusion Formulation: The proposed formulations for sketch and extrusion steps are inspired by [43, 47]. A sketch $s$ is composed of one or more loops (see left panel of Fig. 1). Each loop $\\{p_j\\}_{j=1}^{L_p} \\in \\mathcal{P}$, where $L_p$, denoting the number of loops, consists of one primitive (i.e. circle) or a combi-nation of primitives (i.e. lines and arcs). In contrast to [43] which specifies the type of primitives in their representation, the proposed primitive representa-tion is type-agnostic (see right panel of Fig. 1). In particular, each primitive $\\delta$ is represented by three 2D coordinates of start, mid, and end points, i.e. $\\delta = [(x_{\\text{start}}, y_{\\text{start}}), (x_{\\text{mid}}, y_{\\text{mid}}), (x_{\\text{end}}, y_{\\text{end}})] \\in \\mathbb{R}^6$. This representation has the advantage that the type of primitive can be deduced from the configurations of the points, hence reducing the search space and facilitating the transition between different types during training. In practice, the mid point of a line is replaced by a dummy value.\nAs in [43], we ensure that the loops are always closed by using the end point of a primitive as the start point of the next one. Further, a similar to [43] quantization is considered to reduce the parameters search space. As a result, a loop of $n_p$ primitives $p_j \\in \\mathbb{R}^{6 \\times n_p}$ is considered as a quantized representa-tion $p \\in [0..d_q]^{6 \\times n_p}$, where $d_q$ denotes the quantization interval. As for extru-sion, similarly to [43], a quantized representation $e \\in [0..d_q]^{11}$ is consid-ered to represent the sketch plane/scale and extrusion type/distances. Note that $e^* \\in [0..d_q]^{11 \\times L_e}$ and $p^* \\in [0..d_q]^{6 \\times n_p \\times L_p}$ will be used in the following to denote sequences of quantized extrusions and loops, respectively. Here, $L_p$ and $L_e$ denote the length of loop and extrusion sequences, respectively."}, {"title": "4 Hierarchical CAD Sequence Learning from Point Clouds", "content": "TransCAD non-autoregessively learns to predict a CAD sequence from an in-put point cloud in the format described in Section 3. First, the point cloud is encoded into point features using a standard point cloud encoder. In order to facilitate the learning of CAD sequences, a hierarchical CAD sequence decoding is proposed. In particular, a high-level sequence of embedding corresponding to the loop and extrusion steps is learned. Those embedding are then fed to ei-ther a loop or extrusion decoder based on predicted type to learn the loop and extrusion parameters. Finally, the predicted loop parameters are further refined using the actual unquantized loop parameters (ground truth). The overall model architecture is depicted in Fig. 2 and the different components are described be-low."}, {"title": "4.1 Point Cloud Encoder", "content": "The point cloud encoder $\\Phi_p$ consists of 4 layers of PointNet++ [32] and operates on an input point cloud $P$. It outputs per-point features encoding local neighbor-hood information, $F_p = [f_1...f_n] \\in \\mathbb{R}^{n \\times d_p}$, that encode local neighborhood information, $d_p$ denotes the dimension of the features. Note that point normals of $P$ are estimated using [35] and are provided as input to $\\Phi_p$ along with its 3D coordinates."}, {"title": "4.2 Loop-Extrusion Decoder", "content": "The main objective of the loop-extrusion decoder $\\Phi_{p,e}$ is to learn a high-level sequence of embedding $F_{p,e} = [f^e_1,...,f^e_{L_{p,e}}] \\in \\mathbb{R}^{d_z \\times L_{p,e}}$ corresponding to loops and extrusions from the point cloud features $F_p$. Here, $d_z$ and $L_{p,e}$ denote the embedding dimension and the length of the sequence, respectively.\nThe decoder $\\Phi_{p,e}$ is composed of multi-head transformer-based blocks [37]. In the first block, learned constant embedding $F_c \\in \\mathbb{R}^{d_z \\times L_{p,e}}$ undergo a self-attention operation [37] and the resulting representation cross-attends to the point cloud features $F_p$ to produce loop extrusion embedding for the first block $F^1_{p,e} \\in \\mathbb{R}^{d_z \\times L_{p,e}}$ as follows,\n$$F^1_{p,e} = CA(SA(F_c), F_p),$$\nwhere $SA(.)$ and $CA(.,.)$ denote the self and cross attention operators [37], re-spectively. The same self and cross attention operations are conducted in the subsequent blocks by feeding the output of each block as input to the next one,\n$$F^i_{p,e} = CA(SA(F^{i-1}), F_p),$$\nyielding the final sequence of embedding $F_{p,e}$ after the last block. In order to ensure that each element $f^e_i$ in the sequence embedding $F_{p,e}$ corresponds to the right type (i.e. loop, extrusion, or end of sequence), a 3 layer MLP followed by softmax that operates on each $f^e_i$ and predicts its type is introduced. A cross-entropy loss, $\\mathcal{L}_{p,e}$, is computed between the predicted and ground truth types to supervise the learning of $F_{p,e}$. Note that the loop-extrusion decoder is solely used to obtain a high-level sequence of loop and extrusion embedding. These embedding can then be decoded through either a loop decoder or an extrusion decoder to obtain their parameters. At training time, the ground truth type labels are used to identify which decoder should be used for each embedding, while at inference time the predicted types are used. The identification of loop and extrusion types results into separate loop embedding $F_{p} \\in \\mathbb{R}^{d_z \\times L_{p}}$ and extrusion embedding $F_{e} \\in \\mathbb{R}^{d_z \\times L_{e}}$ by splitting $F_{p,e}$ according to loop and extrusion types."}, {"title": "4.3 Loop and Extrusion Parametrization", "content": "After obtaining the representation and the type of loop and extrusion steps, the parameters of both loops and extrusions are decoded from these representations using separate decoders.\nExtrusion Decoder: As mentioned in Section 3, the extrusion sequence is described by a sequence of 11 quantized parameters $e^* \\in [0..d_q]^{11 \\times L_e}$. In order to obtain these parameters from the extrusion sequence embedding $F_e$, an extrusion decoder $\\Phi_e$ consisting of 3 MLP layers followed by softmax is used. The predicted probabilities of the extrusion sequence parameters $\\breve{e}^* \\in [0,1]^{11 \\times d_q \\times L_e}$ are compared to the ground truth one-hot-encoded parameters in $e^*$ using a cross-entropy loss, $\\mathcal{L}_e$.\nLoop Decoder: Similarly to the extrusion decoder, the loop decoder $\\Phi_p$ pre-dicts the quantized parameters of the loop sequence $p^* \\in [0..d_q]^{6 \\times n_p \\times L_p}$ as explained in Section 3. Nevertheless, 4 layers of multi-head transformer blocks are employed instead of simple MLP layers. This is due to the sequential na-ture of loop decoding in contrast to extrusions. Note that a similar strategy as loop-extrusion decoder is opted for the transformer block of loop decoder. The first block performs self-attention on learned constant embedding of loops $F_l \\in \\mathbb{R}^{d_z \\times n_p L_p}$ and the result cross-attends to loop embedding $F_{p}$, as in Eq.(2). The same self and cross attention operations in Eq.(3) are conducted in subse-quent blocks to yield a final representation at the last block $F^b \\in \\mathbb{R}^{d_z \\times N_p L_p}$. A linear layer followed by softmax is used to obtain predicted probabilities for the loop sequence parameters $p^* \\in [0,1]^{6 \\times d_q \\times n_p L_p}$ which are compared to ground truth one-hot-encoded loop parameters of $p^*$ using a cross-entropy loss, $\\mathcal{L}_p$.\nLoop Refiner: As in many transformer-based architectures [3, 37, 43, 47], the quantization of loop parameters helps to reduce the search space and facilitates the learning. However, it has been observed in our case that it can lead to accu-mulation of quantization approximation errors. To overcome this issue, unquan-tized ground truth loop parameters are leveraged. In particular, a loop refiner $\\Phi_r$ composed of a 4 layer MLP is introduced. This refiner takes as input a con-catenation of loop embedding $F_{p}$, and their corresponding predicted parameter probabilities $\\breve{p}^*$. It attempts to predict the offset $Off \\in \\mathbb{R}^{6 \\times n_p L_p}$ between the predicted quantized loop parameters $\\breve{p}^* \\in [0..d_q]^{6 \\times n_p L_p}$ and the unquantized ground truth loop parameters $p^* \\in \\mathbb{R}^{6 \\times n_p L_p}$. An MSE loss, $\\mathcal{L}_r$, is computed between the predicted offset $\\widehat{Off}$ and the one given by $Off = p^* -\\breve{p}^*$, to supervise the refiner and the rest of the network. Once the offset is predicted, it is added to the predicted quantized loop parameters yielding unquantized predicted loop parameters as follows $\\hat{p} = \\breve{p}^* + \\widehat{Off}$.\nTotal Loss: TransCAD is an end-to-end network with a training objective guided by the sum of the individual losses, $\\mathcal{L}_{total} = \\mathcal{L}_{p,e} + \\mathcal{L}_{p} + \\mathcal{L}_{e} + \\mathcal{L}_{r}$."}, {"title": "5 Proposed Evaluation", "content": "In this section, we first outline the limitations of existing evaluation methods in CAD sequence. Then, our new proposed evaluation metric framework for assess-ing the performance of CAD sequence inference from point clouds is described.\nDeepCAD [43] Evaluation for Feature-based Reverse Engineering: An evaluation framework for CAD sequence was originally introduced in [43] and later used in [27]. This framework includes both accuracy for assessing the fidelity of the predicted sequence and Chamfer Distance (CD) to measure the quality of the recovered 3D geometry. Accuracy is assessed using two metrics, specifically Command Type Accuracy ($\\text{ACC}_{cmd}$) and Parameter Accuracy ($\\text{ACC}_{param}$) de-fined by\n$$\\text{ACC}_{cmd} = \\frac{1}{N_c} \\sum_{i=1}^{N_c} \\mathbb{I}[t_i = \\hat{t_i}],$$\n$$\\text{ACC}_{param} = \\frac{1}{N_c} \\sum_{i=1}^{N_c} [\\frac{1}{K} \\sum_{j=1}^{K} \\mathbb{I}[|p_{i,j} - \\hat{p}_{i,j}| < \\eta] \\mathbb{I}[t_i = \\hat{t_i}],$$\nwhere $t_i$ and $\\hat{t_i}$ are the ground truth and predicted command types (for com-mands representing primitives and extrusions), $p_{i,j}$ and $\\hat{p}_{i,j}$ are ground truth and predicted command parameters, $N_c$ denotes the total number of CAD com-mands and $\\mathbb{I}[.]$ is the indicator function. $K = \\frac{1}{\\mathbb{I}[t_i = \\hat{t_i}]|p_i|}$ is the total number of parameters of the correctly recovered commands and $\\eta$ is a tolerance threshold. The 3D geometry is evaluated with Chamfer Distance (CD) computed by sampling 2000 points on the ground truth and predicted shapes.\nLimitations: We identify the following limitations of the aforementioned evalu-ation. (1) The proposed ACCcmd overlooks the possibility of over-prediction in the CAD sequence. As indicated in Eq.(4), the computation of this metric sums across the set of ground truth CAD commands Nc. A predicted sequence could erroneously include extra loop-extrusion operations and still achieve a full score, as exemplified on the left panel of Fig. 3. (2) The evaluation of ACCparam is conducted solely on the subset of K accurately identified commands, thus in-troducing a trade-off between ACCparam and ACCcmd. This interdependence complicates the interpretation of results. (3) Assessment of parameter quality via ACCparam solely in terms of accuracy is failing to distinguish between the magnitudes of errors. A parameter inaccurately placed in an adjacent quan-tization bucket incurs the same penalty as one with a larger deviation, despite potentially minor implications on the CAD model's final geometry. These limita-tions cannot be entirely mitigated by complementing CAD command accuracies with the chamfer distance (CD) metric. While CD is a valuable assessment of shape similarity, it does not address the core objective of reverse engineering: to accurately recover the designer's original CAD sequence. Two CAD models might be close in terms of CD yet possess vastly different CAD construction steps (see right panel of Fig. 3).\nProposed Evaluation Framework: To overcome the identified challenges, we introduce the mean Average Precision of CAD Sequence (APCS), a novel eval-uation metric tailored for feature-based reverse engineering. APCS adopts the concept of Average Precision (AP) commonly used in other tasks, to quantify the similarity between predicted and ground truth CAD sequences. We intro-duce the CAD Sequence Similarity Score (CSSS) that can be computed between predicted and ground truth CAD sequences as follows\n$$CSSS(C, \\hat{C}) = \\frac{1}{N_p} \\sum_{j=1}^{N_p} \\sum_{i=1}^{N_{P}} [S(s_{j,i}, \\hat{s}_{j,i}) \\cdot \\mathbb{I}[typ(s_{j,i}) = typ(\\hat{s}_{j,i})]] + \\frac{1}{N_e} \\sum_{j=1}^{N_e}S(e_j, \\hat{e}_j),$$\nwhere $typ(.)$ is a function that determines the type of each primitive $s$ (arc, line, etc.), and $S(p,\\hat{p}) = e^{-k||p-\\hat{p}||}$ is a scoring function with $S(p,\\hat{p}) \\in [0,1]$ with 1 assigned when predicted parameterization is identical to the ground truth. We define $N^p_f = max(|p_i|,|\\hat{p_i}|)$ where $|.|$ denotes set cardinality, $N_s = max (L_p, \\hat{L}_p)$ where $L_p$, is the number of predicted primitives and $\\hat{L_p}$ is the number of ground truth primitives for loop $p_i$ and $N_e = max (L_e, \\hat{L}_e)$ where $L_e$ is the number of predicted extrusions and $\\hat{L}_e$ is the number of ground truth extrusions. The proposed CSSS metric evalu-ates both the operation type and parameter prediction. It assigns a score of 0 to loops with incorrectly predicted types, which gradually increases to 1 as parame-ter prediction improves. Assessment is conducted on the unquantized parameter space and calculates the score based on the maximum count of either predicted or ground truth primitives. This approach ensures that both over and under pre-dicted sequences are penalized equally. We aggregate CSSS scores across various thresholds to derive the mean Average Precision of CAD Sequences (APCS). Furthermore, the median CD is used to measure shape similarity as in [43] with the difference that it is evaluated on 4096 points instead of 2000 in order to decrease the uncertainty in the CD measurement. All the reported CD mea-surements in this work are multiplied by 103. Moreover, the ratio of predictions that cannot be reconstructed using [30] is reported as the invalidity ratio, IR."}, {"title": "6 Experiments", "content": "In this section, the experimental setup is first presented. Then, qualitative and quantitative results are analyzed. Afterwards, the components of TransCAD are ablated. Finally, the limitations of our model are outlined."}, {"title": "6.1 Experimental Setup", "content": "Dataset: For training and evaluation, the DeepCAD dataset [43] is used. The sketch extrusion sequences of the CAD models are processed in quantized (8 bits) and unquantized space. The size of the train, validation and test sets are 140 294, 7773, and 7036 CAD models, respectively. Moreover, cross-dataset evaluation is conducted on the Fusion360 dataset [41] that contains 6794 samples.\nTraining Details: The network is trained for 100 epochs with a batch size of 72 and an Adam optimizer is employed with a learning rate of 0.001 and a linear warm-up period of 2000 steps as in [43]. The training is conducted on an NVIDIA RTX A6000 GPU. The input point clouds are extracted using [30] and are made of n = 4096 points. The dimension of the point features dp is set to 16 and of loop-extrusion features dz to 256.\nBaselines: In order to evaluate the performance of TransCAD, two state-of-the-art methods, MultiCAD [27] and DeepCAD [43], and a retrieval baseline are used. As the code for MultiCAD [27] is not available, we report the results from the original paper. DeepCAD [43] is retrained with the same parameters and procedure as outlined in the original paper. One of the known limitations of the DeepCAD dataset is that it contains duplicate models [47]. While the works in [47] proposed a method to remove duplicate models that contain exactly the same CAD sequence, we find that this method does not remove all the duplicates as some models can have the same geometry but are constructed through a slightly different sequence of sketch extrusion operations (see supplementary material for more details). In order to address this issue, we propose a retrieval baseline. The retrieval baseline uses the point cloud encoder from a trained DeepCAD [43] to identify the closest latent vector from the train set for each test sample. As a result, the solution is always a train set CAD sequence."}, {"title": "6.2 Experimental Results", "content": "Qualitative Results: Fig. 4 shows some qualitative results for the retrieval baseline, DeepCAD [43] and TransCAD (Ours) on both the DeepCAD [43] and Fusion360 [41] datasets. As mentioned in Section 6.1, the DeepCAD dataset contains many duplicates, not just in terms of CAD sequence but also in terms of geometrical shape. As a result, the retrieval baseline is able to identify accurately duplicates (most right column of the DeepCAD dataset panel) and in other cases the baseline manages to retrieve shapes with similar geometry as the ground truth CAD model. On the other hand, it can be noticed that DeepCAD [43] can even fail at retrieving duplicates. TransCAD is able to predict models that are similar in shape and also in terms of loop-extrusion sequence, even though it sometimes fails to predict the parameters accurately (second and fifth columns of DeepCAD dataset and fourth column of Fusion360 dataset).\nQuantitative Results: The trends observed from the qualitative results are further supported by the quantitative results presented in Table 1. The APCS, on both DeepCAD [43] and Fusion360 [41] datasets, show that TransCAD is the most capable model at predicting correct CAD sequences. However, it can be noted that the retrieval baseline obtains the lowest CD by a small margin on the DeepCAD dataset and by a more significant margin on the Fusion360. One of the reasons is that this baseline always outputs a CAD model that is of roughly similar shape as the input even if the retrieved CAD sequence can vary from the ground truth. To further analyse the results, the variations of the APCS and CD w.r.t. model complexity on the DeepCAD dataset are displayed in Fig. 5. We define the model complexity as the lowest possible CD of a test point cloud sample with respect to the train samples. In other words, the model complexity quantifies the amount by which a test sample is out of distribution from the train set in terms of shape. While TransCAD consistently outperforms on average all other baselines in terms of APCS for all model complexities, DeepCAD [43] can only perform better than the retrieval baseline for the more complex models. This shows that DeepCAD [43] often fails at retrieving the CAD sequence for simple models. In terms of CD, it can be observed that TransCAD and the retrieval baseline have similar performance. TransCAD can predict a CAD sequence that is closer to the ground truth one but the predicted overall shape can vary from the ground truth for more difficult samples.\nAblation Study: In this section, the different components of the proposed net-work architecture are ablated. Table 2 shows the results for Ours w/o hier., in which the learning is done without both the loop-extrusion decoder and the refining network, Ours w/o refining where the refining component is ablated and Ours. It can be noted that each component leads to an improvement in all the metrics. It is worth noting that the F1 score on the loop-extrusion type prediction introduced for the hierarchical learning of TransCAD is 0.79. This implies that on most cases both loop and extrusion decoders receive embedding of the correct type. Moreover, while the refining network is only applied on the loop parameters, we observe that the component of the APCS for the extrusion parameters are also higher when the network is trained with the refining com-ponent. This suggests that the refining network is able to provide a useful signal for guiding the learning process. More details are in the supplementary material.\nInput point cloud perturbation: Reverse engineering is a real-world practical problem. The results in the previous section are obtained from sampling points from the B-Rep representations of the CAD models. While modern 3D sensors can reconstruct the mesh of models with high resolution, they still suffer from some artifacts such as noise and small missing parts. In order to evaluate the per-formance of our network in such realistic conditions, we run experiments in two scenarios, one in which noise is added and one in which small holes are created on the point cloud. In order to simulate realistic noise, Perlin noise [31] is added to the mesh from which the point coordinates and normals are extracted. More details about the noise and hole generations can be found in the supplementary materials. Table 3 shows that TransCAD is more robust to such perturbations than other methods. As the noise also adds a disturbance to the direction of the input point normals, it leads to a larger drop in performance."}, {"title": "Failure Cases and Limitations:", "content": "In this section, we describe the reasons that lead TransCAD to predict invalid CAD models as measured by IR. Among the predictions of TransCAD on DeepCAD test set, only one contains a loop parametrization that results in an invalid CAD sequence. This shows that the proposed representation of the loop sequence leads to syntactically correct loops on practically all cases. However,in some cases the representation of the CAD sequence can be syntactically valid, yet it is not possible to reconstruct a B-Rep from it. For 75 test samples, the loop-extrusion decoder fails to predict an extrusion token, this implies that the predicted model is therefore an infinitely thin sketch and not a 3D model as expected. The rest of the invalid models are mostly due to a loop being made of a single line within a model, which cannot be extruded into a valid 3D shape within our context. Finally, Fig. 6 shows ex-amples for which the predicted sequences do not lead to a shape that is close to the ground truth one. In these examples, the input CAD models contain a large number of small features that TransCAD is unable to capture."}, {"title": "7 Conclusion", "content": "In conclusion, we propose TransCAD an end-to-end transformer-based neural network that learns to recover the CAD sequence from a given point cloud. Two of the main features of TransCAD are a hierarchical structure that enables the learning of a high-level loop-extrusion sequence and a loop refiner that aims at correcting errors in loop parameter predictions. We also propose a primitive representation in which each primitive is described by the same number of pa-rameters. We identify the limitations of current metrics in the emerging domain of 3D reverse engineering and propose a new metric, the APCS that leads to a fair comparison of parametric CAD sequences. Thorough experiments show that TransCAD achieves state-of-the-art results in different realistic scenarios."}]}