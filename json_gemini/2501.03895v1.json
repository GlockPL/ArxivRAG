{"title": "LLAVA-MINI: EFFICIENT IMAGE AND VIDEO LARGE MULTIMODAL MODELS WITH ONE VISION TOKEN", "authors": ["Shaolei Zhang", "Qingkai Fang", "Zhe Yang", "Yang Feng"], "abstract": "The advent of real-time large multimodal models (LMMs) like GPT-40 has sparked considerable interest in efficient LMMs. LMM frameworks typically encode visual inputs into vision tokens (continuous representations) and integrate them and textual instructions into the context of large language models (LLMs), where large-scale parameters and numerous context tokens (predominantly vision tokens) result in substantial computational overhead. Previous efforts towards efficient LMMs always focus on replacing the LLM backbone with smaller models, while neglecting the crucial issue of token quantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal vision tokens. To achieve a high compression ratio of vision tokens while preserving visual information, we first analyze how LMMs understand vision tokens and find that most vision tokens only play a crucial role in the early layers of LLM backbone, where they mainly fuse visual information into text tokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to fuse visual information into text tokens in advance, thereby facilitating the extreme compression of vision tokens fed to LLM backbone into one token. LLaVA-Mini is a unified large multimodal model that can support the understanding of images, high-resolution images, and videos in an efficient manner. Experiments across 11 image-based and 7 video-based benchmarks demonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token instead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by 77%, deliver low-latency responses within 40 milliseconds, and process over 10,000 frames of video on the GPU hardware with 24GB of memory.", "sections": [{"title": "INTRODUCTION", "content": "Large multimodal models (LMMs), such as GPT-40 (OpenAI, 2024), equip large language models (LLMs) (OpenAI, 2022; 2023) with the ability to understand visual information, exhibiting a common trend toward low-latency responses to enable real-time multimodal interactions. Recently, the most widely adopted LMMs (Liu et al., 2023b; 2024a; Zhu et al., 2024), exemplified by the LLaVA series (Liu et al., 2023b), involves embedding image patches into vision tokens through a vision encoder (Radford et al., 2021) and incorporating them into the LLM's context to facilitate visual information comprehension, leading to strong performance in image and video understanding.\nHowever, the substantial computational costs of LMMs present ongoing challenges. Unlike LLMs (Touvron et al., 2023a;b; Dubey et al., 2024), which only process textual inputs, LMMs must incorporate a large number of additional vision tokens into the LLM's context to represent visual information (Liu et al., 2023b), significantly increasing computational complexity. For instance, in the widely used vision encoder CLIP ViT-L/336px, a single image is encoded into 24 \u00d7 24 = 576 vision tokens (Radford et al., 2021), where integrating such a large number of vision tokens into"}, {"title": "RELATED WORK", "content": "As Large multimodal models (LMMs) are increasingly deployed in real-time applications (OpenAI, 2024), enhancing their efficiency has become a critical concern. Recent efforts focus on either"}, {"title": "How DOES LLAVA UNDERSTAND VISION TOKENS?", "content": "To compress visual tokens while preserving visual understanding, we sought to figure out how LMMs understand visual tokens. Given the complexity of this issue, our preliminary analysis concentrated on the LLaVA architecture (Liu et al., 2023b), focusing on the role of visual tokens (particularly their quantity) in LMMs from an attention-based perspective (Xiao et al., 2024)."}, {"title": "LLAVA ARCHITECTURE", "content": "LLaVA (Large Language and Vision Assistant) (Liu et al., 2023b) is an advanced multimodal ar-chitecture that integrates vision and language processing capabilities. Building upon vision Trans-formers (ViT) (Dosovitskiy et al., 2021) for visual inputs and LLMs for text, LLaVA can generatelanguage response X\u00aa based on the given language instruction X\u00ba and visual inputs XV.\nTypically, a pre-trained CLIP ViT-L/14 (Radford et al., 2021) and a projection layer are employedto encode the visual inputs X into vision tokens (i.e., continuous representations) HV. Then, visiontokens Hv and language instruction's embedding Hq are fed into an LLM, such as Vicuna (Chianget al., 2023) or Mistral, to generate the response X\u00aa. In practice, vision tokens are often insertedinto the middle of the language instruction, so the inputs of LLM can be formally represented as:\n{H\u2081,\u2026\u2026\u2026, H,H,..., H2+1,\u00a8\u00a8\u00a8, H} (1)\nwhere lv and la denote the lengths of the vision tokens and language instruction, respectively. Forinstance, in LLaVA-v1.5, the system prompts are positioned before the image (i.e., H\u2081,\u2026\u2026\u2026, H),while the user inputs follow the image (i.e., H+1,\u00a8\u00a8\u00a8, H) (Liu et al., 2023b)."}, {"title": "PRELIMINARY ANALYSES", "content": "We begin by analyzing the significance of visual tokens in LMMs to guide the strategies for com-pressing vision tokens. Specifically, we evaluate the importance of visual tokens at each layer ofLMMs from an attention-based perspective. Our analysis encompasses several LMMs, includingLLaVA-v1.5-Vicuna-7B, LLaVA-v1.5-Vicuna-13B, LLaVA-v1.6-Mistral-7B, and LLaVA-NeXT-Vicuna-7B (Liu et al., 2023b; 2024b), to identify common characteristics across models of varying sizes and training datasets. Appendix A gives the formal expression of the preliminary analyses.\nVision Tokens are More Important in Early Layers To find out which layers in LMM the visiontokens play a more important role, we measure the attention weights assigned to different token"}, {"title": "LLAVA-MINI", "content": "We introduce LLaVA-Mini, an efficient large multimodal model with minimal vision tokens. Like previous work, LLaVA-Mini uses a vision encoder to encode an image into several vision tokens. To enhance the efficiency, LLaVA-Mini significantly reduces the number of vision tokens fed into LLM backbone through a compression module. To retain visual information during compression, based on previous findings that vision tokens play a crucial role in the early layers for fusing visual information, LLaVA-Mini introduces a modality pre-fusion module before the LLM backbone to fuse the visual information into the text tokens. The details of LLaVA-Mini are as follows."}, {"title": "ARCHITECTURE", "content": "The architecture of LLaVA-Mini is illustrated in Figure 6. For the visual inputs X, a pre-trained CLIP vision encoder (Radford et al., 2021) is employed to extract visual features from each image. These features are then mapped into the word embedding space via a projection layer, producing vision tokens H\u2228 \u2208 RN\u00b2\u00d7dn, where N2 is the number of vision tokens and dh is the LLM's embed-ding dimension. For the language instruction X\u00ba, LLM's embedding layer is used to generate text token representations Hq \u2208 Rlq\u00d7dn, where lq is the number of text tokens.\nVision Token Compression To enhance the efficiency of LMMs, LLaVA-Mini reduces the num-ber of vision tokens fed into the LLM backbone by utilizing a query-based compression module. To learn compression of the vision tokens, LLaVA-Mini introduces C \u00d7 C learnable compression queries Q. These queries interact with all vision tokens HV through cross-attention (Li et al., 2023a), selectively extracting the important visual information to produce C \u00d7 C compressed vision tokens \u0124' \u2208 RC\u00b2\u00d7dn. To preserve the spatial information in the image during compression, we introduce a 2D sinusoidal positional encoding PE(\u00b7) (He et al., 2021) on the learnable queries and original vision tokens. Formally, the compression can be expressed as:\n\u0124 = A \u2022 H', where A = Softmax ((Q\" + PE(Q')) \u00b7 (H' + PE(H')), (2)\nwhere A \u2208 RC2\u00d7N\u00b2 is the similarity and \u0124' are C \u00d7 C compressed vision tokens.\nModality Pre-fusion The compression of vision tokens inevitably results in some loss of visual information. To retain as much visual information during compression as possible, LLaVA-Mini introduces a modality pre-fusion before the LLM backbone, enabling text tokens to fuse relevant visual information from all vision tokens in advance. Based on our previous observations, where this fusion stage occurs implicitly within the early layers of the LLM, the modality pre-fusion module f(.) consists of Nfusion Transformer blocks (Vaswani et al., 2017), where each Transformer block shares the same structure and hyperparameters with LLM backbone. Vision tokens H and text\""}, {"title": "HIGH-RESOLUTION IMAGE AND VIDEO MODELING", "content": "LLaVA-Mini uses minimal vision tokens to represent visual information, making it possible to han-dle high-resolution images and videos much more efficiently.\nHigh-Resolution Image The resolution of LMM is typically determined by the vision encoder, such as CLIP's ViT-L, which encodes at a resolution of 336*336 pixels. To perceive images at a higher resolution, we divide each image into four sub-images by splitting it horizontally and verti-cally into two parts (Liu et al., 2024b). Each of these sub-images is processed by the vision encoder and projection individually, yielding N2 \u00d7 4 vision tokens with a high resolution of 672*672 pixels. The proposed compression module is then employed to reduce these N2 \u00d7 4 vision tokens into C2 compressed vision tokens \u0124'. The modality pre-fusion module takes the 4 sub-images (N2 \u00d7 4 vision tokens), the original image (N\u00b2 vision tokens), and the language instruction (lq text tokens) as inputs, and then generates la fusion tokens \u0124q with richer global and local visual information. Finally, the number of tokens input to the LLM is C\u00b2 + lq. Note that when handling high-resolution images, C is set slightly higher than in standard-resolution settings to preserve more details.\nVideo When handling videos, LMMs often extract multiple frames from the video (Li et al., 2023b), which incurs significant computational costs. For instance, in the case of LLaVA-v1.5, ex-tracting frames at a rate of 1 frame per second (fps) from an 8-second video results in 576 \u00d7 8 = 4608 vision tokens, leading to substantial VRAM usage. LLaVA-Mini can represent each image with min-imal vision tokens, providing a significant advantage in processing long videos. For a video consist-ing of M frames, LLaVA-Mini processes each frame individually, generating C\u00b2 vision tokens and lq fusion tokens per frame. C2 vision tokens from each of M frames are sequentially concatenated to yield a total of M \u00d7 C2 vision tokens, i.e., \u0124V. Then, la fusion tokens corresponding to M frames are aggregated through pooling operation to generate the video's fusion tokens \u0124\u00ba. As a result, the number of tokens fed to the LLM is reduced from MN2 + lq to MC2 + lq for a video of M frames."}, {"title": "TRAINING", "content": "LLaVA-Mini follows the same training process as LLaVA, consisting of two stages.\nStage 1: Vision-Language Pretraining In this stage, compression and modality pre-fusion mod-ules are not yet applied (i.e., the N\u00b2 vision tokens remain unchanged). LLaVA-Mini learns to align vision and language representations using visual caption data. The training focuses solely on the projection module while the vision encoder and LLM remain frozen (Liu et al., 2023b).\nStage 2: Instruction Tuning In this stage, LLaVA-Mini is trained to perform various visual tasks based on minimal vision tokens, using instruction data. Compression and modality pre-fusion are introduced to LLaVA-Mini, and all modules except the frozen vision encoder (i.e., the projection, compression, modality pre-fusion, and LLM backbone) are trained in an end-to-end manner."}, {"title": "EXPERIMENTS", "content": "EXPERIMENTAL SETTING\nBenchmarks We evaluate LLaVA-Mini on image and video understanding tasks. Experiments are conducted on 11 image benchmarks and 7 video benchmarks. Refer to Appendix C for details.\nBaselines LLaVA-Mini is an image/video LMM, so we compare it with several advanced image-based and video-based LMMs. Detailed description of baselines refer to Appendix D."}, {"title": "MAIN RESULTS", "content": "Image-based Evaluation We compare LLaVA-Mini with LLaVA-v1.5 across 11 benchmarks to thoroughly assess the performance of LLaVA-Mini with minimal vision tokens. The results are"}, {"title": "EFFICIENCY", "content": "With the performance comparable to LLaVA-v1.5, we further explore the computational efficiency offered by LLaVA-Mini. Figures 7, 8, 9 illustrate the advantages of LLaVA-Mini in terms of com-putational load, inference latency, and memory usage, where FLOPs are calculated by calflops (Ye, 2023), and latency is tested on the A100 without any engineering acceleration techniques.\nFLOPs and Inference Latency As shown in Figure 7, LLaVA-Mini significantly reduces com-putational load by 77% compared to LLaVA-v1.5, achieving a speedup of 2.9 times. LLaVA-Mini achieves response latency lower than 40 ms, which is crucial for developing low-latency real-time LMMs. As shown in Figure 8, when modeling at high resolutions, the efficiency advantages of LLaVA-Mini are even more pronounced, yielding 82% FLOPs reduction and 3.76 times speedup.\nMemory Usage Memory consumption poses another challenge for LMMs, particularly in video processing. Figure 9 demonstrates the memory requirements of LMMs when processing videos of varying lengths. In previous methods, each image requires approximately 200-358 MB memory (Liu et al., 2023b; Lin et al., 2023a), limiting them to handle only about 100 frames on a 40GB GPU. In contrast, LLaVA-Mini with one vision token requires just 0.6 MB per image, enabling it to theoretically support video processing of over 10,000 frames on RTX 3090 with 24 GB of memory."}, {"title": "ANALYSES", "content": "SUPERIORITY OF MODALITY PRE-FUSION\nThe proposed modality pre-fusion is central to LLaVA-Mini, as it integrates visual information into text tokens in advance, facilitating extreme compression of vision tokens. To investigate the ef-fects of modality pre-fusion, we conduct an ablation study in Table 6. Without pre-fusion, token compression results in a performance drop of around 5%, even with 144 vision tokens retained, the performance of LMMs falls short of LLaVA-v1.5. This also explains why previous token merging methods often exhibit poor performance (Ye et al., 2024c) or can only achieve a compression rate of over 40% (Shang et al., 2024). Notably, under the same FLOPs, increasing the number of pre-fusion layers yields greater benefits than increasing the number of compression vision tokens. This sup-"}, {"title": "EFFECT OF COMPRESSION", "content": "LLaVA-Mini employs query-based compression to achieve a high compression ratio for vision tokens. We compare the performance of query-based compression with direct average pooling in Table 8. Query-based compression can adaptively capture important features in the image while requiring only a minimal additional computational cost, demonstrating a significant advan-"}, {"title": "PERFORMANCE WITH VARIOUS VISION TOKENS", "content": "LLaVA-Mini uses 1 vision token for standard images and 64 for high-resolution images. We explore the potential of LLaVA-Mini when further increasing the number of vision tokens (larger C) in Table 7. The results indicate that as the number of vision tokens increases, LLaVA-Mini continues to improve in performance. In particular, LLaVA-Mini outperforms LLaVA-v1.5 when both using 576 vision tokens, demonstrating its effectiveness when computational resources are plentiful."}, {"title": "CASE STUDY", "content": "Figures 10 and 11 present examples of LLaVA-Mini in image and video understanding tasks (refer to Appendix G for more cases). Despite using only one vision token, LLaVA-Mini performs effec-tively in capturing visual details, such as accurately identifying price information (OCR) in website screenshots. For video understanding, Video-LLaVA extracts 8 frames per video, regardless of video duration (Lin et al., 2023a). Training on only 8 frames (sometimes missing key frames) can cause hallucinations (Khattak et al., 2024), encouraging LMM to forge information beyond the extracted frames. For instance, given a celebration scene, Video-LLaVA mistakenly imagines \u201ca group of men playing soccer on a field\u201d before the celebration. This fixed-length frame extraction is a forced com-promise due to the large number of vision tokens required per image while LLM's context length is limited. In contrast, LLaVA-Mini, utilizing just one vision token per frame, can process videos at 1 fps, resulting in more robust video understanding. Overall, LLaVA-Mini ensures strong visual understanding while enhancing efficiency, making it a practical solution for multimodal interaction."}, {"title": "CONCLUSION", "content": "We introduce LLaVA-Mini, an efficient LMM with minimal vision tokens. LLaVA-Mini excels in image and video understanding while exhibiting superiority in computational efficiency, inference latency, and memory usage, facilitating the real-time multimodal interaction with efficient LMMs."}, {"title": "DETAILED SETTING OF PRELIMINARY ANALYSES", "content": "In Sec.3.2, we analyze the importance of visual tokens in LMMs from an attention-based perspective to inform strategies for compressing vision tokens. Here, we give a detailed introduction of the experimental setup for the attention analysis.\nWe focus on the LLaVA series architecture, where the input tokens to the LLM are composed of instruction tokens, vision tokens, and response tokens, as shown in Eq.(1). We compute the average attention received by each type of token to reveal how the importance of different token categories changes across layers.\nCalculation of Attention Weights Formally, we denote the attention of the ith token hi to the jth token hj as aij, where aij is the average attention across all attention heads. All tokens fed to the LLM are divided into instruction tokens, vision tokens, and response tokens according to inputs type, denoted as sets Tinstruction, Tvision, and Tresponse respectively. Finally, denoted the target and source token types as tgt_type, src_type \u2208 {instruction, vision, response}, the average attention weights from tgt_type type tokens to src_type type tokens in our analyses are calculated as:\nAttn(tgt-type \u2192 src_type) =\n\u03a3hi\u2208Tigt-type \u03a3h:Tigt-type \u03a3hj\u2208Tsrc-type aij > 0\n hjETsrc.type aij (4)\n\u03a3hj\u2208Tsrc-type aij >0\n1\n0\nif hjETsrc.type aij > 0\notherwise (5)\nSpecifically, Shi\u2208Tigt-type h; ETsrc-type aij calculates the sum of attention weights from all tgt_type type tokens to all src-type type tokens, ShieTigt type 12h; ETsrc.type @ij>0 counts the number of tgt_type type tokens, thus Attn(tgt_type \u2192 src_type) represents the average attention weight from tgt_type type tokens to src_type type tokens. Attn(tgt_type \u2192 src_type) is consistent with the legend in Figure 2.\nCalculation of Attention Entropy The calculation of attention entropy is similar to that of atten-tion weights, with the key difference being the addition of a normalization step. When computing the entropy of a specific type of token (e.g., vision tokens), the sum of attention weights for this token type may not equal 1. Thus, we perform a normalization on the attention of these tokens (e.g., vision tokens) to ensure the definition of entropy is satisfied.\nIn practice, for LLaVA-v1.5 (pad) (Liu et al., 2023b) and LLaVA-NeXT (anyres) (Liu et al., 2024b), which may involve different resolution vision inputs, we use their original settings. In our analysis, we do not further distinguish between different types of vision tokens (e.g., global or local), but treat them collectively as vision tokens."}, {"title": "TRAINING DETAILS", "content": "Implementation Details The compression method of LLaVA-Mini can be easily plugged into ex-isting multi-modal pipelines, as it only requires the addition of two extra modules (the compression module and the modality pre-fusion module) before the LLM, while the other components (such as the vision encoder, the LLM, and the training loss) remain unchanged. The pre-fusion module applies the same decoder-only architecture as the LLM, including both the structure and hyper-parameters. The motivation behind this setting is to ensure flexible adaptation to existing LLM frameworks and other acceleration techniques.\nTraining The overall training process follows a two-stage paradigm similar to LLaVA, consisting of vision-language pretraining followed by instruction tuning. Table 9 reports the two-stage training details of LLaVA-Mini."}, {"title": "BENCHMARKS", "content": "We conduct a comprehensive evaluation of LLaVA-Mini, including both image and video under-standing benchmarks."}, {"title": "INTRODUCTION TO BASELINES", "content": "LLaVA-Mini is an image and video LMM, so we compare it with several advanced image-based and video-based LMMs."}, {"title": "IMAGE-BASED LMMS", "content": "We compare LLaVA-Mini with LLaVA-v1.5 (Liu et al., 2023b) and other advanced LMMs of sim-ilar data and model scales, including BLIP-2 (Li et al., 2023a), InstructBLIP (Liu et al., 2024a), IDEFICS (Lauren\u00e7on et al., 2023), Qwen-VL (Bai et al., 2023), Qwen-VL-Chat (Bai et al., 2023), SPHINX (Lin et al., 2023b), mPLUG-Owl2 (Ye et al., 2024b)."}, {"title": "LMMs with Fewer Vision Tokens", "content": "Additionally, we assess LLaVA-Mini against various efficient LMMs that utilize fewer vision tokens, showing advantages in compression rate and performance. Most of these models share the same architecture and training data as LLaVA, primarily focusing on the merging of vision tokens in the vision encoder. These efficient LMMs are introduced as follows.\nMQT-LLaVA (Hu et al., 2024) introduces a flexible query transformer that allows encoding an image into a variable number of visual tokens (up to a predefined maximum) to adapt to different tasks and computational resources.\nPruMerge (Shang et al., 2024) reduces visual tokens in LMMs by identifying and merging im-portant tokens based on the attention sparsity in vision encoder. PruMerge has a variant, named PruMerge++, which enhances the original PruMerge method by evenly adding more vision tokens (about 144 vision tokens) to further improve performance.\nLLAMA-VID (Li et al., 2023b) LLaMA-VID compresses the instruction and image into one token respectively, with a total of two tokens representing each image, thus facilitating the understanding of longer videos.\nVoCo-LLAMA (Ye et al., 2024c) compresses all vision tokens using language models, significantly improving computational efficiency.\nTokenPacker (Li et al., 2024e) is a visual projector that efficiently reduces visual tokens by 80% using a coarse-to-fine approach.\nPrevious methods have often focused on reducing the number of vision tokens output by the vision encoder. LLaVA-Mini takes this a step further by shifting attention to how vision tokens and text tokens interact within the LLM backbone. Based on this insight, we propose modality pre-fusion, which enables better performance even under the extreme compression of reducing vision tokens to just one token."}, {"title": "VIDEO-BASED LMMS", "content": "LLaVA-Mini can also perform high-quality video understanding, so we compare LLaVA-Mini with the current advanced video LMMs, including LLaMA-Adaptor (Zhang et al., 2024), InternVideo (Wang et al., 2022), VideoChat (Li et al., 2024c), Video-LLaMA (Zhang et al., 2023), mPLUG-Owl (Ye et al., 2024a), Video-ChatGPT (Maaz et al., 2024), BT-Adapor (Liu et al., 2023c), LLaMA-VID (Li et al., 2023b), and Video-LLaVA (Lin et al., 2023a).\nWe also compare LLaVA-Mini with several video LMMs specifically designed for long videos, including MovieChat (Song et al., 2024a), Movie-LLM (Song et al., 2024b), TimeChat (Ren et al., 2023), MA-LMM (He et al., 2024). Note that among these video LMMs, LLaVA-Mini and Video-LLaVA can complete image and video understanding with a unified model."}, {"title": "EXTENDED EXPERIMENTAL RESULTS", "content": "EFFECT OF COMPRESSION MODULE\nTo verify the effectiveness of the compression module, we compared the compression module in LLaVA-Mini with previous advanced token merging methods. To ensure a fair comparison of"}, {"title": "DETAILED RESULTS ON MVBENCH", "content": "Table 15 reports the detailed results on each subset of MVBench, corresponding to Table 3."}]}