{"title": "Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal Nuances", "authors": ["Zehui Wu", "Ziwei Gong", "Lin Ai", "Pengyuan Shi", "Kaan Donbekci", "Julia Hirschberg"], "abstract": "This paper introduces a novel approach to emotion detection in speech using Large Language Models (LLMs). We address the limitation of LLMs in processing audio inputs by translating speech characteristics into natural language descriptions. Our method integrates these descriptions into text prompts, enabling LLMs to perform multimodal emotion analysis without architectural modifications. We evaluate our approach on two datasets: IEMOCAP and MELD, demonstrating significant improvements in emotion recognition accuracy, particularly for high-quality audio data. Our experiments show that incorporating speech descriptions yields a 2 percentage point increase in weighted F1 score on IEMOCAP (from 70.111% to 72.596%). We also compare various LLM architectures and explore the effectiveness of different feature representations. Our findings highlight the potential of this approach in enhancing emotion detection capabilities of LLMs and underscore the importance of audio quality in speech-based emotion recognition tasks. We'll release the source code on Github.", "sections": [{"title": "1 Introduction", "content": "Emotion detection in speech is a crucial component of human-computer interaction, with applications ranging from customer service to mental health monitoring. While Large Language Models (LLMs) have shown remarkable capabilities in processing and understanding text, they lack the ability to directly process audio inputs. This limitation has hindered their potential in multimodal emotion recognition tasks that involve both textual and audio data.\nOur research addresses this challenge by introducing a novel approach that enables LLMs to leverage speech characteristics for emotion detection without requiring architectural modifications. We propose a method that translates audio features into natural language descriptions, which can be seamlessly integrated into text prompts for LLMs. This approach bridges the gap between audio and text modalities, allowing LLMs to perform multimodal emotion analysis.\nIn this paper, we evaluate our method using two widely recognized datasets in the field of emotion recognition: IEMOCAP (Busso et al., 2008) and MELD (Poria et al., 2019). We explore the effectiveness of various feature representations and compare the performance of different LLM architectures. Our study aims to demonstrate the potential of this approach to enhance the emotion detection capabilities of LLMs and to provide insights into the factors that influence its success.\nBy enabling LLMs to consider both textual and audio-derived information, we open up new possibilities for more nuanced and accurate emotion recognition systems. This research not only contributes to the field of affective computing but also provides a framework for incorporating non-textual information into language-model-based systems, with potential implications for a wide range of multimodal AI applications."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 LLM for emotion recognition", "content": "Recent advancements in Emotion Recognition in Conversation (ERC) have leveraged Large Language Models (LLMs) to enhance performance and generalizability. Our work builds upon these developments, particularly the InstructERC framework (Lei et al., 2024), which reformulated ERC as a generative task using LLMs. InstructERC introduced a retrieval template module and additional emotion alignment tasks, achieving state-of-the-art results on common ERC datasets. We extend this approach by incorporating speech descriptions into the template, aiming to capture crucial paralinguistic information."}, {"title": "2.2 Speech Incorporation in LLM", "content": "The integration of speech features into LLM-based systems for emotion recognition and related tasks has been explored in various recent works, each offering unique approaches and insights. SECap (Xu et al., 2023) introduces a framework that uses LLaMA as a text decoder to generate coherent speech emotion captions instead of labels. While SECap demonstrates the potential of LLMs in describing speech emotions, it differs from our approach by requiring an additional audio encoder (HuBERT) and a bridging network (Q-Former) to process speech inputs. Similarly, (Zhang et al., 2024) proposes an approach to integrate speech into LLMs for depression detection by adding acoustic landmark tokens to the input prompt. This method shares our goal of incorporating speech information directly into LLM inputs but necessitates additional parameters and pre-training steps to introduce audio information.\nLanSER (Gong et al., 2023) takes a different approach by leveraging LLMs to infer weak emotion labels from speech transcripts, enabling the use of unlabeled data in SER tasks. Building upon this concept, (Santoso et al., 2024) explores using LLMs for emotional speech annotation by incorporating both conversation context and textual acoustic feature descriptors. While this approach shares our interest in combining textual and acoustic information in the prompt, it primarily uses LLMs for weak label generation rather than direct emotion recognition training.\nRecent research has also explored the application of LLMs in Automatic Speech Recognition (ASR), demonstrating the versatility of LLMs in speech-processing tasks. Seed-ASR (Bai et al., 2024) introduces an audio-conditioned LLM framework that incorporates continuous speech representations by developing a unique audio encoder. (Baskar et al., 2024) proposes a speech prefix-tuning approach with RNNT loss to optimize speech prefixes for LLMs in ASR tasks. SLAM-ASR (Ma et al., 2024) takes a simpler approach, combining off-the-shelf speech encoders and LLMs with only a trainable linear projector, achieving state-of-the-art performance on the Librispeech benchmark.\nFurther extending the capabilities of LLMs in speech processing, AudioChatLlama (Fathullah et al., 2024) presents an end-to-end model that integrates general-purpose speech abilities into the Llama-2 model. This approach enables the model to handle audio prompts, perform spoken question answering, speech translation, and audio summarization, among other tasks. Similarly, Audio-LLM (Li et al., 2024) introduces a method for encoding audio data into embedded representations that LLMs can comprehend, enabling tasks such as automatic speech recognition, emotion recognition, and music captioning. These advancements highlight the growing trend of integrating comprehensive speech-processing capabilities into LLMs.\nOur work distinguishes itself from these approaches in several key aspects: (a) Direct Integration: We achieve seamless integration without requiring additional neural network components (e.g., audio encoders or projection layers) to process speech inputs, thereby reducing computational complexity. (b) End-to-End Training: We train the LLM directly on combined textual and speech description inputs, avoiding the need for weak label generation and additional training stages. (c) Flexibility and Generalizability: Our method of translating speech features into natural language descriptions allows for easy integration as a plug-in for other approaches and tasks, offering a versatile solution for incorporating speech information into LLM-based systems."}, {"title": "3 Methodology", "content": "Our novel approach incorporates speech characteristic descriptions in natural language into the prompt for LLMs. Previous LLM-based approaches have primarily focused on textual information, neglecting audio signals due to LLMs' inability to process direct audio inputs. This omission results in the loss of crucial emotional cues, as the same sentence can convey different emotions through variations in audio features such as pitch, volume, and intonation. Our method bridges this gap by translating audio signals into natural language descriptions of speech characteristics that are instrumental in emotion detection.\nOur approach offers several advantages in the field of emotion detection. First, it enables multimodal analysis by seamlessly integrating speech characteristics into text prompts, allowing LLMs to process audio-derived information without requiring architectural modifications. This method is highly compatible with existing LLM infrastructures, facilitating easy implementation and integration into current systems. Moreover, by preserving paralinguistic information such as volume, pitch, and speaking rate in our natural language descriptions, we capture crucial emotional cues that are often lost in text-only analysis. This enhanced contextual understanding empowers the LLM to grasp the full spectrum of the utterance, potentially leading to more accurate and nuanced emotion classification."}, {"title": "3.1 Large Language Model Prompt Template", "content": "Our methodology utilizes a comprehensive LLM Prompt Template designed to enhance emotion detection capabilities. This template consists of four key components:\n1. Instruction: This component sets the stage by positioning the LLM as an expert in sentiment and emotion analysis, priming it for the task ahead.\n2. Context: A crucial element that provides the conversational background, allowing the LLM to understand the broader situation and interpersonal dynamics at play. We also add speech features behind the last three utterances to provide additional speech context.\n3. Speech Descriptions: This innovative addition translates audio signals into natural language descriptions of speech characteristics.\n4. Question: This component presents the specific task to the LLM, asking it to select an emotional label for a target utterance from a predefined set of options. Importantly, it explicitly instructs the model to consider both the contextual information and the audio features described.\nThis structured approach ensures that the LLM has access to both textual and audio-derived information, enabling a more nuanced and accurate emotion detection process. By incorporating speech characteristics into the prompt, we allow the LLM to perform multimodal analysis without requiring direct audio input processing capabilities."}, {"title": "3.2 Audio Features", "content": "In our approach to emotion detection, we leverage five intuitive audio features that are easily interpretable by both humans and LLMs. These features capture essential aspects of speech that convey emotional information. volume is represented by two sub-features: average volume, which indicates the overall loudness of speech, and volume variation, which captures dynamic changes in intensity. Pitch, another crucial indicator of emotion, is similarly divided into average pitch and pitch variation, allowing us to detect both the general tone and the modulation of a speaker's voice. The fifth feature, speaking rate, represents the speed of speech, offering insights into the speaker's emotional state and urgency. By focusing on these fundamental yet comprehensive audio characteristics, we ensure that the extracted features are not only rich in emotional content but also readily understandable by LLMs when described in natural language."}, {"title": "3.2.1 Audio Feature Processing", "content": "In our approach to audio feature processing, we transform raw numerical features into categorical representations to enhance interpretability and facilitate natural language descriptions. This process involves four key steps:\na) Threshold Calculation: We compute thresholds for each audio feature (such as average volume, pitch variation, and speaking rate) using quantile-based segmentation. Depending on the desired granularity, we divide the feature space into 3, 4, 5, or 6 classes, with thresholds calculated using appropriate quantiles (e.g., 25th and 75th percentiles for 3 classes, 10th, 25th, 75th, and 90th percentiles for 5 classes).\nb) Speaker-Specific Normalization: Recognizing the individual differences in speech patterns, we calculate speaker-specific thresholds, falling back to overall thresholds for less frequent speakers. This approach allows for a more accurate categorization of speech features relative to each speaker's typical patterns.\nc) Categorization: Each numerical feature is then categorized based on these thresholds. For instance, in a 5-class system, a feature value might be classified as 'very low', 'low', 'medium', 'high', or 'very high'. This categorization provides a more intuitive representation of the audio features, bridging the gap between numerical values and human-interpretable descriptions.\nd) Feature-Specific Descriptions: For each key audio feature (volume, pitch, speaking rate), we create descriptive phrases based on their categorical values. For example, \"high volume with moderate variation\" or \"low pitch with high variation\"."}, {"title": "3.2.2 Audio Impression", "content": "To make the audio features more accessible and meaningful for both human analysis and LLM processing, we generate natural language impressions based on the categorized features. Going beyond mere description, we generate interpretative impressions that suggest potential emotional or psychological states associated with the observed speech patterns. For instance, \"speaking loudly with significant volume changes\" might be interpreted as indicating \"excitement, confidence, or urgency\".\nTo account for the inherent uncertainty in interpreting speech patterns, we incorporate confidence-adjusting phrases (e.g., \"likely\", \"may\") based on the feature's proximity to category thresholds. This nuanced approach helps prevent overconfident interpretations of borderline cases.\nThe final impression synthesizes information about pitch, volume, and speaking rate into a coherent, flowing sentence. This integration provides a nuanced description of the speaker's vocal characteristics and their potential implications, offering valuable context for emotion detection tasks."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset", "content": "For our experiments, we utilized two widely recognized datasets in the field of emotion recognition: IEMOCAP and MELD.\nIEMOCAP (Interactive Emotional Dyadic Motion Capture): IEMOCAP consists of dyadic conversations between actors, providing a rich set of emotional expressions in a controlled setting. It contains approximately 12 hours of audiovisual data from 10 actors, split into 5 sessions. Each utterance is annotated with one of nine emotion labels: anger, happiness, excitement, sadness, frustration, fear, surprise, neutral, and other. IEMOCAP is particularly valued for its high-quality audio recordings and motion capture data, making it ideal for multimodal emotion analysis. The dataset includes both scripted and improvised scenarios, offering a balance between controlled emotional expressions and more naturalistic interactions.\nMELD (Multimodal EmotionLines Dataset): MELD is derived from the popular TV series \"Friends,\" making it particularly suitable for analyzing emotions in conversational contexts. It contains 13,708 utterances from 1,433 dialogues, each annotated with one of seven emotion categories: anger, disgust, fear, joy, neutral, sadness, and surprise. MELD is multimodal, providing audio, visual, and textual data for each utterance. The dataset is pre-split into training (9,989 utterances), validation (1,109 utterances), and test (2,610 utterances) sets. Its multi-party conversational nature and situational diversity make it an excellent resource for evaluating emotion detection in complex, realistic scenarios.\nFor our experiments, we focused on the audio modality from both datasets, extracting and processing acoustic features as outlined in our methodology. In the case of the MELD dataset, we implemented speaker-based standardization of audio features to account for individual vocal characteristics. For the IEMOCAP dataset, we applied gender-based standardization, recognizing the physiological differences that can impact vocal features. Additionally, for IEMOCAP, we confined our analysis to six emotion categories (anger, happiness, excitement, sadness, frustration, and neutral), aligning with common practices in the field.\nOur utilization of both MELD and IEMOCAP datasets was strategically designed to evaluate our approach across diverse audio recording environments. MELD, with its origin in television shows, presents more chaotic and naturalistic scenarios, while IEMOCAP offers a controlled, studio-based environment. This dual-dataset approach enables a comprehensive assessment of our emotion detection method across varied recording conditions."}, {"title": "4.2 Primary Results", "content": "Our results from Table 1 demonstrate that incorporating speech descriptions into the text input yields performance improvements across both datasets, albeit to varying degrees. For the IEMOCAP dataset, we observe a substantial improvement of nearly 2 percentage points when adding speech descriptions to the text input (from 70.111% to 72.021%). By providing additional speech features for context, the model improves to 72.596%. This significant gain underscores the value of integrating acoustic information in emotion recognition tasks, particularly for high-quality audio recordings.\nIn contrast, the MELD dataset shows a more modest improvement with the addition of speech impression, increasing from 67.44% to 67.604%. This disparity in performance gain between the two datasets can be attributed to the differences in audio quality and recording conditions. IEMOCAP, recorded in a controlled studio environment, provides clean and consistent audio, allowing for more accurate extraction of speech features. On the other hand, MELD, derived from the TV show \"Friends,\" presents more challenging audio conditions. The presence of background laughter, varying audio quality, overlapping speakers, and short utterance durations (averaging 2-3 seconds) in MELD likely contribute to less reliable speech feature extraction, thereby limiting the potential benefits of speech descriptions.\nInterestingly, the use of speech impressions does not yield further improvements over speech descriptions for either dataset. For IEMOCAP, the performance (71.542%) falls between that of text-only and text with speech descriptions, while for MELD, it performs similarly to the text-only baseline (67.023%). This suggests that our hard-coded impressions, despite being more interpretative, may introduce noise or inaccuracies that the model struggles to leverage effectively. The lack of improvement with speech impressions indicates that allowing the model to learn directly from more objective speech descriptions may be a more robust approach.\nThese findings highlight the potential of integrating speech characteristics into text-based emotion recognition models, particularly when working with high-quality audio data. However, they also underscore the challenges posed by real-world audio conditions and the need for careful consideration when designing multimodal emotion recognition systems."}, {"title": "4.3 Model Comparison", "content": "To evaluate the effectiveness of our approach across different language models, we conducted experiments using various popular open-sourced LLMs, including LLaMA-2, LLaMA-3, and Phi-3, and a powerful closed-sourced model, Claude Sonnet 3.5. Table 2 presents the emotion recognition accuracy on the IEMOCAP dataset for these models under different input conditions: text-only, text with speech descriptions, and text with speech impressions.\nOur results demonstrate a consistent improvement in performance when incorporating either speech descriptions or impressions across all models. This underscores the value of integrating audio-derived features in emotion recognition tasks, regardless of the underlying language model. For most models, speech descriptions yield slightly better results than speech impressions, although the difference is often marginal. This suggests that more objective, feature-based descriptions may be more reliable for emotion detection tasks.\nInterestingly, increasing model size does not necessarily lead to improved performance in this task. Despite their stronger performance on public benchmarks, larger models like LLaMA-3 and Phi-3 do not consistently outperform the smaller LLaMA-2 model when fine-tuned for emotion detection. In fact, Phi-3, despite being the largest model tested (13B parameters), performs worse than both LLaMA-2 and LLaMA-3 across all input conditions. This indicates that model size alone is not a determining factor for success in this specific task. While LLaMA-3-8b-instruct achieves the highest accuracy (72.098%) among all tested models when provided with speech descriptions, the improvement over LLaMA-2-7b-base (72.021%) is minimal. Additionally, we observe that using the instruct or base version of these models does not result in significant performance differences for this task. These findings suggest that while there may be some benefit to the architectural improvements in newer models, the gains are not as significant as one might expect given the increase in model size and the advancements claimed in public benchmarks. Certain features of the LLaMA architecture may be particularly well-suited for emotion recognition tasks, but the task-specific nature of emotion detection may limit the impact of general language model improvements.\nThe substantial performance gap between the zero-shot Claude model and the fine-tuned models highlights the importance of task-specific fine-tuning for emotion recognition. However, it is noteworthy that even in a zero-shot setting, Claude benefits from the addition of speech features, further emphasizing the value of multimodal inputs in emotion detection.\nThese findings emphasize the complexity of model selection for specific tasks like emotion recognition. While larger, more recent models often excel in general language understanding tasks,"}, {"title": "4.4 Evaluation of Speech Features Using Large Language Model (LLM)", "content": "our results show that task-specific fine-tuning and the integration of multimodal features can sometimes yield superior results with smaller, specialized models. This underscores the importance of empirical testing and careful model selection in developing effective emotion recognition systems.\nThe results in Table 4 demonstrate the potential of LLMs in leveraging speech features for emotion classification. As a reference point, recall that the 5-class categorical representation with ML models achieved a weighted F1 score of 31.333%, significantly outperforming the random guess baseline of 16.67%. When using LLMs with natural language descriptions of speech characteristics, we observe a slight decrease in performance compared to the ML approach. The 5-class categorical representation with LLMs yields a weighted F1 score of 27.895%, while the 3-class representation achieves 27.602%.\nThis performance gap between ML models and LLMs can be attributed to the inherent differences in how these models process information. ML models, designed specifically for discriminative tasks, can directly leverage the structured, one-hot encoded categorical features. In contrast, LLMs receive the information as natural language descriptions, which introduces a layer of abstraction and potential for ambiguity. Despite this, the LLM's performance remains substantially above the random guess baseline, indicating that it successfully extracts meaningful emotional cues from the speech descriptions.\nThe relatively small difference between 3-class and 5-class representations in LLM performance (27.602% vs. 27.895%) suggests that increasing the granularity of speech feature categorization does not significantly impact the LLM's ability to discern emotions. This could imply that LLMs are adept at interpreting broader emotional cues rather than fine-grained acoustic details.\nNotably, we also examined the performance of LLMs using only speech impressions, which are derived from the 5-class categorical speech descriptions but provide a more interpretative representation of the audio features. This approach yields a weighted F1 score of 27.794%, slightly lower than 5-class categorical descriptions. This finding aligns with our earlier results when combining speech features with textual information, where speech impressions also showed a marginal decrease in performance compared to direct speech descriptions. The consistency of this pattern across different experimental setups suggests that, while speech impressions offer a more human-interpretable format, they may introduce a level of abstraction or subjectivity that slightly reduces the model's ability to accurately classify emotions. This underscores the delicate balance between feature interpretability and classification performance in emotion recognition tasks.\nWhile there is some loss of information when transitioning from raw categorical features to natural language descriptions, the LLM's ability to"}, {"title": "5 Conclusion", "content": "Our study demonstrates that integrating speech characteristics into LLM-based emotion detection systems significantly improves performance, especially for high-quality audio data. By translating audio features into natural language descriptions, we enable LLMs to perform multimodal emotion analysis without architectural changes.\nKey findings show that incorporating speech descriptions enhances emotion recognition accuracy across different LLM architectures. However, the effectiveness of this approach heavily depends on audio quality, as evidenced by the contrasting results between IEMOCAP and MELD datasets.\nThis research opens up new possibilities for developing more flexible and interpretable emotion recognition systems that seamlessly integrate textual and acoustic information. Future work should focus on improving feature extraction methods for noisy, real-world audio data and exploring more sophisticated ways of representing speech characteristics in natural language.\nOur approach not only enhances the emotion detection capabilities of LLMs but also provides a framework for incorporating non-textual information into language model-based systems, with potential implications for a wide range of multimodal AI applications."}, {"title": "6 Limitation", "content": "Our approach to enhancing LLM-based emotion detection shows promise, but it is important to acknowledge its limitations. A significant constraint is the method's heavy reliance on audio quality. As evidenced by the contrasting results between the IEMOCAP and MELD datasets, our system's performance degrades considerably with noisy or low-quality audio input.\nAnother limitation lies in the current feature extraction process. Our approach utilizes a limited set of audio features, which may not capture the full spectrum of emotional nuances present in speech. More complex emotional cues embedded in prosody, rhythm, or spectral characteristics could be overlooked, potentially limiting the depth and accuracy of our emotional analysis. This simplification of audio features may result in a loss of subtle emotional information that humans can easily discern.\nThe nature of our training datasets presents another challenge. Our study primarily relies on acted (IEMOCAP) or scripted (MELD) emotional expressions, which may not fully represent the complexity and subtlety of emotions in natural, spontaneous speech. This dataset bias could limit the generalizability of our findings to real-world scenarios where emotions are often more nuanced, mixed, or ambiguously expressed. The gap between performed emotions in our datasets and the diverse, spontaneous emotional expressions in everyday life may impact the real-world effectiveness of our emotion recognition system.\nLastly, the computational resources required for this approach pose a practical limitation. Fine-tuning large language models demands significant computational power, which may restrict the implementation of this method in resource-constrained environments. This requirement could limit the accessibility and widespread adoption of our approach, particularly in settings where high-performance computing resources are not readily available.\nAddressing these limitations in future research will be crucial for developing more robust, versatile, and widely applicable emotion recognition systems that can handle the complexities of real-world emotional expressions across various audio conditions and computational environments."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Speech features for context", "content": ""}, {"title": "A.2 Evaluation of Speech Features Using Simple Machine Learning Models", "content": "To evaluate the effectiveness of our extracted speech features in capturing emotion-related information, we conducted experiments using various machine learning models on both numerical and categorical representations of these features. Table 5 and Table 6 presents the performance of Random Forest (RF), Support Vector Machine (SVM), and Multi-Layer Perceptron (MLP) models on the IEMOCAP and MELD datasets.\nOur analysis of the IEMOCAP dataset reveals that the simple five speech features contain substantial emotion-related information. Both weighted F1 and macro F1 scores (approximately 0.32 and 0.30, respectively) are nearly double the random guess baseline of 0.167. This significant improvement over the baseline indicates that our extracted speech features are effectively capturing emotional cues. Interestingly, the conversion of numerical features into categorical features does not result in a substantial loss of emotion-related information. Moreover, dividing the numerical features into five classes yields better performance than other granularity levels, with weighted F1 and macro F1 scores of 0.313 and 0.293, respectively. This suggests that a moderate level of discretization can reserve clear emotional patterns in speech features.\nThe results for the MELD dataset highlight one crucial aspects of speech-based emotion recognition: the importance of audio quality. While the weighted F1 scores (around 0.36) are higher than the random guess baseline of 0.143, the macro F1 scores (approximately 0.15) are only marginally better than the baseline. This discrepancy is crucial to understand: MELD is highly imbalanced, with neutral emotions comprising about 50% of the dataset. The high-weighted F1 score is misleading as it primarily reflects the model's ability to predict the dominant neutral class, rather than its capacity to discern emotions from speech features. In contrast, the near-baseline macro F1 score, which treats all classes equally, reveals that the extracted speech features provide minimal informative value across emotion categories. This lack of substantial improvement in macro F1 scores indicates that the key factor behind this poor performance is likely the low quality and challenging nature of the audio in MELD. This finding underscores the critical importance of audio quality in obtaining reliable speech features for emotion recognition tasks. It also explains the modest gains observed when incorporating speech descriptions in our earlier language model experiments on MELD, contrasting sharply with the significant improvements seen in IEMOCAP.\nIn conclusion, these results not only validate the informativeness of our extracted speech features for emotion detection but also emphasize the crucial role of audio quality in this process. The stark contrast between IEMOCAP and MELD results highlights that high-quality audio data, like that in IEMOCAP, allows for the extraction of informative speech features that significantly enhance emotion recognition. Conversely, challenging audio conditions, as in MELD, can render speech features unreliable and minimally informative. This underscores the need for robust feature extraction methods or pre-processing techniques when dealing with real-world, noisy audio data. Despite these challenges, the performance of categorical representations for IEMOCAP supports our strategy of using discretized speech descriptions in language model prompts for emotion recognition tasks, providing a foundation for future improvements in handling diverse audio qualities."}]}