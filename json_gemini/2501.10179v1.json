{"title": "A Simple but Effective Closed-form Solution for Extreme Multi-label Learning", "authors": ["Kazuma Onishi", "Katsuhiko Hayashi"], "abstract": "Extreme multi-label learning (XML) is a task of assigning multiple labels from an extremely large set of labels to each data instance. Many current high-performance XML models are composed of a lot of hyperparameters, which complicates the tuning process. Additionally, the models themselves are adapted specifically to XML, which complicates their reimplementation. To remedy this problem, we propose a simple method based on ridge regression for XML. The proposed method not only has a closed-form solution but also is composed of a single hyperparameter. Since there are no precedents on applying ridge regression to XML, this paper verified the performance of the method by using various XML benchmark datasets. Furthermore, we enhanced the prediction of low-frequency labels in XML, which hold informative content. This prediction is essential yet challenging because of the limited amount of data. Here, we employed a simple frequency-based weighting. This approach greatly simplifies the process compared with existing techniques. Experimental results revealed that it can achieve levels of performance comparable to, or even exceeding, those of models with numerous hyperparameters. Additionally, we found that the frequency-based weighting significantly improved the predictive performance for low-frequency labels, while requiring almost no changes in implementation. The source code for the proposed method is available on github at https://github.com/cars1015/XML-ridge.", "sections": [{"title": "1 Introduction", "content": "Extreme multi-label learning (XML) aims to assign multiple labels to each instance, by identifying the most relevant labels from a large label set for any given text or image input. XML is a crucial task encompassing numerous real-world problems in the field of information science. It has been actively pursued since the late 2000s, with over 30 models proposed to date, such as [25,23,4,24].\nHowever, many of the current models that achieve high performance are composed of numerous hyperparameters, which increases their flexibility but also their complexity. As a result, these models suffer from low interpretability, and the tuning of hyperparameters is laborious.\nAdditionally, the models often include specialized adaptations for XML, making their implementation challenging. To tackle these issues, we propose applying ridge regression to XML. The method uses least squares with L2 regularization for model optimization, which can be solved in a closed-form. Moreover, the ridge regression primarily involves a single hyperparameter considering the degree of L2 regularization; thus, the model is simple, highly interpretable, and easy to reimplement.\nOn the other hand, since the label distributions in XML datasets follow a power law distribution, predicting low-frequency labels is challenging due to limited data instances. However, these less common labels may contain more valuable information compared to common, frequent labels. Therefore, prediction of low-frequency labels is a key challenge in XML [10] and there have been various approaches to meeting it, such as re-ranking predictions through classifiers specialized in low frequency labels [10], imbalanced clustering considering label frequency [23] and data augmentation for low-frequency labels [27,21].\nIn this paper, we also propose a method that incorporates label-specific weights in the ridge regression. This approach can be integrated into ridge regression almost effortlessly, i.e., without extensive modifications to the implementation. We evaluated the performance of the proposed method on various XML benchmark datasets. Despite its simplicity, our method matched or exceeded the performance of the existing models in the experiments. Furthermore, by considering label frequency, our method improved the prediction performance for low-frequency labels relative to that of existing models.\nFor clarity, we define the following notation and preliminaries. Vectors are represented by boldface lowercase letters, e.g., a. The i-th element of a vector a is denoted by \\(a_i\\). Matrices are represented by boldface capital letters, e.g., A. The i-th row of a matrix A is denoted by \\(A_{i*}\\), and the j-th column of A is denoted by \\(A_{*j}\\). The element (i,j) of a matrix A is denoted by \\(A_{ij}\\). \\(A^T\\) and \\(A^{-1}\\) denote the transpose and inverse of a matrix A, respectively. \\(I_D\\) denotes the D-dimensional identity matrix."}, {"title": "2 Related Work", "content": "Existing models for XML can be classified into two categories based on the input features they use. One category consists of linear methods that use only statistical features created by bag-of-words or TF-IDF, while the other consists of deep-learning-based methods that employ task-specific feature representations created using pre-trained language models from raw text data.\nLinear methods can mainly be classified into three types. The first type is one-vs-all (OVA) methods, which use separate linear classifiers for each label, as in PDSparse [24], DisMEC [1] and Slice [9]. The second type is embedding-based methods like LEML [26], SLEEC [3], and AnnexML [20], which compress high-dimensional label spaces into lower dimensions for learning. The third type is tree-based methods such as Parabel [16], FastXML [17], and PfastreXML [10], which learn hierarchical tree structures to partition the label space.\nDeep learning methods like XML-CNN [12] use a Convolutional Neural Network (CNN) to obtain text representations, and AttentionXML [25] employs bidirectional long short-term memory (BiLSTMs) and an attention mechanism. Recently, pre-trained language models such as BERT [5], XLNet [22] and RoBERTa [13] have been used in XML. X-Transformer [4] pioneered the use of fine-tuning for language models with label clusters by combining these embeddings with TF-IDF for classification. XR-Transformer [28] significantly reduces the cost of fine-tuning by using a hierarchical tree structure and combines embeddings with TF-IDF for a tree-based classifier. APLC-XLNet [23] conducts cluster division considering the label frequency distribution and fine-tunes XLNet accordingly.\nRegression-based models for large-scale data like XML have also been used in the recommendation field. Positioned as item-based collaborative filtering methods, these models acquire item similarities based on regression. For example, SLIM [15], minimizes the squared error under the constraints of L1 and L2 regularization, non-negativity, and zero-diagonal constraints, and EASE [19], removes L1 regularization and non-negativity constraints from SLIM. In particular, EASE has a closed-form solution. Such regression-based models have matched or even outperformed deep-learning-based recommendation models. Research such as [14] has shown that EASE inherently has a zero phase component analysis (ZCA) whitening effect, and the study [7] has indicated that its diagonal constraints apply penalties to less influential principal components. EASE is also applied to tasks such as estimating similarity between words [6]."}, {"title": "3 Proposed method", "content": null}, {"title": "3.1 XML-ridge", "content": "The dataset in XML is represented as \\(\\{(x^{(i)}, y^{(i)})\\}_{i=1}^D\\), where D is the total number of instances. \\(x^{(i)} \\in \\mathbb{R}^N\\) denotes an N-dimensional feature vector for each instance i, and \\(y^{(i)} \\in \\{0,1\\}^L\\) is an L-dimensional label vector. For data instance i, if it belongs to a label j, then \\(y_j^{(i)} = 1\\); if not, \\(y_j^{(i)} = 0\\).\nWhen modeling XML tasks using ridge regression, we consider the feature vectors of each data instance to be explanatory variables and their label vectors to be objective variables. For each data instance, the feature vectors arranged as \\((x^{(1)},...,x^{(D)})\\) form a matrix \\(X \\in \\mathbb{R}^{D \\times N}\\). Similarly, the label vectors arranged as \\((y^{(1)},...,y^{(D)})\\) create a matrix \\(Y \\in \\{0,1\\}^{D \\times L}\\). Using these matrices, ridge regression can be defined for the learning problem in an XML task as follows.\n\n\\begin{equation}\nW = \\text{arg} \\min_{W} \\{||Y - XW||_F^2 + \\lambda ||W||_F^2 \\}.\n\\end{equation}\n\nHere, \\(\\lambda\\) is a hyperparameter that controls the degree of L2 regularization. The purpose of Eq. (1) is to learn the feature representation of labels, and by minimizing the squared error with L2 regularization, we obtain a matrix \\(W \\in \\mathbb{R}^{N \\times L}\\), which stores the feature vectors of each label. Eq. (1) has a closed-form solution:\n\n\\begin{equation}\nW = (X^T X + \\lambda I_N)^{-1}X^T Y \\quad \\text{or} \\quad W = X^T(XX^T + \\lambda I_D)^{-1}Y.\n\\end{equation}\n\nSince Eqs. (2) show two equivalent formulations [18], it is possible to compute the inverse of a matrix of dimension \\(\\min(D, N) \\times \\min(D, N)\\).\nGiven the feature vector \\(x \\in \\mathbb{R}^N\\) of an input evaluation data instance, the score \\(s_l\\) for label l is computed using the matrix W by \\(s_l = x^T W_{*l}\\). In XML tasks, it is standard to assess model performance using ranking metrics. For this reason, scores are computed for all labels, and the labels are evaluated in descending order of score."}, {"title": "3.2 Applying Propensity Scores to XML-ridge", "content": "As shown in Figure 1, label frequencies in XML datasets follow a power-law distribution. There are head labels with high frequencies and tail labels with low frequencies. Predicting tail labels is challenging due to limited data. However, unlike the commonly assigned head labels, tail labels often contain more detailed and informative content. Therefore, in XML, accurately predicting tail labels is considered crucial, as they generally provide a greater amount of information.\nWe addressed this challenge of predicting tail labels. Each label \\(y_l\\) in the label vector y is weighted by \\(\\theta_l\\), resulting in \\(\\hat{y}_l = \\theta_l y_l\\). In this study, we used the inverse of the propensity score defined in previous work [10], as the weight.\nLarge-scale datasets like XML, are difficult to annotate manually, and often, relevant labels remain unassigned. As a way of dealing with this problem, Jain et al. [10] define a propensity score \\(p_l\\) for each label l to reduce the bias from these missing labels. Propensity scores indicate the probability of a label being correctly observed. Tail labels generally have lower propensity scores due to a higher likelihood of being overlooked. As a result, the inverse of the propensity score shows higher values for tail labels. It is thought that when it is employed as a weight, the inverse of the propensity score not only mitigates the bias caused by missing labels, but also facilitates the prediction of tail labels. In fact, Jain et al. [10] empirically showed, based on data, that the inverse of the propensity score effectively serves as a weight. The weight is defined as follows:\n\n\\begin{equation}\n\\theta_l = 1/p_l = 1 + (\\log N - 1)(\\frac{N_l + B}{B + 1})^{-A}\n\\end{equation}\n\nwhere N is the total number of training instances, \\(N_l\\) is the number of training instances for label l, and A and B are unique values tailored to the data's characteristics; and in this study, we employed the values in [10]."}, {"title": "4 Experiment", "content": null}, {"title": "4.1 Experimental Setup", "content": "Datasets and Compared Models We conducted experiments on five XML benchmark datasets: Eurlex-4K, Wiki10-31K, AmazonCat-13K (used in AttentionXML [25]), and Bibtex and Delicious200K (from the Extreme Classification Repository [2]). Each dataset contains hundreds to hundreds of thousands of labels and was split into training and test sets, with hyperparameters tuned using 10% of the training data for validation. We compared our model with several strong baselines, including PfastreXML [10], Parabel [16], SLEEC [3], AttentionXML [25], APLC-XLNet [23], and XR-Transformer [28].\nEvaluation Metrics Our evaluations used Precision@K (P@K) and PSPrecision@K (PSP@K), both commonly used measures in XML.\n\n\\begin{equation}\nP@K = \\frac{1}{K * N_{\\text{test}}} \\sum_{i=1}^{N_{\\text{test}}} \\sum_{j=1}^{K} y_{n(j)}^{(i)} \\quad PSP@K = \\frac{1}{K * N_{\\text{test}}} \\sum_{i=1}^{N_{\\text{test}}} \\sum_{j=1}^{K} \\frac{y_{n(j)}^{(i)}}{p_{n(j)}}\n\\end{equation}\n\nHere, n(j) denotes the label with the j-th highest prediction score, and \\(N_{\\text{test}}\\) denotes the total number of instances in the test dataset, and \\(p_{n(j)}\\) denotes the propensity score for the label n(j). P@K can potentially be artificially enhanced by focusing predictions on more common labels [2]. To counteract this, PSP@K assigns a higher value to the prediction of minor labels.\nModel Configuration We used sparse features (BoW,TF-IDF) and dense features from a fine-tuned language model, following the XR-Transformer [28] approach. To fairly evaluate classifier performance, sparse features were used for Bibtex and Delicious200K, consistent with the features used to evaluate other linear methods. Regarding the deep-learning-based methods, we used concatenated TF-IDF and language model embeddings, as is done in XR-Transformer [28]. However, unlike XR-Transformer, which employs three fine-tuned language models (BERT, ROBERTa, XLNet) for classifier training and creates ensembles of their predictions on Eurlex-4K and AmazonCat-13K, our method evaluates using only concatenated features without an ensemble of classifiers. Additionally, for Delicious200K, we compressed the TF-IDF features to 3,000 dimensions by using singular value decomposition (SVD), and in the case of AmazonCat-13K, reduced the features by using sparse random projection."}, {"title": "4.2 Results", "content": "The top part of Table 1 shows our method's performance across various datasets. Despite its simplicity, our method matched or surpassed the best-performing models on all datasets. Particularly, it outperformed the existing models by around 4% in PSP@5 on Eurlex-4K, whereas on Wiki10-31K, it set new benchmarks in terms of all P@K and improved PSP@5 by around 13%. Additionally, weighting improved predictions for low-frequency labels without significantly harming P@K. On the other hand, the performance decline on Delicious200K is likely due to weighting acting as noise, a consequence of losing crucial low-frequency label information during SVD."}, {"title": "4.3 Analysis", "content": "Matrix Sparsification When assigning scores to each data instance, since \\(W\\in \\mathbb{R}^{N\\times L}\\) stores the feature vectors of each label, a significant number of multiplications with feature vectors \\(x \\in \\mathbb{R}^{N}\\) is required when the number of labels is extreme. To reduce the computational cost, we conducted an experiment to enhance the sparsity of matrix W. The sparsity was improved by discarding elements below a threshold. The bottom part of Table 1 shows that significant reductions in elements do not severely impact performance. For instance, in the Wiki10-31K dataset, performance remains nearly unchanged even when the elements are reduced to as low as 0.8% of the original."}, {"title": "5 Conclusion", "content": "We proposed a simple method based on ridge regression for XML tasks. This method is not only simple, having a closed-form solution and relying on a single hyperparameter; it also shows competitive or superior performance compared with existing models. In particular, it achieved excellent performance in predicting low-frequency labels, due to consideration of the label frequency distribution. In the future, we would like to consider applying this research to tasks such as graph link prediction [8,11]."}]}