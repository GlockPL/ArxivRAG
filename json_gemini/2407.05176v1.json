{"title": "Towards Socially and Environmentally Responsible AI", "authors": ["Pengfei Li", "Yejia Liu", "Jianyi Yang", "Shaolei Ren"], "abstract": "The sharply increasing sizes of artificial intelligence (AI) models come with significant energy consumption and environmental footprints, which can disproportionately impact certain (often marginalized) regions and hence create environmental inequity concerns. Moreover, concerns with social inequity have also emerged, as AI computing resources may not be equitably distributed across the globe and users from certain disadvantaged regions with severe resource constraints can consistently experience inferior model performance. Importantly, the inequity concerns that encompass both social and environmental dimensions still remain unexplored and have increasingly hindered responsible AI. In this paper, we leverage the spatial flexibility of AI inference workloads and propose equitable geographical load balancing (GLB) to fairly balance Al's regional social and environmental costs. Concretely, to penalize the disproportionately high social and environmental costs for equity, we introduce Lq norms as novel regularization terms into the optimization objective for GLB decisions. Our empirical results based on real-world Al inference traces demonstrate that while the existing GLB algorithms result in disproportionately large social and environmental costs in certain regions, our proposed equitable GLB can fairly balance AI's negative social and environmental costs across all the regions.", "sections": [{"title": "Introduction", "content": "In the rapidly evolving field of artificial intelligence (AI), a significant transformation is underway with the emergence of large foundation models as exemplified by Large Language Models (LLMs) like GPTs [4] and Vision Transformers Models (ViTs) [8]. These cutting-edge AI models demonstrate the ability to function effectively in diverse contexts, engaging with extensive vocabularies and image data for unforeseen AI tasks, i.e., zero-shot abilities. To serve inference requests, they are typically deployed across geographically distributed data centers for better service availability, lower transmission latency, and/or privacy regulations.\nEnvironmental inequity. Powerful yet hungry, large AI models require substantial resources not only during training but also in deployment and inference. For some popular Al services such as text and image generation, the total energy consumption for inference can be comparable to or even exceed that for training, resulting in huge carbon emissions and freshwater usage [14, 32]. To curb the growing environment footprint, many recent efforts have been devoted to enhancing the efficiency and reducing the energy consumption of AI models. Example strategies include model compression that reduces Al's computational demand for inference (typically at a sacrifice of model performance) [17, 18] and geographical load balancing (GLB) that leverages spatial heterogeneities to route more workloads to low-cost and/or greener regions [6, 15]. Additionally, on the infrastructure side, there has been a rise in the adoption of carbon-free energy and climate-conscious cooling system designs in the data center industry. For instance, utilizing air-side economizers where climate conditions allow has become increasingly common to cut the direct water consumption [21].\nWhile these approaches can effectively minimize Al's total environmental footprint, the rise of environmental inequity \u2014Al\u2019s negative environmental impact can disproportionately affects certain (often marginalized) regions [2, 15] - has become increasingly worrisome, potentially leading to other unintended social and ecological consequences and widening regional disparities. Importantly, the disproportional distribution of AI's environmental cost across different regions can be amplified by existing approaches to managing Al systems (e.g., load distribution and AI model scaling) that often prioritize the total environmental cost rather than the cost borne by individual regions which are most environmentally vulnerable [15]. Compounded by the sharply growing demand, Al's environmental inequity has received calls for mitigation efforts from various organizations, such as UNESCO [27], Meta [20] and the State of California [5].\nSocial inequity. Going beyond environmental footprints, concerns with Al's social inequity have also emerged [28]. For now, only a few major tech players have the resource and capacity to train and deploy large AI models. Thus, due to the uneven deployment of computing resources across the globe, users from different regions may encounter varying AI model sizes and performances (e.g., larger AI models typically imply better inference performance in terms of the accuracy and task scores), leading to complex societal consequences. For example, studies have indicated that people are becoming increasingly reliant on LLMs for acquiring knowledge, suggesting that subpar LLMs could jeopardize the prospects of these individuals [25]. Thus, Al's potentially unfair model performance has close relevance to its social inequity. Crucially, the existing environmentally conscious approaches to AI system management (e.g., choosing larger Al models with better performances/accuracies when there are more solar energy available) may further reinforce Al's performance unfairness among users from different regions, enlarging the social inequity.\nContributions. With the growing need for AI as a public resource serving the broader society, it becomes increasingly imperative to rectify AI's emerging social and environmental inequities and enable truly responsible AI [20, 24]. In this paper, we focus on the AI inference stage and introduce a novel equity-aware GLB algorithm to fairly balance Al's social and environmental costs across different regions. More specifically, we consider the performance cost of heterogeneous Al models and the carbon and water footprints associated with AI model inference by dynamically scheduling users' inference requests (a.k.a. workloads) using GLB. When optimizing GLB decisions, we leverage Lq norms in terms of Al's social and environmental costs as regularization terms to penalize decisions that disproportionately affect certain regions. In other words, regions with higher environmental and/or social costs will be prioritized and given a larger weight when leveraging GLB to minimize the total cost. By doing so, both the social and environmental costs of AI inference are more evenly distributed across different regions, thus mitigating Al's social and environmental inequities.\nTo assess the effectiveness of our method on promoting socially and environmentally equitable AI, we conduct a simulation-based case study of 10 geographically-distributed data centers serving inference requests for an LLM over an 18-day period. Our empirical results demonstrate that while the existing GLB algorithms result in disproportionately large social and/or environmental costs in certain regions, our proposed equitable GLB can fairly balance Al's negative social and environmental costs across all the regions."}, {"title": "Related Works", "content": "From the social fairness perspective, much attention has been directed towards protecting groups with certain attributes [16, 23, 30]. The issue is partially rooted in inherent biases within datasets and could potentially be exacerbated by models [16, 30]. To address such unfairness, numerous strategies have been developed. For instance, [3, 19] suggest removing sensitive attributes from datasets to prevent the model from relying on them, while others adjust prediction outcomes after training [22, 23]. Additionally, some have advocated for equivalent metrics, such as error rates, among specific groups [1, 7]. These studies typically focus on the model training stage, but the attained fairness can be compromised if Al models of different sizes are not equitably chosen for users from different regions. By stark contrast, we focus on the Al inference stage and judiciously balance the user requests from different regions across geographically distributed data centers hosting heterogenous AI models.\nTo address AI's environmental impacts, existing studies primarily focus on minimizing environmental metrics such as the total carbon emission, water footprint, or a weighted combination thereof, to enable environmentally responsible AI model training and inference [6, 14, 32]. Nonetheless, concerns with Al's environmental inequity across different regions have remained largely unaddressed. A recent study [15] has proposed to tackle the uneven distribution of Al's regional environmental costs via GLB. But, this approach overlooks the social equity dimension, which is equally, if not more, important element of responsible AI."}, {"title": "Problem Formulation", "content": "We focus on the AI inference stage and consider a set of pre-trained Al models denoted by $K = \\{1, 2, \\cdots, K\\}$, each with different performance and energy consumption for serving an inference request. There are a set of geographically distributed data centers $N = \\{1, 2, \\ldots, N\\}$ serving users coming from a set of regions $J = \\{1,2,\\ldots, J\\}$.\nOperational cost. At each time $t$, data center $i$ dynamically selects one or more of the available heterogeneous AI models to serve the incoming workloads. More formally, we denote $y_{j,i}^{(t)} \\geq 0$ as the workload dispatched from region $j$ to data center $i$ served through model $k$ at time $t$. Given the scheduled demand $y_{j}^{(t)}$, we denote the energy consumption and computational resources necessary for deploying model $k$ in data center $i$ as $e_{i,k}(y_{j,i}^{(t)})$ and $r_{i,k}(y_{j,i}^{(t)})$, respectively. For example, both $e_{i,k}(y_{j,i}^{(t)})$ and $r_{i,k}(y_{j,i}^{(t)})$ can be modeled as linearly increasing functions in terms of $y_{j,i}^{(t)}$. Thus, the total energy consumption at data center $i$ can then be calculated as\n$$e_i(t) = \\sum_{j \\in J}\\sum_{k \\in K} e_{i,k}(y_{j,i}^{(t)}).$$ \nFor notational simplicity, we define the set of workload distribution decisions at time $t$ as $y(t) = \\{y_{j,i,k}^{(t)} | i \\in N, j \\in J, k \\in K\\}$. We also take the energy price $p_{i,t}$ and power usage effectiveness (PUE, which accounts for non-IT energy overheads) $\\gamma_i$ of data center $i$ into consideration. As a result,"}, {"title": null, "content": "the total operational cost at time $t$ can be written as\n$$cost(y(t)) = \\sum_{i \\in N} \\gamma_i p_{i,t} \\sum_{j \\in J} \\sum_{k \\in K} e_{i,k}(y_{j,i}^{(t)}).$$\nSocial inequity cost. We define the noramlized performance cost of the AI model $k$ as $s_k(y_{j,i}^{(t)}) = s_k \\cdot y_{j,i}^{(t)} \\geq 0$, where $s_k \\geq 0$ represents the inference performance degradation cost for each request when using model $k$ compared to the best possible model (usually the largest model [26]). For example, when model I has the best performance, its performance cost is zero for any allocated request. Here, the performance cost can be measured in terms of various metrics of an AI model (e.g., average inference accuracy and score of an LLM for a set of target tasks, among others). Thus, the total performance cost of the workload from region $j$ is computed as $\\sum_{i \\in N} \\sum_{k \\in K} s_k(y_{j,i}^{(t)})$, which, when normalized by the total workload $d_{j,t}$, represents the Al model's average social performance for users from region $j$ (i.e., a type of group fairness [23]). To balance Al's performance for users from different regions, we introduce a social fairness function $f(y(t))$ in terms of Lq norm of the average performance costs for users from different regions:\n$$f(y(t)) = \\Big(\\sum_{j \\in J} \\Big(\\frac{\\sum_{i \\in N} \\sum_{k \\in K} s_k(y_{j,i}^{(t)})}{d_{j,t}}\\Big)^q\\Big)^{1/q}$$\nwhere $q \\geq 1$ is a hyperparameter that promotes Al's social equity for users from different regions. Concretely, we only care about the average Al model performance across different regions when $q = 1$ (i.e., no consideration of AI's social equity), whereas we focus on minimizing Al's worst regional model performance when $q \\rightarrow \\infty$ (i.e., solely considering AI model performance for users from the most disadvantaged regions). The priorities for these two conflicting objectives are adjusted by varying $q \\geq 1$.\nEnvironmental inequity cost. Carbon emissions associated with fossil fuels and water consumption are the two main non-negligible factors. Besides the global warming effects, carbon emissions have significant local effects such as high air pollution and even elevated immortality rates [13], thus making it necessary to balance Al's regional carbon emissions. Depending on the fuel mix for electricity generation, the carbon emission rate can vary significantly across different physical locations and times of the day. Specifically, the carbon emission of data center $i$ is denoted as $c_{i,t}(e_i(t))$, where $e_i(t)$ is the total energy consumption for running AI inference in data center $i$ at time $t$. In general, an increased proportion of carbon-intensive energy sources (e.g. hard coals) directly correlates with higher carbon emissions, impacting the function $c_{i,t} (\\cdot)$. The water consumption of deploying AI models is another important environmental cost and can be divided into two categories: onsite and offsite [14]. For each data center, onsite water is evaporated to reject the heat generated by servers into the outside environment (if the data center uses cooling towers), or cool and humidify the air entering the data center (if the data center uses air-side free cooling) [14]. The offset water refers to the water consumed for the electricity generation. In total, we define the water consumption as $w_{i,t}(e_i(t))$, which considers both onsite and offsite water and is linearly increasing with $e_i(t)$ depending on the runtime water usage effectiveness.\nThe total environmental cost of data center $i$ is defined as\n$$H_i(\\sum_{t=1}^T y(t)) = \\sum_{t=1}^T \\mu_c c_{i,t}(e_i(t)) + \\mu_w w_{i,t}(e_i(t))$$\nwhere the hyperparameters $\\mu_w \\geq 0$ and $\\mu_c \\geq 0$ convert the carbon emission and water consumption to a single unit cost and balance their relative importance. By applying the Lq norm, the overall environmental inequity cost is defined as\n$$g(\\sum_{t=1}^T y(t)) = \\Big(\\sum_{i \\in N} \\Big(H_i(\\sum_{t=1}^T y(t))\\Big)^q\\Big)^{1/q}$$\nwhere $q \\geq 1$ prioritizes the minimization of Al's environmental cost in more disadvantaged data center locations/regions. In particular, when $q \\rightarrow \\infty$, (3) becomes AI's worst environmental impact over all the data center locations.\nGLB objective. We formulate the optimization objective of our socially and environmentally equitable GLB (called SE-GLB) as follows:\n$$\\begin{aligned}\n(c) \\min.\\\\ y(t), t=1,...,T &\\sum_{t=1}^T cost_t(y(t)) + f(y(t)) + g(\\sum_{t=1}^T y(t))\\\\\ns.t. &+\\sum_{i \\in N, j \\in J,k \\in K} y_{j,i,k}^{(t)} \\cdot d_{ij},\\\\\n& \\sum_{i \\in N} \\sum_{k \\in K} y_{j,i,k}^{(t)} = d_{j,t}, \\ \\forall j \\in J, t = 1,\\ldots,T,\\\\\n&\\sum_{j \\in J} \\sum_{k \\in K} r_{i,k}(y_{j,i,k}^{(t)}) \\leq M_i, \\ \\forall i \\in N, t = 1,\\ldots,T\n\\end{aligned}$$\nIn (4a), the term $\\sum_{i \\in N, j \\in J,k \\in K}y_{j,i,k}^{(t)} \\cdot d_{ij}$ accounts for the total moving cost for scheduling user requests from region j to data center i, where $d_{ij}$ represents the moving cost for scheduling one unit of request (e.g., in proportion to the distance between region j and data center i). The constraint (4b) means that we need to schedule all the user demand $d_{j,t}$ for each region $j$ without request dropping, and the constraint (4c) denotes the computational resource constraint for Al inference in each data center i. Note that we can also easily add other constraints such as workload routing constraints (i.e., user requests from region j can only be routed to certain data center locations due to data sovereignty regulations or latency constraints).\nCompared to the existing literature on GLB that typically minimizes the total cost or focuses on the environmental impact [9, 15], the key novelty of our formulation is to holistically address Al's social and environmental inequities by using La norms to penalize GLB decisions that lead to disproportionately high social and/or environmental costs in certain disadvantaged regions."}, {"title": "A Case Study", "content": "We run a simulation study to preliminarily validate SE-GLB to mitigate Al's social and environmental inequities."}, {"title": "Setup", "content": "We consider 10 geographically distributed data centers: four in the U.S. (Virginia, Georgia, Texas, and Nevada), four in Europe (Belgium, the Netherlands, Germany, and Denmark), and two in Asia (Singapore and Japan). Each of these locations hosts a large number of data centers. We also consider 10 regions, each corresponding to one distinct data center location in our experiments. To highlight the potential of equity-aware GLB, we consider full GLB flexibility, where workloads can be dispatched from any region to any data center. To host an LLM inference service, each data center contains a cluster of 150 identical Nvidia DGX A100 servers each equipped with eight NVIDIA A100 GPUs and a maximum power of 6.5kW. Excluding other services beyond our scope, each data center has a maximum Al inference server power of ~ 1 MW. The data center PUE is set as 1.1 to adhere to efficient operation standards. The regional environmental impact is assessed using a weighted combination of carbon and water footprints. For inference, we assume three LLMs of different sizes are available: Llama-2-7B, Llama-2-13B, and Llama-2-70B [26].\nDatasets. We utilize the GPU power trace spanning 18 days as used in [15]. We gather evaluation scores of Llama-2 from HuggingFace [31] across the model sizes of 7B, 13B, and 70B on benchmarks AI2 Reasoning Challenge, HellaSwag, and Truthful QA. We then average and normalize these scores for measuring Al's performance costs. Hourly energy prices across the 10 data centers are obtained from [10] for Europe and Asia, and from their respective ISOs for U.S. data centers [29]. Hourly weather data from [11] is utilized to calculate wet bulb temperature from dry bulb temperature and relative humidity. On-site WUE is determined using an empirical formula from [12].\nEvaluation metrics. We consider four metrics: 1) average energy cost, calculated as the total energy cost over 18 days divided by the number of data center locations; 2) average environmental footprint and social cost, representing the total carbon emission, water footprint, and performance cost by the number of data center locations; 3) maximum regional environmental footprint and performance cost, which identifies the highest environmental and performance costs among the 10 data center locations and user regions; 4) max/avg ratio, representing the ratio of the maximum cost to the average cost for relative comparison. A lower value on this metric indicates a more equitable solution.\nBaselines. 1) Cost-GLB: This algorithm optimizes the average energy cost and the performance cost. It can also be seen as a special case of SE-GLB where $\\mu_c$ and $\\mu_w$ are set as zero and q = 1. 2) A11-GLB: This algorithm minimizes the weighted sum of the energy cost, environmental cost and societal cost (i.e., q = 1) based on [12]. 3) E-GLB: The environmentally equitable GLB algorithm which is studied in [15] and does not address AI's social inequity. Note that we do not consider the baseline that solely minimizes energy cost, as this approach would simply force the data centers to always choose the smallest model for inference."}, {"title": "Results", "content": "We run an offline optimizer with all the future information in our case study, while online algorithms that optimize GLB without knowing future information are left as our future work. In Table 1, we show the cost comparison between baselines and SE-GLB. By default, the weights assigned to carbon emission and water consumption are $\\mu_w = 60$ (US\\$/m^3$) and $\\mu_c = 1500$ (US\\$/ton), unless otherwise specified. We can observe that Cost-GLB has the lowest energy cost compared to other GLB approaches since it prioritizes the energy cost minimization. However, it also leads to the highest average carbon emission and water consumption. Additionally, Cost-GLB exhibits the highest max to average ratio in terms of social and environmental equities. Therefore, solely optimizing for energy cost can overburden certain regions with excessive workloads and worsen the Al's inequity. By comparison, A11-GLB takes a weighted sum of energy cost and the environmental footprint, which reduces average water consumption and carbon emission. E-GLB further reduces the max to average ratio of environmental footprint by minimizing the Lq norm of Al's environmental impact across different locations. SE-GLB explicitly considers the Lq norms of both social and environmental costs, thereby achieving a more equitable distribution of AI's model performance and environmental impact across different user regions and data center locations. While this comes at an increased energy cost due to the conflict between equity and energy cost minimization, we argue that the cost increase is acceptable in order to mitigate Al's inequity that would otherwise create unintended socio-ecological consequences as people increasingly rely on AI."}, {"title": "Concluding Remarks", "content": "In this work, we holistically consider Al's social and environmental equity and propose novel equity-aware GLB to balance Al's regional social and environmental costs towards responsible AI. Our key novelty is to introduce Lq norms to penalize GLB decisions that would otherwise lead to disproportionately high social and/or environmental costs in disadvantaged regions. Our empirical evaluation has shown the effectiveness of our proposed approach in improving both social and environmental equity by prioritizing the most disadvantageous data centers and user regions."}]}