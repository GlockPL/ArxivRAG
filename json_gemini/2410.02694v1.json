{"title": "HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly", "authors": ["Howard Yen", "Tianyu Gao", "Minmin Hou", "Ke Ding", "Daniel Fleischer", "Peter Izasak", "Moshe Wasserblat", "Danqi Chen"], "abstract": "There have been many benchmarks for evaluating long-context language models (LCLMs), but developers often rely on synthetic tasks like needle-in-a-haystack (NIAH) or arbitrary subsets of tasks. It remains unclear whether they translate to the diverse downstream applications of LCLMs, and the inconsistency further complicates model comparison. We investigate the underlying reasons behind current practices and find that existing benchmarks often provide noisy signals due to low coverage of applications, insufficient lengths, unreliable metrics, and incompatibility with base models. In this work, we present HELMET (How to Evaluate Long-context Models Effectively and Thoroughly), a comprehensive benchmark encompassing seven diverse, application-centric categories. We also address many issues in previous benchmarks by adding controllable lengths up to 128k tokens, model-based evaluation for reliable metrics, and few-shot prompting for robustly evaluating base models. Consequently, we demonstrate that HELMET offers more reliable and consistent rankings of frontier LCLMs. Through a comprehensive study of 51 LCLMs, we find that (1) synthetic tasks like NIAH are not good predictors of downstream performance; (2) the diverse categories in HELMET exhibit distinct trends and low correlation with each other; and (3) while most LCLMs achieve perfect NIAH scores, open-source models significantly lag behind closed ones when the task requires full-context reasoning or following complex instructions\u2014the gap widens with increased lengths. Finally, we recommend using our RAG tasks for fast model development, as they are easy to run and more predictive of other downstream performance; ultimately, we advocate for a holistic evaluation across diverse tasks.", "sections": [{"title": "1 Introduction", "content": "Long-context language models (LCLMs) unlock a myriad of applications, from summarizing long documents to learning new tasks on the fly with thousands of examples. Many recent benchmarks sought to evaluate language models' long-context abilities (Zhang et al., 2024; An et al., 2024; Shaham et al., 2023; Bai et al., 2024, inter alia). However, recent developments on long contexts (Chen et al., 2023; Xiong et al., 2023; Peng et al., 2024; Fu et al., 2024) still rely on either perplexity or synthetic needle-in-a-haystack tasks (NIAH; Kamradt, 2024; Hsieh et al., 2024). Frontier"}, {"title": "2 Our Benchmark: HELMET", "content": "In this work, we seek to overcome the shortcomings of existing benchmarks by meeting the following desiderata: (1) diverse coverage across different tasks and capabilities of LCLMs, (2) controllable lengths that support more than 128k input tokens, and (3) reliable evaluation for both base and instruction-tuned models. In this section, we describe the datasets used in HELMET and how they improve upon existing evaluation benchmarks in terms of settings and metrics."}, {"title": "2.1 Realistic and diverse long-context applications", "content": "Retrieval-augmented generation (RAG). We use open-domain question answering (ODQA)\u2014which requires retrieving from a knowledge corpus and then generating correct answers (Chen et al., 2017)\u2014as a representation of retrieval-augmented generation (RAG) applications. We use Natural Questions (NQ; Kwiatkowski et al., 2019), TriviaQA (TQA; Joshi et al., 2017), HotpotQA (Yang et al., 2018), and PopQA (Mallen et al., 2023). We use the gold passage information (the passage that can answer the question) from Petroni et al. (2021), or otherwise use any passage that contains the answer."}, {"title": "2.2 Reliable evaluation metrics", "content": "Existing long-context benchmarks (Zhang et al., 2024; Shaham et al., 2023) largely rely on n-gram overlap metrics like ROUGE (Lin, 2004) that have been shown to correlate poorly with human judgment for tasks with long outputs, such as summarization (Goyal et al., 2023; Deutsch et al., 2022). L-Eval (An et al., 2024) uses LLMs to score reference-free \u201cwin-rates\", which neglects the available answer annotations and always requires evaluating model pairs. Instead, we design reference-based model evaluation for long-document QA and summarization that are more reliable and easy to use.\nQuestion answering. In NarrativeQA, we prompt GPT-40 with the question, the ground truth, and the model output to check for fluency and correctness. The fluency score is either 0 (incoherent or repetitive) or 1 (fluent), and the correctness score takes on the value of 0 (incorrect), 1 (partly correct), 2 (correct but not fully relevant), and 3 (correct and relevant). We take the product of the two as the final score and normalize it to a range of [0, 100]."}, {"title": "2.3 Robust prompting and controlled evaluation settings", "content": "Robust prompting reduces noise and enables evaluation on base models. Many long-context benchmarks require models to follow instructions and only support evaluating instruction-tuned models (Zhang et al., 2024; Shaham et al., 2023). However, many model developments do not apply instruction tuning (Chen et al., 2023; Fu et al., 2024) and these models can only rely on perplexity or synthetic tasks. To support long-context research efforts, we design our benchmark so that at least a portion of the datasets supports evaluating base models.\nExisting benchmarks mostly use zero-shot prompting (Shaham et al., 2023; Zhang et al., 2024), which leads to noisy output formats, especially for base models. For example, the model may output a long answer in RAG when a short answer is required. We add two-shot demonstrations in the prompt for all tasks to address this problem. For long-document QA and summarization, we replace the original document with a placeholder phrase to reduce the number of input tokens in the ICL example. As shown in Table 8, we observe that both base and instruct models substantially benefit from the demonstrations.\nFurthermore, we employ the length-instruction-enhanced evaluation from L-Eval for long-generation tasks (i.e., summarization), which is shown to have substantially more consistent and reliable evaluations (An et al., 2024). As a result, we find that our reproduction of previous datasets, such as \u221eBENCH QA tasks, better reflects the capabilities of LCLMs, as shown in Table 7. The use of demonstration and better instructions thus better depicts how models perform in real applications."}, {"title": "3 Analysis", "content": "We evaluate 51 LCLMs with HELMET. To our best knowledge, this is the most thorough and controlled comparison of long-context models on diverse applications. These models cover closed-source models, such as GPT4, Claude, and Gemini, as well as open-source model families, such as Llama-3 (Dubey et al., 2024), Mistral (Jiang et al., 2023), and Phi (Abdin et al., 2024). We also consider models that use different architectures\u2014full-attention transformers (Vaswani et al., 2017), sliding-window attention (Beltagy et al., 2020), and hybrid models with SSM modules (Dao & Gu, 2024). We also benchmark position extrapolation models such as YaRN (Peng et al., 2024) and LongRoPE (Ding et al., 2024). We list all the models evaluated in Table 14. We evaluate each model at input lengths: L \u2208{8k, 16k, 32k, 64k, 128k}, where L is the number of Llama-2 tokens (Touvron et al., 2023), and use greedy decoding for all models for consistency. We randomly sample 100 examples from each dataset, and more details are in \u00a7D."}, {"title": "3.1 Simple synthetic tasks are poor predictors of real-world performance", "content": "Many model developers rely on simple synthetic tasks, such as NIAH, for evaluating long-context language models, but it is unclear if these tasks are representative of real-world performance. To this end, we calculate Spearman's rank correlation p with the performance of 30 instruction-tuned models between synthetic and real-world tasks. First, Figure 3 shows that none of the synthetic tasks achieves an average correlation higher than 0.8. We also make the following observations.\nNot all synthetic tasks are created equal. The original NIAH, which places a needle in the middle of unrelated essays and asks the model to retrieve it, does not correlate well with real-world tasks: all correlations are \u2264 0.8. Using the popular RULER average score\u2014which contains not only NIAH variants but also synthetic aggregation, multi-hop tracing, and QA-also does not give strong correlations (all < 0.85).\nWe take a closer look at different RULER tasks and find that harder recall-type tasks are more reflective of real-world categories\u2014such as RULER MK, which places distracting needles around the target needle. Despite the overall low correlation, we believe they can still serve as a useful sanity check during model development. We put together several such RULER tasks, along with JSON KV, to form the HELMET synthetic recall set (more discussions on this in \u00a7E.1).\nTasks with noisier, more distracting contexts better differentiate models. To understand why synthetic tasks poorly correlate with real-world tasks, we plot the performance of different models on NIAH, RULER MK (one of our recall tasks), and HotpotQA (one of our"}, {"title": "3.2 Diverse LCLM applications call for diverse evaluation", "content": "In long-context language modeling, realistic tasks are often only used in isolated settings (Karpinska et al., 2024; Li et al., 2024c; Dubey et al., 2024), which limits the understanding of LCLMs in a broader context. In this work, we cross-examine model performance over the wide range of real tasks, and find that different categories do not always correlate with each other, as shown in Figure 5.\nNaturally, RAG and passage re-ranking moderately correlate due to the shared retrieval component. However, the added complexity of generating citations in ALCE leads to lower correlation with other categories. As shown in Figure 9, generating the correct answer and valid citations are not well correlated, which suggests that instruction following and recalling facts with long contexts are distinct capabilities.\nFurthermore, some categories\u2014summarization and in-context learning\u2014do not correlate well with other categories at all. Intuitively, summarization tests for the model's ability to aggregate information across the whole input while ICL tests for the model's ability to learn new tasks from many examples. Such capabilities are orthogonal to recall facts in long contexts. Therefore, model developers should evaluate across these distinct axes to draw a more holistic picture of the model's capabilities. Additional analysis are in \u00a7E.2."}, {"title": "3.3 Model performance across tasks and lengths", "content": "We show the performance of instruction-tuned models on HELMET at five different lengths in Figure 6, and the full results are illustrated in Figure 10. We analyze the model performance across two critical dimensions of long-context language modeling: task complexity and input length.\nOpen-source models lag behind closed-source models on complex tasks. First, we consider the performance of frontier LCLMs at the longest input length of 128k tokens. We find that the closed-source models\u2014notably GPT-40-08 and Gemini-1.5-Pro\u2014stand out as the strongest LCLMs. Other than ICL, the closed-source models outperform the open-source models on all tasks. The gap is relatively small on synthetic recall and LongQA, where the task is to retrieve information from the context. There is a stark contrast in the generation with citations and re-ranking performance, where the closed-source models are 30 to 40 absolute points better than the best open-source models.\nPerformance degradation with longer inputs is category-dependent. Most frontier models largely retain performance on recall and RAG with longer inputs; however, even the best models significantly degrade with more contexts on tasks like re-ranking and generation with citations. As illustrated in Figure 7, with increased task complexity from left to right, performance degradation at longer lengths is more pronounced. On generation with citations, open-source models completely collapse at 128k while GPT-40 stays roughly the same. This underscores the importance of evaluating on more complex and realistic long-context applications.\nNo clear winner across all categories. As we observe from the previous sections, the different categories are not always correlated with each other. This is evident in the different winners across different categories: for instance, GPT-40 is stronger on recall and generation with citations, while Gemini is stronger on passage re-ranking and long-document"}, {"title": "4 Related Works", "content": "Long-context language models. Frontier models such as GPT-4 (OpenAI, 2023), Gemini, and Claude claim to have expanded their context window beyond 100k tokens. In the open-source community, there are also efforts to train models with longer input lengths (Dubey et al., 2024; Fu et al., 2024; AI et al., 2024), explore position extrapolation techniques (Peng et al., 2024; Chen et al., 2023; Ding et al., 2024), and experiment with efficient architectures (Gu & Dao, 2023; Dao & Gu, 2024; Bertsch et al., 2023; Beltagy et al., 2020; Yen et al., 2024; Lieber et al., 2024, inter alia).\nSynthetic tasks. Synthetic tasks are often used to evaluate LCLMs since they can be procedurally generated, thereby enabling arbitrarily long input lengths and controlled \u201cneedle\u201d placement (Tay et al., 2021; Liu et al., 2023). In particular, Needle-in-a-Haystack (NIAH; Kamradt, 2024) inserts a \u201cneedle\" at specific depths of a long essay (i.e., the haystack) and asks the model to recall the fact. Recent works have expanded upon it and designed new procedures to test different aspects of LCLMs (Hsieh et al., 2024; Li et al., 2024b; Levy et al., 2024; Arora et al., 2023; Laban et al., 2024; Goldman et al., 2024). However, they do not study how results on synthetic tasks transfer to real applications. In contrast, we evaluate both synthetic and downstream tasks and investigate how they correlate with each other.\nLong-context benchmarks. As discussed in the main paper, existing benchmarks either are limit to relatively short context lengths (Shaham et al., 2022; 2023; Li et al., 2024a; An et al., 2024; Bai et al., 2024; Dong et al., 2024) or lack rigorous evaluation methods (Zhang et al., 2024; Yuan et al., 2024). Many works instead focus on specific domains, such as question answering (Wang et al., 2024b; Karpinska et al., 2024; Wang et al., 2024a), in-context learning (Li et al., 2024c; Bertsch et al., 2024; Xu et al., 2024; Anil et al., 2024; Agarwal et al., 2024), summarization (Chang et al., 2024; Kim et al., 2024; Shen et al., 2022), or RAG (Lee et al., 2024). In this work, we construct a comprehensive benchmark that tests across diverse downstream tasks at long input lengths and also present a unified comparison and analysis across 51 LCLMs."}, {"title": "5 Conclusion", "content": "In this work, we first identify the shortcomings of long-context evaluation settings and existing benchmarks\u2014over-reliance of synthetic tasks, low coverage of realistic applications, and unreliable metrics among others. We seek to address these issues by constructing HELMET, a application-centric benchmark with diverse domains and reliable evaluation settings. We then present a comprehensive evaluation of 51 frontier LCLMs across mul-"}, {"title": "A.3 Positional embedding extrapolation remains a challenge", "content": "A key component of LCLMs is its positional embeddings, as it's essential to how the model process positional information and extrapolate to long sequences. Thus, we also consider models that leverages positional extrapolation during inference. Specifically, we show Llama-3-Inst with the original RoPE embedding and changing the Theta base to 8M during inference, and Qwen2-Inst with YaRN scaling. Formally, RoPE defines $d_a = b^{-2d/|D|}$, where da is the angle at the d-th hidden state, b is a constant called the base, and |D| is the"}]}