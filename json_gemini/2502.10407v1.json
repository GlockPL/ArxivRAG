{"title": "Addressing Bias in Generative AI:\nChallenges and Research Opportunities in Information Management", "authors": ["Xiahua Wei", "Naveen Kumar", "Han Zhang"], "abstract": "Generative AI technologies, particularly Large Language Models (LLMs), have transformed information\nmanagement systems but introduced substantial biases that can compromise their effectiveness in informing\nbusiness decision-making. This challenge presents information management scholars with a unique\nopportunity to advance the field by identifying and addressing these biases across extensive applications of\nLLMs. Building on the discussion on bias sources and current methods for detecting and mitigating bias,\nthis paper seeks to identify gaps and opportunities for future research. By incorporating ethical\nconsiderations, policy implications, and sociotechnical perspectives, we focus on developing a framework\nthat covers major stakeholders of Generative AI systems, proposing key research questions, and inspiring\ndiscussion. Our goal is to provide actionable pathways for researchers to address bias in LLM applications,\nthereby advancing research in information management that ultimately informs business practices. Our\nforward-looking framework and research agenda advocate interdisciplinary approaches, innovative\nmethods, dynamic perspectives, and rigorous evaluation to ensure fairness and transparency in Generative\nAl-driven information systems. We expect this study to serve as a call to action for information management\nscholars to tackle this critical issue, guiding the improvement of fairness and effectiveness in LLM-based\nsystems for business practice.", "sections": [{"title": "1. Introduction", "content": "Generative AI (GenAI), particularly Large Language Models (LLMs), is a transformative technology with\nunprecedented capabilities in natural language processing (NLP), content generation, and a myriad of other\napplications (Abbasi et al., 2024; IBM, 2024; Chowdhery et al., 2023; Touvron et al., 2023). Leveraging\nvast multimodal datasets that encompass text, images, audio, and video, GenAI produces new content based\non patterns learned from training data (Yin et al., 2024). Prominent examples include GPT-4 for text\ngeneration and DALL-E and Midjourney for image creation. GenAI has sparked significant enthusiasm\nrecently due to its potential to revolutionize business operations, drive efficiency, and create value across\nindustries (Subramanian, 2024).\nDespite its promising advantages, implementing GenAI is fraught with significant challenges, particularly\nconcerning biases inherent in its data and algorithms (Chamberlain, 2024). Unlike traditional classification\nsystems, which primarily categorize data into predefined classes, GenAI's generative nature amplifies the\nrisks of perpetuating biases in training data and algorithms, thereby reinforcing gender, racial, and cultural\nstereotypes. These biases undermine trust and pose ethical, reputational, and regulatory risks for businesses\nand society.\nAs LLMs play a bigger role in business decision-making across functions such as operations, finance,\nmarketing, and human resources (e.g., Davis et al., 2024; Manis and Madhavaram, 2023), they must align\nwith human preferences (Shankar et al., 2024). Misalignment can lead to biased outcomes and unfair\ndecisions (Konsynski et al., 2024; Dai et al., 2024). For instance, biased recruitment algorithms might favor\nspecific genders (An et al., 2024), and biased models in healthcare could exacerbate inequities in patient\ncare (Haltaufderheide and Ranisch, 2024).\nAs a result, bias in GenAI has become a pressing concern (Abbasi et al., 2024). A recent survey revealed\nthat 32% of respondents believe they lost opportunities, such as financial or job prospects, due to biased AI\nalgorithms. Additionally, 40% feel that companies using GenAI are not sufficiently protecting consumers"}, {"title": "2. Background and Context", "content": "In this section, we begin by defining bias in GenAI (LLM) models and explain how systematic errors can\nperpetuate favoritism and stereotypes. This is followed by an analysis of bias sources, encompassing both\ndata-related and algorithmic factors. We then delve into methods for detecting and quantifying bias and\nsummarize debiasing techniques aimed at mitigating these biases."}, {"title": "2.1. Defining Generative AI Bias", "content": "Bias in GenAI models refers to systematic errors or distortions in the model's processing of information,\nleading to favoritism towards certain groups or incorrect assumptions based on learned patterns (Mehrabi\net al., 2021). These models, trained on vast corpora from diverse sources, inherit and amplify the biases in\nthe data and algorithms (Bommasani et al., 2023). For instance, models like GPT-40 and their predecessors\nmay generate outputs that prioritize certain perspectives, reinforce societal prejudices, and allocate\nopportunities unfairly if the training data predominantly reflects specific demographics or viewpoints\n(Shahriar et al., 2024)."}, {"title": "2.2. Sources of Bias", "content": "Bias in LLMs is rooted in the foundations of artificial intelligence, arising from multiple interconnected\nsources, including data, algorithms, and human subjectivity (Susarla et al., 2023). While some biases stem\nfrom non-human factors, others are shaped by human decisions. Recognizing these origins, as well as their\ndistinctions and relationships, allows information management scholars to comprehend the multifaced\nnature of bias and develop more targeted strategies for identifying and mitigating biases throughout the\nLLM lifecycle.\nFirst, training datasets often contain inherent biases due to the source material. When data predominantly\nrepresents certain demographics or perspectives, models trained on them inevitably reflect and perpetuate\nthese biases (Bommasani et al., 2023). Second, algorithms themselves can amplify bias through their\ninherent properties, such as mathematical assumptions, statistical properties, or the autonomous learning\nbehaviors of complex models. These factors may introduce biases even when the training data seems\nunbiased."}, {"title": "2.3 Detecting and Quantifying Bias", "content": "Detecting and quantifying bias in LLMs, as the first step to reducing bias, is a challenging yet critical area\nof research in information management. Various methodologies have been established in this area, many\nof which build on each other and evolve together. This interconnectedness reflects the complexity of bias\ndetection and the need for a multifaceted approach to effectively address it in future research.\nEmbedding-based metrics measure bias by calculating conceptual distances between target words (e.g.,\nnationalities) and attributes (e.g., races). While the Word Embedding Association Test uses cosine\nsimilarity to assess bias between word pairs (Caliskan et al., 2017), the Sentence Encoder Association Test\nextends this to sentence embeddings, capturing bias in more complex linguistic structures (e.g., Dolci et al.,\n2023). In addition, probability-based approaches measure systematic deviations from an unbiased outcome\nusing probabilistic models and statistical inference such as Bayesian networks. These methods have been\nused to quantify racial, gender, and other discriminations in hiring, pay gaps, and criminal justice (Mehrabi\net al., 2021).\nFurthermore, the counterfactual evaluation tests for bias by modifying sentences, such as changing\ndemographic indicators or key attributes, and then observing changes in model outputs. Significant bias\nimpacts on gender and race have been identified using these methods (Boyer et al., 2023; Kusner et al.,\n2017). Additionally, template-based approaches evaluate biases by using predefined templates that vary\ndemographic attributes while holding other variables constant, isolating their effect on outputs (Dong et al.,\n2024). Extensive templates have been created to assess various biases (Stanczak and Augenstein, 2021),\nwith recent advancements in models to automatically generate prompts, reducing manual effort (Radcliffe\net al., 2024)."}, {"title": "2.4. Debiasing", "content": "Debiasing aims to enhance the accuracy of model predictions and recommendations while ensuring equity\nacross different demographic groups (Susarla et al., 2023; Subramanian et al., 2021). Debiasing techniques\ncan be applied at various stages of LLM implementation: the Preprocessing Stage, the Training Stage, and\nthe Post-processing Stage. Below we discuss the existing debiasing techniques in each stage that have been\nestablished in the literature.\nData used to train or finetune LLMs can introduce downstream bias, making debiasing at the preprocessing\nstage critical, especially with imbalanced datasets. One common technique is Counterfactual Data\nAugmentation (CDA), which rebalances data by altering specific associations (Lu et al., 2020). For\ninstance, sentences like \u201cShe is a nurse\u201d can be replaced with \u201cHe is a nurse\u201d to promote gender neutrality.\nSimilarly, Counterfactual Data Substitution involves probabilistically replacing biased terms without\naltering the dataset size, mitigating bias while preserving the data structure (Maudslay et al., 2019). Other\napproaches remove biased examples from datasets, though this risks data loss and reduced coverage (Le\nBras et al., 2020). An alternative method involves masking biased model weights during testing, allowing\nthe model to bypass bias without the need for finetuning (Du et al., 2021). Further, training diverse\ndatasets such as across multiple languages-helps reduce ethnic and cultural biases, offering broader"}, {"title": "3. Future Directions for Information Management Research", "content": "Addressing bias in LLMs is both an ethical and technical imperative, especially as these models play an\nincreasingly prominent role in information systems. Firms may prioritize high-risk areas or those with\nregulatory requirements for bias mitigation, trading off other areas and leaving them under-addressed due\nto resource constraints. As a result, the challenges posed by LLM bias, such as perpetuating inequalities\nand unfair practices, demand robust mitigation strategies for bias and practical applications. Information\nmanagement scholars are uniquely positioned to lead this effort. Future research should not only advance\ntechnical methods for reducing LLM bias but also explore how organizational and individual efforts can\naddress these biases within real-world constraints. Developing frameworks that tackle the complex,\nmultifaceted nature of LLM bias will be essential. Such efforts can enhance fairness, transparency, and\nequity, which ultimately improve individual, organizational, and societal benefits of LLMs, demonstrating\ntheir positive contribution to information management.\nStrategies for advancing research in LLM bias encompass three interconnected areas: research design,\ntechnical development, and policymaking/social impact. Improved research design focuses on frameworks\nthat capture the nuances of bias across various contexts, incorporating diverse perspectives (Section 3.1).\nMethodological advancements involve creating and refining metrics and algorithms to detect and mitigate\nLLM bias, ensuring greater accuracy and fairness (Section 3.2). Policymaking efforts aim to establish\ncomprehensive guidelines and regulations that promote ethical practices in deploying LLMs, ensuring\naccountability and transparency (Section 3.3)."}, {"title": "3.1 Research Design", "content": "Interdisciplinary Approaches and Collaboration: Addressing LLM bias spans multiple domains, including\ncomputer science and engineering, information systems and information management, ethics, law,\npsychology, behavioral economics, sociology, political science, and other social sciences (Jiao et al., 2024).\nThis interdisciplinary approach integrates diverse perspectives from stakeholders with distinct priorities.\nFor instance, firms prioritize productivity and cost-effectiveness, end-users and advocacy groups often\nadvocate for equity and inclusion, while policymakers emphasize fairness, transparency, and ethics.\nComputer scientists optimize performance, social scientists explore systemic inequalities perpetuated by\nLLMs, and legal experts assess liability and accountability. This challenge presents a unique opportunity\nfor information management scholars to embrace an interdisciplinary approach. By collaborating with\nexperts from diverse fields, they can create holistic solutions to tackle the complex nature of LLM bias,\nenhancing the fairness and accountability of information systems (Narayan et al., 2024).\nThe following research questions explore how interdisciplinary collaboration can help address the technical\nand societal dimensions of LLM bias: (1) What frameworks can information management scholars develop\nto reconcile tensions between technical priorities (e.g., performance and efficiency) and societal imperatives\n(e.g., fairness and transparency) in ways that address LLM bias unique in specific research contexts? (2)\nHow can interdisciplinary research studies be designed to evaluate, prioritize, and trade off competing\nstakeholder objectives to create effective and equitable debiasing strategies? By answering these questions,\nwe can develop collaborative approaches to detect and mitigate biases in LLMs while promoting fairness\nacross diverse contexts.\nDynamic Framework and Continuous Monitoring: Bias mitigation in LLMs is not a one-time task but an\nongoing process requiring continuous monitoring and adaptation as social norms and languages evolve\n(Brown, 2024). Practical constraints, such as high computational costs (especially in cloud computing),\nlack of expertise, and time pressures for extensive model retraining or continuous bias monitoring-which\nmay conflict with regulatory requirements\u2014often hinder effective resource-intensive debiasing efforts,\nespecially for smaller organizations that often rely on generic solutions and prioritize rapid deployment\nover fairness. Additionally, constant monitoring demands continuous resources and iterative updates, which\ncan potentially compromise extensive bias testing, complicating sustained bias mitigation efforts. To\naddress these challenges, information management scholars must develop adaptive, resource-efficient\nframeworks that account for these constraints while maintaining effectiveness. For instance, lightweight\nmonitoring tools and scalable debiasing techniques can alleviate computational burdens.\nThe following research questions focus on developing dynamic frameworks and continuous monitoring\nmechanisms under practical constraints to address the evolving nature of LLM bias: (1) What dynamic\nmethodologies can information management scholars design to integrate real-time user feedback for\ncontinuous detection and mitigation of LLM bias, ensuring scalability and operational efficiency? (2) How\ncan scholars develop and implement resource-efficient longitudinal studies to evaluate the effectiveness of\ncontinuous monitoring frameworks in mitigating bias over time while addressing expertise gaps and"}, {"title": "3.2 Technical Development", "content": "Improved Fairness Metrics: While existing metrics provide a foundation for identifying LLM bias, they\nrequire further refinement to capture its complex and nuanced nature. Key challenges include balancing\ngeneralizability with context-specificity, adapting to evolving societal norms and biases, and managing\ncomputational costs associated with running comprehensive tests across multi-dimensional fairness\nevaluations. Information management scholars can develop adaptive, interpretable, and resource-efficient\nfairness metrics that account for diverse user groups and contexts while remaining practical for real-world\nimplementation.\nWe propose the following research questions to advance this area: (1) How can fairness metrics balance\ngenerality and specificity to ensure broad applicability while capturing nuanced and context-specific\nbiases? (2) What resource-efficient methodologies can operationalize fairness metrics, accounting for\ncomputational constraints and scalability requirements in real-world applications? (3) How can theories\nfrom information management guide the design of fairness metrics that reconcile conflicting stakeholder\ndefinitions of fairness, ensuring equitable treatment across diverse demographic groups while maintaining\npractical feasibility? Addressing these questions will facilitate the creation of more precise fairness metrics,\nultimately leading to fairer and more inclusive AI systems across various applications.\nAdvanced Debiasing Techniques: Debiasing LLMs is critical, as post-deployment corrections are often\ncostly, risky, and less effective (Gallegos et al., 2024). Although various debiasing techniques exist, their\neffectiveness can degrade significantly with minor modifications to datasets or evaluation settings (Sun et\nal., 2024). While these techniques represent a range of approaches, they may inadvertently increase bias\nover time (Tokpo et al., 2023). A further complexity arises from the tension between bias mitigation and\nalgorithmic innovation. Prioritizing debiasing can discourage experimentation with novel designs, as\ndevelopers may gravitate toward \u201csafer\u201d models with established methods, potentially stifling innovation."}, {"title": "3.3 Policymaking and Social Impact", "content": "Policy Development and Ethical Guidelines: Establishing robust policies and ethical guidelines is crucial\nfor ensuring the transparency, accountability, and fairness of LLMs. However, designing such frameworks\nis challenging due to the tension between universal standards and the need for context-specific flexibility.\nWhile standardized policies provide consistency and clarity, they may fail to address regional, cultural, or\nsector-specific nuances. Adding to this complexity is the rapid evolution of LLM technologies, which often\noutpaces existing policies and ethical frameworks, creating gaps that demand urgent attention. Information\nmanagement scholars play a pivotal role in addressing these challenges by analyzing current policies,\nidentifying gaps, and collaborating with policymakers to create practical and enforceable guidelines.\nKey research questions on developing regulatory frameworks and ethical guidelines include: (1) How can\ninformation management researchers balance the tension between fostering innovation in LLMs and\nmitigating unintended societal consequences, particularly when ethical imperatives conflict with\ncommercial pressures? (2) What ethical guidelines can direct the responsible integration of LLMs in high-\nstakes sectors like healthcare and finance, where conflicting demands for rapid deployment, accuracy, and\ninclusivity create unique regulatory approaches? Delving into these questions will help create policies that\nsafeguard against bias, promote innovation, ensure responsible LLM deployment at scale, and support\nbroader societal goals."}, {"title": "4. Applied Areas of Information Management Practice", "content": "LLMs have extensive applications in business practices, including human resources, healthcare, finance,\nmarketing, etc. However, biases in LLMs can undermine the effectiveness and fairness of these\napplications, posing significant challenges to information management. This section identifies key areas\nwhere these biases manifest and suggests avenues for future research. We encourage scholars to apply the\nstrategies proposed in Section 3 to explore these specific areas.\nHuman Resources Management: LLMs in human resource (HR) management, such as resume screening\nand performance evaluations, have been documented to perpetuate societal biases, perpetuating gender and\nracial discrimination that limits career opportunities for women and minority groups (Armstrong et al.,\n2024; Lucas, 2024). At the same time, hiring managers' efficiency goals might conflict with job applicants'\nconcerns regarding fair representation in AI-driven recruitment systems. Overall, these LLM biases\nunderscore the need for transparency, rigorous testing, and explainable AI methodologies to mitigate\ndiscrimination while balancing business constraints, such as minimizing adverse impacts or inefficiencies\nin achieving business outcomes. Key questions to address include: (1) How can bias in LLM-driven\nrecruitment systems be designed to minimize bias and prevent gender and racial discrimination in hiring\nand evaluations? (2) What role can explainable AI play in enhancing the transparency and fairness of LLM-\ndriven HR processes? Investigating these questions will help reduce discrimination in HR practices and\nprovide a foundation for integrating explainable AI into HR systems.\nHealthcare Information Systems (HIS): Biases in LLM-driven HIS can exacerbate health disparities and\nunderserved marginalized populations (Santurkar et al., 2023). To ensure equitable healthcare, researchers\nshould focus on real-time bias detection, transparency, and adapted methods for healthcare contexts (Levy\net al., 2024). Relevant research questions include: (1) How can LLMs be tailored for healthcare systems to\nreduce biases that lead to disparities in patient treatment quality, particularly for marginalized groups? (2)\nHow can researchers distinguish between intended differentiation, such as customizing treatment plans for\nspecific populations (e.g., African Americans), and unintended differentiation that may lead to inequitable\noutcomes? Addressing these questions will advance LLM-driven healthcare systems that prioritize fairness,\ndetect biases in real time, and prevent disparities in care delivery for marginalized groups."}, {"title": "5. Conclusion", "content": "GenAI technologies have transformative potential for enhancing productivity and economic value across\nvarious business sectors (Yee and Chui, 2023). However, the inherent biases within these models,\nparticularly in LLMs, raise significant ethical concerns. Trained on extensive internet text data, LLMs often\nmirror societal biases, stereotypes, and cultural assumptions, thereby influencing decision-making,\nentrenching stereotypes, and perpetuating inequalities. The dynamic nature of cultural and social norms\nfurther complicates the integration of these values into GenAI models, demanding a nuanced understanding\nof diverse and dynamic perspectives (Triandis, 2018).\nThis study highlights the critical issue of bias in GenAI and its profound implications for information\nmanagement. Drawing on current research on bias detection, measurement, and mitigation, we propose\nfuture research directions specifically tailored for information management scholars. These include\nstrategies for practical implementation and application in business contexts, guiding key research questions.\nWe recognize that stakeholders in the LLM ecosystem, including developers, companies, users, and\npolicymakers, often have varied and sometimes conflicting objectives. Hence, our research questions\nincorporate the need to balance these tensions to develop solutions that ensure more equitable LLMs.\nFurther, with the increasing technical and societal complexity of LLMs, we emphasize the need for\ninterdisciplinary efforts. By integrating insights from computer science, ethics, law, and social sciences,\ninformation management scholars can develop more robust and comprehensive research designs. This\ninterdisciplinary approach not only enhances the rigor and relevance of information management research\nbut also ensures that solutions to LLM bias are holistic and effective. As such, our study provides a valuable\nframework for information management scholars to navigate and address the multifaceted challenges posed\nby LLM bias, which ultimately contributes to the development of fairer and more accountable Al systems.\nGiven the central role of information management in organizational operations, professionals must\nrecognize the challenges associated with bias issues when adopting GenAI. This article fills a gap in the\nliterature by providing insights and stimulating further discussions. We call for collaboration between\npractitioners and researchers to develop best practices in tackling these challenges.\nEngaging actively in interdisciplinary research and collaboration, conducting continuous monitoring and\nlongitudinal studies, and advocating for robust ethical frameworks and regulatory policies will enable\nscholars to build AI systems that are powerful, efficient, fair, and just. Information management curricula\nshould also emphasize bias issues in GenAI, preparing students to navigate responsible implementation by\nunderstanding data integrity and ethical principles in GenAI practices. Through these efforts, we can\ncontribute to a future where AI technologies drive positive societal outcomes and foster equity."}]}