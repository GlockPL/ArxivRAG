{"title": "Evaluating the Impact of Underwater Image Enhancement on Object Detection Performance: A Comprehensive Study", "authors": ["Ali Awad", "Ashraf Saleem", "Sidike Paheding", "Evan Lucas", "Serein Al-Ratrout", "Timothy C. Havens"], "abstract": "Abstract-Underwater imagery often suffers from severe degradation that results in low visual quality and object detection performance. This work aims to evaluate state-of-the-art image enhancement models, investigate their impact on underwater object detection, and explore their potential to improve detection performance. To this end, we selected representative underwater image enhancement models covering major enhancement categories and applied them separately to two recent datasets: 1) the Real-World Underwater Object Detection Dataset (RUOD), and 2) the Challenging Underwater Plant Detection Dataset (CUPDD). Following this, we conducted qualitative and quantitative analyses on the enhanced images and developed a quality index (Q-index) to compare the quality distribution of the original and enhanced images. Subsequently, we compared the performance of several YOLO-NAS detection models that are separately trained and tested on the original and enhanced image sets. Then, we performed a correlation study to examine the relationship between enhancement metrics and detection performance. We also analyzed the inference results from the trained detectors presenting cases where enhancement increased the detection performance as well as cases where enhancement revealed missed objects by human annotators. This study suggests that although enhancement generally deteriorates the detection performance, it can still be harnessed in some cases for increased detection performance and more accurate human annotation.", "sections": [{"title": "I. INTRODUCTION", "content": "UNDERWATER computer vision has gained more attention in recent years with an increasing number of ap- plications such as ocean mapping, ecological monitoring, and inspection. This is greatly attributed to the recent availability of large underwater datasets facilitated by the widespread use of Remotely Operated Vehicles (ROVs). Most recent datasets such as [10], [11], [12] are fully or partially obtained by ROVs due to their relatively low cost, availability, and ease of use. Because light propagates differently in water compared to air, underwater images are usually associated with limited visibility, reduced contrast, mono-color casts, and poor overall quality. In particular, light underwater suffers from absorption by which the energy of the light is dissipated and scattering by which the direction of light is changed [13]. Because of these challenges with underwater imagery, re- searchers have developed numerous enhancement approaches to mitigate these effects [1], [4], [6]. In addition, some studies [14], [15] suggested using image enhancement as a preprocessing step to increase classification and detection performance. However, very little previous work has addressed the use of Underwater Image Enhancement (UIE) for high- level tasks such as Underwater Object Detection (UOD) and classification. Thus, more work is required to investigate the effect of image enhancement on detection performance. Pei et al. [16] concluded that image degradation negatively impacts classification performance but image enhancement algorithms do not help regain classification performance. The authors of [17], [18] concluded that image enhancement has a negative effect on detection performance when used as a preprocessing step. Conversely, Chen et al. [19] concluded that mixing enhanced and non-enhanced images to train a classifier does not lead to higher performance, but rather more robust performance. Motivated by previous studies, we conducted a thorough evaluation of the underwater enhancement-detection frame- work and tried to overcome the shortcomings of previous studies by quantitatively and qualitatively linking image qual- ity with detection, investigating the underlying reasons behind the degradation of detection performance after image enhance- ment, and showcasing how enhancement has the potential to improve detection. To support our study, we selected representative image enhancement models with innovative and very recent architec- tures based on the taxonomy presented in Section II with focus on recent learning-based methods, then used two publicly available datasets to ensure the generalizability of our conclu- sions: CUPDD [20] and RUOD [17]. Furthermore, the RUOD is one of the largest publicly available underwater datasets combining images from different datasets and containing dif- ferent color casts, environments and aquatic creatures, while CUPDD is a relatively smaller dataset containing severely degraded challenging images. Since we do not have reference images (i.e. ground truth images with no water effects), we"}, {"title": "II. LITERATURE SURVEY", "content": "We summarized and extended current underwater image en- hancement taxonomies [30], [31], [32] comprising three main categories: non-physical, physical, and deep learning-based as illustrated in Fig. 1. These are further broken down into seven enhancement subcategories: histogram, fusion, retinex, dark channel prior, optical imaging, CNN-based, and GAN- based. In general, non-physical models work directly on images using image processing techniques without using any prior knowledge, so they are fast and simple but lead to sub-optimal enhancement details [33]. Conversely, physical models rely on simulating the underwater imaging process [34] and addressing ill-defined problems by using physical priors to mitigate the degradation process and obtain enhanced images [35]. However, the assumptions made for unknown parameters may not be entirely suitable or precise in intricate underwater environments, which restricts their use cases [36]. Learning-based methods are based on neural networks and can generate good enhancement results but require parameter tuning and a sufficient amount of data [37]. Since many recent enhancement algorithms are CNN-based, we select multiple models from this subcategory and insured that the"}, {"title": "A. Combined Enhancement-Detection Studies", "content": "In this section, we summarize the studies that addressed the problem of combined image enhancement and object detection. For instance, Fu et al. [17] proposed a new method for combining image enhancement and object detection called shared loss, indicating a shared loss function for the enhancer and detector to be trained simultaneously. They proposed RUOD dataset comprising 14,000 images and 10 classes and used it to evaluate their method. The finding of their research reveals that the cascaded enhancement-detection approach leads to a lower detection performance while a joint learning approach increases it. The authors do not provide much details about the implementation of their proposed joint-learning framework. The authors of [19] studied the effect of different and mixed domains, and environments, on detection and addressed the enhancement role in the process. The study used the Underwater Robotic Picking Contest (URPC2018) [39] dataset and split it into three domains based on CIELAB color space analysis [40]. Then, a restoration technique was applied and multiple experiments were conducted using different detec- tors. The authors concluded that although image restoration does not increase within-domain detection performance, but does produce a more generalizable performance. One of the drawbacks of the approach presented in [19] is that the effects of different enhancement models are not studied. Aligning with the same topic, Pei et al. [16] studied the effect of synthetic image degradation and restoration on clas- sification performance. In particular, the Caltech-256 dataset [41] was used, on which multiple degradation effects were applied including haze, motion blur, fish-eye, underwater, low- resolution, salt and pepper noise, white Gaussian noise, Gaus- sian Blur, and out-of-focus blur. The summary of the work presented in [16] is that although image degradation severely affects the classification performance, image degradation- removal does not lead to a restored or improved performance. We believe that this study should be extended to other higher- level tasks such as object detection. In another study [42], a new video dataset called Under- water Visual Object Tracking (UVOT) and an enhancement"}, {"title": "B. Underwater Image Enhancement (UIE)", "content": "Underwater image enhancement is categorized into three main groups: non-physical, physical, and learning-based meth- ods. The most recent models belonging to each category are summarized in this section, beginning with non-physical methods. 1) Non-physical: Zhang et al. [1] introduced a method for enhancing underwater images using a combination of various techniques. Their proposed method included attenu- ated color channel correction (ACCC), a fusion-based con- trast improvement method, and multiscale unsharp masking (MSUM) strategy to further sharpen the edges and details of the result. Extensive experiments were conducted on four benchmark datasets for underwater image enhancement. The results showed that their approach outperforms 12 other ex- isting methods in both qualitative and quantitative metrics. Moreover, their method generalizes well to low-light images and hazy images. Nevertheless, there are some limitations associated with their method where the color of quality- degraded underwater images may be reduced after enhance- ment. Besides, the introduction of a dual-histogram-based iterative threshold may increase computational complexity. However, certain limitations were identified in their study."}, {"title": "III. EVALUATION PROCEDURES AND METRICS", "content": "In this section we describe our experimental settings includ- ing the utilized datasets, metrics, and image enhancement and object detection models."}, {"title": "A. Selected Datasets", "content": "We implemented our experiments on two underwater datasets: 1) RUOD [17], comprising 14,000 high-resolution underwater images, 9,800 of which are used for training, with approximately 75,000 annotations of 10 aquatic objects, such as divers, plants, and different types of aquatic animals; and 2) CUPDD introduced in [20], comprising 414 images, 313 of which are used for training, with three general categories of aquatic plants collected in the Great Lakes region in the US. The use of both datasets help us to draw generalized conclusions since RUOD is very large, recent and extensive combining images from different smaller datasets and contain- ing different color casts, environments and aquatic creatures while CUPDD is very challenging and has the potential of testing enhancement algorithms at the extreme end."}, {"title": "B. Selected Enhancement and Detection Models", "content": "To accurately represent underwater image enhancement, we select several SOTA image enhancement models covering major categories and subcategories of image enhancement [30], [31], [32], including physical, non physical, and learning- based image enhancement categories. We also insure that the selected models are very recent, well-designed and acquired from credible sources. Since learning-based algorithms be- came recently very common, we select multiple models from this category to better represent it. The selected models include ACDC [1], TEBCF [2], BayesRet [3] are non-physical meth- ods that use histogram, fusion, and retinex-based enhancement, respectively. The physical methods are PCDE [4] and ICSP [5], which use a dark channel prior and optical imaging properties-based enhancement, respectively. The deep-learning methods are AutoEnh [6], which uses a vision transformer; Semi-UIR [7], which uses contrastive learning; USUIR [8] and TUDA [9], which are based on CNNs and GANs, respectively. The source codes and instructions to run these models can be found in a Github link within their corresponding provided references. We implemented all enhancement models without modification or additional training, using the source codes and the trained models provided by the original authors. On the detection side, the YOLO detection family has been extensively used in underwater scenarios showing its efficiency [54], [60], [62] with YOLO-NAS being one of the recent top performers [27], [28], [29]. In this study, the SOTA YOLO- NAS [26] is fine-tuned with the original images and the enhanced images from each image enhancement algorithm for both datasets totaling of 20 model instances, each trained and evaluated separately. The image resolution for both datasets is set to 800 \u00d7 600. We used the SuperGradients [26] library to implement the detector on a Linux server with two Nvidia Tesla V100 GPUs. The training was performed with a batch size of 16 and the AdamW optimizer with a decay of 0.00001. We use the YOLO-NAS large (L) architecture and start from the COCO pre-trained weights."}, {"title": "C. Selected Evaluation Metrics", "content": "Researchers have developed a number of ways to quanti- tatively evaluate underwater images, which mainly fall into two categories: full-reference and reference-free evaluation [73]. In full-reference evaluation, a ground truth reference image, resembling an image without water effects, is compared with the degraded image. In contrast, reference-free evaluation does not require a reference image, and instead is measured directly on the degraded image. In this study, we opted to use reference-free evaluation because reference images do not exist for the selected datasets [20], [17]. We also believe that this is a more practical study, as reference images are often impossible to obtain, except in very controlled laboratory experiments. We used four underwater image enhancement metrics:\n1) UIQM [22], which is comprised of color, contrast, and sharpness indices. A higher UIQM score indicates better image quality.\n2) UCIQE [23], representing a linear combination of sat- uration, contrast and chroma. A higher UCIQE score indicates better image quality.\n3) CCF [24], which is a linear regression model of color- fulness, contrast, and fog density indices. A higher CCF score indicates better image quality.\n4) Entropy, a statistical measure that can be used to de- scribe the randomness of the texture in images [74]. Here we calculated it as the Discrete Entropy [25] using Matlab. A higher Entropy score indicates richer details, i.e., higher image quality. All of the utilized metrics are used without any modification and are implemented with original weights and hyperparame- ter values from the original papers."}, {"title": "D. Proposed Quality Index (Q-index)", "content": "We use the selected metrics described in Section. III-C to generate a quality index (Q-Index) which is later used to describe and analyze the quality distribution of images. The use of the Q-index would facilitate such analysis by making it more focused and providing a general impression about the quality of the images. Essentially, the Q-index is generated from the four selected metrics through outlier removal, global rescaling, and averaging processes to produce a single bounded metric between zero and one as introduced in Algorithm 1. A value is flagged as an outlier if it is three Median Absolute Deviations (MAD) away from the median as in the Matlab implementation in [75]. MAD is defined as\n$MAD = c \\cdot median(|A \u2013 median(A)|),$\nwhere $c = \\frac{1}{\\sqrt{2} erfc^{-1} (3)}$\nand $erfcinv$ is the Inverse Complementary Error Function [76]. Moreover, global re-scaling means that the minimum and maximum values are taken across all images from the original and enhanced datasets to fairly compare the Q-index values across different models. On the other hand, local Maxima and Minima refers to the Maxima and Minima of a certain metric belonging to a certain enhancement model as shown in Algorithm 1"}, {"title": "IV. ENHANCEMENT EVALUATION", "content": "In this section, a quantitative and qualitative evaluation of the selected image enhancement models is presented for both RUOD and CUPDD datasets. In addition, we generate the Q-index based on the selected metrics to show the quality distribution of the images after enhancement and to provide a mixed quantitative and qualitative analysis."}, {"title": "A. Quantitative Evaluation", "content": "The images from the selected datasets are enhanced using the selected enhancement models and evaluated using the"}, {"title": "B. Quality Distribution", "content": "We used the proposed Q-index in Sec. III-D to facilitate a quality distribution analysis of images before and after en- hancement by each enhancement model. In particular, a violin plot is used to show the Q-index distribution of original images and the change in the Q-index distribution after enhancement, as shown in Figs. 2 and 3. The change in the Q-index (\u2206 of Q-index) for an image is simply calculated as\n$\\Delta of Q-index = Q-index_{(Enh)} \u2013 Q-index_{(Org)},$\nwhere (Enh) refers to the enhanced image and (Org) refers to the original. This helps us understand how images of different"}, {"title": "C. Mixed Quantitative-Qualitative Evaluation", "content": "As a further and final evaluation of image enhancement, we divide the Q-index into 10 quality bins and randomly select an image representing each quality bin for both datasets, namely CUPDD and RUOD as shown in Figs. 4 and 5. Some quality bins at both ends are omitted since no images are found in those extreme bins. The Q-index value is shown under each image and color-mapped for easier inference of the quality. Fig 4 shows a random image from each quality bin available in CUPDD dataset with their enhanced versions and correspond- ing Q-index values. A gradual but not accurate improvement in quality can be seen as the Q-index value of the Original images increases from left to right. However, we would argue that the image belonging to the 0.5-0.6 quality bin looks more visually pleasing than its successor with soft details, less noise, and a mild color cast. The successor image appears to have a severe color cast and more noise but yet achieved a higher Q-index. This observation is further demonstrated by the enhanced images, especially with the TEBCF images where they all look very sharp and noisy but achieve very high Q-index values of higher than 0.75. In contrast, the Q-index accurately predicted our qualitative evaluation of the PCDE and the ICSP models at values ranging from 0.52 to values as low as 0.08 with the first producing very foggy images and the latter producing extremely overexposed images further deteriorating the quality of original images. The ACDC and BayesRet produce less colored tones but maintain the details of the images. The AutoEnh, Semi-UIR, USUIR and the TUDA seem to be producing the most visually pleasing images with vibrant colors, less noise and fog effects compared to other models. However, this is not always reflected by their Q-index values. It is notable that the average increase in the Q-index value of low-quality images is much higher than the average increase in the Q-index values of high-quality images. For instance, the Q-index of the image from the lowest quality bin almost doubled with some models while the Q-index of the image from the highest quality bin barely witnessed a slight improvement with most models. In accordance with CUPDD, the random images from RUOD dataset shown in Fig. 5 observe a similar gradual improvement from left to right. Images with low Q-index tend to have a mono-color cast with very few details and low visibility. A less severe color cast with more details and colors can be noted as the Q-index increases. Images with high Q-index tend to have a richer red channel, more vibrant color, and clear fine details. Although it is hard to distinguish a slight difference in the Q-index, the general trend of quality improvement with the increase of the Q-index is evident. For instance, the quality of the first two images at 0.16 and 0.21 is hardly distinguishable. The performance of the enhancers on RUOD is very similar to CUPDD with TEBCF being quantitatively the most superior but qualitatively the most inferior. We elect the TUDA and the Semi-UIR as the best enhancers for producing visually pleasing images. We conclude that enhancement techniques are most effective on low-quality images, and deep learning- based enhancement models are better at producing visually pleasing images. In addition, the Q-index provides a general indication of image quality; however, it can be easily misled by artifacts produced by enhancement models, revealing the sensitivity of its composing metrics."}, {"title": "V. DETECTION EVALUATION", "content": "In this section, we conduct a per class quantitative eval- uation of the trained YOLO-NAS models on the Original and enhanced images from both datasets CUPDD and RUOD using mean Average Precision (mAP50_95). Furthermore, we conduct a qualitative analysis using random samples from both datasets."}, {"title": "A. Quantitative Evaluation", "content": "We have a total of nine selected enhancement models and two datasets, thus we train a total of 20 YOLO-NAS models including the Original images from both datasets. We call YOLO-NAS models that are trained on the Original images as Original detectors, and YOLO-NAS models that are trained on enhanced images as domain detectors. All models are trained using the same parameters and configurations. The first experiment is conducted on CUPDD and is shown in Tab. III where the Original detector outperformed all domain detectors by a margin of 4% at 0.38 mAP. Moreover, the Original detector excels in the bushy class by a fair margin of 2%. In contrast, it is outperformed by the AutoEnh. domain detector in the leafy class by a notable margin of 3% possibly due to the fact that the bushy class has a more complex shape and edges that are being lost by the enhancement noise compared to the simpler leafy class. In addition, the images of CUPDD are severely degraded with very low visibility explaining the low Original performance. Surprisingly the PCDE achieves very close overall performance to the Original detector although we have discussed in section IV how the PCDE produces very low-quality and non-visually pleasing images. On the contrary, the ICSP has a severely deteriorated mAP performance at 0.22 aligning with the severe deterioration in its enhancement"}, {"title": "B. Qualitative Evaluation", "content": "The inference from the original and domain detectors on five random testing images from both CUPDD and RUOD is provided Figs. 6 and 7. For a more convenient evaluation, we opt for a confidence threshold of 0.5. As shown in Fig. 6, The original model achieves an acceptable top performance with most objects getting detected correctly apart from the second and last images where some objects got missed resulting in False Negatives (FN) and other objects being confused resulting in False Positives (FP). With domain detectors, we notice that the detected bounding boxes, even if True Positive (TP), are not as accurate as those from the Original images. Since we are using the mAP50_95, a lesser Intersection over Union (IoU) with the ground truth will greatly impact the detection performance. This could be attributed to the enhancement diffusing the edges and introducing noise on some occasions. All models fairly performed well except for the PCDE and ICSP which have very foggy or overexposed images with little to no details present. In the second image, we observe an interesting case where the Original domain detector was not able to detect the Leafy plant but most domain detectors were able to detect at least parts of the Leafy plant. Although such detected bounding boxes would still not be considered as TPs because the IoU is still less than 0.5, such case might reveal hidden potential in image enhancement for detection. Another interesting case is in the third image where the Semi-UIR and the USUIR detected a leafy plant that is not in the ground truth. This encouraged us to further investigate such cases as we will present in Sec. VI. On the Other hand, similar trends are found in Fig. 7 of RUOD dataset where the Original detector outperformed domain detectors in most images. Some models, although not producing visually pleasing images, were still able to achieve good performance such as the ACDC. In contrast, some models that produce visually pleasing images perform poorer than other domain detectors. The TEBCF images are very noisy resulting in a higher number of FPs compared to other models. The third image is a special case where most domain detectors outperformed the Original detector by a large margin. We conclude that: 1) visually pleasing images do not necessarily lead to higher detection performance, and 2) image enhancement can increase the detection performance in some cases."}, {"title": "VI. ENHANCEMENT-DETECTION DISCUSSION", "content": "In this section, we provide a combined enhancement- detection analysis. In particular, we study the correlation between the enhancement metrics and the mAP. In addition, we conduct a further subjective analysis and provide cases where enhancement improves detection performance. To this end, we generate scatter plots for the mean enhancement metrics and the mean mAP for all selected models and for both datasets as shown in Fig. 8. Since the Q-index is made up of the four selected metrics, it will show similar results, hence, we opted for a more detailed analysis of individual metrics. The prevailing noticeable trend in Fig. 8 across all metrics is that the increase in enhancement metrics is not leading to an increase in the mAP. In fact, enhancement metrics and the mAP have a slightly inverse relationship. For example, the mAP maintained similar values with the increase in the UIQM for CUPDD while slightly decreased by the increase of the UIQM in RUOD. Furthermore, some enhancement models achieved lower scores on some metrics compared to the original, yet performed similarly in terms of the mAP. For instance, the PCDE, ICSP, AutoEnh, TUDA, BayesRet and the Semi-UIR all performed worse than the original in terms of the CCF on RUOD dataset. The ICSP model is an outlier on most plots and gained lower enhancement and mAP values due to its extremely overexposed images. We, conclude that enhancement performance does not predict detection performance. This discrepancy can be attributed to two primary factors: 1) enhancement metrics are sensitive to noise and may not always align with human perception and 2) object detectors rely on different visual cues than humans to identify objects. To further investigate the detector's behavior, we conduct a further subjective analysis of the detection inference results. This time we only look at cases where a domain detector performed better than the Original as shown in Figs. 9 and 10. Starting with CUPDD in Fig. 9, the ACDC increased the contrast and removed the color cast of the Original image resulting in an image that is not necessarily visually pleasing but increased the detection performance with a much tighter box around the ground truth object. The TEBCF, BayesRet and TUDA achieved a good enhancement performance with clearer object's edges and turned an FP into a TP. Surprisingly, the ICSP and PCDE also had cases where they enhanced the detection performance despite their overexposed or foggy images. Moreover, the AutoEnh, Semi-UIR, and USUIR had similar performance and recognized undetected objects. On the other hand, Similar trends can be seen in Fig. 10 for RUOD dataset. It is worth noting that some models enhance one as- pect of the image and deteriorate others but still achieve better detection performance. For instance, the TEBCF removed the color cast of the image but reduced too much noise in the process but was able to detect the diver object in the image anyway. This could explained by the diver class being one of the easiest and most distinct classes to detect in RUOD dataset. The TUDA produced a very visually pleasing image making it easier for its domain detector to distinguish the turtle from the underlying rocks. Since most of the presented cases in Figs. 9 and 10 belong to low Q-index bins, we hypothesize that enhancement for low quality images could increase the detection performance compared to enhancement of the entire dataset. We conclude that enhancement has the potential to increase detection performance. This improvement may be due to the fact that the enhancement process reduces the disparity between low and high-quality images, as discussed in Section IV-B. i.e., By making low-quality, hard-to-detect testing scenes more similar to high-quality easy-to-detect training scenes within the same dataset, enhancement can facilitate better detection outcomes. This finding contradicts previous studies [17], [19], [16], [18] which suggested that enhancement negatively impacts detection performance. While such negative impacts may be observed when evaluating overall enhance- ment performance, our evaluation demonstrates that specific cases exist where enhancement can indeed boost detection performance. Therefore, we recommend a more detailed quan- titative analysis to explore these effects further. Finally, during our investigation, we discovered cases where the enhancers revealed objects that were hard to see in the original images and were missed by the human labelers but got detected by the domain detectors and rarely by the original detector. Unfortunately, those instances were flagged as false positives (FPs) by the detectors despite being actual true positives due to wrong ground truth as shown in Figs. 11 and 12. For example, the ACDC revealed a Bushy plant in the background while the TEBCF revealed a hidden Leafy plant in the background in Fig. 11 with similar trends across all classes and domain detectors. Clearer examples can be seen in Fig. 12 for RUOD dataset where many Scalop, Echinus, starfish, and other objects were revealed from the background and detected by the domain detectors. For instance, the AutoEnh was able to reveal and detect a Holothurian object that was almost impossible to see in the original image. The BayesRet detects an extra fish in the upper background despite producing a worse image than the original in color, contrast, and sharpness. We believe this is because the BayesRet provided a similar noisy scene for its domain detector in the training data. Some cases are presented in Figs. 11 and 12 exhibit high Q-index, while others show low Q-index. Therefore, we differentiate between two types of annotation errors: those caused by severe degradation of the underwater scene, and those due to complex underwater scenes. This distinction is more evident in the BayesRet, PCDE, Semi-UIR cases of RUOD dataset, as shown in Fig. 12. We conclude that enhancement should be utilized at an earlier stage in the enhancement-detection framework, specifically during the annotation phase. This approach would help annotators to recognize hidden objects. In addition, while enhancement can make it easier for the human annotator to avoid missing objects, sometimes simply running a detector on the Original images can reveal unnoticed objects. This finding suggests an addition to the claim made by the authors in [18] that enhancement is increasing the false positive rate by tampering with the objects' edges, which are essential"}, {"title": "VII. CONCLUSION", "content": "In this work, we conducted a comprehensive investigation of underwater image enhancement methods and their effects on object detection. Our analysis included a quantitative and qualitative evaluation of different SOTA enhancement models, each representing a distinct enhancement category using two datasets, namely RUOD and CUPDD. A Quality index (Q- index) was proposed to assess the quality distribution of the images produced by these models. Subsequently, separate ob- ject detection models were trained and tested on enhanced and original images. A correlation study is also conducted between image quality and detection performance. Our findings reveal that enhancement significantly improves the quality of low- quality images but detrimentally impacts high-quality images. In addition, we found no significant relationship between enhancement metrics and detection scores. Although the detec- tion model of the original non-enhanced images outperformed the detection models of the enhanced images, we revealed that enhanced images sometimes outperformed the original"}]}