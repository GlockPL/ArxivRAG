{"title": "A Comprehensive Framework for Evaluating API-oriented Code Generation in Large Language Models", "authors": ["YIXI WU", "PENGFEI HE", "ZEHAO WANG", "SHAOWEI WANG", "YUAN TIAN", "TSE-HSUN (PETER) CHEN"], "abstract": "Large language models (LLMs) like GitHub Copilot and ChatGPT have emerged as powerful tools for code\ngeneration, significantly enhancing productivity and accelerating software development. However, existing\nbenchmarks primarily focus on general code generation without considering API-oriented code generation,\ni.e., generating code that invokes APIs from specific libraries. Given the growing demand for API-oriented code\ngeneration, there is a pressing need for a systematic and automated approach to evaluate LLM on API-oriented\ncode generation. To address this gap, we propose AUTOAPIEVAL, a lightweight and automated framework\ndesigned to evaluate the capabilities of LLMs in API-oriented code generation. Our framework works with\nany library that provides API documentation and focuses on two unit tasks: API recommendation and code\nexample generation, along with four metrics to evaluate the generated APIs and code examples, such as the\nproportion of incorrect API recommendations for Task 1, and the proportion of code examples where no\nspecific API is invoked and uncompilable/unexecutable code examples for Task 2. In addition, we conducted a\ncase study on three LLMs (ChatGPT, MagiCoder, and DeepSeek Coder) and Java Runtime Environment 8 to\ndemonstrate the framework's effectiveness. Our findings reveal substantial variability in LLM performance\nacross tasks, with ChatGPT adhering better to instructions, while sharing similar effectiveness in code example\ngeneration with its counterparts (i.e., MagiCoder and DeekSeek Coder). We also identify key factors associated\nwith code quality, such as API popularity and model confidence, and build classifiers that achieve high accuracy\nin detecting incorrect API recommendations and erroneous code examples. Retrieval-augmented generation\nenhances the quality of code generated by LLMs, though its effectiveness varies across different LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "With the technological advancements in AI, various large language models (LLMs) have been devel-\noped for code-related tasks [9, 26, 37, 41, 42, 70, 71], such as GitHub Copilot [3] and ChatGPT [48].\nThose LLMs have significantly propelled the field of code generation. When used correctly, those\nLLMs can significantly enhance productivity and accelerate software development through various\ntasks [14, 51, 54]. To address the growing demand for LLM-based code generation, prior studies have\ndevoted substantial effort to evaluating and understanding the effectiveness of LLMs in generating\ncode [4, 19, 20, 38, 63]. For instance, to assess LLM's ability of code generation, researchers proposed\nvarious code generation benchmarks (e.g., HumanEval [4] and ClassEval [20]). These benchmarks\nrely on a set of pre-defined tests to evaluate the correctness of the generated code.\nAccording to the latest survey on developers' perspectives regarding code generation tools [11],\ndevelopers frequently expect these tools to incorporate more contextual knowledge, particularly by\nleveraging specific libraries. They emphasize the importance of API-oriented code generation, where\ntools should be capable of generating code that invokes APIs from specific libraries. However, the\nmajority of the existing studies [4, 19, 20, 33, 38, 63] focus on the general code generation based on\nspecific functional requirements described in natural language, without specifying APIs. Little work\nhas been done to assess the quality of API-oriented code generated by LLMs. While some research\nhas explored API-oriented code generation for specific libraries [39, 79], these studies typically\nderive their tasks from Stack Overflow or general code generation benchmarks, which involve APIs\nfrom the target libraries (e.g., Pandas). They then manually create test cases to verify if the generated\ncode meets the required functionality. However, a key limitation of these approaches is that the\ncoverage of tested APIs is limited and cannot be scaled to a broader range of libraries due to the\nmanual effort required for test creation and the difficulty of collecting relevant tasks for the target\nlibraries. Hence, a systematic and automated approach is needed to evaluate API-oriented code\ngeneration by LLMs for a broad range of libraries. Enabling the assessment of API-oriented code\ngeneration can help practitioners evaluate and analyze the code generation capabilities of LLMs for\nspecific libraries, thereby offering new insights and solutions to enhance their performance.\nTo bridge this gap, we propose a lightweight framework AutoAPIEVAL, which enables automatic\nand systematic evaluation of LLMs on API-oriented code generation. AutoAPIEVAL works with any\nlibrary that has API documentation. We consider four requirements when designing AutoAPIEVAL:\nFirst, it should be fully automated to enable scalability, unlike existing benchmarks that require\nmanually crafted test cases [4, 20]. Second, it should be applicable to a wide range of libraries,\nleveraging easily accessible datasets such as API documentation. Third, the framework's tasks\nmust be simple enough for LLMs to understand without complex prompt engineering, ensuring\nconsistent benchmarking [27]. Lastly, the framework should mimic how developers interact with\nLLMs [28], helping them identify available APIs and providing examples of working code.\nTo fulfill the requirements, we design two unit tasks in AUTOAPIEVAL to benchmark LLMs' ability\nin API-oriented code generation, using only API documentation as input. We refer to them as\n\u201cunit tasks\u201d because, similar to unit tests, they are performed independently on each class within a\npackage, allowing for focused evaluation of the LLM's performance at the class level. The tasks are:\n1) API recommendation, and 2) code example generation, evaluated using four specific metrics.\nSpecifically, in Task 1, given a library, we iteratively query the LLM to recommend a list of APIs for\neach class (i.e., unit) in the library. In Task 2, we query the LLM to generate code examples for a\ngiven API. We propose four evaluation metrics to evaluate the API and code examples generated by\nLLMs, such as the proportion of incorrect APIs for Task 1, the proportion of code examples where no\nspecific API is invoked, and uncompilable/unexecutable code examples for Task 2. Our framework\nenables further analysis to 1) understand errors that occur in the API recommendation and code"}, {"title": "2 AUTOAPIEVAL: FRAMEWORK FOR EVALUATING API-ORIENTED CODE\nGENERATION", "content": "We propose a lightweight framework named AUTOAPIEVAL. This framework enables the automatic\nand systematic evaluation of LLMs for API-oriented code generation, which is applicable to any\nspecific library and requires only API documentation as input.\nWe consider four key requirements when designing the framework. First, Fully Automated:\nThe evaluation must be conducted automatically. Without full automation, scaling to new libraries\nis impractical. Existing benchmarks, such as HumanEval [4] and ClassEval [20], rely on predefined\ntest cases to assess the correctness of generated code. While these benchmarks are meticulously\ncrafted, creating test cases requires substantial manual effort, making it challenging to extend them"}, {"title": "2.1 Tasks", "content": "Given a library, in Task 1, the LLM is iteratively prompted to recommend methods for each class in\nthe library. In Task 2, the LLM generates code examples for a specified method in the given library.\nNote that Task 2 is only applied to methods that were successfully recommended by the LLM in\nTask 1, as we assume that the LLM has adequate knowledge of these APIs. We elaborate on those\ntwo tasks in the following sub-sections."}, {"title": "2.1.1 Task 1: API Recommendation.", "content": "For Task 1, we test LLM's knowledge of the available APIs in a\nspecific library. Therefore, we prompt the LLM to recommend APIs (methods) in a given class in the\nspecific library. As revealed by prior studies, hallucinations are common in code generation [32, 63],\nsuch as generating non-existing and fake APIs. Therefore, we then examine if the recommended\nAPIs by LLM are deemed in the class. We design our prompt template for Task 1 as follows."}, {"title": "2.1.2 Task 2: API-oriented Code Example Generation.", "content": "For Task 2, we instruct the LLM to generate\na code example for a given API (i.e., method). We present the prompt template for Task 2 below:"}, {"title": "2.2 Evaluation Metrics", "content": "For Task 1, our goal is to assess whether LLMs can accurately recommend APIs within a specified\npackage. To evaluate the quality of these recommendations, we compute the proportion of incor-\nrectly recommended APIs relative to the total recommended APIs, denoted as IncorrectAPI. This\nAll recommended APIs\nmetric is calculated as . A lower value indicates that the LLM provides more\naccurate API recommendations, reflecting higher quality. A recommended API is considered correct\nonly if it exists within the specified package and all elements of its signature (i.e., return type,\nmethod name, number of parameters, and parameter types) match exactly with the corresponding\nAPI in the package. To compute this metric, we cross-reference the actual APIs in the package to\nensure that the recommended APIs not only exist but also have signatures that precisely match our\nrecords. It is important to note that if an LLM recommends incorrect APIs, this can be considered a\nform of hallucination, as the recommended APIs do not exist within the package.\nFor Task 2, we propose three metrics to evaluate the quality of the generated code examples,\neach capturing a different type of error:"}, {"title": "\u2022 NoAPIInvoked:", "content": "This metric assesses whether the specified API is invoked in the gener-\nated code example. When the LLM fails to include the requested API, it be categorized as\ninstruction inconsistency hallucinations, where the model does not follow the given instruc-\ntion [30]. For instance, consider a scenario where the model is asked to generate a code ex-\nample for the 'getDecoder()' method from 'java.util.Base64', but the method is absent in the\ngenerated code. This situation would be classified as a NoAPIInvoked case. The metric is\nNoAPIInvoked\ncalculated as the proportion of examples where the specified API is not invoked, defined as\n|All recommended Code|\n|All recommended Code| where |All recommended Code| represents the total number of generated\nexamples, and |NoAPIInvoked| is the number of examples where the API is missing."}, {"title": "\u2022 Uncompilable:", "content": "After excluding the NoAPIUsed cases, we examine whether the generated code\nexamples can be compiled successfully. We calculate the proportion of code examples that\nare not compilable with compilation errors, denoted as Uncompilable, which is defined as\nUnCompilable\n|All recommended Code|\n|UnCompilable| is the number of uncompilable code examples."}, {"title": "\u2022 Unexecutable:", "content": "We calculate the proportion of code examples that cannot be executed with\nruntime errors while can be compiled successfully, be denoted as Unexecutable. It is calculated\nUnexecutable\nas |All recommended Code| where |Unexecutable| is the number of Unexecutable code examples."}, {"title": "3 CASE STUDY DESIGN", "content": "In this section, we illustrate a case study, where we utilize our framework AutOAPIEVAL to perform\nan empirical study to evaluate and understand API-oriented code generation by LLMs. We present\nour research questions (RQs), dataset, LLMs, and approach for each RQ in the following subsections."}, {"title": "3.1 Research Questions", "content": "We conduct our case study around four research questions:\n\u2022 Quality Assessment - RQ1: What is the quality of API-oriented code generation by LLMs?\n\u2022 Error Analysis - RQ2: What types of errors occur in API-oriented code generation?\n\u2022 Factor Analysis - RQ3: What factors are associated with the quality of API-oriented code\ngeneration by LLMs?\n\u2022 Error Mitigation - RQ4: Does RAG help mitigate the errors in API-oriented code generation?\nIn RQ1, we investigate the quality of API-oriented code generated by LLMs, including both the\nrecommended APIs and the generated code examples. In RQ2, we focus on understanding the\ntypes of errors that occur in the APIs and code examples produced by LLMs. This analysis provides\ninsights into the strengths and limitations of different LLMs in generating API-oriented code. In\nRQ3, we explore the factors that may be associated with the quality of API-oriented code generated\nby LLMs. Understanding these factors can offer insights into improving LLM performance and help\ndevelop models to predict low-quality code. Lastly, Retrieval-Augmented-Generation (RAG) has\nbeen shown to enhance code generation [8, 15, 52]. Accordingly, in RQ4, we investigate whether\nRAG can help reduce errors and improve the quality of API-oriented code generated by LLMs."}, {"title": "3.2 Datasets", "content": "In this case study, we focus on all packages from the Java Runtime Environment 8 (JRE 8), released\non July 19, 2016 [49]. We chose JRE packages due to the widespread usage of their APIs in code\nrepositories, such as those hosted on GitHub. Since these open-source repositories are frequently\nused to train LLMs, it is likely that the models have substantial knowledge of these APIs. To collect\nall packages and APIs in each package, we crawled the API documents of JRE 8 from its official\nwebsite [49] and stored them in our database. Our dataset consists of 217 packages, comprising\n2,397 classes. For Task 1, we generate prompts based on a predefined template for each class, and\nfor Task 2, we construct prompts using the APIs correctly recommended in Task 1.\nTo determine whether a code example is compilable and executable, we compile and run the Java\nfile as a subprocess from our main script and collect any errors. Furthermore, we have implemented\na timeout mechanism to terminate subprocesses that exceed a threshold. Specifically, if the code\nsnippet runs for over 15 seconds, it is automatically terminated and marked as a failure."}, {"title": "3.3 Base LLMs", "content": "In our study, we employed three different LLMs as the base models: ChatGPT [48], Magicoder [71],\nand DeepSeek Coder [16]. These models were chosen due to their representation of both commercial\ngeneral-purpose LLMs and open-source LLMs specialized for code tasks, as well as their ranking as\ntop performers in code generation tasks\u00b9.\nFor ChatGPT, we use gpt-3.5-turbo [48] for state-of-the-art capabilities in both general-purpose\nnatural language processing tasks and code generation [18, 77]. For MagiCoder, we use Magicoder-\nS-DS-6.7B [5], which is trained specifically for designed for code generation and coding-related\ntasks. DeepSeek Coder [2], built upon Deepseek-LLM 7B, was chosen for its superior ability to\nsolve code-related tasks. For all LLMs, we set the temperature to 0.6 by following, and all other\nhyper-parameters were maintained at their default values.\nWe conducted our analysis on all the three LLMs for RQ1. Since the performance of MagiCoder\nand DeepSeek Coder are similar as shown in the results of RQ1 (ref. Table 2), we conducted our\nexperiments and analysis on MagiCoder and ChatGPT for the rest RQs (RQ2-RQ4)."}, {"title": "3.4 Approaches for RQs", "content": "In RQ1, we perform the two unit tasks on each of the 2,397 target\nclasses extracted from JRE 8 leveraging three selected LLMs. We record the inference output from\neach prompt and extract the generated code. We then evaluate the quality of generated code using\nthe metrics outlined in Section 2.2 for both tasks. We repeated each task 10 times to reduce the bias\nfrom randomness."}, {"title": "3.4.2 RQ2 - Error Analysis.", "content": "For Task 1 - API recommendation, as discussed in Section 2.2, we\nclassify any recommended APIs that do not exist within the given package as incorrect, which can\nbe considered hallucinations generated by the LLM. Consequently, we adopt an established catego-\nrization scheme from prior hallucination studies [30] to classify the errors in API recommendations\nas follows:\n\u2022 Factual Fabrication: The recommended API is entirely fabricated, meaning the API name\ndoes not exist in the specified package.\n\u2022 Factual Inconsistency: Unlike factual fabrication, the recommended API name does exist;\nhowever, other elements in the signature (e.g., return type or parameters) are incorrect."}, {"title": "3.4.3 RQ3 - Factors Analysis.", "content": "In this RQ, we aim to investigate the factors at the API level that may\nbe associated with the quality of code generation. Specifically, for Task 1, we seek to investigate\nfactors that are associated with whether a recommended API is correct. For Task 2, we focus on\nunderstanding the factors that are associated with the erroneous code examples (i.e., the case where\nis either noAPIinvoked, unexecutable, or uncompilable).\nWe examine these factors from two perspectives: 1) the API itself and 2) the model used. From\nthe API perspective, we consider two factors: the API's popularity (i.e., API_popularity) and its\nlength (i.e., API_length). From the model perspective, we analyze three factors: Self-Probing (i.e.,\nProbing), Perplexity (i.e., PPL) and self-consistency (i.e., Consistency). Each of these factors is\ndiscussed in more detail below.\nAPI_popularity: LLMs are trained on public datasets, so APIs that are widely used are more\nlikely to have substantial representation in the training data. Hence, an LLM is more likely to\ngenerate high-quality code for these popular APIs. To measure the popularity of APIs across\nGitHub repositories, we employ Google BigQuery [24] to analyze the frequency of API package\nimports. Developers commonly import specific packages to access corresponding APIs. By querying\nBigQuery's public GitHub dataset [29], we quantify API popularity by counting the number of\nrepositories referencing each package, offering a reliable metric for API usage across GitHub."}, {"title": "3.4.4 RQ4 - Error mitigation.", "content": "Retrieval-augmented generation (RAG) has been shown to enhance\ncode generation by integrating relevant external knowledge into the language model's context [8,\n15, 40, 47, 52, 65]. We aim to investigate whether employing RAG can improve API recommendation\nand code example generation. For Task 1, we enrich the context provided to the LLM by including\ndescriptions of both the package and the class in front of the \u201cInstruction\u201d section in the prompt\ntemplate, as detailed in Section 2.1.1. We denote this RAG strategy as RAGdesc. For example, when\nrequesting API recommendations for the class \u201cHashTable\u201d in the \u201cjava.util\u201d package, we prepend\nthe prompt with relevant descriptions: \u201cPackage description: package description of java.util; Class\ndescription: class description of HashTable\u201d. In addition, we explore a variant of RAGdesc, where we\nadd a list of existing APIs within the class as the additional context to test if this further improves the\nLLM's ability to recommend correct APIs, when the existing correct APIs are actually provided. We\ndenote this RAG strategy as RAGdesc+API. In this study, we provide a list of 10 APIs. For Task 2, we\nextend the context used in Task 1 by adding a detailed description of the specific API, including its\nsummary, return type, and input parameters. Our goal is to determine if this additional information\nimproves the quality of code example generation. We denote the RAG strategy as RAGdesc."}, {"title": "4 RESULTS OF CASE STUDY", "content": ""}, {"title": "4.1 RQ1 - Quality Assessment", "content": "Hallucinations are prevalent in the API recommendation task, with 58.1% to 84.1% of the\nrecommended APIs not existing in the specified package. Table 2 summarizes the quality of\nrecommended APIs for Task 1. Specifically, 84.1%, 82.9%, and 58.1% of the recommended APIs from\nMagiCoder, DeepSeek Coder, and ChatGPT, respectively, do not exist in the specified package. In\naddition, we analyze the errors that occurred within incorrect API recommendations by analyzing\nwhere the errors occurred in the method signature. Table 3 presents the types of errors produced\nby the three LLMs. 1.8% - 15.6% of the errors are due to not recommending method APIs (i.e.,\nNotMethod), where LLMs suggested other class elements, such as fields, instead of methods in\nthese cases. The majority of the errors involved recommending method names that do not exist.\nThe remaining errors were attributed to incorrect return types or parameters (Incorrect Return-\nType/Parameter). Notably, we only assessed the correctness of return types and parameters when\nthe method name was accurate. Therefore, it is possible that multiple errors could occur in different\nparts of a single recommendation. Interestingly, among the Incorrect Return/Parameter cases, a\nremarkable portion of errors (85.6% - 86.2%) resulted from combining return types and parameters\nfrom multiple overloaded methods. For example, the method \"boolean remove(Object o)\" was rec-\nommended by MagiCoder for the java.util.Hashtable class. However, only two overloaded methods\n\u201cV remove(Object key)\u201d and \u201cboolean remove(Object key, Object value)\u201d exist in this class. This\nis likely because of the common scenario of method overloading in Java, making LLMs confused\nwhen recommending API methods."}, {"title": "4.2 RQ2 - Error Analysis", "content": "For the API recommendation task, most errors are due to Factual Fabrication Halluci-\nnations (46.0%/51.3%), followed by Instruction Inconsistencies (30.0%/31.5%), and finally,\nFactual Inconsistencies for both MagiCoder and ChatGPT. Table 4 presents the distribution\nof error types for Task 1. A similar trend is observed for both MagiCoder and ChatGPT. The\nmajority of errors stem from Factual Fabrication. For example, when asked to recommend APIs for\n\"java.util.Arrays\u201d, the LLM suggested \u201ccreateCompatibleGraphics()\u201d, which does not exist. Upon\nreviewing the API documentation, we found a similar method, \u201ccreateCompatibleImage()\u201d, indi-\ncating that the LLM likely confused the terms \u2018Graphics\u201d and \u201cImage\u201d after generating the prefix\n\u201ccreateCompatible\u201d. The second most common error is Instruction Inconsistency. For instance, the\nLLM often disregards the required format, omitting return types or parameters for certain static\""}, {"title": "4.3 RQ3 - Factors Analysis", "content": "For both tasks, all studied factors show a significant difference between the two groups of\ngenerated APIs/code examples. Table 5 presents the statistical test results on the studied factors\nfor Task 1 and Task 2. For Task 1, in both LLMs, all factors exhibit significant differences between\nthe two groups of APIs with non-negligible effect sizes with non-negligible effect size, except\nProbing on MagiCoder. For example, the popularity of APIincorrect is substantially lower than that\nof APIcorrect with a large effect size in both LLMs. This aligns with the expectation that a more\npopular API, which is likely to have more related usage in LLMs' training data, increases the model's"}, {"title": "4.4 RQ4 - Error Mitigation", "content": "In general, RAG improves the quality of generated Code by LLMs, while RAG's improve-\nments differ for different LLMs. Table 7 compares the quality of code generated by LLMs\nwith and without using RAG. Across both tasks, RAG improves code quality when MagiCoder\nand ChatGPT are used as the base LLMs. However, the magnitude of these improvements differs\nbetween the two models. Notably, RAG brings more substantial improvements for ChatGPT than\nfor MagiCoder. For example, in Task 2, RAG reduces TotalError from 44.4% to 43.2% for MagiCoder,\na modest improvement of 2.7%. In contrast, for ChatGPT, RAG decreases IncorrectAPI from 57.3%\nto 30.8%, representing a much larger improvement of 39.6%.\nFor Task 1, it is surprising that even when provided with a list of correct APIs in the\ncontext, the LLMs still fail to recommend APIs accurately. As shown in Table 7, despite\nhaving the correct APIs listed along with the context, LLMs still make a significant amount of\nerrors in their recommendations. Specifically, 40.3% of the APIs recommended by MagiCoder and\n27.9% by ChatGPT do not exist in the specified package. This is unexpected, as the task should be\nstraightforward - selecting from the provided list of correct APIs. One possible explanation is that\nLLMs sometimes disregard the given context and rely instead on their internal knowledge that is\nencapsulated in the model [44, 64]."}, {"title": "5 DISCUSSION", "content": ""}, {"title": "5.1 Implications of our findings", "content": "Our research identifies model-related indicators for predicting incorrect API-oriented\ncode generation by LLMs. As shown in RQ4, our proposed factors could be used to build well-\nperformed classifiers to identify low-quality API-oriented code generated by LLMs. More specifically,\nmodel-related factors (e.g., Consistency) are strongly correlated with the quality of APIs and code\nexamples generated by LLMs in both tasks. The importance scores from the constructed models\nalso highlight that PPL and Consistency are critical factors. Therefore, model-related factors could\nserve as indicators of an LLM's capability for API-oriented code generation for a specific library.\nFor example, developers could directly probe the LLM by asking if it knows the library or its APIs\nand observe the PPL of output. Actually, we test the models that are only built with model-related\nfactors, it achieves an F1-score of 0.96 and 0.63 for Task 1 and Task 2, respectively.\nHallucinations are prevalent in API-oriented code generation, and future research\nis encouraged to mitigate these issues. As observed in RQ1 and RQ2, various hallucinations\noccur across both tasks. For example, RQ1 shows that 4.0% to 9.9% of the generated code examples\nby LLMs do not include the specified APIs, consistent with findings from previous studies [63].\nFactual Fabrication and Factual Inconsistency are the most frequent types of hallucinations, where\nfabricated APIs are generated. Our study suggests that these hallucinations may stem from factors\nsuch as a lack of training data and confusion over overloaded methods. Future research should\nexplore methods to mitigate hallucinations in API-oriented code generation. Approaches from\nthe NLP field, like RAG, fine-tuning, and self-reflection [30], could be adapted for this context.\nFor instance, RAG appears promising, as indicated by the results in RQ5, where it reduced errors.\nAnother direction is enhancing self-reflection with fact-checking, as RQ4 shows that self-probing\ncan be a good indicator of poor code generation. Additionally, API documentation and runtime\nresults could provide valuable information for quick fact-checking of LLM-generated code [32].\nFuture research could develop approaches that combine self-reflection and fact-checking to reduce\nhallucinations (e.g., Chain-of-Verification [17]).\nFuture research is strongly encouraged to develop more effective approaches to leverage\nthe API-related information in RAG. As we observed in RQ5, even providing a list of APIs in the\ncontext, LLMs cannot recommend fully correct APIs, although it reduces the proportion of incorrect\nAPIs. One possible reason is that LLMs sometimes disregard the given context (context knowledge),\nand only rely on their parametric knowledge, which is encapsulated in LLM's parameters, when\nthe context knowledge and parametric knowledge conflict as prior studies reported, typically when\nthe prompt is long [44, 62, 64]. Future research is strongly encouraged to develop more effective\nRAG approaches to leverage external knowledge (e.g., API documentation) to mitigate errors. For\ninstance, approaches that align with external knowledge and emphasize context prioritization, such"}, {"title": "5.2 Threats to validity", "content": "Internal Validity Prompt engineering has a significant impact on the LLM's performance [25].\nDifferent prompts probably can lead to different results. However, as we discussed in Section 2, our\ntasks are basic and straightforward, LLMs usually can follow the instructions specified in prompts\nto complete our tasks easily as the results in Section 4. In our framework, we propose two basic\ntasks, i.e., API recommendation and code examples generation, to benchmark an LLM's ability\nof API-oriented code generation. One threat is that an LLM's ability in our designed two tasks\nprobably does not closely align with the LLM's ability to generate code for a specific task. However,\nas discussed in section 2, the goal of our framework is to evaluate LLMs on any given library with\nAPI documentation automatically. Therefore, we do not include tasks such as code generation\nwith specific requirements which typically need test cases in our framework. We believe our\nframework provides a lower boundary to assess LLMs' capability for API-oriented code generation.\nNevertheless, we encourage future research to include more tasks to reflect the LLMs' ability to\ngenerate code using specific libraries with specific requirements. Previous studies suggest that\nLLM settings, such as temperature and decoding strategies, can significantly affect the quality of\ngenerated content [59, 66]. In this study, we use default settings for the studied LLMs for all RQs.\nHowever, our framework enables such analysis and we examined actually whether different LLM\nsettings such as different temperatures and different decoding strategies (i.e., beam search, top\nK, and greedy search) have a measurable impact on the quality of generated code (due to space\nlimit, we do not present here). In general, a lower temperature tends to produce code of similar\nor higher quality for both tasks and across both LLMs. Greedy Search, Beam Search, and Top-K\nshare similar performance. Another threat is that certain APIs are version version-sensitive. We\nencourage future work to take this into consideration when using our framework for evaluation.\nExternal Validity relates to the generalizability of our findings. Even though we conducted our\nempirical study on three different state-of-the-art LLMs (i.e., ChatGPT, MagiCoder, and DeepSeek\nCoder) and JRE 8, our findings may not generalize well to other LLMs and libraries. We propose a\nframework to enable automatic and systematical analysis on other LLMs and libraries and encourage\nfuture research on more LLMs and libraries."}, {"title": "6 RELATED WORK", "content": "Code recommendation and generation with LLM. In recent years, there has been increas-\ning interest in using Large Language Models (LLMs) for generating code from natural language\nprompts [9, 37, 41, 42, 71]. Lu et al. initiated this field with CodeGPT, based on GPT-2 and specifi-\ncally trained on source code [41]. Chen et al. advanced this by fine-tuning GPT-3 models to create\nCodeX, which excels at generating both natural language and code [9]. More recent models, such\nas starCoder [37], WizardCoder [42], and MagiCoder [71], further enhance code generation capabil-\nities. In addition to generating code from natural language, integrating Application Programming\nInterfaces (APIs) is crucial. Although a few research has explored API-oriented code generation\nfor specific libraries [39, 79], most of these efforts primarily focused on developing LLM-based\napproaches to generate code that interacts with APIs. Several studies explored API integration\nduring code generation and revealed issues, such as license issues [34, 80] and hallucination [63].\nDifferent from prior studies, we focus on developing automated framework to evaluate LLMs on\nAPI-oriented code generation and enable further analysis, rather than analyzing the errors.\nBenchmarking for code generation. To evaluate the functional correctness of generated code,\nthe most effective method is to test its execution against predefined test cases. Several benchmarks"}, {"title": "7 CONCLUSION", "content": "We propose AutoAPIEVAL, a lightweight and automated framework for evaluating LLMs in API-\noriented code generation. Compatible with any library that provides API documentation, our\nframework focuses on two unit tasks: API recommendation and code example generation, along\nwith four evaluation metrics, including the proportion of incorrect API recommendations and the\nproportion of code examples where no specific API is invoked and uncompilable/unexecutable\ncode examples.\nTo demonstrate the framework's effectiveness, we conducted a case study with three LLMs\nChatGPT, MagiCoder, and DeepSeek Coder on JRE 8. Our findings show notable variability in LLM\nperformance across tasks, with ChatGPT generally following instructions better but generating more\nunexecutable code compared to the other models. We identify crucial factors that are associated with\ncode quality, such as API popularity and model confidence. We develop classifiers that achieve high\naccuracy in detecting low-quality API recommendations and code examples. Additionally, while\nretrieval-augmented generation improves code quality, its effectiveness varies between different\nLLMs. Our findings offer valuable insights for future research directions."}, {"title": "8 DATA AVAILABILITY", "content": "We have made our replication package available, which contains all the code and datasets available\nhere [7]."}]}