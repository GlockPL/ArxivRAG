{"title": "DDFAD: Dataset Distillation Framework for Audio Data", "authors": ["Wenbo Jiang", "Rui Zhang", "Hongwei Li", "Xiaoyuan Liu", "Haomiao Yang", "Shui Yu"], "abstract": "Deep neural networks (DNNs) have achieved significant success in numerous applications. The remarkable performance of DNNs is largely attributed to the availability of massive, high-quality training datasets. However, processing such massive training data requires huge computational and storage resources. Dataset distillation is a promising solution to this problem, offering the capability to compress a large dataset into a smaller distilled dataset. The model trained on the distilled dataset can achieve comparable performance to the model trained on the whole dataset.\nWhile dataset distillation has been demonstrated in image data, none have explored dataset distillation for audio data. In this work, for the first time, we propose a Dataset Distillation Framework for Audio Data (DDFAD). Specifically, we first propose the Fused Differential MFCC (FD-MFCC) as extracted features for audio data. After that, the FD-MFCC is distilled through the matching training trajectory distillation method. Finally, we propose an audio signal reconstruction algorithm based on the Griffin-Lim Algorithm to reconstruct the audio signal from the distilled FD-MFCC. Extensive experiments demonstrate the effectiveness of DDFAD on various audio datasets. In addition, we show that DDFAD has promising application prospects in many applications, such as continual learning and neural architecture search.", "sections": [{"title": "INTRODUCTION", "content": "DNNs have achieved remarkable performance in a wide range of applications. The superior performance of DNNs often requires large-scale training datasets. For example, the commonly used image classification dataset ImageNet [1] has about 1.2 million training samples and 100,000 testing samples, taking up about 148G of storage space; the commonly used object detection dataset COCO [2] has 118,287 training samples and 40,670 testing samples, taking up about 44G of storage space. However, the management of such massive data entails significant challenges, including the collection, storage, transmission, and pre-processing. Moreover, training models on such massive data comes with huge computational overhead. Such storage and computing requirements are huge and impractical for personal users. To address this problem, an emerging technology known as dataset distillation has garnered considerable research attention in recent years. As illustrated in Figure 1, dataset distillation extracts the knowledge from a large-scale dataset and generates a smaller synthetic distilled dataset. The models trained on the distilled dataset can achieve performance comparable to those trained on the original dataset. This technique can significantly decrease the computational resources of training a DNN."}, {"title": "RELATED WORK", "content": "Audio data classification is an important topic with many potential applications, such as speech classification in industrial automation, environmental sound classification in weather prediction, etc. Early works on audio classification used Support Vector Machines (SVM) [9], K-Nearest Neighbors (KNN) [10] and Hidden Markov Models (HMMs) [11] for classification. More recently, audio classification based on DNN is gradually becoming mainstream and achieving leading performance [12], [13], [14], [15]. Thus, we mainly focus on DNN-based audio classification methods in this work.\nIn addition to choosing different machine learning models for classification, the extraction of audio features is also critical for audio data classification. Concretely, audio features can broadly be categorized into time-domain, frequency-domain, and cepstral-domain features.\nTime-domain feature characterizes audio signals in relation to time. It directly uses the one-dimensional (1D) audio signals as the input of the classification model, which is computation-efficient. Notable time-domain features include Short-Time Energy (STE) [16], Zero Crossing Rate (ZCR) [17], Short-Time Autocorrelation Function (STAF) [18], etc.\nFrequency-domain feature refers to the characteristics of audio signals varying in frequency. It no longer uses the original 1D signals, but 2D signals (i.e., spectrograms) as the model input. Therefore, frequency-domain features contain more information compared to time-domain features. Common frequency domain features include Fourier Transform (FT) [19], Discrete Cosine Transform (DCT) [20], etc.\nCepstral-domain feature is mostly obtained by inverting some frequency-domain signals and their variants. Currently, the cepstral-domain feature is the most commonly used feature type and achieves leading performance in DNN-based audio classification. Representative cepstral-domain features include linear predictive cepstral coefficient (LPCC) [5], Mel frequency cepstral coefficient (MFCC) [6], etc."}, {"title": "Dataset Distillation", "content": "The concept of dataset distillation is originated from knowledge distillation [21]. Knowledge distillation is designed to transfer the knowledge of a large-scale model to a more lightweight model, whereas dataset distillation is a technique to distill the knowledge of the large-scale dataset a small-scale distilled dataset. Their underlying technologies are different.\nIn recent years, various dataset distillation algorithms have been proposed and they can be mainly categorized into three approaches: performance matching, parameter matching, and distribution matching.\nPerformance matching. This approach focuses on optimizing a distilled dataset to ensure that neural networks trained on it exhibit minimal loss on the original dataset, thereby achieving comparable performance between models trained on distilled and original datasets. Performance matching was first proposed by Wang et al. [22], who formulated dataset distillation as a bi-level optimization problem. However, the inner loops in their approach require extensive backpropagation gradient computation, which is highly inefficient. Subsequent works [23], [24], [25] proposed to replace the neural network in the inner loop with a kernel model, bypassing the backpropagation gradient computation process.\nParameter matching. This approach focuses on optimizing the consistency of trained model parameters between the distilled dataset and the original dataset. It was initially proposed by Zhao et al. [26], who formulated the objective as a minimization problem between two sets of gradients of the network parameters. Following [26], numerous algorithms [7], [27], [28], [29], [30] have been proposed to improve parameter matching. For example, Cazenavette et al. [7] proposed a multi-step parameter matching approach known as matching training trajectory (MTT); Zhao et al. [30] employed model augmentation techniques, such as utilizing early-stage models and parameter perturbation, to accelerate the training speed of dataset distillation.\nDistribution matching. Instead of matching training effects or model parameters, distribution matching focuses on obtaining a distilled dataset whose distribution closely approximates that of the original dataset. For instance, Zhao et al. [31] utilized the metric of Maximum Mean Discrepancy (MMD) metric to optimize the distance between the distribution of the distilled dataset and the original dataset; Wang et al. [32] proposed CAFE, which ensures that statistics of features for the distilled and original samples extracted by each network layer except the final one are consistent."}, {"title": "Coreset Selection", "content": "Coreset selection is another strategy to select a representative subset of the whole dataset through heuristic selection criteria. It is commonly used in active learning to identify training samples. For instance, random selection [33] picks samples arbitrarily; Herding selection [34] chooses samples closest to each class center; In K-Center selection [35], multiple center points are chosen for each class in order to minimize the maximum distance between data points and their nearest center point. However, these coreset selection methods may not always yield optimal results for downstream audio data classification task. Furthermore, identifying an informative coreset can be challenging, particularly when dataset information is not concentrated in a few samples. In contrast, the dataset distillation methods can achieves better result for downstream audio data classification task through generating synthetic distilled data."}, {"title": "METHODOLOGY", "content": "In this section, we present the details of DDFAD. It mainly contains three phases: feature extraction for audio data, dataset distillation for FD-MFCC and audio signal reconstruction. Below we describe the details of each phase."}, {"title": "FD-MFCC Feature Extraction", "content": "Currently, MFCC is the most commonly used feature type for audio data and achieves leading performance in DNN-based audio classification. Although MFCC feature performs well on the entire source dataset, in the case of dataset distillation, MFCC struggles to extract sufficient discriminative features to maintain the accuracy of the model trained on the distilled dataset. To overcome this limitation, we propose the Fused Differential MFCC (FD-MFCC), which fuses the features of MFCC, the first-order difference of MFCC and the second-order difference of MFCC. Since different difference orders of MFCC can represent features from different aspects, FD-MFCC can make full use of the complementarity between them and enhance the feature representation capability.\nAs illustrated in Figure 4, the feature extraction process of FD-MFCC consists of 9 steps:\n1) Pre-emphasis. This step compensates for the loss of the high-frequency part of the audio signal, which is beneficial for feature extraction.\n2) Frame Blocking. This step splits the audio data into small segments with a frame length of 20ms in order to ensure the smoothness of the audio signal.\n3) Windowing. This step enhances the strength of the middle part of the signal in each frame and weakens the discontinuities at the endpoints.\n4) FFT. FFT can transform time-domain signals into frequency-domain signals, which is conducive to obtaining more information in audio data.\n5) Mel Filter Banks. Since human ears are not sensitive to the low-frequency part of the audio signal, the Mel filter bank is designed to enhance the signal in the middle of the triangle wave region and weaken the signal on both sides, which is beneficial for feature extraction.\n6) Log. Since the perception of human ears grows in a logarithmic way, it also needs to take the logarithm of the obtained features to simulate the perception of human ears.\n7) DCT. The function of DCT is to remove the correlation between the signals of different orders and map the signals back into a lower dimensional space. The MFCC feature can be obtained by performing DCT on the Mel spectrum feature.\n8) The first-order and second-order difference of MFCC. After obtaining MFCC, we further calculate the first-order and second-order difference of MFCC to extract more features:\n$\\Delta MFCC(t) = \\frac{MFCC(t + 1) \u2013 MFCC(t)}{2}$,\n$\\\\Delta^2 MFCC(t) = \\frac{\\Delta MFCC(t+1) \u2013 \\Delta MFCC(t)}{2}$, where $MFCC(t)$ denotes the value of MFCC at time t; $\\Delta MFCC$ and $\\Delta^2 MFCC$ denote the first-order and second-order difference of MFCC, respectively.\n9) Feature Fusion. The MFCC, $\\Delta MFCC$ and $\\Delta^2 MFCC$ are spliced and fused together to get FD-MFCC.\nIn our experiments, we conduct extensive ablation studies to show the superiority of our proposed FD-MFCC compared with traditional MFCC (refer to Section 4.4 for more details)."}, {"title": "Dataset Distillation for FD-MFCC", "content": "Given the source training dataset $S$ with $|S|$ samples, the objective of dataset distillation is to extract the knowledge of $S$ into a small distilled dataset $D$ with $|D|$ samples ($|S| \\gg |D|$), and the model trained on $D$ can achieve comparable performance to the model trained on $S$.\nIn this work, we adopt the state-of-the-art (SOTA) matching training trajectory (MTT) distillation method [7] in DDFAD to distill FD-MFCC. Algorithm 1 illustrates the detailed process of dataset distillation for FD-MFCC. Specifically, MTT first trains models on $S$ and collects the trajectories of the model (referred to as the teacher model) in the buffer. Subsequently, ingredients in the buffer are randomly chosen to initialize the student model (the model trained on $D$). After collecting the trajectories of the student model, the distilled dataset is updated by matching the two training trajectories. The objective loss of MTT is defined as:\n$L_D = \\sum_{t=0}^{M-1} ||\\theta_S^{(t+N)} - \\theta_D^{(t+M)}||_2^2$, where $\\theta_S^{(t)}$ represents the parameter of the teacher model at training epoch $t$, which is stored in the buffer; $\\theta_D^{(t+N)}$ represents the parameter of the student model trained on $D$ for $N$ epochs with the initialization of $\\theta_S^{(t)}$."}, {"title": "Audio Signal Reconstruction from the Distilled FD-MFCC", "content": "After distilling the dataset of FD-MFCC, the final phase is to reconstruct the audio signal from the distilled FD-MFCC. We propose an audio signal reconstruction algorithm based on GLA to reconstruct the audio signal from the distilled FD-MFCC.\nAs presented in Algorithm 2, we begin by applying the inverse DCT (IDCT) to the distilled FD-MFCC to obtain the dB-scaled spectrogram. Subsequently, we employ the dB-to-power function\u00b2 to map the dB-scaled spectrogram to the mel power spectrogram. After that, we use the mel-to-stft function\u00b2 to approximate Short-Time Fourier Transform (STFT) magnitude from a Mel power spectrogram. Finally, we employ GLA [8] to reconstruct the audio signal from the STFT magnitude.\nConcretely, the objective of GLA is to reconstruct a spectrogram that is consistent with the given amplitude A. This is achieved through the following alternative projection procedure:\n$X[i] = P_C (P_A (X[i-1]))$, where $X[i]$ denotes the reconstructed audio signal at the ith iteration, $C$ is defined as the set of all spectrograms that corresponds to a time-domain signal, $A$ is defined as the set of all spectrograms that have the given magnitude spectrogram $A$. $P_{A,C}$ represents the projection onto set $A, C$ and they are defined as follows:\n$P_A(X) = A \\frac{X}{|X|}$,\n$P_C(X) = STFT(iSTFT(X))$, where STFT represents the short-time Fourier transform and iSTFT represents the inverse STFT. The reconstruction process is executed iteratively for I rounds to obtain the final reconstructed audio signal. The reconstruct distilled audio data can be seen in Figure 6."}, {"title": "EVALUATION", "content": "In this section, we evaluate the performance of DDFAD. It mainly contains three phases: feature extraction for audio data, dataset distillation for FD-MFCC and audio signal reconstruction."}, {"title": "Experimental Setup", "content": "We consider three model architectures, including ResNet18 [36], ConvNet (which mainly contains multiple Conv-ReLU-AvgPooling blocks) and VGG11 [37] for audio data classification."}, {"title": "Datasets", "content": "Free Spoken Digit Dataset (FSDD) [38]. FSDD comprises recordings of spoken digits at a sampling rate of 8kHz. It contains 3,000 audio data clips of English pronunciation of numbers (0-9) recorded by six speakers.\nUrbanSound-8k Dataset [4]. The UrbanSound dataset includes 8,732 labeled audio clips, each lasting up to 4 seconds. It covers 10 categories of urban sounds, such as air conditioner, car horn, drilling, etc.\nRyerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) [39]. The RAVDESS dataset contains 7,356 audio clips depicting seven different speech emotions, e.g., calm, happy, sad, etc."}, {"title": "Distillation Methods", "content": "In our experiments, we consider three SOTA distillation methods for experimental evaluations.\nDataset Condensation with Gradient Matching (DCGM) [26]. DCGM aims to minimize the gap between two sets of gradients of the network parameters, where the gradients are computed based on the training loss over both the original dataset and the distilled dataset.\nDataset Condensation with Differentiable Siamese Augmentation (DCDSA) [40]. DCDSA proposes a differentiable siamese augmentation method to synthesize distilled data to obtain better performance.\nMatching Training Trajectory (MTT) [7]. MTT first trains models on the source training dataset and collects the trajectories of the model in the buffer. After that, it also collects the trajectories of the network trained on the distilled dataset. The distilled dataset is updated by matching the two training trajectories.\nBesides, we also consider two coreset selection methods for comparison.\nRandom selection [33]. This is a simple approach that randomly selects samples as the coreset.\nHerding selection [34]. This is a distance-based algorithm that selects samples whose center is close to the center of each class."}, {"title": "Distillation Performance", "content": "We perform our DDFAD with five considered distillation methods (DCGM [26], DCDSA [40], MTT [7], Random selection [33], Herding selection [34]), to synthesize 1, 10, and 50 clips per class (CPC) respectively.\nAs presented in Table 1, dataset distillation methods are much more effective than coreset selection methods with the same CPC. This is because the dataset information can not be concentrated in a few samples. The synthetic distilled data can better represent the information of the whole dataset. Furthermore, among these dataset distillation methods, the DDFAD incorporating with the MTT outperforms other dataset distillation methods. Thus, for subsequent experiments, we adopt the MTT distillation method for evaluations of DDFAD.\nFor instance, in the case of the FSDD dataset with CPC=50, where the distilled dataset accounts for only 1/6 of the whole dataset, the test accuracies of DDFAD with MTT (97.30% for ResNet18, 89.67% for ConvNet and 94.38% for VGG11) are very close to the test accuracies of the model trained on the whole dataset (98.73% for ResNet18, 90.52% for ConvNet and 98.44% for VGG11). The storage space and computational resources required by the whole dataset is six times more than the storage space and computational resources required by the distilled dataset."}, {"title": "Cross-architecture Generalization", "content": "In dataset distillation, it is crucial for the distilled dataset constructed on one model to yield similar training effects on downstream models with arbitrary architectures. Thus, in this subsection, we evaluate the cross-architecture generalization performance of DDFAD. Specifically, we utilize the distilled datasets constructed on ResNet18, ConvNet and VGG11 to train models with different architectures, including ResNet18, ConvNet, and VGG11. The results in Table 2 show that the performance of DDFAD remains consistent across different models used for distillation, which demonstrates the good cross-architecture generalization of DDFAD."}, {"title": "Ablation Study of FD-MFCC", "content": "In this subsection, we conduct ablation studies to compare the effectiveness of our proposed FD-MFCC with MFCC [6] and LPCC [5] in DDFAD. As depicted in Figure 5, for different datasets and CPC settings, FD-MFCC consistently outperforms traditional MFCC and LPCC. This superiority arises from the ability of FD-MFCC to leverage the complementary nature of different difference orders of MFCC, thereby enhancing feature representation capability. In scenarios where the number of training samples is limited, such as dataset distillation, FD-MFCC is more informative and therefore can achieve higher accuracy."}, {"title": "Analysis of the Resources Requirement of DDFAD", "content": "For dataset distillation, it is also important to consider the cost of resources of the algorithm. Hence, we report the computational overhead and the occupied GPU memory of DDFAD across different datasets, model architectures and CPC settings in Table 3. All the experiments are run on NVIDIA RTX A6000 GPUs.\nThe results indicate that more complex model architectures are often accompanied by larger computational overhead and more GPU memory. Overall, the time overhead and occupied GPU memory of DDFAD is also acceptable for data owners and can be further reduced on better devices."}, {"title": "Visualizations of the Distilled Audio Data", "content": "We select the digital seven audio data from the FSDD dataset; dog bark and gun shot audio data from the UrbanSound-8k dataset; and calm speech emotions audio data from the RAVDESS dataset as examples to show the waveform diagram of the original audio data and distilled audio data (recovered by Algorithm 2). As presented in Figure 6, similar with dataset distillation for image data (see Figure 1), the distilled audio data may not closely resemble the original audio data. It may be synthetic audio data with no practical meaning, but is conductive to the subsequent training process of audio data classification tasks."}, {"title": "Robustness against Noise", "content": "In this subsection, we evaluate the performance of DDFAD in the presence of additional noise. Specifically, we select the FSDD dataset as an example, and artificially add Gaussian noise with different variance \u03c3 to the waveforms of the distilled data to train the classification model. As presented in Figure 7, the accuracy remains stable with increasing Gaussian noise levels. It demonstrates that the distilled training dataset is robust to additional Gaussian noise."}, {"title": "POTENTIAL APPLICATIONS", "content": "In this section, we discuss some potential applications of DDFAD, such as continual learning and neural architecture search."}, {"title": "Continual Learning", "content": "We apply our DDFAD to a Class Incremental Continual Learning (CICL) task, where the objective of CICL is to learn a new class while preserving the performance in old classes. Following the SOTA continual learning baseline EEIL [34], we construct a limited budget rehearsal memory comprising representative samples from old classes. The function of this memory is to alleviate the catastrophic forgetting problem of DNNs. In our experiments, we replace the representative sample selection strategy in EEIL (i.e. herding), with our DDFAD and random selection and keep the rest the same to perform the CICL task. The representative samples pool is set to 20 audio clips for each old class.\nAs illustrated in Table 4, DDFAD outperforms EEIL and random selection in the CICL task. This suggests that the memory of old classes constructed by our DDFAD contains more informative data for model training compared to EEIL and random selection methods. This improvement stems from the ability of DDFAD to distill knowledge from old classes more effectively, thereby aiding in mitigating catastrophic forgetting in CICL scenarios."}, {"title": "Neural Architecture Search", "content": "Our proposed DDFAD can also be applied to neural architecture search (NAS), which searches for the best network architecture for a given dataset. It typically needs significant training efforts of many candidate neural network architectures on the given dataset. Benefiting from the small size of the distilled dataset, it can be served as a sub-dataset to accelerate model evaluation in NAS.\nSpecifically, we follow the previous work [26] to design a set of candidate neural network architectures based on the considered ConvNet. We vary the depth, width, pooling, activation, and normalization layers of the ConvNet, producing 720 candidate architectures. These models are trained on the entire FSDD training dataset to establish ground-truth performance. Four NAS methods are considered for comparison, including random selection, herding selection, early-stopping and DDFAD. In terms of random selection, herding selection and DDFAD, we generate three sub-datasets using these methods with 20 audio clips per class. The models are trained for 100 epochs. For early-stopping, we train the model on the entire original training dataset for 10 epochs. Finally, we identify the top-performing architectures of these NAS methods and report their testing performance.\nWe illustrate the distribution of correlation between the NAS-based performance and the ground-truth performance on 5% top-performing architectures in Figure 8. It can be seen that the NAS-based performance by DDFAD achieves the highest correlation (0.73) with the ground-truth performance. It demonstrates the promising applications of our DDFAD in NAS.\nIn addition to continual learning and neural architecture search, DDFAD can also be utilized to protect the privacy of the training dataset. Because the original training data is hard to recover from the distilled data, even in the case of training data leakage and member inference attacks [41]. The evaluations of data privacy protection are left for future work."}, {"title": "CONCLUSIONS AND FUTURE WORKS", "content": "In this work, we propose a dataset distillation framework for audio data (DDFAD). Specifically, we propose the Fused Differential MFCC (FD-MFCC) as extracted features for audio data. It fuses the features of MFCC, the first-order difference of MFCC and the second-order difference of MFCC, which is more informative in the case of the small-scale distilled dataset. After that, we employ the matching training trajectory (MTT) distillation method to distill the FD-MFCC features. Finally, we propose an audio signal reconstruction algorithm based on the Griffin-Lim to rebuild the audio signal from the distilled FD-MFCC. Extensive experiments demonstrate the effectiveness and promising application prospects of DDFAD.\nFor future works, we intend to explore other potential applications of audio dataset distillation, such as data privacy protection. In addition, we aim to investigate dataset distillation methods under other tasks, such as object detection and natural language processing."}]}