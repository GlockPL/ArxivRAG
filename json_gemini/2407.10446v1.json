{"title": "DDFAD: Dataset Distillation Framework for Audio Data", "authors": ["Wenbo Jiang", "Rui Zhang", "Hongwei Li", "Xiaoyuan Liu", "Haomiao Yang", "Shui Yu"], "abstract": "Deep neural networks (DNNs) have achieved significant success in numerous applications. The remarkable performance of DNNs is largely attributed to the availability of massive, high-quality training datasets. However, processing such massive training data requires huge computational and storage resources. Dataset distillation is a promising solution to this problem, offering the capability to compress a large dataset into a smaller distilled dataset. The model trained on the distilled dataset can achieve comparable performance to the model trained on the whole dataset.\nWhile dataset distillation has been demonstrated in image data, none have explored dataset distillation for audio data. In this work, for the first time, we propose a Dataset Distillation Framework for Audio Data (DDFAD). Specifically, we first propose the Fused Differential MFCC (FD-MFCC) as extracted features for audio data. After that, the FD-MFCC is distilled through the matching training trajectory distillation method. Finally, we propose an audio signal reconstruction algorithm based on the Griffin-Lim Algorithm to reconstruct the audio signal from the distilled FD-MFCC. Extensive experiments demonstrate the effectiveness of DDFAD on various audio datasets. In addition, we show that DDFAD has promising application prospects in many applications, such as continual learning and neural architecture search.", "sections": [{"title": "INTRODUCTION", "content": "DNNs have achieved remarkable performance in a wide\nrange of applications. The superior performance of\nDNNs often requires large-scale training datasets. For ex-\nample, the commonly used image classification dataset Ima-\ngeNet [1] has about 1.2 million training samples and 100,000\ntesting samples, taking up about 148G of storage space;\nthe commonly used object detection dataset COCO [2] has\n118,287 training samples and 40,670 testing samples, taking\nup about 44G of storage space. However, the management\nof such massive data entails significant challenges, includ-\ning the collection, storage, transmission, and pre-processing.\nMoreover, training models on such massive data comes with\nhuge computational overhead. Such storage and computing\nrequirements are huge and impractical for personal users.\nTo address this problem, an emerging technology known\nas dataset distillation has garnered considerable research\nattention in recent years. As illustrated in Figure 1, dataset\ndistillation extracts the knowledge from a large-scale dataset\nand generates a smaller synthetic distilled dataset. The\nmodels trained on the distilled dataset can achieve perfor-\nmance comparable to those trained on the original dataset.\nThis technique can significantly decrease the computational\nresources of training a DNN."}, {"title": "RELATED WORK", "content": "Audio data classification is an important topic with many\npotential applications, such as speech classification in in-\ndustrial automation, environmental sound classification in\nweather prediction, etc. Early works on audio classification\nused Support Vector Machines (SVM) [9], K-Nearest Neigh-\nbors (KNN) [10] and Hidden Markov Models (HMMs) [11]\nfor classification. More recently, audio classification based\non DNN is gradually becoming mainstream and achieving\nleading performance [12], [13], [14], [15]. Thus, we mainly\nfocus on DNN-based audio classification methods in this\nwork.\nIn addition to choosing different machine learning mod-\nels for classification, the extraction of audio features is\nalso critical for audio data classification. Concretely, au-\ndio features can broadly be categorized into time-domain,\nfrequency-domain, and cepstral-domain features.\nTime-domain feature characterizes audio signals in re-\nlation to time. It directly uses the one-dimensional (1D)\naudio signals as the input of the classification model, which\nis computation-efficient. Notable time-domain features in-\nclude Short-Time Energy (STE) [16], Zero Crossing Rate\n(ZCR) [17], Short-Time Autocorrelation Function (STAF)\n[18], etc.\nFrequency-domain feature refers to the characteristics\nof audio signals varying in frequency. It no longer uses the\noriginal 1D signals, but 2D signals (i.e., spectrograms) as the\nmodel input. Therefore, frequency-domain features contain\nmore information compared to time-domain features. Com-\nmon frequency domain features include Fourier Transform\n(FT) [19], Discrete Cosine Transform (DCT) [20], etc.\nCepstral-domain feature is mostly obtained by inverting\nsome frequency-domain signals and their variants. Cur-\nrently, the cepstral-domain feature is the most commonly\nused feature type and achieves leading performance in\nDNN-based audio classification. Representative cepstral-\ndomain features include linear predictive cepstral coefficient\n(LPCC) [5], Mel frequency cepstral coefficient (MFCC) [6],\netc."}, {"title": "Dataset Distillation", "content": "The concept of dataset distillation is originated from knowl-\nedge distillation [21]. Knowledge distillation is designed to\ntransfer the knowledge of a large-scale model to a more"}, {"title": "Coreset Selection", "content": "Coreset selection is another strategy to select a representa-\ntive subset of the whole dataset through heuristic selection\ncriteria. It is commonly used in active learning to identify\ntraining samples. For instance, random selection [33] picks\nsamples arbitrarily; Herding selection [34] chooses samples\nclosest to each class center; In K-Center selection [35], mul-\ntiple center points are chosen for each class in order to min-\nimize the maximum distance between data points and their\nnearest center point. However, these coreset selection meth-\nods may not always yield optimal results for downstream\naudio data classification task. Furthermore, identifying an\ninformative coreset can be challenging, particularly when\ndataset information is not concentrated in a few samples. In\ncontrast, the dataset distillation methods can achieves better\nresult for downstream audio data classification task through\ngenerating synthetic distilled data."}, {"title": "METHODOLOGY", "content": "In this section, we present the details of DDFAD. It mainly\ncontains three phases: feature extraction for audio data,\ndataset distillation for FD-MFCC and audio signal recon-\nstruction. The workflow of DDFAD is illustrated in Figure\n3. Below we describe the details of each phase."}, {"title": "FD-MFCC Feature Extraction", "content": "Currently, MFCC is the most commonly used feature type\nfor audio data and achieves leading performance in DNN-\nbased audio classification. Although MFCC feature per-\nforms well on the entire source dataset, in the case of dataset\ndistillation, MFCC struggles to extract sufficient discrimina-\ntive features to maintain the accuracy of the model trained\non the distilled dataset. To overcome this limitation, we\npropose the Fused Differential MFCC (FD-MFCC), which\nfuses the features of MFCC, the first-order difference of\nMFCC and the second-order difference of MFCC. Since\ndifferent difference orders of MFCC can represent features\nfrom different aspects, FD-MFCC can make full use of the\ncomplementarity between them and enhance the feature\nrepresentation capability.\nAs illustrated in Figure 4, the feature extraction process\nof FD-MFCC consists of 9 steps:"}, {"title": "Dataset Distillation for FD-MFCC", "content": "Given the source training dataset S with |S| samples, the\nobjective of dataset distillation is to extract the knowledge\nof S into a small distilled dataset D with |D| samples (|S| \u226b\n|D|), and the model trained on D can achieve comparable\nperformance to the model trained on S.\nIn this work, we adopt the state-of-the-art (SOTA) match-\ning training trajectory (MTT) distillation method [7] in\nDDFAD to distill FD-MFCC. Algorithm 1 illustrates the\ndetailed process of dataset distillation for FD-MFCC. Specif-\nically, MTT first trains models on S and collects the trajec-\ntories of the model (referred to as the teacher model) in the\nbuffer. Subsequently, ingredients in the buffer are randomly\nchosen to initialize the student model (the model trained on\nD). After collecting the trajectories of the student model, the\ndistilled dataset is updated by matching the two training\ntrajectories. The objective loss of MTT is defined as:\n$L_o =  \\frac{||\\theta^{(t+N)}_D - \\theta^{(t+M)}_S||_2^2}{|S|}$,\nwhere $\\theta^{(t)}_S$ represents the parameter of the teacher model\nat training epoch t, which is stored in the buffer; $\\theta^{(t+N)}_D$\nrepresents the parameter of the student model trained on D\nfor N epochs with the initialization of $\\theta^{(t)}_S$."}, {"title": "Audio Signal Reconstruction from the Distilled FD-MFCC", "content": "After distilling the dataset of FD-MFCC, the final phase is\nto reconstruct the audio signal from the distilled FD-MFCC.\nWe propose an audio signal reconstruction algorithm based\non GLA to reconstruct the audio signal from the distilled\nFD-MFCC."}, {"title": "EVALUATION", "content": "We consider three model architectures, including ResNet18\n[36], ConvNet (which mainly contains multiple Conv-ReLU-\nAvgPooling blocks) and VGG11 [37] for audio data classifi-\ncation."}, {"title": "Distillation Performance", "content": "We perform our DDFAD with five considered distillation\nmethods (DCGM [26], DCDSA [40], MTT [7], Random se-\nlection [33], Herding selection [34]), to synthesize 1, 10, and\n50 clips per class (CPC) respectively.\nAs presented in Table 1, dataset distillation methods are\nmuch more effective than coreset selection methods with the\nsame CPC. This is because the dataset information can not\nbe concentrated in a few samples. The synthetic distilled\ndata can better represent the information of the whole\ndataset. Furthermore, among these dataset distillation meth-\nods, the DDFAD incorporating with the MTT outperforms\nother dataset distillation methods. Thus, for subsequent\nexperiments, we adopt the MTT distillation method for\nevaluations of DDFAD.\nFor instance, in the case of the FSDD dataset with\nCPC=50, where the distilled dataset accounts for only 1/6\nof the whole dataset, the test accuracies of DDFAD with"}, {"title": "Cross-architecture Generalization", "content": "In dataset distillation, it is crucial for the distilled dataset\nconstructed on one model to yield similar training effects\non downstream models with arbitrary architectures. Thus,\nin this subsection, we evaluate the cross-architecture gen-\neralization performance of DDFAD. Specifically, we utilize\nthe distilled datasets constructed on ResNet18, ConvNet\nand VGG11 to train models with different architectures,\nincluding ResNet18, ConvNet, and VGG11. The results in\nTable 2 show that the performance of DDFAD remains con-\nsistent across different models used for distillation, which\ndemonstrates the good cross-architecture generalization of\nDDFAD."}, {"title": "Ablation Study of FD-MFCC", "content": "In this subsection, we conduct ablation studies to compare\nthe effectiveness of our proposed FD-MFCC with MFCC_\n6] and LPCC [5] in DDFAD. As depicted in Figure 5, for\ndifferent datasets and CPC settings, FD-MFCC consistently\noutperforms traditional MFCC and LPCC. This superiority\narises from the ability of FD-MFCC to leverage the com-\nplementary nature of different difference orders of MFCC,\nthereby enhancing feature representation capability. In sce-\nnarios where the number of training samples is limited, such\nas dataset distillation, FD-MFCC is more informative and\ntherefore can achieve higher accuracy."}, {"title": "Analysis of the Resources Requirement of DDFAD", "content": "For dataset distillation, it is also important to consider the\ncost of resources of the algorithm. Hence, we report the\ncomputational overhead and the occupied GPU memory of\nDDFAD across different datasets, model architectures and\nCPC settings in Table 3. All the experiments are run on\nNVIDIA RTX A6000 GPUs."}, {"title": "Visualizations of the Distilled Audio Data", "content": "We select the digital seven audio data from the FSDD\ndataset; dog bark and gun shot audio data from the\nUrbanSound-8k dataset; and calm speech emotions audio\ndata from the RAVDESS dataset as examples to show the\nwaveform diagram of the original audio data and distilled\naudio data (recovered by Algorithm 2). As presented in\nFigure 6, similar with dataset distillation for image data (see\nFigure 1), the distilled audio data may not closely resemble\nthe original audio data. It may be synthetic audio data with\nno practical meaning, but is conductive to the subsequent\ntraining process of audio data classification tasks."}, {"title": "Robustness against Noise", "content": "In this subsection, we evaluate the performance of DDFAD\nin the presence of additional noise. Specifically, we select the\nFSDD dataset as an example, and artificially add Gaussian\nnoise with different variance \u03c3 to the waveforms of the\ndistilled data to train the classification model. As presented\nin Figure 7, the accuracy remains stable with increasing\nGaussian noise levels. It demonstrates that the distilled\ntraining dataset is robust to additional Gaussian noise."}, {"title": "POTENTIAL APPLICATIONS", "content": "In this section, we discuss some potential applications of\nDDFAD, such as continual learning and neural architecture\nsearch."}, {"title": "Continual Learning", "content": "We apply our DDFAD to a Class Incremental Continual\nLearning (CICL) task, where the objective of CICL is to learn\na new class while preserving the performance in old classes.\nFollowing the SOTA continual learning baseline EEIL [34],\nwe construct a limited budget rehearsal memory comprising\nrepresentative samples from old classes. The function of this\nmemory is to alleviate the catastrophic forgetting problem\nof DNNs. In our experiments, we replace the representative\nsample selection strategy in EEIL (i.e. herding), with our\nDDFAD and random selection and keep the rest the same to\nperform the CICL task. The representative samples pool is\nset to 20 audio clips for each old class.\nAs illustrated in Table 4, DDFAD outperforms EEIL and\nrandom selection in the CICL task. This suggests that the\nmemory of old classes constructed by our DDFAD contains\nmore informative data for model training compared to EEIL\nand random selection methods. This improvement stems\nfrom the ability of DDFAD to distill knowledge from old\nclasses more effectively, thereby aiding in mitigating catas-\ntrophic forgetting in CICL scenarios."}, {"title": "Neural Architecture Search", "content": "Our proposed DDFAD can also be applied to neural archi-\ntecture search (NAS), which searches for the best network\narchitecture for a given dataset. It typically needs significant\ntraining efforts of many candidate neural network architec-\ntures on the given dataset. Benefiting from the small size\nof the distilled dataset, it can be served as a sub-dataset to\naccelerate model evaluation in NAS.\nSpecifically, we follow the previous work [26] to design\na set of candidate neural network architectures based on\nthe considered ConvNet. We vary the depth, width, pool-\ning, activation, and normalization layers of the ConvNet,\nproducing 720 candidate architectures. These models are\ntrained on the entire FSDD training dataset to establish\nground-truth performance. Four NAS methods are consid-\nered for comparison, including random selection, herding\nselection, early-stopping and DDFAD. In terms of random\nselection, herding selection and DDFAD, we generate three\nsub-datasets using these methods with 20 audio clips per\nclass. The models are trained for 100 epochs. For early-\nstopping, we train the model on the entire original train-\ning dataset for 10 epochs. Finally, we identify the top-\nperforming architectures of these NAS methods and report\ntheir testing performance.\nWe illustrate the distribution of correlation between the\nNAS-based performance and the ground-truth performance\non 5% top-performing architectures in Figure 8. It can be\nseen that the NAS-based performance by DDFAD achieves\nthe highest correlation (0.73) with the ground-truth perfor-\nmance. It demonstrates the promising applications of our\nDDFAD in NAS.\nIn addition to continual learning and neural architecture\nsearch, DDFAD can also be utilized to protect the privacy\nof the training dataset. Because the original training data"}, {"title": "CONCLUSIONS AND FUTURE WORKS", "content": "In this work, we propose a dataset distillation framework\nfor audio data (DDFAD). Specifically, we propose the Fused\nDifferential MFCC (FD-MFCC) as extracted features for\naudio data. It fuses the features of MFCC, the first-order\ndifference of MFCC and the second-order difference of\nMFCC, which is more informative in the case of the small-\nscale distilled dataset. After that, we employ the matching\ntraining trajectory (MTT) distillation method to distill the\nFD-MFCC features. Finally, we propose an audio signal\nreconstruction algorithm based on the Griffin-Lim to rebuild\nthe audio signal from the distilled FD-MFCC. Extensive\nexperiments demonstrate the effectiveness and promising\napplication prospects of DDFAD.\nFor future works, we intend to explore other potential\napplications of audio dataset distillation, such as data pri-\nvacy protection. In addition, we aim to investigate dataset\ndistillation methods under other tasks, such as object detec-\ntion and natural language processing."}]}