{"title": "GEOBIKED: A DATASET WITH GEOMETRIC FEATURES AND AUTOMATED LABELING TECHNIQUES TO ENABLE DEEP GENERATIVE MODELS IN ENGINEERING DESIGN.", "authors": ["Phillip Mueller", "Sebastian Mueller", "Lars Mikelsons"], "abstract": "Purpose - We provide a dataset for enabling Deep Generative Models (DGMs) in engineering design and propose methods to automate data labeling by utilizing large-scale foundation models.\nMethodology \u2013 GeoBiked is curated to contain 4 355 bicycle images, annotated with structural and technical features and is used to investigate two automated labeling techniques:\n\u2022 The utilization of consolidated latent features (Hyperfeatures) from image-generation models to\ndetect geometric correspondences (e.g. the position of the wheel center) in structural images.\n\u2022 The generation of diverse text descriptions for structural images. GPT-4o, a vision-language-\nmodel (VLM), is instructed to analyze images and produce diverse descriptions aligned with the\nsystem-prompt.\nFindings - By representing technical images as Diffusion-Hyperfeatures, drawing geometric corre-spondences between them is possible. The detection accuracy of geometric points in unseen samples\nis improved by presenting multiple annotated source images.\nGPT-40 has sufficient capabilities to generate accurate descriptions of technical images. Grounding\nthe generation only on images leads to diverse descriptions but causes hallucinations, while grounding\nit on categorical labels restricts the diversity. Using both as input balances creativity and accuracy.\nResearch implications \u2013 Successfully using Hyperfeatures for geometric correspondence suggests\nthat this approach can be used for general point-detection and annotation tasks in technical images.\nLabeling such images with text descriptions using VLMs is possible, but dependent on the models\ndetection capabilities, careful prompt-engineering and the selection of input information.\nOriginality \u2013 Applying foundation models in engineering design is largely unexplored. We aim to\nbridge this gap with a dataset to explore training, finetuning and conditioning DGMs in this field and\nsuggesting approaches to bootstrap foundation models to process technical images.\nKeywords Deep Generative Models, Data-driven design, AI-driven engineering design\nPaper type Research paper", "sections": [{"title": "1 Introduction", "content": "Rapid advancements in the field of machine learning underscore the pivotal role of high-quality datasets in propelling technological breakthroughs. In Computer Vision, the introduction of quality, publicly available datasets has acted as a catalyst, enabling researchers to evaluate the performance of diverse methodologies. Datasets like ImageNet (Deng et al. 2009), CIFAR (Krizhevsky 2009) and MNIST (LeCun et al. 1998) are crucial to level the playing field and set benchmarks that define the state of the art."}, {"title": "2 GeoBiked Dataset", "content": "The first curation step in the derivation of GeoBiked from BIKED is a visual inspection of the 4512 provided bike images for faulty and out-of-distribution samples. We categorize samples as faulty if their geometric integrity is not ensured or the frame design is visibly unrealistic"}, {"title": "2.1 Related Work", "content": "Our dataset is based on the BIKED-dataset (Regenwetter et al. 2021), which originated from the BikeCAD software (Curry 1998), a specialized tool for bicycle design. From an initial collection of 4791 bikes and 23813 descriptive parameters, the authors distilled 4512 bicycles and 1314 parameters. These parameters correlate to the CAD-models and describe every bicycle in detail. This curation process aimed to retain the raw richness of the data, facilitating a broad spectrum of data-driven design applications. Their methodology exemplifies the utility of the dataset through the training of two Variational Autoencoders (VAEs): one dedicated to generating new bike images and another to reconstructing bike parameters, showcasing the versatility of the dataset in supporting innovative design synthesis.\nThe initial version of the dataset comes with a number of shortcomings, limiting its applicability in DGM-driven engineering design tasks. Each sample is annotated with a total of 1314 parameters that were extracted from the BikeCAD software. Despite large in quantity, these parameters are largely non-interpretable and contain no meaningful information about design, style or structural composition of the bicycles they describe. Having such information is crucial to enable DGMs in engineering and concept design, as they are the basis for conditioning and control modalities of the generatice process (Alam et al. 2024; Joskowicz and Slomovitz 2024; Regenwetter et al. 2022). Furthermore, the dataset contains a number of infeasible designs and unrealistic samples. It lacks a uniform scaling of the objects within the image resolution, preventing direct geometric correspondences between the images."}, {"title": "2.2 Methodology", "content": "The next step is the centering and geometric normalization of the images. In the original dataset, the images are not normalized to the same scale. Using the provided information about the scaling factor in the BIKED dataset, we ensure that all bikes are scaled equally. This step solidifies geometric consistency. Furthermore, we maximize the size of the bikes within the image resolution by precisely fitting the largest sample to the image resolution in length (x-dimension). This leads to one pixel equaling 10.19 mm in 256 \u00d7 256 images and 1.27 mm in 2048 \u00d7 2048 images.\nTo add geometric reference points to the dataset, we first define characteristic points and intersections in the bike geometry, aiming to allow the representation of each sample solely by a combination of these points. We describe the points in more detail in Section 2.3.3. Of the 12 geometric points, we derive six from the provided parameter set in the original dataset. The remaining parameters have to be defined manually. The coordinate-values of the geometric points are stored in millimeters for simplicity. They can be translated into pixel-values with the previously discussed scaling for both image resolutions. We filter and modify the initial set of descriptive parameters for features with semantic, geometric, or technical relevance, keeping a total of nine. We discuss the provided features in the next section in more detail."}, {"title": "2.3 Dataset Features", "content": "We categorize the samples in the dataset into 19 different bicycle styles. This categorization is adapted from the original BIKED dataset. The style distribution is shown in Figure 2a. In the dataset, Road-Bikes are the most common style, followed by Mountain-Bikes and Track-Bikes.\nWe further add the diameters of the front and the rear wheel to the dataset. These are essential for the overall bicycle composition and geometry and can therefore be used as technical or design parameters. Furthermore, we provide the possibility of categorizing the samples by their Rim-style. The Rim-style has a significant impact on the overall design and appearance of the bicycle. The dataset distinguishes between spoked rims, tri-spoked rims and disked rims. Front-and rear wheel rim styles are separate categories, as several samples have a combination of two different rims. We note that spoked rims are by far the most common category in the dataset, making up 93.5% of samples for the front wheel and 92.9% for the rear wheel.\nAnother design-related feature that we provide in our dataset is the fork type. We distinguish between rigid forks, suspension forks and single-sided forks. Rigid forks make up 77.1% of samples while suspension forks make up 20.2% and single-sided forks 2.6%. The final set of design features are the bottles on the seat-tube and the down-tube. We added information about their presence as Boolean-values to the dataset."}, {"title": "2.3.1 Design Features", "content": "We categorize the samples in the dataset into 19 different bicycle styles. This categorization is adapted from the original BIKED dataset. The style distribution is shown in Figure 2a. In the dataset, Road-Bikes are the most common style, followed by Mountain-Bikes and Track-Bikes.\nWe further add the diameters of the front and the rear wheel to the dataset. These are essential for the overall bicycle composition and geometry and can therefore be used as technical or design parameters. Furthermore, we provide the possibility of categorizing the samples by their Rim-style. The Rim-style has a significant impact on the overall design and appearance of the bicycle. The dataset distinguishes between spoked rims, tri-spoked rims and disked rims. Front-and rear wheel rim styles are separate categories, as several samples have a combination of two different rims. We note that spoked rims are by far the most common category in the dataset, making up 93.5% of samples for the front wheel and 92.9% for the rear wheel.\nAnother design-related feature that we provide in our dataset is the fork type. We distinguish between rigid forks, suspension forks and single-sided forks. Rigid forks make up 77.1% of samples while suspension forks make up 20.2% and single-sided forks 2.6%. The final set of design features are the bottles on the seat-tube and the down-tube. We added information about their presence as Boolean-values to the dataset."}, {"title": "2.3.2 Technical Features", "content": "To provide technical features, we categorize the samples by their tube-sizes, their frame-sizes and the number of teeth on the chainring. For the tube-sizes, each bicycle consists of four major tubes. To categorize the samples in a meaningful way, we calculate the average tube diameter for each of the four tubes. The average seat-tube-diameter is 31.5mm, the average down-tube-diameter is 35.5mm, the average head-tube is 42.9mm in diameter and the top-tube measures at 32.0mm on average. The classification is carried out by counting how many diameters of a given sample are greater than the average. If all diameters are smaller than their class-average (nts = 0), the sample is considered to have a \"minimal\" tube-size. For nts = 1, the tube-size is considered \"lite\" and \"standard\" for nts = 2. If nts = 3 the tubes are \"reinforced\" and for nts = 4 they are categorized as \u201cextreme\". The frequency of each category is visualized in Figure 2b.\nIn addition to the tube sizes, we provide information about the bicycles' frame sizes, as this is a common metric for categorization. The most common way to determine the frame size is to measure the length of the seat-tube between the bottom bracket and the top edge of the tube (Norman 2024). We consider all samples with a seat-tube length smaller than 360 mm to be \"XS\". For seat-tube lengths between 360mm and 420mm, the samples are considered \"S\" and for lengths between 420mm and 480mm they are considered size \"M\". A bicycle is of frame-size \"L\" if the seat-tube length is between 480mm and 540mm and \"XL\" if it is longer than 540mm. The distribution of frame sizes is shown in Figure 2c."}, {"title": "2.3.3 Geometric Features", "content": "With the selection of the 12 reference points, we aim to capture bicycles in all styles and sizes and characterize them by their geometrical layout. The final selection of 12 geometric points is shown in Figure Figure 3. We select the center points of the rear wheel (RWC) and the front wheel (FWC) for obvious reasons as they define the wheelbase of the bicycle. The point \"BB\" marks the center of the intersection of the seat-tube and the chain-stay and therefore the center of the bottom bracket. The head-tube-top (HTT) marks the upper end of the tube that connects the handlebars to the fork, given that this is where the stem intersects with the fork-tube. The stem-top (ST) marks the end of the stem and is of significant influence for the reach of the bicycle frame (Norman 2024). We define another point on the front-fork (FF), characterizing its shape and potential bends or angles. The seat-tube-top (STT) marks the upper end of the seat-tube and the saddle-top (SAT) describes the highest point of the saddle.\nIn addition to the points already mentioned, we define every intersection of the tubes that make up the bicycle frame. Namely, the intersections of top-tube and seat-tube (TTST), top-tube and head tube (TTHT), down-tube and head tube (DTHT), rear-tube and seat-tube (RTST). All points are defined by their x- and y-coordinate values, given in millimeters. The values are relative to the rear-wheel-center, which we defined as the center of the local coordinate system. Since the position of RWC in the image varies from sample to sample, we provide two additional values, Xzero and Yzero, that describe the distance of RWC to the bottom-left corner of the image. This allows for accurate localization of the geometric points within an image as well as comparisons of the geometric layout and bicycle sizes."}, {"title": "2.4 DGM-driven Engineering Design Applications", "content": "Our objective with GeoBiked is to enable DGMs for engineering design applications. We provide a dataset that can be used for initial experiments and investigations which then lead to a faster and more efficient realization of real-world engineering design use-cases. In this section, we briefly discuss potential applications of GeoBiked and give some examples.\nModel verification. For the case of training or finetuning visual DGMs in engineering design applications, the dataset can be used to help find a suitable model architecture. By training different types of DGMs on GeoBiked, practitioners get valuable insights into the capabilities of DGMs to handle structural image data. Such experiments furthermore allow for the evaluation of required computational ressources and training durations as well as dataset quality and size. This lets the user draw quick correspondences about the suitability and the limitations of given models for the task at hand. As a comprehensible example of this application, we train three different visual DGMs on GeoBiked. We train a convolutional VAE (Kingma and Welling 2013; Sohn et al. 2015; Rombach et al. 2022), a variant of the same VAE with adversarial training (Blattmann et al. 2021), a simple diffusion model (DDIM) (Ho et al. 2020; Song et al. 2022; Fan et al. 2023a) and a latent diffusion model (LDM) (Rombach et al. 2022) to reproduce the structural images. A brief summary of the results is provided in Table I and Figure 4 shows some qualitative examples. Details about the implementation can be found in Section 6.\nWhile both diffusion-based models are able to reproduce the structural image data, the VAE-architecture struggles to synthesize clean images. Even with the introduction of an adversarial loss, the results that can be synthesized remain very blurry. In terms of compute requirements, the VAE is much cheaper but its capabilities are not sufficient for detailed images with fine-grained structures. Even though there exist Autoencoder-based approaches for such image data (Fan et al. 2023b), we observe that off-the-shelf diffusion models handle this data much better. Comparing both diffusion-models, we observe that in both quantitative (FID-score (Szegedy et al. 2014)) and qualitative measures (Figure 4), the LDM is outperformed by the DDIM. The DDIM, although significantly more expensive in training, reproduces the fine-grained structures better. Due to the images being encoded into a latent representation before learning the diffusion model, the LDM loses some structural and geometric information and the results are more blurred.\nConditioning. For engineering design applications, the possibility to condition the DGM on relevant modalities is a fundamental requirement. Developing custom conditioning mechanisms either requires training data, for example to train a ControlNet adapter (Zhang et al. 2023), or test data to optimize and verify the method. GeoBiked can help with these tasks. In a different work, the dataset is used to enable a training-free architecture for visualization of engineering design images in realistic scenes (Mueller et al. 2024).\nEvaluation of pretrained models. The GeoBiked dataset also provides an opportunity to evaluate pretrained models with regards to their context understanding and capability to handle detailed, CAD-like image data. For example, users can use the dataset to verify if a pretrained vision-language model posses enough visual understanding and context knowledge to accurately categorize or describe the images. We discuss this task in detail in Section 3 and Section 4."}, {"title": "3 Geometric Feature Detection with Diffusion Hyperfeatures", "content": "Adding structural or geometric information to technical images is fundamentally important for the application of DGMs in engineering design. Despite the recent explosion in publications about visual DGMs, there still is a shortage of datasets with domain-specific modalities that enable conditional control over the generative process. However, this control is a key requirement for the successful application of DGMs for technical and design tasks (Alam et al. 2024; Mueller and Mikelsons 2024). In GeoBiked, we provide information about the geometric layout of each image sample. This might be used to investigate geometry-aware image generation or conditioning a model on a geometric layout. In annotating the geometric points largely by hand, we come to the conclusion that this time-intensive manual task poses a significant barrier for domain-specific applications. We therefore aim to utilize the spatial and semantic understanding inherited by pretrained, large-scale diffusion models for image generation (Stable Diffusion) and automate this task (Po et al. 2023; Tang et al. 2023; Luo et al. 2023)."}, {"title": "3.1 Related Work", "content": "A number of works have recently addressed the idea of utilizing and transferring learned latent representations from large-scale models for downstream tasks (Goodwin et al. 2022; Oquab et al. 2024; Tumanyan et al. 2022). From various studies and applications, we know that those representations carry information about the underlying structure and composition of images (Fan et al. 2023a; Sabour et al. 2024). The possibility to draw semantic correspondences with these deep features has also been proven (Amir et al. 2022; Caron et al. 2021; Oquab et al. 2024).\nRecently, Luo et al. 2023 have made significant progress in this domain by consolidating the latent feature maps from SD, extracted over multiple layers and timesteps, into an interpretable, per-pixel descriptor. The so-called Hyperfeature map allows to draw semantic correspondences between two images by comparing their respective Hyperfeature maps. In their work, they train an aggregation network to consolidate an image into its Hyperfeature Map. This is done by caching the intermediate feature maps obtained by either generating a synthetic image or inverting an existing image through multiple diffusion timesteps. Each extracted feature map is upsampled to a standard resolution and passed through a bottleneck layer for standardization. Subsequently, mixing weights are learned that identify the most significant features. For the task of semantic correspondence, the cosine similarity is computed between the flattened descriptor maps of image pairs that are labeled with corresponding keypoints. The aggregation network is trained by minimizing a symmetric cross-entropy loss similar to CLIP (Radford et al. 2021) between the predicted and ground truth keypoints."}, {"title": "3.2 Method", "content": "We propose to utilize the aggregation network pretrained for semantic correspondence to draw geometric correspon-dences in the GeoBiked images. In their work, Luo et al. 2023 already show that their approach outperforms existing alternatives for matching keypoints in natural images. We apply their methodology for our dataset and extend it to be able to handle multiple annotated source images. We draw inspiration from the manual labeling process, where a human annotator is shown a small number of annotated reference images that show the relevant features and subsequently identifies them in unseen images showing similar concepts. The possibility to base the prediction of the geometric reference points on multiple source images is introduced to make the prediction more reliable given the variety of bicycle geometries and styles in the dataset.\nThe source images i \u2208 Is are annotated with their corresponding geometric points pik. It is worth noting that while the points can be chosen freely, they have to be consistent inbetween the source images, meaning that they mark the same geometric characteristics. Using the pretrained aggregation network, the Hyperfeature map Hsi \u2208 \\mathbb{R}^{C \\times 64 \\times 64} is computed for each source image. Depending on the resolution of the source images (H \u00d7 W), the Hyperfeatures are interpolated back to the original image size to obtain H' \u2208 \\mathbb{R}^{C \\times H \\times W}. They are subsequently flattened and normalized to form Fsi \u2208 \\mathbb{R}^{(H \\cdot W) \\times C}. Per source image, each labeled point k \u2208 pik is translated into its index idxik to extract the corresponding Hyperfeatures Vsi from the flattened map Fsi:\n$V_{s_{i}} = F_{s_{i}}[:, idx_{ik}, :] with V_{s_{i}} \\in \\mathbb{R}^{N \\times C}$\nWhen processing an entire dataset, the unlabeled images to be annotated are processed one after another. For each target image It, we compute its Hyperfeature map Ht \u2208 \\mathbb{R}^{C \\times 64 \\times 64}, interpolate it to the original image size H \u2208 \\mathbb{R}^{C \\times H \\times W} and also flatten and normalize it for F\u2081 \u2208 \\mathbb{R}^{(H \\cdot W) \\times C}. Given the Hyperfeature representations of the source points Vsi and the target image Ft, we now can compute the similarity matrix:\n$S_{i} = V_{s_{i}} \\times F_{t}^{T}$ with $S_{i} \\in \\mathbb{R}^{N \\times (H \\cdot W)}$.\nWe obtain one similarity matrix of size N \u00d7 (H. W) per combination of source and target image, where N is the number of points. The similarity matrix contains the cosine similarities between the Hyperfeatures of the annotated source points and the Hyperfeatures of the entire target image. Now we extract the maximum cosine similarities per row in Si to obtain vi, which describes the Hyperfeatures in the target image that have the highest correspondence to the Hyperfeatures of source image i with respect to the points. Since we are processing multiple source images, we can concatenate the similarity vectors v\u2081 along the y-axis and now extract the row-wise maximum similarity:\n$V_{max}[k] = max(v_{i}[k])$.\nThis increases flexibility in the prediction of the geometric points as per-point, the source-target combination with the highest correspondence is selected. In simple terms, this allows us to predict the positions of the geometric reference points in the target images using the information from all the source images. We always chose the position of the point where the correspondence between the Hyperfeatures of the source and target image is highest."}, {"title": "3.3 Experiments", "content": "In our experiments, we aim to verify the hypothesis that we can in fact use the diffusion Hyperfeatures to detect the geometric features in unseen structural images. Furthermore, we want to find the optimale selection and quantity of annotated source images to accurately label the diverse bicycle images in the dataset. For the evaluation, we select a subset of 150 diverse samples from our dataset. All calculations are conducted on an NVIDIA RTX A4500 with 20GB of VRAM. The results are summarized in Table II as well as in Figure 11. For evaluation, we use the pixel-wise MAE and MSE between the predicted location of the point and the ground truth location that we annotated by hand. We average the errors over all annotated points per image to gain insights on how well the entire geometric layout is captured and predicted."}, {"title": "3.3.1 Results", "content": "Single Source Image. When providing only one source image, we observe that the accuracy of the point-location prediction is heavily dependant on the type of source image that is provided as reference (see Table IV and Figure 9). We selected various bicycle styles as reference images and observed that an average geometry, which captures a wide selection of bicycle frames, leads to good prediction accuracy as the MAE is below 3 pixels. However, with this kind of source image, the method is not able to draw accurate correspondences to bicycles with a significantly different geometric layout. This is evident in the large MSE. When samples are chosen as source images that are not in the middle of the geometric distribution, the prediction accuracy deteriorates significantly, as seen in Table IV. For the source image with the best prediction accuracy, we still observe typical error patterns. Tube intersections are often annotated inaccurately as well as the saddle top and points around the handlebars. For the annotation of outlier samples, only the wheel centers are captured reliably.\nThe observed ambiguities most likely originate from the structure of the bicycles in the images. Due to them being plain grayscale structures on a white background, areas with tube intersections look very similar. A single source image does not capture the different options of the bicycle layout with respect to the saddle position, stem and handlebars and tube intersections. For example, if multiple tube intersections fall into the same position in the source image, but are in different positions in the target image, they will not be detected with high precision.\nMultiple Source Images. Comparing an unlabeled target image with multiple source images for geometric corre-spondence noticeably improves the accuracy of the point prediction. By just using a second source image showing a different type of bicycle, we reduce the MSE by a factor of 2.4. The variety of layouts in the dataset gets captured much more reliable. Since we can precompute the Hyperfeatures for the source images once and then use them for processing the entire dataset, the computational overhead is insignificant. For two source images, we test a variety of combinations. We observe that using the sample that performs best when used as the only input together with a sample showing a different, but common, style leads to the best accuracy. Typical error patterns observed in the previous section are largely eliminated (see Figure 5 and Figure 10). Nevertheless, for uncommon styles the prediction is still inaccurate.\nBased on the observations with one and two source images, we add a third source image showing such sample. With that, we are able to further reduce the MSE by about 18%. In our experiments, we observe that increasing the number of source images beyond three does not have a significant impact on the prediction accuracy (see Table II and Figure 11). We therefore propose that, in the case of the GeoBiked dataset, using three source images presents a good balance of manual annotation effort and prediction accuracy of the automated process. Using the three source images shown in Figure 5, we also process the entire GeoBiked dataset, achieving an accuracy of MAE = 1.837 and MSE = 14.009 in a processing duration of 11.33 hours."}, {"title": "3.3.2 Implications", "content": "For engineering design images with fine-grained details, the provided source images have to capture the variety of structures in the target image space as much as possible. It is important to select the references so that they show different layouts of the object geometry. In our case, this correlates to bicycles with overlapping and non-overlapping tube intersections. When processing a diverse dataset, outliers and uncommon samples require specific attention. They need to be annotated by and or at least evaluated for sufficient accuracy. When these limitations are regarded for, pretrained diffusion models can be used to automate the process of data annotation. Finetuning of the aggregation network might be necessary for some applications where the off-the-shelf accuracy is not sufficient.\nOur experiments show that representing images through their diffusion Hyperfeatures generally allows us to process domain-specific, structural image data. Even though the diffusion model was not specifically trained on such data, the learned semantic correspondences and spatial understanding can be transferred and applied to engineering design tasks. Efficiently using the inherited capabilities however is not straightforward, as the feature representations extracted from the diffusion model need to be further processed. In this case, training an encoder-like model is required to draw semantic correspondences. Nevertheless, compared to a full-scale training or finetuning of a diffusion model this is relatively cheap. The aggregation network was trained on a single NVIDIA RTX TITAN 24GB GPU (Luo et al. 2023). Computing the Hyperfeatures of an image and using them for downstream applications can also be considered efficient as it is possible on a consumer-grade GPU.\nIn an outlook, we propose that Diffusion Hyperfeatures of images can also be used for other engineering design applications in image generation and modification. A future research direction can be to utilize them in order to improve object consistency in generative processes by comparing the Hyperfeature representations of the generated object with a learned distribution or ground-truth example. In a different work building up upon Diffusion Hyperfeatures, the authors address this task in broad terms (Luo et al. 2024)."}, {"title": "4 Automatic Generation of Text Descriptions with Vision-Language Models", "content": "The possibility of text-based conditioning is a key factor of the recent success in image generation as it provides an intuitive modality to control the generated content. Numerous works discuss this topic. Most notably (Ramesh et al. 2021; Rombach et al. 2022; Saharia et al. 2022b; Ruiz et al. 2022; Inc. 2023) in image generation and (Saharia et al. 2022a; Brooks et al. 2023; Yang et al. 2023; Tumanyan et al. 2022) in image-to-image editing. It provides a way to dynamically adjust the amount of information passed to the model. For engineering design applications, the DGM can be trained to adhere to many design constraints and requirements as well as fill in the blanks for short, high-level descriptions. Training such models and corresponding conditioning mechanisms requires pairs of images and diverse text-descriptions. For better generalisation, the text-descriptions have to contain varying amounts of information.\nLabeling domain-specific images with diverse text-prompts by hand is generally unfeasible. In addition to the high manual effort on a repetitive task, the descriptions would follow the bias of the annotators and be limited in their diversity and creativity. Therefore, we provide a recipe on how to bootstrap the capabilities of large-scale (vision-) language models to automate this task. We employ GPT-40 (OpenAI 2024b), which build upon GPT-4 (OpenAI 2024a) and GPT-4(Vision) (OpenAI 2023) with the additional benefit of providing noticeably faster inference. We use this model because of its state-of-the-art vision-language understanding and reasoning and its easy accessibility through the API. In addition to providing a method for automatic labeling, this task provides insights on the capabilities of GPT-40 for context understanding and reasoning in technical images with fine-grained details."}, {"title": "4.1 Related Work", "content": "Our approach fundamentally is an image captioning task. Previous works address this topic with a variety of approaches, utilizing CLIP-embeddings (Mokady et al. 2021) or vision-language transformers (Zhou et al. 2019; L. H. Li et al. 2019). The development of GPT-4(V) is a major improvement as it possesses vast context understanding and broad general reasoning capabilities to accurately caption contents of an image (Singh et al. 2023; Hwang et al. 2023). Besides the multi-modal variations of GPT-4, other vision-language models like LLaVA (Liu et al. 2023; Liu et al. 2024) are available, but are outperformed on the VisIT-Benchmark for VLMs (Bitton et al. 2023) by GPT-4(V) in terms of model performance across a diverse set of instruction-following tasks.\nThe comprehensive study by Picard et al. 2023 investigates the feasibility of including GPT-4(V) in an engineering design process and thereby underlines our assumptions that the capabilities of the model are sufficient for domain-specific captioning tasks on technical images.\nIn terms of using language models for generating synthetic descriptions of images, a few works are to be named. Cosmopedia (Ben Allal et al. 2024) is a dataset of synthetic textbooks, blogposts, stories, posts and WikiHow articles generated by Mixtral-8x7B-Instruct (Jiang et al. 2023).The dataset contains over 30 million files and 25 billion tokens.\nBLIP-2 employs a two-stage pre-training approach to enhance image-grounded text generation. By using transformers for both image and text processing, BLIP-2 generates accurate and contextually relevant descriptions from images. It demonstrates the ability to perform zero-shot image captioning effectively, making it versatile for various applications (J. Li et al. 2023).\nLAVIS is a whole suite for developing multi-modal models, allowing to rapidly employ and benchmark models for different tasks, image captioning being one of them (D. Li et al. 2023). DreamSync (Sun et al. 2024) employs VLMs to automate the selection of high-quality image-text pairs to finetune a text-to-image model. Garg et al. 2024 employ a VLM together with human annotators to create a dataset of hyper-detailed, synthetic image descriptions."}, {"title": "4.2 Method", "content": "We access GPT-40 through the API and prompt it to generate descriptions of the GeoBiked bicycle images. To obtain diverse descriptions of the images, we prompt GPT-40 to adhere to specific description characteristics. The system-prompt passed to GPT-40 specifically outlines the format of the generated description. The mask for the system-prompt is provided in Appendix C.1.\n\u2022 Length: We distinguish between short (5-10 words), medium (10-20 words) and long (20-40 words) descrip-tions.\n\u2022 Vibe: The model is prompted to generate descriptions with either a technical character or a casual description.\n\u2022 Style: The descriptions have to be either in style of a marketing message or of a prompt for a text-to-image model (Midjourney).\nWe generate text-descriptions with three different data sources that are provided to GPT-40. For the first one, only the bicycle image is provided. It is wrapped in the system-prompt after the general task description and the definition of the required description characteristics. The general task instructs to model to analyse the input information and create a description for it while adhering to the formulated description characteristics. The second data source are labels in the form of technical categories describing the bike. No image is used for this configuration. From GeoBiked, we use bicycle style, rim styles, fork type and whether there are bottles on the down- or seattubes. In the third configuration, we pass the image together with the technical categories."}, {"title": "4.3 Experiments", "content": "First and foremost, we aim to verify if GPT-4o is able to generate creative and diverse descriptions of the samples from GeoBiked, given that the images are of a technical character and contain fine-grained visual details. We further analyze if the generated descriptions follow the required length, vibe and style. By also passing ground-truth labels to the model, we evaluate the possibility of generating diverse, but hallucination-free descriptions as hallucination is a well-known issue with LLMs (Huang et al. 2023).\nIn terms of evaluation metrics, we measure diversity by counting the number of unique outputs generated with different description requirements. This metric provides insights on the repetitiveness of the generated descriptions and therefore indirectly measures how well the VLM can capture different inputs and create unique descriptions from them. Additionally, we compute the Levenshtein-distance between unique outputs as a measurement for the difference between two linguistic sequences (Haldar and Mukhopadhyay 2011). A higher average Levenshtein-distance indicates more diverse text-descriptions. We use the implementation by (Bachmann 2024).\nTo evaluate the accuracy of the generated text-description with the ground-truth labels, we again use GPT-40. In this setting, the model is utilized as a classifier. We instruct it to infer the categorical labels from the text description and then compare the extracted labels from the text-description"}]}