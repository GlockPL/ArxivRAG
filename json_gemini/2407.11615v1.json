{"title": "Graph Dimension Attention Networks for Enterprise Credit Assessment", "authors": ["Shaopeng Wei", "B\u00e9ni Egressy", "Xingyan Chen", "Yu Zhao", "Fuzhen Zhuang", "Roger Wattenhofer", "Gang Kou"], "abstract": "Enterprise credit assessment is critical for evaluating financial risk, and Graph Neural Networks (GNNs), with their advanced capability to model inter-entity relationships, are a natural tool to get a deeper understanding of these financial networks. However, existing GNN-based methodologies predominantly emphasize entity-level attention mechanisms for contagion risk aggregation, often overlooking the heterogeneous importance of different feature dimensions, thus falling short in adequately modeling credit risk levels. To address this issue, we propose a novel architecture named Graph Dimension Attention Network (GDAN), which incorporates a dimension-level attention mechanism to capture fine-grained risk-related characteristics. Furthermore, we explore the interpretability of the GNN-based method in financial scenarios and propose a simple but effective data-centric explainer for GDAN, called GDAN-DistShift. DistShift provides edge-level interpretability by quantifying distribution shifts during the message-passing process. Moreover, we collected a real-world, multi-source Enterprise Credit Assessment Dataset (ECAD) and have made it accessible to the research community since high-quality datasets are lacking in this field. Extensive experiments conducted on ECAD demonstrate the effectiveness of our methods. In addition, we ran GDAN on the well-known datasets SMEsD and DBLP, also with excellent results.", "sections": [{"title": "I. INTRODUCTION", "content": "ENTERPRISE credit assessment is a crucial process, essential for evaluating the credit risk of businesses. Timely identification of these risks provides a solid foundation for economic health and financial stability, helping to avoid considerable financial losses. Graph Neural Networks (GNNs) possess advanced capabilities for modeling relationships between entities, making them a natural tool for comprehending financial networks and addressing the challenge of enterprise credit assessment.\nHowever, current GNN-based risk evaluation models focus on learning entity-level attention for nodes, often overlooking the fact that different feature dimensions encapsulate distinct aspects of an enterprise's risk profile. To illustrate, consider the case of Enterprise Heterogeneous Graphs (EHG) depicted in Figure 1. Specifically, Enterprise A is connected to four neighbors\u2014B, C, D, and E-through three types of relationships: customer (orange), investor (green), and supplier (blue). Each enterprise is characterized by five features: registered capital, time of establishment, number of legal disputes, administrative rewards, and administrative penalties. Notably, Enterprise B is a recently established high-tech firm that heavily relies on government subsidies during its early stages. In this context, government penalties can significantly impact its future cash flow, thereby elevating its credit risk. These two critical risk factors-short establishment time and government penalties are reflected by the second and fifth dimensions in deep red. Enterprise C is part of a large supply chain and faces the risk of disrupted order quantities due to legal disputes with upstream or downstream enterprises, which is reflected in the third dimension. It is noted that, although Enterprise C is not as significant as Enterprise B at the entity level, it still possesses a critical feature that should not be under-weighted. Conventional GNN-based methods assign entity-level weights to neighbors for the target enterprise (e.g., Enterprise A), thereby lacking the ability to discern differences among various dimensions of features, which is critical for capturing risk-related information. Consequently, these methods inadequately represent contagion risk, also known as risk spillover [1].\nTo address this crucial issue, we introduce Graph Dimension Attention Networks (GDAN) as a novel solution to comprehensively model enterprise credit assessment (see Definition 2) within EHG (see Definition 1). In particular, GDAN leverages a dimension-level attention mechanism to autonomously ascertain the significance of individual risk dimensions, thereby enabling the model to discern subtle distinctions among different risk aspects of neighboring nodes. Furthermore, we consider the specific type of each relationship to enhance the modeling of risk-related information."}, {"title": "Definition 1. Enterprise Heterogeneous Graph", "content": "An enterprise heterogeneous graph (EHG) is a graph G = (V, E, A, R, H), where V denotes the set of all nodes, E denotes the set of edges, A denotes the set of node types, R denotes the set of edge types and H denotes the enterprise feature matrix (node input features) consisting of business, lawsuit, and other risk information. An EHG has two associated mapping functions: (i) an node mapping function \u03a6: V \u2192 A, where \\(v_i \\in V\\), \\(\u03a6(v_i) \\in A\\), \\(|A| \\ge 1\\). (ii) an edge mapping function \u222e: E \u2192 R, where \\(e_i \\in E\\), \\(\u03c6(e_i) \\in R\\), \\(|R| \\ge 2\\)."}, {"title": "Definition 2. Enterprise Credit Assessment (ECA)", "content": "Given an enterprise heterogeneous graph, which consists of enterprise risk information and multiple relationship types among enterprises, we aim to assess the enterprise credit level of each node. This can be treated as a multi-class node classification problem."}, {"title": "II. RELATED WORK", "content": "Another significant issue pertains to the model interpretabil- ity, which is a pronounced concern in financial domain. While GDAN is built on deep neural networks, rendering it black- box systems for decision-makers. In the context of GNNs, interpretability hinges on the message-passing process and the complex network of interactions. Specifically, GDAN can change the distribution of input features through the message passing process, which we define as distribution shift, to make them easier to distinguish. Therefore, we propose a simple and effective explainer, termed GDAN-DistShift, to provide data-centric edge-level interpretability for GDAN. To enhance the transparency of DistShift, we employ a simple machine learning (ML) model as a proxy for measuring distribution in tandem with a straightforward aggregation technique.\nFurthermore, the accessibility of data presents a notable impediment to advancing research within the financial domain. The research contributions of studies such as [2]\u2013[5] have inspired advancements in this field. However, it is regrettable that the datasets used in these works are not openly available. Consequently, we collect a comprehensive Enterprise Credit Assessment Dataset (ECAD), which we release for public use. The ECAD encompasses detailed information about 48,758 small and medium-sized enterprises (SMEs), encompassing fundamental business data, litigation records, and records of administrative licenses, rewards and penalties, as well as over one million relationships between them. Finally, we conduct extensive experiments on both the ECAD dataset and publicly available datasets, SMEsD and DBLP, to validate the efficacy of our proposed GDAN model in capturing critical risk information and to assess the effectiveness of DistShift for interpretability.\nThe contributions of this work are threefold:\n\u2022 We propose Graph Dimension Attention Network (GDAN), a novel GNN model. GDAN leverages the dimension attention mechanism within the message-passing process, enabling it to discern and capture pivotal risk-related information. To the best of our knowledge, this is the first work to propose dimension-level attention for enterprise credit assessment.\n\u2022 Furthermore, we present GDAN-DistShift, a simple and effective data-centric interpretability tool for GDAN. DistShift evaluates edge importance as interpretability by capturing the changes in feature distributions caused by message passing along edges. This approach offers greater transparency compared to previous deep neural network-based GNN interpretability methods.\n\u2022 To alleviate the problem of the lack of high-quality, open-access datasets in finance, we manually collect a multi-source dataset, ECAD, for enterprise credit assessment and make it available to the broader research community. We conduct comprehensive experiments utilizing the ECAD dataset, SMEsD dataset, and the widely-recognized DBLP dataset, which demonstrates the effectiveness of GDAN and DistShift."}, {"title": "A. Financial Risk Assessment", "content": "Financial risk encompasses the potential for an enterprise to encounter financial difficulties, default on payments or declare bankruptcy. It is often quantified by credit levels, a comprehensive indicator taking multiple factors into account. Most previous studies in this area primarily center on financial metrics and econometric methodologies [6], [7]. For example, Lopez and Saidenberg [8] introduce a credit assessment approach involving cross-sectional simulations based on panel data spanning multiple years and various assets. Contemporary investigations have integrated machine learning and deep learning techniques, employing methods such as Support Vector Machines (SVM), decision trees, and deep neural networks [9]\u2013[11]. For instance, Zhang et al. [12] employ deep neural networks to amalgamate demographic and behavioral data, enhancing the modeling of credit risk for SMEs. Recent research has also begun to investigate the impact of multi-modal data on financial issues, incorporating sources such as enterprise annual reports, conference calls, and social media. For example, Craja et al. [13] utilize a hierarchical attention network to extract risk-related information from structured documents, such as annual reports, and integrate this with financial ratios to detect instances of financial statement fraud. Borchert et al. [14] take advantage of textual website content and apply deep neural networks to forecast business failures."}, {"title": "B. Graph Neural Networks", "content": "Graph Neural Networks (GNNs) aim to utilize the topological information inherent in network structures. This structure can originate from real-world data sources [17], [18] or be constructed from unstructured data, including images and text [19], [20]. For example, Hu et al. [18] integrate transformer-based attention mechanisms into GNNs for a huge academic network. Xu et al. [21] generate scene graphs from original input images and acquire structured scene representations through GNNs. Early GNNs predominantly focused on homogeneous graphs, such as Graph Convolution Networks (GCN) [22], wherein target nodes aggregate information equally from neighboring nodes. Tsitsulin et al [23] find that GNN pooling methods doesn't work well at clustering graphs. Therefore, they introduce Deep Modularity Networks (DMoN), an unsupervised pooling method for clustering structure of real-world graphs. Many recent GNN applications are based on heterogeneous graphs, which can have different node and relationship types. For example, Wang et al. [24] apply a hierarchical attention mechanism to model recommendation systems, considering multiple relationships between users and items. From the perspective of technique, applying attention mechanism is one of the most popular way for GNNs [25]. Graph Attention Networks (GAT) [26] introduced an attention mechanism to enable GNNs to autonomously learn weights for neighboring nodes. Lee et al. [27] propose AEROGNN, aiming to mitigate over-smoothed features and smooth cumulative attention related to deep graph attention.\nThe application of GNNs in financial contexts is natural due to the wealth of structural information present in financial markets. For instance, Yang et al. [28] propose a dynamic graph-based GNN to model the default risk of supply chain enterprises. Zheng et al. [29] propose triple-level attention-based GNNs to predict the bankruptcy of SMEs. Xiang et al. [30] leverage transaction records to construct temporal transaction graphs and use GNNs to find fraud patterns. Shi et al. [31] first construct edges using kNN and then utilize GNNs to predict credit risk. Their findings show significant improvements compared to using only internal information. Wei et al. [32] also leverage GNNs to combine enterprise intra-risk and inter-risk for bankruptcy prediction.\nNevertheless, it is worth noting that existing GNN-based methods primarily employ entity-level attention, potentially overlooking critical risk factors within specific dimensions."}, {"title": "C. Interpretability of GNNs", "content": "Interpretability has garnered significant attention in the era of deep learning [33], [34]. It is critical, particularly in domains such as finance, where individuals require a comprehensive understanding of model causality to make confident, informed, and explainable decisions. Liu et al. [35] focus on both the performance and interpretability of credit risk assessment. They propose an automatic feature-crossing method to generate cross features for Logistic Regression (LR), thereby endowing the final model with improved per-formance and interpretability. Dumitrescu et al. [36] employ decision trees to augment the capabilities of LR, with the dual objective of capturing non-linear effects while preserving intrinsic interpretability. It is noteworthy that the quest for interpretability in the context of GNNs necessitates a unique consideration of interactions among data points. Specifically, optimal interpretability should elucidate the impact of edges, pinpointing the crucial edges that contribute to a model's decision. For example, Ying et al. [37] propose GNNExplainer, a classical model designed to identify important features, nodes, and subgraphs for GNN interpretability. GNNExplainer achieves this by maximizing the mutual information between the prediction of the original graph and learned patterns. Yuan et al. [38] develop SubgraphX, which aims to identify important subgraphs through Monte Carlo tree search. Yin et al. [39] refer to t pre-training techniques and propose to distill the universal interpretability of GNNs by pre-training over synthetic graphs with ground-truth explanations.\nHowever, most existing research primarily focuses on node representation, with less attention given to modeling the edges. Furthermore, current GNN explanation methods predominantly emphasize model behavior rather than graph data. These methods often employ deep learning techniques to identify significant patterns in GNNs, which may introduce opacity into the interpretability process and potentially obscure the intrinsic nature of GNNS."}, {"title": "III. METHODOLOGY", "content": "This section formally defines GDAN. As illustrated in Figure 2, the GDAN model consists of five parts: (1) Input Projection Layer; (2) Graph Convolution Layer; (3) Heterogeneous Dimension Attention Layer; (4) Risk Merging Layer; (5) Classifier and Optimization.\n1) Input Projection Layer: The Input Projection Layer maps input node features into a common vector space whilst aiming to extract important features. Given an input feature matrix \\(H\\in \\mathbb{R}^{N \\times d}\\), where N denotes the number of enterprises and d denotes the number of risk dimensions. The input projection layer transforms it into the \\(\\mathbb{R}^{N \\times d}\\) latent space:\n\\(\\begin{equation}\\nonumber\\begin{aligned}\\begin{equation} \\label{eq:1} \\mathbf{H} = \\sigma(\\text{Norm}(\\mathbf{H} \\mathbf{W} + \\mathbf{b})), \\end{equation}\\end{aligned}\\end{equation}\\)\nwhere \\(W\\in \\mathbb{R}^{d\\times d}\\) and \\(b \\in \\mathbb{R}^d\\) are trainable parameters, Norm denotes batch normalization, and \u03c3 is the Relu activation function.\nFor general heterogeneous graphs with multiple node types and node type mapping function \u03a6 : \u03a6(vi) \u2192 A, and for all"}, {"title": "A. Graph Dimension Attention Networks", "content": "a \u2208 A, \\(\\mathbb{V}_a := {v \\in \\mathbb{V} \\mid \\Phi(v) = a} \\subseteq \\mathbb{V}\\) and \\(\\mathbb{H}_a \\in \\mathbb{R}^{|\\mathbb{V}_a| \\times d_a}\\), we project nodes of different types into the same latent space:\n\\(\\begin{equation}\\nonumber\\begin{aligned}\\begin{equation} \\label{eq:2} \\mathbf{H}'_\\alpha = \\sigma (\\mathbf{H}_a\\mathbf{W}_a + \\mathbf{b}_a), \\end{equation}\\end{aligned}\\end{equation}\\)\nwhere \\(\\mathbf{W}_a \\in \\mathbb{R}^{d_a\\times d}\\) and \\(\\mathbf{b}_a \\in \\mathbb{R}^d\\) are trainable parameters and \u03c3 is the Relu activation function. We then implement Eq. (1) based on the concatenation of all types of nodes representations \\(\\mathbf{H}'\\).\n2) Graph Convolution Layer: We generate contagion risk representations for each node as follows:\n\\(\\begin{equation}\\nonumber\\begin{aligned}\\begin{equation} \\label{eq:3} \\mathbf{h}_{cont} = \\mathbf{W}_{cont}\\mathbf{h}_i + \\mathbf{b}_{cont}, \\end{equation}\\end{aligned}\\end{equation}\\)\nwhere \\(\\mathbf{W}_{cont}\\) and \\(\\mathbf{b}_{cont}\\) are trainable parameters. \\(\\mathbf{h}_{cont}\\) denotes learned contagion risk. To learn the natural structure information, we conduct basic graph convolution as follows:\n\\(\\begin{equation}\\nonumber\\begin{aligned}\\begin{equation} \\label{eq:4} \\mathbf{h}^\\text{GC}_i = \\sum_{j \\in \\mathcal{N}_i} \\frac{1}{\\sqrt{d_g(i) d_g(j)}} \\mathbf{h}^\\text{cont}_j, \\end{equation}\\end{aligned}\\end{equation}\\)\nwhere \\(d_g()\\) represents the degree function, \\(\\mathcal{N}_i\\) denotes neigh- bors of node i. \\(\\mathbf{h}^\\text{cont}_j\\) and \\(\\mathbf{h}^\\text{GC}_i\\) are the contagion representation of neighbor node j and the Graph-Convolution risk representation of node i, respectively.\n3) Heterogeneous Dimension Attention Layer: Afterwards, we augment the risk representation by enhancing specific dimensions automatically, enabling the model to capture risk-related information across different dimensions. Moreover, to sufficiently model the heterogeneous relations in EHG, we transform heterogeneous neighbor information according to the type of the relation: \\(e_{ij} = \\mathbf{h}^\\text{cont}_j \\mathbf{W}_r\\), where \\(j \\in \\mathcal{N}_i\\) and \\(\\mathbf{W}_r\\) denotes the transformation matrix for \\(r \\in \\mathcal{R}\\).\nThen we calculate the attention score of dimension k regarding target node i and its neighbors of all types. Specifically, we use the Softmax function to normalize the dimension importance across all neighbors that target node i has as follows:\n\\(\\begin{equation}\\nonumber\\begin{aligned}\\begin{equation} \\label{eq:5} {\\alpha}_{ij}^{k} = \\text{Softmax}(e^{k}_{ij}) = \\frac{\\exp(e^{k}_{ij})}{\\sum_{m \\in \\mathcal{N}_i} \\exp(e^{k}_{im})}, \\end{equation}\\end{aligned}\\end{equation}\\)\nwhere \\({\\alpha}_{ij}^{k}\\) is the learned importance of dimension k in terms of target node i and neighbor node j. \\(m \\in \\mathcal{N}_i\\) denotes one neighbor node of i. We aggregate the contagion risk for each node based on the learned attention score of dimensions. Each dimension of Dimension-attention-based risk representation is calculated as follows:\n\\(\\begin{equation}\\nonumber\\begin{aligned}\\begin{equation} \\label{eq:6} {\\mathbf{h}_{i}^{DA,k}} = \\sum_{j \\in \\mathcal{N}_i} {\\alpha}_{ij}^{k} . {\\mathbf{h}^{cont,k}_j}, \\end{equation}\\end{aligned}\\end{equation}\\)\n4) Risk Merging Layer: We merge the Dimension-Attention-based and Graph-Convolution-based risk representations to generate comprehensive risk representations. We utilize a hyper-parameter \\(\\beta\\) to balance the two representations as follows:\n\\(\\begin{equation}\\nonumber\\begin{aligned}\\begin{equation} \\label{eq:7} \\mathbf{h}^{f}_i = \\text{Dropout} (\\sigma (\\mathbf{W}^f (\\mathbf{h}^{GC}_i + \\beta \\cdot \\mathbf{h}^{DA}_i) + \\mathbf{b}^f)), \\end{equation}\\end{aligned}\\end{equation}\\)\nwhere \\(\\mathbf{h}^{f}_i\\) denotes the enhanced risk representation of node i. \\(\\mathbf{W}^f\\) and \\(\\mathbf{b}^f\\) are trainable parameters. \u03c3 is the activation function; we use Relu in our models. We use dropout to alleviate potential problems with over-fitting.\n5) Classifier and Optimization: Then, we utilize an MLP layer to generate final risk representations of the target node and normalize it using Softmax:\n\\(\\begin{equation}\\nonumber\\begin{aligned}\\begin{equation} \\label{eq:8} \\mathbf{y}_i = \\text{Softmax(MLP}(\\mathbf{h}^{f}_i)), \\end{equation}\\end{aligned}\\end{equation}\\)\nwhere \\(\\mathbf{y}_i\\) denotes the probability that node i belongs to risk level c. During inference, the risk level with the highest probability \\(\\hat{y}_i\\), is taken as the prediction.\nThe enterprise credit assessment can be treated as a node classification problem, thus we train our model by minimizing the cross-entropy loss as follows:\n\\(\\begin{equation}\\nonumber\\begin{aligned}\\begin{equation} \\label{eq:9} \\mathcal{L} = - \\sum_{i \\in \\mathbb{V}_L} {y_i} \\log(\\hat{y}_i), \\end{equation}\\end{aligned}\\end{equation}\\)\nwhere \\(\\mathbb{V}_L\\) is the set of enterprises with ground truth credit levels. \\(y_i\\) is the one-hot encoding of the ground truth label for node i and \\(\\hat{y}_i\\) is the vector of predicted probabilities for each credit level for node i."}, {"title": "B. Discussion of Dimension Attention", "content": "Comparing with Entity Level Attention. As illustrated in Figure 3 part (a), an EHG comprises an adjacency matrix \\(A\\in \\mathbb{R}^{N \\times N}\\) and node features \\(H \\in \\mathbb{R}^{N\\times d}\\). Conventional GNNs with entity-level attention generate attention scores, creating an N\u00d7N matrix, as shown in part (b). However, these models overlook variations in importance across different feature dimensions. In GNNs with dimension attention, attention scores are learned by accounting for the contributions of various feature dimensions. This results in an Nx d matrix for each node, as depicted in part (c). When all dimensions contribute equally, the dimension attention scores degenerate into entity-level attention scores. We believe that the dimension attention mechanism empowers GNN models to capture more nuanced information, thereby enhancing their ability to better fit the data.\nTime Complexity Analysis. We confirm that it is feasible to apply the approach to large datasets because the time complexity of our method correlates linearly with the size of the graphs. In our approach, we perform three crucial steps in the dimension attention operation, detailed in \\(e_{ij} = \\mathbf{h}^{cont}_j \\mathbf{W}_r\\), Eq. (5) and Eq. (6). To break it down further, when we consider the edge count |E| and node count |V| to represent the graph's scale, we find that:\n\u2022 The time complexity of the neighbor-relation based op- eration, \\(e_{ij} = \\mathbf{h}^{cont}_j \\mathbf{W}_r\\), is O(|E|).\n\u2022 For the equation \\({\\alpha}_{ij}^{k} = \\text{Softmax}(e^{k}_{ij}) = \\frac{\\exp(e^{k}_{ij})}{\\sum_{m \\in \\mathcal{N}_i} \\exp(e^{k}_{im})}\\), the denominator's time complexity is O(|V|) due to node summarization. This operation can be done with parallel computing in GPU, which makes it more efficient in practice. Meanwhile, the numerator's count of \\(e^{k}_{ij}\\) scales linearly with E.\n\u2022 Furthermore, the summarization operation in Eq. (6) scales linearly with V.\nTaking all these factors into account, the total time complexity sums up to O(|V|+|E|). This finding assures the scalability of our proposed model when dealing with large datasets."}, {"title": "C. DistShift for GDAN", "content": "In the realm of financial decision-making, interpretability is as crucial as performance itself. To address this need, we propose GDAN-DistShift, a simple yet effective data-centric explanation method for GDAN.\nGNNs have demonstrated superior efficacy compared to earlier machine learning and deep learning approaches when applied to graph data. This heightened performance can be attributed to GNNs' unique capacity to leverage both the feature information of data points and the intricate interactions among nodes.\nGiven an input that includes the original features of nodes, H, it is feasible to employ various techniques to extract salient features and subsequently perform classification, as follows:\n\\(\\begin{equation}\\nonumber\\begin{aligned}\\begin{equation} \\label{eq:10} \\hat{y}_0 = \\text{Classifier}(f(H)), \\end{equation}\\end{aligned}\\end{equation}\\)\nThe reason we use GNNs is that there are edges that help to improve the model features of input nodes. In essence, all GNN-based classification models can be simplified as follows:\n\\(\\begin{equation}\\nonumber\\begin{aligned}\\begin{equation} \\label{eq:11} \\hat{y}_1 = \\text{Classifier} (GNN(H, G)), \\end{equation}\\end{aligned}\\end{equation}\\)\nWhen comparing Equation (11) and Equation (10), a notable observation emerges: alterations in the input characteristics of classifiers yield consequential shifts in the embeddings, which can ultimately lead to changes in model performance. Consequently, as Figure 4 shows, when employing a common ML model like LR as the classifier, models founded on GNNS can perform better than models relying solely on the traditional ML paradigm.\nWe therefore introduce a novel method to assess edge importance by quantifying the distribution shift during message passing. We believe that this approach is effective for GDAN, as it is a typical message-passing based GNN model. In an ideal scenario with ground truth labels for all the nodes in a dataset, we would compare the probability for the true class with an edge and without it. If the presence of the edge increases the true class prediction, then we would consider this an important edge. However, in EHG scenario, the ground truth labels are sparse. So we instead utilize the entropy of the predicted probabilities as a proxy. The idea is that low entropy outputs are easier to separate in the embedding space, and therefore edges that decrease entropy for a node will be considered important.\nTo enhance stability, we employ an unbalanced sampling strategy for subgraph selection, thereby comprehensively evaluating edge significance across a range of subgraphs."}, {"title": "IV. EXPERIMENTS", "content": "ECAD. Due to the lack of high quality public financial risk datasets, we construct a new dataset ECAD, for enterprise credit assessment, by collecting data from multiple sources. We first sample a subset of 512 enterprises, from the population of enterprises with credit rating records. We then search for their one-hop neighbors according to three relationship types, which are detailed below. Afterwards, we crawl public risk information for those enterprises. The final dataset consists of 48,758 enterprises, including their multi-source risk information and more than one million connections between them. There are 1274 enterprises that have labels, including good, normal and bad, which refer to AAA, AA and others (i.e., A, BBB, BB etc.), respectively. The risk information includes an enterprise's basic business information, lawsuit information, administrative licenses, rewards and penalties. We preprocess the raw risk information to transform it into numbers as features. There are three types of relationships among enterprises: Investor_holder relations, Branch relations and Shareholder relations. We split the enterprises with labels into train, validation, and test sets according to a (0.6, 0.2, 0.2) ratio. We show the statistics of the dataset in Table I.\nSMESD. To further validate the performance of GDAN, we also implement experiments on another financial risk dataset,"}, {"title": "A. Datasets", "content": "B. Baselines\nWe compare the proposed model with two types of current state-of-the-art (SOTA) baselines: (1) Machine learning (ML) models, including LR [44], SVM [45] and GBDT [46], which only consider the feature information of enterprise nodes or authors nodes; (2) Graph Neural Networks, which take the whole EHG graph into consideration. Specifically, the GNNS baselines can be further divided into homogeneous graph-based GNNs (GCN [22], GAT [26]), heterogeneous graph-based GNNs (HAN [24], HGT [18] and SeHGNN [47]) and GNNs for financial risk prediction (HAT [29] and ComRisk [32]). More details are shown as following:\nMachine Learning Models:\n\u2022 Logistic Regression (LR) [44]: This is a widely-used simple and efficient classification method.\n\u2022 Support Vector Machine (SVM) [45]: This model utilizes support vectors to divide vector spaces into different classes.\n\u2022 Gradient Boosting Decision Tree (GBDT) [46]: This is a classic tree classification model of conventional ML.\nGraph Neural Networks\n\u2022 Graph Convolutional Networks (GCN) [22]: This model updates node embeddings iteratively by averaging neigh- boring representations.\n\u2022 Graph Attention Networks (GAT) [26]: This model em- ploys an attention mechanism to automatically assign different weights to neighbors during the message-passing process.\n\u2022 Heterogeneous Graph Attention Network (HAN) [24]: This model proposes to learn heterogeneous graphs with a hierarchical attention mechanism."}, {"title": "C. Experiment Setting", "content": "\u2022 Heterogeneous Graph Transformer (HGT [18]): This model applies the transformer attention mechanism in the message-passing process on heterogeneous graphs.\n\u2022 Simple and Efficient Heterogeneous Graph Neural Net- work (SeHGNN [47]): This model highlights the use of multi-hop metapath-based neighbor information using an attention mechanism.\n\u2022 Heterogeneous-attention-network-based model (HAT) [29]: This model leverages triple-level attention for SMEs' bankruptcy prediction.\n\u2022 Graph Neural Networks for Enterprise Bankruptcy Pre- diction (ComRisk [32]): This model utilizes hierarchi- cal heterogeneous attention mechanism for enterprise bankruptcy prediction.\nWe implement the proposed model and all baseline models on three datasets (i.e., ECAD and DBLP) and evaluate them with three indicators (i.e., accuracy, macro-F1, and AUC). We use the scikit-learn package to run machine learning models. For SeHGNN, we use the official code in the paper [47] and we use the implementation code provided by PyG for all other GNN models. We run all the models five times and report the average performance. All the GNNs models are based on PyTorch and PyG and trained with AdamW [48]. The learning rate is adjusted between 0.001 to 0.01 by the Cosine Annealing Learning Rate Scheduler [49]. For the ECAD dataset, we set the input dimension, hidden dimension and output dimension (before classifier) for all GNN models as 99, 64, and 24, respectively. For the SMEsD dataset, we set the input dimension, hidden dimension and output dimension as 23, 18 and 12, respectively. For DBLP, we fine-tune GNN models with dimensions from {512, 256, 128, 64, 32} for the hidden layers and the output layer (before classifier). We run for 200 epochs on ECAD, 400 epochs on SMEsD, and for 100 epochs on DBLP. We choose the model with the best performance in terms of accuracy for ECAD and SMEsD, and in terms of macro-F1 for DBLP on the validation set and test it on the test set. We fine-tune all GNN models with layer numbers from {1,2,4,8}, head number from {2,4,8} and weight decay from {0.1, 0.01, 0.001}."}, {"title": "D. ECA and Node Classification", "content": "We report the experimental results of the proposed model and all baseline models in Table II. We can observe that GDAN outperforms all baseline models on all three datasets. Specifically, our model achieves improvements of 3.82% accuracy, 3.42% macro-F1, and 2.41% AUC against the state-of-art baseline models (ComRisk and SeHGNN) on ECAD. Our model also improves upon ComRisk, HAT and SeHGNN performances on the SMEsD and DBLP dataset by all mea-sures. The results demonstrate the effectiveness of GDAN in learning representations on complex heterogeneous graphs.\nAnalysis. (1) We observe from Table II that the three ML models (i.e., LR, SVM and GBDT) achieve commendable results on the ECAD dataset, which confirms that the collected multi-source risk information enhances the assessment of a company's credit risk. Our model, when compared to these machine learning models, exhibits substantial improvements across all metrics on all datasets, confirming GDAN's ability to model topological information. (2) Our investigation reveals that all GNN models outperform traditional ML models, underscoring the pivotal role of topology information. Furthermore, among both heterogeneous graph-based models (RGCN, HAN, HGT, SeHGNN) and GNNs for financial risk prediction (HAT and ComRisk), most of them outperform homogeneous graph-based models (GAT) on the ECAD dataset, underscoring the need to model heterogeneous relationships. However, it is worth noting that GCN performs exceptionally well on the ECAD dataset, likely due to the predominance of edges of the same type, which can lead to over-fitting challenges for heterogeneous GNNs when encoding edge-type information in EHG. (3) In contrast, our model consistently outperforms all other graph neural network models on all datasets, emphasizing the significance of distinguishing between feature dimensions in the message-passing process."}, {"title": "E. Node Clustering", "content": "We employ node clustering to demonstrate GDAN's effi-cacy. To begin, we execute a feed-forward process across all GNN models on the test set of ECAD to obtain the enterprise representations. Subsequently, we apply K-Means clustering to these representations to derive node clustering labels. We then assess the quality of the clustering results by comparing the predicted labels with the ground truth labels of the enterprises, utilizing normalized mutual information (NMI) and adjusted Rand index (ARI) as evaluation metrics. In order to enhance result reliability, we repeat this entire process 10 times and present the average results in Table III. The experimental outcomes substantiate the superior performance of our model when compared with all the baseline models."}, {"title": "G. Ablation Study", "content": "H. Dimension Attention as Plug-in Module\nTo demonstrate the effectiveness of the dimension atten- tion mechanism, we conduct an ablation study. We compare the performance of GDAN with an identical implementation without dimension attention. We can observe from Figure 7 that, compared with the model without dimension attention, GDAN does better in all indicators on the three datasets. The results confirm the notion that dimension-based attention can enhance the performance of traditional GNNs.\nWe demonstrate that the dimension attention module can serve as a plug-in module to augment various GNNs. Specif- ically, we integrate this module into popular GNNs such as GCN, GAT, RGCN [51], HAN, and HGT, treating them as base models. We show the performances of GNNs and the augmented versions regarding the two financial risk datasets (i.e., ECAD and SMEsD) and three indicators (i.e., Accuracy, F1 and AUC). As depicted in Figure 6, it is evident that all the augmented base models consistently outperform their original counterparts."}, {"title": "I. Parameter Analysis", "content": "The balance between graph convolution-based representa- tions and dimension attention-based representations is impor- tant for the final capability. We show the performance of GDAN for different values of \\(\\beta\\) on the DBLP dataset, ranging from {0, 0.01, 0.1, 0.5, 1}. We can observe from Figure 8 that a good \\(\\beta\\) value helps the model achieve better performance. When comparing the performance under the setting \\(\\beta\\) = 0 and \\(\\beta\\) = 0"}]}