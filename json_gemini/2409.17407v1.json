{"title": "POST-HOC REWARD CALIBRATION: A CASE STUDY ON LENGTH BIAS", "authors": ["Zeyu Huang", "Zihan Qiu", "Zili Wang", "Edoardo M. Ponti", "Ivan Titov"], "abstract": "Reinforcement Learning from Human Feedback aligns the outputs of Large Language Models with human values and preferences. Central to this process is the reward model (RM), which translates human feedback into training signals for optimising LLM behaviour. However, RMs can develop biases by exploiting spurious correlations in their training data, such as favouring outputs based on length or style rather than true quality. These biases can lead to incorrect output rankings, sub-optimal model evaluations, and the amplification of undesirable behaviours in LLMs alignment. This paper addresses the challenge of correcting such biases without additional data and training, introducing the concept of Post-hoc Reward Calibration. We first propose an intuitive approach to estimate the bias term and, thus, remove it to approximate the underlying true reward. We then extend the approach to a more general and robust form with the Locally Weighted Regression. Focusing on the prevalent length bias, we validate our proposed approaches across three experimental settings, demonstrating consistent improvements: (1) a 3.11 average performance gain across 33 reward models on the RewardBench dataset; (2) enhanced alignment of RM rankings with GPT-4 evaluations and human preferences based on the AlpacaEval benchmark; and (3) improved Length-Controlled win rate of the RLHF process in multiple LLM-RM combinations. Our method is computationally efficient and generalisable to other types of bias and RMs, offering a scalable and robust solution for mitigating biases in LLM alignment.", "sections": [{"title": "1 INTRODUCTION", "content": "Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Kaufmann et al., 2023) is driving the success of Large Language Models (LLMs) (OpenAI, 2023; Anil et al., 2023; Dubey et al., 2024). By integrating human preferences into the training loop, RLHF aligns models with desired behaviours and human values (Chakraborty et al., 2024), enabling LLMs to generate content that is conceptually appropriate (Sun et al., 2023), ethically sound (Hendrycks et al., 2021a; Dai et al., 2024), and reflecting nuanced human judgements (Wu et al., 2023). A key component in this process is the reward model (RM), which translates qualitative human feedback into a signal for optimising LLMs. The primary purpose of RM training is to approximate human preferences, allowing it to evaluate the content generated by LLMs by assigning scores. These scores (rewards) are then used to optimize the model, shaping its behaviour to better meet human expectations.\nHowever, there is a risk that an RM may exploit spurious correlations in the training data that do not genuinely reflect the output quality (Stiennon et al., 2020; Gao et al., 2023; Singhal et al., 2023; Pang et al., 2023; Lambert & Calandra, 2023). For instance, an RM might favour outputs based on length (Park et al., 2024a) or style (Lambert et al., 2024; Dubois et al., 2023a), leading to biased assessments that do not accurately represent human preferences. The involvement of a biased RM in developing LLMs can have significant drawbacks. Firstly, it may lead to incorrect rankings of outputs, favouring content that aligns with learned biases over genuinely high-quality outputs. This issue is particularly problematic when RMs are used to rank different models during LLM evaluations, as a biased RM could produce misleading results, endorsing suboptimal models and impeding progress in model development. Moreover, when biased RMs are employed in RLHF, these biases can be further amplified (Zheng et al., 2023b), with the model learning to exploit these biases rather than improving genuine task performance\u2014a phenomenon known as reward hacking.\nRecent advancements introduce various strategies to mitigate these biases and alleviate reward hacking during different stages of RLHF, from data handling (Park et al., 2024a) and reward model engineering (Coste et al., 2024) to the final reinforcement learning phase (Chen et al., 2024). Most of these approaches, however, either require additional data collection, re-engineering of the RM, or modifications to reinforcement learning algorithms, which raises an intriguing question: is it possible to correct or mitigate biases in reward signals in a training-free manner? Specifically, given a set of scored outputs from the RM and a specific bias that the model may exhibit, how can we calibrate these reward signals to reflect the true quality of the outputs more accurately?\nThis paper makes an initial attempt at answering this question by framing it as Post-hoc Reward Calibration. Specifically, for a particular characteristic of the output (e.g., the length), we assume the biased reward from the RM can be decomposed into the sum of (1) the underlying calibrated reward and (2) a bias term that solely depends on that characteristic. Under certain assumptions, we propose to use the local average reward to provide an estimation for the bias term, which can then be removed, thereby approximating the true reward. We then extend it to be more general and robust using Locally Weighted Regression for bias estimation. Among different biases (Zhang et al., 2024b), a particularly prevalent one is the sequence length. Prior works shown that models generate significantly longer responses after RLHF without improving the genuine quality of the content (Kabir et al., 2023; Wu et al., 2023; 2024). Singhal et al. (2023) point out that even a purely length-based reward can reproduce most downstream RLHF improvements of existing RMs, highlighting that current RMs fail to convincingly outperform simple heuristics and may be strongly biased by the length. The issue is also evident in powerful proprietary models like GPT-4, which is increasingly used as a judge, resulting in LLM evaluations that favour more verbose responses. Focusing on length bias, we validate our method in three settings and observe consistent improvements: (1) Benchmark performance: a 3.11 performance gain averaged over 33 RMs on the RewardBench; (2) RM as LLMs evaluators: based on AlpacaEval, we utilise 8 open-source RMs to rank 184 LLMs. After applying our calibration, the rankings correlate better with GPT-4 evaluations and human preferences. (3) RM for LLMs alignment: We assess calibrated rewards for RLHF process. Testing with four LLM-RM configurations, we observe consistent improvements in AlpacaEval2 Length-controlled win rates.\nIn addition to being computationally efficient (e.g., calibrating over 300k samples takes only 30 seconds with a single CPU), we empirically reveal that our method can generalise to other biases, such as the presence of markdown-style features in the outputs, and to pairwise GPT4-as-Judge models. Importantly, the calibration effect is strongly correlated with the RM's preference for specific properties: for strongly biased RMs, the calibration yields greater improvements, while for weakly biased RMs, it only slightly alters its rewarding behaviour. These findings suggest that our proposed Post-hoc Reward Calibration method enhances the reliability of RMs and offers a practical, scalable solution for mitigating biases in RLHF. This approach could be instrumental in advancing RLHF methodologies and developing more robust and aligned LLMs."}, {"title": "2 RELATED WORKS", "content": "Reward Models Given a prompt q and two corresponding responses y1 and y2, training an RM usually involves training a classifier to predict the human preference probability p* (Y1 > Y2 q) of two responses y1 and y2 with the Bradley-Terry (BT) model (Bradley & Terry, 1952):\n$P* (Y1 > Y2|q) = \\sigma(r*(x1) - r*(x2))$\nwhere xi = (q, yi) represents the prompt-response pair, r* is the latent true reward we cannot access, and o is the logistic function. Then, given a reward modelling dataset D = {(q\u00b2, y1, y2)}}1 = {(x1,x)}1, where y\u2081 is annotated as a better response to prompt q\u2081 compared to y, training a RM re is to minimise the negative log-likelihood for a binary classification problem:\n$L(0) = -E(x1,x2)~D[logo (re(x1) \u2013 ro(x2))]$\nIn practice, RMs are often implemented by adding an extra linear layer to LLMs, which projects the representations of the last layers of LLMs to a scalar as the reward. There are also two other types of RMs: (1) DPO-based: Models optimised with Direct Preference Optimisation (DPO) (Rafailov et al., 2023) could function as an implicit RM. (2) LLM-as-Judge: A generative LLM can also be prompted to function as an RM to generate feedback, usually consisting of a piece of chain-of-thought reasoning and a final score representing the overall quality (Zheng et al., 2023a; Kwon et al., 2023; Kim et al., 2024). In this paper, we validate the effectiveness of our methods across all three types of judge models. Specifically, we employ our method to calibrate BT-based and DPO-based RMs on the RewardBench dataset, and to calibrate the pairwise judge of GPT-4 with the AlpacaEval benchmark.\nBias Mitigation for RLHF Recent works explore the bias mitigation, also termed as mitigate reward over-optimisation, for the RLHF process mainly from the perspective of (1) Data handling: Park et al. (2024a) identifies six types of inherent bias in various judge models and employ LLMs to generate the de-biasing dataset for further fine-tuning. Zhu et al. (2024) propose to iteratively replace hard labels in data with soft labels during RM training, while Rita et al. (2024) rely on human expert demonstration to recalibrate the reward objective optimized with Reinforcement Learning. (2) Reward Model Engineering: Coste et al. (2024); Eisenstein et al. (2023); Sun et al. (2024); Zhang et al. (2024a) implement the reward ensemble; Ram\u00e9 et al. (2024) first fine-tune multiple RMs and then average them in the weight space; Dai et al. (2024) decouple the reward model training for helpfulness and harmlessness and Chen et al. (2024) disentangle the length signal and the quality signal into two different reward heads. Shen et al. (2023); Wang et al. (2024); Quan (2024) exploit the idea of Mixture-of-Experts (Qiu et al., 2024a;b) in RM training to capture different facets of human preference to avoid the potential conflict between them; (3) Reinforcement Learning methods: Another research line (Moskovitz et al., 2024; Sun et al., 2024; Zhou et al., 2024; Zhai et al., 2024) modifies the algorithm or optimisation objective during reinforcement learning with Park et al. (2024b); Zhu et al. (2024); Lu et al. (2024) specifically focusing on the length bias. Our work takes a different approach from previous methods by employing post-hoc reward calibration, i.e., using only a batch of scored prompt-response examples to mitigate the reward model's bias, without intervening the preference data collection, RM training, and the RLHF phase."}, {"title": "3 METHOD", "content": "3.1 PROBLEM STATEMENT: REWARD CALIBRATION\nThe reward calibration problem could be formalised as follows: given an input-output pair x to be evaluated, the reward model re assigns a scalar re(x) to represent its quality. However, having been trained on the human preference dataset, the reward model re may learn shortcuts to approximate the human preference and thus have biases regarding some characteristics of the input-output pair x. For example, it tends to assign higher rewards to outputs with longer length or containing knowledge commonly encountered in the real-world data (Park et al., 2024a). Supposing a measurable characteristic of interest c is a function that maps the input-output pair x to a scalar c(x):\n$c: X \\rightarrow c(x)$\nWe assume that the biased reward from the reward model ro(x) could be decomposed into two terms, an underlying calibrated reward r(x) and a bias bo(c(x)) with regards characteristic c:\n$ro(x) = r(x) + bo(c(x))$\nThe RM is usually used to calculate the margin between two outputs (x1, x2) to predict the human preference:\n$\\Delta ro (X1,X2) = ro(x1) - ro(x2) = r(x1) - ro(x2) + bo(c(x1)) - bo(c(x2))$\nThus, the reward calibration problem is to estimate the reward difference owing to the bias term and subtract it to recover the underlying true reward margin \u2206*, (x1, x2) = r(x1) - ro(x2).\nPost-hoc Reward Calibration: The proposed reward calibration problem is applicable to various stages of RLHF. Depending on the focus, it can be tailored either toward collecting debiased preference data, refining the training of reward models, or even enhancing the alignment procedure by adjusting the alignment objectives. In contrast, this paper focuses on the post-hoc setting, performing reward calibration after the reward assignment without interfering with any data collection or RM training stage. Formally, given N already-rewarded data points (x1, x2, Aro (x1, x2)}{1, we aim to find the calibrated reward margin A*(x1, x2) that is closer to the underlying oracle margin *(x1, x2)."}, {"title": "3.2 BIAS ESTIMATION", "content": "Assumptions We make the following hypothesis about the decomposition in Eq. 3. (1) Independence of the biased characteristic: consider a general reward modeling dataset composed of prompts and corresponding responses pairs D = {(q\u00b2, y1, y2)}=1 = {(x1, x2)}1, the expectation of the underlying gold reward margin should be 0 and is independent of the biased characteristic c,\n$E(x1,x2~D) [\\Delta (X1, X2) | C(x1) = C1, C(x2) = C2] = 0$\n(2) Sufficient Density: the function values of the characteristic function e is assumed to be sufficiently dense in its range c(X). (3) Lipschitz Continuity: the bias term be is a slow-varying function of characteristic c, which means if pairs x1 and 12 are close to each other regarding the characteristic c, their corresponding bias term in Eq. 3 should be close as well. Mathematically, given c(x0) and a neighbourhood V(xo) = {c(x) | |c(x) - c(xo)| < \u20ac} of xo, for all c(x1), c(x2) \u2208 V, there exists a constant K such that bo(c(x1)) - bo(c(x1))| <K|c(x) - c(xo)|;\n3.2 BIAS ESTIMATION\nUniform Averaging Approach According to Eq. 4, the reward margin between 11 and 12 consists of (1) the underlying calibrated margin \u2206*, (x1, x2) and (2) the biased \u201cbonus reward\u201d that solely depend on the characteristic c(x1) and c(x2). Intuitively, to estimate b\u00ba(c(x1)) \u2013 bo(c(x2)), we ask:\nGiven the characteristic measures c(x1) and c(x2) for an arbitrary pair (x1, x2), what would be their reward margin?\nA straightforward estimation for this question could be\n$E[ro(x) | c(x) = c(x1)] - E[ro(x) | C(x) = c(x2)].$\nUnder the assumption of Lipschitz continuity and sufficient density, we can obtain a simple calibrated reward margin described in the Eq. 6:\n$\\Delta*(x1, x2) = \\Delta re (x1, x2) \u2212 (E[ro(x) | |c(x) - c(x1)| < d] \u2013 E[ro(x) | |c(x) - c(x2)| < d])$\nwhere d is a threshold distance that governs the neighbourhood size around 11 and 12.\nLocally-Weighted-Regression Method How do we choose the threshold distance d in Eq. 6? (1) It must be sufficiently small to ensure that the bias term can be approximated as a constant function within the local neighbourhood, enabling the application of Eq.6; and (2) it must also be large enough to meet the sufficient density assumption, ensuring sufficient data points used for estimation. Satisfying these conflicting requirements may necessitate thorough hyper-parameter tuning of d to employ the intuitive approach in practice. Furthermore, the density may not be uniform in practice, so that the optimal threshold distance d may vary for different data points c(x). Therefore, a desirable approach should be able to estimate the expectations in Eq. 5 in a more general, robust, and practical way by (1) relaxing the constant-function assumption and (2) mitigating the sensitivity to the choice of neighbourhood size, possibly by incorporating certain adaptive mechanisms to optimise the neighbourhood size or the contribution for each datapoint dynamically. Both desiderata motivate us to employ Locally Weighted Regression (LWR) towards a more robust estimation.\nSpecifically, given a point of interest (e.g., a characteristic value c(x) in our case), the Locally Weighted Regression (LWR) method first uses a bandwidth parameter f to define the fraction of the dataset utilised for regression. Then the method assigns weights to neighbouring data points, giving higher weights to points closer to the target and lower weights to those farther away. This approach ensures that nearby points have a greater influence on the regression. LWR then fits a weighted linear regression to model the local behaviour of the target function, effectively approximating the weighted average within the specified local context.\nLWR addresses the desiderata by offering a more adaptable approach to estimate the Eq. 5. It relaxes the constant-function assumption by introducing the linear approximation to capture the subtle local variations in the data. This is reasonable because the bias terms are slow-varying functions and can be linearly approximated using Taylor's expansion. Furthermore, LWR mitigates sensitivity to neighbourhood size by using a distance-based weighting mechanism, which assigns greater importance to closer data points. This adaptive weighting scheme dynamically adjusts the influence of each data point, reducing reliance on a fixed neighbourhood size and enhancing the robustness and flexibility of the estimation process. Formally, given a dataset consisting of the characteristic e in outputs and corresponding rewards De = {(c(xi), ro(x))}=1 and a bandwidth f, our general method could be formulated as:\n$\\Delta*(x1, x2) = \\Delta ro (X1,X2) \u2013 (ro (C(X1)) \u2013 ro(c(x2)))$\nwhere re(c(x1)) and re(c(x2)) are the LWR prediction for the characteristic c(x1) and c(x2), respectively, given the dataset De. To determine the preference, we use the sign of reward margin between x1 and 22: sgn(A*(x1,x2)). In practice, we employ the Robust Locally Estimated Scatterplot Smoothing (LOWESS), a robust version of LWR. Based one LWR, it defines different weights \u03b4\u03ae for the point (c(x), re(xj)) according to the size of residual re(xj) \u2013 fe(c(xj)). It assigns smaller weights to data points with large residuals and large weights to those with small residuals. New fitted values are computed using LWR again but with wj replaced by djwj. The computation of new weights and fitted values is repeated several times to converge. The full algorithm of using LOWESS for reward calibration is described with Algorithm 1. We directly employ off-the-shelf library statsmodels.api and pass the dataset De to it for our implementation.\nLength Bias Estimation This paper prioritises the prevalent length bias presented in the reward model. Thus, the characteristic of interest here is the character length of the output, i.e., c(x) = |x|. The dataset used for regression consists of the length of the output and its corresponding rewards."}, {"title": "4 EXPERIMENTS", "content": "We test the performance of the proposed reward calibration method in three settings: (1) RM Benchmark performance: we test the calibrated RM's performance on the RewardBench dataset (Lambert et al., 2024). (2) RM As LLMs Evaluator: Based on the AlpacaEval benchmark (Dubois et al., 2023a), we employ RMs to label the output of different LLMs from AlpacaEval leaderboard and produce a ranking of LLMs. Then, we measure the correlation between the RM's ranking and GPT-4 ranking (AlpacaEval2) or human preference ranking (ChatbotArena). (3) RM for LLMs alignment: We utilise the calibrated rewards rather than the original reward to label the preference data, and then we conduct the Direct Preference Optimization (Rafailov et al., 2023) with the labelled preference pair. We evaluate the aligned model's performance using AlpacaEval2 and eight popular benchmarks.\nIn all three settings, we are given a group of data points D\\x\\ = {xi, ro(xi)} for calibration. Because LOWESS requires calculating the residual for every fitted data point, we employ the entire D|x| for regression. Therefore, for RewardBench, there are 2,985 test data points, so we employ all 5,970 data points to perform the regression (every test data is a pair of responses for one prompt). Regarding AlpacaEval, there are 184 LLMs and 805 instructions for each model, resulting in 151k samples used for calibration. For LLMs alignment, five responses are generated from the model for about 60k instructions, leading to about 300k samples used for bias estimation. The proposed method is computation-efficient, and calibrating 300k samples only takes 30 seconds using a single CPU.\nCompared Algorithms For all three experimental settings, we try to compare the following algorithms: (1) Original reward; (2) Length penalty: a broadly adopted approach to mitigate the length bias in reward models. It adds a penalty term to the original reward (Singhal et al., 2023; Park et al., 2024b; Liu et al., 2024; Dong et al., 2024), formalised as r*(x) = ro(x) -\u03b1xx. Following Dong et al. (2024), we use the character length of each output and set a = 0.001. (3) RC-Mean: our proposed uniform averaging calibration method described in Eq. 6. (4) RC-LWR: our proposed Locally Weighted Regression calibration method shown in Eq. 7. (5) RC-LWR-Penalty: to see whether our method could be applied on top of the penalty-based methods, we first apply the length penalty to the original reward and then use the RC-LWR. All hyper-parameters related to RC-Mean, RC-LWR and RC-LWR-Penalty are presented in Appendix B."}, {"title": "4.1 LENGTH CALIBRATED REWARDS ON REWARDBENCH", "content": "Setting and Metrics We first validate our proposed calibration algorithms using the RewardBench dataset. Each data point in RewardBench comprises one prompt q and two LLMs-generated responses Yw and y\u0131. The final score is calculated as the averaged accuracy of reward models in identifying Yw over four subsets: Chat, Chat Hard, Safety, and Reasoning. We mainly investigate 33 BT-based RMS"}, {"title": "4.2 LENGTH CALIBRATED REWARD AS LLMS EVALUATORS", "content": "Settings A critical application for RM is to serve as a judge to replace human evaluation for evaluating LLMs. Most existing methods (Dubois et al., 2023b; 2024; Li et al., 2024; Kwan et al., 2024) use strong generative LLMs as the judge model. For instance, AlpacaEval (AE1) uses the GPT4's output logits to calculate the preference probability of the evaluated LLM's output, y, against the responses of the baseline LLM (GPT4-1106-preview), yb, for each prompt q. This preference probability is then averaged to determine the win rate to rank different LLMs. AlpacaEval2 (AE2) further introduced a length-controlled win rate, a regression-based method to mitigate GPT4's length bias. In our setting, we primarily use BT-based RMs to score the LLM's output. Given a prompt q and corresponding output y from the evaluated LLM and output ys from the baseline LLM, the RM scores them as r and rs, respectively; then the Bradley-Terry model yields the preference probability p(y > yb|q) = \u03c3(r \u2013 rb). We then compute the average of p(y > yb|q) across all prompts to determine the final win rate for ranking all evaluated LLMs. We directly use the model's output from the AlpacaEval official repo. Additionally, we apply the length-controlled regression method for our BT-based RMs as one of our baselines, denoted as LC. To further validate the generality of our proposed method, we also apply RC-LWR to AE1. Since AE1 uses GPT4 to compare two responses pairwise and uses its output logits to compute the preference probability p(y > yb|x), following Eq. 1, we employ the inverse function of sigmoid to calculate the reward margin r rb. We then utilise the LWR to directly predict the biased reward margin given the length margin |y| - Yb.\nMetrics We look at the following metrics following Dubois et al. (2024): (1) Gameability: this measures whether the win rate given by the evaluator is significantly affected by simply promoting the same model to be more concise and verbose. Specifically, five models are selected to be prompted three times: (a) normally, (b) to \u201cgive as much detail as possible\u201d, (c) to \u201cbe as concise as possible while still providing all the necessary information to answer the question\u201d and get three win rates. Then, the gameability is defined as the normalised variance between those three win rates and averaged across these five models. Higher gameability indicates that the evaluator is more sensitive to the output's length. (2) Spearman correlation of reward model produced ranking with official AlpacaEval2 leaderboard over 184 models. (3) Spearman correlation of reward model produced ranking with ChatbotArena. We employ three versions of ChatbotArena (CA): CA Feb. is used by Dubois et al. (2024), we introduce it to ensure that we reproduce their results, CA Aug. is the latest one that is more recent and includes more models, and the CA. Length is the CA Aug. disentangled with length proposed by Chiang et al. (2024).\nIdentifying RMs with strong length bias Before calibration, we first evaluate the top-10 BT-based RMs from RewardBench leaderboard as of Aug. 2024 using the Gameability and the length bias subset of the EvalBiasBench dataset (Park et al., 2024a). The latter contains 17 test examples and is denoted as LengthBiasBench. We aim to identify RMs demonstrating severe length bias, i.e., high Gameability and low accuracy on LengthBiasBench. Their results are shown in Fig. 2. We observe the prevalence of length bias for BT-based RMs. 8 of 10 models exhibit high Gameability and poor performance on LengthBiasBench. The two metrics demonstrate consistency, i.e., the model with high Gameability usually achieves low accuracy on LengthBiasBench, achieving -0.879 Spearman correlation score. As Llama-3-OffsetBias-RM and internlm2-20b-reward do not demonstrate severe length bias, we primarily report results for other 8 models.\nResults The Spearman correlations of RM-produced rankings with the AE2 and the ChatbotArenas are presented in the Fig. 3. Our findings include: (1) Overall, RC-Mean and RC-LWR provide effective calibrations, leading to a higher correlation with AE2 and ChatbotArena, achieving comparable calibration effects with LC on all eight RMs. Note that LC is specifically designed for the AE leaderboard and can not be straightforwardly employed for reward calibration because it requires the model ID and instruction ID for regression, which are not available for general reward calibration. (2) Compared with AE2, RC-LWR calibrated AE achieves comparable Spearman correlations with Chatbot Arena, suggesting that our method can generalise to LLM-as-Judge results. (3) The calibration GRM-llama3-8B-distill achieves 0.975 with ChatbotArena Feb., and the calibrated FsfairX-LlaMA3-RM-v0.1 achieves 0.975 with AE2 and 0.951 with ChatbotArena Aug., demonstrating the potential of calibrated open-source RM to provide reliable LLM evaluations. (4) The Length Penalty brings improvements in most cases. But it can also lead to degeneration or can be not effective at all (Eurus-RM-7b). This is because different RMs have different reward scales, making it harder to employ the same penalty weight for different RMs. On the contrary, our proposed RC-LWR is more robust to different hyper-parameter choices, rendering it easier to use in practice. (5) Unlike the results on RewardBench, the Length Penalty and RC-LWR-Penalty are less effective in this setting, likely due to differences in length margin ( denoted as ||x1| - |x2||) distributions of two dataset. For instance, in the RewardBench, 60% of data points have a length margin below 600, while in the AlpacaEval setting, this figure is less than 40%. Consequently, the penalty term has a more significant impact on the AlpacaEval calibration. More relevant analyses are shown in the Appendix C. This underscores the practicality of our method over the penalty-based approach, as the latter may require extensive hyperparameter tuning for penalty weight. Such tuning is influenced not only by the reward model's scale but also by the output's length distribution. (6) The rankings calibrated by LC, RC-Mean, RC-LWR are usually more aligned with CA Length compared with CA Aug., further validating the effectiveness of our proposed methods. (7) As shown in Tab. 1, RC-Mean and RC-LWR effectively reduce the Gameability for various reward RMs, demonstrating the calibrated rewards are less sensitive to different prompting techniques and thus more reflective of the response quality."}, {"title": "4.3 LENGTH CALIBRATED REWARDS FOR LLMS ALIGNMENT", "content": "Setting In this setting, we investigate whether the calibrated rewards could serve as a better training signal for LLMs alignment. Because the post-hoc reward calibration assumes a scored dataset used for calibration, which could be seamlessly integrated with the offline RL, we choose to first employ the RMs to score the on policy dataset and then perform DPO. Our experiment pipeline could be described as follows: for each prompt q in a prompt dataset, multiple responses yi are sampled from an LLM, then scored as ri by an RM. Among these responses, the response with the highest or lowest score is selected to create a preference pair (p, Yw, Y\u0131) for Direct Preference Optimisation (DPO) tuning. Specifically, to validate the effectiveness of our method, we choose two instruct models: Meta-Llama-3-8B-Instruct (AI@Meta, 2024) and gemma2-9b-it (Team, 2024) and two reward models: FsfairX-LLAMA3-RM-v0.1 (Dong et al., 2023) and GRM-1lama3-8B-sftreg (Yang et al., 2024), resulting in 4 LLM-RM configurations: (1) Llama-3-8B-Fsfairx; (2) Llama-3-8B-GRM; (3) gemma2-9b-Fsfairx; and (4) gemma2-9b-GRM. For each LLM-RM combination, we compare the Original Reward, Length Penalty, and RC-LWR. We directly utilise the on-policy generations\u00b3 of two models generated by Meng et al. (2024). They employ the 60k instructions from the UltraFeedback (Cui et al., 2024) dataset and sample 5 responses for each instruction from the model. Then, we use those two reward models to score the dataset and to conduct DPO tuning. All hyper-parameters related to DPO tuning are set following Meng et al. (2024).\nMetrics We evaluate the model with the Length-Controlled win rate from the AE2 for open-ended generation. We also test the model on eight popular benchmarks: MMLU (Hendrycks et al., 2021b), GSM8K (Cobbe et al., 2021), ARC-challenge (Clark et al., 2018), HellaSwag (Zellers et al., 2019), Winogrande (Sakaguchi et al., 2021), IFEval (Zhou et al., 2023), GPQA (Rein et al., 2023), and BBH (Suzgun et al., 2023) to see how the proposed calibration affects the benchmark performance. We employ the OpenCompass (Contributors, 2023) for benchmark evaluation and report the average score of these benchmarks. All evaluation configurations are set as a default in OpenCompass."}, {"title": "5 ANALYSIS", "content": "Original Reward Length Penalty RC-Mean RC-LWR RC-LWR-Penalty\nPlavg\n0.2930\u00b10.1836 0.2092\u00b10.1706 0.0390\u00b10.0369 0.0233\u00b10.0223 0.0229\u00b10.0223\nDoes the calibration mitigate the length preference of models? One desideratum for an ideal calibration method is that the rewards should be weakly correlated with the bias characteristics of interest, thus cancelling the model's preference for specific characteristics and preventing reward hacking in its downstream applications. Therefore, we calculate the average and variance of the absolute Spearman correlation score over RMs for different methods on the RewardBench dataset and show them in Tab. 3. Our observation includes: (1) Length bias is significant for the board spectrum of RMs (0.293 for original reward). (2) The Length Penalty cannot effectively mitigate the model's preference for length. The added penalty term renders RM to prefer shorter outputs. (3) In contrast to Length Penalty method, our proposed methods could effectively calibrate the RM's preference on output length, i.e., the correlation score between output length and reward is approximately 0 for all RMs after calibration.\nCalibration behaviours on models with different length preferences. If an RM exhibits a weak preference for the characteristic of interest, an optimal calibration should not significantly alter its rewarding behaviour and vice versa. Rewarding accuracy alone may not effectively capture this aspect, as a calibration might result in an equal number of correct and incorrect predictions, thereby leaving the overall accuracy unchanged. We thus count how many preference pairs are reversed by the calibration on the RewardBench dataset. We report the fraction of it over the entire dataset. The results are shown in the Fig. 4. Our observations are as follows: (1) Length Penalty demonstrates the moderate correlation between RM length bias and the preference overturned and is unstable. It does not calibrate any pairs for some models with strong positive length preferences. For models with a strong negative preference for length, it is ineffective as the model already prefers shorter outputs in general. (2) Both RC-Mean and RC-LWR methods exhibit a strong correlation:"}, {"title": "6 CONCLUSION", "content": "This paper introduces a post-hoc reward calibration method to mitigate biases in RMs. Our approach effectively reduces the impact of prevalent biases, such as length bias, and has been validated across multiple experimental settings. The results consistently show substantial performance gains. Other empirical findings confirm that our method satisfies several critical desider"}]}