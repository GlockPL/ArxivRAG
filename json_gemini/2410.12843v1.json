{"title": "Exploring Prompt Engineering: A Systematic Review with SWOT Analysis", "authors": ["Aditi Singh", "Abul Ehtesham", "Gaurav Kumar Gupta", "Nikhil Kumar Chatta", "Saket Kumar", "Tala Talaei Khoei"], "abstract": "In this paper, we conduct a comprehensive SWOT analysis of prompt engineering techniques within the realm of Large Language Models (LLMs). Emphasizing linguistic principles, we examine various techniques to identify their strengths, weaknesses, opportunities, and threats. Our findings provide insights into enhancing AI interactions and improving language model comprehension of human prompts. The analysis covers techniques including template-based approaches and fine-tuning, addressing the problems and challenges associated with each. The conclusion offers future research directions aimed at advancing the effectiveness of prompt engineering in optimizing human-machine communication.", "sections": [{"title": "I. INTRODUCTION", "content": "PROMPT engineering is a rapidly evolving field within artificial intelligence, particularly focused on optimizing the interaction between humans and Large Language Models (LLMs) [1]\u2013[3]. At its core, prompt engineering involves designing and structuring inputs-known as prompts to elicit the most accurate, relevant, and useful responses from AI systems. This practice is grounded in linguistic principles, leveraging an understanding of language patterns and structures to craft prompts that guide AI behavior effectively. The emergence of Large Language Models [4]\u2013[8] has highlighted the importance of prompt engineering [9]. These models have demonstrated remarkable capabilities in generating human-like text, text-to-images, text-to-videos [10], answering questions [11]\u2013[15], and performing various language tasks [16]\u2013[19]. However, their performance heavily depends on how well the prompts are crafted. Effective prompt engineering can significantly enhance the accuracy and relevance of AI responses, making the interaction more intuitive and productive. Various techniques have been developed to refine prompt engineering, including template-based approaches, where fixed structures are used to standardize prompts, and fine-tuning methods that adapt the model to specific tasks or domains [20]\u2013[22]. These techniques aim to mitigate common issues such as ambiguity, bias, and context sensitivity, thereby improving the robustness and reliability of AI outputs. As AI continues to integrate more deeply into everyday applications, the role of prompt engineering becomes increasingly vital in ensuring seamless and meaningful human-AI communication."}, {"title": "A. Key Findings", "content": "The key findings of the paper are as follows:\n\u2022 Synergies: Identified synergies between AI, Linguistics and Prompt Engineering.\n\u2022 Techniques: Identified and categorized numerous prompt engineering methods.\n\u2022 Metrics: Identified numerous metrics for evaluation of different prompt engineering methods, including BLEU, BERTScore, ROUGE, and Perplexity.\n\u2022 SWOT Analysis: Identified strengths, weaknesses, opportunities, and threats for various prompt engineering techniques."}, {"title": "II. METHODOLOGY", "content": "This survey offers an extensive examination of the field of prompt engineering, incorporating insights from over 100 papers sourced from prominent academic databases and online platforms such as IEEE Xplore, ACM Digital Library, Google Scholar, and more. Queries utilizing keywords related to prompt engineering were employed to gather a comprehensive set of publications."}, {"title": "III. BACKGROUND", "content": ""}, {"title": "A. Prompt Engineering", "content": "Prompt engineering involves crafting tailored instructions or prompts to direct the responses of advanced language models, such as GPT-3, towards a specific outcome (for instance, instructing ChatGPT to produce a particular text) [23]. Prompt engineering involves designing input prompts that elicit accurate and valuable responses from large language models (LLMs) [24]. Prompt engineering refers to the practice of crafting and improving input queries, known as \"prompts,\" to obtain specific outcomes from Large Language Models (LLMs). These prompts play a key role in directing LLMs to produce outputs that are both relevant and beneficial [25]. Prompt engineering creates a method for designing prompts that solve different problems, allowing for customization across various fields. It enhances LLM outputs by merging multiple prompt strategies and fosters knowledge sharing among users and developers of LLMs [26]. Prompt engineering streamlines LLM application development, saving time and offering customizable interactions. It simplifies solving common issues, improving response accuracy and aiding conversational AI progress [27]. Prompt engineering is set to significantly enhance the capabilities of Large Language Models (LLMs), facilitating precise and swift language output. This emerging field not only promises to boost efficiency and optimize operations across sectors but also opens up new career paths for those proficient in prompt crafting. With ongoing advancements in sophisticated prompts, we can expect more intuitive user interfaces for LLM management, allowing for refined content generation and the exploration of previously unattainable LLM applications [28], [29]. Prompt engineering enhances LLM applications by fostering a deeper comprehension of LLM behaviors and capabilities, guiding LLMs towards delivering truthful and informative responses. It boosts few-shot learning by integrating optimized prompts with traditional learning techniques, leading to more efficient chatbots, virtual assistants, and specialized prompt engineering tools for conversational AI. Thus, it plays a crucial role in advancing NLP tasks through improved LLM performance [30], [31].\nPrompt engineering guides generative AI to desired outputs by crafting detailed instructions using specific words and formats. This creative process, involving trial and error, ensures AI interacts meaningfully with users and meets application expectations [32]."}, {"title": "B. Linguistic Principles in Prompt Engineering", "content": "Marjorie McShane and Sergei Nirenburg [33] suggested that Linguistics for the age of AI rests on four major pillars:\n\u2022 Pillar 1: Development of language processing within a unified agent framework.\n\u2022 Pillar 2: Human-inspired modeling for explanatory AI and actionable insights.\n\u2022 Pillar 3: Contribution to and learning from linguistic scholarship.\n\u2022 Pillar 4: Use of all heuristic evidence for meaning extraction and representation.\nThe four pillars of Linguistics for AI proposed by Marjorie McShane and Sergei Nirenburg [33] share similarities with several aspects of prompt engineering as defined in the descriptions provided:\nDevelopment within a unified agent framework (Pillar 1) aligns with the goals of prompt engineering to streamline LLM application development and offer customizable interactions, improving the efficiency of language output and response accuracy [27], [28]. This connection highlights the integration of complex systems and the aim for coherence in both fields.\nHuman-inspired modeling for explanatory AI (Pillar 2) mirrors the intention behind prompt engineering to foster a deeper comprehension of LLM behaviors and capabilities, guiding LLMs to deliver truthful and informative responses [30], [31]. Both emphasize the importance of human-like understanding and reasoning in Al systems.\nLearning from and contributing to linguistic scholarship (Pillar 3) is paralleled by the aspect of prompt engineering that involves crafting and improving prompts based on trial and error, which necessitates an understanding of language and its nuances [32]. This reflects a mutual interest in advancing linguistic knowledge and applying it to enhance AI capabilities.\nIncorporation of heuristic evidence for meaning extraction (Pillar 4) can be seen in the approach of prompt engineering to design prompts that elicit accurate and valuable responses from LLMs [24]. Both areas utilize comprehensive data and insights to refine the interpretation and generation of language."}, {"title": "IV. RELATED WORKS", "content": ""}, {"title": "A. Brief survey of prompt engineering techniques", "content": "A number of surveys [34]\u2013[37] have been conducted to provide overviews and summaries of existing prompt engineering techniques, emphasizing advancements, applications, and practical insights (see Table II). However, our study differs by conducting a comprehensive Strength Weakness Opportunity and Threat (SWOT) analysis, focusing specifically on the strengths, weaknesses, opportunities, and threats associated with each technique. Additionally, we delve deeper into the linguistic principles shaping prompt design and offer targeted research directions to address current challenges and enhance future Al interactions."}, {"title": "B. Different types of Prompt Engineering Techniques", "content": "The different types of prompt engineering techniques are as follows. A summary of each of these is provided in Table III.\n1) Automatic Reasoning and Tool-use: Automatic Reasoning and Tool-use (ART) [35], [38] is a computational framework designed to augment the capabilities of Large Language Models (LLMs) for complex problem-solving in few-shot and zero-shot settings. ART combines LLM-generated \"chain of thought\" (CoT) reasoning with the execution of external tools, thereby enabling tasks that surpass standard linguistic processing. This integration allows ART to automate the generation of intermediate reasoning steps, formatted as executable programs, which strategically incorporate external data through tool interactions.\nART operates by selecting appropriate multi-step reasoning templates from a task library and dynamically incorporating responses from external tools into the LLM's workflow. This process is mathematically managed by pausing and resuming the LLM's output generation based on tool interaction points, formalized as:\n$Outputfinal = f(OutputLLM, Tooloutput)$                                                                                                          (1)\nwhere f denotes the function that integrates tool outputs into the LLM's reasoning process.\n2) Chain-of-Thought (CoT): Chain-of-Thought (CoT) Prompting [35], [36], [39]\u2013[41] is a technique designed to facilitate complex reasoning by generating intermediate reasoning steps. This approach allows Large Language Models (LLMs) to articulate their thought processes step-by-step, thereby enhancing their ability to tackle more intricate tasks that demand preliminary reasoning before producing a response. The Chain-of-Thought Prompting can be represented as:\n$Ko = s,$\n$r\u2081 = g(s, Ko),$\n$r2 = g(s, K1),$\n$ :$\n$rn = g(s, Kn-1),$\n$K\u2081 = Ki-1\u222a{ri},$\n$Output final = h(Kn),$\n(2)\nHere,\n\u2022  $Ko$ is defined as the starting point containing the initial problem statement $s$.\n\u2022 $ri$ denotes each reasoning step, where g is a function modeling the LLM's processing to generate the reasoning step based on the current state of knowledge $Ki-1$ and the initial problem statement $s$.\n\u2022 $Ki$ accumulates each reasoning step into the knowledge base, effectively building upon each previous step.\n\u2022 $Outputfinal$ represents the final outcome, which $h$ computes from the fully accumulated knowledge $Kn$ after all reasoning steps.\nCoT prompting has been adapted for use in multilingual contexts through several innovative approaches. One such method is XLT (Cross-Lingual Thought) Prompting, developed by Huang et al. [42], which utilizes a prompt template incorporating six distinct instructions, including role assignment, cross-lingual reasoning, and CoT. Additionally, Cross-Lingual Self Consistent Prompting (CLSP), proposed by Qin et al. [43] (2023a), employs an ensemble technique to construct reasoning paths in diverse languages, further broadening the applicability and effectiveness of CoT prompting in multilingual settings. While Chain-of-Thought (CoT) prompting has shown substantial success in English, its application in low-resource languages remains limited. To address this gap, Chai et al. developed xCoT, a framework that transfers knowledge from high-resource to low-resource languages, enhancing multilingual CoT reasoning capabilities [44]. Despite progress in cross-lingual Chain-of-Thought (CoT) reasoning, existing methods face limitations due to the manual specification of languages and static weight allocation across different language reasoning paths. To overcome these challenges, Zhang introduces AutoCAP, a framework that automates language selection and dynamically allocates weight scores to different reasoning paths for zero-shot CoT, significantly enhancing performance and generalizability [45]. Shi et al. [46] explore the multilingual reasoning abilities of large language models through Chain-of-Thought (CoT) prompting, demonstrating that their effectiveness in solving diverse language tasks, such as the Multilingual Grade School Math (MGSM) benchmark, enhances with model scale and extends to both well-represented and underrepresented languages. However, the study highlights a critical gap in the dependency on model size for robust multilingual performance using CoT prompting, underscoring the need for more efficient architectures or training strategies that can achieve similar results without extensive scaling. Chen et al. [47] address critical gaps in existing multi-modal Chain-of-Thought (CoT) benchmarks by introducing a novel benchmark that incorporates multi-domain, multi-step, and multi-modal reasoning capabilities. Despite these advancements, their findings reveal that Vision Large Language Models (VLLMs) struggle to perform accurately within this complex CoT framework, highlighting a significant performance disparity between VLLMs and human capabilities. This pioneering work sets a foundation for future exploration and enhancement of multi-modal reasoning systems.\n3) Directional Stimulus Prompting: Directional Stimulus Prompting (DSP) [48] is a method in prompt engineering that embeds specific guidance or stimuli within prompts to direct the language model's responses towards a desired outcome. This method enhances the model's performance and relevance by including subtle cues or explicit instructions alongside the task description.\nIn DSP, discrete tokens known as \"directional stimuli\" are introduced into the prompt to guide the model. For instance, in a summarization task, these stimuli might include essential keywords to be reflected in the summary.\nThe mathematical representation of this process is as follows:\n$y ~ PLLM(y | x, z),$\n(3)\n$z ~ PPOL(z | x).$\n(4)\nWhere:\n\u2022 $x$ is the original input.\n\u2022 $PPOL(z | x)$ is a policy language model generating the directional stimulus z from $x$.\n\u2022 $PLLM(Y | x, z)$ is the language model generating the output y based on both the input x and the directional stimulus z.\nThe parameters of $PLLM$ remain unchanged, maintaining the efficiency and stability of the model while providing precise guidance through the additional stimuli.\n4) Few-Shot Prompting: Few-shot prompting [35], [36], [49]\u2013[54] is a technique that enhances in-context learning by including example demonstrations within the prompt. These examples guide the model to generate accurate responses for subsequent tasks based on the provided context. In few-shot prompting, the model's behavior is influenced by a small number of examples. Let's denote:\n\u2022 $x$ as the original input or query.\n\u2022 ${(xi, Yi)}_{i=1}^{k}$ as the set of k few-shot examples, where each example consists of an input xi and a corresponding output Yi.\n\u2022 $PLM(Y | x, {(xi, Yi)}_{i=1}^{k})$ as the language model generating the output y based on the input x and the few-shot examples.\nThe mathematical representation of few-shot prompting can be written as:\n$y ~ PLM(y | x, {(xi, Yi)}_{i=1}^{k}),$\n(5)\nwhere ${(xi, Yi)}_{i=1}^{k}$ are the k few-shot examples provided to the model to guide its generation of the output $y$.\nLee et al. [51] explores the efficacy of ChatGPT and prompt engineering for automatic question generation in English education, demonstrating significant improvements in question validity through few-shot prompting techniques. However, it highlights a gap in the optimization of certain question types via few-shot prompting, indicating a need for further refinement to enhance the versatility and reliability of AI-generated educational content. Timo Schick and Hinrich Sch\u00fctze [52] demonstrate that the Pet method, which combines textual instructions with example-based finetuning, performs strongly in true few-shot settings without requiring a development set, achieving new state-of-the-art results on the RAFT benchmark. However, the study highlights a gap in understanding the specific design choices and configurations necessary for optimal performance in true few-shot learning scenarios, indicating a need for further research into intelligent prompt handling and configuration. Xi Ye and Greg Durrett [55] investigated whether prompting large language models (LLMs) like GPT-3 with explanations enhances in-context learning for textual reasoning tasks. Their study finds that while explanations provide small to moderate accuracy improvements for most models, text-davinci-002 benefits more significantly. However, explanations often lack alignment with the models' predictions or factual grounding. Chengyu Wang et. al. [56] introduced TransPrompt, a framework that leverages transferable prompt embeddings for few-shot text classification across similar NLP tasks. TransPrompt uses a meta-learner trained through a multi-task meta-knowledge acquisition process, employing de-biasing techniques to remain task-agnostic. Extensive experiments show TransPrompt outperforms strong baselines. However, optimizing transferability and de-biasing techniques for varied tasks remains a challenge, requiring further research.\n5) Generated Knowledge Prompting: Generated knowledge prompting [57] is a technique that involves enhancing a language model's performance on a multiple-choice commonsense reasoning task through two key steps: Knowledge Generation and Knowledge Integration.\nIn such tasks, we predict an answer \u00e2 \u2208 Aq given a question q \u2208 Q, where Aq is the set of choices for the question q. The method involves two steps:\n1. Knowledge Generation: Generate knowledge statements Ka conditioned on the question:\n$Kq = {km | km ~ PG(k | q), m = 1,..., M}.$\n(6)\n\u2022 Kg: The set of generated knowledge statements related to the question q.\n\u2022 km: An individual knowledge statement generated for the question q.\n\u2022 PG(k | q): The probability distribution used to generate the knowledge statements km given the question q.\n\u2022 m: An index denoting the different knowledge statements in the set Kq.\n\u2022 M: The total number of knowledge statements generated.\n2. Knowledge Integration: Integrate the generated knowledge into the decision process for inference:\n$a = arg \\underset{a\u2208 Aq}{max} pr(a | q, Kq).$\n(7)\n\u2022 a: The predicted answer to the question q.\n\u2022 Aq: The set of possible answer choices for the question q.\n\u2022 pr(a | q, Kq): The probability of answer a given the question q and the generated knowledge Kq.\n\u2022 arg maxa\u2208 Aq: The operation to find the answer a that maximizes the probability.\nIn comparison, without using generated knowledge, the inference model yields:\n$a = arg \\underset{a\u2208 Aq}{max} pr(a|q).$\n(8)\n\u2022 a: The predicted answer to the question q without additional generated knowledge.\n\u2022 pr(a | q): The probability of answer a given only the question q, without the generated knowledge Kq.\n\u2022 arg maxa\u2208 Aq: The operation to find the answer a that maximizes the probability based solely on the question q.\nBuilding on the concept of knowledge prompting, Jiajin Tang et al. [58] introduced a framework called CoTDet, which integrates knowledge prompting with chain-of-thought reasoning for task-driven object detection. CoTDet uses knowledge prompting to extract and apply essential affordance knowledge from large language models, focusing on attributes that enable various objects to perform specific tasks. It then employs multi-level chain-of-thought (MLCoT) reasoning to systematically link this knowledge to object attributes through rationales. This combination enhances object detection and localization, with CoTDet achieving significant improvements in both box and mask AP compared to existing methods. Similarly, Jianing Wang et al. [59] introduced KP-PLM, a framework for enhancing pre-trained language models with factual knowledge using natural language prompts. This approach avoids complex modifications to PLM architectures and redundant information from knowledge bases. KP-PLM employs a knowledge sub-graph and two self-supervised tasks to improve performance. Experiments show it outperforms current methods in natural language understanding tasks. Additionally, Jianing Wang et al. [60] proposed Chain-of-Knowledge (CoK) prompting to address the limitations of Chain-of-Thought (CoT) prompting in reasoning tasks. CoK prompting aims to elicit explicit knowledge evidence in the form of structured triples, inspired by human reasoning processes. To enhance reliability, the authors introduce the F2-Verification method to assess the factuality and faithfulness of reasoning chains, prompting the model to reconsider unreliable responses. Extensive experiments show that CoK prompting further boosts performance across various reasoning tasks, including commonsense, factual, symbolic, and arithmetic reasoning. Lihui Zhang and Ruifan Li [61] presented the Knowledge Prompting with Contrastive Learning (KPCL) model for unsupervised commonsense question answering. KPCL improves performance by using dropout noise for augmentation, unsupervised contrastive learning for nuanced question handling, and generic prompts for zero-shot knowledge generation. Xiaohan Zhang et al. [62] combined knowledge prompting with vision-language models through their framework, DKPROMPT. This method integrates domain knowledge from PDDL-based classical planning to enhance VLMs for open-world task planning. DKPROMPT effectively bridges the gap between VLMs' vision-language capabilities and classical planning's robustness, leading to superior task completion rates compared to traditional and VLM-only methods.\n6) Graph Prompting: Graph Prompting [63]\u2013[70] is a technique in prompt engineering that leverages graph data to create more effective prompts for machine learning models. It reformulates tasks to resemble pretext tasks, enabling the direct use of pre-trained models. This method integrates relational and contextual information from graphs, enhancing the prompts' precision and relevance. There are two primary types of Graph Prompting:\n\u2022 Discrete Prompts: These utilize natural language or specific graph elements to create prompts. They involve manually or automatically crafted templates that incorporate graph-based knowledge, making them suitable for tasks that require explicit contextual information.\n\u2022 Continuous Prompts: These involve learned representations or embeddings. Continuous prompts dynamically adjust the input data in the embedding space, utilizing graph representation learning methods to generate contextually enriched prompts.\nThe mathematical framework for Graph Prompting can be outlined as follows:\n$x' = fprompt(x; @prompt)$\n(9)\nWhere x is the input sample, and @prompt represents the task-related knowledge parameters incorporated into the prompt.\nFor the pre-training phase, a link prediction task to learn generalizable knowledge from a graph $G = (V, E)$ is employed:\n$V(S) = {u \u2208 V | d(u, v) \u2264 \u03b4}$\n(10)\n$E(Sv) = {(u, u') \u2208 E | u \u2208 V(Sv), u' \u2208 V(Sv)}$\nWhere S is the contextual subgraph of node v, and \u03b4 is a predetermined threshold.\nThe subgraph representation sx is computed using a Read-Out operation:\n$Sx = ReadOut({hr : v \u2208 V(Sx)})$\n(11)\nFor downstream tasks, such as link prediction, node classification, and graph classification, the following formulations is used:\n$sim(sv, sa) > sim(sv, Sb)$\n(12)\n$lj = arg max sim(svj, \u0160c)$\n(13)\n$CEC$\n$Lj = arg max sim(SG\u2081, \u00a7c)$\n(14)\n$CEC$\nLearnable prompts can further refine the subgraph representations for specific tasks:\n$St,x = ReadOut({pth : v \u2208 V(Sx)})$\n(15)\nWhere pt is a learnable prompt vector, and \u2299 denotes element-wise multiplication.\nFinally, the most probable answer for a given prompt is determined through:\n$z = arg \\underset{z'EZ}{max} P(f(x'), z')$\n(16)\nWhere Z is the set of possible answers, and P is the probability or similarity function.\nGraph Prompting effectively utilizes graph structures to generate contextually enriched prompts, improving the performance and adaptability of machine learning models in various graph-related tasks.\n7) Iterative Prompting: Iterative prompting [71]\u2013[75] is a method in prompt engineering where the prompts given to a Generative AI tool are progressively refined to enhance the relevance, accuracy, and depth of its responses. This approach is similar to a conversational exchange, where each answer helps shape the next question, allowing for continuous learning and adjustment based on feedback.\nIn iterative prompting, the process involves several essential steps:\n1) Initial Prompt: Begin with a broad, open-ended prompt to assess the Al's initial understanding of the task.\n2) Response Analysis: Examine the Al's responses for relevance and depth, identifying gaps or areas that need improvement.\n3) Prompt Refinement: Adjust the prompt based on initial responses, incorporating specific keywords or phrases that were particularly insightful or relevant.\n4) Feedback Loop: Treat the process as a continuous feedback loop, where each iteration of prompting is informed by the responses from previous iterations.\n5) Experimental Testing: Test different prompt styles and validate the refined prompts on multiple examples to ensure robustness and effectiveness.\nThe iterative prompting process can be mathematically represented as follows: Given 20 (initial input), Initialize:\n$Po = fprompt(xo),$\n(17)\nFor each iteration t do:\n$rt = fresponse(Pt),$\n$et = ferror(rt),$\n$Pt+1 = frefine(Pt, et),$\n(18)\nUntil convergence or maximum iterations.\nOutput:\n$y = ffinal(rT),$\n(19)\nWhere:\n\u2022 20: Initial input or problem statement.\n\u2022 pt: Prompt at iteration t.\n\u2022 rt: Response from the AI model at iteration t.\n\u2022 et: Error or feedback at iteration t.\n\u2022 fprompt: Function generating the initial prompt.\n\u2022 fresponse: Function generating the Al's response.\n\u2022 ferror: Function evaluating the response to identify errors.\n\u2022 frefine: Function refining the prompt based on errors.\n\u2022 ffinal: Function producing the final output.\nIterative prompting, akin to iterative research, focuses on continuous improvement through design, learning, and refinement, ensuring the AI tool aligns accurately with research objectives and enhances the efficiency and effectiveness of data analysis.\n8) Least-To-Most Prompting: Least-to-most prompting [76]\u2013[82] is a technique in prompt engineering that teaches language models to solve complex problems by breaking them down into simpler subproblems. It involves two main stages: Decomposition: The initial prompt demonstrates how to decompose a complex problem into manageable subproblems. Subproblem Solving: The subsequent prompts guide the model to solve each subproblem sequentially until the original problem is solved.\nThis method can be mathematically represented as follows:\nStage 1: Decomposition:\n$D = {d1,d2,...,dn} = fdecompose(P),$\n(20)\nwhere D is the set of subproblems {d1,d2,...,dn}, and fdecompose is the decomposition function applied to the original problem P.\nStage 2: Subproblem Solving:\n$Si = fsolve(di|{$1,82,...,Si\u22121})$\n(36)\nfor i = 1,2,...,n, where si is the solution to subproblem di, and fsolve is the solving function considering previous solutions.\nStage 3: Integration:\n$Final Answer = fintegrate({81, 82,..., Sn}),$\n(37)\nwhere fintegrate is the integration function combining all sub-problem solutions.\n12) Self-Consistency: Self-consistency [35], [36], [50] is a technique in prompt engineering that improves the accuracy of language models by generating multiple candidate outputs for a given prompt and aggregating the results. This approach leverages diverse reasoning paths to enhance answer reliability.\nGiven a prompt and a question, self-consistency introduces a latent variable ri, where ri represents the reasoning path in the i-th output, leading to the answer ai. The final answer is chosen based on majority voting over the candidate answers a1, a2,..., am.\n$Final answer = arg max \\sum_{i=1}^{m} 1(a\u2081 = a),$\n(38)\nwhere 1(ai = a) is 1 if a\u2081 = a and 0 otherwise.\nIn more detail, assume the generated answers ar are from a fixed answer set, a\u017c \u2208 A, where i = 1, ..., m indexes the m candidate outputs sampled from the decoder. Given a prompt and a question, self-consistency introduces an additional latent variable ri, which is a sequence of tokens representing the reasoning path in the i-th output. This couples the generation of (ri, ai) where ri \u2192 ai, i.e., generating a reasoning path ri is optional and only used to reach the final answer ai.\nThe method can be formally expressed as:\n$P(ri, ai prompt, question) = P(ai | ri, prompt, question)$\n$\u00b7 P(ri | prompt, question),$\nfor i = 1,2,..., n.\n(39)\nwhere the joint probability P(ri, ai prompt, question) is decomposed into the probability of generating the answer given the reasoning path and the prompt, and the probability of the reasoning path given the prompt and question.\nTo compute P(ri, ai prompt, question), we can either take the unnormalized probability of the model generating (ri, ai) given (prompt, question), or we can normalize the conditional probability by the output length:\n$exp{\\frac{1}{K}\\sum_{k=1}^{K}log P(tk|prompt, question, t_{1},..., t_{k-1}) }$\n(40)\nwhere log P(tk|prompt, question, t\u2081,..., tk-1) is the log probability of generating the k-th token tk in (ri,ai) conditioned on the previous tokens, and K is the total number of tokens in (ri, ai).\nSelf-consistency can be applied to problems where the final answer is from a fixed answer set. By introducing diversity in the reasoning processes, this technique enhances the robustness and accuracy of the language models' outputs.\n13) Sequential Prompting: Sequential prompting [95]\u2013[97] is a strategy used in natural language processing tasks to improve the accuracy of predictions by using the results of previous steps as prior knowledge for the next prediction. In this approach, the task involves extracting elements ei based on previous predictions and the initial input. The process uses the output of one step as an input prompt for the next step.\nGiven an input X = [x1, x2,..., Xm], the goal is to extract a collection of elements E = {e}].\n\u2022 Initial Extraction:\n$e1 = argmax P(e|X)$\n(41)\n\u2022 Subsequent Predictions:\n$ei = argmax P(e|e1, C2,..., Ci-1, X)$\n(42)\nHere, P(e|X) is the probability of element e given the input X, and P(ele1, 2, ..., ei-1, X) is the probability of element e given the previous elements (e1, C2, ..., ei\u22121) and the input X.\nThe sequential prompting strategy leverages these conditional probabilities to iteratively refine the predictions using the prior results.\n14) Tree of Thoughts (ToT): Tree of Thoughts (ToT) [35], [36], [98]\u2013[102] is an advanced framework for enhancing language models' performance on complex tasks introduced by [98]. ToT extends chain-of-thought prompting by maintaining a hierarchical structure of intermediate steps, or \"thoughts,\" toward solving a problem. This framework allows language models to self-evaluate progress and systematically explore different pathways using search algorithms like breadth-first search and depth-first search, incorporating lookahead and backtracking techniques. By doing so, ToT enables more effective exploration and strategic planning, improving the models' reasoning and problem-solving capabilities. Long et. al. [99] built on this idea with a ToT framework that uses a reinforcement learning-trained \"ToT Controller\" to adapt and learn from new data, offering a more dynamic approach than traditional search methods.\nHulbert [100] simplified the ToT concept into a single prompt technique, where language models evaluate intermediate thoughts step-by-step, making it more accessible and straightforward. Sun [101], [102] took ToT further with large-scale experiments and introduced PanelGPT, a creative approach that simulates panel discussions among language models to benchmark and enhance the prompting technique.\nThe Tree of Thoughts (ToT) framework enhances problem-solving by leveraging a tree-like search strategy. Each node in the tree represents a partial solution s [X, 21:i], where x is the initial input and 21:i is the sequence of thoughts so far. The process involves four main components:\n1) Thought Decomposition: Break down the problem into manageable thought steps.\n2) Thought Generation: Generate multiple candidates for the next thought step using strategies like i.i.d sampling or propose prompt sampling.\n3) State Evaluation: Evaluate each state to assess progress towards the solution using heuristics or deliberate reasoning prompts.\n4) Search Algorithm: Use search strategies like breadth-first search (BFS) or depth-first search (DFS) to explore and expand the most promising thought paths.\nMathematically, ToT can be represented as:\n$S$\n$[X, Z1:i]$\n(43)\nThought Generation:\n$G(po,s,k)$\n(44)\nState Evaluation:\n$V(pe, S)$\n(45)\nThis allows systematic exploration, lookahead, and backtracking, leveraging pre-trained language models without additional training.\n15) Zero-Shot Prompting: Contemporary large language models (LLMs) like GPT-3.5 Turbo, GPT-4, and Claude 3 are trained on extensive datasets and optimized to follow instructions. This comprehensive training equips these models to execute tasks in a \"zero-shot\" fashion. Zero-shot prompting [103] involves giving the model a task instruction without providing any specific examples or demonstrations. The model is directly instructed to perform the task based solely on the given prompt."}, {"title": "V. METRICS FOR EVALUATING PROMPT ENGINEERING", "content": "Prompt engineering involves the strategic formulation of inputs to guide language models (LLMs) towards desired outputs. Evaluating the efficacy of prompt engineering requires considering several key metrics. Table IV presents a compilation of key metrics used in the evaluation of prompt engineering. These metrics span categories such as Semantic Similarity, Diversity, and Language Acceptableness. For instance, metrics like BERTScore and STS-B focus on semantic similarity between generated and reference texts, while ROUGE and BLEU measure diversity through comparisons of n-grams and word sequences. Metrics such as CoLA and Perplexity evaluate language acceptableness and predictive performance, respectively. Understanding and applying these metrics are crucial for optimizing prompt design and enhancing the capabilities of language models in various NLP tasks."}, {"title": "VI. CONCLUSION", "content": "In conclusion, this survey paper has provided a comprehensive analysis of prompt engineering techniques within the context of Large Language Models (LLMs). By conducting a SWOT analysis, we have highlighted the strengths, weaknesses, opportunities, and threats associated with various methods such as zero-shot prompting, few-shot prompting, chain-of-thought prompting, and more. Our findings underscore the critical role of linguistic principles in shaping effective prompt design and the potential of these techniques to enhance AI interactions and understanding of human prompts. The key findings include identifying synergies between AI, Linguistics, and Prompt Engineering, categorizing numerous prompt engineering methods, identifying metrics such as BLEU, BERTScore, ROUGE, and Perplexity for evaluation, and conducting a SWOT analysis of various prompt engineering techniques. Despite the notable advancements, challenges such as prompt complexity, computational demands, and domain-specific limitations persist. Future research should focus on addressing these challenges, optimizing prompt engineering strategies, and exploring novel applications to further improve the efficacy and reliability of LLMs in diverse real-world scenarios."}]}