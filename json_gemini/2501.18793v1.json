{"title": "OT-Transformer: A Continuous-time Transformer Architecture with Optimal Transport Regularization", "authors": ["Kelvin Kan", "Xingjian Li", "Stanley Osher"], "abstract": "Transformers have achieved state-of-the-art performance in numerous tasks. In this paper, we propose a continuous-time formulation of transformers. Specifically, we consider a dynamical system whose governing equation is parametrized by transformer blocks. We leverage optimal transport theory to regularize the training problem, which enhances stability in training and improves generalization of the resulting model. Moreover, we demonstrate in theory that this regularization is necessary as it promotes uniqueness and regularity of solutions. Our model is flexible in that almost any existing transformer architectures can be adopted to construct the dynamical system with only slight modifications to the existing code. We perform extensive numerical experiments on tasks motivated by natural language processing, image classification, and point cloud classification. Our experimental results show that the proposed method improves the performance of its discrete counterpart and outperforms relevant comparing models.", "sections": [{"title": "1. Introduction", "content": "Transformers were first introduced in (Vaswani et al., 2017) for natural language processing (NLP) tasks. The key feature of the model is the self-attention mechanism, which can capture dependencies of long sequences of data in a parallel manner. This renders the training of transformers more efficient than other architectures, such as RNNs and CNNs, especially when long sequences of data are involved. Since then, not only did it achieve state-of-the-art results in NLP (Radford et al., 2019), but it also found various successful applications, including computer vision (Dosovitskiy et al., 2021), program synthesis (Chen et al., 2021b), computational biology (Jumper et al., 2021), speech processing (Baevski et al., 2020), reinforcement learning (Chen et al., 2021a; Lin et al., 2024), operator learning (Li et al., 2023; Yang et al., 2023) and climate modeling (Gao et al., 2023; Nguyen et al., 2023; 2024).\n\nThe basic structure of a transformer architecture is transformer blocks, where self-attention is a key characteristic. In each transformer block, the self-attention layer can capture relationships within the input data in a parallel and efficient manner. The parallel computation of self-attention enhances the transformer's efficiency while preserving its representational power.\n\nEach transformer block also incorporates a skip-connection structure. Inspired by the popular Neural ODE framework (Chen et al., 2018), we propose a continuous-time formulation for transformers, where the hidden states evolve over time according to an ODE. We further leverage optimal transport theory to regularize the hidden state dynamics. We justify this regularization both theoretically and experimentally."}, {"title": "2. Background", "content": "In this section, we discuss the related work that motivate our approach.\n\nNotations In this paper, we use bold uppercase letter (e.g., X) to denote matrices and bold lowercase letter (e.g., x) to denote vectors. Moreover, we use xj (resp. xi,j) to represent the jth column of X (resp. Xi).\n\nTransformers In general, a transformer architecture is formulated as follows. Given an input Z = [z1, z2, ..., zn] \u2208 Rdf\u00d7n, where n is the number of tokens and df is their dimension, it first computes the input embedding of each token by\n\\[X_{0,j} = g_1(z_j; \\gamma_1),  \\quad \\text{for } j = 1, 2, ..., n. \\qquad(1)\\]\nHere x0,j \u2208 Rd. The input embedding g1 parametrized by weights y embeds each token into a d-dimensional space and incorporates sequential order information into each token. Then, it is processed through a series of transformer blocks, where the output of each block serves as the input to the next. At each step, the model sequentially applies the operation Xi+1 = fi+1(Xi) given by (Thickstun, 2021)\u00b9\n\\[\\begin{aligned} \\mathbf{u}_{i, j} &= \\mathbf{x}_{i, j}+\\sum_{h=1}^{H} \\mathbf{W}_{h}^{v} \\mathbf{X} \\operatorname{softmax}\\left(\\frac{\\left(\\mathbf{K}_{h}^{\\top} \\mathbf{X}\\right) \\mathbf{Q}_{h} \\mathbf{x}_{i, j}}{\\sqrt{k}}\\right) \\\\\\\\ \\mathbf{x}_{i+1, j} &= \\mathbf{u}_{i, j}+g_{f}\\left(\\mathbf{u}_{i, j} ; \\boldsymbol{\\theta}_{i}\\right), \\end{aligned}\\qquad(2)\\]\n\\[ \\qquad(3)\\]\nfor j = 1, 2, ..., n, and i = 0, 1, ..., D \u2013 1, where D is the total number of transformer blocks, H is the number of self-attention heads, Qh, Kh, Vh \u2208 Rk\u00d7d are known as query, key, and value matrices, and Wh\u2208 Rdxk. In (3), a fully connected layer gf, parametrized by weights \u03b8i, is applied individually to each of the n tokens. The first equation (2) is known as self-attention layers and is the key feature of transformer architectures. Their matrix multiplication formulation enables the parallel computation of dependencies among tokens, rendering them particularly effective for handling long sequences of tokens, that is, when n is large. This self-attention mechanism enables models to focus on the most relevant parts of an input sequence, adapting dynamically to the context. Its flexibility allows it to capture complex, long-distance relationships within data, different from CNNs which primarily focus on local patterns, and RNNs, which experience a sharp performance decrease with long sequences. Such features make transformers particularly powerful for tasks such as language understanding and image recognition. This series of transformer blocks is also called an encoder in the literature.\n\nEventually, XD is either passed to a decoder comprising another series of transformer blocks and then a multilayer perceptron (MLP) for sequence generation tasks or directly to an MLP for various downstream tasks, including classification and regression. The transformer output \u1ef9 is therefore computed by\n\\[ \\widetilde{\\mathbf{y}}=g_{o}\\left(\\mathbf{x}_{D} ; \\boldsymbol{\\gamma}_{o}\\right),\\qquad(4)\\]\nwhere go is either the composition of a decoder and an MLP or an MLP, parametrized by weights Yo.\n\nLayer normalization is commonly applied in each transformer block (Xiong et al., 2020). For brevity of exposition, it is omitted in the discussion. But it is included in our experiments."}, {"title": "ResNets and Neural ODEs", "content": "Residual networks (ResNets) (He et al., 2016) are an extensively employed model which features a skip-connection structure in their layers. Given input x0, the output of the ith layer is computed by\n\\[\\mathbf{x}_{i+1}=\\mathbf{x}_{i}+g_{i}\\left(\\mathbf{x}_{i}\\right).\\qquad(5)\\]\nHere, gi is a network layer, and the skip-connection (5) is a key feature of ResNet. This architecture is often compared with the explicit Euler discretization of an ordinary differential equation (ODE) (Weinan, 2017; Haber & Ruthotto, 2017; Ruthotto & Haber, 2020). Based on this insight, (Chen et al., 2018) proposed Neural ODEs (NODES), whose formulation is given by\n\\[\\frac{d \\mathbf{x}(t)}{d t}=f_{\\mathrm{NODE}}(\\mathbf{x}(t), t).\\qquad(6)\\]\nHere t \u2208 [0, T] is artificial time and fNODE is a neural network parametrizing the dynamics. Given an input x(0), the final output x(T) is obtained by integrating (6). A notable and relevant advantage of Neural ODEs is their parameter efficiency, as the continuous formulation allows them to model complex transformations over time with fewer parameters compared to traditional architectures."}, {"title": "OT-based CNFS", "content": "A prominent application of NODEs is continuous normalizing flows (CNFs) (Chen et al., 2018). CNFs use (6) to paramtrize invertible mappings between a standard Gaussian distribution and an unknown target distribution. The ill-posed nature of the CNF formulation can often add to the complexity and computational cost for solving a problem. Optimal transport (OT) based regularization has prominent applications in CNFs and is a powerful tool in improving accuracy and at times reducing cost. Among the infinitely many mappings between the two distributions, OT-based CNFs (Finlay et al., 2020; Yang & Karniadakis, 2020; Onken et al., 2021; Vidal et al., 2023) target to find the optimal transport mapping. This is done by incorporating into the training objective regularization term(s) enforcing straight trajectories in (6). This renders the training problem well-posed (Huang et al., 2023; Zhang & Katsoulakis, 2023). The straight trajectories also offer numerical advantages, as they make the numerical integration of (6) more tractable."}, {"title": "3. OT-Transformers", "content": "In this section, we introduce the continuous-time transformer with optimal transport regularization (OT-Transformer). A key feature of OT-Transformer is that, the model uses a combination of transformer blocks and NODE formulation. Specifically, the model parametrizes an ODE using transformer blocks, with the embedded inputs (1) serving as the initial state of the ODE, and the terminal state will be passed to the output layer (4). An optimal transport regularization is used in the training problem, and we demonstrate its benefits empirically and theoretically.\n\nModel Formulation Motivated by the connection between ResNet and neural ODEs, and the inherent skip-connection structure of transformer blocks (2) and (3), we formulate a continuous-time transformer.\n\nGiven an input sequence Z = [z1, z2, ..., zn] of length n, we first apply the input embedding (1) to obtain the initial state X(0) = X0 \u2208 Rd\u00d7n. The dynamics of the hidden state is then governed by the ODE2\n\\[\\frac{d \\mathbf{X}(t)}{d t}=f(\\mathbf{x}(t), t ; \\boldsymbol{\\theta}), \\quad \\text { for } t \\in[0, T].\\qquad(7)\\]\nwhere f is the composition of a sequence of transformer blocks defined in (2) and (3), that is, f = fDfD\u22121\u00b0.\u00b0 f1, and \u03b8 collectively denotes their trainable parameters \u03b8i, Kh, Vh, Qh and Wh for all h and i. In real implementation, we adopt a discretize-then-optimize approach (Onken & Ruthotto, 2020; Onken et al., 2021) and compute the terminal state X(T) by using numerical integration schemes such as forward Euler or Runge-Kutta methods (Butcher, 2016). Finally, we obtain the transformer output \u1ef9 by applying (4) to the terminal state X(T).\n\nOur framework is flexible in that it can be applied to almost any existing transformer architectures. It can directly reuse the architecture of an existing transformer's input embedding, decoder and output layers and use its transformer blocks fi's to construct the ODE (7)\u00b3. This only requires slight modifications to existing code. Our framework generalizes the discrete formulation of transformer blocks to continuous-time, effectively enables a continuous-depth formulation of transformer blocks. When T = 1 and a single step forward Euler integration scheme is used, our framework is identical to the original discrete transformer formulation. Hence, our framework is consistent with the standard transformer architecture.\n\nProblem Formulation We formulate the training objective as\n\\[\\min _{\\boldsymbol{\\theta}, \\boldsymbol{\\gamma}} \\mathbb{E} \\mathcal{L}(\\mathbf{X}(T), \\mathbf{y} ; \\boldsymbol{\\gamma})+\\frac{\\lambda}{2 d n} \\int_{0}^{T}\\|f(\\mathbf{x}(t), t ; \\boldsymbol{\\theta})\\|^{2} d t\\qquad(8)\\]"}, {"title": "4. Related Work", "content": "This section provides a review of relevant work.\n\nContinuous-time Architecture There has been some works on a continuous-time interpretation of transformers. And there is a key distinction between the formulations of OT-Transformer and existing models. In OT-Transformer, we use the composition of all transformer blocks to parametrize a single dynamical system (7) governing the hidden states. To the best of our knowledge, the existing works use each transformer block to parametrize a dynamical system. For a transformer with D transformer blocks, the continuous-time model is represented as the output of D different dynamical systems. In particular, it is formulated as\n\\[\\begin{aligned} &\\mathbf{X}_{0}(0)=\\mathbf{X}_{0}, \\\\ &\\mathbf{X}_{i}(0)=\\mathbf{X}_{i-1}(T), \\quad \\text { for } 1 \\leq i \\leq D-1, \\\\ &\\frac{d \\mathbf{X}_{i}(t)}{d t}=f_{i}\\left(\\mathbf{X}_{i}(t), t ; \\boldsymbol{\\theta}_{i}\\right), \\quad \\text { for } t \\in[0, T], 0 \\leq i \\leq D-1,\\end{aligned}\\qquad(12)\\]\nwhere fi is the ith transformer block parametrized by weights \u03b8i and defined in (2) and (3), except that the fully-connected layer (3) has no skip-connection.\n\nThis formulation is introduced in (Baier-Reinio & De Sterck, 2020), the model of which is conceptually the closest to our approach. Here, we highlight several key differences between their work and ours. Firstly, they only conduct the simple task of determining the parity of a binary sequence, rather than investigating its performance in general applications. When their approach is applied, it fails to improve performance over the vanilla transformer and instead degrades it. While they also propose the use of the transport cost, the regularization cannot improve the performance of their model when the sequence length exceeds eight. We observe similar issues when testing their model on other applications; see Section 5. This is potentially due to their choice of formulation. Specifically, in (12), as the model transitions from one transformer block to the next, it effectively switches to a different dynamical system, introducing non-smoothness to the overall dynamics. This undermines the purpose of the transport cost regularization, which seeks to obtain a continuous and more constant velocity. In contrast, our model is formulated using only one dynamical system. The resulting dynamics is smoother and thus inherently better suited to incorporate the transport cost regularization. This is evident in our experimental results, while the regularization can always improve the generalization of OT-Transformer to a significant extent, this is not the case with their model; in certain scenarios, the regularization may even degrade their model's performance. Moreover, they do not provide theoretical analysis to the regularization or demonstrate its numerical advantages. We also mention that, while (Baier-Reinio & De Sterck, 2020) proposes alternative formulations for further investigation, it does not consider ours, highlighting the novelty and non-triviality of our approach.\n\nSince then, there have been a number of follow-up works that build on the formulation Equation (12) to perform different tasks, including sequence generation (Lu et al.; Li et al., 2021; 2022; Zhong et al., 2022), time series forecasting (Xu et al., 2023; Cheng et al., 2024), and image classification (Niu et al., 2024; Okubo et al., 2024). But most of these methods only use the formulation (12) as motivation and are discrete architectures in nature, and none of them consider transport cost regularization in their approach. Moreover, these models focused on a specific type of application and not general-purpose.\n\nIn order to access the performance of our OT-Transformer more comprehensively, we also include the existing transformer formulation (12) as a benchmark in our experiments. It is referred to as \u201cN-ODE Transformer\u201d in our experimental results, following the terminology in (Baier-Reinio & De Sterck, 2020).\n\nMathematical Analysis There have been works that theoretically analyze a continuous-time formulation of transformers. In (Geshkovski et al., 2023; 2024a), they show that a continuous-time formulation can be interpreted as an interacting particle system, where each token can be perceived as a particle. They demonstrate that there is a clustering behavior among the tokens. Since then, there has been a number of works that further investigate the dynamics of tokens through this interpretation, including (Adu & Gharesifard, 2024; Bruno et al., 2024; Biswal et al., 2024; Geshkovski et al., 2024b; Karagodin et al., 2024), to name a few. However, we note that the aforementioned work is primarily theoretical and lacks evaluations beyond toy experiments. In (Sander et al., 2022), they show that, under some restriction on the weights, a continuous-time formulation of self-attention layers can be interpreted as a gradient flow. However, no experiments have been conducted following this analysis."}, {"title": "5. Experimental Results", "content": "We demonstrate the advantage of our proposed OT-transformers through four extensive experiments arising from point cloud classification, image classification, and text sentiment analysis.\n\nFor each task, we use commonly used transformer architectures as baselines. All the hyperparameters of the experiments, including architectures of baseline models, number of epochs, learning rates, layer normalization, etc., are identical to those used in (Sander et al., 2022). We also compared against N-ODE Transformer, an existing continuous-time transformer formulation which is introduced in (Baier-Reinio & De Sterck, 2020) and has been considered in other works. For details about the formulation and specific applications, see the discussion in Section 4. In the reported results, we refer to N-ODE Transformer with and without transport cost as unregularized N-ODE Transformer and regularized N-ODE Transformer, respectively.\n\nFor the continuous-time models, we employ the same architectures as the baselines but with a reduced hidden dimensions or number of layers for the transformer blocks. This is for investigating their parameter efficiency. To demonstrate the effectiveness of the transport cost on OT-Transformer, we also perform the experiments with X = 0 in (8), effectively creating an unregularized model. We label this model unregularized OT-transformer in the reported results. For the continous-time models, we use an explicit Euler scheme to numerically integrate the dynamical systems.\n\nFor more details of the experiments, we refer our readers to Appendix B. Our program is implemented using PyTorch (Paszke et al., 2017) and executed using NVIDIA A100 GPUs."}, {"title": "5.1. Point Cloud Classification", "content": "We use the ModelNet 40 dataset (Wu et al., 2015), which is among the most widely used benchmark for point cloud classification (Uy et al., 2019). The dataset contains roughly 10,000 Computer-Aided Design (CAD) models that are categorized into 40 distinct classes, including common objects such as airplanes, cars, and furniture.\n\nWe experiment with the Set Transformer model (Lee et al., 2019). It has an encoder-decoder architecture and is specifically designed to process unordered data, such as point clouds, ensuring that the output remains permutation invariant to its input. Following the setup of (Sander et al., 2022), we use the baseline architecture with two Induced Self Attention Blocks (ISABs) (Lee et al., 2019) in the encoder, where each ISAB contains two transformer blocks, and experiment with 5,000 uniformly sampled points for each shape. For the continuous-time models, we use the same architecture except that we put a fully-connected layer before the transformer blocks so that the dimension is consistent for continuous-time dynamics. Also the hidden dimensions d and k of the ISABs are reduced from 256 to 200. This reduces the number of parameters for the ISABs by 24%.\n\nWe perform the experiment over five random trials and report the best test accuracies in Table 1. The unregularized continuous-time models encountered gradient explosion, resulting in NaN outputs, and the issue persists even with slight regularization. We found that the models never suffered from gradient explosion with sufficient regularization, indicating that transport cost effectively stabilizes the training process. Hence, we only report the performance of the regularized models. The baseline Set Transformer obtains an average test accuracy of 87.4%. The regularized N-ODE Transformer achieves an accuracy of 87.5%, indicating negligible improvement over the vanilla model. Our OT-Transformer shows a sizable improvement and reports an average 89.9% test accuracy even with a smaller model. From the learning curves in Figure 2, we see that our model reports a lower data-fitting loss for training data compared to the vanilla model, despite the inclusion of a regularization term,"}, {"title": "5.2. Image Classification", "content": "To further demonstrate the applicability of our proposed method, we also perform experiments on imaging tasks. We consider the Vision Transformer (ViT), which was introduced in (Dosovitskiy et al., 2021). Since then, the model and its variants have achieved state-of-the-art performance in computer vision tasks (Ruan et al., 2022; Xia et al., 2024). The key feature of ViTs is that they divide an image into fixed-size patches, which are treated as sequences of data. ViTs then apply self-attention mechanisms to capture relationships between these patches, enabling it to learn complex structures across the entire image. We perform two image classification experiments following the same setup as in (Sander et al., 2022).\n\nMNIST Classification We first conduct a small-scale image classification experiment with the MNIST dataset (LeCun, 1998). Following (Sander et al., 2022), the baseline model ViT has one transformer block with a single-head self-attention layer and no fully-connected layer. Since it has only one transformer block, N-ODE Transformer and our OT-Transformer share the same formulation, and we report the results as OT-Transformer.\n\nThe OT-Transformer uses the same model architecture as the baseline model, except that the hidden dimensions d and k of the self-attention layer are reduced to 64 from 128. This reduces the number of parameters by over 80%. The experiments are conducted over five random trials. The best test accuracies are reported in Table 2. OT-Transformer demonstrates significant improvements over the baseline in both accuracy and model efficiency. The baseline model achieved a test accuracy of 93.0%. The unregularized OT-Transformer improves the test accuracy to 96.8%, although it uses a much smaller model architecture. The transport cost regularization further improves the test accuracy to 97.1% while maintaining the same reduced parameter count. Notably, OT-Transformer also exhibits significantly lower standard deviation across five trials when compared to the baseline and unregularized model, indicating enhanced stability and reliability in its performance. Interestingly, when we compare the learning curves of the unregularized and regularized OT-Transformers in Figure 3, we observe that including the transport cost regularization also reduces the training loss for data-fitting and accuracy."}, {"title": "Cats and Dogs Classification", "content": "We perform experiments on a binary cats and dogs image classification task, following (Sander et al., 2022). The baseline ViT has six layers of transformer blocks. We choose the continuous-time counterparts to have five layers; this reduces the number of parameters for the transformer blocks by around 20%. We report in Table 3 the test accuracies after the last epoch, which demonstrate a more significant improvement. The best test accuracy is also reported in Appendix B, where our model also performs best. We observe again that our OT-Transformer has the best performance and obtains a test accuracy of 79.0%, improving from the baseline's 77.6%. The standard deviation of the test accuracy, at 0.31%, is significantly lower than the baseline value of 0.86%, showing our proposed approach is more robust and reliable. We also observe that incorporating the transport cost regularization improves generalization and stability of OT-Transformer; without it, the average and standard deviation of test accuracy worsen to 78.2% and 0.39%, respectively. Both the unregularized and regularized N-ODE Transformers report a test accuracy of 75.6%, which is worse than the baseline model, making them undesirable methods for the problem. Unlike our model, incorporating the regularization also has little effect on the performance of N-ODE Transformer. This is likely due to the incompatibility of N-ODE Transformer and the regularization; see Section 4. We report the learning curves in Figure 4. When we compare the learning curves of the unregularized and regularized OT-Transformers, we see that including the transport cost regularization also improves the training loss for data-fitting and accuracy."}, {"title": "5.3. Sentiment Analysis", "content": "We perform sentiment analysis on the IMDb movie review dataset (Maas et al., 2011), aiming to predict whether each movie review is positive or negative. We use an identical baseline transformer architecture as in (Sander et al., 2022), which has six layers of transformer blocks. The OT-Transformer counterpart has only 3 layers, reducing the number of parameters of the transformer blocks by half.\n\nWe repeat the experiment for five random trials. In all trials, the unregularized N-ODE Transformer and OT-Transformer experienced issues with exploding gradients, resulting in NaN outputs. In order to estimate how the unregularized model would perform under more stable conditions, we impose a slight transport cost with X = 0.01. We note that the continuous-time models with slight and standard regularization completed all trials without issues. This shows the effectiveness of the transport cost regularization in stabilizing the training process and avoiding exploding gradients.\n\nThe best test accuracies are reported in Table 4. The baseline architecture achieved a test accuracy of 83.9%. The N-ODE Transformers with slight and standard regularization report a test accuracy of 83.6% and 83.9%, respectively, which are not better than the baseline model. The N-ODE Transformer with slight regularization reports a test accuracy of 83.6%. With a standard regularization, the test accuracy slightly increases to 83.9%. However, both results are not better than that of the baseline model. The OT-Transformer with slight regularization reported a test accuracy of 82.7%, which is subpar compared to the baseline model. On the other hand, the standard OT-Transformer achieves the best test accuracy of 84.6%, which is 0.7% higher than the baseline model, in spite of using a smaller model. The test accuracy is also 0.7% higher than that of the N-ODE Transformer's. We note that with the incorporation of transport cost, the accuracy of N-ODE Transformer is improved by only 0.3%. In contrast, the accuracy of OT-Transformer is boosted by 1.9%. Again, this is likely due to that our continuous-time formulation is inherently more suited for transport cost regularization than that of N-ODE Transformer; see Section 4 for the more detailed discussion.\n\nThe learning curves are reported in Figure 5. When we compare the results of the unregularized and regularized OT-Transformers, we see that the regularization effectively reduces overfitting by increasing training loss while simultaneously lowering test loss. Overall, we see that the combination of our continuous-in-time formulation and transport cost regularization enhances parameter efficiency and generalization of transformers."}, {"title": "6. Discussion and Summary", "content": "We proposed OT-Transformer, a continuous-time formulation of transformers. OT-Transformer is flexible and is general-purpose, as it can be easily adapted to different variations of the vanilla transformer architecture, making it suitable for a wide class of tasks. It is also distinctive from existing continuous-time transformer architecture. Our training objective includes a transport cost regularization, which we justified through theory and extensive experimentation. In particular, we showed that the training problem is ill-posed without the regularization. We also illustrated that the regularization stabilizes the training process and enhances the generalization of our model. Through multiple tests across different applications, we demonstrate that our model improves the baseline transformer architecture in terms of parameter efficiency and accuracy, while reducing the variance among trials at the same time. This is particularly beneficial during inference; without the need for gradient tracking, our smaller models are more memory efficient. Contributing to the point, we also notice that it is possible to reduce the number of time steps at the cost of minor decrease in performance during inference, see Appendix A. Most importantly, it outperforms the existing continuous-time transformer architecture. These results showcase the effectiveness and potential of our model."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}, {"title": "A. Different Time Stepping at Inference", "content": "In this section, we briefly discuss the numerical results of selecting different time step sizes at inference and the changes in model performance. It is evident in the Neural ODE literature, such as (Chen et al., 2018; Onken & Ruthotto, 2020) that using smaller step sizes in time discretization improves integration accuracy and can enhance overall model performance. However, in (Onken et al., 2021) it is also pointed out that such requirement can be relaxed if the underlying dynamics is sufficiently regular, particularly at inference. We here use the aforementioned MNIST experiment in Section 5 to verify the performance change when using different time step sizes to evaluate a pretrained model. Note that the models reported in Section 5 are trained using 20 time steps from time 0 to T = 1.\n\nWe display the results in Table 5. Here, we test different number of time steps from 1 to 20 for both the unregularized model and the OT-Transformer model. Notice here first regularized models performance consistently better than that of the unregularized model, indicating the importance of OT regularization. More importantly we find that it is possible to reduce the number of time steps in evaluation with little decrease in model performance. Specifically we note that decreasing the number of time steps from 20 to 8 only resulted in about 0.5% decrease in test accuracy. We believe this finding can be meaningful, as it suggests further efficiency improvement at the model deployment stage. However, additional testing may be required for other examples, we will leave further investigation of this point for future work."}, {"title": "B. Experimental Details and Results", "content": "We report the detailed experimental setups here. We adapted the code provided by (Sander et al., 2022), maintaining the same default data processing setup, hyperparameters, and other experimental settings as used in their implementation.\n\nPoint Cloud Classification We use the ModelNet40 dataset. For each instance, we uniformly sample 5000 points from each element in the dataset. We use a Set Transformer (Lee et al., 2019) with two Induced Self Attention Blocks (ISABs) in the encoder, where each ISAB contains two transformer blocks, and with a Pooling by Multihead Attention (PMA) Module in the decoder. We use an Adam optimizer, with batch size 64, 200 training epochs, and learning rate of 1 \u00d7 10-3. For the baseline transformer model, the hidden dimensions of the ISABs are d, k = 256, and for the continuous-time models, they are reduced to 200. For the regularized N-ODE Transformer and OT-Transformer, the regularization hyperparameters are x = 0.1 and x = 1, respectively, as they provide the optimal performance in our tests. We use T = 1 and a total of 8 time steps for the numerical integration.\n\nMNIST Classification We use a Vision Transformer (ViT) (Dosovitskiy et al., 2021) with self-attention layer with a single head. The patch size is 7 \u00d7 7. We use an Adam optimizer. The number of epochs is 45 and the batch size is 100. The learning rate is set to 5 \u00d7 10\u20134 for the first 35 epochs, then decreased to 5 \u00d7 10-5 until the 41st epoch, at which point it is reduced to 5 \u00d7 10-6. For the baseline model, the hidden dimensions d and k are 128. For the continuous-time models, they are reduced to 64. For OT-Transformer, the regularization hyperparameter is \u5165 = 0.01 as it provides the optimal performance in our tests. We use T = 1 and a total of 20 time steps for the numerical integration.\n\nCats and Dogs Classification We again use ViT. The patch size is 16 \u00d7 16. We use an Adam optimizer. The learning rate is 3 \u00d7 10-5. The number of epochs is 250, and the batch size is 64. The hidden dimensions d and k are 128. For the baseline model, it has 6 transformer blocks. For the continuous-time models, the number of transformer blocks is reduced to 5. For the regularized N-ODE Transformer and OT-Transformer, the regularization hyperparameters are X = 0.005 and X = 0.01, respectively, as they provide the optimal performance in our tests. We use T 1 and a total of 20 time steps for the numerical integration.\n\nSentiment Analysis We follow (Sander et al., 2022) to use a baseline model with 6 layers of transformer blocks. For the continuous-time models, the number of transformer blocks is reduced to 3. We use an Adam optimizer with 15 epochs. The learning rate is 1 \u00d7 10\u22124 for the first 12 epochs and 1 \u00d7 10-5 afterward. The batch size is 64. The hidden dimensions d and k are 256. The batch size is 64. For both the regularized N-ODE Transformer and OT-Transformer, the regularization hyperparameter is A = 0.5, as it provides the optimal performance in our tests. We use T = 1 and a total of 8 time steps for the numerical integration."}]}