{"title": "A Generalization Bound for Nearly-Linear Networks", "authors": ["Eugene Golikov"], "abstract": "We consider nonlinear networks as perturbations of linear ones. Based on this approach, we present novel generalization bounds that become non-vacuous for networks that are close to being linear. The main advantage over the previous works which propose non-vacuous generalization bounds is that our bounds are a-priori: performing the actual training is not required for evaluating the bounds. To the best of our knowledge, they are the first non-vacuous generalization bounds for neural nets possessing this property.", "sections": [{"title": "1 Introduction", "content": "Despite huge practical advancements of deep learning, the main object of this field, a neural network, is not yet fully understood. As we do not have a complete understanding of how neural networks learn, we are not able to answer the main question of deep learning theory: why do neural networks generalize on unseen data?\nWhile the above question is valid for any supervised learning model, it is notoriously difficult to answer for neural nets. The reason is that modern neural nets have billions of parameters and as a result, huge capacity. Therefore among all parameter configurations that fit the training data, there provably exist such configurations that do not fit the held-out data well [12]. This is the reason why classical approaches for bounding a generalization gap, i.e. the difference between distribution and train errors, fall short on neural networks: such approaches bound the gap uniformly over a model class. That is, if weights for which the network performs poorly exist, we bound our trained network's performance by performance of that poor one.\nAs we observe empirically, networks commonly used in practice do generalize, which means that training algorithms we use (i.e. gradient descent or its variants) choose \"good\" parameter configurations despite the existence of poor ones. In other words, these algorithms are implicitly biased towards good solutions.\nUnfortunately, implicit bias of gradient descent is not fully understood yet. This is because the training dynamics are very hard to integrate, or even characterize, analytically. There are two main obstacles we could identify. First, modern neural networks have many layers, resulting in a high-order weight evolution equation. Second, activation functions we use are applied to hidden representations elementwise, destroying a nice algebraic structure of stacked linear transformations.\nIf we remove all activation functions, the training dynamics of gradient descent can be integrated analytically [9] under certain assumptions. However, the resulting model, a linear network, is as expressive as a linear model, thus loosing one of the crucial advantages of neural nets.\nIdea. The idea we explore in the present paper is to consider nonlinear nets as perturbations of linear ones. We show that the original network can be approximated with a proxy-model whose parameters can be computed using parameters of a linear network trained the same way as the original one. Since the proxy-model uses the corresponding linear net's parameters, its generalization gap can be meaningfully bounded with classical approaches. Indeed, if the initial weights are fixed, the result of learning a linear network to minimize square loss on a dataset $(X \\in \\mathbb{R}^{d\\times m}, Y \\in \\mathbb{R}^{d_{out}\\times m})$ is solely determined by $YX^T \\in \\mathbb{R}^{d_{out}\\times d}$ and $XX^T \\in \\mathbb{R}^{d\\times d}$, where $d, d_{out}$ are the input and output dimensions, and $m$ is the dataset size. The number of parameters in these two matrices is much less than the total number of parameters in the network, making classical counting-based approaches meaningful.\nContributions. Our main contribution is a generalization bound given by Theorem 4.2, which is ready to apply in the following setting: (1) fully-connected networks, (2) gradient descent with vanishing learning rate (gradient flow), (3) binary classification with MSE loss. The main disadvantage of our bound is that it"}, {"title": "2 Comparison to previous work", "content": "Our bounds has the following advantages over some other non-vacuous bounds available in the literature (e.g. [1, 3, 2, 13]):\n1. They are a-priori bounds, i.e. getting the actual trained the model is not required for evaluating them. To the best of our knowledge, they are the first non-vacuous a-priori bounds available for neural nets. All works mentioned above, despite providing non-vacuous bounds, could be evaluated only on a trained network thus relying on the implicit bias phenomenon which is not well understood yet.\n2. The bound of our Theorem 4.2 does not require a held-out dataset to evaluate it, compared to [3] and coupled bounds of [1]. Indeed, if one had a held-out dataset, one could just use it directly to evaluate the trained model, thus questioning the practical utility of such bounds.\n3. It does not grow with network width. In contrast, PAC-Bayesian bounds, [1, 2, 13], might grow with width.\n4. Similarly to [1, 3], we bound the generalization gap of the original trained model, not its proxy. In contrast, [2] introduces a Gaussian noise to the learned weights, while [13] crucially relies on quantization and compression after training.\nFor a fair comparison, we also list the disadvantages our present bound has:\n1. The bound of our Theorem 4.2 becomes non-vacuous only when the following two conditions hold. First, a simple counting-based generalization bound for a linear model evaluated in the same setting should be non-vacuous. Such a bound is vacuous even for binary classification on the standard MNIST dataset, but becomes non-vacuous if we downsample the images.\n2. Second, the activation function has to be sufficiently close to being linear. To be specific, for a two-layered leaky ReLU neural net trained on MNIST downsampled to 7x7, one needs the negative slope to be not less than 0.99 (1 corresponds to a linear net, while ReLU corresponds to 0).\n3. One may hope for the bound to be non-vacuous only for a partially-trained network, while for a fully-trained network the bound diverges.\n4. Even in the most optimistic scenario, when we manage to tighten the terms of our bound as much as possible, our bound stays non-vacuous only during the early stage of training when the network has not started \"exploiting\" its nonlinearity yet (Section 8). However, the minimal negative ReLU slope for which the bound stays non-vacuous is much smaller, 0.6."}, {"title": "3 Related work", "content": "Non-vacuous generalization bounds. While first generalization bounds date back to [7], the bounds which are non-vacuous for realistically large neural nets appeared quite recently. Specifically, [2] constructed a PAC-Bayesian bound which was non-vacuous for small fully-connected networks trained on MNIST and FashionMNIST. The bound of [13], also of PAC-Bayesian nature, relies on quantization and compression after training. It ends up being non-vacuous for VGG-like nets trained on large datasets of ImageNet scale. The bound of [1] is the first non-vacuous bound that applies directly to the learned model and not to its proxy. It is not obvious whether their construction could be generalized to neural nets with more than two layers. The bound of [3] is not PAC-Bayesian in contrast to the previous three. It is based on the notion of effective depth: it assumes that a properly trained network has small effective depth, even if it is deep. Therefore its effective capacity is smaller than its total capacity, which allows for a non-vacuous bound. See the previous section for discussion of some of the features of these bounds.\nLinear networks training dynamics. The pioneering work that integrates the training dynamics of a linear network under gradient flow to optimize"}, {"title": "4 Main result", "content": "Notations. For integer $L\\geq 0$, $[L]$ denotes the set $\\{1,..., L\\}$. For integers $l<L$, $[l,L]$ denotes the set $\\{1, ..., L\\}$. For a vector $x$, $||x||$ denotes its Euclidean norm. For a matrix $X$, $||X||$ denotes its maximal singular value, while $||X||_F$ denotes its Frobenius norm.\n4.1 Setup\nModel. The model we study is a fully-connected LeakyReLU network with $L$ layers:\n$f_\\theta(x) = W_L x_{L-1}(x), \\; x_{0}(x) = x$,                                                      (1)\n$x_l(x) = \\varphi_\\epsilon (W_l x_{l-1}(x)) \\; \\forall l \\in [L-1]$,                                                  (2)\nwhere $\\theta \\in \\mathbb{R}^N$ denotes the vector of all weights, i.e. $\\theta = cat(\\{\\text{vec}(W_l)\\}_{l=1}^L)$, $W_l \\in \\mathbb{R}^{n_l \\times n_{l-1}} \\; \\forall l \\in [L]$, and $\\varphi_\\epsilon$ is a Leaky ReLU with a negative slope $1-\\epsilon$, that is:\n$\\varphi_\\epsilon(x) = x - \\epsilon \\min(0, x)$.                                                                              (3)\nSince we are going to consider only binary classification in the present work, we take $n_L = 1$. We also define $d=n_0$ to denote the input dimension.\nData. Data points $(x, y)$ come from a distribution $D$. We assume all $x$ from $D$ to lie on a unit ball, $||x|| \\leq 1$, and all $y$ to equal $\\pm 1$. During the training phase, we sample a set of $m$ points iid from $D$ to form a dataset $(X, Y)$, where $X \\in \\mathbb{R}^{d\\times m}$ and $Y \\in \\mathbb{R}^{1\\times m}$.\nTraining. Assuming $rk\\; X = d$ (which implies $m > d$, i.e. the data is abundant), we train our model on $(X, Y)$ with gradient flow to optimize square loss on whitened data, i.e. on $(\\bar{X}, Y)$ for $\\bar{X} = \\Sigma_\\chi^{-1/2}X$, where $\\Sigma_\\chi = \\frac{1}{m}XX^T \\in \\mathbb{R}^{d\\times d}$ is an empirical feature correlation matrix. That is,\n$\\frac{dW_l(t)}{dt} = \\frac{1}{m} \\frac{\\partial}{\\partial W_l} \\frac{1}{2m} ||Y-f_{\\theta(t)}(\\bar{X})||_F^2, \\; \\forall l \\in [L]$.                                                                                                      (4)\nNote that $\\bar{X}\\bar{X}^T = mI_d$.\nInference. To conform with the above training procedure, we take the model output at a point $x$ to be $f_{\\theta}(t)(\\Sigma^{-1/2}x)$, where $\\Sigma$ is a (distribution) feature correlation matrix: $\\Sigma = E_{(x,y)~D}(xx^T) \\in \\mathbb{R}^{d\\times d}$. We assume this matrix to be known; in practice, we could substitute it with $\\Sigma_X$.\nPerformance measure. We take the risk function to be misclassification loss: $r(z, y) = \\text{Ind}[zy < 0]$, where $z, y \\in \\mathbb{R}$ and $\\text{Ind}[.]$ is an indicator of a condition. The empirical (train) risk on the dataset $(X, Y)$ and the distribution risk of the model trained for time $t$ are then defined as\n$\\hat{R}(t) = \\frac{1}{m} \\sum_{k=1}^m r \\big(f_{\\theta(t)} (\\Sigma_X^{-1/2}x_k), Y_k\\big),$                                                                                                  (5)\n$R(t) = E_{(x,y)~D} r \\big(f_{\\theta(t)} (\\Sigma^{-1/2}x), y\\big).$                                                                                                  (6)\nFor a given model $f$, we also define $R(f)$ and $\\hat{R}(f)$ accordingly.\nIn addition, for $\\gamma > 0$, we define $\\gamma$-margin loss: $r_{\\gamma}(z, y) = \\text{Ind}[z \\text{sgn} y < \\gamma]$, and its continuous version:\n$\\hat{r}(z,y) = \\begin{cases} 1, & zy < 0, \\\\ 1-zy/\\gamma, & zy \\in [0, \\gamma],\\\\ 0, & zy > \\gamma. \\end{cases}$                                                                                  (7)\nWe define $\\hat{R}^\\gamma$ and $R^\\gamma$ analogously to $\\hat{R}$, and $\\hat{R}^\\gamma$ and $\\mathbb{R}^\\gamma$ analogously to $R$.\n4.2 Generalization bound\nWe will need the following assumption on the training process:"}, {"title": "4.3 Choosing the training time", "content": "As we see, $\\Delta_{\\kappa}(t)$ diverges super-exponentially as $t \\to \\infty$ for $L=2$ and as $t \\to \\frac{\\infty}{(e^{S^{2-L}})}$ for $L\\geq 3$, so the bound eventually becomes vacuous. For the bound to make sense, we should be able to find $t$ small enough for $\\Delta(t)$ to stay not too large, and at the same time, large enough for the training risk $\\hat{R}(t)$ to get considerably reduced.\nIn the present section, we are going to demonstrate that for values of $t$ which correspond to partially learning the dataset (i.e. for which $\\hat{R}(t) \\in (0,1)$), the last term of the bound, $\\Delta_{\\kappa,\\beta}(t)/\\gamma$, admits a finite limit as $\\beta\\to 0$ (the saddle-to-saddle regime of [4]).\nWe do not know how $\\hat{R}(t)$ decreases with $t$ in our case. However, when the network is linear, this can be computed explicitly when either the weight initialization is properly aligned with the data, or the initialization norm $\\beta$ vanishes. We are going to perform our whole analysis for $L=2$ in the main, and defer the case $L\\geq 3$ to Appendix B.2.\n$\\,^1 \\Gamma(s,x)$ is an upper-incomplete gamma-function defined as $\\Gamma(s,x) = \\int_x^\\infty t^{s-1} e^{-t} dt$.\\n"}, {"title": "5 Experiments", "content": "Setup. We consider an L-layer bias-free fully-connected network of width 64 and train it to classify 0-4 versus 5-9 digits of MNIST (i.e. $m = 60000$). In order to approximate the gradient flow dynamics, we run gradient descent with learning rate 0.001. By default, we take L = 2, the floating point precision to be p = 32,"}, {"title": "6 Proof of the main result", "content": "6.1 Proof of Theorem 4.2 for $\\kappa = 1$\nWe start with approximating our learned network $f_{\\theta}$ with $f_{0}$, i.e. with a nonlinear network that uses the weights learned by the linear one. This approximation deviates from the original network the following way: $\\forall x \\in \\mathbb{R}^d, \\epsilon \\in [0, 1]$,\n$\\begin{aligned} ||f_{\\theta\\epsilon}(x) - f_{\\theta_0}(x) || = & \\Big|\\Big| \\int_0^\\epsilon \\frac{\\partial f_{\\theta_\\tau}(x)}{\\partial \\tau} d\\tau \\Big|\\Big| \\leq \\int_0^\\epsilon  \\Big|\\Big| \\frac{\\partial f_{\\theta_\\tau}(x)}{\\partial \\tau} \\Big|\\Big| d\\tau \\leq \\epsilon \\sup_{\\tau\\in [0, \\epsilon]} \\Big|\\Big| \\frac{\\partial f_{\\theta_\\tau}(x)}{\\partial \\tau} \\Big|\\Big| \\\\ = & \\epsilon \\sup_{\\tau\\in [0, \\epsilon]} \\Big|\\Big|  \\sum_{l=1}^L \\frac{\\partial W_l}{\\partial \\tau}  \\frac{\\partial f_{\\theta_\\tau}(x)}{\\partial W_l} \\Big|\\Big| \\leq  \\epsilon \\sum_{l=1}^L \\sup_{\\tau\\in [0, \\epsilon]} \\Big|\\Big| \\frac{\\partial W_l}{\\partial \\tau}  \\frac{\\partial f_{\\theta_\\tau}(x)}{\\partial W_l} \\Big|\\Big|  \\\\ & \\leq \\epsilon \\sum_{l=1}^L  \\sup_{\\tau\\in [0, \\epsilon]} \\big( || \\frac{\\partial W_l}{\\partial \\tau} ||  \\big|\\big|   || \\frac{\\partial f_{\\theta_\\tau}(x)}{\\partial W_l} \\Big|\\Big|  \\big) \\leq  \\epsilon \\sum_{l=1}^L  \\sup_{\\tau\\in [0, \\epsilon]}   \\Big|\\Big| \\frac{\\partial W_l}{\\partial \\tau} \\Big|\\Big|    \\prod_{k \\neq l} ||W_k|| \\big|\\big|     ||x||, \\end{aligned}$                                                                                                 (23)\nsince we use Leaky ReLUs. We omitted the argument t for brevity. We get a similar deviation bound on the training dataset:\n$\\begin{aligned} \\frac{1}{m} ||f_{\\theta\\epsilon}(\\bar{X}) - f_{\\theta_0}(\\bar{X}) || \\leq &  \\frac{1}{m} \\epsilon  \\sum_{l=1}^L \\sup_{\\tau\\in [0, \\epsilon]} || \\frac{\\partial W_l}{\\partial \\tau} || \\big(  ||W_l\\bar{X} ||_F \\prod_{k \\neq l} ||W_k|| \\big)  \\\\ & \\leq \\epsilon \\sup_{\\tau\\in [0, \\epsilon]}  || \\frac{\\partial W_1}{\\partial \\tau} || ||W_1\\bar{X} ||_F \\prod_{k=2}^L ||W_k|| + \\epsilon \\sum_{l=2}^L \\sup_{\\tau\\in [0, \\epsilon]} || \\frac{\\partial W_l}{\\partial \\tau} || ||W_l\\bar{X} ||_F \\prod_{k\\in[2:L]\\setminus \\{l\\}} ||W_k|| \\big). \\end{aligned}$                                                                                                      (24)\nWe complete the above deviation bound with bounding weight norms and norms of weight derivatives:\nLemma 6.1. Under Assumption 4.1, $\\forall \\tau \\in [0, 1], t>0  \\\\\\frac{1}{\\sqrt{m}}||W_1(t)\\bar{X}||_F \\leq \\rho u(t), \\sqrt{m} || \\frac{\\partial W_i(t)}{\\partial \\tau}||_F \\leq v(t),  and $\\forall l \\in [L] \\; ||W_l(t)|| \\leq u(t), \\; ||\\frac{\\partial W_l (t)}{\\partial \\tau}|| \\leq v(t)$ for $u,v$ defined in Theorem 4.2.\nSee Section 6.3 and Appendix B.1 for the proof.\nNow we can relate the risk of the original model with the risk of the approximation. Since $r \\leq r^\\gamma < \\hat{r}^\\gamma$ and $\\hat{r}^\\gamma$ is 1/$\\gamma$-Lipschitz,\n$\\begin{aligned} \\hat{R}(f_{\\theta\\epsilon}) - \\hat{R}^\\gamma (f_{\\theta\\epsilon}) &  \\\\ \\leq & R(f_{\\theta\\epsilon}) - \\hat{R}^\\gamma (f_{\\theta\\epsilon}) \\leq R(f_{\\theta_0}) - \\hat{R}^\\gamma (f_{\\theta_0}) \\\\ & + E \\frac{1}{\\gamma} || f_{\\theta}(x) \u2013 f_{\\theta_0}(x) || + \\frac{1}{\\gamma} \\frac{1}{m}||f_{\\theta}(\\bar{X}) \u2013 f_{\\theta_0}(\\bar{X})||_1.  \\end{aligned}$                                                                      (25)\nwhere the expectation is over the data distribution $D$, and $\\bar{x} = \\Sigma^{-1/2}x$ is the actual input of the network.\nAs for the last term, the deviation on the train dataset, we use that $||z||_1 \\leq \\sqrt{m} ||z||$ for any $z \\in \\mathbb{R}^m$, and Equation (24). As for the deviation on the test dataset, due to Equation (23) and Lemma 6.1, in order to bound the last term, it suffices to bound $E||\\bar{x}||$. Since $\\Sigma = E[xx]$, we get\n$\\begin{aligned} E||\\Sigma^{-1/2}x||^2 = & E [\\bar{x}^\\top\\bar{x}]  \\\\ = & E \\text{tr} [\\bar{x}\\bar{x}^\\top] = \\text{tr}[I_d] = d. \\end{aligned}$                                                                                                         (26)\nThis gives $E||\\Sigma^{-1/2}x|| \\leq \\sqrt{E||\\Sigma^{-1/2}x||^2} < \\sqrt{d}$.\nThe first two terms is a generalization gap of the proxy-model. We use a simple counting-based bound:\n$|\\hat{R}^\\gamma (f_{\\theta_0}(t)) - \\hat{R}^\\gamma (f_{\\theta_0}(t))| \\leq \\sqrt{\\frac{\\log |\\mathcal{F}_1(t; \\theta(0))| - \\log \\delta}{2m}},$                                                                                                                     (27)\nw.p. $\\geq 1-\\delta$ over sampling the dataset $(X,Y)$, where $\\mathcal{F}_1(t; \\theta(0))$ denotes the set of functions representable with $f_{\\theta_0}(t)$ for a given initial weights $\\theta(0)$, where $\\theta_0(t)$ is a result of running the gradient flow Equation (4) for time t. As long as we work with finite precision, this class is finite:"}, {"title": "6.2 Proof of Theorem 4.2 for $\\kappa = 2$", "content": "What changes for $\\kappa=2$ is a proxy-model. Consider the following:\n$\\bar{f}_{\\theta_0,\\theta\\epsilon}(x) = f_{\\theta_0}(x) + (f_{\\theta \\epsilon}(\\bar{X}) - f_{\\theta_0}(\\bar{X})) \\bar{X}^+ x.$                                                                                        (28)\nThat is, we take the same proxy-model as before, but we add a linear correction term. This correction term aims to fit the proxy model to the original one, $f_{\\theta \\epsilon}$, on the training dataset $\\bar{X}$. We prove the following lemma in Appendix A.1:\nLemma 6.3. Under the premise of Lemma 6.1, $\\forall t \\geq 0 \\; \\forall \\epsilon \\in [0, 1] \\; \\forall x \\in \\mathbb{R}^d$ we have\n$\\begin{aligned} & || f_{\\theta \\epsilon}(t) (x) - \\bar{f}_{\\theta_0(t), \\theta_{\\epsilon}(t)} (x) ||  \\\\ & \\leq \\frac{u^{L-1}(t) v(t) }{ \\big( (L-1)(L+1+\\rho(L-1))  \\big)} ||x|| \\epsilon^2;  \\end{aligned}$                                                                                                                      (29)\n$\\begin{aligned} & || f_{\\theta \\epsilon}(t) (\\bar{X}) - \\bar{f}_{\\theta_0(t), \\theta_{\\epsilon}(t)} (\\bar{X}) ||  \\\\ & \\leq \\frac{u^{L-1}(t) v(t)  }{ \\big(  2(L-1)(1+\\rho(L-1)) \\big)} \\sqrt{m} \\epsilon^2.  \\end{aligned}$                                                                                                                        (30)\nThe deviation now scales as $\\epsilon^2$ instead of $\\epsilon$.\nWhat remains is to bound the size of $\\mathcal{F}_2(t; \\theta(0))$, which denotes the set of functions representable with our new proxy-model $\\bar{f}_{\\theta_0(t), \\theta_{\\epsilon}(t)}$ for given initial weights $\\theta(0)$. Since $\\bar{f}_{\\theta_0(t), \\theta_{\\epsilon}(t)}$ is a sum of $f_{\\theta_0}(t)$ and a linear model, its size is at most $2^{pd}$ times larger:\n$|\\mathcal{F}_2(t; \\theta(0))| \\leq |\\mathcal{F}_1(t; \\theta(0))|^{2pd} \\leq 2^{2pd}.$                                                                                                          (31)\nThis finalizes the proof of Theorem 4.2 for $\\kappa = 2$.\n6.3 Proof of Lemma 6.1\nWe are going to present the proof for $L=2$ in the present section, and defer the case of $L \\geq 3$ to Appendix B.1."}, {"title": "6.3.1 Weight norms", "content": "Let us expand the weight evolution Equation (4):\n$\\frac{dW_1}{dt} = [\\frac{\\partial f_\\epsilon}{\\partial W} D^\\epsilon] \\bar{X}^T,$                                                                                                            (32)\n$\\frac{dW_2}{dt} = [\\frac{\\partial f_\\epsilon}{\\partial W_1} D^\\epsilon] \\bar{X}^T.$                                                                                                                (33)\nwhere we define\n$\\frac{\\partial f_\\epsilon}{\\partial W} = \\frac{1}{m}(Y-W_2 [D^\\epsilon W_1 \\bar{X}]) ,$                                                                                                          (34)\nand $D^\\epsilon = (\\varphi_\\epsilon)' (W_1 \\bar{X})$ is a $n_2 \\times m$ matrix with entries equal to $1$ or $1-\\tau$. We express it as $D^\\epsilon = (1-\\tau)\\mathbb{1}_{n_2 \\times m} + \\tau \\Delta$, where $\\Delta$ is a $n_2 \\times m$ 0-1 matrix.\nLet us bound the evolution of weight norms:\n$\\begin{aligned} & \\frac{d ||W_1^T||}{dt} \\leq \\Big|\\Big| \\frac{dW_1}{dt} \\Big|\\Big|_F \\leq  \\Big|\\Big| (1-\\tau)\\frac{\\partial f_\\epsilon}{\\partial W}  \\bar{X}^T + \\tau \\Delta \\frac{\\partial f_\\epsilon}{\\partial W_1}  \\bar{X}^T  \\Big|\\Big|_F  \\\\ & \\leq ((1 - \\tau)|||| \\frac{\\partial f_\\epsilon}{\\partial W} ||  ||\\bar{X}^T||_F + \\tau || \\Delta \\circ \\frac{\\partial f_\\epsilon}{\\partial W_1}   || ||\\bar{X}^T||_F) ||W_2||.  \\end{aligned}$                                                                                              (35)\nSince multiplying by a 0-1 matrix elementwise does not increase Frobenius norm, we get\n$\\begin{aligned} ||\\Delta \\circ  \\frac{\\partial f_\\epsilon}{\\partial W_1} ||_F \\leq ||\\Delta \\circ  \\frac{\\partial f_\\epsilon}{\\partial W_1} ||_F \\leq  || \\frac{\\partial f_\\epsilon}{\\partial W_1}   ||_F.  \\end{aligned}$                                                                                                  (36)\nNoting that $\\Xi'$ and $\\bar{X}^T$ are row matrices, this results in\n$\\frac{d ||W_1^T||}{dt} \\leq ((1 - \\tau)|||| \\Xi' \\bar{X}  ||_F  + \\tau || \\Xi' || ||\\bar{X}^T||_F) ||W_2||.$                                                                                                           (37)\nBy a similar reasoning,\n$\\frac{d ||W_2||}{dt} \\leq ((1 - \\tau)|||| \\Xi' \\bar{X}  ||_F  + \\tau || \\Xi' || ||\\bar{X}^T||_F) ||W_1||.$                                                                                                                (38)\nBounding norms, option one. A this point, we could bound $||W_1\\bar{X}||_F < ||W_1||||\\bar{X}||_F$. We then make use of the following lemma which we prove in Appendix A.2:\nLemma 6.4. $|| \\Xi' \\bar{X}^T|| = || \\Xi' \\bar{X}^T ||_F \\leq (1 + \\rho \\beta^L)$. If we additionally take Assumption 4.1 then $||\\Xi' \\bar{X}^T || = || \\Xi' \\bar{X}^T ||_F < s + \\rho \\beta^L$.\nSince $\\bar{X}\\bar{X}^T = mId$, we have $||\\bar{X}|| = \\sqrt{m}$ and $||\\bar{X}||_F = \\sqrt{md}$. From the above lemma and since $\\tau < \\epsilon$, $W_{1,2}(t) \\leq u(t)$, where $u$ satisfies\n$\\frac{du(t)}{dt} = s \\epsilon u(t), \\; u(0) = \\beta.$                                                                                                                      (39)\nIts solution is given by $u(t) = B e^{s\\epsilon t}$."}, {"title": "6.3.2 Norms of weight derivatives", "content": "In Appendix A.3, we follow the same logic to demonstrate that $\\frac{\\partial dW_1}{\\partial \\tau}$ and $\\frac{\\partial dW_2}{\\partial \\tau}$ are all bounded by the same v which satisfies\n$\\frac{dv}{dt} = v [(1 + \\rho)u^2 + s] + u [1 + \\rho u^2], \\; v(0) = 0.$                                                                                             (48)\nIt is a linear ODE in $v(t)$, and $u(t)$ is given: $u(t) = B e^{s\\epsilon t}$. We solve it in Appendix A.4 to get $v(t)$ from Theorem 4.2."}, {"title": "7 Discussion", "content": "7.1 Assumptions\nWhitened data. Overall, the assumption on whitened data is not necessary for a result similar to Theorem 4.2 to hold. We assume the data to be whitened for two reasons. First, it legitimates the choice of training time $t_{\\alpha}(\\beta)$ since it is based on the analysis of [9], which assumed the data to be whitened. If we dropped it, we could still evaluate the bound of Theorem 4.2 at $t = t_{\\alpha}(\\beta)$, but it would be less clear whether the training risk $\\hat{R}^\\gamma$ becomes already small by this time.\nSecond, we had to bound $||\\bar{X}||$ throughout the proof of Theorem 4.2 and $||\\bar{X}^+||$ in the proof of Lemma 6.3. For whitened data, these are simply $\\sqrt{m}$ and $1/\\sqrt{m}$, which is a clear dependence on $m$, making the final bound look cleaner. Otherwise, they would be random variables whose dependence on $m$ would be not obvious.\nThird, if we considered training on the original dataset $(Y, X)$ instead of the whitened one, $(Y, \\bar{X})$, we would have to know $Y\\bar{X}^T$ and $\\bar{X}\\bar{X}^T$ in order to determine $\\theta^\\epsilon(t)$ for a given $t$ and initialization $\\theta^\\epsilon(0)$. These two matrices have $d + \\frac{d(d+1)}{2}$ parameters, compared to just $d$ for $Y\\bar{X}^T$. This way, $\\Upsilon_{\\kappa}$ would grow as $d$ instead of $\\sqrt{d}$.\nGradient flow. We expect our technique to follow through smoothly for finite-step gradient descent. Introducing momentum also seems doable. However, generalizing it to other training procedures, e.g. the ones which use normalized gradients, might pose problems since it is not clear how to reasonably upper-bound the norm of the elementwise ratio of two matrices.\nAssumption 4.1. We use this assumption to prove the second part of Lemma 6.4. If we dropped it, the bound would be $|| \\Xi' \\bar{X}^T ||_F \\leq || \\Xi' || || \\bar{X}^T || \\leq 1 + \\rho \\beta^L$ instead of $s+\\rho \\beta^L$. This would result in larger exponents for the definition u in Theorem 4.2."}, {"title": "7.2 Proof", "content": "We expect the bounds on weight norms $u(t)$ to be quite loose since we use Lemma 6.4 to bound the loss. This lemma bounds the loss with its value at the initial-ization, while the loss should necessarily decrease. If we could account for the loss decrease, the resulting $u(t)$ would increase with a lower exponent, or even stay bounded as $\\bar{u}(t)$, which corresponds to a linear model, does. This way, we would not have to assume $\\epsilon$ to vanish as $\\beta$ vanishes in order to keep the bound non-diverging for small $\\beta$ at the training time $t_{\\alpha}(\\beta)$. Also, the general bound of Theorem 4.2 would diverge with training time $t$ much slower. We leave it for future work.\nAs we see from our estimates, $\\Upsilon_{\\kappa}$ becomes the main bottleneck of our bound for small $\\epsilon$. The bound we used for $\\Upsilon_{\\kappa}$ is very naive; we believe that better bounds are possible. Improving the bound for $\\Upsilon_{\\kappa}$ will increase the maximal"}]}