[{"title": "8 How far could we get with our approach?", "authors": ["Eugene Golikov"], "abstract": "We consider nonlinear networks as perturbations of linear ones. Based on this approach, we present novel generalization bounds that become non-vacuous for networks that are close to being linear. The main advantage over the previous works which propose non-vacuous generalization bounds is that our bounds are a-priori: performing the actual training is not required for evaluating the bounds. To the best of our knowledge, they are the first non-vacuous generalization bounds for neural nets possessing this property.", "sections": [{"title": "1 Introduction", "content": "Despite huge practical advancements of deep learning, the main object of this field, a neural network, is not yet fully understood. As we do not have a complete understanding of how neural networks learn, we are not able to answer the main question of deep learning theory: why do neural networks generalize on unseen data?\nWhile the above question is valid for any supervised learning model, it is notoriously difficult to answer for neural nets. The reason is that modern neural nets have billions of parameters and as a result, huge capacity. Therefore among all parameter configurations that fit the training data, there provably exist such configurations that do not fit the held-out data well [12]. This is the reason why classical approaches for bounding a generalization gap, i.e. the difference between distribution and train errors, fall short on neural networks: such approaches bound the gap uniformly over a model class. That is, if weights for which the network performs poorly exist, we bound our trained network's performance by performance of that poor one.\nAs we observe empirically, networks commonly used in practice do generalize, which means that training algorithms we use (i.e. gradient descent or its variants) choose \"good\" parameter configurations despite the existence of poor ones. In other words, these algorithms are implicitly biased towards good solutions.\nUnfortunately, implicit bias of gradient descent is not fully understood yet. This is because the training dynamics are very hard to integrate, or even characterize, analytically. There are two main obstacles we could identify. First, modern neural networks have many layers, resulting in a high-order weight evolution equation. Second, activation functions we use are applied to hidden representations elementwise, destroying a nice algebraic structure of stacked linear transformations.\nIf we remove all activation functions, the training dynamics of gradient descent can be integrated analytically [9] under certain assumptions. However, the resulting model, a linear network, is as expressive as a linear model, thus loosing one of the crucial advantages of neural nets.\nIdea. The idea we explore in the present paper is to consider nonlinear nets as perturbations of linear ones. We show that the original network can be approximated with a proxy-model whose parameters can be computed using parameters of a linear network trained the same way as the original one. Since the proxy-model uses the corresponding linear net's parameters, its generalization gap can be meaningfully bounded with classical approaches. Indeed, if the initial weights are fixed, the result of learning a linear network to minimize square loss on a dataset $(X \\in \\mathbb{R}^{d\\times m}, Y \\in \\mathbb{R}^{d_{out}\\times m})$ is solely determined by $YX^T \\in \\mathbb{R}^{d_{out}\\times d}$ and $XX^T \\in \\mathbb{R}^{d\\times d}$, where $d, d_{out}$ are the input and output dimensions, and $m$ is the dataset size. The number of parameters in these two matrices is much less than the total number of parameters in the network, making classical counting-based approaches meaningful.\nContributions. Our main contribution is a generalization bound given by Theorem 4.2, which is ready to apply in the following setting: (1) fully-connected networks, (2) gradient descent with vanishing learning rate (gradient flow), (3) binary classification with MSE loss. The main disadvantage of our bound is that it"}, {"title": "2 Comparison to previous work", "content": "Our bounds has the following advantages over some other non-vacuous bounds available in the literature (e.g. [1, 3, 2, 13]):\n1. They are a-priori bounds, i.e. getting the actual trained the model is not required for evaluating them. To the best of our knowledge, they are the first non-vacuous a-priori bounds available for neural nets. All works mentioned above, despite providing non-vacuous bounds, could be evaluated only on a trained network thus relying on the implicit bias phenomenon which is not well understood yet.\n2. The bound of our Theorem 4.2 does not require a held-out dataset to evaluate it, compared to [3] and coupled bounds of [1]. Indeed, if one had a held-out dataset, one could just use it directly to evaluate the trained model, thus questioning the practical utility of such bounds.\n3. It does not grow with network width. In contrast, PAC-Bayesian bounds, [1, 2, 13], might grow with width.\n4. Similarly to [1, 3], we bound the generalization gap of the original trained model, not its proxy. In contrast, [2] introduces a Gaussian noise to the learned weights, while [13] crucially relies on quantization and compression after training.\nFor a fair comparison, we also list the disadvantages our present bound has:\n1. The bound of our Theorem 4.2 becomes non-vacuous only when the following two conditions hold. First, a simple counting-based generalization bound for a linear model evaluated in the same setting should be non-vacuous. Such a bound is vacuous even for binary classification on the standard MNIST dataset, but becomes non-vacuous if we downsample the images.\n2. Second, the activation function has to be sufficiently close to being linear. To be specific, for a two-layered leaky ReLU neural net trained on MNIST downsampled to 7x7, one needs the negative slope to be not less than 0.99 (1 corresponds to a linear net, while ReLU corresponds to 0).\n3. One may hope for the bound to be non-vacuous only for a partially-trained network, while for a fully-trained network the bound diverges.\n4. Even in the most optimistic scenario, when we manage to tighten the terms of our bound as much as possible, our bound stays non-vacuous only during the early stage of training when the network has not started \"exploiting\" its nonlinearity yet (Section 8). However, the minimal negative ReLU slope for which the bound stays non-vacuous is much smaller, 0.6."}, {"title": "3 Related work", "content": "Non-vacuous generalization bounds. While first generalization bounds date back to [7], the bounds which are non-vacuous for realistically large neural nets appeared quite recently. Specifically, [2] constructed a PAC-Bayesian bound which was non-vacuous for small fully-connected networks trained on MNIST and FashionMNIST. The bound of [13], also of PAC-Bayesian nature, relies on quantization and compression after training. It ends up being non-vacuous for VGG-like nets trained on large datasets of ImageNet scale. The bound of [1] is the first non-vacuous bound that applies directly to the learned model and not to its proxy. It is not obvious whether their construction could be generalized to neural nets with more than two layers. The bound of [3] is not PAC-Bayesian in contrast to the previous three. It is based on the notion of effective depth: it assumes that a properly trained network has small effective depth, even if it is deep. Therefore its effective capacity is smaller than its total capacity, which allows for a non-vacuous bound. See the previous section for discussion of some of the features of these bounds.\nLinear networks training dynamics. The pioneering work that integrates the training dynamics of a linear network under gradient flow to optimize"}, {"title": "4 Main result", "content": "Notations. For integer $L\\geq 0$, $[L]$ denotes the set ${1,..., L}$. For integers $l < L$, $[l, L]$ denotes the set ${1, ..., L}$. For a vector $x$, $||x||$ denotes its Euclidean norm. For a matrix $X$, $||X||$ denotes its maximal singular value, while $||X||_F$ denotes its Frobenius norm.\n4.1 Setup\nModel. The model we study is a fully-connected LeakyReLU network with $L$ layers:\n$f_{\\Theta}(x) = W_L x_{L-1}(x), \\quad x_{0}(x) = x,$   (1)\n$x_l(x) = \\varphi_\\epsilon (W_l x_{l-1}(x)) \\quad \\forall l \\in [L - 1],$   (2)\nwhere $\\Theta \\in \\mathbb{R}^N$ denotes the vector of all weights, i.e. $\\Theta = cat({vec(W_l)}{l=1}^L)$, $W_l \\in \\mathbb{R}^{n_l \\times n_{l-1}} \\quad \\forall l \\in [L]$, and $\\varphi_\\epsilon$ is a Leaky ReLU with a negative slope $1 - \\epsilon$, that is:\n$\\varphi(x) = x - \\epsilon \\min(0, x).$   (3)\nSince we are going to consider only binary classification in the present work, we take $n_L = 1$. We also define $d = n_0$ to denote the input dimension.\nData. Data points $(x, y)$ come from a distribution $D$. We assume all $x$ from $D$ to lie on a unit ball, $||x|| \\leq 1$, and all $y$ to equal $\\pm 1$. During the training phase, we sample a set of $m$ points iid from $D$ to form a dataset $(X, Y)$, where $X \\in \\mathbb{R}^{d\\times m}$ and $Y \\in \\mathbb{R}^{1\\times m}$.\nTraining. Assuming $rk\\, X = d$ (which implies $m > d$, i.e. the data is abundant), we train our model on $(X, Y)$ with gradient flow to optimize square loss on whitened data, i.e. on $(\\tilde{X}, Y)$ for $\\tilde{X} = \\Sigma_X^{-1/2}X$, where $\\Sigma_X = \\frac{1}{m}XX^T \\in \\mathbb{R}^{d\\times d}$ is an empirical feature correlation matrix. That is,\n$\\frac{dW_l(t)}{dt} = \\frac{1}{m} (Y - f_{\\Theta(t)}(X)) (\\frac{\\partial f_{\\Theta}(t)}{\\partial W_l} (\\tilde{X}))^T \\quad \\forall l \\in [L].$   (4)\nNote that $\\tilde{X}\\tilde{X}^T = m I_d$.\nInference. To conform with the above training procedure, we take the model output at a point $x$ to be $f_{\\Theta}(t) (\\Sigma^{-1/2}x)$, where $\\Sigma$ is a (distribution) feature correlation matrix: $\\Sigma = E_{(x,y)\\sim D}(xx^T) \\in \\mathbb{R}^{d\\times d}$. We assume this matrix to be known; in practice, we could substitute it with $\\Sigma_X$.\nPerformance measure. We take the risk function to be misclassification loss: $r(z, y) = \\text{Ind}[zy < 0]$, where $z, y \\in \\mathbb{R}$ and $\\text{Ind}[.]$ is an indicator of a condition. The empirical (train) risk on the dataset $(X, Y)$ and the distribution risk of the model trained for time $t$ are then defined as\n$\\hat{R}(t) = \\frac{1}{m} \\sum_{k=1}^m r (f_{\\Theta(t)} (\\Sigma_X^{-1/2}x_k), Y_k),$   (5)\n$R(t) = E_{(x,y)\\sim D} r (f_{\\Theta(t)} (\\Sigma^{-1/2}x), y).$   (6)\nFor a given model $f$, we also define $R(f)$ and $\\hat{R}(f)$ accordingly.\nIn addition, for $\\gamma > 0$, we define $\\gamma$-margin loss: $r_\\gamma(z, y) = \\text{Ind}[z \\text{sgn} y < \\gamma]$, and its continuous version:\nr_{\\gamma}(z,y) = \\begin{cases}\n1, & zy < 0, \\\\\n1-zy/\\gamma, & zy \\in [0, \\gamma], \\\\\n0, & zy > \\gamma,\\end{cases}$   (7)\nWe define $R^\\gamma$ and $\\hat{R}^\\gamma$ analogously to $R$, and $R^\\gamma$ and $\\hat{R}^\\gamma$ analogously to $\\hat{R}$.\n4.2 Generalization bound\nWe will need the following assumption on the training process:"}, {"title": "4.3 Choosing the training time", "content": "As we see, $\\Delta_{\\kappa}(t)$ diverges super-exponentially as $t \\to \\infty$ for $L = 2$ and as $t\\to \\frac{1}{\\Gamma(L-2)}$ for $L \\geq 3$, so the bound eventually becomes vacuous. For the bound to make sense, we should be able to find $t$ small enough for $\\Delta_{\\kappa}(t)$ to stay not too large, and at the same time, large enough for the training risk $\\hat{R}(t)$ to get considerably reduced.\nIn the present section, we are going to demonstrate that for values of $t$ which correspond to partially learning the dataset (i.e. for which $\\hat{R} (t) \\in (0,1)$), the last term of the bound, $\\Delta_{\\kappa,\\beta}(t)/\\gamma$, admits a finite limit as $\\beta \\to 0$ (the saddle-to-saddle regime of [4]).\nWe do not know how $\\hat{R}(t)$ decreases with $t$ in our case. However, when the network is linear, this can be computed explicitly when either the weight initialization is properly aligned with the data, or the initialization norm $\\beta$ vanishes. We are going to perform our whole analysis for $L = 2$ in the main, and defer the case $L \\geq 3$ to Appendix B.2."}, {"title": "5 Experiments", "content": "Setup. We consider an $L$-layer bias-free fully-connected network of width 64 and train it to classify 0-4 versus 5-9 digits of MNIST (i.e. $m = 60000$). In order to approximate the gradient flow dynamics, we run gradient descent with learning rate 0.001. By default, we take $L = 2$, the floating point precision to be $p = 32,"}, {"title": "6 Proof of the main result", "content": "6.1 Proof of Theorem 4.2 for \u03ba = 1\nWe start with approximating our learned network $f_{\\Theta}$ with $f_{\\Theta_0}$, i.e. with a nonlinear network that uses the weights learned by the linear one. This approximation deviates from the original network the following way: $\\forall x \\in \\mathbb{R}^d, \\epsilon \\in [0, 1]$,\n$\\left\\| f_{\\Theta\\epsilon} (x) - f_{\\Theta_0} (x) \\right\\| = \\left\\| \\int_0^\\epsilon \\frac{\\partial f_{\\Theta_\\tau}(x)}{\\partial \\tau} d\\tau \\right\\| \\\\\n< \\sup_{\\tau \\in [0,\\epsilon]} \\left\\| \\frac{\\partial f_{\\Theta_\\tau}(x)}{\\partial \\tau} \\right\\| = \\int_0^1 \\sum_{l=1}^L \\left\\| \\frac{\\partial f_{\\Theta_\\tau}(x)}{\\partial \\tau} \\right\\| d\\tau \\\\\n\\leq \\sum_{l=1}^L \\sup_{\\tau \\in [0,\\epsilon]} \\left\\| \\frac{\\partial f_{\\Theta_\\tau}(x)}{\\partial \\tau} \\right\\| < \\sum_{l=1}^L \\sum_{k \\neq l} \\sup_{\\tau \\in [0,\\epsilon]} \\left\\| \\frac{\\partial W_l}{\\partial \\tau} \\right\\|,$   (23)\nsince we use Leaky ReLUs. We omitted the argument $t$ for brevity. We get a similar deviation bound on the training dataset:\n$\\frac{1}{\\epsilon} \\left\\| f_{\\Theta\\epsilon} (\\tilde{X}) - f_{\\Theta_0} (\\tilde{X}) \\right\\| \\leq \\sup_{\\tau \\in [0,\\epsilon]} \\left\\| \\frac{\\partial f_{\\Theta\\tau}(\\tilde{X})}{\\partial \\tau} \\right\\| + \\sup_{\\tau \\in [0,\\epsilon]} \\left\\| \\sum_{k=2}^L \\frac{\\partial W_k}{\\partial \\tau} \\right\\| \\cdot \\sum_{k=2}^L \\sup_{\\tau \\in [0,\\epsilon]} \\left\\| \\prod_{k=2}^L W_l \\right\\| \\left\\|\\frac{\\partial W_k}{\\partial \\tau} \\right\\|, k \\in [2:L] \\backslash {1} (24)\nWe complete the above deviation bound with bounding weight norms and norms of weight derivatives:\nLemma 6.1. Under Assumption 4.1, $\\forall \\tau \\in [0, 1], t > 0 \\quad ||W_1(t)\\tilde{X}||_F \\leq \\rho u(t), \\quad m || \\frac{\\partial W_l(t)}{\\partial W_2} \\tilde{X}||_F \\leq v(t)$, and $\\forall l \\in [L] \\quad ||W_l(t)|| \\leq u(t), \\quad || \\frac{\\partial W_l(t)}{\\partial \\tau} || \\leq v(t)$ for $u, v$ defined in Theorem 4.2\nSee Section 6.3 and Appendix B.1 for the proof. Now we can relate the risk of the original model with the risk of the approximation. Since $r \\leq r^\\gamma < r_{\\epsilon}$ and $r_{\\epsilon}$ is $1/\\gamma$-Lipschitz,\n$R(f_{\\Theta\\epsilon}) - \\hat{R}_\\gamma (f_{\\Theta\\epsilon}) \\\\\n< R(f_{\\Theta\\epsilon}) - \\hat{R}^\\gamma (f_{\\Theta\\epsilon}) < R^\\gamma (f_{\\Theta_0}) - \\hat{R}^\\gamma (f_{\\Theta_0}) + \\frac{1}{\\gamma} E \\left\\| f_{\\Theta} (x) - f_{\\Theta_0}(x) \\right\\| + \\frac{1}{\\gamma m} \\left\\| f_{\\Theta} (\\tilde{X}) - f_{\\Theta_0} (\\tilde{X}) \\right\\|_1$   (25)\nwhere the expectation is over the data distribution $D$, and $\\tilde{x} = \\Sigma^{-1/2}x$ is the actual input of the network.\nAs for the last term, the deviation on the train dataset, we use that $||z||_1 \\leq \\sqrt{m}||z||$ for any $z \\in \\mathbb{R}^m$, and Equation (24). As for the deviation on the test dataset, due to Equation (23) and Lemma 6.1, in order to bound the last term, it suffices to bound $E \\left\\| \\tilde{x} \\right\\|$. Since $\\Sigma = E [xx^\\top]$, we get\n$E\\left\\|\\Sigma^{-1/2}x\\right\\|^2 = E [x^\\top \\Sigma^{-1}x]\\\\$\n= E \\text{tr} [xx^\\top \\Sigma^{-1}] = \\text{tr}[I_d] = d.$   (26)\nThis gives $E \\left\\|\\Sigma^{-1/2}x\\right\\| \\leq \\sqrt{E\\left\\|\\Sigma^{-1/2}x\\right\\|^2} < \\sqrt{d}$. The first two terms is a generalization gap of the proxy-model. We use a simple counting-based bound:\n$|\\hat{R}^\\gamma (f_{\\Theta_0(t)}) - R^\\gamma (f_{\\Theta_0(t)})| \\leq \\sqrt{\\frac{\\log |F_1(t; \\Theta(0))| - \\log \\delta}{2m}},$   (27)\nw.p. $\\geq 1 - \\delta$ over sampling the dataset $(X, Y)$, where $F_1(t; \\Theta(0))$ denotes the set of functions representable with $f_{\\Theta_0(t)}$ for a given initial weights $\\Theta(0)$, where $\\Theta_0(t)$ is a result of running the gradient flow Equation (4) for time $t$. As long as we work with finite precision, this class is finite:"}, {"title": "6.2 Proof of Theorem 4.2 for \u03ba = 2", "content": "What changes for $\\kappa = 2$ is a proxy-model. Consider the following:\n$F_{\\Theta_0,\\Theta_\\epsilon} (x) = f_{\\Theta_0} (x) + (f_{\\Theta\\epsilon} (\\tilde{X}) - f_{\\Theta_0}(\\tilde{X})) \\tilde{X}^+x.$   (28)\nThat is, we take the same proxy-model as before, but we add a linear correction term. This correction term aims to fit the proxy model to the original one, $f_{\\Theta\\epsilon}$, on the training dataset $\\tilde{X}$. We prove the following lemma in Appendix A.1:\nLemma 6.3. Under the premise of Lemma 6.1, $\\forall t \\geq 0 \\quad \\forall \\epsilon \\in [0, 1] \\quad \\forall x \\in \\mathbb{R}^d$ we have\n$\\left\\| f_{\\Theta\\epsilon (t)} (x) - F_{\\Theta_0 (t),\\Theta_\\epsilon (t)} (x) \\right\\| \\\\\n< u^{L-1}(t)v(t)||x||\\epsilon^2; (29)\n(L \u2212 1)(L + 1 + \\rho(L \u2212 1))\n$\\left\\| f_{\\Theta\\epsilon (t)} (\\tilde{X}) - F_{\\Theta_0 (t),\\Theta_\\epsilon (t)} (\\tilde{X}) \\right\\| \\\\\n< \\frac{u^{L-1}(t)v(t)\\sqrt{m}\\epsilon^2}{2(L \u2212 1)(1 + \\rho(L \u2013 1))}.$   (30)\nThe deviation now scales as $\\epsilon^2$ instead of $\\epsilon$.\nWhat remains is to bound the size of $F_2(t; \\Theta(0))$, which denotes the set of functions representable with our new proxy-model $F_{\\Theta_0(t),\\Theta_\\epsilon(t)}$ for given initial weights $\\Theta(0)$. Since $F_{\\Theta_0(t),\\Theta_\\epsilon(t)}$ is a sum of $f_{\\Theta_0(t)}$ and a linear model, its size is at most 2$^{\\text{nd}}$ times larger:\n$|F_{\\Theta(t};\\Theta(0))| \\leq |F_{\\Theta(t;\\Theta(0))}|^2^{pd} < 2^{2pd}.$   (31)\nThis finalizes the proof of Theorem 4.2 for $\\kappa = 2."}, {"title": "6.3 Proof of Lemma 6.1", "content": "We are going to present the proof for $L = 2$ in the present section, and defer the case of $L \\geq 3$ to Appendix B.1.\n6.3.1 Weight norms\nLet us expand the weight evolution Equation (4):\n$\\frac{dW_1}{dt} = [D^\\epsilon \\copyright W_2] \\tilde{X}Y^T,$   (32)\n$\\frac{dW_2}{dt} = [D^\\epsilon \\copyright W_1\\tilde{X}] Y^T,$   (33)\nwhere we define\n$\\frac{1}{m} \\sum_i (Y - W_2 [D^\\epsilon \\copyright W_1\\tilde{X}])\\, \\frac{\\partial}{\\partial W_i}(Y - W_2 [D^\\epsilon \\copyright W_1\\tilde{X}]),$   (34)\nand $D^\\epsilon = (\\varphi_\\epsilon)' (W_1\\tilde{X})$ is a $n \\times m$ matrix with entries equal to 1 or $1-\\tau$. We express it as $D^\\epsilon = (1-\\tau)I_{n\\times m} + \\tau\\Delta$, where $\\Delta$ is a $n \\times m$ 0-1 matrix.\nLet us bound the evolution of weight norms:\n$\\frac{d ||W_1||}{dt} \\leq \\left\\| \\frac{dW_1}{dt} \\right\\| \\leq (1 - \\tau) ||D^\\epsilon|| \\left\\| W_2 \\right\\| \\left\\| \\tilde{X}Y^T \\right\\| + \\tau \\left\\| \\Delta \\copyright W_2 \\right\\| \\left\\| \\tilde{X}Y^T \\right\\|.$   (35)\nSince multiplying by a 0-1 matrix elementwise does not increase Frobenius norm, we get\n$\\left\\| \\Delta \\copyright W_2 \\right\\|_F \\leq \\left\\| \\Delta \\copyright W_2 \\right\\|_F \\leq \\left\\| W_2 \\right\\|_F \\left\\| \\Delta \\right\\|_F.$   (36)\nNoting that $\\Delta$ and $\\tilde{X}Y^T$ are row matrices, this results in\n$\\frac{d ||W_1||}{dt} \\leq ((1 - \\tau)||\\tilde{X}Y^T ||_F + \\tau ||\\Delta^\\epsilon || ||\\tilde{X}Y^T ||) ||W_2||.$   (37)\nBy a similar reasoning,\n$\\frac{d ||W_2||}{dt} \\leq (1 - \\tau)||D^\\epsilon \\tilde{X}Y^T ||_F ||W_1|| + \\tau ||\\Delta^\\epsilon \\tilde{X}Y^T || \\left\\| W_1\\tilde{X}\\right\\|_F.$   (38)\nBounding norms, option one. A this point, we could bound $||W_1\\tilde{X}||_F < ||W_1|| \\left\\| \\tilde{X} \\right\\|_F$. We then make use of the following lemma which we prove in Appendix A.2:\nLemma 6.4. $\\left\\| \\Delta_\\epsilon \\right\\| = \\left\\| \\Delta^\\epsilon \\tilde{X}Y^T \\right\\|_F \\leq (1 + \\rho\\beta^L)$. If we additionally take Assumption 4.1 then $\\left\\| \\Delta^\\epsilon \\tilde{X}Y^T \\right\\| = \\left\\| \\Delta^\\epsilon \\tilde{X}Y^T \\right\\|_F < s + \\rho\\beta^L$.\nSince $\\tilde{X}\\tilde{X}^T = m I_d$, we have $||\\tilde{X}|| = \\sqrt{m}$ and $||\\tilde{X}||_F = \\sqrt{md}$. From the above lemma and since $\\tau < \\epsilon$, $W_{1,2}(t) \\leq u(t)$, where $u$ satisfies\n$\\frac{du(t)}{dt} = sau(t), \\quad u(0) = \\beta.$   (39)\nIts solution is given by $u(t) = \\beta e^{sat}."}, {"title": "7 How far could we get with our approach?", "content": "Broadly speaking, the approach we apply in this work could be described as follows. Suppose we have a model $f$ learned on a dataset $(X, Y)$, and another \"proxy\" model $g$ which we construct having an access to the same dataset $(X, Y)$. We bound the distribution risk of the \"original\" model using the following relation:\n$R(f) - \\hat{R}(f) < \\hat{R}(f) < R(g) - \\hat{R}(g) + \\frac{1}{E_{(x,y)\\sim D}}|g(x) - f(x)| + \\frac{1}{||g(\\tilde{X}) - f(\\tilde{X})||_1$   (49)\nIn words, we say that performance of $f$ is worse than that of $g$ at most by some deviation term.\nThe bound ends up to be good whenever (a) the generalization gap of $g$ could be well-bounded, and (b) $g$ does not deviate from $f$ much. That is why we considered proxy-models based on linear learned weights: their generalization gap could be easily bounded analytically, and they do not deviate much from corresponding leaky ReLU nets as long as ReLU negative slopes are close to one.\nThe biggest conceptual disadvantage of this approach is that, given both $f$ and $g$ learn the training dataset, we have no chance proving that $f$ performs better than $g$, we could only prove that $f$ performs not much worse than $g$. Do the proxy-models we use in the present paper perform well, and how much do they deviate from original models? Our main theoretical result, Theorem 4.2, bounds the proxy-model generalization gap and the deviation from above. These bounds are arguably not optimal. It is therefore instructive to examine how well the bound would perform, if we could estimate Equation (49) exactly.\n8.1 Empirical validation\nSetup. We work under the same setup as in Section 5, but instead of evaluating the bound of Theorem 4.2, we actually train a linear model with exactly the same procedure as for the original model, in order to get trained linear weights $\\Theta_0$. We then evaluate the proxy-models considered in the present work: (1) the one for $\\kappa = 1$, $f_{\\Theta_0}$, (2) the one for $\\kappa = 2$, see Equation (28), and also (3) the linear network, $f_{\\Theta_0}$. We then evaluate the rhs of Equation (49) using a test part of the MNIST dataset. For this \"optimistic\" bound, we"}, {"title": "7.1 Assumptions", "content": "Whitened data. Overall, the assumption on whitened data is not necessary for a result similar to Theorem 4.2 to hold. We assume the data to be whitened for two reasons. First, it legitimates the choice of training time $t_{\\lambda}(\\beta)$ since it is based on the analysis of [9], which assumed the data to be whitened. If we dropped it, we could still evaluate the bound of Theorem 4.2 at $t = t_{\\lambda}(\\beta)$, but it would be less clear whether the training risk $\\hat{R}_\\gamma$ becomes already small by this time.\nSecond, we had to bound $||\\tilde{X}||$ throughout the proof of Theorem 4.2 and $||\\tilde{X}^+||$ in the proof of Lemma 6.3. For whitened data, these are simply $\\sqrt{m}$ and $\\frac{1}{\\sqrt{m}}$, which is a clear dependence on $m$, making the final bound look cleaner. Otherwise, they would be random variables whose dependence on $m$ would be not obvious.\nThird, if we considered training on the original dataset $(Y, X)$ instead of the whitened one, $(Y, \\tilde{X})$, we would have to know $Y \\tilde{X}^T$ and $\\tilde{X} \\tilde{X}^T$ in order to determine $\\Theta(t)$ for a given $t$ and initialization $\\Theta(0)$. These two matrices have $\\frac{d + d(d+1)}{2}$ parameters, compared to just d for YXT. This way, $\\Upsilon_{\\kappa}$ would grow as $d$ instead of $\\sqrt{d}$.\nGradient flow. We expect our technique to follow through smoothly for finite-step gradient descent. Introducing momentum also seems doable. However, generalizing it to other training procedures, e.g. the ones which use normalized gradients, might pose problems since it is not clear how to reasonably upper-bound the norm of the elementwise ratio of two matrices.\nAssumption 4.1. We use this assumption to prove the second part of Lemma 6.4. If we dropped it, the bound would be $\\left\\| \\Delta^\\epsilon \\tilde{X}Y^T \\right\\|_F \\leq \\left\\| \\Delta^\\epsilon \\right\\|_F \\left\\| \\tilde{X}Y^T \\right\\| \\leq 1 + \\rho\\beta^L$ instead of $s + \\rho\\beta^L$. This would result in larger exponents for the definition $u$ in Theorem 4.2."}, {"title": "7.2 Proof", "content": "We expect the bounds on weight norms $u(t)$ to be quite loose since we use Lemma 6.4 to bound the loss. This lemma bounds the loss with its value at the initial- ization", "holds": "n1. $f_{\\Theta"}, "x)$ is linear in $x$ for any $\\Theta$;\n2. $\\frac{\\partial^2```json\n{\n        \"content\":", "f_{\\Theta}(x)}{\\partial \\Theta \\partial \\Theta}$ is continuous as a function of $(\\epsilon, \\Theta, x)$;\n3. the result of learning $\\Theta(t)$ is differentiable in $\\epsilon$ for any $t$.\nThis is directly applicable to convolutional networks with no other nonlinearities except for ReLU's; in particular, without max-pooling layers. One may introduce max-poolings by interpolating between average- poolings (which are linear) for $\\epsilon = 0$ and max-poolings for $\\epsilon = 1$. This is not applicable to Transformers [10] since attention layers are inherently nonlinear: queries and keys have to be multiplied.\nCompared to the fully-connected case of the present work, our bound might become even tighter for convolutional nets since $d$ becomes the number of color channels (up to 3) instead of the whole image size in pixels. However, the corresponding proxy-models might be over-simplistic: the linear net they will deviate from is just a global average-pooling followed by a linear $\\mathbb{R}^d \\to \\mathbb{R}$ map. We leave exploring the convolutional net case for future work."], "content": "Broadly speaking, the approach we apply in this work could be described as follows. Suppose we have a model $f$ learned on a dataset $(X, Y)$, and another \"proxy\" model $g$ which we construct having an access to the same dataset $(X, Y)$. We bound the distribution risk of the \"original\" model using the following relation:\n$R(f) - \\hat{R}(f) < \\hat{R}(f) < R(g) - \\hat{R}(g) + \\frac{1}{E_{(x,y)\\sim D}}|g(x) - f(x)| + \\frac{1}{||g(\\tilde{X}) - f(\\tilde{X})||_1$   (49)\nIn words, we say that performance of $f$ is worse than that of $g$ at most by some deviation term.\nThe bound ends up to be good whenever (a) the generalization gap of $g$ could be well-bounded, and (b) $g$ does not deviate from $f$ much. That is why we considered proxy-models based on linear learned weights: their generalization gap could be easily bounded analytically, and they do not deviate much from corresponding leaky ReLU nets as long as ReLU negative slopes are close to one.\nThe biggest conceptual disadvantage of this approach is that, given both $f$ and $g$ learn the training dataset, we have no chance proving that $f$ performs better than $g$, we could only prove that $f$ performs not much worse than $g$. Do the proxy-models we use in the present paper perform well, and how much do they deviate from original models? Our main theoretical result, Theorem 4.2, bounds the proxy-model generalization gap and the deviation from above. These bounds are arguably not optimal. It is therefore instructive to examine how well the bound would perform, if we could estimate Equation (49) exactly.\n8.1 Empirical validation\nSetup. We work under the same setup as in Section 5, but instead of evaluating the bound of Theorem 4.2, we actually train a linear model with exactly the same procedure as for the original model, in order to get trained linear weights $\\Theta_0$. We then evaluate the proxy-models considered in the present work: (1) the one for $\\kappa = 1$, $f_{\\Theta_0}$, (2) the one for $\\kappa = 2$, see Equation (28), and also (3) the linear network, $f_{\\Theta_0}$. We then evaluate the rhs of Equation (49) using a test part of the MNIST dataset. For this \"optimistic\" bound, we"}, {"title": "7.1 Assumptions", "content": "Whitened data. Overall, the assumption on whitened data is not necessary for a result similar to Theorem 4.2 to hold. We assume the data to be whitened for two reasons. First, it legitimates the choice of training time $t_{\\lambda}(\\beta)$ since it is based on the analysis of [9], which assumed the data to be whitened. If we dropped it, we could still evaluate the bound of Theorem 4.2 at $t = t_{\\lambda}(\\beta)$, but it would be less clear whether the training risk $\\hat{R}_\\gamma$ becomes already small by this time.\nSecond, we had to bound $||\\tilde{X}||$ throughout the proof of Theorem 4.2 and $||\\tilde{X}^+||$ in the proof of Lemma 6.3. For whitened data, these are simply $\\sqrt{m}$ and $\\frac{1}{\\sqrt{m}}$, which is a clear dependence on $m$, making the final bound look cleaner. Otherwise, they would be random variables whose dependence on $m$ would be not obvious.\nThird, if we considered training on the original dataset $(Y, X)$ instead of the whitened one, $(Y, \\tilde{X})$, we would have to know $Y \\tilde{X}^T$ and $\\tilde{X} \\tilde{X}^T$ in order to determine $\\Theta(t)$ for a given $t$ and initialization $\\Theta(0)$. These two matrices have $\\frac{d + d(d+1)}{2}$ parameters, compared to just d for YXT. This way, $\\Upsilon_{\\kappa}$ would grow as $d$ instead of $\\sqrt{d}$.\nGradient flow. We expect our technique to follow through smoothly for finite-step gradient descent. Introducing momentum also seems doable. However, generalizing it to other training procedures, e.g. the ones which use normalized gradients, might pose problems since it is not clear how to reasonably upper-bound the norm of the elementwise ratio of two matrices.\nAssumption 4.1. We use this assumption to prove the second part of Lemma 6.4. If we dropped it, the bound would be $\\left\\| \\Delta^\\epsilon \\tilde{X}Y^T \\right\\|_F \\leq \\left\\| \\Delta^\\epsilon \\right\\|_F \\left\\| \\tilde{X}Y^T \\right\\| \\leq 1 + \\rho\\beta^L$ instead of $s + \\rho\\beta^L$. This would result in larger exponents for the definition $u$ in Theorem 4.2."}]