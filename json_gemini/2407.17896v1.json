{"title": "3D Hole Filling using Deep Learning Inpainting", "authors": ["Marina Hern\u00e1ndez-Bautista", "F.J. Melero"], "abstract": "The current work presents a novel methodology for completing 3D surfaces produced from 3D digitization technologies in places where there is a scarcity of meaningful geometric data. Incomplete or missing data in these three-dimensional (3D) models can lead to erroneous or flawed renderings, limiting their usefulness in a variety of applications such as visualization, geometric computation, and 3D printing. Conventional surface estimation approaches often produce implausible results, especially when dealing with complex surfaces. To address this issue, we propose a technique that incorporates neural network-based 2D in-painting to effectively reconstruct 3D surfaces. Our customized neural networks were trained on a dataset containing over 1 million curvature images. These images show the curvature of vertices as planar representations in 2D. Furthermore, we used a coarse-to-fine surface deformation technique to improve the accuracy of the reconstructed pictures and assure surface adaptability. This strategy enables the system to learn and generalize patterns from input data, resulting in the development of precise and comprehensive three-dimensional surfaces. Our methodology excels in the shape completion process, effectively filling complex holes in three-dimensional surfaces with a remarkable level of realism and precision.", "sections": [{"title": "1 Introduction", "content": "In recent years, there has been a growing prominence of 3D digitization, which has had a significant impact on various disciplines. This technology enables the creation of highly detailed digital replicas of physical objects. Digital models acquired through the use of scanners or photogrammetry are extensively utilized in various fields such as architectural modeling, cultural heritage preservation, reverse engineering, medical simulations, product design, and even in the facilitation of asset creation for the cinema and video game industries. Nevertheless, the efficacy of these 3D digital models is contingent upon the quality of their mesh. The quality in question is not solely associated with precision and resolution, but also encompasses the property of watertightness. An incomplete surface can result in visual artifacts in the final render, hinder the ability to perform geometric computations on the 3D model, or even cause failures in 3D printing.\nIn parallel to the acquisition methods, the computer graphics community has developed algorithms for model repair. In the specific context of addressing voids on a surface, scholars have employed two primary methodologies: volumetric and surface-oriented algorithms. Volumetric algorithms execute a comprehensive remeshing process on the surface by converting an intermediate point cloud. They ensure the creation of models that are free from gaps or leaks. However, these algorithms often result in a reduction of the surface resolution and a loss of intricate features that were initially captured during the digitization process [6]. In contrast, surface-oriented algorithms operate at a localized level, making modifications to the model solely in the regions directly associated with the hole. Traditional surface-oriented hole-filling algorithms [42, 74] fail to consider the global characteristics of the model or the resemblance to the adjacent surface of the hole. Therefore, a distinct category of algorithms, known as context-based algorithms, can be found in the existing literature. These algorithms aim to replicate the characteristics of the surrounding surface in the vicinity of the area that is missing [29, 71, 17]. In recent years, the increasing prominence of deep learning methods has integrated generative models to accomplish this objective. These models primarily utilize the point cloud representation, thereby disregarding the surface's topological information [32, 72, 68, 76].\nUndoubtedly, deep learning methods have demonstrated remarkable performance in point cloud completion tasks[69, 47, 64], as well as in 2D image completion tasks [84]. The present work introduces a novel approach for filling holes in 3D surface models by leveraging the concept of 2D inpainting and the adaptability of deep learning.\nThe utilization of a point cloud representation of the object is not relied upon, as a novel patch of polygons is generated to enclose the surface. The consideration of the topology and original shape of the model is achieved through the utilization of a planar representation of the curvature at vertices, which serves as the 2D image that is processed by the neural network. The image containing the holes serves as the input, while the resulting image after applying the trained algorithm is referred to as the inpainted image. The method employed in our study integrates surface-oriented techniques with context-aware information and leverages the creative potential of deep learning. Recently, there has been a utilization of inpainting techniques to tackle the issue of hole-filling in disk-homeomorphic surfaces [51, 25]. These approaches have demonstrated promising outcomes by leveraging a substantial neighborhood surrounding the hole as contextual information for the reconstruction process. In this study, we present a novel approach that extends the existing methods for mesh completion. Our proposed method aims to address the mesh completion task by incorporating information not only from the damaged mesh itself, but also from other polygonal models that share similarities with the one requiring repair.\nOur primary goal has been to improve the precision of the reconstructed surface and to fully automate the restoration process, thereby surpassing the constraints of general-purpose inpainting neural networks, as those used in similar works [33]. In order to accomplish these objectives, we have trained the inpainting networks with over 1 million curvature images from a large 3D model dataset. The use of ad-hoc trained neural networks improves the accuracy of the deep learning inpainting algorithm, as they are specialized in a well-defined type of images with the same color range. Then, a coarse-to-fine surface deformation algorithm is implemented to conform the newly generated surface patch to the anticipated outcome. Additionally, we have enhanced this autonomous virtual sculpting algorithm to generate surfaces that are more detailed and precise. Our method is noteworthy for its ability to optimize the repair of 3D models with a substantial number of polygons and intricate shapes, as exemplified in Fig. 1, transcending the limitations of conventional methods that typically address simpler models.\nThe remainder of the paper is organized as follows. The purpose of Section 2 is to contextualize our work and provide the most recent and pertinent literature that motivates us in our research. Subsequently, Section 3 illustrates the dataset generation, network training, and surface reconstruction processes. We conduct a quantitative and qualitative comparison of our reconstructed surfaces with some of the most advanced algorithms in Section 4. Finally, in Section 5, we discuss the current limitations and challenges that will inspire our future research, as well as illustrate a few interesting implications of this approach."}, {"title": "2 Related Work", "content": "The development of robust restoration algorithms within the computer graphics community has been closely correlated with the evolution of 3D acquisition methods. The presence of holes or absent data on the acquired surface is one of the most frequently encountered issues. In the past, researchers have investigated a context-aware combination of both volumetric and surface-oriented algorithms. A new category of methods that utilize generative models for hole-filling has been introduced as a result of the recent surge in deep learning techniques. These methods predominantly rely on point cloud representations of the surface [78, 22], which may result in the omission of valuable topological information. Although these methods demonstrate potential, ongoing research is working to incorporate both topological and geometric information to facilitate even more precise and detail-preserving repairs.\nThe utilization of 2D inpainting techniques has become an essential element in the process of restoring images within the realm of computer vision. The aim of these techniques is to reconstruct missing or damaged regions within an image by leveraging information from the neighboring intact areas. The field of CNN-based 2D inpainting has experienced significant advancements due to state-of-the-art techniques that utilize convolutional neural networks (CNN) [84, 79]. These methods have demonstrated exceptional results and have outperformed traditional patch-based approaches. The aforementioned methods possess the ability to not only complete areas with absent information, but also thoroughly examine the surrounding context of the image, reproducing textures and patterns, and facilitating the reconstruction of intricate regions with exceptional accuracy.\nAs previously stated, our objective is to achieve the completion of the 3D surfaces through the reconstruction of 2D curvature images derived from the parametrization of mesh patches. The restored image generated by the algorithm provides a prediction of the shape of the surface that is missing from the polygonal model. Therefore, this section will analyze the most relevant advancements in both domains: shape completion and 2D inpainting, which are the fundamental components of our proposal."}, {"title": "2.1 Shape Completion", "content": "A significant amount of research has been undertaken to tackle the problem of filling voids or imperfections in both basic and intricate three-dimensional objects. The classification of classical approaches can be broadly divided into three categories:\n\u2022 Surface-based approaches. The initial set of methodologies centers around utilizing the surface data of the mesh in order to fill in areas that are incomplete or missing. The estimation of the shape of missing areas is frequently accomplished through the utilization of local or global surface fitting techniques, as discussed in previous studies [42, 1, 9, 5, 7, 53, 23]. Nevertheless, these techniques may not be appropriate for larger or more intricate holes.\n\u2022 Volumetric approaches [17, 27, 49, 38] represent shapes by utilizing grids or voxel grids. The estimation of missing data is accomplished through the use of extrapolation or interpolation techniques applied within the volume. The surface is then represented using either dense voxel grids [16, 65] or octrees [57], which effectively address challenges associated with memory usage and computational complexity. The aforementioned methods have demonstrated notable efficacy in addressing intricate voids and have been employed in a multitude of research investigations.\n\u2022 Context-based approaches have been widely studied and implemented in various fields. The utilization of pre-existing information or patterns within the mesh is employed to complete the missing sections. The authors in [29, 67] utilize contextual relationships among various components of the shape in order to fill in the gaps in the regions that are missing. Alternatively, they also concentrate on utilizing local geometric patterns or features to deduce the missing parts. The task can be accomplished using either surface-based techniques [29] or volumetric techniques [63].\nAs artificial intelligence has advanced, new neural network-based hole-filling techniques have appeared. These strategies use machine learning to anticipate shape information that is lacking by using patterns that have been learned from complete data that already exists.\nThe most often used architectures are based on autoencoders, which are used to acquire various prior information and generate a latent space of forms that they map to their partial inputs [81, 12, 65, 48, 16]. Utilizing variational autoencoders (VAE) for training, the methods of [65, 2] have proven to be accurate when applied to 3D shapes from the ShapeNet [10] and KITTI [24] datasets.\nA coarse-to-fine approach is implemented in both [16] and [81] to facilitate shape completion. In the former, a 3D shape classification network is implemented to provide additional information. However, this approach may be limited in its adaptability and is contingent upon training data. Conversely, the latter abstracts a point cloud into a feature vector, resulting in a coarse-to-fine-grained output.\nGAN architectures [68, 85] have also been employed for 3D shape completion, in which a generator and a discriminator collaborate to acquire the features of a 3D model, similar to their application in image generation. These methods have demonstrated potential in the field of 3D shape completion. Reinforcement learning is also implemented [60]."}, {"title": "2.2 Image inpainting", "content": "Inpainting is the term commonly used to describe the process of completing what is missing in a 2D image. Various strategies have been employed to resolve the inpainting problem over the years, as the input images may differ in the context of the missing part or the size of the damages. Classical methods are either relying on interpolation or diffusion of the image properties [36, 3, 4] or relying on exemplar-based patch completion [14, 19, 41, 21]. Diffusion strategies have yielded favorable outcomes, particularly when applied to vast regions. Furthermore, hybrid methods [8] integrate elements of both diffusion and patch-based techniques to resolve intricate regions of the image.\nRecent research has concentrated on the utilization of neural networks to achieve superior outcomes, which has been facilitated by the emergence of deep learning. These methods are typically categorized based on their network architecture. One notable category is Generative Adversarial Network (GAN) architectures, which utilize both a generator and a discriminator network. The problem is converted into a binary classification task by this configuration, in which the discriminator endeavors to differentiate between real and generated outputs, and the generator endeavors to generate outputs that can deceive the discriminator. The integration of a variety of other structures within both the generator and discriminator networks is facilitated by the flexibility of GAN architectures, including Convolutional Neural Networks (CNNs) [84, 79, 50] and Transformers [37].\nPure CNN architectures comprise an additional category of methodologies. These methods frequently employ variations such as U-Net [59] and Fully Convolutional Networks (FCN) [44] for image inpainting tasks. FCNs employ convolutional layers as encoders to extract high-dimensional features and reduce noise, while deconvolutional layers function as decoders to produce the reconstructed image. Some methods, such as [54, 87, 61], are included in this category. In contrast to FCN, U-Net has a symmetrical architecture that includes an encoder (down-sampling) path and a decoder (up-sampling) path. Various authors have suggested various approaches that are based on U-Net, including learnable bidirectional attention maps [77], partial convolution [43], and many others [46, 82].\nGiven the abundance of methodologies available in the literature, we have prioritized those that generate cutting-edge outcomes. The subsequent methods are highly pertinent for our investigation due to their superior evaluation metrics on Places2 [90], an important image dataset.\nCRFill [84] is a two-stage method that initially employs dilated convolutions to expand receptive fields for coarse prediction. A refinement network with two encoders comprises the second stage. These encoders concentrate on the capture of fine details through a contextual attention layer and hallucinate new patches.\nThe Co-mod-GAN model [88] utilizes modulation techniques inspired by style transfer methods to improve its performance. The central concept underlying Co-mod-GAN is the incorporation of co-modulation, an innovative technique that greatly enhances the generator's capacity to generate a wide range of visually captivating outputs. The modulation process in Co-mod-GAN is effectively guided by incorporating both the input image features and the latent vector. The approach described strikes an optimal equilibrium between producing a wide range of outputs and maintaining high visual quality, making it especially valuable in addressing extensive image completion tasks.\nAnother advanced inpainting technique is the Mask-Aware Transformer (MAT) [40]. This method employs a transformer architecture to effectively fill in large holes by incorporating non-local information into the attention component. The MAT system also includes a module for style manipulation, which offers a range of image completion suggestions.\nMask-Aware Dynamic Filtering (MADF) [91], in contrast, employs a dynamic approach to generate kernels for individual convolution windows by utilizing mask features. The system utilizes a recovery decoder and multiple refinement decoders within a coarse-to-fine encoder-decoder framework.\nIn recent years, there have been notable advancements in the field of 2D image inpainting, particularly with the emergence of general-purpose text-to-image platforms such as DALLE 2 [56] and Stable Diffusion [58], which have demonstrated impressive performance. The enhanced generalization capability of these models is achieved through the utilization of an extensive training dataset and a distinctive training process. This results in more robust and accurate inpainting outcomes, as illustrated in [33].\nThe DALLE 2 model [56] is composed of an encoder and a decoder. The text label is processed by a CLIP transformer [55] to produce a condensed vector representation known as a text embedding. The text embedding is subsequently employed to generate an image embedding using either an autoregressive or diffusive prior. Following this, a diffusion decoder is employed to produce the ultimate image.\nThe Latent Diffusion Models (also referred to as Stable Diffusion) serve as an open source alternative to DALL-E 2. These models, as described in [58], are conditional diffusion models that have been trained for various tasks. The particular inpainting diffusion model is grounded in the Lama architecture [66], a cutting-edge approach specifically developed for the purpose of inpainting images with extensive masks. The utilization of fast Fourier convolutions in Lama enables the incorporation of global context starting from the initial layers of the neural network. Additionally, a loss function with a high receptive field is employed to encourage the consistency of global shapes. Furthermore, the model incorporates a comprehensive mask"}, {"title": "2.3 Shape Completion using Inpainting Techniques", "content": "Significant progress has been achieved in the domain of shape completion in recent years, with a particular focus on the utilization of 2D inpainting techniques. The objective of these techniques is to restore or fill in absent or obscured portions of an object in an image by utilizing the capabilities of deep learning and neural networks to deduce and produce credible reconstructions. This subsection examines the most pertinent and advanced methodologies that closely correspond to our research, with a specific emphasis on those that employ 2D inpainting techniques to accomplish shape completion with a high level of accuracy.\nA limited number of solutions that employ a variety of various approaches have been the result of recent research on this novel methodology. Several recent methods have employed the self-prior concept in the context of 3D inpainting, where recoveries are made by directly performing inpainting over the 3D surface. This approach overcomes the disadvantages of data-driven methods by learning prior knowledge from a single damaged point on the surface. This context enables us to evaluate methods such as those proposed in [30, 31], which utilize Graph Convolutional Networks for mesh denoising and shape completion tasks, respectively. Point2Mesh [28] is another remarkable method that employs a self-prior to reconstruct the entire surface from an input point cloud without the need for pre-training. In addition, the Deep Geometric Prior method [73] reconstructs various local surface patches as a manifold atlas. A comprehensive reconstruction of the surface is generated by sampling this atlas.\nWithin this particular domain, there is a prevalent trend of employing NeRFs (Neural Radiance Fields) as a means of representing the three-dimensional geometry and radiance of a given scene through the utilization of neural networks. For example, [70] explores the application of a 2D diffusion model in the context of general scene completion tasks. The model is utilized to generate a coherent multi-view output for the completed 3D scene. In a similar vein, [52] employs a text-to-image diffusion model to facilitate the synthesis of 3D representations from textual input.\nA series of approaches that employ analogous techniques, such as geometrical procedures and 2D inpainting techniques, have been chosen to facilitate the most accurate comprehension of the problem at hand. Inpainting is employed to recover fabric filaments in [26]. Depth images are repaired by both [51, 35]. The former employs a classical inpaint approach and subsequently conducts the 2D to 3D transformation, while the latter employs partial depth images that are repaired by a conditional generative network and reproject computations of consistency distances. Lastly, [45] employs a comparable methodology to ours, integrating deep learning with geometric techniques to execute inpainting on the mesh texture."}, {"title": "3 Methodology", "content": "The challenge of restoring complex objects with multiple holes is tackled in this work through the utilization of a curvature-based approach. This approach entails the creation of a curvature feature image and the subsequent application of inpainting techniques to effectively fill in the missing components. The iterative process of deformations ensures the successful restoration of complex surfaces by inferring missing geometry over the mesh surface until alignment with the inpainted curvature image is achieved. The focus of our research is on automating and improving the methodology used to address the limitations of generic inpainting networks.\nThis procedure is further clarified upon in this section. At the initial phase, we generate a dataset of 3D models that are all of a specific dimension and manifoldness. The image dataset that will be employed in the deep learning training is built upon this curated collection of meshes. The entire process entails the conversion of the 3D mesh representations into 2D images, thereby guaranteeing that the resultant images accurately depict the geometrical details of the original surface. These 3D meshes will not only be used to produce 2D images but also as a benchmark for assessing the efficacy of our shape completion algorithm. Section 3.1 delineates all aspects of the procedure.\nFollowing that, we proceed to train a range of cutting-edge ad-hoc architectures for image inpainting utilizing our image dataset, as depicted in Section 3.2. The automatic algorithm that conducts virtual sculpting of the mesh surface will be guided by the resulting repaired images. This method, as illustrated in 3.3, offers a comprehensible process that results in an accurate solution to the 3D model shape reconstruction problem."}, {"title": "3.1 Creation of the training dataset", "content": "The process of creating the dataset involves multiple stages, resulting in the formation of two separate databases. Firstly, we provide an explanation in Section 3.1.1 regarding the specific criteria that were selected for the inclusion of 3D models in the dataset. These criteria ensure that the models are suitable for use as a training dataset for 2D inpainting tasks. After the selection of the 3D models, the creation of realistic holes on the surface is carried out according to the procedures described in Section 3.1.2. The initial collection of three-dimensional models, consisting of intact meshes as well as their corresponding damaged counterparts, will be utilized for the purpose of conducting tests and obtaining an unbiased assessment of the shape completion algorithm.\nAs outlined in 3.1.3, the complete models undergo a secondary procedure where their 3D surfaces are partitioned into patches and subsequently parametrized to yield multiple 2D surfaces per mesh. Subsequently, artificial holes are introduced into the images, and through the implementation of traditional augmentation techniques, a dataset consisting of over one million images is obtained. This dataset is then utilized for training the inpainting neural network."}, {"title": "3.1.1 Selection of 3D models", "content": "To ensure the success of the parametrization or reconstruction process, it is imperative that all meshes in the dataset possess the quality of being watertight, meaning they should not contain any holes. Additionally, they should be manifold, indicating that they are continuous and have no self-intersections. Furthermore, the meshes should consist of a single component with a substantial number of polygons. The aforementioned characteristics play a pivotal role, as the correct topology enables us to execute geometric operations on the mesh surface while repairing the affected areas, thereby ensuring the efficacy of our reconstruction algorithm.\nWe have conducted an analysis on a comprehensive set of seven distinct 3D model datasets that are frequently referenced in academic literature [10, 18, 13, 20, 39, 75, 15]. Our investigation has revealed a number of issues, which are detailed in Table 1. The majority of the datasets fail to meet our requirement for watertightness as a result of incorrect topology. The datasets in question exhibited erroneous topology or were specifically designed for mesh segmentation purposes, resulting in the presence of disconnected faces. One frequently encountered issue is the quantity of polygons per mesh. Meshes with a limited number of faces can result in inadequate levels of detail, as models with low resolution tend to generate color images with reduced variability in patterns. Consequently, this can negatively impact the training process and result in less accurate reconstructions. Conversely, an excessive quantity of polygons will lead to a dataset that requires significant resources for both training and evaluation. In order to ensure the quality of our dataset, we implemented a filtering process to specifically select 3D models that possessed a suitable number of polygons and exhibited correct topology.\nUpon thorough examination of the aforementioned datasets, we have successfully collated an extensive catalog consisting of 941 models out of the original 1032 models from the Google Dataset. The exclusion criteria for the remaining 91 models was based on their inability to be integrated into a single component without any modifications to the mesh topology. In order to augment our dataset with an additional set of natural figures, we integrated two models sourced from the Stanford Repository, as well as sixteen models that were digitized by the authors themselves in different projects. The final curated database consists of a collection of 959 3D manifold models that meet the essential criteria for being watertight and possessing optimal resolution. The distribution of the number of polygons across the dataset is presented in Fig. 2."}, {"title": "3.1.2 Creating holes in 3D models for testing", "content": "The benchmark models utilized for the shape completion algorithms necessitate the presence of one or more holes in the surface. Consequently, we were compelled to fabricate authentic holes within the meshes, as illustrated in the upper branch of Fig. 5.\nTo simulate perforations, a scripted procedure is employed for each model, whereby the mesh is initially divided into multiple evenly distributed patches. By employing this approach, we ensure that the placement of each subsequent hole center is maximally distant from the centers of the other holes. As a result, this sampling strategy enables us to target distinct regions of the surface and prevent any potential overlap between the holes.\nThe algorithm utilizes a farthest point sampling strategy on the surface mesh to choose five vertices in such a way that each vertex is maximally distant from the others, thereby guaranteeing a well-distributed set of starting points, as depicted in Fig. 3. The algorithm computes the squared Euclidean distances between each vertex and the remaining vertices, and then selects the vertices with the largest minimum distance to the vertices that have already been chosen.\nWhen five seed vertices are chosen, the selected vertices area is expanded iteratively, introducing unique perforations. This expansion continues until a random number of vertices (which is less than 10% of the total number of vertices in the mesh) are selected. In the end, the process involves the elimination of individual patches made up of chosen vertices and faces, resulting in the creation of a modified representation of the 3D model that exhibits damage.\nBy employing a systematic methodology, every model contained within this database exhibits unique and accurately simulated holes that are frequently encountered in real-life situations, as illustrated in the examples shown in Fig. 4. Currently, the database contains ground truth models that are complete and do not have any holes in them, as well as incomplete models that have organic deficiencies introduced through our perforation simulation method."}, {"title": "3.1.3 Curvature images for training", "content": "The ground truth models are utilized to create a dataset of images, which will subsequently be employed for training the neural network. The process is visually illustrated in the lower branch of Fig. 5.\nThe central concept involves the representation of the models' surfaces as a collection of two-dimensional images. This is achieved by sampling the mean curvature values and parametrizing distinct patches for each mesh. The concept of mean curvature is used to quantify the curvature of a surface, allowing for a deeper understanding of its geometric characteristics. The mean curvature at any given point on a surface can be defined as the arithmetic mean of its two principal curvatures, which correspond to the maximum and minimum curvatures in the principal directions passing through that particular point. After obtaining the normalized mean curvature for each vertex, we proceed to assign these values to RGB tuples using a color rainbow map. In this map, the color blue corresponds to low values (indicating convexity), green represents medium values (indicating flatness), and red signifies high values (indicating concavity).\nIn order to segment the model and acquire the patches for parametrization, two approaches are employed. In the initial step, a predetermined quantity of seed points (2, 5, and 10) is chosen, employing a similar methodology as that used for generating the holes, in order to guarantee an equitable dispersion of the patches throughout the mesh. By strategically placing the initial points at maximum distance from one another, we are able to generate patches of uniform size that encompass the entire surface. Following the aforementioned subdivisions, a total of 17 patches are obtained per mesh.\nIn addition, the intact mesh is also divided into distinct significant patches, using the methodology described in [62]. This resulted in a variable number of patches, as it is contingent upon the shape of the 3D model.\nAs proposed in [33], the approach involves parameterizing the patches of the 3D models to generate 2D images that capture a partial representation of the mesh surface. Indeed, the images generated in this manner can be considered as the ground truth images, as they accurately depict the curvature of the submesh surface.\nThe subsequent stage involves the generation of visual impairments, specifically the introduction of voids within the images. It is crucial to observe that the method employed does not involve utilizing the previously mentioned holes, but rather involves the creation of novel and distinct holes. By employing this approach, we ensure that the inpainting network is not trained using the voids generated in the 3D meshes. Utilizing the 3D model dataset for evaluation purposes will be facilitated by this.\nThe curvature image patch is not directly damaged; instead, a separate image is created to represent the hole mask. The creation of each hole mask follows a similar approach to that used on the 3D mesh. A seed vertex is randomly chosen within a central region of the image. The selection process is conducted with a certain level of randomness. The entryway is more likely to be positioned closer to the center of the resulting image. The process involves the selection of a random angle and a distance that follows a normal distribution, as illustrated in Fig. 6.\nSubsequently, the selection is systematically expanded across the surface topology until it encompasses a proportion exceeding 10 percent of the vertices within the specific concrete patch. The identified faces and vertices are eliminated from the parameterization and from the two-dimensional image.\nThe image that has been damaged is subsequently converted into a binary mask using black and white colors. In this mask, white regions indicate the absence of the surface, or the presence of a hole, while black regions indicate the remaining surface.\nThe accuracy of the trained neural network will increase as the number of images in the dataset increases. Given the limited quantity of 3D models available, which is approximately 1,000, it is necessary to perform various operations to augment the final image dataset. This is a common practice in computer vision methodologies. The paired images, consisting of curvature and binary masks, undergo rotations of 90, 180, and 270 degrees, as well as vertical and horizontal flips. In addition, each mask is modified with random displacements and rotations to the white area. This results in the creation of five additional holes per patch, as illustrated in Fig. 7.\nTo summarize, the process of generating images begins with 959 models from our mesh dataset. Each model is segmented into 2, 5, 10, and, on average, 20 significant submeshes. The dataset consists of over 35,000 images, which are enhanced by applying four rotations and two flips. This augmentation process results in a total of more than 212,000 images, each associated with a hole mask. The masks are ultimately enhanced through the process of displacing and rotating the hole five times, with each rotation being done to a random degree and direction.\nThe final image dataset consists of a total of 2,029,660 images, specifically 1,014,830 images of curvature patches and their corresponding 1,014,830 binary hole masks.\nIn order to conduct the training and validation of the algorithms, the dataset has been partitioned into separate subsets for training and testing purposes, as illustrated in Table 2."}, {"title": "3.2 Neural Network Training", "content": "The image dataset will function as a standardized source for the training of a neural network that is specifically designed to identify curvature color patterns. In comparison to general-purpose networks such as Stable Diffusion [58], this specialization is anticipated to improve inpainting outcomes. Co-mod-GAN [88], Lama [66], CRFill [84], AOT [83], and TFill [89] are five distinct inpainting approaches that we have refined in order to accomplish this ad-hoc training. Their capacity to train with binary masks that indicate missing pixels in the images, the public availability of their code, and their ability to generalize to our specialized color maps were all factors that were selected. Their primary characteristics include:\n\u2022 Co-mod-GAN is an adversarial architecture that uses co-modulation to inpaint large missing regions.\n\u2022 Lama architecture uses Fast Fourier Convolutions to achieve better performance.\n\u2022 CRFill introduces contextual reconstruction loss (CR loss), which promotes the plausibility of the output generated even when reconstructed using contextual information.\n\u2022 TFill deploys a transformer to directly capture long-range dependence.\n\u2022 The AOT GAN architecture aggregates contextual transformations from a variety of receptive fields, enabling the capture of both informative distant image contexts and complex patterns of interest.\nThe fine-tuning process requires retraining each model, which has already been trained on a large dataset of general images. We use our smaller, domain-specific dataset to adapt the network specifically for the task of completing color curvature maps. This process utilizes the pre-learned features and representations from the initial training. In the previous section, we talked about how the approaches used for inpainting tasks are highly advanced, especially when it comes to the Places2 dataset [90]. The process is shown in Fig. 8.\nIn order to assess the effectiveness of our fine-tuning, we have chosen a range of metrics that allow us to objectively measure the quality of the predicted images. These metrics are based on widely accepted standards for evaluating image inpainting similarity:\n\u2022 FID (Frechet inception Distance) metric [80] measures the distance between feature representations of real and generated images using an inception network. Lower indicates better image quality.\n\u2022 LPIPS (Learned Perceptual Image Patch Similarity) [86] uses deep learning to calculate the perceptual difference between images.\n\u2022 PSNR (Peak Signal-to-Noise Ratio) [34] quantifies level of noise or distortion in the reconstructed image.\n\u2022 SSIM (Structural Similarity Index) [34] indicates image quality degradation, where higher SSIM value means higher image quality and similarity to the ground truth.\n\u2022 The L1 loss is a metric that measures the difference between images on a pixel level. A lower L1 loss indicates higher accuracy of the network.\n\u2022 As in [88], we also added P-IDS and U-IDS metrics to offer a more comprehensive comparison. These metrics align with user preferences regarding subtle differences in image quality.\nThe performance of the ad-hoc trained algorithms on the curvature images is displayed in Table 3. The metrics have been calculated both before and after performing fine-tuning. The evaluation metrics for the fine-tuned methods demonstrate their superior performance in restoring missing portions of a curvature map image compared to the generic Stable Diffusion method. In the inpainting task, Co-mod-GAN stands out as the top method, outperforming others in metrics such as FID, P-IDS, U-IDS, PSNR, and SSIM. Lama also performs well in this regard. Although Stable Diffusion produces satisfactory qualitative results, its use of general-purpose training data sometimes leads to outputs that differ greatly from the original image, as shown in Fig. 9."}, {"title": "3.3 3D Reconstruction Algorithm", "content": "We have devised a fully automated, optimized mesh deformation process that modifies the surface to follow the predicted curvature patch, based on the work of [33]. This process effectively repairs the model by updating the surface with new curvature values, taking into consideration the curvature values of the neighborhood vertices of the hole and the image information from the inpainting process.\nThe initial step in our coarse-to-fine reparation procedure is the normalization of the mesh. Then, we employ a conventional method [42] to fill the holes. This method triangulates the holes and refines the resulting triangulation to align with the density of triangles in the surrounding area. As was expected, the level of detail in this coarsely reconstructed region is exceedingly low.\nNext, the mesh is segmented into local patches around every hole to preserve contextual information. The curvature values are used to color the per-vextex of these patches, which are parameterized to produce planar representations of the surface. The hole mask image is the result of the newly generated vertices in the previous phase.\nAt this stage, the neural networks must be provided with the curvature bidimensional image and its corresponding hole mask in order to execute the inpainting procedure and generate a new curvature patch image.\nLastly, the surface is refined through an iterative deformation process, as illustrated in Fig. 10. At each iteration, the new curvature image is calculated and the vertices are displaced along their normal vector. This process attempts to adjust the position of vertices by analyzing the color differences between the vertex in the curvature image of the patch and its corresponding pixel in the inferred inpainted curvature image. The direction of the vertex displacement is determined by the color distance between the two pixels. For instance, the vertices in the coarse repaired patch that correspond to green pixels (smooth curvature or flat surface) are repositioned in the direction of the normal vector if the inpainted pixel is red (convex surface) and in the opposite direction to the normal vector if the inpainted pixel indicates a concave surface area.\nIn order to preserve the original vertices of the mesh, we designate a region of interest (ROI) for the deformation algorithm as a portion of the reconstructed surface. We identify the vertices within the ROI that exhibit the most substantial color discrepancies between the curvature color and the corresponding pixel color of the inpainted image. Then, we identify the vertices whose color difference exceeds a threshold of 30 units in any color channel, such as (0,254, 228) vs. (28, 250, 5). These vertices serve as control points for the deformation process, which is conducted using an elastic deformation approach that is based on [11]. The direction and magnitude of displacement for the entire ROI area are determined by the distance of the control locations and the sign of the color difference, as previously mentioned. Vertex positions are updated in response to applied forces, resulting in deformations that appear realistic.\nUpon the algorithm's convergence, the ROI boundaries are subsequently smoothed to ensure a fluid transition between the original and new portions of the surface.\nWe conducted an ablation study in Fig. 11 to validate our hole-centric reconstruction method and emphasize the significance of the ROI. When the deformation is not constrained to the ROI and the elastic deformation takes into account the entire mesh, we can observe visually similar outcomes. On the other hand, the distance metrics represented by colors in each vertex show significant color variations throughout the mesh when considering a large region of interest (Fig. 11.d). This indicates that vertices outside of the new patch have been moved (shown in yellow and blue) when they should have remained unchanged. When applying the algorithm with a modified ROI (Fig. 11.c), all vertices remain unchanged (red) while only the hole area is repaired."}, {"title": "4 Results", "content": "Our approach has been assessed quantitatively and qualitatively through a series of experiments. In order to guarantee impartial and equitable comparisons and measurements, we offer a concise summary of the experimental conditions in Section 4.1. Consequently, in Section 4.2, we examine the quantitative experiments and present the corresponding results. Lastly, we investigate the qualitative experiments in Section 4.3 to illustrate the quality of the reconstructed meshes and the performance of our approach."}, {"title": "4.1 Experiment conditions", "content": "We opted to establish a restricted inference time of 9 minutes for running our inpainting neural networks. This choice allows for equitable comparisons between our approach and other cutting-edge methods. The hole completion of our complete dataset, which includes 959 models, was exceuted in approximately one week, as each mesh correction operation needs a maximum of nine minutes.\nWe conducted our experiments by applying the two inpainting methods that produced the most favorable visual loss metrics to the curvature images: Lama and Co-Mod-GAN (refer to Table 3). The performance of our approach is compared to classical techniques that have software available for comparison purposes, as well as some deep learning-based methods for hole completion, in our analysis. In particular, we compared our findings to well-known benchmarks in the field, including MeshFix [1], Ramesh [9], and Screened Poisson [38] for classical methods. For deep learning-based methods, we introduced SAL [2] and Point2Mesh [28], two approaches that were developed to perform shape completion tasks.\nWe have conducted a comprehensive qualitative and quantitative evaluation of the results after executing our algorithm on the incomplete meshes that were reserved for testing. The visual appearance and similarity of details of the new patches are the primary focus of the qualitative evaluation, while the distances between the repaired mesh and the ground truth mesh are computed for the quantitative evaluation."}, {"title": "4.2 Qualitative Results", "content": "For qualitative assessment, we have selected four baseline models that have distinct features. These four models showcase a variety of organic forms and both artificial and real-world scenarios. They include a real human face, the classical Armadillo mesh from the Stanford repository, a scanned baroque 3D sculpture, and a Christmas bear figure. Our models have been through a rigorous process to create authentic-looking holes on their surfaces. We then repaired them using various methods and conducted experiments to ensure consistent results. Deep learning-based models often demand substantial computational time and graphical resources in order to produce satisfactory inference results.\nFig. 12 displays the results. In order to offer a more comprehensive visual analysis, we have implemented a color scheme that corresponds to the signed distance value of each mesh in relation to the ground truth mesh. It is important to note that our evaluation focuses solely on the final shape. Although the majority of the approaches yield similar results (excluding SAL and Point2Mesh), a more detailed examination of the repaired regions reveals that our approach accurately simulates more lifelike surfaces. The Virgin Mary sculpture showcases a distinct approach to reconstructing the eye, focusing on accurately recovering the original shape rather than simply extending local geometry around the hole.\nAdditional visual comparisons of selected meshes from the 3D models dataset are provided in Fig. 13. Our method stands out in its ability to repair surfaces with incredibly lifelike and natural results, resulting in meshes that are noticeably smoother than those produced by other cutting-edge methods."}, {"title": "4.3 Quantitative Results", "content": "In our comprehensive assessment of the methodology, we concentrated on the comparison of the Hausdorff distance error between the original mesh and the repaired mesh. Table 4 contains the summarized results of this comparison of the four models that were previously employed. These metrics will further facilitate the understanding of the visual reparation results that were previously provided.\nOur method consistently demonstrates a significantly reduced maximum distance in comparison to alternative approaches when the Hausdorff distance is taken into account. This finding implies that our methodology is proficient in mitigating substantial errors within the reconstructed meshes. In addition, the mean distance metric consistently outperforms other methods, suggesting that our reconstructions produce more consistent and stable results across the test models.\nThe effectiveness of our method in reducing significant errors (as indicated by the smaller maximum distance) while maintaining overall stability and accuracy (as indicated by the smaller mean distance) across a variety of test scenarios is underscored by these results.\nWe conducted a comprehensive evaluation of the repairing algorithms on the entire mesh dataset to verify that our approach substantially outperforms others in terms of Hausdorff distance. The experiment conditions outlined in Section 4.1 were taken into account during the execution of these operations.\nThe mean and maximal Hausdorff distance calculations for a dataset of 959 models are presented in Table 5. It is important to note that the SAL method produces inf values as a result of its incapacity to align the inference with the ground truth mesh, which leads to exponentially increased distance values. This problem renders it impossible to automate the measure calculations, which results in the necessity of manual mesh alignment. Furthermore, the Ramesh method has been omitted from this calculation due to the fact that the associated software is supplied as an executable and the code is unavailable. Consequently, it is impossible to script the reparation process across the entire 3D models dataset.\nThe Lama and Co-mod-GAN inpainting methods exhibit substantial superiority over other state-of-the-art methods, as evidenced by their mean values. This suggests that our method is capable of recovering surfaces with reduced distance differences between their inference and ground truth surfaces. Furthermore, it is important to observe that Co-mod-GAN outperforms Lama, a result that was anticipated by the image loss metrics calculated in Section 3.2."}, {"title": "5 Conclusions and future works", "content": "Based on the results presented in the previous section, our approach excels in mesh repair tasks for a variety of reasons. Firstly, by utilizing deep learning techniques, our method is able to acquire intricate surface curvature color patterns, leading to repairs that are highly accurate. Additionally, our thorough repair process focuses on preserving the original mesh shape and details. This results in repaired surfaces that closely resemble the ground truth meshes, without any over-smoothing or unnatural deformations that are often found in other methods. Our quantitative evaluations provide evidence of this. Our approach brings unique strengths that greatly contribute to the field of shape completion tasks.\nAlthough our approach has shown significant strengths in mesh repair tasks, it is crucial to recognize certain limitations. The effectiveness of our method can be affected by the quality and diversity of the training dataset due to the use of deep learning techniques and extensive training data. Inadequate or skewed training data may lead to less than optimal repairs. In addition, handling severely damaged or incomplete meshes can pose challenges when trying to preserve mesh details during the repair process. A noteworthy example is when meshes show the existence of separate elements or faces in the geometry. Our method might encounter difficulties in effectively repairing such meshes, as the segmentation and curvature calculations involved necessitate watertight geometries.\nThese particular instances highlight the importance of further research and development. In addition, the computational resources needed for training and inference in deep learning-based methods, including ours, can be significant, which can restrict the scalability of our approach for real-time or resource-limited applications. Considering future needs, it is important to work on these networks to optimize time and resource utilization."}]}