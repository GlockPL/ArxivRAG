{"title": "3D Hole Filling using Deep Learning Inpainting", "authors": ["Marina Hern\u00e1ndez-Bautista", "F.J. Melero"], "abstract": "The current work presents a novel methodology for completing 3D surfaces produced from 3D digitization technologies in places where there is a scarcity of meaningful geometric data. Incomplete or missing data in these three-dimensional (3D) models can lead to erroneous or flawed renderings, limiting their usefulness in a variety of applications such as visualization, geometric computation, and 3D printing. Conventional surface estimation approaches often produce implausible results, especially when dealing with complex surfaces. To address this issue, we propose a technique that incorporates neural network-based 2D in-painting to effectively reconstruct 3D surfaces. Our customized neural networks were trained on a dataset containing over 1 million curvature images. These images show the curvature of vertices as planar representations in 2D. Furthermore, we used a coarse-to-fine surface deformation technique to improve the accuracy of the reconstructed pictures and assure surface adaptability. This strategy enables the system to learn and generalize patterns from input data, resulting in the development of precise and comprehensive three-dimensional surfaces. Our methodology excels in the shape completion process, effectively filling complex holes in three-dimensional surfaces with a remarkable level of realism and precision.", "sections": [{"title": "1 Introduction", "content": "In recent years, there has been a growing prominence of 3D digitization, which has had a significant impact on various disciplines. This technology enables the creation of highly detailed digital replicas of physical objects. Digital models acquired through the use of scanners or photogrammetry are extensively utilized in various fields such as ar-"}, {"title": "2 Related Work", "content": "The development of robust restoration algorithms within the computer graphics community has been closely correlated with the evolution of 3D acquisition methods. The presence of holes or absent data on the acquired surface is one of the most frequently encountered issues. In the past, researchers have investigated a context-aware combination"}, {"title": "2.1 Shape Completion", "content": "A significant amount of research has been undertaken to tackle the problem of filling voids or imperfections in both basic and intricate three-dimensional objects. The classification of classical approaches can be broadly divided into three categories:\n\u2022 Surface-based approaches. The initial set of methodologies centers around utilizing the surface data of the mesh in order to fill in areas that are incomplete or missing. The estimation of the shape of missing areas is frequently accomplished through the utilization of local or global surface fitting techniques, as discussed in previous studies [42, 1, 9, 5, 7, 53, 23]. Nevertheless, these techniques may not be appropriate for larger or more intricate holes.\n\u2022 Volumetric approaches [17, 27, 49, 38] represent shapes by utilizing grids or voxel grids. The estimation of missing data is accomplished through the use of extrapolation or interpolation techniques applied within the volume. The surface is then represented using either dense voxel grids [16, 65] or octrees [57], which effectively address challenges associated with memory usage and computational complexity. The aforementioned methods have demonstrated notable efficacy in addressing intricate voids and have been employed in a multitude of research investigations.\n\u2022 Context-based approaches have been widely studied and implemented in various fields. The utilization of pre-existing information or patterns within the mesh is employed to complete the missing sections. The authors in [29, 67] utilize contextual relationships among various components of the shape in order to fill in the gaps in the regions that are missing. Alternatively, they also concentrate on utilizing local geometric patterns or features to deduce the missing parts. The task can be accomplished using either surface-based techniques [29] or volumetric techniques [63].\nAs artificial intelligence has advanced, new neural network-based hole-filling techniques have appeared. These strategies use machine learning to anticipate shape information that is lacking by using patterns that have been learned from complete data that already exists.\nThe most often used architectures are based on autoencoders, which are used to acquire various prior information and generate a latent space of forms that they map to their partial inputs [81, 12, 65, 48, 16]. Utilizing variational autoencoders (VAE) for training, the methods of [65, 2] have proven to be accurate when applied to 3D shapes from the ShapeNet [10] and KITTI [24] datasets.\nA coarse-to-fine approach is implemented in both [16] and [81] to facilitate shape completion. In the former, a 3D shape classification network is implemented to provide additional information. However, this approach may be limited in its adaptability and is contingent upon training data. Conversely, the latter abstracts a point cloud into a feature vector, resulting in a coarse-to-fine-grained output.\nGAN architectures [68, 85] have also been employed for 3D shape completion, in which a generator and a discriminator collaborate to acquire the features of a 3D model, similar to their application in image generation. These methods have demonstrated potential in the field of 3D shape completion. Reinforcement learning is also implemented [60]."}, {"title": "2.2 Image inpainting", "content": "Inpainting is the term commonly used to describe the process of completing what is missing in a 2D image. Var-"}, {"title": "2.3 Shape Completion using Inpainting Techniques", "content": "Significant progress has been achieved in the domain of shape completion in recent years, with a particular focus on the utilization of 2D inpainting techniques. The objective of these techniques is to restore or fill in absent or obscured portions of an object in an image by utilizing the capabilities of deep learning and neural networks to deduce and produce credible reconstructions. This subsection examines the most pertinent and advanced methodologies that closely correspond to our research, with a specific emphasis on those that employ 2D inpainting techniques to accomplish shape completion with a high level of accuracy.\nA limited number of solutions that employ a variety of various approaches have been the result of recent research on this novel methodology. Several recent methods have employed the self-prior concept in the context of 3D inpainting, where recoveries are made by directly performing inpainting over the 3D surface. This approach overcomes the disadvantages of data-driven methods by learning prior knowledge from a single damaged point on the surface. This context enables us to evaluate methods such as those proposed in [30, 31], which utilize Graph Convolutional Networks for mesh denoising and shape completion tasks, respectively. Point2Mesh [28] is another remarkable method that employs a self-prior to reconstruct the entire surface from an input point cloud without the need for pre-training. In addition, the Deep Geometric Prior method [73] reconstructs various local surface patches as a manifold atlas. A comprehensive reconstruction of the surface is generated by sampling this atlas.\nWithin this particular domain, there is a prevalent trend of employing NeRFs (Neural Radiance Fields) as a means of representing the three-dimensional geometry and radiance of a given scene through the utilization of neural networks. For example, [70] explores the application of a 2D diffusion model in the context of general scene completion tasks. The model is utilized to generate a coherent multi-view output for the completed 3D scene. In a similar vein, [52] employs a text-to-image diffusion model to facilitate the synthesis of 3D representations from textual input.\nA series of approaches that employ analogous techniques, such as geometrical procedures and 2D inpainting techniques, have been chosen to facilitate the most accurate comprehension of the problem at hand. Inpainting is employed to recover fabric filaments in [26]. Depth images are repaired by both [51, 35]. The former employs a classical inpaint approach and subsequently conducts the 2D to 3D transformation, while the latter employs partial depth im-"}, {"title": "3 Methodology", "content": "The challenge of restoring complex objects with multiple holes is tackled in this work through the utilization of a curvature-based approach. This approach entails the creation of a curvature feature image and the subsequent application of inpainting techniques to effectively fill in the missing components. The iterative process of deformations ensures the successful restoration of complex surfaces by inferring missing geometry over the mesh surface until alignment with the inpainted curvature image is achieved. The focus of our research is on automating and improving the methodology used to address the limitations of generic inpainting networks.\nThis procedure is further clarified upon in this section. At the initial phase, we generate a dataset of 3D models that are all of a specific dimension and manifoldness. The image dataset that will be employed in the deep learning training is built upon this curated collection of meshes. The entire process entails the conversion of the 3D mesh representations into 2D images, thereby guaranteeing that the resultant images accurately depict the geometrical details of the original surface. These 3D meshes will not only be used to produce 2D images but also as a benchmark for assessing the efficacy of our shape completion algorithm. Section 3.1 delineates all aspects of the procedure.\nFollowing that, we proceed to train a range of cutting-edge ad-hoc architectures for image inpainting utilizing our image dataset, as depicted in Section 3.2. The automatic algorithm that conducts virtual sculpting of the mesh surface will be guided by the resulting repaired images. This method, as illustrated in 3.3, offers a comprehensible process that results in an accurate solution to the 3D model shape reconstruction problem."}, {"title": "3.1 Creation of the training dataset", "content": "The process of creating the dataset involves multiple stages, resulting in the formation of two separate databases.\nFirstly, we provide an explanation in Section 3.1.1 regarding the specific criteria that were selected for the inclusion of 3D models in the dataset. These criteria ensure that the models are suitable for use as a training dataset for 2D inpainting tasks. After the selection of the 3D models, the creation of realistic holes on the surface is carried out according to the procedures described in Section 3.1.2. The initial collection of three-dimensional models, consisting of"}, {"title": "3.1.1 Selection of 3D models", "content": "To ensure the success of the parametrization or reconstruction process, it is imperative that all meshes in the dataset possess the quality of being watertight, meaning they should not contain any holes. Additionally, they should be manifold, indicating that they are continuous and have no self-intersections. Furthermore, the meshes should consist of a single component with a substantial number of polygons.\nThe aforementioned characteristics play a pivotal role, as the correct topology enables us to execute geometric operations on the mesh surface while repairing the affected areas, thereby ensuring the efficacy of our reconstruction algorithm.\nWe have conducted an analysis on a comprehensive set of seven distinct 3D model datasets that are frequently referenced in academic literature [10, 18, 13, 20, 39, 75, 15]. Our investigation has revealed a number of issues, which are detailed in Table 1. The majority of the datasets fail to meet our requirement for watertightness as a result of incorrect topology. The datasets in question exhibited erroneous topology or were specifically designed for mesh segmentation purposes, resulting in the presence of disconnected faces. One frequently encountered issue is the quantity of polygons per mesh. Meshes with a limited number of faces can result in inadequate levels of detail, as models with low resolution tend to generate color images with reduced variability in patterns. Consequently, this can negatively impact the training process and result in less accurate reconstructions. Conversely, an excessive quantity of polygons will lead to a dataset that requires significant resources for both training and evaluation. In order to ensure the quality of our dataset, we implemented a filtering process to specifically select 3D models that possessed a suitable number of polygons and exhibited correct topology.\nUpon thorough examination of the aforementioned datasets, we have successfully collated an extensive catalog consisting of 941 models out of the original 1032 models from the Google Dataset. The exclusion criteria for the remaining 91 models was based on their inability to be in-"}, {"title": "3.1.2 Creating holes in 3D models for testing", "content": "The benchmark models utilized for the shape completion algorithms necessitate the presence of one or more holes in the surface. Consequently, we were compelled to fabricate authentic holes within the meshes, as illustrated in the upper branch of Fig. 5.\nTo simulate perforations, a scripted procedure is employed for each model, whereby the mesh is initially divided into multiple evenly distributed patches. By employing this approach, we ensure that the placement of each subsequent hole center is maximally distant from the centers of the other holes. As a result, this sampling strategy enables us to target distinct regions of the surface and prevent any potential overlap between the holes.\nThe algorithm utilizes a farthest point sampling strategy on the surface mesh to choose five vertices in such a way that each vertex is maximally distant from the others, thereby guaranteeing a well-distributed set of starting"}, {"title": "3.1.3 Curvature images for training", "content": "The ground truth models are utilized to create a dataset of images, which will subsequently be employed for training the neural network. The process is visually illustrated in the lower branch of Fig. 5.\nThe central concept involves the representation of the models' surfaces as a collection of two-dimensional images. This is achieved by sampling the mean curvature values and parametrizing distinct patches for each mesh. The concept of mean curvature is used to quantify the curvature of a surface, allowing for a deeper understanding of its geometric characteristics. The mean curvature at any given point on a surface can be defined as the arithmetic mean of its two principal curvatures, which correspond to the maximum and minimum curvatures in the principal directions passing through that particular point. After obtaining the normalized mean curvature for each vertex, we proceed to assign these values to RGB tuples using a color rainbow map. In this map, the color blue corresponds to low values (indicating convexity), green represents medium values (indicating flatness), and red signifies high values (indicating concavity).\nIn order to segment the model and acquire the patches for parametrization, two approaches are employed. In the initial step, a predetermined quantity of seed points (2, 5, and 10) is chosen, employing a similar methodology as that used for generating the holes, in order to guarantee an equitable dispersion of the patches throughout the mesh. By strategically placing the initial points at maximum distance from one another, we are able to generate patches of uniform size that encompass the entire surface. Following the aforementioned subdivisions, a total of 17 patches are obtained per mesh.\nIn addition, the intact mesh is also divided into dis-"}, {"title": "3.2 Neural Network Training", "content": "The image dataset will function as a standardized source for the training of a neural network that is specifically designed to identify curvature color patterns. In comparison to general-purpose networks such as Stable Diffusion [58], this specialization is anticipated to improve inpainting outcomes. Co-mod-GAN [88], Lama [66], CRFill [84], AOT [83], and TFill [89] are five distinct inpainting approaches that we have refined in order to accomplish this ad-hoc training. Their capacity to train with binary masks that indicate missing pixels in the images, the public availability of their code, and their ability to generalize to our specialized color maps were all factors that were selected. Their primary characteristics include:\n\u2022 Co-mod-GAN is an adversarial architecture that uses co-modulation to inpaint large missing regions.\n\u2022 Lama architecture uses Fast Fourier Convolutions to achieve better performance.\n\u2022 CRFill introduces contextual reconstruction loss (CR loss), which promotes the plausibility of the output generated even when reconstructed using contextual information.\n\u2022 TFill deploys a transformer to directly capture long-range dependence.\n\u2022 The AOT GAN architecture aggregates contextual transformations from a variety of receptive fields, enabling the capture of both informative distant image contexts and complex patterns of interest.\nThe fine-tuning process requires retraining each model, which has already been trained on a large dataset of general images. We use our smaller, domain-specific dataset to adapt the network specifically for the task of completing color curvature maps. This process utilizes the pre-learned features and representations from the initial training. In the previous section, we talked about how the approaches used for inpainting tasks are highly advanced, especially when it comes to the Places2 dataset [90]. The process is shown in Fig. 8.\nIn order to assess the effectiveness of our fine-tuning, we have chosen a range of metrics that allow us to objectively measure the quality of the predicted images. These metrics are based on widely accepted standards for evaluating image inpainting similarity:\n\u2022 FID (Frechet inception Distance) metric [80] measures the distance between feature representations of real and generated images using an inception network. Lower indicates better image quality.\n\u2022 LPIPS (Learned Perceptual Image Patch Similarity) [86] uses deep learning to calculate the perceptual difference between images.\n\u2022 PSNR (Peak Signal-to-Noise Ratio) [34] quantifies level of noise or distortion in the reconstructed image.\n\u2022 SSIM (Structural Similarity Index) [34] indicates image quality degradation, where higher SSIM value means higher image quality and similarity to the ground truth.\n\u2022 The L1 loss is a metric that measures the difference between images on a pixel level. A lower L1 loss indicates higher accuracy of the network."}, {"title": "3.3 3D Reconstruction Algorithm", "content": "We have devised a fully automated, optimized mesh deformation process that modifies the surface to follow the predicted curvature patch, based on the work of [33]. This process effectively repairs the model by updating the surface with new curvature values, taking into consideration the curvature values of the neighborhood vertices of the hole and the image information from the inpainting process.\nThe initial step in our coarse-to-fine reparation procedure is the normalization of the mesh. Then, we employ a conventional method [42] to fill the holes. This method triangulates the holes and refines the resulting triangulation to align with the density of triangles in the surrounding area. As was expected, the level of detail in this coarsely reconstructed region is exceedingly low.\nNext, the mesh is segmented into local patches around every hole to preserve contextual information. The curvature values are used to color the per-vextex of these patches, which are parameterized to produce planar representations of the surface. The hole mask image is the result of the newly generated vertices in the previous phase.\nAt this stage, the neural networks must be provided with the curvature bidimensional image and its corresponding hole mask in order to execute the inpainting procedure and generate a new curvature patch image.\nLastly, the surface is refined through an iterative deformation process, as illustrated in Fig. 10. At each iteration, the new curvature image is calculated and the vertices are displaced along their normal vector. This process attempts to adjust the position of vertices by analyzing the color differences between the vertex in the curvature image of the patch and its corresponding pixel in the inferred inpainted curvature image. The direction of the vertex displacement is determined by the color distance between the two pixels. For instance, the vertices in the coarse repaired patch that correspond to green pixels (smooth curvature or flat surface) are repositioned in the direction of the normal vector if the inpainted pixel is red (convex surface) and in the opposite direction to the normal vector if the inpainted pixel indicates a concave surface area.\nIn order to preserve the original vertices of the mesh, we designate a region of interest (ROI) for the deformation algorithm as a portion of the reconstructed surface. We identify the vertices within the ROI that exhibit the most substantial color discrepancies between the curvature color and the corresponding pixel color of the inpainted image.\nThen, we identify the vertices whose color difference exceeds a threshold of 30 units in any color channel, such as (0, 254, 228) vs. (28, 250, 5). These vertices serve as control points for the deformation process, which is conducted using an elastic deformation approach that is based on [11]. The direction and magnitude of displacement for the entire ROI area are determined by the distance of the control locations and the sign of the color difference, as previously mentioned. Vertex positions are updated in response to applied forces, resulting in deformations that appear realistic.\nUpon the algorithm's convergence, the ROI boundaries are subsequently smoothed to ensure a fluid transition between the original and new portions of the surface.\nWe conducted an ablation study in Fig. 11 to validate our hole-centric reconstruction method and emphasize the significance of the ROI. When the deformation is not constrained to the ROI and the elastic deformation takes into account the entire mesh, we can observe visually similar outcomes. On the other hand, the distance metrics represented by colors in each vertex show significant color variations throughout the mesh when considering a large region of interest (Fig. 11.d). This indicates that vertices outside of the new patch have been moved (shown in yellow and blue) when they should have remained unchanged. When applying the algorithm with a modified ROI (Fig. 11.c), all vertices remain unchanged (red) while only the hole area is repaired."}, {"title": "4 Results", "content": "Our approach has been assessed quantitatively and qualitatively through a series of experiments. In order to guarantee impartial and equitable comparisons and measurements, we offer a concise summary of the experimental conditions"}, {"title": "4.1 Experiment conditions", "content": "We opted to establish a restricted inference time of 9 minutes for running our inpainting neural networks. This choice allows for equitable comparisons between our approach and other cutting-edge methods. The hole completion of our complete dataset, which includes 959 models, was exceuted in approximately one week, as each mesh correction operation needs a maximum of nine minutes.\nWe conducted our experiments by applying the two inpainting methods that produced the most favorable visual loss metrics to the curvature images: Lama and Co-Mod-"}, {"title": "4.3 Quantitative Results", "content": "In our comprehensive assessment of the methodology, we concentrated on the comparison of the Hausdorff distance error between the original mesh and the repaired mesh. Table 4 contains the summarized results of this comparison of the four models that were previously employed. These metrics will further facilitate the understanding of the visual reparation results that were previously provided.\nOur method consistently demonstrates a significantly reduced maximum distance in comparison to alternative approaches when the Hausdorff distance is taken into account. This finding implies that our methodology is proficient in mitigating substantial errors within the reconstructed meshes. In addition, the mean distance metric consistently outperforms other methods, suggesting that our reconstructions produce more consistent and stable results across the test models.\nThe effectiveness of our method in reducing significant errors (as indicated by the smaller maximum distance) while maintaining overall stability and accuracy (as indicated by the smaller mean distance) across a variety of test scenarios is underscored by these results.\nWe conducted a comprehensive evaluation of the repairing algorithms on the entire mesh dataset to verify that our approach substantially outperforms others in terms of Hausdorff distance. The experiment conditions outlined in Section 4.1 were taken into account during the execution of these operations.\nThe mean and maximal Hausdorff distance calculations for a dataset of 959 models are presented in Table 5. It is important to note that the SAL method produces inf values as a result of its incapacity to align the inference with the ground truth mesh, which leads to exponentially increased distance values. This problem renders it impossible to automate the measure calculations, which results in the necessity of manual mesh alignment. Furthermore, the Ramesh method has been omitted from this calculation due to the fact that the associated software is supplied as an executable and the code is unavailable. Consequently, it is impossible to script the reparation process across the entire 3D models dataset.\nThe Lama and Co-mod-GAN inpainting methods exhibit substantial superiority over other state-of-the-art methods, as evidenced by their mean values. This suggests that our method is capable of recovering surfaces with reduced distance differences between their inference and ground truth surfaces. Furthermore, it is important to observe that Co-mod-GAN outperforms Lama, a result that was anticipated by the image loss metrics calculated in Section 3.2."}, {"title": "4.2 Qualitative Results", "content": "For qualitative assessment, we have selected four baseline models that have distinct features. These four models showcase a variety of organic forms and both artificial and real-world scenarios. They include a real human face, the classical Armadillo mesh from the Stanford repository, a scanned baroque 3D sculpture, and a Christmas bear figure. Our models have been through a rigorous process to create authentic-looking holes on their surfaces. We then repaired them using various methods and conducted experiments to ensure consistent results. Deep learning-based models often demand substantial computational time and graphical resources in order to produce satisfactory inference results.\nFig. 12 displays the results. In order to offer a more comprehensive visual analysis, we have implemented a color scheme that corresponds to the signed distance value of each mesh in relation to the ground truth mesh. It is important to note that our evaluation focuses solely on the final shape. Although the majority of the approaches yield similar results (excluding SAL and Point2Mesh), a more detailed examination of the repaired regions reveals that our approach accurately simulates more lifelike surfaces. The Virgin Mary sculpture showcases a distinct approach to reconstructing the eye, focusing on accurately recovering the original shape rather than simply extending local geometry around the hole.\nAdditional visual comparisons of selected meshes from the 3D models dataset are provided in Fig. 13. Our method stands out in its ability to repair surfaces with incredibly lifelike and natural results, resulting in meshes that are noticeably smoother than those produced by other cutting-edge methods."}, {"title": "5 Conclusions and future works", "content": "Based on the results presented in the previous section, our approach excels in mesh repair tasks for a variety of reasons. Firstly, by utilizing deep learning techniques, our method is able to acquire intricate surface curvature color patterns, leading to repairs that are highly accurate. Additionally, our thorough repair process focuses on preserving the original mesh shape and details. This results in repaired surfaces that closely resemble the ground truth meshes, without any over-smoothing or unnatural deformations that are often found in other methods. Our quantitative evaluations provide evidence of this. Our approach brings unique strengths that greatly contribute to the field of shape completion tasks.\nAlthough our approach has shown significant strengths in mesh repair tasks, it is crucial to recognize certain limitations. The effectiveness of our method can be affected by the quality and diversity of the training dataset due to the use of deep learning techniques and extensive training data. Inadequate or skewed training data may lead to less than optimal repairs. In addition, handling severely damaged or incomplete meshes can pose challenges when trying to preserve mesh details during the repair process. A noteworthy example is when meshes show the existence of separate elements or faces in the geometry. Our method might encounter difficulties in effectively repairing such meshes, as the segmentation and curvature calculations involved necessitate watertight geometries.\nThese particular instances highlight the importance of further research and development. In addition, the computational resources needed for training and inference in deep learning-based methods, including ours, can be significant, which can restrict the scalability of our approach for real-time or resource-limited applications. Considering future needs, it is important to work on these networks to optimize time and resource utilization."}]}