{"title": "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models", "authors": ["Yiming Huang", "Jianwen Luo", "Yan Yu", "Yitong Zhang", "Fangyu Lei", "Yifan Wei", "Shizhu He", "Lifu Huang", "Xiao Liu", "Jun Zhao", "Kang Liu"], "abstract": "We introduce DA-Code, a code generation benchmark specifically designed to assess LLMs on agent-based data science tasks. This benchmark features three core elements: First, the tasks within DA-Code are inherently challenging, setting them apart from traditional code generation tasks and demanding advanced coding skills in grounding and planning. Second, examples in DA-Code are all based on real and diverse data, covering a wide range of complex data wrangling and analytics tasks. Third, to solve the tasks, the models must utilize complex data science programming languages, to perform intricate data processing and derive the answers. We set up the benchmark in a controllable and executable environment that aligns with real-world data analysis scenarios and is scalable. The annotators meticulously design the evaluation suite to ensure the accuracy and robustness of the evaluation. We develop the DA-Agent baseline. Experiments show that although the baseline performs better than other existing frameworks, using the current best LLMs achieves only 30.5% accuracy, leaving ample room for improvement. We release our benchmark at https://da-code-bench.github.io.", "sections": [{"title": "Introduction", "content": "Data science is pivotal in extracting insights from data (Wang et al., 2021), fundamentally shaping decision-making and knowledge discovery. Traditionally, this field has required high proficiency in programming and specialized knowledge, which poses significant barriers to non-experts. However, the rapid advancement of Large Language Models (LLMs) (OpenAI, 2023; Anthropic, 2024; Team et al., 2023) has greatly enhanced their capabilities in code generation, grounding, and planning. This raises an intriguing question: Can LLMs become"}, {"title": "Data Science Agent Task", "content": "In this section, we introduce the data science agent task and categories of tasks for DA-Code."}, {"title": "Task Definition", "content": "The traditional coding task can be represented as:\n$code = f(C, I)$\nwhere code is the result of a function f that translates contextual information C (environmental factors, constraints) and specific instructions I (requirements, tasks) into executable code.\nIn our coding task, interaction with the environment involves iterative code modification. The following sets are defined: S (state space), A (action space), O (observation space), C (code space), and H (history space) defined as $H : A \\times C \\times O$. The process can be represented as follows:\nAction Generation. The agent takes the memory $m_t \\in H$ and current state $s_t \\in S$ to generate the next action $a_{t+1} \\in A$ and the updated code $code_{t+1} \\in C$. We use the function $f^* : H \\times S \\rightarrow$"}, {"title": "Task Categories", "content": "DA-Code focuses on data science tasks and categorizes them into three major categories: data wrangling, machine learning, and exploratory data analysis.\nData Wrangling. Data wrangling is the process of transforming and mapping raw data from one form into another to prepare it for analysis. It involves cleaning, loading, and transforming raw data into a more usable format. This can include handling missing values, correcting errors, and merging datasets from different sources. The goal of"}, {"title": "DA-Code", "content": "In this section, we describe the statistical information and construction pipeline of DA-Code."}, {"title": "Challenging Tasks and Diverse Data", "content": "In Table 1 and 2, we conduct a statistical analysis where the distribution of tasks among DW, ML, and EDA is in a 1:1:3 ratio. In contrast, DA-Code integrates a diverse array of agent tasks across the entire data science pipeline, covering a broad spectrum of task types and data types (Figure 7), and result formats (such as Tables, Databases, Charts, Text, etc.). During the annotation process, our experienced annotators also categorize the difficulty of each task into three levels: easy, medium, and hard. Additionally, each example in DA-Code involves multiple files, averaging 5.7 files per task. This setup more closely mirrors real data analysis scenarios."}, {"title": "Complex Solution", "content": "As shown in Table 1, we curate solution codes for each example, requiring an average of 85 lines of code to complete the tasks. Unlike previous benchmarks, DA-Code uses a controllable executable environment to construct complex coding tasks that require interaction with the environment, planning and coding to complete tasks. Many tasks require the use of languages like SQL and Python, which aligns closely with real-world data science analysis scenarios."}, {"title": "Evaluation Suite", "content": "We meticulously develop an accompanying evaluation suite that ensures a comprehensive and systematic assessment of the LLM-Agent performance on DA-Code.\nData Standardization. For each data type, we implement carefully designed scripts to extract standardized information essential for evaluation. For tables, we do not compare the entire table but instead extract specific columns. For charts, we identify plotting scripts (e.g., plot.py) and use scripts to extract both numerical data and plotting parameters, which are then stored in numpy and JSON formats. For text-based outputs, we parse them into JSON format for comparison.\nEvaluation Configure. The evaluation setup for each task is customized through a specific configuration, providing flexibility and ease in managing multiple tasks within the evaluation suite. Each task is uniquely identified, and necessary evaluation details, including output files, metrics, and options, are defined to meet the diverse requirements of different tasks. This structured approach enhances the efficiency and accuracy of the evaluation process.\nScore Calculation. Building on the evaluation suite, we develop a scoring methodology to assess LLM-Agent performance across various outputs, including tables, charts, and machine learning predictions. Each output type has tailored metrics for comprehensive"}, {"title": "Annotation Pipeline", "content": "We recruit ten annotators who are highly proficient in data analysis, SQL, and Python to carry out data collection and annotation. As shown in Figure 2, the data annotation pipeline consists of the following steps:\nManually Selecting Data Source. The data must come from actual data analysis and engineering projects. We require the datasets to be genuinely large and real, not simulated tables or texts. The data source must meet four principles: (1) real-world relevance, (2) complexity, (3) timeliness, and"}, {"title": "DA-Agent", "content": "To effectively address the challenges of the DA-Code benchmark, we develop an LLM-based agent, depicted in Figure 1, which operates within a versatile and robust framework designed for dynamic interaction and execution with the environment."}, {"title": "Environment", "content": "Inspired by the work of Yang et al. (2024b), the environment of DA-Agent is built on the Docker"}, {"title": "Action Space", "content": "Previous approaches typically define actions in terms of editing or executing files. However, in our system, we innovatively combine these stages into single, streamlined actions that edit and execute code simultaneously. This approach not only reduces the complexity of interactions but also minimizes the number of steps required, thereby saving computational resources and enhancing model comprehension. Our action space is designed to efficiently manage diverse tasks, encompassing the following actions:\n\u2022 Bash(command): Executes single-line bash commands directly. This enables quick file and directory manipulation and system command execution, providing direct interaction with the operating system.\n\u2022 Python(save_path, code): Requires path and code content of the Python code, allowing the agent to handle complex data processing tasks and utilize Python's extensive libraries.\n\u2022 SQL(file_path, command, output): Executes SQL queries by specifying the database file, SQL command, and the output format. Results can be saved to a specified file or displayed directly.\n\u2022 Terminate(output): Concludes the task, specifying the result file or output text. This final action ensures that results are summarized and appropriately directed, marking a clear end to the session.\nThis diverse range of actions equips the agent with the capabilities to handle complex tasks across different environments, making it a versatile tool in data manipulation and system operations."}, {"title": "Response Mechanism", "content": "Responses of the agent are categorized into these types based on the system's feedback to executed actions:\n\u2022 Standard Output. The output from successfully executed commands, provides direct feedback or results from the executed actions.\n\u2022 Error Message. In cases where execution fails, error messages are generated to aid in debugging and corrective measures.\n\u2022 Execution Success without Output. Some commands execute successfully without producing visible output, in which case the system simply acknowledges their successful execution.\n\u2022 Unacceptable Action. When the output format does not match the Action format, or the action is the same as the last one, please provide a different action.\n\u2022 Execution Timeout. The action execution time has exceeded the time limit."}, {"title": "Memory Windows", "content": "To manage the context for the agent's operations, a memory window records the history of actions taken, constrained by a max history length parameter. This parameter limits the number of previous steps the agent can recall. If the required context exceeds this limit, the history is automatically truncated to maintain efficient memory management and focus on the most recent relevant actions."}, {"title": "Experiment and Analysis", "content": "In this section, we present the experimental results and analysis of several LLMs evaluated using our DA-Agent baseline on DA-Code benchmark."}, {"title": "Experiment Settings", "content": "We experiment with state-of-the-art LLMs from open-source representatives such as Mixtral-8x22B (Jiang et al., 2024), DeepseekCoder-V2.5 (Zhu et al., 2024), Qwen2.5-72B-Instruct (Team, 2024) and closed-source ones including Claude-3-Opus (Anthropic, 2024) and GPT (OpenAI, 2023) families.\nWe also compare our DA-Agent with three widely-used agent frameworks, namely OpenDevin (OpenDevin Team, 2024), AutoGen (Wu et al., 2023) and X-Agent (Team, 2023).\nFor all experiments, we employ a greedy sampling strategy with a maximum step length of 20 and a max history length of up to 15 steps. The action execution time limitation is 300 seconds."}, {"title": "Main Results", "content": "DA-Agent with Different LLMs. In Table 3, we compare the performances of DA-Agent based on advanced LLMs. In Figure 3, we conduct fine-grained performance statistics for the DA-Code categories. From the score results, we can conclude that 1) Existing data agents are far from satisfactory in completing these data science coding tasks."}, {"title": "Ablation Study of DA-Agent", "content": "Reference Plan. DA-Code aims to assess the combined abilities of planning and grounding in LLM-agents. To further investigate the factors affecting model performance, we asked annotators to annotate the reference plan of DA-Code-100, as shown in Figure 4. This type of instruction describes a step-by-step plan for solving a task, serving as a reference for the LLM Agents. Table 4 shows that LLMs improve with a reference plan, highlighting planning ability as a key performance factor. Additionally, annotating reference plans provides valuable resources for research exploration.\nMax History Length. We investigate the impact of max history length on the performance of DA-Agent. As shown in Table 4, using the DA-Code-100 dataset, the model's performance shows minimal change with variations."}, {"title": "Step into Trajectories", "content": "Task Completion Efficiency. We examine the success and incompletion rates across various models over a sequence of steps, as depicted in Figure 5."}, {"title": "EEEA Pattern", "content": "Based on our in-depth analysis of DA-Agent's task-solving steps using different LLMs and the classification of action types detailed in Table 5. As shown in Figure 6, we observe a prevalent Exploration-Execution-Evaluation-Adjustment pattern in the agents' trajectories, which aligns well with our task scenarios. At the beginning of the task, barring instances of action extraction failure, each model tends to prioritize the \"File Viewing\" action to explore file contents and gain an understanding of the environment. As the task progresses, actions related to coding, such as invoking Python or executing SQL queries, become more prevalent. In the later stages of the task, higher-performing models like GPT-4"}, {"title": "Error Analysis", "content": "From our detailed examination of the DA-Agent's actions across various models,"}, {"title": "Related Work", "content": "Code Generation Benchmark As models become increasingly capable, researchers start to build increasingly difficult and general code generation benchmarks. Most coding benchmarks (e.g. SQL-Spider (Yu et al., 2018); Bash - NL2Bash (Lin et al., 2018); Python - HumanEval (Chen et al., 2021); Execution-S3Eval (Lei et al., 2023); Competition code generation (Huang et al., 2024)) frame the coding problem as a sequence-to-sequence problem (from instruction to code). DS-1000 (Lai et al., 2023) and Arcade (Yin et al., 2023) are pioneering works that collected high-quality examples from communities and proposed corresponding data science to define code generation tasks. Intercode (Yang et al., 2024b) was the first to propose defining code generation tasks in an interactive environment. SWE-Bench (Jimenez et al., 2023) proposed numerous repository-level tasks, while MLAgentBench (Huang et al., 2023) defined auto machine learning tasks in an interactive environment. Some researchers have also proposed benchmarks (Xie et al., 2024; Cao et al., 2024) to explore the model's multimodal capabilities in data science and engineering. ML-Bench (Liu et al., 2023) focuses on machine learning bash scripts generation. DA-Bench (Hu et al., 2024) also evaluate agents on"}, {"title": "Conclusion", "content": "We introduce DA-Code, a challenging benchmark designed for agent-based code generation tasks in data science. This benchmark comprises 500 examples characterized by diverse data sources, complex task settings, and an executable environment. We develop DA-Agent, a robust LLM-Agent baseline, to tackle this challenging benchmark. However, experiments reveal that even the most advanced LLMs perform poorly on DA-Code, achieving only about a 30.5% score. Future work will focus on 1) developing a more sophisticated data agent framework, 2) training more effective agents based on open-source LLMs."}, {"title": "Limitations", "content": "DA-Code introduces a challenging benchmark for agent code generation. The current version presents the following limitations: While utilizing a substantial amount of data science data to fine-tune LLMS is meaningful, this approach has not been explored in this paper. Although this work proposes a general benchmark for data science, it warrants more thorough investigation. In future efforts, we plan to delve deeper into the performance of fine-tuning open-source LLMs on DA-Code."}, {"title": "Task Examples", "content": "In this section, we present diverse examples in DA-Code. We have performed a more detailed classification of tasks into three categories: DW, ML, and EDA, as shown in Figure 7. Data wrangling can be divided into data cleaning, data loading, and data transformation.\n\u2022 Data Cleaning: Focuses on enhancing the data's quality by eliminating errors, imputing missing values, normalizing data in databases or raw datasets, and resolving inconsistencies to maintain the accuracy and trustworthiness of the data.\n\u2022 Data Loading: Entails the consolidation of data from diverse sources into a unified storage system, loading data according to specified standards and requirements, enabling streamlined access and consistent analytical practices.\n\u2022 Data Transformation: Involves reformatting and restructuring data to better suit analytical models for targeted analysis."}, {"title": "Data Wrangling Example", "content": "Task Instruction According to the pre-defined database information, transform the data in the data folder and load it into the database.\nVerbose Instruction\n1. Review the predefined schema to identify the tables and columns; note that there are five tables and record their column names.\n2. Check which files are present in the ./data folder.\n3. Examine the file information; since there are no minutes and hours information, data transformation is necessary.\n4. Get trip_minutes and trip_hours by the trip_duration.\n5. The columns for rain and snow do not match with those in the raw data; type conversion is required.\n6. Merge five months of JC data and compare with the schema to identify any missing columns.\n7. Split the table JC by columns to match it with the database schema.\n8. Write SQL insert statements to insert the data into the database."}, {"title": "Machine Learning Task", "content": "Task Instruction This is a dataset for a Bank customer data for churn prediction competition, with the description available in README.md. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.\nVerbose Instruction 1. Load the training, testing, and submission datasets from CSV files.\n2. Check the dimensions and basic statistics of each dataset, including the number of rows, columns, and presence of null values.\n3. Handle missing values in the datasets using appropriate methods such as imputation or removal.\n4. Scale numeric columns to ensure consistent ranges.\n5. Encode categorical text features using TF-IDF vectorization to transform them into numerical representations.\n6. Use a One-hot encoder to encode categorical features to convert them into a format suitable for machine learning models.\n7. Define feature columns for training the model, excluding non-predictive columns.\n8. Utilize CatBoostClassifier within a StratifiedKFold cross-validation framework to train and validate the model, ensuring robustness and performance assessment.\n9. Use the trained model to make predictions, and prepare the submission file by mapping predicted probabilities to the \u2018Exited' column for submission."}, {"title": "Exploratory Data Analysis (EDA)", "content": ""}, {"title": "Visualization", "content": "Task Instruction Create a stacked horizontal bar chart, which illustrates the average days per order stage for the top 10 cities by sales. Save the chart as 'result.png' with settings from 'plot.yaml'.\nVerbose Instruction\n1. Check Available Resources and Current Directory: View the resources provided and examine the contents of the current directory.\n2. Database Content Review: Read what is contained in the database and identify the tables present.\n3. Identify Top 10 Cities by Sales: To determine the top 10 cities by sales, join the \u2018orders' and 'customers' tables using the \u2018customer_id'. Record the names of these cities.\n4.Create an SQL query to evaluate order processing times in the top 10 cities by joining the \u2018orders\u2019 and 'customers' tables using \u2018customer_id'. Calculate average durations for key milestones in the order process and include only orders from these cities, identified possibly via a subquery based on order volumes. Group and display results by \u2018customer_city', showing averages for each stage.\n5. Read Plot Configuration: Load the \u2018plot_config.yml' file to review plotting requirements.\n6. Create a Pie Chart of Average Order Processing Times: Prepare a summarized DataFrame, configure the pie chart with appropriate labels and colors, enhance its aesthetics with a title and legend, and then save and display the result."}, {"title": "Data Manipulation", "content": "Task Instruction Utilize the Open Food Facts database. Identify the list of ingredients and their countries of origin, and record the results in the ingredient_origins.csv file.\nVerbose Instruction\n1. Read in the avocado data. Read the avocado data from a tab-delimited CSV file. Subset the DataFrame to include only a smaller number of relevant columns.Read in the relevant category tags for avocados from a text file.\n2. Filter avocado data using relevant category tags - Drop rows with null values in the \u2018categories_tags' column. Convert the 'categories_tags' column from comma-separated strings to lists. Filter the DataFrame to keep only rows with relevant category tags.\n3. Determine the top origin country for UK avocados. Filter the avocado DataFrame for rows where 'countries' equals \"United Kingdom\". Count and order the unique values in the \u2018origins_tags' column. Identify the top country of origin for avocados in the UK. Lean up the country string to remove any leading characters or hyphens.\n4. Create a user-defined function for ingredient analysis Create a function called 'read_and_filter_data()' that: Takes a filename and a list of relevant categories as arguments.Performs the same steps as above to read, subset, filter, and analyze the data. Returns the top country of origin for the ingredient.\n5. Analyze other ingredients. Use the relevant categories data to determine the top origin countries for olive oil and sourdough by calling the \u2018read_and_filter_data()' function"}, {"title": "Statistical Analysis", "content": "Task Instruction You have a Statistical thinking dataset, with details described in the README.md file. Calculate 10,000 bootstrap replicates of the variance in annual rainfall at the Sheffield Weather Station. Divide the data into 50 bins, compute the bin center and corresponding probability density function (PDF) for each bin. For convenience, convert the variance to units of square centimeters. Save the results to a file named result.csv, following the template provided in sample_result.csv. (Set the random seed to 42)\nVerbose Instruction\n1. Bootstrap Helper Functions:\nDefine a function \u2018bootstrap_replicate_1d(data, func)' to generate a bootstrap replicate of 1D data. Define another function 'draw_bs_reps(data, func, size=1)' to draw multiple bootstrap replicates.\n2. Data Preparation:\nRead the weather station CSV file considering it is space-delimited and does not have a header. Assign appropriate column names.\nRemove the first several rows if it contains non-numeric data.\nConvert the year column to integers and rain column to floats, handling conversion errors gracefully and dropping any resulting NaN values.\nCompute the total annual rainfall by grouping data by year and summing the rain values for each year. Convert the resulting annual rainfall sums to a NumPy array.\n3. Bootstrap Analysis:\nGenerate 10,000 bootstrap replicates of the variance of annual rainfall using the \u2018draw_bs_reps' function. Adjust the variance units if needed (e.g., put the variance in units of square centimeters).\n4. Probability Density Function (PDF):\nCreate a histogram of the bootstrap replicates with 50 bins, normalized to represent a PDF. Calculate the center points for each bin and the corresponding PDF values.\nStore the bin centers and PDF values in a DataFrame.\n5. Save Results."}, {"title": "Data Insights", "content": "Task Instruction What strategies could be implemented at electric vehicle charging stations to better accommodate the high volume of users and long-duration charging sessions observed at popular locations and peak times?\nVerbose Instruction\n1. Access Data Directory: Start by accessing the directory where data files are stored, including CSVs,"}, {"title": "Experiments Details", "content": "This section includes a detailed agent action space in Table 5."}]}