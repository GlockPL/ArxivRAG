{"title": "GraphTool-Instruction: Revolutionizing Graph Reasoning in LLMs through Decomposed Subtask Instruction", "authors": ["Rongzheng Wang", "Shuang Liang", "Jiasheng Zhang", "Qizhi Chen", "Ke Qin"], "abstract": "Large language models (LLMs) have been demonstrated to possess the capabilities to understand fundamental graph properties and address various graph reasoning tasks. Existing methods fine-tune LLMs to understand and execute graph reasoning tasks by specially designed task instructions. However, these Text-Instruction methods generally exhibit poor performance. Inspired by tool learning, researchers propose Tool-Instruction methods to solve various graph problems by special tool calling (e.g., function, API and model), achieving significant improvements in graph reasoning tasks. Nevertheless, current Tool-Instruction approaches focus on the tool information and ignore the graph structure information, which leads to significantly inferior performance on small-scale LLMs (less than 13B). To tackle this issue, we propose GraphTool-Instruction, an innovative Instruction-tuning approach that decomposes the graph reasoning task into three distinct subtasks (i.e., graph extraction, tool name identification and tool parameter extraction), and design specialized instructions for each subtask. Our GraphTool-Instruction can be used as a plug-and-play prompt for different LLMs without fine-tuning. Moreover, building on GraphTool-Instruction, we develop GTools, a dataset that includes twenty graph reasoning tasks, and create a graph reasoning LLM called GraphForge based on Llama3-8B. We conduct extensive experiments on twenty graph reasoning tasks with different graph types (e.g., graph size or graph direction), and we find that GraphTool-Instruction achieves SOTA compared to Text-Instruction and Tool-Instruction methods. Fine-tuned on GTools, GraphForge gets further improvement of over 30% compared to the Tool-Instruction enhanced GPT-3.5-turbo, and it performs comparably to the high-cost GPT-40. Our codes and data are available at https://anonymous.4open.science/r/GraphTool-Instruction/.", "sections": [{"title": "1 Introduction", "content": "Although Large Language Models (LLMs) excel in fields such as natural language processing, they encounter significant challenges when dealing with graph data. Graph structures exhibit high connectivity, rich combinatorial properties and Non-Euclidean characteristics, making their processing fundamentally different from traditional text or image data [13, 29]. Research [28] shows that LLMs possess a basic understanding of graph properties and the capabilities to solve graph reasoning tasks, but their accuracy remains considerably deficient because of two challenges.\n\u2022 Challenge 1 Graph Understanding (GU): Can LLMs really understand the correct topology information of the graph with natural language?\n\u2022 Challenge 2 Graph Processing (GP): Can LLMs really have the ability to solve the graph algorithm (e.g., Shortest Path Problem) with generative model inference?\nTo enhance the GP ability of LLMs, researchers explore the Text-Instruction approaches which initially begin with the Chain of Thought (CoT) [30]. This method emphasizes that LLMs can solve graph problems through step-by-step reasoning. However, the performance improvement by CoT is limited in GU challenge when dealing with complex graph reasoning tasks. Recent studies have proposed methods that rely on providing additional graph information prompts [5] or explicit reasoning steps [3, 18] to enhance the LLMs' GU ability (see Figure 1 a). While these methods show improvements within their specific prompted tasks, they may compromise their effectiveness in out-of-domain tasks.\nSome researchers realize that LLMs cannot deal with all professional problems with only generative model inference. Tool- former [22] first introduces the concept of tool learning into LLMs, which not only generates text but also can call on and utilize specific tools to solve more complex and professional problems. Inspired by"}, {"title": "2 Related Works", "content": "Amidst the proliferation of LLMs, there is growing scholarly interest in integrating these computational frameworks with graph data [4, 11, 12, 35, 37]. On the one hand, practical performance assessments are conducted by some researchers, investigating whether LLMs have the capabilities of reasoning on graphs such as NLGraph [28], GPT4Graph [8] and GraphInstruct [18]. On the other hand, several studies, exemplified by Talk like a Graph [5], explore the impact of different graph description languages on LLMs' understanding of graph data. These studies inspire researchers to develop a comprehensive method that enables LLMs to solve various types of graph reasoning tasks."}, {"title": "3 Method", "content": "In this section, we formally illustrate the GraphTool-Instruction with three components Graph-Instruction, Task-Instruction and Parameter-Instruction oriented towards three subtasks: graph extraction, tool name identification and tool parameter extraction. Based"}, {"title": "3.1 GraphTool-Instruction Construction", "content": "We select eleven classic graph reasoning tasks and generate examples for both directed and undirected graphs for each task. The list of tasks is presented in Table 2 and the detailed descriptions are in Appendix C. Notably, we define the Maximum Triangle Sum as finding the triangle with the largest sum of edge weights, therefore this task is exclusive to undirected graphs. While Topological Sorting is specific to directed graphs. In total, we identify twenty distinct graph reasoning tasks. For these graph reasoning tasks, we classify them into two primary categories as shown in Figure 2. The first category is Basic Graph Analysis Task (BGA-Task), which generally requires information on the graph structure and the tool name such as Cycle Detection. The second category is Parametric Graph Query Task (PGQ-Task) with the necessity for additional parameter inputs, e.g., the Shortest Path necessitates the starting and ending nodes.\nWe propose GraphTool-Instruction in addressing graph reasoning tasks with three components: Graph-Instruction, Task-Instruction and Parameter-Instruction. We employ Graph-Instruction and Task-Instruction as shared components for both BGA-Task and PGQ-Task. Graph-Instruction is designed to extract the graph structure information, whereas Task-Instruction is designed to enable LLMs to identify the tool names and extract parameters associated with these tools. It should be noted that, while the tool names identified from Task-Instruction are highly accurate, the associated tool parameters frequently suffer from issues such as omissions, incorrect order and non-compliance with the required execution formats. Consequently, for PGQ-Tasks, we propose Parameter-Instruction to enable LLMs to output tool parameters in a more accurate and standardized format based on the tool name provided by Task-Instruction. Therefore, these three instructions can be summarized serving three subtasks: graph extraction, tool name identification and tool parameter extraction.\nGraph-Instruction (for GU): This instruction is designed to enable LLMs to extract graph structure information from the given tasks. To assess and enhance the capabilities of LLMs in processing graphs of various sizes, we establish a benchmark using the commonly accepted maximum token length of 4096 for current LLMs. This threshold serves to categorize graph sizes into Within Limit Graph (WL-Graph) and Exceeds Limit Graph (EL-Graph). WL- Graph ensures that the entire graph can be directly input into LLMs in textual form. Meanwhile, EL-Graph accommodates larger graph structures and we store the graph in files with file paths provided to LLMs.\nConsequently, we have elaborately crafted two types of Graph-Instruction. For WL-Graph, we employ a Two-shot Prompt of extracting graph information for both weighted and unweighted"}, {"title": "3.2 GTools Construction", "content": "Graph Generation: The graph in tasks is generated based on the number of nodes and the probability of connections between two nodes. Furthermore, we develop specific generation rules tailored to ensure a reasonable distribution of both graph and corresponding answers:\n\u2022 Diverse sizes: For WL-Graph, we ensure that graphs cover a node range from 2 to 40 and an edge range for a maximum of 300 to examine the LLMs' capabilities to handle graph information."}, {"title": "3.3 Fine-tune GraphForge", "content": "We develop GraphForge by fine-tuning on the GTools dataset using Llama3-8B as the base model. The fine-tuning process employs Low- Rank Adaptation (LoRA) [10], a parameter-efficient fine-tuning method that is effective for LLMs. The GTools used for training consist of triples as follows:\n$D = {{(I^{(s)}_i, x^{(s)}_i,y^{(s)}_i) \\ s \\in S} \\ | \\ i \\in {1,2,..., |D|}\\}$,\nwhere $I^{(s)}$ is the instruction for subtasks, $x^{(s)}$ is the graph reasoning task, and $y^{(s)}$ is the expected output for the input $x^{(s)}$ under the instruction $I^{(s)}$.\nFormally, for a linear layer represented by $h^{(s)} = Wx^{(s)}$ where $x^{(s)}$ is a combination of the graph reasoning task $x^{(s)}$ and the instruction $I^{(s)}$, $W \\in R^{d_m\\times d_n}$ is the weight matrix. LoRA introduces a low-rank update as follows:\n$h^{(s)} = x^{(s)} + \\Delta W^{(s)} = x^{(s)} + BAx^{(s)}$,\nwhere $h^{(s)}$ is the actual output by LLMS, $A \\in R^{r\\times d_n}$ and $B\\in R^{d_m\\times r}$ are the low-rank matrices, r is the chosen rank significantly smaller than min ($d_m$, $d_n$), and $ \\alpha$ controls the magnitude of the updates to the original weight matrix W. During the learning process, only the matrices A and B are updated.\nThe cross-entropy loss function measures the discrepancy between the actual labels and the predicted labels. By comparing the LLMs' output $h_i^{(s)}$ with the expected output $y_i^{(s)}$, we define the loss function for fine-tuning the GraphForge as:\n$L(D) = \\frac{1}{|D|} \\Sigma_{(I^{(s)},x^{(s)},y^{(s)}) \\in D} cross-entropy(h_i^{(s)}, y_i^{(s)})$, \n$cross-entropy (h_i^{(s)}, y_i^{(s)}) = - \\Sigma_{j=1}^{d_m} y_i^{(s)} (j) logh_i^{(s)} (j)$.\nDuring the training process, a learning rate of 1e-5 and a warmup ratio of 0.1 are utilized. The batch size is set to 4, and a cosine learning rate scheduler is implemented to adjust the learning rate cyclically. The GraphForge is fine-tuned over 3 epochs on an 80GB NVIDIA A800 GPU."}, {"title": "4 Experiments", "content": "We conduct extensive experiments to evaluate our method and model, i.e., GraphTool-Instruction and GraphForge, covering both in-domain and out-of-domain tasks. We implement GraphTool-Instruction on three open-source LLMs to validate the effectiveness of our method. Our experiment is designed to answer the following three research questions:\n\u2022 RQ1 (Main results): How does GraphTool-Instruction perform compared to Text-Instruction and Tool-Instruction methods?\n\u2022 RQ2 (Ablation Study): How does GraphTool-Instruction address two challenges (i.e., Graph Understanding (GU) and Graph Processing (GP)) mentioned in section 1?\n\u2022 RQ3 (Error Analysis): What are the reasons for the errors in the tool's execution results?"}, {"title": "4.1 Experimental Setting", "content": "Test Datasets: According to the existing generation rules of GTools, we have created an additional 500 test instances for each task as our test dataset. Besides, we select Path Existence, Cycle Detection, Topological Sorting, Maximum Flow, and Shortest Path tasks from NLGraph. Since GraphForge has not been trained on NL-Graph, we directly combine the test dataset and training dataset of NLGraph for testing. In addition, we select two tasks from NLGraph: Bipartite Graph Matching and GNN, as out-of-domain tasks to validate the effectiveness of GraphForge. (The GNN task in NLGraph is defined as: Given an undirected graph and a two-dimension node embedding for each node, update the node embedding with the sum of all the neighbors' embeddings.)\nBaselines: We carefully select baseline models from four categories for a comprehensive evaluation. We summarize the methods and their base models in Table 3:\n\u2022 Closed-source LLM: We selected the most powerful closed- source LLMs currently available. In our experiments, we use the CoT method with a Two-shot prompt based on two series of"}, {"title": "4.2 Experimental Setup", "content": "We use an NVIDIA A800 GPU to fine-tune GraphForge and Graph-Toolformer based on LoRA. For inference with all open-source LLMs, we employ a total of 16 NVIDIA Tesla T4 GPUs. For all closed-source LLMs, we leverage the official API interfaces. During inference process, we set the max number of new tokens to 4096, with a sampling parameter top_p of 1 and a temperature of 0.7."}, {"title": "4.3 MainResults (RQ1)", "content": "In this experiment, we first evaluate GraphForge compared to state-of-the-art baselines based on Text-Instruction. Due to LLMs' capabilities constraints, we conduct experiments solely on WL-Graph for all LLMs listed in Table 4. Subsequently, we evaluate GraphForge against Tool-Instruction methods. For all LLMs listed in Table 5, experiments are conducted on both WL-Graph and EL-Graph. We employ answer accuracy as the evaluation metric, which calculates the percentage of questions that the LLM correctly predicts out of the total questions in the test dataset. We demonstrate ten types of graph reasoning tasks, and the complete experiment results are in Appendix A. We observe the following results:\n\u2022 GraphForge demonstrates graph reasoning capabilities that significantly surpass those of all Text-Instruction methods across various tasks, achieving an average performance of 98.4%, in contrast to the highest average of 46.2% recorded by other methods.\n\u2022 Compared with Graph-ToolFormer, GraphForge has consistently surpassed by over 40% on both WL-Graph and EL-Graph. This marked improvement is primarily due to the inherent limitations of the Graph-ToolFormer's method in accurately identifying graph reasoning tasks and extracting tool parameters.\n\u2022 In comparison to GPT-40-FC, GraphForge performs comparably to the high-cost GPT-40-FC on both WL-Graph and EL-Graph."}, {"title": "4.4 Ablation Studies (RQ2)", "content": "In this study, we conduct experiments to assess the effectiveness of Graph-Instruction for challenge GU and Parameter-Instruction for challenge GP. We select two BGA-Tasks and two PGQ-Tasks for detailed analysis of the results in Table 7."}, {"title": "4.5 Error Analysis (RQ3)", "content": "To systematically analyze the shortcomings of Tool-Instruction methods and our method in WL-Graph, we select the Shortest Path as representative. We manually examine the results and classify them into five categories:\n\u2022 Correct: When the final result matches the answer, the current result is considered correct.\n\u2022 Syntax Error: A Syntax Error is considered to have occurred in the following two scenarios: (1) For Function Calling, if the argument tag returns empty; (2) The predefined regular expression encounters parsing errors for graphs, names or parameters.\n\u2022 Graph Mismatch: A Graph Mismatch is considered to have occurred when the graph extracted by LLMs does not match the graph in the task, even if the answer is correct.\n\u2022 Name Mismatch: A Name Mismatch is considered to have occurred when the tool name identified by LLMs does not match the given name in the task, even if the answer is correct.\n\u2022 Para Mismatch: A Parameter Mismatch is considered to have occurred when there is a mismatch in parameters, even if the answer is correct.\nWe show the statistics in Figure 5 and error cases in Appendix F. Among the four methods, the distributions of error types are different:\n\u2022 GLM4-0520-FC and GPT-3.5-turbo-FC exhibit similar distributions of error types. However, there are significant differences in the causes of these errors. For GLM4-0520-FC, during the Function Calling API invocation process, even setting the API retry count to 5 to eliminate network-related reasons, there are multiple instances where the arguments return null. We speculate that this is due to the internal parameter parsing mechanism of GLM4-0520 not handling graph structure information with weights specifically, leading to numerous Syntax Errors. While GPT-3.5-turbo-FC behaves differently. Both GPT-3.5-turbo-FC and GPT-4-turbo-FC's output contains significant information gaps and omissions even based on a two-shot prompt for graph structure extraction, leading to various Syntax Errors. However, we have discovered the impressive ability of GPT-40-FC to follow"}, {"title": "5 Conclusion", "content": "In this work, we propose an innovative GraphTool-Instruction to enhance LLMs' graph reasoning capabilities. Experiment results have demonstrated the robustness of our method, which notably surpasses existing Text-Instruction and Tool-Instruction instruction methods. We have also proposed our dataset GTools, which encompasses twenty graph reasoning tasks, further enhancing the capabilities of LLMs in reasoning on graph tasks. Three new evaluation metrics: Graph, Tool Name and Tool Parameter are employed to ensure the reliability of our dataset. Furthermore, our model GraphForge fine-tuned on the GTools, showcases outstanding performance, achieving an accuracy of more than 98% accuracy. Our future work aims to design a framework that can accommodate a greater variety of graph tools and enhance the capabilities of LLMs to tackle real graph reasoning challenges such as Recommendation and Knowledge Graph."}, {"title": "6 Acknowledgement", "content": "This work is supported by National Natural Science Foundation of China (No.62406057, No. 62176046), Fundamental Research Funds for the Central Universities No.ZYGX2023K010 and Noncommunicable Chronic Diseases-National Science and Technology Major Project (2023ZD0501806)."}, {"title": "B Regular Expressions for Information Extraction", "content": "We define the Graph extract regular expression as:\n$(\\d+), (\\d+).\nFor tasks with edges that have weights, we add weight to the regular expression to extract the starting and ending nodes of the edges as well as the edge weights. Specifically, for the maximum flow problem, we use capacity instead of weight:\n$(\\d+), (\\d+), {'weight':\\s*(\\d+)}, \n$(\\d+), (\\d+), {'capacity':\\s*(\\d+)}.\nWe define the tool name extraction regular expression as:\nAPI_name:\\s*(\\w+|\\n\\s*\\w+)\".\nFor parameter extraction, we define a general template, the number of parameters to be extracted will be adjusted based on the needs of the task:\n(?:source\\s*=\\s*(\\d+)[,\\s]*target\\s*=\\s*(\\d+)).\nAdditionally, to address the potential omission of parameter names during the model's output process, we design a general template to enhance the accuracy of parameter extraction.\n(?:(?:G,\\s*(\\d+),\\s*(\\d+)."}, {"title": "C Detailed Graph Reasoning Tasks Definition", "content": "The details of twenty graph reasoning tasks are listed in Table 10."}, {"title": "D Detailed Version of LLMs", "content": "Table 11 presents the detailed version of both open-source and closed-source LLMs used in our experiments."}, {"title": "E Examples of GraphTool-Instruction", "content": "Based on three components: Graph-Instruction, Task-Instruction and Parameter-Instruction. We exemplify the details of how Graph-Forge solves the corresponding three subtasks: graph extraction (in Figure 6, 7), tool name identification (in Figure 8) and tool parameter extraction (in Figure 9)."}, {"title": "F Error Analysis", "content": "In this section, we present error cases about the Tool-Instruction methods and our method. Figure 10, 11, 12 present the errors of GLM-0520-FC. It should be noted that due to GLM-0520-FC's inability to extract graph edges with weights, we use a one-shot prompt to let GLM-0520-FC output edge list in a triplet form. Figure 13 shows the omission of the graph edges and tool parameters from GPT-3.5-turbo which causes Syntax Errors. Figure 14 shows the Mismatch and Syntax Error of Graph-Toolformer. Figure 15 shows some errors of GraphForge."}]}