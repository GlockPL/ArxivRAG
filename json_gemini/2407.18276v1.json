{"title": "Rome was Not Built in a Single Step: Hierarchical Prompting for LLM-based Chip Design", "authors": ["Andre Nakkab", "Sai Qian Zhang", "Ramesh Karri", "Siddharth Garg"], "abstract": "Large Language Models (LLMs) are effective in computer hardware synthesis via hardware description language (HDL) generation. However, LLM-assisted approaches for HDL generation struggle when handling complex tasks. We introduce a suite of hierarchical prompting techniques which facilitate efficient stepwise design methods, and develop a generalizable automation pipeline for the process. To evaluate these techniques, we present a benchmark set of hardware designs which have solutions with or without architectural hierarchy. Using these benchmarks, we compare various open-source and proprietary LLMs, including our own fine-tuned Code Llama-Verilog model. Our hierarchical methods automatically produce successful designs for complex hardware modules that standard flat prompting methods cannot achieve, allowing smaller open-source LLMs to compete with large proprietary models. Hierarchical prompting reduces HDL generation time and yields savings on LLM costs. Our experiments detail which LLMs are capable of which applications, and how to apply hierarchical methods in various modes. We explore case studies of generating complex cores using automatic scripted hierarchical prompts, including the first-ever LLM-designed processor with no human feedback.", "sections": [{"title": "1 Introduction", "content": "Hierarchical design is a key concept for creating complex computer hardware in an organized fashion. The goal of hierarchy is to break complex modules into manageable submodules, the way one might define a function in high-level code. However, recent efforts into LLM-based generation of hardware description language (HDL)"}, {"title": "2 Background and Related Work", "content": ""}, {"title": "2.1 An Introduction of LLM Operation for Text Generation", "content": "Transformer-based deep neural networks (DNN) have enabled advances across a wide range of domains, excelling in language-related tasks. LLMs operate by processing text inputs structured as tokens, When presented with a sequence of input tokens, LLMs output a probability distribution spanning the complete vocabulary to predict next token in the sequence. This process repeats until a full sequence of tokens, referred to as a completion, is produced. Consider an LLM $P_{\\Theta}(\\hat{y}|x)$, where $\\Theta$ denotes the set of model parameters, x and y represent the vector of input and output tokens. The LLM will generate the probability distribution for the next token $y_n$, and output y = {$y_n|1 \\leq n \\leq N$} is produced autoregressively:\n\n$y_n = arg \\max_{v \\in V} P_{\\Theta}(v/x, y_{<n})$\n\n$1 \\leq n \\leq N$, N is the length of the output, and V is the set of vocabulary. Equation 1 continues iterating until a designated end-of-sequence token is encountered."}, {"title": "2.2 Related Work", "content": "In recent years, LLMs have demonstrated their proficiency in code generation for software programming languages like C and Python [4, 6, 7, 13, 14, 16, 21]. This is possible because LLMs are trained using extensive datasets of code that encompass either one programming language or a combination of multiple languages. Training datasets used can be substantial in size, reaching hundreds of gigabytes of text. Inputs supplied to these LLMs come in various formats, including instructions, comments, code excerpts, or some combination thereof. LLMs can further be tasked to generate hierarchical high-level models [8]. These approaches use Chain-of-Thought prompting to improve LLM reasoning by granularizing problems into sub-problems. This technique improves performance on mathematical word problems [22]. Thus, LLMs are capable of understanding the functional intent of the design task at hand when the task is hierarchically structured.\nIt is possible to use an LLM to conversationally generate synthesizeable HDL at the processor-scale using feedback from a human"}, {"title": "3 Hierarchical Prompting", "content": "Standard flat prompting involves straightforwardly asking the LLM to generate your desired module. This is effective for small, simple modules, but will often lead to messy, incorrect outputs when applied to more complex hardware structures. The goal of introducing architectural hierarchy via prompting is to allow an LLM to mimic the design process employed by a human engineer. Rather than writing every step of a given hardware element sequentially, we break it down into functional components and pick out reusable blocks that are simpler to produce, and slot them into the greater design."}, {"title": "3.1 Sources of Hierarchy", "content": "Hierarchical prompting can take a few forms depending on the resources available to the user. As a baseline, we can utilize a human-defined hierarchy as an input to the pipeline, which results in solely LLM-generated Verilog and no human involvement beyond the planning stage. This is effective when there are specific design constraints for the final module that the LLM might not recognize on its own. We refer to this method as human-driven hierarchical prompting (HDHP).\nAt its most automated, hierarchical prompting can be used to generate our final module purely from the outputs of the LLM. We can describe some desired module and ask the LLM to give us a breakdown of the necessary blocks. From there, we can ask the LLM to generate the next block in the sequence. We refer to this method as purely generative hierarchical prompting (PGHP). This is effective for the more common hierarchical modules, as information about them likely appears in standard LLM training datasets, but is a very difficult task when applied to rarely implemented modules.\nOur benchmark presumes HDHP-based design methods by default, as knowledge of the hierarchy allows us to create effective unit tests for each submodule in order to fully implement our feedback loop. Conversely, when utilizing PGHP the submodules which the LLM selects are consistent across runs. We can work around this by including a thorough, human-written testbench for the top module whose inputs & outputs we know, and then optionally allowing the LLM to generate its own unit tests as we go."}, {"title": "3.2 Hierarchical Generation Pipeline", "content": "As seen in Figure 1, we formulate an automatable design pipeline which implements hierarchical HDL generation to create a complete hierarchical hardware module from end to end. Our proposed method broadly works in three phases:\nHierarchy Extraction. In Step 1, we extract a list of submodules necessary to implement the design from a natural language description provided by the user. Extraction of submodules could be from a list provided by the user in HDHP mode, or extracted from the LLM in the more challenging PGHP mode.\nSubmodule Implementation. Next, we iterate through the extracted list of submodules, and in each iteration, ask the LLM to produce HDL for one submodule (Step 2 in Figure 1). If unit tests for submodules are provided, the generated HDL is simulated via an HDL simulator, in our case Icarus Verilog (iVerilog) as it is open-source and easy to automate. Errors from the simulation output are then extracted and fed back to the LLM with an automated request to fix the design, forming a feedback loop that iteratively corrects errors in a given submodule. (Steps 3, 4, 5 in Figure 1). Once a submodule passes tests, the generated code is logged, and we proceed to generate the next submodule (Steps 6,7). If no unit tests are provided as in the case of PGHP mode, the first generated submodule instance is picked.\nTop-Level Module Integration. Finally, we request the LLM to integrate all generated submodules into a top-level module, which in turn has its own testbench. It is then put through the same tool-based feedback loop as the submodules, before finally being output as a completed hierarchical design. This is even possible in PGHP mode, as we know the expected behavior of the top-level module. We consider the run a success if the top module passes its testbenches."}, {"title": "3.3 Prompting Structure and Techniques", "content": "Within the pipeline, we employ several methods to ensure that our prompts are informative at each step. We begin with a system prompt which precedes all prompting with every model regardless of what module is being generated. This seeks to reduce output randomness by setting constraints for the LLMs to abide by. Our system prompt was structured as:\n\nWe then use a benchmark-specific global prompt describing the overall design objective, akin to non-hierarchical approaches and, in the HDHP mode, we iteratively append a submodule prompt, one for each submodule in the list provided by the designer. Therefore, the first prompt provided to the LLM for a 64-to-1 multiplexer designer looks as below, where the top-level prompt is in blue and the first submodule prompt is in green. Note that for each submodule, we only provide its name and the submodule interface to the LLM."}, {"title": "4 Methods and Model Selection", "content": "We selected eight relevant LLMs that are at the cutting edge of the field. Table 1 shows the full list of models selected and their results on each of the module benchmarks. Of those eight, six are open-source. For those open-source models, all benchmarking inference was run on a single NVIDIA A100 80GB GPU. Inference for the GPT models was run using the OpenAI API. We lock two important model parameters during inference: temperature, which is commonly thought of as a \"creativity\" value and determines how random the LLMs token generation is, and top-p, which sets a probability threshold for generated tokens, allowing only those tokens above the threshold to be selected from the probability distribution. Higher temperature and lower top-p lead to more random outputs, and vice-versa. Temperature values for each model were locked at 0.5, and top-p for each model was set at 0.9.\nWe chose to test the unspecialized Llama 2 [20], and the more recent Llama 3 [1] models to see how generalist open-source LLMs compare to specialized models. We hypothesized that even these models would see considerable performance improvements via hierarchical prompting. As a more specialized option, we include the Code Llama [16] model which is fine-tuned on code. It is effective at HDL generation despite not being its intended purpose. We test a pair of LLMs fine-tuned for Verilog generation, namely VeriGen 16b [18] and RTL-Coder [11]. We elected not to use the baseline models for each of these, as generalist Llama models of similar size are included in our list. Finally, we fine-tuned our own Code Llama-Verilog model to make the already competitive baseline Code Llama more effective. Our open-source contenders were compared against GPT-3.5 Turbo, GPT-4 black-box LLMs [15]."}, {"title": "5 Results and Evaluation", "content": "We ran each benchmark using the selected LLMs for 10 iterations per model-method-module combination to get a more statistical sense of how each model performs. The NH experiments still utilized the tool feedback loop for error fixing, but had no hierarchy applied to prompting. Ten iterations per module were allowed. We tracked both the pass@k values and the wall-clock time it took to generate the outputs. The pass@k metric is defined as the likelihood that one or more of the top-k LLM-generated modules will pass the testbench [4]. Mathematically:\n\n$pass@k = 1 - \\frac{\\binom{n-c}{k}}{\\binom{n}{k}}$\n\nn is the number of generation attempts, c is the number of correct attempts that pass testing, and k is success threshold.\nTable 1 shows the pass@k values for each model-method-module permutation on the benchmark. Hierarchical prompting boosts performance on complex designs, and especially so on on weaker models that fail completely with standard NH prompting. Consider the performance of Llama-2, which is not intended for code generation, much less HDL. Without hierarchical prompting, it fails at every task on the benchmark as it often cannot generate Verilog syntax. It fails on simpler submodules like 8-to-1 multiplexers. However, when guided hierarchically, it has the potential to succeed at even some complicated tasks."}, {"title": "5.1 Purely Generative Results", "content": "We implemented PGHP techniques for our benchmarks, but saw consistent failure for models except for GPT-4. To diagnose the source, we selected 3 of the simplest benchmarks, and used a subset of our LLMs to generate 20 hierarchies each and evaluated them against the golden hierarchy plan from the HDHP version. Most LLMs performed inconsistently on this task, missing key submodules or inserting extraneous submodules (Table 2).\nOn the other hand, we find that GPT-4 significantly improves over flat prompting with PGHP, as shown in Table 3. PGHP is able to generate valid implementations for Systolic Array and UART on which GPT-4 fails completely in flat NH mode. Further, we see substantial gains in accuracy for the three simpler benchmarks. As we will see next, PGHP with GPT-4 is also successful in automatically designing a single-cycle MIPS processor."}, {"title": "5.2 Identifying Common Failure Modes", "content": "We often see errors when LLMs generate text for too long and lose the original context of their goal. This occurs both for conversational and text-completion LLMs. We circumvented this by requesting no additional elements be generated via our system prompt, and re-inputting earlier context as a global prompt. Once a task is completed, text-completion LLMs tend to hallucinate. A common example is the unnecessary generation of testbenches or a random additional module. This is avoided by truncating outputs at a useful end-token, usually the \"endmodule\" in Verilog.\nConversational LLMs can fall into \u201cperseverative\" loops, a termed borrowed from neurology [3], continuing to repeat actions or words when the stimulus that brought on those behaviors has stopped, or when a competing stimulus has occurred that would normally trigger new behavioral routes. One example is the continued use of an unnecessary always block when writing barrel shifter with non-hierarchical prompting, which can occur even when the LLM receives direct/detailed human feedback. As seen in Figure 5 in the Appendix, the LLM will confirm it has done as asked, while continuing to output the same syntax as before. Avoiding such behavioral loops is another benefit of hierarchical prompting."}, {"title": "6 Case Studies and Processor Generation", "content": "To stress test our techniques, we hierarchically generated a full MIPS 16-bit single-cycle processor using GPT-3.5 and our Code Llama-Verilog model based on the PGHP paradigm. Flat prompting is unable to approach a functional processor design without considerable human oversight [2], but we hypothesized that hierarchy would bridge this gap and allow for automation. We tasked each model to first define a hierarchical structure for the processor as a list of submodules, then generate the processor stepwise.\nThe models generated most necessary submodules, but missed key elements and struggled with assigning wire and signal names uniformly across modules, as seen in prior experiments. Tool feedback was helpful, but insufficient to bridge these issues. Ensuring all input and output wires/signals were named appropriately required human intervention and certain submodules like the control unit had to be directly requested, but all functional components were LLM-generated. After these interventions, we were able to synthesize the processors in Vivado, and successfully simulate processor instructions. We repeat this process by generating a RISC-V 32-bit processor utilizing GPT-4. Many of the issues present in GPT-3.5 are less problematic in GPT-4, and required much less human intervention. The newer model is better at wiring up interconnected modules and produces detailed descriptions of hierarchical architectures.\nTo fully test this capability, we implemented the PGHP technique once more to generate another MIPS core via GPT-4 with no human"}, {"title": "7 Conclusion", "content": "In this paper, we have proposed and evaluated Hierachical Prompt-ing as a key tool for automated HDL code generation for complex modules. We show that with hierarchical prompting, even smaller fine-tuned LLMs can correctly generate HDL for complex modules, when traditional flat prompting fails. On powerful models like GPT-4, hierarchical prompting is even more impressive, enabling the automatic generation of a single-cycle MIPS core. Overall, these methods give considerable insight into the potential of LLMs with either manually specified or automatically extracted design hierarchy.\nThere is considerable potential in this line of inquiry. We hope to include additional hardware design methods as part of a larger pipeline in the future. Considering the successes of methods like high-level synthesis (HLS), it stands to reason that leveraging different tools for different tasks could further improve results. We plan to fine-tune additional models with hierarchy in mind. Careful training dataset formulation could potentially lead to models which excel at hierarchical tasks, and may bridge the gap on PGHP performance for smaller models. We hope to expand evaluation resources for future benchmarking efforts to increase the strength of our pass@k metric, ideally n = 200 samples per test."}]}