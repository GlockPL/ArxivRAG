{"title": "Rome was Not Built in a Single Step: Hierarchical Prompting for LLM-based Chip Design", "authors": ["Andre Nakkab", "Sai Qian Zhang", "Ramesh Karri", "Siddharth Garg"], "abstract": "Large Language Models (LLMs) are effective in computer hardware synthesis via hardware description language (HDL) generation. However, LLM-assisted approaches for HDL generation struggle when handling complex tasks. We introduce a suite of hierarchical prompting techniques which facilitate efficient stepwise design methods, and develop a generalizable automation pipeline for the process. To evaluate these techniques, we present a benchmark set of hardware designs which have solutions with or without architectural hierarchy. Using these benchmarks, we compare various open-source and proprietary LLMs, including our own fine-tuned Code Llama-Verilog model. Our hierarchical methods automatically produce successful designs for complex hardware modules that standard flat prompting methods cannot achieve, allowing smaller open-source LLMs to compete with large proprietary models. Hierarchical prompting reduces HDL generation time and yields savings on LLM costs. Our experiments detail which LLMs are capable of which applications, and how to apply hierarchical methods in various modes. We explore case studies of generating complex cores using automatic scripted hierarchical prompts, including the first-ever LLM-designed processor with no human feedback.", "sections": [{"title": "1 Introduction", "content": "Hierarchical design is a key concept for creating complex computer hardware in an organized fashion. The goal of hierarchy is to break complex modules into manageable submodules, the way one might define a function in high-level code. However, recent efforts into LLM-based generation of hardware description language (HDL) code [11, 18, 19] generate modules non-hierarchically, i.e., as single blocks of straight-line code. Although these methods succeed on simple designs like bit-parallel adders and shift registers [17], they struggle on complex designs in recent benchmarks, such as finite-state machines (FSM), large-scale many-to-1 multiplexers, and larger arithmetic blocks [10]. Since straight-line code blocks for complex designs are longer than the hierarchical alternatives, they may hallucinate [9]; the LLM generates incorrect or unrelated text. Additionally, long outputs increase response latency and sometimes fail due to output length limits.\nIn this paper, we develop and evaluate hierarchical prompting techniques to facilitate automated generation of modular HDL code. We explore hierarchical Verilog generation in two major modalities, each occurring in the real-world. In the human-driven mode, the prompt contains a human-proposed hierarchy that the LLM must extract and implement, as well as iterative compiler feedback from unit tests for each submodule. In the more challenging purely generative mode, the LLM gets only a basic (non-hierarchical) prompt and therefore must make its own design decisions to implement the target module. We implement an 8-stage pipeline to automate these techniques in a generalizable fashion. This allows an LLM to closely emulate human HDL development practices.\nExisting benchmark suites like VerilogEval [10] and RTLLM [12] do not address hierarchy. We introduce a new benchmark suite of complex modules with explicit hierarchical solutions, including associated prompts and testbenches for both the top-level modules, and unit tests for submodules. The target modules in our benchmark pose unique challenges to LLMs. These include a 32-bit data-dependent left rotation (also known as a barrel shifter) that requires rarely-seen syntax which is difficult to generate from scratch; an Advanced"}, {"title": "2 Background and Related Work", "content": ""}, {"title": "2.1 An Introduction of LLM Operation for Text Generation", "content": "Transformer-based deep neural networks (DNN) have enabled advances across a wide range of domains, excelling in language-related tasks. LLMs operate by processing text inputs structured as tokens, When presented with a sequence of input tokens, LLMs output a probability distribution spanning the complete vocabulary to predict next token in the sequence. This process repeats until a full sequence of tokens, referred to as a completion, is produced. Consider an LLM $P_\\phi(y|x)$, where $\\phi$ denotes the set of model parameters, x and y represent the vector of input and output tokens. The LLM will generate the probability distribution for the next token $y_n$, and output y = {$y_n$|1 \u2264 n \u2264 N} is produced autoregressively:\n$y_n = arg \\max_{y\\in V} P_\\phi(v/x, y_{<n})$                                             (1)\n1 \u2264 n \u2264 N, N is the length of the output, and V is the set of vocabulary. Equation 1 continues iterating until a designated end-of-sequence token is encountered."}, {"title": "2.2 Related Work", "content": "In recent years, LLMs have demonstrated their proficiency in code generation for software programming languages like C and Python [4, 6, 7, 13, 14, 16, 21]. This is possible because LLMs are trained using extensive datasets of code that encompass either one programming language or a combination of multiple languages. Training datasets used can be substantial in size, reaching hundreds of gigabytes of text. Inputs supplied to these LLMs come in various formats, including instructions, comments, code excerpts, or some combination thereof. LLMs can further be tasked to generate hierarchical high-level models [8]. These approaches use Chain-of-Thought prompting to improve LLM reasoning by granularizing problems into sub-problems. This technique improves performance on mathematical word problems [22]. Thus, LLMs are capable of understanding the functional intent of the design task at hand when the task is hierarchically structured.\nIt is possible to use an LLM to conversationally generate synthesizeable HDL at the processor-scale using feedback from a human hardware designer [2]. These methods are relatively expedient, but struggle with consistency and are difficult to evaluate due to the subjective and non-reproducible nature of human feedback. Human intervention also precludes automation. It has alternatively been shown once in the past that one can automate the generation of entire CPUs using traditional deep learning methods [5]. However, these methods take on the order of multiple hours to successfully train and produce a given RTL design for tape-out. Finally, some success has been found by fine-tuning open-source LLMs specifically to produce HDL [11, 18]. Benchmarks have recently been developed to evaluate these LLMs on their Verilog generation performance [10, 12]. Based on these benchmarks, even bespoke fine-tuned models struggle to compete with powerful proprietary LLMs like GPT-3.5 and GPT-4."}, {"title": "3 Hierarchical Prompting", "content": "Standard flat prompting involves straightforwardly asking the LLM to generate your desired module. This is effective for small, simple modules, but will often lead to messy, incorrect outputs when applied to more complex hardware structures. The goal of introducing architectural hierarchy via prompting is to allow an LLM to mimic the design process employed by a human engineer. Rather than writing every step of a given hardware element sequentially, we break it down into functional components and pick out reusable blocks that are simpler to produce, and slot them into the greater design."}, {"title": "3.1 Sources of Hierarchy", "content": "Hierarchical prompting can take a few forms depending on the resources available to the user. As a baseline, we can utilize a human-defined hierarchy as an input to the pipeline, which results in solely LLM-generated Verilog and no human involvement beyond the planning stage. This is effective when there are specific design constraints for the final module that the LLM might not recognize on its own. We refer to this method as human-driven hierarchical prompting (HDHP).\nAt its most automated, hierarchical prompting can be used to generate our final module purely from the outputs of the LLM. We can describe some desired module and ask the LLM to give us a breakdown of the necessary blocks. From there, we can ask the LLM to generate the next block in the sequence. We refer to this method as purely generative hierarchical prompting (PGHP). This is effective for the more common hierarchical modules, as information about them likely appears in standard LLM training datasets, but is a very difficult task when applied to rarely implemented modules.\nOur benchmark presumes HDHP-based design methods by default, as knowledge of the hierarchy allows us to create effective unit tests for each submodule in order to fully implement our feedback loop. Conversely, when utilizing PGHP the submodules which the LLM selects are consistent across runs. We can work around this by including a thorough, human-written testbench for the top module whose inputs & outputs we know, and then optionally allowing the LLM to generate its own unit tests as we go."}, {"title": "3.2 Hierarchical Generation Pipeline", "content": "As seen in Figure 1, we formulate an automatable design pipeline which implements hierarchical HDL generation to create a complete hierarchical hardware module from end to end. Our proposed method broadly works in three phases:\nHierarchy Extraction. In Step 1, we extract a list of submodules necessary to implement the design from a natural language description provided by the user. Extraction of submodules could be from a list provided by the user in HDHP mode, or extracted from the LLM in the more challenging PGHP mode.\nSubmodule Implementation. Next, we iterate through the extracted list of submodules, and in each iteration, ask the LLM to produce HDL for one submodule (Step 2 in Figure 1). If unit tests for submodules are provided, the generated HDL is simulated via an HDL simulator, in our case Icarus Verilog (iVerilog) as it is open-source and easy to automate. Errors from the simulation output are then extracted and fed back to the LLM with an automated request to fix the design, forming a feedback loop that iteratively corrects errors in a given submodule. (Steps 3, 4, 5 in Figure 1). Once a submodule passes tests, the generated code is logged, and we proceed to generate the next submodule (Steps 6,7). If no unit tests are provided as in the case of PGHP mode, the first generated submodule instance is picked.\nTop-Level Module Integration. Finally, we request the LLM to integrate all generated submodules into a top-level module, which in turn has its own testbench. It is then put through the same tool-based feedback loop as the submodules, before finally being output as a completed hierarchical design. This is even possible in PGHP mode, as we know the expected behavior of the top-level module. We consider the run a success if the top module passes its testbenches."}, {"title": "3.3 Prompting Structure and Techniques", "content": "Within the pipeline, we employ several methods to ensure that our prompts are informative at each step. We begin with a system prompt which precedes all prompting with every model regardless of what module is being generated. This seeks to reduce output randomness by setting constraints for the LLMs to abide by. Our system prompt was structured as:"}, {"title": "4 Methods and Model Selection", "content": "We selected eight relevant LLMs that are at the cutting edge of the field. Table 1 shows the full list of models selected and their results on each of the module benchmarks. Of those eight, six are open-source. For those open-source models, all benchmarking inference was run on a single NVIDIA A100 80GB GPU. Inference for the GPT models was run using the OpenAI API. We lock two important model parameters during inference: temperature, which is commonly thought of as a \"creativity\" value and determines how random the LLMs token generation is, and top-p, which sets a probability threshold for generated tokens, allowing only those tokens above the threshold to be selected from the probability distribution. Higher temperature and lower top-p lead to more random outputs, and vice-versa. Temperature values for each model were locked at 0.5, and top-p for each model was set at 0.9.\nWe chose to test the unspecialized Llama 2 [20], and the more recent Llama 3 [1] models to see how generalist open-source LLMs compare to specialized models. We hypothesized that even these models would see considerable performance improvements via hierarchical prompting. As a more specialized option, we include the Code Llama [16] model which is fine-tuned on code. It is effective at HDL generation despite not being its intended purpose. We test a pair of LLMs fine-tuned for Verilog generation, namely VeriGen 16b [18] and RTL-Coder [11]. We elected not to use the baseline models for each of these, as generalist Llama models of similar size are included in our list. Finally, we fine-tuned our own Code Llama-Verilog model to make the already competitive baseline Code Llama more effective. Our open-source contenders were compared against GPT-3.5 Turbo, GPT-4 black-box LLMs [15]."}, {"title": "5 Results and Evaluation", "content": "We ran each benchmark using the selected LLMs for 10 iterations per model-method-module combination to get a more statistical sense of how each model performs. The NH experiments still utilized the tool feedback loop for error fixing, but had no hierarchy applied to prompting. Ten iterations per module were allowed. We tracked both the pass@k values and the wall-clock time it took to generate the outputs. The pass@k metric is defined as the likelihood that one or more of the top-k LLM-generated modules will pass the testbench [4]. Mathematically:\n$pass@k = 1 - {n-c \\choose k} / {n \\choose k}$                                                                 (2)\nn is the number of generation attempts, c is the number of correct attempts that pass testing, and k is success threshold.\nTable 1 shows the pass@k values for each model-method-module permutation on the benchmark. Hierarchical prompting boosts performance on complex designs, and especially so on on weaker models that fail completely with standard NH prompting. Consider the performance of Llama-2, which is not intended for code generation, much less HDL. Without hierarchical prompting, it fails at every task on the benchmark as it often cannot generate Verilog syntax. It fails on simpler submodules like 8-to-1 multiplexers. However, when guided hierarchically, it has the potential to succeed at even some complicated tasks."}, {"title": "5.1 Purely Generative Results", "content": "We implemented PGHP techniques for our benchmarks, but saw consistent failure for models except for GPT-4. To diagnose the source, we selected 3 of the simplest benchmarks, and used a subset of our LLMs to generate 20 hierarchies each and evaluated them against the golden hierarchy plan from the HDHP version. Most LLMs performed inconsistently on this task, missing key submodules or inserting extraneous submodules (Table 2).\nOn the other hand, we find that GPT-4 significantly improves over flat prompting with PGHP, as shown in Table 3. PGHP is able to generate valid implementations for Systolic Array and UART on which GPT-4 fails completely in flat NH mode. Further, we see substantial gains in accuracy for the three simpler benchmarks. As we will see next, PGHP with GPT-4 is also successful in automatically designing a single-cycle MIPS processor."}, {"title": "5.2 Identifying Common Failure Modes", "content": "We often see errors when LLMs generate text for too long and lose the original context of their goal. This occurs both for conversational and text-completion LLMs. We circumvented this by requesting no additional elements be generated via our system prompt, and re-inputting earlier context as a global prompt. Once a task is completed, text-completion LLMs tend to hallucinate. A common example is the unnecessary generation of testbenches or a random additional module. This is avoided by truncating outputs at a useful end-token, usually the \"endmodule\" in Verilog.\nConversational LLMs can fall into \u201cperseverative\" loops, a termed borrowed from neurology [3], continuing to repeat actions or words when the stimulus that brought on those behaviors has stopped, or when a competing stimulus has occurred that would normally trigger new behavioral routes. One example is the continued use of an unnecessary always block when writing barrel shifter with non-hierarchical prompting, which can occur even when the LLM receives direct/detailed human feedback. As seen in Figure 5 in the Appendix, the LLM will confirm it has done as asked, while continuing to output the same syntax as before. Avoiding such behavioral loops is another benefit of hierarchical prompting."}, {"title": "6 Case Studies and Processor Generation", "content": "To stress test our techniques, we hierarchically generated a full MIPS 16-bit single-cycle processor using GPT-3.5 and our Code Llama-Verilog model based on the PGHP paradigm. Flat prompting is unable to approach a functional processor design without considerable human oversight [2], but we hypothesized that hierarchy would bridge this gap and allow for automation. We tasked each model to first define a hierarchical structure for the processor as a list of submodules, then generate the processor stepwise.\nThe models generated most necessary submodules, but missed key elements and struggled with assigning wire and signal names uniformly across modules, as seen in prior experiments. Tool feedback was helpful, but insufficient to bridge these issues. Ensuring all input and output wires/signals were named appropriately required human intervention and certain submodules like the control unit had to be directly requested, but all functional components were LLM-generated. After these interventions, we were able to synthesize the processors in Vivado, and successfully simulate processor instructions. We repeat this process by generating a RISC-V 32-bit processor utilizing GPT-4. Many of the issues present in GPT-3.5 are less problematic in GPT-4, and required much less human intervention. The newer model is better at wiring up interconnected modules and produces detailed descriptions of hierarchical architectures.\nTo fully test this capability, we implemented the PGHP technique once more to generate another MIPS core via GPT-4 with no human intervention. After iterative tool feedback, GPT-4 converged on a synthesizeable processor that covered a version of the full MIPS ISA. Design and simulation results are shown in the Appendix. Figure 6 shows the RTL and Figure 7 shows a waveform for this PGHP-sourced processor.\nWe posit that this is the first-ever purely LLM-designed processor. That is, the design decisions were made entirely by the LLM with no human input, and all error handling was done automatically with tool feedback. Beyond the initial prompt of \"Please define the necessary submodules in a 16-bit single cycle MIPS processor,\" no human design intervention was required. We also see that the time taken to generate our processors is on the order of minutes, rather than multiple hours as seen in past methods. The time taken to complete the PGHP-based processor was 23 minutes, 37.85 seconds.\nIn order to get a sense of financial cost savings, we calculate the price-per-token and number of tokens generated when applying our hierarchical pipeline to GPT-3.5. We compare results for our full multiplexer hierarchy, our 32-bit barrel shifter, our MIPS processor, and our RISC-V processor. Table 4 contains the full cost analysis. As one might expect, complex modules lead to higher costs and achieve more financial savings when generated hierarchically."}, {"title": "7 Conclusion", "content": "In this paper, we have proposed and evaluated Hierachical Prompt-ing as a key tool for automated HDL code generation for complex modules. We show that with hierarchical prompting, even smaller fine-tuned LLMs can correctly generate HDL for complex mod-ules, when traditional flat prompting fails. On powerful models like GPT-4, hierarchical prompting is even more impressive, en-abling the automatic generation of a single-cycle MIPS core. Overall, these methods give considerable insight into the potential of LLMs with either manually specified or automatically extracted design hierarchy.\nThere is considerable potential in this line of inquiry. We hope to include additional hardware design methods as part of a larger pipeline in the future. Considering the successes of methods like high-level synthesis (HLS), it stands to reason that leveraging dif-ferent tools for different tasks could further improve results. We plan to fine-tune additional models with hierarchy in mind. Care-ful training dataset formulation could potentially lead to models which excel at hierarchical tasks, and may bridge the gap on PGHP performance for smaller models. We hope to expand evaluation resources for future benchmarking efforts to increase the strength of our pass@k metric, ideally n = 200 samples per test."}]}