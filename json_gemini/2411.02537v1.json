{"title": "INQUIRE: A Natural World Text-to-Image Retrieval Benchmark", "authors": ["Edward Vendrow", "Omiros Pantazis", "Alexander Shepard", "Gabriel Brostow", "Kate E. Jones", "Oisin Mac Aodha", "Sara Beery", "Grant Van Horn"], "abstract": "We introduce INQUIRE, a text-to-image retrieval benchmark fdesigned to challenge-multimodal vision-language models. on expert-level queries INQUIRE includes iiNaturalist 2024 (iNat24) a new dataset of five million natural world images, along with 250 expert-level retrieval queries. These queries are paired with all relevant images comprehensively labeled within iNat24, comprising 33,000 total matches. Queries span categories such as species identification, context, behavior, and appearance, emphasizing tasks that require nuanced image understanding and domain expertise. Our benchmark evaluates two core retrieval tasks: (1) INQUIRE-FULLRANK, a full dataset ranking task, and (2) INQUIRE-RERANK, a reranking task for refining top-100 retrievals. Detailed evaluation of a range of recent multimodal models demonstrates that INQUIRE poses a significant challenge, with the best models failing to achieve an mAP@50 above 50%. In addition, we show that reranking with more powerful multimodal models can enhance retrieval performance, yet there remains a significant margin for improvement. By focusing on scientifically-motivated ecological challenges, INQUIRE aims to bridge the gap between Al capabilities and the needs of real-world scientific inquiry, encouraging the development of retrieval systems that can assist with accelerating ecological and biodiversity research.", "sections": [{"title": "1 Introduction", "content": "Recent advances in multimodal learning have resulted in advanced models [60; 43; 3] that demonstrate remarkable generalization capabilities in zero-shot classification [60; 83], visual question-answering (VQA) [39; 80; 4; 40], and image retrieval [80; 40]. These models offer the potential to assist in the exploration, organization, and extraction of knowledge from large image collections. However, despite this success, there remains a significant gap in the evaluation of these models on domain-specific, expert-level queries, where nuanced understanding and precise retrieval are critical. Addressing this gap is essential for future deployment in specialized fields such as biodiversity monitoring and biomedical imaging, among other scientific disciplines.\nPrevious studies of the multimodal capabilities of this new generation of models have primarily focused on the task of VQA. In VQA, it has been demonstrated that there remains a large performance gap between state-of-the-art models and human experts in the context of challenging perception and reasoning queries such as those found on college-level exams [81; 84]. However, no such expert-level benchmark exists for image retrieval. The most commonly used text-to-image retrieval benchmarks are derived from image captioning datasets, and contain simple queries related to common everyday categories [79; 42]. Current multimodal models achieve near perfect performance on some of these benchmarks, indicating that they no longer pose a challenge (e.g., BLIP-2 [40] scores 98.9 on Flickr30K [79] top-10). Existing retrieval datasets are generally small [58; 59; 79; 42], limited to a single visual reasoning task (e.g., landmark-location matching [58; 59; 74]), and lack concepts that would require expert knowledge [58; 59; 74; 79; 42]. These limitations impede our ability to track and improve image retrieval capabilities.\nA domain that is well-suited for studying this problem is the natural world, where images collected by enthusiast volunteers provide vast and largely uncurated sources of publicly available scientific data. In particular, the iNaturalist [2] platform contains over 180 million species images and contributes immensely to research in biodiversity monitoring [16; 48]. These images also contain a wealth of \"secondary data\" not reflected in their species labels [57], including crucial insights into interactions, behavior, morphology, and habitat that could be uncovered through searches. However, the time-consuming and expert-dependent analysis needed to extract such information prevents scientists from taking advantage of this valuable data at scale. This cost is amplified as scientists typically want to retrieve multiple relevant images for each text query, so that they can track changes of a property over space and time [78]. This domain serves as an ideal testbed for expert image retrieval, as these images contain expert-level diverse and composite visual reasoning problems, and progress in this field will enhance impactful scientific discovery.\nIn this work, we introduce INQUIRE, a new dataset and benchmark for expert-level text-to-image retrieval and reranking on natural world images. INQUIRE includes the iNat24 dataset and 250 ecologically motivated retrieval queries. The queries span 33,000 true-positive matches, pairing each text query with all relevant images that we comprehensively labeled among iNat24's five million natural world images. iNat24 is sampled from iNaturalist [2], and contains images from 10,000 different species collected and annotated by citizen scientists, providing significantly more data for researchers interested in fine-grained species classification. The queries contained within INQUIRE come from discussions and interviews with a range of experts including ecologists, biologists, ornithologists, entomologists, oceanographers, and forestry experts."}, {"title": "2 Related Work", "content": "Vision-Language Models (VLMs). Large web-sourced datasets containing paired text and images have enabled recent advances in powerful VLMs [15; 85]. Contrastive methods such as CLIP [60] and ALIGN [32], among others, learn an embedding space where the data from the two modalities can be encoded jointly. The ability to reason using natural language and images together has yielded impressive results in a variety of text-based visual tasks such as zero-shot classification [60; 83], image captioning [39; 80; 4; 30; 40], and text-to-image generation [53; 7; 61; 64; 8]. However, the effectiveness of these contrastive VLMs for more complex compositional reasoning is bottlenecked by the information loss induced by their text encoders [33].\nThere also exists a family of more computationally expensive VLMs that connect the outputs of visual encoders directly into language models. Models like LLaVA [43; 44], BLIP [39; 40; 21], and GPT-40 [3; 55] have demonstrated impressive vision-language understanding. However, despite their potential for answering complex vision-language queries, these models are not suitable for processing large sets of images at interactive rates, which is essential for retrieval, due to their large computational requirements during inference. In this paper, we do not introduce new VLMs, but aim to better understand the capabilities and shortfalls of existing methods for text-to-image retrieval.\nImage Retrieval. Effective feature representations are essential for achieving strong image retrieval performance. Earlier approaches from image-to-image used hand-crafted features [49; 12] but these have largely been replaced with deep learning-based alternatives [36; 9; 6; 11]. More recently, in the context of text-to-image retrieval, we have seen the adoption of contrastive VLMs [60; 32] trained on web-sourced paired text and image datasets. These models enable zero-shot text-based retrieval and have been demonstrated to exhibit desirable scaling properties as training sets become larger [26; 24]. However, despite the potential of VLMs for image retrieval, their evaluation has been mostly limited to small datasets adapted from existing image captioning benchmarks, such as Flickr30k [79] and COCO [42], which contain just 1,000 and 5,000 images, respectively. Furthermore, recent models are saturating performance on these less challenging datasets, e.g., BLIP-2 [40] scores 98.9 on Flickr30K and 92.6 on COCO top-10 text-to-image retrieval. As most text-to-image benchmarks have been derived from image captioning datasets, each query is a descriptive caption that matches exactly one image. In contrast, real-world retrievals often involve multiple images relevant to a single query, and the query itself typically does not describe every aspect of the images as thoroughly as a caption does. We compare INQUIRE to common text-to-image retrieval datasets in Table 1.\nMore recent datasets have been purpose-built to probe specific weaknesses of retrieval systems, such as compositionality [50; 29; 62], object relationships [82], negation [72; 66], and semantic"}, {"title": "3 The INQUIRE Benchmark", "content": "Here we describe INQUIRE, our novel benchmark for assessing expert-level image retrieval for fine-grained queries on natural world image collections. INQUIRE consists of a collection of 250 queries, where each query is represented as a brief text description of the concept of interest (e.g., \u201cAlligator lizards mating\" [56]), and contains its relevant image matches comprehensively labeled over a dataset of five million natural world images. These queries represent real scientific use cases collected to cover diverse, expert sources including discussions with scientists across environmental and ecological"}, {"title": "3.1 The iNaturalist 2024 Dataset", "content": "As part of the INQUIRE benchmark, we create a new image dataset, which we refer to as iNaturalist 2024 (iNat24). This dataset contains five million images spanning 10,000 species classes collected and annotated by community scientists from 2021\u20132024 on the iNaturalist platform [2]. iNat24 forms one of the largest publicly available natural world image repositories, with twice as many images as in iNat21 [70]. To ensure cross-compatibility for researchers interested in using both datasets, iNat24 and iNat21 have the same classes but do not contain the same images, freeing iNat21 to be used as a training set. The sampling and collection process of iNat24 is in Appendix H."}, {"title": "3.2 Query and Image Collection Process", "content": "Query Collection. To ensure that INQUIRE comprises text queries that are relevant to scientists, we conducted interviews with individuals across different ecological and environmental domains including experts in ornithology, marine biology, entomology, and forestry. Further queries were sourced from reviews of academic literature in ecology [57]. Representative queries and statistics can be seen in Figures 1, 2, and 3. We retained only queries that (1) could be discerned from images alone, (2) were feasible to comprehensively label over the entire iNat24 dataset, and (3) were of interest to domain experts.\nImage Annotation. All image annotations were performed by a small set of individuals whose interest and familiarity with wildlife image collections enabled them to provide accurate labels for challenging queries. Annotators were instructed to label all candidate images as either relevant (i.e., positive match) or not relevant (i.e., negative match) to a query, and to mark an image as not relevant if there was reasonable doubt. To allow for comprehensive labeling, where applicable, iNat24 species labels were used to narrow down the search to a sufficiently small size to label all relevant images for the query of interest. For queries in which species labels could not be used, labeling was performed over the top CLIP ViT-H-14 [24] retrievals alone. In this case, the resulting annotations were only kept if we were certain that this labeling captured the vast majority of positives, including labeling until at least 100 consecutive retrievals were not relevant (see Appendix H). Queries that were deemed too easy, not comprehensively labeled, or otherwise not possible to label were excluded from our benchmark. In total, this process resulted in 250 queries which involved labeling 194,334 images, of which 32,696 were relevant to their query. Further details are in Appendix H.\nQuery Categories. Each query belongs to one of four supercategories (appearance, behavior, context, or species), and further into one of sixteen fine-grained categories (e.g., Animal Structures and Habitats). Figure 2 shows the distribution of query categories, and Figure 3 shows the distribution of iconic groups of the species represented by each query (e.g., Mammals, Birds). We also note queries that use scientific terminology, words typically used only within scientific contexts (e.g., \u201cA godwit performing distal rhynchokinesis\u201d).\nData Split. We divide all queries into 50 validation and 200 test queries using a random split, stratified by category."}, {"title": "3.3 Retrieval Tasks", "content": "We introduce two tasks to address different aspects of the text-to-image retrieval problem. Real-world retrieval implementations often consist of two stages: an initial top-k retrieval with a more computationally efficient method (e.g., CLIP zero-shot using pre-computed image embeddings), followed by a reranking of the top-k retrievals with a more expensive model. To enable researchers to explore both stages, while ensuring that those with more limited computational resources can participate, we follow previous large-scale reranking challenges like TREC [19; 20] by offering both a full dataset retrieval task and a reranking task (see Figure 4)."}, {"title": "4 Retrieval Methods", "content": "The goal of text-to-image retrieval is to rank images from a potentially large image collection according to their relevance to an input text query. Here, we describe the retrieval and reranking methods that we evaluate, covering current state-of-the-art approaches.\nEmbedding Similarity. Models such as CLIP [60] are well suited for the text-to-image retrieval setting as they operate on a joint vision and language embedding space. In this setting, similarity between an image and text query is simply determined by their cosine similarity. The key advantage of embedding models is that the embedding for each image can be pre-computed once offline as they do not change over time. At inference time, only the embedding of the text query needs to be computed and then compared to the cached image embeddings for retrieval. This is helpful as the number of images we wish to search over can be on the order of millions, or even billions [65]. Thus to speed up retrieval, the image embeddings can be pre-computed and indexed using approximate nearest neighbor methods [23], allowing for near-instantaneous retrievals on large collections. This is beneficial both for end-to-end retrieval and as the first step for a multi-stage retrieval approach. We also benchmark recent models such as WildCLIP [25] and BioCLIP [67] which are adapted versions of CLIP that explicitly target natural world use cases.\nReranking with Multimodal Models. Reranking is a common paradigm in text retrieval, where a rapid search through pre-computed document indexes for potential matches is followed by a more expensive reranking of the top retrievals [54; 35; 34]. In the image domain, reranking has been"}, {"title": "5 Results", "content": "Here we present a comprehensive evaluation of retrieval methods on INQUIRE. All results are reported on the test set. Additional results, including on the validation set, are in Appendix E."}, {"title": "5.1 Metrics", "content": "We evaluate using Average Precision at k (AP@k), Normalized Discounted Cumulative Gain (nDCG), and Mean Reciprocal Rank (MRR). We primarily discuss AP as we find that this metric is the most discriminative of model performance. While these metrics have been commonly used to evaluate text retrieval, especially in the context of large-scale document retrieval [71; 19], they have not found use in image retrieval due to the nonexistence of benchmarks like INQUIRE containing many relevant images for retrieval, rather than just one. Thus, we include them in our analysis to encourage their use in future image retrieval research. We note that the utilized AP@k metric uses a modified normalization factor suited to the retrieval setting.\nExisting image retrieval benchmarks typically evaluate using the recall@k metric (e.g., [40]), measuring if any of the top k images are relevant. While this makes sense in the setting where just one image is relevant, INQUIRE has potentially many relevant images and thus, we employ metrics that measure both relevance and ranking of retrievals."}, {"title": "5.2 Fullrank Retrieval Task Results", "content": "We report full retrieval evaluation on INQUIRE in Tables 2 and 3. The per-category performance of selected CLIP models is reported in Figures 5 and 6. Further detailed results are in Appendix E.\nThe best CLIP models leave significant room for improvement. Table 2 shows that the top performing CLIP model achieves a moderate mAP@50 of 35.6. Although scaling models increases"}, {"title": "5.3 Rerank Retrieval Task Results", "content": "The results for the INQUIRE-RERANK task are presented in Table 4, where we evaluate reranking performance of both CLIP-style models like ViT-B-32 and larger vision-language models such as GPT-40. Since the total number of images for each query is small (i.e., 100), we also show the expected results of a random reranking for baseline comparison. In Table 5 we further break down INQUIRE-RERANK results by queries containing scientific terminology and by query supercategory.\nCurrent models struggle with expert-level text-to-image retrieval on INQUIRE. In Table 4 we observe that the highest AP of 59.6, achieved by GPT-40, is far below the perfect score of 100, showing substantial room for improvement. Smaller models like CLIP ViT-B-32 only slightly outperform random chance. Since the top retrieved images are often visually or semantically similar, lower-performing models may be confused into promoting irrelevant images, leading to poorer ranking.\nQueries with scientific terminology are significantly more challenging, showing that models might not understand domains-specific language. For example, the query \u201cAxanthism in a green frog\u201d\u2014 referring to a mutation limiting yellow pigment production, resulting in a blue appearance\u2014 uses specialized terminology that a model may not understand. As a result, a model may incorrectly rank typical green frogs higher than axanthic green frogs, leading to worse-than-random performance. We show the performance of reranking models on queries with scientific terminology in Table 5. Interestingly, GPT-40 appears to be closing this gap, with an average difference of 7 points between queries with and without scientific terminology (AP scores of 53 and 60, respectively), compared to a 16-point difference for the next best model, VILA-40B (AP of 39 and 55). Nevertheless, this gap remains. Future work should explore methods to improve models' comprehension of domain-specific language, which is critical for accurate retrieval in scientific contexts.\nReranking effectiveness varies widely by the query type. Table 5 shows that CONTEXT queries, often requiring general visual understanding, benefit substantially from reranking. Conversely, SPECIES queries, requiring fine-grained visual understanding, see minimal improvement, with the"}, {"title": "6 Limitations and Societal Impact", "content": "While the species labels for each image in iNat24 are generated via consensus from multiple citizen scientists, there may still be errors in the labels which our evaluation will inherit. However, this error rate is estimated to be low [47]. INQUIRE contains natural world images, which while diverse, may hinder the relevance of some of our insights to other visual domains. In spite of this, we believe that due to the wide range of visual queries contained within, progress on INQUIRE will likely be indicative of multimodal model performance on other challenging domains.\nThere could be unintended negative consequences if conservation assessments were made based on the predictions from biased or inaccurate models evaluated in this paper. Where relevant, we have attempted to flag these performance deficiencies. While we have filtered out personally identifiable information from our images, the retrieval paradigm allows for free-form text search and thus care should be taken to ensure that appropriate text filters are in-place to prevent inaccurate or hurtful associations being made between user queries and images of wildlife."}, {"title": "7 Conclusion", "content": "We introduced INQUIRE, a challenging new text-to-image retrieval benchmark which consists of expert-level text queries that have been exhaustively annotated across a large pool of five million natural world images called iNat24. This benchmark aims to emulate real world image retrieval and analysis problems faced by scientists working with these types of large-scale image collections. Our hope is that progress on INQUIRE will drive advancements in the real scientific utility of AI systems. Our evaluation of existing methods reveals that INQUIRE poses a significant challenge even for the current largest state-of-the-art multimodal models, showing there is significant room for innovations to develop accurate retrieval systems for complex visual domains."}, {"title": "G Evaluation Metrics", "content": "Average Precision at k. Average Precision (AP) is a well-known metric computed by taking the weighted mean of precision scores at a set of thresholds. This metric has been adapted to the retrieval setting, where it possible to calculate the Average Precision at k (AP@k) among just the top k retrieved items. Since calculating AP@k requires both the relevance and position of the top k items,\n$AP@k = \\frac{\\sum_{i=1}^{k} P@i \\cdot rel(i)}{NF}$\nwhere $P@i$ is the precision at $i$ (i.e., among the first $i$ items), $rel(i) \\in \\{0,1\\}$ is the binary relevance score, and $NF$ is a normalization factor.\nIn a typical implemenation of AP we would see $NF = r$, the total number of relevant items in the top k. However in a retrieval setting with a total of R relevant items, this normalization technique creates a problematic and unintuitive situation where promoting an item into the top k retrievals can decrease the score.\nIn particular, consider the situation where we have 100 images of which 2 are relevant and 98 are not relevant. Using a normalization factor of $NF = R$, we measure AP@5 for the following two top-5 retrievals:\nOrdered retrieval relevance: (1, 0, 0, 0, 0) $\\Rightarrow$ AP@5 = 1\nOrdered retrieval relevance: (1, 0, 0, 0, 1) $\\Rightarrow$ AP@5 = 0.7\nWe observe that promoting a relevant item into the top 5 resulted in a decreased AP@5, which is undesirable. Our criteria for an AP@ metric is that (1) the measure strictly increases whenever a relevant document is promoted into the top-k, and (2) the has a full range of 0 to 1. Of the range of proposed AP@k variants [10; 27; 71], just [71] meets our desired criteria This modified average precision normalizes using min(k, R). In the case above, we now have NF = min(k,R) = min(5, 2) = 2, yielding:\nOrdered retrieval relevance: (1, 0, 0, 0, 0) $\\Rightarrow$ AP@5 = 0.5\nOrdered retrieval relevance: (1, 0, 0, 0, 1) $\\Rightarrow$ AP@5 = 0.7\nOur end-to-end retrieval evaluations use AP@k with this desirable normalization factor of $NF = min(k, R)$. Since the reranking challenge evaluates solely using the fixed set, the normalization factor for this challenge is always r, the number of relevant items within the top k.\nFor further discussion of Average Precision at k, we refer readers to [18].\nnDCG. Normalized discounted cumulative gain is a weighted ranking metric that considers the relative ordering of the retrieved items. To compute nDCG@k for a single query, first we compute the discounted cumulative gain at k (DCG@K):\n$DCG_k = \\sum_{i=1}^{k} \\frac{rel(i)}{log_2(i + 1)}$\nwhere $rel(i) \\in \\{0, 1\\}$ is the binary relevance score for the ith retrieved item. Then, we define the ideal DCG at k (IDCG@k) as the maximum achievable DCG@k:\n$IDCG_k = \\sum_{i=1}^{min(k, R)} \\frac{rel(i)}{log_2(i + 1)}$\nwhere R is the total number of relevant items for the query. Finally, we can compute nDCG@k as\n$nDCG_k = \\frac{DCG_k}{IDCG_k}$\nwhere the normalization by IDCG@k allows nDCGk to range fully between the interval 0 to 1.\nMRR. Mean reciprocal rank is a measure for the rank of the first correct retrieval. MRR can be computed as\n$MRR = \\frac{1}{Q} \\sum_{i=1}^{Q} \\frac{1}{rank(i)}$\nwhere $Q$ is the number of queries, and rank(i) gives the rank of the first relevant retrieval for the ith query (1 for 1st position, 2 for 2nd position, etc.). If no relevant retrievals are present in the retrieved list, we let rank(i) = \u221e, i.e., 1/rank(i) = 0."}, {"title": "HiNat24 Image Collection and INQUIRE Annotation Protocol", "content": "In this sections we describe in detail our data collection protocol for collecting the iNat24 dataset and annotating the INQUIRE benchmark."}, {"title": "H.1 iNat24 Dataset Curation", "content": "We follow a similar paradigm used to organize the iNaturalist Competition Datasets from 2017 [69], 2018 [1], 2019 [1], and 2021 [70]. For the 2024 version we start from an iNaturalist observation database export generated on 2023-12-30. Observations are then filtered to include only those observed in the years 2021, 2022, or 2023. This ensures the images in iNat24 are unique and do not overlap with images from prior dataset versions (e.g., iNat21 [70] only contains images up until September 2020). To utilize the iNat21 taxonomy (for easy compatibility with that dataset) we detect taxonomic changes between the iNat21 taxonomy and the iNaturalist taxonomy included in the 2023-12-30 database export. We then modify species labels (where necessary) so that observations conform to the iNat21 taxonomy. Some of these taxonomic changes can be quite complicated (splits, merges, etc.) resulting in cases where an iNat21 species is no longer valid, however we are able to recover 9,959 out of the original 10,000 species from iNat21. We then filter to include observations exclusively from the iNat21 taxonomy. Additional filtering ensures that all observations have valid metadata (i.e., location and time information) and that associated image files are not corrupted. These steps result in a candidate set of 33M observations to sample from to build the iNat24 dataset.\nOur process of selecting the set of images to include for each species in the iNat24 dataset deviates from the prior dataset building schemes [69; 70]. Random sampling of observations, or even random sampling from unique users, generates collections of images that are biased towards North America and Europe. To decrease this bias we sample from spatio-temporal clusters of \"observations groups\". Observation groups are formed by grouping observations together if they are observed on the same day within 10km of each other, regardless of the observer. When sampling observations for a species, we cluster their associated observation groups using a spatio-temporal distance metric and then sample one observation per cluster in a round-robin fashion until we hit a desired sample size. When sampling within a cluster, we prioritize novel observation groups and novel users. We sample at most 550 observations per species to include in iNat24. This sampling process results in a total of 4,816,146 images for 9,959 species.\nUnlike previous versions of the iNaturalist dataset, we performed one final round of filtering to remove images that are inappropriate for a research dataset or not relevant for the query. We use the INQUIRE annotation process to find images containing human faces, personally identifiable information, \"empty\" images, images of spectrograms, etc.. We additionally run a RetinaFace [22] Resnet50 face detection model across the entire dataset, and manually inspect all high confidence predictions. In total this filtered out an additional 2,603 images. The final dataset contains 4,813,543 images for 9,959 species.\nThe iNat24 dataset does not have a validation or test split, i.e., all observations are assigned to the train split. The validation and test splits can be used from the iNat21 dataset to benchmark classification performance. As in previous years, we keep only the primary image for each observation, and resize all images to have a max dimension of 500px on the longest side. All images have three channels and are stored as jpegs. We provide location, time, attribution, and licensing information in the associated json file."}, {"title": "H.2 Data Annotation", "content": "Image annotation was performed by a carefully selected team of paid MSc students or equivalent, many with expertise in ecology allowing for labeling of difficult queries. Annotators were instructed to label all candidate images as either relevant (i.e., positive match) or not relevant (i.e., negative match) to the query, and to mark an image as not relevant if there was reasonable doubt as to its"}, {"title": "H.3 Data Format and Structure", "content": "iNat24. iNat24 is provided as a metadata file and a tar file containing all images. The metadata file is given in the commonly used JSON COCO format. The information in this metadata file includes each image's ID, file path, width, height, image license, rights holder, taxonomic classification, latitude, longitude, location uncertainty, and date.\nINQUIRE. The INQUIRE benchmark is provided as a two CSV files. The first is a list of queries, where each row includes fields for the query id, query text, organism category, query category type, and query category. The second file is a list of annotations, where row corresponds includes fields for the query id, image id, and relevance label. The image id can be matched to the iNat24 metadata to get additional information mentioned above, such as the taxonomy, date, and geographic location."}, {"title": "H.4 Ethical Considerations", "content": "Copyright and Licensing. We adhere strictly to copyright and licensing regulations. All images included in the dataset fall under a license allowing copying and redistribution. In particular, all images are licensed under one of the following: CC BY 4.0, CC BY-NC 4.0, CC BY-NC-ND 4.0, CC BY-NC-SA 4.0, CC0 1.0, CC BY-ND 4.0, or CC BY-SA 4.0.\nData Privacy and Safety. Although users approved all images considered for research use, we take further steps to ensure data privacy and safety. We filter all images for content that is contains personally identifiable information or images of people. We do not exclude most images containing gore, as these are often ecologically relevant, e.g., using image of road-killed animals to asses impacts of roads on biodiversity.\nViolations of Rights. We respect the rights of iNaturalist community volunteer observers by constructing iNat2024 using only images and metadata appropriately licensed by their respective creators for copying, distribution, and non-commercial research use. Nevertheless, we bear responsibility in case of a violation of rights.\nParticipant Risks. We received internal ethical approval for our query collection and data labeling (Edinburgh Informatics Ethics Review Panel 951781 and MIT Committee on the Use of Humans as Experimental Subjects Protocol 2404001276)."}, {"title": "H.5 Participant Compensation", "content": "We hired annotators at the equivalent of $15.50 per hour and spent a total of $2325 on annotation."}, {"title": "I Multi-Modal Model Prompting Details", "content": "We include the various prompts used in our evaluation of large multimodal models in Table A5. We note that while we aim to keep the prompt broadly the same across models, they are ultimately different due to different prompting requirements for each model. The proprietary models (GPT-4V and GPT-40) were queries on October 14, 2024."}, {"title": "J Full List of INQUIRE Queries", "content": "Table A6 lists all INQUIRE queries."}, {"title": "K Datasheet", "content": "K.1 Motivation\nFor what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.\n\u2022 The purpose of INQUIRE is to provide a challenging benchmark for text-to-image retrieval\non natural world images. Prior retrieval datasets are small and do not possess a challenge\nfor existing models", "affiliations": "niNaturalist, the Massachusetts Institute of Technology, University College London, Univer-\nsity of Edinburgh, and University of Massachusetts Amherst. The dataset was created from\ndata made publicly available by the citizen science platform iNaturalist [2"}]}