{"title": "R.I.P.  : Better Models by Survival of the Fittest Prompts", "authors": ["Ping Yu", "Weizhe Yuan", "Olga Golovneva", "Tianhao Wu", "Sainbayar Sukhbaatar", "Jason Weston", "Jing Xu"], "abstract": "Training data quality is one of the most important drivers of final model quality. In this work, we introduce a method for evaluating data integrity based on the assumption that low-quality input prompts result in high variance and low quality responses. This is achieved by measuring the rejected response quality and the reward gap between the chosen and rejected preference pair. Our method, Rejecting Instruction Preferences (RIP) can be used to filter prompts from existing training sets, or to make high quality synthetic datasets, yielding large performance gains across various benchmarks compared to unfiltered data. Using Llama 3.1-8B-Instruct, RIP improves AlpacaEval2 LC Win Rate by 9.4%, Arena-Hard by 8.7%, and WildBench by 9.9%. Using Llama 3.3-70B-Instruct, RIP improves Arena-Hard from 67.5 to 82.9, which is from 18th place to 6th overall in the leaderboard.", "sections": [{"title": "1. Introduction", "content": "In large language model (LLM) development, a primary driver for advancing frontier models is curating high-quality training examples. This curation is crucial during both the pretraining (Rae et al., 2021; Touvron et al., 2023a) and post-training (finetuning) phases (Touvron et al., 2023b). Despite the widespread adoption of the \u201cscaling hypothesis\" (Kaplan et al., 2020), merely increasing the size of training datasets does not guarantee improved performance if the data are of low quality (Chen et al., 2024; Li et al., 2024c; Zhou et al., 2024). Without sufficient data quality, model training tends not to be fully robust to the associated noise, and final response quality from the model suffers.\nCurrently, there are a number of investigated techniques to curate data most of which are based on heuristics or model judgments given the training inputs. In this work, we hypothesize that better judgments of data quality can be made by taking into account the model responses on those data. Specifically, if the prompt is of low quality, then responses exhibit high variability and low quality as well. This insight leads us to develop a method for either selecting prompts, or for creating high quality synthetic prompts, both of which yield significant performance gains during post-training.\nOur method, Rejecting Instruction Preferences (RIP), considers the case of instruction finetuning via preference optimization. It starts with a set of preference pairs consisting of input prompts and chosen and rejected responses. RIP considers specific characteristics of the preference pairs, in particular rejected response quality and the reward gap between the chosen and rejected preference pair. If the rejected"}, {"title": "3. Rejecting Instruction Preferences (RIP)", "content": "We start by defining the prompt selection problem in the pairwise preference optimization setting. In this context, we present our proposed prompt-response-pair-based filtering method, which develops key descriptive metrics and their use in filtering training prompts. Lastly, we describe how our method can be applied to self-instruction setups where synthetic prompts are generated from the model itself."}, {"title": "3.1. Data Curation Problem", "content": "The goal of data curation is to remove low-quality prompts that can negatively affect the general instruction following capability of the model. Given a set of prompts X = {x}, we aim to find a subset S C X to be used for fine-tuning a seed LLM M. We consider the preference optimization setting, with winning (chosen) and losing (rejected) response pairs {yw, y\u0131} with rewards r(yw|x) > r(y\u0131|x) for each prompt x. The response pairs and their rewards can come from human preference data, or can be generated from the model itself M and then scored using an external reward model. For the latter we use the \"best-vs-worst\" preference pairing method (Pace et al., 2024), where N responses are sampled, and the ones with highest and lowest rewards are the chosen and rejected, respectively:\n$\\begin{aligned}\n\\{y_i\\}^N_{i=1}\\sim M(x) \\text{ then } \\begin{cases}\n y_w = \\text{argmax}_{y_i} r(y_i|x) \\\\\n y_\\iota = \\text{argmin}_{y_i} r(y_i|x)\n\\end{cases}\n\\end{aligned}$\nWe also consider alternate pairing methods in Section A.4. We then use the preference data {x, yw, Yl}xes for training the model M. Note that our focus is on filtering prompts entirely, not responses to those prompts."}, {"title": "3.2. Hypothesis on Data Selection", "content": "Although preferences are extensively used to train state- of-the-art LLMs, there is limited research on identifying unhelpful training examples in this setting. We posit that analyzing the paired model responses to given input prompts can provide valuable insights into the quality of the prompts. Specifically, we test the following two hypotheses.\nHypothesis 1: Low-quality prompts are likely to produce low-quality responses. Low-quality prompts - for example those that are unclear, ambiguous, or containing conflict- ing information are likely to lead to noisy or inaccurate model responses. While those inaccurate responses can still be used as training targets in pairwise preference optimiza- tion, studies indicate that training on pairs with low-quality rejected responses might be sub-optimal. Yasunaga et al. (2024) for example shows that pairing the best with random responses works well comparing to pairing the best with the worst one with lowest reward. This suggests a potential correlation of the quality of the rejected example with the alignment outcome. Additionally, several studies (Wu et al., 2024b; Zhao et al., 2024a; Yuan et al., 2024a) have found a strong correlation between the length of responses, includ- ing rejected ones, and final performance. Therefore, we consider the reward r(y\u0131|x) and length len(y\u0131) of rejected responses as indicators of quality of the training prompts x, i.e. large values of either of these metrics relative to other examples indicate higher quality.\nHypothesis 2. Low-quality prompts are likely to produce responses with larger variance Low quality prompts introduce uncertainty and ambiguity, leading to a broader range of interpretations. As the model or human generat- ing the response might guess or fill in gaps in the prompt, this results in higher variance in responses. While some responses might align well with the intent, others may devi- ate significantly. A preliminary study in Wu et al. (2024a) finds low-gap pairs, where chosen and rejected responses are similar, are high-quality informative pairs, leading to better performing DPO models. We therefore consider the reward gap r(yw|x) -r(y\u0131|x) as another indicator of quality of a training prompt, i.e. small reward gaps suggest that the prompt has higher quality."}, {"title": "3.3. RIP filtering", "content": ""}, {"title": "3.3.1. RIP FOR EXISTING TRAINING PROMPTS", "content": "Given the above hypotheses, we thus consider the following three metrics mk(X, Yw, Y\u0131) that are based on the responses:\n\u2022 Rejected response reward: m\u2081 = r(yi|x)\n\u2022 Rejected response length: m2 = len(y\u0131)\n\u2022 Reward gap: m3 = r(yw|x) \u2013 r(y\u0131x)\nFor each metric, we define threshold values that can be used for filtering. For the first two metrics, higher values are desired so we choose a lower-bound threshold\nS = {x | Tk \u2264 mk(x, Yw, Y\u0131)}.\nThe last reward gap metric requires an upper threshold as we"}, {"title": "3.3.2. SELF-RIP FOR SYNTHETIC PROMPTS", "content": "Prompt curation by RIP can also naturally be used to generate synthetic data. First, RIP is used to create a seed pool of high-quality prompts. Few-shot examples from this seed pool guide the model to generate training prompts, which can be further filtered by RIP. We thus propose Self-RIP, a new approach to creating high-quality synthetic prompts:\nStep 1. Few-shot prompting with RIP curated instruc- tions We start with the set of prompts S curated by our proposed method RIP as described in Section 3.3.1. To gen- erate new prompts S' we sample from our seed model M following Self-Instruct (Wang et al., 2023; Honovich et al., 2023). For each new example we randomly select 8 prompts from S and feed them as few-shot examples to the model M to generate a prompt with similar characteristics. We apply the exact processing steps in Wang et al. (2023) to new prompts S', such as removing similar prompts (ROUGE-L similarity with any existing instructions < 0.7), and exclud- ing those that contain certain keywords (e.g., image, picture, graph) that usually can not be processed by text-only LLMs.\nStep 2. Filtering with RIP We further apply RIP on top of the synthetically generated prompts S' from the previous step, filtering out the self-instructions using the same thresh- old values as used before. Then the remaining subset S\" is used for training the seed model M.\nNote we use RIP filtering twice here, once in each step. This is to ensure the quality of synthetic prompts. We also explore Self-RIP using a smaller subset of S as seed instruc- tions in Section A.4 as part of our ablation studies."}, {"title": "4. Experimental Setup", "content": "We perform preference optimization using DPO, beginning with the Llama 3.1-8B-Instruct model as our seed model M. We evaluate both the selection and creation of prompts, focusing on two categories: human-written instructions and synthetically generated instructions. Finally, we extend our evaluation of RIP with the Llama 3.3-70B-Instruct model."}, {"title": "4.1. Human-Written Prompts", "content": "For human-written instructions, we specifically investi- gate two setups: human-written input prompts 1) paired with model-generated responses and annotated by a reward model; 2) with existing responses that have been annotated with human-assigned rewards. We use the WildChat and Helpsteer2 datasets, see statistics in Appendix Table 7."}, {"title": "4.1.1. WILDCHAT DATASET", "content": "Prompt Set We start with a large pool of over 250k human-written prompts from the WildChat (Zhao et al., 2024b) dataset. We exclude any non-English prompts based on WildChat annotations, and remove around 70k Midjourney-related instructions\u00b9, yielding 190k unique first- turn prompts. These prompts are collected from real user in- teractions without human annotations, making them highly diverse. While there are many high-quality prompts, there are also a significant number of low-quality ones, such as nonsensical text or those lacking a clear question.\nResponse Generation Following Yuan et al. (2024b); Meng et al. (2024); Wu et al. (2024b) we generate our cho- sen and rejected response pairs on the WildChat prompts using our seed model M to make our setup closer to the on-policy setting. We use best-vs-worst as described in Sec- tion 3.1, generating N responses for each prompt x using M with sampling parameters of T = 0.8, top-p = 0.95.\nReward Annotation We then evaluate candidate re- sponses using two different judges:\n\u2022 Reward Classifier: We used the ArmoRM reward model (Wang et al., 2024a) to score each response.\n\u2022 LLM-as-a-Judge (Zheng et al., 2023): We prompt LLama 3.1-405B-Instruct using the prompt template outlined in Yasunaga et al. (2024) to assign a score ranging from 0 to 10 for each response. For each re- sponse, we conduct 10 independent evaluations and use the average score as the final reward.\nThe training example (x, yw, Y\u0131) is selected by appointing the highest-reward one as yw and the lowest-reward one as yr. For our primary experiments, we use the default value of N = 64. However, results for N = 8, 16, 32 are provided as part of our ablation studies in Table 17, and we use N = 32 for the Llama 3.3-70B-Instruct experiments. We perform early stopping using a validation set of 470 examples: 253 valid set examples from Li et al. (2024c) and 218 examples from the evol-test set of Xu et al. (2023a), with prompts that overlap with AlpacaEval2 removed."}, {"title": "4.1.2. HELPSTEER2 DATASET", "content": "HelpSteer2 (Wang et al., 2024c) consists of around 10k human-written prompts each with a response pair sam- pled from 10 different LLMs. Each response has human-"}, {"title": "4.2. Synthetic Prompts", "content": "In this setup, we generate prompts from the seed model M itself for training instead of using human-written prompts. By varying the set of seed pool prompts used as few-shot examples, we collect two sets of training prompts:\n\u2022 Self-Instruct: randomly select 8-shot examples from the unfiltered WildChat.\n\u2022 Self-RIP: randomly select 8-shot examples from high quality WildChat prompts filtered by RIP.\nIn each case, we create 20k training prompts sampled with decoding parameters T = 0.8, top-p = 0.95. The rest of the setup including response generations and DPO training is exactly the same as the WildChat setup where we use ArmoRM to construct response pairs (yw, y\u0131), and do early stopping on the same validation set of 470 examples."}, {"title": "4.3. Baselines", "content": "We compare our method with the existing methods below. For instruction-tuning data selection methods which handle a single (non-pairwise) response per prompt, we apply them to the chosen responses within the response pairs. Addi- tional details on the implementation of each baseline are provided in Appendix Section A.5."}, {"title": "4.3.1. PROMPT-BASED FILTERING", "content": "InsTag Complexity Lu et al. (2023) leveraged ChatGPT to create semantic and intent-based tags, subsequently fine-tuning an LLM as a data tagger using these tags. They then used the tag counts as a measure of complexity. This is used to filter out prompts with fewer tags to enhance complexity.\nInsTag Diversity The InsTag Diversity filtering method (Lu et al., 2023) characterizes a dataset as more diverse when it includes a greater variety of unique tags, as an- notated by the specified tagger. Using this approach, we greedily filter out data samples whose associated tags are already present in the selected dataset.\nLLM-as-Prompt-Judge Employing LLMs as prompt qual- ity judges has proven its efficacy in curating high-quality data (Chen et al., 2024; Dubey et al., 2024; Liu et al., 2023)."}, {"title": "4.3.2. PROMPT-AND-CHOSEN-RESPONSE-BASED FILTERING", "content": "Perplexity We compute perplexity (ppl) of the chosen response yw with the Llama 3.1-8B Instruct in a zero-shot manner as a filtering metric to curate training prompts. In particular, we retain examples with large ppl (yw|x) values, which may indicate the difficulty of the prompt.\nInstruction-Following Difficulty (IFD) Li et al. (2023a) introduced the IFD to measure the model-specific difficulty of a data sample. A lower IFD score indicates that this par- ticular instruction-response pair is considered relatively easy for the language model to understand and follow without further training. We filter out examples with low IFD metric of a given pair of prompt x and chosen response Yw."}, {"title": "4.3.3. CHOSEN-AND-REJECTED-RESPONSE BASED FILTERING", "content": "Jaccard Similarity In addition to the reward gap between chosen and rejected responses, we explore Jaccard similar- ity, defined as the number of overlapping words divided by the overall word counts, as an alternative similarity mea- surement. We thus filter out examples with low Jaccard similarity scores (i.e. fewer overlapping words) between chosen and rejected response pairs."}, {"title": "4.4. Training And Evaluation Setting", "content": "Following the Instruct setup in Meng et al. (2024), we utilize the DPO training approach with the off-the-shelf LLama 3.1- 8B-Instruct and LLama 3.3-70B-Instruct models, leveraging the fairseq2 library (Balioglu, 2023). We use a batch size of 64 and sweep over learning rates of 5e-7, 1e-6 for the LLama 3.1-8B-Instruct model, and a learning rate of 1e-6 with a batch size of 256 for the LLama 3.3-70B-Instruct model. Both models are trained with a dropout rate of 0.0 and a \u1e9e value of 0.1 throughout the experiments. We conduct RIP with various cutoff thresholds, e.g. at the 25%, 50% and 75% percentile of each metric.\nWe primarily assess models' general instruction-following capabilities on three evaluation benchmarks: AlpacaEval2 (Li et al., 2023b), Arena-Hard (Li et al., 2024b) and Wild- Bench (Lin et al., 2024). These benchmarks cover a wide range of natural yet challenging real-world user queries, and have been widely adopted by the research community."}, {"title": "5. Experiment Results", "content": "Due to the large amount of unfiltered WildChat prompts, we first assess whether standard DPO training saturates as the size of the training prompts grows. As shown in Appendix Figure 2, the Armo Score on the valid set dramatically im- proves as we increase the size of training prompts, and begins to plateau afterwards. This shows growing the size of the training prompts arbitrarily does not bring additional gains, and hence quality control of the preference dataset could be important. We thus focus on 20k unique Wild- Chat prompts, denoted as WildChat-20k for Llama3.1-8B- Instruct experiments, and 40k for Llama 3.3-70B-Instruct.\nWe report Alpaca-Eval2 Length-Controlled (LC) win rate, Arena-Hard score and WildBench WB-Score along with the number of training examples (after filtering if any) using WildChat-20k in Table 2, on HelpSteer2 in Table 4, and on Self-Instruction data in Table 5. Existing filtering methods are provided in Table 2 as baseline comparisons. Further details, such as hyperparameters, are in Appendix Table 8 and Table 9. Our findings lead to several key observations.\nWhen filtering human-written instructions, RIP achieves the best performance on both human-scored and model-scored preference datasets. On the WildChat dataset where pairs are annotated by the ArmoRM model, we conduct RIP with various cutoff thresholds, at the 25%, 50% and 75% percentile of each metric. Our best model is trained on examples with rejected length larger than the 50% percentile of all rejected lengths, and rejected rewards larger than the 50% percentile of all rejected rewards, and reward gap smaller than the 50% percentile. Table 2 shows that RIP significantly improves LC win rate from the LLama3.1-8B- Instruct DPO baseline without filtering from 48.4% to 57.8% by filtering out 77% training examples, surpassing GPT-4 Omni (05/13) on AlpacaEval2. Similarly, RIP scores the highest on Arena-Hard (43.1) compared to LLM-as-Prompt- Judge filtering (42.0), Jaccard Similarity (42.6), and the no filtering baseline (37.9). RIP also achieves the highest WB-score on WildBench (45.6) compared to other filtering and no filtering baselines (41.5). As shown in Appendix Table 8 using LLM-as-a-Judge annotated rewards, RIP also performs well. Finally, Table 4 demonstrates RIP is equally effective on HelpSteer2 where preference pairs are deter- mined by human annotators, achieving the highest scores across all 3 evaluation benchmarks as compared to the base- lines (no filtering and LLM-as-Prompt-Judge filtering).\nRIP scales to different and larger models We also tried RIP on a different base LLM \u2013 from the Llama 3.3 family rather than 3.1, and of a larger scale, 70B rather than 8B. As shown in Table 3, RIP also works on this larger model. Filtering dramatically boosts Llama 3.3-70B-Instruct DPO trained models, with AlpacaEval2 LC win rate improved from 54.3% to 67.7%, Arena Hard from 70.5 to 82.9 and WildBench from 55.3 to 58.8, surpassing SOTA models as shown in Table 1. The prompt filtering threshold we applied to the 70B model was the same as in Llama 3.1-8B-Instruct + RIP (see Appendix Table 9), indicating potential weak-to- strong generalizability (Li et al., 2024a) of our method.\nExisting filtering methods derived from supervised- finetuning do not work as well on preference datasets As demonstrated in Table 2, compared to the baseline WildChat- 20k DPO (no filtering) trained on WildChat 20k prompts without any filtering, existing prompt-based filtering meth- ods such as InsTag-Difficulty, InsTag-Diversity or LLM-as- Prompt-Judge filtering methods all lead to lower win rates on Alpaca-Eval2. LLM-as-Prompt-Judge, while outper- forming certain filtering methods such as InsTag, achieves marginal gains compared to no filtering even though they are facilitated by querying a poweful LLM, Llama 3.1- 405B-Instruct. Out of all the alternative methods tried, Jaccard Similarity based filtering that takes into account response pairs for filtering achieves relatively the highest scores across the 3 benchmarks, indicating that filtering that only takes into account prompts or chosen responses does not generalize well to the pairwise preference case.\nThe Self-RIP method to generate synthetic data outper- forms Self-Instruct data. As shown in Table 5, Self-RIP yields better alignment results across all 3 evaluation bench- marks as compared to those trained on Self-Instruct data. In particular, win rate improves from 49.1% to 60.2% on AlpacaEval2, and from 38.5% to 42.1% on Arena-Hard. This result implies that our method generates better quality instructions than generating via few-shot examples from unfiltered prompts as in Self-Instruct.\nSelf-RIP synthetic data outperforms human-written in- structions In Table 5, models trained on synthetic prompts outperform those trained on 20k human-written WildChat prompts. Applying Self-RIP few-shot generation without post-filtering gives an equal amount of 20k prompts, but still increases the AlpacaEval2 LC win rate from 48.4% to 53.6%, Arena-Hard win rate from 37.9% to 43.7% and WB- Score on WildBench from 41.5 to 44.8. This further illus- trates the importance of training on high-quality instructions. When applying the full Self-RIP method with post-filtering results are further improved, for example achieving the best AlpacaEval2 LC win rate of 60.2%.\nRIP seed data selection and RIP post-filtering are both important for generating Self-RIP synthetic data In Table 5, we perform ablations on Self-RIP. We try: (i) us- ing RIP to select high quality few-shot examples but not for curating the resulting generations (post-filtering); (ii) apply- ing standard (Self-Instruct) few-shot generation, but then applying RIP post-filtering; or (iii) applying RIP to both few-shot generation and post-filtering (our default method)."}, {"title": "6. Understanding why RIP works", "content": "6.1. Filtering prompts with low quality responses\nTo understand what instructions are filtered out, we first visualize instructions with low quality rejected responses (as measured by low reward and short lengths) by compar- ing the t-SNE plots of unfiltered and filtered instructions (shown in Appendix Figure 4). We investigated a few clus- ters present in that t-SNE plot of unfiltered prompts that are missing from the t-SNE plot of filtered ones on the right- hand-side. We find that instructions from those clusters being filtered out from the training set are either obscure, non-sensical, or they fail to elicit meaningful responses from the model, leading to lower-quality rejected responses. Such instructions can be caught by measuring the rewards and lengths of the rejected responses, with supporting evidence given in Appendix Table 25."}, {"title": "6.2. Filtering prompts with larger response variance", "content": "Similarly, we visualize instructions that are filtered out by measuring the reward gap between chosen and rejected re- sponses in Appendix Figure 5, and further expand some representative groups of filtered instructions in Appendix Table 26. In particular, instructions that cover specialized domains such as coding, software, and other technical ques- tions often require precise details, well-defined objectives or targeted solutions. In those cases, a lack of specificity in the instructions might lead to more variable responses. As shown in Table 26, instructions with larger reward gap are not necessarily low-quality, however we hypothesize that the combination of lack of specificity in the instruction and"}, {"title": "7. Conclusion", "content": "This work introduces Rejecting Instruction Preferences (RIP), a method for improving preference data quality by measuring the rejected response quality and the reward gap between the chosen and rejected response pair. Filtering instructions using RIP remarkably improves model align- ment results on both human-written and synthetic instruc- tions, and for different reward signals. In addition, we show that Self-RIP, synthetic instructions generated by few-shot prompts curated by RIP, outperforms organic user instruc- tions and the standard Self-Instruct method, achieving the highest AlpacaEval2 win rate in our experiments."}, {"title": "Impact Statement", "content": "This work demonstrates the possibility of dramatically im- proving LLMs by identifying and producing high-quality training data. Studying how filtering criteria affect outputs will continue to be important for LLM training. While we have primarily focused on preference optimization, the RIP approach is general and can potentially work for any training scheme, e.g. other RL training techniques \u2013 which future work should explore.\nFor such models, safety will also be crucial, and future work should additionally address this aspect. In our experiments, the reward is not explicitly constrained by safety-related criteria. Therefore, a clear further avenue of study is to conduct safety evaluations \u2013 and to explore safety filtering using our methods, with reward models built exclusively for safety in existing systems (Touvron et al., 2023b).\nGiven that we have shown that RIP can filter potentially unsafe prompts, this could mean in the best case that the safety of the model could potentially improve after filtering as well, with RIP being able to catch and mitigate more chal- lenging safety situations that earlier iterations cannot. From a broader perspective, this work could pave the way for methods that produce higher-quality training instructions, that are also potentially safer than organic user instructions in the wild."}]}