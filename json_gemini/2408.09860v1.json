{"title": "3D-Aware Instance Segmentation and Tracking\nin Egocentric Videos", "authors": ["Yash Bhalgat", "Vadim Tschernezki", "Iro Laina", "Jo\u00e3o F. Henriques", "Andrea Vedaldi", "Andrew Zisserman"], "abstract": "Egocentric videos present unique challenges for 3D scene un-\nderstanding due to rapid camera motion, frequent object occlusions, and\nlimited object visibility. This paper introduces a novel approach to in-\nstance segmentation and tracking in first-person video that leverages 3D\nawareness to overcome these obstacles. Our method integrates scene ge-\nometry, 3D object centroid tracking, and instance segmentation to create\na robust framework for analyzing dynamic egocentric scenes. By incorpo-\nrating spatial and temporal cues, we achieve superior performance com-\npared to state-of-the-art 2D approaches. Extensive evaluations on the\nchallenging EPIC Fields dataset demonstrate significant improvements\nacross a range of tracking and segmentation consistency metrics. Specif-\nically, our method outperforms the next best performing approach by\n7 points in Association Accuracy (AssA) and 4.5 points in IDF1 score,\nwhile reducing the number of ID switches by 73% to 80% across var-\nious object categories. Leveraging our tracked instance segmentations,\nwe showcase downstream applications in 3D object reconstruction and\namodal video object segmentation in these egocentric settings.", "sections": [{"title": "Introduction", "content": "Egocentric videos, which capture the world from a first-person perspective, are\na focus of increasing attention in computer vision due to their importance in\napplications such as augmented reality and robotics. Among various tools for\nvideo analysis, object tracking is of particular importance, but also faces signif-\nicant challenges, in the egocentric case. Most video object segmentation (VOS)\nmethods [6,20,24,37], in fact, assume that the videos contains slow, steady cam-\nera motions that keep the view centered on the object of interest [1,4,27]. In\ncomparison, egocentric videos are taken from a first-person perspective, where\nthe camera wearer's movements introduce rapid and unpredictable changes in\nviewpoint. Additionally, objects frequently move in and out of the field of view,\nand thus are often partially or wholly occluded and/or truncated.\nFor example, in the EPIC KITCHENS dataset [10], the person taking the\nvideo might move a pan on top of a hob and leave it there for several minutes\nwhile moving around in the kitchen. During that time, they might observe more\nobjects that look similar to the pan, which may cause an algorithm to incorrectly\nassociate them to the pan itself. In general, video segmenters tend to lose track\nof the object partially or entirely due to occlusion or truncation. These issues\nare exacerbated when tracking multiple objects simultaneously.\nExisting state-of-the-art video object segmenters try to overcome these lim-\nitations by aligning segments with dense or sparse correspondences. These are\nobtained from optical flow or point tracking [30] and serve as a proxy for spatial\nreasoning. However, these methods can establish correspondences only in rela-\ntively short video windows due to their computational cost and poor reliability\nduring severe viewpoint changes. The result are fragmented and incomplete ob-\nject tracks, which limit their usefulness, particularly in egocentric videos.\nIn order to address these shortcomings, we can look at how humans locate\nobjects. An important cue that helps correct reassociation is object permanence,\na concept that human infants develop very early [31]. Permanence captures the\nidea that objects do not cease to exist when they are not visible. Combined with\nspatial awareness, this means that the 3D location of objects at rest should not\nchange when they are out of view or occluded.\nThis brings us to the question of how to incorporate such spatial awareness\nin an object tracking algorithm. We achieve this by extracting scene geometry\nfrom the video stream and using it as an additional supervisory signal to refine\ntracks produced by a video segmentation model. More specifically, we obtain\ndepth maps and camera parameters for the frames of the video and use this\ninformation to calculate the 3D location of the object instances. We then propose\na novel approach for refining instance segmentation and tracking in egocentric\nvideos that leverages 3D awareness to overcome the limitations of 2D trackers.\nBy integrating a scene-level 3D reconstruction, coarse 3D point tracking, and 2D\nsegmentation, we obtain a robust framework for analyzing dynamic egocentric\nvideos. In particular, by incorporating both spatial and temporal cues from the\n3D scene, our method handles occlusions and re-identifies objects that have been\nout of sight for some time, leading to more consistent and longer object tracks.\nOur experiments on the challenging EPIC Fields dataset [34] demonstrate sig-\nnificant improvements in tracking accuracy and segmentation consistency com-\npared to state-of-the-art video object segmentation approaches. Furthermore, we\nshowcase the potential of our method in downstream applications such as 3D\nobject reconstruction and amodal video object segmentation, where the consis-\ntent and accurate object tracks produced by our method enable more accurate\nand complete reconstructions."}, {"title": "Related Work", "content": "Video object segmentation. Video object segmentation (VOS) has seen sig-\nnificant advancements over the past decade [43], driven by the need to accu-\nrately segment and track objects across video frames. Traditional methods often\nrelied on frame-by-frame processing, which struggled with maintaining consis-\ntent object identities over long sequences. Early approaches such as MaskTrack\nR-CNN [39] and FEELVOS [35] introduced the concept of using temporal in-\nformation to improve segmentation consistency. MaskTrack R-CNN extended\nMask R-CNN to video by adding a tracking head that links instances across\nframes, while FEELVOS utilized a pixel-wise matching mechanism to propa-\ngate segmentation masks. The introduction of memory networks and attention\nmechanisms marked a significant leap in performance. STM [25], AOT [40] and\nXMem [7] leveraged memory networks to store and retrieve information across\nframes, enabling more robust handling of occlusions and reappearances. Many\nrecent works [8,9,29,36] have proposed end-to-end approaches for video object\nsegmentation as well as panoptic segmentation. VisTR [36] and SeqFormer [37]\nemployed transformers to model long-range dependencies and global context.\nVisTR treated video segmentation as a direct set prediction problem, while Se-\nqFormer introduced a sequential transformer architecture that processes video\nframes in a temporally coherent manner.\nAdditionally, methods like DEVA [6] employed decoupled video segmentation\napproaches, combining image-level segmentation with bi-directional temporal\npropagation to handle diverse and data-scarce environments effectively. This also\nhelps tackle open-vocabulary settings. MASA [20] uses the Segment Anything\nModel (SAM) as a robust segment proposer, and learns to match segments that\ncorrespond to the same object. An adapter can be trained to map those segments\nto a closed set of classes, in zero-shot settings.\nPoint tracking-based methods. Point tracking-based methods have been\npivotal in advancing VOS by providing a means to establish correspondences\nacross frames. Many powerful point trackers have been recently proposed such\nas TAP-Vid [11] benchmark that focused on tracking physical points in a video\nand works such as CoTracker [16] and PIP [14]. CenterTrack [45] combined ob-\nject detection with point tracking, leveraging the strengths of both approaches.\nTAPIR [12] trains an initial matching network (analogous to Seqformer) and an\niterative refinement network (which focuses on continuous adjustments to pre-\ndicted points' positions), using synthetic data, to predict accurate point tracks.\nSAM-PT [30] is a point-centric interactive video segmentation, which propagates\na sparse set of points, chosen by a user, to other frames.\n3D-informed instance segmentation and tracking. A recent line of work\nclosely related to the problem we address here involves lifting and fusing incon-\nsistent 2D labels or segments into 3D models. In particular, Panoptic Lifting [33],"}, {"title": "Method", "content": "Given an egocentric video, our objective is to obtain long-term consistent object\ntracks by leveraging 3D information as well as an initial set of object segments\nand tracks obtained from a 2D-only video object segmentation (VOS) model. Our\nproposed method overcomes the limitations of 2D VOS models in maintaining\nlong-term consistent object identities in egocentric scenarios and produces object\ntracks that persist despite severe occlusion and objects intermittently moving out\nof sight.\nFigure 1 provides a high-level overview of the method. We take as input an\ninitial set of image-level segments and object tracks obtained from a pretrained\nVOS model. Then, we lift these 2D segments into 3D using per-frame depth from\na pretrained depth estimator along with scene geometry information, and link\nthem across time using our proposed tracking cost formulation. We first define\nthe above problem statement more concretely in Section 3.1, and the 3D-aware\ntracking algorithm in Section 3.2. Then, we describe our design that includes\ndifferent attributes we extract for the 2D segments in Section 3.3, followed by\nour cost formulation in Section 3.4."}, {"title": "Problem statement", "content": "We begin with an egocentric video sequence consisting of N frames \\(I_t\\), \\(t \\in\\)\n{1, ..., N}, along with the output of an off-the-shelf 2D VOS model. This ini-"}, {"title": "3D aware tracking", "content": "First, we decompose the initial tracks into per-frame segments \\(M^t = \\{ m_i^t | 1 \\leq\ni \\leq |M^t| \\}\\). Specifically, each \\(M^t\\) contains a set of 2D segments \\(m_i^t\\), representing\nthe objects detected in frame t. For each segment \\(m_i^t\\), we compute an attribute\nvector \\(b_i^t = (l_i^t, v_i^t, c_i^t, s_i^t)\\) that encodes various characteristics of the seg-\nment including its initial ID \\(s_i^t\\) from the 2D VOS model, 3D location, visual\nfeatures, and category information. These attribute vectors play a crucial role\nin our method, as they allow us to establish correspondences between segments\nacross frames.\nWe employ a frame-by-frame track refinement approach using the Hungarian\nalgorithm. At each frame t, we consider the existing tracks \\(T^{t-1}\\) formed in the"}, {"title": "Attributes for 3D-aware cost formulation", "content": "Our method leverages 3D information to improve the initial object tracks ob-\ntained from an off-the shelf 2D-only VOS model. In addition to 3D location\ninformation, we leverage appearance information (visual features), as well as"}, {"title": "3D locations as segment attributes", "content": "We are given for each image \\(I_t\\), \\(t \\in\\)\n\\{1,..., N\\}, a camera pose \\(C_t\\), camera intrinsics K and a depth map \\(D^t\\). In\norder to optimise the associations with 3D information, we lift the 2D centroid\nof each segment into 3D. We define the 3D centroid of segment \\(m_i^t\\) in frame t as\n\\(l_i^t\\), representing one out of several attributes of \\(b_i^t\\). We calculate the location of\nthis segment by projecting its 2D centroid into 3D with\n\\(l_i^t = C_t \\left[ d_i^t K^{-1} \\begin{bmatrix} x_i^t \\\\ y_i^t \\\\ 1 \\end{bmatrix} \\right]\\)\nwhere \\(d_i^t\\) is the depth value obtained from \\(D^t\\) that corresponds to the centroid\nof segment \\(m_i^t\\) of frame t, and \\(x_i^t, y_i^t\\) are the 2D coordinates of the centroid."}, {"title": "Visual features as segment attributes", "content": "While the 3D location of a seg-\nment plays a crucial role in overcoming the mentioned problems of associating\nsegments throughout occlusions, viewpoint changes and similar issues, we also\nmake use of 2D-level visual features \\(v_i^t\\) as one of the attributes \\(b_i^t\\) that corre-\nspond to each segment. Specifically, for an image \\(I_t\\) and each segment \\(m_i^t\\) of the\nimage, we use a pretrained vision encoder, e.g. DINOv2 [26], to obtain the visual\nfeature \\(v_i^t\\) as:\n\\(v_i^t = V(crop(I_t \\odot m_i^t)),\\)\nwhere V is the vision encoder and \\(\\odot\\) denotes Hadamard product. The\u2018crop\u2019\noperation extracts the smallest patch with a 1:1 aspect ratio enclosing mask \\(m_i^t\\)."}, {"title": "Initial instance and category labels as segment attributes", "content": "Our pro-\nposed method refines the initial tracks obtained from a purely 2D video object\nsegmentation model. Let \\(c_i^t\\) and \\(s_i^t\\) denote the initial category and instance labels\nfor segment \\(m_i^t\\) obtained from the 2D model. We use \\(c_i^t\\) as an attribute to dis-\ncourage the optimisation from matching instances which did not initially belong\nto the same category. And similarly, we use \\(s_i^t\\) to encourage the optimization\nto preserve the initial tracks of instances across frames obtained from the 2D\nmodel. We mathematically define the associated costs below."}, {"title": "Attributes for a track", "content": "A track \\(T_i^{t-1}\\) that exists at time t \u2212 1 is a sequence of\nsegments assigned to it so far. We associate each track with an attribute vector\n\\(b_i^{t-1} = (\\bar{l}_i^{t-1}, \\bar{v}_i^{t-1}, \\bar{c}_i^{t-1}, \\bar{s}_i^{t-1})\\), where \\(\\bar{l}_i^{t-1}, \\bar{v}_i^{t-1}, \\bar{c}_i^{t-1}\\), and \\(\\bar{s}_i^{t-1}\\) are defined to be the corresponding\nattributes of the most recent segment assigned to this track. The visual feature\nattribute \\(\\bar{v}_i^{t-1}\\) is defined to be the mean visual feature of the 100 most recent\nsegments assigned to the track."}, {"title": "Cost functions", "content": "The attributes used for refining the tracks are thus \\(b_i^t = (l_i^t, v_i^t, c_i^t, s_i^t)\\), consisting\nof the 3D location, the visual features, initial category label and initial instance"}, {"title": "Experiments", "content": "We evaluate our proposed method on 20 challenging scenes from the EPIC\nFields [34] dataset. EPIC Fields comprises of complex real-world videos with\na high diversity of activities and object interactions, making it an ideal testbed\nfor our evaluation. The scenes we selected include varied lighting conditions,\nocclusions as well as objects intermittently being out of sight. Each video con-\ntaining several thousand frames, averaging around 10 minutes in length.\nWe compare against the following baselines:\n1. DEVA [6] employs a decoupled video segmentation approach that combines\ntask-specific image-level segmentation with a class-agnostic bi-directional\ntemporal propagation model. This method is particularly effective in diverse\nand data-scarce environments, as it separates image and video segmentation\ntasks to improve overall tracking accuracy by reducing the impact of image\nsegmentation errors."}, {"title": "Sensitivity to hyperparameters", "content": "We evaluate the sensitivity of our method\nby varying the values of the 4 hyperparameters: \\(\\alpha_s, \\alpha_c, \\alpha_l, \\alpha_v\\) in the cost-\nmatching formulation Eq. (2) from Eqs. (5) to (8). We perform this analysis\non a subset of 5 videos, using 3 representative values for each hyperparameter,\nresulting in \\(3^4 = 81\\) configurations. Figure 3 shows that 57 out of these 81\nhyperparameter configurations lead to a HOTA score in the range 27.2 \u00b1 0.2,\nwhich shows the robustness of our method to these parameters. There are some\nconfigurations, e.g. when \\(\\alpha_s = 100\\) or \\(\\alpha_c = 100\\), that lead to a degradation in\nperformance."}, {"title": "Influence of different components on tracking", "content": "Our tracking formulation\nconsists of four components (Eqs. (5) to (8)): instance cost, category cost, 3D lo-\ncation cost, and visual feature cost. We evaluate the influence of each component\nby turning off the corresponding cost one at a time in the cost-matching formu-\nlation. Table 3 shows that all components contribute positively to the tracking\nperformance, but to varying degrees. Removing the visual features has the least\nimpact, reducing the HOTA score from 27.72 to 27.17. The 3D location informa-\ntion proves more important, with its removal causing the HOTA score to drop"}, {"title": "Cost functions", "content": "label for the segment \\(m_i^t\\) of frame t. Now, we define the cost functions \\(\\delta_p\\) used\nin Eq. (2) for these individual attributes.\n1. Similar to [28], we model the location cost \\(\\delta_l\\) with the exponential distribu-\ntion as follows:\n\\(\\delta_l(l_i^t, \\bar{l}_j^{t-1}) = -log \\left( \\frac{1}{\\alpha_l} exp(-\\frac{||l_i^t - \\bar{l}_j^{t-1}||_2}{\\alpha_l}) \\right)\\)\n2. We model the cost for the visual features, \\(\\delta_v\\), using a Cauchy distribution:\n\\(\\delta_v(v_i^t, \\bar{v}_j^{t-1}) = - log \\left( \\frac{1}{1 + \\alpha_v||v_i^t - \\bar{v}_j^{t-1}||} \\right)\n3. For the category and instance label, we use a 0 - 1 cost function and refer\nto it with \\(\\delta_c\\) and \\(\\delta_s\\):\n\\(\\delta_c(c_i^t, \\bar{c}_j^{t-1}) = \\begin{cases}\n0 & \\text{if } c_i^t = \\bar{c}_j^{t-1} \\\\\n\\alpha_c & \\text{if } c_i^t \\neq \\bar{c}_j^{t-1}\n\\end{cases}\\)\n\\(\\delta_s(s_i^t, \\bar{s}_j^{t-1}) = \\begin{cases}\n0 & \\text{if } s_i^t = \\bar{s}_j^{t-1} \\\\\n\\alpha_s & \\text{if } s_i^t \\neq \\bar{s}_j^{t-1}\n\\end{cases}\\)\nHere, \\(\\alpha_l, \\alpha_v, \\alpha_c\\) and \\(\\alpha_s\\) are used to modulate the importance of each cost func-\ntion. The cost parameters for the category and instance labels discourage the\nmatching of segments that are inconsistent with the category and instance la-\nbels from the input segments. As described in Section 3.2, we consider the tracks\nformed in previous t \u2013 1 frames and match them to the new observations from\nthe current frame t using the Hungarian algorithm."}, {"title": "Downstream applications", "content": "Our 3D-aware instance segmentation and tracking method yields longer and\nmore consistent tracks than 2D methods. This improvement enables two key\ndownstream applications: 3D object reconstruction and amodal segmentation.\nReconstruction of objects. The longer, more consistent tracks produced by\nour method allow us to extract the same object from many frames using the\noutput instance ID. This multi-view information is crucial for achieving high-\nquality 3D reconstructions. This is something that fragmented or inconsistent\ntracks from 2D methods often fail to achieve. Additionally, our 3D tracking ap-\nproach, which uses lifted centroids, allows us to determine the time ranges when\nan object remains static. We leverage these static periods for reconstruction,\nas they provide the most reliable geometric information. This selective use of\nframes is only possible due to maintaining long-term tracks of objects.\nAmodal segmentation. Amodal segmentation aims to estimate the full ex-\ntent of objects, including parts that are occluded. Building upon our 3D object\nreconstructions, we render the reconstructed 3D object from multiple viewpoints"}, {"title": "Evaluation Data", "content": "We evaluate our method and baselines using the VISOR dataset, which pro-\nvides pixel-level annotations for active objects in kitchen environments. These\nannotations include any objects used for cooking or cleaning. From these annota-\ntions, we derive ground truth tracks and segmentations. The dataset's annotation\nstructure supports instance-level tracking, as segments of a particular object cat-\negory often correspond to the same instance throughout a video. As mentioned\nin the main paper, we evaluate our approach using the VISOR annotations for\n20 videos from EPIC Fields [34] dataset."}, {"title": "Comparison with OSNOM [28] for Video Segmentation", "content": "Our approach is inspired by OSNOM [28]. But while OSNOM focuses only on\ncoarse and class-agnostic 3D centroid tracking, ours is tailored to be more suit-\nable for long-term video instance segmentation by taking in account the initial\ncategorical information provided by the base 2D model. To facilitate a direct\ncomparison, we simulate OSNOM's functionality within our framework by dis-\nabling the instance and category terms in our cost formulation, relying solely on\n3D location and visual feature costs."}, {"title": "Details on Obtaining Amodal Segmentations", "content": "As briefly described in Section 4.6 of the main paper, our method for obtaining\namodal segmentations builds upon the long-term instance segmentations and\ncoarsely tracked 3D centroids generated by our primary algorithm. This section\nelaborates on the process, which involves three main steps: identifying static\nobject frames, 3D object reconstruction, and amodal segmentation projection.\nIdentifying Static Object Frames: We begin by analyzing the tracked 3D\ncentroid of the object of interest across the video sequence. By identifying pe-\nriods where the centroid remains relatively stationary (using a threshold on 3D\nlocation differences between frames), we can isolate a range of frames where the\nobject is static. This step is crucial as it allows us to gather multiple views of the\nobject from different camera angles while minimizing the complexity introduced\nby object motion.\n3D Object Reconstruction: Once we have identified the static frames, we uti-\nlize the corresponding 2D instance segmentations and associated camera param-\neters to reconstruct the 3D shape of the object. This reconstruction is achieved\nthrough a technique known as Gaussian Splatting\u00b3. In this approach, we rep-\nresent the 3D object as a collection of Gaussian functions in 3D space. Each\nGaussian is characterized by its mean position and covariance matrix, which\ndefine its location and shape respectively. Given Gas the set of 3D Gaussians\nand a camera viewpoint Ci, the differentiable Gaussian Splatting renderer [17]\nproduces an image\n\\(\\hat{I} = \\Pi(G,C_i) \\in \\mathbb{R}^{H \\times W \\times 3}\\)\nThe same renderer can be used to render an alpha-map (equivalent to a seg-\nmentation map) by setting the colors for each Gaussian to be 1. The Gaussian\nSplatting model for the object of interest is optimized by minimizing this loss\nfunction across multiple views:\n\\(L= \\sum_t (I_t \\odot m_t - \\Pi(G, C_i))^2\\)\nwhere \\(m_t\\) and \\(I_t\\) represents the observed 2D segmentation map and RGB values\nin frame t respectively.\nProjecting Amodal Segmentations: Once we obtain a satisfactory 3D re-\nconstruction of the object, we can generate amodal segmentations for any desired\nviewpoint. This is done by rendering the entire 3D Gaussian representation back\nonto the image plane, regardless of occlusions present in the original views. As\nexplained above, we set the Gaussian colors to 1 which provides an alpha map\nusing the renderer as\n\\(m = \\Pi(G, C_i) \\in \\mathbb{R}^{H \\times W}\\)"}, {"title": "Conclusion", "content": "In this paper, we presented a novel 3D-aware approach to instance segmentation\nand tracking in egocentric videos, addressing the unique challenges of first-person\nperspectives. By integrating 3D information, our method significantly improves\ntracking accuracy and segmentation consistency compared to state-of-the-art\n2D approaches, especially over long periods. Our ablation studies highlight the\nimportance of 3D information and the category as well as instance cost terms\nin matching, while also showing robustness to hyperparameter changes. Beyond\nimproved tracking, our approach enables valuable downstream applications such\nas high-quality 3D object reconstructions and amodal segmentation. This work\ndemonstrates the power of incorporating 3D awareness into egocentric video\nanalysis, opening up new possibilities for robust object tracking in challenging\nfirst-person scenarios."}]}