{"title": "3D-Aware Instance Segmentation and Tracking in Egocentric Videos", "authors": ["Yash Bhalgat", "Vadim Tschernezki", "Iro Laina", "Jo\u00e3o F. Henriques", "Andrea Vedaldi", "Andrew Zisserman"], "abstract": "Egocentric videos present unique challenges for 3D scene understanding due to rapid camera motion, frequent object occlusions, and limited object visibility. This paper introduces a novel approach to instance segmentation and tracking in first-person video that leverages 3D awareness to overcome these obstacles. Our method integrates scene geometry, 3D object centroid tracking, and instance segmentation to create a robust framework for analyzing dynamic egocentric scenes. By incorporating spatial and temporal cues, we achieve superior performance compared to state-of-the-art 2D approaches. Extensive evaluations on the challenging EPIC Fields dataset demonstrate significant improvements across a range of tracking and segmentation consistency metrics. Specifically, our method outperforms the next best performing approach by 7 points in Association Accuracy (AssA) and 4.5 points in IDF1 score, while reducing the number of ID switches by 73% to 80% across various object categories. Leveraging our tracked instance segmentations, we showcase downstream applications in 3D object reconstruction and amodal video object segmentation in these egocentric settings.", "sections": [{"title": "1 Introduction", "content": "Egocentric videos, which capture the world from a first-person perspective, are a focus of increasing attention in computer vision due to their importance in applications such as augmented reality and robotics. Among various tools for video analysis, object tracking is of particular importance, but also faces significant challenges, in the egocentric case. Most video object segmentation (VOS) methods [6,20,24,37], in fact, assume that the videos contains slow, steady camera motions that keep the view centered on the object of interest [1,4,27]. In comparison, egocentric videos are taken from a first-person perspective, where the camera wearer's movements introduce rapid and unpredictable changes in viewpoint. Additionally, objects frequently move in and out of the field of view, and thus are often partially or wholly occluded and/or truncated.\nFor example, in the EPIC KITCHENS dataset [10], the person taking the video might move a pan on top of a hob and leave it there for several minutes while moving around in the kitchen. During that time, they might observe more objects that look similar to the pan, which may cause an algorithm to incorrectly associate them to the pan itself. In general, video segmenters tend to lose track of the object partially or entirely due to occlusion or truncation. These issues are exacerbated when tracking multiple objects simultaneously.\nExisting state-of-the-art video object segmenters try to overcome these limitations by aligning segments with dense or sparse correspondences. These are obtained from optical flow or point tracking [30] and serve as a proxy for spatial reasoning. However, these methods can establish correspondences only in relatively short video windows due to their computational cost and poor reliability during severe viewpoint changes. The result are fragmented and incomplete object tracks, which limit their usefulness, particularly in egocentric videos.\nIn order to address these shortcomings, we can look at how humans locate objects. An important cue that helps correct reassociation is object permanence, a concept that human infants develop very early [31]. Permanence captures the idea that objects do not cease to exist when they are not visible. Combined with spatial awareness, this means that the 3D location of objects at rest should not change when they are out of view or occluded.\nThis brings us to the question of how to incorporate such spatial awareness in an object tracking algorithm. We achieve this by extracting scene geometry from the video stream and using it as an additional supervisory signal to refine tracks produced by a video segmentation model. More specifically, we obtain depth maps and camera parameters for the frames of the video and use this information to calculate the 3D location of the object instances. We then propose a novel approach for refining instance segmentation and tracking in egocentric videos that leverages 3D awareness to overcome the limitations of 2D trackers. By integrating a scene-level 3D reconstruction, coarse 3D point tracking, and 2D segmentation, we obtain a robust framework for analyzing dynamic egocentric videos. In particular, by incorporating both spatial and temporal cues from the 3D scene, our method handles occlusions and re-identifies objects that have been out of sight for some time, leading to more consistent and longer object tracks.\nOur experiments on the challenging EPIC Fields dataset [34] demonstrate significant improvements in tracking accuracy and segmentation consistency compared to state-of-the-art video object segmentation approaches. Furthermore, we showcase the potential of our method in downstream applications such as 3D object reconstruction and amodal video object segmentation, where the consistent and accurate object tracks produced by our method enable more accurate and complete reconstructions."}, {"title": "2 Related Work", "content": "Video object segmentation. Video object segmentation (VOS) has seen significant advancements over the past decade [43], driven by the need to accurately segment and track objects across video frames. Traditional methods often relied on frame-by-frame processing, which struggled with maintaining consistent object identities over long sequences. Early approaches such as MaskTrack R-CNN [39] and FEELVOS [35] introduced the concept of using temporal information to improve segmentation consistency. MaskTrack R-CNN extended Mask R-CNN to video by adding a tracking head that links instances across frames, while FEELVOS utilized a pixel-wise matching mechanism to propagate segmentation masks. The introduction of memory networks and attention mechanisms marked a significant leap in performance. STM [25], AOT [40] and XMem [7] leveraged memory networks to store and retrieve information across frames, enabling more robust handling of occlusions and reappearances. Many recent works [8,9,29,36] have proposed end-to-end approaches for video object segmentation as well as panoptic segmentation. VisTR [36] and SeqFormer [37] employed transformers to model long-range dependencies and global context. VisTR treated video segmentation as a direct set prediction problem, while SeqFormer introduced a sequential transformer architecture that processes video frames in a temporally coherent manner.\nAdditionally, methods like DEVA [6] employed decoupled video segmentation approaches, combining image-level segmentation with bi-directional temporal propagation to handle diverse and data-scarce environments effectively. This also helps tackle open-vocabulary settings. MASA [20] uses the Segment Anything Model (SAM) as a robust segment proposer, and learns to match segments that correspond to the same object. An adapter can be trained to map those segments to a closed set of classes, in zero-shot settings.\nPoint tracking-based methods. Point tracking-based methods have been pivotal in advancing VOS by providing a means to establish correspondences across frames. Many powerful point trackers have been recently proposed such as TAP-Vid [11] benchmark that focused on tracking physical points in a video and works such as CoTracker [16] and PIP [14]. CenterTrack [45] combined object detection with point tracking, leveraging the strengths of both approaches. TAPIR [12] trains an initial matching network (analogous to Seqformer) and an iterative refinement network (which focuses on continuous adjustments to predicted points' positions), using synthetic data, to predict accurate point tracks. SAM-PT [30] is a point-centric interactive video segmentation, which propagates a sparse set of points, chosen by a user, to other frames.\n3D-informed instance segmentation and tracking. A recent line of work closely related to the problem we address here involves lifting and fusing inconsistent 2D labels or segments into 3D models. In particular, Panoptic Lifting [33],"}, {"title": "3 Method", "content": "Given an egocentric video, our objective is to obtain long-term consistent object tracks by leveraging 3D information as well as an initial set of object segments and tracks obtained from a 2D-only video object segmentation (VOS) model. Our proposed method overcomes the limitations of 2D VOS models in maintaining long-term consistent object identities in egocentric scenarios and produces object tracks that persist despite severe occlusion and objects intermittently moving out of sight.\nFigure 1 provides a high-level overview of the method. We take as input an initial set of image-level segments and object tracks obtained from a pretrained VOS model. Then, we lift these 2D segments into 3D using per-frame depth from a pretrained depth estimator along with scene geometry information, and link them across time using our proposed tracking cost formulation. We first define the above problem statement more concretely in Section 3.1, and the 3D-aware tracking algorithm in Section 3.2. Then, we describe our design that includes different attributes we extract for the 2D segments in Section 3.3, followed by our cost formulation in Section 3.4."}, {"title": "3.1 Problem statement", "content": "We begin with an egocentric video sequence consisting of N frames $I_t$, $t \\in {1, ..., N}$, along with the output of an off-the-shelf 2D VOS model. This initial output contains a set of object tracks that, while partially correct, often contain errors particularly when objects temporarily leave the field of view or are occluded. Our goal is to refine and reassemble these tracks, leveraging 3D information to correct errors and achieve more consistent long-term tracking. Crucially, we don't discard the initial track IDs obtained from the 2D-only VOS model. Instead, we incorporate this information into our refinement process, using it as a valuable prior for maintaining object identities. The objective of the method is to compute a set of tracks for the entire video ${\\mathcal{T}}$ with associated segment IDs {$s_i^t$} that have the desired temporal consistency."}, {"title": "3.2 3D aware tracking", "content": "First, we decompose the initial tracks into per-frame segments $M^t = {m_i^t | 1 < i \\leq |M^t|}$. Specifically, each $M^t$ contains a set of 2D segments $m_i^t$, representing the objects detected in frame t. For each segment $m_i^t$, we compute an attribute vector $b_i^t = (\\ell_i^t, v_i^t, c_i^t, s_i^t)$ that encodes various characteristics of the segment including its initial ID $s_i^t$ from the 2D VOS model, 3D location, visual features, and category information. These attribute vectors play a crucial role in our method, as they allow us to establish correspondences between segments across frames.\nWe employ a frame-by-frame track refinement approach using the Hungarian algorithm. At each frame t, we consider the existing tracks $T^{t-1}$ formed in the previous t - 1 frames and new segments $M^t$ from the current frame t. The i-th track within $T^{t-1}$ is associated with an attribute vector $b_i^{t-1} = (\\bar{\\ell}_i^{t-1}, \\bar{v}_i^{t-1}, \\bar{c}_i^{t-1}, \\bar{s}_i^{t-1})$, computed as an aggregate of the attributes of segments assigned to it (c.f. Sec. 3.3), and refined segment ID $\\bar{s}_i^{t-1}$. We match the new segments at time t to the tracks $T^{t-1}$ by solving the following optimization problem to obtain the new refined segment IDs {$s_i^t$}:\n$\\displaystyle \\mathop{\\text{argmin}}_{\\{s_i^t\\}} \\sum_{i,j} \\mathbb{I}(s_i^t, \\bar{s}_j^{t-1}) J(s_i^t, b_i^t, \\bar{s}_j^{t-1}, \\bar{b}_j^{t-1})$\nsubject to $s_i^t \\in {1,...,S}$ and $s_i^t \\neq s_j^t$ if $i \\neq j$, where S is the total number of unique object identifiers. The second condition enforces that no two segments in the same frame can have the same identifier. The cost function J is defined as:\n$J(s_i^t, b_i^t, \\bar{s}_j^{t-1}, \\bar{b}_j^{t-1}) = (1-\\mathbb{I}(s_i^t = \\bar{s}_j^{t-1})) + \\sum_{p=1}^n \\delta_p(b_{i,p}^t, \\bar{b}_{j,p}^{t-1})$\nHere, $\\mathbb{I}(s_i^t = \\bar{s}_j^{t-1})$ is an indicator function. $\\delta_p(b_{i,p}^t, \\bar{b}_{j,p}^{t-1})$ is the consistency cost for the p-th attribute between segment $m_i^t$ in frame t and track $T_j^{t-1}$. Importantly, one of these $\\delta_p$ functions specifically accounts for the initial track IDs (c.f. Eq. (8)), encouraging our optimization to maintain these associations when appropriate.\nWe use the Hungarian algorithm to solve for the new segment IDs and update the initial segment IDs only if optimisation cost from Eq. (2) is below a cost threshold $\\gamma$. This ensures that our algorithm does not change associations when cost is too high. Notably, for new observations that don't match any existing track (i.e., their matching cost exceeds $\\gamma$), we initialize new tracks. Importantly, we don't terminate tracks that fail to match with a new observation in the current frame. Instead, we maintain these tracks in our database, propagating their attributes from time t - 1 to time t. This approach allows our method to handle temporary occlusions or brief disappearances of objects, maintaining object identity over longer periods.\nBy iteratively applying this process across the entire video sequence, we refine the initial tracks, correcting errors while still leveraging the valuable information provided by the 2D VOS model. Our method's ability to incorporate both the initial 2D tracking information and additional 3D cues, combined with its frame-by-frame processing and track maintenance strategy, enables it to effectively handle the challenges of egocentric videos, including frequent occlusions, objects moving in and out of view, and rapid camera motion. Next, we describe how we define and compute the segment attributes $b_i^t$ as well as the associated cost functions $\\delta_p$."}, {"title": "3.3 Attributes for 3D-aware cost formulation", "content": "Our method leverages 3D information to improve the initial object tracks obtained from an off-the-shelf 2D-only VOS model. In addition to 3D location information, we leverage appearance information (visual features), as well as categorical information (i.e., the initial category and instance labels from the 2D model) to refine the segment associations. We denote the attributes for each segment as $b_i^t = (\\ell_i^t, v_i^t, c_i^t, s_i^t)$, where $\\ell_i^t$ is the 3D location of the segment, $v_i^t$ is the visual feature, $c_i^t$ is the category label and $s_i^t$ is the instance label.\n3D locations as segment attributes. We are given for each image $I_t$, $t \\in {1,..., N}$, a camera pose $C_t$, camera intrinsics $K$ and a depth map $D^t$. In order to optimise the associations with 3D information, we lift the 2D centroid of each segment into 3D. We define the 3D centroid of segment $m_i^t$ in frame t as $\\ell_i^t$, representing one out of several attributes of $b_i^t$. We calculate the location of this segment by projecting its 2D centroid into 3D with\n$\\ell_i^t = C_t \\left[d_t K^{-1} \\begin{bmatrix} x_i^t \\\\ y_i^t \\\\ 1 \\end{bmatrix}\\right]$\nwhere $d_t$ is the depth value obtained from $D^t$ that corresponds to the centroid of segment $m_i^t$ of frame t, and $x_i^t, y_i^t$ are the 2D coordinates of the centroid.\nVisual features as segment attributes. While the 3D location of a segment plays a crucial role in overcoming the mentioned problems of associating segments throughout occlusions, viewpoint changes and similar issues, we also make use of 2D-level visual features $v_i^t$ as one of the attributes $b_i^t$ that correspond to each segment. Specifically, for an image $I_t$ and each segment $m_i^t$ of the image, we use a pretrained vision encoder, e.g. DINOv2 [26], to obtain the visual feature $v_i^t$ as:\n$v_i^t = V(\\text{crop}(I_t \\odot m_i^t))$,\nwhere V is the vision encoder and $\\odot$ denotes Hadamard product. The \u2018crop' operation extracts the smallest patch with a 1:1 aspect ratio enclosing mask $m_i^t$.\nInitial instance and category labels as segment attributes. Our proposed method refines the initial tracks obtained from a purely 2D video object segmentation model. Let $c_i^t$ and $s_i^t$ denote the initial category and instance labels for segment $m_i^t$ obtained from the 2D model. We use $c_i^t$ as an attribute to discourage the optimisation from matching instances which did not initially belong to the same category. And similarly, we use $s_i^t$ to encourage the optimization to preserve the initial tracks of instances across frames obtained from the 2D model. We mathematically define the associated costs below.\nAttributes for a track. A track $T^{t-1}$ that exists at time t \u2212 1 is a sequence of segments assigned to it so far. We associate each track with an attribute vector $\\bar{b}_j^{t-1} = (\\bar{\\ell}_j^{t-1}, \\bar{v}_j^{t-1}, \\bar{c}_j^{t-1}, \\bar{s}_j^{t-1})$, where $\\bar{\\ell}_j^{t-1}, \\bar{v}_j^{t-1}, \\bar{c}_j^{t-1}$ and $\\bar{s}_j^{t-1}$ are defined to be the corresponding attributes of the most recent segment assigned to this track. The visual feature attribute $\\bar{v}_j^{t-1}$ is defined to be the mean visual feature of the 100 most recent segments assigned to the track."}, {"title": "3.4 Cost functions", "content": "The attributes used for refining the tracks are thus $b_i^t = (\\ell_i^t, v_i^t, c_i^t, s_i^t)$, consisting of the 3D location, the visual features, initial category label and initial instance label for the segment $m_i^t$ of frame t. Now, we define the cost functions $\\delta_p$ used in Eq. (2) for these individual attributes.\n1. Similar to [28], we model the location cost $\\delta_\\ell$ with the exponential distribution as follows:\n$\\delta_\\ell(\\ell_i^t, \\bar{\\ell}_j^{t-1}) = - \\log \\left(\\frac{1}{\\alpha_\\ell} \\exp{\\left(-\\frac{|\\ell_i^t - \\bar{\\ell}_j^{t-1}|^2}{\\alpha_\\ell}\\right)}\\right)$\n2. We model the cost for the visual features, $\\delta_v$, using a Cauchy distribution:\n$\\delta_v(v_i^t, \\bar{v}_j^{t-1}) = -\\log \\left(\\frac{1}{1 + \\alpha_v |v_i^t - \\bar{v}_j^{t-1}|}\\right)$\n3. For the category and instance label, we use a 0 - 1 cost function and refer to it with $\\delta_c$ and $\\delta_s$:\n$\\delta_c(c_i^t, \\bar{c}_j^{t-1}) = \\begin{cases} 0 & \\text{if } c_i^t = \\bar{c}_j^{t-1} \\\\ \\alpha_c & \\text{if } c_i^t \\neq \\bar{c}_j^{t-1} \\end{cases}$\n$\\delta_s(s_i^t, \\bar{s}_j^{t-1}) = \\begin{cases} 0 & \\text{if } s_i^t = \\bar{s}_j^{t-1} \\\\ \\alpha_s & \\text{if } s_i^t \\neq \\bar{s}_j^{t-1} \\end{cases}$\nHere, $\\alpha_\\ell, \\alpha_v, \\alpha_c$ and $\\alpha_s$ are used to modulate the importance of each cost function. The cost parameters for the category and instance labels discourage the matching of segments that are inconsistent with the category and instance labels from the input segments. As described in Section 3.2, we consider the tracks formed in previous t \u2013 1 frames and match them to the new observations from the current frame t using the Hungarian algorithm.\nWe refer the reader to Appendix A for the implementation details and to Appendix C for hyperparameter settings."}, {"title": "4 Experiments", "content": "We evaluate our proposed method on 20 challenging scenes from the EPIC Fields [34] dataset. EPIC Fields comprises of complex real-world videos with a high diversity of activities and object interactions, making it an ideal testbed for our evaluation. The scenes we selected include varied lighting conditions, occlusions as well as objects intermittently being out of sight. Each video containing several thousand frames, averaging around 10 minutes in length.\nWe compare against the following baselines:\n1. DEVA [6] employs a decoupled video segmentation approach that combines task-specific image-level segmentation with a class-agnostic bi-directional temporal propagation model. This method is particularly effective in diverse and data-scarce environments, as it separates image and video segmentation tasks to improve overall tracking accuracy by reducing the impact of image segmentation errors."}, {"title": "4.1 Benchmark and baselines", "content": "Our method leverages 3D information to improve the initial object tracks obtained from an off-the-shelf 2D-only VOS model. In addition to 3D location information, we leverage appearance information (visual features), as well as categorical information (i.e., the initial category and instance labels from the 2D model) to refine the segment associations. We denote the attributes for each segment as $b_i^t = (\\ell_i^t, v_i^t, c_i^t, s_i^t)$, where $\\ell_i^t$ is the 3D location of the segment, $v_i^t$ is the visual feature, $c_i^t$ is the category label and $s_i^t$ is the instance label.\n3D locations as segment attributes. We are given for each image $I_t$, $t \\in {1,..., N}$, a camera pose $C_t$, camera intrinsics $K$ and a depth map $D^t$. In order to optimise the associations with 3D information, we lift the 2D centroid of each segment into 3D. We define the 3D centroid of segment $m_i^t$ in frame t as $\\ell_i^t$, representing one out of several attributes of $b_i^t$. We calculate the location of this segment by projecting its 2D centroid into 3D with\n$\\ell_i^t = C_t \\left[d_t K^{-1} \\begin{bmatrix} x_i^t \\\\ y_i^t \\\\ 1 \\end{bmatrix}\\right]$\nwhere $d_t$ is the depth value obtained from $D^t$ that corresponds to the centroid of segment $m_i^t$ of frame t, and $x_i^t, y_i^t$ are the 2D coordinates of the centroid.\nVisual features as segment attributes. While the 3D location of a segment plays a crucial role in overcoming the mentioned problems of associating segments throughout occlusions, viewpoint changes and similar issues, we also make use of 2D-level visual features $v_i^t$ as one of the attributes $b_i^t$ that correspond to each segment. Specifically, for an image $I_t$ and each segment $m_i^t$ of the image, we use a pretrained vision encoder, e.g. DINOv2 [26], to obtain the visual feature $v_i^t$ as:\n$v_i^t = V(\\text{crop}(I_t \\odot m_i^t))$,\nwhere V is the vision encoder and $\\odot$ denotes Hadamard product. The \u2018crop' operation extracts the smallest patch with a 1:1 aspect ratio enclosing mask $m_i^t$.\nInitial instance and category labels as segment attributes. Our proposed method refines the initial tracks obtained from a purely 2D video object segmentation model. Let $c_i^t$ and $s_i^t$ denote the initial category and instance labels for segment $m_i^t$ obtained from the 2D model. We use $c_i^t$ as an attribute to discourage the optimisation from matching instances which did not initially belong to the same category. And similarly, we use $s_i^t$ to encourage the optimization to preserve the initial tracks of instances across frames obtained from the 2D model. We mathematically define the associated costs below.\nAttributes for a track. A track $T^{t-1}$ that exists at time t \u2212 1 is a sequence of segments assigned to it so far. We associate each track with an attribute vector $\\bar{b}_j^{t-1} = (\\bar{\\ell}_j^{t-1}, \\bar{v}_j^{t-1}, \\bar{c}_j^{t-1}, \\bar{s}_j^{t-1})$, where $\\bar{\\ell}_j^{t-1}, \\bar{v}_j^{t-1}, \\bar{c}_j^{t-1}$ and $\\bar{s}_j^{t-1}$ are defined to be the corresponding attributes of the most recent segment assigned to this track. The visual feature attribute $\\bar{v}_j^{t-1}$ is defined to be the mean visual feature of the 100 most recent segments assigned to the track."}, {"title": "4.2 Metrics", "content": "We evaluate our method using the HOTA (Higher Order Tracking Accuracy) metric [22]. HOTA assesses multi-object tracking (MOT) performance by combining detection accuracy (DetA), association accuracy (AssA), and localization IoU (Loc-IoU). It is calculated as the geometric mean of DetA and AssA over various Loc-IoU thresholds a:\n$\\text{HOTA} = \\frac{1}{|S|} \\sum_{\\alpha \\in S}\\text{HOTA}(\\alpha) = \\frac{1}{|S|} \\sum_{\\alpha \\in S}\\sqrt{\\text{DetA}(\\alpha) \\times \\text{AssA}(\\alpha)}$\nwhere S is the set of IoU thresholds. We use S = {0.05, 0.1, . . ., 0.9, 0.95} following standard protocol [22]. DetA measures the overlap between the set of all predicted segments and all ground-truth (GT) segments. It is defined as:\n$\\text{DetA}(\\alpha) = \\frac{|TP_{\\alpha}|}{|TP_{\\alpha}| + |FP_{\\alpha}| + |FN_{\\alpha}|}$\nTrue Positives (TP\u03b1) are identified by matching predicted segments to GT segments with an IoU > a using Hungarian matching. Unmatched predictions are False Positives (FP\u03b1), and unmatched GT segments are False Negatives (FNa). Whereas, AssA measures the tracker's ability to maintain consistent object identities over time:\n$\\text{AssA}(\\alpha) = \\frac{1}{|TP_{\\alpha}|} \\sum_{c \\in TP_{\\alpha}} \\frac{|TPA(c)|}{|TPA(c)| + |FPA(c)| + |FNA(c)|}$\nwhere we iterate over all TP pairs, measuring the alignment between the predicted and ground-truth segment's whole track. True Positive Associations (\u03a4\u03a1\u0391) represents the number of TP matches between the two chosen tracks for a pair."}, {"title": "4.3 Results", "content": "We evaluate our method against DEVA [6] and MASA [20] using the HOTA, DetA, AssA, and IDF1 metrics. Table 1 presents the overall results as well as scene-specific performance. Figure 2 provides a qualitative comparison of results.\nOur approach consistently outperforms both baselines across various metrics. Compared to DEVA, our method achieves an overall HOTA score of 27.72, a notable improvement over DEVA's 25.14. This enhancement is even more pronounced in the AssA metric, which measures the tracker's ability to maintain consistent object identities over time. Our method attains an AssA score of 43.90, substantially higher than DEVA's 36.72.\nThis further underscores our method's superior performance in maintaining consistent object identities throughout the video sequences. Our method also shows significant improvements in IDF1 scores, achieving 26.63 compared to DEVA's 22.17. Similar improvements are observed when comparing to MASA, which demonstrates our approach's adaptability to different base models.\nNotably, DetA scores remain relatively consistent across all methods (e.g. 18.40 for MASA vs. 18.38 for our method when using MASA as the base model). This is because our method improves the instance and category assignments for the segments using 3D information but does not alter the segments themselves. Since the DetA metric only evaluates the segments regardless of IDs, it results in similar scores for both the base 2D method and our method.\nScene-specific analysis. Our method shows remarkable improvements in complex scenes, such as P01_01, where we achieve a HOTA score of 41.91 compared to DEVA's 33.60, a 24% improvement. This scene likely contains frequent object occlusions or out-of-view instances where our 3D-aware approach excels. Significant improvements are also observed in scenes like P07_101 and P22_117, with improvements of 25% and 22% respectively in HOTA scores.\nThe AssA metric shows the most significant improvements. For example, in P02_121, our method achieves an AssA of 20.96 compared to DEVA's 10.32, a 103% improvement. However, the degree of improvement varies across scenes. In some, like P04_11, the improvement is marginal, suggesting that not all scenes benefit equally from 3D awareness.\nAnalysis of ID switches by object class. To further understand our method's performance in maintaining consistent object identities, we analyze the number"}, {"title": "4.4 Ablations", "content": "Influence of different components on tracking. Our tracking formulation consists of four components (Eqs. (5) to (8)): instance cost, category cost, 3D location cost, and visual feature cost. We evaluate the influence of each component by turning off the corresponding cost one at a time in the cost-matching formulation. Table 3 shows that all components contribute positively to the tracking performance, but to varying degrees. Removing the visual features has the least impact, reducing the HOTA score from 27.72 to 27.17. The 3D location information proves more important, with its removal causing the HOTA score to drop"}, {"title": "Sensitivity to hyperparameters.", "content": "We evaluate the sensitivity of our method by varying the values of the 4 hyperparameters: \u03b1s, \u03b1c, \u03b1\u03b9, \u03b1\u03c5 in the costmatching formulation Eq. (2) from Eqs. (5) to (8). We perform this analysis on a subset of 5 videos, using 3 representative values for each hyperparameter, resulting in 34 = 81 configurations. Figure 3 shows that 57 out of these 81 hyperparameter configurations lead to a HOTA score in the range 27.2 \u00b1 0.2, which shows the robustness of our method to these parameters. There are some configurations, e.g. when as = 100 or ac = 100, that lead to a degradation in performance."}, {"title": "Metrics across IoU thresholds.", "content": "As described in Section 4.2, HOTA, DetA, and AssA can be calculated at different IoU thresholds. Figure 4 illustrates how these metrics change as the IoU threshold increases. As expected, all metrics decrease with higher thresholds, as stricter overlap requirements lead to fewer True Positive matches between predicted and ground-truth segments. Notably, our method consistently outperforms DEVA across all thresholds for both HOTA and AssA metrics, while he AssA curve shows a more pronounced improvement. This suggests that our 3D-aware approach is particularly effective at maintaining consistent object identities throughout the video sequence, even under strict evaluation criteria."}, {"title": "4.5 Downstream applications", "content": "Our 3D-aware instance segmentation and tracking method yields longer and more consistent tracks than 2D methods. This improvement enables two key downstream applications: 3D object reconstruction and amodal segmentation.\nReconstruction of objects. The longer, more consistent tracks produced by our method allow us to extract the same object from many frames using the output instance ID. This multi-view information is crucial for achieving high-quality 3D reconstructions. This is something that fragmented or inconsistent tracks from 2D methods often fail to achieve. Additionally, our 3D tracking approach, which uses lifted centroids, allows us to determine the time ranges when an object remains static. We leverage these static periods for reconstruction, as they provide the most reliable geometric information. This selective use of frames is only possible due to maintaining long-term tracks of objects.\nAmodal segmentation. Amodal segmentation aims to estimate the full extent of objects, including parts that are occluded. Building upon our 3D object reconstructions, we render the reconstructed 3D object from multiple viewpoints"}, {"title": "5 Conclusion", "content": "In this paper, we presented a novel 3D-aware approach to instance segmentation and tracking in egocentric videos, addressing the unique challenges of first-person perspectives. By integrating 3D information, our method significantly improves tracking accuracy and segmentation consistency compared to state-of-the-art 2D approaches, especially over long periods. Our ablation studies highlight the importance of 3D information and the category as well as instance cost terms in matching, while also showing robustness to hyperparameter changes. Beyond improved tracking, our approach enables valuable downstream applications such as high-quality 3D object reconstructions and amodal segmentation. This work demonstrates the power of incorporating 3D awareness into egocentric video analysis, opening up new possibilities for robust object tracking in challenging first-person scenarios."}, {"title": "A Implementation details", "content": "In the EPIC Fields [34", "32": "which also provides a sparse point cloud representing the static parts of the scene. We follow [28"}]}