{"title": "Bi-Fact: A Bidirectional Factorization-based Evaluation of Intent Extraction from UI Trajectories", "authors": ["Sapir Caduri"], "abstract": "Evaluating intent extraction from GUIs demands accurate, fine-grained metrics. This paper introduces Bi-Fact, a novel method that decomposes intents into atomic facts and performs bidirectional comparisons to assess precision and recall. Experiments demonstrate Bi-Fact's superior correlation with human judgments compared to existing metrics, establishing a more robust evaluation framework for UI-driven intent understanding.", "sections": [{"title": "1 Introduction", "content": "Understanding user intent from interactions with graphical user interfaces (GUIs) is an area of growing interest. This involves translating sequences of UI events, such as clicks, scrolls, and text inputs, into a high-level description of the user's desired outcome. This extracted intent is crucial for various downstream applications, including personalized user experiences, and proactive suggestions.\nHowever, these applications depend on the accuracy of individual extracted facts. For instance, consider the following gold intent \"book an business flight to Paris\" if the model prediction was \"book a flight to Paris\", although its not fully correct most of the facts were correctly identified. Knowing them allows the model to generate followup actions for the user and learn useful memory items about the users activity. Therefore, a robust evaluation framework is needed to assess the accuracy of intent predictions at a granular, fact-based level.\nRecent research has begun to address intent extraction from UI interactions (Berkovitch et al., 2024; Sun et al., 2024; Huang et al., 2024; Mart\u00ednez-Rojas et al., 2024; Zhang et al., 2024; Yang et al., 2024). However, evaluating these extracted intents remains a hurdle. Existing metrics struggle to capture the nuances of UI-driven intents, especially regarding factual accuracy.\nTraditional text similarity measures like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) rely on lexical overlap and are ill-suited for capturing semantic equivalence in UI contexts. Similarly, semantic similarity metrics and inference-based methods like Natural Language Inference (NLI) (Bowman et al., 2015) often fall short. While full-intent level NLI can assess overall entailment between two intents, it may not capture the fine-grained factual discrepancies crucial for downstream tasks. For example, a detailed request for a trip booking on Monday and a similar request for that trip on Tuesday are related but do not strictly entail or contradict each other, potentially leading NLI models to miss the pivotal differences in dates. Fact-level evaluation methods, such as FactScore (Min et al., 2023), offer a more granular approach by decomposing generated text into factual claims and checking if these are supported by a reference text.\nHowever, evaluating intent understanding in UI interactions requires considering both the recall and precision of factual information. Consider a UI trajectory where a user searches for \"restaurants near Central Park.\" A model extracts the intent \"find restaurants open after 10 PM in Central Park.\" While assessing whether this extracted intent recalls the facts from the user's search is valuable, it doesn't address whether the extracted intent introduces overly specific information. A comprehensive evaluation requires assessing both precision (are all extracted facts correct and relevant?) and recall (are all user-intended facts captured?).\nIn this work, we propose Bi-Fact, a novel fact-level evaluation method specifically designed for assessing the equivalence of intents extracted from UI interactions. Bi-Fact leverages a fact-based comparison, enabling a more detailed and accurate evaluation than existing metrics. The core idea involves decomposing intents into their constituent facts and comparing these facts' validity. We demonstrate that Bi-Fact exhibits significantly high correlation with human judgments of fact-level intent equivalence. This indicates that Bi-Fact provides a more robust and informative evaluation framework for accurately assessing intent predictions in UI interaction scenarios."}, {"title": "2 The Bi-Fact method of Evaluating Intent Extraction", "content": "Evaluating the accuracy of intent extraction requires a nuanced, fact-based approach."}, {"title": "2.1 Approach", "content": "At a high-level, given a gold intent and a predicted intent, we would like to decompose both intents into atomic facts, representing single, indivisible pieces of information. For example, the intent \"book a flight to Paris for a weekend business trip\" might be decomposed into facts"}, {"title": "2.2 Automatic Evaluation with Bi-Fact", "content": "We introduce Bi-Fact, an automatic evaluation method utilizing a large language model (LLM). The methodology consists of the following two primary stages, with the assessment stage further divided into three reasoning steps:\nPart 1: Factoring the Gold Sentences: Initially, the gold sentences are factorized into atomic facts. During this stage, the factual content of the gold sentences is frozen, providing a stable basis for subsequent comparison of future models. The prompt used for this factorization is detailed in the Appendix 2.\nPart 2: Factual Coverage Assessment: This stage involves assessing the factual coverage of the predicted sentences against the factorized gold sentences. This assessment is conducted in three distinct reasoning steps:\n1. Reasoning Step 1: Factoring the Prediction: The LLM first decomposes the predicted intent into its atomic facts, ensuring a consistent level of granularity for comparison.\n2. Reasoning Step 2: Assessing the Expert Facts: The LLM analyzes each expert-provided fact (from the gold reference) and labels it as either 'derived' or 'not derived' based on its presence or absence in the factorized prediction. This step focuses on recall, measuring whether all facts from the gold standard are captured.\n3. Reasoning Step 3: Assessing the Predicted Facts: The LLM analyzes each fact in the factorized prediction and labels it as either 'derived' or 'not derived' based on its presence or absence in the gold standard. This step focuses on precision, ensuring that the predicted intent does not contain extraneous or incorrect information.\nThe prompt used for this process is detailed in Appendix 3.\nBi-Fact's strength lies in its atomic-level operation. This granular approach breaks down complex sentences for accurate comparison, preserves meaning by focusing on factual content, and provides fine-grained assessments with 'derived'/'not derived' labels. By reducing ambiguity and estimating recall/precision through atomic-level evaluation, Bi-Fact ensures a comprehensive assessment."}, {"title": "3 Evaluating Effectiveness of Bi-Fact", "content": ""}, {"title": "3.1 Datasets", "content": "We utilize two datasets to evaluate the effectiveness of our proposed Bi-Fact metric."}, {"title": "3.2 Comparison to other approaches on Intent-match", "content": "We compare Bi-Fact against standard text similarity metrics, including BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (?), T5 cosine similarity (?), and NLI-based methods (Bowman et al., 2015). We binarized the continuous scores using thresholds that yielded the best performance in terms of F1 score on the development set (testing 30 equally spaced increments between 0.01 and 1.0). For the NLI-based method, we used a bidirectional approach, taking the average of the NLI scores for both directions (premise vs. hypothesis and hypothesis vs. premise). We also include the Automatic Rater (AutoRater) introduced in (Berkovitch et al., 2024). To assess the effectiveness of these metrics, we evaluate the correlation between each of these scores and human judgments on the intent-match data using recall, precision, F1 score, and kappa. The results are in Table 1. Bi-Fact achieves the highest F1 score (0.722) and Kappa coefficient (0.508) among all tested metrics, indicating better agreement with human judgments. The results highlight the limitations of purely lexical overlap based metrics for capturing the nuances of intent in UI interactions."}, {"title": "3.3 Comparison using Fact-level annotations", "content": "We further validated Bi-Fact by analyzing its correlation with manual fact-level annotations. This analysis was conducted on the Manual Fact-Level dataset described in Section 3.1, providing insights into Bi-Fact's ability to capture fine-grained factual information that is highly crucial to the downstream tasks. Bi-Fact's F1 score"}, {"title": "4 Discussion", "content": "The superior performance of Bi-Fact can be attributed to its ability to decompose intents into atomic facts, enabling it to capture deeper semantic and functional similarities between intents, which are crucial for UI-based applications. The high correlation with manual fact-level annotations further demonstrates Bi-Fact's effectiveness in accurately assessing the factual alignment between intents."}]}