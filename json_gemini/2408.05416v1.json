{"title": "High-fidelity and Lip-synced Talking Face Synthesis via Landmark-based Diffusion Model", "authors": ["Weizhi Zhong", "Junfan Lin", "Peixin Chen", "Liang Lin", "Guanbin Li"], "abstract": "Audio-driven talking face video generation has attracted increasing attention due to its huge industrial potential. Some previous methods focus on learning a direct mapping from audio to visual content. Despite progress, they often struggle with the ambiguity of the mapping process, leading to flawed results. An alternative strategy involves facial structural representations (e.g., facial landmarks) as intermediaries. This multi-stage approach better preserves the appearance details but suffers from error accumulation due to the independent optimization of different stages. Moreover, most previous methods rely on generative adversarial networks, prone to training instability and mode collapse. To address these challenges, our study proposes a novel landmark-based diffusion model for talking face generation, which leverages facial landmarks as intermediate representations while enabling end-to-end optimization. Specifically, we first establish the less ambiguous mapping from audio to landmark motion of lip and jaw. Then, we introduce an innovative conditioning module called TalkFormer to align the synthesized motion with the motion represented by landmarks via differentiable cross-attention, which enables end-to-end optimization for improved lip synchronization. Besides, TalkFormer employs implicit feature warping to align the reference image features with the target motion for preserving more appearance details. Extensive experiments demonstrate that our approach can synthesize high-fidelity and lip-synced talking face videos, preserving more subject appearance details from the reference image.", "sections": [{"title": "I. INTRODUCTION", "content": "Audio-driven talking face video generation, a challenging task of cross-modal synthesis, aims to create talking videos with lip movements that are accurately synchronized with the input audio. This task has attracted increasing interest from the research community due to its broad applications such as visual dubbing [1], virtual avatars [2], and digital humans [3]. To generate high-fidelity talking face videos, a prevalent manner is to gather data of the target individual to learn a person-specific model [4]\u2013[7]. While effective, these person-specific methods are hindered by the costly data collection and extensive training process. In contrast, person-generic methods can generalize to unseen subjects without further training. However, these methods often grapple with challenges in maintaining the appearance details of subjects as well as lip-audio synchronization. In this study, we endeavor to develop a person-generic talking face generation framework to generate high-fidelity facial details for general subjects while ensuring accurate lip-audio synchronization.\nTo achieve faithful talking face generation, two critical issues need to be considered: the high fidelity of the subject appearance details and the synchronization of lip movement with the audio input. To generate lip-synced talking videos, prior methods [8]\u2013[10] directly model the mapping between audio signal and visual content. However, such audio-visual mapping is often uncertain and ambiguous, as one phonetic unit potentially matches various visual forms due to the diversities in illumination, emotion, and appearance. This often leads to flawed results and the loss of details in subject appearance. To ease the ambiguity for improved fidelity of subject appearance, another line of works [1], [11]\u2013[15] instead first establish a correlation between the audio signals and intermediate structural representation such as facial landmarks or 3D Morphable Model (3DMM) [16], which primarily reflect the motion information and facilitate the less ambiguous mapping from audio to motion. Then, another stage converts these motion representations into realistic facial images. However, a notable drawback of these approaches is the isolated training of different stages, potentially resulting in inaccuracies in lip-audio synchronization stemming from error in the pre-estimated structural representations. Additionally, prior methods [8], [9], [11], [15] frequently rely on either misaligned reference images or images warped by imprecise optical flow predictions as generation conditions, neglecting the influence of such misalignment on the preservation of appearance details. Moreover, most previous approaches employ generative adversarial networks (GANs) [17] for talking face generation, which often suffers from training instability and the issue of mode collapse.\nTo tackle the challenges above, we propose an innovative landmark-based diffusion model to learn the audio-visual relation. Figure 1 compares previous methods and ours. Our method utilizes facial landmarks as intermediate representation to ease the ambiguity of audio-visual mapping while enabling the end-to-end optimization of distinct stages, facilitating the generation of high-fidelity and lip-synced talking face video. Specifically, our two-stage approach initially converts the input audio signal into a set of landmark representations using a landmark completion module [11]. To enable the end-to-end optimization of distinct stages and align the synthesized motion with the motion represented by landmarks, we devise a novel conditioning module called TalkFormer to integrate landmark representations into the diffusion model using differentiable cross-attention. This end-to-end approach significantly reduces error accumulation resulting from pre-estimated landmark inaccuracies, thereby improving lip-audio synchronization. Besides, to enhance the fidelity of facial appearance, our approach converts a reference facial image into multi-scale features capturing intricate details. Then, our proposed TalkFormer module spatially aligns these features with the target motion using an implicit warping technique. Without using imprecise optical flow, the implicit warping automatically establishes semantic correlations for improved alignment between the reference features and the synthesized content. These aligned reference features are then integrated into the denoising process, enhancing the preservation of subject appearance details from reference image. Our extensive experiments validate our framework's effectiveness in producing realistic talking face videos with high-fidelity subject appearance and lip movements accurately synchronized to the input audio. We summarize our contributions as follows:\n\u2022\nWe propose a novel method to learn the audio-visual relationship for generating high-fidelity and lip-synced talking face videos, utilizing facial landmarks as intermediate representation to ease the ambiguity of audio-visual mapping while enabling end-to-end optimization to minimize error accumulation.\n\u2022\nWe introduce a novel conditioning module, TalkFormer, to align the synthesized motion with the motion represented by landmarks in a differentiable manner, enabling the joint optimization of distinct stages. Additionally, TalkFormer aligns the reference image features with the target motion based on semantic correlations, enhancing the preservation of subject appearance details.\n\u2022\nWe conduct comprehensive experiments to demonstrate our method's effectiveness in producing high-fidelity and lip-synced talking face videos, which can generalize to any unseen subject without additional fine-tuning."}, {"title": "II. RELATED WORK", "content": "Audio-driven talking face video generation techniques can be mainly divided into two types: person-specific or person-generic. Many person-specific methods can generate vivid videos [4]\u2013[7], [18], [19], but they require videos of the target subject for additional training. On the contrary, person-generic methods [1], [8]\u2013[14], [20] enable inference on unseen subjects without any retraining or fine-tuning. However, there is still a gap in achieving high-fidelity and lip-synced talking face generation for person-generic methods.\nWav2Lip [8], PD-FGC [10], and PC-AVS [20] attempt to generate lip-synced videos by directly conditioning the generator on audio representation. Nevertheless, these approaches exhibit notable flaws and loss of appearance details in the subjects due to the inherent uncertainty and ambiguity in audio-visual mapping. IP-LAP [11] and other methods [1], [12] propose a two-stage framework that utilizes facial landmarks as intermediate representation. However, the projection of their intermediate landmark representation into the sketch image is indifferentiable. Subsequently, an image-to-image translation network is employed to synthesize realistic faces from the sketch images. Therefore, these methods train distinct stages independently and suffer from the inaccuracies of pre-estimated landmarks. Besides, prior methods [13]\u2013[15], [21] including IP-LAP [11] utilize estimated optical flow to align the reference image with target facial expression and pose, such that more appearance details from reference image can be preserved during the generation process. However, accurate optical flow estimation is challenging especially when there is significant variation in head pose, leading to distorted results. To tackle the drawbacks of GANs [17] in unstable training and mode collapse, DiffTalk [9] crafts a diffusion model to learn the direct audio-to-visual mapping for generalized talking face generation. It directly models the audio-to-lip translation with the landmarks of upper-half face concatenated as auxiliary condition. However, the landmarks of upper-half face are insufficient to alleviate the uncertainty of direct audio-to-visual mapping. Besides, its usage of the misaligned reference image hinders the preservation of subject appearance details from reference image. Recently, GAIA [22] proposes to disentangle motion and appearance using VAE [23] and utilizes diffusion models to predict motion from the speech. However, it can not achieve end-to-end learning of the framework to reduce error accumulation. Besides, it leverages misaligned reference appearance features as generation conditions, which hinders the preservation of facial details from reference images."}, {"title": "III. METHODOLOGY", "content": "The framework overview of our method is presented in Figure 2. For talking face video generation, our method inputs audio and a template video, masking the lower-half face. The framework then inpaints these areas with realistic content synchronized with the audio. Information about the subject appearance and facial contours is derived from a single reference image and reference full-face landmarks from the template video, respectively. More specifically, the audio signal drives the completion of lip and jaw landmarks, guided by reference full-face landmarks and pose landmarks detected from the upper-half face of the template video. The completed landmarks, as well as the reference image, are then fed into the latent diffusion model via TalkFormer, influencing the synthesized motion and appearance. During training, the network diffuses the lower half of the ground-truth face in latent space, focusing on noise reduction. Upcoming sections Section III-A, Section III-B, and Section III-C will introduce latent diffusion models and provide a comprehensive explanation of our approach."}, {"title": "A. Preliminaries of Latent Diffusion Models", "content": "Latent diffusion models [27] carry out diffusion and the denoising process in the encoded latent space of an autoencoder D(E(\u00b7)), with E(\u00b7) being the encoder and D(\u00b7) being the decoder. A U-Net-based [44] denoising network $\\epsilon_{\\theta}(z_t,t)$ is trained to predict the noise added to the image latent $z_0$, where $z_0=E(x)$, x is the input image, $z_t$ represents the noisy version of $z_0$ at time step condition $t \\in \\{1,2,...,T\\}$ and $\\theta$ refers to the learnable parameters. The optimization objective during training is as follows:\n$\\mathcal{L}_{ldm} = E_{z, \\epsilon ~ N(0,1), t} [ || \\epsilon - \\epsilon_{\\theta}(z_t, t) ||^2 ]$  (1)\nwhere $\\epsilon$ is the ground-truth noise added to the image latent $z_0$ and t is uniformly sampled from $\\{1, ..., T\\}$. During the inference phase, these models progressively denoise a normally distributed variable $z_T ~ N(0, 1)$ until it reaches a clean latent $z_0$. This clean latent variable can then be decoded by D to synthesize realistic images.\nIn our framework, both diffusion and denoising processes are exclusively performed in the lower half of the encoded latent, with the remaining upper half also being incorporated into denoising U-Net to provide more context. During inference, the masked input face is encoded as $z_m$, of which the lower half is diffused to obtain the initial $z_T$. During training, the ground-truth face is first encoded to $z_0$ and subsequently diffused to $z_t$ by the noise."}, {"title": "B. Audio-driven Landmark Completion", "content": "Instead of directly conditioning the diffusion model on the audio signal, our framework first establishes the less ambiguous mapping from audio to landmarks motion of lip and jaw. Following [11], we devise a transformer-based landmark completion module to predict the lip and jaw landmarks from the input audio. Specifically, we first encode the pose landmarks from the upper half of the face into a pose embedding using a 1D convolutional module. For facial contour information, we extract reference full-face landmarks from N video frames within the input video, and encode them into N reference embeddings via another 1D convolutional module. The mel spectrogram of the input audio is encoded into an audio embedding by a 2D convolutional module. These pose, reference, and audio embeddings are subsequently fed into a transformer encoder to predict the lip and jaw landmark coordinates, denoted as $C^{lip} \\in \\mathbb{R}^{2\\times n_i}$ and $C^{jaw} \\in \\mathbb{R}^{2\\times n_j}$, respectively, where $\\tau$ indicates landmarks for the $\\tau$-th frame, $n_i$ and $n_j$ are the number of landmarks used to represent the lip and jaw, respectively. To ensure temporally stable landmark prediction, we adopt the batched sequential training strategy following the common practice of previous methods [45]\u2013[47]. Specifically, the completion module predicts landmarks of L successive frames for each video during training. The training objective for the landmark completion module is defined as follows:\n$\\mathcal{L}_{L} = \\sum_{i=0}^{L-1}( || C^{lip}_i - \\hat{C}^{lip}_i ||_1 + || C^{jaw}_i - \\hat{C}^{jaw}_i ||_1 )$ (2)\nwhere $\\hat{C}^{lip}$ and $\\hat{C}^{jaw}$ are the ground-truth landmarks coordinates of lip and jaw, respectively. The predicted lip and jaw landmarks are then combined with the input pose landmarks to form the comprehensive target full-face landmarks.\nHowever, the objective in Equation (2) is insufficient to synchronize the lip and jaw motion with input audio due to potential inaccuracies inherent in the pre-estimated ground-truth landmarks. Therefore, we expect the predicted landmarks to be integrated into the image generation stage (Section III-C) in a differentiable manner, enabling end-to-end optimization to improve lip synchronization."}, {"title": "C. Inpainting Lower Half via Latent Diffusion Model", "content": "As GANs [17] suffer from training instability and mode collapse, we resort to powerful latent diffusion models [27] to inpaint the lower half of the face, conditioning on the completed landmarks and reference image. For the end-to-end optimization of the whole framework and improved alignment between the reference image and the synthesized content, we introduce a novel conditioning module called TalkFormer, as demonstrated in the green section of Figure 2. TalkFormer aligns the synthesized motion with the motion represented by facial landmarks via differentiable cross-attention [48], and aligns the reference image features via an implicit warping manner implemented by another cross-attention layer. In our denoiser U-Net, TalkFormer modules exist at all scales, except the first scale which only contains residual convolution blocks. In the following subsections, we will detail the core components of TalkFormer and the reference appearance encoder for encoding reference facial image."}, {"title": "1) TalkFormer: Align Talking Motion Differentiably", "content": "Previous researches [1], [11], [12] project the intermediate landmark representation on the image plane, forming the sketch image as generation condition in an indifferentiable manner. In contrast, our TalkFormer first uses a 1D-convolution embedding module to encode the target full-face landmarks from the landmark completion module into n landmark embeddings $\\{e_i, i = 1,2,...,n\\}$, where n is the number of landmarks to represent the full face. Then, these landmark embeddings are integrated into cross-attention layers as keys and values, denoted as $K_1$ and $V_1$, respectively. Simultaneously, the queries $Q_1$ are extracted from the hidden features after ResNet [49] blocks through an MLP layer. The output of cross-attention is computed as Y according to the following equation:\n$Y = Softmax( \\frac{Q_1K_1}{\\sqrt{d_1}} )V_1$ (3)\nwhere $d_1$ is the dimension of queries and keys. Subsequently, the results Y go through a zero-initialized convolution layer and are added to the hidden features of U-Net in a residual manner. In this way, the final generated face is ensured to have talking motion aligned with the motion represented by landmarks, and the diffusion model can be jointly optimized with the landmark completion module for improved lip synchronization."}, {"title": "2) Reference Appearance Encoder", "content": "To enable generalized talking face generation, a single reference face image is typically utilized as condition, ensuring that the synthesized appearance remains consistent with the subject appearance. As illustrated in the pink section of Figure 2, the reference face image is initially encoded to the latent space as $z_r$. To retain more fine-grained details from the reference face image, we devise an appearance encoder similar to the U-Net encoder consisting of residual convolution blocks. This appearance encoder converts the latent $z_r$ into multi-scale reference features symbolized as $F_a = \\{F^a_i | i = 1, 2, .., I\\}$, where I represents the number of scales in U-Net. The dimensions of these features are identical to those of the hidden features in the encoder of U-Net denoiser."}, {"title": "3) TalkFormer: Align Reference Appearance Features", "content": "To make the denoiser model aware of more appearance details from the reference image, we align the multi-scale reference appearance features in an implicit warping manner through another cross-attention layer. Specifically, we denote the hidden features after talking motion alignment as $F_r \\in \\mathbb{R}^{D\\times H \\times W}$, where scale i $\\in \\{2,3,...,I\\}$. To spatially align the reference appearance features $F^a_i$ with the hidden features $F_r$, the $F_r$ is first transformed to the queries $Q_2 \\in \\mathbb{R}^{HW\\times d_2}$ through an MLP layer while the $F^a_i$ are projected to the keys $K_2 \\in \\mathbb{R}^{HW\\times d_2}$ and values $V_2 \\in \\mathbb{R}^{HW\\times D}$, where $d_2$ is the dimension of the keys. Then, the correlation matrix between $F^a_i$ and $F_r$ is computed as follows:\n$S = Softmax( \\frac{Q_2K^T_2}{\\sqrt{d_2}} )$ (4)\nwhere $S \\in \\mathbb{R}^{HW \\times HW}$, and each element $s_{jk}$ of it indicates the semantic correspondence between the hidden feature in location j and the reference feature in location k, with $j, k \\in \\{1,2,..., HW\\}$. Based on this correlation matrix, we can obtain the aligned reference features by referring to the relevant features in the reference appearance features. Specifically, the reference appearance features $F^a_i$ are warped implicitly via a weighted sum of the values in $V_2$ as follows:\n$\\hat{F}^a_i = Reshape(SV_2)$ (5)\nwhere $F_i \\in \\mathbb{R}^{D\\times H \\times W}$, and its semantic contents are spatially aligned with those of the hidden features $F_r$. Eventually, the aligned reference appearance features $\\hat{F}^a_i$ are passed through a zero-initialized convolution layer and added to the hidden features $F_r$ using a residual way. Consequently, the denoising process can better preserve the subject appearance details from reference images, facilitating high-fidelity talking face video generation."}, {"title": "D. Optimization", "content": "Benefit from TalkFormer, the joint optimization of the landmark completion module and the latent diffusion model can be achieved by employing the following objective function:\n$\\mathcal{L}_{total} = \\mathcal{L}_{ldm} + \\lambda \\mathcal{L}_{L}$ (6)\nwhere $\\mathcal{L}_{ldm}$ is the denoising objective defined in Equation (1) and $\\lambda$ represents the weight assigned to the $\\mathcal{L}_{L}$ loss term (Equation (2)). In this way, the $\\mathcal{L}_{L}$ loss will guide the landmark completion module to predict more accurate landmarks for better denoising, thus enhancing lip-audio synchronization. Similar to the landmark completion module for improved temporal continuity, the latent diffusion model adopts the batched sequential training strategy [45]\u2013[47] where L successive images are synthesized for each video during training."}, {"title": "IV. EXPERIMENTS", "content": "Dataset: We conduct experiments on two public audio-visual datasets, VoxCeleb [50] and HDTF [14]. VoxCeleb is a collection of over 100,000 utterances from 1,251 celebrities, all extracted from videos uploaded to YouTube. HDTF is a high-resolution audio-visual dataset consisting of approximately 362 distinct videos, spanning over 15.8 hours, in 720P or 1080P resolutions. Compared to HDTF, the large-scale VoxCeleb dataset is a more standard benchmark commonly used in prior work. To ensure a fair comparison, all comparison methods, including ours, are trained on the VoxCeleb dataset and evaluated using the test sets of both VoxCeleb and HDTF.\nEvaluation Metric: We quantitatively evaluate all methods regarding visual quality and lip synchronization. Pixel-level visual quality is assessed through the Peak Signal-to-Noise Ratio (PSNR) and Structured Similarity (SSIM) [51], while feature-level visual quality is evaluated using Learned Perceptual Image Patch Similarity (LPIPS) [52] and Fr\u00e9chet Inception Distance (FID) [53]. Compared to pixel-level measurements, the feature-level measurements are more in line with human perception [52], [54]. Additionally, we employ the cosine similarity (CSIM) of identity vectors extracted by the ArcFace face recognition network [55] to assess the preservation of subject identity. SyncScore [56] is commonly used by prior work to evaluate the lip-audio synchronization quality, despite some limitations.\nComparison Methods: We compare our approach against several state-of-the-art person-generic audio-driven talking face video generation methods. DiffTalk (CVPR'23) [9] constructs a Diffusion-based framework for generalized talking face synthesis by conditioning the latent diffusion model on audio signal. PD-FGC (CVPR'23) [10] employs a progressive disentangled representation learning strategy to achieve fine-grained controllable talking face synthesis (e.g., eye, pose control). IP-LAP (CVPR\u201923) [11] is a two-stage landmark-based method that trains different stages separately and utilizes predicted optical flow to align the reference image with the target pose and expression. PC-AVS (CVPR\u201921) [20] proposes a GAN-based framework to generate pose-controllable talking face videos by modularizing audio-visual representations. Wav2Lip (MM'20) [8] utilizes a lip sync discriminator to guide the generator in generating lip-synced talking face videos.\nComparison Setups: Wav2Lip [8], IP-LAP [11], DiffTalk [9], and our method all generate talking face videos by inpainting the lower half of the face. Therefore, during the quantitative comparison, the lower half of the face in the input video is masked. Then, these methods reconstruct the masked area guided by the input audio and reference image. The original input video serves as the ground truth for metric calculation. We train DiffTalk [9] using the official code until convergence, but it generates temporally unstable results. It relies on additional frame interpolation to smooth the results, affecting comparison fairness. Therefore, the frame interpolation post-processing was not employed for fair comparison. PC-AVS [20] utilizes a pose source video, an audio input, and a reference image to generate a talking face video. In our implementation, we substitute its pose source video with the ground-truth video. PD-FGC [10] requires a pose source video, an expression source video, an eye blink source video, an audio input, and a reference image to generate a talking face video. Our version replaces its pose source, expression source, and eye blink source videos with the ground-truth video.\nImplementation Details: In our framework, the input face images are resized to 256\u00d7256, and the latent space of autoencoder D(E(\u00b7)) has a spatial dimension of 64\u00d764. Facial landmarks are extracted from video frames using the mediapipe tool [57]. We represent the lip with $n_i$ = 41 landmarks, the jaw with $n_j$ = 16 landmarks, and the entire face with a total of n = 131 landmarks. We set N to 5 and the reference full-face landmarks are detected from the randomly selected frames of input videos. The hyperparameter $\\lambda$ is set to 10 and L set to 5. To generate talking face videos, we employ the DDIM [26] diffusion sampler with 200 steps. We set the number of diffusion steps T to 1000. The number of scales I is 4, but we illustrate the case of I = 3 in Figure 2 for clarity. The reference face image can be any face image from the input video that reflects as many appearance details of the subject as possible. All comparison methods use the same reference face image to ensure a fair evaluation. Our implementation of the proposed method closely follows the code implementation of latent-diffusion [58], while incorporating TalkFormer, Appearance Encoder, and Landmark Completion module [11] as additional components. The Appearance Encoder is designed based on the encoder structure of U-Net denoiser in latent-diffusion [58], excluding self-attention layers.\nWe train our framework on 2 NVIDIA A100(40GB) GPUs for 500 epochs with Adam optimizer [59]. The batch size is 64, and the learning rate is 4e-5. The pre-trained autoencoder is frozen during training. The Landmark Completion module is jointly trained from scratch with the latent diffusion model. Our method focuses on inpainting the lower half of the face based on the input audio. Hence, to generate talking face videos, we first crop the face area from the input template video as network input. After obtaining the generated face, we employ a post-processing technique following the previous method [11] to seamlessly blend it with the background, producing final talking face videos. For fair comparison, this post-processing technique was not employed during the quantitative and qualitative comparisons. The input template video has no length requirement and can be looped to match the length of the input audio. We will release our code upon acceptance."}, {"title": "B. Quantitative Evaluation", "content": "We conduct a comprehensive quantitative comparison with state-of-the-art methods regarding visual quality and lip synchronization. The visual quality metrics are calculated solely based on the lower half of the generated face, since the generated upper-half face in Wav2Lip [8], IP-LAP [11], DiffTalk [9], and ours almost inherit from the input video (i.e., ground truth). The comparison results are reported in Table I.\nVisual Quality: Our method outperforms other methods in all visual quality metrics (PSNR, SSIM, LPIPS, FID, CSIM) on both VoxCeleb [50] and HDTF [14] datasets. Specifically, on the perceptual distance metric LPIPS and FID, our method significantly improves over other methods. This verifies that our method can produce high-fidelity talking face videos that align with human perception, preserving more appearance details. Besides, the highest CSIM score achieved by ours also indicates our method can preserve more identity information of the target subject. Although Wav2Lip exhibits a slight lag in terms of PSNR and SSIM metrics compared to our method, its FID and LPIPS values are approximately twice as high as ours, suggesting the presence of artifacts that is not aligned with human perception in their results. The performance of IP-LAP is closely comparable to ours in terms of the PSNR, SSIM, and CSIM metrics, but there remains a certain gap between IP-LAP and ours when assessed on the LPIPS, FID, and SyncScore metrics. While DiffTalk exhibits comparable performance in the FID metric, it still lags behind our approach when assessed on the LPIPS and CSIM metrics.\nLip Synchronization: Due to different speaking styles among individuals, accurate quantitative assessment of lip-audio synchronization remains a persistently challenging task. A common practice is to calculate the SyncScore based on the audio and visual features of SyncNet [56]. Wav2Lip, PC-AVS, and PD-FGC directly model the audio-visual mapping and obtain better SyncScore than ours. However, our approach notably excels in visual quality metrics, particularly in preserving the finer details of subject appearance, an aspect where others have room for improvement. Wav2Lip utilizes SyncNet [56] as a discriminator during training. Hence, it achieves a very high SyncScore, even higher than that of the ground truth. Besides, PD-FGC and PC-AVS adopt audio-visual contrastive learning similar to SyncNet [56], which contributes to a higher SyncScore but compromises visual quality. IP-LAP leverages facial landmarks as intermediate representations, but its lip synchronization is inferior to ours due to the isolated training of different stages. DiffTalk directly models the audio-visual mapping and generates temporally unstable videos with poor lip synchronization. We suspect its issue might stem from the mapping ambiguity magnified by the multi-step iteration of diffusion model."}, {"title": "C. Qualitative Evaluation", "content": "As shown in Figure 3, we present some representative comparison results on the HDTF [14] and VoxCeleb [50] datasets. It can be observed that our results are visually closer to the ground-truth images than other methods', with more appearance details (e.g., beard, lip, teeth) preserved. It implies that our TalkFormer module could effectively align the reference image features based on semantic correlation, providing valuable features for the diffusion model. Besides, the lip shapes of ours are also closer to the ground truth. DiffTalk, PD-FGC, PC-AVS, and Wav2Lip directly learn the audio-visual mapping and utilize a misaligned reference image as generation condition. Therefore, their results lose some appearance details of the subject and appear blurry. IP-LAP generates results that are blurrier than ours and exhibit some artifacts, possibly due to the inaccurate optical flow estimation for aligning reference images. For more qualitative comparisons, please refer to the supplementary video as detailed in the following subsection."}, {"title": "2) Supplementary Video", "content": "We have provided a short video as supplementary material. Please download and watch it. If there are any issues downloading the video from the review system, the same file can also be downloaded through this backup link. The time schedule of the video is as follows:\nVideo title.\nDemonstration of Ours. We demonstrate a generated result of our method where the driving audio is sourced from the text-to-speech technique, and the subject is from the HDTF [14] dataset. We also visualize the intermediate landmarks after the landmark completion module.\nMethod Comparison. We present the results of all methods as well as ground-truth videos for comparison.\nAblation Study. We present the qualitative results of the ablation study, which will be further analyzed in the Section IV-D.\nMore results of our method. We provide the testing results of our method for more cases."}, {"title": "3) User Study", "content": "For comprehensive evaluation, we conduct a user study where 16 volunteers are invited to assess the generated videos of all comparison methods. We randomly sample 10 videos for testing, 5 from HDTF [14] dataset and 5 from VoxCeleb [50] dataset. Volunteers are asked to give their rates (0-5) for each generated video regarding image quality, lip-audio synchronization, and fidelity of appearance details. The videos are presented to participants in a random order and the evaluation criteria is explained in detail to the participants. The mean opinion scores (MOS) of each method are presented in Table II. Our method receives better evaluations from participants across three dimensions than other approaches."}, {"title": "D. Ablation Study", "content": "In this section, we conduct an ablation study on the HDTF [14] dataset to verify the effectiveness of the proposed end-to-end framework and TalkFormer conditioning module. The numerical results are reported in Table III, while the qualitative results are presented in Figure 4, as well as in the supplementary video."}, {"title": "1) Effect of End-to-End Training", "content": "Our two-stage landmark-based method achieves the joint optimization of landmark completion module and latent diffusion model to improve lip synchronization. To validate the effectiveness of end-to-end optimization, we devise a variant where these two modules are trained separately. Specifically, the landmark completion module is optimized using only $\\mathcal{L}_{1}$ loss (Equation (2)). In the denoiser U-Net of latent diffusion model, TalkFormer accepts the pre-estimated ground-truth landmarks as condition. The latent diffusion model is then optimized using only $\\mathcal{L}_{ldm}$ loss (Equation (1)).\nThe numerical results of this variant are reported in the \"Ours w/o End2End\" row of Table III. In terms of visual quality metrics, this variant exhibits similar performance to our full model. However, the SyncScore of this variant drops 9.71% compared to the full model's, verifying the effectiveness of end-to-end training in improving lip-audio synchronization."}, {"title": "2) Effect of Talking Motion Alignment in TalkFormer", "content": "Our TalkFormer module aligns the synthesized motion with the motion represented by landmarks via cross-attention layer. To implement a variant without talking motion alignment, we remove the first cross-attention layer in TalkFormer that integrates landmarks embeddings as condition. Following the practice of DiffTalk [9], we redesign the landmark embedding module composed of multiple MLP layers to encode the target full-face landmarks into a single landmark embedding. This landmark embedding is added to all spatial locations of the hidden features in U-Net.\nThe numerical results of this variant are reported in the \"Ours w/o M-Align\" row of Table III. It can be seen that the SyncScore drops significantly compared to the full model's. This is because the synthesized motion of the lip and jaw could not be accurately controlled through a simple addition operation. Besides, the adding operation may introduce artifacts into the generated results, deteriorating the visual quality metrics."}, {"title": "3) Effect of Reference Features Alignment in TalkFormer", "content": "To develop a variant without reference appearance features alignment, we remove the second cross-attention layer in TalkFormer that incorporates the reference appearance features, and replace it with a self-attention layer akin to DDPM [25]. Besides, the appearance encoder is removed. Following the common practice of previous researches [8], [9], the reference face image is first encoded into the latent space as $z_r$. The $z_r$ is then concatenated with the noisy latent $z_t$ along the channel dimension and fed into the U-Net denoiser network.\nThe numerical results of this variant are reported in the \"Ours w/o R-Align\" row of Table III. It can be seen that all the visual quality metrics deteriorates compared to the full model's. Although the pixel-level metrics (PSNR, SSIM) do not change significantly, the feature-level metrics (LPIPS, FID), which are more in line with human perception, increase by a large margin. The potential reason is that the diffusion model can not extract meaningful features from the misaligned reference features, resulting in the loss of subject appearance details. Besides, the CSIM metric based on identity vectors might not be sensitive to subject appearance details, therefore the CSIM decreases slightly without reference features alignment in TalkFormer. Moreover, the lip shape of the misaligned reference image might have a negative impact on the synthesized lip shape. Therefore, the SyncScore decreases without reference features alignment. Furthermore, as can be seen in the \"Ours w/o R-Align\" row of Figure 4, in the absence of reference appearance features alignment, the generated results lose some subject appearance details, resulting in some unrealistic contents. Besides, the lip shapes are less accurate influenced by the misaligned reference face image."}, {"title": "V. CONCLUSION AND DISCUSSION", "content": "In this paper, we propose a novel landmark-based framework to learn the audio-visual relationship for person-generic talking face video generation. Our framework utilizes facial landmarks as intermediate representations to alleviate the ambiguity of audio-visual mapping, while enabling end-to-end optimization to minimize error accumulation resulting from the inaccuracies of pre-estimated facial landmarks. This accomplishment can be attributed to our innovative"}]}