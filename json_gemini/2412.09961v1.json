{"title": "What constitutes a Deep Fake?", "authors": ["Kristof Meding", "Christoph Sorge"], "abstract": "When does a digital image resemble reality? The relevance of this question increases as the generation of synthetic images so called deep fakes\u2014becomes increasingly popular. Deep fakes have gained much attention for a number of reasons- -among others, due to their potential to disrupt the political climate. In order to mitigate these threats, the EU AI Act implements specific transparency regulations for generating synthetic content or manipulating existing content. However, the distinction between real and synthetic images is even from a computer vision perspective-far from trivial. We argue that the current definition of deep fakes in the AI act and the corresponding obligations are not sufficiently specified to tackle the challenges posed by deep fakes. By analyzing the life cycle of a digital photo from the camera sensor to the digital editing features, we find that: (1.) Deep fakes are ill-defined in the EU AI Act. The definition leaves too much scope for what a deep fake is. (2.) It is unclear how editing functions like Google's \"best take\" feature can be considered as an exception to transparency obligations. (3.) The exception for substantially edited images raises questions about what constitutes substantial editing of content and whether or not this editing must be perceptible by a natural person.\nOur results demonstrate that complying with the current AI Act transparency obligations is difficult for providers and deployers. As a consequence of the unclear provisions, there is a risk that exceptions may be either too broad or too limited. We intend our analysis to foster the discussion on what constitutes a deep fake and to raise awareness about the pitfalls in the current AI Act transparency obligations.", "sections": [{"title": "Introduction", "content": "Generative AI tools that can generate photorealistic images, or manipulate existing photos in a convincing manner, are now widely available. The term \"deep fake\" has been coined, referring to images generated or manipulated using deep neural networks. Actual deep fakes, and allegations of their use, play an increasingly important role in political debates and election campaigns as well as in pornographic content [Becker(2024)]. Given the potential impact of manipulated images, regulators increasingly identify a need to regulate the use of such images. Countries that have already initiated efforts to regulate deep fakes include the US, China, the UK, and South Africa [Ramluckan(2024), Murray(2024)].\nThe European Union (EU)'s AI Act includes a transparency rule for AI systems generating deep fakes and other synthetic content. The AI Act's definition relates to \u201cmanipulated\" content, which raises the question of how to differentiate between manipulation and permissible editing. Photos have never been an objective depiction of reality. In the context of photojournalism, an author has called for framing \"the production and use of reality images as 'mediated communication' rather than an 'objective truth.\u201d [Newton(2013)] The photographer's location and the focal length used, the depth of field and the lighting can create completely different visual im-"}, {"title": "Image processing \u2013 Lifecycle of a digital photo", "content": "In order to be intelligible to a viewer, digital photos always have to be processed. The following steps are often performed automatically (e.g., within the Android camera subsystem), and without the user necessarily being aware of them [Minhaz(2020)]:\n\u2022 Image sensors usually sense only one of the RGB colors per pixel, so the actual color of each pixel has to be interpolated.\n\u2022 Faulty pixels are difficult to avoid during the production of image sensors, so they have to be detected and addressed.\n\u2022 Sensor noise (random fluctuations of brightness and color), which is most problematic in low light situations, can be removed, potentially at the cost of losing some image detail.\n\u2022 Camera lenses introduce additional image errors, such as vignetting (reduction of brightness and saturation towards the corners of the image) and distortion, which are often fixed automatically as well.\n\u2022 Differences in lighting (e.g. daylight vs. artificial light) change the color of an object; to match human perception, which accounts for these changes, a correction needs to be applied. Colors are also corrected to account for the dynamic range of output devices (tone mapping).\n\u2022 The image contrast around edges may be increased so the image appears sharper.\nInstead of performing these steps in the camera, professional photographers often choose to work on a mostly uncorrected (raw) image to get more control over the process. For instance, they may want to correct distortions caused by the camera perspective in addition to those introduced by the lens, or adapt"}, {"title": "2.2 AI-supported post-processing", "content": "Photographers use AI-based post-processing to an increasing extent: While marketing claims about AI use are not a reliable indicator for a tool to be classified as AI, available tools provide functions that cannot be plausibly achieved without the use of AI.\nSome of these functions replace processing that was previously available, but required a greater extent of human intervention. The removal of image sensor dust commonly required users to mark the spots to be removed, but can now be performed automatically. Similarly, power lines can be easily removed from an image as well. In order to blur the background in post-processing (which achieves a similar effect to choosing a wide aperture during the photo shoot, see Fig. 3), photographers previously needed to mark the foreground; nowadays, this task can often be performed without intervention.\nOther functions are only realistically possible due to the use of AI. Adobe Photoshop includes an \u201cAI Image Extender\" feature that allows to \"magically add more background\u201d using generative AI. Luminar Neo's BodyAI tool features a slider to \"slim or add volume to a subject's torso\u201d.\nAI-based post-processing is commonly available on modern smartphones as well. This makes photo editing options easily accessible to many users, and they might not even be aware of the functionality.\nOn March 10, 2023, Reddit user ibreakphotos reported that \"Samsung 'space zoom' moon shots are fake\". The post provides compelling evidence that the Scene Optimizer feature on at least some Samsung smartphones adds the texture of the moon to photos in which the moon is recognized, improving the image quality beyond the capabilities of current smartphone cameras. Within a few days, Samsung confirmed the use of deep learning to improve moon"}, {"title": "3 Regulation of deep fakes in the European AI Act", "content": "Article 3(60) defines deep fakes, and Article 50 states resulting transparency obligations. Recitals 132 to 137 include further information on the interpretation of the transparency obligations. In the following sections, we discuss the core definition of deep fakes before presenting the resulting obligations."}, {"title": "3.1 Definition of deep fakes in the AI Act", "content": "According to Article 3(60),\n'deep fake' means AI-generated or manipulated image, audio or video content that resembles existing persons, objects, places, entities or events and would falsely appear to a person to be authentic or truthful.\nAccording to the definition, four conditions need to be met in order for a content to be considered a deep fake. First the content needs to be generated or manipulated. Generation can be understood as prompt based content creation [Gils(2024)]. In contrast, manipulation refers to given input data, for example, an already taken image, being altered. As discussed in section 2, an image taken by a camera never shows the (true) reality but instead is a result of the image processing chain as well as settings chosen by the photographer. For image content, it is problematic to define the difference between traditional image processing steps and novel deep fake manipulation. We get back to this point in section 4.1.\nSecond, image, audio, or video content is needed for a deep fake. Other content, such as text, are not considered deep fakes in the AI Act"}, {"title": "3.2 Transparency Obligations", "content": "Article 50 of the AI Act includes the main transparency regulations and obligations concerning synthetic content and deep fakes. These obligations apply regardless of whether the underlying system is a high-risk system or general-purpose AI, as noted in recital 132.\nArticle 50 (2) demands that\nProviders of AI systems, including general-purpose AI systems, generating synthetic audio, image, video or text content, shall ensure that the outputs of the AI system are marked in a machine-readable format and detectable as artificially generated or manipulated.\nThe first observation is that Article 50(2) only covers the providers (see Article 3(3) for a definition of providers). In contrast, deployers (see Article 3(4)) are governed by Article 50(4).\nSecond, Article 50(2) mentions only the generation of synthetic content. Here the question arises whether the manipulation of existing content is also covered by the provision in Article 50 (2) [Gils(2024)]. There are strong reasons to believe it is. In the latter part of the first sentence, Article 50(2) mentions the obligation to mark the synthetic content as generated or manipulated. Recital 133 explicitly also mentions the manipulation of existing inputs. Thus, we assume here that the generation and manipulation of input are covered by Article 50(2).\nIn contrast to the core deep fake regulation in Article 50(4), here, text content is also listed. Thus the obligation of Article 50(2) include deep fakes but also extends beyond core deep fake regulation.\nThe main obligation is to mark the synthetic content in a machine-readable format to ensure that the content is detectable as artificially generated or manipulated. On a technical side, watermarking synthetic outputs is far from a trivial task [Gils(2024)], although Recital 133 explicitly mentions some techniques. It remains to be seen whether the community will invest some joint effort to develop global standards [Romero Moreno(2024)].\nArticle 50(2) does not explicitly mention that the marking of the synthetic output has to be detectable by human viewers per se. Thus, it remains unclear whether there has to be a human-visible declaration of the synthetic content [Gils(2024)]. However, Article 50(5) states that the information referred to in Article 50(2) has to be provided to natural persons in a clear and distinguishable manner. Marking content only in a machine-readable way without visibility to humans would be deceptive. So in practice, it does not matter whether Article 50(2) alone or in conjunction with Article 50 (5) requires some human-visible marking. Additionally providers of (large) social media platforms already have an obligation to label deep fakes through Article 35(1) DSA [Block (2024)].\nIn contrast to the aforementioned minor issues in the transparency obligation, two exceptions in Article 50(2) pose a major difficulty. Article 50(2) states that\nThis obligation [of Article 50(2)] shall not apply to the extent the AI systems perform an assistive function for standard editing or do not substantially alter the input data provided by the deployer[. . . ]\nSo the first exception targets systems that perform assistive functions for standard editing tasks. However, the legislator's intention remains unclear [Block(2024)]. There is no legal definition nor a clarification in the recitals for either of the terms \u201cassistive function\u201d and \u201cstandard editing", "is": "when is an image substantially altered? Are changes in the pixel-space or visible/perceptible alterations required to assume a \u201csubstantial alteration", "mentioned": "nDeployers of an AI system that generates or manipulates image, audio or video content constituting a deep fake, shall disclose that the content has been artificially generated or manipulated.\nIn contrast to Article 50(2), this obligation only targets the deployers of an AI system. Furthermore, only deployers of systems processing image, audio, or video are regulated. Text systems, on the other hand, are not considered as deep fakes under the AI Act, and are regulated in Article 50(4) subparagraph 2. It is unclear whether Article 50(4) subparagraph 1 requires the AI content itself to be labeled or whether labelling the system, for example through a banner on the webpage, is sufficient. However, Recital 134 clearly indicates that the AI content itself needs a label disclosing the artificial content. Thus, deployers of an AI system that create deep fakes need to ensure that humans can observe the origin [Romero Moreno(2024)].\nCompared to the non-trivial exceptions in Article 50(2), the exceptions for the deployers in Article 50(4) are clearer. There are exceptions and limitations to this obligation for the law enforcement regime as well as for evidently artistic, creative, satirical, fictional or analogous work. The latter exception refers to the freedom of expression and artistic creation protected by Articles 11 and 13 EU Charter [Romero Moreno(2024)].\nThe interplay between the obligations for the provider in Article 50(2) and the obligation for the deployer in Article 50(4) is complex, and remains largely unclear. Recital 134 explicitly mentions that further to the technical solutions employed by the providers of the AI system, the deployers also have to comply with the transparency obligations. It seems questionable if double labelling adds a benefit for the transparency. Standardized labeling efforts are needed [Gils(2024)].\nBoth obligations try to regulate a technology that is, without the underlying intention of the user, neutral [\u0141abuz(2024), De Ruiter(2021)]. However, the overarching theme of both obligations is to ensure"}, {"title": "4 Challenges and edge cases", "content": "As discussed above, an image never depicts the objective truth. Traditional processing steps (see section 2.1) are commonly considered to lead to authentic outcomes. Moon photos processed by Samsung smartphones (see section 2.2) do not depict the reality as recorded by an image sensor, but are closer to the human perception of the moon than unaltered photos. As a result, a moon photo processed in this way does not constitute a deep fake per se; as the appearance of the moon is not expected to change, the image can in fact be considered as authentic. In summary, the photos depict the actual moon, and the AI function merely helps to overcome a technical limitation; the moon is therefore authentic in the resulting photos.\nIt is, however, conceivable that an image containing other objects besides the (post-processed) moon might have to be considered as a deep fake: A viewer might assume that those other objects are depicted with the same quality as the moon, and thus falsely consider the lack of structure in these objects as authentic. A deep fake would also be created if the AI system falsely identifies an object as the moon; for example, Reddit user ibreakphotos (see section 2.2) demonstrated Samsung's moon shot feature by taking photos of a computer screen, not the actual moon."}, {"title": "4.2 Best Take", "content": "While replacing an image of the moon with a better version may only result in deep fakes in edge cases, Google's \u201cbest take\u201d seems to be an obvious example for manipulation: By exchanging a face in a group photo, the feature creates a depiction of a situation that never occured. For such an image to be considered a deep fake, it would have to \u201cfalsely appear to a person to be authentic or truthful\" (Article 3 (60)). If the photo is objectively authentic and truthful, it is not a deep fake. On one hand, in a typical group photo setting, an average viewer would probably still consider the resulting photo as authentic. The smile which is inserted existed within a couple of seconds from the remaining photo being taken. On the other hand, the ten second time frame of the best take feature is sufficient for a mood change. A person might have stopped smiling while the rest of the group laughs about a joke at their expense.\nAs a consequence, we assume that such a group photo may well constitute a deep fake. The feature might still be \"an assistive function for standard editing\" according to Article 50(2), in which case the transparency obligation for providers of AI systems would not apply. Article 50(4), which applies to deployers of AI sytems, does not include a similar exception; we still discuss whether features similar to \"best take\" can be considered as an assistive function for standard editing.\nOne could argue that combining parts of several images goes beyond editing an image. The wording of the law, however, does not specifically refer to editing of only one individual image. Combining information from several images has also been common at least since the advent of digital photography. The dynamic range of a picture can be improved this way (HDR composites). In case of panoramic shots, the individual pictures are often taken several seconds apart, but are not usually considered as problematic. Both HDR and panoramic photos are common enough to be considered as standard editing. The use of AI alone does not suffice to not consider a post-processing tool as a standard editing tool-otherwise, the exeption in Article 50(2) would not make sense.\nOn the other hand, despite being available on widely used smartphone models, \"best take\" is marketed as a special feature (as opposed to a standard in post-processing). The difference to HDR and panoramic composites is not just the use of AI. Instead, the targeted alteration of parts of the image, and the consideration of a person's facial expression differentiate the best take feature from traditional (or \"standard\") editing.\nWe conclude that there is indeed a potential for the generation of deep fakes, which do not fall under the exception of Article 50(2).\""}, {"title": "4.3 Altering an image: Pixel space vs. visible space?", "content": "Besides the \"standard editing\" exception, the transparency requirement of Article 50(2) does not apply to AI systems that \"do not substantially alter the input data provided by the deployer or the semantics thereof\". We consider this wording to be problematic. While changes to the semantics are the exact reason why legislators deemed it necessary to regulate deep fakes, an exception for minor changes to the input data introduces an unnecessary backdoor. Since the exception for minor changes to the input is given equal weight alongside the semantics exception, there is little scope for interpreting the term 'input data' to include content-related aspects. The extent of alteration of input data is not an appropriate measure for the impact of that alteration. shows an original image and two edited versions. In both modifications, the average alteration of the input data (measured by the root mean squared differences of the red, green, and blue channels for each pixel) w.r.t. the original image is identical, but the semantics are clearly different.\nThe exception of Article 50(2) applies when either the input data are not substantially altered, or the semantics are not substantially altered. For further discussion we use the stylized example in Figure 4. In Figure 4(b), the semantics are substantially altered compared to Figure 4(a). But are the input data substantially altered? The manipulation in Figure 4(c) does not lead to a significant perceptible visual difference. The root mean squared differences of Figures 4(b) and Figure4(c) compared to Figure 4(a) are identical. Thus, one can argue that even in the case of Figure 4(b), the input data are not substantially altered. Consequently, the exception applies and Figure 4(b) would fall under the scope of the exception of Article 50(2). Article 50(2) would not be applicable in the stylized example.\nOur example illustrates that the AI Act would benefit from a clear definition of what constitutes substantially altered content."}, {"title": "5 Conclusion and Outlook", "content": "The AI Act's definition of deep fakes, and the obligations applying to providers and deployers of AI systems that generate deep fakes, are less clear than they appear at first glance. As (digital) photos are never shown exactly as recorded by the image sensor, there has always been room for interpretation and alteration of their visual impact. The delineation between legitimate processing (with the result of an image that is still considered as \u201cauthentic\") and deep fakes (which would falsely appear to a person to be authentic or truthful\u201d) is a problematic one, and the mere use of AI tools in image processing does not suffice to cross the threshold of a deep fake. We note that regulating the use of AI systems for image manipulation does make sense, as those tools are easily accessible and produce convincing images. Moreover, even legitimate AI-based image manipulation tools used without malicious intent introduce risks, as mistakes can happen in rare cases. Considering the reliance on photos (e.g., in journalism or as evidence in court), transparency requirements should not be interpreted too narrowly. A stronger focus on human oversight might have been a more appropriate approach for regulating the use of AI in image editing.\nWe expect discussions around deep fakes in other law areas as well. For example, there is an increasing use of deep fakes in pornography, including CSAM (child sexual abuse material) and youth pornography. Prosecutors face challenges of identifying such material and have to consider its consequences for prosecution, and legislators may be urged to react to this trend as well.\nWith this paper, we aim to initiate an interdisciplinary discussion on the definition and regulation of deep fakes. Furthermore, we see our paper as a starting point for a dialogue between computer scientists and legal scholars on deep fake regulation. Through this interdisciplinary discussion, the regulation of deep fakes can be improved."}]}