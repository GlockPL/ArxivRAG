{"title": "BiasScanner: Automatic Detection and Classification of News Bias to Strengthen Democracy*", "authors": ["Tim Menzner", "Jochen L. Leidner"], "abstract": "The increasing consumption of news online in the 21st century coincided with increased publication of disinformation, biased reporting, hate speech and other unwanted Web content.\nWe describe BiasScanner, an application that aims to strengthen democracy by supporting news consumers with scrutinizing news articles they are reading online. BiasScanner contains a server-side pre-trained large language model to identify biased sentences of news articles and a front-end Web browser plug-in. At the time of writing, BiasScanner can identify and classify more than two dozen types of media bias at the sentence level, making it the most fine-grained model and only deployed application (automatic system in use) of its kind. It was implemented in a lightweight and privacy-respecting manner, and in addition to highlighting likely biased sentence it also provides explanations for each classification decision as well as a summary analysis for each news article.\nWhile prior research has addressed news bias detection, we are not aware of any work that resulted in a deployed browser plug-in (c.f. also biasscanner.org for a Web demo).", "sections": [{"title": "1 Introduction", "content": "Democracy faces an existential threat when most citizens get their news from online platforms focused on controversy rather than balanced reporting. Such behavior increases advertising revenue, contributing to media bias and the spread of fake news [20,15,25,14,31]."}, {"title": "2 Related Work", "content": "Foundational Language Models. The neural transformer model was first described in [30]. Google BERT [11] and OpenAI's GPT-3 [5] GPT-4 (and their application ChatGPT [24]) and Meta's Llama [29] have been early foundational models that have introduced a paradigm shift in NLP by demonstrating how large, pre-trained language models can dramatically reduce the development time of NLP systems by using large quantities of un-annotated text to train general-purpose \"foundational\" models.\nNews Bias. Groeling [14] presents a survey of the literature covering partisan bias. Conrad et al. [8] focused on content mining to measure credibility of authors on the web. The topic of bias in mass media was dealt with in detail by [20] and [25]. Hamborg et al. [16] provided an interdisciplinary literature review to suggest methods how bias could be bias detection could be automated.\nBias Detection. Media bias datasets with different focus where released by [2],[17], and [27,26] After early pioneering work on bias from economics [15], Arapakis et al. [2] labeled 561 articles along 14 quality dimensions including subjectivity. Horne et al. [17] released a larger dataset annotated for political partisanship bias, but without grouping articles by event, which makes apples-to-apples comparison harder; Chen et al. [6] addressed this issue by resorting to another corpus sampled from the website allsides.com, which includes human labels by U.S. political orientation (on the ordinal scale {LL, L, C, R, RR}); they also present an ML model to flip the orientation to the oppositite one. Yano, Resnik and Smith [33] also on the liberal-conservative axis, manually annotating sentence-level partisanship bias.\nMBIB, the first media bias identification benchmark, was introduced by Wessel et al. [32], who evaluated Transformer techniques on detecting nine different types of bias across 22 selected datasets. Baumer et al. focused on detecting framing language. [3] Chen et al. [7] demonstrated that incorporating second-order information, such as the probability distributions of the frequency, positions, and sequential order of sentence-level bias, can enhance the effectiveness of article-level bias detection, especially in cases where relying solely on individual words or sentences is insufficient. Spinde et al. published a dataset containing biased sentences and evaluated detection techniques on it.\n[27,26]"}, {"title": "3 System", "content": "This section describes BiasScanner, our system, which is also deployed on the World Wide Web at https://biasscanner.org. This address also contains a separate Web demo where users can experiment with our model before installing the Web browser plug-in."}, {"title": "3.1 Architecture", "content": "Architecture. We designed BiasScanner with ease and convenience of use and respect for the user's privacy in mind. A frond-end application deals with the user interface and communicates with our server, which provides a bias classification service, and which shields the originating IP address of the user when invoking OpenAI GPT - current model: a gpt-3.5-turbo-16k fine-tuned on articles constructed from the BABE dataset [28] with information about bias type and strength added using GPT-4 \u2013 via a REST API on a US server, but without any transfer of PII data. Our server layer also deals with payment authentication for the transformer model use to hide this aspect from users, as we believe dealing with cumbersome API keys would exclude some users. The nature of our architecture also permits easy switching of the model working behind the scenes (we are considering switching to an Open Source Model long-term) without disruption for users.\nWe designed BiasScanner in a user-friendly way and with privacy protection in mind. The overall architecture is shown in Figure 1. Our front-end application, a Web browser plug-in, handles the user interface and connects to our"}, {"title": "3.2 User Interface", "content": "User Interface.\nFigure 2 shows the graphical user interface of BiasScanner (Web plug-in version).\nThe prompt used for instructing the language model was developed iteratively and aims to provide consistent and high-quality output by considering best practices, like a clear definition of every searched-for bias type and by including an example for the desired JSON output format. The answer given by the model is then post-processed and filtered to prevent potential errors before being used to highlight biased sentences directly on the site. A more detailed report including the type of bias, a short explanation and a score indicating the strength of the bias, is also available for the user to view. This bias report concludes by providing a general assessment of the article's bias(es).\nIt calculates a score by normalizing the sum of two components: the ratio of biased sentences to total sentences in the article and the average bias score across all biased sentences in the article. The prompt for instructing the language model was developed in several iterations to ensure consistent and high-quality output. It includes a clear definition of each searched-for bias type and an"}, {"title": "Currently Supported Types of Bias.", "content": "In general, we define media bias as the tendency to, consciously or unconsciously, report a news story in a way that supports a pre-existing narrative instead of providing unprejudiced coverage of an issue. Our implementation explicitly searches for 27 different types of Bias, namely Ad Hominem Bias, Ambiguous Attribution Bias, Anecdotal Evidence Bias, Causal Misunderstanding Bias, Cherry Picking Bias, Circular Reasoning Bias, Discriminatory Bias, Emotional Sensationalism Bias, External Validation Bias, False Balance Bias, False Dichotomy Bias, Faulty Analogy Bias, Generalization Bias, Insinuative Questioning Bias, Intergroup Bias, Mud Praise Bias, Opinionated Bias, Political Bias, Projection Bias, Shifting Benchmark Bias, Source Selection Bias, Speculation Bias, Straw Man Bias, Unsubstantiated Claims Bias, Whataboutism Bias and Word Choice Bias:"}, {"title": "4 Evaluation", "content": "Quantiative Evaluation. While a detailed evaluation is beyond the scope of this system paper, we presented detailed quantitative and qualitative evaluations for the English language in [21] and [22]. Table 1 shows some quality"}, {"title": "Qualitative Evaluation.", "content": "The achieved quality level is satisfying for practical use of the browser plug-in; a common error is the mis-classification of neutral reporting sentences with embedded radical quotes as \"biased\"; we believe embedded quotes ought to be removed before judging a sentence, which we will address in future work. We are particularly encouraged by the quality of our generated explanations, the evaluation of which is left for future work."}, {"title": "Beyond English.", "content": "At the time of writing, BiasScanner can deal with news in English through our fine-tuned model, and also with other languages via said model's transfer capabilities; in future work we want to fine-tune models for additional specific languages and evaluate them, as well as compare their performance with our existing model's transfer abilities."}, {"title": "5 Limitations and Ethical Concerns", "content": "BiasScanner may not identify all instance of biases, and while we do not claim it does, the users may wrongly believe otherwise, consciously or unconsciously, after getting used to it. It can also not recognize all types of bias: notably, underreporting bias and other types that need across across several articles, are beyond its scope, as it only analyzes one individual news story at a time; we leave news coverage comparison for future work. It should also be noted that bias detection is always, to an extent, a subjective matter. Often a sentence might be considered biased by one person while another considers it to still be objective, therefore no classification will probably ever satisfy everyone at once.\nOur current back-end implementation still depends on an underlying proprietary foundational model; in future work, we plan to become independent and port to an open model, even if this may mean a slight reduction of accuracy, as this may limit the ability to manipulate the system's behavior from the outside."}, {"title": "6 Summary, Conclusions/Limitations and Future Work", "content": "We introduced BiasScanner, a new system for enhancing online news consumption by highlighting biased individual sentences in news articles, by offering news story analysis within Web browsers. We have successfully realized our design goals, including user privacy, rapid implementation and accurate bias classification.\nBiasScanner may not identify all biases, as to date it focuses on individual news stories and does not compare across articles.\nTo date, BiasScanner has mainly been tested with English articles, introducing a development bias. Sending plain text to a server for security is required, but it is done anonymously. The system has been released as experimental browser extension available free of charge for Firefox trough the Mozilla plug-in marketplace[18] (Available on Desktop and Android). Future Releases for Chrome and Safari are planned. It can also be installed from GitHub [19].\nWe are also already using BiasScanner in the classroom for the teaching of critical reading and engaging students with the topic of media manipulation and its effects on a democracy (in Summer Semester 2024, the second author used it to support his course Media Manipulation, Propaganda and Fake News at Coburg University of Applied Sciences in Germany).\nIn future work, we aim to support open-source language models [29] to reduce cost and decrease reliance on commercial model vendors. We intent to support"}]}