{"title": "Explaining Graph Neural Networks for Node Similarity on Graphs", "authors": ["Daniel Daza", "Michael Cochez", "Cuong Xuan Chu", "Trung-Kien Tran", "Daria Stepanova", "Paul Groth"], "abstract": "Similarity search is a fundamental task for exploiting information\nin various applications dealing with graph data, such as citation\nnetworks or knowledge graphs. While this task has been inten-\nsively approached from heuristics to graph embeddings and graph\nneural networks (GNNs), providing explanations for similarity has\nreceived less attention. In this work we are concerned with ex-\nplainable similarity search over graphs, by investigating how GNN-\nbased methods for computing node similarities can be augmented\nwith explanations. Specifically, we evaluate the performance of two\nprominent approaches towards explanations in GNNs, based on\nthe concepts of mutual information (MI), and gradient-based expla-\nnations (GB). We discuss their suitability and empirically validate\nthe properties of their explanations over different popular graph\nbenchmarks. We find that unlike MI explanations, gradient-based\nexplanations have three desirable properties. First, they are action-\nable: selecting inputs depending on them results in predictable\nchanges in similarity scores. Second, they are consistent: the effect\nof selecting certain inputs overlaps very little with the effect of\ndiscarding them. Third, they can be pruned significantly to obtain\nsparse explanations that retain the effect on similarity scores.", "sections": [{"title": "1 INTRODUCTION", "content": "While large parts of Web data are still unstructured, both the re- search community and industry have made great efforts to create structured or semi-structured data such as graphs [13, 35], which form the cornerstone for various applications [44]. In such appli- cations, similarity search has evolved into a major topic [61]. For example, similarity search can be used in recommendation systems to recommend content to users based on the similarity of the con- tent with user preferences. In information retrieval, as used for web search similarity search provides results that are similar to a query. We are concerned with similarity search over graphs, where given a query node, the goal is to retrieve a list of similar nodes ranked by a certain score. Several methods to solve this problem have been proposed in the literature, ranging from heuristic-based methods to data-driven, machine learning methods. Heuristics for similarity search on graphs exploit various graph statistics, or tech- niques based on hashing to solve the problem [60, 61]. Machine learning methods, on the other hand, avert the need to design hand-engineering heuristics or features and instead they seek to exploit domain-specific patterns in the graph to learn node representations, or embeddings, so that similarities are captured via functions such as cosine similarity. Graph neural networks (GNNs), in particu-lar, have become a standard in machine learning approaches that process graph-structured data [19, 28, 56]. While GNNs offer several advantages due to their capacity to adapt to specific properties of the graph at hand, these benefits may be compromised when interpretability becomes a necessity [1, 6]. Given their demonstrated effectiveness on different tasks, there are compelling motivations to explore methods for explaining their predictions [87], which would enable applications that require ac- countable decision-making to leverage their predictive power. While extensive works on explaining GNNs exist, the majority of the methods focus on supervised learning problems, where the predicted target is well-defined based on some ground-truth data, as in the case of node classification [33, 34, 39, 86]. The applicability of such methods to the problem of explaining node similarities, often done via unsupervised learning in GNNs, is an open question. In this work, we are interested in the problem of explaining node similarities computed by GNN-based approaches. We investigate the properties of two prominent methods for explaining GNNs, based on the mutual information (MI) between the graph and the prediction, and gradient-based (GB) explana-tions. We discuss their properties, contrast them with desirable explanations in the context of node similarity, and find that their applicability changes, in comparison with other problems such as node classification. We empirically evaluate the performance of explanations by measuring the effect of intervening on the graph,"}, {"title": "2 RELATED WORK", "content": "Similarity learning. The problem of computing node similarities on graphs has been addressed in previous methods that rely on heuristics, rather than representations learned from the data. Some examples of such methods rely on statistics of connectivity [5, 23], co-occurrence statistics [24], meta-paths in heterogeneous net-works [66], and metrics for measuring structural similarities [81]. Other methods employ ideas from hashing techniques to compute vector representations useful for similarity search [20, 61, 89]. Such heuristics are useful when they are broad enough to be applicable to different graphs. Graph neural networks, on the other hand, are able to adapt to specific signals present in the data, such as domain-specific topological properties and rich multi-modal features like text and images [17, 36]. Their demonstrated effectiveness for differ-ent tasks thus warrants an investigation on how explanations can be provided for them, in the event of applications where rationales for predictions of GNNs are valuable.\nUnsupervised learning on graphs. In contrast with tasks like node classification or regression where labeled data is available, simi-larity learning is rarely accompanied with ground truth data. An alternative consists of learning representations that capture pat-terns already present in the graph [30, 31, 79]. In the absence of labels that could be used for training, learning in this setting relies on optimization algorithms that produce representations useful for a pretext task. Examples of pretext tasks are maximizing the mutual information between different views of a graph [45, 65, 70], embedding shortest path distances [4, 14], reconstructing parts of the input [27, 71], or maintaining invariance with respect to small changes in the input [69, 78]. The resulting representations can then be employed in tasks such as clustering and similarity search. Most of the research in this area has focused on studying different ways of designing pretext tasks. However, the area of explainability in unsupervised learning on graphs is underexplored [31, 79]. A recently proposed method is Task-Agnostic Graph Explanations (TAGE) [77], which proposes explaining specific dimensions of embeddings obtained via unsupervised learning. The motivation for explaining embedding dimensions is transferring the explainer module of TAGE to supervised learning tasks. The performance of TAGE for generating explanations for problems where labeled data is not available, such as similarity computations, is not explored.\nExplaining graph neural networks. Graph neural networks (GNNs) are neural networks tailored to the irregular structure of graphs, that are able to learn representations of a node in a graph taking into consideration arbitrary subgraphs around it [74, 85, 92]. A growing number of methods have been proposed in the literature that pro-vide explanations to predictions computed by GNNs, in the form of edges and features responsible for a prediction [87]. Existing methods assume a trained GNN and provide post hoc mechanisms for explaining their predictions [34, 86, 88], or propose methods that are explainable a priori [29, 39]. Fundamentally, these methods are focused on explaining GNNs for supervised learning. In this work, we are interested in providing explanations for predictions of similarity without access to labeled data. We further elaborate"}, {"title": "3 LEARNING AND EXPLAINING\nSIMILARITIES", "content": "Let G = (A,X) be a graph with n nodes, where A is an n\u00d7n adjacency matrix with Aij = 1 if nodes i and j are connected, and 0 otherwise, and X \u2208 Rnxm is a feature matrix, where the i-th row xi contains the m-dimensional feature vector of the node i. In the following sections, we discuss the problems of learning representations of nodes for the similarity task, and our proposals on how similarity scores can be explained.\n3.1 Learning representations for similarity\nGraph neural networks have become a standard architecture for processing graph-structured data, due to their ability to incorporate arbitrary neighborhoods around a node [9, 19, 28, 37, 80]. They can easily be extended to graphs with rich edge features and multimodal data [12, 16, 55, 56]. Furthermore, the fact that GNNs implement an explicit function that maps node neighborhoods and features to an embedding offers the opportunity for determining which parts of the input are responsible for a certain output. This is a desirable property when explaining computations such as similarity scores.\nA prominent example of a graph neural network is the Graph Convolutional Network (GCN) [28]. A single layer of the GCN implements the following propagation rule:\nGCN(X, \u0391) = \u03c3 = \u03c3 (\u00c3X\u0398), (1)\nwhere \u00c3 is the normalized adjacency matrix, \u00c3 = \u00d4\u00db\u00af\u00bd\u00c2\u00ce\u00af\u00bd. Let In be the n x n identity matrix. Then \u00c2 = A + In is the adjacency matrix, adding self-loops, and D is the degree matrix after adding self loops, such that Dii = \u2211j Aij.\nThe weight matrix in Eq. 1 contains the parameters of the layer to be learned during training. When composing together multiple GCN layers, we obtain a function fo(X, A) = Z \u2208 Rn\u00d7d that maps each node and its features to an embedding, conditioned on the features of nodes in its neighborhood.\nWe approach the problem of training a GNN to learn node em-beddings from the perspective of unsupervised learning: In the absence of labeled data containing ground-truth similarity informa-tion, we resort to methods that learn node embeddings by capturing patterns existing in the graph, such as communities or structural roles [22]. The resulting node embeddings are vectors zi e Rd, with i = 1, . . . n, where such patterns are preserved by the geometry of the space. This allows us to address the problem of similarity search for a given query node i, by ranking the rest of the nodes in the graph according to a function such as cosine similarity:\ny(i, j) = zTizj/||zi||||zj||, (2)\nwhere j = 1, ..., n and ||zi|| is the l\u00b2-norm of zi.\nSeveral methods are available in the literature for unsupervised learning on graphs [22, 25, 31]. Examples include Graph Autoen-coders and Variational Graph Autoencoders [27], which optimize node embeddings so that they are able to reconstruct the adjacency matrix; Deep Graph Infomax [70], that learns node embeddings by maximizing the mutual information between them and a summa- rized representation of the graph; and Graph Contrastive Represen- tation Learning [93], which compares different views of a node by perturbing its neighborhood and features.\n3.2 Explaining GNNs\nThe success of GNNs at various tasks has been accompanied by increased interest in explaining the predictions they provide [87]. Informally, methods for explaining GNNs aim to determine i) which parts of the input graph G = (X, A) are responsible for a particular prediction, and ii) how they are responsible. The mechanisms used to answer these questions vary with each method.\nA recent survey [87] classifies methods for explaining GNNS into two main groups: instance-level and model-level methods. Instance-level methods produce a distinct explanation for a par-ticular prediction (such as the label predicted for a specific node in the graph), while model-level methods aim to understand the behavior of the GNN under different inputs. Since we are interested in explaining similarity scores computed for specific pairs of nodes, we focus on the class of instance-level explanations.\nTwo important classes of instance-level methods are perturbation methods and gradient-based methods. They represent an explana-tion as an assignment of values to parts of the input (for example,"}, {"title": "4 EXPERIMENTS", "content": "We are interested in answering the following research question: Do mutual information and gradient-based methods provide explana-tions of similarities learned by GNNs that are actionable, consistent,and sparse? To answer it, we implement different methods for un-supervised learning on graphs and then analyze the properties of explanations provided by MI and GB methods quantitatively and qualitatively.\n4.1 Datasets\nWe study the problem of learning and explaining similarities by considering six graph datasets of different sizes and domains: Cora, Citeseer, and Pubmed [42, 58, 84] are citation networks from the computer science and medical domains, where each node corre-sponds to a scientific publication and an edge indicates that there is a citation from one publication to another. These graphs are known to exhibit high homophily: similar nodes (such as publica-tions within the same field) are very likely to be connected [38].\nTo consider graphs with different structural properties, we also carry out experiments with heterophilic graphs where connected nodes are not necessarily similar. Chameleon and Squirrel are graphs obtained from Wikipedia, where each node is a web page and an edge denotes a hyperlink between pages [54]. Actor is a graph where each node is an actor, and an edge indicates that two actors co-occur on a Wikipedia page [68]. Furthermore, we also experiment with the DBpedia50k knowledge graph [59], a subset of the DBpedia knowledge graph [2]. The DBpedia50k graph does not contain node features, therefore for this dataset we also train input node embeddings for the GNN.\nIn all graphs, each node is associated with a feature vector. Sta-tistics of all datasets is presented in Table 1.\n4.2 Learning node embeddings for similarity\nWe implement the following unsupervised learning methods: Graph Autoencoders (GAE) and Variational Graph Autoencoders (VGAE) [27], Deep Graph Infomax (DGI) [70], and Graph Contrastive Represen-tation Learning (GRACE) [93]. We use them to train a 2-layer GNN as defined in Eq. 1. We tune the hyperparameters of the GNN and specific hyperparameters of each unsupervised learning method via grid search, selecting the values with the lowest training loss."}, {"title": "4.3 Evaluating explanations", "content": "Given a trained GNN fo, we evaluate the properties of explanations for node similarities by measuring quantities that assess changes in the similarity score, after performing interventions in the graph on the basis of the explanation. More concretely, let (i, j) be a pair of nodes in the graph. Given the set of node embeddings Z = fo (X, A), we select the embeddings of i and j from it and compute the cosine similarity y(i, j) as defined in Eq. 2. The explanation method is then executed on this value, which results in an explanation matrix M. In our experiments, we employ GNNExplainer [86] as an instance of MI methods. For GB methods, we consider directly using the gradient with respect to the adjacency matrix (as defined in Eq. 6), and Integrated Gradients [67].\nGiven M, we compute two matrices Ma and M\u2081 that select values above or below a threshold t, respectively, such that\nMa,ij = Mij if Mij \u2265 t else 0 (7)\nMb,ij = Mij if Mij < t else 0, (8)\nwhere the threshold for GNNexplainer is 0.5 and 0 for GB methods.\nWe use these matrices to intervene in the graph, by computing the element-wise multiplication of these matrices with the adja-cency matrix, and re-computing the node embeddings, which yields\nZa = fo (X, A \u00a9 Ma) (9)\nZb = fo (X, A \u00a9 \u041c\u044c). (10)\nGiven these embeddings, we then re-compute the similarity scores, which for each case we denote as ya(i, j) and yb (i, j) respectively.\nBased on these new similarity scores, we first compute a fidelity metric [50], which measures the change in the similarity score after the intervention with respect to the original similarity score:\nFida = ya(i, j) \u2013 y(i, j) (11)\nFid\u2081 = yb (i, j) \u2013 y(i, j) (12)\nWith fidelity metrics, we aim to determine whether the explanations are actionable, since they measure the effect on similarity scores after intervening on the graph with explanations that are either above or below the threshold. We compute the average values of\nFida and Fid over a sample of 1,000 randomly selected pairs of nodes from the graph (without replacement).\nEqs.11 and 12 imply that the effect on the similarity score can be to increase it (in which case fidelity is positive) or to decrease it (when fidelity is negative). To evaluate the property of consistency, we obtain a tuple (a1, a2) where a1 is the number of times Fida is positive over the 1,000 pairs of nodes, and a2 is the number of times it is negative. We obtain another tuple (b1, b2) in the same way based on the values of Fid. We then measure the effect overlap (EO) between Fida and Fid, by computing the generalized Jaccard similarity:\nEO = 2\u03a3i=1 min(ai, bi)/\u03a3i=1 max(ai, bi) (13)\nAn explanation method with an EO of zero indicates that the effect observed in Fida is always positive, and always negative in Fid (or viceversa). This indicates that the effects are distinct and thus the explanations are consistent. The maximum value of EO is 1 and it occurs if the effect is always positive or always negative, or if the counts of effects are the same.\n4.4 Results\nWe present the results of the fidelity and effect overlap metrics in Tables 2 for the homophilic and heterophilic graphs, and Table 3 for DBpedia50k. We denote GNNExplainer as MI, directly using the gradient as GB1, and Integrated Gradients as GB2.\nGB explanations are actionable. The values of Fida and Fidy for GB methods show that across all unsupervised learning methods and datasets, keeping edges above the explanation threshold always results in an increase of the similarity score, while keeping the edges below the threshold always results in a lower score. This means that GB explanations are actionable, as they allow interventions that result in a predictable effect on the similarity score. Relying on these explanations would allow to determine what edges contribute to increase (or decrease) the score, and to interact with them by re-computing the similarity score with the knowledge provided by the explanation. This property is not observed with GNNExplainer,"}, {"title": "5 CONCLUSION", "content": "We have investigated the problem of explaining node similarities learned by graph neural networks. We discuss the properties of two prominent methods for explainability on GNNs, based on the idea of mutual information, which selects parts of the input relevant for a prediction; and gradients, which measure changes in the pre-diction with respect to the inputs. By contrasting their properties with desirable explanations in the context of node similarity, we find that the applicability changes, in comparison with other prob-lems in which they have been applied, such as node classification. We conclude that gradient-based methods are better suited for ex-plaining similarities, by providing explanations with a predictable and consistent effect of increasing or decreasing similarity scores. Furthermore, we observe that the complexity of the explanations can be reduced while maintaining their desirable properties.\nThe properties we present in our work can be extended to the general problem of explaining similarities on graphs via methods other than GNNs, as well as the design of methods for similarity search on graphs that are explainable a priori, which we plan to explore in future work."}]}