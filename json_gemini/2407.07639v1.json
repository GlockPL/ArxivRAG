{"title": "Explaining Graph Neural Networks for Node Similarity on Graphs", "authors": ["Daniel Daza", "Michael Cochez", "Cuong Xuan Chu", "Trung-Kien Tran", "Daria Stepanova", "Paul Groth"], "abstract": "Similarity search is a fundamental task for exploiting information\nin various applications dealing with graph data, such as citation\nnetworks or knowledge graphs. While this task has been inten-\nsively approached from heuristics to graph embeddings and graph\nneural networks (GNNs), providing explanations for similarity has\nreceived less attention. In this work we are concerned with ex-\nplainable similarity search over graphs, by investigating how GNN-\nbased methods for computing node similarities can be augmented\nwith explanations. Specifically, we evaluate the performance of two\nprominent approaches towards explanations in GNNs, based on\nthe concepts of mutual information (MI), and gradient-based expla-\nnations (GB). We discuss their suitability and empirically validate\nthe properties of their explanations over different popular graph\nbenchmarks. We find that unlike MI explanations, gradient-based\nexplanations have three desirable properties. First, they are action-\nable: selecting inputs depending on them results in predictable\nchanges in similarity scores. Second, they are consistent: the effect\nof selecting certain inputs overlaps very little with the effect of\ndiscarding them. Third, they can be pruned significantly to obtain\nsparse explanations that retain the effect on similarity scores.", "sections": [{"title": "1 INTRODUCTION", "content": "While large parts of Web data are still unstructured, both the re-search community and industry have made great efforts to create\nstructured or semi-structured data such as graphs [13, 35], which\nform the cornerstone for various applications [44]. In such appli-cations, similarity search has evolved into a major topic [61]. For\nexample, similarity search can be used in recommendation systems\nto recommend content to users based on the similarity of the con-tent with user preferences. In information retrieval, as used for web\nsearch similarity search provides results that are similar to a query.\nWe are concerned with similarity search over graphs, where\ngiven a query node, the goal is to retrieve a list of similar nodes\nranked by a certain score. Several methods to solve this problem\nhave been proposed in the literature, ranging from heuristic-based"}, {"title": "2 RELATED WORK", "content": "Similarity learning. The problem of computing node similarities\non graphs has been addressed in previous methods that rely on\nheuristics, rather than representations learned from the data. Some\nexamples of such methods rely on statistics of connectivity [5, 23],\nco-occurrence statistics [24], meta-paths in heterogeneous net-works [66], and metrics for measuring structural similarities [81].\nOther methods employ ideas from hashing techniques to compute\nvector representations useful for similarity search [20, 61, 89]. Such\nheuristics are useful when they are broad enough to be applicable\nto different graphs. Graph neural networks, on the other hand, are\nable to adapt to specific signals present in the data, such as domain-specific topological properties and rich multi-modal features like\ntext and images [17, 36]. Their demonstrated effectiveness for differ-ent tasks thus warrants an investigation on how explanations can\nbe provided for them, in the event of applications where rationales\nfor predictions of GNNs are valuable.\nUnsupervised learning on graphs. In contrast with tasks like node\nclassification or regression where labeled data is available, simi-larity learning is rarely accompanied with ground truth data. An\nalternative consists of learning representations that capture pat-terns already present in the graph [30, 31, 79]. In the absence of\nlabels that could be used for training, learning in this setting relies\non optimization algorithms that produce representations useful\nfor a pretext task. Examples of pretext tasks are maximizing the\nmutual information between different views of a graph [45, 65, 70],\nembedding shortest path distances [4, 14], reconstructing parts of\nthe input [27, 71], or maintaining invariance with respect to small\nchanges in the input [69, 78]. The resulting representations can\nthen be employed in tasks such as clustering and similarity search.\nMost of the research in this area has focused on studying different\nways of designing pretext tasks. However, the area of explainability\nin unsupervised learning on graphs is underexplored [31, 79]. A\nrecently proposed method is Task-Agnostic Graph Explanations\n(TAGE) [77], which proposes explaining specific dimensions of\nembeddings obtained via unsupervised learning. The motivation\nfor explaining embedding dimensions is transferring the explainer\nmodule of TAGE to supervised learning tasks. The performance of\nTAGE for generating explanations for problems where labeled data\nis not available, such as similarity computations, is not explored.\nExplaining graph neural networks. Graph neural networks (GNNs)\nare neural networks tailored to the irregular structure of graphs,\nthat are able to learn representations of a node in a graph taking into\nconsideration arbitrary subgraphs around it [74, 85, 92]. A growing\nnumber of methods have been proposed in the literature that pro-vide explanations to predictions computed by GNNs, in the form\nof edges and features responsible for a prediction [87]. Existing\nmethods assume a trained GNN and provide post hoc mechanisms\nfor explaining their predictions [34, 86, 88], or propose methods\nthat are explainable a priori [29, 39]. Fundamentally, these methods\nare focused on explaining GNNs for supervised learning. In this\nwork, we are interested in providing explanations for predictions\nof similarity without access to labeled data. We further elaborate"}, {"title": "3 LEARNING AND EXPLAINING\nSIMILARITIES", "content": "Let G = (A,X) be a graph with n nodes, where A is an n\u00d7n\nadjacency matrix with $A_{ij} = 1$ if nodes i and j are connected,\nand 0 otherwise, and $X \\in R^{n x m}$ is a feature matrix, where the\ni-th row xi contains the m-dimensional feature vector of the node\ni. In the following sections, we discuss the problems of learning\nrepresentations of nodes for the similarity task, and our proposals\non how similarity scores can be explained."}, {"title": "3.1 Learning representations for similarity", "content": "Graph neural networks have become a standard architecture for\nprocessing graph-structured data, due to their ability to incorporate\narbitrary neighborhoods around a node [9, 19, 28, 37, 80]. They can\neasily be extended to graphs with rich edge features and multimodal\ndata [12, 16, 55, 56]. Furthermore, the fact that GNNs implement\nan explicit function that maps node neighborhoods and features to\nan embedding offers the opportunity for determining which parts\nof the input are responsible for a certain output. This is a desirable\nproperty when explaining computations such as similarity scores."}, {"title": "3.2 Explaining GNNs", "content": "The success of GNNs at various tasks has been accompanied by\nincreased interest in explaining the predictions they provide [87].\nInformally, methods for explaining GNNs aim to determine i) which\nparts of the input graph G = (X, A) are responsible for a particular\nprediction, and ii) how they are responsible. The mechanisms used\nto answer these questions vary with each method.\nA recent survey [87] classifies methods for explaining GNNs\ninto two main groups: instance-level and model-level methods.\nInstance-level methods produce a distinct explanation for a par-ticular prediction (such as the label predicted for a specific node\nin the graph), while model-level methods aim to understand the\nbehavior of the GNN under different inputs. Since we are interested\nin explaining similarity scores computed for specific pairs of nodes,\nwe focus on the class of instance-level explanations.\nTwo important classes of instance-level methods are perturbation\nmethods and gradient-based methods. They represent an explana-tion as an assignment of values to parts of the input (for example,"}, {"title": "3.2.1 Mutual information methods", "content": "A common approach for iden-tifying explanations for GNNs consists of determining what edges\nare relevant for computing a prediction, by relying on the concept\nof Mutual Information (MI) [34, 39, 73, 86].\nGiven two random variables U, V, the mutual information (MI)\nbetween them is defined as\n$I(U; V) = \\int\\int p(u, v) \\log \\frac{p(u, v)}{p(u)p(v)} dudv$,\nwhere p(u, v) is the joint probability distribution of U and V, and\np(u) and p(v) are the marginal distributions of U and V, respec-\ntively. Intuitively, the mutual information measures the reduction\nin the uncertainty of U given the knowledge of V. For two inde-pendent random variables, the mutual information is 0 [10].\nIn the context of explaining GNNs, existing works have proposed\nexplaining a prediction y = g(fo (X, A)) by finding a subgraph from\nthe original graph that has high mutual information with the pre-diction. This implies that only a region of the graph is relevant for\ncomputing a prediction, whereas the rest can be discarded with-out affecting it. This mechanism for finding an explanation can be\nformalized by assuming that the matrix M of explanation values\nis a sample of a random variable M with values in {0, 1}, and then\nmaximizing the mutual information between the original prediction\n(now a random variable Y) and the prediction after \"masking\" the"}, {"title": "3.2.2 Gradient-based methods", "content": "We now describe an alternative\napproach for computing explanations for node similarities, which\nwe refer to as gradient-based (GB) methods.\nAn early approach for identifying parts of the inputs relevant\nfor a prediction computed by a neural network is to compute the\ngradient of the output with respect to the input [57, 62, 63, 67]. This\nis motivated by the fact that the gradient indicates the direction\nand rate with which the outputs change with respect to the inputs.\nThe extension of this approach to explaining GNNs is natural:\nthe explanation matrix is equal to the gradient of the prediction\nwith respect to the adjacency matrix,\n$M_{GB} := \\frac{\\partial y}{\\partial A} = \\frac{\\partial g(f_{\\theta}(X, A))}{\\partial A}$.\nRelying on the gradient alone might become problematic in deep\nneural networks using non-linearities like the ReLU activation\nfunction, whose derivative is zero over half of its domain. To address\nthis issue, more advanced methods based on the gradient have been\nproposed, such as Guided Backpropagation [64], which ignores zero\ngradients, or Integrated Gradients [67], which computes the total\nchange from different values of the gradient, rather than relying\non a single gradient."}, {"title": "3.2.3 Desiderata for explanations of similarity", "content": "Several works in\nthe literature have highlighted the importance of explainability in\nartificial intelligence systems, particularly when they face human\nusers that could benefit from an understanding of their predic-tions [1, 40, 41, 49, 83]. These works define a series of properties\nthat explanations should have. For example, they should \"produce\ndetails or reasons to make its functioning clear or easy to under-stand\" [1], they should be useful for debugging algorithms [83],\nthey should provide answers to why questions [40] -e.g. why is\nthis the similarity score?-; and they should have properties such as\nfidelity (how much the explanation agrees with the input-output\nmap of the prediction under explanation), low ambiguity, and low\ncomplexity, among others [49].\nThe properties defined in such works are applicable to a broad\nclass of explanation methods, and they can serve as a guide for defin-ing desirable properties of explanations of GNNs in the context of\nnode similarity. Given that the explanation methods we have con-sidered provide an explanation value for each edge involved in the\ncomputation of a prediction, we propose the following properties\nthat such explanations should meet:\n\u2022 Explanations are actionable: We can use the edges whose\nexplanation value is above or below the threshold t to make\ninterventions in the graph, resulting in a predictable effect\non the original similarity score. This would facilitate an\nunderstanding of the specific effect of some edges on the\nsimilarity score, and follows desiderata on understanding\nmodel decisions [1, 40], interactivity via interventions [1],\nmodel debugging [83], and fidelity [49].\n\u2022 Explanations are consistent: The effect of keeping edges\nabove the threshold is distinct from the effect of discard-ing them. This would imply that the explanations capture\nspecific behaviors of the similarity under explanation, thus\nindicating fidelity and low ambiguity [49].\n\u2022 Explanations are sparse. Rather than presenting the com-plete set of explanation values, a subset can be selected that\npreserves the original effects of keeping or discarding edges\non the similarity score. The result is an explanation that\nremains actionable and consistent, while enabling simpler,\nparsimonious explanations [49] that might be preferable in\ncertain situations [40].\nOur previous discussion on the interpretation of explanation\nmatrices provided by MI and GB methods suggests that the latter"}, {"title": "4 EXPERIMENTS", "content": "We are interested in answering the following research question: Do\nmutual information and gradient-based methods provide explana-tions of similarities learned by GNNs that are actionable, consistent,\nand sparse? To answer it, we implement different methods for un-supervised learning on graphs and then analyze the properties of\nexplanations provided by MI and GB methods quantitatively and\nqualitatively."}, {"title": "4.1 Datasets", "content": "We study the problem of learning and explaining similarities by\nconsidering six graph datasets of different sizes and domains: Cora,\nCiteseer, and Pubmed [42, 58, 84] are citation networks from the\ncomputer science and medical domains, where each node corre-sponds to a scientific publication and an edge indicates that there\nis a citation from one publication to another. These graphs are\nknown to exhibit high homophily: similar nodes (such as publica-tions within the same field) are very likely to be connected [38].\nTo consider graphs with different structural properties, we also\ncarry out experiments with heterophilic graphs where connected\nnodes are not necessarily similar. Chameleon and Squirrel are\ngraphs obtained from Wikipedia, where each node is a web page\nand an edge denotes a hyperlink between pages [54]. Actor is a\ngraph where each node is an actor, and an edge indicates that two\nactors co-occur on a Wikipedia page [68]. Furthermore, we also\nexperiment with the DBpedia50k knowledge graph [59], a subset\nof the DBpedia knowledge graph [2]. The DBpedia50k graph does\nnot contain node features, therefore for this dataset we also train\ninput node embeddings for the GNN.\nIn all graphs, each node is associated with a feature vector. Sta-tistics of all datasets is presented in Table 1."}, {"title": "4.2 Learning node embeddings for similarity", "content": "We implement the following unsupervised learning methods: Graph\nAutoencoders (GAE) and Variational Graph Autoencoders (VGAE) [27],\nDeep Graph Infomax (DGI) [70], and Graph Contrastive Represen-tation Learning (GRACE) [93]. We use them to train a 2-layer GNN\nas defined in Eq. 1. We tune the hyperparameters of the GNN and\nspecific hyperparameters of each unsupervised learning method\nvia grid search, selecting the values with the lowest training loss."}, {"title": "4.3 Evaluating explanations", "content": "Given a trained GNN fo, we evaluate the properties of explanations\nfor node similarities by measuring quantities that assess changes in\nthe similarity score, after performing interventions in the graph on\nthe basis of the explanation. More concretely, let (i, j) be a pair of\nnodes in the graph. Given the set of node embeddings Z = fo (X, A),\nwe select the embeddings of i and j from it and compute the cosine\nsimilarity y(i, j) as defined in Eq. 2. The explanation method is then\nexecuted on this value, which results in an explanation matrix M.\nIn our experiments, we employ GNNExplainer [86] as an instance\nof MI methods. For GB methods, we consider directly using the\ngradient with respect to the adjacency matrix (as defined in Eq. 6),\nand Integrated Gradients [67].\nGiven M, we compute two matrices Ma and M\u2081 that select values\nabove or below a threshold t, respectively, such that\n$M_{a,ij} = M_{ij} \\text{ if } M_{ij} \\geq t \\text{ else } 0$\n$M_{b,ij} = M_{ij} \\text{ if } M_{ij} < t \\text{ else } 0,$\nwhere the threshold for GNNexplainer is 0.5 and 0 for GB methods.\nWe use these matrices to intervene in the graph, by computing\nthe element-wise multiplication of these matrices with the adja-cency matrix, and re-computing the node embeddings, which yields\n$Z_{a} = f_{\\theta}(X, A \\odot M_{a})$\n$Z_{b} = f_{\\theta}(X, A \\odot M_{b}).$\nGiven these embeddings, we then re-compute the similarity scores,\nwhich for each case we denote as ya(i, j) and yb (i, j) respectively.\nBased on these new similarity scores, we first compute a fidelity\nmetric [50], which measures the change in the similarity score after\nthe intervention with respect to the original similarity score:\n$Fid_{a} = y_{a}(i, j) - y(i, j)$\n$Fid_{b} = y_{b}(i, j) - y(i, j)$.\nWith fidelity metrics, we aim to determine whether the explanations\nare actionable, since they measure the effect on similarity scores\nafter intervening on the graph with explanations that are either\nabove or below the threshold. We compute the average values of"}, {"title": "4.4 Results", "content": "We present the results of the fidelity and effect overlap metrics in\nTables 2 for the homophilic and heterophilic graphs, and Table 3\nfor DBpedia50k. We denote GNNExplainer as MI, directly using the\ngradient as GB1, and Integrated Gradients as GB2.\nGB explanations are actionable. The values of Fida and Fidy for\nGB methods show that across all unsupervised learning methods\nand datasets, keeping edges above the explanation threshold always\nresults in an increase of the similarity score, while keeping the edges\nbelow the threshold always results in a lower score. This means\nthat GB explanations are actionable, as they allow interventions\nthat result in a predictable effect on the similarity score. Relying on\nthese explanations would allow to determine what edges contribute\nto increase (or decrease) the score, and to interact with them by\nre-computing the similarity score with the knowledge provided by\nthe explanation. This property is not observed with GNNExplainer,"}, {"title": "5 CONCLUSION", "content": "We have investigated the problem of explaining node similarities\nlearned by graph neural networks. We discuss the properties of two\nprominent methods for explainability on GNNs, based on the idea\nof mutual information, which selects parts of the input relevant"}]}