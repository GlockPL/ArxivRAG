[{"title": "How Numerical Precision Affects Mathematical Reasoning Capabilities of LLMS", "authors": ["Guhao Feng", "Kai Yang", "Yuntian Gu", "Xinyue Ai", "Shengjie Luo", "Jiacheng Sun", "Di He", "Zhenguo Li", "Liwei Wang"], "abstract": "Despite the remarkable success of Transformer-based Large Language Models (LLMs) across various domains, understanding and enhancing their mathematical capabilities remains a significant challenge. In this paper, we conduct a rigorous theoretical analysis of LLMs' mathematical abilities, with a specific focus on their arithmetic performances. We identify numerical precision as a key factor that influences their effectiveness in mathematical tasks. Our results show that Transformers operating with low numerical precision fail to address arithmetic tasks, such as iterated addition and integer multiplication, unless the model size grows super-polynomially with respect to the input length. In contrast, Transformers with standard numerical precision can efficiently handle these tasks with significantly smaller model sizes. We further support our theoretical findings through empirical experiments that explore the impact of varying numerical precision on arithmetic tasks, providing valuable insights for improving the mathematical reasoning capabilities of LLMs.", "sections": [{"title": "1 Introduction", "content": "Transformer-based Large Language Models (LLMs), such as GPT (OpenAI, 2023), Claude (Anthropic, 2024), and LLaMA (Dubey et al., 2024), have achieved impressive performance across a broad range of natural language tasks (Zhu et al., 2024; Basyal and Sanghvi, 2023; Shao et al., 2023). Despite the great success, significant challenges remain when applying LLMs to mathematical problem-solving. Unlike many typical NLP tasks, which often depend on pattern recognition and statistical correlations (Blei et al., 2003), mathematical reasoning requires rigorous logical deduction in a specific order (Frieder et al., 2024; Bubeck et al., 2023). \u03a4\u03bf address these challenges, various strategies have been proposed, including carefully designed prompting strategies (Wei et al., 2022b; Yamauchi et al., 2023; Imani et al., 2023) and inference-based searching method (Kang et al., 2024; Wu et al., 2024a; Snell et al., 2024; Brown et al., 2024). However, a comprehensive understanding of the intrinsic limitations that restrict the mathematical reasoning capabilities of LLMs remains elusive.\nIn principle, mathematical reasoning, built on basic arithmetical operations, requires the accurate computation of intermediate results throughout the reasoning process (Bubeck et al., 2023; Lee et al., 2024). There exist works (Feng et al., 2023; Yang et al., 2024) exploring the arithmetic capabilities of LLMs with Chain of Thought prompting (Wei et al., 2022b). However, unlike the tokenization methods of modern LLMs (Dubey et al., 2024; OpenAI, 2023), where each digit in a number is tokenized individually, these works treat a whole number as one token. Under this assumption, each distinct number occupies a unique position in the vocabulary, leading to an essential mismatch with practical implementations. Moreover, recent studies have demonstrated that LLMs operating with reduced numerical precision (e.g., int4) exhibit a significant decline in performance on mathematical tasks (Jin et al., 2024; Marchisio et al., 2024).\nIn this paper, we provide a rigorous theoretical investigation of the arithmetical abilities of LLMs under the autoregressive paradigm. Specifically, we follow the tokenization method of modern LLMs, allowing the models to process and generate numbers digit by digit. Under these assumptions, we identify numerical precision as a key factor influencing their performance in arithmetical tasks. Our analysis focuses on three elementary arithmetic tasks: integer addition, iterated addition, and integer multiplication, which serve as elementary building blocks in solving complex real-world math problems."}, {"title": "2 Preliminary", "content": "An autoregressive Transformer, or decoder-only Transformer (Radford et al., 2019; Dai et al., 2019), is a neural network designed to model sequence-to-sequence mappings. For an input sequence $\\mathbf{s}$ of length $n$, each input token $s_i$ (for $i \\in [n]$) is transformed into a $d$-dimensional vector $\\mathbf{x}_i^{(0)} = \\text{Embed}(s_i) + \\mathbf{p}_i \\in \\mathbb{R}^d$, where $\\text{Embed}(\\cdot)$ represents the token embedding function, and $\\mathbf{p}_i$ denotes learnable positional embeddings. The model then consists of $L$ Transformer blocks, each following the form:\n$\\mathbf{h}_i^{(l)} = \\mathbf{x}_i^{(l-1)} + \\text{Attn}^{(l)}(\\mathbf{x}_i^{(l-1)}; \\{\\mathbf{x}_j^{(l-1)}\\}: j < i\\}),$\n$\\mathbf{x}_i^{(l)} = \\mathbf{h}_i^{(l)} + \\text{FFN}^{(l)}(\\mathbf{h}_i^{(l)}),$\nwhere $l \\in [L]$. Here, $\\text{Attn}^{(l)}$ and $\\text{FFN}^{(l)}$ denote the multi-head self-attention layer and the feed-forward network of the $l$-th Transformer block:\n$\\text{Attn}^{(l)}(\\mathbf{x}, S) = \\sum_{h=1}^H (\\mathbf{W}_O^{(l,h)})^T \\mathbf{H}^{(l,h)}(\\mathbf{x}, S),$\n$\\mathbf{H}^{(l,h)}(\\mathbf{x}, S) = \\text{softmax}_{z \\in S} ((\\mathbf{W}_Q^{(l,h)}z)^T (\\mathbf{W}_K^{(l,h)} \\mathbf{x})) \\mathbf{W}_V^{(l,h)} z,$\n$\\text{FFN}^{(l)}(\\mathbf{x}) = \\mathbf{W}_2^{(l)} \\sigma(\\mathbf{W}_1^{(l)} \\mathbf{x}),$\nwhere $\\mathbf{W}_Q^{(l,h)}, \\mathbf{W}_K^{(l,h)}, \\mathbf{W}_V^{(l,h)}, \\mathbf{W}_O^{(l,h)} \\in \\mathbb{R}^{d \\times d}$ are the query, key, value, and output matrices of the $h$-th head in the $l$-th layer. The weight matrices in the feed-forward network are denoted as $\\mathbf{W}_1^{(l)}, \\mathbf{W}_2^{(l)} \\in \\mathbb{R}^{d \\times d}$. The activation function $\\sigma$ is chosen to be GeLU (Hendrycks and Gimpel, 2016), following the work of (Radford et al., 2019; Devlin et al., 2019).\nThe computed embedding $\\mathbf{x}_i^{(L)}$ is then used to predict the next token $s_{n+1}$, which is concatenated to the input to continue the sequence generation process. This process terminates when an <EOS> token is generated. Further discussions on related work are listed in Appendix A."}, {"title": "3 Problem Setup", "content": "This paper explores the arithmetic reasoning capabilities of LLMs by focusing on three elementary arithmetic tasks: integer addition, iterated addition, and integer multiplication under the autoregressive paradigm. Below, we define the integer representations used throughout the study and provide formal descriptions for each task.\nInteger Representations. We assume all integers are non-negative and represented in the base-p form, where $p > 2$ is a fixed base. Specifically, an integer with $n$ digits is expressed as $(x_{n-1}\\cdots x_0)_p$, where the sequence of digits $\\mathbf{x} = [x_{n-1},\\cdots, x_0]$ is the input to the Transformer model. Each digit $x_i$ is treated as an individual token according to the model's tokenization scheme. During generation, the Transformer also produces the outcome digit by digit, respecting the established tokenization structure.\nUnlike prior works (Feng et al., 2023; Yang et al., 2024), which represent entire integers as single tokens, we treat each digit of an integer as an individual token, allowing Transformers to process and generate numbers digit by digit. Given that most modern LLMs tokenize large numbers in this manner, our approach not only aligns with practical tokenization strategies but also mitigates the issue of inflated vocabulary size.\nInteger Addition. Let $a = (a_{n_1-1}\\cdots a_0)_p$ and $b = (b_{n_2-1}\\cdots b_0)_p$ denote two integers encoded in base-p. The input sequence is constructed by concatenating the tokenized representations of $a$ and $b$, separated by the addition operator '+'. The task is to generate the sum $s = (s_{n} \\cdots s_0)_p$, which contains $n+1$ digits where $n = \\max(n_1, n_2)$ and represents the base-p sum of $a$ and $b$, output digit-by-digit.\nIterated Addition. Consider $k$ integers $a_1 = (a_{1,n_1-1}\\cdots a_{1,0})_p, \\ldots, a_k = (a_{k,n_k-1}\\cdots a_{k,0})_p$ where $n = \\max\\{n_1,\\ldots, n_k\\}$. The input sequence consists of the tokenized representations of these integers, concatenated and separated by the addition operator '+'. The Transformer model must output the sum $s = (s_{n+\\lceil\\log_p k\\rceil-1} \\cdots s_0)_p$, which contains $n+ \\lceil\\log_p k\\rceil$ digits and represents the base-p sum of the $k$ integers."}, {"title": "4 Low-Precision Transformers Struggle with Basic Arithmetic Tasks", "content": "Recent studies (Marchisio et al., 2024; Jin et al., 2024) have shown that LLMs operating under low-precision constraints encounter significant challenges in performing basic mathematical tasks. In this section, we examine the expressive limitations of Transformers under such constraints and seek to explain the sharp decline in their arithmetical capabilities. Specifically, we demonstrate that Transformers restricted to low-precision arithmetic exhibit substantial difficulty in solving even elementary arithmetic problems.\nTo formalize these limitations, we build on the framework introduced by Li et al. (2024) and utilize the setting of a constant-precision Transformer (See formal definition in Appendix B). In this setting, the internal states of the model's neurons are constrained to represent real numbers using only $c$ bits, where $c$ is a small constant independent of the input sequence length. These numbers may be represented by floating point in IEEE 754 formats (Kahan, 1996) or fixed point formats. This configuration mirrors many practical deployment scenarios, in which LLMs often employ reduced-precision formats such as float8, int8, or even int4, particularly during inference (Han et al., 2015). Given that these models typically process input sequences comprising thousands of tokens, it is reasonable and realistic to assume that the numerical precision remains fixed at a small constant, independent of sequence length. Under the constant-precision setting, we examine the expressiveness of the Transformer model in elementary arithmetic problems.\nTheorem 4.1. For any fixed integers $p$, there exist constant-precision Transformers with constant depth $L$ and hidden dimension $d = O(n^2)$ that can solve the ADD(n, p) task.\nTheorem 4.1 suggests that the bounded-depth Transformers with reasonable hidden dimensions are capable of solving the integer addition task. However, as we will show in subsequent theorems, constant-precision Transformers exhibit pronounced limitations when considering more complex arithmetic problems.\nTheorem 4.2. For any integers $p$ and $L$, and for any polynomial $f$, there exist problem scales $n$ and $k$ such that no constant-precision autoregressive Transformer with $L$ layers and hidden dimension $d < f(n, k)$ can correctly solve the IterADD(n, p) task.\nTheorem 4.3. For any integers $p$ and $L$, and for any polynomial $f$, there exist problem scales $n$ and $I$ such that no constant-precision autoregressive Transformer with $L$ layers and hidden dimension $d < f(n,l)$ can correctly solve the MUL(n,l,p) task.\nWhat accounts for this limitation? As presented in Appendix D, our proof is grounded in circuit complexity theory. By modeling the constant-precision Transformer as a computational circuit, we rigorously analyze its expressive limitations through the lens of circuit complexity (Feng et al., 2023; Merrill et al., 2022; Merrill and Sabharwal, 2023; Li et al., 2024). Specifically, Li et al. (2024) proves that the expressiveness of constant-precision Transformers with polynomial size and bounded depth is upper-bounded by the computation complexity class $AC^0$. In contrast, we demonstrate that the complexity of tasks such as IterADD and MUL exceeds that of $AC^0$, using reductions from Majority, a well-established problem that has been provably unsolvable by the circuits in $AC^0$."}, {"title": "5 Standard-Precision Transformers Are Sufficient for Arithmetic Tasks", "content": "In Section 4, we demonstrated that low-precision Transformers struggle with arithmetic tasks due to their expressive limitations. In this section, we will show that increasing numerical precision is essential to overcoming this limitation. In particular, we focus on standard-precision Transformers and show that such models can overcome these limitations and solve arithmetic problems efficiently.\nTo formalize the notion of standard precision (e.g., float32), we follow Feng et al. (2023) and adopt the setting of a logarithmic-precision Transformer (See formal definition in Appendix B). In this setting, the Transformer's internal neurons can represent real numbers with up to $O(\\log n)$ bits, where $n$ denotes the maximum input sequence length. Given that modern LLMs often limit their context length to hundreds of thousands of tokens (OpenAI, 2023; Touvron et al., 2023; Anthropic, 2024), it is natural to treat 32 as the logarithmic scale corresponding to 100,000. Hence, the logarithmic-precision setting reflects practical deployment scenarios.\nWe first establish that, under logarithmic precision, a Transformer with constant depth and dimension can solve both the integer addition and iterated addition tasks for arbitrarily large input lengths, as shown in Theorems 5.1 and 5.2.\nTheorem 5.1. For any integers $n$ and $p$, there exists a logarithmic-precision Transformer with constant depth $L$ and constant hidden dimension $d$ (independent of $n$) that can generate the correct output for any input on the ADD(n, p) task.\nTheorem 5.2. For any integers $n, k$, and $p$, there exists a logarithmic-precision Transformer with constant depth $L$ and constant hidden dimension $d$ (independent of $n$ and $k$) that can generate the correct output for any input on the IterADD(n, k, p) task.\nWe now turn to integer multiplication. As established in Theorem 5.3, a logarithmic-precision Transformer with constant depth and polynomial hidden dimensions is capable of solving the integer multiplication task.\nTheorem 5.3. For any integers $n, l$, and $p$, there exists a logarithmic-precision Transformer with constant depth and hidden dimensions $O(n^2)$ that can generate the correct output for any input on the MUL(n, l, p) task.\nTheorems 5.1 to 5.3 demonstrate that, under standard precision, a bounded-depth Transformer with reasonable size can solve all elementary arithmetic tasks. Compared to the theoretical results for low-precision Transformers (Theorems 4.1 to 4.3), even a modest increase in numerical precision leads to a substantial improvement in expressiveness for arithmetic tasks.\nThe Reason for Increased Expressiveness. In Section 4, we highlighted low-precision Transformers' difficulties in performing elementary arithmetic operations. A critical insight is that low-precision Transformers struggle to store intermediate results, even for simple operations like adding two digits. This issue is mitigated by using standard precision. For instance, float32 can accurately represent integers within a sufficiently large range to store intermediate results during computation. Thus, standard precision allows Transformers to perform arithmetic tasks more effectively.\nPractical Implications. Our theoretical results underscore the critical importance of numerical"}, {"title": "6 Experiments", "content": "In the preceding sections, we employed complexity theory to demonstrate that low-precision Transformers face significant challenges in performing elementary arithmetic tasks. To validate these theoretical insights, we conduct a series of experiments to compare the performance of Transformer models under different precisions. The results provide empirical evidence that the model's ability to execute arithmetic operations drops as precision decreases, reinforcing our theoretical results."}, {"title": "6.1 Experimental Setup", "content": "Tasks and datasets. We evaluate three elementary arithmetic tasks: integer addition, iterated addition, and integer multiplication, as presented in Figure 1. Each task involves a series of experiments with base $p = 2, 10$ and varying choices of digit length $n$. For integer addition, we examine the addition of integers in both base-2 and base-10, with digit lengths $n \\in \\{4, 8, 16, 32, 64\\}$. For iterated addition, we examine the addition of three numbers in base-2, with digit lengths $n \\in [2, 11]$, as well as in base-10, with digit lengths $n \\in [1, 4]$. Similarly, for integer multiplication, we run experiments in base-2 with digit lengths $n \\in [2, 14]$, and in base-10 with digit length $n \\in [2,5]$. We dynamically generated datasets for both training and testing, with further details on dataset construction available in Appendix F.\nModel configurations. For all experiments, we use Transformer models with hidden dimension $d = 256$, heads $H = 4$, and model depth $L \\in \\{3, 5\\}$. The causal self-attention layer employs rotary positional embeddings (Su et al., 2024), which replace the traditional sinusoidal embeddings. For activation, we choose NewGeLU, the variant of GeLU (Hendrycks and Gimpel, 2016), and apply Xiaver initialization across the parameters (Glorot and Bengio, 2010)."}, {"title": "6.2 Experimental Results", "content": "Integer addition proves to be too easy, maintaining more than 94% accuracy even as digit length increases to 32 in both base-2 and base-10 with both float32 and bfloat16 (detailed results in Appendix F). The results of iterated addition and multiplication in base-2 are shown in Figure 2, while the corresponding results in base-10 are shown in Figure 3. Each sub-figure corresponds to a task with the x-axis representing the maximum length of the addends and the multiplicands respectively, and the y-axis representing the test accuracy. In the context of the iterated addition task, the accuracy shows a significantly more pronounced decline in 16-bit precision compared to 32-bit precision as the maximum digits length increases, across all model depths. Specifically, the 16-bit precision exhibits a marked decline in performance for lengths ranging from 7 to 10 in base-2, whereas the 32-bit precision maintains near-complete accuracy across these ranges. In base-10, 32-bit can achieve 90% correctness in contrast to the 16-bit representation, which struggles to yield accurate results. In the context of multiplication tasks, as the maximum digit length increases, 32-bit precision achieves significantly higher accuracy compared to 16-bit precision. Notably, when the length reaches 13 in base-2, the accuracy of 16-bit drops to a very low level, indicating their inability to generate correct results. Additionally, in base-10, we observe a noticeable reduction in accuracy when transitioning from 32-bit to 16-bit precision, particularly for inputs with a maximum length of 3 in 3-layer models and a maximum length of 4 in 5-layer models. This suggests that the differences of precision for performing elementary arithmetic tasks are consistent with our theoretical results."}, {"title": "6.3 Further Experiments on LLMs", "content": "We further conducted an extensive set of experiments on LLMs to empirically support our theoretical findings. Specifically, we evaluated the models' performance across three elementary arithmetic tasks, as illustrated in Figure 1. In the integer addition task, we tested the addition of two base-10 integers, varying their digit lengths from 1 to 13. For the iterated addition task, we expanded the setup to include the addition of three and five base-10 numbers, also with digit lengths ranging from 1 to 9 and 1 to 5. Similarly, in the integer multiplication task, we examined the multiplication of two base-10 numbers, with digit lengths spanning from 1 to 5. The process of generating data is the same as in previous experiments.\nAll experiments utilized the LLaMA 3.1 8B Instruct model (Dubey et al., 2024) as the base model, employing a few-shot learning approach for inference. Detailed specifications of the prompt construction and generation parameters can be found in the Appendix F. During inference, the LLMs were tasked with producing exact solutions to the given arithmetic problems. For each task, we evaluate the model on 1k samples to compute the accuracy serving as the evaluation metric. Since the LLAMA 3.1 model is trained with bfloat16 precision, we adopted its performance under bfloat16 as our baseline. To further investigate the impact of reduced numerical precision, we additionally evaluated the model's performance when quantized to 4-bit precision via the AWQ algorithm (Lin et al., 2024).\nThe results of the experiments are shown in Figure 4. Each sub-figure presents the results of a task, where the x-axis denotes the maximum length of the addends or multiplicands, and the y-axis represents the test accuracy. In the integer addition task, reducing the numerical precision from 16-bit to 4-bit has no observable effect on accuracy when the number length is less than or equal to 7, and only a slight reduction in accuracy is observed when the number length exceeds 7. This comparable performance between 16-bit and 4-bit precision supports our theorem, which asserts that both low-precision and standard-precision arithmetic can successfully handle integer addition tasks. In contrast, for iterated addition and integer multiplication tasks, the reduction in numerical precision results in a significant decrease in accuracy. Specifically, in the iterated addition task, accuracy drops by nearly 50% as the length of the addends increases. The impact is even more pronounced in the integer multiplication task, where performance deteriorates further. These experimental findings support our theoretical results that numerical precision is a critical factor in the success of iterated addition and integer multiplication tasks. Overall, the results underscore the consistency between the precision requirements for these elementary arithmetic tasks and our theoretical predictions."}, {"title": "7 Conclusion", "content": "In this work, we have theoretically analyzed the impact of numerical precision on LLMs for mathematical reasoning. By focusing on three elementary arithmetic tasks, integer addition, iterated addition, and integer multiplication, we constructively"}, {"title": "8 Limitations", "content": "One limitation of this work is that we have not fully explored all key components of mathematical reasoning. While the arithmetic tasks considered are foundational, there remain other essential elements of mathematical reasoning whose dependence on numerical precision is still unclear. Additionally, our focus was exclusively on numerical precision, but we acknowledge that other factors are likely to play a significant role in applying LLMs to mathematical reasoning. We leave these explorations for future work."}, {"title": "A Related Work", "content": "A.1 LLMs for Mathematical Reasoning\nMathmetical Reasoning. Recent studies highlight the limitations of current LLMs in mathematical reasoning (Ahn et al., 2024; Srivastava et al., 2024). Satpute et al. (2024) demonstrated that advanced models like GPT-4 can generate relevant answers, but these answers are not always accurate. Additionally, Mao et al. (2024) found that current LLMs struggle even with verifying the solutions to mathematical problems. To enhance the mathematical capabilities of LLMs, several studies have carefully designed prompting strategies (Shakarian et al., 2023; Cheng and Yu, 2023; Gu, 2023; Lu et al., 2024) or finetuned LLMs on mathematics-related datasets (An et al., 2024; Liang et al., 2024; Raiyan et al., 2023; Mishra et al., 2022; Yue et al., 2024). Other approaches include inference-based searching methods (Kang et al., 2024), the application of external tools (Yamauchi et al., 2023; He-Yueya et al., 2023; Chen et al., 2023), and the introduction of simulated interaction processes (Wu et al., 2024b) or self-verification mechanisms (Wang et al., 2023; Zhou et al., 2024a).\nArithmetical Reasoning. Bubeck et al. (2023) highlighted arithmetical reasoning as a key component of true mathematical ability. However, Saxton et al. (2019); Dziri et al. (2023) identified significant challenges that LLMs encounter when solving elementary arithmetic tasks, such as multi-digit addition and multiplication. A common approach to mitigate these difficulties is to reverse the output digit order (Shen et al., 2024), or both the input and output digit order simultaneously (Lee et al., 2024). Other studies have focused on developing improved positional encodings (Golkar et al., 2024; McLeish et al., 2024) or positional tokens (Nogueira et al., 2021) that are more suitable for arithmetic tasks. Zhou et al. (2024b,c) further examined the length extrapolation capabilities of LLMs in solving basic arithmetic problems, emphasizing the importance of data formats and positional embeddings for better generalization."}, {"title": "A.2 Computational Powers of Transformers", "content": "Another more relevant line of work investigates the theoretical expressive power of Transformers from a computational perspective.\nUniversal Approximation. Early theoretical work on Transformers primarily focused on their function approximation capabilities. Yun et al. (2019) demonstrated that Transformers can universally approximate any continuous sequence-to-sequence functions, given sufficient size. This universality result has since been extended to various Transformer variants, such as Sparse Transformers (Yun et al., 2020), Linear Transformers (Alberti et al., 2023), and Transformers with relative positional encodings (RPE) (Luo et al., 2022). Additionally, previous studies established that infinite-precision Transformers are Turing-complete (P\u00e9rez et al., 2019, 2021), while Wei et al. (2022a) showed that finite-precision Transformers are approximately Turing-complete. Although these results highlight Transformers' computational capacity, our work develops expressiveness results under more practical settings, exploring the differences in expressiveness across varying levels of numerical precision.\nFormal Language Learning. Another line of research focuses on the ability of Transformers to learn formal languages. Liu et al. (2023) explored how Transformers simulate finite state automata, while Bhattamishra et al. (2020); Yao et al. (2021) studied their ability to recognize counter languages and Dyck languages, respectively. On the negative side, Hahn (2020) showed that Transformers are not capable of learning distributions over languages. In addition to affirmative results, several works have characterized the limitations of Transformers from the perspective of formal language modeling (Hahn, 2020; Bhattamishra et al., 2020; Weiss et al., 2021; Yao et al., 2021; Chiang et al., 2023) or circuit simulation (Hao et al., 2022; Merrill et al., 2022; Merrill and Sabharwal, 2023). However, few of these studies focus on the autoregressive Transformers commonly used in LLMs, which we investigate in this paper.\nChain-of-Thought and In-Context Learning. Chain-of-Thought (CoT) prompting (Wei et al., 2022b) plays a crucial role in tasks requiring complex reasoning structures, and several studies aim to understand its underlying mechanisms. For instance, Feng et al. (2023); Li et al. (2024) analyzed CoT from an"}, {"title": "B Additional Background and Preliminary", "content": "B.1 Circuit Complexity\nCircuit complexity classes capture various aspects of computational complexity, typically bounding circuit width and depth. For a more detailed introduction, we refer to Arora and Barak (2009).\nWe begin by defining Boolean circuits. A Boolean circuit over a basis of gates is represented as a finite-size directed acyclic graph (DAG), where each vertex corresponds to either a basis function (or gate) or an input bit. Some internal nodes are designated as outputs, and the fan-in of a vertex is defined as its in-degree. Building on this definition, we can define the complexity classes NC\u00b2, AC\u00b2, and TC\u00b2:\n\u2022 NC\u00b2: This class consists of constant fan-in, polynomial-sized circuits made up of AND, OR, and NOT gates, with a depth of O(log n).\n\u2022 AC\u00b2: This class includes unbounded fan-in, polynomial-sized circuits composed of AND, OR, and NOT gates (with NOT gates allowed only on inputs), also having a depth of O(log n).\n\u2022 TC\u00b2: This class extends AC\u00b2 by allowing majority gates.\nThe relationships among the NC, AC, and TC hierarchies are as follows:\nNC\u00b2 \u2282 AC\u00b2 \u2282 TC\u00b2 \u2282 NC\u00b2+1, NC\u00ba \u2286 AC\u00ba \u2286 TC\u00ba.\nB.2 Constant-precision Transformer\nPrevious work has investigated the expressiveness of constant-precision Transformers (Li et al., 2024), utilizing a simplified version of the IEEE 754 standards (IEEE, 2019). Our constant-precision setting is analogous, and we will introduce the floating-point representations we consider here.\nA (e + 2s + 1)-floating point representation includes e exponent bits, 2s precision bits, and one sign bit. The numbers representable under this representation are defined as follows:\n$\\mathcal{F}_{e,s} := \\{S \\cdot 2^{-s+E} | -2^{2s} + 1 < S < 2^{2s} - 1, -2^{e-1} < E \\leq \\max(2^{e-1} - 1, 0), S, E \\in \\mathbb{Z}\\}.$\nFor any $x \\in \\mathbb{R}$, its representation under this floating-point format is determined by rounding to the nearest value in $\\mathcal{F}$. In the event of a tie, we select the number with the smaller absolute value. In this paper, we focus on the case where $e = 0$, which means all representable numbers take the form $S \\cdot 2^{-s}$, with $S \\in \\mathbb{Z}$ such that $-2^{2s} + 1 < S < 2^{2s} - 1$. However, this is necessary only for Theorem 4.1, while Theorems 4.2 and 4.3 do not depend on specific numerical representations.\nLi et al. (2024) demonstrated that constant-depth Transformers with constant precision belong to the complexity class AC\u00ba.\nB.3 Logarithmic-precision Transformer\nA key limitation of constant-precision representation is that it fails to capture the input size $n$ within a single neuron. To address this, we consider logarithmic precision, allowing for $O(\\log n)$ bits for numerical representations. Logarithmic-precision Transformers possess several advantageous properties (Feng et al., 2023; Feng and Zhong, 2023):\n\u2022 For floating-point representations with $O(\\log n)$ bits, any real number $x \\in O(\\text{poly}(n))$ can be represented with $O(\\text{poly}(1/n))$ error."}, {"title": "C Technical Lemmas", "content": "C.1 Technical Lemmas for Logarithmic Precision MLP\nWe will first provide some basic results for logarithmic precision MLP", "f": "mathbb{R"}, 2, "rightarrow \\mathbb{R}$ with 4 hidden dimension and GeLU activation, such that for any $a, b \\in [-M, M"], "g": "mathbb{R"}, {"g": "mathbb{R"}, {"mathbb{R}^d$": "g(x", "operations": "COPY and MEAN, showing that the standard attention layer with log-precision is capable of these operations under some regularity assumptions. Here, we will provide the results and further consider a special operation called SINGLE COPY.\nConsider a sequence of vectors $\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_n$ where $\\mathbf{x}_i = (x_i, r_{i,1}) \\in [-M, M]^{d+2}$ and $"}]