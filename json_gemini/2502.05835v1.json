{"title": "Contrastive Representation Distillation via Multi-Scale Feature Decoupling", "authors": ["Cuipeng Wang", "Tieyuan Chen", "Haipeng Wang"], "abstract": "Knowledge distillation is a technique aimed at enhancing the performance of a smaller student network without increasing its parameter size by transferring knowledge from a larger, pre-trained teacher network. Previous approaches have predominantly focused on distilling global feature information while overlooking the importance of disentangling the diverse types of information embedded within different regions of the feature. In this work, we introduce multi-scale decoupling in the feature transfer process for the first time, where the decoupled local features are individually processed and integrated with contrastive learning. Moreover, compared to previous contrastive learning-based distillation methods, our approach not only reduces computational costs but also enhances efficiency, enabling performance improvements for the student network using only single-batch samples. Extensive evaluations on CIFAR-100 and ImageNet demonstrate our method's superiority, with some student networks distilled using our method even surpassing the performance of their pre-trained teacher networks. These results underscore the effectiveness of our approach in enabling student networks to thoroughly absorb knowledge from teacher networks.", "sections": [{"title": "1. Introduction", "content": "The past few decades have witnessed remarkable achievements of neural networks in the field of computer vision. With the proposal of the residual network architecture (He et al., 2016), the depth of neural networks has increased significantly. Deeper networks with more parameters have brought improved performance; however, they also come with trade-offs. As network depth increases, both computational and storage costs escalate accordingly, posing a significant challenge to deployment on resource-constrained devices. To address this issue, various model compression techniques have been proposed, including model pruning (Frankle & Carbin, 2018; Li et al., 2016; Liu et al., 2018; Luo et al., 2017), model quantization (Jacob et al., 2018; Courbariaux et al., 2015), lightweight network design (Howard, 2017; Sandler et al., 2018; Zhang et al., 2018), and knowledge distillation (Hinton, 2015; Zagoruyko & Komodakis, 2016; Romero et al., 2014).\nIn light of the significant potential of knowledge distillation for model compression, we focus on the application and optimization of knowledge distillation techniques in this paper. Knowledge distillation is a specialized form of transfer learning, where the \"knowledge\" from a larger pre-trained network (also known as the teacher) is transferred to a smaller student network (a.k.a. the student) to enhance the performance of the latter. Hinton et al. (Hinton, 2015) first propose distilling a teacher's knowledge into a student by minimizing the Kullback-Leibler (KL) divergence between their predictions. However, logit-based KD approaches fail to fully utilize the \"knowledge\" embedded in the teacher network. To address this limitation, FitNet (Romero et al., 2014) introduces distillation using intermediate-layer features of the network. AT (Zagoruyko & Komodakis, 2016) further leverages attention maps to facilitate knowledge transfer. CRD (Tian et al., 2019) improves feature distillation by incorporating contrastive learning for knowledge transfer. Reviewing existing methods based on feature distillation, these approaches essentially achieve knowledge transfer on features by applying certain transformations on the features of each layer or through elaborate design of loss functions. Despite yielding favorable outcomes, these methods remain suboptimal as they primarily focus on transferring global features and do not adequately address the local information within each layer's features or conduct corresponding processing. As shown in Figure 1, the features at different stages within the network may include information from multiple categories. Consequently, local features at different positions and scales may predominantly focus on distinct category information. Zhang et al. (Zhang et al., 2022) have shown that the process of knowledge distillation primarily emphasizes the transfer of foreground \"knowledge points,\" while feature information from different categories, considered as background \"knowledge points,\" is often overlooked during the distillation process. Consequently, when only global features are considered as a whole for distillation, the student network may fail to capture the local information in the teacher network fully. This can result in the student being unable to fully assimilate the knowledge from the teacher, leading to suboptimal performance.\nTo address this issue, we propose a method of feature multi-scale decoupling, we propose a method of multi-scale feature decoupling that decouples the features of the student and teacher networks across multiple scales during the feature distillation process. Decoupling is an effective approach for addressing scenarios where factors of varying importance are sub-optimally treated as a unified whole (Chen et al., 2024). This method ensures that the student not only learns the global feature knowledge from the teacher network but also fully captures the local knowledge.\nAdditionally, we propose an improved and budget-saving feature distillation method based on contrastive learning. CRD (Tian et al., 2019) requires complex sample processing and a large memory buffer to store feature representations of all samples from both the teacher and student networks, along with additional parameters. By introducing multi-scale feature decoupling to contrastive representation distillation, we not only enrich the number of samples, enabling us to acquire abundant feature information with merely a single batch of samples, but also process global and local features separately. This enables features from different-category samples within the same batch to be treated as negative samples, while even distinct local parts of the same sample, belonging to different categories, are also considered negative samples. Furthermore, the feature multi-scale decoupling process is parameter-free and significantly enriches the sample information. Eventually, the rich and decoupled feature sample information is input into our designed contrastive loss function(CL), leading to enhanced performance of the student network. We also employ the Attention-Based Projector (ABP, detailed in Section 4.2) to enhance discriminative regions through channel-wise attention. The overall framework is shown in Figure 2\nIn general, we summarize our contributions as follows:"}, {"title": "2. Related Work", "content": "Knowledge distillation transfers the \"dark knowledge\" of a complex teacher network to a lightweight student network, enhancing the performance of the student network. Depending on the type of transferred knowledge, previous knowledge distillation (KD) methods can be categorized into three main groups: based on transferring logits (Hinton, 2015; Luo, 2024; Zhao et al., 2022; Sun et al., 2024; Jin et al., 2023; Li et al., 2023), features (Romero et al., 2014; Tian et al., 2019; Chen et al., 2022; 2021; Heo et al., 2019; Park et al., 2019; Ahn et al., 2019), and attention (Zagoruyko & Komodakis, 2016; Guo et al., 2023).\nMany transferring features methods followed Fit-Net (Romero et al., 2014) by utilizing single-stage features for knowledge distillation. PKT (Passalis et al., 2020) aligned the probability distributions of the teacher and student network features by minimizing their statistical divergence. SimKD (Chen et al., 2022) decoupled the classification head from the feature extractor, enabling effective knowledge transfer by directly reusing the teacher's classifier to guide the student's feature learning. CRD (Tian et al., 2019) combined contrastive learning with knowledge distillation by leveraging a memory buffer to optimize contrastive objectives.\nIn the feature-based distillation methods, many works proposed to utilize multi-level feature distillation. OFD (Heo et al., 2019) enhanced student network performance by adjusting the placement of feature distillation layers, introducing a novel activation function called Margin ReLU, and employing partial L2 distance as the feature alignment metric. ReviewKD (Chen et al., 2021) improved knowledge distillation by introducing a review mechanism, in which the lower-layer features of the teacher guide the higher-layer features of the student.\nHowever, previous methods primarily focused on global feature information, without addressing the decoupling of relationships between global and local features. Therefore, we introduce multi-scale decoupling of feature into feature distillation to decouple different levels of knowledge. Furthermore, we improve the previous contrastive representation distillation method. Although also grounded in the mutual information maximization theory, our method eliminates the reliance on a large memory buffer for updating the feature information of positive and negative samples, achieving budget-saving by relying solely on single-batch samples(see Appendix A.1 for detailed analysis). We have also achieved state-of-the-art performance in numerous experiments, further demonstrating the excellence of our method."}, {"title": "3. Method", "content": "In Sec. 3.1, we describe the feasibility and implementation of multi-scale feature decoupling. Then in Sec. 3.2, we combine it with a contrastive loss to derive the feature distillation loss function. Finally, the complete training objective is introduced in Sec. 3.3.\nNotation. Given a batch of input images $X_i, x_j$ (where i, j = 1, 2, ..., B), i = j indicates the same input image. We use $f_T$ and $f_S$ to denote the feature extractors of the teacher and student networks, respectively. For the penultimate layer features in the feature extractor, we do not apply any processing to the teacher network's features $f_T(x_j) \\in \\mathbb{R}^{c_T\\times h_T\\times w_T}$, denoted as $T_i$. For the student network's features $f_S(x_i) \\in \\mathbb{R}^{c_s\\times h_s\\times w_s}$, we project them using ABP to match the same number of channels as the teacher network, and the resulting features are denoted as $S_i$, where $c_s$ and $c_T$ represent the number of feature channels for the student and teacher networks, respectively. Similarly, $h_s, w_s$ and $h_T, w_T$ are the spatial dimensions of the student and teacher network features, respectively. The classifier of the teacher network, denoted as $f_{c_T}$, maps the feature $T_i$ to its corresponding class $C_i = f_{c_T}(T_i)$.\n3.1. Multi-Scale Feature Decoupling\nAn entire image often couples the information of multiple classes, and during the feature extraction process in neural networks, different stages, scales, and spatial locations of features (referred to as local features) may focus on category-specific information that different from the global features. Therefore, when transmitting the features of each stage, solely transmitting the fused global features of various fine-grained details may lead to ambiguous knowledge transfer to the student network, preventing it from acquiring the knowledge of other category-specific local features within this global feature and resulting in suboptimal performance.\nFor the multi-scale decoupling module, the main process involves first decoupling the features of each sample from both the teacher and student networks at multiple scales. Afterward, the decoupled feature samples are classified to facil-"}, {"title": "3.2. Contrastive Loss", "content": "When designing the contrastive loss function, we eliminate the reliance on the large memory buffer used in CRD (Tian et al., 2019), which stores feature information of both teacher and student networks, as well as other parameters. Instead, we enrich the sample data by decoupling the student and teacher features at multiple scales. For a batch of input data with size B, we can obtain B pairs of student-teacher network feature maps $(S_i, T_i)$. After performing multi-scale decoupling on the features, we can obtain $N = M \\times B$ pairs of student-teacher network feature maps $(S_m, T_n)$. We would like to push closer the representations $S_{im}$ and $T_{in}$, while pushing apart $S_{im}$ and $T_{in}$ (where i \u2260 jor m\u2260n). Specifically, we consider the joint distribution $p(S_{im}, T_{in})$ between the student network feature samples and the teacher network feature samples, as well as the product of the marginal distributions $p(S_m)p(T_n)$, in order to maximize the mutual information between related positive sample pairs:\n$I(S_{im}, T_{in}) = E_{p(S,T)} (log(\\frac{p(S_{im}, T_{in})}{p(S_m)p(T_n)}))$"}, {"title": "3.3. Training Objective", "content": "In the process of feature extraction by neural networks, the focus of feature information varies across stages: shallow features focus on gradient information, middle-layer features capture local information of the target region, and deep-layer features emphasize the global information of the target region. Through experimental studies, we find that different student models have distinct requirements for information at various layers of the teacher network(Details are in Section 4.2). Therefore, when only a single layer is utilized for knowledge distillation, the student network may be unable to effectively learn the knowledge from the teacher network, leading to suboptimal performance. Some student networks may only require knowledge from a single layer of the teacher network to learn effectively, while others may need multiple layers to achieve optimal performance.\nIn the design of the feature distillation loss function, we carefully consider and decide to apply multi-scale feature decoupling separately to features at each stage, followed by the computation of the contrastive loss as described in Equation 12. Then, the losses from each individual stage are aggregated:\n$L_{kd} = \\sum_{Layer} L_{single\\_kd}$\nwhere Layer refers to all layers from the first stage to the penultimate stage in the feature extraction process. Finally, the distillation loss is combined with the cross-entropy loss $L_{ce}$ from hard labels to obtain the final loss function.\n$L = L_{ce} + \\beta L_{kd}$"}, {"title": "4. Experiments", "content": "Datasets. Experiments on CIFAR-100 (Krizhevsky et al., 2009) and ImageNet (Russakovsky et al., 2015) are conducted. CIFAR-100 (Krizhevsky et al., 2009) dataset is a well-established benchmark for image classification encompassing 100 categories. It contains 50,000 training images and 10,000 validation images, with each image having a resolution of 32x32 pixels. ImageNet (Russakovsky et al., 2015) is a larger-scale image classification dataset, consisting of images from 1,000 categories. The training set contains 1.28 million images, while the validation set includes 50,000 images.\nImplementation Details. On CIFAR-100 dataset, we conduct experiments on various classic network architectures, including VGG (Simonyan, 2014), ResNet (He et al., 2016), WideResNet (Zagoruyko, 2016), MobileNet (Sandler et al., 2018), and ShuffleNet (Ma et al., 2018; Zhang et al., 2018)."}, {"title": "4.1. Main Results", "content": "Results on CIFAR-100. We divide the previous methods into three groups as described in Section 2. In the logits-based distillation methods, we use the classic KD method as well as the more advanced CTKD and DKD methods. In the attention-based distillation methods, we use AT and CAT-KD. In the feature-based distillation methods, we use several popular and cutting-edge methods.\nTable 1 summarizes results on CIFAR-100 with the teacher and student having identical architectures but different configurations. Table 2 shows the results where the teacher and student have distinct architectures. It is clearly shown that our method achieves state-of-the-art results in the vast majority of cases, with all results outperforming the previous contrastive learning-based feature distillation method CRD significantly. Moreover, our method enables many student networks to outperform the teachers. The results above demonstrate the superiority of our method to fully learn the knowledge from the teacher network.\nResults on ImageNet. Table 3 compares the differences between different methods in terms of top-1 and top-5 accuracy. We experiment with two settings of distillation from ResNet50 to MobileNet, and from ResNet34 to ResNet18 respectively. The results indicate that our method outperforms all other methods, whether in the architecture of the same style or different architecture. This further demonstrates the robustness of our method's performance across various teacher-student network pairs in large-scale datasets.\nMore Analysis. As shown in Table 1 and Table 2, many student networks outperform their respective teacher networks after applying our distillation method. This finding contradicts the previous empirical belief that the student network should merely mimic the distribution of teacher networks as closely as possible. Some studies, such as (Nagarajan et al., 2023), indicate that student networks learn a systematic bias from the teacher network's distribution. This systematic bias, induced by the regularization effect"}, {"title": "4.2. Extension Experiments", "content": "Ablation Study. We conduct ablation experiments by incrementally adding components to measure their individual effects. The results are shown in Table 4, we conduct each experiment three times and report the average accuracy. We"}]}