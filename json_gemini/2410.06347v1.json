{"title": "Solving Multi-Goal Robotic Tasks with Decision Transformer", "authors": ["Paul Gajewski", "Dominik \u017burek", "Marcin Pietro\u0144", "Kamil Faber"], "abstract": "Artificial intelligence plays a crucial role in robotics, with reinforcement learning (RL) emerging as one of the most promising approaches for robot control. However, several key challenges hinder its broader application. First, many RL methods rely on online learning, which requires either real-world hardware or advanced simulation environments-both of which can be costly, time-consuming, and impractical. Offline reinforcement learning offers a solution, enabling models to be trained without ongoing access to physical robots or simulations.\nA second challenge is learning multi-goal tasks, where robots must achieve multiple objectives simultaneously. This adds complexity to the training process, as the model must generalize across different goals. At the same time, transformer architectures have gained significant popularity across various domains, including reinforcement learning. Yet, no existing methods effectively combine offline training, multi-goal learning, and transformer-based architectures.\nIn this paper, we address these challenges by introducing a novel adaptation of the decision transformer architecture for offline multi-goal reinforcement learning in robotics. Our approach integrates goal-specific information into the decision transformer, allowing it to handle complex tasks in an offline setting. To validate our method, we developed a new offline reinforcement learning dataset using the Panda robotic platform in simulation. Our extensive experiments demonstrate that the decision transformer can outperform state-of-the-art online reinforcement learning methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning (RL) is a paradigm that allows an agent to learn by interacting with its environment. RL has proven to be ground-breaking in multiple domains, such as self-driving cars, games, and robotics [1], [2], [3], [4], [5], [6], [7], [8].\nMost existing algorithms work in an online methodology, where the agent interacts with the current state of the environment, takes a specific action, and receives feedback, known as a reward [9]. An alternative is to take advantage of a simulated environment that allows the agent to take action and receive feedback without all the disadvantages described before [10], [11]. However, this solution is not perfect, as creating a simulated environment can be costly and often will be just a simplified model of reality, not covering all possible factors, which may lead to discrepancies between simulation and real-life environment [12].\nThese issues led to the invention of an alternative ap- proach: offline reinforcement learning, which allows the agent to train using a pre-collected data set. The agent can learn by replaying episodes that contain observations of the environment, actions taken, and rewards received [13], [14]. The dataset is created by recording multiple episodes of a straightforward or random agent interacting with the envi- ronment. This recording can then be reused multiple times without the necessity of repeating the collection process, allowing for model development without a costly real-life environment.\nIn robotics, single-goal environments are often insufficient to achieve the generalization required for successful work in realistic environments [8]. For example, even the ability to reach the robot's end effector to a desired position in space is impossible to achieve with a single goal training process. The solution for this challenge is multi-goal environments, where the agent can learn more general goals, such as controlling its motors to reach any desired position [10].\nThe recent advancements in transformer neural network architectures have significantly impacted the reinforcement learning field, including the robotics domain [15], [16]. One of the most recent advancements, the decision trans- former, casts the problem of RL as conditional sequence modeling, leveraging modern neural network mechanisms such as attention to provide high-quality decision-making capabilities [17], [18]. The decision transformer proved to work efficiently in domains such as robotics, games, task planning, prompting, [15], [16], [19], [20], [21], [22].\nHowever, while all three aspects mentioned-offline re- inforcement learning, multi-goal learning, and transformer architecture-are present in robotic research, they have never been coupled together. To fill this gap, we explore the possibility of adopting a transformer for multi-goal problems in robotics domain and offline settings. To achieve this purpose, we first create a dataset for offline reinforcement learning leveraging a Franka Emika Panda robotic environment with multiple tasks. Then, we modify a decision transformer, enriching it with the capability to receive and interpret information about the desired goal. Moreover, we leverage a sparse reward system instead of dense to ensure a high-quality definition of rewards. Finally, we conduct an exhaustive experimental study showcasing that our modified decision transformer can perform better than online rein- forcement learning methods.\nThe contributions of these paper can be summarized as follows:\n\u2022 Extending a decision transformer architecture with the capability to interpret the goal objectives, effectively creating a multi-goal decision transformer able to learn in an offline setting."}, {"title": "II. RELATED WORK", "content": "Reinforcement learning (RL) is a domain of machine learning concerned with how an agent ought to perform ac- tions in a dynamic environment to maximize the cumulative reward [23]. The reward is a feedback from the environment [23]. It has been applied successfully to various problems, including energy storage operation, robot control, hardware design, photovoltaic generator dispatch, backgammon, Go (AlphaGo), atari games or autonomous driving systems [24], [25], [26], [27], [28], [29]. Most of the Reinforcement Learn- ing solutions are based on Markov Decision Process and Bellman equation [30], [23]. These methods can be divided to three main groups: value-based methods (e.g Q-learning, Double Q-Learning (DQL) which store state to action tran- sitions in tables [23], [31], policy-based methods (Policy Gradient which relies upon optimizing parametrized policies with respect to the expected return by gradient descent [23], [32], Proximal Policy Optimization (PPO), [33], [34], Trust Region Policy Optimization (TRPO), [35]) and actor-critic methods that combine aspects of both policy-based methods (Actor) and value-based methods (Critic) (e.g. A2C and A3C [36], [37]). Additionally, the deep reinforcement learning (DRL) approach incorporates neural network architectures to model value function or policy distribution when the state and the action space is huge and high-dimensional [38], [31], [29].\nRobot training consists of continuous control tasks. This aspect increases the complexity of the problem. Many of the methods mentioned earlier have limitations that often prevent them from achieving satisfactory results during robot training. One of the method which achieves good performance in a range of continuous control benchmark tasks, outperforming many prior on-policy and off-policy methods, is Soft Actor Critic (SAC) [27]. It is an off-policy actor-critic DRL algorithm based on the maximum entropy reinforcement learning framework. In contrast to other off- policy algorithms, SAC achieves very similar performance across different random seeds.\nThe Truncated Quantile Critics (TQC) investigates a novel way to alleviate the overestimation bias in a continuous control setting [28]. It combines three ideas: distributional representation of a critic, truncation of critics' prediction, and ensembling of multiple critics. Distributional representation and truncation allow for arbitrary granular overestimation control, while ensembling provides additional score improve- ments. TQC outperforms SAC in all environments in the continuous control benchmark suite (demonstrating 25% im- provement in the most challenging Humanoid environment).\nThe important aspect in robot training is that the reward space is very sparse. Hindsight Experience Replay (HER) allows sample-efficient learning from rewards that are sparse and binary and, therefore, avoid the need for complicated reward engineering, [26]. Ablation studies show that Hind- sight Experience Replay is a crucial ingredient which makes training possible in robot arm challenging environment. HER is used with TQC as a baseline state-of-the-art in our comparative studies.\nMost of the mentioned RL algorithms work as a single- goal. In case of training the robot to better adapt to real conditions, it is very helpful to carry out a multi-goal strategy. The paper [39] discusses multi-goal strategy ex- tensively. The tasks presented in this work include pushing, sliding, and picking and placing with a Fetch robotic arm. All tasks have sparse binary rewards and follow a Multi- Goal Reinforcement Learning (RL) framework. The authors present a set of concrete research ideas for improving RL algorithms, most of which are related to Multi-Goal RL and Hindsight Experience Replay.\nAll these approaches still use actor-critic algorithms for optimization, focusing on novelty in architecture or efficient sampling. In this work, we propose a completely novel approach for multi-goal offline training.\nVarious works have studied guided generation for images and language using Transformer-based architectures [40], [41], [42]. However, these approaches mostly assume con- stant 'classes', while in reinforcement learning the reward signal is varying over time. Transformers have been success- fully applied to many tasks in natural language processing and computer vision [43], [40], [41], [42]. However, trans- formers are relatively unstudied in RL, mostly due to the different nature of the problem, such as the higher variance in training. There were some trials to adapt the attention mechanism in the RL environment (e.g., in [44] authors showed that iterative self-attention allowed RL agents to better utilize episodic memories).\nOne of the first approaches using the Transformers ar- chitecture without the actor-critic approach is the Decision Transformer, presented in [19]. It is an architecture that casts the problem of RL as conditional sequence model- ing. Decision Transformer outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, Decision Transformer model can gener- ate future actions that achieve the desired return. Decision Transformer matches or exceeds the performance of state- of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks. In this work, we show the process of adapting the Decision Transformer in multi-goal sparse rewards environment.\nIn [45], which describes the multi-objective decision transformer, the authors reformulate offline RL as a multi- objective optimization problem, where the prediction of"}, {"title": "III. METHODOLOGY", "content": "In this section, we explain the specifics of our methodol- ogy. First, we describe how a decision transformer leverages transformer architecture for modeling reinforcement learning problems. Second, we explain the dataset creation process, along with the description of the environments. Finally, we explain how we adapt the decision transformer to work in multi-goal environments.\nThe main component of the presented approach is the Decision Transformer.\nThe main component of it is the Transformer, which was proposed in [43] as an architecture to efficiently model se- quential data. These models consist of stacked self-attention layers with residual connections:\n$T = T_n(T_{n-1}(...T_1(z)))$ \nEach self-attention layer receives the embeddings. The i- th token is mapped via linear transformations to a key $k_i$, query $q_i$ and value $v_i$. The i-th output of the self-attention layer is given by weighting the values $v_j$ by the normalized dot product between the query $q_i$ and other keys $k$:\n$z_i = \\sum_{j=1}^{n} softmax(\\frac{<q_i, k_j'>}{\\sqrt{d_k}}) v_j$ \nThe input to the network consists of a sequence of past rewards, actions, and current states (see Figure 2):\n$z = {R_1, s_1, a_1, R_2, s_2, a_2, \u2026, r_T, s_T, \u03b1_T}$ \nThe Decision Transformer (DT) is trained entirely offline using a fixed dataset. For this purpose, we generated datasets of two types: expert and random, for all environments. Expert data sets were created using well-trained TQC agents as demonstrators. These agents were evaluated in their respec- tive environments for 1 million timesteps, during which their trajectories were recorded. In contrast, random data sets were generated using agents that sampled actions randomly at each timestep.\nFor a more detailed evaluation of the DT, we also com- bined expert and random datasets to create mixtures with varying ratios, or selected specific subsets of the expert data. Throughout the experiments conducted with the various combinations of random and expert datasets, we try to demonstrate that it is not essential to have large expert datasets. This fact is desirable in the case of generating optimal samples, which is time consuming.\nThe datasets used in this article, containing one million expert and random demonstration transitions, are publicly available\u00b9. All datasets for expert-random mixtures and ex- pert subsets were derived from these original datasets.\nMulti-goal robotic environments [10], like the ones used in this research, represent a specialized subset of reinforce- ment learning environments. The observation space in these environments is always a dictionary composed of three key elements: the current observation, the desired goal, and the achieved goal. The current observation serves the same role as in classic Gym environments, providing a description of the state of the environment as perceived by the agent. The desired goal specifies the target state that the agent should aim to achieve through its actions and is typically a subset of the observation. The achieved goal has the same structure as the desired goal, but represents the state the agent has reached at the current timestep. Effectively the state, action, reward triple can be expressed as:\n$s_t \u2208 {O,G, \\hat{G}}, a_t \u2208 R^n, R_t \u2208 R$ \nWhere $s_t$ is the state at timestep t, O is the observation space of the environment, G is the domain in which goals are represented. There are two goals: achieved and desired therefore state consists of a tuple:\n$s_t = {O_t, g_d, g_a}$"}, {"title": "IV. EXPERIMENTAL SETUP", "content": "Our experimental evaluation aims to answer three main research questions:\n\u2022 RQ1: Can Decision Transformer (DT) match the effec- tiveness of state-of-the-art online algorithms for multi- goal robotic environments, such as TQC with HER?\n\u2022 RQ2: How well does Decision Transformer cope with sparse rewards compared to dense rewards?\n\u2022 RQ3: How does the number of training examples affects the performance of Decision Transformer and what is the minimum dataset size for DT to perform effectively?\n\u2022 RQ4: How does the ratio between random and expert demonstrations induced in the dataset affect the perfor- mance of Decision Transformer?\nWe carry out each experiment three times with different random seeds and reports the averaged results along with standard deviation. All agents were evaluated over 10,000 time steps. The experiments were carried out in simulated OpenAI Gym environments [10] using a Franka Emika Panda robotic arm mounted on a table [11]. The tasks spanned three robotic scenarios: Reach, Push, and PickAndPlace, with dense and sparse reward structures. The datasets are com- posed of complete episodes, which means that each episode is fully included or excluded, with no partial episodes used for training.\nTo answer the first research question, we carry out the experiments with Decision Transformer and TQC with HER agent. We train DT leveraging our devised offline expert- level dataset containing 1 million transitions and evaluate it in the online environment. To ensure fair comparison, we leverage the same number of transitions and the same conditions for training and evaluating TQC agent.\nIn order to compare the efficiency of the proposed algo- rithms, we use two metrics: success rate, which describes the percentage of tasks carried out successfully, and return, which corresponds to the total reward that the agent accu- mulates during the episode.\nThe second research question examines how well the Decision Transformer performs in environments with sparse reward signals (rewards provided only upon success) com- pared to environments with dense rewards (where feedback is given at every step, guiding the agent toward the goal).\nOur third research questions aims to test the DT data efficiency by reducing the size of the training datasets. We evaluate DT with dataset sizes of 1 million, 750,000, 500,000, 250,000, and 100,000 transitions. In each case, a DT agent is trained from scratch on the corresponding dataset subset and then evaluated in the online environment.\nOur fourth research questions evaluates the resilience of the DT to noisy data by systematically increasing the number of random trajectories while decreasing the number of expert trajectories. The size of the data set was fixed at 1 million transitions, but the ratio of expert to random trajectories was varied in steps of 100%, 75%, 50%, 25%, and 0% expert data. This experiment aimed to determine how well DT can handle mixed-quality demonstrations.\nBoth learning algorithms, DT and TQC with HER, require careful tuning of hyperparameters. For TQC with HER, we use the best available hyperparameters at the time of writing, as reported in the Stable-Baselines-Zoo [46] repository for TQC with HER in Panda environments. For the Decision Transformer, we adopt the hyperparameters from the original DT paper.\nAll the code used for training and evaluation in this"}, {"title": "V. RESULTS AND DISCUSSION", "content": "In this section, we present the results of our experimen- tal evaluation and provide discussion related to previously formulated research questions.\nTable I showcases the performance of the Decision Trans- former (DT) and TQC+HER method on all three previ- ously described environments - Reach, Push, and PickAnd-Place-evaluated with both Dense and Sparse reward types.\nFocusing on Dense reward, we can observe that both methods achieve perfect success rate (100%) and the same average return (-0.21) for the Reach environment. This outcome is expected, given that the Reach task is the simplest among those considered in our experiments.\nOn the other hand, analyzing the results for the Push environment, we can see that DT achieves better results in both cases in terms of both Average Return (-0.95 vs -1.04) and success rate (99.54% vs 98.69%). We can observe a similar pattern for the most challenging PickAndPlace envi- ronment, where DT yields better Return value and success rate than TQC+HER (Return value -1.30 vs. -1.35; success rate 98.89% vs. 98.73%).\nIn contrast, when examining the Push environment, DT demonstrates superior performance in both Average Re- turn (-0.95 vs. -1.04) and success rate (99.54% vs. 98.69%). A similar trend is also evident in the more complex PickAndPlace environment, where DT outperforms TQC+HER, achieving a higher Return (-1.30 vs. -1.35) and success rate (98.89% vs. 98.73%).\nThese findings are significant, as they highlight not only the ability of our enhanced Decision Transformer to handle multi-goal environments with remarkable efficiency but also its capability to surpass the limitations of its online counter- part, TQC+HER (RQ1).\nNotably, the training times for the Decision Transformer and TQC with HER are approximately 80 minutes and 240 minutes, respectively. In this comparison, the number of online transitions for TQC is equal to the size of the offline dataset used to train DT.\nWe now turn our attention to the impact of sparse rewards on both DT and TQC+HER methods. As discussed in Section III-C, sparse reward systems are sometimes utilized in robotics reinforcement learning research due to a more straightforward definition of reward function in some do- mains.\nAs indicated in Table I, sparse rewards affect the perfor- mance of both DT and TQC+HER methods. Starting with DT, we observe that it retains a perfect success rate (100) in the Reach environment, but its performance decreases in the Push and PickAndPlace environments. Specifically, the success rate drops from 99.54% to 95.00% in Push and from 98.89% to 97.79% in PickAndPlace. Nevertheless, it is essential to emphasize that DT maintains a high success rate (over 95%) across all environments, demonstrating its robustness to the more challenging domains in which only sparse reward is available (RQ2).\nFocusing on TQC+HER for comparison, we can observe a slight reduction in the success rate for the Reach en- vironment, decreasing marginally from 100% to 99.95%. Interestingly, introducing sparse rewards improves the results for the Push environment, where the success rate increases from 98.69% to 99.46%. However, TQC+HER experiences a significant decline in performance in the most challenging PickAndPlace environment, achieving only a success rate of 76.99% (this low score of TQC+HER is due to its instability, as for some seeds it fails to learn properly).\nFor the Reach task, which is relatively simple, the success rate remains perfect (100%) across all dataset sizes between 1 million and 100 thousand samples. On the other hand, we observe a tendency for the success rate to decline as the dataset size decreases in Push and PickAndPlace envi- ronments with dense reward functions. The first noticeable drop occurs when the model is trained with less than 250 thousand samples. However, even with a trimmed dataset containing just 100 thousand samples, the success rate re- mains above 96%, demonstrating DT's ability to perform well with smaller datasets, a crucial advantage in scenarios where collecting large datasets is challenging (RQ3).\nWhen examining the results for Push and PickAndPlace under sparse rewards, we can observe a higher degree of variance, with success rates fluctuating significantly as the dataset size changes. While for the PickAndPlace envi- ronment, performance drops considerably when the dataset size falls below 250 thousand samples, it is impossible to recognize a clear trend for Push environments due to the high variance and results fluctuations.\nAs expected, the Reach environment remains manageable for DT, even with a minimal percentage of expert-generated data. However, a negative trend emerges for the more com- plex environments as the expert percentage decreases. This decline becomes especially evident when the expert data falls below 25%, drastically reducing the success rate to less than 10% when the dataset is primarily built on random knowledge. However, it is noteworthy that DT still achieves decent performance, maintaining a success rate above 80%, even when only 25% of the dataset is created leveraging the"}, {"title": "VI. CONCLUSIONS & FUTURE WORK", "content": "In this study, we tackled the challenge of multi-goal offline reinforcement learning for robotics by leveraging the strength of transformer architectures. We enhanced the decision trans- former to follow specific goal objectives, thereby creating a multi-goal decision transformer capable of learning in an offline setting. Additionally, we developed and publicly released a new robotic dataset for offline reinforcement learning, utilizing the state-of-the-art TQC+HER as the expert agent. Our comprehensive experimental evaluation demonstrated that the decision transformer can outperform its online counterpart. We also showed that our approach is effective even with a limited number of training examples and does not require the dataset to be fully generated by an expert agent.\nIn future work, we will extend this approach to incorporate continual learning and explore how the decision transformer handles datasets containing a mixture of examples from multiple heterogeneous experts."}]}