{"title": "Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large\nLanguage Models", "authors": ["Jialiang Wu", "Yi Shen", "Sijia Liu", "Yi Tang", "Sen Song", "Xiaoyi Wang", "Longjun Cai"], "abstract": "Despite their impressive capacities, Large lan-\nguage models (LLMs) often struggle with the\nhallucination issue of generating inaccurate\nor fabricated content even when they pos-\nsess correct knowledge. In this paper, we\nextend the exploration of the correlation be-\ntween hidden-state prediction changes and out-\nput factuality into a deeper, token-wise level.\nBased on the insights, we propose cross-layer\nEntropy eNhanced Decoding (END), a decod-\ning method that mitigates hallucinations with-\nout requiring extra training. END leverages\ninner probability changes across layers to in-\ndividually quantify the factual knowledge re-\nquired for each candidate token, and adjusts the\nfinal predicting distribution to prioritize tokens\nwith higher factuality. Experiments on both\nhallucination and QA benchmarks demonstrate\nthat END significantly enhances the truthful-\nness and informativeness of generated content\nwhile maintaining robust QA accuracy. More-\nover, our work provides a deeper perspective\non understanding the correlations between in-\nherent knowledge and output factuality.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated\nremarkable capabilities across numerous natural\nlanguage processing (NLP) applications (Zhao\net al., 2023; OpenAI, 2024). Despite their impres-\nsive performance, the issue of generating fabricated\ncontent, commonly referred to as \"hallucinations\"\n(Ji et al., 2023; Zhang et al., 2023b), remains a per-\nsistent challenge for LLMs. This problem hinders\nthe broader application of LLMs in industries that\nhighly demand trustworthiness and accuracy.\nRecently, various methods have been proposed\nto mitigate hallucinations, including training with\nhigh-quality data, aligning with de-hallucination\npair, and integrating external knowledge sources.\nHowever, these methods often involve high compu-\ntational costs or demands knowledge bases, which\nmay not be accessible in many application scenar-\nios. Also, some studies (Wei et al., 2022; Saunders\net al., 2022) have found that even when LLMs pos-\nsess the corresponding knowledge, they can still be\nsusceptible to generating hallucinations. To tackle\nthis issue, recent research has focused on the inter-\nnal representations of models, exploring the correla-\ntion between hidden states and output truthfulness.\nEspecially, Chuang et al. (2023) discovered that the\nprediction distribution of LLMs remains actively\nfluctuating in higher layers when generating factual\ntokens, while it remains almost unchanged when\nproducing other easy tokens. Similarly, Chen et al.\n(2024) reveals that the correct generation typically\nexhibits sharper context activation within the inner\nlayers across in-context tokens. Furthermore, we\nextend the investigation from step-level to token-\nlevel, analyzing the hidden-state change across lay-\ners for each token within a generation step. Our\nfindings reveal that tokens, associated with factual\nknowledge and answer correctness, exhibit a sharp\ngrowing trend of predicting probability with no-\ntable changes in the higher layers. This aligns with\nprevious findings and provides a more granular ex-\nplanation at the token level.\nTo this end, we propose cross-layer entropy en-\nhanced decoding (END), a novel decoding method\nthat leverages the change of cross-layer predictions\nto amplify the emerging of factual knowledge. As\nshown in Fig. 1, instead of selecting a certain caliber\nlayer, our method processes the overall growing\ntrend across model layers, offering a more reliable\nquantification of factual knowledge for each can-\ndidate token individually. Without extra training\nrequired, END could be directly applied to LLMs\nand effectively improve generation factuality.\nWe evaluate our method on both hallucination\nbenchmarks (TruthfulQA and FACTOR) and gen-\neral QA benchmarks (TriviaQA and Natural Ques-\ntions). Experimental results demonstrate that our\nproposed END significantly enhances the factuality\nof model generation while maintaining robust basic\nQA performance. Also, we further extend the ex-\nperiments to various LLM backbones of different\nscales and types, verifying its generalizability of ap-\nplication. Overall, our work not only introduces an\neffective decoding method to enhance generation\nfactuality, but also provides a new perspective on\nexploring correlation between inner hidden states\nand output truthfulness at a token level."}, {"title": "2 Related Work", "content": "Recently, various methods have been proposed to\nimprove LLM's generating factuality to mitigate\nhallucinations. These include, but are not limited to,\nsupervised fine-tuning with high-quality data (Tian\net al., 2023; Zhou et al., 2024), reinforcement learn-\ning with truthful preference pairs (Sun et al., 2023;\nYang et al., 2023), retrieval-augmented generation\nthat integrates external knowledge (Chern et al.,\n2023), and editing knowledge-related inner repre-\nsentations or parameter-efficient modules (Zhang\net al., 2024; Hu et al., 2024).\nOur research focuses on the field of constraint\ndecoding, which involves applying intervention\nstrategies during model's generation process. No-\ntably, Inference-Time Intervention (ITI) (Li et al.,\n2024) employs probes to locate truthfulness sig-\nnals within attention heads, while Repe (Zou et al.,\n2023) locates those within critical layers, then edit-\ning on the direction of truthfulness to modify model\ndecoding. Contrast Decoding (CD) (Li et al., 2022)\nand later Induced-then-Contrast Decoding (ICD)\n(Zhang et al., 2023a) contrasts logits from an expert\nmodel against those from a weak model, amplify-\ning the knowledge reflected in their differences. Ac-\ntivation Decoding (Chen et al., 2024) leverages the\ncorrelation between context activation sharpness\nand answer correctness, incorporating in-context\nentropy into decoding to improve factuality.\nThe most relevant work to ours is DOLA\n(Chuang et al., 2023), which selects a single, most\ndistinct layer to contrast with the final layer, am-\nplifying the factual knowledge boosted in higher\nlayers. However, the change of inner predictions\nvaries by candidate tokens, which means that, at a\ngiven generation step, factual tokens may exhibit\ndifferent growing trends. Therefore, selecting a\nsingle caliber layer for all tokens is not accurate\nand can lead to false negative and false positive\nproblems. Unlikely, we propose to process the pre-\ndiction changes across layers individually for each\ntoken. By quantifying their growing trend, we can\nleverage internal information more accurately to\nenhance the factuality of generation."}, {"title": "3 Empirical Findings", "content": "Previous works (Chuang et al., 2023; Halawi et al.,\n2023; Schuster et al., 2022) discovered that, when\ngenerating tokens that require factual knowledge,\nsuch as name entities, dates and locations, model\ntends to be still changing its predictions in the last\nfew layers since it is potentially injecting more\nfactual knowledge into inference. Contrarily, pre-\ndiction changes are minimal from the middle layers\nonward when generating 'easy' tokens, such as syn-\ntactic or functional tokens. This may be because\nmodel has already decided the token to generate at\nmiddle and keeps the prediction almost unchanged\nin afterwards higher layers. Later work (Chen et al.,\n2024) also digs into hidden states and finds that,\nsuccessful activation with sharp in-context logits\nindicates higher chance of answer correctness."}, {"title": "4 Methodology", "content": "Based on these findings, we propose END, a de-\ncoding enhancement method that can be directly\napplied to mitigate hallucinations without incurring\nadditional training costs. As illustrated in Fig.3, our\nmethod measures the internal prediction changes\nof candidate tokens, introduces cross-layer entropy\nto quantify the factual knowledge of inference, and\nadjusts model's next-token prediction by favoring\nfactuality token to improve the informativeness and\ntruthfulness of the generated content."}, {"title": "4.1 Cross-Layer Entropy", "content": "Large language models typically consist of N\nstacked transformer layers, followed by an affine\nlayer that maps the internal representations to a\nnext-token probability distribution. We denote the\nhidden state of the l-th layer as $h^{(l)}$, the classifi-\ncation head as $\\theta(\\cdot)$, and $v_t$ as generation token at\nstep t over the vocabulary set V. The prediction\nprobability from the l-th layer can be expressed as:\n$P_l(v_t | V_{1:t-1}) = \\text{softmax}(\\theta(h^{(l)})), v_t \\in V$ (1)\nHere, the prediction probability $p_l(v_t)$ is a k-\ndimension vector that includes the l-th layer's pre-\ndiction values for all k candidate tokens in the\nmodel vocabulary.\nFor each candidate token, we extract its predic-\ntion values of higher layers and use them to consti-\ntute a cross-layer probability distribution D, which\ncharacterizes the token's prediction change across\nlayers, reflecting its growing trend. The probability\nused to build cross-layer distribution is calculated\nas Equation 2, where Layer represents the set of\nlayers ranging from middle to high.\n$q_l(v_t) = \\frac{P_l(v_t)}{\\Sigma_{i \\in Layer} P_i(v_t)}$ (2)\nWe normalize the prediction values to bring them\ninto a consistent range so that the trend of predic-\ntion change could be directly compared across can-\ndidate tokens regardless of differences in absolute\nprobability values.\nAs the finding suggests, functional tokens with\nfactual knowledge injected during inference often\nexhibit sudden growing with unstable changes. As\na result, their cross-layer distributions are likely\nto present a sharp or highly volatile trend. In con-\ntrast, for other easy or unrelated tokens, the predic-\ntion probabilities grow slightly or remain relatively\nunchanged in higher layers, leading to flatter and\nmore stable distributions.\nTherefore, to quantify the factual knowledge re-\nquired of each candidate token, we introduce cross-\nlayer entropy to measure the growing trend within\nthe cross-layer probability distribution:\n$Entropy(v_t) = \\sum_{l \\in Layer} q_l(v_t) \\log q_l(v_t)$ (3)\nA low cross-layer entropy value represents a sharp\npredicting distribution, indicating that the candi-\ndate token is more closely associated with factual\nknowledge and answer correctness."}, {"title": "4.2 Factuality Enhanced Decoding", "content": "To improve generation quality and mitigate halluci-\nnations, tokens associated with factual knowledge\nshould be amplified during decoding while unre-\nlated ones should be suppressed. We implement\nthis by using cross-layer entropy to adjust the next-\ntoken prediction from the final layer:\n$P_{Final}(v_t) = e^{-\\lambda \\text{Entropy}(v_t)} P_N(v_t)$ (4)\nwhere $\\lambda$ is a hyperparameter controlling the influ-\nence of cross-layer entropy and N is the index of\nfinal layer.\nAdditionally, following the approach of Li et al.\n(2022), we also introduce a filtered subset $V_{head}$\nto improve inference efficiency under open-ended\ngeneration settings, a is a threshold parameter\n$V_{head}(v_t) = \\{ v_t \\in V : P_N(v_t) \\geq \\alpha \\max_w P_N(w) \\}$ (5)\nwhere $\\alpha$ is a threshold hyperparameter. Calcu-\nlating cross-layer entropy and adjusting probabil-\nity distribution can lead to substantial computa-\ntional cost, especially in open-generation settings.\nFor instance, under LLaMa-series model settings,\neach generation step involves a vocabulary size\nof 32,000 tokens. By filtering out low-probability\ncandidates, we only process a small number of\ncandidates and retain the original logits for others.\nThis approach effectively improves the efficiency\nof open-ended generation."}, {"title": "5 Experiment", "content": "5.1 Setup\nDatasets We consider three types of datasets with\nvarious tasks to evaluate our method:\nTruthfulQA (Lin et al., 2021) is the most\nwidely used benchmark for assessing the truthful-\nness of LLMs. It includes two tasks: multiple\nchoice and open-ended generation. For multiple\nchoice, the model selects an answer from given op-\ntions and is evaluated by multiple-choice accuracy\n(MC1/MC2/MC3). For open-ended generation, the\nmodel generates output responses directly, and two\nfined GPT-3 models\u00b2 are introduced to assess\ntruthfulness and informativeness.\nFACTOR (Muhlgay et al., 2023) is a reading\ncomprehension benchmark designed to evaluate a\nmodel's factuality in long-paragraph contexts. It\nconsists of three subsets Expert, News and Wiki,\nall presented in a multiple-choice format, with per-\nformance measured by accuracy.\nNatural Questions (Kwiatkowski et al., 2019)\nand TriviaQA (Joshi et al., 2017) are well-\nestablished Question Answering benchmarks, eval-\nuated with F1 and Exact Match scores. We include\nthem to assess general QA capabilities.\nBaselines We mainly compare our method with\nlight-weight decoding methods that could be di-\nrectly applied to inference without extra training:\n1) Greedy deocding, model's original decoding\nmethod that selects the next token with the highest\nprobability; 2) DoLa (Chuang et al., 2023), that\nenhances factuality by contrasting logits from inner\nlayers with the final layer; 3) Activation decod-\ning (Chen et al., 2024), that quantify in-context\nsharpness to adjust decoding correctness.\nImplementation Details We use Llama-2-7B-\nchat as the backbone model for experiments. Simi-\nlar to Chuang et al. (2023), we also divide all layers\ninto buckets and use the same strategy to select\none as Layer set to construct cross-layer distribu-\ntion. The filter threshold a is set to [0.001,0.1].\nThe entropy adjustment coefficient $\\lambda$ is set to [1, 3]\nfor open-ended generation task, and [0.25, 0.5] for\nmultiple choice and QA task. The exact hyperpa-\nrameter values are determined through validation\nruns on the respective benchmark."}, {"title": "5.2 Main Results", "content": "Results on TruthfulQA The experiment results\non TruthfulQA are presented in Table 1. In the\nmultiple-choice task, our method achieves the high-\nest MC1 score and the equal highest MC2 and\nMC3 scores with the former SOTA method, out-\nperforming the greedy decoding by 0.9/0.8/0.8\npoints respectively. More noticeably, our method\nmakes significant improvements in the open-ended\ngeneration task, with increases in the overall\n(%Truth*Info) scores by 12.24%-21.79%, and re-\nductions in the rejection rate by 4.16%-14.81%\ncompared to all baseline methods. As former\nworks (Zhang et al., 2024) analyzed, some meth-\nods achieve high scores by answering only when\nconfident and output 'I have no comment.' to uncer-\ntain questions, resulting in low informative score\nand high rejection rates. In contrast, our method\neffectively avoids this tendency with an even higher\ninformative score. This may be because the adjust-\nment of entropy indeed compresses high probabil-\nity non-fact candidates and leaves more opportu-\nnities for fact tokens. Even at non-decisive gen-\neration steps or in equal-truthful output scenarios,\ngenerating these tokens contributes knowledge-rich\ncontent rather than simple judgments, making the\nmodel's responses more informative.\nMoreover, except for the generation of factual\ntokens at decisive steps, which directly affect the\ntruthfulness of the response, those generated at\nnon-decisive steps also make contributions. The\ninclusion of factual knowledge tokens may help\nconstitute a logical context, forming as a Chain-of-\nThought (Wei et al., 2022). Unlike simply judgmen-\ntal responses, such outputs improve correctness by\nproviding reasoning-style statements.\nResults on FACTOR The experimental results\non FACTOR are presented in Table 3. Our method\nachieves the best performance on the Expert and\nNews subsets, and the second-best result on Wiki.\nThis demonstrates the effectiveness of our method\nin handling factual multiple-choice tasks within\nlong-paragraph reading comprehension scenarios.\nWe also observe that all listed methods show\nlimited improvements and some even fail to en-\nhance performance on this benchmark. This may\nbe because FACTOR is strongly relevant to real-\nword domains and requires corresponding exter-\nnal knowledge while those decoding methods can\nonly amplify model's inherent knowledge. Also,\nas noted by Chuang et al. (2023), the processing\nof long sentences in FACTOR often focuses more\non non-fact tokens that do not require knowledge\nduring inference. This may explain the negative\nimpact and our inferiority on the Wiki set."}, {"title": "5.3 Effectiveness on More Model Scales", "content": "Except for Llama-2-7B, we extend the experiments\nto 13B and 70B models to evaluate performance\nacross different parameter scales. The implementa-\ntion details remain the same as the main experiment.\nAs shown in Table 2, our method demonstrates con-\nsistent improvements across all model scales. No-\ntably, on the open-ended generation task, the 70B\nmodel shows the lowest Truth*Info score and the\nhighest rejection rate among the three scales. This\ncan be attributed to the model's robust intrinsic\npredictions, which make it challenging for cross-\nlayer entropy adjustments to significantly alter the\nprobability distribution."}, {"title": "5.4 Extension on More Model Backbones", "content": "To further verify the generalizability of our method,\nwe apply it to other widely-adopted LLMs, includ-\ning Qwen (Bai et al., 2023) and Misrtral (Jiang\net al., 2023). As indicated in Table 4, our method\ngeneralizes well and shows effective improvements\nacross both models. It is worth mentioning that,\nwhen our method is applied on Mistral-Instruct-\nv0.1, which is also a 7B model, it results in dis-\nruptive behavior in open-ended generation. This\nreveals our method's requirement of models with a\nrobust baseline capacity. The adjustment of cross-\nlayer entropy works only when the model possesses\na reliable original prediction distribution."}, {"title": "6 Analysis", "content": "6.1 Impact of Adjustment\nThe cross-layer entropy is introduced to help en-\nhance model decoding by quantifying required\nknowledge rather than directly taking the place\nof model's original decoding. The impact of adjust-\nment on model's next-token prediction should not\nlead to a radically distinct probability distribution.\nThrough exploring into experiments, we find\nthat, in most cases, our method processes as shown\nin Figure 4, where the candidate tokens with high\nprediction probabilities usually possess low cross-\nlayer entropies as well. Such phenomenon aligns\nwith the common understanding that models gen-\nerally have a reliable base capacity and make right\npredictions in usual scenarios. This also explains\nwhy our method does not damage LLM's original\ngeneration capacity while previous decoding meth-\nods like CD (Li et al., 2022) and DoLa (Chuang\net al., 2023) suffer from false negative and false\npositive problem."}, {"title": "6.2 Qualitative Study", "content": "To showcase the practical improvements of our\nmethod over the baselines, we present several rep-\nresentative cases from TruthfulQA in Table 6.\n\u2022 Q1: Our method produces the correct real-\nworld answer while others produce halluci-\nnated responses involving fictional content,\nhighlighting the direct effectiveness of our\nmethod in mitigating hallucinations.\n\u2022 Q2: While other methods output 'I have\nno comment' to obtain truth/info scores of\n1.0/0.0, ours provides a reasonable and truth-\nful answer that is also informative. We ob-\nserve that our approach tends to generate di-\nverse yet accurate responses, avoiding the ten-\ndency to simply reject uncertain queries.\n\u2022 Q3 and Q4: Even when all methods gener-\nate the correct answers without hallucinations,\nour method enriches the response with more\ndetailed factual knowledge, including date,\nname and address. This makes the output\nmore informative and enhances the truthful-\nness by providing additional factual details."}, {"title": "6.3 Decoding Efficiency", "content": "To further clarify the time cost of our decod-\ning methods, we conduct experiments to evaluate\nthroughput on TruthfulQA open-ended generation\ntask with a fixed token number and the results\nare shown in Table 7. While the calculation of\ncross-layer entropy does introduce a noticeable\nslowdown in decoding, its efficiency still shows\nan improvement over that of DOLA, which is con-\nsidered a negligible cost in the application."}, {"title": "7 Conclusion", "content": "In this work, we extend the analysis of the corre-\nlation between hidden-state changes and factual\nknowledge to a deeper candidate-token level, pro-\nviding a new perspective of research. We propose\na novel decoding method END which introduces\ncross-layer entropy to individually quantify the pre-\ndiction changes for candidate tokens, and use this\nto adjust the final next-token prediction so as to\nimprove generation factuality. Experiment results\nshow that our method could comprehensively im-\nprove the output quality and mitigate hallucinations\nwithout incurring additional training costs."}, {"title": "Limitation", "content": "Hallucination Type Decoding methods cannot\ninject additional knowledge into LLMs, they can\nonly amplify the model's inherent knowledge to im-\nprove next-token predictions and reduce erroneous\noutputs. Our method aims at helping models accu-\nrately express what they know while models still\ndon't know what they don't know. Furthermore,\nif the inherent knowledge is incorrect or outdated,\namplifying it will not improve any generation qual-\nity. Therefore, hallucinations caused by a lack of\ninformation or outdated data fall outside the scope\nof this approach.\nTheoretical Foundation Our approach is based\non observed patterns of hidden-state changes, lever-\naging these empirical findings to enhance decoding.\nHowever, the overall underlying mechanism behind\nstill remains unexplored. We have yet to establish\na clear definition of what constitutes a \"factual to-\nken\" or how entropy adjustments should be applied\nacross different scenarios. More comprehensive\nresearch is needed to deepen the theoretical under-\nstanding in this area."}]}