{"title": "Causal-Guided Active Learning for Debiasing Large Language Models", "authors": ["Zhouhao Sun", "Li Du", "Xiao Ding", "Yixuan Ma", "Yang Zhao", "Kaitao Qiu", "Ting Liu", "Bing Qin"], "abstract": "Although achieving promising performance, recent analyses show that current generative large language models (LLMs) may still capture dataset biases and utilize them for generation, leading to poor generalizability and harmfulness of LLMs. However, due to the diversity of dataset biases and the over-optimization problem, previous prior-knowledge-based debiasing methods and fine-tuning-based debiasing methods may not be suitable for current LLMs. To address this issue, we explore combining active learning with the causal mechanisms and propose a casual-guided active learning (CAL) framework, which utilizes LLMs itself to automatically and autonomously identify informative biased samples and induce the bias patterns. Then a cost-effective and efficient in-context learning based method is employed to prevent LLMs from utilizing dataset biases during generation. Experimental results show that CAL can effectively recognize typical biased instances and induce various bias patterns for debiasing LLMs.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are growing to be the foundation of Natural Language Processing. Through the generative pretraining process upon a large-scale corpus, the LLMs have demonstrated impressive performance in understanding the language and conducting complex reasoning tasks (Achiam et al., 2023), demonstrating immense potential in real-world applications.\nHowever, the generative pretraining process is a double-edged sword, as it would also inevitably incur dataset bias into the LLMs such as position bias and stereotype bias (Schick et al., 2021; Navigli et al., 2023; Zheng et al., 2023; Shaikh et al., 2023). This is because, the LLMs only passively learn to model the correlation between contexts in the pretraining corpus, and the pretraining corpus is biased as it reflects the inherent preference or prejudice of human beings. For example, the existence of position bias is due to the subconscious human belief that the first option is better, leading to a higher frequency of the first option in corpora, and LLMs trained to model the corpus distribution would also capture such biased correlation. Such biases would lead to poor generalizability and harmfulness of LLMs (Navigli et al., 2023; Huang et al., 2023). For instance, when an LLM is asked to evaluate which option is better, the LLM may utilize position bias and tend to choose the first option. However, which option is better is completely unrelated to its position. Therefore, when the second option is generally better in some datasets, the performance of the LLM will significantly decline. While biases such as stereotyping bias would make LLMs generate harmful content such as women are less capable in STEM fields, which in turn reinforces harmful stereotypes.\nThese problems highlight the necessity of debiasing LLMs. The key issue to debias LLMs lies in how to recognize the dataset biases and prevent it from utilizing biases during inference. To this end, prevalent methods rely on researchers' prior knowledge to artificially recognize the potential dataset biases, and then eliminate such biases through aligning or prompt-based regularization (Schick et al., 2021; Oba et al., 2023; Liu et al., 2023b). However, due to the diversity and complexity of dataset biases (Poliak et al., 2018; Schuster et al., 2019; Schick et al., 2021), it's impractical to identify them one by one manually. A vast amount of biases remains unrecognized in different tasks (Nie et al., 2020) and new biases are continually being discovered.\nHence, there is an urgent need for methods to automatically identify biases of generative LLMs. However, previous automatic debiasing methods"}, {"title": "2 Preliminary", "content": "2.1 Dataset Bias within Textual Corpus under Causal Perspective\nText records and reflects the thoughts of human beings. Inherent biases such as gender and racial biases persist in the human mind, and thus are also reflected in various corpora (Schick et al., 2021; Navigli et al., 2023). Due to potential annotation artifacts, various biases such as position and verbosity biases still broadly exist in task-specific datasets.\nFormally, as shown in Figure 1 (a), given a piece of text X, the subsequent text Y within a corpus D would be affected by two factors: (1) The semantic relationship between X and Y, (2) The existence of dataset bias within D. For example, given X = The physician hired the secretary because, due to the existence of gender bias, the following text Y in the corpus would more likely be he was overwhelmed with clients, rather than she. Such biased relationship characterizes the unwanted correlation between the context brought by dataset bias. In the following sections, for clarity, we denote the semantic relationship as fs(), and denote the biased relationship as gB(\u00b7). Hence, given X, the conditional distribution of Y given X in corpus D can be formalized as\nP(Y|X) = P(f_s(X), g_B(X)|X).\nThe key difference between the semantic relationship and the biased relationship is that the semantic relationship possesses the causal invariance, while the biased relationship does not. Specifically, for all instances upon all datasets, given preceding text X, the subsequent text Y would be determined by the semantic relationship (Pearl et al., 2000; Pearl, 2009), while the biased relationship only describes certain superficial statistical correlation between X and Y. Consider the example where an LLM acts as a judge to assess the responses of two AI assistants, as illustrated in Figure 1 (a): The answer (Y) is determined by the semantic relationship between the prompt X and answer Y. While in the corpus, certain biases such as the position of the responses that show a correlation with the answer can be predictive (Wang et al., 2023). However, Y is not determined by the bias and such a correlation may fail to be predictive in other instances. Hence, as Y is determined by X, their semantic relationship is a \u201ccausal\u201d relationship and invariant upon all instances. While the biased relationship is only correlative."}, {"title": "2.2 Biases of Generative LLMs", "content": "During the pretraining and task-specific supervised fine-tuning process, the training objective of generative LLMs is consistent, i.e., learn to generate the subsequent text Y given input text X. Given X in corpus D, the distribution of Y can be formalized as\nP(Y|X) = P(f_s(X), g_B(X)|X),\nthe generative LLMs would inevitably be trained to model both fs(X) and gB(X). Therefore, given preceding text X\u1d62, LLMs would not only attend to the semantics of X\u1d62 but also would attend to the biased patterns such as negation word, gender indicator, position of choices, etc, to generate Y. As a result, during inference, the model generation Y would inevitably be affected by the dataset biases. For brevity, we denote the semantic information within X\u1d62 as S\u1d62 and denote the biased patterns as B\u1d62."}, {"title": "2.3 Active learning", "content": "Active learning aims at selecting the most informative instances, and then querying external information source(s) to label these data points (Cohn et al., 1994; Zhang et al., 2022). The key of active learning lies in how to devise query strategies to select the most informative instances (Zhan et al., 2022). For example, uncertainty-based active learning methods aim at finding the most uncertain instances, and then send them to annotators for labeling (Liu et al., 2022). In this paper, under the automatic debiasing scenario, two key issues are: (1) finding which instance contains bias; (2) finding the most informative biased instances. Hence, we propose a causal-guided active learning framework, which identifies the biased instances under"}, {"title": "3 Methodology", "content": "As Figure 1 (b) shows, CAL contains two main components: (i) causal invariance-based biased instance identification; (ii) typical biased instances selection and bias pattern induction. Given the recognized bias patterns, we propose an in context learning-based debiasing method for regularizing LLMs."}, {"title": "3.1 Causal Invariance Based Biased Instances Identification", "content": "We first identify a set of biased instances that reflect the inherent biases within LLMs using the difference between semantic information and biased information in the perspective of causal variance.\nCompared to semantic information, the essential characteristic of biased information is that B does not have an invariant causal relationship with the subsequent text, which enables the disentanglement of biased information with semantic information. Moreover, note that, the generative LLMs would capture biased information to obtain the representations (e.g. the hidden states) of input texts. Hence, if we can find the instances where the model obtains representations that are not invariant predictive, then the representations of these instances would contain biased information, which indicates that these instances are very likely to contain bias and could be identified as biased instances."}, {"title": "3.2 Selection of Informative Biased Instances and Bias Pattern Induction", "content": "Using the criterion mentioned above, we could identify a set of instances that contain bias (i.e., counter instance pairs) as they violate the causal invariance criterion. Next, we hope to select a subset that is more informative and contains typical dataset bias. So that we can further induce explainable patterns of biases to prevent the LLMs from utilizing bias. To this end, we consider that:\nTypical Biased Instances Identification Firstly, for any input text X\u1d62, if the probability that Y is properly generated is rather low, it suggests that biased information significantly hinders the LLM. Hence, such examples would contain a high level of bias and could be informative biased instances. Secondly, for a counter instance pair ((X\u1d62, Y\u1d62), (X\u2c7c, Y\u2c7c)), if the corresponding generation of LLM \u0176\u1d62 and \u0176\u2c7c is rather different, it means the influences of dataset bias are diversified and hence it would be challenging to summarize a unified bias pattern based on these samples. Conversely, if \u0176\u1d62 and \u0176\u2c7c are similar, it would be easier to conclude the influence caused by the bias, as the influence of dataset bias is typical. Based on the two characteristics, we introduce the following two criteria to select the informative biased instances:\nInfluential Criterion:  p_{j,l_j} < \u03c4_p, s.t. Sim(\u0176_i,\u0176_j) < \u03b1,\nTypical Criterion: Sim(\u0176_i,\u0176_j) > \u03b2,\nwhere lj is the gold subsequent text, P\u1d62,\u2097\u2c7c is the predicted probability of gold subsequent text, and \u03c4p \u2208 [0, 1] is a threshold for controlling the probability that M generates gold subsequent text.\nBias Pattern Induction Based on the identified informative biased instances, we further induce certain explainable patterns that characterize several"}, {"title": "3.3 In Context Learning-based Bias Suppression", "content": "To prevent the LLMs from utilizing dataset biases for making generation, meanwhile avoiding the drawbacks of fine-tuning-based methods, we propose a cost-effective and efficient in-context learning (ICL) based method. Concretely:\nIn the zero-shot scenario, as shown in Figure 1 (b), we use the automatically induced bias patterns to explicitly tell the LLM what kind of information it should not use during inference by appending the text \u201c[bias xxx] is not related to [the goal of the task]\u201d to the end of the original prompt."}, {"title": "4 Experiments", "content": "4.1 Experimental Details\nIn this work, we use llama2-13B-chat (Touvron et al., 2023) and vicuna-13B-v1.5 (Chiang et al., 2023) for our experiments. Without loss of generality, we examine our approach on datasets that have a clear set of possible answers, e.g., multiple-choice question-answering task. So that we can implement the Sim(\u00b7) function in Equation 1 using an exact match of strings. If matched, the function's value is 1, otherwise it's 0. So \u03b1 and \u03b2 can be any value between 0 and 1. Additionally, we derive the representation of input text by employing the embedding vector of the last token at the top of the LLM's layer, and the cosine function is employed as the scoring function S(\u00b7) to measure the similarity between these hidden states.\nTo derive bias representation vector of a counter example pair, we need to extract similar parts in the hidden states corresponding to two examples of the counter example pair. This is because, the similar parts in the hidden states carry the biased"}, {"title": "4.2 Evaluation Tasks", "content": "We examine the effectiveness of CAL by investigating whether CAL could debias LLMs to improve the generalizability and unharmfulness of LLMs.\nTo evaluate the improvement of generalizability, we conduct experiments by deriving biased instances and bias patterns on dataset A and utilizing the identified instances and biased patterns to debias both dataset A and dataset B. Heuristically, two datasets A and B may share different dataset bias distributions. If an LLM only adapts to dataset A, then its performance upon dataset B would be impacted. On the contrary, if an LLM can focus more on semantics, the performance on both datasets would be improved. Hence, the generalizability could be evaluated by the performance improvement compared to baseline methods. Specifically, We evaluate our approach on benchmarks representing two categories of bias: (1) Generative-LLM-specific biases. We employ the Chatbot and the MT-Bench datasets (Zheng et al., 2023) as benchmarks. On both datasets, LLM is required to choose a better response from two candidates. We induce the bias patterns on the Chatbot dataset, then test whether the Chatbot-based bias patterns can be utilized to debias LLMs on both the Chatbot and the MT-Bench dataset. (2) Task-specific biases. We choose the natural language inference dataset MNLI (Williams et al., 2018) and the corresponding manually debiased dataset HANS (McCoy et al., 2019) as benchmarks. Hence, models that only utilize the biased information often perform close to a random baseline on HANS. The bias patterns are induced from the MNLI dataset, then test whether CAL can utilize the induced bias patterns to debias LLMs on both the MNLI and the HANS datasets.\nTo evaluate the improvement of unharmfulness, we conduct experiments on the BBQ (Parrish et al., 2022) and the UNQOVER (Li et al., 2020) dataset, which is designed for evaluating stereotype biases (such as gender bias and racial bias) of LLMs. These two datasets containing 9 and 4 types of stereotype bias, respectively. On these two datasets, if the model achieves a higher accuracy, then it could be regarded as having a lower likelihood of containing stereotypes.\nOn Chatbot and MT-Bench dataset, model performance is evaluated based on the agreement ratio between human-majority annotations and LLMs. On other datasets, model performance is evaluated using accuracy."}, {"title": "4.3 Baseline Methods", "content": "We compare the casual-guided active learning method with two categories of baseline methods:"}, {"title": "4.4 Main Results", "content": "We list the experimental results of two LLMs on six datasets in Table 1. From which we find that:\n(1) Compared to the vanilla zero-shot shows that, in general, the prior knowledge-based zero-shot debiasing methods show improved performance on all the datasets. This indicates that through ICL, LLMs can both effectively debias themselves and avoid the in-distribution performance degradation which is always associated with fine-tuning-based approaches (Du et al., 2023), suggesting the superiority of ICL-based debiasing methods.\n(2) Compared to the zero-shot baselines and few-shot baselines, in general, few-shot CAL achieves consistent performance improvement on the two"}, {"title": "4.5 Case Study", "content": "We argue that one of our potential major contributions is that by utilizing the causal invariance together with the influential and typical criterion, we can identify a set of typical biased instances, and then autonomously summarize explainable bias patterns from data. In Figure 2, we present the results of clustering analysis based on the bias representations derived from bias instances, and bias patterns summarized from the clustered categories. Experiments are conducted using llama2-13B-chat.\nOverall, it can be observed that bias representations are concentrated in several distinct groups after dimensionality reduction through PCA. Moreover, the bias patterns summarized based on different clustering categories are also distinguished. This indicates that our method could discover different types of biased instances and then induce bias patterns.\nBased on the counter example pairs derived from the Chatbot dataset, CAL can simultaneously induce position bias, verbosity bias, and format bias, which is separately identified by several previous research (Zheng et al., 2023; Zhu et al., 2023), suggesting the efficiency and effectiveness of our approach. Furthermore, we also observe several potential bias patterns such as \u201clength or complexity of a response\" and \"the presence of specific details or a confident tone\u201d, that are previously unreported. When we tell llama2-13B-chat not to make predictions based on these biases, its performance increases on both Chatbot and MT-Bench datasets, suggesting that these patterns could be the truly existing biases. Among the 9 known types of stereotype biases in the BBQ dataset (Parrish et al., 2022), our method can automatically identify 7 of them without prior knowledge (the bias of gender, sex-"}, {"title": "4.6 Generalizablity of the Induced Bias Patterns", "content": "The pretraining corpus of different LLMs share unnegligible overlaps, so they would also possess common biases. Hence, we investigate the generalizability of the automatically induced bias patterns by testing if it is possible to debias LLM-A based on the bias pattern identified from another LLM-B. Specifically, we attempt to debias GPT-4 based on the bias pattern (and the corresponding debiasing prompt) identified from llama2-13b-chat. Experimental results are shown in Table 2, from which we can observe that compared to vanilla zero-shot, ZS-CAL achieves higher performance in most cases. This demonstrated that different LLMS might share similar bias patterns and we can debias an LLM based on the bias pattern identified from other LLMs, which further demonstrates the universality of our method."}, {"title": "4.7 Influence of the Choice of Bias Pattern Induction Model", "content": "In the above sections, we induce the explainable bias patterns using GPT-4. We also attempt to use the open-source LLM Qwen1.5-72B-Chat for in-"}, {"title": "4.8 Influence of the Dataset size", "content": "To investigate the influence of the dataset size used in our framework, We conducted experiments using a 20% subset of the MNLI dataset utilized in our main experiments, employing the llama2-13b-chat model. As Table 4 shows, the performance of CAL keeps relatively stable with 20% data. Moreover, our approach still far outperforms the baseline method on the HANS dataset, which demonstrates the effectiveness of our approach to debias LLMs. This indicates that our method is still effective in situations where data is relatively scarce."}, {"title": "5 Related Work", "content": "Previous analyses demonstrate that LLMs still suffer from biases such as position bias (Zheng et al., 2023) and stereotyping bias (Shaikh et al., 2023). To mitigate the LLMs' biases, one line of methods relies on researchers' prior knowledge to artificially recognize the potential dataset biases, followed by debiasing through prompt-based regularization or aligning with human through instruct tuning (Oba et al., 2023; Liu et al., 2023b; Wang et al., 2023; Ganguli et al., 2023). However, these methods are limited by the dependence on researchers' prior. Moreover, due to the diversity of dataset biases (Poliak et al., 2018; Schuster et al., 2019; Schick et al., 2021), it is unrealistic to identify them one by one manually. To tackle these issues, automatic debiasing methods are proposed. They automatically extract bias features characterizing the dataset biases by training certain biased models (Utama et al., 2020; Du et al., 2023; Sanh et al., 2020; Lyu et al., 2023) for regularizing the main model. How-"}, {"title": "6 Conclusion", "content": "In this paper, we propose a causal-guided active learning framework. Depending on the difference between the dataset biases and semantics in causal invariance, we can automatically identify counter example pairs that contain bias. Then we utilize an influential and a typical criterion to select counter example pairs that are more informative for inducing bias patterns. Finally, a cost-saving yet effective ICL-based debiasing method is proposed to prevent the LLM from utilizing biases for generation. Experimental results show that our approach can effectively recognize various bias patterns automatically, and debias LLMs to enhance their generalizability and unharmfulness."}, {"title": "8 Limitations", "content": "Although our method can automatically debias LLMs, the identification of typical bias instances relies on the hidden state and the predicted probability of the gold subsequent text, which are inaccessible in proprietary models such as GPT-4. This limitation makes it challenging for us to comprehensively uncover the bias patterns present in closed-source models."}, {"title": "3.2 Selection of Informative Biased Instances and Bias Pattern Induction", "content": "Specifically, as described in Sec. 2.1, since the input preceding text X consists of both the semantics S and dataset biases B, hence, for an arbitrary instance (Xi, Yi) within a large enough dataset, there could exist other instance(s) (Xj, Yj), which has the following relationship with (Xi, Yi):\n(Bi, Si) \u2282 Xi, (Bj, Sj) \u2282 Xj, Bi = Bj, Si \u2260 Sj.\nIn other words, this pair of instances shares almost the same kind of dataset biases, while the semantic information entailed in the input text is different. The existence of such instance pairs enables the identification of biased instances using causal invariance.\nUnder such assumption, considering an instance pair ((Xi, Yi), (Xj, Yj)), if M has mainly captured the semantic information Si and Sj, and H^M_i is close to H^M_j, then S_i is similar to S_j, so that Sim(Yi, Yj) \u2192 1. In other words, the LLM has captured invariant predictive information for making generations.\nInstances on which the model fails to capture invariant predictive information\nHence, on the contrary, if we can find an instance pair ((Xi, Yi), (Xj, Yj)), on which H^M_i is close to H^M_j, whereas Sim(Yi, Yj) is low, then ((Xi, Yi), (Xj, Yj)) can be regarded as instances on which M violates the causal invariance, and such instance pair can be utilized for characterizing the biases captured by LLMs. For clarity, we define such an instance pair ((Xi, Yi), (Xj, Yj)) as a counter example pair:\nDefinition 1 (Counter Example Pair): \u2200(Xi, Yi), (Xj, Yj) \u2208 D, i \u2260 j, if:\nS(H^M_i, H^M_j)>\u03c4, s.t. Sim(Yi, Yj) < \u03b1,\nwhere D is the dataset, S(\u00b7) is a score function measuring the similarity between H^M_i and H^M_j, \u03c4 is a threshold controlling the confidence that H^M_i and H^M_j can be regarded as close enough, and \u03b1 is another threshold ensuring that Yi and Yj can be regarded as sufficiently different.\nDefinition 1 enables us to detect all counter example pairs within the dataset D. On these counter example pairs, the invariance is violated so that subsequent texts are generated based on biased information. Hence, H^M_i and H^M_j contains the bias information Bi = Bj. However, the aforementioned theory is built upon the assumption that LLMs have captured the predictive information (including bias and semantic information). In fact, when Xi is very difficult or ambiguous, it cannot be ruled out that the LLM does not capture any predictive information. To rule out such instances, we introduce an additional filtering process using a Predictive Criterion, which requires that M should at least make a proper generation for the instance i or j, since if on both i and j model generation are improper, it is rather probable that M has not captured any predictive information in Xi or Xj:\nSim(\u0176i, Yi) > \u03b2 \u2228 Sim(\u0176j, Yj) > \u03b2,\nwhere \u0176_i, and \u0176_j are the generated subsequent text, \u03b2 is a threshold ensuring that \u0176_i and Y_i can be regarded as similar enough so that Y_i can also be seen as a correct answer (the same for Yj)."}, {"title": "4 Information about equations.", "content": "We have found equations with this latex. Here are the equations:\n1. P(Y|X) = P(fs(X),\u0434\u0432(X)|\u0425).\n2. f(Hik, Hjk)= {Hik-Hjk(Hik+Hjk)/2 if Hik+Hjk <\u03bc, 0 otherwise\n3. S(HM, HM)>r, s.t. Sim(Yi, Yj) <a\n4. Sim(Yi, Yi) > \u1e9e v Sim(\u0176j, Y\u2081) > \u1e9e,\n5. Pj,1; <Tp, s.t. Sim(\u0176;,Y;)<a\n6. Sim(\u0178\u00bf,\u0176;) >\u1e9e,"}]}