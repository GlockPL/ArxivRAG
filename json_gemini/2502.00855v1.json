{"title": "PSYCHOMETRIC-BASED EVALUATION FOR THEOREM PROVING\nWITH LARGE LANGUAGE MODELS", "authors": ["Jianyu Zhang", "Yongwang Zhao", "Long Zhang", "Jilin Hu", "Xiaokun Luan", "Zhiwei Xu", "Feng Yang"], "abstract": "Large language models (LLMs) for formal theorem proving have become a prominent research\nfocus. At present, the proving ability of these LLMs is mainly evaluated through proof pass rates\non datasets such as miniF2F. However, this evaluation method overlooks the varying importance\nof theorems. As a result, it fails to highlight the real performance disparities between LLMs and\nleads to high evaluation costs. This study proposes a psychometric-based evaluation method for\ntheorem proving with LLMs, comprising two main components: Dataset Annotation and Adaptive\nEvaluation. First, we propose a metric calculation method to annotate the dataset with difficulty and\ndiscrimination metrics. Specifically, we annotate each theorem in the miniF2F dataset and grade\nthem into varying difficulty levels according to the performance of LLMs, resulting in an enhanced\ndataset: miniF2F-Graded. Experimental results show that the difficulty grading in miniF2F-Graded\nbetter reflects the theorem difficulty perceived by LLMs. Secondly, we design an adaptive evaluation\nmethod to dynamically select the most suitable theorems for testing based on the annotated metrics\nand the real-time performance of LLMs. We apply this method to evaluate 10 LLMs. The results\nshow that our method finely highlights the performance disparities between LLMs. It also reduces\nevaluation costs by using only 23% of the theorems in the dataset.", "sections": [{"title": "1 Introduction", "content": "The rapid development of large language models (LLMs) has led to the creation of specialized models across various\nfields [1, 2]. One area that has garnered significant attention is theorem proving in formal methods. The application\nof theorem proving is widespread, especially in mathematical research and in verifying the correctness and security\nof safety-critical systems. However, theorem proving remains a complex and labor-intensive task, often requiring\nsubstantial effort from experts to complete [3, 4]. The advent of LLMs has made automated theorem proving feasible.\nPre-trained LLMs can now assist developers in rapidly completing complex proof tasks, such as DeepSeek-Prover [5]\nand LEGO-Prover [6].\nIn automated theorem proving, evaluating the theorem-proving ability of LLMs is a crucial task, as accurate evaluation\nstandards are essential to guide the continuous improvement of LLM training methods [7, 3]. Currently, the mainstream\nevaluation method is testing the proportion of theorems successfully proven by the model within N attempts (Pass@N).\nWhile this evaluation method is intuitive, it faces several challenges:\n\u2022 The evaluation results fail to highlight the disparities in theorem-proving abilities between LLMs, as the pass\nrate overlooks the varying importance of theorems and assigns equal weight to both difficult and simple ones.\nFor instance, DeepSeek-prover-RL introduced several meaningful improvements over DeepSeek-Prover-SFT\nbut only achieved a slight increase in Pass@128 [5], suggesting that the evaluation results lack a fine-grained\ndistinction.\n\u2022 Evaluation incurs high computational costs, as it does not filter the test theorems and requires testing all the\ntheorems in the dataset. For example, testing the Pass@128 score of a 7B-metric model on 488 theorems in\nour experiment took 39.3 hours on a single A100 GPU. For larger models or datasets, the computational cost\nwould increase even further.\nTo address these issues, this study draws on concepts from psychometrics: adaptive evaluation based on the model's\nreal-time performance, rather than relying on a fixed test set. We propose a psychometric-based evaluation method\nfor theorem proving with LLMs. This method annotates each theorem with difficulty (the ability level required for\nan LLM to prove it) and discrimination (how well the theorem distinguishes LLMs of varying abilities). Based on\nthese annotations, the method dynamically selects the most suitable theorems to efficiently evaluate the LLM's proving\nability. The overview of this method is shown in 1, consisting of two parts: Dataset Annotation (3.1) and Adaptive\nEvaluation (3.2).\nIn summary, the core contributions of this study are as follows:\n\u2022 This study proposes a psychometric-based evaluation method for theorem proving with LLMs, which evaluates\nthe LLM's proving ability through dataset annotation and adaptive evaluation, enabling a more fine-grained\nand efficient assessment. To the best of our knowledge, this is the first study to combine psychometrics with\nthe evaluation of LLMs' theorem-proving ability. Specifically, this study designs a metric calculation method,\na theorem selection algorithm, and an adaptive test algorithm, all tailored to the characteristics of LLMs in the\ncontext of theorem proving.\n\u2022 Building on the widely used formal theorem dataset miniF2F, we propose an enhanced version with metric\nannotations and difficulty grading: miniF2F-Graded\u00b9. Experimental results indicate that, compared to the\nprevious manual grading method, miniF2F-Graded more accurately reflects the difficulty of the theorems as"}, {"title": "2 Related Work", "content": "This study proposes a psychometric-based evaluation method for theorem proving with LLMs. Our research builds on\nexisting literature in Theorem Proving with LLMs, LLM evaluation methods, and psychometrics.\nTheorem Proving with LLMs. In recent years, a growing number of studies have explored training LLMs for theorem\nproving [3]. These works have developed models capable of generating proofs in formal languages such as Lean [8],\nIsabelle [9], and Coq [10]. Based on proof generation strategies, existing methods can be broadly categorized into tree\nsearch methods and whole-proof generation methods. Tree search methods formulate the proof process as a search\nproblem, incrementally exploring potential proof paths within a search tree. Notable approaches include LeanDojo\n[11], Thor [12], LEGO [6], DSP [13], among others. In contrast, whole-proof generation methods treat the proof as\na complete sequence, generating the entire proof path in a single step. Representative works in this category include\nBaldur [14], DeepSeek [5], TheoremLlama [15], and MetaMath-Llemma [16]. Among them, DeepSeek-Prover-V1.5\nleverages an enhanced formal theorem proving dataset for supervised fine-tuning, achieving a 60.2% pass rate on the\nminiF2F test set using a single-pass full-proof generation approach [5].\nIn this study, we focus on evaluating whole-proof generation models, as their evaluation should adhere to consistent\ncriteria. In contrast, tree search methods, due to their distinct search strategies, face challenges in establishing uniform\nevaluation standards.\nLLM Evaluation Methods. Evaluating the capabilities of large language models remains a critical challenge. In\nthe field of Theorem Proving with LLMs, proof pass rate is widely regarded as the primary evaluation criterion [7].\nHowever, an increasing number of studies advocate for more comprehensive evaluation methods. For instance, [17, 18]\npropose Chain-of-Thought (CoT) evaluation strategies, which assess LLM performance at each critical reasoning\nstep rather than relying solely on the final answer. [19, 20, 21] highlight the importance of analyzing errors in the\nreasoning process to gain deeper insights into the common mistakes made by LLMs. [7, 22] explore evaluation methods\nbased on perturbation analysis, examining how LLMs respond to variations in input. Lastly, [23] suggest categorizing\nand annotating evaluation data based on the number of reasoning steps and question types, enabling a more detailed\nassessment of LLM performance across different data types.\n[24] explores the use of human psychometrics in LLM evaluation and proposes that adaptive evaluation, which adjusts\nto a model's performance rather than relying on fixed test sets, will become the new standard in AI model assessment.\nThis concept serves as an important reference for our study. Specific applications of adaptive evaluation in LLM\nassessment can be found in works such as [25, 26], which achieve efficient evaluation with fewer examples. This study\naims to investigate how adaptive evaluation methods can be applied to evaluating the theorem-proving abilities of\nLLMs.\nPsychometrics. Psychometrics is a field dedicated to the effective measurement of psychological traits. In exam\nassessments, it provides the scientific foundation and methodological tools for designing, analyzing, and interpreting\nexam results, ensuring that assessments accurately and reliably measure the true abilities of students or test subjects\n[27, 28, 29]. Item Response Theory (IRT), introduced by [30], models both the characteristics of individual test items\n(e.g., difficulty, discrimination) and the ability scores of test subjects, offering a more precise measurement framework.\nIRT remains one of the most influential theoretical models in psychometrics [31]. Building on IRT's approach to\nmodeling item difficulty and discrimination, this study develops a metric calculation method and an adaptive test\nalgorithm specifically designed to address the limited number of test subjects (LLMs) in theorem proving, achieving\npromising results."}, {"title": "3 Method", "content": null}, {"title": "3.1 Dataset Annotation", "content": null}, {"title": "3.1.1 miniF2F dataset", "content": "This study uses the MiniF2F dataset [32] as the basis for annotation. The MiniF2F dataset is one of the most widely\nused datasets in the field, with many LLMs evaluating their performance based on the pass rate achieved on MiniF2F\n[6, 33]. A more detailed introduction of the MiniF2F dataset can be found in the A.1.\nTheorems in the MiniF2F dataset are formalized using a range of languages, including Metamath, Lean, Isabelle, and\nHOL Light. The difficulty of proving the same theorem can vary depending on the formal language used. For instance,\na typical proof in Lean is significantly shorter than its counterpart in Metamath, as Lean offers many powerful tactics\nthat aid in formalization [32]. As a result, many studies opt to use Lean data for training and evaluation. To ensure\nthe comparability of evaluation results, this study focuses on evaluating theorems in MiniF2F that are formalized in\nLean. The dataset in [34] updates the theorems from miniF2F to a newer version of Lean and includes natural language\ndescriptions. Therefore, we use this version of miniF2F for annotation and evaluation.\nIn the MiniF2F paper [32], it is noted that 260 theorems in the dataset are derived from the MATH dataset [35], with\nthe theorems in MATH being classified into five difficulty levels. However, this grading was done manually by humans.\nWe believe that the perceptions of difficulty between humans and LLMs may differ, as confirmed by the results in 3.\nTherefore, in this study, we re-annotate each theorem's difficulty and discrimination, categorizing them into difficulty\nlevels."}, {"title": "3.1.2 Metric Calculation", "content": "This study argues that there is a similarity between LLM test sets and human exam questions: not all questions hold\nequal importance [24]. For instance, an LLM should receive higher scores for solving high-difficulty theorems than\nfor solving low-difficulty ones. Furthermore, for an LLM that has already proven several relatively difficult theorems,\ntesting a high-difficulty theorem is more valuable than testing a simpler one.\nTo fully exploit the differences between the theorems, this study uses the miniF2F dataset as an example and calculates\nthe difficulty and discrimination of each theorem based on the performance of multiple LLMs (four Annotation LLMs\nwere selected, as described in 4.1).\nIn Item Response Theory from psychometrics [31, 36], metrics like difficulty and discrimination are typically estimated\nthrough statistical models. A detailed explanation of the statistical modeling methods can be found in the B.1. By\napplying the statistical modeling methods from IRT and the theorem-proving results from LLMs, we estimated the\ndifficulty and discrimination metrics for the theorems. However, the results were suboptimal, with the estimated metric\ndistributions being too concentrated. The suboptimal results stem from the limited number of test subjects, as we used\nonly four LLMs, far fewer than the number of examinees in human exams used in IRT models. However, it is currently\nimpossible to find the same number of open-source LLMs as human test-takers.\nTo address the issue of insufficient test subjects, this study developed a metric calculation method that does not rely\non metric estimation, instead using a more direct formula to compute the difficulty and discrimination metrics for the\ntheorems.\nNotations and Definitions To accurately calculate the difficulty and discrimination metrics, we need to know the\nprior ability values of the models. We use the pass rate across the entire dataset (Pass@128) to represent the prior\nability values of the models. The models used for annotation are denoted as M1, M2, M3, M4, with their relative prior\nability values ranked as: \\( \\theta(M_1) < \\theta(M_2) < \\theta(M_3) < \\theta(M_4) \\). We also compute the attempt success rate for each\ntheorem across the four models. The attempt success rate is defined as the number of successful proof attempts divided\nby the total number of attempts, with each model making 128 attempts. The attempt success rates for the 4 models\non theorem x are represented as PM\u2081 (x), PM2 (x), PM3(x), and PM\u2084(x), and P(x) is their average, representing the\noverall attempt success rate for theorem x. Finally, we use Di f ficulty(x) and Discrimination(x) to denote the\ndifficulty and discrimination metrics for theorem x.\nDifficulty Calculation. To calculate the difficulty more precisely, we designed a new formula that incorporates a\ncorrection term into P(x), as shown in 1. We subtract from P(x) the value of each model that has successfully proven\nthe theorem, divided by its prior ability score, and multiply the correction term by a weight \\( \\epsilon \\) (0.005). This approach\naims to highlight the difficulty differences between the theorems. The more models that successfully prove the theorem,\nand the lower the ability of those models, the more it suggests that the theorem's difficulty should be lowered. We also"}, {"title": null, "content": "P'(x) = P(x) - \\epsilon \\sum_{i=1}^{4} \\frac{\\mathbb{1}_{\\{P_{M_i}(x)>0\\}}}{\\theta(M_i)}     (1)"}, {"title": null, "content": "We then use 2 to calculate the difficulty for each theorem. Instead of directly using the attempt success rate, we\nemploy a reciprocal relationship derived from logistic regression. This approach more effectively captures the nonlinear\nrelationship between the success rate and difficulty. The rationale behind this is that P'(x) exhibits smaller changes near\n0 and 1, even though the actual difficulty may differ significantly. For example, the difference between P'(x) = 0.01\nand P'(x) = 0.1 may be more substantial than the difference between P'(x) = 0.6 and P'(x) = 0.7. By amplifying\nthe differences near 0 and 1, 2 highlights variations in the marginal intervals. This mapping enables a clearer distinction\nbetween high- and low-difficulty theorems by better separating items with low and high success rates."}, {"title": null, "content": "Difficulty(x) = \\frac{P'(x)}{1 - P'(x)}     (2)"}, {"title": null, "content": "Discrimination Calculation. We use 3 to calculate the discrimination for each theorem. P represents the set of all\npossible pairs of model combinations, which consists of 6 pairs."}, {"title": null, "content": "Discrimination(x) = \\sum_{(i,j) \\in P} \\frac{P_i(x) - P_j(x)}{\\theta(i) - \\theta(j)}    (3)"}, {"title": null, "content": "Using 3, we calculate the ratio of the difference in attempt success rates to the difference in ability levels between two\nmodels for a given theorem. If the ability difference between the two models is small but the pass rate difference is\nlarge, it indicates that the theorem has high discrimination, effectively distinguishing between models of different ability\nlevels. A negative discrimination value may suggest data contamination or that a low-ability model is particularly adept\nat proving the theorem, warranting further investigation. To reduce potential bias introduced by any single model, we\nsum the discrimination ratios for all model pairs and take the average, yielding a more stable discrimination metric.\nTo facilitate subsequent information content calculations, we linearly normalize the difficulty and discrimination metrics\nto the intervals [0, 1] and [-1,1], respectively."}, {"title": "3.2 Adaptive Evaluation", "content": null}, {"title": "3.2.1 Theorem Selection Algorithm", "content": "After completing the dataset annotation, a theorem selection algorithm is developed to identify the most informative\ntheorems for testing, based on their difficulty, discrimination, and the model's current ability score during adaptive\nevaluation. In Item Response Theory, the information of a question reflects how effectively it measures ability at a\nspecific ability score [36]. The greater the information, the better the question can differentiate between candidates with\nvarying ability scores at that level, leading to more accurate measurements.\nInformation is typically calculated using the Fisher information function [37]. In this study, we apply a slightly modified\nversion of the Fisher information function, as shown in 4. Here, P(x, \\( \\theta \\)) represents the probability of theorem x being\ncorrectly proven by a model with ability score \\( \\theta \\), calculated using the 5 from Item Response Theory [31]. a(x) denotes\nthe discrimination, while a(x)f represents a modification introduced in this study to adjust and amplify the influence of\nthe discrimination metric on the information, prioritizing the selection of theorems with higher discrimination. Here, f\nis a hyperparameter."}, {"title": null, "content": "I(x, \\theta) = a(x)^f \\cdot P(x,\\theta) \\cdot (1 - P(x, \\theta)).   (4)"}, {"title": null, "content": "The selection algorithm is outlined in C. We rank the candidate theorems in D based on their information scores,\nexcluding Tlast, which have already been selected in the previous 10 rounds of testing to prevent overfitting the ability\nscore by repeatedly choosing the same theorems. Finally, the top 5 theorems, Tselected, are selected for this round\nbased on the information ranking."}, {"title": "3.2.2 Adaptive Test Algorithm", "content": "In the adaptive evaluation, our goal is to obtain an ability score that accurately reflects the model's theorem-proving\nability. To achieve this, we perform multiple rounds of testing and ability score updates, gradually converging to the\ntrue ability score. As shown in 1, we first assign an initial ability score of 0.5. This initial score allows the model to\nbegin testing with theorems of moderate difficulty. Next, based on the theorem selection algorithm, we select theorems\nfor testing. The ability score, denoted as \\( \\theta \\), is updated according to the model's performance on these theorems, and\nnew theorems are selected for further testing using the updated ability score. This process continues until the model's\nability score changes by less than 0.01 over 10 consecutive rounds. At this point, we consider the model's ability score\nto have converged to its true value.\nIn 1, we designed the ability score update rule. When the success rate on a theorem ri is greater than 0 but less than\n0.1, we consider that the model's ability to prove this theorem may be influenced by random factors. For instance,\nwhen the model's success rate is 0.023, meaning it succeeded in only 3 out of 128 attempts, such a small number\nof successful attempts suggests a smaller score for this proof. Therefore, we apply a special transformation to these\nmodels: ri\u2190 log(1 + ri). The step size for updating the ability score is controlled by the hyperparameter \\( \\eta \\).\nFor the two hyperparameters appearing in the algorithm of this chapter: the discrimination influence factor f in 4\nand the step size adjustment factor \\( \\eta \\) in 1, we selected an optimal set of parameters through tuning experiments:\n[f, \\( \\eta \\)] = [0.49, 0.004]."}, {"title": "4 Experiment", "content": null}, {"title": "4.1 Experiment Setup", "content": "In this section, we introduce two distinct sets of LLMs: Annotation LLMs, used for dataset annotation, and Evaluation\nLLMs, responsible for adaptive evaluation, as described in 3. Additionally, we present the proof settings.\nAnnotation LLMs. When selecting the Annotation LLMs, we aimed to choose a model that could achieve the current\nstate-of-the-art (SOTA) level for generating complete Lean proofs. To ensure more reasonable difficulty annotations\nfor the theorems, we selected DeepSeek-Prover-V1.5-RL [5] as one of the Annotation LLMs. This model achieved a\nPass@128 of 58.61% on miniF2F2 (including the validation and test sets; this result and the following pass rates were\nobtained through our own testing), which, to the best of our knowledge, represents the highest pass rate currently.\nTo achieve appropriate discrimination metrics, a sufficient gap in the proof capabilities of the Annotation LLMs is\nnecessary. Therefore, we selected the following LLMs: codegeex4 [38], llemma [39], TheoremLlama[40], DeepSeek-"}, {"title": "4.2 Main Results", "content": "Dataset Annotation Results. Using the annotation models described in 4.1 and the methods outlined in 3.1, we\ncompleted the annotation and grading of the dataset, resulting in the miniF2F-Graded dataset. The detailed results of\nthe annotation and grading can be found in E. We classified 488 theorems by type and created a scatter plot based on\ndifficulty and discrimination, as shown in 2. It can be observed that, except for some theorems in mathd_numbertheory,\nwhich exhibit high discrimination at lower difficulty levels due to one model's exceptional performance in this category,\nother theorems show an increase in discrimination as difficulty rises. The theorems within the difficulty range of 0.4-0.6\ngenerally demonstrate higher discrimination, but as the difficulty continues to increase, the discrimination begins to\ndecrease. This difficulty-discrimination distribution trend largely aligns with our understanding of test problems. For\nsome unusual observations in the figure, we provide a detailed analysis in E.2.\nModel Evaluation Results. Based on the annotated dataset (miniF2F-Graded) and the adaptive evaluation method\ndescribed in 3.2, we evaluated six Evaluation LLMs and four Annotation LLMs. The evaluation results are presented in\n1. We also present in F.1 the process of model evaluation, including the selection of theorems, the changes in ability\nscores, and the evaluation results. It can be observed that the evaluation method proposed in this study significantly"}, {"title": "5 Analysis on Results", "content": null}, {"title": "5.1 Analysis on Data Annotation Results", "content": "Effectiveness of Difficulty Grading. We divided the miniF2F-graded theorems into Level 1-4 based on the sorting\nof the difficulty metric. Among them, Level 4 theorems are the most difficult, with a pass rate of 0 in all Annotation\nLLMs, corresponding to a difficulty metric of 1, and there are 127 theorems in this category. The remaining three levels\nare divided based on the principle of equal quantity, and the difficulty coefficient range and quantity for each level are\nshown in 2."}, {"title": "5.2 Analysis on Model Evaluation Results", "content": "Authenticity of Evaluation Results. In 1, we present the evaluation results obtained using our method. This section\nexamines whether these results accurately reflect the true proof abilities of the LLMs. The results from our evaluation\nmethod are represented by ability scores, which belong to a different dimension than pass rates and cannot be directly\ncompared. However, we can still compare the differences in proof abilities between models as reflected by these\ntwo metrics.We compared the pass rate rankings of the LLMs with different numbers of attempts to the rankings\nobtained from our ability scores, and the resulting confusion matrix is shown in 3. From the results in the figure, it is\nclear that the ability score rankings align well with those of Pass@16, Pass@32, and Pass@64. The only exception\nis in the comparison with Pass@128, where one pair of models-Qwen2.5-Coder-7B and DeepSeek-Prover-V1.5-"}, {"title": "6 Conclusion and Future Work", "content": "This study proposes a psychometric-based evaluation method for theorem proving with LLMs, which includes dataset\nannotation and adaptive evaluation. It effectively leverages the varying importance of theorems, enabling accurate\nevaluation of proof abilities with a minimal number of high-information theorems. By annotating the miniF2F dataset,\nthis research introduces a new dataset with metric annotations and difficulty grading: miniF2F-Graded. The difficulty\ngrading in miniF2F-Graded is more closely aligned with LLMs' difficulty perception compared to manual grading. In\nthe evaluation of multiple open-source LLMs, the adaptive evaluation method reduced the evaluation cost by 76.13%\nand yielded results that more clearly highlight the performance disparities between LLMs.\nCurrently, this method has been implemented using the miniF2F dataset as a case study. However, we believe\nthe approach is applicable to more extensive and diverse datasets and is expected to offer even greater efficiency\nimprovements when applied to larger-scale data collections. In the future, we plan to expand the method to additional\ndatasets and models, continuously updating the annotations and grading in miniF2F-Graded using progressively\nenhanced SOTA models, thereby providing a reliable and efficient evaluation benchmark for LLMs in theorem proving."}, {"title": "Impact Statement", "content": "This paper presents a psychometric-based evaluation method for large language models in theorem proving. By\nintroducing the miniF2F-Graded dataset, which includes difficulty and discrimination metrics. The adaptive evaluation\nmethod efficiently highlights performance disparities between LLMs using fewer theorems.\nThe societal impact of this research lies in its potential to optimize LLM evaluation in automated theorem proving,\nmaking the process more efficient and reliable. Ethical considerations focus on ensuring the fairness and transparency of"}, {"title": "A MiniF2F Dataset and Theorem Categories.", "content": null}, {"title": "A.1 Introduction to MiniF2F", "content": "The miniF2F dataset [32] comprises formalized mathematical problem statements at the Olympiad level. It includes\n488 formalized problems spanning high school and college-level mathematics, as well as questions from prestigious\ncompetitions such as the International Mathematical Olympiad (IMO), American Mathematical Competitions (AMC),\nand American Invitational Mathematics Examination (AIME). Although smaller in size compared to datasets like LISA\n[44] and ProofNet [45], miniF2F is widely recognized for its high-quality data and extensive adoption, making it an\nideal benchmark dataset for this study."}, {"title": "A.2 Theorem Categories", "content": "The theorems in miniF2F are classified using multiple criteria. By purpose, they are divided into a test set and a\nvalidation set. By source, they are categorized into IMO (International Mathematical Olympiad), AIME (American\nInvitational Mathematics Examination), AMC (American Mathematics Competitions), MATH [35], and CUSTOM\n(datasets from high school and undergraduate mathematics courses). By problem type, they are classified into number\ntheory, algebra, and induction."}, {"title": "B Different Methods of Metric Calculation.", "content": null}, {"title": "B.1 Metric Calculation by Statistical Modeling.", "content": "In Item Response Theory (IRT), models are categorized based on the number of metrics they incorporate, such as\none-metric, two-metric, or three-metric models. The two-metric model considers difficulty, which represents the\nability score required to correctly answer a question, and discrimination, which measures how effectively a question\ndifferentiates between varying ability levels [36]. The mathematical formulation for the two-metric model is as follows:"}, {"title": null, "content": "P(X = 1 | \\theta, x) =  \\frac{e^{a(x)(\\theta-b(x))}}{1 + e^{a(x)(\\theta-b(x))}}.   (5)"}, {"title": null, "content": "In 5, \\( \\theta \\) represents the ability level of the test subject, a(x) and b(x) represent the discrimination and difficulty of item x,\nrespectively, and P(X = 1 | \\( \\theta \\), x) represents the probability of correctly answering item x with an ability level of \\( \\theta \\).\nAfter determining the statistical model, methods such as Maximum Likelihood Estimation (MLE), Bayesian Estimation,\nand the EM algorithm are commonly used to estimate the difficulty and discrimination metrics. The more data from\ntest-takers, the more accurate the estimates become."}, {"title": "B.2 Difficulty Calculation without the Correction Term.", "content": "In the calculation of the difficulty metric, if we do not add a correction term to P(x), the result of the metric calculation\nis shown in 4. By comparing 4 and 2, we can observe that adding the correction term to the difficulty metric calculation\nresults in a more dispersed distribution of difficulty values. Specifically, for the theorems with difficulty values between\n0.4 and 0.6, the points in 4 are almost clustered together, making it difficult to distinguish them. However, in 2, where\nthe correction term is included, these theorems are spread across multiple difficulty levels, which is more conducive to\ntheorem grading and selection."}, {"title": "C Theorem Selection Algorithm.", "content": null}, {"title": "D Experiment LLMs and Prompts.", "content": null}, {"title": "D.1 Annotation LLMs.", "content": "1)codegeex4-9B (Pass@128=7.99%) is the open-source version of the latest CodeGeeX4 model series. It is a multilin-\ngual code generation model trained on GLM-4-9B, which significantly enhances its code generation capabilities[38].\n2)llemma-7B (Pass@128=20.29%) is a mathematical language model. The Llemma model is particularly outstanding\nin terms of chain-of-thought mathematical reasoning and formal theorem proving[39].\n3)TheoremLlama (Pass@128=41.60%) is a fine-tuned model for writing Lean4 proofs. Using the Open Bootstrapped\nTheorem dataset for fine-tuning, TheoremLlama can achieve quite good performance when writing Lean4 proofs based\non natural language theorem statements[40].\n4)DeepSeek-Prover-V1.5-RL (Pass@128=58.61%) is an open-source language model for theorem proving in Lean 4.\nThe model was pre-trained on DeepSeekMath-Base, focusing on formal mathematical language, and uses reinforcement\nlearning (RL) to improve performance based on the validation feedback of the Lean4 prover, resulting in a significant\nincrease in proving ability[5]."}, {"title": "D.2 Evaluation LLMs.", "content": "1)Code-Llama-7B (Pass@128=12.30%) is a large code language model series based on Llama 2", "benchmarks[41": ".", "43": ".", "39": ".", "series": "DeepSeek-Prover-V1 (Pass@128=52.05%)", "capability[33": "V1.5-Base was pre-trained on DeepSeekMath-Base[46", "explanatory\nannotations.[5": ".", "reasons": "By comparing the evaluation results between different versions of the DeepSeek-Prover series\nmodels, we can verify the effectiveness of the evaluation methods used in this study. For example, DeepSeek-Prover-\nV1.5-RL, compared to DeepSeek-Prover-V1.5-SFT, has enhanced its capabilities through a combination of online\nreinforcement learning (RL) and validation feedback. However, the difference between the two in terms of Pass@128 is\nonly 0.41% [5"}]}