{"title": "LLM-ProS: Analyzing Large Language Models' Performance in Competitive Problem Solving", "authors": ["Md Sifat Hossain", "Anika Tabassum", "Md. Fahim Arefin", "Tarannum Shaila Zaman"], "abstract": "The rapid advancement of large language models has opened new avenues for automating complex problem-solving tasks such as algorithmic coding and competitive programming. This paper introduces a novel evaluation technique, LLM-ProS, to assess the performance of state-of-the-art LLMs on International Collegiate Programming Contest (ICPC) problems. Using a curated dataset of 166 World Finals problems from 2011 to 2024, we benchmark the models' reasoning, accuracy, and efficiency. We evaluate the five models-GPT-40, Mistral Large, Llama-3.1-405B, and the o1 family, consisting of o1-mini and o1-preview, across critical metrics like correctness, resource utilization, and response calibration. Our results reveal significant differences in the models' abilities to generalize, adapt, and solve novel problems. We also investigated the impact of training methodologies, dataset contamination, and chain-of-thought reasoning on model performance. The findings provide new insights into optimizing LLMs for algorithmic tasks, highlighting both strengths and limitations of current models.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) [1] have revolutionized natural language processing (NLP) [2] and their applications in various technical domains, including algorithmic problem-solving and code generation. Trained on extensive and diverse datasets, these models exhibit remarkable capabilities in understanding and generating code [3]. However, their performance on complex, real-world coding challenges remains an area of active investigation. Competitive programming problems, such as those from the International Collegiate Programming Contest (ICPC) [4], offer a unique opportunity to rigorously evaluate LLMs due to their intricate constraints, computational demands, and emphasis on algorithmic efficiency.\nICPC problems are particularly well-suited for evaluating LLM performance [5] for several reasons. First, these problems require a combination of logical reasoning, algorithmic thinking, and precise implementation, reflecting real-world software engineering challenges. Second, their diverse categories, including graph theory, dynamic programming, and computational geometry, offer a broad spectrum of difficulty levels, testing an LLM's [6] ability to generalize across various problem types. Third, the focus on correctness, efficiency, and edge-case handling aligns with critical evaluation metrics such as runtime performance and memory usage [7]-[9].\nExisting works use LLMs at different stages of code generation and testing. For example, Fakhoury et al. [10] improve solutions through test feedback and refine them step-by-step. Liu et al. [7] identify problems like data contamination and over-reliance on memorized patterns, which reduce the reliability of results. A recent study [11] explores how adding syntax and grammar rules during training can enhance the accuracy and robustness of LLMs. Similarly, Coignion et al. [12] analyze the efficiency of code generated by LLMs on Leetcode, revealing performance trends and benchmarks. However, these studies primarily focus on specific aspects of code generation, such as correctness or efficiency, without comprehensively evaluating LLM performance in solving complex, real-world competitive programming problems. To address this gap, we propose an approach, LLM-ProS, to assess the capabilities of advanced LLMs, including OpenAI's o1 family [13], GPT-40 [14], Mistral Large [15], and Llama-3.1-405B [16], in tackling ICPC problems.\nTo develop LLM-ProS, we collect a curated dataset of 166 ICPC World Finals problems from the years 2011 to 2024, providing a comprehensive benchmark for assessing the reasoning, accuracy, and efficiency of LLMs. The study assessed five state-of-the-art models\u2014GPT-40, Mistral Large, Llama-3.1-405B, ol-preview and o1-mini-selected for their diverse architectures and training methodologies. Comprehensive data preprocessing involved extracting problem components, standardizing prompts, cleaning text, customizing templates for each model, and validating the preprocessed data. Solutions generated by the LLMs were submitted to Codeforces Gym ICPC contests, receiving automated feedback on correctness and efficiency. All experiments were conducted in a controlled environment, with reproducible submissions and publicly available scripts to ensure consistency and transparency. In summary we make the following contributions:\n\u2022 We propose a performance analyzer, LLM-ProS, to assess the performance of five different LLMs on new ICPC"}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "Despite their remarkable capabilities, the effectiveness of LLMs in tackling complex, real-world coding challenges remains inadequately explored. Traditional benchmarks often fall short in assessing the nuanced reasoning, adaptability, and efficiency required for high-stakes programming tasks.\nThe International Collegiate Programming Contest (ICPC), the oldest, largest, and most prestigious programming contest in the world, presents a unique and rigorous environment for evaluating LLM performance [18]. ICPC-style problems are characterized by their intricate constraints, diverse problem categories, and emphasis on algorithmic efficiency and edge-case handling. These attributes make them ideal for benchmarking the reasoning capabilities, accuracy, and resource utilization of LLMs in a controlled yet challenging setting [7], [8].\nMoreover, the increasing reliance on LLMs for automated code generation in software engineering necessitates a deeper understanding of their strengths and limitations. Evaluating LLMs against competition-level programming problems not only highlights their problem-solving prowess but also identifies critical areas for improvement, such as generalization to unseen problems and efficient resource usage [10], [19]. This study aims to bridge the existing research gaps by providing a comprehensive assessment of state-of-the-art LLMs using a curated set of ICPC World Finals problems.\nTo thoroughly evaluate the performance of LLMs on ICPC-style problems, we selected a diverse range of models representing various architectures, training methodologies, and optimization strategies. The models under consideration include GPT-40, Mistral Large, Llama-3.1-405B, 01-mini, and 01-preview. Each model was chosen for its distinct characteristics and potential applicability to structured problem-solving tasks.\n1) GPT-40: GPT-40 [12], [17], [20] is a general-purpose language model renowned for its high accuracy in code generation and reasoning tasks. Leveraging its extensive training on diverse datasets, GPT-40 [21] demonstrates strong capabilities in understanding complex problem statements and generating syntactically correct code. However, its performance may be less optimized for structured problem-solving compared to models specifically fine-tuned for reasoning and iterative refinement.\n2) Mistral Large: Mistral Large specializes in handling domain-specific tasks with a focus on efficient resource utilization. Designed to balance performance with computational efficiency, Mistral Large offers insights into the trade-offs between model complexity and practical deployment in resource-constrained environments. Its performance on ICPC problems provides valuable data on how domain specialization impacts problem-solving efficacy [11].\n3) Llama-3.1-405B: Llama-3.1-405B [12], [22] is a computationally efficient model tailored for general-purpose use. Serving as a benchmark for lightweight LLMs in competitive programming, Llama-3.1-405B facilitates comparisons regarding scalability and efficiency without significant compromises in accuracy. Its performance metrics shed light on the feasibility of deploying smaller models in high-stakes programming scenarios. It is also particularly interesting as the best-performing open source model.\n4) OpenAI ol Family: The 01-mini and o1-preview models [9], [19] from the OpenAI 01 family are specifically fine-tuned for chain-of-thought (CoT) reasoning and iterative refinement processes. These models are engineered to excel in structured, multi-step problem-solving tasks, making them highly suitable for ICPC-style evaluations. Their design emphasizes enhanced reasoning pathways and reduced hallucination rates, enabling more consistent and accurate code generation under complex constraints.\nBy systematically applying each model to this diverse set of problems, the study evaluates not only the accuracy and robustness of the solutions but also the computational efficiency and adaptability of the models in handling unseen and complex programming tasks. This rigorous evaluation framework provides a holistic view of the current capabilities of LLMs in competitive programming contexts, informing future developments in model design and training methodologies."}, {"title": "III. METHODOLOGY", "content": "Figure 1 illustrates the overview of our proposed technique, LLM-ProS. LLM-ProS consists of four major steps: data collection, data preprocessing, model testing, and solution generation and submission. The details of each step are discussed in the following subsections.\nWe scrape a total of 166 competitive programming problems from the ICPC official website [4], specifically from the World Finals editions spanning the years 2011 to 2024 World Finals. We select these years to avoid potential overlap with the"}, {"title": "IV. EXPERIMENTAL EVALUATION", "content": "We use Selenium [24] for the automated downloading of PDFs from the website. For data pre-processing, we utilized PyPDF2 [25] to extract text and re (regular expressions) [26] for cleaning and formatting the extracted problem statements. We evaluate five large language models (LLMs): GPT-40 [12], Mistral Large [17], Llama-3.1-405B [9], and both o1-mini and ol-preview [19]. Solutions [27] generated using their respective APIs [28] are submitted to Codeforces Gym ICPC contests [4]. To evaluate LLM-ProS, we consider four research questions:\nRQ1: How do LLMs perform on new ICPC problems compared to those potentially seen during training?"}, {"title": "V. THREATS TO VALIDITY", "content": "In this section, we discuss four types of threats, similar to prior research [31].\nData contamination poses a major threat to our study's internal validity. Some of the ICPC problems we used might already exist in the evaluated LLMs' training data. We select problems from years (2011-2024) that likely do not overlap with the training cutoffs of popular models, but ensuring no overlap is challenging. If any problem appears in the training data, they could artificially inflate performance scores, especially for models exposed to similar problem statements [32]. Therefore, we evaluated the models on ICPC 2024's problems within 24 hours of the contest to ensure none of the models were trained on that data.\nThe study focuses on ICPC World Finals problems, which may limit the generalizability of the findings. While ICPC problems are diverse and complex, they represent only a subset of the programming challenges faced in broader software engineering and real-world application development. As a result, the performance of LLMs on ICPC problems may not directly translate to other programming tasks, potentially limiting the applicability of our conclusions to different domains [7], [8], [22]. To mitigate this bias, we use a diverse set of 166 ICPC World Finals problems, covering various categories and difficulty levels, to strengthen the robustness of the evaluation.\nWe test the models in a zero-shot setting without additional fine-tuning or iterative interactions. This approach may not fully leverage the models' capabilities, as fine-tuning or interactive prompting could improve performance on specific tasks. Additionally, differences in how each model interprets prompts or handles specific problem components may introduce variability not fully accounted for in our study. These constraints could limit the ability to generalize the findings to scenarios where models are fine-tuned or interactively guided during problem-solving [10], [17], [19]. To mitigate this bias, we document all experimental procedures and provide access to the scripts used for data preprocessing and solution evaluation to ensure reproducibility and transparency.\nOur evaluation relies exclusively on the Codeforces Gym platform for submitting solutions and determining verdicts. This creates a dependency on the platform's specific execution environment and judging criteria. Variations in compiler versions, runtime environments, or hidden test cases used by the platform can affect the consistency and reliability of the verdicts. Additionally, platform-specific nuances in error reporting or resource measurement may influence the evaluation of resource utilization metrics [11], [12]. To minimize this bias, we ensure that all submissions are made under the same conditions within Codeforces Gym, maintaining consistency in verdict determination and resource measurement."}, {"title": "VI. RELATED WORK", "content": "Advancements in large language models (LLMs) have spurred significant interest in evaluating their capabilities across various domains. In competitive programming, several studies emphasize the importance of rigorous benchmarks and highlight challenges such as reasoning ability, data contamination, and evaluation methodologies.\nProgramming challenges from platforms like Codeforces and the International Collegiate Programming Contest (ICPC) offer unique evaluation benchmarks for LLMs due to their complexity and diversity. These problems require a deep understanding of algorithms, mathematics, and reasoning,"}, {"title": "VII. CONCLUSION", "content": "This study underscores the efficacy of competition-level programming problems, specifically those from the International Collegiate Programming Contest (ICPC), as robust benchmarks for evaluating large language models (LLMs). Utilizing a curated dataset of 166 ICPC World Finals problems from 2011 to 2024, we systematically assessed state-of-the-art LLMs, including OpenAI's o1 family, GPT-40, Mistral Large, and Llama-3.1-405B. Our findings reveal that the o1 models, particularly 01-mini and o1-preview, significantly outperform others in terms of accuracy, robustness, and computational efficiency, owing to their advanced chain-of-thought (CoT) reasoning and diverse training methodologies. In contrast, models such as GPT-40, Mistral Large, and Llama-3.1-405B demonstrate limited generalization abilities and higher error rates on unseen 2024 problems, highlighting their reliance on pretraining data and the need for improved generalization skills [7], [33]. Additionally, the analysis of verdict distributions indicates that ol models consistently achieve higher proportions of \"Accepted\" (AC) verdicts while minimizing errors like \u201cCompile Error\u201d (CE) and \u201cTime Limit Exceeded\u201d (TLE), in contrast to other models which show higher frequencies of these errors. These insights emphasize the importance of developing contamination-free benchmarks and enhancing reasoning capabilities through advanced training methodologies like CoT and iterative refinement [10], [19]. Overall, LLM-ProS contributes to a deeper understanding of LLM performance in real-world problem-solving scenarios and paves the way for further advancements in LLM design and evaluation, ensuring that they can meet the complex demands of technical problem-solving tasks."}]}