{"title": "Synchronous Multi-modal Semantic Communication System with Packet-level Coding", "authors": ["Yun Tian", "Jingkai Ying", "Zhijin Qin", "Ye Jin", "Xiaoming Tao"], "abstract": "Although the semantic communication with joint semantic-channel coding design has shown promising performance in transmitting data of different modalities over physical layer channels, the synchronization and packet-level forward error correction of multimodal semantics have not been well studied. Due to the independent design of semantic encoders, synchronizing multimodal features in both the semantic and time domains is a challenging problem. In this paper, we take the facial video and speech transmission as an example and propose a Synchronous Multimodal Semantic Communication System (SyncSC) with Packet-Level Coding. To achieve semantic and time synchronization, 3D Morphable Mode (3DMM) coefficients and text are transmitted as semantics, and we propose a semantic codec that achieves similar quality of reconstruction and synchronization with lower bandwidth, compared to traditional methods. To protect semantic packets under the erasure channel, we propose a packet-Level Forward Error Correction (FEC) method, called PacSC, that maintains a certain visual quality performance even at high packet loss rates. Particularly, for text packets, a text packet loss concealment module, called TextPC, based on Bidirectional Encoder Representations from Transformers (BERT) is proposed, which significantly improves the performance of traditional FEC methods. The simulation results show that our proposed SyncSC reduce transmission overhead and achieve high-quality synchronous transmission of video and speech over the packet loss network.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the explosion of intelligent applications and network traffic, conventional communication systems are facing the challenge of big data transmission. In recent years, deep learning enabled semantic communication systems have gathered attention [1]. Compared to symbol transmission in traditional communication systems, semantic communication systems with focusing on transmitting the semantics of the data, reduce efficiently bandwidth and improving communication efficiency [2]. Despite these advantages from semantic coding, the synchronous transmission of multimodal data presents significant challenges. During human-computer interaction, a user typically generates multimodal data simultaneously, such as video, audio, text, etc. Multimodal data from the same source show strong correlations and ambiguous segmentation [3]. These multimodal data are highly aligned in both the semantic and time domains. Because highly compressed semantic features from semantic encoders designed independently lack alignment information, it is challenging to synchronize the transmitted data. Besides, when faced with packet loss network, semantic features are only received perfectly or lost. However, physical layer semantic communication systems have no higher-level joint designs. The semantics transmitted through existing network protocols are difficult to cope with changing network environments. Therefore, the semantic packet-level forward error correction is needed to be addressed."}, {"title": "A. Piror Work", "content": "Compared to conventional communication systems, semantic communication systems designed with Deep Neural Network (DNN) perform well for various type of information sources and tasks. According to the modality of the transmission source, current semantic communication systems can be divided into two categories, namely single-modal and multimodal systems. For single-modal sources or tasks such as text [4], [5], [6], speech [7], image [8], [9], [10], and video [11], semantic communication systems extract and transmit tailored semantic features related to tasks at the receiver side. These works have reduced the transmission overhead for single-modal sources and shown notable performance in low signal-to-noise (SNR) regime.\nFor multimodal tasks or transmission, existing work mainly utilizes the correlation between multimodal data for semantic coding. A multi-user multimodal semantic communication system in [12] extracts semantics for Visual Question Answersing (VQA) tasks. The multimodal semantic commucation system based on knowledge graphs is proposed in [13], which shows channel robustness for VQA tasks. A distributed semantic communication system for audio visual parsing (AVP) tasks in [14] reduces visual semantics transmission through audio semantic guidance. These proposed systems are oriented towards multimodal tasks. In addition, the correlation between multimodal data is used to further compress the semantics of transmission. A multimodal semantic communication system in [15] is designed for video, audio, and tactile signals. The sharing and private semantic information is considered in [16] for image and text, which reduces the cost of image transmission. To perform cross-modal alignment of semantics,"}, {"title": "B. Motivation and Contributions", "content": "Future 6G wireless networks are required to support immersive intelligent services [21] in scenarios such as video conferencing and Extended Reality (XR), it is important to synchronous transmission of multimodal data. Although numerous works have focused on multimodal semantic transmission, the synchronization between different modalities in terms of both the time and semantics domains has not been well addressed. Furthermore, considering that semantics are transmitted in the form of packets across networks, semantic communication systems based on physical layer designs at the symbol level are hard to synchronize. In response to network variations, packets are either perfectly received or dropped. Consequently, we consider Packet-Level Forward Error Correction (FEC) based on semantic context redundancy to address packet loss network, thereby enhancing the system's robustness.\nTherefore, a synchronous semantic communication system (SyncSC) for video-speech transmission is proposed. To more effectively compress the transmission bandwidth, the transmitter utilizes 3D Morphable Model (3DMM) coefficients as the semantics for facial representation and text from speech recognition as the semantics for speech. Initially, a reference image and a speech clip are transmitted as shared knowledge to guide the user's appearance and speech tone. The 3DMM coefficients effectively characterize facial movements and expressions, while the text from speech recognition efficiently represents the content. To maintain time synchronization, video semantics and text are time-stamped before being assembled into data packets. The receiver then synchronizes these packets on the basis of the Real-time Transport Protocol (RTP). For synchronization in the semantic domain, we design a visual-guided speech synthesis module at the receiver, which decodes synchronized speech and facial images. At the receiver, the facial semantics and text are aligned through an attention mechanism to obtain latent features, which are then input into a voice generator to produce speech with the same duration. Additionally, to combat packet loss, we design a packet-level semantic coding module based on Masked Autoencoders (MAE), which applies erasure coding to video semantics. Considering that the bandwidth occupied by text is already minimal, we further design a packet loss cancellation module based on pre-trained large language models to predict text content, aiming to supplement semantically similar content. Simulation results indicate that our proposed method outperforms traditional approaches in terms of performance and reduces transmission overhead, demonstrating robustness in packet loss networks.\nThe main contributions of this paper can be concluded as follows:\n1) To the best of our knowledge, we are the first to propose a multimodal synchronous semantic communication system with the design of semantic packet-level FEC method above the physical layer. The semantics extracted by the transmitter remain synchronized in both the time and semantic domains and robust over packet loss network at the receiver.\n2) For synchronization in the semantic domain and time domain, we design a visual-guided speech synthesis module with timestamp which uses received text and facial semantics to guide the speech synthesis.\n3) We develop a semantic packet-level coding method, called PacSC, for video semantic packets based on masked autoencoders to cope with the packet loss. By leveraging semantic redundancy, our method performs better than traditional Reed-Solomon (RS) code when the network packet loss rate is high.\n4) We develop a packet loss concealment module, called TextPC, for text packets based on pre-trained language models. When the packet loss rate is above 0.5, compared with RS with coderate, this module brings about a gain of 0.1 in Bilingual Evaluation Understudy (BLEU) and Semantic Similarity."}, {"title": "C. Organization", "content": "The rest of this paper is organized as follows. The framework of multimodal synchronous semantic communication system (SyncSC) is introduced in Section II. The details of the SyncSC are given in Section III. The simulation results are given in Section IV, and Section VI provides a conclusion."}, {"title": "II. FRAMEWORK OF MULTIMODAL SYNCHRONOUS SEMANTIC COMMUNICATIONS", "content": "In this section, the framework of the proposed system model is introduced. The system model consists of a video-speech transmitter and a multi-modal receiver."}, {"title": "A. Video-Speech Transmitter", "content": "As shown in Fig. 1, the proposed transmitter consists of the video semantic encoder and the speech semantic encoder. The facial semantics are extracted by the video semantic encoder, while the speech semantic encoder converts speech to text.\n1) Video Semantic Encoder: The input facial video consists of a series of frames. In particular, the input video \\(P = [p_0, p_1, ..., p_n, \u2026\u2026\u2026,p_{N_v}]\\), where \\(N_v\\) is the total number of video frames, and \\(p_n \u2208 \\mathbb{R}^{256\u00d7256\u00d73}\\) is the n-th frame in RGB format. The user's appearance information serves as shared knowledge between the transmitter and receiver. The first frame \\(p_0\\) as the key frame is sent at the initial stage. Meanwhile, the video semantic encoder extracts the semantics of the other frames {\\(p_n, n > 0\\)}. Therefore, the encoded facial semantics of \\(p_n\\) can be expressed as\n\\[\\delta_n = T_v(p_n),\\]\nwhere \\(T_v(\u00b7)\\) indicates the video semantic encoder with respect to parameters \\(\\alpha_v\\). The output \\(\\delta_n\\) is a collection of 3D Morphable Mode (3DMM) [22] coefficients, which is a subset of the projection coefficient of the face in a three-dimensional space, expressed by\n\\[\\delta_n = {\\delta_{exp_n}, \\delta_{eye_n}, \\delta_{rot_n}, \\delta_{trans_n}, \\delta_{crop_n}},\\]\nwhere \\(\\delta_{exp_n} \u2208 \\mathbb{R}^6\\) indicates the facial expression coefficients, \\(\\delta_{eye_n} \u2208 \\mathbb{R}^1\\) indicates the eye motion coefficients, \\(\\delta_{rot_n} \u2208 \\mathbb{R}^3\\) indicates the head rotation coefficients, \\(\\delta_{trans_n} \u2208 \\mathbb{R}^3\\) indicates the head translation coefficients, and \\(\\delta_{crop_n} \u2208 \\mathbb{R}^3\\) indicates the crop parameters of 3DMM.\nThese coefficients are extracted directly by the pre-trained face reconstruction model [23], and the parameters are not need to be finetuned. The actions and shape of the speaker's face in our semantic encoder correspond to these coefficients. Therefore, unlike existing keypoint-based semantic encoders, explicit semantic knowledge has advantages over implicit semantic features in terms of semantic editing and understanding.\n2) Speech Semantic Encoder: In order to reduce the volume of speech transmission data as much as possible, following the process in Txt2vid [20], speech recognition is used to convert speech into text. The original input speech \\(S = [s_0, s_1, ..., s_n, \u2026\u2026\u2026s_{N_s}]\\) is divided into \\(N_s\\) fragments and \\(s_n\\) is fed to the speech semantic encoder. In order to obtain the knowledge base of the speaker's speech style, the first \\(N_0\\) fragments are shared between the transmitter and receiver as the user's timber knowledge. The text content of the speech sample \\(s_n\\) can be expressed by\n\\[l_n = T_s(s_n),\\]\nwhere \\(T_s(\u00b7)\\) indicates the speech semantic encoder with respect to the parameters \\(\\alpha_s\\).\n3) Video Semantic Packet Encoder: Due to the similarity in user behavior between adjacent video frames, there is semantic redundancy because of intra-frame semantic encoding. Therefore, semantic redundancy is used in the video semantic packet FEC. As shown in Fig. 1, we define the semantics of an image as the encoding symbol, and a coding block consists of \\(N_{pv}\\)"}, {"title": "B. Multi-modal Receiver", "content": "At the receiver, arrival semantic packets are synchronized in the time domain based on their sequence number and timestamp. Our proposed multi-modal receiver first decodes the lost video packets and predicts the missing words of text. After that, the reconstructed semantics are used to generate video with speech through the Image Generator and Visual-Guided Speech Synthesis.\n1) Video Semantic Packet Decoder: Within the decoding block, the packet decoder decodes partially lost data packets and reconstructs semantically consistent video semantic packets. Therefore, the reconstructed semantics of the video can be expressed by\n\\[\\hat{\\delta}_i, \\hat{\\delta}_{i+1}, ..., \\hat{\\delta}_{i+N_{pv}-1} = R_v(\\check{e}_i, \\check{e}_{i+1}, ..., \\check{e}_{i+N_{pv}-1}),\\]\nwhere \\(R_v\\) indicates the video semantic packet decoder with respect to the parameters \\(\\theta_v\\) and \\(\\check{e}_k, i \u2264 k \u2264 i + N_{pv}-1\\) indicates that \\(e_k\\) may be lost with a \\(p, 0 < p < 1\\) probability.\n2) Text Packet-loss Concealment Module: The existing deep learning based semantic-channel coding methods for text, such as DeepSC [5], indicate that utilizing text redundancy requires sacrificing significant bandwidth. For example, an English word is usually represented by embedded vectors ranging from 32 to 128 dimensions, but traditional methods, such as ASCII code, only require a few bytes. Therefore, we use the traditional text encoding methods and design a packet loss concealment method based on BERT [24] to predict the lost part at the receiver.\n\\[\\hat{l}_i = R_l(\\check{l}_i, \\check{l}_{i+1},..., \\check{l}_{i+N_{ps}-1}),\\]\nwhere \\(\\hat{L} = [\\hat{l}_i, \\hat{l}_{i+1}, ..., \\hat{l}_{i+N_{ps} -1}]\\) indicates the reconstructed text, \\(R_l\\) indicates the text packet-loss concealment module with respect to the parameters \\(\\theta_s\\), and \\(\\check{l}_k, i \u2264 k \u2264 i + N_{ps}-1\\) indicates that \\(l_k\\) may be lost with a \\(p, 0 < p < 1\\) probability.\n3) Image Generator: After obtaining the decoded semantic data packet, each block \\(\\hat{\\delta}_i\\) corresponds to an image frame through the image generator. The reconstructed image \\(\\hat{p}_i\\) can be expressed as\n\\[\\hat{p}_i = R_I(\\hat{\\delta}_i, p_0),\\]\nwhere \\(p_0\\) is the shared image between transmitter and receiver, and \\(R_I(\u00b7)\\) indicates the image generator with parameters \\(\\xi_v\\).\n4) Visual-guided speech synthesis: At the receiving end, a word corresponds to a certain length of speech and several image frames. Therefore, to align the features of the video and speech, we define a text \\(l_k\\) with the same timestamp \\(t_k\\) as \u03bb video frames, where \u03bb is a positive integer, and its value depends on the block size setting of the speech recognizer and the video frame rate. Specifically, the video frames {\\(p_{\\lambda k},p_{\\lambda k+1},\u2026\u2026,p_{\\lambda (k+1)-1}\\)} and the text \\(l_k\\) are synchronized with the same timestamp \\(t_k\\). The visual-guided speech synthesis module generates lip-synchronized speech \\(s_k\\), by the following\n\\[s_k = R_s(l_k, \\delta_{exp_{\\lambda k}}, \\delta_{exp_{\\lambda k+1}},..., \\delta_{exp_{\\lambda (k+1)-1}}),\\]\nwhere \\(R_s\\) indicates the visual-guided speech synthesis with parameter \\(\\xi_s\\). Therefore, the synchronized speech and video {\\(s_k,p_{\\lambda k}, p_{\\lambda k+1},\u2026\u2026, p_{\\lambda (k+1)-1}\\)} are received."}, {"title": "III. ARCHITECTURE OF THE PROPOSED SYSTEM", "content": "In this section, the details of each modules in the proposed system are given. Specifically, we utilize existing deep network models to extract facial 3DMM coefficients and speech recognition for source semantic encoding. We propose MAE-based [25] packet-level FEC and BERT-based [24] packet loss concealment methods for video and speech packets to cope with the erasure channel. Moreover, at the receiver, we change the existing map-warp-edit model [26], and propose a speech synthesis module guided by facial expression coefficients."}, {"title": "A. Semantic Encoder", "content": "The goal of the video semantic encoder is to convert the original facial frame to accurate representations of motion. In the proposed system, we employ a subset of 3DMM parameters to represent the motion semantics. In 3DMM, the 3D shape S of a face is given by\n\\[S = \\bar{S} + d_{id}S_{id} + d_{exp}S_{exp},\\]\nwhere \\(\\bar{S}\\) indicates the average face shape. \\(d_{id} \u2208 \\mathbb{R}^{80}\\) indicates the facial shape coefficient of the identity base vector \\(S_{id}\\). Inspired by [27], \\(d_{exp} \u2208 \\mathbb{R}^6\\) is the first six of \\(d_{exp} \u2208 \\mathbb{R}^{64}\\), indicating the coefficient of facial shape of the expression base vector \\(S_{exp}\\). For pose semantics, head rotation is expressed as \\(r \u2208 SO(3)\\) and translation is expressed as \\(t \u2208 \\mathbb{R}^3\\). Obtaining the 3DMM coefficient of a facial image is a regression problem. The existing 3D face reconstruction model [23] can extract the 3DMM coefficients of an image. However, after experimentation, we find that \\(d_{exp}\\) is difficult to represent eye movements. Therefore, we employ the methods in [28] to calculate the eye blink coefficient \\(d_{eye} \u2208 \\mathbb{R}^1\\).\nFor the speech semantic encoder, Zipformer [29], an efficient automatic speech recognition (ASR) framework, is used to convert speech to text in real time. Specifically, the speech fragment \\(s_n\\) is input into the Zipformer ASR and returns the final text \\(l_n\\) and the timestamp \\(t_n\\)."}, {"title": "B. Video Packet Encoder and Decoder", "content": "As shown in Fig. 2, we propose a network based on the mask autoencoder to encode the semantics of video for the packet loss network. The mask autoencoder methods randomly mask partial patches at the image level and predict masked patches through adjacent patches, which is an effective method to reduce information redundancy [30]. The semantics are the projection of images in 3DMM space, so there is also significant redundancy between adjacent frames. Therefore, we model packet-level coding as a mask autoencoder problem and use the transformer to predict lost semantics. The input of the encoder \\(X \u2208 \\mathbb{R}^{N_{pv} \u00d7 D_v}\\) consists of \\(N_{pv}\\) video semantics with \\(D_v\\) dim and is divided into M patches with size of \\(n_p \u00d7 n_p\\) through the linear projection. Following ViT [31], linear projection is achieved through convolution and can be represented as\n\\[E_p = X * K + B,\\]\nwhere K is a convolution kernel with size of \\(n_p\\), which maps the semantics to the feature space of the \\(D_p\\) dimension, and B is a \\(D_p\\)-dim bias. After adding position encoding \\(E_{pos}\\) to the output embedded vector \\(E_p\\), the output of linear projection can be expressed by\n\\[E_e = E_p + E_{pos},\\]\nwhere \\(E_e \u2208 \\mathbb{R}^{M\u00d7D_p}\\) and \\(M = [\\frac{N_{pv}D_v}{n_p^2}]\\). Therefore, the original video semantics are transformed into M patches, fed to several Transformer Blocks [32], and normalized through a Norm Layer to obtain the final encoded output \\(E \u2208 \\mathbb{R}^{M\u00d7D_p}\\). The symbols of encoded semantics \\(E \u2208 \\mathbb{R}^{M\u00d7D_p}\\) are divided into servral data packets according to the RTP protocol and sent to the network for transmission. In order to end-to-end train the packet encoder and decoder, inspired by [4], the dropout layer is used to simulate the network. But unlike the bit-level channels in [4], we use the dropout2d layer for the packet-level network. Therefore, the packet loss network can be represented as\n\\[Y = dropout2d(E, p),\\]\nwhere Y is the \\(N_p\\) data packets observed at the receiver after RTP time synchronization, and p represents the probability that a packet is dropped.\nAt the receiver, we design a model with similar structure to decode partially lost packets. The linear layer is used to change the size of \\(Y \u2208 \\mathbb{R}^{M \u00d7 D_p}\\) to \\(Y_d \u2208 \\mathbb{R}^{M\u00d7D_d}\\). Then \\(Y_d\\) is fed to several Transformer Blocks and a Norm Layer, \\(Y_{dn} \u2208 \\mathbb{R}^{M\u00d7D_d}\\) is obtained. Finally, through a Multilayer Perceptron (MLP), the feature \\(Y_{dn}\\) is transformed into the size of the original input semantics \\(X \u2208 \\mathbb{R}^{N_{pv} \u00d7 D_v}\\)."}, {"title": "C. Text Packet Loss Concealment", "content": "As shown in Fig. 3, at the transmitter, we first random interleaved encode the original text at word level. To cope with continuous packet loss, the original text sequence, \\(L = (l_1, l_2, ..., l_{N_l})\\), is first interleaved to the sequence \\(L_1 = (l'_1, l'_2, ..., l'_{N_l})\\). We adopt a simple random interleaving strategy in a sending window. Then, the source encoding and conventional packet-level FEC, such as Huffman-RS coding, are performed on the interleaved text sequence \\(L_1\\) to form several RTP data packets. These data packets are sent to the packet loss network. At the receiver, lost words after interleaved decoding are predicted by our BERT-based model which is finetuned on the knowledge base between the transmiter and receiver."}, {"title": "D. Semantic Decoder", "content": "After the lost packets are decoded, the proposed semantic decoder synchronously generates images and speech of the speaker. The input of the semantic decoder consists of four parts: shared knowledge including reference image, reference speech samples, estimated semantics, and text.\nFor decoding video semantics, as shown in Fig. 4, the Image Generator is made up of three networks, namely the mapping-warping-editing network. The reference frame \\(p_0\\) serves as the user's appearance feature, and the received semantics drive the generation of the target frames. Different from the structure in [26], the eye feature is added as input to the generator. Specifically, \\(\\delta_i\\) are first mapped to 256-dimension feature \\(f_i\\). Then, the reference image \\(p_0\\) and the mapped feature \\(f_i\\) are used to generate the flow field \\(w_i\\) and the warped image \\(p_{wn}\\) through the warping network. The warped image \\(p_{wn}\\) lacks details of the target image. Therefore, the reference image and the warped image are fed to the editing network to restore more details of the image \\(\\hat{p}_i\\).\nFor the speech semantic decoder, we propose a visual guided speech synthesis module. If only text content is considered, it is hard to synthesis the speech which is synchronized with video. The variation, style, and duration of the speech are difficult to estimate through text. In the talking-face scenario, the speaker's mouth movement provides visual guidance for speech synthesis. As shown in Fig. 5, the proposed visual-guided speech synthesis module consists of a lip encoder, a content encoder [33], a style encoder [33], a duration aligner [34], and an audio generator. In a certain time, we employ the expression coefficient sequence of the received semantics \\(\\zeta_{T_v} = [\\delta_{exp1}, \\delta_{exp2}, ..., \\delta_{expT_v}]\\) and the text sequence \\(\\mathcal{L}_{T_l} = [l_1,l_2,..., l_{T_l}]\\) to guide the synchronous synthesis of speech \\(s_T\\), where \\(T\\) is the length of received expression coefficients, \\(T_l\\) is the number of words in the received text, and \\(T_m\\) is the duration of the generated speech. The data of these three modalities all have the same start and end timestamps to ensure temporal alignment. Specifically, the text sequence received \\(\\mathcal{L}_{T_l}\\) is encoded to a series of phoneme embeddings \\(\\alpha = [a_1, a_2, ..., a_{T_l}]\\) by the content encoder. Meanwhile, the expression coefficients sequence \\(\\zeta_{T_v}\\) is encoded to lip embeddings \\(E_{lip}\\) by the lip encoder. We employ a MLP to encode the expression semantics, which extracts the short-term relationship between lip motions. Then, to align the feature representations of text and facial expression, the duration aligner is used to learn the synchronous context of lip movement and phoneme. After positional encoding added to lip and phoneme embeddings, Feed-Forward-Transformer (FFT) Blocks and Multi-head Attention are used to learn the context of input features. Specifically, phoneme embeddings are the query and \\(\\alpha\\) is the key and the value of Scaled Dot-Product Attention [32]. Therefore, the context sequence \\(E_{lip-text}\\) of phoneme and lip embeddings is given by:\n\\[E_{lip-text} = Attention(E_{lip}, a, a)\\]\\[= Softmax(\\frac{E_{lip} a^T}{\\sqrt{d}}) a\\]\\[= Aa \u2208 \\mathbb{R}^{T_v\u00d7d},\\]\nwhere A is learnable attention weights matrix, \\(d\\) is the dimension of context. The context sequence \\(E_{lip-text}\\) needs to be extended to the length of generated speech. According to"}, {"title": "E. Loss function", "content": "Due to the fact that the trained model is used as proposed semantic encoders, further training of the parameters \\(\\alpha_v\\) and \\(\\alpha_s\\) is not required. The parameters of the rest of the proposed system need to be trained."}, {"title": "IV. NUMERICAL RESULTS", "content": "In this section, we show the numerical results of the proposed semantic system."}, {"title": "A. Simulation Configuration", "content": "1) Datasets: For training and testing video semantic coding, packet coding and image generator, we use the VoxCeleb dataset [40] as the transmission source. All videos in the VoxCeleb dataset are cropped to 256\u00d7256, which only contain facial parts. There are a total of 17913 training videos and 514 testing videos, which contain about 500 speakers. Due to the lack of alignment of speech and text in the VoxCeleb dataset, to train and test the visual-guided speech synthesis module, the Chem dataset [35] is used as the transmission source of speech and text. We crop the videos in the Chem dataset based on subtitle files to obtain sentence-level video clips. Then, the video clips without faces are removed. There are a total of 9734 training video clips and 1500 testing sentence-level video clips.\n2) Hyperparameter settings: In our proposed synchronous semantic communication system, the code rate of video packet coding can be expressed as \\(cr = \\frac{L}{N_{pv}} = \\frac{M\u00d7D_p}{N_{pv}\u00d7D_v} = [\\frac{n_p^2}{N_{pv}D_v}]\u00d7[\\frac{M\u00d7D_p}{n_p^2}] = [\\frac{N_{pv}D_v}{N_{pv}D_v}]\\). Therefore, we have set different parameters {\\(cr, N_{pv}, n_p, D_v\\)} for different encoding rates, such as {\\( \\frac{1}{2},4,2,8\\)} and {\\( \\frac{1}{4}, 4, 2, 16\\)}. For text packet loss concealment, we set Npl = 6 to balance the performance and delay. For the loss function, we set \\(\\lambda_1 = \\lambda_2 = 0.5\\), \\(\\lambda_w = 2.5\\), \\(\\lambda_r = \\lambda_s = 4\\) and \\(\\lambda_p = \\lambda_e = \\lambda_m = 1\\) to train the proposed model.\n3) Baseline: We have considered end-to-end transmission scenarios in networks with a certain probability of packet loss. For source coding, H.264, H.265 and AV1 are used to encode video, Advanced Audio Coding (AAC), Opus are used to encode speech, and Huffman coding is used to encode the text as the conventional system. The keypoint-based method, FOM, used in existing semantic communication systems [19] is the baseline for comparing video performance. Text-to-speech methods, Tacotron2 [41] and Fastspeech2 [36], of existing semantic communication systems [42] are baselines to compare speech performance. For packet-level forward correction of data packets, Reed solomon (RS) code in GF(28) is a commonly used method in real-time video communication."}, {"title": "B. Evaluation Metrics", "content": "The synchronous transmission of video and speech can be described as three tasks: facial image transmission task, speech transmission task, and synchronization task. Therefore, we use the transmission quality indicators of three tasks as evaluation metrics.\n1) Facial Image Transmission Task: The visual quality of reconstructed images can serve as a standard for measuring transmission performance. For facial image transimission, the Structural Similarity Index Measure (SSIM) [43], the Learned Perceptual Image Patch Similarity (LPIPS) [44] and the Deep Image Structure and Texture Similarity (DISTS) [45] are utilized as metrics to measure the similarity of images between the transimter and the receiver. A lower value of LPIPS or DISTS indicates better reconstruction quality, while a higher value of SSIM indicates better quality.\n2) Speech Transmission Task: In this task, to compare the performance of the rate distortion with conventional methods, we use UTMOS [46] and Perceptual Evaluation of Speech Quality (PESQ) [47] as quality indicators. UTMOS is an effective model to predict the mean opinion score (MOS) indicator to evaluate the auditory quality of audio. PESQ is one of the universal objective quality indicators. To compare the quality of text-to-speech methods, we exploit the similarity of Mel spectrograms between the reconstructed speech and ground truth as the semantic similarity. Mel Cepstral Distortion (MCD) [48] is utilized as the metric, which measures the distance of Mel Frequency Cepstral Coefficient (MFCC) vectors.\nA lower value of MCD represents a better quality of speech synthesis. The Structural Similarity Index (SSIM), MFCC Cosine Similarity (mfccCOS) and Fr\u00e9chet Inception Distance based on MFCC (mfccFID) in [49] are used to evaluate the synthesis performance of speech in various dimensions.\n3) Synchronization Task: To measure the synchronization between the reconstructed video and speech, Lip Sync Error Distance (LSE-D) and Lip Sync Error Confidence (LSE-C) [50] are utilized as the metrics. LSE-D calculates the average error based on the distance between the lip and speech representations. LSE-C is the confidence score that the speech and the video are synchronized with a certain time offset. Specifically, the confidence score of a synchronization error is the difference between the minimum and the median of the Euclidean distances. A lower value of LSE-D and a higher value of LSE-C represent better synchronization."}, {"title": "C. Performance of Proposed Semantic Coding", "content": "The performance versus Bpp of the proposed video semantic coding method and baselines tested on the Voxceleb dataset are shown in Tabel. I. Our method only transmits 16 floating point numbers per frame. Similarly to the setting of the semantic communication system in [19], the reference image serves as a shared knowledge base between the transmitter and receiver, and its data are not included in the calculation of the bits per pixel (bpp). We compare the FOM-based video semantic encoding methods in SVC [19] and set the number of keypoints to 8 and the quantization of float16 with 0.0039 bpp. Under similar bandwidths, our method achieves an improvement of 0.035 in SSIM metrics and 0.051 in LPIPS performance in the Voxceleb dataset.\nCompared with traditional methods, the H.264 with 0.0244 bbp, H.265 with 0.0119 bbp and AV1 with 0.0077 bpp methods have similar LPIPS performance in the Voxceleb dataset. However, the SSIM performance of traditional methods is always higher than that of our method. We present visualizations of different methods in Fig. 6. Fig. 6(a) and Fig. 6(b) are the ground truth of the I-frame and P-frame, while Fig. 6(c), Fig. 6(d), and Fig. 6(e) are the predicted encoding results from I-frames to P-frames. As shown in Fig. 6(c), the frame coded by H.265 appear blocky visually, and our deep learning-based method has advantages in visual quality. For the FOM method, as shown in Fig. 6(d) and Fig. 6(e), our method has good visual performance when the character's head rotates. Therefore, our video semantic coding method can save approximately 84% and 67% bandwidth compared to H.264 and H.265, respectively. Moreover, compared to the AV1 method, it can save 49% bandwidth."}, {"title": "E. Performance of Proposed Packet Coding", "content": "The erasure channel is considered to train and test the effectiveness of our video packet-level coding method. Data packets are either perfectly received or completely lost in the network. The packet-level coding module is trained with a fixed packet loss probability, and we test the robustness with the method in networks under different packet loss probabilities.\nThe traditional packet-level FEC method has also been compared. We simulate the RS code in GF(28) with a source length of 32 bytes based on RFC5510. H.264 and H.265 adopt the IPPP mode and encode with Group of Pictures (GOP) as an encoding block. We set the size of GOP for H.264 and H.265 to be consistent with our method at different encoding rates. Our models are trained with a packet loss rate of 0.4 in both datasets. Specifically, if packet loss makes H.264 or H.265 unable to decode, the LPIPS of the video is set to be 1. As shown in Fig. 8, we compare the LPIPS performance of the H.264 and H.265 source encoding with RS packet-level coding under different coderates in Voxceleb and Chem dataset. For Voxceleb dataset with multiple speakers, when cr = \\( \\frac{1}{2}\\), H.264+RS and H.265+RS can repair all lost packets when p \u2264 0.3. When cr = \\( \\frac{1}{4}\\), the LPIPS performance of H.264+RS, H.265+RS"}]}