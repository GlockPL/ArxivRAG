{"title": "Physics-informed Neural Networks for Functional Differential Equations: Cylindrical Approximation and Its Convergence Guarantees", "authors": ["Taiki Miyagawa", "Takeru Yokota"], "abstract": "We propose the first learning scheme for functional differential equations (FDEs). FDEs play a fundamental role in physics, mathematics, and optimal control. However, the numerical analysis of FDEs has faced challenges due to its unrealistic computational costs and has been a long standing problem over decades. Thus, numerical approximations of FDEs have been developed, but they often oversimplify the solutions. To tackle these two issues, we propose a hybrid approach combining physics-informed neural networks (PINNs) with the cylindrical approximation. The cylindrical approximation expands functions and functional derivatives with an orthonormal basis and transforms FDEs into high-dimensional PDEs. To validate the reliability of the cylindrical approximation for FDE applications, we prove the convergence theorems of approximated functional derivatives and solutions. Then, the derived high-dimensional PDEs are numerically solved with PINNs. Through the capabilities of PINNs, our approach can handle a broader class of functional derivatives more efficiently than conventional discretization-based methods, improving the scalability of the cylindrical approximation. As a proof of concept, we conduct experiments on two FDEs and demonstrate that our model can successfully achieve typical L\u00b9 relative error orders of PINNs ~ 10-3. Overall, our work provides a strong backbone for physicists, mathematicians, and machine learning experts to analyze previously challenging FDEs, thereby democratizing their numerical analysis, which has received limited attention. Code is available at https://github.com/TaikiMiyagawa/FunctionalPINN.", "sections": [{"title": "1 Introduction", "content": "Functional differential equations (FDEs) appear in a wide variety of research areas [91, 92, 79]. FDEs are partial differential equations (PDEs) involving functional derivatives, where a functional F is a function of an input function \\( \\theta(x) \\) to a real number, i.e., \\( F : \\theta \\mapsto F([\\theta]) \\in \\mathbb{R} \\), and a functional derivative is defined as the derivative of functional w.r.t. the input function at x, denoted by \\( \\frac{\\delta F([\\theta])}{\\delta \\theta(x)} \\). FDEs play a fundamental role in Fokker-Planck systems [27], turbulence theory"}, {"title": "2 Related Work", "content": "FDEs are prevalent across numerous research fields [27, 24, 32, 16, 18, 81, 67, 71]. Research on FDEs has mainly focused on their theoretical aspects and formal solutions, with very few algorithms available to numerically solve general FDEs [18, 79, 91, 104]. In [18], a numerical method specialized for the Hamilton-Jacobi functional equation for optimal control problems in density space is proposed, based on spacetime discretization. Similarly, [79] employs spacetime discretization with tensor decomposition. The state-of-the-art algorithm proposed in [91], the CP-ALS (Canonical Polyadic tensor expansion & Alternating Least Squares) algorithm, uses the cylindrical approximation along with the finite difference method and tensor decomposition, requiring O(m\u00b3) (see App. F.2 for the derivation), whereas our model requires only \\( O(m^2) \\) (r is 1 or 2 in most FDEs). Furthermore, our model does not require such discretization, making it mesh-free. See App. B for an additional introduction to FDEs and their approximations.\nThe cylindrical approximation originates from the theory of stochastic processes [34, 88]. It is reminiscent of the spectral method for PDEs [66] and is a generalization to FDEs. Convergence theorems of the cylindrical approximation are summarized in a recent seminal paper [92]. Note that the cylindrical approximation in this paper (Eq. (2)) is different from the one in [92], tailored for practical use. Consequently, our convergence theorems also differ from those in [92]. See App. C.4.2 for technical details. See App. A for more comparisons with other studies."}, {"title": "3 Proposed Approach", "content": "We first introduce the cylindrical approximation of functionals, functional derivatives, and FDEs, beginning with the expansion of input functions and culminating in the transformation of FDEs into high-dimensional PDEs. Additionally, we prove the convergence theorems for this approximation. The rigorous mathematical background is reviewed in App. C for interested readers."}, {"title": "3.1 Step 1: Cylindrical Approximation", "content": "Firstly, we define the cylindrical approximation of functionals [7, 29, 34, 88]. Any function \\( \\theta \\) in a real separable Hilbert space \\( \\mathcal{H} \\) can be represented uniquely in terms of an orthonormal basis \\( {\\phi_k}_{k=0}^{ \\infty } \\) as \\( \\theta(x) = \\sum_{k=0}^{ \\infty } a_k \\phi_k(x) \\), where \\( a_k := (\\theta,\\phi_k)_{ \\mathcal{H} } \\) are the coefficients (or spectrum) of \\( \\theta \\) in terms of \\( {\\phi_k}_{k>0} \\), and \\( (\\cdot,\\cdot)_{ \\mathcal{H} } \\) denotes the inner product of \\( \\mathcal{H} \\). Substituting this expansion to functional \\( F([\\theta]) \\), we can define a multivariable function \\( f({a_k}_{k=0}^{ \\infty }) := F([\\sum_{k=0}^{ \\infty } a_k \\phi_k]) \\) for any functional \\( F: \\mathcal{H} \\rightarrow \\mathbb{R} \\). Truncating k at \\( m-1 \\in \\mathbb{Z}_{>0} \\) gives the cylindrical approximation of functionals:\n\\[ f({a_k}_{k=0}^{m-1}) := F([P_m \\theta]), \\qquad\\qquad (1) \\]\nwhere \\( P_m \\) is the projection operator s.t. \\( P_m \\theta(x) := \\sum_{k=0}^{m-1} a_k \\phi_k(x) \\), and m is referred to as the degree of approximation. See Thm. C.19 and Thm. C.20 for the uniform convergence and convergence rate of this approximation, originally given by [75, 92].\nSecondly, we define the cylindrical approximation of functional derivatives. The functional derivative of F w.r.t. \\( \\theta \\) at x is defined as \\( \\frac{\\delta F([\\theta])}{\\delta \\theta(x)} := \\lim_{ \\epsilon \\rightarrow 0} \\frac{F([\\theta(y)+\\epsilon \\delta(x-y)])-F([\\theta(y)])}{ \\epsilon } \\), where \\( \\delta(x) \\) denotes the Dirac delta function. This definition is impractical to simulate on computers with spacetime discretization; thus, we employ the expansion \\( \\frac{\\delta F([\\theta])}{\\delta \\theta(x)} = \\sum_{k=0}^{ \\infty } (\\frac{\\delta F([\\theta])}{\\delta \\theta},\\phi_k)_{ \\mathcal{H} }\\phi_k(x) \\). The expansion is possible because \\( \\frac{\\delta F([\\theta])}{\\delta \\theta} \\) itself is a function of x in \\( \\mathcal{H} \\) and thus can be represented as an orthonormal basis expansion. Note that the expansion coefficients \\( (\\frac{\\delta F([\\theta])}{\\delta \\theta},\\phi_k)_{ \\mathcal{H} } \\) are known to be equal to \\( \\frac{\\partial f}{\\partial a_k} \\) (see App. C.4.2 for the proof). Hence, truncating \\( \\infty \\) at \\( m-1 \\) gives the cylindrical approximation of functional derivatives:\n\\[ P_m \\frac{\\delta F([P_m \\theta])}{\\delta \\theta(x)} = \\sum_{k=0}^{m-1} \\frac{\\partial f}{\\partial a_k} \\phi_k(x). \\qquad (2) \\]\nNote that Eq. (2) is different from the cylindrical approximation adopted in [91, 92]. They do not apply \\( P_m \\) to \\( \\frac{\\delta F([P_m \\theta])}{\\delta \\theta(x)} \\), and the emerging \u201ctail term\u201d \\( \\sum_{k=m}^{ \\infty } (\\frac{\\delta F([\\theta])}{\\delta \\theta},\\phi_k)\\phi_k(x) \\) is simply ignored without any rationale.\nThe first main theoretical contribution of our work is the following convergence theorem of Eq. (2)."}, {"title": "3.2 Step 2: Solving Approximated FDEs with PINNS", "content": "We briefly introduce the foundation of PINNs [77]. PINNs are universal approximators and can solve PDEs. Let us consider a PDE \\( \\frac{df(t,x)}{dt} = \\mathcal{N}[f] \\) with an initial condition \\( \\mathcal{B}[f]|_{t=0} = 0 \\), where \\( t \\in [0,1] \\), \\( x \\in [-1,1] \\). \\( \\mathcal{N} \\) and \\( \\mathcal{B} \\) are operators defining the PDE and the initial condition, respectively. The PINN aims to approximate the solution \\( f(t,x) \\). Thus, the inputs to the PINN are t and x, randomly sampled from [0, 1] and [-1,1], respectively. Note that \\( (t=0,x) \\) with \\( x \\in [-1,1] \\) are also input"}, {"title": "4 Experiment", "content": "As a proof of concept for our approach, we numerically solve the FTE and BHE. These two FDEs are suitable for numerical experiments because their analytic solutions are available, allowing for the computation of absolute and relative errors, major metrics in the numerical analysis of PDEs and FDEs. Note that the analytic solutions for most FDEs are currently unknown due to their mathematical complexity.\nSetups. We use a 4-layer PINN with 3\u00d7 (linear + sin activation + layer normalization [6]) + last linear layer. The total loss function is the smooth L\u00b9 loss or the sum of L\u00b9 and \\( L^\\infty \\) losses. Softmax loss-reweighting is employed. The optimizer is AdamW [59]. The learning rate scheduler is the linear warmup with cosine annealing with warmup [58]. Latin hypercube sampling [64, 39] is used for the training, validation, and test sets. For the BHE, the sampling range is decayed quadratically in terms of \\( k \\in \\{ 0,1,..., m-1 \\} \\) to stabilize the training. We use L\u00b9 relative and absolute errors, standard performance metrics for numerical analysis of PDEs and FDEs. Absolute error is used instead of relative error when the analytic solution is close to zero because relative error in such a region blows up by definition, regardless of the model's prediction. \\( \\upsilon_0 \\), \\( \\rho_0 \\), \\( \\mu \\), and \\( \\sigma^2 \\) are set to 1, 1, 8, and 10, respectively. In App. G, we provide more detailed setups for reproducibility, including the range of \\( a_k \\)."}, {"title": "4.1 Result: Functional Transport Equation", "content": "Tab. 1 shows the main result.\u00b2 Our model achieves typical relative error orders of PINNs ~ 10\u22123, even when the degree is as large as 1000, which means our model's capability of representing \\( \\theta \\) and \\( \\delta F([\\theta], t)/\\delta \\theta(x) \\) as polynomials of degree 1000. This is a notable improvement from the state-of-the-art algorithm [91], which can handle \\( m \\leq 6 \\).\nFig. 4 visualizes the analytic solution, prediction, and absolute error of a model trained on the FTE with degree 100 under the linear initial condition. Note that some of the collocation points used for plotting Fig. 4 are not in the training set, as can be seen from Figs. 11\u201316 in App. H.3. This aspect highlights the model's ability to extrapolate beyond its training data (\\( a_k \\sim 0 \\).\nFig. 5 shows the absolute error of the first-order functional derivative estimated at t = 1 and \\( \\theta = 0 \\). Again, \\( \\theta = 0 \\) is not included in the training set. The errors in the top four panels increase at the edges of intervals (\\( x = \\pm 1 \\)) due to Runge's phenomenon [80]."}, {"title": "4.2 Result: Burgers-Hopf Equation", "content": "Tab. 2 shows the main result. Again, our model successfully achieves ~ 10\u00af\u00b3, the typical order of relative error of PINNs. See Fig. 2, App. H.6, and the footnote in Sec. 4.1 for the assessment of the theoretical convergence of the cylindrical approximation.\nFig. 6 shows the relative error of first-order functional derivatives at \\( \\theta = 0 \\). Note again that some of the collocation points used for this figure are not included in the training dataset, highlighting the model's ability to extrapolate beyond its training dataset (\\( a_k \\sim 0 \\)). Additionally, the error is reduced by a factor of 10-1 by incorporating a loss term corresponding to the identity \\( W([\\theta], t) = 0 \\) (bottom four panels).\nFig. 7 visualizes the analytic solution, prediction, and absolute error of a model trained on the BHE with degree 20 under the delta initial condition. The absolute error w.r.t. \\( a_0 \\) is 10-4 times smaller than the scale of the solution; i.e., the model learns \\( \\Theta \\) well in the direction of \\( a_0 \\), which dominates the analytic solution. Conversely, the absolute error w.r.t. \\( a_{19} \\) is on par with the scale of the solution. This result is anticipated because the dependence of the solution on \\( a_{19} \\) is much smaller than \\( a_0 \\). This relationship is evident from Eq. (9), which indicates that the higher degree terms decay exponentially in terms of k, l, and t, and the solution is dominated by \\( a_k \\) with \\( k \\leq 1 \\). Therefore, optimizing the model in the direction of \\( a_{19} \\) has only a negligible effect on minimizing the loss function."}, {"title": "5 Limitations", "content": "One limitation of our work is that the spacetime dimension is limited to 1 + 1 (t and x) in our experiments. However, generalization to 1 + d dimensions is feasible, albeit with additional computational costs. For d > 1 dimensional spaces, we have several options for expansion bases [79].\nAnother limitation is that the orders of functional derivatives in FDEs in our experiments are limited to r = 1. However, extending to r > 2 is straightforward. For instance, the cylindrical approximation of the second-order functional derivative is expressed as \\( \\sum_{k,l=0}^{m-1} \\frac{\\delta^2 f(a,t)}{\\delta a_k \\delta a_l} \\phi_k(x) \\phi_l(y) \\), which can be computed via backpropagation twice.\nFurthermore, this paper focuses exclusively on the abstract evolution equation. While this includes important FDEs (see Sec. 3), it does not cover certain equations, such as the Schwinger-Dyson equation or the Wetterich equation. Nonetheless, the mathematical foundations regarding the existence and uniqueness of these FDEs remain unestablished, which is beyond the scope of our paper. Once these foundations are defined, applying our model to these equations would be straightforward. More discussions on limitations, including technical ones, are provided in App. F.1."}]}