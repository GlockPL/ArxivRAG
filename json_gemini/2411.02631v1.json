{"title": "EXTRACTING UNLEARNED INFORMATION\nFROM LLMS WITH ACTIVATION STEERING", "authors": ["Atakan Seyito\u011flu", "Aleksei Kuvshinov", "Leo Schwinn", "Stephan G\u00fcnnemann"], "abstract": "An unintended consequence of the vast pretraining of Large Language Models\n(LLMs) is the verbatim memorization of fragments of their training data, which\nmay contain sensitive or copyrighted information. In recent years, unlearning has\nemerged as a solution to effectively remove sensitive knowledge from models af-\nter training. Yet, recent work has shown that supposedly deleted information can\nstill be extracted by malicious actors through various attacks. Still, current at-\ntacks retrieve sets of possible candidate generations and are unable to pinpoint\nthe output that contains the actual target information. We propose activation\nsteering as a method for exact information retrieval from unlearned LLMs. We\nintroduce a novel approach to generating steering vectors, named Anonymized\nActivation Steering. Additionally, we develop a simple word frequency method\nto pinpoint the correct answer among a set of candidates when retrieving un-\nlearned information. Our evaluation across multiple unlearning techniques and\ndatasets demonstrates that activation steering successfully recovers general knowl-\nedge (e.g., widely known fictional characters) while revealing limitations in re-\ntrieving specific information (e.g., details about non-public individuals). Overall,\nour results demonstrate that exact information retrieval from unlearned models is\npossible, highlighting a severe vulnerability of current unlearning techniques.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language Models (LLMs) are trained on vast amounts of text data curated from diverse\nsources. The extensive training process enables LLMs to generate high-quality responses to a wide\nrange of topics. However, an unintended consequence of this approach is the verbatim memorization\nof fragments of their training data, which may contain sensitive information. Here, the scale of the\ntraining data prevents the reliable identification and removal of all instances of private or copyrighted\ninformation, such as personal addresses, passages from copyrighted books, or proprietary code snip-\npets. Regulations, such as the EU's General Data Protection Regulation (GDPR) (Voigt & Von dem\nBussche, 2017), aim to give individuals control over their personal information through measures,\nsuch as the \"Right to erasure\u201d, requiring companies to delete user data upon request (Zhang et al.,\n2023a). However, as retraining large models from scratch is impractical to delete user information,\nunlearning has emerged as an alternative solution to effectively remove knowledge from models\nwhile preserving their overall performance (Maini et al., 2024).\nEvaluating the success of an unlearning method is a difficult task. Even if the model does not directly\nrespond correctly to questions about the unlearned topic, it doesn't necessarily mean the concept is\nfully forgotten. Several existing approaches aim to recover unlearned or deleted information (Lynch\net al., 2024). Existing works consider an attack successful if the correct answer is present among\nthe candidates but are unable to pinpoint the answer containing the correct information (Patil et al.,\n2024; Schwinn et al., 2024). In contrast, we argue that for real-world applications, information\nretrieval systems should incorporate scoring mechanisms that account for the accuracy of selecting\nthe correct answer, which results in a more accurate assessment of leakage risk during deployment.\nOur contributions in this paper are as follows:\n\u2022 We introduce a novel attack based on activation steering (see \u00a72) against LLM unlearning.\nTo this end, we deploy a novel way of generating pairs of prompts to calculate activation\ndifferences - Anonymized Activation Steering.\n\u2022 We evaluate our approach on three unlearning methods and three different datasets. Con-\ntrary to existing attacks, we demonstrate that our proposed approach can retrieve unlearned\ninformation in an exact manner pinpointing the correct answer among a set of candidates\nwith high accuracy.\n\u2022 We investigate failure scenarios for our approach and find that while exact information\nretrieval is successful on general knowledge (i.e., models unlearned on Harry Potter) it\nfails for specific, less known information.\n\u2022 We provide a new dataset based on existing work (Schwinn et al., 2024) for Harry Potter\ninformation retrieval, enabling more accurate assessments of unlearning methods.\nIn this work, we demonstrate the power of activation steering as the evaluation tool for targeted\nunlearning of LLMs. Our results highlight its effectiveness for broad topics, such as removing\ncopyright-related information, while revealing its limitations when applied to more specific knowl-\nedge. By exploring these strengths and constraints, we provide deeper insights into how activation\nsteering affects the behavior of unlearned LLMs, contributing to the development of safe LLMs."}, {"title": "2 RELATED WORK", "content": "LLM unlearning is a popular research topic due to its efficiency compared to retraining from scratch.\nEldan & Russinovich (2023) make a model forget the entire Harry Potter franchise, while Li et al.\n(2024) unlearn harmful information in biology, chemistry, and cyber-security domains. Apart from\nunlearning entire domains of information, methods exist that solve the privacy issue of LLMs, delet-\ning personal information at request (Jang et al., 2023; Wu et al., 2023). Meng et al. (2022) replace\nmore specific information from an LLM, unlearning a single sentence at a time.\nTo effectively test unlearning methods, a clear benchmark is essential. In this context, Maini et al.\n(2024) introduce the TOFU dataset, which consists of synthetic data about fictitious authors. Be-\ncause the data is artificial, it allows for direct comparison between a model trained on this dataset\nthat has undergone unlearning, against a baseline model, that is guaranteed to never have seen this\ndataset during training. Similarly, Li et al. (2024) provide the WMDP dataset, designed specifi-\ncally for benchmarking unlearning techniques. This dataset contains harmful information, enabling\nevaluation of a model's ability to effectively forget potentially dangerous content.\nSimilar to retrieving information from an unlearned model, researchers perform attacks on standard\nmodels that refuse to answer harmful prompts. A lot of research in this area is in a \u201cjailbreak\u201d\nsetting where the model already knows the information but refuses to answer (Chu et al., 2024).\nShi et al. (2024) identify if a given data was seen during training to recover private information.\nAttacking an unlearned model is similar, as the goal is to recover information, but in this case, the\nmodel does not refuse to answer but outputs seemingly random answers or replaced information\nsuch as in unlearning done with ROME method by Meng et al. (2022). Patil et al. (2024) show that\nROME and other unlearning methods such as MEMIT (Meng et al., 2023) are prone to attacks, and\nthe information is not truly deleted.\nActivation steering is a technique for manipulating LLMs' latent space and guiding the generated\nresponses in the desired direction. This method is applied to overcome refused prompts (Arditi et al.,\n2024; Rimsky et al., 2024), reduce toxicity (Turner et al., 2023), enhance truthfulness (Li et al.,\n2023a), and adjust the tone of responses (von R\u00fctte et al., 2024) in LLMs. It works by calculating\na \"steering vector\" that reflects the target direction, derived from pairs of inputs that differ based on\nthe intended direction of behavior (e.g., toxic and non-toxic pairs). The steering vector is obtained\nthrough simple subtraction or more advanced techniques like Principal Component Analysis (PCA)\nor Logistic Regression (Tigges et al., 2023). This vector is then used during generation to steer the\nmodel's output towards or away from the desired direction."}, {"title": "3 \u0391\u039d\u039f\u039d\u03a5MIZED ACTIVATION (ANONACT) STEERING", "content": "We aim to extract information from an unlearned model about the unlearning topic by asking ques-\ntions. We ask straightforward questions that have specific keywords as their correct answers. Our\nobjective is to develop a method that functions without prior knowledge of the unlearning topic\n(such as finetuning the model on a particular area and querying about others) and reliably deter-\nmines whether the model's response is accurate.\nFor the base, non-unlearned model, this task of determining the correct answer is trivial, as the most\nfrequent response is typically correct if the model possesses the necessary knowledge in the first\nplace. In contrast, for the unlearned model, information leakage occasionally makes the correct\nanswer appear among the sampled responses, though at a much lower frequency. If the correct\nanswer frequency (CAF) is increased to become the most common response, the user can simply\nchoose the most likely answer, effectively undoing the unlearning process. For this purpose of\nincreasing the CAF, we propose Anonymized Activation (AnonAct) Steering."}, {"title": "3.2 \u0391\u039d\u039fNACT", "content": "We employ a standard activation steering scheme from literature (Arditi et al., 2024; Rimsky et al.,\n2024) with two distinct types of prompts. Our novel contribution to generating these pairs of prompts\nis the anonymization of sentences.\nFor a given question Q, we generate multiple anonymized versions. We do it manually for simple\nentities like character first names and using a large language model, GPT-4, to anonymize more\ncomplex terms, like places or institutions. The goal of anonymization is to create prompts close to the original question but without any relation\nto the unlearned domain. This way, the differences between their activations give us the direction of\nthe unlearned domain. Our method is the first to suggest this type of generalized anonymization to\ncreate the difference vectors. Rimsky et al. (2024) use contrastive pairs (such as Yes/No), and Arditi\net al. (2024) use completely different prompts to generate the steering vectors.\nAfter creating the anonymized questions, we follow established methods in the literature for activa-\ntion steering mainly from Arditi et al. (2024). Specifically, we extract the internal representations\nbetween layers of the LLM for each anonymized question. We then com-\npute the difference between these representations and those of the corresponding original questions.\nBy averaging these differences across all anonymized questions, we obtain the steering vector for\nthat layer. During generation, this steering vector is added back with a scaling factor, but only for the\ngeneration of the first token. We limit the application of the steering vector to the first token because\nthe internal model representations are captured at that stage. This approach is motivated by findings\nfrom previous research, which show that influencing the generation of the first token significantly\nimpacts the entire sequence (Zhang et al., 2023b)."}, {"title": "4 EXPERIMENTS", "content": "For the initial experiments, we use the WhoIsHarryPotter (Eldan & Russinovich, 2023) model,\nwhich unlearns all Harry Potter-specific knowledge using sources related to the Harry Potter fran-\nchise (books, articles, news posts). It is based on the Llama-2-Chat model, which is a fine-tuned\nLlama2 (Touvron et al., 2023) for dialog use cases.\nFurthermore, to evaluate our approach on other unlearning methods, we use the base model (that\nwas finetuned on the dataset) and codebase authors of TOFU provide to unlearn information from a\nmodel. Their model was based on Phi-1.5 (Li et al., 2023b). For ROME (Meng et al., 2022), we use\nGPT2-XL (Radford et al., 2019) and the code the authors provide to unlearn single facts one by one."}, {"title": "4.2 DATASETS", "content": "For the Harry Potter unlearning experiments, we curate a dataset of 62 questions using GPT4 by\nbuilding upon an existing dataset by Schwinn et al. (2024). The dataset is in the format of a Q&A\nwith simple \"What\u201d or \u201cWho\u201d questions. We specifically choose questions that are easily answered\nby a base model (Llama2 7B) to ensure that unlearning is the reason for incorrect answers and not\nthe lack of knowledge in the base model.\nFor the TOFU dataset (Maini et al., 2024), we utilize 40 questions provided by the authors, focusing\non two fictitious authors. These questions are originally designed with open-ended responses. To\nensure consistency with our other experiments, we transform this dataset into a keyword-based Q&A\nformat, similar to the Harry Potter dataset, by extracting key terms from the original answers. While\nthe original study uses ROUGE-L score (Lin & Och, 2004) to evaluate answer correctness, we use\nthe extracted keywords to denote an answer as correct.\nFor ROME Meng et al. (2022), we use 20 keyword-based questions from the CounterFact dataset\nprovided by the authors, for example, about a historical figure's nationality or mother tongue.\nTo anonymize the questions, we again use GPT4 to generate 5 to 25 suitable replacements for key-\nwords that carry information about the unlearned domain (names, places, fictitious beings) in the\nquestions. If a single question includes multiple anonymized keywords, we use all possible combi-\nnations to generate multiple anonymized prompts."}, {"title": "4.3 EXPERIMENT SETTING", "content": "For the experiments, we follow the work by Arditi et al. (2024) but apply it in an unlearning setting\ninstead of refusal of models. We create the input text using the prompt templates, system prompts,\nquestions, and answer starts, such that the first token to be selected by the model should be the\nfirst token of the correct keyword. We compute the internal representations (activations) between\nlayers during the generation of this first token. Then, we calculate the mean activations for the\nanonymized prompts we generated. Finally, we subtract these mean activations from the activations\nfor the original text to generate the steering vectors."}, {"title": "5 RESULTS & DISCUSSION", "content": "We conduct an initial sampling experiment using the Harry Potter dataset, applying a coefficient of 2\nand implementing our method just before the model's final layer. We apply the local AnonAct Steer-\ning as it more closely aligns with real-world applications. We\nobserve an increase in the CAFs for many questions, with some showing substantial improvements.\nHowever, we also acknowledge a slight decrease in performance for a small subset of questions. We\nnote that our objective is to increase the CAFs to the point where they become the most common.\nFor example, a keyword with a low frequency is still the most frequent if no other candidate keyword\nappears more often. Thus, just looking at the increase in CAFs is insufficient.\nTo better quantify the success of our method, we employ the simple \"most frequent keywords\"\napproach with RoC plots detailed above. We run this experiment in three settings: Base model,\nunlearned model, and unlearned model using our method. We observe that LLAMA2 gets an almost perfect score (0.98). At the same time,\nthe unlearned model has a score of 0.75, which is better than random, showing that information\nleakage already occurs even without any intervention. Our method sits in the middle of these two\nbut is closer to the base LLAMA2 model with a score of 0.92. The fact that our simple method\nfor determining the correct answer yields a high AUC score indicates that we effectively extract\nadditional information from the model that is supposed to be unlearned.\nNext, we conduct the same experiment on the TOFU model and dataset to evaluate how our method\ngeneralizes to other unlearning techniques. We assess whether the CAF increases for different un-\nlearning methods. This shows that our\nmethod does not generalize to the TOFU unlearning setting.\nTo test the generalization of our method further, we use another unlearning method, ROME. We no-\ntice that many of the answers in the CounterFact dataset consist of a single token. Therefore, instead\nof sampling, we use this subset of questions with single-token answers to plot the probabilities for\nthe top tokens. Note that ROME is different from the previous two\nmethods, as it replaces the information with another one for unlearning. Our method successfully\nchanges the prediction, leading the sampling away from the forced false token, bringing the distri-\nbution to a more uniform state. However, it fails to recover the original true answer: Although the\ncorrect answers are in the top tokens for the questions, this is because the given names\nin the questions are indicative of what the answer might be; not because the model recognizes the\nsubjects.\nOur experiments across the three unlearning schemes show that our method is effective in certain\ncases. We hypothesize that this variation stems from the scope of the subject that was unlearned. The\nkey difference between the successful Harry Potter case and the unsuccessful ones lies in the breadth\nof the subject matter. Harry Potter represents a large media universe, and unlearning it requires\nsevering connections between its many elements, making it difficult to retrieve related information\nfrom any single entity, such as a name. This unlinking effect is effectively mitigated by activation\nsteering with anonymization, allowing the model to restore conceptual associations. In contrast,\nthe TOFU setup involves a single author, and the task is to delete the information about a specific\nindividual. ROME targets even more granular data by removing and replacing a single fact."}, {"title": "6 CONCLUSION", "content": "In this work, we contribute to information retrieval from unlearned models. To this end, we propose\nactivation steering as a powerful method for this task.\nOur novel method employs activation steering in the unlearning context and shows great perfor-\nmance in recovering lost information from broad subjects, such as ones that were unlearned due\nto copyright. We also show and acknowledge the shortcomings of our method when applied to\nnarrower settings, such as more granular information deletion. We attribute this difference in per-\nformance to the way information is unlearned from models. We note that the broader setting, Harry\nPotter, has many links between the in-universe concepts, and activation steering provides a way to\nrecover these, while narrower topics need a different approach to retrieve unlearned information."}]}