{"title": "xAI-Drop: Don't Use What You Cannot Explain", "authors": ["Vincenzo Marco De Luca", "Antonio Longa", "Pietro Li\u00f2", "Andrea Passerini"], "abstract": "Graph Neural Networks (GNNs) have emerged as the predominant paradigm for learning from graph-structured data, offering a wide range of applications from social network analysis to bioinformatics. Despite their versatility, GNNs face challenges such as oversmoothing, lack of generalization and poor interpretability, which hinder their wider adoption and reliability in critical applications. Dropping has emerged as an effective paradigm for reducing noise during training and improving robustness of GNNs. However, existing approaches often rely on random or heuristic-based selection criteria, lacking a principled method to identify and exclude nodes that contribute to noise and over-complexity in the model. In this work, we argue that explainability should be a key indicator of a model's robustness throughout its training phase. To this end, we introduce xAI-Drop, a novel topological-level dropping regularizer that leverages explainability to pinpoint noisy network elements to be excluded from the GNN propagation mechanism. An empirical evaluation on diverse real-world datasets demonstrates that our method outperforms current state-of-the-art dropping approaches in accuracy, effectively reduces over-smoothing, and improves explanation quality.", "sections": [{"title": "1 Introduction", "content": "The capacity to effectively process networked data has a wide range of potential applications, including recommendation Systems [3], drug design [12], and urban intelligence [13]. Graph Neural Networks (GNN) [8,15,6,38] have emerged as a powerful and versatile paradigm to address multiple tasks involving networked data, from node and graph classification to link prediction and graph generation.\nDespite their effectiveness and popularity, GNNs face various challenges that prevent their wider adoption and reliability in critical applications, such as over-smoothing, lack of generalization and poor interpretability. Over-smoothing [28] occurs when the learned representations for the different nodes in a graph (approximately) collapse into a single representation shared by all nodes, due to the exponential growth of the receptive field with respect to the number of layers in the GNN [23]. Dropping [17] has emerged as an effective paradigm to mitigate oversmoothing and increase GNN robustness. Dropping can be performed at different granularities, from dropping single features [4] to dropping edges [27], nodes [3,24] or even entire paths [4]. However, existing approaches often rely on random or heuristic-based selection criteria, and lack a principled method to identify and exclude nodes that contribute to noise and over-complexity in the model.\nIn this paper we argue that explainability should be considered a first class citizen in determining which elements of the graph should be dropped in order to increase the robustness of the learned GNN. Consider a GNN being trained for node classification. Our intuition is that the fact that the prediction for a given node has a poor explanation is a symptom of a suboptimal function being learned, and that this symptom is more harmful if the prediction has a high-confidence. Guided by this intuition we present XAI-DROP, a novel topological-level dropping regularizer that leverages explainability and over-confidence to pinpoint noisy network elements to be excluded from the GNN propagation mechanism during each training epoch.\nAn empirical evaluation on diverse real-world datasets demonstrates that our method outperforms current state-of-the-art dropping approaches in accuracy, effectively reduces over-smoothing, and improves explanation quality. Our main contributions can be summarized as follows:\nWe identify local explainability during training as a driving principle to discard noisy information in the GNN learning process.\nWe introduce XAI-DROP, an explainability-guided dropping framework for GNN training.\nWe show how XAI-DROP consistently outperforms alternative dropping strategies as well as XAI-based regularization approaches on various node classification benchmarks.\nWe demonstrate the effectiveness of XAI-DROP in reducing oversmoothing and improving explanation quality.\nThe rest of the paper is organized as follows. We start by reviewing related work (Section 2) and then introduce the relevant background (Section 3). Our XAI-DROP framework is presented in Section 4 and experimentally evaluated in Section 5. Finally, conclusions are drawn in Section 6."}, {"title": "2 Related Work", "content": "Dropping. Dropping strategies are routinely employed in neural networks to prevent overfitting [33]. Specifically, dropping works by randomly setting to zero a proportion of neurons during training, reducing the capacity of the network and forcing it to learn more robust and generalized features. The rich structure information held in GNNs has motivated the extension of the dropping mechanism to the topological level, in order to alter the propagation of messages between neighboring nodes. The first approach being introduced, DropEdge [27], randomly drops edges according to a given Bernoulli.\nthis work, researchers have suggested random dropping of graph components at various granularity levels. For instance, DropGNN [24] randomly drops nodes together with all their connections. DropMessage [4] performs dropping operations directly on the propagated messages during the message-passing process. DropAGG [11] randomly chooses neighbouring nodes to participate in message aggregation. Lastly, DropPath [35] drops all connections in randomly selected paths. All these methods rely on random sampling to choose the component to drop. Alternative dropping strategies have been explored in the literature, where the components to drop are chosen according to various criteria. Learn2Drop [21] is a learnable graph sparsification procedure deciding which edges to drop to retain maximal similarity to the original network. Beta-Bernoulli Graph Drop Connect (BBGDC) [9] learns and adapts the drop rate of the edges during training based on a Beta-Bernoulli distribution. All these methods rely on random or heuristic-based selection criteria. In this work we show how a more principled XAI-based method to identify potentially harmful components substantially outperforms existing dropping strategies.\nPost-hoc explanability. Several works investigate post-hoc methods to explain the predictions of GNN models. GNN explainers can be categorized into model-level and instance-level explainers. Model-level explainers [1] aim at providing a global understanding of a trained model, e.g., as motifs or rules driving the model to predict a certain class. In contrast, instance-level explainers [19] aim at identifying components of a given input that are responsible for the model's prediction for that input, and are thus more appropriate to design an XAI-based dropping strategy. Instance-level explainers can be grouped into five categories[14]: decomposition, surrogate, gradient, perturbation and generation based. Decomposition-based methods break down the input to identify explanations [25]. Surrogate-based methods rely on an interpretable surrogate to explain the prediction of the original model [10,37]. Gradient-based methods define explanations in terms of the gradient of the network output with respect to the elements of the input graph [34,25]. Perturbation-based methods manipulate the input to obtain interpretable subgraphs [40,20], while generation-based methods generate subgraphs that can explain the model output [18]. By providing explainability scores for individual elements of the network, gradient-based methods are the best candidates for identifying potential elements to be dropped. In this work we used the saliency map approach [31] because of its computational efficiency, but the framework can be applied to any explainer producing node-level explainability scores.\nXAI-based regularization. A few approaches have been recently proposed to explicitly introduce XAI-based regularization strategies during the training stage of GNNs. MATE [32] applies an optimization procedure via meta-learning to enhance explainability of the resulting model. ExPASS [7] works at the message passing level by weighting messages with the importance of nodes as defined by PGExplainer [20], while ENGAGE [30] directly removes low score nodes. These methods however fail to consider the quality of the explanation and are heavily parameterized, resulting in substantial computational overhead, learn-"}, {"title": "3 Preliminaries", "content": "In this section, we provide an overview of the fundamental concepts underlying our approach.\nGraph. A graph is a tuple $G = (V,E, X_V, X_\\epsilon)$, where V is a set of vertices or nodes, $\\epsilon$ is a set of edges between the nodes, $X_V$ and $X_\\epsilon$ are node features and edge features, respectively. Node and edge features may be empty.\nThe set of edges $\\epsilon$ can be represented as an adjacent matrix $A \\in R^{|V|\\times|V|}$, where $A_{ij} = 1$ if $(v_i, v_j) \\in \\epsilon$, 0 otherwise. In this paper we will focus on undirected graphs, in which edges have no directions, i.e., $A_{ij} = A_{ji}$. Given $v \\in V$, the set $N_v = {u \\in V : (u, v) \\in \\epsilon}$ denotes the neighborhood of v in G.\nGraph Neural Network (GNN). A GNN is a class of neural network architecture specifically designed to process graph data [29,22,16]. A GNN leverages a message-passing scheme to propagate information across nodes in a graph. GNNs iteratively learn node representations $h_v^l$ by aggregating information from neighboring nodes. Specifically, the l-th layer of a message-passing GNN is:\n$h_v^l = \\varphi^{(l)}(h_v^{(l-1)}, AGG {\\phi^{(l)}(h_v^{(l-1)}, h_v^{(l-1)}) : u \\in N_v})$\nwhere $\\phi^{(l)}$ is as differentiable functions that computes the message from u to v, AGG is an aggregation operator combining messages sent to v, and $\\varphi^{(l)}$ is a differentiable function combining the learned representation of v with the incoming messages. After l iterations, the vector $h_v^l$ contains both the structural information and the content of the l-hop neighborhood of node v. With an adequate number of iterations, these node representation vectors can be used for classifying both individual nodes (written as $f_v(G)$) and the entire graph (f(G)), by plugging a learnable function (typically an MLP) on top of node embeddings or graph embeddings respectively, the latter being computed with a readout function aggregating node embeddings. In this paper we will focus on node classification, but the approach being developed can be easily adapted to deal with graph classification.\nIn most cases, the propagation mechanism for an entire layer can be represented more compactly using the adjacency matrix A, the node embedding matrix $H^{(l)}$ and one or more layer-specific weight matrices $W^{(l-1)}$. For instance, layerwise propagation in GCN [16] can be written as:\n$H^{(l)} = \\sigma (D^{-\\frac{1}{2}} \\tilde{A} D^{-\\frac{1}{2}} H^{(l-1)}W^{(l-1)})$,\nwhere $\\tilde{A} = A + I_{|v|}$ is the adjacency matrix enriched with self loops, $D_{ii} = \\sum_j \\tilde{A}_{ij}$ and $\\sigma$ is a non-linear activation function such ReLU or sigmoid.\nIn the following we will present dropping strategies in terms of modifications to the adjacency matrix A.\nDropping. Dropping strategies for GNNs can be categorized based on the granularity of the information being dropped. For the sake of compactness, we will formalize here the edge and node dropping strategies, that can both be represented in terms of operations performed on the adjacency matrix A.\nDROPEDGE [27] removes the connection between two nodes, and halts the propagation of the message between them, based on an edge dropping mask $B^{\\epsilon} \\in {0,1}^{|V|\\times|V|}$ defined as follows:\n$B_{ij}^{\\epsilon} \\sim Bernoulli(1 - p)$\nThe filtered adjacency matrix is computed as follows:\n$A' = A \\odot B^{\\epsilon}$\nwhere $\\odot$ is the Hadamard product.\nDROPNODE [5] acts at the node-level on the set of nodes V. It removes nodes from the node set V, and, as a consequence, its incident edges $I_v = {(u, w) \\in \\epsilon : u = v or w = v}$, based on a node dropping mask $b^V \\in {0,1}^{|V|}$ defined as follows:\n$b_i^V \\sim Bernoulli(1 - p)$\nThe filtered adjacency matrix is computed as follows:\n$A' = B^V A B^V$\nwhere $B^V$ is a diagonal matrix having the elements of $b_i^V$ on the main diagonal (and zero elsewhere).\nGNN explainability. Intuitively, given a graph G and a trained GNN f, an explanation is a subgraph $G_{exp} \\subset G$ that contains the information that is relevant for f to perform inference on G. We use $G_{exp}(v)$ to denote the local explanation for the GNN output for node v. In this study, we employ the saliency map method [31], an instance-based explainer that computes the attribution for each input by performing backpropagation to the input space. The general idea is that the magnitude of the derivative provides insights into the most influential features, which, when perturbed, result in the highest difference in the output space. Formally, it is defined as\n$G_{exp(v)} = \\frac{\\partial f_v(G)}{\\partial X_v}$\nWhere $f_v(G)$ is the prediction of the model for node v, and $X_v$ is the feature vector of node v."}, {"title": "4 Explainability-Based Dropping", "content": "XAI-DROP is based on the combination of two concepts: explainability and (over)confidence. On the one hand, a a poor local explanation can be seen as a symptom of an unreliable prediction for the corresponding node, making it a good candidate for being dropped to reduce noise during training. On the other hand, a highly confident prediction for a node indicates that the network is very confident about the features the prediction is based upon, that in principle should correspond to the local explanation. A confident prediction with a poor explanation is thus a combination one would like to avoid as much as possible. Building on these intuitions, XAI-DROP implements a dropping strategy that targets nodes with poor explanations and high certainty.\nFigure 1 presents a graphical representation of the XAI-DROP approach, which consists of two main phases: node selection and dropping. The node selection phase (further detailed in Section 4.1) consists of three steps of increasing complexity: an initial sample of nodes is selected at random; the most certain nodes in the sample are extracted as candidates for dropping; candidate nodes are then ranked according to their fidelity, and the fidelity score is turned into a dropping probability. In the second phase of the model, for each candidate node v we use its dropping probability p(v) to drop (some of) its connections (XAI-DROPEDGE) or the node itself (XAI-DROPNODE). In the former case, a fraction p(v) of the node edges is selected at random and removed, in the latter case the node itself (with all its edges) is removed with probability p(v). See Section 4.2 for the details.\n4.1 Node selection\nInitially, a fraction $\\alpha$ of the nodes is selected at random, producing the subset $V'$. This step is mostly done for computational efficiency, and it can be skipped if the graph is not too large. The candidate dropping set $V'' \\subset V'$ is then created by selecting the $\\beta$ most confident nodes in $V'$, with confidence computed as:\n$C(v) = max_y P(y|X_v)$\nwhere $X$ is the node features associated with the node v. For each $v \\in V''$ its local explanation $G_{exp}(v)$ is computed using the saliency map [31] method. We opted for saliency maps because they are inexpensive to compute and they do not require ground truth explanations, but the method is agnostic with respect to the explanation method being used. Nodes in $V''$ are then ranked in decreasing order of explanation quality, as measured by fidelity sufficiency (Eq. 7).\nThe next step consists in assigning dropping probabilities to the nodes in $V''$. Given a predefined dropping probability $\\rho$ (a hyper-parameter of the model), the idea is to adjust dropping probabilities for individual nodes according to their fidelity, without affecting the expected fraction of nodes to be selected for dropping. The dropping probability of node $v \\in V''$ is adjusted as:\n$\\rho(v) = \\rho + \\Delta_{\\rho}(v)$"}, {"title": "4.2 Dropping", "content": "where $\\Delta_{\\rho}(v)$ is an offset that depends on the ranking of v. It is computed as follows:\n$\\Delta_{\\rho}(v) = \\frac{2\\omega}{|V''|}i_v - \\omega$\nwhere $i_v \\in [0, |V''| - 1]$ is the position of node v in the ranked list, and $\\omega \\in [0, min(\\rho, 1 - \\rho)]$ is a hyper-parameter that controls the entity of the adjustment being introduced. This method ensures a linear distribution of probabilities across the nodes directly correlating with their ordered fidelity rankings. All nodes $u \\in V \\backslash V''$ that do not belong to the sampled set retain the default dropping probability, $\\Delta_{\\rho}(u) = 0$. Overall, this procedure guarantees that the expected number of nodes selected for dropping is equal to the predefined dropping probability $\\rho$.\nOnce the biased dropping probabilities $\\rho(v)$ have been computed, the propagation of the information can be altered to regularize the learning. Any of the random dropping strategies introduced in the literature can be adjusted to leverage the node-specific dropping probabilities $\\rho(v)$. In this manuscript we focus on the two coarser level strategies, namely drop edges (XAI-DROPEDGE) and drop nodes (XAI-DROPNODE), that were formalized in Section 3. Their XAI-DROP variants are described in the following.\nXAI-DROPEDGE. While in the random DROPEDGE strategy, all edges have the same probability $\\rho$ of being dropped, in the XAI-DROPEDGE strategy this probability depends on the dropping probability of the nodes they connect. More formally, we need to introduce an edge dropping mask $B^{\\epsilon}$ defined as follows:\n$B_{ij}^{\\epsilon} \\sim Bernoulli(1 - \\rho(v_i))$\nThe filtered adjacency matrix is computed according to Eq. 3. Notice that the filtered adjacency matrix will not be symmetric, even if the original graph was undirected.\nXAI-DROPNODE. This dropping strategy works exactly as random DROPNODE, with the generic node dropping probability $\\rho$ replaced by a node specific dropping probability $\\rho(v)$. More formally, we need to introduce a node dropping mask $b^V$ defined as follows:\n$b_i^V \\sim Bernoulli(1 - \\rho(v_i))$\nThe filtered adjacency matrix is computed according to Eq. 5."}, {"title": "4.3 Overall procedure", "content": "The overall algorithm for XAI-DROP is outlined in Algorithm 1. The algorithm takes as input a graph G, the GNN architecture to be trained f, and the hyperparameters $\\alpha$, $\\beta$, $\\rho$ and $\\omega$. In each epoch, the algorithm randomly selects a subset of a nodes from V. It then selects the top $\\beta$ nodes with highest prediction confidence, and computes their explainability according to the current version of f in terms of fidelity sufficiency. Fidelity sufficiency values (collectively indicated as Fsuf) are then used to determine the dropping probabilities. These probabilities can be applied to drop edges (XAI-DROPEDGE) or nodes (XAI-DROPNODE). Finally, the adjusted adjacency matrix $A'$ is used to further train the GNN f."}, {"title": "5 Experiments", "content": "Our experimental evaluation aims to address the following research questions:\nQ1: Does XAI-DROP outperform alternative dropping strategies?\nQ2: Does XAI-DROP outperform alternative xAI-driven strategies?\nQ3: Does XAI-DROP improves explainability?\nQ4: Does XAI-DROP help preventing oversmoothing?\nWe start by presenting the experimental setting and then discuss the results answering these questions.\n5.1 Experimental setting\nDatasets: We employed three widely used datasets for node classification: Cite- seer, Cora, and PubMed. Each dataset is composed of a single graph with thousands of labeled nodes. We utilize the publicly available train, validation, and test node splits [39]. Detailed dataset statistics are presented"}, {"title": "6 Conclusion", "content": "In this work we introduced a simple XAI-based regularization framework for GNN training that selects nodes with highly confident predictions but poor explanations as candidates for dropping. Our experimental evaluation clearly showed that the proposed framework outperforms alternative dropping strategies as well as other XAI-based regularization techniques in terms of accuracy, explainability and oversmoothing. These promising results highlight how encouraging high quality explanations at training time can effectively improve training dynamics. Future work include the exploration of the connection be-"}]}