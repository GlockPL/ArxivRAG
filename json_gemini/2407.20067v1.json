{"title": "xAI-Drop: Don't Use What You Cannot Explain", "authors": ["Vincenzo Marco De Luca", "Antonio Longa", "Pietro Li\u00f2", "Andrea Passerini"], "abstract": "Graph Neural Networks (GNNs) have emerged as the pre-dominant paradigm for learning from graph-structured data, offering a wide range of applications from social network analysis to bioinformatics. Despite their versatility, GNNs face challenges such as oversmoothing, lack of generalization and poor interpretability, which hinder their wider adoption and reliability in critical applications. Dropping has emerged as an effective paradigm for reducing noise during training and improving robustness of GNNs. However, existing approaches often rely on random or heuristic-based selection criteria, lacking a principled method to identify and exclude nodes that contribute to noise and over-complexity in the model. In this work, we argue that explainability should be a key indicator of a model's robustness throughout its training phase. To this end, we introduce xAI-Drop, a novel topological-level dropping regularizer that leverages explainability to pinpoint noisy network elements to be excluded from the GNN propagation mechanism. An empirical evaluation on diverse real-world datasets demonstrates that our method outperforms current state-of-the-art dropping approaches in accuracy, effectively reduces over-smoothing, and improves explanation quality.", "sections": [{"title": "1 Introduction", "content": "The capacity to effectively process networked data has a wide range of potential applications, including recommendation Systems [3], drug design [12], and urban intelligence [13]. Graph Neural Networks (GNN) [8,15,6,38] have emerged as a powerful and versatile paradigm to address multiple tasks involving networked data, from node and graph classification to link prediction and graph generation.\nDespite their effectiveness and popularity, GNNs face various challenges that prevent their wider adoption and reliability in critical applications, such as oversmoothing, lack of generalization and poor interpretability. Over-smoothing [28] occurs when the learned representations for the different nodes in a graph (approximately) collapse into a single representation shared by all nodes, due to the exponential growth of the receptive field with respect to the number of layers in the GNN [23]. Dropping [17] has emerged as an effective paradigm to mitigate"}, {"title": "2 Related Work", "content": "Dropping strategies are routinely employed in neural networks to prevent overfitting [33]. Specifically, dropping works by randomly setting to zero a proportion of neurons during training, reducing the capacity of the network and forcing it to learn more robust and generalized features. The rich structure information held in GNNs has motivated the extension of the dropping mecha-nism to the topological level, in order to alter the propagation of messages be-tween neighboring nodes. The first approach being introduced, DropEdge [27], randomly drops edges according to a given Bernoulli. Taking inspiration from"}, {"title": "3 Preliminaries", "content": "In this section, we provide an overview of the fundamental concepts underlying our approach.\nGraph. A graph is a tuple $G = (V,E, X_V, X_\\epsilon)$, where $V$ is a set of vertices or nodes, $\\epsilon$ is a set of edges between the nodes, $X_V$ and $X_\\epsilon$ are node features and edge features, respectively. Node and edge features may be empty.\nThe set of edges $\\epsilon$ can be represented as an adjacent matrix $A \\in R^{|V|\\times|V|}$, where $A_{ij} = 1$ if $(v_i, v_j) \\in \\epsilon$, 0 otherwise. In this paper we will focus on undirected graphs, in which edges have no directions, i.e., $A_{ij} = A_{ji}$. Given $v \\in V$, the set $N_v = {u \\in V : (u, v) \\in \\epsilon}$ denotes the neighborhood of $v$ in G.\nGraph Neural Network (GNN). A GNN is a class of neural network architec-ture specifically designed to process graph data [29,22,16]. A GNN leverages a message-passing scheme to propagate information across nodes in a graph. GNNs iteratively learn node representations $h_v$ by aggregating information from neigh-boring nodes. Specifically, the l-th layer of a message-passing GNN is:\n$h_v^{(l)} = \\varphi^{(l)}(h_v^{(l-1)}, AGG {\\phi^{(l)}(h_u^{(l-1)}, h_v^{(l-1)}) : u\\in N_v})$                                                                                                  (1)\nwhere $\\phi^{(l)}$ is as differentiable functions that computes the message from u to v, AGG is an aggregation operator combining messages sent to v, and $\\varphi^{(l)}$ is a differentiable function combining the learned representation of v with the in-coming messages. After l iterations, the vector $h_v^{(l)}$ contains both the structural information and the content of the l-hop neighborhood of node v. With an ad-equate number of iterations, these node representation vectors can be used for classifying both individual nodes (written as $f_v(G)$) and the entire graph (f(G)), by plugging a learnable function (typically an MLP) on top of node embeddings or graph embeddings respectively, the latter being computed with a readout function aggregating node embeddings. In this paper we will focus on node clas-sification, but the approach being developed can be easily adapted to deal with graph classification.\nIn most cases, the propagation mechanism for an entire layer can be rep-resented more compactly using the adjacency matrix A, the node embedding matrix $H^{(l)}$ and one or more layer-specific weight matrices $W^{(l-1)}$. For instance, layerwise propagation in GCN [16] can be written as:\n$H^{(l)} = \\sigma (\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l-1)}W^{(l-1)})$ \nwhere $\\tilde{A} = A+I_{|V|}$ is the adjacency matrix enriched with self loops, $\\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}$ and $\\sigma$ is a non-linear activation function such ReLU or sigmoid."}, {"title": "4 Explainability-Based Dropping", "content": "XAI-DROP is based on the combination of two concepts: explainability and (over)confidence. On the one hand, a a poor local explanation can be seen as a symptom of an unreliable prediction for the corresponding node, making it a good candidate for being dropped to reduce noise during training. On the other hand, a highly confident prediction for a node indicates that the network is very confident about the features the prediction is based upon, that in principle should correspond to the local explanation. A confident prediction with a poor explanation is thus a combination one would like to avoid as much as possible. Building on these intuitions, XAI-DROP implements a dropping strategy that targets nodes with poor explanations and high certainty.\nFigure 1 presents a graphical representation of the XAI-DROP approach, which consists of two main phases: node selection and dropping. The node selec-tion phase (further detailed in Section 4.1) consists of three steps of increasing complexity: an initial sample of nodes is selected at random; the most certain nodes in the sample are extracted as candidates for dropping; candidate nodes are then ranked according to their fidelity, and the fidelity score is turned into a dropping probability. In the second phase of the model, for each candidate node v we use its dropping probability p(v) to drop (some of) its connections (XAI-DROPEDGE) or the node itself (XAI-DROPNODE). In the former case, a fraction p(v) of the node edges is selected at random and removed, in the latter case the node itself (with all its edges) is removed with probability p(v). See Section 4.2 for the details."}, {"title": "4.1 Node selection", "content": "Initially, a fraction $\\alpha$ of the nodes is selected at random, producing the subset $V'$. This step is mostly done for computational efficiency, and it can be skipped if the graph is not too large. The candidate dropping set $V'' \\subset V'$ is then created by selecting the $\\beta$ most confident nodes in $V'$, with confidence computed as:\n$C(v) = max_yP(y|X_v)$(8)\nwhere X is the node features associated with the node v. For each $v \\in V''$ its local explanation $G_{exp(v)}$ is computed using the saliency map [31] method. We opted for saliency maps because they are inexpensive to compute and they do not require ground truth explanations, but the method is agnostic with respect to the explanation method being used. Nodes in $V''$ are then ranked in decreasing order of explanation quality, as measured by fidelity sufficiency (Eq. 7).\nThe next step consists in assigning dropping probabilities to the nodes in $V''$. Given a predefined dropping probability $\\rho$ (a hyper-parameter of the model), the idea is to adjust dropping probabilities for individual nodes according to their fidelity, without affecting the expected fraction of nodes to be selected for dropping. The dropping probability of node $v \\in V''$ is adjusted as:\n$p(v) = \\rho + \\Delta \\rho(v)$(9)"}, {"title": "4.2 Dropping", "content": "Once the biased dropping probabilities p(v) have been computed, the propa-gation of the information can be altered to regularize the learning. Any of the random dropping strategies introduced in the literature can be adjusted to lever-age the node-specific dropping probabilities p(v). In this manuscript we focus on the two coarser level strategies, namely drop edges (XAI-DROPEDGE) and drop nodes (XAI-DROPNODE), that were formalized in Section 3. Their XAI-DROP variants are described in the following.\nXAI-DROPEDGE. While in the random DROPEDGE strategy, all edges have the same probability $\\rho$ of being dropped, in the XAI-DROPEDGE strategy this probability depends on the dropping probability of the nodes they connect. More formally, we need to introduce an edge dropping mask $B^\\epsilon$ defined as follows:\n$B^\\epsilon_{ij} \\sim Bernoulli(1 - p(v_i))$ (11)\nThe filtered adjacency matrix is computed according to Eq. 3. Notice that the filtered adjacency matrix will not be symmetric, even if the original graph was undirected.\nXAI-DROPNODE. This dropping strategy works exactly as random DROPNODE, with the generic node dropping probability $\\rho$ replaced by a node specific dropping probability p(v). More formally, we need to introduce a node dropping mask $b^v$ defined as follows:\n$b^v_i \\sim Bernoulli(1 - p(v_i))$ (12)\nThe filtered adjacency matrix is computed according to Eq. 5."}, {"title": "4.3 Overall procedure", "content": "The overall algorithm for XAI-DROP is outlined in Algorithm 1. The algorithm takes as input a graph G, the GNN architecture to be trained f, and the hyper-parameters $\\alpha$, $\\beta$, $\\rho$ and w. In each epoch, the algorithm randomly selects a subset of a nodes from V. It then selects the top $\\beta$ nodes with highest prediction confi-dence, and computes their explainability according to the current version of f in terms of fidelity sufficiency. Fidelity sufficiency values (collectively indicated as Fsuf) are then used to determine the dropping probabilities. These probabilities can be applied to drop edges (XAI-DROPEDGE) or nodes (XAI-DROPNODE). Finally, the adjusted adjacency matrix A' is used to further train the GNN f."}, {"title": "5 Experiments", "content": "Our experimental evaluation aims to address the following research questions:\nQ1: Does XAI-DROP outperform alternative dropping strategies?\nQ2: Does XAI-DROP outperform alternative xAI-driven strategies?\nQ3: Does XAI-DROP improves explainability?\nQ4: Does XAI-DROP help preventing oversmoothing?\nWe start by presenting the experimental setting and then discuss the results answering these questions."}, {"title": "5.1 Experimental setting", "content": "Datasets: We employed three widely used datasets for node classification: Cite-seer, Cora, and PubMed. Each dataset is composed of a single graph with thou-sands of labeled nodes. We utilize the publicly available train, validation, and test node splits [39]. Detailed dataset statistics are presented in Table 1."}]}