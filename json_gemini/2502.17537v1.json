{"title": "On the Vulnerability of Concept Erasure in Diffusion Models", "authors": ["Lucas Beerens", "Alex D. Richardson", "Kaicheng Zhang", "Dongdong Chen"], "abstract": "The proliferation of text-to-image diffusion models has raised significant privacy and security concerns, particularly regarding the generation of copyrighted or harmful images. To address these issues, research on machine unlearning has developed various concept erasure methods, which aim to remove the effect of unwanted data through post-hoc training. However, we show these erasure techniques are vulnerable, where images of supposedly erased concepts can still be generated using adversarially crafted prompts. We introduce RECORD, a coordinate-descent-based algorithm that discovers prompts capable of eliciting the generation of erased content. We demonstrate that RECORD significantly beats the attack success rate of current state-of-the-art attack methods. Furthermore, our findings reveal that models subjected to concept erasure are more susceptible to adversarial attacks than previously anticipated, highlighting the urgency for more robust unlearning approaches. We open source all our code at https://github.com/LucasBeerens/RECORD.", "sections": [{"title": "1. Introduction", "content": "Denoising diffusion models have become the standard for text-to-image generative models (Yang et al., 2024). The widespread proliferation and adoption of this technology has many ethical and societal impacts (Bendel, 2023), including the accessibility to production of harmful (violent, gory, pornographic, etc.) content and ease of copyright violation. Many large datasets include harmful or copyrighted data and, due to their size, it is difficult to clean them. Even so, retraining existing large models on cleaned datasets is expensive - especially if the definition of undesired output changes (such as changes in copyright law), requiring several re-trains. This has motivated the development of machine unlearning (or concept erasure) methods (Xu et al., 2023), where trained models are fine tuned to 'forget' small subsets of the training data. As many text-to-image generators are trained once on very large datasets that often include harmful material, and then hosted for the public to use, the importance of being able to cheaply and effectively 'sanitize' models by unlearning undesired behavior is clear.\nWithin the literature of machine unlearning, there are several standard goals (Xu et al., 2023). Models are fine tuned to unlearn specific objects (such as churches or parachutes); artistic styles (such as Van Gogh or Rembrandt); specific images (such as starry night by Van Gogh) and harmful concepts (such as nudity or gore). In this work, we verify the robustness of a wide range of unlearned models (Fan et al., 2023; Gandikota et al., 2023a; Kumari et al., 2023; Lyu et al., 2024; Wu et al., 2024; Wu & Harandi, 2024; Zhang et al., 2024a;b), on a range of tasks - van Gogh painting style, the concept of nudity, and objects (church, parachute, garbage truck). These are chosen as they are widespread tasks in the literature, with respectively unlearned model weights available. To address unlearning copyrighted data, it is sufficient to use a style or concept as a proxy. For example, if an artist requested that their work be removed from a training set, would unlearning the artist style be sufficient?\nWe find that all unlearned models are vulnerable to text based adversarial attacks (Figure 1a). This vulnerability also shows that 'unlearning' is misleading - model weights still contain information derived from undesirable images; and as such are still capable of producing undesired behavior. Instead, unlearning methods force models to suppress behaviors, under a range of standard prompt inputs, which is a form of model misalignment (Carlini et al., 2023). We believe this could have important consequences for copyright law - unlearning doesn't remove information derived from copyrighted data from a model's weights. We also compare the behaviors of different unlearned models to the stable diffusion (Rombach et al., 2022) they are based on, to highlight some inherent vulnerabilities of diffusion models, and to gain insight into what concept erasure is actually doing.\nOur contributions:\n\u2022 We propose a novel algorithm RECORD to recall the unlearned behavior from any unlearned diffusion model. We find adversarial prompts that beat the current state-of-the-art by 10x on attack success rate.\n\u2022 By optimizing prompt embeddings from many random initializations, we explore this space and demonstrate that near any random prompt embedding, there exist adversarially recalling prompt embeddings.\n\u2022 We explore the behavior of recalled unlearned models by comparing semantically meaningful latent activations of their U-nets during inference. This suggests to us that unlearning algorithms do not erase concepts from the model weights, but instead deliberately misalign models to not generate specific concepts."}, {"title": "2. Background", "content": "2.1. Text-to-Image Diffusion Models\nDiffusion Models are a class of generative model that generate images by learning to reverse the forward diffusion process. Starting with a Gaussian noise xt ~ N(0,1), a trained denoiser, commonly U-Net (Ronneberger et al., 2015) or Vision Transformer (Dosovitskiy et al., 2020), iteratively denoises xt over the timestep t \u2208 [0,T] until a clear image xo is reached. To perform text-to-image generation, the denoiser is trained conditioned on text embeddings c = T(y), where y is some natural language prompt, T is a pre-trained text encoder, commonly CLIP (Radford et al., 2021) or BLIP (Li et al., 2022). By using datasets of text-image pairs, highly effective image denoisers can be trained that are conditioned on text input. In practice, many popular diffusion models (such as Stable Diffusion (Rombach et al., 2022)) perform the denoising on a latent space, where zt = E(xt), then zt is iteratively denoised to zo and then decoded to xo = D(zo). Here the latent space is defined by some image autoencoder, trained such that xo ~ D(E(xo)). To train a diffusion model, Gaussian noise e is iteratively added to zo to reach zt. The denoiser eo can then be trained by optimizing\nargmin Ez~E(x),t,e~N(0,1),c ||\u20ac \u2013 \u20aco (zt, t, c)||2, (1)\n\u03b8\nwhere eo is predicting the noise e that takes zt-1 to zt, effectively learning to reverse the Gaussian noise diffusion process."}, {"title": "2.2. Concept Erasure", "content": "The simplest and most effective way to prevent the generation of undesired content is to remove such images from the training dataset. However, this manual removal process is unfeasible and requires the model to be re-trained from scratch. Instead, much research effort focuses on model-editing-based approach, developing frameworks to finetune models in a post-hoc manner to remove or suppress undesired content while maintaining the quality of other generated images. Some prominent methods that we will adversarially recall from include: Erased Stable Diffusion (ESD) (Gandikota et al., 2023a), Concept Ablation (CA) (Kumari et al., 2023), EraseDiff (ED) (Wu et al., 2024), Forget-Me-Not (FMN) (Zhang et al., 2024a), Unified Concept Editing (UCE) (Gandikota et al., 2023b), ScissorHands (SH) (Wu & Harandi, 2024), Saliency Unlearning (SalUn) (Fan et al., 2023), Concept-SemiPermeable Membrane (SPM) (Lyu et al., 2024), and AdvUnlearn (Zhang et al., 2024b). Most of these methods (ESD, CA, ED, FMN, UCE, SH, SalUn, SPM) involve fine-tuning the denoising U-net of the diffusion model. The exception is AdvUnlearn, which fine tunes the text embedding network."}, {"title": "2.3. Prompt Tuning", "content": "Manipulating prompts to elicit specific behaviors from language models, also known as prompt tuning, is an important topic in Natural Language Processing research. Ebrahimi et al. (2018) introduced HotFlip, generating adversarial examples through minimal character-level flips guided by gradients. Extending this, Wallace et al. (2021) developed Universal Adversarial Triggers-input-agnostic token sequences optimized by using a first order taylor-expansion around the current token to select candidate tokens for exact evaluation. Shin et al. (2020) presented AutoPrompt, which uses this algorithm to automatically generate prompts for various use cases. Addressing the lack of fluency in these prompts, Shi et al. (2022) introduced FluentPrompt, incorporating fluency constraints and using Langevin Dynamics combined with Projected Stochastic Gradient Descent, where projection is done onto the set of token embeddings.\nWen et al. (2023) developed PEZ, an algorithm inspired by FluentPrompt. It allows for prompt creation in both text-to-text and text-to-image applications. In text-to-image models, Gal et al. (2022) applied Textual Inversion, learning \"pseudo-words\" in the embedding space to capture specific visual concepts. Further advancements include GBDA (Guo et al., 2021), enabling gradient-based optimization over token distributions using the Gumbel-Softmax reparametrization (Jang et al., 2017) to stay on the probability simplex, and ARCA (Jones et al., 2023), optimizing discrete prompts via an improvement to AutoPrompt. ARCA will inspire our method."}, {"title": "2.4. Concept Restoration", "content": "Recently methods have been developed that aim to restore concepts intentionally unlearned from models by leveraging advanced optimization techniques inspired by prompt tuning. Concept Inversion (CI) (Pham et al., 2023) introduces a new token with a learnable embedding to represent the erased concept, optimized by minimizing reconstruction loss during denoising-directly applying Textual Inversion from prompt tuning to the concept restoration paradigm (Gal et al., 2022). Prompting Unlearned Diffusion Models (PUND) (Han et al., 2024) enhances this approach by iteratively erasing and searching for the concept by also updating model parameters, improving transferability across models.\nOther methods focus on discrete token optimization. UnlearnDiffAtk(UD) (Zhang et al., 2023) performs optimization over token distributions, similar to GBDA (Guo et al., 2021), but utilizes projection onto the probability simplex instead of the Gumbel-Softmax reparametrization. Prompting4Debugging (P4D) (Chin et al., 2023) optimizes prompts in the embedding space and projects onto discrete embeddings, akin to the approach used in PEZ (Wen et al., 2023). Additionally, Ring-A-Bell (Tsai et al., 2023) constructs an empirical representation of the erased concept by averaging embedding differences from prompts with and without the concept, then employs a genetic algorithm to optimize the prompt.\nThese methods demonstrate that optimization techniques from prompt tuning-both in continuous embeddings and discrete token spaces can be used for algorithms to circumvent concept unlearning."}, {"title": "3. Method", "content": "3.1. Motivation: Vulnerability of Machine Unlearning\nVulnerability Verifying that a model has truly unlearned a concept is challenging. We consider an unlearned model\u00b9 \u03b5\u03b8\u03b9 to be robust if it consistently fails to generate the erased content, even when subjected to any adversarial prompt. In this work we demonstrate that all unlearned models are not robust, rather they are vulnerable. We construct a loss function (Chin et al., 2023) that, when minimized, directly breaks this robustness definition:\nL(y) = Et,z [||eo (zt, t, T(y)) \u2013 eo (zt, t, T (Ytarget) ||2] (2)\nwhere zt is obtained through the forward diffusion process with zo~ Ptarget coming from the target data distribution,\n\u00b9With 'unlearned' we mean a trained model that has been fine tuned to unlearn something, not an untrained model."}, {"title": "3.2. RECORD", "content": "We introduce RECORD (Recalling Erased Concepts via Coordinate Descent), an algorithm for evaluating and challenging the robustness of concept erasure methods. RECORD searches for adversarial prompts that cause unlearned models to generate images containing erased concepts, thereby revealing any residual concept information retained after unlearning. Given an original model 0, an unlearned model \u03b8', and a target prompt Ytarget representing the erased concept, our goal is to find an adversarial prompt y* that minimizes Equation 2. Let L(y) denote this expectation for a prompt y with optimal prompt\ny* = argmin L(y). (4)\ny\nSince computing the exact expectation over all latents and timesteps is intractable, we approximate L(y) by sampling batches:\nL(y, Z) = \u03a3t,z [||eo (zt, t, T(y))\u2013\u20aco (zt, t, T (Ytarget) ||2] (5)\nwhere Z is a set of noised images and their corresponding timesteps. We maintain a reference set R of noised images and timesteps to consistently evaluate optimization progress. The key challenge in optimizing this objective lies in the discrete nature of the token sequence space and the non-differentiable mapping from tokens to embeddings. Standard gradient-based methods are not directly applicable. Instead, we employ coordinate descent, iteratively optimizing one token at a time while keeping others fixed.\nRECORD begins by initializing a random token sequence y = [y1,..., ys] of length S. To iteratively refine this sequence, we perform N passes over y. In each pass, we generate a random permutation of the token positions to mitigate positional bias during updates. For each position s in the permuted sequence, the algorithm samples a batch of K images z(n) and corresponding timesteps t(n,s). Candidate tokens for position s are then selected using a gradient-based approximation method. These candidates are evaluated using a predefined objective function, and if any candidate yields an improvement according to the evaluation metric, ys is updated. This iterative process continues for all positions in the permutation and repeats for N passes, progressively enhancing the token sequence over time. A pseudocode can be found in Algorithm 1.\nTo efficiently select candidate tokens at each position, we use an approximation strategy. We sample J random tokens and compute gradients with respect to their embeddings:\ngj = vc; L(y(cj, s), Z(n, s)). (6)\nThe average gradient g = \u03a3j=1 gj provides a linear approximation of how the loss changes with different token embeddings. By multiplying the embedding table E with g, we efficiently score all possible tokens and select the top C candidates for exact evaluation.\nTo ensure stable optimization in the discrete token space,\nL(y(c*, s), R) < \u00ce(y, R). (7)\nSince each accepted token replacement strictly decreases the loss and the number of possible token sequences is finite, the algorithm is guaranteed to converge to a coordinate-wise local minimum."}, {"title": "4. Experiments", "content": "We compare RECORD against UD (Zhang et al., 2023) and P4D (Chin et al., 2023) on diffusion models unlearned with a large variety of methods (ESD (Gandikota et al., 2023a), ED (Wu et al., 2024), SH (Wu & Harandi, 2024), FMN (Zhang et al., 2024a), CA (Kumari et al., 2023), SPM (Lyu et al., 2024), SalUn (Fan et al., 2023), UCE (Gandikota et al., 2023b) and AdvUnlearn (Zhang et al., 2024b)). All the unlearned models we consider were unlearned from the baseline of a trained Stable Diffusion 1.4 (Rombach et al., 2022) text-to-image generator, which we just refer to as the 'baseline'. The erased concepts include art style (Van Gogh), objects (church, garbage truck, parachute) and nudity, and the weights of the unlearned models are obtained from (Zhang et al., 2024b). We run each attack on one Nvidia H100 GPU. We also analyze the behavior of recalled models to gain insight into how they are vulnerable. Note that not all unlearning techniques have been developed on all tasks.\n4.1. Evaluation metrics\nIt is common in the concept erasure literature to use a large variety of classifier models from different sources to evaluate the attack methods, as no one classifier is trained on all the erased concepts. However we alternatively use a single zero-shot diffusion classifier (Stable Diffusion v2.1) (Li et al., 2023; Clark & Jaini, 2023) to classify the generated image. The classification results are obtained by computing\ny* = argmin Et ||\u20ac \u2013 \u20ac9(zt, t, T(Yi))||2, (8)\nYi EY\nwhere t ~ U(0,T), Y = {y1, Y2, ..., Yn} is a set of n pre-specified classification classes, and the expectation is computed over 10 samples, which for our experiments is sufficient to provide good classification accuracies. For art style and object attacks, we build sets of 50 classes using prompt templates \u2018a painting in the style of {artist_name}' and 'a photorealistic image of {object}', where the artist names are randomly chosen from a list of famous painters, e.g. Leonardo Da Vinci, and object names are from the classification classes of YOLOv3 (Redmon & Farhadi, 2018). For nudity attacks, a set of 4 classes are built with the same template as the object attacks. For all attacks, we also additionally add one empty class, '', which empirically can help the classifier capture images that fall significantly outside the distributions of the specified classes. All results presented in this section use 500 images generated by the corresponding models and attacks. For all the various methods and models we report the Attack Success Rate (ASR) in percentage. This is calculated by dividing the number of images classified as the target by the total number of generated images.\n4.2. Prompt Attacks\nWe evaluate RECORD by comparing it with UD (Zhang et al., 2023) and P4D (Chin et al., 2023). For a fair comparison, we give each method 64 tokens to optimize. Each method starts with a random prompt, except UD. Since UD optimizes on a token distribution, we give it a uniform distribution for all tokens except the first few that are initialized to be the target prompt. Without this type of initialization UD would not be able to get any significant performance.\nFor RECORD, we use N = 20 passes through the token list. We use batches of 1 image each. During the candidate selection, we make use of J = 64 samples. The chosen candidate set has size K = 64. These numbers are chosen to best use the available VRAM.\nEach of the three methods are used to created 50 adversarial prompts. Every prompt is used to create 10 images for a total of 500 images per method to be used for ASR calculations."}, {"title": "4.3. Prompt embedding manifolds", "content": "To assess the vulnerability of unlearned U-nets, we sample the space of prompt embeddings to find where these models are vulnerable (Figure 3). We consider just ESD here, but find that all the other models behave the same. We also include the embedding optimization process on the baseline model for comparison. We perform a 2D isomap projection (Figure 3a) to visualize how trajectories of prompt embedding optimizations behave. We also include histograms of distance metrics between: initial and target (Figure 3b); optimized and target (Figure 3c); and initial and target positions (Figure 3d). Initial conditions are sampled 'close' to the target prompt of 'a painting in the style of van Gogh', by padding the target prompt with a random length of random characters. Initial conditions 'far' from the target were generated by uniformly randomly sampling characters. These classes of initial conditions separate into clusters of the latent space, but there is no discernible difference here between baseline and unlearned models. In any region of the prompt embedding space we sample, there exists a nearby adversarial prompt embedding, suggesting that unlearned models are vulnerable to small perturbations of prompt embeddings almost everywhere. Initial prompt embeddings of unlearned models that start near the target prompt embedding, move away from the target prompt embedding during embedding optimization. However this is less pronounced in the baseline model.\nWhen RECORD optimizations are projected onto this same space (Figure 3), we see that they take much larger jumps than the continuous embedding optimization. We also find that, although initialized randomly, RECORD stays far away from the target prompt, despite taking large jumps over the space. This shows that unlearned models are also vulnerable to prompts that have little relation to the target prompt. We distinguish between ESD (Gandikota et al., 2023a) and AdvUnlearn (Zhang et al., 2024b) because AdvUnlearn has unlearned text encoder weights, whereas ESD has unlearned U-net weights. We find little difference in behavior."}, {"title": "4.4. Semantic Latents", "content": "To further explore the vulnerability of unlearned methods, we compare the behavior of recalled unlearned models and the baseline stable diffusion on the generation of van Gogh paintings. The bottleneck of the U-nets used for T2I diffusion models learn to represent a latent space with semantic meaning (Surkov et al., 2024; Park et al., 2023). We borrow the nomenclature from literature (Surkov et al., 2024) where 'mid.0' refers to the bottleneck of the U-net, and 'down.2.1' is the final downsampling block before this bottleneck.\nBy comparing the trajectories (during inference) of the activations in this semantic latent space, we can assess whether a recalled unlearned model is 'close' to the baseline with the target prompt 2. Most of the unlearning methods we compare work by modifying the weights near the U-net bottleneck, so inspecting the latent space defined by the activations at this region is reasonable. We find that recalled unlearned models have trajectories that are as 'far away' as unrelated prompts in this space from the baseline (Figure 2a-d). The fact that these trajectories still succeed in generating paintings in the style of van Gogh suggests that unlearning does not remove information from the U-net, but instead transforms how the U-net responds to text embeddings. Furthermore, we find that unlearned models with the target prompt produce trajectories much closer to the baseline-target prompt (Figure 2e-h), despite not rendering van Gogh paintings. This indicates that RECORD finds prompts that take far away trajectories in this semantic latent space, but they still yield van Gogh paintings. This highlights the inherent complexity of such spaces, and suggests that unlearning requires a better understanding of how these models store semantic meaning."}, {"title": "5. Discussion", "content": "We have demonstrated that existing machine unlearning methods do not erase the desired concept from diffusion models. We note that not all unlearning methods perform equally, in particular Scissorhands (Wu & Harandi, 2024) is the hardest to recall from, although it is still possible. As text-to-image generators proliferate and become ubiquitous, it is crucial that we understand how to minimize their misuse. Unfortunately the current paradigm of post-hoc unlearning harmful concepts from models trained on vast datasets is flawed.\nAlthough we have demonstrated and explored how machine unlearning is currently vulnerable, we have not yet provided a solution. We suspect that robust unlearning would require far greater understanding of exactly how diffusion models store information, which in turn leads to problems in interpretability research. Work on semantic guidance during inference (Park et al., 2023; Surkov et al., 2024) could provide tools for isolating specific concepts in models, however there is no guarantee this would yield robust unlearning mechanisms.\nOur methods have a few important weaknesses worth discussing. Firstly we obviously white-box access, which limits the utility of RECORD for recalling unlearned text-to-image generators that one has black-box access to. Second is computational cost - with the limits of VRAM on Nvidia H100, we could only run RECORD with a batch size of 1."}, {"title": "6. Conclusions", "content": "Existing machine unlearning algorithms do not remove information from diffusion models. We have demonstrated, with a range of techniques, that these unlearned models can always be recalled to produce images of the so called erased concept. The behavior of recalled unlearned models when assessed during optimization of the recalling prompt/embedding, and during inference, suggests that these vulnerabilities are widespread and more serious than previously thought."}]}