{"title": "Variables are a Curse in Software Vulnerability Prediction", "authors": ["Jinghua Groppe", "Sven Groppe", "Ralf M\u00f6ller"], "abstract": "Deep learning-based approaches for software vulnerability prediction currently mainly rely on the original text of software code as the feature of nodes in the graph of code and thus could learn a representation that is only specific to the code text, rather than the representation that depicts the 'intrinsic' functionality of a program hidden in the text representation. One curse that causes this problem is an infinite number of possibilities to name a variable. In order to lift the curse, in this work we introduce a new type of edge called name dependence, a type of abstract syntax graph based on the name dependence, and an efficient node representation method named 3-property encoding scheme. These techniques will allow us to remove the concrete variable names from code, and facilitate deep learning models to learn the functionality of software hidden in diverse code expressions. The experimental results show that the deep learning models built on these techniques outperform the ones based on existing approaches not only in the prediction of vulnerabilities but also in the memory need. The factor of memory usage reductions of our techniques can be up to the order of 30,000 in comparison to existing approaches.", "sections": [{"title": "1 Introduction", "content": "A number of efforts have been dedicated to applying deep learning (DL) to predict the vulnerabilities of software code. However, DL-based approaches have not achieved significant breakthroughs in this field and still have a limited capability to distinguish vulnerable code from non-vulnerable one [1]. Currently, DL approaches, both unstructured [2, 5, 8, 11] or structure-based [1, 6, 7, 9, 10], borrowed the method used in the natural language processing to define the semantics of the full code or nodes in a code graph. The full code or a piece of the code is considered plain text like a natural language and it is first split into tokens, and each token is represented by a real-valued vector called embedding. Unstructured approaches learn the representation of the code only based on the"}, {"title": "2 Breaking the Curse of Variables", "content": "In order to help DL models of software vulnerability prediction to improve their generalization ability, in this section, we suggest techniques of how to transform an infinite number of text representations of varable names into a finite number."}, {"title": "2.1 Name Dependence and Abstract Syntax Graph", "content": "In programming languages, a variable is related to its declaration (which is either explicitly given or implied). We can determine this relation by the name of the variable. Software engineering uses the term 'dependence' to describe the relations between two components, like data dependence, and control dependence. To align with it, we define a new kind of dependence called name dependence to express the relation between a variable and its declaration. In an AST with full information, the name dependence between two nodes can be inferred by the names of variables and identifiers. When we remove the names of variables and identifiers from the AST, we lose the information on name dependence. Without the information, we will not be able to restore the semantics of the original code. So, we need a way to express the name dependence when names are absent. A solution is to add an edge of name dependence between two related nodes. After adding such edges, the tree structure turns into a graph structure as illustrated"}, {"title": "2.2 3-Property Encoding Scheme", "content": "Apart from the ASG, we further suggest a method to efficiently represent the nodes in a code graph, 3-property encoding, which provides a consistent description of the feature of nodes and allows DL models to infer the commons and differences between nodes easily. This 3-property encoding is developed in the context of our ASG but it can be applied to other code graphs and it is also programming languages agnostic.\nIn a code graph, every node represents an executable syntactic construct in code, which can be an expression, a statement or its constituent parts, like variables and constants (which are of course also executable). Currently, the piece of code that consists of the construct (with or without a notation to the construct like 'varDecl' and 'add') is used as the feature of the node. The feature is encoded by first splitting the piece of code into tokens and then averaging the embeddings of all the tokens. The code-based encoding uses the original piece of code to present the feature of a node, and at the same time, the result of encoding blurs the semantics of the original code since the averaging operation. Our 3-property encoding avoids these two issues by introducing additional information related to the language constructs.\nEach language construct has its properties, which may not explicitly appear in the raw code text. Independent of specific programming languages, we found that it is enough to use three properties to describe different constructs: class, name and type of data if any, and each value of the properties will be represented by a unique token. With this property-based approach, we can encode all nodes in a consistent way, and this is a very valuable characteristic for many applications. So far, this 3-property encoding has not removed the diversity of text representations and we will further normalize this"}, {"title": "3 Evaluation", "content": "In order to evaluate our techniques, we build four types of code graphs, AST, AST+, ASG and ASG+, for training DL models of software vulnerability prediction. AST+ is an AST extended with flow and data dependencies and control flow. ASG is an AST with the edges of name dependence and variable names removed, and ASG+ is ASG with flow and data dependencies and control flow.\nModels: We develop two models (3propASG and 3propASG+), which use our graph structures and 3-property node encoding scheme, and two baselines (codeAST and codeAST+), which adopt the common graph structures and the pieces of code as the feature of nodes (i.e., the code-based encoding presented in Section 2) that is currently adopted by existing models [1,10]. All models share the following architecture: the input data is delivered to the layer of GGRU with one time step, the least expensive option. The output of GGRU is sent to each of three 1D convolution (Conv1d) layers with 128 filters each and perceptive fields of 1, 2, and 3 respectively, and one 1D max pooling (MaxPoolld) is applied over the output of each Conv1D to perform downsample. The results of the MaxPoolld layers are concatenated together and sent to the hidden layer with 128 neurons, and a 25% dropout is applied to the output of convolution and the hidden layer. We apply Relu for non-linear transformation and the embeddings of 100 dimensions to encode tokens.\nDatasets: We use several real-world datasets from different open-source projects: Chromium+Debian [1], which contains 10,699 samples and 7.05% of which are flawed; FFmpeg+Quemu [10] with 13,428 samples and 43.68% flawed; VDISC [8] with 68,398 samples and 46.38% flawed. The tool Joern\u00b9 is utilized to create the AST and AST+ from source code and our AST+ corresponds the code property graph of Joern."}, {"title": "4 Conclusions", "content": "In order to break the curse of variables, we introduce the edges of name dependence and ASG extending AST with this new type of edges and suggest a 3-property node encoding scheme based on the ASG. These techniques not only allow us to represent the semantics of code without using its variable names but also allow us to encode all nodes in a consistent way. The evaluation shows that our techniques do improve the abilities of DL models to predict software vulnerabilities. Furthermore, we also believe that the 3-property encoding will be also a useful technique for many tasks in software analysis and software engineering."}]}