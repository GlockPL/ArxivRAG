{"title": "Towards Efficient Convolutional Neural Networks with Structured Ternary Patterns", "authors": ["Christos Kyrkou"], "abstract": "High efficiency deep learning models are necessary to facilitate their use in devices with limited resources but also to improve resources required for training. Convolutional neural networks typically exert severe demands on local device resources and this conventionally limits their adoption within mobile and embedded platforms. This paper presents work towards utilizing static convolutional filters generated from the space of local binary patterns and Haar features to design efficient convolutional neural network architectures. These are referred to as Structured Ternary Patterns (STEP) and can be generated during network initialization in a systematic way instead of having learnable weight parameters thus reducing the total weight updates. The ternary values require significantly less storage and with the appropriate low level implementation, can also lead to inference improvements. The proposed approach is validated using four image classification datasets, demonstrating that common network backbones can be made more efficient and provide competitive results. It is also demonstrated that it is possible to generate completely custom STeP-based networks that provide good trade-offs for on-device applications such as Unmanned Aerial Vehicle based aerial vehicle detection. The experimental results show that the proposed method maintains high detection accuracy while reducing the trainable parameters by 40-80%. This work motivates further research towards good priors for non-learnable weights that can make deep learning architectures more efficient without having to alter the network during or after training.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning (DL) and artificial intelligence (AI) are rapidly being deployed in on-device scenarios, such as autonomous driving, robotic systems, and unmanned aerial vehicles [1]. This proliferation can be attributed to several factors, including privacy concerns, the need for real-time decision-making, and the necessity to address connectivity limitations. Despite the adoption of deep learning models in mobile applications, achieving consistently high-performance inference on mobile and embedded platforms remains a formidable challenge, primarily due to the substantial com-putational requirements involved. In pursuit of enabling on-device AI capabilities, researchers are constantly seeking strategies aimed at enhancing the computational efficiency of inference processes [2]."}, {"title": "II. RELATED WORK", "content": "Works such as [4] demonstrate a methodology that utilizes full-precision latent weights during the training process to ac-cumulate gradients. Subsequently, these accumulated gradients are employed to compute the corresponding binary weights. Building upon this foundation, other studies have extended the exploration of various properties, including scaling factors, for binarized networks [5]. Binary weight neural networks exhibit an advantageous compression ratio; however, they often incur a substantial sacrifice in accuracy when compared to baseline full-precision networks. Techniques like those introduced in [6] and [7] necessitate additional training for weight quantization, typically restricting weights to fixed-point representations. Alternatively, rather than training a full-precision network followed by quantization/binarization, alternative research avenues delve into the design of binary architectures. For instance, the work in [8] dissects convo-lutional kernels into binary basis functions, generating fil-ters as a linear combination of orthogonal binary codes in real-time. This transformation in computation, however, does not alleviate the count of learnable parameters. In a similar fashion, [9] introduces local binary convolutional networks that replace traditional convolutional layers with a learnable module inspired by local binary patterns. This approach neces-sitates learning pivot point locations and incorporates a non-linear thresholding operation, which can influence the flow of gradients. Similarly, [10] implements the local binary operator as a sampling operation. This involves learning the points to generate multiple output channels, each equipped with its own binary output code. Finally, [11] demonstrates the benefits of software-hardware codesign for extremely sparse networks with binary connections for image classification. Such accel-erators can also be utilized for the proposed work as well. Despite the promising advantages that binary networks offer over their full-precision counterparts, vanilla binary networks frequently suffer from substantial accuracy degradation, as highlighted in [4]. Furthermore, existing schemes fail to reduce the overall number of learnable parameters and necessitate gradient tracking for all parameters, as pointed out in [12]."}, {"title": "B. Quantization of Neural Networks", "content": "Quantization, in general, is a method employed to map input values from a large, often continuous set to output values within a smaller, often finite set. Examples of quantization techniques include rounding and truncation [3]. Within the context of neural networks, quantization plays a pivotal role in enabling the conversion of network weights and activa-tion functions from floating-point operations to fixed-point or mixed-precision operations. This transition not only re-duces the model's storage footprint and memory consump-tion but also enhances both latency and power efficiency [13]. Traditionally, uniform quantization, where all layers of the network share the same bit-width, has been utilized [14]. More recently, adaptive deployment strategies, which involve varying the bitwidth of weights and intermediate activations, have emerged as an alternative approach [15]. [16] demonstrates optimal bitwidth assignment for weight and activation quantization of deep convolutional neural networks using dynamic programming. Similarly, [17] focuses on image super-resolution applications using dynamic thresholds for quantization. Ternary quantized networks have been demon-strated for federated learning settings in [18]. It is worth noting that while quantization techniques offer substantial benefits, they are often accompanied by a noticeable decline in model accuracy, necessitating re-training in many cases [19]. Nevertheless, these techniques can be applied independently of the weight and architectural modifications proposed in this paper, potentially yielding further improvements. This synergy is elaborated further in Section IV-B."}, {"title": "C. Efficient Neural Networks", "content": "One critical aspect of designing efficient neural networks pertains to their architectural considerations. A multitude of works have tackled this challenge, with a primary focus on architectural and layer modifications aimed at optimizing the size and speed of deep learning models [20]. Notably, experts in the field have sought to engineer lightweight, parameter-efficient networks, introducing significant changes at both the micro- and macro- architecture levels [21]. These architectural adjustments encompass various techniques, such as the de-composition of convolutional layers, as exemplified by Wang et al. [22], who advocated splitting a 3 \u00d7 3 kernel into two consecutive layers employing 1 \u00d7 3 and 3 \u00d7 1 kernels, re-spectively. Another innovation, demonstrated by Koonce et al. [23], involves the utilization of \"fire modules\u201d that employ 1x1 convolutions to compress parameters efficiently. Additionally, the introduction of \"bottleneck\" blocks has gained attention, as seen in MobileNetV2 [24], which employs two convolutional layers with a 1 \u00d7 1 kernel to reduce the dimensionality of features processed by larger filters (e.g., 3 \u00d7 3). This concept is further refined in Ma et al.'s work [25], where convolutional layers are decomposed into a sequence of smaller, independent layers using grouped convolutions. EfficientNet, introduced by Tan et al. [26], stands as a notable convolutional neural net-work architecture and scaling method. It adopts a compound coefficient to uniformly scale the depth, width, and resolution dimensions of the network, thereby achieving efficiency across"}, {"title": "III. PROPOSED APPROACH", "content": "The proposed approach involves the generation of convo-lutional filter weights based on structured patterns that are known to exhibit exceptional performance in computer vision tasks, such as in the domains of face and pedestrian detection [28], [29]. The utilization of predetermined binary/ternary filters offers several notable advantages, primarily the po-tential reduction of memory and computational requirements during both training and inference phases. Specifically, this approach leverages two well-established pattern extractors: Local Binary Patterns (LBPs) [30], [31] and Haar-like features [32], [33]. LBPs have proven their versatility across a broad spectrum of applications for constructing robust visual object detection systems. In this research, Center-Symmetric Local Binary Patterns (CS-LBP) [30], [34] are utilized, which exhibit enhanced resilience to flat regions and a heightened ability to capture gradient information compared to the original LBP formulation (as employed in [9]), all without necessitating the use of any thresholds. Moreover, the approach introduces the incorporation of Haar-based features, which had a seminal impact on real-time object detection [32]. This integration represents a novel exploration within this context. Herein, these two distinct feature families are synergistically employed as the foundation for generating convolutional kernels that possess specific structural patterns. Notably, these kernels only necessitate ternary representation values of [-1,0,1], thus making them especially efficient in terms of computation and memory, requiring only addition/subtraction for the ternary weight kernels.\nOther approaches, such as [35], share a common foundation inspired by the original Local Binary Pattern (LBP) formula-tion. However, they diverge in their approach by randomly initializing the convolutional kernel values within the range of [-1,0,1] while imposing only sparsity constraints and lacking structural considerations. In contrast to the stochastic initial-ization utilized in [35], the proposed methodology explores a"}, {"title": "B. Networks Composed of Structured Ternary Patterns", "content": "Basic Block: The proposed approach is capable of gen-erating weight patterns based on underlying data structures, which can be utilized to construct a fundamental building block referred to as STeP (Structured Ternary Pattern). This block can be seamlessly integrated into both classification and detection networks. The formation of this block involves an initial step of calculating outputs from the convolution process using both CS-LBP weights and HAAR features. Subsequently, these outputs can be combined either through element-wise addition or via a 1 \u00d7 1 convolution operation, which itself can have weight values within the range of [-1, 1]. Furthermore, the block incorporates pooling, normalization, and activation functions after the feature fusion process. This primary block can be directly integrated into standard ConvNet architectures to replace computationally expensive operations.\nCustom network: The primary objective of the basic block is to facilitate the construction of custom networks that incorpo-rate the STEP block. In the context of these custom networks, batch normalization and LeakyReLU activations are employed within the STeP block. During the testing phase, it is possible to optimize the LeakyReLU activation function by zeroing out its negative components and rounding the positive values to the nearest integer. This refinement serves to enhance the model's hardware compatibility and efficiency. Consequently, the CS-LBP/HSF block can consistently be delegated to a hardware accelerator for streamlined processing, while also reducing the computational load on the CPU. Additionally, a detection layer can be incorporated into the architecture to perform tasks such as bounding box regression and object classification. It's important to note that non-binary operations, such as batch normalization with scaling and shifting, remain in floating-point precision. Similarly, the activation functions have not undergone any alterations."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "The conducted experiments encompass both classification and detection tasks, with the primary objective of demonstrat-ing the versatile utility of the STeP blocks. Specifically, this research seeks to establish that these STeP blocks can serve a dual purpose: 1) enhancing the computational efficiency of"}, {"title": "A. Image Classification", "content": "To assess the effectiveness of the proposed approach, exper-iments are conducted using four distinct classification datasets. The CIFAR-10 dataset (referred to as C-10) [38] consists of 32 \u00d7 32 RGB images, containing 50,000 images for training and 10,000 images for testing, distributed across ten classes. The CIFAR-100 dataset (referred to as C-100) encompasses a more complex classification task, comprising 100 distinct classes. This dataset consists of 600 images per class, divided into 500 training images and 100 testing images, all with a resolution of 32 \u00d7 32 pixels. In addition, the Tiny-ImageNet dataset (referred to as Tiny200), it also utilized, which serves as a subset of the larger ImageNet dataset [39]. Tiny200 contains 100,000 images distributed across 200 classes, with each class containing 500 images. These images have been resized to 64 \u00d7 64 pixels, offering a balance between image quality and computational complexity. Lastly, the Imagenet-16 dataset (referred to as ImgN16) is another subset of the renowned ImageNet dataset. It features a more compact classification task with 16 distinct classes. Notably, ImgN16 maintains the original image resolutions, which are typically resized to 224 \u00d7 224 pixels for standard ImageNet evalu-ations. Collectively, these datasets provide a comprehensive evaluation set, encompassing a wide range of class counts and image resolutions, thereby enabling a thorough assessment of the proposed approach.\nFour prominent network architectures in the field of com-puter vision are used in the evaluation with distinct characteris-tics and performance. The models considered are the ResNet50 [37], MobileNetV2 [24], EfficientNet [26], and VGG16 [36] which provide a comprehensive assessment across different design philosophies and trade-offs. The comparative analysis also encompasses three distinct weight generation settings. The original model utilizing weight initialization and up-date through backpropagation, models initialized with random binary weights, and models utilizing the STeP block with structured ternary weights.\nIt is worth noting that not all network blocks were converted to non-learnable ternary values. The changes concerned mainly the spatial kernels. Bottleneck layers applied to shortcut con-nections, squeeze and excitation layers, and the final classifica-tion layer weights were not converted for the purposes of these experiments. This is left as additional area of investigation, while benefits are still tangible even with partial modification.\nAll networks are trained from scratch for 200 epochs, with a starting learning rate of 0.1 and weight decay of 5 \u00d7 10-4, and a cosine annealing scheduler. An initial batch size of 128 is used. Where feasible for the binary/STeP models larger batch sizes are applied. During the training process, the data are"}, {"title": "B. Object Detection", "content": "The second evaluation scenario illustrates the potential for further enhancements by optimizing the architecture and extending upon the proposed STeP block. In this experiment, the focus is on aerial vehicle detection using Unmanned Aerial Vehicles (UAVs), which serves as a quintessential example of an on-device AI application where the utility of lightweight models is particularly evident [43].\nA comparative analysis is conducted involving a net-work architecture comprised of successive STEP blocks, benchmarked against other lightweight networks featuring ImageNet-pretrained backbones, such as MobileNets [24], [41] and Darknet-19 [42], as well as a network with LBP convolutional layers from [35]. For the evaluation a dataset"}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "This work has demonstrated the effectiveness of utilizing structured ternary patterns, a combination of local binary patterns and Haar features, as a foundation for competitive networks in resource-constrained on-device AI applications. The proposed approach has yielded promising results and has demonstrated performance comparable to that of fully trainable convolutional networks, as validated by the experi-ments. Distinguishing itself from much of the related work on compression techniques, the proposed method enables direct learning of efficient models. The use of a large number of static parameters in the model offers significant advantages for updating model parameters during deployment and facilitating more efficient federated learning approaches.\nFuture research should prioritize the enhancement of both feature and architectural spaces to further optimize designs for maximum efficiency. Additionally, it is imperative to delve deeper into the training process to formulate an optimized training strategy tailored to networks of this nature. Given the reduced number of trainable parameters, consideration should be given to adopting smaller learning rates and potentially extending the number of training epochs. Finally, with the recent trend in using Neural Architecture Search, it would be worthwhile to build a search space based on structured ternary patterns and formulate an automated network design process that also encodes efficiency measures."}]}