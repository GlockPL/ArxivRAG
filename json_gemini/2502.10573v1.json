{"title": "An Innovative Next Activity Prediction Approach Using Process Entropy and DAW-Transformer", "authors": ["Hadi Zare", "Mostafa Abbasi", "Maryam Ahang", "Homayoun Najjaran"], "abstract": "In Business Process Management (BPM), accurate prediction of the next activities is vital for operational efficiency and decision-making. Current Artificial Intelligence (AI)/Machine Learning (ML) models struggle with the complexity and evolving nature of business process event logs, balancing accuracy and interpretability. This paper proposes an entropy-driven model selection approach and DAW-Transformer, which stands for Dynamic Attribute-Aware Transformer, to integrate all attributes with a dynamic window for better accuracy. This paper introduces a novel next-activity prediction approach that uses process entropy to assess the complexity of event logs and dynamically select the most suitable ML model. A new transformer-based architecture with multi-head attention and dynamic windowing mechanism, DAW-Transformer, is proposed to capture long-range dependencies and utilize all relevant event log attributes. Experiments were conducted on six public datasets, and the performance was evaluated with process entropy. The results demonstrate the effectiveness of the approach across these publicly available datasets. DAW-Transformer achieved superior performance, especially on high-entropy datasets such as Sepsis exceeding Limited window Multi-Transformers by 4.69% and a benchmark CNN-LSTM-SAtt model by 3.07%. For low-entropy datasets like Road Traffic Fine, simpler, more interpretable algorithms like Random Forest performed nearly as well as the more complex DAW-Transformer and offered better handling of imbalanced data and improved explainability. This work's novelty lies in the proposed DAW-Transformer, with a dynamic window and considering all relevant attributes. Also, entropy-driven selection methods offer a robust, accurate, and interpretable solution for next-activity prediction.", "sections": [{"title": "1 Introduction", "content": "Process mining is a business process management (BPM) technique that offers information extracted from event logs to help improve organizational operations and also service performance. (Burattin, 2015; Turner et al., 2012). One critical application of process mining is predicting the next activity, which provides precise execution insights for ongoing or incomplete process instances (Dentamaro et al., 2023). Predicting the most likely subsequent steps in a process allows proactive resource allocation, optimization of workflow, and defect detection early, assuring smooth execution and the achievement of the proper execution of the process goals according to (Sun et al., 2024). These capabilities are particularly critical in healthcare and manufacturing where anticipating the following steps can significantly improve operational efficiency and outcomes. Also, In customer service, understanding customer expectations is essential for delivering high-quality services and gaining a competitive advantage (Kim and Kim, 2001). Predicting the next activity accurately is crucial in customer service, as it enables businesses to align service processes with customer needs, improve responsiveness, and enhance overall satisfaction.\nMost businesses rely on event logs and the result of process mining to support decision-making and identify bottlenecks (Rivera Lazo and \u00d1anculef, 2022). In this context, various machine learning approaches, such as Decision Trees, Random Forests, Long Short-Term Memory (LSTM) networks, Convolutional Neural Networks (CNNs), and others, have been explored for the next activity prediction. Among these, deep learning methods have gained significant attention because of their ability to model complex patterns and sequences (Abbasi et al., 2025).\nDeep learning is widely adopted for sequence modeling tasks because it can autonomously learn complex data representations (Sun et al., 2024; LeCun et al., 2015). Using layered neural networks, deep learning is able to detect patterns and correlations in large datasets so that data can be analyzed at"}, {"title": "", "content": "multiple levels of abstraction (LeCun et al., 2015). These capabilities make it particularly suitable for the prediction of the following activity in BPM, where algorithms such as Recurrent Neural Networks (RNNs) and LSTM networks have shown considerable success (Musa and Bouras, 2023; Di Mauro et al., 2019). These models are trained in event logs, learning relationships between activities, and forecasting subsequent process steps (Wang et al., 2023b). However, most of the current models have a strong focus on activity sequences (Wang et al., 2023b), overlooking the rich contextual information that significantly influences process outcomes. This limitation reduces their ability to fully capture the context of a process.\nOn the other hand, transformer architecture greatly improves sequence-to-sequence modeling (Vaswani, 2017), offering a reliable solution for this task in process mining. Transformers utilize an encoder-decoder model with multi-head self-attention, which makes it possible to incorporate multi-view information and effectively capture long-distance dependencies through scaled dot-product attention (Vaswani, 2017; Wang et al., 2023b). Transformers have recorded exceptional performance in machine translation, revolutionizing natural language processing and giving rise to many advanced models. This renders Transformers central to modern AI (Vaswani, 2017).\nTo address these issues, we propose the DAW-Transformer, a multi-head transformer-based method that integrates multiple attributes of event logs. By leveraging a self-attention mechanism, the DAW-Transformer delivers high-accuracy predictions, particularly for long sequences (Vaswani, 2017). It incorporates all relevant event log attributes, ensuring no loss of information and enabling more precise predictions of subsequent activities. Additionally, the model employs a dynamic window mechanism, allowing it to effectively incorporate long sequences into the process models.\nIn addition, selecting the most appropriate predictive model for a dataset remains a critical challenge in Business Process Management (BPM). The ideal model must trade-off between accuracy, efficiency, and resource usage. Interpretability is also a key consideration, as clear algorithms foster trust in decision-making. Traditional models such as Decision Trees and Random Forests are often more interpretable, offering explainability over more complex deep learning models like Transformers and CNNs (Kumar et al., 2024). As a result, companies tend to favor explainable models for applications where transparency is crucial.\nThe proposed research addresses key gaps in next-activity prediction. One major gap is the lack of a systematic method to identify the most efficient and accurate predictive model based on dataset complexity, coupled with the absence of a proper metric to guide decision-making. Current approaches often rely on trial and error, leading to wasted time and resources. Another gap is the underutilization of event attributes in following activity prediction tasks, as well as the absence of"}, {"title": "", "content": "an appropriate approach to capture these attributes, which limits the contextual understanding of processes.\nThe rest of this paper is organized as follows: Section 2 describes related work on the next activity prediction. Section 3 provides preliminaries for understanding our approaches. Section 4 introduces the proposed methods and methodology, and Section 5 discusses experiments. Sections 6 and 7 cover the results, discussion, and conclusions. Lastly, Section 8 provides acknowledgments."}, {"title": "2 Related Work", "content": "Over the past decade, predicting the next activity in business process management (BPM) has attracted attention for improving organizational efficiency and supporting better decision-making. Numerous studies have focused on predicting the next activity in ongoing cases. Early approaches primarily relied on traditional machine learning techniques Models like Decision Trees and Random Forests were first applied because of their explanatory simplicity (Breiman, 2001; Song and Ying, 2015). However, with increasing complexity in event logs, deep learning techniques were increasingly researched to better respond to complexities inherent in temporal and sequential information.\nIn the initial application of deep learning in BPM, RNNs were extensively employed to forecast the next activity (Abbasi et al., 2024). Although RNNs showed promising performance, they failed to remember the earlier context in lengthy sequences, and thus their performance on sequence prediction tasks was limited (Wang et al., 2023b). To amend this limitation, Di Mauro et al. (2019) explored the use of CNNs stacked inception modules as alternatives to RNNs like LSTMs in next-activity prediction of process mining. Their CNN model performs better than RNNs in accuracy and computational expense, with an average accuracy improvement of 12.17% and halving the training time on real-world data (Di Mauro et al., 2019). The findings demonstrate that CNNs are a strong alternative for sequential data tasks, and future work includes predicting the following activities and execution times using advanced inception modules (Di Mauro et al., 2019). Building on these findings, other researchers further explored LSTMs for next-activity prediction. For example, in (Musa and Bouras, 2023), LSTMs were applied to event logs to enhance predictions in BPM, with a particular focus on real-world applications and anomaly detection.\nThe potential of LSTM-based models was further advanced with methods like Data-Aware Explainable Next Activity Prediction (DENAP) (Aversano et al., 2023). DENAP combines LSTM networks with Layer-Wise Relevance Propagation (LRP) to provide accurate predictions (80\u201397%) alongside interpretability."}, {"title": "", "content": "Transformer models have only recently been discovered as powerful tools for next activity prediction. Transformers leverage self-attention mechanisms to avoid the limitations of traditional RNNS and LSTMs in capturing long-range dependencies. For instance, The Multi-View information Fusion Method (MiFTMA) employs transformers with multi-view which more precisely capture long-term dependencies compared to the baseline methods (Wang et al., 2023b). Similarly, the Multi-Task Learning Guided Transformer Network (MTLFormer) combines transformers' self-attention with multi-task parallel training. This approach reduces complexity but improves accuracy in long-distance predictions, where relevant information may be spread across distant input parts (Wang et al., 2023a). In (Bukhsh et al., 2021), authors proposed ProcessTransformer, a transformer-based model capable of learning high-level representations from event logs with minimal preprocessing. The approach surpasses 80% accuracy in next-activity prediction across nine datasets, improving traditional baselines by capturing long-range dependencies without the need for recurrence. Remarkably, ProcessTransformer demonstrates strong performance even with no context and attribute-aware model.\nHowever, existing transformer-based models (i.e., MiFTMA and ProcessTransformer) employ a sliding window approach, segmenting traces into fixed-length k-prefixes for processing. With this approach, they failed to consider long-term process behavior, and they mostly relied on the last recent behavior of the process. To address this, our proposed DAW-Transformer prepares sequences for each attribute using an extended sliding window, providing a dynamic mechanism to effectively capture and incorporate long sequences into the model. By incorporating all historical events, our method ensures a more comprehensive understanding of the process, improving predictive accuracy."}, {"title": "3 Preliminaries", "content": "This section introduces foundational concepts relevant to process entropy and process mining."}, {"title": "3.1 Event log", "content": "Event logs record data about various event types and their timestamps, typically collected during the operation of modern industrial systems and machines (Huang et al., 2021). These logs are valuable for analyzing and anticipating critical events, enabling proactive responses that improve system efficiency and reliability (Huang et al., 2021). Each event log consists of three main components: CaseID, Activities, and Timestamps."}, {"title": "3.2 Process Entropy", "content": "The entropy of business process models is a measure of quantifying the uncertainty of process execution (Jung, 2008). Systems characterized by high variability and uncertainty struggle to execute precise planning and scheduling, leading to the wastage of human and system resources. In information theory, information uncertainty is typically quantified by information entropy, commonly known as Shannon's entropy (Jung, 2008). It is defined as:\n$H(X) = - \\sum_{i=1}^{n} p(x_i) \\log(p(x_i))$ (1)\nIn this expression, X represents a discrete random variable that can assume possible values X1,X2,...,Xn with corresponding probabilities p(x1),p(x2),...,p(xn). For 1 \u2264 i \u2264 n, the probabilities satisfy p(xi) > 0 and \u03a3=1P(xi) = 1."}, {"title": "4 Methodology", "content": "This section presents the details of the DAW-Transformer and the proposed next activity prediction. This method in this study aim to predict the next activity in the most efficient and interpretable way that considers all the attributes over time. In this section, we will first provide a detailed discussion of the DAW-Transformer, covering input data preprocessing, the multi-head attention transformer, and evaluation details. We will then analyze the evaluation results, and their confusion matrices, and compare process entropy with model accuracy. This comparison helps determine the most suitable machine learning model for each specific dataset.\nThe DAW-Transformer integrates multiple attributes from event logs, allowing for model training over all relevant data perspectives. Existing works focused on using just sequence of activities or timestamp-realted attributes. Unlike traditional approaches that rely solely on activities, in contrast, this one utilizes all important features to improve prediction accuracy."}, {"title": "4.1 Data Preprocessing", "content": "Each event log consists of several attributes such as activity, timestamp, case ID, and context-specific features (e.g., resource). During preprocessing, categorical attributes are encoded, and all data is standardized to ensure compatibility with machine learning algorithms."}, {"title": "4.2 Multi-Feature Embedding and Position Encoding", "content": "This component aims to comprehensively represent each sequence by embedding categorical and numerical features while incorporating positional encoding to capture the order of events. Embedding is crucial for the model to understand the relationships between categorical and numerical features and their temporal evolution. Positional encoding is significant as the attention mechanism lacks awareness of the sequence order. By assigning a specific position to each, the model can naturally understand the progression and order of sequence and in turn, have a deeper grasp of temporal dependencies (Vaswani, 2017)."}, {"title": "4.3 Multi-Transformer", "content": "The transformer encoder block is central to this model, enabling the integration of numerical data for prediction. This block begins by applying multi-head self-attention, which captures the relationships within each sequence. The attention mechanism is computed as follows:\ntextAttention(Q,K,V) = softmax $\\frac{QK^T}{\\sqrt{d_k}}$ V (2)\nV, with Q, K, and V representing queries, keys, and values, respectively, and dk the key dimen-"}, {"title": "", "content": "sion (Vaswani, 2017). A residual connection and layer normalization stabilize learning and enhance performance.\nNext, a feed-forward network (FFN) introduces non-linearity and transforms the data using the following equation:\nFFN(x) = ReLU(xW1+b1)W2+b2 (3)\nThe embeddings are then transformed, flattened, and concatenated with additional numerical features to enhance the input representation (Vaswani, 2017).\nFinally, a dense output layer with a softmax activation function generates a probability distribution over the prediction classes. This approach effectively integrates sequential and numerical information, ensuring comprehensive and accurate predictions."}, {"title": "4.4 Process Entropy and Next Activity Prediction", "content": "This work formally assesses activity sequence datasets' process entropy, and in doing so, provides a quantitative measure of process complexity. The algorithm begins with loading a dataset and case organization into traces, with each representing a sequence of specific activities.\nNext, the transition probabilities are computed by dividing the frequency of each transition by the total number of transitions. Using these probabilities, the process entropy is calculated as the sum of the negative product of each transition probability and its logarithm, considering only non-zero probabilities. The entropy calculation is a tool for gauging uncertainty and unpredictability in a specific process, and through it, one can gain important insights about its variance and complexity. Figure 1 describes an entropy-driven method for the next activity prediction. In this process entropy stands out as a key consideration in choosing an effective prediction model. By calculating the entropy of a given dataset's activity sequences, the method quantifies the uncertainty and complexity inherent in the process. Datasets with high entropy, indicative of complex and unpredictable behaviors, will be managed by the DAW-Transformer model, known for its accuracy in handling such complex patterns. On the other hand, datasets with low entropy, characterized by their simple and predictable sequence, will be handled through less complex models such as Decision Trees and Random Forest. With this adaptive approach, we intend to ensure that the selected prediction model aligns with the specific characteristics of the dataset, optimizing both accuracy and interpretability in the next activity prediction task."}, {"title": "5 Experiments", "content": "This section presents an experimental evaluation of the DAW-Transformer model, comparing its performance with that of CNN-LSTM, CNN-BiLSTM, XGBoost, Decision Trees, and Random Forest. Additionally, the datasets are analyzed based on their process entropy to determine the most suitable model for each dataset.\nDataset - Six publicly available datasets, widely used in process mining, were selected for this study. The properties of each dataset are detailed in Table 1 below.\nSepsis: This real-world event log includes events of sepsis cases from a hospital, documented by the ERP (Enterprise Resource Planning) system. Each case in the log represents a patient's journey through the hospital. 1\nHelpdesk: This dataset comprises events from the ticket management process of the help desk of an Italian software company. Each case in the log commences with a new ticket entry into the ticket management system and concludes with the issue's resolution and the ticket's closing. 2\nRoad Traffic Fine: A real-world event log from an information system that manages road traffic violations. 3\nBPI Challenge 2020_Prepaid Travel Costs: This file includes events associated with prepaid"}, {"title": "6 Results and Discussion", "content": "Various ML methods are employed alongside the proposed DAW-Transformer. For the Sepsis dataset, a CNN-BiLSTM model was used, with carefully tuned hyperparameters to enhance performance, as shown in Table 3. Key parameters included an initial filter size of 64 for the first convolutional layer, which progressively increased to 256 in subsequent layers, facilitating the extraction of complex features. To counteract overfitting, dropout values of 0.4 and 0.5 were added. To extract temporal relations in the sequence, a 400-unit bidirectional LSTM layer was added. The Adam optimizer with a learning rate of 0.001 was employed, and the model was trained over 300 epochs with a batch size of 32; early stopping and a learning rate scheduler were used to refine the training process and"}, {"title": "", "content": "enhance generalization. In Table 2 hyperparameters for the DAW Transformer Model on the Sepsis Dataset are shown."}, {"title": "", "content": "The evaluation results, summarized in Table 4, highlight the effectiveness of different models across various datasets. It illustrates how these models handle dataset features, including complexity and variation. Additionally, the results indicate that considering the entropy of each dataset can have diverse impacts. In other words, Sepsis is a complex dataset that varies with patient conditions, and it is challenging for less complex models to work effectively with it. With its ability to capture intricate long-range dependencies and complex interactions between various patient factors, the DAW-Transformer demonstrated superior performance, achieving an accuracy of 70.14%. In contrast, simpler models like Random Forest and Decision Tree struggled to effectively model the intricate dynamics of the Sepsis data, achieving lower accuracies of 59.81% and 58.86%, respectively. This indicates the crucial role of advanced architectures in effectively handling the challenges posed by high-entropy datasets."}, {"title": "", "content": "In contrast to high-entropy datasets, for low-entropy datasets such as Road Traffic Fine, traditional ML models like Random Forest (99.71%) and Decision Tree (99.69%) performed on par with the DAW-Transformer (92.36%). Simpler models can be competitive or even superior on low-entropy datasets. In this situation, it is better to use simple and more interpretable models which are transparent and faster and also need fewer sources. This finding supports the entropy-driven next activity prediction strategy, which tailors model selection based on dataset entropy to balance performance and interpretability.\nFor a better understanding of this method, confusion matrices for one high-entropy dataset (Sepsis) and one low-entropy dataset (Road Traffic Fine) are shown in Figure 2 and Figure 3. In Figure 2, the confusion matrices for the Sepsis dataset highlight the comparison between the DAW Transformer and Random Forest methods. These matrices have been normalized based on each row in the True label. The DAW Transformer confusion matrix exhibits a higher density along the diagonal and fewer false predictions outside it, indicating better classification performance. In contrast, the Random Forest confusion matrix (Figure 2a) shows more false predictions, reflecting a lower classification accuracy. This is further supported by the calculated confusion matrix entropy value of 1.30 for the Random Forest and 0.92 for the DAW Transformer, a 41% improvement. Having a lower value for entropy for the DAW Transformer signifies less scattering and a concentrated distribution of predictions regarding actual labels. These results clearly demonstrate the superior performance of the DAW Transformer, particularly for high-entropy datasets, and it is effective in dealing with challenging classification issues.\nFigure 3 presents the normalized confusion matrices for the Road Traffic Fine dataset, comparing the performance of the DAW Transformer and Random Forest classifiers. This dataset is both low-entropy and imbalanced, with the class distribution shown in Table 5. To correct the class imbalance, under-sampling was performed such that all classes have an equivalent number of examples to the least represented one (Vijay et al., 2023). Before under-sampling, the confusion matrix for the DAW Transformer (Figure 3a) exhibits significant misclassifications, with most predictions concentrated"}, {"title": "", "content": "in a single class. After under-sampling (Figure 3b), the confusion matrix becomes more diagonal, indicating improved classification performance."}, {"title": "7 Conclusions", "content": "This paper presents the entropy-driven approach for optimizing next-activity prediction in business process management (BPM) by leveraging process entropy to guide ML-based model selection. This method addresses the trade-off between predictive performance and interpretability by aligning model complexity with the inherent uncertainty of the process.\nFor high-entropy datasets, the DAW-Transformer, a powerful multi-head attention-based model, effectively integrated all relevant event log attributes to enhance prediction accuracy with a dynamic windows which consider all the eventlog for each case. Experimental evaluations on six public datasets confirmed its effectiveness, particularly for high-entropy datasets like the Sepsis dataset, where it achieved a 70.14% accuracy- a 9.51% improvement over CNN-BiLSTM, a 4.69% over Limited window Multi-Transformers, and a 3.07% improvement over the literature's best deep learning model (CNN-LSTM-SAtt).\nIn datasets characterized by low entropy, such as the Road Traffic Fine dataset, traditional machine learning models, including Random Forest (accuracy: 99.71%) and Decision Tree (accuracy: 99.69%), either outperformed or achieved comparable performance to deep learning models, with the DAW-Transformer yielding an accuracy of 92.36%. The subsequent experiment focused on handling imbalanced low-entropy datasets highlighted that, prior to performing under-sampling, the DAW-Transformer faced challenges related to class imbalance, resulting in several misclassifications. Following the application of under-sampling, classification performance improved; however, Random Forest still exhibited a lower confusion matrix entropy (0.12) compared to the DAW-Transformer (0.35), demonstrating a 191% improvement. This demonstrates that while under-sampling can be a convenient approach for balancing classes, it may not always be the most effective solution, as it risks discarding valuable information. In contrast, methods such as Random Forest are more adept at handling imbalanced distributions, and preserving important data while improving model performance. These results highlight how well interpretable models can be accurate while still being transparent for informed decision-making in less complex settings.\nThis approach necessitates a comprehensive understanding of entropy and requires users to evaluate trade-offs among entropy, accuracy, cost, and resource utilization, thereby restricting its accessibility to experts. To mitigate this limitation, future research should investigate the development of a more autonomous and automated framework that accounts for various parameters, such as dataset characteristics, computational costs, and resource constraints, thereby minimizing the need for expert intervention."}]}