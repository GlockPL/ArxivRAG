{"title": "Dynamic Knowledge Selector and Evaluator for recommendation with Knowledge Graph", "authors": ["Feng Xia", "Zhifei Hu"], "abstract": "In recent years recommendation systems typically employ the edge information provided by knowledge graphs combined with the advantages of high-order connectivity of graph networks in the recommendation field. However, this method is limited by the sparsity of labels, cannot learn the graph structure well, and a large number of noisy entities in the knowledge graph will affect the accuracy of the recommendation results. In order to alleviate the above problems, we propose a dynamic knowledge-selecting and evaluating method guided by collaborative signals to distill information in the knowledge graph. Specifically, we use a Chain Route Evaluator to evaluate the contributions of different neighborhoods for the recommendation task and employ a Knowledge Selector strategy to filter the less informative knowledge before evaluating. We conduct baseline model comparison and experimental ablation evaluations on three public datasets. The experiments demonstrate that our proposed model outperforms current state-of-the-art baseline models, and each module's effectiveness in our model is demonstrated through ablation experiments.", "sections": [{"title": "1. Introduction", "content": "The recommendation algorithm is a vital component in applications such as e-commerce[1-3], news[4-6] and short videos[7-8]. As part, it takes on the important task of outputting personalized information flow. Early recommendation algorithms[9-12] often employed the K-nearest neighbor method, such as item-based collaborative filtering algorithm[13-16] by comparing the similarity between items and recommend the most similar items to the historical click items of the user. The recommendation algorithm is based on the nearest neighbor search method, which benefits from strong interpretability, high performance, and simplicity, but it is not able to analyze user characteristics, item characteristics, and context characteristics models, and it performs poorly in recommendation scenarios with fewer user behavior data.\nIntroducing the knowledge graph[17-18] to the recommendation system[19-25] effectually reduces the cold start problem in the recommendation system. Because the rich semantic relationship information between entities in the knowledge graph provides a large amount of auxiliary information for the items (i.e., side information), it hence results in rising the richness of the recommended items and enhancing a recommendation algorithm based on knowledge graphs. Shi et al.[26] designed a meta-path-based random walk sampling method, generating multiple nodes starting from the target node, and consisting of multiple nodes such that the points constitute the meta-path. On this basis, a meta-path embedding learning algorithm (i.e., heterogeneous information network (HIN)) was proposed. Sun [27] proposed a loop-based network approach, which represents a modular algorithm (i.e., recurrent knowledge graph embedding (RKGE)) that compensates for the embedding learning algorithm. This methodology only cares about the semantic layer connectivity of nodes, but overlooks the path information between nodes. Wang [28] proposed a method for modeling path order dependencies (i.e., knowledge-aware recurrent network (KPRN)), which can model the complex relationship of paths connecting users and items and combine them through pooling operations on the importance of various paths. Wang [29] proposed a method based on the knowledge graph and attention machine, a system-based algorithm (i.e., knowledge graph attention network (KGAT)), two parts between users and items. The graph is then combined with the knowledge graph and the graph relationship, whereas users and items are displayed in the same graph space. By employing a network graph attention mechanism to extract high-level and semantic relationships between entities from mixed graphs, Wang [30] proposed a model based on the perceptual domain (the so-called knowledge graph attention network (KGCN)). According to the user's historical click on the item, it looks for neighboring nodes in the knowledge map, employs the graph attention mechanism to continuously collect, and then updates and disseminates information. Wang et al. [31] proposed an algorithm model for label smoothing (i.e., knowledge-aware graph neural networks with label smoothing regularization (KGNN-LS)). This model introduces a regular label smoothing term based on the KGCN to adjust the loss function constraints. Wang et al. [32] proposed a water wave network (RippleNet), which is cleverly borrowed from water wave transmission. The broadcasting idea, using the items that the user has clicked on in the history as the user's interest seeds, broadcasts the items knowledge map to the outer layer, combines the representation-based learning method with the path- based method, and achieve to propagate user preferences to discover potential interests of users.\nThe complexity algorithm of the above graph is based on the mechanism of aggregation-propagation-update, which is very complicated for calculation, and the importance assessment of each entity is not enough, and the information of each entity cannot be utilized. We will explain this in some detail in the motivation section.\nBased on the above research background, this paper also aims to propose a dynamic knowledge selection and evaluator to extract information from knowledge graphs with the help of supervised information. First, all relationships and entities between each central entity and neighboring entities are taken into account as knowledge links. The knowledge selector is employed to select the information that represents the importance of this link, the evaluator is then utilized to score the importance of each entity, and finally, all entities are appropriately aggregated as context information of the current entity to increase the semantic representation. We conduct experiments on three open-source datasets. A large number of experimental results indicate the effectiveness of the proposed model. Meanwhile, we design a set of ablation tests for the internal analysis of the model. Our contributions and innovations are:\n\u2022\tA dynamic knowledge selection and evaluator is proposed to maximize the use of structural information in KG for knowledge distillation."}, {"title": "2. Motivation", "content": "This section revisits existing KG-based recommendation methods and further identifies the limitations originating from the information aggregation attention mechanism, clarifying the motivation of our work.\n2.1. Revisiting KG-based recommendation methods\nKG-based recommendation methods recursively propagate an embedding from its neighbors to refine the embedding. Moreover, such methods calculate the inner product of user and item representations to predict their matching score and employ specific attention mechanisms to discriminate the neighbors' importance. KG-based recommendation methods can be formulated as follows:\nKnowledge-aware Attention: Information propagation between an entity and its neighbors can be achieved by computing the normalized score \\$\\hat{\\pi}_{uv}\\$ to control the decay factor on the edge, indicating how much information is propagated from the tail entity. The score \\$\\hat{\\pi}_{uv}\\$ is computed using function \\$f_s\\$, which is specifically designed for this purpose.\n\\$\\hat{\\pi}_{uv} = \\frac{\\exp(f_s(\\pi_{uv}))}{\\sum_{e\\in\\mathcal{N}(v)}\\exp(f_s(\\pi_{uv}))}\\$\n(1)\nInformation Aggregation: After specifying the varying importance of the neighbors, the entity and neighbor representations are aggregated using the aggregator \\$f_{agg}\\$.\nInformation Propagation: To explore the high-order connectivity information and collect the information propagated from the higher-order neighbors, the embedding is refined by recursively propagating from its neighbors:\n\\$e^{l+1}_v = f_{agg}(e^{l+1}_v, e^l_v)\\$\n(2)\nFinally, KG-based recommendation methods adopt a layer- aggregation \\$f_{concat}\\$ mechanism to concatenate the representations of each layer into a single vector:\n\\$e_v = f_{concat}(e^1_v, ..., e^L_v)\\$\n(3)\nIt is worth noting that vector \\$e_v\\$ is simply a weighted summation of entities linked to node v and the key point is to obtain a function f learned by collaborative signals to output weight w\u2081:\n\\$e = \\sum w_i l_i\\$\n(4)\nTwo factors are important in calculating w\u2081: First, preserving the features' integrity in the entity-relation chain route between the current entity and the target entity, differentiate each entity more when calculating weight w\u2081. Second, The grouping methods affect the ability to filter irrelevant entities. Grouping entities in appropriate ways enables the discriminator to aggregate better useful information. Typical group methods divide, the entities linked to the same entity and place them in one group.\n2.2. Limitations of existing models\nExisting methods suffer from some limitations when calculating W\u2081,\nhindering the effective and efficient training of KG-based recommendation models.\n\u2022\tLimitation I: Existing methods cannot unify all entities and relations involved in the link between entity i and entity j, including users and items, leading to feature leakage, e.g., KGAT implements a relational attention mechanism \u03c0(h, r, t), ignoring the user and item information.\n\u2022\tLimitation II: Current attention mechanisms often follow the key-value paradigm and only perform two or three feature crossing, which is insufficient to calculate weight Wi.\n\u2022\tLimitation III: Existing approaches often make intra-group comparisons, which may not discover the effective entity for the recommendation task and filters the irrelevant entity."}, {"title": "3. Theory and framework", "content": "This section first introduces the terminology and describes the problem of recommender systems based on knowledge graphs. Then we present the proposed Dynamic Knowledge Selector and Evaluator Network (DKSE), and then we apply the multi-stream attention mechanism to distill the effective information from the noise, which conducts feature crossing in multi-combinations. Finally, we describe the prediction layer and optimization algorithm formally.\n3.1. Task Definition\nUser-item bipartite graph: We regard user-item interaction data as a bipartite graph G\u2081. Specifically, given a user set U = {u\u2081, U\u2082 ...}, item set V = {V1, V2 ...}, and a user-item interaction matrix Y = {yuvu \u2208 U,\u03bd \u0395 V...}, the items and users are denoted as nodes in the user-item bipartite graph and Yu,v = 1 indicates an edge connection between user u and itemv. Otherwise, there is no edge connection.\nKnowledge graph: Formally, a knowledge graph, which contains real- world entities, is defined as a triplet Gk = {(h,r,t)|h, t\u2208 E,r\u2208 R}, where the head, relation, and tail entity are denoted as h, r, t, respectively. Besides, if we regard e as any entity in the knowledge graph, then N(e) is the set of neighbor entities of entity e in Gk.\nCollaborative Unified graph: The collaborative, unified graph Go combines the user-item bipartite graph and knowledge graph seamlessly through item-entity alignment.\nChain Route: Chain Route is the relation and entity involved in the link between entity i and entity j in a Collaborative Unified graph and is a directed sequence comprising entities and relations, such as R = {U1, 1, V1, V2, e\u2081}. Chain Route R contains all information used to calculate the relevance between user u\u2081 and entity e\u2081.\nTask Definition: We formulate the recommendation task as follows. Based on a Collaborative Unified graph Gc, we aim to learn the prediction function f to predict the connection possibility between the user and the item.\n3.2. Multi-Stream Attention Layer\nThe multi-stream attention layer(MSAL) comprises two major modules: Knowledge Selector and Chain Route Evaluator. The former explicitly chooses entities or relations (including use/item information) from the chain route that should be involved in generating the attentive weights. The latter learns from the collaborative signals how to suggest which chain route should be given more attention to capture knowledge associations more effectively. Each part is discussed exhaustively next.\nKnowledge Selector: Intuitively, surrounding entities of the user/item in a collaborative, unified graph can represent the user's preferences for an item. For example, the recommended movie Avengers: Endgame has six entities connected to it in the collaborative, unified graph, including director Anthony Russo, actor Robert Downey Jr, and actor Chris Evans, film Sherlock Holms, Iron Man, and Captain America. Each entity has a unique chain route containing all knowledge that could be beneficial for representing the user's preference for the movie Avenger: Endgame. In order to filter the less informative knowledge before evaluating all six chain routes for the recommendation, we initialize n query vectors q \u2208 Rd to select useful entities and relations. Considering a chain route C = {U\u2081, \u2081, V1, V2, e\u2081... }, that is a collection of n entities/relations, we apply the following nonlinear transformations fk:\n\\$k = ReLU(w_k e + b_k)\\$\n(5)\nwhere wk \u2208 Rdxd, bk \u2208 Rdare trainable weight matrices and e \u2208 Rd are initialized entity/relation/user/item vectors. Function g, e.g., inner product, is used to compute the score \u03c0\u03bf:\n\\$\\pi_0 = g(q,k)\\$\n(6)\n\\$\\hat{\\pi_0} = \\frac{\\pi_0}{\\sum_{e \\in \\mathcal{N}(e)} \\pi_0}\\$\n(7)\nwhere \\$\\hat{\\pi_0}\\$ is the normalized score. We aggregate knowledge in the chain route with a normalized score, which acts as an automatic selector and generates selective features for each chain route ec.\n\\$e_c = \\frac{L}{Len(e)} \\sum \\hat{\\pi_0} e\\$\n(8)\nChain Route Evaluator: To effectively discriminate the contributions of different neighborhoods for the recommendation task, we use the chain route evaluator to obtain the neighborhood vector en(v). Considering entity v in the Collaborative Unified graph with m chain routes C = {C1, C2, C3 ... Cm}, we compute s to characterize the importance of the neighborhood entities:\n\\$s_c = w_c e_c + b_c\\$\n(9)\n\\$\\hat{s_c} = \\frac{s_c}{\\sum_{t \\in C_i} s_c}\\$\n(10)\n\\$e_n = \\sum_{e\\in e_c} \\hat{s_c} e\\$\n(11)\nwhere s is the normalized score and C, is a subset of C. We explore several methods to divide entities into groups during normalization, which affects how entities are compared. Currently, some typical grouping methods exist, e.g., dividing all entities into one group and normalizing the score. Thus, we design four methods to find the appropriate one.\n\u2022\tGlobal Grouping: Map all entities into one group.\n\u2022\tVertical Grouping. Divide all entities in one chain route into a group.\n\u2022\tHorizontal Grouping. Divide all entities in the same depth from v into a group.\n3.3. Prediction\nAfter applying the knowledge selector and chain route evaluation, we obtain the user/item vector /e by adding a neighborhood vector en(v):\n\\$c_u = c_u + c_{N(u)}\\$\n(12)\n\\$l_v = e_v + e_{N(v)}\\$\n(13)\nWe calculate the predicted click probability Yu with eu/ev:\n\\$\\Gamma_{uv} = \\sigma(v)\\$\n(14)\nwhere o is the sigmoid function. The entire algorithm process is presented below:"}, {"title": "3.4. Loss function", "content": "DKSE's loss function comprises two parts. First, we calculate the cross- entropy loss Lbase between the user click probability and label, defined as follows:\n\\$L_{base} = \\sum_{uv} (\\sum_{v \\in {V}(u, v) \\in D} \\Phi(Y_{uv}, \\hat{Y_{uv}}))\\$\n(15)\nwhere I is the cross-entropy loss, and D denotes the training dataset discussed in detail in the Dataset Description Section. Second, we randomly select a user-item pair without interaction in a batch data B as negative samples and calculate the contrastive loss Lcl:\n\\$L_{cl} = -log \\frac{e^{\\sigma(v)}}{\\sum_{k \\in U v \\in V|(u, v) \\notin D} e^{\\sigma(v)}}\\$\n(16)\nMoreover, we add an L2 regularization term F for all parameters to alleviate the model from overfitting. Finally, we use the Adam optimizer to optimize all parameters.\n\\$L = L_{base} + L_{cla} + \\lambda||F||^2\\$\n(17)\n3.5. Time Complexity Analysis\nThe computational complexity of DKSE is divided into two parts: KGIR and UER. Given the item sampling depth lk for a neighborhood sample size M, and the user sampling depth lp for the neighborhood sample size N and the dimension of the embedding size d, the computational complexity for KGIR and UER is O(lkMd) and O(lpNd), respectively. Thus, the overall training complexity of DKSE is O(lkMd + lpNd).\nWe conducted several experiments on an RTX-3060 GPU and compared the training speed of DKSE against the baseline models KGAT [29], KGCN [30], and RippleNet [32]. The training time per epoch for"}, {"title": "4. Experiments", "content": "This section evaluates the performance of DKSE on three real-world datasets and verifies its effectiveness. Moreover, we aim to answer the following questions:\n\u2022\tRQ1: How does DKSE perform compared to the state-of-the-art baseline models?\n\u2022\tRQ2: How do different components affect DKSE's performance?\n\u2022\tRQ3: How do different parameter settings affect DKSE?\n4.1. Dataset Description\nTo evaluate the effectiveness of DKSE, we utilize three publicly accessible real-world datasets, MovieLens-1M, LFM-1b, and Amazon-book, which are varied in scenarios, size, and sparsity. The statistics of the three datasets are listed in Table 1, and descriptions of the datasets are as follows:\n\u2022\tMovieLens-1M: A widely used benchmark dataset for movie scenarios, containing about 1 million explicit ratings (from 1 to 5) for 2445 items by 6036 users.\n\u2022\tLFM-1b: A dataset collected from the online music system Last.fm that records user listening history, e.g., artists and albums, containing about 3 million detailed rating records for 15,471 songs by 12,134 users.\n\u2022\tAmazon-book: It contains information such as users, items, ratings, and rating time and has about 500,000 rating records for 9,854 books by 7,000 users, widely used for book recommendation.\nWe transform the explicit interactions into implicit feedback among the above datasets. Specifically, we indicate positive/negative samples as 1/0 instead of using raw ratings as labels. For MovieLens-1M, ratings over 4 are considered positive, and below 4 as negative. For the LFM-1b and Amazon- book datasets, all rating records are considered positive samples, and we randomly sample from unwatched interactions to obtain the negative data. To ensure the quality of the datasets, we applied a 20-core setting, i.e., discard users and items with less than 20 interactions. We randomly select 60%, 20%, and 20% of historical interactions for each dataset to constitute the training/ validation/test datasets."}, {"title": "4.2. Baselines", "content": "DKSE is challenged against the following baselines:\n\u2022\tKGCN [30]: An end-to-end framework that utilizes graph convolution networks to encode high-order structural and semantic entity information from knowledge graphs to enhance item representations.\n\u2022\tRippleNet [32]: This method is similar to a storage network, which represents a user by his click history through integrating knowledge graphs into recommender systems, overcoming the limitations of existing embedding-based and path-based KG-aware recommendation methods.\n\u2022\tKGAT [29]: A recommendation model based on a graph neural network, which uses a hybrid structure of knowledge graph and user-item graph as a collaborative knowledge graph to extract higher-order connections in CKG in an end-to-end manner. KGAT uses an attention mechanism to distinguish the importance of neighbors.\n\u2022\tNFM [11]: A factorization-based method that employs the Bilinear Interaction pooling operation to learn low-level combination features.\n\u2022\tBPRMF [12]: A pairwise sorting algorithm based on matrix decomposition, which assumes that the parameters obey a normal distribution to alleviate model overfitting.\n\u2022\tCKE [33]: A recommendation algorithm that combines knowledge graphs and multi-modal features into knowledge graphs, text descriptions, and cover images. In this paper, we only consider knowledge graphs as side information.\n\u2022\tSGL [34]: combines contrastive learning with a graph encoder to enhance the graph data by randomly removing and masking the features of nodes and edges to construct positive and negative samples so that the model learns more essential features.\n\u2022\tMVIN [35]: learns representations of items from multiple perspectives from both the user and entity perspectives, enriching the user-item interactions and refining the entity-entity interactions.\n4.3. Experimental Settings\nOur experimental parameter settings for DKSE are reported in Table 2:"}, {"title": "4.4. Performance Comparison (RQ1)", "content": "We experimentally compare all baseline models (CKE, NFM, BPRMF, RippleNet, KGCN, KGAT, SGL, MVIN) with DKSE on three datasets, using the AUC, ACC, and F1 performance metrics, widely used in CTR prediction. The corresponding experimental results are reported in Table 3. We also use the Precision and NDCG as the evaluation indicators for the TOP-K recommendation task, with the experimental results illustrated in Figure 3. Intuitively, our proposed DKSE outperforms all state-of-the-art competitor models on all datasets. From the experimental results, the following can be observed:\n\u2022\tAccording to Table 3, DKSE achieves a significant performance improvement on all datasets. Specifically, compared with the best baseline, the AUC of DKSE on MovieLens-1M, music, and Amazon- book is increased by 0.54%, 2.27%, and 3.95%, respectively.\n\u2022\tThe DKSE model has the greatest improvement on the Amazon-book dataset and achieved the lowest performance on the MovieLens-1M dataset due to the relatively high average clicked items compared with other datasets. Moreover, our model's performance is heavily correlated with the average clicked items of the dataset. When the average clicked items are higher, there is less improvement space for DKSE because for higher average clicked items, the users are more likely to click the same items. Thus, when we use the user click history to express the interests and preferences of different users, it is more difficult to distinguish them.\n\u2022\tFigure 3 highlights that in the top-K recommendation task, DKSE has achieved the best performance in Precision@K on LFM-1b, followed by Amazon-book and MovieLens-1M. When K is 1, 2, or 5, the performance of DKSE on the LFM-1b is significantly improved compared with the other baseline models. As K increases, this improvement shows a slightly decreasing trend. For k>1, the DKSE has no distinct advantage over the competitor models on MovieLens-1M. To compensate for the insensitivity of Precision@K regarding the order of the sorting positions, we record the NDCG@K of all experiments. On all datasets, DKSE outperforms the baseline models in NDCG@K. To our surprise, although DKSE does not perform as well as expected in Precision@K on MovieLens-1M, it still presents an appealing performance in NDCG@K. This is because DKSE is still in the lead in AUC compared to the baseline models, which often reflects the model's sorting ability.\n\u2022\tTable 3 suggests that the performance of all baseline models on the three datasets are ranked from high to low as follows: LFM-1b, MovieLens-1M, and Amazon-book, potentially due to the average interaction of each user in the three datasets. The LFM-1b has the highest average user clicks, and the higher they are, the stronger the model's generalization ability. Regarding LFM-1b and MovieLens-1M,"}, {"title": "4.5. Study of DKSE (RQ2)", "content": "Next, we evaluate the effectiveness of different group methods of DKSE by conducting some experiments. We experiment with the four model variants DKSE(Hor), DKSE(Glo), DKSE(Ver), and DKSE(base), to analyze the impact of the group method. Next, to further explore the influence of different sub-components, we split DKSE into 5 sub-components for detailed comparison-w/o U/V, w/o H, w/o R, w/o T, and w/o CL.\n\u2022\tGroup Method Comparison\nHere, we design three model variants, i.e., DKSE(Hor), DKSE(Glo), DKSE(Ver), DKSE(base) to perform experiments, which provide the following observations: Figure 4 highlights that on three datasets, the rank of AUC is DKSE(Hor), DKSE(Glo), DKSE(Ver), and DKSE(base). The BASE model, which discards comparison between entities, has the lowest AUC. Solely DKSE(Hor) positively affects the results, while DKSE(Glo) achieves the best performance as it exploits all entities comparison.\n\u2022\tComparison subcomponents\nWe conduct experiments on five components of three datasets, with the experimental results reported in Table 4. The following conclusions can be drawn:\nw/o U/V information: On LFM-1b, U/V contributes the most, while LFM-1b has the highest user average click number of 152.3. IUPP(V/T) compares the similarity between the item and the user's historically clicked items, which performs better when the user-clicked item sequences are longer.\nw/o R information: On the Amazon-book, this component makes the most contributions compared with the other components because this database has the most categories of relations, which is beneficial to aggregate the neighborhood entities of the item. In other words, this component helps aggregate entity information in a finer-grained and personalized way by meticulously depicting user preferences for different relations.\nw/o T information: On the MovieLens-1M, this component is the most prominent. Compared with the other two datasets, KG in MovieLens- 1M has the largest number of entities when aggregating entity information. The more entities, the more the T component contributes to discovering user perception of different entities.\n4.6. Parameter Sensitivity (RQ3)\nNext, we investigate the effects of different parameters on DKSE."}, {"title": "5. Conclusion and future work", "content": "This paper proposes a novel knowledge-distilling method called DKSE for recommendation, which comprises a Knowledge Selector and a Chain Route Evaluator. A knowledge Selector filters the less informative knowledge involved in the chain route, and the Chain Route Evaluator is used to select informative entities to represent user and item features for recommendation. Extensive experiments show the superiority of DKSE compared with several state-of-the-art models. In addition, we conduct a series of ablation experiments to demonstrate the effectiveness of each module."}]}