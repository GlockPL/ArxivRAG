{"title": "Evaluating the Robustness of Analogical Reasoning in Large Language Models", "authors": ["Martha Lewis", "Melanie Mitchell"], "abstract": "Large language models (LLMs) have performed well on several reasoning benchmarks, including ones that test analogical reasoning abilities. However, there is debate on the extent to which they are performing general abstract reasoning versus employing shortcuts or other non-robust processes, such as ones that overly rely on similarity to what has been seen in their training data. Here we investigate the robustness of analogy-making abilities previously claimed for LLMs on three of four domains studied by [35]: letter-string analogies, digit matrices, and story analogies. For each of these domains we test humans and GPT models on robustness to variants of the original analogy problems-versions that test the same abstract reasoning abilities but that are likely dissimilar from tasks in the pre-training data. The performance of a system that uses robust abstract reasoning should not decline substantially on these variants.\nOn simple letter-string analogies, we find that while the performance of humans remains high for two types of variants we tested, the GPT models' performance declines sharply. This pattern is less pronounced as the complexity of these analogy problems is increased, as both humans and GPT models perform poorly on both the original and variant problems requiring more complex analogies. On digit-matrix problems, we find a similar pattern but only on one out of the two types of variants we tested. Lastly, we assess the robustness of humans and GPT models on story-based analogy problems, finding that, unlike humans, the performance of GPT models are susceptible to answer-order effects, and that GPT models also may be more sensitive than humans to paraphrasing.\nThis work provides evidence that, despite previously reported successes of LLMs on zero-shot analogical reasoning, these models often lack the robustness of zero-shot human analogy-making, exhibiting brittleness on most of the variations we tested. More generally, this work points to the importance of carefully evaluating AI systems not only for accuracy but also robustness when testing their cognitive capabilities.", "sections": [{"title": "1 Introduction", "content": "The degree to which pre-trained large language models (LLMs) can reason\u2014deductively, inductively, analogically, or otherwise\u2014remains a subject of debate in the AI community. Many studies have shown that LLMs perform well on certain reasoning benchmarks [12, 36, 37]. However, other studies have questioned the extent to which these systems are able to reason abstractly, as opposed to relying on shortcuts [1, 33] or other heuristics, including \"approximate retrieval\" from encoded training data [16]. Several groups have shown that LLMs' performance on reasoning tasks degrades,"}, {"title": "2 Letter-String Analogies", "content": ""}, {"title": "2.1 Background", "content": "Letter-string analogies were proposed by Hofstadter [9] as an idealized domain in which processes underlying human analogy-making could be investigated. The following is a sample problem:\nabcd \u2192 abce ; ijkl\u2192 ?"}, {"title": "2.2 Variants of Letter-String Analogies", "content": "To assess the robustness of WHL's results on letter-string analogies, we created variants of the same letter-string analogy problem types, and evaluated both humans and three GPT models on these versions."}, {"title": "2.3 Methods For Experiments On Letter-String Analogies and Variants", "content": "Human Study Methods For all human studies, we collected data on Prolific Academic.2 Participants were screened to have English as a first language, to currently be living in the UK, the USA, Australia, New Zealand, Canada, or Ireland, and to have a 100% approval rate on Prolific.\nIn order to assess humans' abilities on the original and variants of letter-string problems, we collected data from 263 participants. Each participant was asked to solve 14 letter-string analogy problems drawn from original, permuted, and symbol alphabets, with different numbers of generalizations. The 14 problems given to each participant were sampled from different transformation types.\nIn addition to the 14 problems, participants were also given two attention-check questions at random points during the experiment, with a warning that if the attention checks were failed, then payment ($7 for the experiment, which was expected to take about 20-30 minutes) would be withheld.\nGPT Study Methods We evaluated the performance of three GPT models\u2014GPT-3 (text-davinci-003, which was the model tested by WHL), GPT-3.5 (gpt-3.5-turbo-0613), and GPT-4 (gpt-4-turbo-0613)\u2014on the same problems given to humans. Following WHL, all GPT experiments were done with temperature set to zero. GPT-3 takes in a single prompt, whereas GPT-3.5 and GPT-4 take in a list of messages that define the role of the system, input from a \"user\" role, and optionally some dialogue with simulated responses from the model given under the role \u201cassistant.\u201d\nFor our experiments on the original letter-string problems used by WHL, our system and user prompts for GPT-3.5 and GPT-4, have the following format:\nSystem: You are able to solve letter-string analogies.\nUser: Let's try to complete the pattern:\\n\\n[abcd] [abce]\\n[ijkl] ["}, {"title": "2.4 Replication of WHL's Studies", "content": "Our first set of experiments attempted a replication of WHL's experiments testing humans and GPT-3 on letter-string analogies.\nReplication: Human Study Results Figure 2 compares the results of our human study with that of WHL on the original letter-string problems, averaged across transformation types for different numbers of generalizations. The participants in our study achieved higher average accuracy than those of WHL on zero-generalization problems, and similar accuracy on problems with one or more generalizations. The differences on zero-generalization problems may be due to differences in experimental protocols or in the participant pools.\nReplication: GPT Study Results Figure 3 shows GPT-3 data from WHL compared with data from our computational experiments with GPT-3, GPT-3.5 and GPT-4, averaged across transformation types for different numbers of generalizations. In all cases our GPT-3 results are similar to those of WHL. GPT-3.5 and GPT-4 show slightly lower accuracy than GPT-3, possibly due to their fine-tuning beyond a strict prompt-completion objective.\nIn summary, our human and GPT-model replication results are generally consistent with those of WHL, although our human study yields higher human performance on zero-generalization problems."}, {"title": "2.5 Results on Variants of Letter-String Problems", "content": "Counterfactual Comprehension Check On Fictional Alphabets For problems involving permuted alphabets, we follow Wu et al. [38] by providing counterfactual comprehension checks"}, {"title": "3 Digit Matrices", "content": ""}, {"title": "3.1 Background", "content": "WHL proposed digit matrices as a novel analogy-making domain inspired by Raven's Progressive Matrices (RPM)[28]. The following is a sample digit-matrix problem:\n[2] [3] [4]\n[3] [4] [5]\n[4] [5] []\nAs in RPMs, the challenge is to recognize patterns across the rows and columns, and to fill in the blank ([]) cell.\nThe original RPM problems were created manually, and only a small number (108) were in the original formulation. Matzen et al. [19] identified a number of rules used in creating these problems; WHL used these rules to programmatically generate digit-matrix problems of different problem types:\n\u2022 Constant: the same digit occurs across each row or column. Example:"}, {"title": "3.2 Variants of Digit-Matrix Problems", "content": "To assess the robustness of WHL's results on digit matrices, we created two types of variants on the digit-matrix task: one in which we randomly assign the matrix position of the \u201cblank\" (answer element), and the other in which we replace digits with non-numeric symbols. As in our letter-string problems, these variants rely on the same abstract reasoning processes needed to solve the original digit matrices.\nAlternate Blank Position Using WHL's generation process, we generated digit matrices similar to those of WHL, but with the position of the blank element chosen randomly, instead of always being in the bottom-right position.\nSymbol Matrices We replaced the digits by non-numerical symbols in each of the digit-matrix problems used by WHL in their experiments (excepting the \u201cprogression\u201d-type problems, since the symbols have no inherent ordering)."}, {"title": "3.3 Methods for Experiments on Digit Matrices", "content": "Human Study Methods We collected data on Prolific Academic. Participants were screened to have English as a first language, to currently be living in the UK, the USA, Australia, New Zealand, Canada, or Ireland, and to have a 100% approval rate on Prolific with five or more studies approved.\nIn order to assess humans' abilities on the original and variants of digit-matrix problems, we collected data from 301 participants. Each participant was asked to solve 10 matrix problems, all of which were either original digit-matrix problems, problems with alternative blank positions, or problems with symbols in place of digits. The 10 problems given to each participant were sampled from different problem types.\nIn addition to the 10 problems, participants were also given two attention-check questions at random points during the experiment, with a warning that if the attention checks were failed, then payment ($6 for the experiment, which was expected to take less than 20 minutes) would be withheld.\nGPT Study Methods We evaluated the performance of GPT-3.5 (0613) and GPT-4 (0613)3 on the same digit matrices given to humans. At the time of our experiments, GPT-3 was not longer available for testing.\nFollowing WHL, all GPT experiments were done with temperature set to zero."}, {"title": "3.4 Replication of WHL's Studies", "content": "We experimented with different prompt formats, and found that the following gave the best performance for both models:\nSystem: You are a genius at solving analogy problems.\nUser: Try to complete the pattern below. Give ONLY the answer as briefly as possible.\\n[6] [6] [6]\\n[9] [9] [9]\\n[8] [] [8]"}, {"title": "3.5 Results on Human and GPT Experiments on Variants of Digit-Matrix Problems", "content": "Counterfactual Comprehension Check On Alternative Blank Positions For the digit-matrix problems with alternate blank positions, we tested GPT-3.5 and GPT-4 to make sure they comprehended the format of the task. For the simple digit-matrix problem\n[2] [2] [2]\n[5] [5] [5]\n[6] [6] []\nwe tested each each model nine times, where on each (independent, zero-temperature) run the blank was in a different position. The prompt we used was as follows:\nSystem: You are a genius at solving analogy problems.\nUser: The pattern below is incomplete. What is the position of the missing element? \\n[6] [6] [6]\\n[9] [9] [9]\\n[8] [] [8]\nGPT-3.5 correctly identified the location of the blank in 6 out of 9 cases, and GPT-4 correctly identified the location of the blank in 9 out of 9 cases. This suggests that when asked to complete the pattern, GPT-4 will comprehend what the task is, while GPT-3.5 might have some problems comprehending it.\nResults on the Two Types of Variant Problems Table 3 gives mean accuracy (across all problem types and numbers of generalizations) for humans, GPT-3.5, and GPT-4 on the original"}, {"title": "4 Story Analogies", "content": ""}, {"title": "4.1 Background", "content": "WHL tested humans, GPT-3, and GPT-4 on a set of 18 story-analogy problems from [7]. These problems were designed to test how well humans can identify analogies involving causal relations between story elements. For each problem, a participant or model is presented with a source story (Story 1), paired with two other stories (Story A and Story B), one of which is analogous to the source story, meaning that it has the same causal structure as the source story, although actors, objects, and events may differ. The other story has the same actors, objects, and events as the correct-analogy story, but the causal relations between the story elements differ from those in the source story. In WHL, these were termed, respectively, \"Far analogy-correct target story\" and \"Far analogy-incorrect target story.\"4\nWHL presented humans, GPT-3, and GPT-4 prompts that gave Story 1, Story A, and Story B, and asked, \"Which of Story A and Story B is a better analogy to Story 1? Is the best answer Story A, Story B, or both are equally analogous?\" To mitigate ordering biases, for each of the 18 story problems half of the human participants received prompts in which Story A was the correct answer, and for the other half, Story B was the correct answer. GPT-3 and GPT-4 were tested on two versions of each story problem, with opposite ordering of the correct and incorrect answers.\nWHL found that while humans perform better on average than GPT-3 and GPT-4 on these \"far analogies,\" both GPT models perform well above the random-guessing baseline of 50% accuracy."}, {"title": "4.2 Robustness Tests for Story Analogies", "content": "Testing Ordering Biases In reviewing the detailed data collected by WHL, we noticed that in their experiments, GPT-4's accuracy on the 18 story-analogy problems was biased by the order of the candidate answers: when Story A was the correct answer, GPT-4 was 89% accurate (16/18 correct), but when Story B was correct, GPT-4's accuracy decreased to 61% (11/18 correct).5 To further test the effects of answer order, we performed experiments similar to those of WHL, testing both human participants and GPT-4 (0613) on the same 18 story-analogy problems using both orderings for candidate answers. A human or machine employing robust analogy-making abilities should not be affected by the order of candidate answers.\nTesting With Paraphrased Stories One potential confounding factor in evaluating GPT models using previously published tests such as Gentner et al.'s story-analogy problems is that these tests are likely to have appeared in the models' pre-training data. If so, it is not clear what effect this would have on the model's accuracy, but it does require caution in interpreting the results.\nAnother potential confounder is the possibility of \"shortcuts\" in the text that is, features of the candidate answers that can be used to predict the correct answer without requiring a robust analogy-making capability. In reading the 18 story-problems, we noticed one possible source of shortcuts: in addition to sharing causal structure with the source story, the correct-answer story often seems more similar to the source story at the sentence level than the incorrect-answer story.\nTo test this possible source of bias, for each story problem we wrote a paraphrased version of the correct target story that lacked sentence-by-sentence similarity with the source story. We then tested humans and GPT-4 on the story-problems with paraphrased correct targets. A human or machine employing robust analogy-making abilities should not be affected by paraphrasing of candidate answers."}, {"title": "4.3 Methods for Studies on Humans and GPT Models on Story Analogies", "content": "Human Study Methods As for other studies, we collected data on Prolific Academic. Participants were screened to have English as a first language, to currently be living in the UK, the USA,\nGPT Study Methods Following WHL, we evaluated GPT-4 (0613) on each of the 18 story-analogy problems twice, once with each answer candidate listed first. We did not evaluate GPT-3 as it was no longer available at the time of our experiments.\nThe prompt we used was as follows:\nSystem: You are a helpful assistant.\nUser: Consider the following story:\\n\\nStory 1: [Text of Story 1]\\n\\nNow consider two more stories:\\n\\nStory A: [Text of Story A]\\n\\nStory B: [Text of Story B]\\n\\nWhich of Story A and Story B is a better analogy to Story 1? Is the best answer Story A, Story B, or both are equally analogous?"}, {"title": "4.4 Results of Human and GPT Studies of Ordering Effects", "content": "Robustness to Answer Order Table 4 gives the accuracies reported by WHL for GPT-4, along with the accuracies we recorded in our GPT-4 and human studies, for story-problem presentations in which the first or second answer was correct, and over all presentations. In our GPT-4 study, we found a similar ordering bias to that seen in WHL's data: the model is correct more often when the correct answer is given first. In contrast, we see no ordering bias for humans.\nRobustness to Paraphrasing Our experiments on stories with the correct answer paraphrased were run exactly as our experiments on the original stories. Table 5 gives the accuracies obtained in our GPT-4 and humans studies for both original and paraphrased cases. Both GPT-4 and human performance decreases on the paraphrased stories, suggesting that the superficial similarities in the original stories we described above may have contributed to the original accuracies for both GPT-4 and for people. GPT-4's performance decreases more than humans' performance on paraphrased stories, but with only 18 stories it is difficult to determine if this effect is statistically significant."}, {"title": "5 Related Work", "content": "In the last few years there has been considerable work on evaluating the robustness of reasoning in LLMs, with many studies showing that the performance of LLMs on reasoning tasks decreases, sometimes quite substantially, when tested on variants of tasks that are likely to differ from those seen in the training data [4, 11, 14, 20, 22, 24, 25, 27, 31, 38, 39]."}, {"title": "6 Conclusions", "content": "The purpose of the work described in this paper was to investigate the robustness of claims of emergent zero-shot analogical reasoning in GPT models [35]. In particular, we evaluated GPT models' performance on three of the four domains used by WHL: letter-string analogies, digit matrices, and story analogies, and tested the robustness of these models and of humans on variants of problems in those domains, ones that required the same abstract analogical reasoning but were unlikely to be similar to those seen in the models' pre-training data. In addition, we tested the effects of answer order for the story analogies.\nIn the letter-string-analogy domain, we evaluated humans and GPT models on two types of variant problems: ones using permuted alphabets and ones using symbol alphabets. On the simplest problems (zero-generalization) humans' performance remained relatively high across variants, whereas GPT models' performance decreased, particularly on symbol alphabets. This is in spite of the fact that GPT models seemed to comprehend basic relationships in the variant alphabets, as shown by our counterfactual comprehension checks. On more difficult problems (one to three generalizations), this effect was less prominent as both humans and GPT models performed poorly across original and variant problems.\nIn the digit-matrix domain we also evaluated humans and GPT models on two types of variant problems: ones in which the answer \"blank\" position was randomly selected, and ones in which we replaced digits with symbols. For the problems with alternate blank positions, the human participants performance was essentially unchanged from that on the original problems, whereas the performance of the GPT models we tested dropped dramatically. For problems with symbols"}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Letter-String Prompts", "content": "For the our experiments on letter-string-analogy problems (and variants), we experimented with three different prompts for GPT models. The prompt from Hodel & West [8] described in Section 2.3 resulted in the highest performance for all GPT models, and was used for our letter-string experiments. We also tested a prompt similar to the instructions given to human participants (\"Humanlike Prompt\"), and a minimal version of the human prompt (\u201cMinimal Prompt\"). These prompts are given below.\nHumanlike Prompt:\nSystem: You are able to solve letter-string analogies.\nUser: In this study, you will be presented with a series of patterns involving alphanumeric characters, together with an example alphabet.\\n\\nNote that the alphabet may be in an unfamiliar order. \\n\\nEach pattern will have one missing piece marked by [?].\\n\\nFor each pattern, you will be asked to guess the missing piece.\\n\\nUse the given alphabet when guessing the missing piece.\\n\\nYou do not need to include the \u2018[ ]' or spaces between letters in your response.\\n\\nabchefgdijklmnopqrstuvw\nxyz \\n\\n[aaa] [bbb]\\n\\n[ccc] [?]\nAssistant:hhh\nUser: In this case, the missing piece is 'hhh' \\n\\nNote that in the given alphabet, 'b' is the letter after 'a' and 'h' is the letter after 'c'\nUser: Use the following alphabet to guess the missing piece.\\n\\n[aucdefghijklmnopqrstbvwxyz]\\n\\nNote that the alphabet may be in an unfamiliar order. Complete the pattern using this order. \\n\\n[aucd] [auce]\\n\\n[ijkl] [?]\nMinimal Prompt:\nSystem: You are able to solve letter-string analogies.\nUser: Use the following alphabet to complete the pattern.\\n\\n[aucdefghijklmnopqrstbvwxyz]\\n\\nNote that the alphabet may be in an unfamiliar order. Complete the pattern using this order. \\n\\n[aucd] [auce]\\n\\n[ijkl]["}, {"title": "A.2 Digit Matrix Prompts", "content": "For our experiments on digit-matrix problems (and variants) we experimented with three different prompts for GPT models:\nWe found that the prompt described in Section 3.3 performed the best for all models; that was the prompt we used in our digit-matrix experiments. The other prompts we tested were as follows:"}, {"title": "A.3 Letter-String Problems: Counterfactual Comprehension Checks", "content": "Here we give details of the results of the counterfactual comprehension checks (CCCs) for letter-string-analogy problems we described in Section 2.5. The prompts for these checks have the following format:\nSystem: You are able to solve simple letter-based problems.\nUser: Use this fictional alphabet: [auc....]. \\nWhat is the next letter after a?\\nThe next letter after a is:\nSystem: You are able to solve simple letter-based problems.\nUser: Use this fictional alphabet: [auc....]. \\nWhat is the letter before c?\\nThe letter before c is:\nFor the non-permuted alphabet, each permuted alphabet an(i), and for two symbol alphabets 10(1) and 10(2), we performed the Successor and Predecessor CCCs on each letter (or symbol) as described above. Note that we ran Predecessor CCCs after GPT-3 was no longer available, so we only include results for that model on Successor CCCs.\nOn the non-permuted (n = 0) alphabet, all three models scored 100% accuracy on the Successor test, and both GPT-3.5 and GPT 4.0 scored 100% accuracy on the Predecessor test."}, {"title": "A.4 Digit Matrix Problems: More Detailed Results", "content": "Here we give more detailed results of human and GPT-model performance on our variants on digit-matrix problems. Figure 12b(a) gives the performance of our human participants on the original digit-matrix problems (blue points) and on the problems with alternate blank positions (orange points), for different numbers of rules, and for logic problems. Human performance does not change significantly when alternate blank positions are used. Figure 12b(b) gives the performance of GPT-3.5 and GPT-4 on the original problems (blue and green points) and on problems with alternate blank positions (orange and red ploints). It can be seen that the performance of both models drops substantially for alternative blank positions in all cases.\nFigure 13(a) gives the performance of our human participants on the original digit-matrix problems (blue points) and on ones in which symbols replace numbers (orange points). The performance of humans does not show a substantial decrease when digits are replaced by symbols, except in the case of three-rule problems. Figure 13(b) shows a similar effect for GPT models, except in the case of logic problems, in which the performance of the GPT models does show a small but significant decrease when digits are replaced by symbols."}, {"title": "A.5 Preliminary Error Analysis on Letter-String Analogies", "content": "A crucial aspect of letter-string analogy problems is that they do not necessarily have a \"correct\" answer, although, as we mentioned above, humans generally agree on what are the \"best\" rules describing letter-string transformations in this domain. However, there are other rules that can"}]}