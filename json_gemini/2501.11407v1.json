{"title": "A Truly Sparse and General Implementation of Gradient-Based Synaptic Plasticity*", "authors": ["Jamie Lohofft", "Anil Kayat", "Florian Assmuth", "Emre Neftci"], "abstract": "Online synaptic plasticity rules derived from gradient descent achieve high accuracy on a wide range of practical tasks. However, their software implementation often requires tediously hand-derived gradients or using gradient backpropagation which sacrifices the online capability of the rules. In this work, we present a custom automatic differentiation (AD) pipeline for sparse and online implementation of gradient-based synaptic plasticity rules that generalizes to arbitrary neuron models. Our work combines the programming ease of backpropagation-type methods for forward AD while being memory-efficient. To achieve this, we exploit the advantageous compute and memory scaling of online synaptic plasticity by providing an inherently sparse implementation of AD where expensive tensor contractions are replaced with simple element-wise multiplications if the tensors are diagonal. Gradient-based synaptic plasticity rules such as eligibility propagation (e-prop) have exactly this property and thus profit immensely from this feature. We demonstrate the alignment of our gradients with respect to gradient backpropagation on an synthetic task where e-prop gradients are exact, as well as audio speech classification benchmarks. We demonstrate how memory utilization scales with network size without dependence on the sequence length, as expected from forward AD methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advances in deep learning continually push the boundary of tackling and solving new challenges previously thought to be unsolvable. However, deep learning achieves its performance through gradient backpropagation which is not biologically plausible. Backpropagation violates two fundamental properties of physical computing, which are spatial locality and online learning. In the brain, synaptic plasticity plays an important role in achieving mid- to long-term adaptation of behavior. A common ground between biological heuristics and optimization algorithms in deep learning is provided by synaptic plasticity rules derived from gradient descent. However, in many cases the implementation of these gradient-based plasticity rules sacrifices generality by being meticulously handcrafted for a certain problem and neuron model. Machine learning software frameworks are generally designed for efficient execution of backpropagation on various general purpose accelerators. Synaptic plasticity rules derived from gradient descent can leverage these frameworks and backpropagation\u00b9 at the cost of sacrificing online operation and requiring memory that grows with sequence length.\nHowever, gradient-based plasticity rules operate on the principle of forward-mode Automatic Differentiation (AD) [Zenke and Neftci, 2021] with approximations that constrain the rule to be local [Murray, 2019]. In the case of recurrent neural networks that are unrolled along the time axis, forward-mode AD is also called Real-Time Recurrent Learning (RTRL) [Williams and Zipser, 1989]. With RTRL, the information necessary for computing gradients is propagated forward in time, allowing for online learning. However, na\u00efve implementations of forward-mode AD have high memory and time complexity compared to backpropagation (reverse-mode AD) [Williams and Zipser, 1995]. Such na\u00efve implementations calculate the exact gradients, and miss the improvements gained with local approximations.\nBy providing a novel extension of the popular machine learning framework JAX [Bradbury et al., 2018] this work aims to harnesses the advantageous properties of approximate gradient-based synaptic plasticity rules. We employ the recently proposed AD library, Graphax [Lohoff and Neftci, 2024] to showcase the efficient implementation of gradient-based plasticity rules and exploiting their inherently sparse formulation to arrive at the computational and memory complexities as expected from theory (see Table I). Our contributions are summarized as follows:\n\u2022\n\u2022\n\u2022\nWe provide the first AD package for spiking neural networks that leverages the inherent sparsity of gradient-based synaptic plasticity rules,\nOur AD package can automatically differentiate through arbitrary neuron dynamics and determine the resulting eligibility traces and their sparsity structure,\nWe demonstrate how custom sparse AD operates faster and more memory efficient than current state-of-the-art BPTT methods.\nThe source code of the project is available under https://github.com/jamielohoff/synaptax."}, {"title": "II. PRIOR WORK", "content": "Using various forms of surrogate functions, a wide range of methods have been developed for training spiking neural networks that approximate gradient backpropagation [Bohte et al., 2000, Neftci et al., 2019, Shrestha and Orchard, 2018]. In shallow, feed-forward networks, these gradient-based rules can be written in an online form [Bellec et al., 2020, Kaiser et al., 2020, Zenke and Ganguli, 2018] that can be interpreted as a three-factor rule [Gerstner et al., 2018]. In deep networks, the spatial credit assignment problem can be addressed by backpropagating in space and forward propagating in time [Bohnstingl et al., 2020, Kaiser et al., 2020]. Such synaptic plasticity rules generally have to be hand-crafted, especially for the forward propagation. While some methods have shown that temporal gradients can be ignored [Meng et al., 2023, Yin et al., 2023], these either use other approximate methods for computing traces or test on inherently static datasets where temporal integration is not necessary. Other approaches that do not use gradient backpropagation are event-prop, which uses the adjoint method to compute gradients exactly when the number of events is fixed [Wunderlich and Pehle, 2021]; parameter updates based on spike times [G\u00f6ltz et al., 2021]; and prospective coding which leverages the advanced response of the neuron with respect to their inputs [Brandt et al., 2024].\nTo study gradient-based synaptic plasticity rules, several software frameworks for SNN training have been developed. These are typically built on top of common machine learning libraries like PyTorch, TensorFlow or JAX. Particularly Py-Torch is very popular for implementation of such simulators with snnTorch and SpikingJelly being the most prominent [Eshraghian et al., 2023, Fang et al., 2023]. Additionally, Norse implements a spike-time based approach to gradient computation [Pehle and Pedersen, 2021]. These simulators enable simulation and training of SNNs with almost arbitrary network topology and neuron types on modern general purpose accelerators like GPUs. Another strain of frameworks puts more emphasis on the training of SNNs dedicated to specific neuromorphic hardware backends, thereby (sometimes artificially) limiting the set of operations. Examples include Lava and SINABS [Intel Labs, 2021, Sheik et al., 2023]. Other works have focused on accelerating gradient computations since they pose a significant bottleneck to SNN training where sequence lengths are generally in the hundreds. They leverage implicit differentiation and custom CUDA kernels for simple LIF models [Bauer et al., 2022].\nHowever, these methods require meticulously handcrafted kernels for every neuron type which makes them often unfeasible in practice. Recent works instead implemented SNN simulators in JAX, a general-purpose numerical computing library with numpy-like API. JAX-based simulators like SNNAX, Slax and Spyx take advantage of Just-In-Time (JIT) compilation to accelerate training, thereby achieving similar performance as methods with custom kernels [Heckel et al., 2024, Lohoff et al., 2024, Summe and Joshi, 2024]. One disadvantage of these works is the lack of interoperability with existing neuromorphic hardware, although initiatives like the Neuromorphic Intermediate Representation (NIR) aim to alleviate this issue [Pedersen et al., 2024].\nGiven the inherent inefficiency of gradient backpropagation in SNNs, several research efforts have investigated gradient computations with sparse activations [PerezNieves and Goodman, 2021]. However, taking advantage of this sparsity requires specialized accelerators and optimizing custom kernels [Finkbeiner et al., 2024]. Nearly all SNN simulators implement gradient-based plasticity rules using backpropagation, either because the underlying software framework only has limited support for forward propagation (e.g. PyTorch) or because it ignores the sparsity patterns induced by the approximations. Furthermore, this dependency on backpropagation forces these implementations to be inherently offline. Our work tackles both of these issues by using the recently published Graphax AD package to exploit the inherent sparsity patterns of popular gradient-based synaptic plasticity rules. At the same time our is automatically computes traces and their sparsity patterns for arbitrary neuron types using a AD method vertex elimination. This leads to a significant reduction in runtime and memory cost as shown in Table I."}, {"title": "III. GRADIENT-BASED SYNAPTIC PLASTICITY RULES AND AUTOMATIC DIFFERENTIATION", "content": "In machine learning, parameters are typically updated by computing gradients with respect to some cost function. AD computes gradients by first decomposing the computational graph of the desired model into its elemental operations whose derivatives are known and implemented into the respective framework. The gradients are then accumulated using the chain rule. Fundamentally, this involves only summation and product operations acting on the derivatives of the elemental functions. The result is the Jacobian or gradient of the respective model with respect to its parameters. Note that AD computes the Jacobian up to machine precision since there are no approximations made on the known partial derivatives or their accumulation at any stage.\nForward-mode and reverse-mode are the two most popular AD methods with reverse-mode being synonymous with backpropagation. Their main difference lies in the way the two methods traverse the computational graph to accumulate the gradient. Forward-mode traverses the computational graph in"}, {"title": "A. The Role of Time and Recursion Relations", "content": "Allowing the network to evolve in time, as for the case in Recurrent Neural Networks (RNNs), adds a new dimension to the development of gradient-based learning algorithms. Typically modern RNNs are modeled by discretizing time and binning data into time frames. In this case the general RNN recursion relation reads\n$h_{t+1} = f(h_t, x_t, \\theta),$\n(1)\nwhere $h_t \\in \\mathbb{R}^n$ denotes the hidden states, $x_t \\in \\mathbb{R}^k$ denotes external inputs, and $\\theta \\in \\mathbb{R}^p$ are the models parameters. These hidden states can be unrolled and treated similarly as a deep network. To compute the gradient, we run forward-mode or reverse-mode AD over the unrolled computational graph and arrive at Real-Time Recurrent Learning (RTRL) or Backpropagation Through Time (BPTT), respectively. The general mathematical formulation of RNN training with a cost function $L$ can often be decomposed into a sum of \u201cper-time-step\" cost functions:\n$L(Y,\\hat{Y}) = \\sum_t L_t(Y_t, \\hat{Y}_t)$\n(2)\nwhere, for some activation function $g$, $y = g(h_t, \\theta)$ are the predicted values and $\\hat{y}$ are the target values. Computing the gradient with respect to the model parameters $\\theta$, we get:\n$\\frac{dL}{d\\theta} = \\sum_t \\frac{\\partial L_t}{\\partial h_t} \\frac{dh_t}{d\\theta}$\n(3)\nThe total derivative of the hidden states with respect to the weights $(\\frac{dh_t}{d\\theta})$ can be computed in a recursive manner:\n$\\frac{dh_t}{d\\theta} = \\frac{\\partial h_t}{\\partial h_{t-1}} \\frac{dh_{t-1}}{d\\theta} + \\frac{\\partial h_t}{\\partial \\theta}$\n(4)\nWe define $H_t := \\frac{\\partial h_t}{\\partial h_{t-1}} \\in \\mathbb{R}^{n \\times n}, G_t := \\frac{dh_t}{d\\theta} \\in \\mathbb{R}^{n \\times n \\times p}, F_t := \\frac{\\partial h_t}{\\partial \\theta} \\in \\mathbb{R}^{n \\times n \\times p}$, and $G_0$ is initialized with zeros:\n$G_t = H_{t} G_{t-1} + F_t$.\n(5)\nThis is the recursion relation of RTRL [Williams and Zipser, 1989]. Note that the memory requirement of RTRL is independent of the number of time-steps, since it is not necessary to store any intermediate variables. However, for a simple neuron layer with $n$ neurons and $n$ output neurons, the tensor contraction $H_tG_{t-1}$ has $O(n^2p)$ time complexity which becomes $O(n^4T)$ for a fully connected layer where $p$ is the number of parameters per neuron. Since $G_t$ has to be stored and updated, memory scales as $O(n^3)$. It is also possible to find the total derivative in an anti-causal manner using a recursion relation that traverses the temporal axis in the opposite direction:\n$\\frac{dL}{dh_t} = \\frac{dL}{dh_{t+1}} \\frac{\\partial L}{\\partial h_{t+1}} \\frac{\\partial h_{t+1}}{\\partial h_{t}} + \\frac{\\partial L}{\\partial h_{t}}$\n(6)\nDefining $c_t := \\frac{\\partial L}{\\partial h_t} \\in \\mathbb{R}^{n*}$ and $d_t := \\frac{\\partial L}{\\partial h_t} \\in \\mathbb{R}^n$, we get:\n$c_t = c_{t+1}H_{t+1} + d_t$\n(7)\nThis is the recursion relation of BPTT [Williams and Zipser, 1989]. Computational complexity is more favorable for BPTT over RTRL because $c_t$ is a single vector, resulting in a $O(n^3T)$ scaling. It is important to note that we have to compute and store the $c_t$ at all times $t$ which leads to a memory scaling linearly with the number of time-steps $T$, e.g. $O(nT)$. Such a scaling can quickly become problematic for RNNs which require many time-steps, for example in SNN simulations. Furthermore, the traversal of the computational graph opposite of the temporal axis and the constant accessing of past internal states renders this algorithm none bio-plausible. RTRL is in principle able to compute gradients online, i.e. without requiring an additional backward pass or accessing past internal states, making it closer to how learning might take place in the brain. However, its substantial computational complexity and $O(n^3)$ memory, as is shown in Table I, severely limit its practical use.\nIn this work, we propose an AD framework that leverages the favorable memory scaling and online learning capabilities of RTRL for computations along the time dimension while still using backpropagation to compute the gradient of the network at every step. We achieve this by taking advantage of the inherent sparsity structure of equation (5) that is well known from other popular sparse approximate gradient-based synaptic plasticity rules [Bellec et al., 2020, Kaiser et al., 2020, Zenke and Ganguli, 2018]."}, {"title": "B. Exploiting Sparsity", "content": "The considerable computational cost of RTRL can be reduced by taking a closer look at the sparse approximations of the tensors $H_t$ and $F_t$: We will show that for SNNS $H_t$ can be decomposed as the sum of a diagonal matrix $H_{I,t}$ and an off-diagonal matrix corresponding to recurrent connections $H_{E,t}$. To understand why the sparsity arises, it is helpful to examine what these matrices track. $H_{I,t}$ tracks the influence of a neuron on itself and is thereby diagonal by definition. Influences by other neurons through recurrent connections are"}, {"title": "C. Sparse AD with Graphax", "content": "Graphax is a novel state-of-the-art AD interpreter that can leverage the sparsity of individual Jacobians to accelerate gradient computations [Lohoff and Neftci, 2024]. It builds on the conceptual framework of tensor-valued vertex elimination that utilizes efficient sparse matrix multiplications and additions.\n1) Computing Jacobians with Vertex Elimination: Vertex elimination is an AD method that accumulates the Jacobian by defining a vertex elimination operation that acts on the computational graph. We define a computational graph as a tuple $G = (V, E)$ with vertices $V$ as elemental operations $i$ and directed edges $E$ as outputs $v_i$ of $i$. The relation between vertices is stated with $i\\to j$, where $i$ has a directed edge connecting it to $j$ and elemental operation $i$ has an output $v_i$ which becomes the input to $j$. Every computational graph has as set of input nodes, which only have outbound edges and a set of output nodes, which only have inbound edges. We can identify the partial derivative of a function $j$ relative to an input $v_i$, i.e. $C_{ij} = \\frac{\\partial \\phi_j}{\\partial v_i}$, with the corresponding directed edge $(i, j) \\in E$. This enables us to define the vertex elimination operation:\n2) Handling Tensors in Vertex Elimination: In practice, many applications of AD require vector or tensor-valued functions such as element-wise multiplications or matrix multiplications. In the version presented above, vertex elimination would require a total decomposition of these highly structured operations into their single component operations, which is neither very useful nor computationally efficient. Instead, we allow the vertices themselves to be vector or tensor-valued, thereby promoting the partial derivatives associated with the edges to Jacobians in their own right which we call partial Jacobians.\nDepending on the structure of the operation $i$ performed on vertex $i$, we can infer the sparsity structure of the partial Jacobians. In most cases, these Jacobians will have a diagonal sparsity structure. In the LIF neuron layer example in Figure 2, most partial Jacobians are diagonally sparse. Diagonal sparsity is a form of sparsity in which the non-zero values of a matrix are found exclusively on its main diagonals. For real numbered matrices this is usually written as $M = diag(v)$ where $v \\in \\mathbb{R}^n$ and $M\\in \\mathbb{R}^{n \\times n}$. As with any kind of sparse matrix, a significant amount of computation can be avoided when multiplying or adding such matrices. For matrix multiplications and additions of size $n \\times n$, computational complexity can go from $O(n^3)$ to $O(n)$ and from $O(n^2)$ to $O(n)$ respectively. This insight can be extended to tensors of greater dimensions, in which the sparse diagonal can appear in two or more dimensions. For example, in a three-dimensional tensor $T \\in \\mathbb{R}^{n \\times m \\times m}$, with the last two dimensions being sparse diagonal, $T_{i} = diag(M_{i})$, where $M \\in \\mathbb{R}^{n \\times m}$ is the non-zero values of T. This can be written in simpler terms with the use of the Kronecker delta $\\delta_{ij}$: $T_{ijk} = M_{ij}\\delta_{jk}$.\n3) Efficient Sparse Tensor Contractions: The Graphax package is a JAX-based package that is able perform sparse tensor contractions efficiently and replace expensive generalized matrix multiplications with simple element-wise multiplications if the structure of the partial Jacobians allows. Graphax stores diagonally sparse tensors in their compressed form, e.g. a diagonal matrix is stored as a vector, a diagonal three-tensor is stored as a matrix etc. Internally, Graphax keeps track of how the compressed form and the actual tensor shape relate to each other and automatically finds the most efficient way to add or multiply two sparse tensors. This leads to significant improvements in runtime and memory utilization. Since it builds as an additional JAX function transformation, it is fully compatible with all other JAX transformations, including JIT compilation, vectorization, device parallelization and even JAX' own AD library. We utilize Graphax to implement an efficient version of gradient-based synaptic plasticity rules that are able to exploit the inherent sparsity discussed in previous sections. We start by fist computing the Jacobians $H_{1,t}$ and $F_t$ using the reverse-mode version of vertex elimination so that"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we support our theoretical claims by examining the peak memory usage as well as execution time per time-step. All of the following experiments were done with a simple feed-forward network with a single hidden layer of LIF neurons with an adaptive threshold (ALIF) and a non-spiking readout layer. Furthermore, all experiments either used the SHD benchmark data or similar synthetic data with the 700 input channels pooled by a factor of 5 to 140 input channels. We start by examining the gradients yielded by the combination of Graphax and the RTRL recursion relation. In the case of a feed-forward network with a single hidden layer, the gradients of this e-prop-inspired approach should exactly match the gradients computed with BPTT. The results are demonstrated in Table II where $\\Delta_e$ is the median absolute per-parameter deviation with the upper and lower values being the 2.5% and 97.5% quantiles. The gradients typically agree up to 10-6 since they are both computed up to machine precision.\nTo further verify the exactness of our method, we trained this simple SNN on the audio benchmark dataset Spiking Heidelberg Digits (SHD) [Cramer et al., 2020]. As would be expected from the equivalence of the gradients, the both training methods achieve the same average test accuracy over 10 training runs with different random seeds. As discussed in section III, our work is expected to have a more favorable performance when compared with respect to number of time-steps compared to BPTT and with respect to number of neurons compared to RTRL. In this work we verify these expectations and also demonstrate that leveraging the inherent sparsity of $H_{I,t}$, $G_t$ and $F_t$, as given in equation (5). In the following experiments, we typically compare four different cases to demonstrate the efficacy of our approach:\n\u2022\n\u2022\n\u2022\n\u2022\nBPTT We train the the SNN by differentiating the for-loop with reverse-mode AD using the JAX jax.lax.scan and jax.jacrev primitives. Since we only have a feed-forward network with a single hidden layer of spiking neurons, BPTT and e-prop gradients agree.\nRTRL The setup is the same as in BPTT, but instead of reverse-mode AD, we use forward-mode with jax.jacfwd.\nNa\u00efve e-prop We implement the recursion relation explicitly into the for-loop and compute the gradients of $H_{I,t}$ and $F_t$ using jax.jacrev. The experiment is necessary to show that the exploitation of the inherent sparsity in the recursion relation leads to a signification increase in performance. Note that in this case, we do the full tensor-contraction of equation Equation 5.\nOur work Our work uses Graphax' sparse AD primitive graphax.jacve to directly compute sparse gradients using vertex elimination. We then implement the recursion relation using these sparse gradients to accelerate the gradient computations and reduce memory consumption.\nWe set out to verify both the favorable compute and memory scaling by performing two experiments for each case. The first aimed to test different sequence lengths, where we fixed the"}, {"title": "A. Execution Time", "content": "We measured computational complexity as the average number of milliseconds for a single time-step. First we measured computational complexity for different sequence lengths as seen in Figure 4. We used a logarithmic scale for the time axis. We expected that they are constant for different sequence lengths. However, we observed a significant change from 2 time-steps to approximately 500 time-steps across all implementations. Since we used loop-unrolling in the jax.lax.scan primitive which enables the XLA compiler used by JAX to create a single kernel for multiple time-steps at once, we presume that this unrolling together with the scheduling of such operations on the GPU might be causing the observed behavior. For longer sequences, the time per step becomes approximately constant for three of four cases, with the exception of RTRL. This counter-intuitive behavior is intriguing, but a detailed analysis, possible at the compiler level, is out of the scope of this publication. We notice that the na\u00efve implementation performs over two orders of magnitude worse than our implementation, strongly supporting our claim that the exploitation of sparsity can lead to significant gains in runtime. Surprisingly, our implementation even seems to outperform the commonly used BPTT.\nSecondly we measured computational complexity for different number of hidden neurons as seen in Figure 5. We again used a logarithmic scale for the time axis. We see our"}, {"title": "B. Peak Memory Usage", "content": "The first experiment shows peak memory usage with a fixed number of hidden neurons (Figure 6). The peak memory usage for RTRL was at around 3.6 GB and stayed constant across time-steps (not shown in figure). As expected, BPTT depends linearly on the number of time-steps, while our implementation, like RTRL, is constant in memory. An unexpected result came from the na\u00efve implementation, in which it slowly decreased in peak memory usage. We speculate that this is due to loop optimizations in the XLA compiler. Nevertheless, our implementation outperforms all of the others, beating B\u0420\u0422\u0422 across the board due to its constant memory requirements.\nFigure 7 shows peak memory usage as a function of the number of hidden neurons with fixed number of time-steps. We expect the peak memory usage of BPTT and our implementation to scale approximately linearly with the number of hidden neurons. Meanwhile, the na\u00efve implementation and RTRL are expected to have a quadratic increase with their peak memory usage with increasing number of neurons as seen in Table I. We observe a slightly superlinear scaling among the results for BPTT and our implementation, but still verify that both significantly outperform RTRL and the na\u00efve e-prop implementations. Although the peak memory usage between our implementation and BPTT is at most 250 MB, our implementation remained consistently lower across different numbers of hidden neurons."}, {"title": "V. CONCLUSION", "content": "We demonstrated a general, sparsity-aware gradient-based synaptic plasticity, which shows improved compute and memory complexity across scales. Namely, we demonstrated the superior constant-in-time memory scaling of our approach similarly to RTRL, which enables the training spiking neuron models on long sequences. This is in stark contrast to many of the current state-of-the-art implementations that rely on BPTT and do not utilize the inherent sparsity, whose memory grows with sequence length Furthermore, our Graphax AD framework made our implementation scalable and generalizable across different hardware backends and arbitrary neuron types (e.g. LIF and ALIF) without putting any additional requirements on the user. Thus, our work enables bio-plausible synaptic plasticity at scale with AD that truly exploits the features of the underlying approximations. While all our experiments were performed on a simple feed-forward network with a single hidden layer of spiking neurons and a non-spiking readout layer, they can be straight-forwardly generalized to the multi-layer case. In this case, the equivalence of the gradients with BPTT is no longer true. However, a rich body of past work has addressed this issue already and successfully illustrated that this does not significantly inhibit learning[Bohnstingl et al., 2020, Kaiser et al., 2020]. Generalizing our approach to the multi-layer case is an exciting research avenue that we intent to pursue in future work."}]}