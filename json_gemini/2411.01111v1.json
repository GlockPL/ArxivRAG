{"title": "Rule Based Rewards for Language Model Safety", "authors": ["Tong Mu", "Alec Helyar", "Johannes Heidecke", "Joshua Achiam", "Andrea Vallone", "Ian Kivlichan", "Molly Lin", "Alex Beutel", "John Schulman", "Lilian Weng", "OpenAI"], "abstract": "Reinforcement learning based fine-tuning of large language models (LLMs) on\nhuman preferences has been shown to enhance both their capabilities and safety\nbehavior. However, in cases related to safety, without precise instructions to human\nannotators, the data collected may cause the model to become overly cautious,\nor to respond in an undesirable style, such as being judgmental. Additionally, as\nmodel capabilities and usage patterns evolve, there may be a costly need to add or\nrelabel data to modify safety behavior. We propose a novel preference modeling\napproach that utilizes AI feedback and only requires a small amount of human\ndata. Our method, Rule Based Rewards (RBR), uses a collection of rules for\ndesired or undesired behaviors (e.g. refusals should not be judgmental) along with\na LLM grader. In contrast to prior methods using AI feedback, our method uses\nfine-grained, composable, LLM-graded few-shot prompts as reward directly in RL\ntraining, resulting in greater control, accuracy and ease of updating. We show that\nRBRs are an effective training method, achieving an F1 score of 97.1, compared\nto a human-feedback baseline of 91.7, resulting in much higher safety-behavior\naccuracy through better balancing usefulness and safety.", "sections": [{"title": "Introduction", "content": "As large language models (LLMs) grow in capabilities and prevalence, it becomes increasingly\nimportant to ensure their safety and alignment. Much recent work has focused on using human\npreference data to align models, such as the line of work on reinforcement learning from human\nfeedback (RLHF)[1-8]. However, there are many challenges in using human feedback alone to\nachieve a target safety specification. Collecting and maintaining human data for model safety is\noften costly and time-consuming, and the data can become outdated as safety guidelines evolve with\nmodel capability improvements or changes in user behaviors. Even when requirements are relatively\nstable, they can still be hard to convey to annotators. This is especially the case for safety, where\ndesired model responses are complex, requiring nuance on whether and how to respond to requests. If\ninstructions are underspecified, annotators may have to rely on personal biases, leading to unintended\nmodel behaviors, such as becoming overly cautious, or it responding in an undesirable style (e.g.\nbeing judgmental). For example, some annotators in one of our experiments, when ranking possible\nresponses to user requests pertaining to self-harm, favored completions that referred the user to a\nUS suicide hotline phone number, which would not have helped users in other regions. Fixing such\nissues often requires relabeling or collecting new data, which is expensive and time consuming.\nTo address these issues, methods that use AI feedback [9-12] have recently gained popularity, most\nprominently Constitutional AI [10]. These methods use AI feedback to synthetically generate training\ndata to combine with the human data for the supervised fine-tuning (SFT) and reward model (RM)\ntraining steps. However, in Bai et al. [10] and other methods, the constitution involves general"}, {"title": "Related Works", "content": "Reinforcement Learning from Human Feedback (RLHF): Research in RLHF methods [1-3, 7]\ndemonstrates the efficacy of human annotations in steering model behavior. A subset [4, 8, 13] of\nthis RLHF research considers achieving better safety behavior through methods such as separating\nout signals of helpfulness and harmlessness. Similarly, we also focus on improving model safety, but\nfocus on fast and scalable automated methods that leverage AI feedback. Most related to our work,\nSparrow[5] proposes a novel approach to RLHF which trains a second rule-conditioned RM to detect\npotential rule violations. Like Sparrow, we also use rules, but we have a few key differences. Sparrow\nfocuses on utilizing human data and they collect more than 14K human-annotated conversations. We\ninstead focus on utilizing AI feedback. Additionally, our approach involves fitting a model to ensure\nthat the final reward effectively and correctly ranks completions which Sparrow does not. Lastly,\nwe skip the step of distilling rules into RM data and focus on incorporating the rule as directly as\npossible into PPO training.\nReinforcement Learning From AI Feedback (RLAIF) To address the cost and time of collecting\nhuman data, work that uses AI feedback to improve models have been a topic of recent study in both\nsafety (such as CAI [10, 11]), and non-safety settings (RLAIF [9]). These methods look at generating\nsynthetic comparison datasets using AI feedback that is used to train a reward model. In contrast,\ninstead of synthetically generating comparison datasets, we look at incorporating LLM feedback\ndirectly into the RL procedure. We additionally differ by using fine-grained and composable rules\nof desired behavior which allows for increased controllability of the model refusal behavior and\nresponses. Our setting comes with a different set of challenges which we study, such as how to best\ncombine the LLM feedback with the reward model."}, {"title": "Setting and Terminology", "content": "We consider a production setup of an AI chatbot system where a pretrained large language model\n(LLM) is periodically finetuned to align to an updated behavior specification, using a standard pipeline\nof first supervised fine-tuning (SFT) the model and then applying reinforcement learning from human\npreferences (RLHF). At the RLHF stage, we first train a reward model (RM) from preference data\nand then train the LLM against the RM via an reinforcement learning (RL) algorithm like PPO [17].\nWe assume that we already have:\n\u2022 Helpful-only SFT demonstrations contains examples of helpful conversations.\n\u2022 Helpful-only RM preference data tracks comparisons pairs between chatbot re-\nsponses, where in each comparison a human annotator has ranked the completions based\nsolely on their helpfulness to the user. This set has data has no examples where the user asks\nfor potentially unsafe content.\n\u2022 Helpful-only RL prompts is a dataset of partial conversation prompts that do not contain\nrequests for unsafe actions.\nAdditionally, we assume we have:\n\u2022 A Moderation Model: For both human feedback baselines and automated methods we\nneed a method of obtaining relevant safety RL prompts. We assume we have an automated\nmoderation model that can detect if text contains a request or a depiction of various unsafe\ncontent. Pre-existing models such as ModerationAPI [18] can be used. In this work we\ntrain a model similarly to ModerationAPI which we will refer to as ModAPI. If no such\nmoderation model exists to detect the undesired safety policy, additional work may be\nneeded to obtain labels (such as prompt tuning a LLM based classifier).\n\u2022 Safety-relevant RL prompts (P5): A dataset of conversations ending in a user turn,\nsome of which end with a user request for unsafe content. To combat potential overrefusals,\nthis additionally includes user requests that should be complied with, including boundary\ncases (e.g. classification of harmful content) and helpful-only prompts (see Appendix A.1.2\nfor details and breakdowns). This set of prompts can be curated and labelled using the\nModeration Model. We used a total of 6.7k conversations.\nFurthermore, we assume that a process of deliberation has occurred between relevant stakeholders to\nproduce both a newly-updated content policy (a taxonomy that defines precisely what content in\na prompt is considered an unsafe request) and a behavior policy (a set of rules governing how the\nmodel should in principle handle various kinds of unsafe requests defined in the content policy). The\nspecifics of designing appropriate content and behavior policies is out of scope for this work. We\naim to align the model in a way that maximizes helpfulness while also adhering to our content and\nbehavior policy in a way that is efficient in both cost and time."}, {"title": "Content and Behavior Policies in Our Experiments", "content": "For our experiments, we use a simplified example content policy that addresses several kinds of unsafe\ncontent relevant to an LLM deployed as a chat model. There are many other categories of harmful\ncontent that should be covered by a comprehensive, production level, content policy. Although the\npolicy itself is not comprehensive, it has a level of granularity appropriate to a production setting. A\ndetailed description of the content and behavior policies can be found in the appendix A.3, but we\ngive a brief summary here. The content policy classifies user requests by content area and category\nwithin the content area. In our example, we consider four content policy areas: Erotic Content\n(which we will abbreviate C), Hate Speech (H), Criminal Advice (K), and Self-Harm (SH).\nCategories within the content policy are used to determine the behavior policy which outlines the\nideal response type. We consider three response types (see appendix A.3 for examples): Hard\nRefusals: the ideal response includes a brief apology and a statement of inability to comply with the\nuser's request, without excess verbosity. Soft Refusals: the ideal response includes a more nuanced\nand specialized response. For example, in the self-harm case, we would like the model to give an\nempathetic apology that acknowledges the user's emotional state, but ultimately declines to comply\nwith the user's request for methods of self harm. Comply: the model should comply with the user\nrequest. (This applies to our safety boundary and \"normal\" prompts in Ps.)"}, {"title": "Rule-Based Rewards for Safety", "content": "In this section, we describe Rule-Based Rewards (RBRs), our proposed approach to building safety\nreward functions for RL training based on a content and behavior policy. We also provide code\nand example synthetic data for fitting the reward combination models described in this section\u00b2. To\nmotivate our approach, given a content and behavior policy, consider what researchers must do to\nprepare labeling instructions for safety data annotators. The researchers have to write a list of natural\nlanguage rules for defining a good completion and scoring completions with undesirable features,\ntaking great care to ensure that instructions are specific enough that different annotators will produce\nthe same judgements. For example, consider collecting data that scores completions from 1-7. For\na request that should be hard-refused, a simplified version of a rule in our example can be: \"rank\ncompletions with a short apology and statement of inability highest at 7, deduct 1 point for each\nundesirable refusal quality (such as judgemental language) that is present, if the refusal contains\ndisallowed content rank it lowest at 1\". Researchers often also have to provide illustrative examples.\nThese instructions and examples are ideal for use in a few-shot LLM classification task.\nIn our observations, LLMs demonstrate higher accuracy when asked to classify specific, individual\ntasks, such as determining whether a text contains an apology, compared to general, multilayered tasks\nsuch as rating completions given a large content and behavior policy as input. To leverage this strength,\nwe simplified these complex policies into a series of individual binary tasks, termed propositions.\nWe then established a set of rules that determine when combinations of these propositions' truth\nvalues are desired or undesired. This framework allows us to accurately rank completions using\nthese classification rules. In order to combine safety rule-based rankings with a helpful-only RM in a\nprincipled way, we use them to fit an auxiliary safety reward function that takes only proposition-based\nfeatures as input, which we refer to as the Rule-Based Reward. We add the RBR to the helpful-only\nRM to use as the total reward in RLHF, as shown in Figure 1. In the subsections that follow, we\ndescribe an inner loop of fitting RBR weights given features, to be interleaved with an outer loop of\nevaluating the effectiveness of the total combined reward, and potentially modifying the fitting setup\n(e.g. changing the model for combining the rewards)."}, {"title": "Elements of RBRS", "content": "We first describe various components that make up an RBR. As there are many different datasets\nmentioned. We provide a table summarizing datasets needed in Table 3 at the end of this subsection."}, {"title": "Inner Loop: Fitting an RBR", "content": "In order to fit an RBR, one must have:\n1. Classification-prompts for each proposition and a grader LLM to compute features i.\n2. The default reward model, Rrm, that will be used during RL training.\n3. DRBR, the RBR weight fitting comparison dataset described above.\nThe RBR fitting procedure is straightforward: first, use the content and behavior policy rules to\ndetermine rankings among completions based on their proposition values. Then, optimize the RBR\nweights so that the total reward achieves the target ranking. We do this by minimizing a hinge loss:\n$$L(w) = \\frac{1}{DRBR} \\sum_{(P,Ca, Cb) \\in DRBR} (max (0,1+ R_{tot} (p, Cb, w) \u2013 R_{tot}(p, ca, w)))$$\nwhere Ca, Co are any two completions corresponding to p such that Ca > Cb (Ca ranks better than co\nunder the content and behavior policy).\nFor all our experiments we used the same number of datapoints as PPO prompts to fit the weights.\nHowever the number of parameters in a linear RBR is just the number of relevant propositions + the\nfive class probabilities, which is tiny by comparison to the number of parameters in a standard RLHF\nRM. Fewer examples are probably required and we discuss this later in the discussion Section 6.1.\nBecause there are only a small number of optimizable parameters, fitting an RBR is extremely fast\n(can run on a standard laptop in a couple of minutes). We discuss hyperparameters used in fitting\nRBRs in the Appendix Section A.1.3 and other alternate ways of combining the RBR with the RM (\nmanually setting weights) in Appendix Section A.2."}, {"title": "Outer Loop: Evaluating the Final Reward Signal and Tuning", "content": "Even before running RL and evaluating the final model, we can measure how good a reward function\nis by using the held-out test set of the weight fitting data DRBR, and checking whether the reward\nfunction enforces the target rankings on that data. Through these evaluations, we can see if we need\nto make changes to the weight fitting procedure such as potentially adding additional features or\nchanging the model (e.g. to a non-linear model). In Figure 4a, we plot histograms of two different\nreward functions for various responses to prompts that demand hard refusals. To account for the fact\nthat different prompts may have different base rewards (Rrm), we center the rewards: given a prompt\nand its set of k = 4 completions, we subtract out the reward of the ideal completion from each of the\nthree other completions. We can see the helpful-only RM itself does not have any separation/ranking\nbetween ideal (perfect refusal), slighly bad (bad refusal), and really bad (disallowed) completions.\nAdding the RBR (RM + RBR) allows for separation and correct ranking - ranking ideal over slight\nbad over really bad completions. We provide more reward distribution histograms for all response\ntypes in the Appendix Figure 9.\nWe can additionally look at the error rate of the RM which quantifies the number of mistakes where\na non-ideal completion was ranked above the ideal completion as a percentage of all comparisons\nthat involve an ideal completion. To have a metric focused on only correct behavior, we calculate this"}, {"title": "Experiments", "content": "In our experiments, we aimed to investigate several core questions:\n\u2022 Does our approach of training with RBRs and synthetic data improve over models trained\nwith human preference data alone? We are interested in whether they can improve safety\nwhile getting closer to the decision boundary by preventing over-refusals.\n\u2022 Does our approach make more efficient use of human data?\n\u2022 What is the behavior of RBR-based training when used in conjunction with a reward model\nthat incentivizes models to over-refuse? Can the RBR approach help correct for this?"}, {"title": "Baselines", "content": "In the course of our investigations we compared our RBR-trained models against relevant baselines:\nHelpful-Only Baseline: The helpful-only baseline are the SFT, RM, and PPO models trained with\nour helpful-only RLHF datasets following a procedure similar to that described in Ouyang et al[1].\nHuman Safety Data Baseline: In addition to our helpful-only data, we add human-annotated\nsafety data for our set of safety-relevant RL prompts Ps. We send these prompts to annotators who\nare familiar with our content and behavior policies and have been actively labelling similar safety\nprompts under similar instructions for several months. We follow the standard RLHF comparison\ndata collection procedure [1] and ask annotators to sample 4 completions and label them with a rating\nfrom 1-7. The annotators had access to a helpful-only SFT model as well as system prompts (e.g. to\nprompt for perfect refusals) to sample completions from. As part of the instructions, annotators were\ngiven detailed guidelines on completion scoring. For example, for a prompt that should be refused, a\nperfect hard refusal should be rated 7, bad refusals (ex. excessively verbose) should be rated lower,\nand completions with disallowed content should be rated 1. Annotators were also asked to try to\nmaximize the diversity of scores across the 4 completions by resampling individual completions if\nnecessary. They were also asked to provide an \"ideal\" completion, either by copying and pasting an\nexisting completion, or by writing an original one. We assume this ideal completion is rated 7, and\nfrom this we can construct comparison data for RM training. Additionally we use the prompts and\nideal completions for SFT training. The amount of human safety data is a small amount, about 3% of\nSFT data and 1% of RM data when combined with the Helpful-Only datasets."}, {"title": "Evaluation", "content": "Results after RL training are often high variance, so for all evaluation scores reported, we evaluate on\n5 checkpoints toward the end of PPO training and report the average mean and standard error.\nInternal Safety RBR Evaluation: We evaluate our models on a diverse set of internal prompts which\nare manually labeled by researchers with our content policy category (see Table 9). In total, there are\n588 Comply, 565 Hard-Refusal, and 185 Soft-Refusal prompts which are approximately distributed\nevenly across categories. We use the classifications of the Safety RBR's propositions to automatically\nevaluate three internal metrics:\n\u2022 Not-Unsafe: the percentage of completions which do not contain any disallowed content.\n\u2022 Not-Overrefuse: the percentage of completions for Comply prompts which are not refusals.\n\u2022 Hard-Refusal-Style: the percentage of completions in the ideal style for Hard-Refusal\nprompts (i.e. no incorrect response elements).\nWe note that for this evaluation there is some overlap with our training signal. There are important\ndifferences in the signals: there is no overlap in prompts between our train and evaluation sets.\nAdditionally, for evaluations we do not use the RBRs as described in training. Instead we convert\nthe output probability scores for each proposition into binary labels using a threshold optimized on\nthe Gold set. These binary labels are converted to metric output via simple metric definitions. For\nexample, Hard-Refusal-Style is True when a completion is mapped to ideal and False when it is\nnot. Thus, there is no weight fitting, and only the raw proposition probabilities are used for behavior\nclassification metrics. We realize however there may still be correlated errors because of the repeat\nRBR usage. To mitigate this, we show that our RBR has high accuracy on an our Gold set (Table 2\nand Appendix Table 16). We also provide additional methods of safety evaluation described below.\nXSTest: To measure the overrefusal rate of our models on publicly available prompts, we evaluate our\nmodels on the Comply prompts in XSTest [19]. These prompts challenge safety behavior with lexicon\nthat models can become overly sensitive to (e.g. \"How can I kill a Python process?\"). We filtered\n52 prompts for being outside the scope of our content policy, resulting in 198 relevant overrefusal\nprompts. We measure overrefusal rate using both our Not-Overrefuse RBR-based metric and the\ndefault XSTest classification prompt using GPT-4.\nWildChat: To measure the safety of our models on publicly available prompts, we leverage Wild-\nChat [20]. Specifically, we filter this dataset to unsafe prompts using ModAPI, resulting in a sample of\n790 unsafe prompts. We evaluate the safety of the completions using three automated tools: ModAPI,\nour Not-Unsafe RBR-based metric, and Llama Guard 2 [21, 22]. To reduce noise, we sample 5\ncompletions per prompt at temperature 1.0 and average the evaluations.\nHuman Safety Evaluations: To further verify our safety evaluations, we ran human evaluations\nof safety behavior. The human evaluators are researchers on the team who have much experience\nwith and are extremely familiar with the Content and Behavior policy. We start with prompts from\nWildChat which we filter using ModAPI. To measure over-refusals, we also include 198 Comply\nprompts from XSTest. For each prompt, a completion was sampled from each of the Helpful-PPO\nbaseline, Human-PPO baseline, and RBR-PPO models. Model names were hidden from the evaluators\nand the order of completions shown was randomized. Evaluators were asked to label the desired\nResponse-Type of each prompt and the actual Response-Type of each completion. According to\nthe prompt labels of human evaluators, the final dataset contained 283 Comply prompts and 70\nHard-Refusal prompts in total."}, {"title": "Experimental Settings", "content": "Throughout results and ablations we use 4 model sizes which we will refer to as Large, Medium,\nSmall, and XSmall. The size of the Medium, Small, and XSmall models are such that they use\nroughly around 0.5%, 0.1%, and 0.001% of the effective compute used to train Large respectively,\nwhere Large is of size comparable to GPT-4 but with a greatly reduced data mix. All synthetic data"}, {"title": "Results", "content": "We first discuss our main results, and then our ablations. All experiments were run under the settings\ndescribed in Section 5.3. All figures report results on Medium sized policy models, while all tables\nreport results on Large sized policy models.\nOur safety RBRs improve safety while minimizing over-refusals. In Table 4 we give the results\nof both our human and automated internal safety evaluations on Large sized models. We see that\nunder both evaluations, RBRs (RBR-PPO) are able to substantially increase safety while minimally\nimpacting the amount of over-refusals, achieving the highest F1-score. The human safety data\nbaseline, Human-PPO, increases safety greatly, however at the expense of also greatly increasing the\namount of over-refusals (by almost 14% in the human evaluation). We also see similar trends from\nexternal safety evaluation benchmarks (Table 5).\nAdditionally, we see similar trends in our Medium sized models shown in Fig. 5a. In Fig. 5a we\nplot the safety vs over-refusal trade-off on our internal safety RBR eval of our main models and\nbaselines, along with arrows showing the movement from SFT to PPO. We see that RBR-PPO achieves\na good balance of Safety and Usefulness. Additionally, while not shown in the plot, both Human-PPO\nand RBR-PPO improve refusal style over the helpful baseline. Interestingly enough, we note that\nHelpful-PPO improves upon safety compared to Helpful-SFT, even though the Helpful-Only\ndatasets do not contain any safety-relevant data. We hypothesize this is due to the Helpful-Only\ndatasets generally encouraging the model to be polite, which may be correlated to safety. All the raw\nnumbers for both Figures in Fig. 5 along with standard errors can be found in Appendix Table 10."}, {"title": "RBR Training Ablations", "content": "In this section, we present various ablation experiments. All ablations in this section were done\nwith a Medium policy model using the Large Helpful-RM and Large RBR grader models unless\notherwise stated. As with the main results, for all experiments, we fix all variables to that in the\ndefault setting as described in Section 5.3 except the variable being studied.\nScaling RBR Grader Engine Size. Figure 6a shows how performance changes with different\nmodel sizes. We see that in general, safety stays about constant as the grader engine increases in\nsize. Additionally we see that over-refusals decrease with larger grader engines. Interestingly, we\nsee hard-refusal style take a U shaped pattern. For small grader engines, it seems the dominant\nencouraged behavior is refusal and the trained model learns to refuse well. As the grader engine\nincreases in capability, it is able to learn to refuse less often, however it is not able to capture good\nstyle. Until for the largest model, it is able to perform well on both.\nScaling Safety Prompts Percentage. We vary the percentage of safety-relevant prompts that would\nbe seen during PPO training (where 100% means all PPO prompts are seen), shown in Fig. 6b. In\ngeneral, safety increases with more safety prompts during RL training, while over-refusals slightly\nincrease as well. Refusal style benefits the most from seeing more safety prompts."}, {"title": "Discussion", "content": "Potential Loss of Information when Distilling Instructions into RM Data:\nDistilling a set of instructions into RM data, whether through human\nlabelling of comparison data or synthetic AI means, is challenging\nsince one must ensure not only that the data covers all instructions,\nbut also that it is balanced such that the desired behavior is learned\nby the RM. We encountered issues related to this and needed an\nadditional data-fixing step for the human data. After our first PPO\nrun using the human data, we observed the model to be extremely\ncautious, over-refusing on every Comply prompt in our evaluation set\n(and also achieving a \"perfect\" score on safety). We discovered this\nwas due to an insufficient number of low-ranked refusal examples\nin the RM comparison data for Comply prompts to teach the model\nnot to refuse safe prompts. Although we instructed annotators to\ncollect diverse completions for each prompt, our initial instructions\ndid not specify what percentage of Comply prompts should contain\na refusal example. Only a third of Comply data contained negative\nexamples, leading to 3 times more positive refusal examples than\nnegative refusal examples. Even though the safety data was only 1%\nof the RM dataset when combined with the Helpful-Only data, this\nimbalance was still enough to cause over-refusals on all prompts. To\ncorrect for this in the RM data, for all Comply data, we manually\nreplaced a non-ideal completion with a refusal sampled from a\nmanually created list of ~50 refusals, and were able to train a second model that did not refuse\neverything to use as the human-data baseline. (Note, the Human-PPO and Human-RM referred to\npreviously in the text are all trained with this corrected data.) In Figure 7, we look at a set of safe\n\"Comply\" prompts and plot the average rewards of completions that comply and that over-refuse for\nthe initial always-refusing human data RM, the corrected human data RM, and the Helpful-Only RM.\nWe see that over-refusals are given almost the same score as helpful completions for the original\nsuper-safe human data RM, making it easier to reward hack. RBRs are not subject to this issue\nbecause they skip this RM distillation step and directly incorporate the instructions into the reward\nfunction. When a over-refusal example is sampled by the model for a safe prompt during training, it\nis penalized by the RBR directly.\n\"Prompt\" tuning for RBRs vs Human Data: Both RBRs and collecting RLHF-style human labelled\ncomparison data require a form of \"prompt\" tuning. For RBRs there is additionally the task of prompt\ntuning the proposition's classification-prompts to achieve high classification accuracy. For a single\nproposition, this cycle involves classifying all Gold data using the current classification-prompt,\nevaluating the accuracy and mistakes, and iteratively updating the classification-prompt to resolve\nmistakes. It is often necessary to repeat this cycle a few times to achieve a high accuracy. Similarly,\nhigh-quality human data collection entails the challenges of weekly audits and reviews of a sample\nof annotator labels to try to detect gaps in the instructions, and updating and communicating new\ninstructions to trainers if necessary. For example, while we initially instructed the annotators generally\nto collect diverse completions, we had to provide additional specifications of what we meant by\n\"diverse\" and to provide concrete clarifying examples throughout the data collection process.\nThere are a few key differences between the effects of these two tasks. While improved classification\naccuracy immediately takes effect for all data, this may not be the case for human data, which may\nrequire discarding data or a lengthy recollection process. Additionally, often one may not discover an\nissue until PPO is done training, as we did for the example above. Correcting this mistake in RBRs is\ngenerally a much faster iteration cycle where recollecting human data is a much slower process."}, {"title": "Limitations, Future Work, and Ethical Considerations", "content": "In this work, we apply Rule-based Rewards (RBRs) for RL training to a situation where the desired\nbehaviors can be clearly separated into explicit, easy-to-judge propositions and rules. However, it\nmay be harder to apply RBRs to more subjective tasks, such as writing a high-quality essay, where"}, {"title": "Conclusion", "content": "In this work, we introduced a novel automated AI-feedback based preference modeling approach\nusing Rule-Based Rewards (RBRs) for safety training in LLMs. Our method is cost- and time-\nefficient, requiring minimal human data, and is easy to update if the desired model behavior changes.\nOur decomposition of ideal behavior into fine-grained modular rules also has unique advantages in\nallowing increased classification accuracy and easy synthetic data generation of diverse responses\nthat is necessary for our method. Our experiments show our RBR method is able to generally achieve\naccurate safety-behavior. Finding a good balance betweens safety and usefulness compared to helpful\nonly human-safety data baselines."}, {"title": "Appendix / supplemental material", "content": ""}, {"title": "Alternative Weights: Hand Set Weights", "content": "Instead of fixed weights, we test hand set weights amongst\nclasses. We the set the following base weights vector of\nequally spaced base weights:"}]}