{"title": "Quantum Machine Learning Architecture Search via Deep Reinforcement Learning", "authors": ["Xin Dai", "Tzu-Chieh Wei", "Shinjae Yoo", "Samuel Yen-Chi Chen"], "abstract": "The rapid advancement of quantum computing (QC) and machine learning (ML) has given rise to the burgeoning field of quantum machine learning (QML), aiming to capitalize on the strengths of quantum computing to propel ML forward. Despite its promise, crafting effective QML models necessitates profound expertise to strike a delicate balance between model intricacy and feasibility on Noisy Intermediate-Scale Quantum (NISQ) devices. While complex models offer robust representation capabilities, their extensive circuit depth may impede seamless execution on extant noisy quantum platforms. In this paper, we address this quandary of QML model design by employing deep reinforcement learning to explore proficient QML model architectures tailored for designated supervised learning tasks. Specifically, our methodology involves training an RL agent to devise policies that facilitate the discovery of QML models without predetermined ansatz. Furthermore, we integrate an adaptive mechanism to dynamically adjust the learning objectives, fostering continuous improvement in the agent's learning process. Through extensive numerical simulations, we illustrate the efficacy of our approach within the realm of classification tasks. Our proposed method successfully identifies VQC architectures capable of achieving high classification accuracy while minimizing gate depth. This pioneering approach not only advances the study of AI-driven quantum circuit design but also holds significant promise for enhancing performance in the NISQ era.", "sections": [{"title": "I. INTRODUCTION", "content": "Quantum computing (QC) holds the potential to revolutionize computational tasks, offering distinct advantages over classical computers [1]. The convergence of advancements in quantum hardware and machine learning applications has sparked a growing interest in exploring the synergies between these cutting-edge technologies. Although existing quantum computers still suffer from noise, a promising solution lies in a hybrid quantum-classical framework. Here, computational tasks are divided into two parts: one executed on a quantum computer and the other on a classical computer [2], [3]. Central to this paradigm is the Variational Quantum Algorithm (VQA) [2], [3], which serves as the cornerstone of hybrid computing.\nQuantum machine learning (QML) algorithms heavily rely on VQAs, utilizing variational quantum circuits (VQCs) as trainable components akin to classical neural networks. QML has demonstrated remarkable success across various domains, including classification [4]\u2013[8], time-series modeling [9], natural language processing [10]\u2013[13], generative modeling [14]\u2013[16], and reinforcement learning [17]\u2013[24]. While existing QML models have shown promise, they often require expert knowledge to design effective quantum circuit architectures. For instance, the configuration of encoding and variational subcircuits within VQCs significantly influences model performance and the realization of potential quantum advantages [25]. Moreover, the vast search space of VQCs presents a challenge, given the multitude of possible circuit architectures. The necessity for expertise in quantum circuit design poses a barrier to the widespread adoption of QML techniques beyond the quantum computing community, limiting their application in other scientific domains.\nIn this paper, we address the challenge of designing QML models by introducing a novel approach called deep reinforcement learning with adaptive search of learning targets (RL-QMLAS). Our method, shown in Figure 1, focuses on classification tasks, wherein the RL agent's objective is to discover an optimal quantum gate sequence. Furthermore, we incorporate an adaptive learning threshold, dynamically adjusting the reward scheme during RL training to enhance the agent's learning process, enabling the model to acquire high-performing policies effectively. Through numerical simulations, we demonstrate that our proposed methods can effectively generate VQC architectures. These architectures achieve high classification accuracy without requiring prior physical knowledge and maintain a shallower circuit depth compared to manually crafted architectures. In addition, the adaptive learning target can enhance the agent learning process and reduce the need for a high pre-defined learning target. This paper is organized as follows: SectionII provides a brief survey on current development of QAS. In SectionIII, we describe the concept of VQC, which is the core of existing QML models and the target the proposed framework is to search for. We formulate the QAS problem in Section IV and describe the RL techniques used in this work in Section V. We provide the details of simulation in Section VI and results in Section VII. Finally conclude in Section IX."}, {"title": "II. RELEVANT WORKS", "content": "Machine learning techniques have been applied to tackle various quantum computing challenges such as quantum architecture search (QAS). The target task of a QAS might be generating a desired quantum state [26]\u2013[36], finding an efficient circuit for solving chemical ground states [36]\u2013[40], solving an optimization task [36], [38], [41]\u2013[46], optimizing a given quantum circuit for a particular hardware architecture [47], compiling a circuit [48]\u2013[50] or performing a machine learning task [42], [43], [51]\u2013[57]. Various approaches are employed to find the optimal circuit for specified tasks. For example, the works [26]\u2013[29], [31], [33], [34], [37], [41], [47], [50] consider the reinforcement learning based methods while the works [32], [51]\u2013[53] works use different variants of evolutionary algorithms to search for the circuit. Differentiable QAS methods are also developed to leverage the highly successful gradient-based methods [43]\u2013[45], [55]. Different ways of encoding the quantum circuit architecture are devised. For example, the works [39], [42] propose graph-based method while the work [47] consider the convolutional neural network based method to encode the quantum circuit architecture.\nRegarding the circuit performance metric, it can be a direct evaluation of the circuit performance on the particular task [37], [38], [51] or the closeness of the generated circuit to the actual circuit [26], [27], [42]. To reduce the computational resource required in direct evaluation, certain predictor-based methods are proposed to use neural network to predict the quantum model performance without direct circuit evaluation [40], [56]. The proposed method in this paper is to further generalize the concepts used in [26], [27] to more than finding a quantum circuit to synthesize a particular quantum state, but can actually perform a QML task. This paper further generalize the methods proposed in the work [37] to QML tasks. Our work is also different from previous works on quantum circuit optimization [47], since in our work, circuit ansatz are not provided. Though optimizing an existing circuit ansatz can decrease training time, it is not without cost; the ansatz necessitates input from quantum experts. Our approach investigates the prospect of whether, in the absence of a predefined ansatz, an agent can autonomously discover high-performing circuits."}, {"title": "III. VARIATIONAL QUANTUM CIRCUITS", "content": "Variational quantum circuits (VQC), also known as parameterized quantum circuits (PQC) is a special kind of quantum circuit with trainable parameters which can be trained via gradient-based [4], [58], [59] or gradient-free [60] methods. This kind of circuits play a crucial role in the hybrid quantum-classical computing paradigm in which certain computing tasks are implemented on quantum computer while tasks not suitable for existing quantum computers are carried out by classical computers. Consider an n-qubit system. The fundamental components of a VQC (illustrated in Figure 2) include the encoding circuit $U(x)$, responsible for transforming the classical input vector $x$ into a quantum state $U(x) |0\\rangle^{\\otimes n}$, the variational circuit $V(\\Theta)$, serving as the actual learning component with trainable parameters $\\Theta$, and the final measurement operation, used to extract information from the circuit. The VQC used in this work can be expressed as $f(x; \\Theta) = \\langle \\hat{Z} \\rangle = (\\langle \\hat{Z}_1 \\rangle, ..., \\langle \\hat{Z}_n \\rangle)$, where $\\langle \\hat{Z}_k \\rangle = \\langle 0|^{\\otimes n} U(x)^{\\dagger}V(\\Theta)^{\\dagger} \\hat{Z}_k V(\\Theta)U(x)|0 \\rangle^{\\otimes n}$. The Z-expectation values can be derived via multiple sampling (shots) on real quantum devices or direct computation when using a simulation software. VQCs have been shown to provide certain advantages over classical neural networks [25], [61] and have demonstrated successful applications in various ML tasks [4], [5], [7], [9], [15], [17]\u2013[19]. The variational circuit $V(\\Theta)$ requires special attention since the design of this circuit component will affect the QML model significantly. In general, several control gates and rotation gates are required in this circuit component, and there are several ansatzes which have been shown to be successful. However, these designs are not tailored for specific QML tasks, therefore may not be the optimal in terms of circuit depth."}, {"title": "IV. QUANTUM ARCHITECTURE SEARCH", "content": "In this paper, we want to solve the following problem: Suppose we are given an initial quantum state $|0\\rangle^{\\otimes n}$, a"}, {"title": "Definition IV.1 (QAS for Quantum Supervised Learning).", "content": "Given an n-qubit system with ground state initialization $|0\\rangle^{\\otimes n}$ and a predefined encoding circuit $U$, the QAS for quantum supervised learning is to find the gate sequence with length $ \\leq L$ composed from the allowed gate sets $G$ to build the trainable circuit $V(\\Theta)$ such that, after the predefined training process,\nthe quantum function $f(x; \\Theta) = \\langle \\hat{Z} \\rangle = (\\langle \\hat{Z}_1 \\rangle, ..., \\langle \\hat{Z}_m \\rangle)$, where $\\langle \\hat{Z}_k \\rangle = \\langle 0|^{\\otimes n} U(x)^{\\dagger}V(\\Theta)^{\\dagger} \\hat{Z}_k V(\\Theta)U(x)|0 \\rangle^{\\otimes n}$ represents the Z expectation value on k-th qubits and $m \\leq n$ equals to the number of outputs, can minimize or maximize the given performance metric $M(y_i, \\hat{y}_i)$. Here the $y_i$ and $ \\hat{y}_i$ represent the ground truth and labels predicted by the quantum model, respectively. The predicted label $ \\hat{y}_i$ is derived from the $f(x; \\Theta)$ and can be represented as $ \\hat{y}_i = g(f(x; \\Theta))$, where $g$ is a post-processing function for $f$. Specifically, for our binary classification task we have $M(y_i, \\hat{y}_i) = -(y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i))$ and $g = (1 + f(x; \\Theta))/2$.\nThe allowed action (gate) set we consider for this particular problem is\n$G = \\bigcup_{i=1}^{n} \\{R_x, R_y, R_z, CNOT_{i, (i+1) \\pmod{2}} \\},$ (1)\nwhere $R_x, R_y,$ and $R_z,$ represent rotations along X, Y and Z axis, respectively."}, {"title": "V. REINFORCEMENT LEARNING", "content": "Reinforcement learning (RL) stands as a machine learning approach where an agent learns decision-making by interacting with environments [62]. In this setup, the agent engages with an environment $ \\mathcal{E}$ over discrete time steps. At each time step t, the agent receives a current state or observation $s_t$ from the environment $ \\mathcal{E}$ and proceeds to select an action $a_t$ from a set of available actions $ \\mathcal{A}$ based on its governing policy $ \\pi$. This policy $ \\pi$ functions to map the state or observation $s_t$ to the action $a_t$. Typically, the policy may adopt a probabilistic nature, implying that given a state s, the action output can be a probability distribution $ \\pi(a_t|s_t)$ conditioned on $s_t$. Upon executing the action $a_t$, the agent encounters the subsequent state $s_{t+1}$ and receives a single reward $r_t$. This iterative process persists until the agent reaches a terminal state or meets predefined termination conditions (e.g., maximum steps allowed). An episode refers to the agent's journey from a randomly chosen initial state through to the terminal state or until it satisfies the stopping criteria.\nWe define the cumulative discounted reward from time step t as $R_t = \\sum_{t'=t}^{T} \\gamma^{t'-t}r_t$, where $ \\gamma$ is the discount factor within the range of (0,1]. Essentially, $ \\gamma$ serves as a parameter set by the investigator to influence how future rewards impact decision-making. A higher $ \\gamma$ assigns greater importance to future rewards, while a lower $ \\gamma$ leads to more emphasis on immediate rewards, gradually discounting future ones. The agent's objective is to maximize the expected return from each state $s_t$ during the training phase. The action-value function, or Q-value function, $Q^{\\pi}(s, a) = \\mathbb{E}[R_t|s_t = s, a]$, represents the anticipated return for choosing action a in state s according to policy $ \\pi$. The optimal action-value function $Q^{*}(s, a) = \\max_{\\pi}Q^{\\pi}(s, a)$ indicates the highest achievable action-value across all conceivable policies. Furthermore, the value of state s under policy $ \\pi$, $V^{\\pi}(s) = \\mathbb{E}[R_t|s_t = s]$, represents the agent's expected return when adhering to policy $ \\pi$ from state s. Various reinforcement learning (RL) algorithms aim to identify the policy that maximizes the value function. Algorithms geared towards maximizing the value function are termed value-based RL algorithms. One of the notable example of value-based RL is the Q-learning [62]."}, {"title": "A. Q-Learning", "content": "Q-learning [62] stands out as one of the predominant and fundamental model-free algorithms in RL. In Q-learning, the agent acquires knowledge of the optimal action-value function and operates as an off-policy algorithm. The learning process starts with the random initialization of the value function $Q(s, a)$ for all states $s \\in S$ and actions $a \\in A$, typically stored in a structured form known as the Q-table. The estimates for $Q^{\\pi}(s, a)$ are then progressively updated according to the Bellman equation:\n$Q (s_t, a_t) \\leftarrow Q (s_t, a_t) + \\alpha \\left[r_t + \\gamma \\max_a Q (s_{t+1}, a) - Q (s_t, a_t)\\right].$ (2)"}, {"title": "B. Double Deep Q-Learning", "content": "The conventional Q-learning approach, as previously elucidated, offers the theoretically optimal action-value function. However, it becomes impractical for problems necessitating extensive memory. Particularly, managing problems characterized by high-dimensional state (s) or action (a) spaces poses significant challenges. Moreover, in environments featuring continuous state values, the efficient storage of Q(s, a) within a table is unclear. To circumvent this memory constraint, neural networks (NNs) are employed to effectively represent $Q^{\\pi}(s, a) d s \\in S, a \\in A$. This technique, termed deep Q-learning, utilizes NNs, with the network itself referred to as a deep Q-network (DQN) [63].\nTo enhance the stability of the deep DQN, methods such as experience replay and the integration of an auxiliary network referred to as the target network are employed [63]. Experience replay involves the agent storing encountered experiences during episodes in memory, preserving transition tuples, denoted as $s_t, a_t, r_t, s_{t+1}$. Upon accumulating a sufficient pool of experiences, the agent randomly selects a batch for computation of loss and subsequent update of DQN model parameters. Furthermore, to mitigate the correlation between target and prediction, a duplicate of the DQN, termed the target network, is utilized. The parameters $ \\theta$ of the DQN"}, {"title": "C. N-Step DDQN", "content": "The N-step DDQN extends the standard DDQN by considering a sequence (trajectory) of N steps when updating the Q-values according to the following loss function,\n$L(\\theta) = \\mathbb{E}\\left[\\left(\\sum_{k=0}^{N-1} \\gamma^{k}r_{t+k+1} + \\gamma^N Q(s_{t+N}, \\underset{a}{argmax} Q (s_{t+N}, a; \\theta), \\theta^-) - Q (s_t, a_t; \\theta) \\right)^2\\right]$ (5)\nwhere $ \\gamma$ represents the discount factor, $ \\theta$ represents the policy net parameters, $ \\theta^-$ represents the target net parameters and $r_{t+k+1}$ is the reward received at timestep $t + k + 1$. Note that here we use MSE loss as an example, however other kind of loss function can be used to fine-tune the model performance. In this work, we use the Smooth_L1 loss. By considering multiple steps, the N-step DDQN provides a more informative signal for updates and allows the agent to consider the long-term consequences of its actions, potentially leading to faster convergence and improved performance compared to the standard DQN and DDQN."}, {"title": "VI. METHODS", "content": "In this work, we use the TENSORCIRCUIT [65] for constructing the variational quantum circuits and PYTORCH [66] for building the deep reinforcement learning model."}, {"title": "A. Experimental setup", "content": "In our experimental setup, we employ two types of datasets from SCIKIT-LEARN [67] to generate data for the binary classification task. The primary dataset is generated using\nsklearn.datasets.make_classification.\nThis\nfunction creates n-dimensional datasets where data points\nform normally distributed clusters (with a standard deviation\nof 1) around the vertices of an $N_{informative}$-dimensional\nhypercube. The remaining $N_{redundant} = n - N_{informative}$\nfeatures are random linear combinations of these informative\nfeatures, adding complexity to the classification task. In\naddition to the make_classification dataset, we also\nutilize the sklearn.datasets.make_moons dataset.\nThis dataset generates two interleaving half-moon shaped\nclusters, which are particularly useful for evaluating the\nmodel's ability to capture non-linear decision boundaries in\nbinary classification tasks.\nFor each episode within our RL-QMLAS framework, we\ninitialize the environment with an empty quantum circuit\nwhich is in the ground state $|0\\rangle^{\\otimes n}$. At every step, the agent\nselects a quantum gate and its location based on the output\nfrom the policy network. The set of permissible quantum gates\nincludes rotation gates ($R_x, R_y$ and $R_z$) and the CNOT gate.\nTo maintain a manageable action space, the agent is tasked\nwith choosing the type of the rotation gate (either X, Y, or\nZ), while the specific rotation angle is optimized through a\nclassical optimizer during the training of the quantum classi-\nfier. This approach balances the complexity of quantum gate\nselection with practical training considerations. Input data is\nencoded into the quantum circuit using an arctan embedding\nstrategy, specifically, for each feature vector $f \\in \\mathbb{R}^n$, we\ncompute angles $ \\theta_i = arctan(f_i)$ and $ \\phi_i = arctan(f_i)$ for\n$i \\in \\{1, 2, ..., n\\}$. These angles are then used to apply rotation\ngates as follows:\n$\\forall i \\in \\{1, 2, ..., n\\}$, apply $R_y(\\theta_i)$ and $R_z(\\phi_i)$ on qubit i. (6)\nUpon the addition of a new gate to the circuit, the quantum\nclassifier is trained for a fixed number of epochs, or until it\nreaches the desired accuracy level. This iterative process of\ngate selection and classifier training continues, evolving the\nquantum circuit step-by-step until the episode concludes, either\nby achieving the desired accuracy or by reaching the maximum\nlimit of quantum gates. See Section VI-B2 for details about the\nreward scheme.\nSignificantly, this method of quantum architecture search\nthrough reinforcement learning negates the need for prior\nphysical knowledge, enabling the algorithm to autonomously\ndiscover efficient and effective quantum circuits. It represents a\nnovel approach where the intricacies of quantum computation\nare navigated and optimized through machine learning, rather\nthan relying on pre-established physical ansatz."}, {"title": "B. Hyperparameters of RL", "content": "1) N-step Double Deep Q-Network (DDQN): In this study,\nwe applied an N-step Double Deep Q-Network (DDQN) [64]"}, {"title": "3) Adaptive Search:", "content": "One potential drawback of our previous approach is that the desired classification accuracy $ \\gamma_{target}$\nis determined a priori. If the $ \\gamma_{target}$ is set too high, the agent\nwill likely to fail in most of the cases and we need to manually\nincrease L, which leads to slow convergence. On the other\nhand, a $ \\gamma_{target}$ that's set too low allows the agent to quickly\nfind simplistic solutions, hindering the development of more\nefficient and sophisticated quantum circuits.\nTo address the need for pre-selecting an appropriate clas-\nsification accuracy target ($ \\gamma_{target}$), we introduce an adaptive\nsearch strategy. This approach dynamically adjusts both $ \\gamma_{target}$\nand the exploration rate $ \\epsilon$ based on the agent's ongoing\nperformance. During the training phase, $ \\gamma_{target}$ increases in-\ncrementally by 0.01 whenever the agent consistently meets or\nexceeds this threshold across a specified number of episodes,\nsuch as 10 successes in 12 consecutive episodes. In the testing\nphase, a similar mechanism is in place. If the agent repeatedly\nachieves higher accuracies over 5 consecutive tests, the $ \\gamma_{target}$\nis further increased by 0.01, challenging the agent to refine its\nperformance. Concurrently, $ \\epsilon$ is decreased to 95% of its value,\nshifting the agent's focus from exploration to exploitation. As\na result, the agent increasingly relies on learned behaviors\nand experiences rather than random exploration, enhancing its\nproficiency."}, {"title": "VII. RESULTS", "content": "Our RL agent was first evaluated using the\nmake_classification dataset. The outcomes are\nillustrated in Figure 3, where we observed a consistent\nimprovement in classification accuracy (Figure 3a) and a\ndecrease in the number of quantum gates required (Figure 3c).\nIn the testing phase, the exploration rate $ \\epsilon$ was set to\nzero, ensuring that the agent's gate selection was entirely\nbased on the learned policy. Notably, a stable and high\ntesting accuracy (Figure 3b) coupled with a minimal number\nof gates (Figure 3d) was achieved after several hundred\ntraining episodes. This indicates the agent's capacity to learn\nand its efficiency in converging to an optimized quantum\ncircuit structure over the course of training. Moreover, the\nrewards pattern in both training (Figure 3e) and testing\n(Figure 3f) further confirmed the agent's proficiency in\nbalancing classification accuracy with circuit simplicity. For\nthe make_moons dataset, similar trends were observed\n(Figure 4), signifying the robustness of our reinforcement\nlearning approach. The agent not only maintained high\nclassification accuracy but also continued to design quantum\ncircuits with an optimized number of gates. This consistency\nacross different datasets indicates the potential of our method\nto be applied broadly in quantum machine learning tasks\nwithout prior physical knowledge."}, {"title": "B. Adaptive Search Results", "content": "The adaptive search strategy has been implemented to\ndynamically adjust the target accuracy $ \\gamma_{target}$ and exploration\nrate $ \\epsilon$ during the training of our RL agent. This approach is\ndesigned to progressively challenge the agent, enhancing its"}, {"title": "VIII. DISCUSSION", "content": "While the adaptive search strategy enables the agents to\nachieve better accuracy adaptively without relying on the\ninitial guessing, the successful outcome of such strategy still\nrelies partially on the specific target accuracy update scheme.\nFor instance, if the target accuracy is increased too quickly,\nit is possible that the agents will get stuck and the resulting\naccuracy become worse. Further investigation will be required\nto establish the optimal schedule of changing the learning\ntarget as well as the connection between model performance\nand the complexity of the dataset. In more realistic scenar-\nios, quantum devices are subject to noise and decoherence.\nConsequently, quantum circuit architectures identified through\nnoise-free simulations may exhibit poor performance when\ndirectly implemented on real quantum devices. It is therefore\ncompelling to explore the generalization of the proposed\nRL-QMLAS framework to noisy quantum devices or more\nchallenging conditions, such as fluctuating or drifting noise\npatterns. Another direction of investigation involves the scaling\nbehavior of the proposed framework as the number of qubits\nincreases. It is noted that as the number of qubits n increases,\nthe potential combinations of quantum gates scale up rapidly.\nFor example, the number of allowed gates described in Equa-\ntion 1 scales at 3n + n(n \u2212 1) = $ \\Omega(n^2)$. This rapid scaling\npresents significant challenges in designing the deep neural"}, {"title": "IX. CONCLUSION", "content": "In this paper, we present a framework that leverages deep reinforcement learning to construct quantum machine learning models tailored for classification tasks. Through extensive numerical simulations across various scenarios, our approach demonstrates the capability to develop high-performing QML models without the need for manually designing the learnable circuit based on prior physical knowledge. Furthermore, our models achieve commendable performance while utilizing a moderate number of quantum gates, making them suitable for implementation on existing noisy quantum devices. These findings offer a pioneering avenue for exploring the potential of automated QML in diverse application domains."}]}