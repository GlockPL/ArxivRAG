{"title": "Quantum Machine Learning Architecture Search via Deep Reinforcement Learning", "authors": ["Xin Dai", "Tzu-Chieh Wei", "Shinjae Yoo", "Samuel Yen-Chi Chen"], "abstract": "The rapid advancement of quantum computing (QC) and machine learning (ML) has given rise to the burgeoning field of quantum machine learning (QML), aiming to capitalize on the strengths of quantum computing to propel ML forward. Despite its promise, crafting effective QML models necessitates profound expertise to strike a delicate balance between model intricacy and feasibility on Noisy Intermediate-Scale Quantum (NISQ) devices. While complex models offer robust representation capabilities, their extensive circuit depth may impede seamless execution on extant noisy quantum platforms. In this paper, we address this quandary of QML model design by employing deep reinforcement learning to explore proficient QML model architectures tailored for designated supervised learning tasks. Specifically, our methodology involves training an RL agent to devise policies that facilitate the discovery of QML models without predetermined ansatz. Furthermore, we integrate an adaptive mechanism to dynamically adjust the learning objectives, fostering continuous improvement in the agent's learning process. Through extensive numerical simulations, we illustrate the efficacy of our approach within the realm of classification tasks. Our proposed method successfully identifies VQC architectures capable of achieving high classification accuracy while minimizing gate depth. This pioneering approach not only advances the study of AI-driven quantum circuit design but also holds significant promise for enhancing performance in the NISQ era.", "sections": [{"title": "I. INTRODUCTION", "content": "Quantum computing (QC) holds the potential to revolutionize computational tasks, offering distinct advantages over classical computers [1]. The convergence of advancements in quantum hardware and machine learning applications has sparked a growing interest in exploring the synergies between these cutting-edge technologies. Although existing quantum computers still suffer from noise, a promising solution lies in a hybrid quantum-classical framework. Here, computational tasks are divided into two parts: one executed on a quantum computer and the other on a classical computer [2], [3]. Central to this paradigm is the Variational Quantum Algorithm (VQA) [2], [3], which serves as the cornerstone of hybrid computing."}, {"title": "II. RELEVANT WORKS", "content": "Machine learning techniques have been applied to tackle various quantum computing challenges such as quantum architecture search (QAS). The target task of a QAS might be generating a desired quantum state [26]\u2013[36], finding an efficient circuit for solving chemical ground states [36]\u2013[40], solving an optimization task [36], [38], [41]\u2013[46], optimizing a given quantum circuit for a particular hardware architecture [47], compiling a circuit [48]\u2013[50] or performing a machine learning task [42], [43], [51]\u2013[57]. Various approaches are employed to find the optimal circuit for specified tasks. For example, the works [26]\u2013[29], [31], [33], [34], [37], [41], [47], [50] consider the reinforcement learning based methods while the works [32], [51]\u2013[53] works use different variants of evolutionary algorithms to search for the circuit. Differentiable QAS methods are also developed to leverage the highly successful gradient-based methods [43]\u2013[45], [55]. Different ways of encoding the quantum circuit architecture are devised. For example, the works [39], [42] propose graph-based method while the work [47] consider the convolutional neural network based method to encode the quantum circuit architecture. Regarding the circuit performance metric, it can be a direct evaluation of the circuit performance on the particular task [37], [38], [51] or the closeness of the generated circuit to the actual circuit [26], [27], [42]. To reduce the computational resource required in direct evaluation, certain predictor-based methods are proposed to use neural network to predict the quantum model performance without direct circuit evaluation [40], [56]. The proposed method in this paper is to further generalize the concepts used in [26], [27] to more than finding a quantum circuit to synthesize a particular quantum state, but can actually perform a QML task. This paper further generalize the methods proposed in the work [37] to QML tasks. Our work is also different from previous works on quantum circuit optimization [47], since in our work, circuit ansatz are not provided. Though optimizing an existing circuit ansatz can decrease training time, it is not without cost; the ansatz necessitates input from quantum experts. Our approach investigates the prospect of whether, in the absence of a predefined ansatz, an agent can autonomously discover high-performing circuits."}, {"title": "III. VARIATIONAL QUANTUM CIRCUITS", "content": "Variational quantum circuits (VQC), also known as parameterized quantum circuits (PQC) is a special kind of quantum circuit with trainable parameters which can be trained via gradient-based [4], [58], [59] or gradient-free [60] methods. This kind of circuits play a crucial role in the hybrid quantum-classical computing paradigm in which certain computing tasks are implemented on quantum computer while tasks not suitable for existing quantum computers are carried out by classical computers. Consider an n-qubit system. The fundamental components of a VQC (illustrated in Figure 2) include the encoding circuit $U(x)$, responsible for transforming the classical input vector $x$ into a quantum state $U(x) |0\\rangle^{\\otimes n}$, the variational circuit $V(\\Theta)$, serving as the actual learning component with trainable parameters $\\Theta$, and the final measurement operation, used to extract information from the circuit. The VQC used in this work can be expressed as $f(x; \\Theta) = \\langle(\\sigma_1^Z),...,(\\sigma_n^Z)\\rangle$, where $\\langle(\\sigma_k^Z)\\rangle = \\langle 0|^{\\otimes n} U^{\\dagger}(x)V^{\\dagger}(\\Theta)\\sigma_k^Z V(\\Theta)U(x)|0\\rangle^{\\otimes n}$. The Z-expectation values can be derived via multiple sampling (shots) on real quantum devices or direct computation when using a simulation software. VQCs have been shown to provide certain advantages over classical neural networks [25], [61] and have demonstrated successful applications in various ML tasks [4], [5], [7], [9], [15], [17]\u2013[19]. The variational circuit $V(\\theta)$ requires special attention since the design of this circuit component will affect the QML model significantly. In general, several control gates and rotation gates are required in this circuit component, and there are several ansatzes which have been shown to be successful. However, these designs are not tailored for specific QML tasks, therefore may not be the optimal in terms of circuit depth."}, {"title": "IV. QUANTUM ARCHITECTURE SEARCH", "content": "In this paper, we want to solve the following problem: Suppose we are given an initial quantum state $|0\\rangle^{\\otimes n}$, an supervised learning dataset $\\{(x_i, y_i)\\}$, and encoding circuit $U$, the maximum gate number $L$, the allowed gate set $G$, an performance metric $M$ (e.g. classification accuracy or the loss function $L$), the goal is to find the quantum gate sequence $S$ such that the performance metric is maximized (or minimized).\nDefinition IV.1 (QAS for Quantum Supervised Learning). Given an n-qubit system with ground state initialization $|0\\rangle^{\\otimes n}$ and a predefined encoding circuit $U$, the QAS for quantum supervised learning is to find the gate sequence with length $\\leq L$ composed from the allowed gate sets $G$ to build the trainable circuit $V(\\Theta)$ such that, after the predefined training process, the quantum function $f(x; \\Theta) = f(\\Theta) = \\langle(\\sigma_1^Z),...,(\\sigma_m^Z)\\rangle$, where $\\langle(\\sigma_k^Z)\\rangle = \\langle 0|^{\\otimes n} U^{\\dagger}(x)V^{\\dagger}(\\Theta)\\sigma_k^Z V(\\Theta)U(x)|0\\rangle^{\\otimes n}$ represents the Z expectation value on k-th qubits and $m \\leq n$ equals to the number of outputs, can minimize or maximize the given performance metric $M(y_i, \\hat{y_i})$. Here the $y_i$ and $\\hat{y_i}$ represent the ground truth and labels predicted by the quantum model, respectively. The predicted label $\\hat{y_i}$ is derived from the $f(x;\\Theta)$ and can be represented as $\\hat{y_i} = g(f(x;\\Theta))$, where $g$ is a post-processing function for $f$. Specifically, for our binary classification task we have $M(y_i, \\hat{y_i}) = -(y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i}))$ and $g = (1 + f(x;\\Theta))/2$.\nThe allowed action (gate) set we consider for this particular problem is\n$G = \\cup_{i=1}^{n} \\{R_x, R_y, R_z,\\text{CNOT}_{i,(i+1)(\\text{mod}2)}\\}\\,,$ (1)\nwhere $R_x, R_y$, and $R_z$ represent rotations along X, Y and Z axis, respectively."}, {"title": "V. REINFORCEMENT LEARNING", "content": "Reinforcement learning (RL) stands as a machine learning approach where an agent learns decision-making by interacting with environments [62]. In this setup, the agent engages with an environment $\\mathcal{E}$ over discrete time steps. At each time step $t$, the agent receives a current state or observation $s_t$ from the environment $\\mathcal{E}$ and proceeds to select an action $a_t$ from a set of available actions $\\mathcal{A}$ based on its governing policy $\\pi$. This policy $\\pi$ functions to map the state or observation $s_t$ to the action $a_t$. Typically, the policy may adopt a probabilistic nature, implying that given a state $s$, the action output can be a probability distribution $\\pi(a_t|s_t)$ conditioned on $s_t$. Upon executing the action $a_t$, the agent encounters the subsequent state $s_{t+1}$ and receives a single reward $r_t$. This iterative process persists until the agent reaches a terminal state or meets predefined termination conditions (e.g., maximum steps allowed). An episode refers to the agent\u2019s journey from a randomly chosen initial state through to the terminal state or until it satisfies the stopping criteria.\nWe define the cumulative discounted reward from time step t as $R_t = \\sum_{t'=t}^{T} \\gamma^{t'-t}r_t$, where $\\gamma$ is the discount factor within the range of $(0,1]$. Essentially, $\\gamma$ serves as a parameter set by the investigator to influence how future rewards impact decision-making. A higher $\\gamma$ assigns greater importance to future rewards, while a lower $\\gamma$ leads to more emphasis on immediate rewards, gradually discounting future ones. The agent\u2019s objective is to maximize the expected return from each state $s_t$ during the training phase. The action-value function, or Q-value function, $Q^{\\pi}(s,a) = \\mathbb{E}[R_t| s_t=s, a]$ represents the anticipated return for choosing action $a$ in state $s$ according to policy $\\pi$. The optimal action-value function $Q^*(s, a) = \\max_{\\pi} Q^{\\pi}(s, a)$ indicates the highest achievable action-value across all conceivable policies. Furthermore, the value of state $s$ under policy $\\pi$, $V^{\\pi}(s) = \\mathbb{E}[R_t| s_t=s]$, represents the agent\u2019s expected return when adhering to policy $\\pi$ from state $s$. Various reinforcement learning (RL) algorithms aim to identify the policy that maximizes the value function. Algorithms geared towards maximizing the value function are termed value-based RL algorithms. One of the notable example of value-based RL is the Q-learning [62]."}, {"title": "A. Q-Learning", "content": "Q-learning [62] stands out as one of the predominant and fundamental model-free algorithms in RL. In Q-learning, the agent acquires knowledge of the optimal action-value function and operates as an off-policy algorithm. The learning process starts with the random initialization of the value function $Q(s, a)$ for all states $s \\in S$ and actions $a \\in A$, typically stored in a structured form known as the Q-table. The estimates for $Q^{\\pi}(s, a)$ are then progressively updated according to the Bellman equation:\n$Q (s_t, a_t) \\leftarrow Q (s_t, a_t) + \\alpha [r_t + \\gamma \\max_a Q (s_{t+1}, a) - Q (s_t, a_t)].$ (2)"}, {"title": "B. Double Deep Q-Learning", "content": "The conventional Q-learning approach, as previously elucidated, offers the theoretically optimal action-value function. However, it becomes impractical for problems necessitating extensive memory. Particularly, managing problems characterized by high-dimensional state $(s)$ or action $(a)$ spaces poses significant challenges. Moreover, in environments featuring continuous state values, the efficient storage of $Q(s, a)$ within a table is unclear. To circumvent this memory constraint, neural networks (NNs) are employed to effectively represent $Q^{\\pi}(s,a) \\forall s \\in S, a \\in A$. This technique, termed deep Q-learning, utilizes NNs, with the network itself referred to as a deep Q-network (DQN) [63].\nTo enhance the stability of the deep DQN, methods such as experience replay and the integration of an auxiliary network referred to as the target network are employed [63]. Experience replay involves the agent storing encountered experiences during episodes in memory, preserving transition tuples, denoted as $\\{s_t, a_t, r_t, s_{t+1}\\}$. Upon accumulating a sufficient pool of experiences, the agent randomly selects a batch for computation of loss and subsequent update of DQN model parameters. Furthermore, to mitigate the correlation between target and prediction, a duplicate of the DQN, termed the target network, is utilized. The parameters $\\theta$ of the DQN are updated iteratively, whereas the parameters $\\theta^-$ of the target network undergo updates at periodic intervals. The DQN training is done via minimizing the mean square error (MSE) loss function:\n$L(\\theta) = \\mathbb{E} \\Big[(r_t + \\gamma \\max_{a'} Q (s_{t+1}, a'; \\theta^-) - Q (s_t, a_t; \\theta) \\Big)^2\\Big]$ (3)\nOther loss functions such as Huber loss or mean absolute error (MAE) can also be used. Despite the considerable success achieved by DQN, instances arise where it tends to overestimate the action-value function [64]. As a remedy, an enhanced variant of DQN, known as Double Deep Q-learning (DoubleDQN), has been devised [64]. The essence of Double Deep Q-learning lies in deconstructing the max operation within the target $y_t = r_t + \\gamma \\max_{a'} Q (s_{t+1},a';\\theta^-)$ into two distinct operations: action selection and action evaluation. Initially, action selection relies on the policy network, $\\text{argmax}_a Q (s_{t+1}, a; \\theta)$, followed by the utilization of the target network to evaluate the action, $Q \\Big(s_{t+1}, \\text{argmax}_a Q (s_{t+1}, a; \\theta), \\theta^- \\Big)$. Consequently, the DoubleDQN target is reformulated as $y_t = r_t + \\gamma Q \\Big(s_{t+1}, \\text{argmax}_a Q (s_{t+1}, a; \\theta), \\theta^- \\Big)$.\nThe loss function $L(\\theta)$ is therefore:\n$L(\\theta) = \\mathbb{E} \\Big[\\Big(r_t + \\gamma Q \\Big(s_{t+1}, \\text{argmax}_a Q (s_{t+1}, a; \\theta), \\theta^- \\Big) - Q (s_t, a_t; \\theta)\\Big)^2\\Big]$ (4)\nThen, $\\theta$ is updated using the gradient descent method and every few iterations we update the target network $\\theta^- \\leftarrow \\theta$."}, {"title": "C. N-Step DDQN", "content": "The N-step DDQN extends the standard DDQN by considering a sequence (trajectory) of N steps when updating the Q-values according to the following loss function,\n$L(\\theta) = \\mathbb{E} \\Bigg[\\bigg(\\sum_{k=0}^{N-1} \\gamma^k r_{t+k+1} + \\gamma^N Q(s_{t+N}, \\text{argmax}_a Q (s_{t+N}, a; \\theta), \\theta^-) - Q (s_t, a_t; \\theta)\\bigg)^2\\Bigg]$ (5)\nwhere $\\gamma$ represents the discount factor, $\\theta$ represents the policy net parameters, $\\theta^-$ represents the target net parameters and $r_{t+k+1}$ is the reward received at timestep $t + k + 1$. Note that here we use MSE loss as an example, however other kind of loss function can be used to fine-tune the model performance. In this work, we use the Smooth_L1 loss. By considering multiple steps, the N-step DDQN provides a more informative signal for updates and allows the agent to consider the long-term consequences of its actions, potentially leading to faster convergence and improved performance compared to the standard DQN and DDQN."}, {"title": "VI. METHODS", "content": "In this work, we use the TENSORCIRCUIT [65] for constructing the variational quantum circuits and PYTORCH [66] for building the deep reinforcement learning model. In our experimental setup, we employ two types of datasets from SCIKIT-LEARN [67] to generate data for the binary classification task. The primary dataset is generated using sklearn.datasets.make_classification.\nThis function creates n-dimensional datasets where data points form normally distributed clusters (with a standard deviation of 1) around the vertices of an $N_{informative}$-dimensional hypercube. The remaining $N_{redundant} = n - N_{informative}$ features are random linear combinations of these informative features, adding complexity to the classification task. In addition to the make_classification dataset, we also utilize the sklearn.datasets.make_moons dataset. This dataset generates two interleaving half-moon shaped clusters, which are particularly useful for evaluating the model's ability to capture non-linear decision boundaries in binary classification tasks.\nFor each episode within our RL-QMLAS framework, we initialize the environment with an empty quantum circuit which is in the ground state $|0\\rangle^{\\otimes n}$. At every step, the agent selects a quantum gate and its location based on the output from the policy network. The set of permissible quantum gates includes rotation gates (Rx, Ry and Rz) and the CNOT gate. To maintain a manageable action space, the agent is tasked with choosing the type of the rotation gate (either X, Y, or Z), while the specific rotation angle is optimized through a classical optimizer during the training of the quantum classifier. This approach balances the complexity of quantum gate selection with practical training considerations. Input data is encoded into the quantum circuit using an arctan embedding strategy, specifically, for each feature vector $f \\in \\mathbb{R}^n$, we compute angles $\\theta_i = \\text{arctan}(f_i)$ and $\\phi_i = \\text{arctan}(f_i^2)$ for $i \\in \\{1,2,..., n\\}$. These angles are then used to apply rotation gates as follows:\n$\\forall i \\in \\{1,2,..., n\\}$, apply $R_y(\\theta_i)$ and $R_z(\\phi_i)$ on qubit $i$. (6)\nUpon the addition of a new gate to the circuit, the quantum classifier is trained for a fixed number of epochs, or until it reaches the desired accuracy level. This iterative process of gate selection and classifier training continues, evolving the quantum circuit step-by-step until the episode concludes, either by achieving the desired accuracy or by reaching the maximum limit of quantum gates. See Section VI-B2 for details about the reward scheme.\nSignificantly, this method of quantum architecture search through reinforcement learning negates the need for prior physical knowledge, enabling the algorithm to autonomously discover efficient and effective quantum circuits. It represents a novel approach where the intricacies of quantum computation are navigated and optimized through machine learning, rather than relying on pre-established physical ansatz."}, {"title": "B. Hyperparameters of RL", "content": "1) N-step Double Deep Q-Network (DDQN): In this study, we applied an N-step Double Deep Q-Network (DDQN) [64] to learn efficient quantum circuits for classification tasks. The discount factor $\\gamma$ is set as $\\gamma = 0.0051/L$, where $L$ represents the maximum number of quantum gates allowed, promoting an approach that favors achieving tasks with minimal gate usage. To stabilize the learning process, we periodically synchronize the parameters of the target network with those of the policy network every 512 steps. An experience replay buffer with a capacity of 16384 transitions is used to break the correlation of sequential learning updates and enhance learning efficiency. The exploration strategy is governed by an $\\epsilon$-greedy policy, with $\\epsilon$ decaying from 1 to 0.1 over time to balance exploration and exploitation.\nThe architecture of the deep Q-network is a multilayer perceptron (MLP) consisting of a sequence of linear layers, each followed by a LeakyReLU activation function and dropout regularization. The input to the MLP is the state (observation) vector, which is a $4 \\times L$ matrix representing the configuration of the quantum circuit at each step, where $L$ is the maximum number of layers. The first two elements of the state vector denote the locations of the control and NOT gates, while the third and fourth elements indicate the location of the rotation gate and the rotation axis, respectively. During training, the DDQN receives a flattened state vector. After each testing episode, the test accuracy is appended to the state vector, providing an additional input feature for the MLP. The output of the MLP corresponds to the Q-values for each action, guiding the agent\u2019s decision-making process in selecting the most promising gate configurations to explore.\n2) Reward Function: The RL agent interacts with a quantum circuit environment, where the agent\u2019s actions involve selecting the control and target qubits for CNOT gates and the qubit and axis for rotation gates. The environment calculates the accuracy of the resulting quantum circuit and provides a reward signal based on the change in accuracy and the number of layers used. The reward function is defined as follows:\nR(l) = \\begin{cases} 0.2 \\cdot (\\frac{1}{Y_{\\text{target}}})(L-l), & \\text{if } y_l \\geq Y_{\\text{target}} \\text{ and } l < L, \\\\ -0.2 \\cdot |\\frac{Y_{\\text{target}}-y_l}{Y_{\\text{target}}}\\frac{l}{L}|, & \\text{if } y_l < Y_{\\text{min}} \\text{ and } l = L, \\\\ \\text{clip}\\bigg(\\frac{Y_{\\text{target}}}{Y_{l-1}+1 \\times 10^{-6}} \\frac{y_l-y_{l-1}}{Y_{l-1+1 \\times 10^{-6}}}, [0.01, -1.5, 1.5]\\bigg), & \\text{otherwise}.\\end{cases} (7)\nHere $\\text{Y}_{\text{target}}$ is the target accuracy. The reward function encourages the agent to achieve or surpass the target accuracy with minimal gate usage. The scaling factor 0.2 moderates the reward magnitude to ensure stability. Moreover, the function imposes a penalty if $\\text{Y}_{\text{target}}$ is not reached when the maximum number of gates is used. Equation 7 also dynamically rewards small improvements in accuracy (the 3rd line), but this reward decreases as more gates are added, steering the agent towards more efficient solutions. To maintain numerical stability and prevent extreme values from skewing the agent\u2019s learning, the dynamic reward is constrained within the range [-1.5, 1.5]. This careful design of the reward function ensures an optimal trade-off between accuracy and efficiency.\n3) Adaptive Search: One potential drawback of our previous approach is that the desired classification accuracy $\\text{Y}_{\text{target}}$ is determined a priori. If the $\\text{Y}_{\text{target}}$ is set too high, the agent will likely to fail in most of the cases and we need to manually increase L, which leads to slow convergence. On the other hand, a $\\text{Y}_{\text{target}}$ that's set too low allows the agent to quickly find simplistic solutions, hindering the development of more efficient and sophisticated quantum circuits.\nTo address the need for pre-selecting an appropriate classification accuracy target $(\\text{Y}_{\text{target}})$, we introduce an adaptive search strategy. This approach dynamically adjusts both $\\text{Y}_{\text{target}}$ and the exploration rate $\\epsilon$ based on the agent\u2019s ongoing performance. During the training phase, $\\text{Y}_{\text{target}}$ increases incrementally by 0.01 whenever the agent consistently meets or exceeds this threshold across a specified number of episodes, such as 10 successes in 12 consecutive episodes. In the testing phase, a similar mechanism is in place. If the agent repeatedly achieves higher accuracies over 5 consecutive tests, the $\\text{Y}_{\text{target}}$ is further increased by 0.01, challenging the agent to refine its performance. Concurrently, $\\epsilon$ is decreased to 95% of its value, shifting the agent\u2019s focus from exploration to exploitation. As a result, the agent increasingly relies on learned behaviors and experiences rather than random exploration, enhancing its proficiency."}, {"title": "VII. RESULTS", "content": "Our RL agent was first evaluated using the make_classification dataset. The outcomes are illustrated in Figure 3, where we observed a consistent improvement in classification accuracy (Figure 3a) and a decrease in the number of quantum gates required (Figure 3c). In the testing phase, the exploration rate $\\epsilon$ was set to zero, ensuring that the agent's gate selection was entirely based on the learned policy. Notably, a stable and high testing accuracy (Figure 3b) coupled with a minimal number of gates (Figure 3d) was achieved after several hundred training episodes. This indicates the agent's capacity to learn and its efficiency in converging to an optimized quantum circuit structure over the course of training. Moreover, the rewards pattern in both training (Figure 3e) and testing (Figure 3f) further confirmed the agent\u2019s proficiency in balancing classification accuracy with circuit simplicity. For the make_moons dataset, similar trends were observed (Figure 4), signifying the robustness of our reinforcement learning approach. The agent not only maintained high classification accuracy but also continued to design quantum circuits with an optimized number of gates. This consistency across different datasets indicates the potential of our method to be applied broadly in quantum machine learning tasks without prior physical knowledge."}, {"title": "B. Adaptive Search Results", "content": "The adaptive search strategy has been implemented to dynamically adjust the target accuracy $\\text{Y}_{\text{target}}$ and exploration rate $\\epsilon$ during the training of our RL agent. This approach is designed to progressively challenge the agent, enhancing its ability to construct efficient quantum circuits. The results of this strategy are discussed below for two datasets.\nFigure 5 presents the results for the make_classification dataset. In Figure 5a, the agent\u2019s training accuracy oscillates around the dynamic target accuracy, $\\text{Y}_{\text{target}}$, demonstrating continuous adjustment and learning in response to its evolution. To improve the stability and performance, we increased the training episodes from 800 (fixed $\\text{Y}_{\text{target}}$ scenario, Figure 3) to 1200. Figure 5c shows an initial increase in the number of gates, followed by stabilization, indicating the agent\u2019s attempt to balance the quantum circuit\u2019s complexity and efficiency. During the testing phase, as depicted in Figure5b and d, we observe more significant performance fluctuations compared to the fixed $\\text{Y}_{\text{target}}$ scenario in Figure 3. Each increase in $\\text{Y}_{\text{target}}$ results in a temporary performance dip, followed by stabilization and alignment with the new $\\text{Y}_{\text{target}}$. After 1200 episodes, the agent efficiently utilizes 4 quantum gates to achieve a classification accuracy of 0.93."}, {"title": "C. Comparison with classical machine learning", "content": "A comparative analysis with classical machine learning methods further elucidates the effectiveness of our quantum classifier. We conducted experiments using logistic regression (LR) and support vector machine (SVM) on the same datasets to benchmark performance.\nFor the make_classification dataset, both LR and SVM models achieved an accuracy of 90.3%. In contrast, for the make_moons dataset, the LR model attained an accuracy of 82.5%, while the SVM model, leveraging its RBF kernel, reached an accuracy of 100%. Notably, the 4-dimensional make_classification dataset highlights the efficiency of our quantum classifier. With merely 2 quantum gates, it paralleled the performance of the LR and SVM models. The LR model utilized 5 parameters (comprising 4 coefficients and 1 intercept), whereas the SVM model employed a total of 110 support vectors in this instance. In the case of the 2-dimensional make_moons dataset, the LR model required 3 parameters, and the SVM model used 44 support vectors. Here again, our quantum classifier demonstrated comparable accuracy, albeit with fewer parameters. This outcome is particularly noteworthy for the make_classification dataset, underscoring the potential of quantum classifiers in achieving high performance with reduced parameterization."}, {"title": "VIII. DISCUSSION", "content": "While the adaptive search strategy enables the agents to achieve better accuracy adaptively without relying on the initial guessing, the successful outcome of such strategy still relies partially on the specific target accuracy update scheme. For instance, if the target accuracy is increased too quickly, it is possible that the agents will get stuck and the resulting accuracy become worse. Further investigation will be required to establish the optimal schedule of changing the learning target as well as the connection between model performance and the complexity of the dataset. In more realistic scenarios, quantum devices are subject to noise and decoherence. Consequently, quantum circuit architectures identified through noise-free simulations may exhibit poor performance when directly implemented on real quantum devices. It is therefore compelling to explore the generalization of the proposed RL-QMLAS framework to noisy quantum devices or more challenging conditions, such as fluctuating or drifting noise patterns. Another direction of investigation involves the scaling behavior of the proposed framework as the number of qubits increases. It is noted that as the number of qubits n increases, the potential combinations of quantum gates scale up rapidly. For example, the number of allowed gates described in Equation 1 scales at $3n + n(n \u2212 1) = \\Omega(n^2)$. This rapid scaling presents significant challenges in designing the deep neural networks for the RL agents, as the number of output neurons would increase quickly. Further studies are required to design more efficient RL agents to generate plausible actions for constructing larger-scale quantum circuits."}, {"title": "IX. CONCLUSION", "content": "In this paper, we present a framework that leverages deep reinforcement learning to construct quantum machine learning models tailored for classification tasks. Through extensive numerical simulations across various scenarios, our approach demonstrates the capability to develop high-performing QML models without the need for manually designing the learnable circuit based on prior physical knowledge. Furthermore, our models achieve commendable performance while utilizing a moderate number of quantum gates, making them suitable for implementation on existing noisy quantum devices. These findings offer a pioneering avenue for exploring the potential of automated QML in diverse application domains."}]}