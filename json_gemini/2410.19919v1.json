{"title": "Provably Adaptive Average Reward Reinforcement Learning for Metric Spaces", "authors": ["Avik Kar", "Rahul Singh"], "abstract": "We study infinite-horizon average-reward reinforcement learning (RL) for Lipschitz MDPS and develop an algorithm ZORL that discretizes the state-action space adaptively and zooms into \"promising regions\" of the state-action space. We show that its regret can be bounded as \\(\\tilde{O}(T^{1-\\frac{d_{eff}}{2}})\\), where \\(d_{eff.}\\) = 2\\(d_s\\) + \\(d_z\\) + 3, \\(d_s\\) is the dimension of the state space, and \\(d_z\\) is the zooming dimension. \\(d_z\\) is a problem-dependent quantity, which allows us to conclude that if an MDP is benign, then its regret will be small. We note that the existing notion of zooming dimension for average reward RL is defined in terms of policy coverings, and hence it can be huge when the policy class is rich even though the underlying MDP is simple, so that the regret upper bound is nearly O(T). The zooming dimension proposed in the current work is bounded above by d, the dimension of the state-action space, and hence is truly adaptive, i.e., shows how to capture adaptivity gains for infinite-horizon average-reward RL. ZORL outperforms other state-of-the-art algorithms in experiments; thereby demonstrating the gains arising due to adaptivity.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) (Sutton and Barto, 2018) is a popular model for systems involving real-time sequential decision-making and has applications in many fields such as robotics, natural language processing (Ibarz et al., 2021; Sodhi et al., 2023). An agent interacts sequentially with an environment by applying actions and gathers rewards. The environment is modeled as a Markov decision process (MDP) (Puterman, 2014). The agent does not know the transition probabilities and reward function of the under-lying MDP. Its goal is to choose actions sequentially so as to maximize the cumulative rewards.\nThe current work studies infinite-horizon average reward MDPs and derives RL algorithms for continuous state-action spaces that are endowed with a metric. Though discrete MDPs and linear MDPs have been studied in detail in RL literature, they are not suitable for real-world applications which mostly involve nonlinear systems that reside on continuous spaces (Kumar et al., 2021). For continuous spaces, the learning regret could grow linearly with time horizon T unless the problem has some structure (Kleinberg et al., 2008). We focus on Lipschitz MDPs, which is a very general class and subsumes several popular classes such as linear MDPs (Jin et al., 2020), RKHS MDPS (Chowdhury and Gopalan, 2019), linear mixture models, RKHS approximation, and the nonlinear function approximation framework (Osband and Van Roy, 2014; Kakade et al., 2020). See Maran et al. (2024a,b) for more details.\nThroughout, we use \\(d_s, d_a\\) to denote dimensions of the state-space and the action-space respectively, and d := \\(d_s\\) + \\(d_a\\). In episodic RL for Lipschitz MDPs, the regret is known to scale as \\(\\tilde{O}(K^{1-\\frac{1}{d_{eff}}})\\)\u00b9, where K is the number of episodes, while \\(d_{eff.}\\) is the effective dimension associated with the underlying MDP and the algorithm. For example, a naive algorithm that uses a fixed discretization has \\(d_{eff.}\\) = d + 2 (Song and Sun, 2019). One can use problem structure to reduce \\(d_{eff.}\\); prior works on episodic Lipschitz MDPs such as Sinclair et al. (2019); Cao and Krishnamurthy (2020) reduce effective dimension to \\(d_z\\)+2, where the zooming dimension \\(d_z\\) measures the size of the near-optimal state-action pairs, where near-optimality is relative to the optimal Qfunction for episodic RL. These gains are achieved by performing an adaptive discretization of the state-action space and \"zooming in\" to only the promising regions of the state-action space by creating a finer grid around these as time progresses. However, it is shown in Kar and Singh (2024) that zooming ideas and algorithms which were developed for episodic MDPs, are inappropriate for average reward RL tasks, in that \\(d_z \\rightarrow d\\) as \\(T \\rightarrow \\infty\\), which is what one would have obtained via a naive fixed discretization scheme. They rectify this issue to some extent by working directly in policy space and showing zooming behavior in this space rather"}, {"title": "1.1 Contributions", "content": "1.  We study infinite-horizon average reward Lipschitz MDPs on continuous state-action spaces and propose a computationally efficient algorithm ZORL that combines adaptive discretization with the principle of optimism, which yields zooming behavior. We provide regret upper bound of ZORL as a function of the zooming dimension \\(d_z\\). We define \\(d_z\\) in terms of the suboptimality gap of the state action pairs (5). We show that the regret of ZORL is upper bounded as \\(\\tilde{O}(T^{1-\\frac{1}{d_{eff}}})\\), where \\(d_{eff.}\\) = 2\\(d_s\\) + \\(d_z\\) + 3 and \\(d_z\\) < d.\n2.  Bypassing Policy Covers: As discussed above, working with policy coverings could lead to a large \\(d\\). The current work addresses this issue by establishing an upper bound on the total number of plays of \\(\\Phi(\\beta)\\) in terms of the \\(\\beta\\)-covering number of the set of all \\(\\beta\\)-suboptimal state-action pairs. Our proof hinges on the existence of certain \"key cells.\" More specifically, we show that whenever ZORL plays a suboptimal policy \\(\\phi\\), there exists a ball in the state-action space that satisfies the following two properties: (i) the number of visits to it so far is less than a certain quantity, which in turn is a function of the suboptimality gaps of all the state-action pairs (5) contained in it, (ii) The probability mass contained within it under the stationary measure under \\(\\phi\\), is sufficiently large. Such a ball is called a key cell for that particular episode, see Fig. 1. We show that for the set of policies \\(\\Phi(\\beta)\\), the total number of such possible key cells is upper bounded by the cardinality of a covering of the state-action space (Lemma F.3). This upper bound turns out to be \\(O(\\beta^{-d_z})\\), where \\(d_z\\) is the zooming dimension. We then show that since a key cell is supposed to contain a large stationary measure, it can not serve as a key cell for policies corresponding to"}, {"title": "1.2 Past Works", "content": "Lipschitz, MDPs on Metric Spaces: Domingues et al. (2021) obtain \\( \\tilde{O}(LH^{\\frac{3}{2}}K^{1-\\frac{1}{2d+1}})\\) regret by applying smoothing kernels. Cao and Krishnamurthy (2020) shows provable gains arising due to adaptive discretization and zooming and obtains \\(\\tilde{O}(H^{2.5+(2d_z+4)^{-1}}K^{1-\\frac{1}{2d+1}})\\), where \\(d_z\\) is the zooming dimension defined specifically for episodic RL. Sinclair et al. (2023) proposes a model-based algorithm with adaptive discretization that has a regret upper bound of \\(\\tilde{O}(L_V H^{\\frac{1}{2}}K^{1-\\frac{1}{d_z+ds}})\\), where \\(L_V\\) is the Lipschitz constant for the value function. We note that as compared with the works on general function approximation, regret bounds obtained in works on Lipschitz MDPs have a worse growth rate as a function of time horizon. However, this is expected since Lipschitz MDPs are a more general class of MDPs, and have a regret lower bound of \\(\\Omega(K^{1-\\frac{1}{d_z+2}})\\) (Sinclair et al., 2023).\nNon-episodic RL: For finite MDPs (Jaksch et al., 2010; Tossou et al., 2019) the minimax regret of state-of-the-art algorithms scales at most as \\(\\tilde{O}(\\sqrt{DSAT})\\) where D is the diameter of the MDP. However, algorithms for continuous MDPs are a less explored but important topic that is pursued in this work. For linear mixture MDPs on finite spaces (Wu et al., 2022), regret is upper bounded as \\(\\tilde{O}(d\\sqrt{DT})\\), where d is the number of component transition kernels, and D is the diameter. Wei et al. (2021) works with a known feature map, assumes that the relative value function is a linear function of the features, and obtains a \\(\\tilde{O}(\\sqrt{T})\\) regret. He et al. (2023) studies model approximation as well as value function approximation using general function classes and obtains \\(\\tilde{O}(\\text{poly}(d_e, B)\\sqrt{d_FT})\\) regret, where B is the span of the relative value function. Ortner and Ryabko (2012) obtains a \\(\\tilde{O}(T^{\\frac{2d+a}{2d+a}})\\) regret for a-H\u00f6lder continuous and infinitely often smoothly differentiable transition kernels. To the best of our knowledge, only (Kar and Singh, 2024) has studied adaptive discretization for average reward Lipschitz MDPs; however, their regret bound depends on a zooming dimension that could be huge, and moreover, the proposed algorithm is computationally infeasible."}, {"title": "2 Problem Setup", "content": "Notation. The set of natural numbers is denoted by N, the set of positive integers by \\(\\mathbb{Z}_+\\). We denote the span of a R-valued function \\(h\\in \\mathbb{R}^X\\) by \\(sp(h)\\), i.e., \\(sp(h) = \\max_{x\\in X} h(x) - \\min_{x\\in X} h(x)\\). We abbreviate \u201cwith high probability\u201d as \u201cw.h.p.\u201d For a \\(\\sigma\\)-algebra \\(\\mathcal{F}\\) and a measure \\(\\mu: \\mathcal{F} \\rightarrow \\mathbb{R}\\), we let \\(||\\mu||_{TV}\\) denote its total variation norm (Folland, 2013), i.e., \\(||\\mu||_{TV} := \\sup{|\\mu(B)| : B \\in \\mathcal{F}}\\). We use \\(\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, p, r)\\) to denote an MDP, where the state-space \\(\\mathcal{S}\\) and action-space \\(\\mathcal{A}\\) are compact sets of dimension \\(d_s\\) and \\(d_a\\), respectively. Let \\(\\mathcal{S}\\) and \\(\\mathcal{A}\\) be endowed with Borel \\(\\sigma\\)-algebra \\(\\mathcal{B}_S\\) and \\(\\mathcal{B}_A\\), respectively. To simplify exposition, we assume that \\(\\mathcal{S} = [0,1]^{d_s}\\) and \\(\\mathcal{A} = [0,1]^{d_a}\\) without loss of generality. For \\(Z \\subseteq \\mathcal{S} \\times \\mathcal{A}\\), \\(\\text{diam}(Z) := \\sup_{z_1,z_2\\in Z} \\rho(z_1, z_2)\\). We denote the system state and action taken at time t by \\(s_t, a_t\\) respectively. The state \\(s_t\\) evolves as follows,\n\\(P (S_{t+1} \\in B | S_t = s, a_t = a) = p(s, a, B), \\text{a.s.}, \\forall (s, a, B) \\in \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{B}_S, t \\in \\mathbb{Z}_+,\\)\nwhere \\(p: \\mathcal{S}\\times \\mathcal{A}\\times \\mathcal{B}_S \\rightarrow [0, 1]\\) is the transition kernel that is not known by the agent. The agent earns a reward \\(r(s_t, a_t)\\) at time t, where the reward function \\(r : \\mathcal{S}\\times \\mathcal{A} \\rightarrow [0,1]\\) is a measurable map. The goal of the agent is to maximize the infinite horizon average reward. The spaces \\(\\mathcal{S}, \\mathcal{A}\\) are endowed with metrics \\(\\rho_S\\) and \\(\\rho_A\\), respectively. The space \\(\\mathcal{S}\\times \\mathcal{A}\\) is endowed with a metric \\(\\rho\\) that is sub-additive, i.e., we have,\n\\(\\rho((s,a), (s', a')) \\leq \\rho_S(s, s') + \\rho_A(a, a'),\\)\nfor all \\((s, a), (s', a') \\in \\mathcal{S} \\times \\mathcal{A}\\).\nDefinition 2.1 (Stationary Deterministic Policy). A stationary deterministic policy is a measurable map \\(\\phi: \\mathcal{S} \\rightarrow \\mathcal{A}\\) that implements the action \\(\\phi(s)\\) when the system state is s. Let \\(\\Phi_{SD}\\) be the set of all such policies.\nThe infinite horizon average reward of a policy \\(\\phi\\) when it acts on an MDP \\(\\mathcal{M}\\) is denoted by \\(J_{\\mathcal{M}}(\\phi)\\), and is defined as,\n\\(J_{\\mathcal{M}}(\\phi) := \\liminf_{T\\rightarrow \\infty} \\frac{1}{T} \\mathbb{E}_{\\mathcal{M},\\phi} \\Big[\\sum_{t=0}^{T-1} r(S_t, a_t)\\Big]\\)\nwhere \\(\\mathbb{E}_{\\mathcal{M},\\phi}\\) denotes expectation taken under consideration that policy \\(\\phi\\) is used to take actions throughout on the MDP \\(\\mathcal{M}\\). The optimal average reward of the MDP \\(\\mathcal{M}\\) is defined as \\(J_{\\mathcal{M}} := \\sup_{\\phi\\in \\Phi_{SD}} J_{\\mathcal{M}}(\\phi)\\). The regret (Lai and Robbins, 1985) of a learning algorithm \\(\\mathcal{A}\\) until T is defined as,\n\\(R(T; \\mathcal{A}) := TJ_{\\mathcal{M}} - \\mathbb{E} \\Big[\\sum_{t=0}^{T-1} r(s_t, a_t)\\Big]\\)\nThe goal of this work is to design a learning algorithm with tight regret upper bound for Lipschitz MDPs. We now introduce the class of Lipschitz MDPs.\nAssumption 2.2 (Lipschitz continuity). (i) The reward function r is \\(L_r\\)-Lipschitz, i.e., \\(\\forall s, s' \\in \\mathcal{S}, a, a' \\in \\mathcal{A}\\), \n\\(|r(s, a) - r(s', a')| \\leq L_r\\rho ((s, a), (s', a')) .\\)\n(ii) The transition kernel p is \\(L_p\\)-Lipschitz, i.e., \\(\\forall s, s' \\in \\mathcal{S}, a, a' \\in \\mathcal{A}\\), \n\\(||p(s, a, \\cdot) - p(s', a',\\cdot)||_{TV} \\leq L_p\\rho ((s, a), (s', a')) .\\)"}, {"title": "3 Algorithm", "content": "Definition 3.1 (Cells). A cell is a dyadic cube with vertices from the set \\(\\{2^{-l}(v_1, v_2, ..., v_d) : v_j \\in [2^0], j = 1,2,...,d\\}\\) with sides of length \\(2^{-l}\\), where \\(l\\in \\mathbb{N}\\). The quantity l is called the level of the cell. We also denote the collection of cells of level l by \\(\\mathcal{P}(l)\\). For a cell \\(\\zeta \\subseteq \\mathcal{S} \\times \\mathcal{A}\\), its S-projection is called an S-cell,\n\\(\\pi_S(\\zeta) := \\{s \\in \\mathcal{S} | (s, a) \\in \\zeta \\text{ for some } a \\in \\mathcal{A}\\},\\)\nand its level is the same as that of \\(\\zeta\\). For a cell/S-cell \\(\\zeta\\), we let l(\\(\\zeta\\)) denote its level. For a cell/S-cell \\(\\zeta\\), we let q(\\(\\zeta\\)) be a point from that is its unique representative point. \\(q^{-1}\\) maps a representative point to the cell/S-cell that the point is representing, i.e., \\(q^{-1}(z) = \\zeta\\) such that \\(q(\\zeta) = z\\). Denote the set of S-cells of level l by \\(\\mathcal{Q}(l)\\).\nDefinition 3.2 (Partition tree). A partition tree of depth l is a tree in which (i) Each node at a depth m \u2264 l of the tree is a cell of level m. (ii) If \\(\\zeta\\) is a cell of level m, where m < l then, a) all the cells of level m + 1 that collectively generate a partition of \\(\\zeta\\), are the child nodes of \\(\\zeta\\). The corresponding cells are called child cells, and we use \\(\\text{Child}(\\zeta)\\) to denote the child cells of \\(\\zeta\\). b) \\(\\zeta\\) is called the parent cell of these child nodes. The set of all ancestor nodes of cell \\(\\zeta\\) is called ancestors of \\(\\zeta\\).\nThe proposed algorithm ZORL (2) maintains a set of \"active cells.\" The following rule is used for activating and deactivating cells.\nDefinition 3.3 (Activation rule). For a cell \\(\\zeta\\) denote,\n\\(N_{max}(\\zeta) := \\frac{c_1 2^{2d_s+2} \\log(\\frac{T}{\\delta})}{\\text{diam}(\\zeta)^{d_s+2}},\\)\nand,\n\\(N_{min}(\\zeta):= \\begin{cases} c_1 \\frac{\\log(\\frac{T}{\\delta})}{\\text{diam}(\\zeta)^{d_s+2}}, & \\text{if } \\zeta = \\mathcal{S} \\times \\mathcal{A} \\\\ 1, & \\text{otherwise}, \\end{cases}\\)\nwhere \\(c_1 > 0\\) is a constant that is discussed in Lemma B.1, and \\(\\delta \\in (0, 1)\\) is the confidence parameter. The number of visits to \\(\\zeta\\) is denoted \\(N_t(\\zeta)\\) and is defined as follows.\n1.  Any cell \\(\\zeta\\) is said to be active if \\(N_{min}(\\zeta) \\leq N_t(\\zeta) < N_{max} (\\zeta)\\).\n2.  \\(N_t(\\mathcal{C})\\) is defined for all cells as the number of times \\(\\zeta\\) or any of its ancestors has been visited while being active until time t, i.e.,\n\\(N_t(\\zeta) := \\sum_{i=0}^{t-1} \\mathbb{1}\\{(s_i, a_i) \\in \\zeta_i\\},\\)\nwhere \\(\\zeta_i\\) is the unique cell that is active at time i and satisfies\\( (s_i, a_i) \\in \\zeta_i\\).\nDenote the set of active cells at time t by \\(\\mathcal{P}_t\\).\nWe note that since the diameter of a child cell is equal to half that of its parent, a parent cell is deactivated, and its child cells are activated simultaneously. Since a cell is partitioned by its child cells, the set of active cells at time t, i.e. \\(\\mathcal{P}_t\\), forms a partition of the state action space. Denote the collection of representative points of the active cells at time t by \\(Z_t := \\{q(\\zeta) : \\zeta \\in \\mathcal{P}_t\\}\\). Denote \\(Q_t = \\mathcal{Q}(l_{max,t})\\), where \\(l_{max,t}\\) is the level of the smallest cells in \\(\\mathcal{P}_t\\). Define the discrete state space at time t as \\(S_t := \\{q(\\zeta) : \\zeta \\in \\mathcal{Q}_t\\}\\). Define"}, {"title": "4 Regret Analysis", "content": "We let \\(\\Delta(\\phi) := J_\\mathcal{M} - J_\\mathcal{M}(\\phi)\\) be the suboptimality of policy \\(\\phi\\). The following result establishes a relation between the suboptimality gap of state-action pairs and the suboptimality of the policies. Its proof is deferred to Appendix A.\nLemma 4.1. Consider the MDP \\(\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, p,r)\\). For any policy \\(\\phi \\in \\Phi_{SD}\\), we have\n\\(\\Delta(\\phi) = \\int_{\\mathcal{S}} gap(s, \\phi(s)) \\mu_{\\phi,p}^{(\\infty)}(s) ds.\\)\nWe now present our main result that provides an upper bound on regret of ZORL. We only provide a proof sketch here and delegate its detailed proof to the appendix.\nTheorem 4.2. \\(R(T; \\text{ZORL})\\) is upper bounded as \\(O(T^{\\frac{2d_s+d_z+3}{2d_s+d_z+2}})\\).\nProof sketch:\nRegret decomposition: We decompose the regret (3) in the following manner. Let K(T) denote the total number of episodes during T timesteps. Then, for any learning algorithm \\(\\mathcal{A}\\),\n\\(R(T; \\text{ZORL}) = T J_{\\mathcal{M}} - \\mathbb{E} \\Big[\\sum_{k=1}^{K(T)} \\sum_{t=T_k}^{T_{k+1}-1} r(s_t, a_t)\\Big] = \\sum_{k=1}^{K(T)} H_k \\Delta(\\phi_k) + \\sum_{k=1}^{K(T)} \\mathbb{E} \\Big[\\sum_{t=T_k}^{T_{k+1}-1} J_{\\mathcal{M}}(\\phi_k) - r(s_t, \\phi_k(s_t))\\Big],\\)\nwhere \\(\\Delta(\\phi_k) := J_{\\mathcal{M}} - J_{\\mathcal{M}}(\\phi_k)\\).\nThe term (a) captures the regret arising due to playing a suboptimal policy \\(\\phi_k\\) during the k-th episode, while (b) captures the possible degradation in performance during the transient stage as compared with the stationary distribution. (a) and (b) are bounded separately.\nBounding (a): Step 1: We show that the policy obtained by solving (17) is almost optimistic, i.e., w.h.p. \\(J_{\\mathcal{M}^t,c} \\geq J_{\\mathcal{M}} - O(1/T)\\) (Corollary D.2). We also show that w.h.p., \\(J_{\\mathcal{M}} \\leq J_{\\mathcal{M}^+,c} + C_1 \\text{diam}_t(\\phi_k) + O(1/T)\\), where \\(C_1 = 3L_r + \\frac{2C_v}{1-\\alpha}\\) (Corollary D.4). As a consequence of the above two results, on a high probability set, a suboptimal policy \\(\\phi\\) satisfying \\(\\Delta(\\phi) > 1/T\\) will never be played from episode k onwards if \\(\\text{diam}_t(\\phi) \\leq \\text{const} \\cdot \\Delta(\\phi)\\). Note that the cumulative regret arising due to policies with \\(\\Delta(\\cdot)\\) less than 1/T is at most a constant, so we can safely restrict analysis to regret arising from play of other policies."}]}