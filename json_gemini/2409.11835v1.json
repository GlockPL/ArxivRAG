{"title": "DPI-TTS: Directional Patch Interaction for Fast-Converging and Style Temporal Modeling in Text-to-Speech", "authors": ["Xin Qi", "Ruibo Fu", "Zhengqi Wen", "Tao Wang", "Chunyu Qiang", "Jianhua Tao", "Chenxing Li", "Yi Lu", "Shuchen Shi", "Zhiyong Wang", "Xiaopeng Wang", "Yuankun Xie", "Yukun Liu", "Xuefei Liu", "Guanjun Li"], "abstract": "In recent years, speech diffusion models have advanced rapidly. Alongside the widely used U-Net architecture, transformer-based models such as the Diffusion Transformer (DiT) have also gained attention. However, current DiT speech models treat Mel spectrograms as general images, which overlooks the specific acoustic properties of speech. To address these limitations, we propose a method called Directional Patch Interaction for Text-to-Speech (DPI-TTS), which builds on DiT and achieves fast training without compromising accuracy. Notably, DPI-TTS employs a low-to-high frequency, frame-by-frame progressive inference approach that aligns more closely with acoustic properties, enhancing the naturalness of the generated speech. Additionally, we introduce a fine-grained style temporal modeling method that further improves speaker style similarity. Experimental results demonstrate that our method increases the training speed by nearly 2 times and significantly outperforms the baseline models.", "sections": [{"title": "I. INTRODUCTION", "content": "Text-to-Speech (TTS) [1]\u2013[3] is the task of generating natural speech from a given text, offering significant application value across various domains. Among the many approaches developed for TTS, speech diffusion models have garnered considerable attention due to their impressive capabilities in producing highly expressive and natural speech. These models have undergone numerous improvements in recent years and evolved into distinct variants, each focusing on different aspects of TTS.\nU-Net [4] is one of the most prominent frameworks in speech diffusion models and has contributed to significant advancements in acoustic modeling and vocoder development. Notable models such as Diff-TTS [5] and Grad-TTS [6] have optimized the TTS diffusion processes, leading to enhanced performance. Models like Prodiff [7] and Comospeech [8] have achieved faster inference times, while NaturalSpeech [9] and NaturalSpeech2 [10] have reached near-human speech quality. DiffWave [11] introduced a flexible diffusion vocoder, and models such as FastDiff [12] and InferGrad [13] have reduced the number of iterations required for vocoder inference. Additional improvements in vocoder performance have been made by SpecGrad [14] and PriorGrad [15]. As research has advanced, it has been observed that the inductive bias of U-Net is not a critical factor in the performance of diffusion models [16]. This finding opens the possibility of replacing U-Net with Transformer [17] architectures, enabling the transfer of existing Transformer enhancement techniques to diffusion models, potentially leading to significant performance gains.\nDiT [16], a novel diffusion model based on Transformer architectures, has garnered significant attention due to its enhanced modeling capabilities. Recent work on DiT-TTS has demonstrated improved performance and more precise modeling of speech characteristics. Notably, U-DiT [18] achieved state-of-the-art (SOTA) performance on the single-speaker dataset LJSpeech [19]. DiTTo-TTS [20] further improves TTS alignment through cross-attention mechanisms and by predicting the total length of speech representations. Dex-TTS [21] advances speech style expressiveness by incorporating style information, categorized into time-invariant and time-variant components. Despite these advancements, current DiT-based TTS models share a common limitation: they treat Mel spectrograms as general images, neglecting the intrinsic acoustic properties of speech.\nGeneral image processing often emphasizes the uniformity and interconnectedness of all parts, typically requiring global computation or information as guidance. However, applying this approach to speech can result in overly smooth outputs that fail to capture the nuanced acoustic properties of speech signals. From an acoustic perspective, Mel spectrograms [22]\u2013[26] exhibit strong temporal correlations, where each frame is most closely related to its preceding frame. Additionally, different frequencies contain varying energy levels, with the human ear being more sensitive to lower frequencies. Therefore, we propose a novel method called DPI-TTS (Directional Patch Interaction for Text-to-Speech), which leverages patch"}, {"title": "II. METHOD", "content": "DPI-TTS features a text encoder with 8 Transformer layers using multi-head self-attention (MHSA) [17] and relative position embeddings (RoPE) [27]. It includes a convolution-based Duration Predictor (DP) [28] aligner that maps text to the initial Mel spectrogram frames hmel. The Diffusion Decoder comprises a Down Conv Block, a Patchify module for segmenting the Mel spectrogram into patches, k Global DiT blocks, N - k Directional DiT blocks, and an Up Conv Block for feature restoration. Global DiT blocks capture global speech information like pitch, while Directional DiT blocks handle style temporal modeling and directional interactions on mel patches.\nSpeech signals change dynamically over time, with the information they convey varying at different moments. Factors such as pauses, emphasis, rhythm, and prosody all possess distinct temporal properties in speech. Furthermore, the energy distribution between low and high-frequency components of speech varies, with the human ear being more sensitive to lower frequencies. By associating each Mel patch with its preceding frame and low-frequency components rather than the entire spectrum, this approach preserves dynamic temporal changes, improves low-frequency information representation, and enhances local details' modeling.\nSpecifically, we first compute the query, key, and value for each Mel spectrogram image patch, with initial shapes of (b, h, w, d), where b denotes the batch size, h is the number of patches along the frequency axis, w is the number of patches along the time axis, and d represents the feature dimension. Next, an additional dimension is inserted into the query, key, and value, altering their shapes to (b, h, w, 1, d). The last row and the first column of the key and value are then concatenated with the key and value, resulting in shapes of (b, h + 1, w + 1, 1, d). Subsequently, the concatenated key and value are split using a window of size (h, w), and the results are concatenated along the third dimension, changing the shape to (b, h, w, 4, d). For implementation details, please refer to Algorithm 1. Finally, all patches are flattened, and"}, {"title": "A. Directional Patch Interaction", "content": "Speech signals change dynamically over time, with the information they convey varying at different moments. Factors such as pauses, emphasis, rhythm, and prosody all possess distinct temporal properties in speech. Furthermore, the energy distribution between low and high-frequency components of speech varies, with the human ear being more sensitive to lower frequencies. By associating each Mel patch with its preceding frame and low-frequency components rather than the entire spectrum, this approach preserves dynamic temporal changes, improves low-frequency information representation, and enhances local details' modeling.\nSpecifically, we first compute the query, key, and value for each Mel spectrogram image patch, with initial shapes of (b, h, w, d), where b denotes the batch size, h is the number of patches along the frequency axis, w is the number of patches along the time axis, and d represents the feature dimension. Next, an additional dimension is inserted into the query, key, and value, altering their shapes to (b, h, w, 1, d). The last row and the first column of the key and value are then concatenated with the key and value, resulting in shapes of (b, h + 1, w + 1, 1, d). Subsequently, the concatenated key and value are split using a window of size (h, w), and the results are concatenated along the third dimension, changing the shape to (b, h, w, 4, d). For implementation details, please refer to Algorithm 1. Finally, all patches are flattened, and"}, {"title": "B. Fine-Grained Speaker Style Temporal Modeling", "content": "Thanks to the DiT-based TTS model's approach of dividing the Mel spectrogram into multiple patches, DPI-TTS enables fine-grained temporal control of style. Conventional methods typically rely on using a style ID or incorporating global style information extracted from reference speech across the entire speech. This often leads to overly smooth style expression and makes precise control challenging. In these conventional approaches, each patch in DPI-TTS only accesses partial style information, which can negatively impact style expression. To address this, DPI-TTS integrates style information sequentially over time, treating all patches at each time point as a cohesive unit. This ensures consistent style representation across high and low frequencies, aligning with the model's design characteristics.\nAs shown in Figure 1, DPI-TTS uses mel patches as queries, with speaker style as keys and values. To enhance the model's temporal awareness of style, time positional information is added to the mel patches. Finally, cross-attention is computed along the time dimension for each group of patches, integrating the style features into the speech."}, {"title": "III. EXPERIMENT", "content": "1) Dataset: To evaluate our method, we used the English single-speaker dataset LJSpeech [19] and the English multi-speaker VCTK dataset [29]. The VCTK dataset contains approximately 400 utterances per 109 speakers. Each dataset was split into training, validation, and test sets in a ratio of 70%, 15%, and 15%, respectively.\n2) Baselines: For comparison, we set the following systems as baselines: 1) GT, the ground-truth. 2)VITS, a multi-speaker TTS model based on VAE [30] and flow [31]. 3)Grad-TTS, a TTS diffusion model based on the U-Net framework, using HiFi-GAN as the vocoder. 4)Dex-TTS, a TTS diffusion model based on the DiT framework, using HiFi-GAN as the vocoder.\n3) Implementation Details: Training uses 1000 epochs for VCTK and 2000 epochs for LJSpeech. Parameters include a batch size of 32, patch size of 7, NN of 4, hidden dimension of 64, diffusion steps of 50, and kk of 2. Experiments were run on an NVIDIA 3090 GPU.\n4) Evaluation Metrics: For objective evaluation, we utilized Word Error Rate (WER%) and Cosine Similarity (COS). WER was obtained using a pre-trained Wav2Vec speech recognition model. At the same time, COS was calculated as the cosine similarity between synthesized and reference speaker features extracted from a pre-trained speaker recognition model, scaled by 100. For subjective evaluation, we assessed naturalness (MOS-N) and speaker similarity (MOS-S) on a scale from 1 to 5. At least 20 evaluators rated a minimum of 50 samples per speaker."}, {"title": "B. Experimental Results", "content": "1) Directional Patch Interaction enables fast training: We compared the training speed of DPI-TTS with the baseline Dex-TTS, as shown in Figure 2. Subfigures (a) and (b) display the loss curves of the baseline and DPI-TTS over 850 training epochs, respectively, while subfigure (c) illustrates the number of epochs completed by DPI-TTS and the baseline within the same training duration.\nExperimental results indicate that, compared to the baseline, our method maintains accuracy within the same number of epochs. In the same time frame, DPI-TTS trains nearly twice as many epochs. This indicates that our method boosts the training speed nearly 2 times without compromising performance.\n2) Fast training does not degrade performance: We demonstrate the effectiveness of Directional Patch Interaction in maintaining speech temporal consistency by comparing WER and MOS-N. The effectiveness of Style Temporal Modeling is validated through comparisons of COS. Experimental results on both single-speaker and multi-speaker datasets are presented in Table I.\nDPI-TTS outperforms the baseline in both WER and MOS-N. This improvement is achieved by allowing each Mel spectrogram patch to interact with preceding frames and low-frequency components based on acoustic properties rather than considering the entire spectrum. This focused interaction enhances the modeling of local details and captures subtle differences, such as variations in energy between high and low frequencies, thereby improving the naturalness of the generated speech. Moreover, the sequential generation of each patch, guided by acoustic properties, enforces effective temporal information modeling. This process strengthens the coherence between frames, resulting in more fluent and consistent TTS.\nDPI-TTS outperforms the baseline in COS. Our analysis suggests that fine-grained style temporal modeling enables each patch to fully incorporate style information. By sequentially integrating all patches with style information at each time point, DPI-TTS enhances the consistency of style representation across both high and low frequencies."}, {"title": "C. Ablation Studies", "content": "1) Directional Patch Interaction: We conducted ablation experiments on the LJSpeech dataset to evaluate the Directional Patch Interaction, which is designed based on acoustic properties. We assessed all reasonable combinations, and the results are presented in Table II. In this table, the following notations are used: n denotes the next frame, p denotes the previous frame, l represents low frequency, h represents high frequency, and ph indicates the high-frequency part of the previous frame, among others.\nThe results indicate that deviations from the low-to-high frequency sequence and the previous-to-next frame order lead to declines in WER, COS, and MOS-N scores to varying degrees. This finding underscores the necessity and effectiveness of inference based on acoustic properties. Adhering to these constraints ensures more accurate and reliable modeling of speech features.\n2) Other: We performed ablation experiments on \u201cpositional information\u201d and \u201cstyle temporal modeling\u201d using the"}, {"title": "IV. CONCLUSION", "content": "This paper proposes a novel method called Directional Patch Interaction for Text-to-Speech (DPI-TTS). By integrating specific acoustic processing into the diffusion framework, DPI-TTS addresses the limitations of existing DiT speech models that treat the Mel spectrogram as a general image. The model employs frame-by-frame inference from low to high frequencies, enhancing the naturalness of synthetic speech while maintaining high training efficiency without compromising accuracy. Additionally, fine-grained style temporal modeling further improves speaker style similarity and sets DPI-TTS apart from traditional methods. Experimental results validate the effectiveness of the proposed method, demonstrating significant improvements in naturalness and style consistency.\nDPI-TTS offers a promising approach for transformer-based speech synthesis, contributing valuable insights and potential advancements to the field."}]}