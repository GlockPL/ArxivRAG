{"title": "Biometric Authentication Based on Enhanced Remote Photoplethysmography Signal Morphology", "authors": ["Zhaodong Sun", "Xiaobai Li", "Jukka Komulainen", "Guoying Zhao"], "abstract": "Remote photoplethysmography (rPPG) is a non-contact method for measuring cardiac signals from facial videos, offering a convenient alternative to contact photoplethysmography (cPPG) obtained from contact sensors. Recent studies have shown that each individual possesses a unique cPPG signal morphology that can be utilized as a biometric identifier, which has inspired us to utilize the morphology of rPPG signals extracted from facial videos for person authentication. Since the facial appearance and rPPG are mixed in the facial videos, we first de-identify facial videos to remove facial appearance while preserving the rPPG information, which protects facial privacy and guarantees that only rPPG is used for authentication. The de-identified videos are fed into an rPPG model to get the rPPG signal morphology for authentication. In the first training stage, unsupervised rPPG training is performed to get coarse rPPG signals. In the second training stage, an rPPG-cPPG hybrid training is performed by incorporating external cPPG datasets to achieve rPPG biometric authentication and enhance rPPG signal morphology. Our approach needs only de-identified facial videos with subject IDs to train rPPG authentication models. The experimental results demonstrate that rPPG signal morphology hidden in facial videos can be used for biometric authentication.", "sections": [{"title": "1. Introduction", "content": "Facial videos contain invisible skin color changes induced by remote photoplethysmography (rPPG) signals, providing valuable cardiovascular information, such as heart rate. Similar to rPPG, contact photoplethysmography (cPPG) captures color changes in fingertips to monitor blood volume changes. cPPG signals, obtained using con- tact sensors, have been used for biometric authentication [12, 11]. Given the similar nature and measurement prin- ciples of rPPG and cPPG [26], rPPG has the potential for biometric authentication. However, the feasibility of rPPG biometric authentication still needs to be validated. Hence, our research questions are: 1) Can rPPG signals be em- ployed for biometric authentication? 2) If so, how can an rPPG-based biometric system be developed? 3) What are the advantages associated with utilizing rPPG biometrics?\nWe first examine the quality and discriminative power of rPPG signals. rPPG signals are derived from subtle changes in facial color caused by blood volume changes during heartbeats. Recent advances [49, 20] have achieved high-quality rPPG measurement, especially when the face has minimal or no movement. Hence, it is feasible to obtain high-quality rPPG signals. However, the question remains whether these high-quality rPPG signals contain subject- specific biometric characteristics. One work [32] has tried using rPPG for biometrics, but the preliminary study was limited by a small-scale dataset and low-quality rPPG, of- fering inadequate authentication performance for practical applications.\nIn this paper, we propose an rPPG-based method for biometric authentication, as shown in Fig. 1(a). Consid- ering facial appearance and rPPG are mixed together in facial videos, we first de-identify facial videos while pre- serving the rPPG information. This step can guarantee that only rPPG information is used for biometric authentication while facial appearance cannot be used. In addition, this step can also conceal sensitive facial appearance informa- tion for privacy protection. The first module is the rPPG model that can extract rPPG signals from the de-identified facial videos. The second module is the rPPG-Authn model that utilizes the rPPG morphology to output person authen- tication results. We design a two-stage training strategy and rPPG-cPPG hybrid training by incorporating external CPPG datasets to exploit rPPG morphology for biometric authenti- cation. Fig. 1(b) illustrates the rPPG morphology enhance- ment. Note that we only use de-identified videos with sub- ject IDs for rPPG biometrics.\nThere are several advantages of rPPG biometrics. Com- pared with facial appearances, the rPPG biometric sys- tem only utilizes de-identified facial videos, eliminating the need for sensitive facial appearance. Moreover, rPPG bio- metrics offers an additional degree of resistance to spoof- ing, as rPPG inherently serves as a countermeasure to pre- sentation attacks [21, 19]. In contrast, without dedicated presentation attack detection (PAD) methods, conventional face recognition algorithms are vulnerable to presentation attacks and less secure than rPPG-based biometrics. Addi- tionally, since both rPPG biometrics and face recognition use facial videos as data sources, combining both biomet- ric modalities can potentially enhance both accuracy and security. When compared with cPPG biometrics, rPPG biometrics offers the advantages of being non-contact and only requiring off-the-shelf cameras, while cPPG biomet- rics necessitates specific contact sensors like pulse oxime- ters. Compared with iris recognition [46, 5] which requires iris scanners, rPPG biometrics only requires cheap RGB cameras and is robust to presentation attacks.\nOur contributions include:\n1.  We propose a new biometric authentication method based on rPPG. We utilize two-stage training to achieve rPPG morphology enhancement and accurate biometric authentication performance. We illustrate that utilizing de-identified facial videos is effective for rPPG biometric authentication and ensures the protec- tion of facial appearance privacy.\n2.  We conduct comprehensive experiments on multiple datasets to validate the discriminative power of rPPG biometrics. We demonstrate that rPPG biometrics can achieve comparable performance with cPPG biomet- rics. We also investigate factors that may influence the performance of rPPG biometrics.\n3.  We discover that our rPPG-based biometric method can enhance rPPG morphology, which opens up pos- sibilities for rPPG morphology learning from facial videos."}, {"title": "2. Related Work", "content": "2.1. rPPG Measurement\n[41] initially proposed measuring rPPG from face videos via the green channel. Subsequent handcrafted methods have been introduced to enhance the quality of the rPPG sig- nal [34, 6, 18, 40, 45]. Recently, there has been rapid growth in deep learning (DL) approaches for rPPG measurement. Several studies [4, 37, 20, 31, 16] utilize 2D convolutional neural networks (CNN) to input consecutive video frames for rPPG measurement. Another set of DL-based meth- ods [28, 29, 23, 24, 7] employ a spatial-temporal signal map obtained from different facial regions, which is then fed into 2DCNN models. 3DCNN-based methods [50] and transformer-based methods [52, 51] have been proposed to enhance spatiotemporal performance and long-range spa- tiotemporal perception.\nAdditionally, multiple unsupervised rPPG methods [8, 43, 39, 36, 47, 53] have been proposed. Since GT signals are expensive to collect and synchronize in rPPG datasets, unsupervised rPPG methods only require facial videos for training without any GT signal and achieve performance similar to the supervised methods. However, most works on rPPG measurement primarily focus on the accuracy of heart rate estimation, while neglecting the rPPG morphology.\n2.2. cPPG-based Biometrics\n[10] was the first attempt to utilize cPPG for biomet- ric authentication. They extracted some fundamental mor- phological features, such as peak upward/downward slopes, for cPPG biometrics. Subsequently, other studies have ex- plored additional morphological features, including cPPG derivatives [48] and fiducial points [22]. More recently, re- searchers have focused on employing DL methods to au- tomatically extract morphological features. [25, 2, 15] di- rectly input cPPG signals into 1DCNN or long short-term memory (LSTM) architectures to conduct biometric authen- tication, while [12, 11] cut cPPG signals into periodic seg- ments and utilize multiple representations of these periodic segments as inputs to a 1DCNN model. Furthermore, [12] has collected datasets for CPPG biometrics and investigated the permanence of cPPG biometrics. There exists one pre- liminary work on rPPG biometrics [32], but only a tradi- tional independent component analysis (ICA) based method"}, {"title": "3. Method", "content": "Our method consists of facial video de-identification and two training stages. As the rPPG signal does not rely on fa- cial appearance, we first de-identify the input video to avoid facial appearance being used by our method. In the first training stage, we perform unsupervised rPPG training on the de-identified videos to achieve basic rPPG signal mea- surement. In the second training stage, we use rPPG-cPPG hybrid training for biometric authentication and rPPG mor- phology enhancement.\n3.1. Face De-identification for rPPG Biometrics\nWe propose to de-identify facial videos using spatial downsampling and pixel permutation. This step aims to ob- fuscate facial appearances while preserving the rPPG infor- mation. Since rPPG signals are spatially redundant at differ- ent facial regions and largely independent of spatial infor- mation as shown by [40, 27], rPPG signals can be well pre- served in this step while facial appearances are completely erased. The reasons for face de-identification are twofold. First, the facial appearance and rPPG information are in- terwined in facial videos. We remove facial appearance to make sure that the biometric model performs recognition solely based on the rPPG information. Second, this step can remove facial appearances to protect facial privacy informa- tion during rPPG authentication.\nThe facial video is de-identified as shown in Fig. 2. Faces in the original videos are cropped using OpenFace [1] by locating the boundary landmarks. The cropped fa- cial video $v \\in \\mathbb{R}^{T \\times H \\times W \\times 3}$, where T, H, and W are time length, height, and width, is downsampled by averaging the pixels in a sample region to get $v_d \\in \\mathbb{R}^{T \\times 6 \\times 6 \\times 3}$. It has been demonstrated that such downsampled facial videos are still effective in rPPG estimation [40, 27]. Since rPPG signal ex- traction does not largely depend on spatial information [40], we further permutate the pixels to completely obfuscate the spatial information to get $v_{de} \\in \\mathbb{R}^{T \\times 6 \\times 6 \\times 3}$. Note that the permutation pattern is the same for each frame in a video but distinct for different videos. Since the spatial informa- tion is eliminated, we reshape the de-identified video $v_{de}$ into a spatiotemporal (ST) map $M \\in \\mathbb{R}^{36 \\times T \\times 3}$ for compact rPPG representation like [27].\n3.2. The 1st training stage: rPPG Unsupervised Pre-training\nThis stage aims to train a basic rPPG model capable of extracting rPPG with precise heartbeats. We use un- supervised training to obtain the basic rPPG model. The main reasons for unsupervised training are: 1) Unsuper- vised rPPG training does not require GT PPG signals from contact sensors, which means only facial videos with sub- ject IDs are required in our entire method. 2) The perfor- mance of unsupervised rPPG training [8, 39] is on par with supervised methods.\nWe adopt and customize the unsupervised Contrast-Phys"}, {"title": "3.3. The 2nd training stage: rPPG-cPPG Hybrid Training", "content": "At the second training stage, we further refine rPPG sig- nals to obtain morphology information. Fig. 5 shows the rPPG-cPPG hybrid training, where the rPPG branch uti- lizes face videos and ID labels during training. On the other hand, the CPPG branch uses external cPPG biomet- ric datasets to encourage the PPG-Morph model H to learn morphology information, which can be incorporated into the rPPG branch through the PPG-Morph model H. The PPG-Morph model H comprises 1DCNN layers and trans- former layers that extract morphological features from peri- odic segments. The two branches are trained alternately to facilitate the sharing of morphology information between the rPPG and cPPG branches. Note that our method only requires de-identified facial videos with subject IDs dur- ing training (enrollment) and only needs de-identified facial videos during inference.\n3.3.1 rPPG Branch\nThe rPPG branch can extract rPPG morphology and use it to differentiate individuals. This branch only requires a de- identified facial video $v_{de}$ and the ID label $i_{rppg}$ and does not need any GT cPPG signal for training. Therefore, de- identified facial videos with ID labels are sufficient for en- rollment in the proposed rPPG biometrics scheme. The ST map M derived from the de-identified facial video $v_{de}$ is fed into the pre-trained rPPG model G to obtain the rPPG signal $s_{rppg}$. Note that the rPPG model G is the pre-trained model from the first unsupervised training stage. To seg- ment the signal, the systolic peaks are located, and the sig- nal $s_{rppg} \\in \\mathbb{R}^{T}$ is divided into K clips. Due to heart rate variability, the K clips may have different lengths, so the clip length is interpolated to 90 in order to obtain rPPG pe- riodic segments. The choice of a length of 90 is based on the fact that the minimum heart rate (40 beats per minute) for a 60 Hz signal produces the longest periodic segment with a length of 90. Consequently, we obtain $C_{rppg} \\in \\mathbb{R}^{K \\times 90}$.\nTo predict an authentication score for an individual, we use the PPG-Morph model H and the rPPG classification head $h_{rppg}$, which provides the rPPG morphology representation $f_{rppg} \\in \\mathbb{R}^{64}$ and ID probability $Y_{rppg} \\in [0, 1]^{K \\times N_{rppg}}$, where $N_{rppg}$ is the number of individuals in the rPPG bio- metric dataset. The cross-entropy loss is used for ID classi- fication, which is\n$L_{rppg-id}(Y_{rppg}, i_{rppg}) = - \\frac{1}{K} \\sum_{k=0}^{K} log(y_{k}^{i_{rppg}})$\n3.3.2 CPPG Branch\nThe CPPG branch utilizes external cPPG biometric datasets including Biosec2 [12], BIDMC [33, 9], and PRRB [14], to learn PPG morphology. Note that the external cPPG biometric datasets are available online and are not related to the facial videos in the rPPG branch. Similar to the rPPG branch, the cPPG signal $S_{cppg} \\in \\mathbb{R}^{T}$ is processed to obtain $K'$ cPPG periodic segments $C_{cppg} \\in \\mathbb{R}^{K' \\times 90}$. The PPG-Morph model H and cPPG classification head $h_{cppg}$ are employed to generate the cPPG morphology rep- resentation $f_{cppg} \\in \\mathbb{R}^{64}$ and the ID probability prediction $Y_{cppg} \\in [0, 1]^{K' \\times N_{cppg}}$, where $N_{cppg}$ is the number of indi- viduals in the external cPPG biometric datasets. Note that the PPG-Morph model H is shared by both the rPPG branch and cPPG branch, allowing the cPPG branch to transfer the learned morphology information to the rPPG branch. The cross-entropy loss is utilized in this branch, which is\n$L_{cppg-id}(Y_{cppg}, i_{cppg}) = - \\frac{1}{K'} \\sum_{k'=0}^{K'} log(y_{k'}^{i_{cppg}})$\nwhere $y_{k'}^{i_{cppg}}$ is the predicted probability of the $k'th$ periodic segment belonging to the ID label $i_{cppg} \\in \\{1, 2, ..., N_{cppg}\\}$.\n3.3.3 Alternate Backpropagation\nWe alternately train the two branches and backpropagate the gradient of the two loss functions $L_{rppg-id}$ and $L_{cppg-id}$ to achieve rPPG-cPPG hybrid training. During the first step, de-identified facial videos and ID labels are sampled from the rPPG biometric dataset to calculate the loss $L_{rppg-id}$, and the gradient of $L_{rppg-id}$ is backpropagated to update the rPPG model G, the PPG-Morph model H, and the rPPG classification head $h_{rppg}$. During the second step, CPPG signals and ID labels are sampled from external cPPG bio- metric datasets to calculate the loss $L_{cppg-id}$, and the gra- dient of $L_{cppg-id}$ is backpropagated to update PPG-Morph model H and the cPPG classification head $h_{cppg}$. These two steps are repeated in an alternating manner, allowing the two branches to be trained in turns. The CPPG branch uses external cPPG datasets to encourage the PPG-Morph model H to learn morphology information. The morphology fea- tures learned from the cPPG branch can then be incorpo- rated into the rPPG branch since the PPG-Morph model H is shared by both cPPG and rPPG branches thus rPPG fea- tures are enhanced. The supplementary materials provide a detailed description of the algorithm."}, {"title": "4. Experiments", "content": "4.1. Implementation Details\nDatasets. We considered three public rPPG datasets, namely OBF [17], PURE [38], and UBFC-rPPG [3]. The scales of these rPPG datasets are enough to validate the feasibility of rPPG biometrics since previous cPPG biomet- ric datasets [12, 11] also have similar scales. These rPPG datasets consist of facial videos, GT cPPG signals, and ID labels, but our method does not require the GT CPPG. OBF dataset [17] consists of data from 100 healthy subjects. Two 5-minute RGB facial videos were recorded for each partici- pant. For each subject, the first facial video was recorded at rest, while the second was recorded after exercise. During the recording, participants remained seated without head or facial motions. Videos have a resolution of 1920\u00d71080 at 60 frames per second (fps). UBFC-rPPG dataset [3] was captured using a webcam at a resolution of 640x480 at 30 fps. In each recording, the subject was positioned 1 meter away from the camera and playing a mathematical game, with the face centrally located within the video frame. The database consists of data from 42 participants, with each one having a 1-minute video. PURE dataset [38] contains data from 10 subjects. Face videos for each subject were captured in 6 distinct scenarios: steady, talking, slow trans- lation, fast translation, small rotation, and medium rotation, leading to a total of 60 one-minute RGB videos. Videos have a resolution of 640\u00d7480 at 30 fps.\nAdditionally, we combined the Biosec2 [12], BIDMC [33, 9], and PRRB [14] datasets to create the external cPPG biometric dataset. These datasets contain cPPG signals from 195 subjects for the CPPG branch in the rPPG-cPPG hybrid training. More details about datasets are provided in the supplementary materials.\nExperimental Setup. Our rPPG biometric experiments follow the previous cPPG biometric protocol [12, 11] where the training and test sets have the same persons but might be recorded in the same session (intra-session test) or recorded in different sessions (cross-session test). For the OBF dataset, we divide each pre-exercise video into three parts: the first 60% length is used for training, the following 20% length is used for validation, and the last 20% length is used for intra-session testing. The post-exercise videos are re- served for cross-session testing. As for the UBFC-rPPG dataset, the same division is applied to each video. Since each subject only contributes one video, only intra-session testing can be conducted on this dataset. Moving on to the PURE dataset, the same division is applied to each steady video. The videos involving head motion tasks are used exclusively for cross-session testing. At the first training stage, we select the best rPPG model with the lowest irrele- vant power ratio (IPR) in the validation set, as conducted in [8, 39]. At the second training stage, we choose the best-performing models based on the lowest equal error rate (EER) in the validation set. Both training stages are carried out on a single Nvidia V100 GPU and employ the Adam op- timizer with a learning rate of 1e-3. During inference, the predicted probabilities from consecutive periodic segments (5 beats, 10 beats, and 20 beats) are averaged.\nEvaluation Metrics. Since the model does multi-class classification, we use the one-vs-rest strategy to get the au- thentication results for each person. Therefore, each person has a binary classification. For each person, we can change the threshold of the model prediction output for that person to get the binary predictions, and we can plot false positive rates and true positive rates in a graph, which is the receiver operating characteristic (ROC) curve. Areas under curve (AUC) is the area under the ROC curve. If we change the threshold, we can find the threshold where the false posi- tive rate and the false negative rate are equal. The EER is the false positive rate or false negative rate at this threshold. The final EER and AUC are averaged across all subjects. To evaluate the rPPG morphology, we calculate the Pearson correlation between the means of periodic segments from rPPG and the GT cPPG. More details are in the supplemen- tary materials."}, {"title": "4.2. Results and discussions", "content": "4.2.1 Results and discussions about rPPG authentica- tion.\nTable 1 presents the results of rPPG authentication with varying signal lengths. The performance of rPPG authen- tication improves with longer signal lengths, such as 20 heartbeats, compared to shorter signal lengths like 10 or 5 beats. On all three datasets, the intra-session performance is satisfactory, with EERs below 1% and AUCs above 99%. However, the performance decreases during cross-session testing. On the OBF dataset, the cross-session (pre-exercise \u2192 post-exercise) performance is slightly lower than the intra-session (pre-exercise \u2192 pre-exercise) performance, but still achieves EER of 2.16%. On the PURE dataset, there is a significant drop in performance during cross-session (steady \u2192 motion tasks) compared to intra-session (steady \u2192 steady) due to the adverse impact of motion tasks on the quality of rPPG signals. Conversely, although the OBF dataset includes exercises to increase heart rates, it does not involve facial movements. This indicates that rPPG biometrics is sensitive to low-quality rPPG caused by facial motions but rPPG has reliable and unique biometric infor- mation evidenced by the varying heart rates from the same people. In practical usage, users will face the camera and keep still (like face recognition), thus such large intended head motions will not be a concern.\nThe observed rPPG periodic segments from different subjects (subject A-I) in Fig. 6 align with the aforemen- tioned quantitative results. The rPPG periodic segments from the OBF dataset exhibit consistent morphology before and after exercises in Fig. 6(a). Conversely, the motion tasks in the PURE dataset significantly alter morphology in Fig. 6(c), resulting in noisy rPPG signals and a drop in performance during cross-session testing. Furthermore, the rPPG periodic segments from all three datasets display dis- tinct morphologies for different subjects, highlighting the discriminative power of rPPG morphology. Fig. 7 shows the subject-specific biometric characteristics of rPPG mor- phology in detail. The rPPG periodic segments from two subjects have distinct fiducial points [22] such as the sys- tolic peaks, diastolic peaks, dicrotic notch, and onset/offset, which contain identity information.\nRegarding fairness, prior studies [30, 42] highlighted skin bias in rPPG signal quality. Dark skin may yield lower-quality rPPG signals, impacting authentication per- formance. We assess authentication performance for light and dark skin groups in the OBF dataset with a 20-heartbeat signal length and cross-session testing. For light skin, EER and AUC are 2.52% and 97.79%, respectively. For dark skin, EER and AUC are 4.04% and 96.74%. The perfor- mance of dark skin slightly falls behind that of light skin, indicating a skin tone bias in rPPG biometrics. Addressing this fairness issue may involve collecting more data from dark-skinned people or developing new algorithms, which remains a topic for future research.\n4.2.2 Comparison with other biometrics.\nIn Table 2, we compare rPPG biometrics with related bio- metric methods, including face and cPPG biometrics, when the signal length is 20 beats. For face recognition, we choose the highly cited face recognition method (FaceNet [35]) to prove how general face recognition works on de- identified facial videos. We use FaceNet to extract embed- dings from de-identified images and train two fully con- nected layers on the embeddings to get the classification results. Table 2 demonstrates that FaceNet [35] fails to work on de-identified videos, indicating that there is no facial appearance information in the de-identified videos. Since our rPPG biometric method is privacy-preserving for facial appearances, we also compare our method with the recent privacy-preserving face recognition [13]. The re- sults show that our method can achieve better performance than privacy-preserving face recognition [13]. Our rPPG biometric authentication completely gets rid of facial ap- pearance while the privacy-preserving face recognition [13] only adds noises to partially remove facial appearances to guarantee face recognition performance, which may still have risks of privacy leakage. In addition, we also compare our method w/ rPPG-cPPG hybrid training to our method w/ rPPG training (only rPPG branch is used for training in Fig. 5, and the cPPG branch is disabled during training). On the OBF dataset, ours w/ rPPG-cPPG hybrid train- ing achieves similar intra-session performance to ours w/ rPPG training, but achieves the best cross-session perfor- mance. This means external CPPG datasets introducing morphology information can improve generalization, such as cross-session performance. Furthermore, our rPPG bio- metrics exhibits better performance than cPPG biometrics.\n4.2.3 Results and discussions about rPPG morphology.\nWe also made an interesting finding that the rPPG-CPPG hybrid training can significantly improve rPPG morphol- ogy reconstruction. Table 3 shows the Pearson correla- tions between the mean periodic segments of GT cPPG and rPPG. High Pearson correlations mean rPPG morphology better resembles the corresponding GT cPPG. Note that our method does not require any GT cPPG for rPPG morphol- ogy reconstruction, so we choose unsupervised rPPG meth- ods including POS [44], ICA [34], and [8] for compari- son. Ours w/rPPG-cPPG hybrid training achieves signif- icantly higher Pearson correlation than the baseline meth- ods, CP2D, and ours w/ rPPG training, as the external CPPG datasets introduce helpful morphology information via the hybrid training to refine the rPPG morphology. Such cPPG datasets are publicly available, and thus do not introduce extra costs of data collection."}, {"title": "5. Conclusion", "content": "In this paper, we validated the feasibility of rPPG bio- metrics from facial videos. We proposed a two-stage train- ing scheme and novel cPPG-rPPG hybrid training by using external cPPG biometric datasets to improve rPPG biomet- ric authentication. Our method achieves good performance on both rPPG biometrics authentication and rPPG mor- phology reconstruction. In addition, our method uses de- identified facial videos for authentication, which can protect sensitive facial appearance information. Future work will focus on collecting a large-scale rPPG biometric dataset and studying influencing factors like temporal stability, lighting, and recording devices."}]}