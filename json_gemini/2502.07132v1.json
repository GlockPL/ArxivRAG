{"title": "Interactive Data Harmonization with LLM Agents", "authors": ["A\u00e9cio Santos", "Roque Lopez", "Eduardo H. M. Pena", "Juliana Freire"], "abstract": "Data harmonization is an essential task that entails integrating datasets from diverse sources. Despite years of research in this area, it remains a time-consuming and challenging task due to schema mismatches, varying terminologies, and differences in data collection methodologies. This paper presents the case for agentic data harmonization as a means to both empower experts to harmonize their data and to streamline the process. We introduce Harmonia, a system that combines LLM-based reasoning, an interactive user interface, and a library of data harmonization primitives to automate the synthesis of data harmonization pipelines. We demonstrate Harmonia in a clinical data harmonization scenario, where it helps to interactively create reusable pipelines that map datasets to a standard format. Finally, we discuss challenges and open problems, and suggest research directions for advancing our vision.", "sections": [{"title": "INTRODUCTION", "content": "Extracting insights from multiple data sources remains an arduous and time-consuming task. In fields such as biomedicine, collecting patient data requires expensive trials that typically involve only a few dozen to hundreds of patients [42]. Since this usually leads to tables with many attributes but few samples, researchers need to combine different cohorts to obtain larger sample sizes. Since data is often collected using differing methods, combining them into compatible and comparable datasets often becomes a challenge [12].\nExample 1 (Clinical Data Harmonization). To obtain a larger dataset for a study on endometrial cancer, researchers aim to combine samples from two patient cohorts collected independently in two studies [17, 18]. These studies produced two tables containing clinical data, which we refer to as T\u2081 and T2. Each row in each table represents an individual patient sample. Tables T\u2081 and T2, contain 179 and 213 columns and 153 and 190 rows, respectively. The goal is to combine the data to obtain a table with 392 rows. However, even though these datasets were produced by the same research consortium, their schemas and naming standards differ significantly. The first challenge is identifying which columns are semantically equivalent. Once all pairs of equivalent columns have been identified, their values must be standardized. Figure 1 shows an example of a pair of equivalent attributes from T\u2081 and T2: Histologic_grade and Histologic_Grade_FIGO. As seen in the figure, the values of these attributes are not represented using the same terminology even though they are semantically equivalent. To produce a harmonized table, the researchers must reconcile these values into a single format before merging the rows. For instance, they may decide that the final harmonized table Ttarget will contain an attribute named histologic_grade and that the format of the value used will be from Histologic_grade, and thus we would need to map the values of T\u2081 to their corresponding values from T2 (e.g., \"FIGO grade 1\"\u2192\"G1 Well differentiated\", \"FIGO grade 2\" \u2192 \"G2 Moderately differentiated\", and so on). Note, however, that some attributes may be mapped differently. E.g., attributes with unique identifiers can be kept as they appear in the original table while ensuring a uniqueness constraint.\nExample 2 (Harmonizing Data to a Standard Vocabulary). In a subsequent effort to foster data reuse and enable research in pan-cancer analysis [42], researchers decided to combine 10 tables containing cohorts of a larger variety of cancer types [7, 14, 18, 23, 26, 36, 49, 55, 62, 65]. They had to map all tables to the Genomic Data Commons (GDC) standard [29], a standard describing attributes commonly used in cancer research. Figure 1 shows how the variables Histologic_grade and Histologic_Grade_FIGO map to their equivalent GDC attribute tumor_grade. Once again, the acceptable values in the GDC vocabulary differ from the values used in the tables of Example 1.\nThe Case for Agentic Data Harmonization. Data harmonization involves several data integration tasks. Practitioners often rely on spreadsheet software, bespoke scripts, and significant manual work to harmonize data [12]. These custom scripts are often not published with the data and publications, creating barriers to reproducibility and replicability of experiments with new data. [50].\nLarge language models (LLMs) open new opportunities to improve data harmonization. They can answer questions about terminology, methodologies and generate code without training data."}, {"title": "PRELIMINARIES", "content": "Broadly speaking, data harmonization refers to the practice of combining and reconciling different datasets to maximize their comparability or compatibility [12]. Although data harmonization goals can vary greatly depending on the data modalities and goals involved, this paper focuses on the following problem:\nDefinition 1 (Tabular Data Harmonization). Given a set of source tables T1, T2, ..., Tn and a target schema Starget composed of a set of attributes, each attribute specifying its acceptable values, i.e., a domain, the goal is to derive a computational pipeline P that takes as input the source tables and applies transformation functions to values and combines tables (e.g., using union and join operations) to generate an output table Ttarget = P(T1, T2, ..., Tn) that adheres to the given target schema Starget.\nNote that instead of aiming at maximizing an objective measure of comparability or compatibility between input and target, we assume the existence of the canonical data representation Starget. As seen in Examples 1 and 2, this specification Starget consists of a set of attribute names and domain specifications that can either come from a standard data vocabulary (such as the GDC) or could be derived from the existing data in the source tables."}, {"title": "AGENTIC DATA HARMONIZATION", "content": "We propose using LLM-based agents to facilitate the interactive construction of harmonization pipelines through natural language and visual interfaces. We aim to simplify the harmonization process and empower domain experts to harmonize their own data effectively.\nOur approach has three main aspects - harmonization primitives, harmonization agents, and human-agent interaction, as illustrated in Figure 2a. The system should support two-way interaction between the users and the harmonization agents. While the former drives the system and defines the tasks to be performed, the latter aims to automate most data harmonization tasks, leaving only key decisions that need external context to users. By capturing user-agent interactions and the derived computational pipelines, we maintain the provenance of the harmonization process. This supports transparency and reproducibility, making it possible to publish the harmonized data with the pipeline used to derive it.\nHarmonization Primitives. The bottom of Figure 2a illustrates a library of components that we refer to as data integration primitives. These are key algorithms or routines that can be developed to solve well-defined data integration tasks such as schema matching, value matching, and others. Some of these components are lower-level routines that support other higher-level tasks, e.g., column-type annotation may be a component of schema-matching algorithms [22], and entity resolution and deduplication may be key to performing data standardization [13]. While the particular set of primitives can be heterogeneous and evolve to support the system capabilities, they have in common that they need to be composable and can be invoked by both user and AI agents.\nPrimitive composability refers to the ability to combine primitives to create a pipeline: the output of a primitive p\u2081 can be used as input of another primitive p2. For example, schema matching primitives can output a list of source-target attribute pairs. These lists form the input for the value matching primitives, which then find equivalences between the values of the source attribute to the target. Finally, the output of value-matching primitives can be used to assemble a harmonization specification that describes the transformation of source tables T\u012f into a target output table Ttarget.\nIn short, primitive composability enables the creation of data harmonization pipelines by allowing the chaining of operations that take source tables as input and produce harmonized data as output.\nData Harmonization Agents. The harmonization agent fulfills user requests by synthesizing pipelines that perform the requested harmonization task. First, it decomposes the problem in a sequence of actions. These actions can be of multiple types, including execution of existing integration primitives (tool calling) or execution of code generated on-demand. To decide on what action to take, the harmonization agent leverages an LLM, which is provided with descriptions of the task and integration primitives available for use. The LLM then returns an action (e.g., a code snippet) that can be executed in a runtime environment (e.g., a Python kernel). The action output is then fed back into the LLM, which decides if it needs to execute additional actions or if the task is completed. This loop executes until the task is deemed complete (see Figure 2b).\nThe main loop is orchestrated by a driver code that takes inputs from the user (i.e., prompts) that describe the task to be performed. This driver is also responsible for (1) communicating with users to request inputs (e.g., when the LLM asks for task clarification or user preferences) and (2) managing the state (memory) of the agent. For example, it can also track and store the history of actions and user interactions in a Provenance DB. This data can support decisions about future actions, and transparency, and be used to generate harmonization specifications or scripts to reproduce the results.\nGiven the complexity of harmonization tasks, it is crucial to have high-level primitives available as building blocks for the pipeline. This allows encoding prior knowledge and using efficient algorithms known to be effective for a specific task. These primitives encompass algorithms for tasks such as schema matching, entity resolution, and value mapping. Of course, primitives can go beyond hard-coded functions that implement deterministic algorithms. For instance, they can be workflows that use LLMs to perform specific tasks such as in [44] or they could generate code on demand (e.g., to extract data from or to transform attribute values [10]).\nHuman-Agent Interaction. A central component of our system architecture is the user-agent interaction. We argue that systems must go beyond text-based conversational interactions: they need to support rich visual data representations and should allow interactions that help users reason about the answers produced by the agent and refine the task definition as well as the pipelines derived by the agent. In complex tasks such as harmonization, this is necessary since many decisions needed to complete the task (e.g., whether two terms represent the same concept) are difficult even for domain experts and may require external knowledge, such as the context in which data was collected.\nIn this paper, we focus on interactions based on natural language via a chatbot interface. However, it should be possible to implement even more intuitive and efficient graphical interfaces that combine natural language with point-and-click components. For example, as done in interactive AutoML tools [54], the system may guide the user through the harmonization process, recommend the available actions, track progress, and provide data visualizations to help the user better make sense of the data. This may help prevent common issues in natural language such as ambiguity [19]."}, {"title": "SYSTEM PROTOTYPE & USE CASE", "content": "The primitives library. We used data harmonization primitives from bdi-kit [4], an open-source Python library that we designed with the explicit goal of composability. Currently, it includes implementations of multiple schema matching and value matching algorithms using a composable API. It includes several classic algorithms from Koutras et al. [35] and language model-based algorithms such as the ones presented in [45] and [44]. Most functions take as input a source parameter that represents the user's input DataFrame and returns the output formatted as another DataFrame. The target parameter can either be a string representing a target standard schema (e.g., 'gdc') or a target DataFrame, this allows switching between the two tasks described in Examples 1 and 2. Figure 3 shows a subset of functions relevant to this paper.\nThe data harmonization agent. We implemented Harmonia, a system prototype that uses the data integration primitives from Figure 3 and interacts with users via a text-based chat box. Implementing an agent entails writing carefully crafted function and task descriptions that are combined to assemble system prompts fed to the LLMs. For our prototype, we implemented tool wrappers for each of the bdi-kit functions, along with descriptions of when each should be used. We also provide general descriptions of the data harmonization steps, when the LLM should request help from the user, and output formatting instructions.\nTo implement the tool calling functionality and the user interface, we used the Archytas [2] library, an open-source tool for building AI agents based on the ReAct framework [73], and Beaker [5], a contextually-aware notebook system that supports the development of chatbot user interfaces. This allows us to implement features that facilitate interaction, such as rendering markdown-formatted outputs in the browser, displaying the reasoning steps taken by the agent, collecting user inputs, and feeding them back to the agent.\nHuman-agent interaction: The GDC use case. To demonstrate the ability to produce good data harmonization plans, we present a use case that harmonizes a dataset from Dou et al. [18] (described in Example 1) with the GDC standard. Below, we show message exchanges between the user and the agent, along with explanations of the actions taken by the agent to answer user queries. The user starts by requesting the system to load a CSV file that contains the attributes of interest:\nResponding to this prompt, the agent automatically (1) generates Python code using the Pandas library that loads and selects the requested columns and (2) executes the generated code to load the CSV file into memory. This example illustrates the capability of on-demand code generation to accomplish a requested task.\nNext, the user requests the system to perform schema matching between the table loaded and the attributes from the GDC standard:\nAs shown in Figure 2c, the system interacts with an LLM multiple times to answer this request. First, it sends the initial user's prompt to the LLM, which recognizes that the function match_schema is adequate to solve the request, and thus returns a tool calling instruction containing the match_schema function name along with the needed parameters. Note that the LLM parses the user's query, and recognizes that gdc must be used as the target schema and ct_learning refers to the argument method of match_schema(). After receiving the tool calling instruction as output, the system then executes the primitive function requested by the LLM, collects the list of column matches returned, and passes it back to the LLM in a follow-up prompt.\nAfter analyzing the column match results received in the follow-up prompt, the LLM detects a mapping mistake for the column Histologic_type, which is wrongly mapped to roots. To fix the error, the LLM triggers a tool calling instruction that uses the primitive function top_matches to find the top 10 alternative candidates for Histologic_type. After receiving the results, it analyzes the results and suggests using primary_diagnosis as a correction to the user (who agrees) as seen in the following messages:\nAs a final response to the task, the agent shows a table of column mappings as output, where the \"Source Column\" contains columns from the source table (dou.csv), \"Target Column\" contains a column match from the GDC standard, and \"Corrected?\" indicates whether the LLM has corrected the original bdi-kit match.\nSince the user is a domain expert who knows the GDC standard and knows that there may be better options for Tumor_Size_cm, it asks the system to find an alternative match. The system then updates the previous list with a more appropriate match found using the top_matches function. This interaction is shown next:\nAfter finding correct matches for all source columns, the next step is to find value mappings for each column match. The following prompt requests the agent to find such mappings:\nAs in the previous prompts, finding value mappings requires multiple LLM interactions. First, the LLM triggers one function call of match_values() for each of the source-target column pairs using the default value matching method tfidf (an efficient algorithm that chooses matches based on the tf-idf similarity over character n-grams). After receiving the value matches, the LLM evaluates if they are correct, and fixes the entries that contain errors. To do so, it calls another function to discover the list of acceptable values in a GDC attribute and picks the correct one. A snippet of the result is shown next (we omit part of the output due to lack of space):\nIn this example, the agent uses materialize_mapping to generate a table and save file 'dou_harmonized.csv'. The LLM first creates a declarative mapping specification according to the bdi-kit's format used to describe a harmonization plan. Figure 4 shows a snippet of the generated harmonization plan, which includes a declarative specification of the mappings to be performed written in JSON. In this example, we only show dictionary-based transformations that map source values into target values. However, bdi-kit's also supports other types of transformations, such as custom mappings that take as input a custom Python function (or lambda). This can potentially be used along with the on-demand code generation.\nThe main advantage of using a declarative language to describe harmonization plans is that it enables reproducibility: once a plan is created, users can feed the plan along with the source data into materialize_mapping function to recreate the harmonized data. This does not require re-running any LLM-based interactions, since all transformations are encoded in the harmonization plan."}, {"title": "RESEARCH OPPORTUNITIES", "content": "Harmonia shows the potential of LLM-based agents to orchestrate actions, evaluate function outputs, detect errors, and generate additional needed functions. However, there are still open research opportunities to expand the system's capabilities. Below we outline some of the immediate steps towards these opportunities.\nAgent Evaluation & Benchmarks. Most existing evaluation benchmarks are focused on isolated tasks, such as schema matching or entity linking [35, 44, 63]. However, agentic systems create a need for end-to-end evaluation benchmarks and metrics to measure progress effectively. Recently, researchers have started developing benchmarks for evaluating agents in various tasks, including data analysis and ML engineering [8, 25, 27, 75]. The data management community should follow this lead and create benchmarks tailored for data integration, ensuring that LLMs improve in these areas.\nData Integration Primitives. Some features, such as uncertainty quantification and explanations, should be exposed by the primitives to guide decision-making [16]. Uncertainty in data integration arises from factors like ambiguous schema mappings and data values [66]. Harmonia tackles this by exposing similarity scores through its primitives. For example, a value matcher can return similarity scores, so the agent can strategically trigger complementary primitives (e.g., value mapping) for deeper analysis. A key challenge is conveying the meaning of uncertainty measures from diverse primitive mechanisms to both LLMs and end users, and instructing LLMs on how to use and act on them properly [1].\nOn the same note, LLMs often lack transparency [28], so primitives must provide interpretable explanations to promote user trust. Primitives should offer clear usage documentation and expose their decision rationale. For instance, a matching algorithm description should document whether its similarity scores derive from syntactic similarity, semantic embeddings, or value distribution analysis. LLMs can also explain their decisions based on domain knowledge and primitive instructions, helping users better understand why a particular path was chosen [59, 76]. A key opportunity is training agents to discern when to rely on LLM explanations, apply alternative strategies, or engage with the user directly.\nMapping data between schemata involves resolving entities and transforming data [34]. LLM-based methods have proven helpful in entity resolution [20, 51], generating or finding transformations functions [10, 11, 15, 41, 52, 60], and evaluating LLM performance on such tasks [48]. However, integrating these methods into agent-based systems requires consistency across diverse data models and alignment with broader agent goals. Also, we need methods that allow agents to identify and recommend appropriate attribute transformations and suitable functions for a given input dataset. This is especially in challenging cases such as table restructuring or non-standard formats [41]. Another immediate step is to curate a library of transformation functions specialized in data transformation tasks that agentic systems can readily reuse.\nRobustness and Reliability. LLMs have shown inconsistency across various scenarios (e.g., as text summarization evaluators [58]), often producing varying results when executed multiple times [3]. This variability can undermine reproducibility and reliability, particularly in critical applications where consistent mappings or transformations are crucial. In our experiments, we observed that while the LLM typically identified and fixed incorrect mappings, it occasionally failed to do so (even when provided with the same prompts).\nHandling large and complex tables with many attributes poses additional challenges, as these can lead to long chat histories that exceed the LLM context window. When this occurs, the LLM may lose access to earlier relevant information, thereby affecting the robustness. While approaches have been proposed to mitigate context window limitations (e.g., [30] [47]), it is not clear if they address the issues in agent systems. Equipping agents with access to read and store data in external databases (such as the Provenance DB discussed in Section 3) may be an effective solution to this issue.\nUser-Agent Interaction. To improve usability, agentic systems must go beyond natural language (NL) interfaces. While NL is flexible, it is also often ambiguous and may lead to under-specified task descriptions [74]. Since the same task can be expressed in multiple unpredictable ways, a mismatch between the user task descriptions and agent prompt specifications may occur. Therefore, detecting when clarifications are needed may help increase overall success [74]. These issues could also be potentially addressed by action-oriented UIs that recommend actions linked to predefined prompts. Moreover, using rich visual representations may be more effective at conveying information to the user.\nProvenance-Aware Agents. Provenance collection systems have demonstrated promising results in data science pipelines [9, 53]. In data harmonization pipelines, we can track all interactions that contribute to obtaining a specific value mapping. For example, we could record all user-agent and agent-primitive interactions involved in determining the mapping of \"FIGO grade 1\" to \"G1\" (see Figure 4). This would allow tracing the lineage of all values in the output data. Moreover, this information could potentially be used to learn user preferences that reduce the need for user interactions. By learning from provenance and pipelines accumulated over time, a system could further streamline the harmonization process by automating all steps and presenting final results directly to users.\nData Harmonization Pipelines. In our system, data harmonization is expressed as a pipeline where multiple primitives (predefined, user-defined, or agent-defined) are interconnected through their inputs and outputs to produce the final harmonized dataset. The pipeline can have various objectives, such as maximining the number of correct column matches and value matches, minimizing the number of interactions with users, or minimizing the computational costs (e.g., runtime or LLM calls). Achieving these objectives represents an optimization problem, requiring the system to navigate a complex search space and balance multiple objectives to determine an optimal sequence of operations.\nRecent work has explored large search spaces for building end-to-end pipelines for various tasks [38, 43, 46, 71]. AutoML provides a relevant example where the focus is usually on model quality, and the system selects the best algorithms, features, and hyperparame-ters [46]. Pipeline generation for data harmonization can build on such advancements. An immediate step is building optimizers that measure and balance multiple objectives like harmonization success and computational costs. Also, our system integrates human-in-the-loop interactions to iteratively refine pipelines based on user feedback and domain expertise. An open challenge is determining how to effectively balance automation and human input [39]."}]}