{"title": "Improve LLM-based Automatic Essay Scoring with Linguistic Features", "authors": ["Zhaoyi Joey Hou", "Alejandro Ciuba", "Xiang Lorraine Li"], "abstract": "Automatic Essay Scoring (AES) assigns scores to student essays, reducing the grading\nworkload for instructors. Developing a scoring system capable of handling essays across\ndiverse prompts is challenging due to the flexibility and diverse nature of the writing task.\nExisting methods typically fall into two categories: supervised feature-based approaches\nand large language model (LLM)-based methods. Supervised feature-based approaches\noften achieve higher performance but require resource-intensive training. In contrast, LLM-\nbased methods are computationally efficient during inference but tend to suffer from lower\nperformance. This paper combines these approaches by incorporating linguistic features\ninto LLM-based scoring. Experimental results show that this hybrid method outperforms\nbaseline models for both in-domain and out-of-domain writing prompts\u00b9.", "sections": [{"title": "1. Introduction", "content": "Research in Automatic Essay Scoring (AES), the task of automatically assessing the quality\nof an essay, dates back to over five decades ago Page (1968). Since then, researchers in\nthis domain have taken various perspectives; some focus on building hand-crafted features\nChen and He (2013); Uto et al. (2020), some leverage the computational power of neural-\nnetwork to learn effective representation of essays Dong et al. (2017); Ridley et al. (2020a);\nJin et al. (2018), and some adapt pre-trained language models as the starting point for\nfine-tuning Wang et al. (2022); Xue et al. (2021).\nHowever, as much of the research above has shown, AES remains an open question.\nThe key challenge of this task lies in the balance between generalization and specification:\nideally, the method should be applicable to any grading scenario given a concrete grading\nrubric. Yet, numerous factors, including but not limited to instructors, education insti-\ntutions, the essay's purpose, and the type of the essay (from the literature point of view)\nmake essay grading context-specific. To this end, the cross-prompt AES system, which aims\nto work similarly well for different essay prompts and scoring rubrics, has been an impor-\ntant direction that draws much attention Ridley et al. (2020a); Jin et al. (2018); Li and Ng\n(2024); Phandi et al. (2015); Ridley et al. (2021a).\nCreating cross-prompt AES systems with sufficient capabilities requires the system to\nexamine more than just simple word surface forms and incorporate more linguistically mo-\ntivated features (e.g., Burstein et al. (1998)). Recent work (Ridley et al. (2020b); Uto et al."}, {"title": "2. Related Works", "content": "(2020)) pairs linguistic features with supervised methods to boost AES quality. How-\never, there is little work on exploring linguistic features in the context of instruction-based\nlarge language models (LLMs), with most work focusing on other features such as rubric-\nincorporation Hashemi et al. (2024) and prompting techniques (e.g., Liu et al. (2023) and\nChiang and Lee (2023). To bridge the gap, we explore adding linguistic features to the\nLLM prompt. We conduct experiments in the cross-prompt AES setting with both open-\n(Mistral) and closed-source (GPT-4) large language models and find that LLMs align better\nwith human judgments when given linguistic features.\nOur main contribution can be summarized as follows: 1) through prompt tuning and\nfeature engineering, we have shown incorporating linguistic features into existing zero-shot\nprompting methods can notably improve the overall score prediction; 2) even for out-of-\ndistribution data (i.e., essay from an entirely different dataset), the improvement holds; 3)\nthere is still notable headroom for open-source LLM to automatically evaluate student essay,\ncompared to their closed-source counterparts and smaller, supervised language models; we\nhypothesize that it is due to poorly-calibrated prior that is built-in to the LLM."}, {"title": "2.1. Automatic Essay Scoring", "content": "Feature Engineering approaches leverage various features to predict essay scores, in-\ncluding linguistic features, e.g., readability metrics and word length Ridley et al. (2020a);\nUto et al. (2020); Jin et al. (2018); Foltz et al. (1999); Chen and He (2013), and content fea-\ntures, e.g., content quality and organization Mathias and Bhattacharyya (2018); Crossley et al.\n(2023). Models that utilize these features range from simple logistic regression models\nChen and He (2013) to deep neural networks Uto et al. (2020). These approaches assess\nthe quality of essays in an interpretable manner with well-defined features."}, {"title": "2.2. LLM as Evaluator", "content": "Language-model-based approaches emerge with the rising popularity of Transformer\narchitecture, including BERT-based methods that require supervised fine-tuning Wang et al.\n(2022); Chen et al. (2024); Xue et al. (2021) and LLM-based methods that focus on prompt-\nengineering Mansour et al. (2024); Stahl et al. (2024). In particular, Stahl et al. (2024) ex-\nplores zero-shot prompting with persona prompts and analysis instructions. Building on\nthis, our work aims to utilize linguistic features in LLM prompting."}, {"title": "3. Methods", "content": "Given the increasing capability of LLMs and their scalable nature, researchers in vari-\nous domains have explored how to use them for the automatic evaluation of text content\nZubiaga et al. (2024); Alhafni et al. (2024); Gao et al. (2024); Fu et al. (2024). Although\nsome research has shown proper prompt tuning, such as explanation-guided generation, clear\nrubric guidance, and chain-of-thought (COT) could improve the alignment between human\nand LLMs Chiang and Lee (2023); Liu et al. (2023); Hashemi et al. (2024), the LLM-based\nevaluators still perform underwhelming in more complex tasks, such as reviewing papers\nZhou et al. (2024) and scoring students essay Mansour et al. (2024); Stahl et al. (2024).\nIn this work, we specifically focus on improving LLMs as student essay graders by incor-\nporating the linguistic features of essays. Additionally, we examine the transferability of\nthe prompts, i.e., how a prompt that is tuned in the in-distribution data would perform\nout-of-distribution in the same task."}, {"title": "3.1. Zero-shot Prompts with LLM", "content": "We build on top of the prompt template and instruction strategy by Stahl et al. (2024)\nfor our zero-shot prompt design. Each prompt follows the following structure: persona\npattern, essay prompt, analysis task, student's essay, additional information, and\nformat instruction. We adopt the best-performing combination of each component based\non Stahl et al. (2024), i.e., educational researcher as the prompt template, Explanation \u2192\nScoring as the analysis instruction. We also add the additional information section to\nincorporate linguistic features of the essay (see Sec. 3.2). Below is the prompt template we\nuse. More details about the prompt structure and examples can be found in Appendix C.\nYou are part of an educational research team analyzing the writing skills of students in grades\n7 to 10. You have been given a student's essay and the prompt they responded to.\n### Essay Prompt: { essay prompt }\n### Analysis Task: { analysis instruction }\n### Analyzed Student Essay: { essay }\n### Additional Information: Studies show that the following features are highly, positively\ncorrelated with the grade of the essay (i.e., higher features typically mean higher end score):\n{ linguistic features }\n### Analysis: Conclude your analysis with a grade and comments in the following format:\n{ format instruction }"}, {"title": "3.2. Linguistic Features", "content": "In addition to the naive zero-shot baseline, we experimented with incorporating linguis-\ntic features into the prompts. We base our model's linguistic features off of Ridley et al.\n(2020b), which were additionally used by Li and Ng (2024) and cited as being some of the\nmost impactful features in essay grading, having a Pearson's correlation score of 0.6 or above\nwith the essay's score. While the original works contain much more linguistic features, limit-\ning our linguistic features helped avoid overly long prompts, which could negatively impact\nLLM performance. The linguistic features we used are listed below.\nUnique Words refers to the number of single-instance words in the essay. For essay\ncharacter length, we only count the number of non-space, non-punctuation characters2.\nWord/Sentence counts are the total number of words/sentences in a given essay. We\nget the individual counts for the total number of lemmas, nouns, and stop-\nwords. Finally, we get the essay's Dale-Chall (Dale and Chall (1948)) word count, total\ncharacter count (all characters) and long word count.\nDuring prompt construction, the linguistic features are formatted as an unordered list\nof a short feature description followed by feature value. The formatted text containing all\nlinguistic features is inserted into the prompt as the additional information section."}, {"title": "3.3. Output Parsing via LLM", "content": "The output format varies across essay sets since each essay set has its own scoring schema.\nTo make the pipeline generic to any input, we implement a few-shot parsing module powered\nby a stand-alone LLM. More details can be found in Appendix E."}, {"title": "4. Experiments", "content": "We conducted our experiments on two widely used essay-grading datasets:"}, {"title": "4.1. Datasets & Linguistic Features", "content": "ASAP ASAP (Hamner et al. (2012)) is one of the most widely used evaluation datasets\nfor AES, with 12,980 essays written by students in grades 7 through 10. It is divided into 8\nsubsets based on the formulation of essay prompts, including argumentative (1, 2), source-\ndependent (3, 4, 5, 6), and narrative (7, 8) essay prompts. The scores are annotated by at\nleast two graders to ensure validity. We split the ASAP dataset into 5 equally sized folds.\nWe use 3 folds for training (BERT baseline fine-tuning), 1 fold for validation (BERT baseline\nmodel selection and prompt tuning for prompting method), and 1 fold for evaluation.\nELLIPSE English Language Learners Insight, Proficiency and Skills Evaluation (EL-\nLIPSE) by Crossley (2024) is composed of 6483 essays from English language learners in\nthe United States educational system's grades 8-12 get from an unnamed standardized test.\nThe dataset features the full essay of each student and demographic information such as\nrace/ethnicity and income background. There are 29 different argumentative essay prompts\ncovering topics such as cell phones in school and community service. Each essay includes\nan overall holistic score as well as six rubric-based scores, all scored from one to five (in\nintervals of 0.5) by one of two annotators (see Appendix A for the full rubric). Note that we\ndo not split the data since we want to treat this dataset as an entirely out-of-distribution\ndataset to examine the generalizability of our method across different essay prompts and\ncontents.\nLinguistic Features While the ASAP dataset had these features pre-calculated thanks\nto the aforementioned prior work, the ELLIPSE dataset needed a custom pipeline to re-\nimplement these features. We followed Ridley et al. (2021b) as exactly as possible to recre-\nate the features from Section 3.2 as closely as possible. See Appendix D for the implemen-\ntation details."}, {"title": "4.2. Evaluation Metrics", "content": "For both ASAP and ELLIPSE, the most important task is to predict the overall scoring. We\nfollow the Kaggle competition evaluation metric of Hamner et al. (2012) and use Quadratic\nWeighted Kappa (QWK) to measure the agreement between the predicted and annotated\noverall scores."}, {"title": "4.3. Models and Experiment Setup", "content": "Our Pipeline As mentioned in Sec. 3, our main method consists of a prompt construction\nmodule and a zero-shot prompting module with an LLM followed by a parsing module pow-"}, {"title": "5. Result and Discussion", "content": "ered by another LLM. For prompt construction, we include three setups to integrate linguis-\ntic features: the most correlated feature (i.e., unique word count), the top 3 correlated fea-\ntures (i.e., unique word count, lemma count, and complex word count), and all 10 features,\ndescribed in Sec. 4.1. For prompting and parsing, we use Mistral-7B-Instruct-v0.2\n(Jiang et al. (2023)) for all LLM-related jobs, and we use 0 temperature and 4096 max\ntoken length, following Stahl et al. (2024)3. We use default sampling parameters in vllm\nframework during decoding\u2074. All experiments are conducted with the VLLM\u2075 (Kwon et al.\n(2023)) on a single-card NVIDIA L40S device.\nUnsupervised Baseline (GPT-4) We also include one of the most capable models\navailable to us, GPT-4 (gpt-4-0613), as the strong, unsupervised baseline. The experiment\nsetup is exactly the same as in our pipeline above, with two differences, both due to the\nlimited budget: 1) we only experiment with no more than 500 randomly sampled essays\nper essay set in ASAP (some essay sets have less than 500 essays in the test set) and 500\nrandomly sampled essays in ELLIPSE; 2) we only experiment with no linguistic feature\nand the best-performing linguistic feature setup (i.e., all top 10 features) based on the\nperformance on the dev set of ASAP.\nSupervised Baseline (BERT) We also experimented a supervised method on ASAP\nto establish a performance upper bound. Our supervised baseline is a BERT-based ar-\nchitecture utilizing three main feature classes: document-, token- and segment-scale fea-\ntures Wang et al. (2022). We base our fine-tuning on the authors' available code. The\nauthors only released the fine-tuned model for ASAP prompt 8, so to obtain models for\nall prompts\u2014we fine-tuned bert-base-uncased as they did in the original paper. We used\nour splits of the ASAP dataset for fine-tuning, validation and testing (see 4.1). More details\nabout fine-tuning is in Appendix B."}, {"title": "5.1. Cross-dataset Performances", "content": "The experimental results are summarized in Table 1. For ASAP, the BERT-based super-\nvised baseline achieved the highest average QWK, aligning with our expectation that AES\nremains a challenging task for naive LLMs prompting. When comparing cross LLMs, the\nperformance between GPT-4 and Mistral showed mixed results across the two datasets. We\nsuspect this is due to the differences in the essay type. ELLIPSE essays are argumentative,\nsimilar to essay sets 1 and 2 in ASAP. However, the grading rubric for ASAP essay set 2 is\nmore specific (focusing on \"Writing Application\" and \"Language Conventions\") compared\nto the broader \"Overall\" quality criteria used for ELLIPSE and ASAP essay set 1. These\nobservations suggest that LLM performance on ELLIPSE is more comparable to ASAP es-\nsay set 1 rather than the entire ASAP dataset, a pattern confirmed by the results in Table 1.\nAnother research Chen et al. (2024) that conducted a zero-shot prompting experiment on\nELLIPSE with ChatGPT 6 also got similar results (QWK = 0.29)."}, {"title": "5.2. Benefit of Linguistic Features", "content": "As shown in Table 1, the prompts with linguistic features almost always perform better\nthan the ones without - with the exception of GPT-4. This trend holds even for out-of-\ndistribution data (ELLIPSE). When it comes to Mistral 7B, for ASAP, Top-10 features are\nthe most effective by both average and subset QWK measurement; for ELLIPSE, the best\nperforming linguistic feature choice is only Top-3 feature, while the difference is marginal.\nIn addition, the improvement brought by the linguistic features even pushes the Mistral\nperformance on ASAP close to GPT-4 performance. Based on these observations, we can\nconclude that including linguistic features can benefit LLM-based zero-shot AES."}, {"title": "6. Conclusion", "content": "In this work, we explore the combination of linguistic features and zero-shot prompting\nwith SOTA LLMs in the task of automatic essay scoring. Empirical experiments show\nperformance improvement when linguistic features are integrated into the zero-shot prompt.\nHowever, the performance improvement varies depending on the type of the essay, proving\nthe challenging nature of generalizability in AES systems. We hope our work can serve as\na starting point for the research in more interpretable and more generalizable LLM-based\nAES methods."}, {"title": "7. Limitations", "content": "During our experiments, we noted several limitations that future work could expand upon\nand resolve. Firstly, there is only one open-source and one close-source one. Future work\ncould look at including more LLMs for comparison. Secondly, the prediction target, holistic\nscore, is still hard to interpret, whereas some subsets (7 and 8) of ASAP and the entire set\nof ELLIPSE do have fine-grained essay score annotations (see examples in Appendix A).\nIncorporating them into the overall score prediction process would make the overall score\nmore transparent. Thirdly, the persona section of the prompt template mentions \"grade 7\nto 10,\" which is the age range for students in ASAP; however, the students in the ELLIPSE\ndataset are from grades 8 to 12, which might lead to performance differences among those\ntwo datasets. Lastly, our datasets have a clear western bias, especially ELLIPSE, which\nfocuses on ESL students in the United States. We believe the community would benefit\nfrom more diverse and inclusive datasets."}, {"title": "Appendix B. Supervised Baseline Details", "content": "Our supervised baseline Wang et al. (2022)7is a BERT-based architecture comprised of two\nsub-components\u2014each pretrained BERT models (Devlin et al.)\u2014which analyze three main\nfeature classes: document-, token- and segment-scale features. The first sub-component\nreceives the document- and token-scale features. It is fine-tuned to learn the document-\nscale feature representation through the [CLS] (start) token and the token-scale features\nthrough the BERT word embeddings. Its output goes through a final max pooling layer to\nrepresent the sub-component's score. The segment-scale features are received by the second\nsub-component, which takes in an essay as a series of segments each of size k (except the\nlast segment, which is smaller). A list of these segment series of varying sizes ki are input\ninto the model sequentially, and a final LSTM and attention and dense pooling layer is used\nto output the sub-component's score. Lastly, the output from the two sub-components are\nadded together to produce the final holistic score. The model's loss function is additive be-\ntween mean squared error (MSE), cosine similarity (CS) and margin ranking loss (MLR):\n$L_{Total}(x, y) = L_{MSE}(x, y) + \\beta L_{CS}(x, y) + \\gamma L_{MLR}(x, y)$.\nWe base our fine-tuning on the authors' available code. The authors only released\ntheir fine-tuned model for ASAP prompt 8, so to obtain models for all prompts\u2014we fine-\ntuned bert-base-uncased as they did in the original paper. We used our splits of the\nASAP dataset for fine-tuning, validation and testing (see 4.1). We fine-tune for 80 epochs,\nour hyperparameters for \u03b1, \u03b2 and \u03b3 were all set to 0.5 and with cosine similarity dim=1\nand margin ranking loss margin=0. Everything is implemented in PyTorch (Paszke et al.\n(2019)) and HuggingFace (Wolf et al. (2019)) using google-bert/bert-base-uncased. We\nrun the test set on the prompt's model with the best loss."}, {"title": "Appendix C. Zero-shot Essay Scoring Prompts", "content": "Here are some examples of zero-shot essay scoring prompts. Note that the exact phrasing\nand wording are not exactly the same as Stahl et al. (2024) paper. That is because we have\nfailed to reproduce the exact same results in their paper, motivating us to conduct a limited\nprompt tuning in the dev set of ASAP. To reduce complexity, the tuning is done only in\nphrasing and formatting, without changing the overall structure of the prompt compared\nto the original design."}, {"title": "C.0.1. No LINGUISTIC FEATURE", "content": "You are part of an educational research team analyzing the writing skills of students in\ngrades 7 to 10. You have been given a student's essay and the prompt they responded to.\n### Essay Prompt: More and more people use computers, but not everyone agrees that\nthis benefits society. Those who support advances in technology believe that computers have\na positive effect on people. They teach hand-eye coordination, give people the ability to learn\nabout faraway places and people, and even allow people to talk online with other people. Oth-\ners have different ideas. Some experts are concerned that people are spending too much time\non their computers and less time exercising, enjoying nature, and interacting with family\nand friends. Write a letter to your local newspaper in which you state your opinion on the\neffects computers have on people. Persuade the readers to agree with you.\n### Analysis Task: Grade the given essay with the following requirements:\nUse those score ranges: Overall: from 1 to 6.\nProvide an explanation for your score as well.\n### Analyzed Student Essay: Dear, @CAPS1 @CAPS2 @CAPS3 More and more people\nuse computers, but not everyone agrees that this benefits society. Those who support ad-\nvances in technology believe that computers have a positive effect on people. Others have\ndifferent ideas. A great amount in the world today are using computers, some for work and\nspme for the fun of it. Computers is one of mans greatest accomplishments. Computers\nare helpful in so many ways, @CAPS4, news, and live streams. Don't get me wrong way to\nmuch people spend time on the computer and they should be out interacting with others but\nwho are we to tell them what to do. When I grow up I want to be a author or a journalist\nand I know for a fact that both of those jobs involve lots of time on time on the computer,\none @MONTH1 spend more time then the other but you know exactly what @CAPS5 get-\nting at. So what if some expert think people are spending to much time on the computer\nand not exercising, enjoying natures and interacting with family and friends. For all the\nexpert knows that its how must people make a living and we don't know why people choose\nto use the computer for a great amount of time and to be honest it's non of my concern\nand it shouldn't be the so called experts concern. People interact a thousand times a day on\nthe computers. Computers keep lots of kids of the streets instead of being out and causing\ntrouble. Computers helps the @ORGANIZATION1 locate most wanted criminals. As you\ncan see computers are more useful to society then you think, computers benefit society.\n### Analysis: Conclude your analysis with a grade and comments in the following format:\n### Explanation:\n### Score:\n- Overall:"}, {"title": "C.0.2. TOP-10 FEATURES", "content": "You are part of an educational research team analyzing the writing skills of students in\ngrades 7 to 10. You have been given a student's essay and the prompt they responded to.\n### Essay Prompt: More and more people use computers, but not everyone agrees that\nthis benefits society. Those who support advances in technology believe that computers have\na positive effect on people. They teach hand-eye coordination, give people the ability to learn\nabout faraway places and people, and even allow people to talk online with other people. Oth-\ners have different ideas. Some experts are concerned that people are spending too much time\non their computers and less time exercising, enjoying nature, and interacting with family\nand friends. Write a letter to your local newspaper in which you state your opinion on the\neffects computers have on people. Persuade the readers to agree with you.\n### Analysis Task: Grade the given essay with the following requirements:\nUse those score ranges: Overall: from 1 to 6.\nProvide an explanation for your score as well.\n### Analyzed Student Essay: Dear, @CAPS1 @CAPS2 @CAPS3 More and more people\nuse computers, but not everyone agrees that this benefits society. Those who support ad-\nvances in technology believe that computers have a positive effect on people. Others have\ndifferent ideas. A great amount in the world today are using computers, some for work and\nspme for the fun of it. Computers is one of mans greatest accomplishments. Computers\nare helpful in so many ways, @CAPS4, news, and live streams. Don't get me wrong way to\nmuch people spend time on the computer and they should be out interacting with others but\nwho are we to tell them what to do. When I grow up I want to be a author or a journalist\nand I know for a fact that both of those jobs involve lots of time on time on the computer,\none @MONTH1 spend more time then the other but you know exactly what @CAPS5 get-\nting at. So what if some expert think people are spending to much time on the computer\nand not exercising, enjoying natures and interacting with family and friends. For all the\nexpert knows that its how must people make a living and we don't know why people choose\nto use the computer for a great amount of time and to be honest it's non of my concern\nand it shouldn't be the so called experts concern. People interact a thousand times a day on\nthe computers. Computers keep lots of kids of the streets instead of being out and causing\ntrouble. Computers helps the @ORGANIZATION1 locate most wanted criminals. As you\ncan see computers are more useful to society then you think, computers benefit society.\n### Additional Information: Studies show that the following features are highly, positively\ncorrelated with the grade of the essay (i.e., higher features typically means higher end score)\ntotal number of unique words in the essay: 113\ntotal number of words in the essay.: 279\ntotal number of sentences present: 14\ntotal number of characters: 279\ntotal number of lemma: 133\ntotal number of nouns: 50\ntotal number of stopwords: 71\ntotal number of words that are not in the Dale-Chall word list of 3000 words recognized by\n80% of fifth graders: 80\ntotal number of characters: 1229"}, {"title": "Appendix D. Linguistic Features", "content": "### Analysis: Conclude your analysis with a grade and comments in the following format:\n### Explanation:\n### Score:\n- Overall:\nUnique words refers to the number of single-instance words in the essay. For essay\ncharacter length, we only count the number of non-space, non-punctuation characters.\nWords are not normalized before these metrics as in the original paper. Total word\ncount and total sentence count per essay are gotten via nltk (Loper and Bird (2002))\ntokenizers. We additionally utilize the en_core_web_sm in spacy (Honnibal et al. (2020))\nto get separate counts for lemma, noun, and stop-words. Finally, we get the Dale-\nChall (Dale and Chall (1948)) word count, total character count and long word\ncount with the readability\u00b9\u2070 Python package.\nOur implementation is based on the code \u00b9\u00b9 from the original paper Ridley et al. (2021b).\nOur implementation will be made available upon acceptance."}, {"title": "Appendix E. Parsing Module", "content": "Appendix E. Parsing Module"}, {"title": "E.1. Configurations", "content": "\u2022 Model: Mistral-7B (the same configuration as the scoring model)\n\u2022 Overall parsing error is less than 7%."}, {"title": "E.2. Few-shot Output Parsing", "content": "You are an AI agent that specialized in converting text input into JSON format.\nInstruction:\nInput: text with one or more score and some other relevant information (e.g., explanation,\nfeedbacks, etc.)\nOutput: JSON text with \u2018Score' as a mandatory key and other information organized by\ntheir field names\nMake sure ONLY return the VALID JSON data, without any additional text or characters.\nHere are some examples\nExample Input:\n### Explanation: The student's essay demonstrates a limited understanding of the topic\nand a lack of cohesion. The essay jumps from one idea to another without a clear connection\nbetween them. The writing is also filled with numerous grammatical errors, misspellings,\nand inconsistent capitalization.\n### Score:\nOverall: 1 The essay demonstrates a very limited understanding of the topic and contains\nnumerous errors in grammar, spelling, and capitalization. The writing lacks cohesion and a\nclear thesis statement, and the arguments are not well-supported with evidence or examples.\nExample Output:\n{ \"Score\": { \u201cOverall\u201d: 1 }, \u201cExplanation\u201d: \u201cThe student's essay demonstrates a limited\nunderstanding of the topic and a lack of cohesion. The essay jumps from one idea to an-\nother without a clear connection between them. The writing is also filled with numerous\ngrammatical errors, misspellings, and inconsistent capitalization.\u201d }\nExample Input:\n### Explanation: The student's essay demonstrates a basic understanding of the topic\nand presents a clear position, but the writing is disorganized and contains numerous errors\nin language conventions. The essay jumps between discussing censorship in libraries and\nspecific examples of offensive music, making it difficult to follow the main argument.\n### Score:\nWriting Applications: 2 The essay presents a viewpoint on the issue of censorship, but\nthe argument is not well-developed or clearly stated. The student uses personal experiences\nand examples. - Language Conventions: 1 The essay contains numerous errors in language\nconventions, including incorrect capitalization, punctuation, and sentence structure.\nExample Output:\n{ \u201cScore\u201d: { \u201cWriting Applications\u201d: 2, \u201cLanguage Conventions\u201d: 1 } \u201cExplanation\u201d: \u201cThe\nstudent's essay demonstrates a basic understanding of the topic and presents a clear posi-\ntion, but the writing is disorganized and contains numerous errors in language conventions.\nThe essay jumps between discussing censorship in libraries and specific examples of offen-\nsive music, making it difficult to follow the main argument.\" }\nExample Input:\n### Explanation: The student's essay demonstrates a moderate level of awareness of the\naudience, as they address the readers directly and use a conversational tone.\n### Feedbacks: the essay could have been more effective if the student had used more for-\nmal language and addressed specific concerns of the local community regarding the overuse\nof computers.\n### Score:\nOverall: 3 The student's essay shows some awareness of the audience, but there is room\nfor improvement in terms of language and organization. The essay could benefit from more\nspecific examples and a clearer, more focused argument.\nExample Output: { \"Score\": { \u201cOverall\u201d: 3}, \u201cExplanation\u201d: \u201cThe student's essay demon-\nstrates a moderate level of awareness of the audience, as they address the readers directly\nand use a conversational tone.\u201d, \u201cFeedbacks\": \"the essay could have been more effective\nif the student had used more formal language and addressed specific concerns of the local\ncommunity regarding the overuse of computers.\u201d }\nNow work on the following input:"}]}