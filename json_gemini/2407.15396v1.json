{"title": "Semantic Diversity-aware Prototype-based Learning for Unbiased Scene Graph Generation", "authors": ["Jaehyeong Jeon", "Kibum Kim", "Kanghoon Yoon", "Chanyoung Park"], "abstract": "The scene graph generation (SGG) task involves detecting objects within an image and predicting predicates that represent the relationships between the objects. However, in SGG benchmark datasets, each subject-object pair is annotated with a single predicate even though a single predicate may exhibit diverse semantics (i.e., semantic diversity), existing SGG models are trained to predict the one and only predicate for each pair. This in turn results in the SGG models to overlook the semantic diversity that may exist in a predicate, thus leading to biased predictions. In this paper, we propose a novel model-agnostic Semantic Diversity-aware Prototype-based Learning (DPL) framework that enables unbiased predictions based on the understanding of the semantic diversity of predicates. Specifically, DPL learns the regions in the semantic space covered by each predicate to distinguish among the various different semantics that a single predicate can represent. Extensive experiments demonstrate that our proposed model-agnostic DPL framework brings significant performance improvement on existing SGG models, and also effectively understands the semantic diversity of predicates.", "sections": [{"title": "1 Introduction", "content": "Scene Graph Generation (SGG) aims to predict relationships between objects within an image and generate a structured graph, in which the nodes and edges denote objects and pair-wise relationships between two objects, respectively. Since this graph contains high-level information of the an image, it is widely used in various downstream tasks such as image captioning [1,28], visual question answering [7,33], and image retrieval [10].\nIn this study, we focus on an inherent limitation of existing benchmark datasets for the SGG task (e.g., Visual Genome (VG) [14]). That is, each subject-object pair in an image is annotated with only a single predicate, even though a single predicate may exhibit diverse semantics, which we refer to as semantic"}, {"title": "2 Related work", "content": "The objective of SGG is to construct a graph that is useful for scene understanding by identifying the relationships between objects within an image. Early methods [5,20,39] have overlooked the visual context and treated objects and pairwise relationships independently. Subsequent SGG approaches propose models that utilize rich contextual information by employing sequential LSTMs [27,37], message passing [17, 30, 32, 34], or tree structure modelling [27]. Furthermore, [8, 24, 36] have attempt to overcome the limitations of SGG datasets by leveraging external knowledge. Recently, PE-Net [40] attempted to obtain compact representations of entities and predicates using prototypes. However, these methods still overlook the semantic diversity of predicates and struggle with biased prediction problems."}, {"title": "2.2 Unbiased Scene Graph Generation", "content": "To address the biased prediction problem, numerous unbiased SGG works have been conducted. Similarly to [16], existing unbiased SGG methods can be broadly categorized into two groups: 1) Re-balancing based method: This involves increasing the number of tail classes or giving them more weight using techniques such as data augmentation [15], re-sampling [17], or re-weighting [31,35]. Moreover, recent research has explored re-labeling [16,38], which involves transforming head classes into tail classes. These approaches can also be considered as re-balancing methods, as they involve adjusting the number of head and tail classes. 2) Unbiased prediction based on biased training: These models perform biased training but make unbiased predictions during inference. TDE [26] introduces a causal inference framework to eliminate the context bias during the inference process. DLFE [3] utilizes positive-unlabeled learning to recover unbiased probabilities. Our method belongs to the second category as we train the model to understand semantic diversity during biased training and apply this understanding during the inference phase."}, {"title": "3 Method", "content": "Our goal is to learn the regions related to each predicate in the semantic space, enabling us to recognize the diverse semantics that a single predicate can express. To this end, we initially introduce prototypes to signify the representative semantics of predicates in the semantic space and train our model to minimize the distance between the relation feature and its ground truth class prototype (Section 3.2). At the same time, we utilize a sampling approach to capture the diverse semantics represented by each predicate, contributing to our understanding of semantic diversity (Section 3.3). Lastly, we make use of the semantic diversity information captured by our model to address the inevitable bias caused by the long-tailed predicate class distribution during the inference phase, leading to unbiased prediction (Section 3.4)."}, {"title": "3.1 Preliminary", "content": "Given an image $I$, the objective of SGG is to generate a scene graph $G = {O, E}$, where $O$ and $E$ denote the set of objects and thier pairwise relationships, respectively. The conventional SGG is generally conducted based on the following pipeline.\nProposal Generation. All objects $O = {o_i}_{i=1}^{N_o}$ are identified in the image $I$ using a pre-trained object detector (e.g., Faster R-CNN [23]), where $N_o$ is the number of objects. Each object $o_i \\in O$ consists of a visual feature $v_i$, an object bounding box $b_i$, and an initial object label $l_i$.\nObject Class Prediction. Object features $x_i$ are extracted based on the output of a proposal (i.e., $v_i$, $b_i$ and $w_i$), and object classification is conducted.\n$x_i = f_{obj}(v_i, b_i, w_i), \\ \\hat{l}_i = \\phi_{obj}(x_i),$                                                                                                                                                   (1)\nwhere $w_i$ is the word embedding [22] of $l_i$, $f_{obj}$ is an object encoder such as BiLSTM [37] or fully connected layers [39], and $\\phi_{obj}$ is an object classifier. An updated object label $\\hat{l}_i$ is obtained in this procedure.\nPredicate Class Prediction. This step involves obtaining a refined object feature $\\hat{x}_i$ using the word embedding $\\hat{w}_i$ of $l_i$. Then, given objects $o_i$ and $o_j$, and their features $\\hat{x}_i$ and $x_j$ along with their union feature $u_{ij}$, we obtain its relation feature $r_{i\u2192j}$ that for predicate classification.\n$\\hat{x}_i = f_{rel}(v_i, x_i, w_i), \\ \\ r_{i\u2192j} = f_p([x_i, x_j]) \\cup u_{ij}, \\ \\ p_{i\u2192j} = \\phi_{rel}(r_{i\u2192j}),$                                                                                                                                (2)\nwhere $\\odot$ denotes element-wise product, $f_{rel}$ is a relation encoder similar to $f_{obj}$, $f_p$ is fully-connected layer and $\\phi_{rel}$ is a predicate classifier. Moreover, $p_{i\u2192j}$ is the predicted predicate between objects $o_i$ and $o_j$, which is selected from a predefined set of predicates $P$."}, {"title": "3.2 Prototype-based Biased Training", "content": "To handle representative semantics corresponding to each predicate, we first create prototypes $C = {c_1, c_2, ..., c_{|P|}}$ , where $c_i \\in R^d$ is the learnable prototype for the $i$-th predicate. Furthermore, we use a trainable projector $\\Phi_{proj}(\\cdot)$ to align the relation feature $r$ with the same dimensional space as the prototype, i.e., $z = \\Phi_{proj}(r)$, where $z \\in R^d$. The prototypes are $L_2$ normalized to ensure that they reside within a consistent range, i.e., $||c_i||_2 = 1$. In this section, we aim to train each relation feature $z$ to become closer to the prototype of its ground-truth class. Specifically, we compute the probability of the relation represented by $z$ belonging to the $i$-th predicate class based on the Euclidean distance [4]:\n$p(i-th \\ class \\ | \\ z) = Softmax(-\\alpha \\ ||z \u2212 c_i ||_2 + b),$                                                                                                                                (3)"}, {"title": "3.3 Semantic Diversity Learning", "content": "Recall that a single predicate may exhibit diverse semantics of subject-object relationships. Therefore, it is necessary to understand the range that each predicate can represent and identify which parts within that range correspond to which"}, {"title": "Semantic Diversity-aware Prototype-based Learning", "content": "semantics. For this purpose, we generate samples from each prototype to capture this phenomenon. Specifically, we generate $N$ samples $s_i = {s_i^{(1)}, s_i^{(2)},..., s_i^{(N)}}$ around a prototype $c_i$ based on a normal distribution with the prototype as the mean and a diagonal covariance matrix induced by the prototype as follows:\n$P(S_i \\ | \\ c_i) \\sim N(\\mu_i, \\sigma_i^2),$                                                                                                                                          (5)\nwhere $\\mu_i = c_i$, $\\sigma_i^2 = f_{\\sigma}(c_i)$, and $f_{\\sigma}$ is a fully connected network.\nSample Matching Loss. To ensure that the samples fully cover the regions represented by each predicate, we introduce a matching function that connects each sample with its corresponding relation feature. Specifically, the matching loss is defined as:\n$L_{match} =  (max(0, min \\ ||z - s_k^{(j)}||_2^2 - R))^2,$                                                                                                                                (6)\nwhere $R$ is a hyperparameter, and $k$ is the index of the ground-truth predicate class of the relation feature $z$. This enforces at least one of the samples of a given predicate to reside within the distance $R$ from the relation feature $z$ that is labeled with the given predicate.\nWe would like to emphasize the importance of the hyperparameters $N$ and $R$. As shown in Fig. 2, most of the relation features are located around the prototype on. From the perspective of the prototype on, relation features labeled as on are distributed all around. Therefore, it requires enough samples to learn regions in all directions (i.e., large $N$). However, from the perspective of the prototype growing on, the relation features labeled with growing on are not spread out but rather located around a specific region. Therefore, we can expect that capturing the region of growing on would be sufficient with relatively fewer samples (i.e., small $N$). Moreover, if $R$ is either too small or too large, the model may fail to learn the generalized regions that each predicate can represent. Detailed analysis demonstrating our intuition is shown in Section 4.3.\nBy leveraging the relationship between relation features and samples, we can differentiate between various semantics represented by the same predicate. For instance, if a relation feature near the prototype on closely aligns with the sample generated from attached to, then that feature would likely exhibit the semantic of attached to. Similarly, if a sample generated from growing on closely aligns with a particular relation feature, then that feature may be associated with the semantic of growing on. Therefore, through our sample-relation feature matching process, we not only learn the range that can be represented by each predicate, but also estimate the specific semantics associated with relation features.\nOrthogonal Loss. Note that the distribution around each prototype shows a symmetric shape, as we have assumed it follows a normal distribution. As a result, if the variance increases in one direction, it can cause an unexpected"}, {"title": "1 |P|( |P|\u22121)", "content": "overlap with other sample distributions as it also increases in the opposite direction. In Fig. 3, the variance of on extends towards parked on because on can express the semantic of parked on. However, the variance also increases in the opposite direction due to the symmetry of the normal distribution. This leads to an overlap with under, which cannot be used together with on. Therefore, to prevent such unexpected overlaps, an orthogonal loss is employed to make the prototypes independent from each other.\n$L_{ortho} = \\frac{1}{ |P|(|P|-1) } \\sum_{i\\neq j}  c_i \\cdot c_j |,$                                                                                                                                              (7)\nThe final training loss is defined as follows:\n$L = L_{ce} + L_{ortho} + \\alpha L_{match},$                                                                                                                                                              (8)\nwhere \u03b1 is hyperparameter."}, {"title": "3.4 Unbiased Inference using Normalization", "content": "During training, the prediction of predicate classes relies on the Euclidean distance between the relation feature $z$ and its corresponding prototype $c_i$. However, relying solely on Euclidean distance leads to biased predictions and overlooks semantic diversity of predicates. Therefore, during inference, we compute the normalized distance using the semantic diversity information encapsulated within\n$\\sigma = [\\sigma_1^{(1)}, \\sigma_1^{(2)}, ..., \\sigma_1^{(d)} ]$ as follows:\n$p(i-th \\ class \\ | \\ z) = Softmax(-\\alpha' \\ ||(z - c_j) \\odot  \\sigma_i^{-1} ||_2 + b),$                                                                                                                                    (9)\nwhere $\\sigma_i^{-1} =  [\\frac{1}{\\sigma_i^{(1)}}, \\frac{1}{\\sigma_i^{(2)}}, ..., \\frac{1}{\\sigma_i^{(d)}} ]$ and $\\alpha' = \\alpha \\frac{max_j || z-c_j ||_2}{max_j || (z-c_j) \\odot  \\sigma_i^{-1} ||_2}$ mitigates the scaling effect caused by $\\sigma_i$."}, {"title": "4 Experiment", "content": "Experiments are conducted on two datasets: VG [14] and GQA [9].\nVG contains 108k images across 75k object categories and 37k predicate categories. Following previous studies on SGG [2, 3, 17, 19, 25], we adopt the most widely-used VG150 split [30], which contains the most frequent 150 object categories and 50 predicate categories. GQA is another vision-language dataset that contains over 3.8 million relation annotations. In the case of GQA, we follow the GQA200 split [6], which select the 200 most frequent object categories and 100 most frequent predicate categories. For both datasets, we allocate 70% of the data to the training set and 30% to the testing set, and sample 5K images from the training set for validation.\nTasks. Following prior studies [30,37], we evaluate models on three conventional SGG tasks: 1) Predicate Classification (PredCls): Predicting predicate class based on ground-truth object bounding boxes and their object classes. This task is not affected by the performance of the object detector. 2) Scene Graph Classification (SGCls): Predicting both predicate classes and object classes, given the ground-truth bounding boxes. 3) Scene Graph Generation (SGGen): Detecting object bounding boxes and predicting both the object classes and pairwise predicate classes. The bounding box is considered valid one if the IoU is over 0.5.\nMetrics. For evaluation, we use Recall@K (R@K) and mean Recall@K (mR@K). Furthermore, following prior studies [12,13,21,38], we adopt the harmonic average of R@K and mR@K, denoted as F@K. Generally, mR@K tends to have lower values in comparison to R@K. This indicates that a simple average between them would be heavily influenced by R@K. Instead, F@K provides a relatively fair comparison between R@K and mR@K as it gives more weight to smaller values.\nImplementation Details. Following prior studies [26], we employ a pre-trained Faster R-CNN [23] with ResNeXt-101-FPN [29]. For the VG dataset, the pre-trained object detector provided by [26] is employed. For the GQA dataset, due to the absence of an officially available object detector, we pretrain a new object detector. Therefore, we cannot directly compare our performance with other studies [6,24] on the GQA dataset. The performance of the object detector used on the GQA dataset is 25.33 mAP. The initial learning rate is 0.01 and we use the SGD optimizer. The batch size is set to 3 on 1 GPU. For each of the three tasks, the training phase consists of 60,000 steps in total. Most of the hyperparameters align with those used in prior studies. The hyperparameter \u03b1 is set to 10, and the best performance is achieved when N and R are set to 20 and 1.0, respectively. The dimension size of the relation feature and prototype is set to 128 (i.e., d=128)."}, {"title": "4.2 Comparison with State-of-the-Arts", "content": "Baselines. To demonstrate the effectiveness of our model-agnostic approach, we applied our method to three baseline SGG models: VTransE [39], Motifs [37], and VCTree [27]. We compared our method with the state-of-the-art models, including IMP [30], KERN [2], GPS-Net [19], BGNN [17], SQUAT [11]. Additionally, we conducted comparisons with model-agnostic SGG methods, which can be categorized into two groups: 1) Unbiased SGG from biased training: TDE [26] and DLFE [3]. These models perform unbiased inference from biased training, utilizing counterfactual causality and frequency estimation, respectively. 2) Re-balancing based method: CogTree [35], PCPL [31], Reweighting [3], and IETrans [38]. These methods employ re-balancing techniques such as a re-weighting-based approach or a re-labeling-based approach."}, {"title": "Quantitative Results", "content": "Table 1 shows the performance comparison with various SGG models on the VG dataset. We observe that DPL greatly improves the performance of the baseline models, demonstrating the effectiveness of our model-agnostic approach. Furthermore, when compared to unbiased SGG methods, DPL outperforms not only TDE and DLFE, which perform unbiased prediction from biased training, but also recent re-balancing methods that have been extensively researched. This suggests that despite biased training, achieving outstanding performance is possible by understanding semantic diversity of predicates and effectively integrating them during inference phase. Additionally, DPL significantly improves the performance of the baseline models on the GQA dataset (Table 2), indicating that DPL is a dataset-independent method."}, {"title": "4.3 Ablation Studies", "content": "We have two main hyperparameters, i.e., the number of samples N and the value of R in Lmatch. These hyperparameters are closely associated with the model's performance, and the related experimental results are presented in Table 3 and Table 4. We also conducted experiments to evaluate the effect of each component in DPL, which are presented in Table 6. These tables display the results of the PredCls task in the Motifs [37] baseline model on the VG dataset."}, {"title": "Influence of Hyperparameter N", "content": "First, an observation from Table 3 is that as N increases, the overall R@K value increases, while mR@K decreases. Especially, we can see a significant change in the performance of R@K, which mainly reflects the performance of head classes. This suggests that a large number of samples are required to capture the semantic diversity of the head classes. Conversely, even with N=1, the mR@K, which represents the performance of tail classes, is sufficiently high. This indicates that a smaller number of samples is enough to capture the semantic diversity of tail classes. Furthermore, due to the trade-off between R@K and mR@K, mR@K slightly decreases as N increases. This decrease is not significant compared to the increase in R@K because increasing N does not result in a loss of semantic diversity information for tail classes. Consequently, the overall F@K peaks when N is round 20."}, {"title": "The total sum of Ns of each predicate remains consistent across both cases, and", "content": "the results are presented in Table 5. Although the total number of samples used is the same in both cases, there is a significant difference in the F@K between Case 1 and Case 2. This is because the head classes are more affected by N, resulting in a greater difference in R@K than in mR@K. Consequently, the overall F@K is much higher in Case 1."}, {"title": "Influence of Hyperparameter R", "content": "Table 4 shows that performance varies depending on the value of R. When R is set to a small or large value, a decrease in performance is observed. This phenomenon occurs because a small R makes the model highly sensitive to even slight variations in relation features, potentially leading to overfitting. Conversely, when R is large, most relation features come within a distance of R from the samples, preventing the model from learning an appropriate variance."}, {"title": "Effectiveness of each component", "content": "Table 6 presents the results of comparing the influence of each component. A clear observation is that even when employing Lortho or Lmatch, their influence on performance improvement is minimal unless unbiased inference is employed. This signifies that even if the model learns a suitable variance of each predicate, the failure to incorporate it during inference prevents the ability to conduct unbiased SGG. The second observation is the significant impact of the orthogonal loss on learning an appropriate variance. Even when making unbiased inference without using the orthogonal loss, we"}, {"title": "4.4 Qualitative Analysis", "content": "Visualization of the semantic space. In Fig. 4, we visualize the representations of prototypes, relation features and samples. We have the following observations: 1) In Fig. 4(a), most of the relation features are indeed close to the prototype on, even though they are not labeled as on. Therefore, when utilizing the conventional Euclidean distance-based prediction, most relationships tend to be classified with the predicate on, indicating biased predictions. 2) Even among relation features labeled as on in Fig. 4(a), their semantics are diverse. For example, the semantic of on in <man, on, horse> is relatively close to riding, while that in <door, on, airplane> is close to attached to. Moreover, the semantic of on in <woman, on, sidewalk> is close to standing on or walking on. 3) In Fig. 4(b), we observe that the samples of each prototype can indeed identify regions near on that are in fact associated with the semantics of other predicates. For example, the right side of the prototype on overlaps with walking on, while upper side of the prototype on overlaps with riding. This validates that DPL indeed captures the diverse semantics that a single predicate may exhibit.\nComparisons with baselines. Fig. 5 compares the actual predicted distribution of Motifs [37], Motifs+reweight [3], and Motifs+DPL for two subject-object pairs. We have the following observations: 1) Although all three mod-"}, {"title": "5 Conclusion", "content": "In this paper, we tackle the problem of biased predictions caused by overlooking the semantic diversity of predicates. To this end, we proposed a model-agnostic Semantic Diversity-aware Prototype-based Learning (DPL) framework for unbiased SGG, aiming to identify the specific semantics of relationships that can be represented by the same predicate. Extensive experiments show that DPL not only significantly improves the performance of existing SGG models, but also yields reasonable qualitative results. There still remain limitations of DPL. Due to the small number of samples (i.e., N) used in this work, we fall short of learning precise regions in the semantic space. We expect further improvements in the performance when a large number of samples along with an appropriate R value are utilized, which we leave as future work."}]}