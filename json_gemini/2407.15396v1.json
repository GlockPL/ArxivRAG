{"title": "Semantic Diversity-aware Prototype-based Learning for Unbiased Scene Graph Generation", "authors": ["Jaehyeong Jeon", "Kibum Kim", "Kanghoon Yoon", "Chanyoung Park"], "abstract": "The scene graph generation (SGG) task involves detecting objects within an image and predicting predicates that represent the re- lationships between the objects. However, in SGG benchmark datasets, each subject-object pair is annotated with a single predicate even though a single predicate may exhibit diverse semantics (i.e., semantic diversity), existing SGG models are trained to predict the one and only predicate for each pair. This in turn results in the SGG models to overlook the semantic diversity that may exist in a predicate, thus leading to biased predictions. In this paper, we propose a novel model-agnostic Seman- tic Diversity-aware Prototype-based Learning (DPL) framework that enables unbiased predictions based on the understanding of the seman- tic diversity of predicates. Specifically, DPL learns the regions in the semantic space covered by each predicate to distinguish among the var- ious different semantics that a single predicate can represent. Extensive experiments demonstrate that our proposed model-agnostic DPL frame- work brings significant performance improvement on existing SGG mod- els, and also effectively understands the semantic diversity of predicates. The code is available at https://github.com/JeonJaeHyeong/DPL.git.", "sections": [{"title": "1 Introduction", "content": "Scene Graph Generation (SGG) aims to predict relationships between objects within an image and generate a structured graph, in which the nodes and edges denote objects and pair-wise relationships between two objects, respectively. Since this graph contains high-level information of the an image, it is widely used in various downstream tasks such as image captioning [1,28], visual question answering [7,33], and image retrieval [10].\nIn this study, we focus on an inherent limitation of existing benchmark datasets for the SGG task (e.g., Visual Genome (VG) [14]). That is, each subject- object pair in an image is annotated with only a single predicate, even though a single predicate may exhibit diverse semantics, which we refer to as semantic diversity."}, {"title": "2 Related work", "content": "2.1 Scene Graph Generation\nThe objective of SGG is to construct a graph that is useful for scene understand- ing by identifying the relationships between objects within an image. Early meth- ods [5,20,39] have overlooked the visual context and treated objects and pair- wise relationships independently. Subsequent SGG approaches propose models that utilize rich contextual information by employing sequential LSTMs [27,37], message passing [17, 30, 32, 34], or tree structure modelling [27]. Furthermore, [8, 24, 36] have attempt to overcome the limitations of SGG datasets by lever- aging external knowledge. Recently, PE-Net [40] attempted to obtain compact representations of entities and predicates using prototypes. However, these meth- ods still overlook the semantic diversity of predicates and struggle with biased prediction problems.\n2.2 Unbiased Scene Graph Generation\nTo address the biased prediction problem, numerous unbiased SGG works have been conducted. Similarly to [16], existing unbiased SGG methods can be broadly categorized into two groups: 1) Re-balancing based method: This involves in- creasing the number of tail classes or giving them more weight using techniques such as data augmentation [15], re-sampling [17], or re-weighting [31,35]. More- over, recent research has explored re-labeling [16,38], which involves transform- ing head classes into tail classes. These approaches can also be considered as re-balancing methods, as they involve adjusting the number of head and tail classes. 2) Unbiased prediction based on biased training: These models perform biased training but make unbiased predictions during inference. TDE [26] in- troduces a causal inference framework to eliminate the context bias during the inference process. DLFE [3] utilizes positive-unlabeled learning to recover unbi- ased probabilities. Our method belongs to the second category as we train the model to understand semantic diversity during biased training and apply this understanding during the inference phase."}, {"title": "3 Method", "content": "Our goal is to learn the regions related to each predicate in the semantic space, enabling us to recognize the diverse semantics that a single predicate can ex- press. To this end, we initially introduce prototypes to signify the representative semantics of predicates in the semantic space and train our model to minimize the distance between the relation feature and its ground truth class prototype (Section 3.2). At the same time, we utilize a sampling approach to capture the di- verse semantics represented by each predicate, contributing to our understanding of semantic diversity (Section 3.3). Lastly, we make use of the semantic diver- sity information captured by our model to address the inevitable bias caused by the long-tailed predicate class distribution during the inference phase, leading to unbiased prediction (Section 3.4)."}, {"title": "3.1 Preliminary", "content": "Given an image $I$, the objective of SGG is to generate a scene graph $G = {O, E}$, where $O$ and $E$ denote the set of objects and thier pairwise relationships, respectively. The conventional SGG is generally conducted based on the following pipeline.\nProposal Generation. All objects $O = {o_i}_{i=1}^{N_o}$ are identified in the image $I$ using a pre-trained object detector (e.g., Faster R-CNN [23]), where $N_o$ is the number of objects. Each object $o_i \\in O$ consists of a visual feature $v_i$, an object bounding box $b_i$, and an initial object label $l_i$.\nObject Class Prediction. Object features $x_i$ are extracted based on the out- put of a proposal (i.e., $v_i$, $b_i$ and $w_i$), and object classification is conducted.\n$$x_i = f_{obj}(v_i, b_i, w_i), \\ \\ \\ \\ \\hat{l}_i = \\phi_{obj} (x_i),$$\nwhere $w_i$ is the word embedding [22] of $l_i$, $f_{obj}$ is an object encoder such as BiLSTM [37] or fully connected layers [39], and $\\phi_{obj}$ is an object classifier. An updated object label $\\hat{l}_i$ is obtained in this procedure.\nPredicate Class Prediction. This step involves obtaining a refined object feature $\\hat{x_i}$ using the word embedding $\\hat{w_i}$ of $\\hat{l_i}$. Then, given objects $o_i$ and $o_j$, and their features $\\hat{x}_i$ and $x_j$ along with their union feature $u_{ij}$, we obtain its relation feature $r_{i \\rightarrow j}$ that for predicate classification.\n$$\\hat{x_i} = f_{rel}(v_i, \\hat{x_i}, \\hat{w_i}), \\ \\ \\ r_{i \\rightarrow j} = f_p([\\\n\\hat{x_i}, x_j]) \\odot u_{ij}, \\ \\ \\ p_{i \\rightarrow j} = \\phi_{rel}(r_{i \\rightarrow j}),$$\nwhere $\\odot$ denotes element-wise product, $f_{rel}$ is a relation encoder similar to $f_{obj}$, $f_p$ is fully-connected layer and $\\phi_{rel}$ is a predicate classifier. Moreover, $p_{i \\rightarrow j}$ is the predicted predicate between objects $o_i$ and $o_j$, which is selected from a predefined set of predicates $P$."}, {"title": "3.2 Prototype-based Biased Training", "content": "To handle representative semantics corresponding to each predicate, we first create prototypes $C = {c_1, c_2, ..., c_{|P|}}$ , where $c_i \\in R^d$ is the learnable prototype for the $i$-th predicate. Furthermore, we use a trainable projector $\\phi_{proj} (\\cdot)$ to align the relation feature $r$ with the same dimensional space as the prototype, i.e., $z = \\phi_{proj}(r)$, where $z \\in R^d$. The prototypes are $L_2$ normalized to ensure that they reside within a consistent range, i.e., $||c_i||_2 = 1$. In this section, we aim to train each relation feature $z$ to become closer to the prototype of its ground- truth class. Specifically, we compute the probability of the relation represented by $z$ belonging to the $i$-th predicate class based on the Euclidean distance [4]:\n$$p(i-\\text{th class} \\ | \\ z) = \\text{Softmax}(-a \\ ||z - c_i \\||_2 + b),$$"}, {"title": "3.3 Semantic Diversity Learning", "content": "Recall that a single predicate may exhibit diverse semantics of subject-object re- lationships. Therefore, it is necessary to understand the range that each predicate can represent and identify which parts within that range correspond to which"}, {"title": "3.4 Unbiased Inference using Normalization", "content": "During training, the prediction of predicate classes relies on the Euclidean dis- tance between the relation feature $z$ and its corresponding prototype $c_i$. However, relying solely on Euclidean distance leads to biased predictions and overlooks se- mantic diversity of predicates. Therefore, during inference, we compute the nor- malized distance using the semantic diversity information encapsulated within $\\sigma = [\\sigma_1, \\sigma_2, ..., \\sigma_d]$ as follows:\n$$p(i-\\text{th class} \\ | \\ z) = \\text{Softmax}(-a' \\ ||(z - c_j) \\odot \\sigma^{-1}\\||_2 + b),$$\n$$\\sigma_i = \\sqrt{\\frac{1}{\\sigma_i}},\\ \\ \\ \\text{and} \\ \\ \\ a' = a \\frac{\\sqrt{\\text{max}_j || z-c_j ||_2}}{\\text{max}_j || (z-c_j) \\odot \\sigma^{-1}||} \\ \\ \\ \\text{mitigates the scaling effect caused by}\\ \\sigma^{-1}$$"}, {"title": "4 Experiment", "content": "4.1 Experimental Settings\nDatasets. Experiments are conducted on two datasets: VG [14] and GQA [9]. VG contains 108k images across 75k object categories and 37k predicate cate- gories. Following previous studies on SGG [2, 3, 17, 19, 25], we adopt the most widely-used VG150 split [30], which contains the most frequent 150 object cate- gories and 50 predicate categories. GQA is another vision-language dataset that contains over 3.8 million relation annotations. In the case of GQA, we follow the GQA200 split [6], which select the 200 most frequent object categories and 100 most frequent predicate categories. For both datasets, we allocate 70% of the data to the training set and 30% to the testing set, and sample 5K images from the training set for validation.\nTasks. Following prior studies [30,37], we evaluate models on three conventional SGG tasks: 1) Predicate Classification (PredCls): Predicting predicate class based on ground-truth object bounding boxes and their object classes. This task is not affected by the performance of the object detector. 2) Scene Graph Classification (SGCls): Predicting both predicate classes and object classes, given the ground-truth bounding boxes. 3) Scene Graph Generation (SGGen): Detecting object bounding boxes and predicting both the object classes and pairwise predicate classes. The bounding box is considered valid one if the IoU is over 0.5.\nMetrics. For evaluation, we use Recall@K (R@K) and mean Recall@K (mR@K). Furthermore, following prior studies [12,13,21,38], we adopt the harmonic av- erage of R@K_and mR@K, denoted as F@K. Generally, mR@K tends to have lower values in comparison to R@K. This indicates that a simple average between them would be heavily influenced by R@K. Instead, F@K provides a relatively fair comparison between R@K and mR@K as it gives more weight to smaller values.\nImplementation Details. Following prior studies [26], we employ a pre-trained Faster R-CNN [23] with ResNeXt-101-FPN [29]. For the VG dataset, the pre- trained object detector provided by [26] is employed. For the GQA dataset, due to the absence of an officially available object detector, we pretrain a new ob- ject detector. Therefore, we cannot directly compare our performance with other studies [6,24] on the GQA dataset. The performance of the object detector used on the GQA dataset is 25.33 mAP. The initial learning rate is 0.01 and we use the SGD optimizer. The batch size is set to 3 on 1 GPU. For each of the three tasks, the training phase consists of 60,000 steps in total. Most of the hyperpa- rameters align with those used in prior studies. The hyperparameter a is set to 10, and the best performance is achieved when $N$ and $R$ are set to 20 and 1.0, respectively. The dimension size of the relation feature and prototype is set to 128 (i.e., d=128)."}, {"title": "4.2 Comparison with State-of-the-Arts", "content": "Baselines. To demonstrate the effectiveness of our model-agnostic approach, we applied our method to three baseline SGG models: VTransE [39], Motifs [37], and VCTree [27]. We compared our method with the state-of-the-art models, including IMP [30], KERN [2], GPS-Net [19], BGNN [17], SQUAT [11]. Addi- tionally, we conducted comparisons with model-agnostic SGG methods, which can be categorized into two groups: 1) Unbiased SGG from biased train- ing: TDE [26] and DLFE [3]. These models perform unbiased inference from biased training, utilizing counterfactual causality and frequency estimation, re- spectively. 2) Re-balancing based method: CogTree [35], PCPL [31], Re- weighting [3], and IETrans [38]. These methods employ re-balancing techniques such as a re-weighting-based approach or a re-labeling-based approach."}, {"title": "4.3 Ablation Studies", "content": "We have two main hyperparameters, i.e., the number of samples $N$ and the value of $R$ in $L_{match}$. These hyperparameters are closely associated with the model's performance, and the related experimental results are presented in Table 3 and Table 4. We also conducted experiments to evaluate the effect of each component in DPL, which are presented in Table 6. These tables display the results of the PredCls task in the Motifs [37] baseline model on the VG dataset.\nInfluence of Hyperparameter N. First, an observation from Table 3 is that as N increases, the overall R@K value increases, while mR@K decreases. Especially, we can see a significant change in the performance of R@K, which mainly reflects the performance of head classes. This suggests that a large number of samples are required to capture the semantic diversity of the head classes. Conversely, even with N=1, the mR@K, which represents the performance of tail classes, is sufficiently high. This indicates that a smaller number of samples is enough to capture the semantic diversity of tail classes. Furthermore, due to the trade- off between R@K and mR@K, mR@K slightly decreases as N increases. This decrease is not significant compared to the increase in R@K because increasing N does not result in a loss of semantic diversity information for tail classes. Consequently, the overall F@K peaks when N is round 20."}, {"title": "4.4 Qualitative Analysis", "content": "Visualization of the semantic space. In Fig. 4, we visualize the represen- tations of prototypes, relation features and samples. We have the following ob- servations: 1) In Fig. 4(a), most of the relation features are indeed close to the prototype on, even though they are not labeled as on. Therefore, when utilizing the conventional Euclidean distance-based prediction, most relationships tend to be classified with the predicate on, indicating biased predictions. 2) Even among relation features labeled as on in Fig. 4(a), their semantics are diverse. For exam- ple, the semantic of on in <man, on, horse> is relatively close to riding, while that in <door, on, airplane> is close to attached to. Moreover, the semantic of on in <woman, on, sidewalk> is close to standing on or walking on. 3) In Fig. 4(b), we observe that the samples of each prototype can indeed identify regions near on that are in fact associated with the semantics of other predicates. For example, the right side of the prototype on overlaps with walking on, while upper side of the prototype on overlaps with riding. This validates that DPL indeed captures the diverse semantics that a single predicate may exhibit.\nComparisons with baselines. Fig. 5 compares the actual predicted distri- bution of Motifs [37], Motifs+reweight [3], and Motifs+DPL for two subject- object pairs. We have the following observations: 1) Although all three mod-"}, {"title": "5 Conclusion", "content": "In this paper, we tackle the problem of biased predictions caused by overlooking the semantic diversity of predicates. To this end, we proposed a model-agnostic Semantic Diversity-aware Prototype-based Learning (DPL) framework for un- biased SGG, aiming to identify the specific semantics of relationships that can be represented by the same predicate. Extensive experiments show that DPL not only significantly improves the performance of existing SGG models, but also yields reasonable qualitative results. There still remain limitations of DPL. Due to the small number of samples (i.e., N) used in this work, we fall short of learning precise regions in the semantic space. We expect further improvements in the performance when a large number of samples along with an appropriate R value are utilized, which we leave as future work."}]}